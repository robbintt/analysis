---
ver: rpa2
title: Group-realizable multi-group learning by minimizing empirical risk
arxiv_id: '2601.16922'
source_url: https://arxiv.org/abs/2601.16922
tags:
- learning
- multi-group
- sample
- complexity
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies multi-group learning under the assumption of
  group-realizability, where for each subpopulation (group) there exists a benchmark
  classifier with zero error on that group. The paper introduces the class of group-realizable
  concepts (CG,H), consisting of functions consistent with the group-realizability
  assumption.
---

# Group-realizable multi-group learning by minimizing empirical risk

## Quick Facts
- **arXiv ID:** 2601.16922
- **Source URL:** https://arxiv.org/abs/2601.16922
- **Reference count:** 6
- **Primary result:** ERM over group-realizable concepts achieves O(1/ε) sample complexity in group-realizable settings, even with infinite VC dimension, but finding consistent classifiers is NP-hard.

## Executive Summary
This paper studies multi-group learning under the group-realizability assumption, where each subpopulation (group) has a benchmark classifier with zero error. The authors introduce the class of group-realizable concepts (C_G,H) and show that empirical risk minimization (ERM) over this class achieves improved sample complexity bounds. Specifically, the sample complexity is O((d_G,H + d_G) log(1/γε) + log(1/δ))/(γε), where d_G,H is the maximum VC dimension of H restricted to any group, d_G is the VC dimension of G, and γ is a lower bound on the probability mass of each group. This improves upon prior work which had a dependence on log|G|. However, the paper demonstrates that finding a consistent classifier in C_G,H is NP-hard, even when G has polynomial size and H is efficiently optimizable. The authors suggest improper learning approaches as a computational workaround while maintaining statistical efficiency.

## Method Summary
The paper proposes learning from data by minimizing empirical risk over the class of group-realizable concepts C_G,H, where each group g ∈ G is consistent with some h ∈ H. The method leverages the group-realizability assumption to achieve improved sample complexity. While ERM over C_G,H is statistically efficient, it is computationally intractable in general due to NP-hardness. As an alternative, the authors suggest improper learning approaches that combine per-group hypotheses via online-to-batch conversion, which can be more computationally tractable while maintaining statistical efficiency in some cases.

## Key Results
- ERM over C_G,H achieves O(1/ε) sample complexity even when C_G,H has infinite VC dimension
- Sample complexity scales as O((d_G,H + d_G) log(1/γε) + log(1/δ))/(γε), improving over O(d_H log(1/γε) + log(|G|))/(γε)
- Finding c ∈ C_G,H consistent with labeled data is NP-hard even when G has polynomial size and H is efficiently optimizable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ERM over C_G,H achieves O(1/ε) sample complexity even when C_G,H has infinite VC dimension.
- **Mechanism:** The "mistake behaviors" counted during evaluation are restricted to individual groups g ∈ G. On any group, a concept c ∈ C_G,H behaves according to some h_g ∈ H, so the effective shattering coefficient is bounded by S_g(H, 2n) rather than the full richness of C_G,H. This is captured by defining F := {x ↦ g(x)(c△c*)(x) | c ∈ C_G,H} and showing |F|_X ≤ S_g(H, 2n).
- **Core assumption:** Group-realizability holds: for each g ∈ G, ∃h*_g ∈ H with err(h*_g|g) = 0.
- **Evidence anchors:**
  - [abstract] "improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension"
  - [section 3, page 7] "the full 'richness' of C_G,H is never encountered because the 'mistake behaviors' that are counted are always restricted to individual groups"
  - [corpus] Weak/missing — corpus neighbors don't address this specific restricted evaluation phenomenon.
- **Break condition:** If evaluation is not restricted to groups (e.g., arbitrary test distributions), the infinite VC dimension of C_G,H would cause ERM to fail.

### Mechanism 2
- **Claim:** Sample complexity scales as O((d_G,H + d_G) log(1/γε) + log(1/δ))/(γε), improving over O(d_H log(1/γε) + log(|G|))/(γε).
- **Mechanism:** The Vapnik-Chervonenkis lemma applied to F := {x ↦ g(x)(c△c*)(x) | g ∈ G, c ∈ C_G,H} yields shattering coefficient bounded by S_X(G, 2n) · sup_{g∈G} S_g(H, 2n). Sauer's lemma converts this to binomial coefficients in d_G and d_G,H := sup_{g∈G} d_{g,H}.
- **Core assumption:** Both G and H have finite VC dimension.
- **Evidence anchors:**
  - [abstract] "even when the family of groups is infinite so long as it has finite VC dimension"
  - [section 3, page 6] Equation (4) derives sample size n ≥ C·(d_G,H + d_G) log(1/γε) + log(1/δ))/(γε)
  - [corpus] "Sample Complexity of Agnostic Multiclass Classification" discusses related VC dimension bounds — provides background context.
- **Break condition:** If d_G or d_G,H is infinite, or γ (minimum group probability) → 0, sample bounds become vacuous.

### Mechanism 3
- **Claim:** Finding c ∈ C_G,H consistent with labeled data is NP-hard even when ERM over H is polynomial-time and |G| is polynomial.
- **Mechanism:** Reduction from ONE-IN-THREE 3SAT. Each clause maps to a group; hypotheses correspond to truth assignments. Hardness arises from requiring hypotheses assigned to different groups to agree on intersections g ∩ g'. This global consistency constraint creates combinatorial explosion despite local optimization being tractable.
- **Core assumption:** Standard complexity assumption P ≠ NP.
- **Evidence anchors:**
  - [abstract] "Implementing this approach is also shown to be computationally intractable"
  - [section 4, pages 7-9] Full NP-hardness reduction construction with soundness/completeness proof.
  - [corpus] Weak/missing — corpus doesn't address this specific computational hardness.
- **Break condition:** If groups are disjoint (no intersections), C_G,H construction becomes trivial — assign any h_g independently per group.

## Foundational Learning

- **Concept: VC Dimension and Shattering Coefficients**
  - Why needed: Central to understanding why ERM works despite C_G,H having infinite VC dimension. The proof bounds shattering coefficients of mistake behaviors restricted to groups.
  - Quick check question: Can you derive why S_X(F, 2n) ≤ (2n choose ≤d_G) · sup_g(2n choose ≤d_{g,H}) for the function class F in Theorem 4?

- **Concept: Multi-Group Learning Objective**
  - Why needed: Understanding that different groups may have different optimal classifiers h*_g that disagree at intersections g ∩ g', making the problem fundamentally different from single-group learning.
  - Quick check question: Why might no single h ∈ H satisfy err(h|g) ≤ inf_{h'∈H} err(h'|g) + ε for all g ∈ G simultaneously?

- **Concept: Realizability vs Agnostic Settings**
  - Why needed: The paper exploits group-realizability to achieve 1/ε dependence rather than 1/ε², analogous to the gap in standard PAC learning.
  - Quick check question: What sample complexity difference exists between realizable and agnostic settings, and how does group-realizability enable the improvement here?

## Architecture Onboarding

- **Component map:**
  - C_G,H: Set of functions c where each g ∈ G is consistent with some h ∈ H (infinite VC dim possible; NP-hard to construct)
  - ERM Oracle: Finds c ∈ C_G,H consistent with labeled data (intractable in general)
  - Improper Ensemble Alternative: Tosh-Hsu Algorithm 2 combining per-group hypotheses via online-to-batch conversion

- **Critical path:**
  1. Verify group-realizability holds for (G, H, D)
  2. Compare log(|G|) vs d_G log(1/γε) to assess whether proper learning's sample advantage matters
  3. If proper learning required: attempt ERM over C_G,H (expect exponential worst-case)
  4. If computational feasibility needed: use improper ensemble approach with ĥ_g ∈ H per group

- **Design tradeoffs:**
  - Proper (C_G,H): Better sample complexity when log(|G|) ≫ d_G log(1/γε); NP-hard
  - Improper (ensemble): Polynomial time when |G| is polynomial and consistent h ∈ H are easy to find; may need more samples

- **Failure signatures:**
  - Group-realizability violated: C_G,H inappropriate — classifiers may not exist (see page 5 counterexample)
  - Small groups (γ → 0): Sample requirements explode for both approaches
  - Attempting C_G,H ERM on large problems: Expect timeout/exponential runtime

- **First 3 experiments:**
  1. **Disjoint groups sanity check:** Construct G with non-overlapping subsets; verify ERM over C_G,H is tractable and matches Theorem 4 bounds.
  2. **NP-hardness replication:** Implement Section 4's ONE-IN-THREE 3SAT reduction; verify exponential scaling on small CNF instances.
  3. **Improper baseline comparison:** Compare sample complexity of Tosh-Hsu ensemble vs direct ERM over C_G,H on synthetic group-realizable data where C_G,H can be brute-forced for small n.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is there a general oracle-efficient multi-group learning algorithm that achieves the sample complexity O((d_G,H + d_G) log(1/γε) + log(1/δ))/(γε) in the group-realizable setting?
- **Basis in paper:** [explicit] Section 6 states: "A problem left open is to find a general oracle-efficient multi-group learning algorithm that achieves the sample complexity from (4) in the group-realizable setting."
- **Why unresolved:** The ERM approach over C_G,H achieves optimal sample complexity but is NP-hard. The improper learning approach via ensemble methods is computationally tractable in some cases but has sample complexity scaling with log(|G|) rather than d_G.
- **What evidence would resolve it:** An algorithm with access only to ERM oracles for H and G that achieves the sample complexity bound (4) and runs in polynomial time.

### Open Question 2
- **Question:** Can the statistical efficiency of ERM over C_G,H extend to the general (agnostic) multi-group learning setting with a suitable relaxation of C_G,H?
- **Basis in paper:** [explicit] Section 6 states this "seems worthy of further investigation in other settings, including general (agnostic) multi-group learning with a suitable relaxation of C_G,H."
- **Why unresolved:** The class C_G,H is defined specifically for the group-realizable setting and is not appropriate for agnostic settings where group-realizability may fail.
- **What evidence would resolve it:** A modified concept class and corresponding sample complexity analysis for the agnostic setting achieving improved rates over existing O(1/ε²) guarantees.

### Open Question 3
- **Question:** Are there structural conditions on (G, H) under which ERM over C_G,H becomes computationally tractable, despite the general NP-hardness?
- **Basis in paper:** [inferred] The NP-hardness reduction in Section 4 uses a specific construction where G has polynomial size and H permits efficient ERM. This suggests hardness is not solely from H or G complexity, but it remains unclear what structural properties might enable tractability.
- **Why unresolved:** The paper proves general hardness but does not characterize the boundary between tractable and intractable instances.
- **What evidence would resolve it:** Identification of non-trivial sufficient conditions on G and H (beyond trivial cases like disjoint groups) guaranteeing polynomial-time algorithms for finding consistent concepts in C_G,H.

### Open Question 4
- **Question:** Can improper learning approaches be extended to handle infinite families of groups G with finite VC dimension while maintaining statistical efficiency?
- **Basis in paper:** [inferred] Section 5's improper learning approach requires enumerating G ("For each g ∈ G"), which is infeasible when G is infinite. The main statistical result handles infinite G, but the computational workaround does not.
- **Why unresolved:** The ensemble method relies on explicitly assigning hypotheses to each group, requiring group enumeration.
- **What evidence would resolve it:** An oracle-efficient improper learning algorithm that achieves near-optimal sample complexity for infinite G using only ERM oracles for H and G without enumeration.

## Limitations

- **NP-hardness barrier:** Finding consistent classifiers in C_G,H is computationally intractable in general, creating a gap between statistical and computational efficiency.
- **Group-realizability requirement:** The approach assumes group-realizability holds; if this assumption is violated, C_G,H may not contain useful classifiers.
- **Small group vulnerability:** When minimum group probability γ approaches zero, sample complexity requirements become prohibitive for both proper and improper approaches.

## Confidence

- **High:** The statistical sample complexity bound O((d_G,H + d_G) log(1/γε) + log(1/δ))/(γε) is mathematically proven and the mechanism (restricted evaluation to groups) is well-explained.
- **Medium:** The NP-hardness reduction is correctly constructed, but the practical impact depends on problem structure. For many real-world applications, the exponential worst-case may not materialize.
- **Medium:** The improper learning approach inherits guarantees from Tosh & Hsu (2021), but implementation details are not fully specified in this paper.

## Next Checks

1. **Empirical scaling verification:** Generate synthetic group-realizable data with controlled VC dimensions and group structures. Measure whether ERM over C_G,H achieves the predicted O(1/ε) sample complexity scaling rather than O(1/ε²).

2. **Computational hardness evaluation:** Implement the ONE-IN-THREE 3SAT reduction on small instances (≤ 20 variables). Track runtime scaling to empirically verify exponential growth in finding consistent classifiers.

3. **Improper vs proper comparison:** For a range of group structures (disjoint, overlapping, polynomial-sized), compare the sample complexity of the Tosh-Hsu ensemble approach against the theoretical bound for proper learning over C_G,H.