---
ver: rpa2
title: Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search
arxiv_id: '2509.25420'
source_url: https://arxiv.org/abs/2509.25420
tags:
- reward
- reasoning
- search
- arxiv
- execution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dual-phase test-time reasoning framework
  that explicitly separates planning and execution, enabling separate search and reward
  evaluation for each phase. The approach introduces a dynamic budget allocation mechanism
  that reallocates computation based on reward feedback, allowing early stopping on
  confident steps and focusing resources on harder ones.
---

# Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search

## Quick Facts
- arXiv ID: 2509.25420
- Source URL: https://arxiv.org/abs/2509.25420
- Authors: Yingqian Cui; Zhenwei Dai; Pengfei He; Bing He; Hui Liu; Xianfeng Tang; Jingying Zeng; Suhang Wang; Yue Xing; Jiliang Tang; Benoit Dumoulin
- Reference count: 40
- Primary result: Dual-phase test-time reasoning with adaptive budget allocation improves accuracy-efficiency trade-offs on GSM8K, MATH, HumanEval, and MBPP benchmarks.

## Executive Summary
This paper introduces a dual-phase test-time reasoning framework that explicitly separates planning and execution into distinct search phases, each with its own reward model. The approach enables separate search and reward evaluation for planning candidates and execution candidates, preventing the discarding of correct plans with incorrect executions. A dynamic budget allocation mechanism reallocates computation based on reward feedback, allowing early stopping on confident steps and focusing resources on harder ones. Experiments demonstrate consistent improvements in accuracy-efficiency trade-offs compared to standard beam search and prior reward-model-based methods, with strong generalization to unseen datasets.

## Method Summary
The method implements a dual-phase beam search where planning and execution are separated into distinct search spaces. For each reasoning step, the system first samples N₁ planning candidates and scores them with a planning reward model (PRM_plan), selecting the top n₁ candidates. Only then does it sample N₂ execution candidates per selected plan, scoring them with an execution reward model (PRM_exec) and selecting the top n₂. Dynamic budget allocation uses threshold-based early stopping (τ₁) and additional sampling (τ₂, m) to adaptively redistribute computational resources. Reward models are trained on rollout-based annotations where steps are labeled positive if any continuation reaches the correct answer. The framework is evaluated on mathematical reasoning (GSM8K, MATH) and code generation (HumanEval, MBPP) benchmarks.

## Key Results
- DREAM achieves superior accuracy-efficiency trade-offs compared to standard beam search and majority voting on GSM8K, MATH, HumanEval, and MBPP benchmarks
- Dynamic budget allocation provides additional gains, with GSM8K showing ~80% early stopping rate versus MATH's ~5% due to dataset difficulty differences
- DREAM generalizes well to out-of-distribution datasets (AMC23) when trained on GSM8K+MATH
- Larger reward models (32B vs 7B) provide better accuracy-efficiency trade-offs at the cost of higher inference computation

## Why This Works (Mechanism)

### Mechanism 1
Separating planning and execution into distinct search phases improves accuracy-efficiency trade-offs by enabling separate exploration and pruning. Planning candidates are sampled and scored by PRM_plan before any execution begins, preventing the discarding of correct plans with incorrect executions and avoiding wasted budget on flawed plans. This assumes planning and execution errors are at least partially independent, allowing non-redundant exploration of separate search spaces.

### Mechanism 2
Threshold-based dynamic budget allocation improves efficiency by enabling early stopping on confident steps and reallocating computation to uncertain steps. At each phase, sampling proceeds until either enough candidates exceed a high threshold (triggering early stopping) or all candidates fall below a low threshold (triggering additional sampling). This reallocates computation from easy steps (high confidence early) to hard steps (low confidence requires more samples), assuming reward scores reliably proxy for step difficulty.

### Mechanism 3
The synergy between dual-phase search and dynamic budget allocation is greater than applying either component alone. Dual-phase search reduces wasted computation by pruning bad plans early (before execution cost is incurred), while dynamic budget allocation captures these savings and reallocates them to harder steps within each phase. Together, they enable both finer-grained pruning and more efficient resource distribution, assuming computational savings from dual-phase pruning are meaningful and non-trivially reallocated.

## Foundational Learning

- **Beam Search with Process Reward Models (PRMs)**: Understanding how PRMs score intermediate steps and how beam search retains top candidates is prerequisite to grasping dual-phase modifications. Quick check: Can you explain how a PRM differs from an outcome reward model (ORM), and why PRMs enable earlier pruning in tree search?

- **Plan-Execution Reasoning Format**: The dual-phase decomposition assumes reasoning steps can be structured as (plan, execution) pairs. Understanding this format (e.g., least-to-most prompting) is necessary to implement the decomposition correctly. Quick check: Given a math word problem, can you decompose a solution step into a high-level plan ("what to do") and a concrete execution ("how to compute it")?

- **Rollout-Based Step Annotation**: DREAM's reward models are trained using rollout-based labeling (generate continuations from each step; label positive if any continuation reaches correct answer). Understanding this annotation strategy is critical for reproducing the reward model training. Quick check: Why might rollout-based annotation be more informative than single-trajectory labeling for training process reward models?

## Architecture Onboarding

- **Component map**: Reasoning Model -> Planning Reward Model -> Execution Reward Model -> Budget Controller -> Beam Manager -> Trajectory Selection

- **Critical path**:
  1. Generate diverse trajectories → rollout-based annotation → fine-tune PRM_plan and PRM_exec
  2. For each step: sample N₁ plans → score with PRM_plan → select top n₁ → sample N₂ executions per plan → score with PRM_exec → select top n₂
  3. Apply threshold-based early stopping and additional sampling; update remaining budget
  4. Return highest-reward path after max steps or beam convergence

- **Design tradeoffs**:
  - Reward Model Size: 32B provides better accuracy-efficiency trade-off than 7B but incurs higher inference cost
  - Threshold Tuning: High τ₁ enables aggressive early stopping but may miss good candidates; low τ₂ enables more exploration but increases cost
  - Beam Width vs. Budget: Larger beam improves coverage but multiplies compute; dual-phase design splits budget across two phases per step

- **Failure signatures**:
  - Reward Model Miscalibration: Check calibration on held-out validation set if consistently low rewards on correct steps or high rewards on incorrect steps
  - Early Stopping Too Aggressive: If many steps stop early but final accuracy degrades, τ₁ may be too high
  - No Early Stopping Triggered: If nearly all steps exhaust full budget, τ₁ may be too high relative to reward distribution
  - Planning-Execution Mismatch: If selected plans are syntactically correct but semantically incompatible with execution format, prompt engineering may need adjustment

- **First 3 experiments**:
  1. Run DREAM vs. standard beam search vs. majority vote on GSM8K with LLaMA-3-8B-Instruct. Plot accuracy vs. token budget curves.
  2. Run DREAM vs. DREAM(+) on both GSM8K and MATH. Quantify % of steps triggering early stopping vs. additional sampling.
  3. Train reward model on GSM8K+MATH training sets, then evaluate DREAM on AMC23. Compare accuracy vs. majority vote.

## Open Questions the Paper Calls Out

### Open Question 1
Can the dual-phase search framework effectively generalize to reasoning domains beyond mathematical problem solving and code generation? The authors state the method can generalize to a wide range of reasoning tasks provided they fit the plan-execution format, but only evaluate on math and code. Successful application to other structured reasoning tasks like logical deduction or agentic planning would resolve this.

### Open Question 2
How sensitive is the dynamic budget allocation mechanism to the specific values of the early-stopping and additional-sampling thresholds? Section 3.1 notes thresholds are set equal for planning and execution "for simplicity," leaving optimal strategy undefined. A sensitivity analysis measuring performance changes when thresholds are decoupled or varied independently across different difficulty levels would resolve this.

### Open Question 3
Does the relative benefit of DREAM diminish as the intrinsic capability of the base LLM increases? Section 4.1 mentions using models with "moderate ability" to demonstrate test-time scaling, leaving interaction with stronger models unexplored. Benchmark results comparing performance gaps between DREAM and standard beam search when applied to significantly larger or more capable base models would resolve this.

## Limitations

- The dual-phase decomposition assumes planning and execution errors are largely independent, but provides limited empirical evidence beyond qualitative improvements in accuracy-efficiency trade-offs
- Dynamic budget allocation relies heavily on reward scores being reliable proxies for step difficulty without presenting calibration analysis showing correlation with actual step correctness
- The synergy claim between dual-phase search and dynamic budget allocation is primarily supported by aggregate performance improvements rather than component-wise ablation studies

## Confidence

**High Confidence**: The empirical results showing DREAM outperforms standard beam search and majority voting on accuracy-efficiency trade-offs are well-supported by presented experiments with clearly specified baseline implementations.

**Medium Confidence**: The claim that dual-phase search enables separate exploration and pruning of plans and executions is theoretically sound but lacks rigorous ablation evidence showing advantages beyond what unified reward-guided search could achieve.

**Low Confidence**: The synergy between dual-phase search and dynamic budget allocation is the weakest claim, as it relies on aggregate performance improvements without component-wise analysis demonstrating complementary benefits.

## Next Checks

1. Run DREAM without dynamic budget allocation (fixed beam widths) and with dynamic allocation but without dual-phase separation (single reward model applied to unified search space). Compare accuracy-efficiency curves to quantify independent contributions and test for synergistic benefits.

2. For both planning and execution reward models, compute calibration curves showing the relationship between predicted reward scores and actual step correctness rates across different difficulty levels. Analyze whether high-reward candidates are systematically more likely to be correct.

3. Design an experiment to test the independence assumption by measuring the correlation between planning errors and execution errors in the unified baseline approach versus the dual-phase approach. Quantify how often the dual-phase method successfully prunes bad plans before execution cost is incurred.