---
ver: rpa2
title: 'From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based
  Vehicle Classification with Potential UAV Applications'
arxiv_id: '2506.22360'
source_url: https://arxiv.org/abs/2506.22360
tags:
- resnet34
- dataset
- performance
- noise
- event-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ResNet34 and ViT B16 for event-based vehicle
  classification using the GEN1 dataset, focusing on their performance under clean
  and noisy conditions. ResNet34 achieves 88% accuracy, while ViT B16 reaches 86%
  accuracy on clean data.
---

# From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications

## Quick Facts
- arXiv ID: 2506.22360
- Source URL: https://arxiv.org/abs/2506.22360
- Authors: Nouf Almesafri; Hector Figueiredo; Miguel Arana-Catania
- Reference count: 17
- Primary result: ResNet34 achieves 88% accuracy on clean GEN1 data, ViT B16 achieves 86% but shows superior robustness to event shifts and noise

## Executive Summary
This study evaluates ResNet34 and ViT B16 for event-based vehicle classification using the GEN1 dataset, focusing on their performance under clean and noisy conditions. ResNet34 achieves 88% accuracy, while ViT B16 reaches 86% accuracy on clean data. ViT B16 demonstrates greater robustness to noise, particularly event shifts, compared to ResNet34, which shows sharper accuracy declines under similar conditions. Both models are fine-tuned using EST representations of event data, and their performance is assessed across varying noise levels. The findings highlight ViT B16’s potential for real-world applications in dynamic environments, such as UAVs, where noise resilience is critical.

## Method Summary
The study compares ResNet34 and ViT B16 for binary classification (cars vs. pedestrians) on the GEN1 event-based dataset. Event data is converted to Event Spike Tensors (EST) voxel grids (9x240x304, cropped to 18x224x224). Both models use ImageNet pre-training and are fine-tuned with Adam optimizer. ResNet34 trains for 35 epochs with batch size 8, while ViT B16 trains for 10 epochs with batch size 10. Synthetic noise is injected during validation including event shifts, loss, and polarity reversal. The EST preprocessing enables standard frame-based architectures to process asynchronous event streams effectively.

## Key Results
- ResNet34 achieves 88% accuracy on clean GEN1 data
- ViT B16 achieves 86% accuracy on clean data but shows superior robustness to event shift noise
- ResNet34 accuracy drops more sharply under noise conditions compared to ViT B16
- Both models demonstrate sufficient accuracy for real-world deployment with ViT showing advantages in noisy environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting sparse, asynchronous event streams into dense Event Spike Tensors (EST) allows standard architectures designed for frame-based vision to process temporal event data effectively.
- **Mechanism:** The EST representation maps discrete events $(x, y, t, p)$ into a voxel grid structure. This process aggregates temporal windows of activity into a format that preserves spatial locality and temporal polarity, effectively "translating" neuromorphic data into a language understandable by standard CNNs (ResNet34) and Vision Transformers (ViT B16) without requiring specialized spiking neural networks.
- **Core assumption:** The structural information required for classification (vehicle vs. pedestrian) is preserved during the voxelization process and is not lost in the accumulation window.
- **Evidence anchors:**
  - [Section II.C]: Describes the EST pipeline mapping events into a multidimensional tensor that retains "temporal resolution and spatial locality."
  - [Section II.F]: Notes that preprocessing raw data into EST frames allowed fine-tuning of standard architectures like ResNet34 and ViT B16.
  - [Corpus]: The paper "UEOF" (arXiv:2601.10054) similarly relies on event-to-frame conversion methods for standard vision tasks, reinforcing the validity of this representation approach.
- **Break condition:** If the temporal accumulation window is too wide, fast-moving objects may blur indistinguishably; if too narrow, the voxel grid may be too sparse for meaningful feature extraction.

### Mechanism 2
- **Claim:** Vision Transformers (ViT B16) exhibit greater robustness to spatial noise (specifically coordinate shifts) than ResNet34 due to differences in feature aggregation mechanisms.
- **Mechanism:** ResNet relies on local convolutional kernels that are strictly translation-equivariant; precise spatial alignment is critical. ViT partitions the image into patches and uses global self-attention, allowing it to weigh relationships between distant patches. The paper suggests this global context allows ViT to maintain classification performance even when individual event coordinates are randomly shifted, whereas ResNet’s local filters effectively receive corrupted input.
- **Core assumption:** The self-attention mechanism creates a representation that is less sensitive to local coordinate jitter than the hierarchical local feature extraction of ResNet.
- **Evidence anchors:**
  - [Abstract]: States "ViT B16 demonstrates greater robustness to noise, particularly event shifts... ResNet34... shows sharper accuracy declines."
  - [Section III.D]: Notes that under X and Y shifts, "ResNet34’s accuracy decreases more sharply... indicating that ResNet34 is more sensitive to event shifts."
  - [Corpus]: "Vision Transformers: the threat of realistic adversarial patches" (arXiv:2509.21084) discusses ViT robustness properties, though specific mechanisms for event noise remain under-explored in the provided corpus.
- **Break condition:** If the spatial shift exceeds the patch size significantly or distorts the object shape beyond recognition, even the global attention mechanism will fail to correlate features.

### Mechanism 3
- **Claim:** Transfer learning from large-scale frame-based datasets (ImageNet) provides a sufficient initialization for event-based classification tasks, even when the target domain differs significantly in modality.
- **Mechanism:** Both models initialize with ImageNet weights. Despite the domain gap between RGB frames and event-based tensors, the low-level edge and texture detectors learned by the models appear to generalize. The fine-tuning process adapts these generic features to the specific "motion contours" present in event data.
- **Core assumption:** Low-level geometric features learned on static images are transferrable to the sparse, dynamic representations of event cameras.
- **Evidence anchors:**
  - [Section II.F]: Both models were "initialized with weights pre-trained on ImageNet" and adapted for the binary classification task.
  - [Section III.B]: ViT B16 showed "competitive results... particularly given its pre-training on a smaller dataset," implying the pre-trained weights compensated for data limitations.
- **Break condition:** If the event representation (EST) differs too drastically from the statistical distribution of ImageNet images (e.g., purely noise), the pre-trained weights may offer no advantage over random initialization.

## Foundational Learning

- **Concept:** **Event-Based Vision (Neuromorphic Sensing)**
  - **Why needed here:** Unlike standard cameras that capture frames at fixed intervals, event cameras only log changes in brightness (pixels turning on/off). Understanding this asynchronous, sparse nature is required to grasp why specialized "EST" preprocessing is necessary before using standard models.
  - **Quick check question:** Does an event camera record a full image if the scene is completely static? (Answer: No).

- **Concept:** **Event Spike Tensor (EST) / Voxel Grid**
  - **Why needed here:** Deep learning models generally expect grid-like inputs (tensors). EST is the bridge converting unstructured event streams into a structured format that encodes time and polarity.
  - **Quick check question:** What three pieces of information are typically encoded in an event tensor? (Answer: Spatial coordinates x/y, timestamp t, and polarity p).

- **Concept:** **ViT Patch Embedding vs. CNN Kernels**
  - **Why needed here:** The core comparison in the paper relies on how these architectures view the image. CNNs look at small local areas (kernels) sliding over the image. ViTs chop the image into larger "patches" (16x16 here) and treat them as a sequence.
  - **Quick check question:** Why might a ViT handle a "shifted" event better than a CNN based on the paper's findings? (Answer: ViT uses global self-attention across patches rather than strictly local convolution).

## Architecture Onboarding

- **Component map:** GEN1 Raw Events -> EST Representation (Voxel Grid) -> Crop (224x224) -> ResNet34/ResNet34 (Conv layers -> Residual Blocks -> FC Layer) or ViT B16 (Patch Embedding -> Transformer Encoder -> MLP Head) -> Training (Adam + CrossEntropy Loss + Noise Injection) -> Binary Class Output

- **Critical path:** The configuration of the **EST Voxel Dimension** and the **Input Adaptation Layer**.
  - ResNet34 requires the first convolutional layer to accept the specific channel depth of the EST tensor (18 channels in this study, derived from the 9x2x2 crop dimensions mentioned or similar EST config) rather than standard RGB (3 channels).
  - ViT B16 requires the Linear Projection layer to accept the flattened patches of the event tensor.

- **Design tradeoffs:**
  - **ResNet34:** Higher clean accuracy (88%) and faster/easier to train on small datasets, but brittle to spatial noise (accuracy drops sharply with event shifts).
  - **ViT B16:** Lower clean accuracy (86%) and typically requires more data, but demonstrates superior resilience to noise and spatial distortions critical for real-world UAV/automotive deployment.

- **Failure signatures:**
  - **Overfitting:** Validation loss fluctuates or rises while training loss decreases (observed in ResNet34 training curves).
  - **Noise Sensitivity:** Sudden accuracy collapse (>10% drop) when introducing low levels of spatial shift, indicating the model learned rigid spatial features rather than robust shapes.

- **First 3 experiments:**
  1. **Baseline EST Validation:** Train ResNet34 on clean GEN1 data using the specified EST parameters to reproduce the ~88% benchmark; verify the input layer correctly handles the event tensor channels.
  2. **Shift Robustness Test:** Apply the "Events Shift" noise (Eq. 1) to the validation set at 10% intensity. Compare the accuracy delta between ResNet34 and ViT B16 to confirm the Transformer's robustness claim.
  3. **Polarity Sensitivity Check:** Run inference with "Polarity Reversal" noise to see if the model relies on the sign of the brightness change (On/Off) or just the magnitude of activity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does pre-training on large-scale, event-specific datasets improve the clean accuracy and noise robustness of ViT B16 relative to ResNet34?
- Basis in paper: [explicit] The authors identify the "availability of large-scale datasets for pre-training ViTs" as a limitation and explicitly recommend future research explore "larger datasets" to improve performance.
- Why unresolved: The current study relies on ImageNet pre-training and fine-tuning on the limited GEN1 dataset, potentially underutilizing the ViT's capacity for data scaling.
- What evidence would resolve it: A comparative study where both architectures are pre-trained on a massive, large-scale event-based dataset before fine-tuning.

### Open Question 2
- Question: Do the observed noise robustness advantages of ViT B16 persist in real-world UAV environments with aerial-specific motion dynamics?
- Basis in paper: [explicit] The paper concludes that the methodologies "hold significant promise for adaptation to UAV contexts" but currently rely on the automotive-focused GEN1 dataset.
- Why unresolved: Ground-based vehicle classification involves different motion patterns and noise profiles compared to aerial dynamics (e.g., UAV vibration, altitude changes), which have not yet been tested.
- What evidence would resolve it: Evaluation of the fine-tuned models on event-based data captured from UAV platforms in dynamic flight scenarios.

### Open Question 3
- Question: How does the robustness of these architectures differ when subjected to real sensor noise compared to the simulated mathematical noise models used in this study?
- Basis in paper: [inferred] The methodology relies on simulated noise (uniform distributions for shifts, loss, and polarity) rather than intrinsic hardware noise.
- Why unresolved: Simulated noise may not accurately reflect the complex, correlated artifacts produced by physical event sensors in extreme environments.
- What evidence would resolve it: Benchmarking the models against datasets containing unmodified, real-world sensor noise and hardware failures.

## Limitations

- **EST implementation details are underspecified:** The specific binning logic or temporal normalization for creating the 18-channel tensor is not described.
- **Channel mapping ambiguity:** The conversion from 18 EST channels to 3 channels for ViT B16 input is unclear.
- **Noise injection implementation:** The mathematical noise models may not accurately reflect real-world sensor artifacts.

## Confidence

- **High Confidence:** Core findings that ViT B16 is more robust to event shift noise than ResNet34 are supported by direct experimental comparison.
- **Medium Confidence:** The EST representation is valid for enabling frame-based models to process event data, though specific implementation details are missing.
- **Low Confidence:** Claims about specific mechanisms (e.g., "self-attention creates global context") are inferred from architecture differences rather than directly tested.

## Next Checks

1. **EST Channel Mapping Validation:** Reproduce the EST pipeline and verify how the 18-channel tensor is reduced to 3 channels for ViT B16 input.
2. **Shift Robustness Quantification:** Systematically apply the shift noise (Eq. 1) at 5% increments and plot accuracy curves for both ResNet34 and ViT B16 to confirm the robustness gap.
3. **Noise Type Ablation:** Test all three noise types (shift, loss, polarity) on both models to identify which specific noise conditions maximize the performance difference.