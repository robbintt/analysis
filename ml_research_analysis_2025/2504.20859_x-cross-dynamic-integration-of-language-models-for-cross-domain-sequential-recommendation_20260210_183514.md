---
ver: rpa2
title: 'X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential
  Recommendation'
arxiv_id: '2504.20859'
source_url: https://arxiv.org/abs/2504.20859
tags:
- recommendation
- domain
- x-cross
- language
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: X-Cross introduces a dynamic integration model for cross-domain
  sequential recommendation that combines multiple domain-specific language models
  fine-tuned with LoRA. The key innovation is a layer-wise integration mechanism that
  computes adaptive weights to combine knowledge from different domains without modifying
  the original adapters.
---

# X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2504.20859
- Source URL: https://arxiv.org/abs/2504.20859
- Authors: Guy Hadad; Haggai Roitman; Yotam Eshel; Bracha Shapira; Lior Rokach
- Reference count: 40
- Primary result: Achieves comparable performance to LoRA fine-tuning while using only 25% of the parameters and 50-75% less training data

## Executive Summary
X-Cross introduces a dynamic integration model for cross-domain sequential recommendation that combines multiple domain-specific language models fine-tuned with LoRA. The key innovation is a layer-wise integration mechanism that computes adaptive weights to combine knowledge from different domains without modifying the original adapters. X-Cross achieves comparable performance to LoRA fine-tuning while using only 25% of the parameters, and requires 50-75% less training data than LoRA to reach effective performance. In experiments on Amazon datasets, X-Cross outperforms alternative cross-domain baselines including state-of-the-art mixture-of-experts approaches, demonstrating robust adaptability across domains while maintaining computational efficiency.

## Method Summary
X-Cross dynamically integrates knowledge from multiple domain-specific language models by placing trainable integrators at each transformer layer. It concatenates LoRA-adapted activations from all source domains, computes adaptive scaling weights via linear transformation, and refines each domain's representation by blending contributions from other domains and their pairwise differences. The method freezes source LoRA adapters and pre-trained weights, training only the integration network, which reduces trainable parameters to ~25% of a new LoRA adapter while enabling gradient-based adaptation to target-domain signals. This layer-by-layer refinement progressively enriches representations without modifying the original domain adapters.

## Key Results
- Achieves performance comparable to LoRA fine-tuning while using only 25% of the additional parameters
- Requires 50-75% less fine-tuning data than LoRA to reach effective performance
- Outperforms alternative cross-domain baselines including state-of-the-art mixture-of-experts approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-wise dynamic integration enables effective cross-domain knowledge transfer without modifying original domain adapters.
- Mechanism: X-Cross places a trainable integrator at each transformer layer. It concatenates LoRA-adapted activations from all source domains, computes adaptive scaling weights via a linear transformation, and refines each domain's representation by blending contributions from other domains and their pairwise differences. This process repeats layer-by-layer, progressively enriching representations.
- Core assumption: Intermediate layers contain domain-transferable signals that can be dynamically recombined; LoRA-adapted activations carry richer domain-specific information than frozen pre-trained activations alone.
- Evidence anchors:
  - [abstract]: "operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next..."
  - [section 3.2]: "X-Cross pools information from all source domain models by introducing a dedicated integrator at each (network) layer, enabling the computation of scaling factors (weights) dynamically for each source domain."
  - [corpus]: Limited direct corpus evidence for layer-wise LoRA integration; related work (WeaveRec, MergeRec) focuses on model merging at inference, not per-layer dynamic integration.
- Break condition: If source domains share minimal latent structure or LoRA adapters capture only domain-unique noise, layer-wise integration may amplify interference rather than transfer.

### Mechanism 2
- Claim: Unconstrained scaling (allowing negative weights) improves integration by suppressing irrelevant domain contributions.
- Mechanism: The dynamic scaling stage (Stage 2) computes weights via a linear projection without applying softmax, allowing both positive and negative values. This lets the model down-weight or subtract representations from domains that introduce noise for a given input.
- Core assumption: Some domain adapters produce activations detrimental to specific target-domain recommendations; negative scaling provides a mechanism for selective suppression.
- Evidence anchors:
  - [section 3.2.2]: "Unlike conventional methods that rely on softmax to produce only positive scaling factors... our approach supports both positive and negative scaling. This flexibility allows X-Cross to suppress irrelevant domain contributions."
  - [corpus]: Not directly addressed in corpus papers; standard MoE approaches typically use softmax gating.
- Break condition: If domain signals are consistently complementary or positively correlated, negative scaling may introduce instability; training dynamics become more sensitive to learning rate and initialization.

### Mechanism 3
- Claim: Data efficiency arises from reusing frozen LoRA adapters and only training a lightweight integration network.
- Mechanism: X-Cross freezes all source-domain LoRA adapters and pre-trained weights. Only the integrator weights (W_concat per layer), final aggregation weights (w_m), pooler, and scorer are trained. This reduces trainable parameters to ~25% of a new LoRA adapter while enabling gradient-based adaptation to target-domain signals.
- Core assumption: Pre-existing domain adapters already encode transferable patterns; the integration network can learn to recombine them with limited target-domain supervision.
- Evidence anchors:
  - [abstract]: "X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters... requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective."
  - [section 4.3, Table 3]: Across four domains, X-Cross reaches baseline-beating performance with 50-83% fewer training samples than LoRA.
  - [corpus]: RecBase and LLM-RecG also pursue parameter-efficient transfer but via different mechanisms (generative pretraining, semantic bias correction); not directly comparable.
- Break condition: If target-domain patterns are fundamentally novel (not latent in any source adapter), integration-only training may underfit; full LoRA fine-tuning becomes necessary.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: X-Cross builds on frozen LoRA adapters; understanding how LoRA factorizes weight updates (W + αAB) is essential to grasp what X-Cross integrates.
  - Quick check question: Given a weight matrix W∈R^(d×k) and LoRA rank r=16, what are the shapes of matrices A and B, and how many trainable parameters does LoRA add per layer?

- Concept: Transformer Layer Activations and Layer-wise Specialization
  - Why needed here: X-Cross integrates per-layer; knowing that higher layers capture more abstract semantics helps explain why integration targets top-9 layers.
  - Quick check question: Why might intermediate transformer layers be more suitable for cross-domain transfer than only the final layer?

- Concept: Cross-Domain Sequential Recommendation
  - Why needed here: The core task—predicting next items in a target domain using models trained on source domains—frames all design decisions.
  - Quick check question: In X-Cross, no user or item overlap is assumed between domains. What implication does this have for how knowledge must be transferred?

## Architecture Onboarding

- Component map: Source encoders (n=2 DeBERTa models with frozen LoRA adapters) -> Integrator (per layer l: concatenates activations, computes adaptive weights) -> Representation refiner (computes refined h̃_m^(l) for each domain) -> Final aggregator (weighted sum of last-layer refined representations) -> Pooler ([CLS] token) -> Scorer (linear layer for candidate items)

- Critical path:
  1. Forward pass through all source encoders to obtain LoRA-adapted activations at each layer.
  2. For layers L-8 to L (top 9), apply integrator -> compute weights -> refine representations.
  3. Aggregate final-layer representations, pool, and score candidate items.
  4. Backpropagate only through integrator, aggregator, pooler, and scorer weights.

- Design tradeoffs:
  - **Number of layers to integrate**: More layers increase capacity but also compute and overfitting risk (paper uses top 9).
  - **Source domain selection**: Paper selects top-2 by zero-shot performance; alternative heuristics (domain similarity, diversity) are unexplored.
  - **Hyperparameters β and γ**: Control strength of direct contribution vs. interaction terms; paper tunes on holdout (β=0.5, γ=0.4).

- Failure signatures:
  - **Performance plateaus below baseline**: May indicate source domains are too dissimilar; consider alternative source selection.
  - **Training instability or loss divergence**: Check learning rate (paper uses 5e-5); negative scaling may require gradient clipping.
  - **Negligible gain over single best source**: Integration may not be learning; inspect weight distributions—constant weights suggest integrator is ineffective.

- First 3 experiments:
  1. **Reproduce data efficiency curve**: Train X-Cross vs. LoRA on subsets {50, 100, 200, 500} samples from one target domain; verify X-Cross reaches reference baseline with fewer samples.
  2. **Ablate negative scaling**: Compare full X-Cross vs. softmax-gated variant; expect performance drop if suppression mechanism is active.
  3. **Vary number of integrated layers**: Test integrating {1, 3, 5, 7, 9} layers; plot Hit@1 to confirm paper's finding that more layers help but with diminishing returns.

## Open Questions the Paper Calls Out

- **Open Question 1**: What defines the optimal selection of source domains for a given target domain in X-Cross?
  - Basis in paper: [explicit] The conclusion states: "As future work, we wish to explore the relationship between source and target domains to better understand how domain selection impacts performance."
  - Why unresolved: While the paper demonstrates successful transfer (e.g., Toys → Tools), it does not quantify the characteristics (e.g., semantic similarity, item overlap) that predict successful knowledge transfer.
  - What evidence would resolve it: A systematic evaluation varying source domain similarity against target accuracy to derive a "domain gap" heuristic.

- **Open Question 2**: Does X-Cross maintain parameter efficiency and accuracy when scaling to a large number of source domains (N > 2)?
  - Basis in paper: [inferred] The experiments are restricted to n=2 source domains, while Equation 4 shows the integrator parameters scale by 2N(N-1).
  - Why unresolved: It is unclear if the quadratic growth of the dynamic scaling weights introduces overfitting or computational bottlenecks when integrating many domain experts.
  - What evidence would resolve it: Performance and training time benchmarks running X-Cross with varying numbers of source domains (e.g., N=5, 10, 20).

- **Open Question 3**: Can modifying prompt characteristics (length and diversity) explicitly improve X-Cross convergence in sparse domains?
  - Basis in paper: [inferred] Section 4.6 notes prompt length/diversity explains 86% of accuracy variance, suggesting "fine-tuning the model with shorter and more diverse prompts results in better accuracy."
  - Why unresolved: The paper identifies these statistical correlations but does not test prompt engineering as an intervention to fix slow convergence in difficult domains (like Tools).
  - What evidence would resolve it: Experiments comparing standard inputs against truncated/diversified prompts for the same user history to observe convergence speed changes.

## Limitations

- The method's effectiveness depends heavily on the quality and relevance of source domain LoRA adapters, with no clear guidance on optimal source selection beyond zero-shot performance.
- The quadratic scaling of integrator parameters with the number of source domains (2N(N-1)) could become prohibitive for large N, though this wasn't tested.
- The approach assumes no item or user overlap between domains, which may not reflect many real-world scenarios where partial overlap exists.

## Confidence

- **High Confidence**: The core mechanism of layer-wise dynamic integration and the parameter efficiency claim (25% parameters, 50-75% less data) are well-supported by the experimental results and architectural description.
- **Medium Confidence**: The data efficiency claims across all four domains are convincing but rely on the specific Amazon dataset splits and filtering criteria, which aren't fully specified.
- **Medium Confidence**: The superiority over alternative baselines (including mixture-of-experts approaches) is demonstrated, but the comparison is limited to specific implementations of those baselines.

## Next Checks

1. **Ablation of Layer Selection**: Systematically vary the number of integrated layers (e.g., top 3, 5, 7, 9) and measure performance to determine if the top-9 choice is optimal or merely sufficient.

2. **Source Domain Selection Sensitivity**: Test alternative source selection strategies (e.g., domain similarity metrics, random selection) to verify that the top-2-by-zero-shot approach is robust.

3. **Negative Scaling Necessity**: Implement a variant with softmax-gated weights only (positive values) and compare performance to assess whether negative scaling is essential or merely beneficial.