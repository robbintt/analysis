---
ver: rpa2
title: 'T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing Text
  Selection in High School Literature through Knowledge Graph-Based Recommendation'
arxiv_id: '2506.12075'
source_url: https://arxiv.org/abs/2506.12075
tags:
- knowledge
- graph
- text
- walk
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T-TExTS is a knowledge-graph-based recommendation system designed
  to support high school English Literature teachers in selecting diverse, pedagogically
  aligned texts. The system uses an ontology created with KNARM to capture domain
  concepts, converted into a knowledge graph and embedded via DeepWalk, biased random
  walk, and a hybrid method.
---

# T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing Text Selection in High School Literature through Knowledge Graph-Based Recommendation

## Quick Facts
- arXiv ID: 2506.12075
- Source URL: https://arxiv.org/abs/2506.12075
- Reference count: 40
- Key outcome: Knowledge graph-based recommendation system for high school English Literature text selection; DeepWalk achieved AUC=0.9431

## Executive Summary
T-TExTS is a knowledge-graph-based recommendation system designed to support high school English Literature teachers in selecting diverse, pedagogically aligned texts. The system uses an ontology created with KNARM to capture domain concepts, converted into a knowledge graph and embedded via DeepWalk, biased random walk, and a hybrid method. Evaluation metrics include AUC (DeepWalk: 0.9431), Hits@K, MRR, and nDCG. DeepWalk outperformed other models, with the hybrid approach yielding balanced results. The study demonstrates the effectiveness of semantic, ontology-driven embeddings in educational recommendation tasks, enabling more inclusive and informed text selection.

## Method Summary
The method constructs a domain-specific ontology using KNARM methodology to capture pedagogical and literary semantics, then converts this into a knowledge graph of RDF triples. Graph embeddings are learned using DeepWalk (uniform random walks), biased random walk (domain-weighted edges), and a hybrid approach combining both. These embeddings are evaluated using link prediction metrics including AUC and ranking metrics (Hits@K, MRR, nDCG). The system recommends books by computing cosine similarity between book embeddings in the learned vector space.

## Key Results
- DeepWalk achieved the highest AUC of 0.9431 among tested embedding methods
- The hybrid model offered balanced performance across multiple metrics
- Ontology-driven semantic embeddings effectively capture pedagogical relationships for text recommendation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific ontology construction captures pedagogical and literary semantics that generic knowledge graphs miss.
- Mechanism: The KNARM methodology elicits expert knowledge through structured/unstructured interviews and iterative validation, encoding concepts (Book, Author, Genre, Theme) and relationships (hasAuthor, hasGenre, hasTheme) as formal RDF triples. Rule-based axioms (e.g., Lexile thresholds → complexity classifications) add derived metadata.
- Core assumption: Domain experts can articulate pedagogical similarity criteria that translate cleanly into graph relationships.
- Evidence anchors:
  - [abstract] "constructed a domain-specific ontology using KNARM...transformed into a knowledge graph"
  - [section 3.2] "if the year of a text is earlier than 1945, then the triple (Book, hasEra, Traditional) is generated"
- Break condition: If expert-elicited relationships don't align with actual teacher selection behavior, recommendations will misalign with real-world needs.

### Mechanism 2
- Claim: Random walk-based graph embedding preserves structural proximity, enabling semantic similarity computation in vector space.
- Mechanism: DeepWalk performs uniform random walks generating node sequences; biased random walk weights transitions by edge importance (domain expert-assigned). Sequences are fed to skip-gram (Word2Vec) to learn embeddings where structurally related nodes cluster. Cosine similarity then ranks book proximity.
- Core assumption: Structural proximity in the knowledge graph correlates with pedagogical/thematic similarity as perceived by teachers.
- Evidence anchors:
  - [abstract] "DeepWalk outperformed in most ranking metrics, with the highest AUC (0.9431)"
  - [section 3.3] "embeddings of nodes and entities are positioned close to one another in high-dimensional vector space if they are structurally related in the graph"
- Break condition: If graph structure doesn't reflect meaningful pedagogical relationships (e.g., incomplete or noisy edges), embeddings will capture irrelevant patterns.

### Mechanism 3
- Claim: Hybrid embeddings combine DeepWalk's structural coverage with biased walk's domain-weighted focus, yielding balanced performance.
- Mechanism: Concatenate DeepWalk embeddings (256-dim) with biased random walk embeddings (512-dim) into a single vector. DeepWalk captures broad structural patterns; biased walk emphasizes expert-weighted edges (e.g., genre importance). Combined vector leverages both.
- Core assumption: The two embedding methods capture complementary information that improves recommendation when fused.
- Evidence anchors:
  - [abstract] "hybrid model offered balanced performance"
  - [table 2] Hybrid achieves AUC 0.8724, matching DeepWalk on Hits@5, Hits@10, nDCG@10
- Break condition: If embedding dimensions encode redundant or conflicting information, concatenation may dilute signal without gains.

## Foundational Learning

- Concept: **Knowledge Graphs & RDF Triples**
  - Why needed here: The entire system depends on representing books, authors, genres, themes as entities connected by typed relationships in a graph structure.
  - Quick check question: Can you explain the difference between a triple like `<Book, hasGenre, Drama>` and a simple database row storing genre as a column?

- Concept: **Skip-gram / Word2Vec for Graph Embeddings**
  - Why needed here: DeepWalk adapts NLP skip-gram to graphs—understanding how context windows work on node sequences is essential for debugging embedding quality.
  - Quick check question: Given a walk sequence `[Romeo&Juliet, Drama, Hamlet, Tragedy]`, what does skip-gram learn when "Hamlet" is the target word with window size 2?

- Concept: **Link Prediction & Recommendation Metrics (AUC, Hits@K, MRR, nDCG)**
  - Why needed here: Model selection and tuning rely on these metrics; understanding what each measures guides interpretation of tradeoffs.
  - Quick check question: Why might a model with high Hits@10 still have low Hits@1, and what does that imply for user experience?

## Architecture Onboarding

- Component map: Domain Experts → KNARM Interviews → Ontology (OWL) → RDF Triple Extraction → Knowledge Graph (GraphDB) → DeepWalk (uniform) → Skip-gram Embeddings → Biased Random Walk (weighted) → Skip-gram Embeddings → Hybrid Concatenation (optional) → Cosine Similarity Ranking → Top-N Book Recommendations

- Critical path: Ontology schema design → Edge weight assignment for biased walk → Hyperparameter tuning (walk length, window size, embedding dim). Poor schema or weights propagate through entire pipeline.

- Design tradeoffs:
  - DeepWalk vs. Biased Walk: DeepWalk better for AUC/ranking; Biased Walk allows domain-weighted exploration but underperforms without well-calibrated weights.
  - Small graph vs. GNNs: Authors chose DeepWalk over Graph Neural Networks due to small dataset (100 books, 2399 triples)—simpler methods may generalize better with limited data.
  - Negative sampling: Generates synthetic negatives by corrupting triples; risks false negatives under open-world assumption.

- Failure signatures:
  - Low AUC (~0.5): Check if graph has meaningful connectivity; walk sequences may be too short or graph too sparse.
  - High Hits@K but low Hits@1: Model retrieves relevant items but ranks poorly at top—consider reranking or adjusting similarity threshold.
  - Cold-start for new books: System cannot recommend books without edges in the graph—new entries need full metadata annotation.

- First 3 experiments:
  1. Reproduce baseline: Clone https://github.com/koncordantlab/TTExTS, load provided graph, run DeepWalk with reported hyperparameters (walk_length=60, num_walks=25, dim=256, window=20), verify AUC ~0.94.
  2. Ablate edge weights: Set all biased walk weights to uniform; compare AUC drop to quantify contribution of domain-weighted edges.
  3. Stress test with graph perturbation: Remove 10% of edges randomly, retrain, measure AUC degradation—establishes robustness baseline before adding new data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do more advanced graph embedding algorithms (e.g., Node2Vec, OWL2Vec*, Heterogeneous Graph Neural Networks) compare to DeepWalk when the T-TExTS knowledge graph is scaled beyond 100 books?
- Basis in paper: [explicit] Authors explicitly state: "Future directions for this research include expanding the training dataset and exploring additional algorithms... such as those proposed in Node2Vec, OWL2Vec*, and Heterogeneous Graph Neural Networks (HGNNs)."
- Why unresolved: The current study was limited to a manually curated dataset of fewer than 100 books due to resource constraints; advanced algorithms were not tested.
- What evidence would resolve it: Performance comparison of multiple embedding algorithms on an expanded dataset (e.g., 500+ books) using the same evaluation metrics (AUC, Hits@K, MRR, nDCG).

### Open Question 2
- Question: Can integrating GraphRAG with T-TExTS enable effective question-answering capabilities for teacher queries about text selection?
- Basis in paper: [explicit] Authors note: "The incorporation of GraphRAG could contribute to the development of a more robust question-answering tool, thereby extending the system's capabilities beyond recommendation."
- Why unresolved: GraphRAG integration was proposed as future work but not implemented or evaluated in the current study.
- What evidence would resolve it: A prototype T-TExTS+GraphRAG system evaluated on teacher-originated queries (e.g., "Find diverse texts about identity for 10th graders") with relevance assessments.

### Open Question 3
- Question: Do high school teachers perceive T-TExTS recommendations as pedagogically useful and aligned with their curriculum goals in real-world classroom settings?
- Basis in paper: [inferred] The paper evaluates technical metrics (AUC, Hits@K, nDCG) but does not report any user study, teacher feedback, or classroom deployment validation.
- Why unresolved: No human-centered evaluation was conducted; the system's practical impact on teacher decision-making remains untested.
- What evidence would resolve it: A user study where teachers use T-TExTS for text selection tasks, followed by surveys/interviews measuring perceived usefulness, relevance, and impact on curricular decisions.

## Limitations
- Reliance on expert-elicited ontology construction introduces potential subjectivity without independent validation against actual teacher selection patterns
- Small knowledge graph scale (100 books, 2,399 triples) may limit generalizability to larger, more diverse corpora
- Negative sampling in open-world assumption context could introduce false negatives affecting model performance

## Confidence
- High confidence: DeepWalk's superior performance metrics (AUC=0.9431, Hits@K) are well-supported by experimental results and reproducible given the specified methodology
- Medium confidence: The ontology construction process captures pedagogical semantics as claimed, though this depends on expert elicitation quality without independent validation
- Medium confidence: The hybrid embedding approach provides balanced performance, though the complementary information assumption lacks direct corpus validation

## Next Checks
1. **Teacher behavior validation**: Conduct user studies where actual teachers use the system for text selection and compare recommended books against their natural choices to validate semantic alignment
2. **Negative sampling audit**: Analyze the impact of false negatives by testing recommendations with different negative sampling rates and filtering strategies to establish optimal corruption parameters
3. **Scalability assessment**: Test the recommendation pipeline on a larger corpus (e.g., expanding beyond 100 books) to evaluate performance degradation and identify bottlenecks in ontology maintenance