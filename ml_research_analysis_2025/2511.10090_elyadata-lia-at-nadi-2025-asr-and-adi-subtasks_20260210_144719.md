---
ver: rpa2
title: 'ELYADATA & LIA at NADI 2025: ASR and ADI Subtasks'
arxiv_id: '2511.10090'
source_url: https://arxiv.org/abs/2511.10090
tags:
- arabic
- speech
- dialect
- dialects
- nadi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes ELYADATA & LIA's participation in the NADI
  2025 shared task, focusing on Arabic Dialect Identification (ADI) and multi-dialectal
  Arabic ASR. For ADI, the team fine-tuned a Whisper-large-v3 encoder using a two-stage
  approach with data augmentation, achieving state-of-the-art results.
---

# ELYADATA & LIA at NADI 2025: ASR and ADI Subtasks

## Quick Facts
- arXiv ID: 2511.10090
- Source URL: https://arxiv.org/abs/2511.10090
- Reference count: 4
- First place in ADI subtask with 79.83% accuracy

## Executive Summary
ELYADATA & LIA participated in the NADI 2025 shared task focusing on Arabic Dialect Identification (ADI) and multi-dialectal Arabic Automatic Speech Recognition (ASR). For ADI, the team employed a two-stage fine-tuning approach using Whisper-large-v3 encoder with data augmentation, achieving state-of-the-art performance. For ASR, they fine-tuned the SeamlessM4T-v2 Large model separately for each of eight Arabic dialects. The ADI system ranked first with 79.83% accuracy, while the ASR system achieved second place with 38.54% WER and 14.53% CER on the test set.

## Method Summary
The team utilized large pre-trained speech models for both tasks. For ADI, they fine-tuned Whisper-large-v3 encoder in two stages with data augmentation techniques. For ASR, they employed separate fine-tuning of SeamlessM4T-v2 Large for each Arabic dialect. The approach leveraged the strengths of large-scale pre-trained models while adapting them to the specific challenges of Arabic dialectal variation through targeted fine-tuning strategies.

## Key Results
- ADI system achieved 79.83% accuracy, ranking first on the test set
- ASR system achieved 38.54% WER and 14.53% CER, ranking second
- Two-stage fine-tuning with data augmentation proved effective for ADI
- Separate dialect-specific fine-tuning yielded strong ASR performance across 8 dialects

## Why This Works (Mechanism)
The success stems from leveraging large pre-trained speech models that capture rich acoustic and linguistic features, then fine-tuning them on dialect-specific data. The two-stage approach for ADI allows the model to first learn general Arabic speech patterns before specializing in dialectal distinctions. For ASR, separate fine-tuning per dialect enables the model to adapt to the unique phonetic and phonological characteristics of each variety while maintaining strong overall performance.

## Foundational Learning
- Whisper-large-v3 encoder architecture - needed to understand the ADI system's foundation; quick check: verify encoder dimensions and attention mechanisms
- SeamlessM4T-v2 Large speech model - needed for ASR system comprehension; quick check: confirm model parameters and pre-training data
- Arabic dialectal phonology - needed to appreciate task complexity; quick check: map phonetic differences across the 8 dialects
- Data augmentation techniques - needed to evaluate ADI improvements; quick check: identify augmentation methods used
- Two-stage fine-tuning methodology - needed to understand ADI approach; quick check: verify training objectives for each stage
- WER/CER evaluation metrics - needed for ASR performance assessment; quick check: confirm calculation methods and normalization

## Architecture Onboarding

**Component Map:**
Pre-trained Model -> Fine-tuning Stage 1 -> Fine-tuning Stage 2 -> ADI Classification
OR
Pre-trained Model -> Dialect-specific Fine-tuning -> ASR Decoding

**Critical Path:**
For ADI: Pre-trained encoder → Two-stage fine-tuning → Classification head
For ASR: Pre-trained model → Dialect-specific fine-tuning → Decoding output

**Design Tradeoffs:**
Separate dialect fine-tuning vs. joint multilingual training (better specialization vs. potential cross-dialect transfer)
Two-stage fine-tuning vs. single-stage (more controlled learning vs. simpler implementation)

**Failure Signatures:**
Poor ADI performance may indicate insufficient dialectal variation in training data or inadequate fine-tuning duration
High ASR error rates may suggest mismatched acoustic conditions or insufficient dialect-specific adaptation

**First Experiments:**
1. Run baseline inference on validation set before any fine-tuning
2. Evaluate intermediate checkpoints after stage 1 fine-tuning for ADI
3. Test cross-dialect ASR performance to assess generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Small test set may not fully represent real-world dialectal variability
- Data augmentation techniques lack detailed description
- Two-stage fine-tuning effectiveness not isolated through ablation studies
- Separate dialect fine-tuning doesn't explore cross-dialect transfer learning
- Standard metrics don't account for class imbalance or dialectal overlap

## Confidence
- ADI state-of-the-art result: High
- ASR second-place ranking: High
- Two-stage fine-tuning effectiveness: Medium
- Data augmentation benefits: Low

## Next Checks
1. Conduct cross-validation with additional held-out datasets to verify results generalize
2. Perform ablation studies to quantify impact of each fine-tuning stage and augmentation method
3. Test cross-dialect transfer learning for ASR to assess joint fine-tuning potential