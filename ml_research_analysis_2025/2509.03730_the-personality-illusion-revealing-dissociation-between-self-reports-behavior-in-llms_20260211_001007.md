---
ver: rpa2
title: 'The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior
  in LLMs'
arxiv_id: '2509.03730'
source_url: https://arxiv.org/abs/2509.03730
tags:
- personality
- traits
- arxiv
- llms
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the dissociation between self-reported personality
  traits and actual behavior in large language models (LLMs). While instructional
  alignment (e.g., RLHF) increases trait stability and coherence, self-reported traits
  like openness and agreeableness poorly predict behavior in tasks like risk-taking
  and honesty.
---

# The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs

## Quick Facts
- **arXiv ID**: 2509.03730
- **Source URL**: https://arxiv.org/abs/2509.03730
- **Reference count**: 40
- **Primary result**: LLMs exhibit linguistic personality coherence without behavioral grounding; self-reported traits poorly predict actual behavior.

## Executive Summary
This study reveals a fundamental gap in LLM personality modeling: while instructional alignment (e.g., RLHF) creates stable, human-like self-reported personality profiles, these traits fail to predict actual behavioral outcomes. The research demonstrates that LLMs can describe themselves as open, agreeable, or conscientious, yet these self-reports bear little relationship to how they behave in psychologically grounded tasks. This "personality illusion" shows that current alignment methods optimize for linguistic coherence rather than functional behavioral consistency, cautioning against interpreting surface-level personality alignment as evidence of genuine trait-based decision-making.

## Method Summary
The study evaluated 12 language models (6 base + 6 instruction-tuned pairs) using Big Five Inventory and Self-Regulation Questionnaire for self-reports, and five behavioral tasks adapted from psychology (Columbia Card Task, Implicit Association Test, calibration-based honesty, sycophancy paradigms, and meta-cognitive consistency measures). Statistical analysis employed mixed-effects regression with random intercepts for temperature and prompt, measuring trait stability, trait-trait correlations, and predictive validity of self-reports on behavior. Persona injection experiments tested controllability through explicit trait prompts.

## Key Results
- Instructional alignment significantly stabilizes self-reported trait expression (40-45% reduction in trait variability) and strengthens human-like trait correlations
- Self-reported traits poorly predict behavior: only ~24% of trait-task associations are statistically significant, with alignment hovering near chance levels (50%)
- Persona injections successfully steer self-reports (β≈3.6 to 4.4, p<.001) but fail to induce corresponding behavioral changes (β≈0)
- Larger models show somewhat higher alignment (e.g., 68% for GPT-4o, 82% for Qwen-235B) but most remain near chance except the largest model

## Why This Works (Mechanism)

### Mechanism 1: Instructional Alignment Consolidates Linguistic Trait Patterns
- Claim: Post-pretraining alignment stabilizes self-reported trait expression at the linguistic surface level only
- Core assumption: Models internalize statistical regularities from human personality discourse without forming behavioral dispositions
- Evidence: Instructionally aligned models are more open and agreeable but less neurotic than pre-trained models, typically sitting ~+1.5SD higher in Openness, +1/2 SD higher in Agreeableness, and -1SD lower in Neuroticism
- Break condition: If alignment were behaviorally grounded, trait-stable models should show consistent trait-behavior relationships; the paper shows they do not

### Mechanism 2: Self-Report Questionnaires Elicit Pattern-Matched Responses, Not Dispositions
- Claim: LLMs complete personality questionnaires by retrieving and reproducing linguistic patterns associated with trait descriptors
- Core assumption: Questionnaire completion and task performance draw from different model capabilities (textual completion vs. decision-making under constraints)
- Evidence: Self-reported traits like openness and agreeableness poorly predict behavior in tasks like risk-taking and honesty; alignment for most models is no better than chance (43-50%)
- Break condition: If questionnaires measured dispositions, high trait-behavior correlations would emerge; observed alignment hovers near 50%

### Mechanism 3: Persona Injection Modifies Self-Description Without Altering Task Policy
- Claim: Explicit persona prompts shift how models describe themselves on questionnaires but fail to modify the decision policies that govern behavioral task choices
- Core assumption: Self-report generation and behavioral task execution use different computational pathways within the model
- Evidence: When injecting the agreeableness persona, logistic regression reveals a significant increase in self-reported agreeableness (β≈3.6 to 4.4, p<.001) but sycophantic responses provide weak and inconsistent evidence for predicting whether the agreeableness persona was used
- Break condition: If persona injection affected behavior, we would observe systematic behavioral shifts matching persona traits

## Foundational Learning

- **Big Five Inventory (BFI) and Self-Regulation Questionnaire (SRQ)**
  - Why needed here: These are the measurement instruments used to elicit self-reported traits. Understanding what they measure is essential to interpreting the dissociation findings
  - Quick check question: If a model scores high on Agreeableness, what behaviors would you expect in human studies? (Answer: reduced stereotyping, higher cooperation; the paper tests sycophancy as an Agreeableness-linked behavior)

- **Mixed-Effects Regression for Clustered Data**
  - Why needed here: The statistical analysis uses mixed-effects models with random intercepts for temperature, prompt, and model to handle repeated measures
  - Quick check question: Why use random intercepts for "model" in the regression? (Answer: Multiple observations from the same model are correlated; random effects account for this non-independence)

- **Behavioral Task Operationalization from Psychology**
  - Why needed here: The five tasks are adapted from human psychology. Knowing what they measure validates the "behavioral" side of the comparison
  - Quick check question: In the Columbia Card Task, what does choosing to flip more cards indicate? (Answer: Higher risk-taking; Extraversion and low Self-Regulation predict more risk-taking in humans)

## Architecture Onboarding

- **Component map:**
  1. Questionnaire Elicitation Pipeline: System prompt → BFI/SRQ items → Likert scale responses (1-5) → trait score aggregation
  2. Behavioral Task Suite: Columbia Card Task (risk), IAT (bias), calibration questions (epistemic honesty), C1-C2 consistency (meta-cognitive honesty), Asch conformity (sycophancy)
  3. Alignment Analysis: Mixed-effects regressions predicting behavior from traits → directional alignment scoring vs. human expectations → bootstrap confidence intervals
  4. Persona Injection Module: Trait-specific prompts (3 strategies × 3 keyword variations) → re-run questionnaires and behavioral tasks → compare effect sizes

- **Critical path:**
  1. Run questionnaires on base vs. instruction-tuned models → confirm trait stabilization (RQ1)
  2. Run behavioral tasks on instruction-tuned models → collect task scores
  3. Fit trait→behavior regressions → compute alignment with human expectations (RQ2)
  4. Inject personas → re-run questionnaires + behavioral subset → compare self-report vs. behavioral effect sizes (RQ3)

- **Design tradeoffs:**
  - Questionnaire selection: BFI/SRQ are well-validated but may suffer from data contamination; the paper argues this doesn't matter because they test coherence not knowledge
  - Behavioral task adaptation: Text-based operationalizations of psychological paradigms may not transfer cleanly but were chosen for real-world LLM application relevance
  - Statistical approach: Clustered bootstrap vs. beta-binomial intervals for alignment uncertainty; bootstrap is more conservative but assumes cluster-level independence

- **Failure signatures:**
  - Near-chance alignment (50%) between self-reports and behavior → indicates dissociation
  - High self-report shift (β>3) with near-zero behavioral shift (β≈0) under persona injection → confirms surface-level-only control
  - High variance in base model traits (Levene's test p<.001 for most traits) → confirms alignment stabilizes expression

- **First 3 experiments:**
  1. Replicate RQ1 on your models: Run BFI + SRQ on a base/instruct pair from your stack. Verify that instruction tuning reduces trait variance (expect 40%+ drop) and strengthens trait-trait correlations
  2. Behavioral probe for your use case: Select one task most relevant to your deployment (e.g., sycophancy if building advisory systems). Run it across your model set with and without persona prompts
  3. Test behavioral grounding intervention: Implement a simple reinforcement signal based on behavioral consistency and compare trait-behavior alignment before/after

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning from behavioral feedback (RLBF), where models are rewarded based on consistent performance in psychologically grounded tasks, successfully ground LLM personality traits in behavioral regularity?
- Basis in paper: The authors propose RLBF as a promising direction for shifting alignment from shaping model outputs to shaping model dispositions
- Why unresolved: Current alignment methods optimize for linguistic plausibility without ensuring behavioral consistency
- What evidence would resolve it: Demonstrate that RLBF-trained models show significantly higher trait-behavior alignment rates than RLHF-trained models

### Open Question 2
- Question: Do reasoning models (e.g., o1, DeepSeek-R1) exhibit the same dissociation between self-reported traits and behavioral outcomes found in non-reasoning transformer-based LLMs?
- Basis in paper: The authors state their analysis is limited to mainstream transformer-based, non-reasoning models and future work should extend to reasoning models
- Why unresolved: The study's findings are limited to conventional transformer architectures; reasoning models have different cognitive-style processes
- What evidence would resolve it: Apply the same three-RQ framework to reasoning models and compare trait-behavior alignment rates

### Open Question 3
- Question: How does scale affect trait-behavior alignment? The study found larger models showed higher alignment but most remain near chance except the largest model
- Basis in paper: The paper reports that larger models show somewhat higher alignment (e.g., 64% for Claude-3.7, 68% for GPT-4o, 82% for Qwen-235B)
- Why unresolved: The study evaluated a limited range of model sizes, and only one model showed statistically significant alignment
- What evidence would resolve it: Conduct a systematic evaluation across a controlled scaling ladder within the same model family

## Limitations
- Questionnaire contamination: BFI/SRQ items may have been present in training data, though the paper argues this doesn't invalidate coherence tests
- Behavioral task validity: Text-based adaptations of psychological paradigms may not capture the same constructs as their human counterparts
- Alignment method specificity: Results are based on RLHF/instruction tuning; other alignment approaches may show different trait-behavior relationships

## Confidence
- **High confidence**: Instructional alignment stabilizes self-reported trait expression and creates human-like inter-trait correlations
- **Medium confidence**: Self-reports poorly predict behavior in the tested tasks (robust dissociation finding, but task validity concerns apply)
- **Medium confidence**: Persona injection affects self-reports but not behavior (effect sizes are clear, but behavioral task limitations constrain interpretation)

## Next Checks
1. **Replication with alternative self-report instruments**: Run the same protocol using HEXACO or Schwartz Values Survey to verify the dissociation pattern holds across different trait frameworks
2. **Multi-observer validation**: Collect third-party assessments of model "personality" (e.g., through conversation analysis) and compare to self-reports and behavioral predictions
3. **Behavioral grounding intervention test**: Implement a simple reward model that reinforces behavioral consistency with stated preferences and measure trait-behavior alignment changes