---
ver: rpa2
title: Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer
arxiv_id: '2505.18713'
source_url: https://arxiv.org/abs/2505.18713
tags:
- task
- performance
- fine-tuned
- tasks
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neural Parameter Search (NPS) for pruning
  fine-tuned models to improve knowledge transfer, fusion, and compression. NPS leverages
  task vectors by decomposing them into parameter subspaces, searching for optimal
  weights using evolutionary algorithms, and pruning based on magnitude.
---

# Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer

## Quick Facts
- arXiv ID: 2505.18713
- Source URL: https://arxiv.org/abs/2505.18713
- Authors: Guodong Du; Zitao Fang; Jing Li; Junlin Li; Runhua Jiang; Shuyang Yu; Yifei Guo; Yangneng Chen; Sim Kuan Goh; Ho-Kin Tang; Daojing He; Honghai Liu; Min Zhang
- Reference count: 33
- One-line primary result: NPS prunes fine-tuned models to improve transfer, fusion, and compression, achieving 40% better compression efficiency and 2.1–3.0% gains in knowledge fusion.

## Executive Summary
This paper introduces Neural Parameter Search (NPS) for pruning fine-tuned models to improve knowledge transfer, fusion, and compression. NPS leverages task vectors by decomposing them into parameter subspaces, searching for optimal weights using evolutionary algorithms, and pruning based on magnitude. Experiments show NPS mitigates catastrophic forgetting in multimodal models (e.g., LLaVa) with 10% sparsity, improves knowledge fusion by 2.1–3.0% across NLP, vision, and LLM tasks, and achieves 40% better compression efficiency while retaining near-original performance. The method is gradient-free, practical, and robust across diverse benchmarks.

## Method Summary
NPS computes a task vector by subtracting pre-trained weights from fine-tuned weights, partitions it into magnitude-ranked subspaces, and uses CMA-ES to search for optimal subspace weights on a calibration dataset. The optimized task vector is then magnitude-pruned to a target sparsity and combined with the pre-trained model for transfer, fusion, or compression applications. The method requires the original pre-trained model and a small calibration dataset, and operates without gradients, relying on evolutionary optimization.

## Key Results
- Mitigates catastrophic forgetting in multimodal models (e.g., LLaVa) with 10% sparsity
- Improves knowledge fusion by 2.1–3.0% across NLP, vision, and LLM tasks
- Achieves 40% better compression efficiency while retaining near-original performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing task vectors into parameter subspaces and searching for optimal weights via evolutionary algorithms improves pruning efficiency.
- Mechanism: The task vector (τ = θ_ft - θ_pre) is partitioned into M subspaces ranked by magnitude. CMA-ES searches for optimal coefficients (w_m) for each subspace using a calibration dataset, redistributing importance within the vector for more effective magnitude-based pruning.
- Core assumption: Different subspaces contribute variably to performance, and optimal coefficients can be found via gradient-free search.
- Evidence anchors: [abstract] "Leveraging the advantages of the task vector mechanism... we introduce a novel method called Neural Parameter Search (NPS-Pruning)..." [section 3.2] "Recognizing that different task vector subspaces contribute variably to model performance... we search through the neural parameters within low-rank subspaces of task vectors."
- Break condition: The method requires the original pre-trained model and a representative calibration dataset.

### Mechanism 2
- Claim: Pruning the task vector before combining it with the pre-trained model mitigates catastrophic forgetting.
- Mechanism: For knowledge transfer, the pruned task vector is interpolated with the pre-trained model. Pruning removes redundant or conflicting parameters from the fine-tuned model, reducing interference with the pre-trained knowledge.
- Core assumption: The task vector contains redundancy that, when removed, leaves only essential task-specific modifications that minimally disrupt pre-trained knowledge.
- Evidence anchors: [abstract] "...combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting... Experiments show NPS mitigates catastrophic forgetting in multimodal models (e.g., LLaVa) with 10% sparsity..." [section 4.3] "...our NPS method effectively mitigates catastrophic forgetting in MLLMs, outperforming current fine-tuning and forgetting mitigation techniques at a sparsity level of 10%."
- Break condition: This mitigation is an experimental finding, not theoretically proven, and depends on sparsity level and task nature.

### Mechanism 3
- Claim: Averaging pruned task vectors from multiple fine-tuned models achieves effective multi-task knowledge fusion.
- Mechanism: Task vectors from different models are pruned using NPS, averaged (with optional scaling λ), and added to a single pre-trained model. Pruning reduces interference between task vectors during averaging.
- Core assumption: Parameter redundancy and conflicts between task vectors are primary causes of performance degradation in model merging, and pruning addresses this issue.
- Evidence anchors: [abstract] "...improves knowledge fusion by 2.1–3.0% across NLP, vision, and LLM tasks..." [section 3.3] "The multi-task model merging via task vectors is expressed as: θ_m = θ_pre + Σ(λ_i · m_i ⊙ τ_i) / Σ λ_i... pruning fine-tuned models... minimizes interference among fine-tuned models during fusion..."
- Break condition: Effectiveness is shown experimentally but not theoretically proven.

## Foundational Learning

- Concept: Task Vectors
  - Why needed here: The entire NPS method is built on representing a fine-tuned model's knowledge as a vector of parameter differences from its pre-trained base. Without this, pruning a "difference" is unclear.
  - Quick check question: Can you explain why pruning a task vector is different from pruning a model's weights directly?

- Concept: Evolutionary Algorithms (CMA-ES)
  - Why needed here: NPS uses CMA-ES to search for optimal weights for parameter subspaces. Understanding this gradient-free, black-box optimization is key to knowing how the method works and its computational cost.
  - Quick check question: Why would a gradient-free search be chosen for this problem over a gradient-based method?

- Concept: Catastrophic Forgetting
  - Why needed here: NPS aims to mitigate catastrophic forgetting in multimodal models. Understanding this problem—where learning a new task degrades performance on old ones—is necessary to appreciate the motivation and result.
  - Quick check question: What assumption does NPS make about the nature of parameters that cause catastrophic forgetting?

## Architecture Onboarding

- Component map: Task Vector Calculator (computes τ = θ_ft - θ_pre) -> NPS Optimizer (CMA-ES searches for w_m on calibration data) -> Final Model Constructor (applies magnitude pruning mask m to τ' and adds to θ_pre)
- Critical path: The NPS search process is the bottleneck. Time complexity is T_total = Generations × (T_pruning + T_validate), determined by generations, validation dataset size, and model inference speed.
- Design tradeoffs: Number of subspaces (M) and sparsity ratio (r) are key hyperparameters. Higher M allows fine-grained optimization but increases search complexity. Lower r results in smaller, more compressed models but risks pruning critical parameters. Requires a representative calibration dataset.
- Failure signatures: Likely to fail if: 1) Pre-trained model is unavailable, 2) Fine-tuned model has deviated significantly from pre-trained, 3) Validation dataset is too small or biased.
- First 3 experiments:
  1. Sparsity Tolerance Baseline: Compare NPS, TIES, and DARE performance vs. sparsity for ViT-B/32 on vision tasks.
  2. Ablation on Search Components: Compare NPS against baseline with fixed, equal weights for all subspaces.
  3. Forgetting Mitigation Test: Measure performance retention on pre-trained tasks with and without NPS-based pruning for LLaVa fine-tuned on new tasks.

## Open Questions the Paper Calls Out
- The method requires additional validation data to guide the search process. The quality and quantity of this data directly impact pruning effectiveness and overall performance.
- The search process introduces a time cost, which varies depending on task complexity. This trade-off should be considered when deploying the method in resource-constrained environments.
- If the fine-tuned model deviates significantly from the original, it may hinder effective knowledge transfer, fusion, and compression.

## Limitations
- Effectiveness of evolutionary search relies on gradient-free optimization without rigorous theoretical justification.
- Catastrophic forgetting mitigation is presented as an experimental finding, not a theoretically proven result.
- Knowledge fusion improvement is demonstrated empirically but lacks theoretical explanation for why pruning reduces interference.

## Confidence
- **High Confidence**: The core algorithmic framework (computing task vectors, partitioning into subspaces, applying magnitude pruning) is well-defined and reproducible.
- **Medium Confidence**: Experimental results showing improvements in knowledge fusion (+2.1-3.0%) and compression efficiency (~40%) are likely robust across diverse benchmarks.
- **Low Confidence**: Theoretical explanation for why evolutionary search improves pruning efficiency and mitigates catastrophic forgetting is not fully developed.

## Next Checks
1. **Ablation on Search Components**: Compare NPS against a baseline that uses fixed, equal weights for all parameter subspaces to isolate the impact of evolutionary search.
2. **Sensitivity Analysis**: Systematically vary the number of subspaces (M) and sparsity ratio (r) to understand their impact on performance and identify critical hyperparameters.
3. **Generalization Across Domains**: Apply NPS to a new domain (e.g., medical imaging or scientific computing) to test if reported benefits generalize beyond NLP, vision, and LLM tasks.