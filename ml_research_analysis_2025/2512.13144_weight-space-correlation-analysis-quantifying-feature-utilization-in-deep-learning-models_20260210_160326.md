---
ver: rpa2
title: 'Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep
  Learning Models'
arxiv_id: '2512.13144'
source_url: https://arxiv.org/abs/2512.13144
tags:
- weight
- classification
- metadata
- correlation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Weight Space Correlation (WSC) analysis,
  a method to determine whether deep learning models in medical imaging actively utilize
  confounding metadata (like scanner type) for clinical predictions. The method quantifies
  feature utilization by measuring cosine similarity between the weight vectors of
  the primary clinical task and auxiliary metadata tasks, projected onto the data
  manifold via PCA.
---

# Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models

## Quick Facts
- arXiv ID: 2512.13144
- Source URL: https://arxiv.org/abs/2512.13144
- Reference count: 20
- Primary result: Method to detect whether deep learning models utilize confounding metadata vs clinically relevant features

## Executive Summary
This paper introduces Weight Space Correlation (WSC) analysis, a method to determine whether deep learning models in medical imaging actively utilize confounding metadata (like scanner type) for clinical predictions. The method quantifies feature utilization by measuring cosine similarity between the weight vectors of the primary clinical task and auxiliary metadata tasks, projected onto the data manifold via PCA. The authors validated their approach by successfully detecting artificially induced shortcut learning and applied it to probe an SA-SonoNet model trained for spontaneous preterm birth prediction. Results showed that while embeddings contained metadata information, the sPTB classifier was highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from irrelevant acquisition factors (e.g., scanner), confirming selective utilization of clinically meaningful features.

## Method Summary
WSC analysis works by training auxiliary classifiers on metadata tasks (scanner type, image view, etc.) alongside the primary clinical task. The method then computes cosine similarity between the weight vectors of the clinical and metadata classifiers, after projecting them onto the principal component subspace of the data manifold. This projection is achieved through PCA on the concatenated weight vectors, capturing the most variance in the model's weight space. High positive correlation indicates the model uses the same features for both tasks, while low or negative correlation suggests the model has learned to separate clinical from confounding information. The approach provides a quantitative measure of whether models are exploiting shortcut learning through metadata rather than learning clinically meaningful patterns.

## Key Results
- Successfully detected artificially induced shortcut learning in controlled experiments
- Applied to SA-SonoNet model for sPTB prediction, showing high correlation with birth weight but low correlation with scanner type
- Demonstrated that embeddings contain metadata information but the sPTB classifier selectively uses clinically relevant features

## Why This Works (Mechanism)
WSC analysis leverages the principle that if a deep learning model uses the same underlying features to solve multiple tasks, the weight vectors representing those tasks should be aligned in weight space. By projecting weight vectors onto the data manifold using PCA, the method captures the most informative directions in the model's learned representation. The cosine similarity between projected weight vectors then quantifies the degree of shared feature utilization. When the clinical task weight vector is highly correlated with a clinically relevant metadata task but uncorrelated with irrelevant metadata, it indicates the model has learned to separate meaningful from confounding information.

## Foundational Learning
- **Weight Space Analysis**: Understanding how model weights encode task-relevant information is crucial for interpreting deep learning decisions
- **Cosine Similarity**: Measures angular alignment between vectors, providing a scale-invariant metric for comparing weight directions
- **Principal Component Analysis (PCA)**: Projects high-dimensional weight vectors onto the most informative subspace of the data manifold
- **Shortcut Learning**: Models may exploit spurious correlations (like metadata) rather than learning true causal relationships
- **Feature Utilization**: The degree to which learned representations align with clinically meaningful vs confounding factors

## Architecture Onboarding
- **Component Map**: Clinical Task Classifier <- Weight Space Correlation Analysis <- Metadata Task Classifiers
- **Critical Path**: Model training → Weight vector extraction → PCA projection → Cosine similarity calculation → Correlation interpretation
- **Design Tradeoffs**: PCA projection assumes linear separability; may miss non-linear relationships in weight space
- **Failure Signatures**: High correlation with irrelevant metadata suggests shortcut learning; low correlation with relevant metadata suggests poor feature learning
- **First Experiments**:
  1. Apply WSC to models with known shortcut learning to verify detection capability
  2. Test on multiple clinical tasks with varying metadata complexity
  3. Compare WSC results with alternative interpretability methods like SHAP

## Open Questions the Paper Calls Out
None

## Limitations
- Validation focused on a single model (SA-SonoNet) and specific metadata types
- PCA projection assumes linear separability on the manifold, which may not hold for highly non-linear representations
- Interpretation of negative correlations between weight vectors requires further theoretical grounding

## Confidence
- **High**: Detection of artificially induced shortcut learning in controlled experiments
- **Medium**: Clinical application results with single-model evaluation
- **Low**: Broader claims about WSC as a universal probing tool across diverse architectures

## Next Checks
1. Apply WSC to multiple model architectures (CNNs, transformers) on the same clinical task to assess consistency
2. Conduct ablation studies removing specific feature dimensions to confirm that measured correlations correspond to actual predictive contributions
3. Validate WSC findings against alternative interpretability methods like SHAP or integrated gradients to ensure convergent evidence