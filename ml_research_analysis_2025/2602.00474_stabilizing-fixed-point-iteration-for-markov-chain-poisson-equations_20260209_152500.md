---
ver: rpa2
title: Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations
arxiv_id: '2602.00474'
source_url: https://arxiv.org/abs/2602.00474
tags:
- then
- lemma
- recurrent
- quotient
- transient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles Poisson equation learning for Markov chains beyond
  ergodicity, including reducible and periodic regimes. The core idea is to identify
  all non-decaying directions via the real peripheral invariant subspace K(P), quotient
  these out to obtain a unique solution class, and fix a gauge using anchor-based
  projections.
---

# Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations

## Quick Facts
- **arXiv ID:** 2602.00474
- **Source URL:** https://arxiv.org/abs/2602.00474
- **Reference count:** 40
- **Primary result:** A method to stabilize Poisson equation learning for Markov chains beyond ergodicity, achieving O(T^{-1/2}) convergence by projecting out non-decaying directions and fixing gauge via anchor constraints.

## Executive Summary
This work tackles the fundamental challenge of learning value functions for Markov chains that are neither ergodic nor purely transient—including reducible and periodic regimes. Standard fixed-point methods like TD fail to converge in these settings due to eigenvalues on the unit circle. The paper introduces a three-phase pipeline that learns the chain structure, estimates phase-offset absorption weights, and runs projected stochastic approximation. By quotienting out the peripheral invariant subspace K(P) and fixing a gauge via anchor constraints, the method achieves stable convergence to the unique quotient solution, even when standard iterations oscillate indefinitely.

## Method Summary
The approach operates in four stages: (1) structure learning identifies recurrent classes, periods, and anchors through K samples per state; (2) weight estimation computes absorption probabilities into phase offsets using M Monte Carlo episodes per transient state; (3) projected stochastic approximation runs TD-style updates while projecting onto the anchor-constrained subspace; (4) residual reconstruction estimates the average reward profile from anchor samples. The key innovation is the quotient-space contraction—removing K(P) transforms the non-contractive Bellman operator into a strict contraction, while anchor-based projections resolve the gauge ambiguity inherent in multichain/periodic settings.

## Key Results
- Achieves O(T^{-1/2}) convergence rate in expectation up to projection estimation error
- Stabilizes value iteration on periodic and multichain MDPs where standard TD oscillates
- Correctly identifies distinct average-reward profiles across multiple recurrent classes
- Empirical results confirm stability and accuracy across diverse test cases while baselines fail

## Why This Works (Mechanism)

### Mechanism 1: Quotient-Space Contraction
Removing the peripheral invariant subspace K(P) transforms the non-contractive Bellman operator into a strict contraction on the quotient space. The induced operator on R^n/K(P) has spectral radius less than 1, enabling stable fixed-point iteration. This works because eigenvalues on the unit circle are precisely those in K(P), so quotienting eliminates the non-decaying modes.

### Mechanism 2: Gauge-Fixed Uniqueness
Solutions to the Poisson equation are unique only up to elements in K(P). By projecting onto a subspace where anchor values are zero, the method selects a unique representative from the infinite solution set. This resolves both the "additive constant" ambiguity of ergodic settings and the "additive subspace" ambiguity of multichain settings.

### Mechanism 3: Biased Stochastic Approximation
The algorithm treats the estimated projection as a perturbation of the true projection. Convergence is driven by the true operator's contraction, while bias is bounded by the weight estimation error. The error floor is O(Nε_b), where N is the number of iterations and ε_b is the weight estimation error.

## Foundational Learning

- **Concept: Markov Chain Decomposition (Recurrent vs. Transient)**
  - Why needed: Algorithm explicitly separates state space into recurrent classes and transient states to compute cyclic structure vs. absorption weights
  - Quick check: Can you identify the closed strongly connected components in a directed graph?

- **Concept: Poisson Equation (Bellman Equation for Average Reward)**
  - Why needed: This is the target fixed-point equation (g + v = r + Pv). Understanding that g captures long-run average reward and v captures transient bias is essential
  - Quick check: How does the Poisson equation differ from the discounted Bellman equation?

- **Concept: Seminorms and Quotient Spaces**
  - Why needed: Paper utilizes a seminorm ||·||_q where the "kernel" is exactly the non-decaying subspace. Intuition about "modding out" null directions is required to grasp the contraction proof
  - Quick check: If a seminorm has a non-trivial kernel, is the mapping x → ||x|| a strict contraction?

## Architecture Onboarding

- **Component map:** Structure Learner -> Weight Estimator -> Projected SA -> Residual Reconstructor
- **Critical path:** Weight Estimator is most sensitive module. If absorption weights b̂_{i,k} are inaccurate, projection Π̂ is misaligned, and Projected SA converges to incorrect value vector
- **Design tradeoffs:**
  - Assumes generative model (can query any state); cannot run online on single trajectory without modification
  - Avoids storing n×n matrix P (stores only graph structure and weights), but requires computing projection Π̂v at every step costing O(n·|I|)
- **Failure signatures:**
  - Oscillating Values: Indicates failure to identify periodic structure (treating periodic chain as aperiodic)
  - Divergence in Transient States: Indicates poor weight estimation violating contraction condition γ̂ < 1
  - Non-constant Gain in Recurrent Class: Indicates residual reconstruction failing to average over cyclic phase correctly
- **First 3 experiments:**
  1. Aperiodic Multichain: Verify algorithm correctly identifies two distinct average rewards for two recurrent classes
  2. Deterministic Cycle (Periodicity): Run on 2-state cycle; standard TD oscillates, verify Projected SA stabilizes
  3. Ablation on Sample Budget M: Reduce samples for weight estimation to verify theoretical error floor O(Nε_b) empirically

## Open Questions the Paper Calls Out

### Open Question 1
Can the quotient-contraction framework be extended to single-trajectory learning without requiring a generative model? The authors note that a single trajectory is insufficient in general because unvisited states make their transition laws unidentifiable. Resolution would require modified algorithm with provable guarantees under trajectory-based sampling.

### Open Question 2
Can the three-phase pipeline be interleaved or performed online rather than sequentially? The current sequential dependency creates sample-complexity bottlenecks. Resolution would require unified algorithm with joint guarantees or analysis of graceful degradation when phases run concurrently.

### Open Question 3
How does the method perform when structure learning is imperfect, particularly when p_min is unknown or very small? Theorem 3 requires K ≥ p_min^{-1} log(n²/δ) for exact support recovery. Resolution would require robustness analysis under misspecified K(P) or adaptive methods handling uncertain structure.

### Open Question 4
Can the quotient framework be combined with function approximation for large or continuous state spaces? The paper focuses on finite-state chains with tabular representations. Resolution would require extension showing how to represent or approximate K(P) with features, with bounds on approximation error propagation.

## Limitations
- Requires generative model access (cannot run online on single trajectory)
- Sample complexity scales with chain structure complexity (multiple recurrent classes, periods)
- Convergence rate O(T^{-1/2}) is slower than standard TD's O(T^{-1}) in ergodic settings
- Sensitive to anchor selection and weight estimation accuracy

## Confidence

- **High:** Quotient-space contraction mechanism - mathematical framework is rigorous and well-established
- **Medium:** Gauge-fixed uniqueness - theoretically sound but sensitive to anchor selection in practice  
- **Medium:** Biased stochastic approximation - convergence proven but error floor from weight estimation can be significant

## Next Checks

1. Test on 4-state deterministic cycle with known periodicity to verify unprojected TD oscillates while projected TD converges
2. Measure actual contraction factor γ̂ on multichain MDP with known recurrent classes to validate γ̂ < 1 holds in practice
3. Perform ablation study varying absorption sample budget M to empirically verify error floor O(Nε_b) prediction