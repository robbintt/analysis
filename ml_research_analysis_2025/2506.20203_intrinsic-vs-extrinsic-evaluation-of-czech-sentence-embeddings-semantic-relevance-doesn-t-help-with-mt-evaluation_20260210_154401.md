---
ver: rpa2
title: 'Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic
  Relevance Doesn''t Help with MT Evaluation'
arxiv_id: '2506.20203'
source_url: https://arxiv.org/abs/2506.20203
tags:
- evaluation
- sentence
- translation
- embeddings
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Czech-specific and multilingual sentence embedding
  models through intrinsic and extrinsic evaluation paradigms. For intrinsic evaluation,
  it employs Costra, a complex sentence transformation dataset, and several Semantic
  Textual Similarity (STS) benchmarks to assess the ability of the embeddings to capture
  linguistic phenomena such as semantic similarity, temporal aspects, and stylistic
  variations.
---

# Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation

## Quick Facts
- **arXiv ID:** 2506.20203
- **Source URL:** https://arxiv.org/abs/2506.20203
- **Authors:** Petra Barančíková; Ondřej Bojar
- **Reference count:** 17
- **Key outcome:** Models excelling at intrinsic semantic similarity tests do not consistently perform better on downstream machine translation evaluation tasks.

## Executive Summary
This paper investigates the relationship between intrinsic and extrinsic evaluation of sentence embeddings using Czech-specific and multilingual models. Through intrinsic evaluation with Costra and STS benchmarks, and extrinsic evaluation via COMET-based fine-tuning for MT evaluation, the authors reveal a striking disconnect: high intrinsic semantic similarity scores do not predict strong performance on downstream translation evaluation tasks. Interestingly, models with seemingly over-smoothed embedding spaces can, through fine-tuning, achieve excellent results, suggesting that operationalizable semantics requires more than just semantic similarity measures.

## Method Summary
The study evaluates 9 sentence encoders (Czech-specific: CZERT, FERNET, RobeCzech; Multilingual: LaBSE, XLM-R, mE5, SimCSE, RetroMAE; and Random BERT baseline) using both intrinsic and extrinsic paradigms. Intrinsic evaluation employs Costra for complex sentence transformations and STS benchmarks for semantic similarity. For extrinsic evaluation, each embedding is fine-tuned within a COMET architecture for machine translation evaluation (MTE) and quality estimation (QE). The COMET estimator uses two hidden layers (3072→1024 for MTE, 2048→1024 for QE), AdamW optimizer with specified learning rates, and a 0.3-epoch freezing period for embeddings before fine-tuning. Performance is measured through system-level and segment-level Kendall's tau correlations with human Direct Assessment scores on WMT test sets.

## Key Results
- Models with the highest intrinsic STS scores (SimCSE) showed weak extrinsic correlations with human DA judgments
- XLM-R and FERNET embeddings, which performed poorly in intrinsic evaluation, became the best performing MTE and QE metrics after fine-tuning
- Small embeddings (SimCSE, RetroMAE) were among the worst performing COMET estimators, while large embeddings like XLM-R offered higher-dimensional spaces capturing nuanced semantic features
- The disconnect between intrinsic and extrinsic performance suggests current intrinsic benchmarks may inadequately capture operationalizable semantics needed for downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuning Can Repurpose Over-smoothed Embeddings
Over-smoothed embedding spaces can be repurposed through task-specific fine-tuning, potentially outperforming embeddings already optimized for semantic similarity. Models with initially undifferentiated representation spaces can have their embedding dimensions reweighted and separated during fine-tuning, allowing the downstream training objective to identify and amplify translation-relevant features while suppressing noise.

### Mechanism 2: Pre-training Constraints Limit Downstream Adaptation
Embeddings pre-optimized for intrinsic semantic tasks may constrain adaptation to downstream objectives with different signal requirements. Models like SimCSE, trained specifically for semantic discrimination, allocate representational capacity toward general-purpose similarity patterns, leaving fewer free dimensions for task-specific reconfiguration when fine-tuned for MT evaluation.

### Mechanism 3: Embedding Dimension Size Influences Fine-tuning Success
Embedding dimension size influences downstream fine-tuning success more than intrinsic evaluation performance. Larger embeddings provide higher-dimensional manifolds where fine-tuning can learn complex non-linear transformations via the added COMET hidden layers, while smaller embeddings bottleneck this flow regardless of their intrinsic quality.

## Foundational Learning

- **Concept:** Intrinsic vs. Extrinsic Evaluation Paradigms
  - **Why needed here:** The paper's central finding is the disconnect between these evaluation types. Intrinsic (Costra, STS) probes semantic properties; extrinsic (MTE, QE) measures downstream utility.
  - **Quick check question:** Can you explain why high STS scores might not predict good MT evaluation performance?

- **Concept:** COMET Architecture (Estimator + Quality Estimation)
  - **Why needed here:** Understanding the dual-encoder design, pooling strategies, and training setup is essential for replicating the fine-tuning experiments.
  - **Quick check question:** What are the two hidden layer sizes used in COMET estimators for MTE vs. QE?

- **Concept:** Embedding Space Geometry (Cosine Similarity, Over-smoothing)
  - **Why needed here:** The paper diagnoses over-smoothed spaces via cosine similarity between shuffled pairs. This diagnostic is critical for model selection.
  - **Quick check question:** What does it mean if a model shows SC(S,R) ≈ 1.0 even for randomly shuffled sentence pairs?

## Architecture Onboarding

- **Component map:** Sentence Encoders → COMET Estimator (Encoder → sparsemax pooling → hidden layers → regression score) → Correlation with human DA scores
- **Critical path:** Encode source, reference, and hypothesis sentences independently → Apply mixed-layer pooling with sparsemax transformation → Pass through COMET regression head → Compute Kendall's tau / Pearson correlation with human DA scores
- **Design tradeoffs:** Freezing embeddings for first 0.3 epochs preserves generalization but may slow convergence; larger embeddings improve fine-tuning but increase inference cost; reference-based (MTE) vs. reference-free (QE) shows minimal difference for top models
- **Failure signatures:** High intrinsic STS but low extrinsic correlation (SimCSE pattern); SC(S,R) ≈ 1.0 for shuffled pairs indicates unusable geometry (XLM-R raw, random BERT); negative system-level correlation (CEMTE(random BERT) = -0.35 in 2022)
- **First 3 experiments:**
  1. **Baseline geometry check:** Compute SC(S,R) for all candidate encoders on a sample of source-reference pairs; reject any model with >0.95 similarity on shuffled pairs.
  2. **Intrinsic-extrinsic correlation audit:** Plot STS scores vs. CEMTE segment-level correlations across all encoders; quantify the disconnect (expect weak or negative correlation per Figure 1).
  3. **Dimension ablation:** Fine-tune COMET on the same encoder with projected embeddings (1024 → 512 → 256) to isolate the effect of dimensionality independent of pre-training quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed disconnect between intrinsic semantic scores and downstream MT evaluation performance generalize to languages other than Czech?
- Basis in paper: [explicit] The authors state in the conclusion their intent to "replicate these experiments across multiple languages to investigate whether the observed behavior is specific to the Czech language."
- Why unresolved: The study is limited to English-to-Czech, leaving open the possibility that the lack of correlation is specific to the morphological complexity of Czech or the specific models tested.

### Open Question 2
- Question: Can intrinsic evaluation methods be designed to better capture the semantic nuances required for machine translation evaluation?
- Basis in paper: [explicit] The paper concludes that "intrinsic criteria alone appear inadequate" and emphasizes the need for "better targeted intrinsic evaluation approaches that reflect downstream application requirements."
- Why unresolved: The paper demonstrates that current intrinsic benchmarks (Costra, STS) show weak or negative correlations with fine-tuned MT evaluation performance.

### Open Question 3
- Question: Does fine-tuning on MT evaluation tasks succeed on "over-smoothed" embeddings by re-weighting dimensions that were previously indistinguishable via cosine similarity?
- Basis in paper: [inferred] The authors hypothesize that models like XLM-R have "over-smoothed" spaces but can be "effectively reconfiguring" them during fine-tuning, noting that "thorough testing is beyond the scope of this article."
- Why unresolved: It is unclear if the fine-tuning process utilizes pre-existing but latent semantic features or if it reshapes the space entirely from scratch.

## Limitations
- The study focuses on a single language pair (English-to-Czech), limiting generalizability to other language families and morphological systems
- The analysis of embedding geometry provides diagnostic insight but doesn't establish causal mechanisms for why fine-tuning succeeds with poor-intrinsic models
- The study focuses on a single downstream task family (MT evaluation), limiting generalizability to other NLP applications

## Confidence

- **High confidence:** The empirical finding that intrinsic and extrinsic evaluation performance are weakly correlated (supported by direct correlation plots and quantitative comparisons)
- **Medium confidence:** The claim about over-smoothing benefiting fine-tuning (supported by XLM-R results but lacks ablation studies isolating the effect)
- **Medium confidence:** The dimension size hypothesis (supported by empirical ranking but not systematically tested across embedding families)

## Next Checks

1. Conduct embedding dimension ablation studies: Fine-tune COMET using projected versions of the same encoder (1024→512→256) to isolate dimensionality effects from pre-training quality
2. Test cross-linguistic generalization: Repeat the intrinsic-extrinsic correlation analysis on English and German MT evaluation datasets to determine if the disconnect is language-specific
3. Implement embedding geometry intervention: Apply dimensionality reduction (PCA) to high-performing but over-smoothed embeddings, then re-evaluate both intrinsic STS and extrinsic MTE performance to test whether geometric constraints drive the observed effects