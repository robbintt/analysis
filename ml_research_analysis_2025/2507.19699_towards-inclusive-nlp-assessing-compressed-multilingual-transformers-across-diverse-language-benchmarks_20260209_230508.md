---
ver: rpa2
title: 'Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across
  Diverse Language Benchmarks'
arxiv_id: '2507.19699'
source_url: https://arxiv.org/abs/2507.19699
tags:
- arabic
- arxiv
- multilingual
- language
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of multilingual and monolingual
  Large Language Models (LLMs) across Arabic, English, and Kannada languages, focusing
  on the effects of model compression strategies like pruning and quantization. The
  study benchmarks six open-access LLMs (BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and
  AraGPT) on standard datasets including ArabicMMLU, EnglishMMLU, and Kannada-ARC-C-2.5K.
---

# Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks

## Quick Facts
- **arXiv ID**: 2507.19699
- **Source URL**: https://arxiv.org/abs/2507.19699
- **Reference count**: 30
- **Primary result**: Multilingual models outperform monolingual ones in low-resource languages; quantization preserves accuracy better than aggressive pruning.

## Executive Summary
This paper evaluates six open-access LLMs across Arabic, English, and Kannada languages, focusing on the effects of model compression strategies like pruning and quantization. The study benchmarks models on standard datasets including ArabicMMLU, EnglishMMLU, and Kannada-ARC-C-2.5K. Results show that multilingual models outperform monolingual ones, especially in low-resource settings. Quantization (4-bit and 8-bit) maintains model accuracy with minimal loss, while aggressive pruning significantly degrades performance, particularly in larger models. The findings highlight the importance of model size, training diversity, and compression techniques for developing scalable and fair multilingual NLP solutions.

## Method Summary
The study evaluates six open-access LLMs (BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT) on standard datasets including ArabicMMLU, EnglishMMLU, and Kannada-ARC-C-2.5K. Models are compressed using post-training quantization (4-bit and 8-bit) and unstructured L1 pruning at 20%, 40%, and 80% sparsity. Zero-shot inference is performed on each dataset, measuring accuracy across languages. The evaluation focuses on the trade-offs between model efficiency and cross-lingual performance, particularly in low-resource settings.

## Key Results
- Multilingual models outperform monolingual counterparts in low-resource languages (Arabic, Kannada) due to cross-lingual transfer benefits.
- Quantization (4-bit and 8-bit) maintains model accuracy with minimal loss (< 2%), while aggressive pruning (80% sparsity) significantly compromises performance.
- Pruning-induced cross-lingual drift is evident: at 40% sparsity, BLOOMZ-7.1B’s accuracy drops by 7.9% on Kannada compared to 3.1% on English.

## Why This Works (Mechanism)

### Mechanism 1: Precision Reduction via Quantization
- **Claim:** Reducing model precision to 4-bit or 8-bit integers preserves cross-lingual reasoning accuracy significantly better than removing parameters via pruning.
- **Mechanism:** Quantization maps floating-point weights to lower-bit integers (e.g., FP16 → INT4). By retaining the network topology and the relative magnitude of weights, the model maintains the structural pathways required for multilingual reasoning while drastically cutting memory usage.
- **Core assumption:** The semantic knowledge required for multilingual tasks is distributed across the network's structure rather than concentrated in the precise numeric values of individual weights.
- **Evidence anchors:**
  - [abstract] "Quantization (4-bit and 8-bit) is effective in maintaining model accuracy... aggressive pruning significantly compromises performance."
  - [section 5.1] "Quantisation is essentially cost-free: converting any model to 8-bit... alters accuracy by < 2%."
  - [corpus] *Statement-Tuning Enables Efficient Cross-lingual Generalization* (supports that efficient generalization is structurally robust).
- **Break condition:** If the target language relies heavily on subtle syntactic nuances encoded in high-precision weights, aggressive sub-4-bit quantization may cause "cliff" performance drops not fully explored in this study.

### Mechanism 2: Cross-Lingual Transfer via Shared Embeddings
- **Claim:** Multilingual models outperform monolingual counterparts in low-resource settings (specifically Arabic/Kannada) due to cross-lingual knowledge transfer.
- **Mechanism:** Models like BLOOMZ pre-train on diverse languages simultaneously, creating a shared embedding space. This allows the model to apply reasoning patterns learned from high-resource data (English) to low-resource languages (Kannada), compensating for data scarcity.
- **Core assumption:** Linguistic features and logical reasoning patterns are partially language-agnostic and can be abstracted across distinct linguistic families.
- **Evidence anchors:**
  - [abstract] "Multilingual versions of the model outperform their language-specific counterparts... indicating substantial cross-lingual transfer benefits."
  - [section 5.4] "Multilingual models like BLOOMZ-7.1B outperform monolingual Arabic models... shared cross-lingual embeddings... improve robustness."
  - [corpus] *Statement-Tuning Enables Efficient Cross-lingual Generalization* (aligns with the premise of robust cross-lingual generalization).
- **Break condition:** Transfer fails when the target language (e.g., Kannada) lacks sufficient lexical overlap or script similarity with the high-resource pre-training data, leading to isolated performance floors.

### Mechanism 3: Pruning-Induced Forgetting in Low-Resource Languages
- **Claim:** Aggressive weight pruning degrades performance in low-resource languages faster than in high-resource languages (English).
- **Mechanism:** Weights associated with low-resource languages are often fewer and less redundantly encoded. Unstructured pruning (removing lowest magnitude weights) disproportionately erases these sparse representations, causing "cross-lingual drift."
- **Core assumption:** "Important" weights for low-resource languages may have lower magnitude than those for dominant languages, making them vulnerable to magnitude-based pruning criteria.
- **Evidence anchors:**
  - [section 5.3] "Pruning-induced cross-lingual drift is evident: at 40% sparsity, BLOOMZ-7.1B’s accuracy drops by 7.9% on Kannada, compared to... 3.1% on English."
  - [section 5.2] "Aggressive pruning significantly compromises performance, especially in bigger models."
  - [corpus] *Poly-FEVER* (suggests multilingual inconsistencies persist, though this paper focuses specifically on compression artifacts).
- **Break condition:** Performance collapses entirely when sparsity exceeds the "safe zone" (approx. 40%), effectively stripping the model of its capacity to represent under-trained languages.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** The study relies heavily on PTQ (4-bit/8-bit) as a primary efficiency lever. Understanding the difference between FP16 and INT4 is essential to interpret the "minimal loss" results.
  - **Quick check question:** How does the scale factor ($S$) and zero point ($Z$) map a continuous floating-point weight to a discrete integer in Eq. (2)?

- **Concept: Unstructured vs. Structured Pruning**
  - **Why needed here:** The paper utilizes L1 unstructured pruning. Knowing that this removes individual weights globally (rather than entire neurons/heads) explains why the model size shrinks but hardware acceleration can be inconsistent.
  - **Quick check question:** Why does removing 80% of individual weights (unstructured) lead to a larger accuracy drop in larger models compared to smaller ones, as seen in Table 2?

- **Concept: Zero-Shot Cross-Lingual Transfer**
  - **Why needed here:** To understand why models like AceGPT (English-tuned) or BLOOMZ (Multilingual) are being tested on Kannada without specific fine-tuning.
  - **Quick check question:** What property of the Transformer architecture allows a model trained on English reasoning tasks to answer questions in Kannada?

## Architecture Onboarding

- **Component map:**
  - **Backbone:** Decoder-only Transformer architectures (LLaMA-2, BLOOMZ, Jais).
  - **Compression Layer:** Post-hoc applied modules:
    - *Quantizer:* `AutoModelForCausalLM.from_pretrained(..., load_in_8bit=True)`
    - *Pruner:* L1-norm filter applied to `linear_layers` (Eq. 1).
  - **Evaluation Head:** Logit extraction at the final token position to select the max-probability answer choice.

- **Critical path:**
  1. **Model Load:** Initialize model (e.g., BLOOMZ-7.1B).
  2. **Compression:** Apply 20%, 40%, or 80% pruning OR 4-bit/8-bit quantization.
  3. **Prompting:** Inject subject/meta-data (e.g., "Subject: Islamic Studies") into the context window.
  4. **Inference:** Extract logits for answer tokens (A, B, C, D).

- **Design tradeoffs:**
  - **Quantization vs. Pruning:** The paper clearly favors Quantization. Use 4-bit for memory saving with <2% accuracy cost. Avoid Pruning >40% unless model size is the only constraint and accuracy is secondary.
  - **Model Scale:** Larger models (13B) resist moderate pruning better than smaller ones initially, but suffer catastrophic collapse at 80% sparsity (Table 2).

- **Failure signatures:**
  - **The "Pruning Cliff":** Accuracy remains stable up to ~20-40% sparsity, then collapses at 80% (e.g., AceGPT-13B drops to 8.82% in Humanities, Table 6).
  - **Low-Resource Drift:** If Kannada accuracy drops significantly more than English accuracy under the same compression, you are observing "Pruning-Induced Cross-Lingual Drift."

- **First 3 experiments:**
  1. **Baseline Profiling:** Run BLOOMZ-560M vs. BLOOMZ-7.1B on EnglishMMLU to establish the "scale advantage" delta.
  2. **Safe Compression Test:** Apply 8-bit quantization to BLOOMZ-7.1B and verify that accuracy delta is < 1% (validating Section 5.1).
  3. **Stress Test:** Apply 80% pruning to AceGPT-13B and confirm the "collapse" phenomenon (accuracy dropping to near-random or lower than small models).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do advanced compression techniques like movement pruning or knowledge distillation preserve the linguistic richness of low-resource languages better than standard L1 pruning?
- Basis: [explicit] The authors explicitly identify "movement pruning, knowledge distillation, and enhanced quantization" as critical future work for optimizing efficiency.
- Why unresolved: The study was limited to standard L1 unstructured pruning and basic quantization, leaving sophisticated methods untested.
- What evidence would resolve it: Comparative benchmarks showing accuracy retention on ArabicMMLU and Kannada datasets when applying these advanced techniques versus standard pruning.

### Open Question 2
- Question: How does model compression impact performance on dialectal Arabic and code-switching scenarios compared to Modern Standard Arabic?
- Basis: [explicit] Section 6 states the study ignored "dialectal differences and the domain of code-switching phenomena," identifying them as promising avenues for future investigation.
- Why unresolved: The utilized benchmarks (ArabicMMLU) primarily assess Modern Standard Arabic, failing to capture the degradation effects on dialects.
- What evidence would resolve it: Evaluation of compressed models on dialect-specific datasets (e.g., dialectal portions of ALGhafa) or code-switched corpora.

### Open Question 3
- Question: Can few-shot prompting effectively mitigate the accuracy degradation caused by aggressive pruning in low-resource languages?
- Basis: [inferred] The limitations section notes that experiments "only included zero-shot scenarios and did not test the impact of a few-shot setting."
- Why unresolved: It remains unknown if providing in-context examples can recover the reasoning capabilities lost through weight removal in pruned models.
- What evidence would resolve it: A comparative study measuring the accuracy delta between zero-shot and few-shot prompts on the Indic-Benchmark for models at 40% and 80% sparsity.

## Limitations
- **Synthetic Benchmark Dependency**: Results rely on standardized MMLU and Kannada-ARC-C-2.5K datasets. Real-world performance in low-resource, domain-specific applications (e.g., medical, legal) may diverge significantly from these controlled benchmarks.
- **Compression Scope**: Only post-training quantization (PTQ) and unstructured L1 pruning are evaluated. Advanced compression methods like quantization-aware training (QAT), structured pruning, or distillation are excluded, limiting generalizability.
- **Language Coverage Bias**: The study focuses on Arabic, English, and Kannada. Findings may not extrapolate reliably to other low-resource languages with different scripts or linguistic families.

## Confidence
- **High Confidence**: Quantization maintains accuracy with minimal loss (< 2% across 4-bit/8-bit settings). This is empirically validated across multiple models and tasks.
- **Medium Confidence**: Multilingual models outperform monolingual ones in low-resource settings due to cross-lingual transfer. While supported by results, the underlying transfer mechanisms are not deeply analyzed.
- **Low Confidence**: Pruning-induced cross-lingual drift disproportionately affects low-resource languages. The study observes this trend but does not fully explain the underlying mechanisms or provide mitigation strategies.

## Next Checks
1. **Domain Transfer Test**: Evaluate compressed models on domain-specific datasets (e.g., biomedical or legal texts) to assess real-world robustness beyond standardized benchmarks.
2. **Compression Method Comparison**: Benchmark PTQ against QAT and structured pruning to identify optimal trade-offs between accuracy, efficiency, and deployment feasibility.
3. **Language Generalization Study**: Extend evaluation to additional low-resource languages (e.g., Swahili, Tamil) to test the scalability of cross-lingual transfer and compression resilience.