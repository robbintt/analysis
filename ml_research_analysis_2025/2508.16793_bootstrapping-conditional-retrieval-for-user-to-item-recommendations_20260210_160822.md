---
ver: rpa2
title: Bootstrapping Conditional Retrieval for User-to-Item Recommendations
arxiv_id: '2508.16793'
source_url: https://arxiv.org/abs/2508.16793
tags:
- retrieval
- user
- condition
- items
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of conditional retrieval in
  recommendation systems, where retrieved items must satisfy specific conditions (e.g.,
  topic relevance) in addition to user engagement. The authors propose a method that
  adapts standard two-tower models by incorporating item-side features as artificial
  conditions in the user tower, enabling feature interactions between user and condition.
---

# Bootstrapping Conditional Retrieval for User-to-Item Recommendations

## Quick Facts
- arXiv ID: 2508.16793
- Source URL: https://arxiv.org/abs/2508.16793
- Reference count: 8
- One-line primary result: CR model +0.26% WAU when deployed for topic-based notifications at Pinterest

## Executive Summary
This paper addresses conditional retrieval in recommendation systems, where items must satisfy specific conditions (e.g., topic relevance) alongside user engagement. The authors propose adapting standard two-tower models by incorporating item-side features as artificial conditions during training, enabling bootstrapping of new conditional retrieval use cases without explicit training data. Experiments show the CR model outperforms standard two-tower models with filters on both engagement metrics and condition relevance, achieving 82.8% topic matching rate versus 20.3% for logistic regression. When deployed at Pinterest for topic-based notifications, the CR model led to a +0.26% increase in weekly active users.

## Method Summary
The method modifies standard two-tower architectures by adding a Condition Extraction Module that samples conditions from item metadata (e.g., topics) during training. These sampled conditions are injected into the user tower's embedding layer through feature crossing layers, enabling the model to learn user-condition-item interactions. The model trains on standard engagement data using contrastive learning with in-batch negatives and sampled softmax loss. At inference, the trained model can handle arbitrary query conditions even if they never appeared during training, enabling efficient conditional retrieval via ANN search.

## Key Results
- CR model achieves 82.8% topic matching rate vs. LR's 20.3% without filters
- CR outperforms standard two-tower models with filters on engagement metrics
- Production deployment at Pinterest yielded +0.26% increase in weekly active users

## Why This Works (Mechanism)

### Mechanism 1
Injecting item-side metadata as artificial conditions during training enables the model to learn user-condition-item interactions without explicit labeled data. The Condition Extraction Module samples conditions from item metadata, which are fed into the user tower's embedding layer, creating conditional user embeddings that represent "what this user wants given this condition." At inference, real conditions can be injected even if they never appeared in training data. This works because item metadata is sufficiently reliable to approximate the distribution of real conditional queries.

### Mechanism 2
Feature crossing layers in the user tower enable higher-order interactions between user features and condition embeddings, improving relevance over naive filtering. Standard two-tower models keep user and item features separate until dot-product scoring. The conditional user tower fuses the condition embedding with user features through MLP layers before scoring, allowing the model to learn nuanced patterns like "user X prefers condition Y differently than user Z."

### Mechanism 3
Training with contrastive learning on engaged user-item pairs, using batch negatives, transfers to conditional retrieval without requiring (user, condition, item) triplets. The model trains on standard engagement data with conditions injected from items. The contrastive loss pushes the conditional user embedding closer to the item embedding while pushing apart from in-batch negatives. This works because the condition signal is informative enough that learning to retrieve items matching their own conditions generalizes to retrieving items matching arbitrary query conditions.

## Foundational Learning

- **Concept: Two-tower architecture with ANN serving**
  - Why needed here: The method modifies a standard two-tower model; understanding the baseline is prerequisite
  - Quick check question: Explain why two-tower models enable efficient ANN retrieval but limit feature interaction modeling

- **Concept: Contrastive learning with sampled softmax**
  - Why needed here: The training objective uses batch negatives and sampled softmax; understanding this clarifies why no explicit negative sampling is needed
  - Quick check question: What is the effect of using in-batch negatives vs. hard negatives for conditional retrieval?

- **Concept: Multi-objective optimization in retrieval**
  - Why needed here: Conditional retrieval balances engagement and condition relevance
  - Quick check question: Why might a model that optimizes only engagement fail at conditional retrieval even with post-filtering?

## Architecture Onboarding

- **Component map:**
  Training Pipeline: Raw data → Engaged (user, item) pairs → Item → Condition Extraction Module → Sampled condition → User features + Condition → Conditional User Tower → User embedding → Dot product → Sampled softmax loss
  Item features → Item Tower → Item embedding → Dot product → Sampled softmax loss
  Serving Pipeline: User features + Query condition → Conditional User Tower → User embedding → ANN search over item embeddings → Retrieved candidates → Optional: Streaming filter for condition relevance

- **Critical path:** Condition Extraction Module correctness → Conditional User Tower training → ANN index consistency (item embeddings from same model)

- **Design tradeoffs:**
  - Filter vs. no filter at serving: CR without filter achieves 82.8% relevance but may return off-topic items; adding filter guarantees relevance but adds serving cost and may exclude "somewhat relevant but highly engaging" items
  - Condition sampling strategy: Random sampling is simple but may underrepresent tail conditions; stratified sampling could help but adds complexity
  - Negative sampling: Current method uses in-batch negatives; hard negatives sharing the same condition could improve discrimination (noted as future work)

- **Failure signatures:**
  - Low condition matching rate at inference: Likely mismatch between training condition distribution and query conditions
  - High serving cost with filter: Model learned weak condition relevance; check condition embedding fusion depth
  - No improvement over baseline: Condition signal may be uninformative; verify condition extraction module outputs

- **First 3 experiments:**
  1. Ablation on condition fusion depth: Compare embedding-level fusion vs. deeper MLP crossing to validate Mechanism 2
  2. Condition sampling strategies: Compare random vs. stratified vs. popularity-weighted sampling to test sensitivity to condition distribution
  3. Relevance-engagement tradeoff curve: Vary filter thresholds on CR output to characterize the multi-objective frontier and confirm "neutral impact" finding

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating hard negatives that share the same condition into the sampled softmax module significantly improve the model's discriminative power?
  - Basis in paper: The authors state in Section 4: "In the future, we plan to improve the model performance by (1) including hard negatives with the same condition in sampled softmax module."
  - Why unresolved: The current model uses in-batch negatives which may not be sufficiently challenging for learning fine-grained distinctions within specific condition classes.
  - What evidence would resolve it: Offline relevance metrics and online A/B test results comparing the current baseline against a model trained with condition-specific hard negative sampling.

- **Open Question 2:** Does adding an explicit loss term between the conditional user embedding and the target condition embedding improve relevance without hurting engagement?
  - Basis in paper: The authors list as future work: "(2) adding a loss between conditional user embedding and target condition to boost relevance."
  - Why unresolved: The current architecture relies on the dot product between user and item embeddings; an explicit constraint on the user embedding itself is hypothesized to help but remains untested.
  - What evidence would resolve it: Experimental results showing the trade-off curve between condition matching rates (relevance) and user engagement metrics (CTR/WAU) when this auxiliary loss is applied.

- **Open Question 3:** Is random sampling the most effective strategy for the Condition Extraction Module, or would weighted sampling improve training robustness?
  - Basis in paper: The paper notes that for the Condition Extraction Module, they "randomly sample a topic from it [item features]" during training.
  - Why unresolved: While effective, random sampling ignores the potential variance in signal quality or prominence of different topics associated with an item, which could introduce noise during the bootstrapping process.
  - What evidence would resolve it: Ablation studies comparing model performance when training conditions are selected randomly versus strategies based on topic confidence scores or frequency.

## Limitations

- **Training data sensitivity:** The method relies on item-side features being representative of real query conditions; performance degrades if metadata distribution differs substantially from actual user queries
- **Architecture specifics:** Critical implementation details are underspecified—tower depths, hidden dimensions, MLP layer configurations, and condition embedding integration methods are not disclosed
- **Generalization scope:** Effectiveness across diverse condition types and domains remains unproven beyond the Pinterest topic feed use case

## Confidence

- **High confidence:** The core mechanism of using item-side features as artificial conditions for training conditional retrieval is well-supported by both theory and empirical results
- **Medium confidence:** The claim of "neutral impact" on engagement metrics is supported but lacks comprehensive tradeoff analysis across multiple operating points
- **Medium confidence:** The superiority over filtering approaches is demonstrated, but the comparison assumes equal computational costs, which may not hold in all scenarios

## Next Checks

1. **Condition sampling sensitivity analysis:** Systematically vary the condition sampling strategy (random, stratified, popularity-weighted) and measure impact on tail condition performance and overall relevance metrics

2. **Ablation on condition fusion depth:** Compare embedding-level condition fusion vs. deeper MLP crossing layers to isolate the contribution of higher-order feature interactions to conditional retrieval performance

3. **Relevance-engagement tradeoff curve:** Vary filter thresholds on CR output and plot the precision-recall curve for condition relevance against engagement metrics to characterize the full multi-objective frontier and validate the "neutral impact" claim across operating points