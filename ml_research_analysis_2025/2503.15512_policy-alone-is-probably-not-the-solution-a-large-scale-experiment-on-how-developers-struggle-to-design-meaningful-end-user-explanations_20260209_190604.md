---
ver: rpa2
title: 'Policy alone is probably not the solution: A large-scale experiment on how
  developers struggle to design meaningful end-user explanations'
arxiv_id: '2503.15512'
source_url: https://arxiv.org/abs/2503.15512
tags:
- policy
- explanations
- participants
- developers
- experiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Developers struggle to create meaningful end-user explanations
  for ML systems. In two controlled experiments with 194 participants, we found that
  developers frequently produced explanations that were technically flawed, difficult
  to interpret, or targeted to other developers rather than end users.
---

# Policy alone is probably not the solution: A large-scale experiment on how developers struggle to design meaningful end-user explanations

## Quick Facts
- arXiv ID: 2503.15512
- Source URL: https://arxiv.org/abs/2503.15512
- Reference count: 40
- Primary result: Developers produce explanations that are inscrutable to end users and policy enforcement alone does not improve explanation quality

## Executive Summary
In two controlled experiments with 194 participants, developers consistently failed to create meaningful end-user explanations for ML systems, even when policy guidance and enforcement were applied. Participants used standard explainability tools (LIME, SHAP, Anchors) to produce explanations that were technically flawed, difficult to interpret, or framed for developers rather than patients and nurses. Policy enforcement increased formal compliance with requirements but did not improve the quality or accessibility of explanations. The study reveals that policy alone is insufficient to ensure responsible AI practices and that additional training, tools, or expert support is necessary.

## Method Summary
The study conducted two experiments with graduate-level ML/SE participants. Participants received 160 minutes of lectures and 80 minutes of labs on explainability techniques (LIME, Anchors, SHAP). They were tasked with creating global explanations (for nurses) and individual explanations (for patients) for a diabetic retinopathy classification model, using the APTOS 2019 Blindness Detection dataset and a pre-trained ResNet50 model with synthetic age/gender augmentation. Six policy conditions were tested in Experiment 1, with stricter enforcement in Experiment 2. Outputs were evaluated using qualitative content analysis for compliance and failure modes, and Flesch-Kincaid Grade Level for readability.

## Key Results
- 88% of all solutions were likely inscrutable to end users without ML expertise
- Zero participants achieved the required 8th-grade reading level for plain language
- Policy enforcement increased formal compliance (e.g., reporting limitations from ~30-50% to 87%) but did not improve explanation quality
- Common failures included ML/medical jargon, inscrutable visualizations, and developer-centric framing

## Why This Works (Mechanism)

### Mechanism 1: Technical-to-User Translation Gap
Developers with technical ML training default to producing explanations meaningful to themselves, not to end users. They used off-the-shelf explainability tools without adapting outputs for non-technical audiences, producing confusion matrices and unlabeled visualizations requiring ML expertise to interpret.

### Mechanism 2: Policy-Compliance Decoupling
Policy guidance and enforcement can increase formal compliance without improving explanation quality. Enforcement increased compliance with specific requirements but did not improve plain language or accessibility, as participants adopted "check-the-box" compliance without engaging with policy purpose.

### Mechanism 3: Perspective-Taking Deficit
Developers fail to anticipate what information non-technical stakeholders need or can comprehend. They did not conduct user research or test explanations with representative users, producing explanations that answered questions developers would ask rather than questions patients or nurses would ask.

## Foundational Learning

- **Human-Centered Explainable AI vs. Technical XAI**
  - Why needed here: Participants conflated debugging explanations with end-user explanations
  - Quick check question: Can you explain why a SHAP feature importance plot that helps you debug a model might confuse a patient?

- **Stakeholder Analysis and Personas**
  - Why needed here: Most participants produced generic explanations without considering different information needs
  - Quick check question: What information would a nurse need before acting on an ML screening result that a patient would not?

- **Plain Language and Readability Standards**
  - Why needed here: Zero participants met the 8th-grade reading level requirement
  - Quick check question: How would you rewrite "The model uses a convolutional neural network to detect microaneurysms" for a patient?

## Architecture Onboarding

- **Component map**: ML model + predictions + policy requirements → Explanation generation (LIME/SHAP/Anchors) → Translation layer (technical-to-user) → Contextualization (workflow integration) → Stakeholder-specific explanations → User testing/validation loop
- **Critical path**: Understanding end-user information needs → selecting appropriate explanation techniques → translating technical outputs to plain language → embedding in workflow context → validating with representative users
- **Design tradeoffs**: Technical completeness vs. comprehensibility, Standardization vs. personalization, Automation vs. human-in-the-loop
- **Failure signatures**: Unlabeled visualizations, jargon without definition, missing actionability, technical metrics for non-technical users
- **First 3 experiments**:
  1. Create explanations for the same ML output targeting three different personas (ML engineer, domain expert, end user); compare what information each requires
  2. Take an existing technical explanation and iteratively rewrite for 8th-grade reading level; test with readability tools
  3. Present your explanation to someone unfamiliar with ML; ask them to paraphrase what they understand and identify confusions

## Open Questions the Paper Calls Out

- What specific training interventions effectively improve developers' ability to design user-centered ML explanations?
- Can LLM-based tools effectively support developers in creating and evaluating end-user explanations?
- What forms of concrete implementation guidance for policy requirements would most effectively improve explanation quality beyond formal compliance?
- Would professional developers with industry experience and domain expertise produce higher-quality end-user explanations than the graduate student population studied here?

## Limitations

- Single dataset (diabetic retinopathy classification) and specific explainability tools limit generalizability
- Synthetic age/gender data augmentation method is not fully specified
- Graduate student sample may not represent professional developers or domain experts

## Confidence

- High confidence: Policy-compliance decoupling mechanism
- Medium confidence: Technical-to-user translation gap
- Medium confidence: Perspective-taking deficit

## Next Checks

1. Replicate the study with a different high-stakes domain (e.g., criminal justice risk assessment) to test generalizability
2. Conduct eye-tracking or think-aloud protocols with actual end users reviewing developer-created explanations
3. Test whether targeted training in user-centered design principles improves explanation quality beyond what policy enforcement achieves