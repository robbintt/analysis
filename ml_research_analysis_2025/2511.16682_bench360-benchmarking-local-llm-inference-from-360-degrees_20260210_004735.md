---
ver: rpa2
title: 'Bench360: Benchmarking Local LLM Inference from 360 Degrees'
arxiv_id: '2511.16682'
source_url: https://arxiv.org/abs/2511.16682
tags:
- inference
- energy
- metrics
- latency
- bench360
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bench360, a comprehensive framework for benchmarking
  local LLM inference across models, inference engines, quantization schemes, and
  deployment scenarios. The framework evaluates both task-specific quality (e.g.,
  accuracy, F1, ROUGE) and system metrics (latency, throughput, energy, memory usage,
  cold start time).
---

# Bench360: Benchmarking Local LLM Inference from 360 Degrees

## Quick Facts
- arXiv ID: 2511.16682
- Source URL: https://arxiv.org/abs/2511.16682
- Reference count: 24
- One-line primary result: Comprehensive benchmarking framework evaluating LLM inference across models, engines, quantization, and deployment scenarios reveals no universal winner—optimal configurations depend on task, hardware, and workload.

## Executive Summary
Bench360 introduces a comprehensive benchmarking framework for local LLM inference that evaluates task-specific quality metrics alongside system performance indicators including latency, throughput, energy, memory, and cold start time. The study systematically compares three NVIDIA GPUs (L4, A10, A30), four inference engines (vLLM, SGLang, LMDeploy, TGI), and multiple quantization schemes across four NLP tasks. Results demonstrate that optimal deployment configurations depend critically on the specific combination of task requirements, hardware constraints, and workload patterns, with no single engine or quantization level dominating across all scenarios.

## Method Summary
The framework evaluates model quality using accuracy, F1, and ROUGE scores across four NLP tasks, then measures system metrics including time-to-first-token (TTFT), tokens-per-second (TPOT), energy consumption, GPU utilization, and cold start latency. Experiments cover four inference engines, three GPUs, and various quantization schemes on models ranging from 1.3B to 70B parameters. The study controls for batch size, context length, and concurrent request patterns to isolate the effects of quantization, engine architecture, and hardware characteristics on performance.

## Key Results
- No single inference engine dominates across all scenarios—LMDeploy excels at single-stream responsiveness, SGLang leads in batch throughput on mid-tier GPUs, and vLLM dominates on high-end hardware and under concurrent load
- Quantization enables deployment of larger models within memory constraints but introduces significant computational overhead that increases latency and energy consumption
- vLLM achieves the best energy efficiency through effective scheduling despite moderate GPU utilization, while larger models show diminishing returns on high-end GPUs

## Why This Works (Mechanism)
The framework works by systematically decoupling the effects of model architecture, quantization, inference engine, and hardware on both quality and system metrics. By maintaining consistent task definitions and measurement protocols across configurations, it enables direct comparison of trade-offs between model size, computational efficiency, and response quality.

## Foundational Learning
1. **Quantization Impact**: Why needed—determines memory efficiency vs. computational overhead trade-offs
   Quick check—compare GPTQ 4-bit vs 8-bit latency on identical hardware

2. **Inference Engine Architecture**: Why needed—different scheduling and optimization strategies significantly affect performance
   Quick check—measure TTFT vs throughput trade-off across engines

3. **Hardware-Software Co-design**: Why needed—GPU characteristics interact differently with engine optimizations
   Quick check—compare engine performance scaling from L4 to A30

4. **Concurrent Request Handling**: Why needed—real-world deployments require understanding multi-stream behavior
   Quick check—measure latency distribution under 1-4 concurrent requests

5. **Cold Start Overhead**: Why needed—startup latency affects interactive applications
   Quick check—measure TTFT variance across multiple engine initializations

6. **Energy Efficiency Metrics**: Why needed—sustainability considerations increasingly important for deployment
   Quick check—compare energy per token across quantization levels

## Architecture Onboarding

**Component Map**
User Request -> Inference Engine -> Model Loader -> GPU Execution -> Response Generation

**Critical Path**
User request arrival → Engine processing → Model loading/initialization → GPU inference → Token generation → Response delivery

**Design Tradeoffs**
Single-stream responsiveness (LMDeploy) vs. batch throughput (SGLang) vs. concurrent request handling (vLLM); memory efficiency (quantization) vs. computational overhead; cold start latency vs. sustained performance

**Failure Signatures**
High TTFT with low TPOT indicates engine optimization for responsiveness over throughput; consistently high GPU utilization suggests memory-bound operations; energy spikes during quantization loading indicate inefficient memory transfers

**First 3 Experiments**
1. Measure TTFT and TPOT for a 7B parameter model across all four engines on L4 GPU
2. Compare energy consumption for 4-bit vs 8-bit quantization of the same model on A10
3. Evaluate concurrent request latency distribution for vLLM vs LMDeploy with 4 simultaneous requests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative quantization techniques (e.g., AWQ, mixed-precision) compare to GPTQ regarding the quality-efficiency trade-offs in local deployments?
- Basis in paper: [explicit] The "Limitations" section states that experiments focused primarily on GPTQ and that extending evaluation to alternative techniques is needed as ecosystem support matures.
- Why unresolved: Current benchmarking was constrained by inference engine support for formats other than GPTQ at the time of writing.
- What evidence would resolve it: Bench360 results comparing model quality (accuracy, F1) and system costs (latency, energy) across AWQ and mixed-precision formats on identical hardware.

### Open Question 2
- Question: Do the observed optimal configurations for single-GPU setups generalize to multi-GPU deployments utilizing tensor parallelism or CPU offloading?
- Basis in paper: [explicit] The authors restrict evaluation to single-GPU deployments to reflect common local designs but list multi-GPU inference and CPU offloading as important directions for future investigation.
- Why unresolved: The study explicitly excludes the complexity of multi-device memory management and scheduling strategies found in distributed environments.
- What evidence would resolve it: Extending Bench360 tests to multi-GPU clusters to measure if LMDeploy or vLLM maintain their dominance in latency and throughput under distributed workloads.

### Open Question 3
- Question: How do specialized inference runtimes like llama.cpp, ExLlamaV2, and TensorRT-LLM perform relative to the evaluated engines within a standardized framework?
- Basis in paper: [explicit] The "Limitations" section notes that while other popular runtimes exist, they rely on specialized formats that hinder controlled comparison, making their inclusion a future goal.
- Why unresolved: The paper focused on engines supporting standardized APIs (OpenAI-compatible) and Hugging Face integration, excluding engines with proprietary optimizations.
- What evidence would resolve it: Integrating these engines into the Bench360 abstraction layer to compare their specific hardware optimizations against the current baseline.

### Open Question 4
- Question: Can pruning techniques effectively mitigate the computational overhead observed when deploying large quantized models on memory-constrained hardware?
- Basis in paper: [inferred] Section 4.2 notes that while quantization reduces memory bottlenecks, it fails to reduce computation, leading to high latency/energy costs, and suggests pruning could address this.
- Why unresolved: The paper evaluates quantization extensively but does not include experimental data on pruning strategies to balance the compute-vs-memory trade-off.
- What evidence would resolve it: Experiments measuring TPOT and energy per token for large, pruned-and-quantized models versus purely quantized models within the 24GB VRAM budget.

## Limitations

- Hardware scope limited to three NVIDIA GPUs, excluding AMD, Intel, and mobile architectures
- Evaluation focused on four NLP tasks, potentially limiting generalizability to other domains
- Does not explore advanced optimization techniques like speculative decoding or continuous batching
- Cold start measurements may be influenced by uncontrolled system-specific factors

## Confidence

- High confidence in relative engine performance rankings under tested conditions
- Medium confidence in quantization overhead findings across multiple models and engines
- Medium confidence in energy efficiency conclusions from dedicated power meter measurements
- Low confidence in absolute latency/throughput numbers for production deployment extrapolation

## Next Checks

1. **Cross-Platform Validation**: Replicate benchmark results on AMD Instinct MI300X and Intel Gaudi 3 GPUs to assess hardware vendor dependency of observed performance patterns, particularly for SGLang's kernel optimizations and vLLM's scheduling efficiency.

2. **Extended Task Diversity**: Evaluate the same engine/model/quantization combinations on code generation benchmarks (HumanEval, MBPP) and mathematical reasoning tasks (GSM8K) to determine if task-specific patterns emerge that contradict the current NLP-focused conclusions.

3. **Alternative Optimization Techniques**: Implement and benchmark speculative decoding and continuous batching across the same configuration space to quantify their impact on the observed trade-offs between LMDeploy's single-stream responsiveness and vLLM's concurrent throughput advantages.