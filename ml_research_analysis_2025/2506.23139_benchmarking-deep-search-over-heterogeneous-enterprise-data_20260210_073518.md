---
ver: rpa2
title: Benchmarking Deep Search over Heterogeneous Enterprise Data
arxiv_id: '2506.23139'
source_url: https://arxiv.org/abs/2506.23139
tags:
- product
- search
- employee
- name
- slack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HERB is a new benchmark for evaluating deep search over heterogeneous
  enterprise data, addressing the limitations of existing RAG benchmarks that rely
  on artificial, shallow multi-hop questions. It uses a query-first synthetic data
  generation pipeline to simulate realistic software development workflows, producing
  interconnected content across structured and unstructured sources such as Slack
  messages, meeting transcripts, GitHub PRs, and documents.
---

# Benchmarking Deep Search over Heterogeneous Enterprise Data

## Quick Facts
- arXiv ID: 2506.23139
- Source URL: https://arxiv.org/abs/2506.23139
- Reference count: 32
- HERB benchmark achieves only 32.96 average performance even with advanced agentic RAG systems

## Executive Summary
HERB introduces a new benchmark for evaluating deep search over heterogeneous enterprise data, addressing the limitations of existing RAG benchmarks that rely on artificial, shallow multi-hop questions. The benchmark uses a query-first synthetic data generation pipeline to simulate realistic software development workflows, producing interconnected content across structured and unstructured sources including Slack messages, meeting transcripts, GitHub PRs, and documents. With 815 answerable and 699 unanswerable queries supported by 39,190 enterprise artifacts, HERB reveals that current state-of-the-art agentic RAG systems struggle significantly with deep search tasks, achieving only 32.96 average performance. The benchmark identifies retrieval as the primary bottleneck limiting effectiveness in heterogeneous enterprise search scenarios.

## Method Summary
HERB employs a query-first synthetic data generation pipeline to create realistic enterprise search scenarios. The approach generates queries based on predefined software development workflow templates (customer support, bug reporting, code review), then synthesizes interconnected content across multiple data sources including Slack messages, meeting transcripts, GitHub PRs, and documents. The pipeline ensures that answers require reasoning over heterogeneous sources and may involve multi-hop reasoning. The benchmark includes both answerable queries (where answers exist in the corpus) and unanswerable queries (where no sufficient evidence exists), creating a comprehensive evaluation framework. The synthetic data generation process maintains coherence and relevance while scaling to create a large corpus of 39,190 enterprise artifacts supporting the 1,514 total queries.

## Key Results
- Current state-of-the-art agentic RAG systems achieve only 32.96 average performance on HERB
- Retrieval identified as the primary bottleneck limiting deep search effectiveness
- Benchmark includes 815 answerable and 699 unanswerable queries across 39,190 enterprise artifacts
- Performance gap highlights significant challenges in handling heterogeneous enterprise data

## Why This Works (Mechanism)
HERB works by simulating realistic enterprise search scenarios through synthetic data generation that mirrors actual software development workflows. The query-first approach ensures that generated content is purposefully created to answer specific questions, maintaining coherence and relevance across heterogeneous data sources. By incorporating both structured and unstructured data types and requiring multi-hop reasoning across different content formats, the benchmark captures the complexity of real-world enterprise search challenges. The inclusion of unanswerable queries prevents systems from simply memorizing patterns and forces genuine understanding of when sufficient evidence exists to answer questions.

## Foundational Learning
- **Synthetic data generation**: Creating realistic training data through controlled processes; needed to scale benchmark creation while maintaining quality; quick check: verify generated content maintains coherence across multiple hops
- **Query-first pipeline**: Generating questions before content to ensure purposeful data creation; needed to avoid post-hoc question generation that may not reflect real search patterns; quick check: validate that all answers are genuinely supported by generated content
- **Multi-hop reasoning**: Solving problems requiring multiple inference steps across different sources; needed to capture complex enterprise search scenarios; quick check: ensure queries require reasoning across at least two different data types
- **Heterogeneous data integration**: Combining structured and unstructured data sources; needed to reflect real enterprise environments; quick check: verify all data types are meaningfully connected in the corpus
- **Answerable vs unanswerable distinction**: Including both types of queries to test genuine understanding; needed to prevent overfitting to positive examples; quick check: validate unanswerable queries truly lack sufficient evidence

## Architecture Onboarding

**Component Map**
Query Generator -> Content Synthesizer -> Corpus Builder -> Evaluation Pipeline -> Retrieval Engine -> Answer Generator

**Critical Path**
Query generation → Content synthesis across heterogeneous sources → Corpus assembly → Retrieval over multi-hop evidence → Answer generation

**Design Tradeoffs**
Synthetic vs real data: HERB uses synthetic data for scalability and control, sacrificing some realism but gaining reproducibility and coverage. Workflow templates limit diversity but ensure relevance. Answerable/unanswerable split increases evaluation rigor but requires careful validation.

**Failure Signatures**
Low precision in retrieval indicates inability to identify relevant evidence across heterogeneous sources. Failed multi-hop reasoning suggests limitations in connecting information across different data types. Poor performance on unanswerable queries indicates systems may hallucinate or make false confidence judgments.

**First 3 Experiments**
1. Evaluate retrieval performance on single-hop vs multi-hop queries to identify reasoning complexity impact
2. Test system performance across different data type combinations (e.g., Slack+GitHub vs Slack+documents)
3. Compare performance on answerable vs unanswerable queries to assess hallucination tendencies

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Synthetic data generation may not fully capture the complexity and nuance of real enterprise search scenarios
- Limited workflow templates may not represent the full spectrum of enterprise search patterns
- Focus on software development contexts may limit generalizability to other enterprise domains
- Potential gaps between benchmark performance and real-world effectiveness due to controlled nature of synthetic data

## Confidence

**High confidence**: Benchmark design addressing limitations of existing RAG benchmarks and reported low performance of current state-of-the-art systems (32.96 average) are well-supported by presented methodology and experimental results.

**Medium confidence**: Claim that retrieval is the primary bottleneck limiting effectiveness is reasonable but could benefit from more detailed analysis of whether failures stem from retrieval quality, generation quality, or interaction effects.

**Medium confidence**: Assertion that synthetic data generation adequately simulates realistic enterprise search scenarios is plausible given methodology described, but would benefit from empirical validation against real-world search patterns.

## Next Checks
1. Conduct human evaluation study comparing HERB query difficulty and realism against real enterprise search queries to validate synthetic data generation approach
2. Test HERB's generalizability by applying benchmark to non-software development enterprise domains (legal, healthcare, finance) to assess domain transferability
3. Perform ablation studies on different retrieval methods and content types to quantify individual contributions to overall system performance and identify specific bottlenecks beyond general "retrieval is the bottleneck" claim