---
ver: rpa2
title: 'Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing:
  A Comparative Study'
arxiv_id: '2505.07576'
source_url: https://arxiv.org/abs/2505.07576
tags:
- images
- detection
- approaches
- dataset
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks modern Visual Anomaly Detection (VAD) methods
  for semiconductor manufacturing using the MIIC dataset of SEM images. It evaluates
  seven feature-based and three reconstruction-based VAD approaches, reporting both
  image-level and pixel-level performance.
---

# Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study

## Quick Facts
- arXiv ID: 2505.07576
- Source URL: https://arxiv.org/abs/2505.07576
- Authors: Manuel Barusco; Francesco Borsatti; Youssef Ben Khalifa; Davide Dalle Pezze; Gian Antonio Susto
- Reference count: 20
- One-line primary result: CFA achieves highest image-level F1 score (87.96%) and pixel-level F1 score (75.66%) on MIIC dataset of SEM images

## Executive Summary
This study benchmarks modern Visual Anomaly Detection (VAD) methods for semiconductor manufacturing using the MIIC dataset of SEM images. It evaluates seven feature-based and three reconstruction-based VAD approaches, reporting both image-level and pixel-level performance. Results show that CFA (feature-based) achieves the highest image-level F1 score (87.96%) and pixel-level F1 score (75.66%), while inpainting-based reconstruction methods like IAD+Inpainting remain competitive (99.27% ROC, 91.23% F1), suggesting reconstruction-based approaches can match feature-based methods when large-scale normal data is available. The findings highlight the importance of domain diversity in VAD evaluation, as methods effective on natural images do not necessarily translate directly to SEM data.

## Method Summary
The study benchmarks 10 VAD methods (7 feature-based: PaDiM, STFPM, CFA, PatchCore, RD4AD, FastFlow, SuperSimpleNet; 3 reconstruction-based: GANomaly, f-anoGAN, IAD+Inpainting) on the MIIC dataset of 25,276 grayscale SEM images (512×512). Images are resized to 224×224 and normalized with ImageNet statistics. Each method uses original hyperparameters from MVTec experiments. PatchCore uses 50% random training subset for memory bank coreset reduction. Performance is evaluated using image-level and pixel-level metrics including F1, ROC, PR, and PRO.

## Key Results
- CFA (feature-based) achieves highest image-level F1 score (87.96%) and pixel-level F1 score (75.66%)
- IAD+Inpainting (reconstruction-based) remains competitive with 99.27% ROC and 91.23% F1
- PaDiM underperforms on MIIC (47.55% F1) compared to MVTec AD, highlighting domain gap issues
- STFPM performs better on MIIC than MVTec, emphasizing importance of domain-specific evaluation

## Why This Works (Mechanism)

### Mechanism 1
Feature-based memory bank methods can achieve strong anomaly detection on SEM images despite domain gap from ImageNet pretraining. CFA stores normal patch embeddings in a memory bank and adapts them into coupled hyperspheres, amplifying separation between normal and abnormal representations. Anomaly scores derive from distance metrics to the learned normal manifold. Core assumption: Pretrained feature extractors produce sufficiently meaningful representations for SEM microscopy images. Evidence: CFA achieves 87.96% image F1 and 75.66% pixel F1; auxiliary concentration network more effectively clusters normal feature representations. Break condition: If SEM images contain texture patterns fundamentally unrelated to ImageNet features, the feature extractor may fail to produce discriminative embeddings.

### Mechanism 2
Reconstruction-based approaches remain competitive when large-scale normal training data is available. IAD+Inpainting trains in two phases—first learning image reconstruction, then applying inpainting where masked regions are predicted. Anomalies are detected via reconstruction error; poorly inpainted regions indicate abnormality. Core assumption: With 25,000+ normal images, the model can learn the underlying structure of defect-free SEM images sufficiently well that anomalies produce distinguishable reconstruction errors. Evidence: IAD+Inpainting achieves 99.27% ROC and 91.23% F1; abundance of data makes it easier for reconstruction-based models to learn underlying patterns. Break condition: If normal images exhibit high heterogeneity, reconstruction may overgeneralize and fail to flag subtle anomalies.

### Mechanism 3
Teacher-student architectures can localize anomalies effectively even when image-level detection performance varies. Student network learns to match teacher's feature maps on normal data. At inference, feature deviations indicate anomalies. The student cannot generalize to anomalous patterns it never trained on. Core assumption: The distillation process creates a student that overfits to normal feature distributions, making anomalies detectable as outliers. Evidence: STFPM performs better on MIIC than MVTec; feature map deviations indicate anomalies. Break condition: If the teacher's pretrained features are poorly suited to SEM textures, both networks may fail to capture discriminative patterns.

## Foundational Learning

- **Concept: One-class unsupervised learning**
  - Why needed here: VAD trains exclusively on defect-free images; understanding how models learn "normality" without seeing anomalies is essential
  - Quick check question: Can you explain why VAD avoids the supervised requirement for labeled anomaly samples?

- **Concept: Domain gap in transfer learning**
  - Why needed here: Feature-based methods use ImageNet-pretrained backbones; SEM images differ fundamentally from natural photographs
  - Quick check question: What risks arise when applying a feature extractor trained on natural images to electron microscopy data?

- **Concept: Anomaly localization vs. detection metrics**
  - Why needed here: The paper distinguishes image-level (is this image anomalous?) from pixel-level (where is the anomaly?) performance; F1, ROC, and PRO capture different failure modes
  - Quick check question: Why might a method excel at detection but underperform at localization?

## Architecture Onboarding

- **Component map**: Feature Extractor Backbone -> Memory Bank / Adaptation Layer -> Anomaly Scoring Module -> Output
- **Critical path**: 1) Extract features from normal training images → populate memory bank or train adaptation; 2) At inference, extract features from test image → compute anomaly scores per patch; 3) Aggregate patch scores → image-level decision; upsample → pixel-level heatmap
- **Design tradeoffs**: Memory bank size vs. inference speed (PatchCore requires coreset sampling; CFA's hypersphere adaptation is more compact); Reconstruction vs. feature-based (reconstruction avoids pretrained features but needs more data; feature-based is faster but assumes transferability); Backbone depth (deeper extractors capture more semantic information but may lose fine-grained texture details)
- **Failure signatures**: High ROC but low F1 (threshold selection fails on imbalanced data); High image-level F1 but low pixel-level F1 (detection works but localization is imprecise); PaDiM underperforms (47.55% F1) relative to MVTec (Gaussian modeling may be too rigid for heterogeneous SEM textures)
- **First 3 experiments**: 1) Baseline comparison: Run CFA, PatchCore, and PaDiM on MIIC using paper's hyperparameters; confirm reproducibility of reported F1 scores; 2) Backbone ablation: Swap ImageNet-pretrained backbone for self-supervised pretrained encoder to assess domain gap impact; 3) Data scaling test: Train IAD+Inpainting and CFA on subsets of training data (10%, 25%, 50%, 100%) to quantify reconstruction vs. feature-based data efficiency

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance of feature-based Visual Anomaly Detection (VAD) methods in semiconductor manufacturing be improved by using feature extractors fine-tuned via self-supervised learning on the target domain, rather than relying solely on ImageNet pretraining? Basis: Authors conclude feature-based methods could be improved by considering feature extractors fine-tuned to the considered dataset instead of being only pretrained on ImageNet. Unresolved because study utilized standard pre-trained backbones common in literature trained on natural images. Evidence would require comparative experiment showing performance delta when using backbones pre-trained on MIIC via self-supervised learning versus standard ImageNet weights.

### Open Question 2
Do modern reconstruction-based VAD approaches consistently match or surpass feature-based methods in specialized imaging domains when large-scale normal data is available? Basis: Authors note inpainting-based reconstruction model still rivals feature-based methods and suggest reconstruction-based VAD approaches remain unaffected by target domain. Unresolved because finding may be specific to IAD+Inpainting architecture or dataset scale, leaving generalizability uncertain. Evidence would require benchmark comparing state-of-the-art feature-based methods against modern reconstruction-based generative models across multiple SEM datasets.

### Open Question 3
How does the relative performance of feature-based versus reconstruction-based VAD methods shift when training data scale is reduced from large-scale (25,000+ images) to standard industrial dataset sizes (hundreds of images)? Basis: Authors attribute success of IAD+Inpainting partially to large-scale dataset containing more than 25,000 images, contrasting with standard datasets having only hundreds of images. Unresolved because study benchmarks on MIIC which is unusually large; performance characteristics under data-scarce conditions typical of other industrial datasets remain untested. Evidence would require data-ablation study on MIIC dataset evaluating ROC and F1 trends as training set size is incrementally reduced.

## Limitations

- Reliance on ImageNet-pretrained features for SEM images represents substantial domain gap that may limit generalizability
- Method hyperparameters are referenced rather than specified, creating barriers to exact reproduction
- Performance differences between feature-based and reconstruction-based methods may be dataset-specific due to MIIC's unusually large size (25,000+ images)

## Confidence

- **High Confidence**: Experimental methodology using standardized metrics (F1, ROC, PRO) and established benchmarking protocols; finding that CFA achieves highest scores on MIIC
- **Medium Confidence**: Claim that domain diversity matters in VAD evaluation; transferability of ImageNet features to SEM data is plausible but requires further validation
- **Low Confidence**: Relative ranking of methods across different domains; performance of IAD+Inpainting which relies on unpublished custom implementation details

## Next Checks

1. **Backbone Transferability Test**: Replace ImageNet-pretrained backbone with self-supervised pretrained encoder (MoCo, DINO) and measure performance drop across all methods to quantify domain gap impact
2. **Data Scaling Analysis**: Train CFA and IAD+Inpainting on progressively smaller subsets (10%, 25%, 50%, 100%) to determine data efficiency breakpoints and identify whether reconstruction methods maintain competitiveness on smaller datasets
3. **Cross-Domain Generalization**: Apply top-performing methods from MIIC (CFA, IAD+Inpainting) to MVTec AD dataset and compare relative rankings to assess domain-specific optimization