---
ver: rpa2
title: A Comparative Approach to Assessing Linguistic Creativity of Large Language
  Models and Humans
arxiv_id: '2507.12039'
source_url: https://arxiv.org/abs/2507.12039
tags:
- creativity
- llms
- humans
- language
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a linguistic creativity test for humans and
  Large Language Models (LLMs), assessing their ability to generate novel words and
  phrases using derivation, compounding, and metaphorical language. The test was administered
  to 24 humans and 24 LLMs, with answers automatically evaluated using OCSAI tool
  for Originality, Elaboration, and Flexibility.
---

# A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans

## Quick Facts
- **arXiv ID:** 2507.12039
- **Source URL:** https://arxiv.org/abs/2507.12039
- **Reference count:** 39
- **Primary result:** LLMs significantly outperformed humans in linguistic creativity test, with mean total score of 0.52 versus 0.48 for humans

## Executive Summary
This study compares linguistic creativity between humans and LLMs using a novel test measuring ability to generate novel words and phrases through derivation, compounding, and metaphorical language. The test was administered to 24 non-native English-speaking humans and 24 different LLM models, with answers evaluated using OCSAI for Originality, Elaboration, and Flexibility. Results show LLMs achieved significantly higher scores across most tasks, though humans demonstrated higher uniqueness in some cases. Manual analysis revealed humans favor extending creativity (E-creativity) through rule-breaking for expressive purposes, while LLMs demonstrate fixed creativity (F-creativity) through systematic rule application.

## Method Summary
The study employed an 8-task linguistic creativity test administered to 24 non-native English speakers (B2+ level) and 24 LLM models. Tasks covered word formation (derivation and compounding) and metaphorical language, requiring participants to generate 3 novel answers per item. Answers were automatically evaluated using OCSAI v1.6, which scores for Originality, Elaboration, and Flexibility based on semantic distance metrics. Uniqueness was calculated using inverse of average pairwise semantic similarity of embeddings. Statistical analysis included t-tests and OLS regression, supplemented by manual qualitative review of output differences.

## Key Results
- LLMs achieved mean total score of 0.52 versus 0.48 for humans
- LLMs significantly outperformed humans in all criteria and six out of eight tasks
- Humans showed higher uniqueness scores in some cases, with maximum individual scores achieved by humans
- Manual analysis revealed humans favor extending creativity (E-creativity) while LLMs favor fixed creativity (F-creativity)

## Why This Works (Mechanism)

### Mechanism 1: Optimization for Fixed (F-) Creativity
LLMs outperform humans because their architecture optimizes for F-creativity (rule-based productivity) rather than E-creativity (rule-breaking expressivity). The paper postulates that LLMs generate novelty by systematically applying morphological and syntactic rules to vocabulary (derivation, compounding), yielding high semantic distance scores from common phrases without violating grammatical logic. Humans prioritize expressivity and intentional deviation (E-creativity), breaking morphological rules for pragmatic effect. If evaluation were tuned to penalize socially inert outputs or reward intentional grammatical violations, the LLM advantage would likely diminish.

### Mechanism 2: Automated Evaluation Alignment with Statistical Plausibility
The performance gap (LLM mean 0.52 vs. Human 0.48) is contingent on OCSAI, an automated evaluator that aligns with statistical probability distributions learned by LLMs. LLMs generate statistically plausible but socially inert compounds (e.g., "chat-dependent") that score high on originality/flexibility via semantic distance metrics while lacking pragmatic richness. If evaluation switched to human ranking of wittiness or emotional impact, LLM performance would likely drop relative to humans.

### Mechanism 3: Uniqueness via Heterogeneous Strategy
While LLMs maintain higher mean performance, humans exhibit higher variance in uniqueness, with outliers producing idiosyncratic responses that LLMs rarely generate. LLMs converge on optimal solutions within sampling parameters, leading to homogenized high performance. Humans, specifically the quirky ones, diverge wildly from the norm, creating unique nonce words that maximize uniqueness scores at the cost of average consistency.

## Foundational Learning

- **Concept: F-Creativity vs. E-Creativity**
  - **Why needed here:** This distinction (Sampson, 2016) explains why LLMs scored high but "felt" different to manual reviewers. F-creativity is combinatorial, while E-creativity extends the system itself.
  - **Quick check question:** Does the response "chat-dependent" represent F-creativity or E-creativity? (Answer: F-creativity, as it follows standard derivation rules).

- **Concept: Semantic Distance Scoring (OCSAI)**
  - **Why needed here:** Understanding how "Originality" is calculated is critical. OCSAI uses semantic distance (likely embeddings) rather than dictionary lookup, explaining why LLMs can score high on novelty without being truly funny or insightful.
  - **Quick check question:** If two words are semantically distant (e.g., "banana" and "philosophy"), does OCSAI necessarily rate a compound of them as high quality? (Answer: High originality, potentially, but not necessarily high quality/feasibility).

- **Concept: Nonce Words & Morphological Productivity**
  - **Why needed here:** The test relied heavily on creating new words (neologisms). Understanding English derivation (suffixes like -ish, -able) is necessary to see why LLMs "playing safe" resulted in higher scores than humans "breaking rules."
  - **Quick check question:** Why might "scrollish" (verb + suffix) be considered more "risky" or E-creative than "chat-dependent" (noun + adjective)? (Answer: "Scrollish" violates standard morphological constraints where -ish typically attaches to adjectives/nouns, not verbs).

## Architecture Onboarding

- **Component map:** Input (Test Prompts) -> Generators (24 Humans, 24 LLMs) -> Evaluation Engine (OCSAI) -> Validation Layer (Statistical Analysis + Manual Review)
- **Critical path:** 1) Prompt Engineering ensuring comparable outputs from different processing modalities, 2) OCSAI Calibration determining result validity, 3) Uniqueness Calculation through embedding generation and cosine similarity
- **Design tradeoffs:** Automation vs. Nuance (trading nuance of human panel scoring for scale and consistency of OCSAI), prioritizing statistical significance over pragmatic validity. Fluency Removal (removed Fluency criterion to avoid bias against humans, focusing purely on quality-per-item).
- **Failure signatures:** High Score, Low Sense (LLMs generating high-scoring semantic compounds lacking referential meaning), "Narrow" Creativity (LLMs generating similar creative compounds potentially inflating Flexibility scores).
- **First 3 experiments:** 1) Temperature Sweep running top-performing LLMs with aggressive temperature settings to test F-creativity vs. E-creativity boundary, 2) Native Speaker Baseline administering test to native English speakers to isolate non-native variable, 3) Blind Human Ranking presenting mixed Human/LLM answers to new human group to check correlation with OCSAI scores.

## Open Questions the Paper Calls Out

- **How would the inclusion of native English speakers alter the comparative performance results between humans and LLMs?**
  - **Basis in paper:** Section 10 explicitly lists "lack of native respondents" as a limitation and asks how results would differ with their inclusion.
  - **Why unresolved:** The study relied solely on non-native speakers (humanities students), so the upper bound of human performance remains unknown.
  - **What evidence would resolve it:** Administering the same test to a control group of native English speakers and comparing their scores against existing LLM benchmarks.

- **Does the automated OCSAI evaluation metric systematically favor the "Fixed-creativity" (rule-based) of LLMs over the "Extending-creativity" (rule-breaking) of humans?**
  - **Basis in paper:** Manual analysis (Section 8) notes humans favor E-creativity while LLMs favor F-creativity, but OCSAI scored LLMs higher, raising questions about whether the algorithm penalizes human rule-breaking.
  - **Why unresolved:** The paper relies on automated scoring without comparative study of how well OCSAI handles "unheard" or novel linguistic constructions versus standard morphological productivity.
  - **What evidence would resolve it:** A study correlating OCSAI scores with qualitative human expert assessments specifically rating novelty of rule-breaking vs. rule-following outputs.

- **To what extent do decoding parameters (temperature and top-p) influence the linguistic creativity scores of different LLM architectures?**
  - **Basis in paper:** Section 6 reports the effect of maximizing temperature and top-p settings was "inconclusive," with mixed results across models.
  - **Why unresolved:** Sample size for parameter tuning was small (4 models), and specific interaction between randomness settings and originality criterion requires clarification.
  - **What evidence would resolve it:** A controlled ablation study measuring creativity scores across standardized grid of temperature and top-p values for wider range of models.

## Limitations
- Non-native English speaker sample (B2 level) introduces language proficiency confound affecting creative expression capacity
- Potential misalignment between OCSAI's automated evaluation and human perception of linguistic creativity
- Test items may favor certain types of creativity over others, potentially biasing results toward LLM strengths

## Confidence

- **High Confidence:** The observed performance gap (LLM 0.52 vs. Human 0.48 mean scores) and statistical significance of differences across most tasks
- **Medium Confidence:** The F-creativity vs. E-creativity distinction as primary explanatory mechanism
- **Low Confidence:** Generalizability of findings to native speakers or different creativity domains

## Next Checks
1. Replicate the study with native English speakers to isolate language proficiency effects
2. Conduct blind human ranking of "wittiest" responses to test correlation with OCSAI scores
3. Implement temperature sweeps across all LLM models to test uniqueness performance boundaries