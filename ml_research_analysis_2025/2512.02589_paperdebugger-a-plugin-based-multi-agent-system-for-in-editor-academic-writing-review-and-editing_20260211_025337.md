---
ver: rpa2
title: 'PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing,
  Review, and Editing'
arxiv_id: '2512.02589'
source_url: https://arxiv.org/abs/2512.02589
tags:
- writing
- paperdebugger
- layer
- academic
- in-editor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PaperDebugger introduces an in-editor, multi-agent system for academic
  writing, directly integrating with Overleaf to eliminate copy-paste workflows. It
  employs a Kubernetes-native backend, Model Context Protocol (MCP) tools, and a Chrome
  extension to provide context-aware critique, literature retrieval, and diff-based
  editing.
---

# PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing

## Quick Facts
- arXiv ID: 2512.02589
- Source URL: https://arxiv.org/abs/2512.02589
- Reference count: 9
- Primary result: In-editor multi-agent system integrated with Overleaf; 4,116 Chrome extension installs, 2,761 registered users, 732 monthly active users, 7,447 writing threads created

## Executive Summary
PaperDebugger is an in-editor academic writing assistant built as a Chrome extension that integrates directly with Overleaf. It uses a Kubernetes-native backend and Model Context Protocol (MCP) tools to provide context-aware critique, literature retrieval, and diff-based editing without copy-paste workflows. The system supports parallel agent execution for review, enhancement, and deep research tasks. Early analytics from real-world deployment demonstrate sustained user engagement and validate the practicality of an editor-native, agentic writing assistant.

## Method Summary
PaperDebugger is a plugin-based multi-agent system that integrates with Overleaf via a Chrome extension. The system uses a Kubernetes-native backend to orchestrate stateless LLM agents running in isolated pods, enabling high concurrency and horizontal scaling. It employs XtraMCP, a Model Context Protocol extension with schema validation, to integrate literature search, reference lookup, document scoring, and revision pipelines. The system captures LaTeX text selection and project context, streams requests via gRPC, and renders edits as before-after diffs that can be applied as patches with a single click.

## Key Results
- 4,116 Chrome extension installs, 2,761 registered users, and 732 monthly active users
- 7,447 writing threads created, demonstrating sustained engagement
- Frequent refinement actions—viewing diffs, copying suggestions, inserting patches—validate the practicality of in-editor integration
- Users report that the CS-focused suggestions feel domain-specific, with performance dropping on long documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-editor integration via Chrome extension eliminates context-switching overhead and preserves revision provenance.
- Mechanism: A Chrome-approved extension injects a floating panel and inline action buttons directly into Overleaf's DOM. When users select LaTeX text, the extension captures both selection and project state, streams this to the backend via gRPC, and renders returned edits as before-after diffs that can be applied as patches with a single click.
- Core assumption: Users prefer inline, structure-aware feedback over external chat-based assistance, and Overleaf's DOM structure remains sufficiently stable for injection.
- Evidence anchors: [abstract] "Chrome-approved extension... minimal-intrusion user interface (UI)... diff-based updates"; [section 2.1 Presentation Layer] "extension injects a floating panel and inline action buttons... captures selected text and project state... edits are presented as before–after diffs, and accepted patches are applied instantly"
- Break condition: If Overleaf significantly changes its editor DOM structure or blocks extension injection, the presentation layer coupling fails.

### Mechanism 2
- Claim: Kubernetes-native pod orchestration enables horizontal scaling of stateless LLM agents for concurrent multi-user workloads.
- Mechanism: The backend (implemented in Go) routes requests to containerized LLM agents running in isolated Kubernetes pods. A pod controller handles scheduling, model selection, permission checks, and schema validation. For full-document reviews, a coordinating agent decomposes tasks into segment-level sub-queries, dispatches them across worker pods in parallel, and merges results.
- Core assumption: Agent workloads are sufficiently stateless that pod-based isolation does not introduce unacceptable latency or state-synchronization overhead.
- Evidence anchors: [abstract] "Kubernetes-native orchestration layer... parallel agent execution"; [section 2.1 Backend Layer & Agent Layer] "orchestrates stateless LLM agents, each running inside isolated pods, enabling high concurrency and horizontal scaling"
- Break condition: If inter-agent communication latency exceeds user-acceptable thresholds for real-time streaming, the parallel decomposition advantage erodes.

### Mechanism 3
- Claim: XtraMCP protocol layer provides modular, schema-validated tool integration that reduces hallucination risk in academic-specific tasks.
- Mechanism: XtraMCP extends the Model Context Protocol with Pydantic-based schema validation and internal consistency checks. It exposes tools for literature search (embedding + LLM reranking), multi-step AI review (guided by conference-style workflows), and XtraGPT for context-aware revision. Agents invoke these tools through a standardized protocol rather than ad-hoc API calls.
- Core assumption: Schema validation and structured tool interfaces sufficiently constrain LLM outputs to reduce hallucinations in academic contexts.
- Evidence anchors: [abstract] "Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines"; [section 2.2 Agentic Design] "XtraMCP... enforces our internal Pydantic-based schemas and internal consistency checks to minimize hallucinations"
- Break condition: If tool outputs require domain-specific validation not captured by schemas, hallucination risk may persist despite protocol enforcement.

## Foundational Learning

- Concept: **gRPC streaming with Server-Sent Events (SSE) compatibility**
  - Why needed here: The protocol layer uses a custom streaming protocol compatible with OpenAI's SSE format to deliver intermediate model outputs in real time during multi-step workflows.
  - Quick check question: Can you explain how gRPC bidirectional streaming differs from REST polling, and why SSE compatibility matters for LLM token streaming?

- Concept: **Kubernetes pod orchestration for stateless workloads**
  - Why needed here: The backend relies on pod-level isolation for agent execution; understanding pod lifecycle, horizontal pod autoscaling, and service discovery is essential for debugging scaling issues.
  - Quick check question: What happens to in-flight agent requests if a pod is terminated mid-execution, and how would you design idempotency?

- Concept: **Diff-based patch generation for LaTeX**
  - Why needed here: The system presents edits as before-after diffs and applies patches; understanding unified diff format and LaTeX-aware patching prevents malformed source injection.
  - Quick check question: How would you handle a patch that conflicts with user edits made after the diff was generated?

## Architecture Onboarding

- Component map:
  - Presentation Layer (Chrome extension) -> Backend Layer (Go service) -> Agent Layer (containerized agents) -> Protocol Layer (XtraMCP server) -> Infrastructure (Kubernetes cluster, database, CDN/proxy)

- Critical path:
  1. User selects text in Overleaf → extension captures selection + project context
  2. Extension streams request via gRPC gateway → backend authenticates and routes
  3. Pod controller schedules agent container → agent invokes XtraMCP tools as needed
  4. Agent returns structured output → backend streams diff-formatted response
  5. Extension renders diff preview → user inspects and applies patch to LaTeX source

- Design tradeoffs:
  - In-editor coupling vs. portability: Deep Overleaf integration provides context awareness but reduces portability to other editors; extension must track Overleaf DOM changes
  - Pod isolation vs. latency: Stateless pod-per-request design scales but introduces cold-start latency; the paper does not disclose caching or warm-pool strategies
  - Schema validation vs. flexibility: XtraMCP's Pydantic schemas reduce hallucination risk but may constrain novel tool integration; extending schemas requires protocol updates

- Failure signatures:
  - Stale document state: If user edits Overleaf during agent processing, applied patches may conflict; watch for rejected patches or user reports of "wrong location" edits
  - Streaming interruption: gRPC stream breaks mid-response; UI shows partial output. Check pod logs for OOM kills or network timeouts
  - Tool invocation failures: XtraMCP tool returns schema-invalid output; agent falls back or errors. Monitor MCP server logs for validation rejections
  - Extension injection failures: Overleaf UI update breaks selector paths; panel fails to appear. Chrome DevTools console will show injection errors

- First 3 experiments:
  1. Trace a single revision request end-to-end: Enable Chrome DevTools network tab, trigger a text enhancement, and trace the gRPC stream from extension → backend → agent → response. Identify latency bottlenecks
  2. Inspect pod lifecycle during concurrent requests: Use `kubectl get pods -w` while multiple users trigger full-document reviews. Observe pod creation, scaling behavior, and termination patterns
  3. Validate XtraMCP schema enforcement: Inject a deliberately malformed tool response (in a staging environment) and verify that Pydantic validation rejects it before agent processing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does writing quality—measured by acceptance rates, reviewer scores, or revision depth—compare between PaperDebugger users and users of external LLM assistants or no assistant?
- Basis in paper: [inferred] The paper reports engagement metrics (installs, threads, refinement actions) but does not isolate whether in-editor integration improves final document quality versus convenience alone
- Why unresolved: Usage analytics capture interaction frequency, not downstream academic outcomes such as acceptance, clarity scores, or expert evaluation
- What evidence would resolve it: A controlled or quasi-experimental study comparing paper quality ratings from blinded reviewers across PaperDebugger, external-tool, and unassisted groups

### Open Question 2
- Question: Can agent workflows and fine-tuned models be adapted to maintain performance across non-CS academic domains without retraining?
- Basis in paper: [explicit] User feedback notes "suggestions feel CS-like" and "domain-specific tone" limitations
- Why unresolved: The current XtraGPT model suite is tuned for academic writing but appears skewed toward CS discourse; no evaluation across disciplines is presented
- What evidence would resolve it: Cross-domain benchmarking (e.g., humanities, social sciences, life sciences) measuring suggestion acceptance rates, user satisfaction, and lexical/style alignment with domain conventions

### Open Question 3
- Question: What architectural or prompting strategies enable robust multi-agent performance on long documents (e.g., 30+ pages) where the paper reports current performance drops?
- Basis in paper: [explicit] "Performance drops on long documents" is reported as user feedback without technical resolution
- Why unresolved: Long documents may exceed context windows, complicate segmentation, or degrade agent coordination; the paper does not detail handling strategies
- What evidence would resolve it: Ablation studies on document segmentation, hierarchical summarization, or retrieval-augmented context management, measuring latency, accuracy, and user satisfaction on full-length manuscripts

### Open Question 4
- Question: What factors explain the gap between 2,761 registered users and 732 monthly active users, and how can the system improve sustained adoption?
- Basis in paper: [inferred] The telemetry shows ~26.5% of registered users are monthly active; no analysis of drop-off drivers is provided
- Why unresolved: Retention barriers (onboarding friction, perceived value, trust, or performance issues) are not investigated
- What evidence would resolve it: User surveys, funnel analytics, or A/B tests targeting onboarding, UI friction, or trust signals to identify modifiable predictors of long-term engagement

## Limitations
- DOM Coupling Fragility: The Chrome extension's injection-based UI depends on Overleaf's DOM structure, which may change without notice, potentially breaking the presentation layer
- Cold-Start Latency: Pod-per-request architecture introduces scaling benefits but may suffer from cold-start delays; the paper does not disclose warm-pool strategies
- Limited External Validation: Early analytics show usage engagement but lack peer review benchmarks or controlled studies comparing writing quality improvements to baseline tools

## Confidence
- High: Multi-agent orchestration via Kubernetes pods and XtraMCP protocol layer design are technically sound and align with established patterns
- Medium: Engagement metrics (installs, MAU, refinement actions) support practical utility but lack causal links to writing quality outcomes
- Low: Claims about hallucination reduction via schema validation lack direct empirical validation or comparative baselines

## Next Checks
1. End-to-End Latency Profiling: Instrument a full revision workflow from text selection to patch application, measuring gRPC streaming latency, pod scheduling delays, and total user-perceived response time
2. DOM Stability Monitoring: Simulate Overleaf DOM changes in a staging environment and assess extension injection failure rates; implement automated DOM selector regression tests
3. Schema Validation Efficacy: Design a controlled test suite of tool outputs (valid and hallucinated) to measure XtraMCP schema rejection rates and compare hallucination frequency against non-schema-constrained agents