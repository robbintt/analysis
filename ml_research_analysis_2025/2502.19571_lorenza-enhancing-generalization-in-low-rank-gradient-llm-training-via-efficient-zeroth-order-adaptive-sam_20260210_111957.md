---
ver: rpa2
title: 'LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training via Efficient
  Zeroth-Order Adaptive SAM'
arxiv_id: '2502.19571'
source_url: https://arxiv.org/abs/2502.19571
tags:
- gradient
- low-rank
- lorenza
- training
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LORENZA, a memory-efficient optimization
  framework for fine-tuning large language models (LLMs) that combines zeroth-order
  sharpness-aware minimization with low-rank gradient updates. The key innovation
  is using randomized zeroth-order estimation to compute SAM's ascent perturbation
  with only one gradient computation per iteration, eliminating the costly double
  backpropagation of traditional SAM variants.
---

# LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training via Efficient Zeroth-Order Adaptive SAM

## Quick Facts
- arXiv ID: 2502.19571
- Source URL: https://arxiv.org/abs/2502.19571
- Reference count: 40
- Key outcome: LORENZA combines zeroth-order SAM with low-rank gradients to achieve better generalization than LoRA/GaLore while using less memory

## Executive Summary
LORENZA introduces a memory-efficient optimization framework for fine-tuning large language models that combines zeroth-order sharpness-aware minimization with low-rank gradient updates. The key innovation uses randomized zeroth-order estimation to compute SAM's ascent perturbation with only one gradient computation per iteration, eliminating the costly double backpropagation of traditional SAM variants. The method leverages the natural low-rank structure of gradients in reversible neural networks and employs efficient randomized SVD for subspace selection. Theoretical analysis establishes convergence guarantees, while experiments on GLUE benchmarks, LLaMA pre-training, and reasoning tasks show LORENZA outperforms state-of-the-art methods like LoRA and GaLore in accuracy while maintaining comparable memory usage.

## Method Summary
LORENZA integrates zeroth-order estimation of SAM's ascent perturbation with low-rank gradient projection for memory-efficient LLM training. The method uses randomized gradient estimation (RGE) with q=1 random direction to approximate the ascent perturbation, avoiding the double backpropagation required by standard SAM. Gradients are projected to a low-rank subspace (dimension r) using efficient randomized SVD (SSRF algorithm), with updates occurring every T steps based on convergence criteria. The SAM gradient is computed at perturbed weights within the subspace, and Adam optimizer states are maintained in low-rank form. Weight updates are projected back to full space, reducing memory from O(mn) to O(mr + nr) for optimizer states.

## Key Results
- Achieves better generalization on GLUE benchmark and GSM8K reasoning tasks compared to LoRA and GaLore
- Maintains comparable memory usage while providing 25-30% memory reduction versus GaLore (no double backprop overhead)
- Demonstrates improved out-of-distribution generalization, with GSM8K zero-shot accuracy of 53.37% vs GaLore's 52.24% on Phi-2
- Theoretical convergence rate matches SAM-style rates: O(Cβ/√T) + β²ρ²

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zeroth-order estimation approximates SAM's ascent perturbation with a single forward pass instead of full backpropagation.
- **Mechanism:** The Randomized Gradient Estimator (RGE) computes the ascent direction via finite differences: ̂∇f(W) = (1/q)Σ[f(W + μUᵢ) - f(W - μUᵢ)]/(2μ) · Uᵢ, where Uᵢ are random Gaussian matrices. This replaces SAM's gradient-based perturbation with function-value queries, reducing backpropagations from 2 to 1 per iteration.
- **Core assumption:** A single random direction (q=1) provides sufficient signal for the ascent direction; the approximation error remains bounded relative to true gradient.
- **Evidence anchors:** Abstract mentions stochastic zeroth-order estimation; Section 3.1 defines RGE with q=1; Theorem 3.1 provides convergence bound matching SAM rates.
- **Break condition:** If μ is too large, finite-difference approximation degrades; if function evaluations are noisy, variance overwhelms signal.

### Mechanism 2
- **Claim:** Low-rank gradient projection exploits natural gradient structure in reversible networks to reduce memory while preserving optimization quality.
- **Mechanism:** The SSRF algorithm computes Q ∈ ℝᵐˣʳ via Y = AΩ → QR decomposition, where r ≪ min(m,n). Gradients and optimizer states are projected: Ĝₜ = Qₜ^T Gₜ, reducing storage from O(mn) to O(mr + nr).
- **Core assumption:** Gradients of reversible layers exhibit monotonically diminishing rank during training; the top-r singular vectors capture most optimization-relevant information.
- **Evidence anchors:** Section 3.2 shows SSRF complexity O(mnr) vs SVD O(min(mn², m²n)); Table 1 compares memory usage; Section 3.3 states low-rank structure naturally diminishes.
- **Break condition:** If the gradient rank doesn't decay or task requires full-rank updates, projection loses critical information.

### Mechanism 3
- **Claim:** Sharpness-aware perturbations in low-rank subspace promote flat minima, improving out-of-distribution generalization.
- **Mechanism:** LORENZA computes perturbation within subspace: GPertₜ = QₜQₜ^T ̂∇f(Wₜ)RₜRₜ^T, then evaluates SAM gradient at Wₜ + ρ·GPertₜ/||GPertₜ||. This seeks parameters where small perturbations don't increase loss significantly.
- **Core assumption:** Flat minima correlation with generalization extends to low-rank subspace optimization.
- **Evidence anchors:** Section 2 illustrates flat vs sharp minima generalization; Tables 5-6 show GSM8K zero-shot accuracy improvements; Section 4 states method significantly improves out-of-distribution generalization.
- **Break condition:** If ρ is too large, perturbation leaves the low-rank subspace's meaningful region; if scheduled incorrectly, may not reach flat region before convergence.

## Foundational Learning

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: LORENZA's core innovation is making SAM efficient; understanding SAM's min-max formulation min_w max_{||ε||≤ρ} f(w+ε) is prerequisite.
  - Quick check question: Can you explain why SAM requires two gradient evaluations per step in its standard form?

- **Concept: Randomized Numerical Linear Algebra (RandNLA)**
  - Why needed here: The SSRF subspace selection uses randomized range finding; understanding why Y = AΩ followed by QR approximates top singular vectors is essential.
  - Quick check question: Why does multiplying by random projection matrix Ω before QR decomposition approximate SVD efficiently?

- **Concept: Zeroth-Order Optimization**
  - Why needed here: The RGE estimator replaces gradient computation with function value differences; understanding finite-difference gradient approximation and its variance properties is critical.
  - Quick check question: What happens to the variance of the RGE estimator as dimension m×n increases while keeping q fixed?

## Architecture Onboarding

- **Component map:**
  Input: Weight matrix W ∈ ℝᵐˣⁿ, batch B
  Block 1: Subspace Selection (every T steps)
  Block 2: Zeroth-Order Ascent (q=1)
  Block 3: SAM Gradient + Adam
  Block 4: Weight Update

- **Critical path:** Block 2 (zeroth-order ascent) → Block 3 (SAM gradient at perturbed weights). If the ascent direction is wrong, the SAM gradient provides no sharpness benefit.

- **Design tradeoffs:**
  - **Rank r:** Lower r → less memory but may lose optimization directions. Paper uses r=64 for 1B-7B models, r=256 for pre-training.
  - **Subspace update frequency T:** Higher T → less overhead but stale subspace. Paper suggests T based on convergence criterion ||Ĝₜ|| ≤ ς.
  - **Perturbation ρ scheduling:** Paper uses ρₜ = ρₘᵢₙ + (ρₘₐₓ - ρₘᵢₙ)(lr - lrₘᵢₙ)/(lrₘₐₓ - lrₘᵢₙ) following GSAM.
  - **μ (smoothing parameter):** Must be small enough for accurate finite difference but large enough to avoid numerical precision issues.

- **Failure signatures:**
  - Training loss plateaus early → subspace rank r too low; increase r
  - Memory doesn't decrease → optimizer not using projected states; check that Mₜ, Vₜ remain low-rank
  - No generalization improvement vs baseline → ρ may be too small or zeroth-order variance too high; try increasing q
  - NaN/Inf in GPert → μ too small relative to numerical precision; increase μ

- **First 3 experiments:**
  1. **Ablation on rank r:** Compare r∈{4, 8, 16, 32, 64} on GLUE with RoBERTa-base. Expect diminishing returns beyond r=16 for simple tasks, r=64 for complex.
  2. **Ablation on q (zeroth-order queries):** Compare q∈{1, 2, 4} on validation loss variance. Paper claims q=1 sufficient; verify stability.
  3. **Memory profiling:** Measure peak GPU memory for LORENZA vs GaLore vs LoRA on LLaMA-7B. Expect ~25% reduction vs GaLore (no double backprop overhead).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an adaptive rank selection mechanism be integrated into LORENZA to dynamically determine the optimal subspace dimension r during training?
- **Basis in paper:** Discussion section states "Future work may explore adaptive rank selection... and evaluating effectiveness in knowledge editing."
- **Why unresolved:** Current work relies on fixed ranks (r=4, 8) chosen a priori, which may be suboptimal across different layers or training stages.
- **What evidence would resolve it:** Convergence and accuracy results from a modified LORENZA algorithm that adjusts rank dynamically based on gradient norms or explained variance thresholds.

### Open Question 2
- **Question:** How robust is LORENZA's zeroth-order ascent estimation when combined with low-precision (quantized) optimizer states or weights?
- **Basis in paper:** Discussion section lists "integration with quantization techniques" as a direction for future work.
- **Why unresolved:** Authors note that 4-bit updates can be "easily incorporated," but the sensitivity of the zeroth-order finite difference step (dependent on small μ) to quantization noise has not been empirically validated.
- **What evidence would resolve it:** Experiments evaluating LORENZA's performance and stability when fine-tuning quantized LLMs (e.g., 4-bit) compared to full-precision baselines.

### Open Question 3
- **Question:** Does the flat minima property induced by LORENZA provide specific advantages for knowledge editing tasks in LLMs?
- **Basis in paper:** Discussion section explicitly mentions "evaluating effectiveness in knowledge editing" as a future application.
- **Why unresolved:** While paper demonstrates generalization on reasoning tasks, it does not test the method's ability to update specific factual knowledge without catastrophic forgetting.
- **What evidence would resolve it:** Benchmarks on knowledge editing datasets (e.g., ZeroShotEdit) showing if LORENZA allows for more stable or efficient knowledge updates than standard LoRA or GaLore.

### Open Question 4
- **Question:** What is the precise computational latency trade-off introduced by the multiple forward passes required for randomized zeroth-order estimation compared to standard gradient backpropagation?
- **Basis in paper:** Paper claims computational efficiency by avoiding double backpropagation, but Algorithm 3 requires computing function values for ascent direction, involving additional forward passes.
- **Why unresolved:** While memory usage is theoretically lower, the wall-clock time impact of these additional forward passes (even with q=1) is not rigorously benchmarked against the latency of the backward pass in methods like GaLore.
- **What evidence would resolve it:** Wall-clock time per iteration comparisons between LORENZA and GaLore across various hardware configurations and batch sizes.

## Limitations
- The low-rank gradient assumption's universality across all LLM architectures lacks comprehensive validation, particularly for non-reversible networks.
- Zeroth-order approximation relies heavily on single random direction (q=1) claim, with limited empirical validation of variance and approximation quality across model scales.
- Computational latency trade-off of multiple forward passes versus avoided backpropagation is not rigorously benchmarked against wall-clock time of existing methods.

## Confidence

- **High confidence:** Memory efficiency claims (25-30% reduction vs GaLore) and convergence rate bounds (O(Cβ/√T) + β²ρ²) are mathematically rigorous and well-supported.
- **Medium confidence:** Generalization improvements on GLUE and GSM8K are demonstrated but could benefit from more extensive ablation studies on rank selection and perturbation scheduling.
- **Low confidence:** The claim that natural gradient structure "naturally diminishes" during training is primarily referenced from external work (Refael et al., 2025) without direct experimental validation in this paper.

## Next Checks
1. **Variance sensitivity analysis:** Systematically evaluate LORENZA's performance across q ∈ {1, 2, 4, 8} on GLUE tasks to quantify zeroth-order approximation error and establish variance bounds.
2. **Architecture universality test:** Apply LORENZA to non-reversible architectures (e.g., standard Transformers without reversibility) to test whether the low-rank gradient assumption holds or breaks down.
3. **Perturbation sensitivity sweep:** Conduct a grid search over ρ ∈ {0.001, 0.01, 0.1} with different scheduling strategies to determine optimal sharpness-aware perturbation magnitude for various task complexities.