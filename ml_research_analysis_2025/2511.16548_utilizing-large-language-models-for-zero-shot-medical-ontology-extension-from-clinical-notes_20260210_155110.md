---
ver: rpa2
title: Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from
  Clinical Notes
arxiv_id: '2511.16548'
source_url: https://arxiv.org/abs/2511.16548
tags:
- ontology
- extension
- clinical
- hierarchical
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CLOZE, a zero-shot framework that uses large
  language models (LLMs) to extend hierarchical medical ontologies from clinical notes.
  CLOZE addresses privacy and scalability by combining two LLM agents: one for de-identifying
  protected health information (PHI) and another for extracting disease-related entities.'
---

# Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes

## Quick Facts
- arXiv ID: 2511.16548
- Source URL: https://arxiv.org/abs/2511.16548
- Reference count: 28
- This paper introduces CLOZE, a zero-shot framework that uses large language models (LLMs) to extend hierarchical medical ontologies from clinical notes.

## Executive Summary
This paper introduces CLOZE, a zero-shot framework that uses large language models (LLMs) to extend hierarchical medical ontologies from clinical notes. CLOZE addresses privacy and scalability by combining two LLM agents: one for de-identifying protected health information (PHI) and another for extracting disease-related entities. These entities are integrated into an existing ontology using a hybrid approach that combines semantic embeddings from a biomedical language model (SapBERT) with hierarchical reasoning via LLMs. Experiments on real clinical notes show that CLOZE outperforms baseline methods, achieving high precision (around 80%) and robust performance across automated and human evaluations. The framework is effective, scalable, and privacy-preserving, with strong potential for real-world biomedical and clinical informatics applications.

## Method Summary
CLOZE uses a pipeline of two LLM agents and a biomedical embedding model to extend medical ontologies from clinical notes. First, a De-identification Agent (LLaMA-3-70B-Instruct) identifies and masks PHI entities into a structured JSON format. Second, an Entity-extraction Agent (LLaMA-3-70B-Instruct) extracts disease-related entities from the de-identified text. The extracted entities are then embedded using SapBERT, and a recursive insertion algorithm traverses the target ontology to find the best placement using both semantic similarity and LLM-based relationship classification (Equivalence, Subsetting, Neither). The method is fully zero-shot and requires no fine-tuning.

## Key Results
- CLOZE achieves high precision (~80%) in extending the Disease Ontology from clinical notes
- Outperforms baseline methods, with SapBERT+LLM-Hierarchical achieving 79.6% precision versus 43.1% for SapBERT+LLM-Onetime
- Human evaluation shows strong accuracy (2.00/2.00 max score) for relevance, accuracy, and importance of inserted nodes
- De-identification agent achieves F1=0.62, outperforming rule-based baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining domain-specific embeddings with LLM reasoning improves hierarchical placement accuracy compared to either approach alone.
- Mechanism: SapBERT provides semantically grounded similarity scores for candidate node selection, while the LLM classifies the relationship (Equivalence/Subsetting/Neither) using contextual understanding. This division of labor prevents LLM hallucination on deep hierarchies by constraining search to embedding-validated candidates.
- Core assumption: SapBERT embeddings capture biomedical semantic similarity sufficiently for coarse localization; LLM can reliably classify sibling vs. parent-child relationships given two entity names.
- Evidence anchors:
  - [abstract] "combining semantic embeddings from a biomedical language model (SapBERT) with hierarchical reasoning via LLMs"
  - [section V-B, Table III] SapBERT+LLM-Hierarchical achieves 79.6% precision vs. 43.1% for SapBERT+LLM-Onetime and 38.9% for LLM-Onetime
  - [corpus] RELATE paper similarly combines LLMs with ontology constraints for relation extraction, suggesting pattern validity
- Break condition: If candidate ontology has very shallow depth (<3 layers), hierarchical recursion provides minimal benefit over one-time placement.

### Mechanism 2
- Claim: Structured prompting with explicit PHI type enumeration improves de-identification recall while maintaining precision.
- Mechanism: The De-identification Agent is prompted to extract a predefined set of 12 PHI types into a structured JSON dictionary, then mask these entities. Explicit type specification constrains the task and improves coverage of borderline cases.
- Core assumption: LLM can reliably identify PHI categories when explicitly enumerated; structured output format reduces missed entities.
- Evidence anchors:
  - [section III-C] "LLM agent outputs a JSON dictionary... This structured output improves interpretability, enables traceability, and promotes high recall"
  - [section V-A, Table I] LLaMA-3-70B-Instruct achieves F1=0.62, outperforming GPT-4 (0.58) and rule-based PhysioNet (0.28)
  - [corpus] Limited direct corpus evidence on PHI extraction architectures; related work focuses on downstream tasks
- Break condition: If clinical notes contain novel PHI types not in the predefined list, recall will degrade without schema updates.

### Mechanism 3
- Claim: Recursive layer-by-layer traversal with early termination prevents misplacement in deep ontologies.
- Mechanism: At each layer, the algorithm identifies the semantically closest node via SapBERT, queries the LLM for relationship classification, and either terminates (Equivalence), recurses to children (Subsetting), or inserts as new child (Neither with no matching children). This prevents premature insertion at shallow levels.
- Core assumption: The ontology has consistent hierarchical semantics where "is-a" relationships are transitive; LLM can reliably distinguish subsetting from unrelated concepts.
- Evidence anchors:
  - [section III-D, Algorithm 1] Recursive loop continues until Equivalence, leaf-level Subsetting, or Neither triggers insertion
  - [section V-C, Table IV] Human evaluation shows Accuracy=2.00/2.00 (max score), vs. 1.53 for non-hierarchical approach
  - [corpus] Ontology-based knowledge representation paper emphasizes hierarchical consistency for diagnostic reliability
- Break condition: If ontology contains cross-linking or multiple inheritance without explicit modeling, single-path recursion may miss valid insertion points.

## Foundational Learning

- Concept: Hierarchical ontology structure (partial ordering, is-a relationships)
  - Why needed here: CLOZE assumes ontologies are tree-like DAGs where entities have single or well-defined parents; understanding this is essential for implementing recursive insertion.
  - Quick check question: Given ontology {Disease → Respiratory → Pneumonia}, where should "viral pneumonia" be inserted?

- Concept: Cosine similarity for semantic embedding matching
  - Why needed here: SapBERT generates dense vectors; cosine similarity identifies the closest candidate node. Without this, you cannot implement the embedding-based selection step.
  - Quick check question: If entity A has embedding [0.8, 0.6] and entity B has [0.6, 0.8], what is their cosine similarity?

- Concept: Zero-shot prompting with structured output constraints
  - Why needed here: All three LLM agents rely on carefully designed prompts to produce JSON outputs without fine-tuning. Prompt design directly affects extraction quality.
  - Quick check question: How would you modify the Entity-extraction prompt to also capture symptom entities alongside diseases?

## Architecture Onboarding

- Component map: Raw clinical notes -> De-identification Agent (LLaMA-3-70B) -> De-identified text -> Entity-extraction Agent (LLaMA-3-70B) -> Disease entities -> SapBERT Embedder -> Entity embeddings -> Relation-determination Agent (LLaMA-3-70B) -> Relationship classification -> Recursive Insertion Controller -> Extended ontology

- Critical path: Entity extraction quality -> embedding quality -> hierarchical placement accuracy. Errors in de-identification do not cascade if PHI is over-masked; errors in entity extraction directly reduce coverage.

- Design tradeoffs:
  - SapBERT vs. general LLM embeddings: SapBERT provides domain-specific similarity but requires separate model deployment; LLM-only is simpler but less accurate (Table III: 36.7% vs. 79.6% precision)
  - Recursive vs. one-time placement: Recursion is computationally more expensive but necessary for deep ontologies (>3 layers)
  - Local vs. cloud LLM deployment: Local deployment ensures HIPAA compliance but requires GPU infrastructure; cloud (Azure OpenAI) offers scalability with BAA agreements

- Failure signatures:
  - Low recall in de-identification: Check if PHI type list covers all 12 categories; verify JSON parsing doesn't silently drop entities
  - Entities inserted at incorrect depth: Likely SapBERT embedding quality issue or LLM relation classification error; inspect "Neither" vs. "Subsetting" confusion
  - Empty extraction results: Prompt may be too restrictive; verify "disease-only" constraint isn't filtering valid entities

- First 3 experiments:
  1. Validate de-identification on 10 notes with manual PHI annotation: compute precision/recall per PHI type to identify weak categories
  2. Test entity extraction with relaxed prompt (include symptoms): measure F1 change to determine if disease-only constraint is too narrow
  3. Compare SapBERT vs. BioBERT embeddings on 20 entity-ontology pairs: verify SapBERT's self-alignment pretraining provides measurable benefit for your target ontology

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CLOZE's performance generalize to larger, multi-institutional clinical datasets beyond the 100-note sample used in this study?
  - Basis in paper: [explicit] The authors state in the Conclusion that the "current evaluation is limited by a small dataset of 100 clinical notes due to the labor-intensive nature of privacy-compliant annotation."
  - Why unresolved: It remains unclear if the high precision (~80%) and low "Not Sure" rates are stable when applied to the stylistic variance found in notes from different hospital systems or when processing volume increases significantly.
  - What evidence would resolve it: Experimental results from applying CLOZE to larger, public benchmarks (e.g., MIMIC-IV discharge summaries) or a wider collection of hospital sources.

- **Open Question 2**: Can the framework effectively extend ontologies with non-disease medical concepts, such as symptoms, treatments, or medications?
  - Basis in paper: [inferred] The paper explicitly restricts the Entity-extraction Agent to "disease-only entity recognition," and the evaluation is conducted solely against the Disease Ontology (DO).
  - Why unresolved: It is unknown if the "Subsetting" relation logic and SapBERT embeddings perform equally well on ontologies with different structural characteristics, such as chemical hierarchies or treatment pathways.
  - What evidence would resolve it: Extending the prompt definitions and testing the pipeline on diverse ontologies like SNOMED CT (for procedures) or ATC (for medications).

- **Open Question 3**: To what extent does the CLOZE-extended ontology improve performance in downstream clinical predictive tasks compared to static ontologies?
  - Basis in paper: [explicit] The Conclusion identifies "alignment with clinical decision-making" as a necessary future direction for supporting real-world healthcare applications.
  - Why unresolved: While the paper validates the structural accuracy of the insertion, it does not demonstrate that the newly added entities provide tangible value for downstream use cases like phenotyping or diagnosis prediction.
  - What evidence would resolve it: A comparative study measuring the accuracy of a clinical prediction model using the CLOZE-extended ontology versus the original seed ontology.

## Limitations

- Reliance on a proprietary clinical note corpus that cannot be independently verified
- Performance may vary significantly across institutions with different PHI patterns
- Human evaluation results represent a small sample size (30 entities), limiting generalizability

## Confidence

- **High Confidence**: The core mechanism of combining SapBERT embeddings with LLM-based hierarchical reasoning (Mechanism 1) is well-supported by the quantitative comparison showing 79.6% precision versus 43.1% for alternatives.
- **Medium Confidence**: The de-identification performance (F1=0.62) is reasonable but context-dependent; effectiveness may vary significantly across institutions with different PHI patterns.
- **Medium Confidence**: Human evaluation results (Accuracy=2.00/2.00) appear strong but represent a small sample size (30 entities), limiting generalizability.

## Next Checks

1. **Cross-institutional validation**: Test CLOZE on clinical notes from a different hospital system (e.g., MIMIC-III) to assess generalizability of both de-identification and entity extraction components.
2. **Prompt sensitivity analysis**: Systematically vary the LLM prompts for entity extraction and relation classification to quantify performance stability and identify optimal configurations.
3. **Ontology coverage assessment**: Measure the proportion of extracted entities that actually extend the Disease Ontology versus those that are already present or semantically unrelated, to validate the precision claims in real-world usage.