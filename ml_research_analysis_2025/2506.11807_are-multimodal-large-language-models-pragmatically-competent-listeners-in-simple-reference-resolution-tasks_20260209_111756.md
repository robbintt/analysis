---
ver: rpa2
title: Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple
  Reference Resolution Tasks?
arxiv_id: '2506.11807'
source_url: https://arxiv.org/abs/2506.11807
tags:
- color
- middle
- language
- left
- right
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether multimodal large language models
  (MLLMs) can perform basic reference resolution tasks requiring contextualized pragmatic
  reasoning about color in simple visual contexts. Using dyadic reference games with
  abstract stimuli (color patches and color grids), the research tests whether MLLMs
  can identify target referents based on human-generated descriptions.
---

# Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?

## Quick Facts
- arXiv ID: 2506.11807
- Source URL: https://arxiv.org/abs/2506.11807
- Reference count: 31
- Primary result: MLLMs show near-human performance on simple color patches (up to 95% accuracy) but struggle with complex color grids (66% accuracy), revealing fundamental limitations in pragmatic reference resolution.

## Executive Summary
This study investigates whether multimodal large language models (MLLMs) can perform basic reference resolution tasks requiring contextualized pragmatic reasoning about color in simple visual contexts. Using dyadic reference games with abstract stimuli (color patches and color grids), the research tests whether MLLMs can identify target referents based on human-generated descriptions. Results show that while larger Qwen models achieve near-human performance on simple color patch tasks (up to 95% accuracy), all models struggle with more complex color grid structures, with best performance at 66% accuracy. Location biases in model responses suggest they often rely on simple keyword matching rather than true understanding of spatial relationships within grids.

## Method Summary
The study evaluates three state-of-the-art MLLM families (LLaVA-NeXT, Qwen2-VL, and Janus-Pro) across different model sizes on datasets containing color patches and color grids with varying visual complexity. Models are prompted with dialogue scripts between human directors and matchers, then asked to identify which visual object is being referenced. The evaluation uses greedy decoding on concatenated visual inputs, with location labels extracted via regex parsing from model responses. Accuracy is measured across three visual complexity conditions based on CIELAB distances, comparing performance against human baselines.

## Key Results
- Qwen 72B achieves 95.1% accuracy on simple color patches (near-human 97%), but only 66.5% on complex color grids
- All models show strong location biases, over-predicting "left" at rates far above chance (expected 33.3%)
- Performance varies dramatically by architecture - Qwen 72B (87.5% on patches) outperforms LLaVA 72B (62.3%) despite similar scale
- No improvement from simplified dialogues suggests visual grounding failures rather than linguistic comprehension issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pragmatic competence in MLLMs emerges with scale, but remains fragile and task-dependent.
- Mechanism: Larger parameter counts enable richer contextual representations that support reasoning about speaker intent and contrastive referents. Qwen 72B achieves 95.1% on color patches (near-human 97%), while Qwen 2B drops to 77.3%. However, the same 72B model falls to 66.5% on grids—a ~29-point gap—suggesting scale extends but does not fundamentally alter reasoning capacity.
- Core assumption: Emergent pragmatic reasoning relies on sufficient representational capacity to encode context-target relationships, not just pattern matching.
- Evidence anchors:
  - [abstract]: "basic pragmatic capabilities, such as context-dependent interpretation of color descriptions, still constitute major challenges"
  - [section 4.1]: "Qwen 7b also outperforms Janus and LLaVA variants of the same size"
  - [corpus]: "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation" (arXiv:2504.16060) reports parallel failures in generation tasks, suggesting systemic limitations beyond this specific paradigm.
- Break condition: When visual structure requires compositional spatial reasoning (grids), scale benefits plateau—Qwen 72B still trails humans by 26 points.

### Mechanism 2
- Claim: Architectural choices in vision encoding and position representation systematically affect pragmatic grounding.
- Mechanism: Qwen2-VL uses ViT with Multimodal Rotary Position Embedding (M-RoPE) and naive dynamic resolution support, trained with multi-stage pretraining. LLaVA-NeXT uses CLIP-ViT-L with fixed higher-res grids. The 34-point gap between Qwen 72B (87.5%) and LLaVA 72B (62.3%) on color patches—despite similar scale—points to architectural rather than capacity differences.
- Core assumption: M-RoPE and multi-stage training better preserve spatial relationships needed for reference resolution.
- Evidence anchors:
  - [section 4.4]: "architectural decisions in MLLMs and fine-tuning protocols can have substantial effects on fundamental capabilities"
  - [table 5]: Qwen uses "Naive Dynamic Resolution support and Multimodal Rotary Position Embedding" vs. LLaVA's "dynamic high resolution"
  - [corpus]: Limited direct evidence on M-RoPE specifically; architectural comparisons remain underexplored in related work.
- Break condition: Architectural advantages may not transfer—Qwen's gains on patches don't fully scale to grids (70.2% far vs. 63.2% close).

### Mechanism 3
- Claim: MLLMs default to keyword-matching heuristics when pragmatic reasoning exceeds capacity.
- Mechanism: When dialogues contain spatial keywords ("left", "middle", "right"), models predict matching labels at rates far above chance. Janus 1B predicts "left" 100% of the time when "left" appears in dialogue (Table 4). Even Qwen 72B shows 47.4% "left" prediction when "left" is mentioned—above the expected 33.3%. This indicates surface-level lexical association rather than grounded spatial understanding.
- Core assumption: Models lack mechanisms to distinguish between position descriptions within grids vs. between referents.
- Evidence anchors:
  - [section 4.2]: "models adopt a simple strategy of spotting spatial keywords and using these in their responses"
  - [table 4]: Qwen 72B predicts mentioned labels at 47.4-69% rates vs. 33.3% expected
  - [corpus]: "Vision Language Models are Blind" (Rahmanzadehgervi et al., 2024) documents similar low-level vision failures in state-of-the-art models.
- Break condition: Keyword matching fails on graded descriptions requiring composition—Example 1e ("middle tile green, lower right purple") stumps all models despite containing clear terms.

## Foundational Learning

- Concept: **Reference Resolution in Dyadic Games**
  - Why needed here: The task structure (director-matcher with distractors) creates pragmatic pressure—descriptions must distinguish targets from alternatives, not just describe them.
  - Quick check question: Given three similar colors, why might "orange" be pragmatically infelicitous if two patches could be called orange?

- Concept: **Contextual Color Semantics**
  - Why needed here: Color terms shift meaning based on available alternatives. "Yellow-green" is sufficient when no other green-yellow exists, but requires elaboration ("the less green one") when distractors share properties.
  - Quick check question: If a director says "the bluish one" among three blue patches, what pragmatic inference is required?

- Concept: **Compositional Spatial Reasoning**
  - Why needed here: Grid tasks require binding multiple spatial relations ("first square is brown" + "bottom right is green") into a unified referent hypothesis.
  - Quick check question: How would you identify a 3×3 grid where "the middle tile is green and the lower right is purple"?

## Architecture Onboarding

- Component map: Vision Encoder → [CLIP-ViT-L | ViT+M-RoPE | SigLIP] → Visual Features → Cross-modal Connection (MLP) → Fused Representation → LLM Backbone → [Qwen2 | Vicuna | Auto-regressive Transformer] → Token Prediction

- Critical path: Image concatenation → Vision encoding → Cross-modal projection → LLM inference on dialogue script → Location extraction via regex

- Design tradeoffs:
  - Qwen's M-RoPE: Better spatial fidelity vs. increased compute for position encoding
  - LLaVA's 34B outlier: Strong performance but inconsistent scaling (7B→72B shows only 2-point gain)
  - Janus's decoupled encoding: Separates understanding (SigLIP) from generation (VQ tokenizer) but shows weakest results

- Failure signatures:
  - Location bias: Systematic over-prediction of "left" (first-mentioned position)
  - Keyword leakage: Predicting spatial terms appearing in dialogue rather than inferred referent
  - Near-chance on complex grids (~33%) indicates ungrounded guessing
  - No improvement from simplified dialogues (Table 3) suggests failure is perceptual, not linguistic

- First 3 experiments:
  1. **Probe spatial bias**: Shuffle position labels in prompt ("right, middle, left" vs. "left, middle, right") to distinguish positional from lexical bias.
  2. **Isolate vision vs. language**: Provide ground-truth grid descriptions in text-only condition to separate visual grounding failures from reasoning failures.
  3. **Scale-only comparison**: Run Qwen 7B with LLaVA 72B's vision encoder (if feasible) to isolate architectural vs. scale contributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific architectural variations or fine-tuning protocols lead to implicit pragmatic competencies in MLLMs?
- Basis in paper: [explicit] The authors state that "Future work should explore such variations more systematically, potentially with smaller LMs, to gain a deeper understanding of the factors that lead to implicit pragmatic competencies in LMs."
- Why unresolved: The study observed significant performance disparities between model families (LLaVA vs. Qwen) but did not isolate the causes, noting that Qwen uses different vision encoders, positional encodings, and pretraining stages.
- What evidence would resolve it: Ablation studies comparing models with controlled differences in vision encoders, positional encoding methods (e.g., M-RoPE vs. standard), and pretraining data compositions.

### Open Question 2
- Question: Can MLLMs achieve higher accuracy in reference resolution tasks through interactive common ground negotiation?
- Basis in paper: [explicit] The discussion notes that human matchers could collaborate by requesting clarification, whereas models acted as "overhearers." The authors propose: "Future work should explore more interactive settings where the agent can negotiate common ground with the director."
- Why unresolved: The current experimental setup forced models to rely solely on the static prompt information without the ability to ask clarifying questions.
- What evidence would resolve it: A new reference game setup where the MLLM is allowed to generate clarification questions before making a final referent selection, measuring accuracy changes.

### Open Question 3
- Question: Does changing the visual input format from a single concatenated image to individual images improve model performance?
- Basis in paper: [inferred] The authors list a limitation regarding their visual input method: "Changing the format of the visual input so that each of the three object is provided as an individual image could enable the models to use the full potential of their visual encoders."
- Why unresolved: The study concatenated stimuli into one image to match the human presentation, potentially limiting the resolution or attention applied to individual grid items.
- What evidence would resolve it: Comparative experiments evaluating the same models on the same tasks using single-image vs. multi-image prompts (if supported by the architecture).

## Limitations

- The evaluation methodology may underestimate model performance by requiring specific output formats and using rigid regex parsing
- Visual complexity definitions are not precisely specified for the grid task, creating uncertainty about relative difficulty calibration
- Dramatic performance differences between architectures cannot be causally attributed without controlled ablation experiments

## Confidence

**High confidence**: The core finding that MLLMs struggle with pragmatic reference resolution in complex visual contexts is well-supported. The systematic location biases and keyword-matching behaviors observed across all model families and sizes are robust patterns.

**Medium confidence**: The claim that larger models show improved pragmatic competence has mixed support. While Qwen 72B outperforms smaller variants on color patches (95.1% vs 77.3% for 2B), the gains are much smaller on color grids (66.5% vs 57.6%), suggesting scale effects are task-dependent rather than fundamental improvements in pragmatic reasoning.

**Low confidence**: The specific architectural superiority of Qwen's M-RoPE and multi-stage training approach remains speculative. The study shows correlation between these choices and performance, but without controlled ablation experiments, causation cannot be established.

## Next Checks

1. **Controlled architecture ablation**: Test whether Qwen's performance advantage stems from M-RoPE position encoding by implementing a variant that uses LLaVA's fixed position encoding while keeping Qwen's other architectural features constant. This would isolate the architectural contribution from scale effects.

2. **Output format robustness**: Design a post-processing pipeline that can handle diverse model output formats (e.g., "the one on the right" → "right") and re-run the evaluation to determine if current accuracy metrics underestimate model performance due to rigid parsing assumptions.

3. **Visual complexity benchmarking**: Create an intermediate visual complexity condition between patches and grids (e.g., 2×2 color grids) to better characterize the performance degradation curve and identify whether the sharp drop from 95% to 66% represents a gradual transition or a phase change in model capability.