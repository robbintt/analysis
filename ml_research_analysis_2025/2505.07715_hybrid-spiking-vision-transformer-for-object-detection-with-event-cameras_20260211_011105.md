---
ver: rpa2
title: Hybrid Spiking Vision Transformer for Object Detection with Event Cameras
arxiv_id: '2505.07715'
source_url: https://arxiv.org/abs/2505.07715
tags:
- detection
- event
- object
- dataset
- fall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HsVT, a hybrid ANN-SNN model for event-based
  object detection. HsVT integrates spatial and temporal feature extraction modules
  to capture spatiotemporal features in event streams.
---

# Hybrid Spiking Vision Transformer for Object Detection with Event Cameras

## Quick Facts
- arXiv ID: 2505.07715
- Source URL: https://arxiv.org/abs/2505.07715
- Reference count: 17
- This paper proposes HsVT, a hybrid ANN-SNN model for event-based object detection that achieves 0.478 mAP on GEN1 dataset

## Executive Summary
This paper introduces HsVT, a hybrid artificial neural network-spiking neural network (ANN-SNN) architecture for event-based object detection. The model integrates spatial and temporal feature extraction modules to capture spatiotemporal features in event streams, using ANN components for spatial processing and SNN components for temporal processing. The authors also introduce a new Fall Detection Dataset captured using event cameras for privacy protection and reduced memory usage. Experiments demonstrate significant performance improvements in event detection with fewer parameters compared to existing methods.

## Method Summary
HsVT processes event streams accumulated into voxel/frame bins using a 4-block architecture. Each block contains MaxViT (ANN) for spatial attention and SpikingMLP (SNN) for feature processing, combined with temporal processing via LSTM (blocks 1-3) and STFE (block 4). The model uses LIF neurons with surrogate gradient functions for training, and features a YOLOX detection head with FPN. Training employs Adam optimizer with OneCycle LR scheduling and mixed precision on RTX 4090 GPUs.

## Key Results
- HsVT-B achieves 0.478 mAP on GEN1 dataset, outperforming other SNN-based detectors
- The model demonstrates significant parameter efficiency compared to pure ANN methods
- Ablation studies show STFE placement and LIF neurons provide substantial performance gains
- Temporal binning strategy (Δt) critically affects performance, with optimal values varying by dataset

## Why This Works (Mechanism)

### Mechanism 1: Multi-Axis Spatial Attention for Sparse Data
The MaxViT component processes input via Block-SA (capturing fine-grained local structural patterns) and Grid-SA (modeling long-range dependencies), combined via SpikingMLP layers. This decoupling likely improves feature extraction from sparse event streams compared to standard monolithic attention.

### Mechanism 2: Asymmetric Temporal Hierarchy (LSTM → STFE)
Blocks 1-3 use LSTMs for dense temporal integration, while block 4 uses lightweight STFE for final fusion. This balances temporal depth with parameter efficiency, with heavy recurrence for early layers and efficient spiking dynamics for deeper semantic features.

### Mechanism 3: LIF Neuron Non-linearity
LIF neurons accumulate voltage over time and fire only upon reaching threshold, providing temporal denoising and feature abstraction superior to static ANN activation functions. The "leak" mechanism effectively filters background noise while retaining persistent motion signals.

## Foundational Learning

- **Event Representation (Δt bins)**: Why needed: Model processes accumulated tensors, not raw async events. Quick check: How does changing Δt from 50ms to 200ms affect input tensor sparsity?
- **Hybrid ANN-SNN Training**: Why needed: Combines floating-point operations (ANN) with binary spike operations (SNN). Quick check: Why are surrogate gradient functions necessary for backpropagation through Spiking Neurons?
- **Horizontal vs. Vertical Propagation**: Why needed: Architecture has distinct Vertical (depth: Block 1→4) and Horizontal (time: t→t+1 via LSTM/STFE) flows. Quick check: Does Block 3 output at time t feed into Block 3 at t+1 or Block 4 at time t?

## Architecture Onboarding

- **Component map**: Input {(x,y,p,t)} → Voxel/Frame bins → MaxViT (spatial) → LSTM (temporal state h_t) → STFE (final fusion) → FPN → YOLOX head
- **Critical path**: Input → MaxViT (spatial attention) → LSTM (temporal state h_t) → STFE (final fusion) → FPN
- **Design tradeoffs**: Accuracy vs. Energy (hybrid uses costly ANN attention for spatial, efficient SNNs for temporal); Dataset Sensitivity (Base may overfit on small datasets vs. Tiny)
- **Failure signatures**: Overfitting on small data (Base worse than Tiny); Temporal Disconnect (disabling LSTM state passing degrades fast-moving object detection)
- **First 3 experiments**: 1) Reproduce Ablation (Table 6): Train "LSTM-LSTM-LSTM-LSTM" vs. "LSTM-LSTM-LSTM-STFE" on Aircraft dataset. 2) Temporal Delta Sweep: Train HsVT-Tiny on Fall dataset with Δt ∈ {40, 200, 1000}ms. 3) Neuron Ablation: Swap LIF for IF nodes in SpikingMLP.

## Open Questions the Paper Calls Out

- **Open Question 1**: Is performance degradation with increased model size on FALL and AIR datasets strictly caused by data scarcity? The authors note larger models don't improve on these datasets and speculate about overfitting due to dataset size limitations, but haven't proven this hypothesis with larger datasets or regularization experiments.

- **Open Question 2**: Can HsVT energy efficiency be optimized to compete with pure SNN baselines? While SNNs are motivated by low energy consumption, HsVT-B consumes 134.5 mJ compared to pure SNN SFOD at 7.26 mJ, leaving the balance between accuracy and low-energy goals unresolved.

- **Open Question 3**: Does HsVT generalize to real-world event data given that the Fall Detection Dataset was generated via simulation? The dataset uses ESIM event camera simulator to convert frame-based video, which may not capture real sensor noise or artifacts, and hasn't been validated on physical event cameras.

## Limitations
- Hybrid ANN-SNN design introduces training complexity due to non-differentiable spiking operations and surrogate gradient limitations
- Performance heavily depends on temporal binning strategy (Δt), with optimal values varying significantly across datasets
- Fall Detection Dataset is newly introduced and hasn't been validated by independent researchers

## Confidence

- **High Confidence**: Parameter efficiency claims are well-supported by Table 11 comparisons showing HsVT-B achieves 0.478 mAP with fewer parameters than pure ANN methods
- **Medium Confidence**: Ablation studies demonstrating STFE effectiveness (Table 6) and LIF neuron advantages (Table 3) are internally consistent but need cross-dataset validation
- **Low Confidence**: Generalizability of optimal Δt values across different event camera hardware and scene dynamics remains uncertain

## Next Checks

1. **Cross-Dataset Δt Robustness**: Train HsVT on GEN1 dataset using Δt values optimized for Fall (200ms) and Air (10ms) datasets to quantify performance degradation and validate dataset-specific optimization hypothesis

2. **Neuron Function Ablation**: Systematically replace LIF neurons with IF neurons across all spiking components (SpikingMLP, STFE) in HsVT-B and measure mAP changes to confirm the 0.640 vs 0.602 performance gap

3. **Pure ANN Baseline Comparison**: Implement a hybrid model where the final STFE block is replaced with a standard CNN block (same channel dimensions) to isolate the contribution of spiking dynamics in temporal feature extraction pathway