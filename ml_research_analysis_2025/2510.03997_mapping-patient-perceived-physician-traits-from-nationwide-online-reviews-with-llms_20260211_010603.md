---
ver: rpa2
title: Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with
  LLMs
arxiv_id: '2510.03997'
source_url: https://arxiv.org/abs/2510.03997
tags:
- trait
- patient
- physician
- score
- doctor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a validated LLM-based pipeline for extracting
  standardized physician personality and professional competency traits from online
  patient reviews. The methodology maps reviews to Big Five personality dimensions
  and five patient-oriented subjective judgments, validated through multi-model comparison
  and human expert benchmarking (correlation coefficients 0.72-0.89).
---

# Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs

## Quick Facts
- arXiv ID: 2510.03997
- Source URL: https://arxiv.org/abs/2510.03997
- Reference count: 40
- LLM pipeline extracts physician personality and competency traits from online reviews with validated accuracy

## Executive Summary
This study presents a validated LLM-based pipeline for extracting standardized physician personality and professional competency traits from online patient reviews. The methodology maps reviews to Big Five personality dimensions and five patient-oriented subjective judgments, validated through multi-model comparison and human expert benchmarking (correlation coefficients 0.72-0.89). Applied to 226,999 physicians across 4.1 million reviews, the approach achieves strong external validity with patient satisfaction correlations ranging from 0.41-0.81 (all p<0.001). Cluster analysis identifies four distinct physician archetypes, revealing systematic patterns including gender disparities and specialty-specific trait distributions.

## Method Summary
The pipeline processes aggregated patient reviews per physician (5-100 reviews each) using structured LLM prompts to extract 10 standardized traits: Big Five personality dimensions (Openness, Conscientiousness, Extraversion, Agreeableness, Emotional Stability) plus five patient-oriented subjective judgments (IQC, PCC, SPS, SCO, STS). Two LangChain-based extraction agents use strict XML schema outputs requiring scores, evidence quotes, consistency ratings, and sufficiency assessments. Validation employs LLM-as-a-judge evaluation (Gemini-2.5 Pro) and human benchmarking on 300 physician profiles, with Gemini-2.5 Flash selected for large-scale deployment. The top-down approach synthesizes complete review corpora per physician rather than processing individual reviews separately.

## Key Results
- Human-LLM agreement correlations of 0.72-0.89 across 10 physician traits
- External validity correlations with patient satisfaction ranging from 0.41-0.81 (all p<0.001)
- Cluster analysis reveals four distinct physician archetypes with systematic gender and specialty patterns
- Exceptional IQC-SPS correlation (r = 0.928) suggests construct overlap in patient perceptions

## Why This Works (Mechanism)

### Mechanism 1
Top-down aggregation of physician reviews enables more consistent trait inference than bottom-up processing. The pipeline processes the complete review corpus per physician as unified context, allowing the LLM to synthesize contradictory evidence, identify consistent patterns, and resolve ambiguities across multiple patient perspectives—mimicking expert psychological evaluation. Core assumption: Modern LLMs have sufficient context windows and reasoning capacity to form holistic impressions across long-form aggregated text without degradation.

### Mechanism 2
Chain-of-thought prompting with structured XML output enforces interpretable, auditable trait extraction. The system requires models to generate four outputs per trait—standardized scores, textual evidence, consistency ratings, and sufficiency assessments—within strict XML schemas. This forces explicit reasoning traces that can be validated against source text. Core assumption: Enforcing structured output improves reliability without constraining model reasoning flexibility.

### Mechanism 3
Dual-trait framework (Big Five + healthcare-specific judgments) captures complementary dimensions that jointly predict patient satisfaction. The Big Five captures stable personality dispositions; the five subjective judgments capture patient-facing behavioral performance. The exceptionally high correlation between IQC and SPS (r = 0.928) suggests overlapping constructs, while PCC's stronger correlation with Conscientiousness (r = 0.710) than Openness (r = 0.568) indicates patients conflate reliability with expertise. Core assumption: Patient-perceived traits are valid proxies for actual physician behaviors and meaningful quality indicators.

## Foundational Learning

- Concept: **Big Five personality model (OCEAN)**
  - Why needed here: Provides the theoretical grounding for half the extracted traits. Without understanding that Openness reflects intellectual curiosity while Conscientiousness reflects reliability, trait definitions become arbitrary.
  - Quick check question: If a physician is described as "rigid when patients propose alternative treatments," which Big Five trait dimension is being assessed and in which direction?

- Concept: **LLM-as-a-judge evaluation paradigm**
  - Why needed here: The validation methodology relies on a high-capability model (Gemini-2.5 Pro) evaluating other models' outputs across five quality dimensions. Understanding this paradigm is essential for interpreting validation results and implementing quality assurance.
  - Quick check question: What are two failure modes specific to LLM-as-a-judge evaluation that would not appear in human evaluation?

- Concept: **Inter-rater reliability vs. external validity**
  - Why needed here: The paper reports both human-LLM agreement (0.72-0.89) and trait-satisfaction correlations (0.41-0.81). These test different things—convergence between raters vs. relationship to outcomes.
  - Quick check question: If human-LLM agreement was 0.95 but trait-satisfaction correlations were 0.10, what would this indicate about the pipeline?

## Architecture Onboarding

- Component map: Data Layer (aggregated reviews) -> Extraction Agents (BigFive + SubFive) -> Evaluation Layer (LLM-as-a-Judge + Human Validation) -> Analysis Layer (Statistics + Clustering) -> Storage (SQLite caching)

- Critical path: Review aggregation → Top-down processing via structured prompting → XML trait extraction → Quality metrics (consistency/sufficiency) → LLM-as-a-judge evaluation → Human validation on subset → Large-scale deployment (Gemini-2.5 Flash selected for cost/performance balance)

- Design tradeoffs:
  - Top-down vs. bottom-up: Top-down provides holistic assessment but requires larger context windows; bottom-up would scale better but loses cross-review synthesis
  - Gemini-2.5 Pro (judge) vs. Gemini-2.5 Flash (extraction): Pro has lower MAE (0.0685 vs. 0.1057) but Flash is more cost-effective for 226,999 physicians
  - Strict XML schema vs. free-form output: Schema enforces consistency but may constrain nuanced reasoning in edge cases

- Failure signatures:
  - U-shaped quality patterns: Extreme trait scores (0 or 1) show higher consistency/sufficiency than moderate scores (0.3-0.7)—expected, not a bug
  - Missing trait data: Some physicians lack sufficient evidence for certain traits; handled via "No Evidence" scores, not imputation
  - Specialty-specific biases: Emergency Medicine and Psychiatry show systematically lower scores—likely structural factors, not extraction errors

- First 3 experiments:
  1. Ablation study on context window: Process physicians with 5 vs. 20 vs. 50 vs. 100 reviews to identify minimum review count for stable trait inference
  2. Cross-platform consistency check: Extract traits separately from Healthgrades-only vs. Yelp-only reviews for same physicians; correlation should exceed 0.6
  3. Temporal stability test: For physicians with reviews spanning >3 years, compare trait scores from first-half vs. second-half; high stability (>0.7 correlation) supports trait persistence

## Open Questions the Paper Calls Out

### Open Question 1
Do patient-perceived physician traits remain stable or evolve significantly over the course of a physician's career? The authors state that "Longitudinal studies tracking trait stability and evolution over physician careers could provide insights into professional development" and note the current "cross-sectional design prevents examination of trait stability over time." This remains unresolved because the study relies on a static dataset providing only a snapshot rather than a temporal view of trait development.

### Open Question 2
Can providing physicians with feedback on their LLM-extracted personality and competency traits causally improve patient satisfaction or clinical outcomes? The paper suggests that "Intervention studies examining whether targeted feedback on specific trait dimensions can improve patient satisfaction and clinical outcomes could test the causal implications of our findings." This is unresolved because the current study is observational and identifies correlations but does not test whether modifying these traits results in better performance.

### Open Question 3
To what extent do the observed gender disparities in ratings reflect actual performance differences versus patient evaluation bias? The authors find male physicians rated higher across all traits but explicitly state that "future research... is needed to disentangle these complex dynamics" regarding whether this reflects "patient perception biases rather than actual performance differences." This remains unresolved because the study lacked access to objective performance data to serve as ground truth.

### Open Question 4
Do LLM-extracted traits from patient narratives correlate with objective clinical safety outcomes and quality metrics? The authors propose that the "methodology could be extended to examine trait-outcome relationships beyond patient satisfaction, including clinical quality metrics, safety outcomes, and physician well-being measures." This is unresolved because validation focused on correlating extracted traits with "patient satisfaction" rather than hard clinical endpoints.

## Limitations
- Framework assumes patient reviews accurately reflect physician traits, but systematic review biases could distort trait distributions
- Exceptionally high correlation between IQC and SPS (r = 0.928) suggests potential construct overlap reducing discriminative power
- Top-down approach requires 5-100 reviews per physician, creating gaps for physicians with sparse online presence

## Confidence
- **High Confidence**: Extraction methodology and validation framework are sound, with strong human-LLM agreement (0.72-0.89) and clear external validity correlations (0.41-0.81 with patient satisfaction)
- **Medium Confidence**: Dual-trait framework's ability to capture complementary dimensions is supported by correlation patterns, but construct validity requires further validation across different contexts
- **Low Confidence**: Stability of extracted traits over time and predictive validity for clinical outcomes remain untested; performance with non-English reviews is unknown

## Next Checks
1. Cross-Cultural Validation Study: Apply framework to physician review data from non-U.S. healthcare systems to test whether Big Five + subjective judgment structure maintains predictive validity across different physician-patient relationship norms

2. Temporal Stability Analysis: For physicians with longitudinal review data spanning 3+ years, track trait score evolution and test correlation with documented practice changes to validate trait persistence assumptions

3. Clinical Outcome Correlation Test: Link extracted physician trait profiles to objective clinical quality metrics (readmission rates, patient safety indicators, guideline adherence scores) to assess whether patient-perceived traits predict actual healthcare quality beyond satisfaction measures