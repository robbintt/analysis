---
ver: rpa2
title: Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document
  Summarization
arxiv_id: '2509.09852'
source_url: https://arxiv.org/abs/2509.09852
tags:
- topic
- summarization
- source
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a topic-guided reinforcement learning approach
  to enhance multi-document summarization (MDS) by improving content selection and
  topical relevance. The authors show that prompting LLMs with topic labels improves
  summary informativeness and propose a novel topic reward within the GRPO framework
  to measure topic alignment between generated summaries and source documents.
---

# Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization

## Quick Facts
- arXiv ID: 2509.09852
- Source URL: https://arxiv.org/abs/2509.09852
- Reference count: 35
- Key outcome: Topic-guided RL improves multi-document summarization by enhancing content selection and topical relevance

## Executive Summary
This paper introduces a topic-guided reinforcement learning approach to enhance multi-document summarization (MDS) by improving content selection and topical relevance. The authors show that prompting LLMs with topic labels improves summary informativeness and propose a novel topic reward within the GRPO framework to measure topic alignment between generated summaries and source documents. Experimental results on Multi-News and Multi-XScience datasets demonstrate consistent improvements over strong baselines. Using Qwen2.5-0.5B as the policy model with Qwen2.5-7B for topic extraction, the approach achieves higher ROUGE scores, better embedding-based similarity, and superior topic alignment metrics compared to both reference-free and reference-based methods. Human evaluation and LLM-as-a-judge assessments confirm that the RL-trained models generate more topically aligned summaries. The results highlight the value of explicit topical guidance in MDS and demonstrate that effective topic alignment can enable smaller models to match or exceed larger ones.

## Method Summary
The approach combines topic-guided prompting with GRPO reinforcement learning. First, topic phrases are extracted from source documents using a 7B LLM. During summarization, the 0.5B policy model generates candidate summaries with topic guidance. The topic-F1 reward computes coverage and precision of summary topics relative to source topics using embedding similarity. Length penalty prevents excessive repetition. GRPO optimizes the policy using relative advantages within groups of sampled outputs, eliminating the need for a separate value function. The framework trains end-to-end to maximize both ROUGE and topic alignment.

## Key Results
- Topic-guided prompting improves informativeness, especially for smaller models (0.5B, 1.5B) which show 2-3 point ROUGE gains
- Topic-F1 reward achieves 2-7 point improvements in topic coverage and 4-8 points in precision over baselines
- RL TOPIC-7B+ROUGE achieves best results with 1.9 point ROUGE gain and superior topic alignment
- Smaller models (0.5B) with RL training match or exceed larger models (1.5B) in both ROUGE and topic metrics
- Human evaluation confirms RL models generate more topically aligned summaries with lower repetition rates

## Why This Works (Mechanism)

### Mechanism 1: Topic-Guided Prompting for Content Selection
Explicitly prompting LLMs with topic labels improves summary informativeness, particularly for smaller models. Topic phrases provide high-level discourse structure that guides models to identify salient content across multiple source documents. A larger "teacher" model (7B) extracts topic phrases that smaller "student" models (0.5B, 1.5B) use as explicit guidance during summarization. Core assumption: smaller models lack intrinsic topic modeling capabilities and benefit more from explicit topic signals than larger models. Evidence anchors: Figure 1 shows Qwen2.5-0.5B and 1.5B improve notably with T5 and T10 topic labels, while the 7B model shows no gains from self-generated topics. Break condition: Single topic label (T1) overly constrains summarization; larger models (7B) with strong intrinsic topic capabilities show no improvement from additional topic guidance.

### Mechanism 2: Topic-F1 Reward for Reference-Free Alignment
A harmonic mean of topic coverage and precision provides an effective reference-free reward signal for training summarization models. The reward computes pairwise similarity between source document topics and generated summary topics via embedding cosine similarity. Coverage measures how well summary topics cover source topics; precision measures relevance of summary topics to source. The harmonic mean balances both objectives. Core assumption: increasing topical similarity between generated summary and source documents improves overall summary quality. Evidence anchors: Topic-guided models achieve 2-7 point improvements in COV_RATIO and 4-8 points in PRE_RATIO compared to baselines. Break condition: Using only coverage or only precision as reward leads to imbalanced optimization, particularly on challenging datasets like Multi-XScience.

### Mechanism 3: GRPO with Relative Advantage Estimation
Group Relative Policy Optimization enables stable RL training by computing relative advantages within groups of sampled outputs rather than using a separate value function. For each input, the policy generates G candidate summaries. Rewards are computed for each, and advantages are calculated as normalized deviations from the group mean. This eliminates the need for a separate value model while providing stable training signals. Core assumption: relative ranking within sampled outputs provides sufficient gradient signal for policy improvement. Evidence anchors: RL-trained models consistently outperform baselines, with RL TOPIC-7B+ROUGE achieving best results. Break condition: High learning rates (≥1e-5) cause training instability and gradient explosions; insufficient samples per input (G<8) reduce performance gains.

## Foundational Learning

- **Concept: Multi-Document Summarization (MDS) Challenges**
  - Why needed here: The paper addresses specific MDS difficulties—integrating information from multiple sources while maintaining coherence and topical relevance. Single-document approaches don't transfer directly.
  - Quick check question: Can you explain why MDS is harder than single-document summarization, particularly regarding content selection across documents with potentially conflicting or redundant information?

- **Concept: Reference-Free vs. Reference-Based Evaluation**
  - Why needed here: The topic-F1 reward is reference-free—it doesn't require gold summaries. Understanding this distinction is critical for implementing the reward function correctly.
  - Quick check question: What are the tradeoffs between ROUGE (reference-based) and the proposed topic-F1 (reference-free) as reward signals? When would each be preferred?

- **Concept: Policy Gradient Methods for Sequence Generation**
  - Why needed here: GRPO builds on policy gradient foundations. Understanding advantage estimation, KL penalties, and clipping helps debug training dynamics.
  - Quick check question: How does GRPO's group-based advantage estimation differ from traditional actor-critic approaches that use a learned value function?

## Architecture Onboarding

- **Component map:** Policy Model (Qwen2.5-0.5B-Instruct) -> Reference Model (Qwen2.5-0.5B-Instruct) -> Topic Extractor (Qwen2.5-7B-Instruct) -> Embedding Model (all-mpnet-base-v2) -> Reward Calculator

- **Critical path:**
  1. Pre-extract source document topics before training (avoid redundant computation)
  2. During rollout: policy generates G=8 candidate summaries per input
  3. Extract topics from each generated summary (on-the-fly)
  4. Compute topic-F1 reward via embedding similarity matrices
  5. Compute length penalty based on target vs. actual token count
  6. Combine rewards with inverse std weighting (emphasis factor 2 for topic reward)
  7. Calculate group advantages and update policy via clipped surrogate objective

- **Design tradeoffs:**
  - Topic count: n=10 for Multi-News, n=5 for Multi-XScience (matched to gold summary sentence counts); m=5 for generated summaries
  - Teacher model size: 7B provides higher-quality topics than 0.5B but adds inference overhead
  - Reward combination: Topic-F1 alone is reference-free; combining with ROUGE yields best performance but requires references
  - Learning rate: 1e-6 required for stability; higher rates cause gradient explosions

- **Failure signatures:**
  - Reward hacking: Models may overuse topic-related words without coherent generation (mitigated by length penalty)
  - Excessive length: SFT models produce >3% repetitive outputs; RL models <0.2%
  - Unstable training: Learning rate ≥1e-5 causes loss spikes and gradient explosions
  - Imbalanced optimization: Using only coverage or precision reward causes tradeoffs

- **First 3 experiments:**
  1. Baseline comparison: Run zero-shot BASE (0.5B) vs. BASE TOPIC-7B (with teacher-extracted topics) on Multi-News validation set to confirm topic-prompting benefit
  2. Reward ablation: Train three RL variants (topic-coverage-only, topic-precision-only, topic-F1) and compare topic alignment metrics to verify harmonic mean advantage
  3. Best-of-n scaling: Generate n=8 candidates from trained RL model, select by topic-F1 score, and compare against single-pass generation to quantify inference-time scaling benefit

## Open Questions the Paper Calls Out

- **Open Question 1:** Can advanced neural topic modeling techniques (e.g., contextualized embeddings from BERT) provide more refined topical guidance than the current LLM-based extraction approach?
  - Basis in paper: The conclusion states: "Looking forward, we aim to enrich our framework by exploring advanced neural topic modeling techniques (Bianchi et al., 2021; Fang et al., 2024) for more refined topical guidance."
  - Why unresolved: The current approach uses Qwen2.5-7B for direct topic extraction, but neural topic models may capture different discourse structures or provide more coherent topic clusters than open-ended LLM extraction.
  - What evidence would resolve it: Comparative experiments replacing the LLM-based topic extractor with neural topic models like CWTM, measuring ROUGE, embedding similarity, and topic alignment scores.

- **Open Question 2:** Can targeted "guard rewards" effectively prevent reward hacking behaviors (e.g., repetitive topic keywords) without compromising topic alignment quality?
  - Basis in paper: The ethical statement notes: "Exploring more targeted 'guard rewards' to prevent such reward hacking represents an interesting direction for future work."
  - Why unresolved: While the length-penalty reward partially mitigates repetition, sophisticated reward hacking (keyword stuffing without overt repetition) could emerge, and specialized countermeasures remain unexplored.
  - What evidence would resolve it: Design guard rewards (repetition penalties, diversity bonuses), analyze generated summaries for keyword gaming, and measure trade-offs with topic alignment metrics.

- **Open Question 3:** Does the topic-guided framework transfer effectively to interactive, query-based summarization where users specify key points to prioritize?
  - Basis in paper: The conclusion states: "Extending our topic-guided approach to interactive, query-based scenarios–where users specify key points to summarize–also presents an exciting future direction."
  - Why unresolved: Current work assumes generic summarization. Query-based scenarios introduce competing topic sources (document-extracted vs. user-specified), and balancing these signals in the reward function remains unaddressed.
  - What evidence would resolve it: Adapt the topic reward to incorporate query-specified topics, evaluate on query-focused MDS datasets like QMSum, and measure user preference satisfaction.

## Limitations

- The topic-F1 reward formulation lacks direct empirical validation against established reference-based metrics beyond ROUGE, making it unclear whether the reference-free alignment truly captures summary quality.
- The assumption that harmonic mean of coverage and precision provides optimal reward signal is supported by ablation but not compared against alternative formulations or similarity measures.
- The teacher model (7B) extraction quality is assumed superior without ablation studies testing whether smaller models could achieve similar results with more topic labels or different extraction strategies.

## Confidence

- **High confidence**: Topic-guided prompting improves informativeness for smaller models (supported by Figure 1 showing clear 0.5B/1.5B improvements vs. no 7B gains)
- **Medium confidence**: Topic-F1 reward effectively optimizes for topical alignment (supported by improved topic metrics but lacks comparison to alternative reward designs)
- **Medium confidence**: GRPO training stability at 1e-6 learning rate (empirically observed but not theoretically justified why this specific rate works)

## Next Checks

1. **Reward Function Robustness**: Implement and compare three alternative topic rewards—coverage-only, precision-only, and F1 with different harmonic mean weights (e.g., F2 score favoring precision). Evaluate whether the chosen F1 formulation is optimal or if other configurations yield better topical alignment without sacrificing ROUGE scores.

2. **Teacher Model Scaling Analysis**: Train models using topic labels extracted by different-sized teacher models (0.5B, 2B, 7B) to quantify the marginal benefit of larger models. This validates whether the 7B teacher is necessary or if smaller models with more topic labels could achieve similar results at lower computational cost.

3. **Cross-Dataset Generalization**: Apply the trained RL TOPIC-7B+ROUGE model to a different multi-document summarization dataset (e.g., WikiSum or DUC) without fine-tuning to test whether the topic alignment capabilities transfer across domains, or if the model overfits to the specific topical patterns in Multi-News and Multi-XScience.