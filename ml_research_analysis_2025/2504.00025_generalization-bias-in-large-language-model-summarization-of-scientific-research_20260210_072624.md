---
ver: rpa2
title: Generalization Bias in Large Language Model Summarization of Scientific Research
arxiv_id: '2504.00025'
source_url: https://arxiv.org/abs/2504.00025
tags:
- summaries
- llms
- prompt
- scientific
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examined whether large language models (LLMs) accurately
  summarize scientific research or tend to overgeneralize findings. It tested 10 prominent
  LLMs on 4,900 summaries of scientific abstracts and full-length articles, comparing
  them to original texts and human-authored summaries.
---

# Generalization Bias in Large Language Model Summarization of Scientific Research

## Quick Facts
- arXiv ID: 2504.00025
- Source URL: https://arxiv.org/abs/2504.00025
- Reference count: 0
- Primary result: Most LLMs produce overgeneralized summaries of scientific research, with overgeneralization rates ranging from 26% to 73%

## Executive Summary
This study reveals that large language models (LLMs) systematically overgeneralize scientific findings when summarizing research, often transforming specific, quantified claims into generic statements. Testing 10 prominent LLMs on 4,900 summaries of scientific abstracts and full-length articles, the researchers found that newer models like ChatGPT-4o and DeepSeek were particularly prone to overgeneralization, with rates up to 73%. The tendency was more pronounced than in human-authored summaries and occurred across multiple prompt types, though lower temperature settings significantly reduced the effect. These findings highlight significant risks for AI-driven science communication, particularly in medical contexts where precision is critical.

## Method Summary
The study tested 10 LLMs on 4,900 summaries of scientific texts from 8 high-impact journals, comparing LLM outputs to original texts and human-authored summaries. Researchers used three prompt types (simple, systematic, accuracy) at two temperature settings (0 and 0.7) and collected UI-default settings where available. They classified conclusions as generalized (generic, present-tense, action-guiding) versus restricted (quantified, past-tense, descriptive), then analyzed differences using GLMM logistic regression. Inter-rater reliability was established through Cohen's kappa (0.79-0.95) across two primary coders with third-party validation on 100 texts.

## Key Results
- LLMs produced overgeneralized summaries 26-73% of the time, significantly more than original texts or human-authored summaries
- Newer models (ChatGPT-4o, DeepSeek) showed the highest overgeneralization rates
- Lower temperature settings (0 vs 0.7) reduced overgeneralization by 76%
- Accuracy prompts paradoxically increased overgeneralization rates compared to simple prompts
- Overgeneralization manifested as transforming quantified claims into generic statements or present-tense conclusions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower temperature settings reduce overgeneralization by constraining token selection variance.
- Mechanism: Temperature controls the softmax sharpness during sampling. At temperature 0, the model selects the highest-probability token deterministically, limiting creative extrapolation. Higher temperatures introduce randomness, enabling broader linguistic transformations that may extend claims beyond source scope.
- Core assumption: The deterministic path preferentially preserves source constraints; stochastic exploration enables unwarranted generalizations.
- Evidence anchors:
  - Table 1: "At LLM temperature 0, summaries containing generalized conclusions were 76% less likely to occur compared to those generated at temperature 0.7" (B = -1.432, p < .001).
  - "Lower temperature settings reduced overgeneralization."
- Break condition: If temperature interacts differently with prompt type (interaction term insignificant in model), the effect may not generalize across prompting strategies.

### Mechanism 2
- Claim: RLHF optimization for helpfulness prioritizes confident, broadly applicable outputs over precise hedging.
- Mechanism: Human evaluators during RLHF may favor responses that sound authoritative and widely relevant. Models learn to produce confident fluency rather than cautious precision, resulting in generic, present-tense, action-guiding statements that appear maximally helpful but extend beyond evidential support.
- Core assumption: RLHF reward signals systematically penalize hedging and reward apparent helpfulness.
- Evidence anchors:
  - Discussion, para 4: "studies have found that while reinforcement learning from human feedback (RLHF) enhanced models' helpfulness, it often led them to express unwarranted confidence or reduced their ability to hedge claims to indicate uncertainty."
  - Discussion, para 4: "models may learn to prioritize confident fluency over caution and precision, increasing their tendency to produce overgeneralized statements."
- Break condition: If Claude models also underwent RLHF but did not overgeneralize (as observed), RLHF alone cannot explain the effectâ€”model architecture or training data differences may moderate.

### Mechanism 3
- Claim: Explicit accuracy prompts trigger an "ironic rebound" effect, increasing the very errors they aim to prevent.
- Mechanism: Negation-focused instructions ("do not introduce inaccuracies") may prime the model toward the forbidden concept, analogous to ironic process theory in human cognition. The model's attention to "inaccuracy" increases its accessibility in output generation.
- Core assumption: LLMs exhibit attentional priming effects analogous to human ironic rebound; mechanism is not empirically isolated in this study.
- Evidence anchors:
  - Results (2): "LLM summaries retrieved with the accuracy prompt were about twice as likely to contain generalized conclusions compared to the simple prompt (OR = 1.90, 95% CI [1.11, 3.26], p = 0.02)."
  - Discussion, para 6: "One potential explanation... may be related to the content of the accuracy prompt... By calling for attempts to free processing of inaccuracy, it may have made the occurrence of inaccuracy more likely."
- Break condition: If the effect is prompt-specific (only one accuracy prompt tested), alternative phrasings may not trigger backfire; further experimentation required.

## Foundational Learning

- Concept: Generic vs. quantified generalizations
  - Why needed here: The paper's core metric distinguishes generic statements ("X causes Y") from quantified claims ("X caused Y in 73% of sample"). Understanding this distinction is prerequisite to detecting overgeneralization.
  - Quick check question: Given the statement "Dulaglutide is effective for type 2 diabetes," can you identify whether this is generic or quantified, and what information is omitted?

- Concept: Temperature parameter in autoregressive models
  - Why needed here: Temperature directly modulates overgeneralization (76% reduction at temp 0 vs 0.7). Engineers must understand this to configure inference appropriately.
  - Quick check question: If you set temperature to 0.7 and observe overgeneralized outputs, what parameter change would the paper recommend, and what tradeoff might this introduce?

- Concept: Logistic regression with odds ratios
  - Why needed here: All effect sizes are reported as odds ratios (e.g., OR = 4.85 for LLM vs. human). Interpreting "nearly five times more likely" requires understanding OR interpretation.
  - Quick check question: An OR of 1.90 for the accuracy prompt means what percentage increase in the odds of overgeneralization?

## Architecture Onboarding

- Component map: Input scientific text -> Apply prompt (simple/systematic/accuracy) -> Model inference (10 LLMs, temperature 0/0.7/UI-default) -> Output classification (generic/present-tense/action-guiding) -> Evaluation (compare to original text)
- Critical path: Retrieve original scientific text -> Apply prompt via API or UI -> Generate summary at specified temperature -> Classify output for each generalization type -> Compare classification to original text -> Compute overgeneralization score
- Design tradeoffs:
  - API access enables temperature control but excludes UI-only models at default settings; UI captures real user experience but loses experimental control.
  - Claude models show minimal overgeneralization but may sacrifice accessibility; GPT/LLaMA models produce more fluent summaries with higher overgeneralization risk.
  - Lower temperature reduces overgeneralization but may produce less natural outputs; optimal setting depends on risk tolerance.
- Failure signatures:
  - Overgeneralization detected when summary contains generic subject ("X is effective") where original had quantified ("X was effective in 73% of sample").
  - Backfire signature: Accuracy-prompted outputs show higher overgeneralization rates than simple-prompt outputs.
  - Model drift: Newer models (ChatGPT-4o, LLaMA 3.3, DeepSeek) show higher overgeneralization than older versions, suggesting regression in generalization accuracy.
- First 3 experiments:
  1. Baseline replication: Test 50 medical abstracts with simple prompt at temperature 0.7 across 3 models (GPT-4o, Claude 3.7, DeepSeek); code for generic/present-tense/action-guiding generalizations; compare OAO scores to paper benchmarks.
  2. Temperature ablation: Retest same abstracts at temperature 0; quantify reduction in overgeneralization; verify whether 76% reduction replicates.
  3. Prompt variation test: Design 3 alternative accuracy prompts avoiding negation ("Ensure all claims match source scope exactly"); compare to original accuracy prompt to isolate ironic rebound vs. prompt-specific effect.

## Open Questions the Paper Calls Out

- Does the tendency of LLMs to overgeneralize vary significantly across different scientific text domains? The authors state in the limitations section that "Future research should examine how LLM summarization differs across scientific text domains." The study primarily focused on medical and multidisciplinary science journals, leaving other domains (e.g., social sciences, humanities) untested.

- Does instructing LLMs to adopt specific personas (e.g., university media office vs. domain expert) affect the rate of algorithmic overgeneralization? The authors note that "LLMs prompted to adopt the role of a domain expert, university media office, or marketing writer may exhibit varying rates of overgeneralization" and explicitly welcome future research on this.

- Is the "backfire effect" of accuracy prompts caused by an algorithmic analog to the human psychological "ironic rebound" effect? The authors hypothesize that accuracy prompts may trigger an "algorithmic version of the 'ironic rebound' effect," but concede this "remains to be tested."

## Limitations
- The study excluded UI-only models at temperature 0, limiting understanding of real-world usage patterns
- The ironic rebound hypothesis for accuracy prompts remains speculative without empirical validation
- Temperature interaction with prompt type was not significant, suggesting the relationship may be more complex than presented
- The exclusion of non-research articles from the corpus may limit generalizability to broader scientific communication

## Confidence
- High confidence: Overgeneralization occurs more frequently in LLM summaries than original texts or human-authored summaries (OR 4.85, p < .001). This finding is statistically robust across multiple models and prompts.
- Medium confidence: Temperature setting affects overgeneralization rates (76% reduction at temp 0 vs 0.7). The effect is well-demonstrated but the underlying mechanism requires further investigation.
- Low confidence: Accuracy prompts increase overgeneralization through ironic rebound. This mechanism is plausible but remains untested beyond one prompt variant.

## Next Checks
1. Replicate ironic rebound effect with 3-5 alternative accuracy prompts avoiding negation (e.g., "Ensure all claims exactly match source scope") to determine if the backfire effect is prompt-specific or a general phenomenon.
2. Test whether RLHF training on scientific text specifically (not just general helpfulness) reduces overgeneralization, isolating RLHF's role from other factors like model architecture.
3. Conduct human evaluation study comparing LLM summaries at temperature 0 (minimal overgeneralization) versus temperature 0.7 (higher overgeneralization) on scientific literacy and decision-making accuracy to quantify real-world impact.