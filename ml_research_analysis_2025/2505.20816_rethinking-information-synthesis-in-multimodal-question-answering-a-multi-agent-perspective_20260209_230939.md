---
ver: rpa2
title: Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent
  Perspective
arxiv_id: '2505.20816'
source_url: https://arxiv.org/abs/2505.20816
tags:
- question
- reasoning
- modality
- agent
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MAMMQA, a fully prompt-driven multi-agent
  framework for multimodal question answering across text, tables, and images. MAMMQA
  decomposes the reasoning process into three interpretable stages: modality-specific
  insight extraction, cross-modal synthesis, and evidence-grounded aggregation.'
---

# Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective

## Quick Facts
- arXiv ID: 2505.20816
- Source URL: https://arxiv.org/abs/2505.20816
- Reference count: 9
- Primary result: MAMMQA achieves 76.37% accuracy on MULTIMODAL QA with GPT-4o-mini

## Executive Summary
This paper introduces MAMMQA, a fully prompt-driven multi-agent framework for multimodal question answering across text, tables, and images. The framework decomposes reasoning into three interpretable stages: modality-specific insight extraction, cross-modal synthesis, and evidence-grounded aggregation. MAMMQA employs specialized agents that operate independently within their domain expertise, using a unified prompt template that allows dynamic activation based on available modalities.

Experiments on MULTIMODAL QA and MANYMODAL QA benchmarks demonstrate that MAMMQA consistently outperforms state-of-the-art baselines including Chain-of-Thought, Tree-of-Thoughts, and several fine-tuned models. The framework provides interpretable reasoning traces and avoids hallucinations by grounding responses in extracted evidence. These results establish MAMMQA as a scalable, zero-shot solution for reliable multimodal QA.

## Method Summary
MAMMQA introduces a three-stage reasoning framework that systematically addresses the challenges of multimodal question answering. The approach begins with modality-specific insight extraction where specialized agents independently process each available modality (text, tables, images) to identify relevant information and relationships. The cross-modal synthesis stage then combines these extracted insights to build a comprehensive understanding, while the evidence-grounded aggregation stage synthesizes the final answer with explicit traceability to source evidence.

The framework's key innovation lies in its prompt-driven architecture that enables dynamic agent activation based on available modalities, combined with a unified prompt template that ensures consistent reasoning across different input types. Each agent maintains domain expertise while contributing to a cohesive reasoning process, allowing the system to handle complex questions that require integrating information from multiple sources.

## Key Results
- MAMMQA achieves 76.37% accuracy on MULTIMODAL QA benchmark using GPT-4o-mini
- Outperforms Chain-of-Thought, Tree-of-Thoughts, and fine-tuned models on multimodal QA tasks
- Shows 67.56% accuracy with Qwen2.5-VL-7B, demonstrating strong performance across different model architectures
- Demonstrates robustness to perturbations and mislabeling while maintaining interpretable reasoning traces

## Why This Works (Mechanism)
The framework's success stems from its systematic decomposition of complex multimodal reasoning into manageable stages, each handled by specialized agents with clear domain expertise. By separating insight extraction from synthesis and aggregation, MAMMQA avoids the cognitive overload that typically plagues end-to-end approaches. The prompt-driven design enables zero-shot operation while maintaining consistency across different modalities and model choices.

## Foundational Learning
- **Multimodal QA fundamentals**: Understanding how to combine text, tables, and images for question answering - needed to build effective cross-modal reasoning pipelines
- **Multi-agent coordination**: Learning how independent agents can collaborate through prompt-driven communication - needed to maintain coherence while preserving specialization
- **Evidence grounding**: Techniques for tracing answers back to source information - needed to avoid hallucinations and ensure reliability
- **Dynamic prompt activation**: Methods for adapting reasoning pipelines based on available modalities - needed for flexible deployment across different input scenarios
- **Zero-shot reasoning**: Approaches that don't require fine-tuning - needed for broad applicability without extensive training data
- **Interpretability in AI systems**: Mechanisms for providing transparent reasoning traces - needed for trust and debugging in complex systems

## Architecture Onboarding

**Component Map**: Modality Extraction Agents -> Cross-Modal Synthesis Agent -> Evidence Aggregation Agent -> Final Answer

**Critical Path**: Question Input -> Modality Detection -> Agent Activation -> Insight Extraction -> Synthesis -> Aggregation -> Answer Output

**Design Tradeoffs**: The framework trades computational efficiency for interpretability and reliability, choosing multiple specialized agents over a single monolithic model. This increases latency but provides clear reasoning traces and reduces hallucination risk.

**Failure Signatures**: Performance degradation occurs when modalities provide conflicting information, when questions require temporal reasoning across modalities, or when evidence is ambiguous or contradictory. The framework may struggle with questions requiring deep domain knowledge not present in the input.

**First 3 Experiments**:
1. Test MAMMQA on a simple multimodal question combining a short text paragraph and image to verify basic functionality
2. Evaluate performance on a question requiring integration of tabular data with image captions to assess cross-modal synthesis
3. Measure response time and resource usage compared to single-agent baselines to quantify computational overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on synthetic benchmarks rather than real-world complexity
- Performance claims may be sensitive to model choice, with substantial differences between GPT-4o-mini and Qwen2.5-VL-7B results
- Zero-shot nature limits handling of domain-specific knowledge that would benefit from fine-tuning

## Confidence

**Major Uncertainty - Performance Claims**: Medium confidence. The reported improvements over baselines are substantial (e.g., 76.37% vs competitive scores), but the experiments rely heavily on GPT-4o-mini. The Qwen2.5-VL-7B results (67.56%) are less compelling, suggesting potential sensitivity to model choice that warrants deeper investigation.

**Major Uncertainty - Interpretability Claims**: High confidence. The paper explicitly describes interpretable reasoning traces and evidence-grounded responses. The three-stage decomposition provides clear visibility into the reasoning process, and the framework's design inherently avoids hallucinations through evidence grounding.

**Major Uncertainty - Generalizability**: Low confidence. While the framework claims to handle "text, tables, and images," the evaluation focuses primarily on benchmark datasets. Real-world applications often involve more diverse modalities and complex information relationships that weren't tested.

## Next Checks

1. **Cross-domain robustness test**: Evaluate MAMMQA on real-world multimodal datasets outside the QA benchmark domain, including scientific literature, business reports, and educational materials to assess generalization.

2. **Ablation study on agent specialization**: Systematically remove individual agents or merge their functions to quantify the contribution of the multi-agent architecture versus unified approaches.

3. **Long-context evaluation**: Test MAMMQA's performance on documents exceeding typical context window limits to validate claims about handling complex, information-rich scenarios.