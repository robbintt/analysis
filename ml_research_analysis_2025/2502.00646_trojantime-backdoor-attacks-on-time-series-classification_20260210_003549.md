---
ver: rpa2
title: 'TrojanTime: Backdoor Attacks on Time Series Classification'
arxiv_id: '2502.00646'
source_url: https://arxiv.org/abs/2502.00646
tags:
- backdoor
- attack
- data
- training
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes TrojanTime, a novel backdoor attack framework
  for Time Series Classification (TSC) that addresses the challenge of attacking models
  when training data is inaccessible. The core idea is a two-step training process:
  first, generating a pseudo-dataset from an external dataset using adversarial attacks
  to create diverse samples across multiple classes; second, training the clean model
  on this pseudo-dataset and its poisoned version using logits alignment and batch
  normalization freezing to maintain generalization on clean samples while embedding
  backdoor patterns.'
---

# TrojanTime: Backdoor Attacks on Time Series Classification

## Quick Facts
- arXiv ID: 2502.00646
- Source URL: https://arxiv.org/abs/2502.00646
- Reference count: 31
- Primary result: Novel backdoor attack framework for TSC that works without training data access, maintaining high clean accuracy while achieving effective backdoor injection.

## Executive Summary
This paper proposes TrojanTime, a backdoor attack framework for Time Series Classification (TSC) that addresses the challenge of attacking models when training data is inaccessible. The core idea is a two-step training process: first, generating a pseudo-dataset from an external dataset using adversarial attacks to create diverse samples across multiple classes; second, training the clean model on this pseudo-dataset and its poisoned version using logits alignment and batch normalization freezing to maintain generalization on clean samples while embedding backdoor patterns. The method is evaluated across five trigger types, four TSC architectures, and UCR benchmark datasets, demonstrating effectiveness in executing backdoor attacks while preserving clean accuracy.

## Method Summary
TrojanTime operates through a two-step process: First, generate a pseudo-dataset by applying unconstrained PGD-50 adversarial attacks to an external dataset, creating samples that approximate the target classes. Second, train the model on both the pseudo-dataset and its triggered counterpart, using logits alignment between current and pre-trained model outputs and freezing BatchNorm layers to maintain clean accuracy while embedding backdoor patterns. The attack achieves high Attack Success Rate (ASR) across multiple trigger types and TSC architectures while preserving Clean Accuracy (CA) on benign samples.

## Key Results
- Achieves average ASR of 86.3% with fixed patch triggers while maintaining CA within 5% of benign models
- Logits alignment and BatchNorm freezing are critical: removing either causes CA to drop below 70% or ASR to fall to random chance levels
- Defense strategy reduces ASR from 83.5% to 14.4% on Coffee dataset while maintaining clean accuracy
- Powerline triggers underperform patch triggers due to frequency sensitivity in TSC models

## Why This Works (Mechanism)

### Mechanism 1
- Adversarial sample synthesis creates class-diverse pseudo-data that approximates the unknown training distribution, enabling effective backdoor injection without access to original training data.
- PGD adversarial attacks perturb external dataset samples toward each target class in the model's classification head, creating representations that cluster near original training data in latent space.
- Core assumption: The pre-trained model's learned representations are sufficiently expressive that adversarial perturbations can approximate the original training distribution's class boundaries.
- Break condition: If the external dataset's domain differs radically from training data (e.g., audio vs. sensor readings), adversarial perturbations may fail to produce semantically meaningful pseudo-samples.

### Mechanism 2
- Logits alignment preserves clean accuracy by constraining the Trojan model's outputs to match the benign model's predictions on pseudo-data.
- MSE loss between current model logits and frozen pre-trained model logits acts as regularization, preventing concept drift while allowing backdoor loss optimization on triggered samples.
- Core assumption: The benign model's logits on pseudo-data encode useful distributional information that generalizes to the true test distribution.
- Break condition: If the alignment parameter λ is set too high (over-regularization), backdoor learning is suppressed; if too low, concept drift degrades clean accuracy beyond acceptable thresholds.

### Mechanism 3
- Freezing BatchNorm layers prevents distribution shift from pseudo-data while forcing convolutional filters to learn trigger-specific features.
- BN statistics from the benign model are preserved, blocking adaptation to pseudo-data distribution and forcing learnable weights to encode trigger patterns rather than adjusting to new data statistics.
- Core assumption: Backdoor triggers activate distinct neural pathways that can be learned without modifying batch normalization statistics.
- Break condition: Architectures without BN layers require alternative stabilization strategies.

## Foundational Learning

- **Backdoor Attacks vs. Adversarial Attacks**: The paper combines both—adversarial attacks for data synthesis, backdoor attacks for trigger embedding. Confusing these leads to misunderstanding the two-phase pipeline.
  - Quick check question: Does the attack modify inputs at inference time (adversarial) or embed a dormant trigger activated by specific patterns (backdoor)?

- **Concept Drift in Continual Learning**: The core challenge is embedding backdoors without "forgetting" original task performance. Logits

## Architecture Onboarding

- **Layer Freezing Strategy**: BatchNorm layers are frozen during TrojanTime training, which is critical for maintaining clean accuracy. Models without BatchNorm (e.g., certain recurrent architectures) would require alternative stabilization methods to prevent distribution shift from pseudo-data.
- **Model Compatibility**: The framework is evaluated on four TSC architectures, suggesting moderate architecture agnosticism. However, effectiveness may vary based on how easily triggers can be embedded into the model's feature space.

## Open Questions the Paper Calls Out

- **Transferability Across Datasets**: The paper does not extensively explore whether TrojanTime-trained models maintain effectiveness when applied to different target datasets than those used for pseudo-data generation.
- **Trigger Robustness**: The sensitivity of backdoor effectiveness to trigger modifications (size, position, noise) is not thoroughly investigated, particularly for the powerline trigger which showed lower performance.

## Limitations

- **External Dataset Dependency**: The attack requires access to an external dataset, which may not always be available or representative of the target domain, potentially limiting real-world applicability.
- **Trigger Design Sensitivity**: The powerline trigger performs worse than patch triggers, suggesting that certain trigger types may be less effective due to architectural sensitivities in TSC models.
- **Defense Evasion**: While the paper demonstrates defense effectiveness, it does not explore advanced adaptive attacks that could circumvent the proposed defense strategy.

## Confidence

High confidence in the technical accuracy of the mechanism descriptions and results. The paper provides sufficient experimental detail and ablation studies to support its claims about the effectiveness of logits alignment and BatchNorm freezing.

## Next Checks

- Verify the mathematical formulation of the logits alignment loss and its impact on concept drift prevention
- Examine the specific PGD attack parameters used for pseudo-dataset generation
- Review the defense mechanism implementation details and its effectiveness across different trigger types