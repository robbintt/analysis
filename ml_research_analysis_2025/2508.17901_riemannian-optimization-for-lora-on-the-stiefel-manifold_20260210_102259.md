---
ver: rpa2
title: Riemannian Optimization for LoRA on the Stiefel Manifold
arxiv_id: '2508.17901'
source_url: https://arxiv.org/abs/2508.17901
tags:
- lora
- stiefel
- matrix
- optimization
- adamw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stiefel-LoRA, a novel parameter-efficient
  fine-tuning approach that enforces orthogonality constraints on the B matrix of
  LoRA by optimizing it on the Stiefel manifold. The method addresses the problem
  of basis redundancy and underutilization of rank in conventional LoRA fine-tuning
  with AdamW.
---

# Riemannian Optimization for LoRA on the Stiefel Manifold

## Quick Facts
- arXiv ID: 2508.17901
- Source URL: https://arxiv.org/abs/2508.17901
- Reference count: 40
- Primary result: Stiefel-LoRA achieves 9-12% average accuracy gains over AdamW on LLaMA-3.2-1B across seven commonsense reasoning datasets by enforcing orthogonality constraints on LoRA's B matrix.

## Executive Summary
This paper introduces Stiefel-LoRA, a novel parameter-efficient fine-tuning approach that optimizes the B matrix of LoRA on the Stiefel manifold to enforce orthogonality constraints. The method addresses basis redundancy and underutilization of rank in conventional LoRA fine-tuning with AdamW by maintaining orthonormal column vectors in B, ensuring full effective rank utilization and enhanced parameter efficiency. Experiments across seven commonsense reasoning datasets and multiple model scales show consistent performance improvements, with Stiefel-LoRA achieving average accuracy gains of 9-12% over AdamW on LLaMA-3.2-1B, and similar improvements on larger models.

## Method Summary
Stiefel-LoRA modifies standard LoRA fine-tuning by constraining the B matrix to the Stiefel manifold St(d,r) where B^T B = I_r. The optimization uses QR-based retraction with Euclidean momentum preconditioning: Adam momentum updates are computed in Euclidean space, projected onto the tangent space at B, then QR decomposition retracts the updated point back to the manifold. Matrix A is updated with standard Adam while B maintains orthogonality throughout training. This approach ensures full effective rank utilization and eliminates basis redundancy that occurs with AdamW optimization.

## Key Results
- Stiefel-LoRA achieves average accuracy gains of 9-12% over AdamW on LLaMA-3.2-1B across seven commonsense reasoning datasets
- Effective rank analysis confirms Stiefel optimization fully utilizes specified rank (e.g., 16/16 dimensions) while AdamW achieves only partial utilization (e.g., 12/16 dimensions)
- Cosine similarity between B columns remains near zero throughout training, confirming orthogonality preservation
- Performance improvements scale with model size and are particularly effective when both LoRA matrices are trained together

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonality constraints on the B matrix eliminate basis redundancy and maximize representational capacity.
- Mechanism: By constraining B to the Stiefel manifold (B^T B = I_r), column vectors remain orthonormal throughout training, preventing basis drift where columns become correlated and ensuring each dimension captures independent information.
- Core assumption: LoRA's representational bottleneck stems from correlated basis vectors in B, not from insufficient capacity in rank r.
- Evidence anchors: [abstract] shows orthogonality and full effective rank; [section 4.2] demonstrates zero cosine similarity for Stiefel vs. 0.003 with std 0.5143 for AdamW; StelLA (arXiv:2510.01938) confirms geometric structure exploitation improves performance.

### Mechanism 2
- Claim: Full effective rank utilization directly translates to better parameter efficiency and task performance.
- Mechanism: Orthonormal columns guarantee linear independence, allowing all r dimensions to contribute meaningfully to the weight update ∇W = BA. With AdamW, only ~75% of specified rank is utilized (12/16 dimensions), wasting parameter budget.
- Core assumption: Task-relevant features span a subspace requiring the full allocated rank r.
- Evidence anchors: [abstract] confirms full vs. partial rank utilization; [section 4.2] Figure 3 shows layer-wise effective rank at r=16 for Stiefel vs. ~12 for AdamW; corpus lacks direct validation of effective rank as causal mechanism.

### Mechanism 3
- Claim: QR-based retraction with Euclidean momentum preconditioning enables stable optimization on the Stiefel manifold.
- Mechanism: Adam momentum updates are computed in Euclidean space, then projected onto the tangent space at B (ξ = M'_B - B·sym(B^T M'_B)). QR decomposition retracts the updated point back to the manifold, preserving orthogonality.
- Core assumption: QR retraction sufficiently approximates exponential map for LoRA optimization scales.
- Evidence anchors: [section 3.1] confirms QR-based retraction robustly ensures orthonormality; [algorithm 1] provides complete update procedure; "Geometric design of the tangent term" (arXiv:2507.15638) discusses landing algorithms for orthogonality.

## Foundational Learning

- Concept: **Stiefel Manifold St(n,p)**
  - Why needed here: This is the constraint set for B matrix optimization. St(n,p) = {X ∈ R^{n×p} | X^T X = I_p} defines all matrices with orthonormal columns.
  - Quick check question: Given B ∈ R^{512×16} on St(512, 16), what is the shape of B^T B? What about BB^T?

- Concept: **Tangent Space Projection**
  - Why needed here: Gradients from backpropagation live in Euclidean space. To update B on the manifold, you must project gradients onto the tangent space T_B St(d,r) = {ξ | ξ^T B + B^T ξ = 0}.
  - Quick check question: Why can't you directly use ∇_B f to update B with gradient descent? What would happen to B^T B after one step?

- Concept: **Retraction and QR Decomposition**
  - Why needed here: Moving along the tangent direction B + αξ yields a matrix off the manifold. QR decomposition extracts the orthonormal Q factor as the new B, ensuring constraints hold.
  - Quick check question: After QR decomposition Y' = QR, which factor becomes the new B? Why must diagonal elements of R be constrained positive?

## Architecture Onboarding

- Component map:
  ```
  LoRA Module: W = W_0 + BA
  ├─ Matrix A: R^{r×k}, optimized with standard Adam in Euclidean space
  ├─ Matrix B: R^{d×r}, constrained to Stiefel manifold St(d,r)
  │   ├─ Gradient computation: Standard backprop → ∇_B f
  │   ├─ Momentum: Adam-style in Euclidean space → M'_B
  │   ├─ Tangent projection: ξ = M'_B - B·sym(B^T M'_B)
  │   └─ Retraction: Y' = B - α·ξ → QR(Y') → Q becomes new B
  └─ Forward pass: W_0 + B @ A (no changes)
  ```

- Critical path:
  1. Initialize B_0 with orthonormal columns (use QR on random matrix or identity subset)
  2. Forward/backward pass as normal LoRA
  3. **Key divergence**: After computing ∇_B f, project momentum to tangent space before update
  4. Apply QR retraction to maintain B ∈ St(d,r) invariant
  5. Matrix A updates with standard Adam—no modifications needed

- Design tradeoffs:
  - **QR vs. SVD retraction**: QR is O(dr^2), faster than SVD but less accurate for ill-conditioned matrices. Paper uses QR for efficiency.
  - **Learning rates**: Stiefel optimizer requires ~1000x higher LR (0.1-0.3) than AdamW (1e-4) due to geometric scaling. This is not a bug—it's manifold-appropriate scaling.
  - **Training both matrices vs. fixed A**: Fixed random A fails because Stiefel-B cannot extract features from unstructured input. Always train both A and B together.

- Failure signatures:
  - **Cosine similarity rising above 0.01**: Retraction is failing; check QR numerical stability
  - **Effective rank dropping below r-1**: Basis vectors collapsing; verify tangent projection formula
  - **Performance worse than AdamW with fixed A**: Expected—Stiefel requires co-adaptation of both matrices
  - **Gradient explosion on B**: Learning rate too high for manifold geometry; reduce by 10x

- First 3 experiments:
  1. **Validate orthogonality preservation**: Fine-tune LLaMA-1B on BoolQ with r=16. After each epoch, compute max |B^T B - I|. Target: < 1e-6 throughout training.
  2. **Effective rank comparison**: Run identical fine-tuning with AdamW vs. Stiefel. Compute effective rank (via Algorithm 2) of ∆W = BA after convergence. Target: Stiefel achieves 15-16, AdamW achieves 10-13.
  3. **Ablation on fixed vs. trainable A**: Compare Stiefel-LoRA with (a) both matrices trained, (b) fixed random A, (c) fixed pretrained-projection A. Expect (a) >> (c) > (b). If not, debug tangent projection.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Stiefel-LoRA perform when applied to instruction-tuned models compared to the base models tested? The authors explicitly state experiments were conducted exclusively on LLaMA base models, omitting instruction-tuned (Instruct) models. This is unresolved because optimization dynamics may interact differently with weights that have undergone alignment or instruction tuning. Benchmark comparisons on LLaMA-3-Instruct variants would resolve this.

- **Open Question 2**: Why does Stiefel-LoRA underperform AdamW when matrix A is fixed to a random initialization? Section 4.3 and Table 4 show Stiefel fails to outperform AdamW with static A, and authors hypothesize lack of "meaningful signal" without verification. This is unresolved because it's unclear if failure stems from random seed, lack of co-adaptation, or fundamental incompatibility. An ablation study varying A initialization would resolve this.

- **Open Question 3**: Can adaptive rank allocation strategies be effectively combined with Stiefel manifold constraints? The authors mention future work could explore "adaptive rank allocation methodologies, similar to approaches like AdaLoRA." This is unresolved because current Stiefel-LoRA uses fixed rank r, and it's unknown if orthogonality constraint aids or hinders dynamic pruning or growing of rank dimensions. An implementation that dynamically adjusts r would resolve this.

## Limitations

- Critical hyperparameters including Adam β₁/β₂/ε values, exact initialization methods for both A and B matrices, and dataset preprocessing details are not specified, limiting faithful reproduction.
- The 9-12% accuracy gains are reported across seven datasets but lack statistical significance testing or variance estimates, making performance claims less robust.
- The claim that basis redundancy is the primary bottleneck in LoRA performance assumes rank underutilization is more limiting than other factors like attention mechanism adaptation or task-specific feature extraction.

## Confidence

- **High Confidence**: The orthogonality preservation mechanism (cosine similarity ≈ 0) and effective rank analysis (Stiefel achieves full rank vs. AdamW's partial utilization) are well-supported by experimental evidence and mathematical formulation.
- **Medium Confidence**: The 9-12% average accuracy improvements across commonsense tasks, as these aggregate results lack variance reporting and statistical validation.
- **Low Confidence**: The claim that basis redundancy is the primary bottleneck in LoRA performance, as this assumes rank underutilization is more limiting than other factors.

## Next Checks

1. **Statistical Validation**: Replicate experiments with variance reporting across multiple random seeds and conduct paired statistical tests (e.g., Wilcoxon signed-rank) comparing Stiefel vs. AdamW accuracy distributions across all seven datasets.

2. **Rank Sensitivity Analysis**: Systematically vary rank r (e.g., 8, 16, 32, 64) for both optimizers on a subset of tasks to quantify the marginal benefit of full rank utilization at different capacity levels and identify break-even points.

3. **Generalization Stress Test**: Evaluate Stiefel-LoRA on non-commonsense tasks including few-shot learning benchmarks (e.g., BIG-bench) and cross-dataset transfer scenarios to assess whether orthogonality benefits generalize beyond the reported task distribution.