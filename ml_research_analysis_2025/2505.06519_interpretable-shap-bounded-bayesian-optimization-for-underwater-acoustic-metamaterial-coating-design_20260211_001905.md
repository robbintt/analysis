---
ver: rpa2
title: Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial
  Coating Design
arxiv_id: '2505.06519'
source_url: https://arxiv.org/abs/2505.06519
tags:
- shap
- design
- optimization
- acoustic
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a SHAP-bounded Bayesian optimization framework
  for designing underwater acoustic metamaterials with improved sound absorption.
  The approach integrates SHapley Additive exPlanations (SHAP) with Bayesian optimization
  to identify influential design parameters and refine the search space, thereby accelerating
  optimization.
---

# Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial Coating Design

## Quick Facts
- arXiv ID: 2505.06519
- Source URL: https://arxiv.org/abs/2505.06519
- Reference count: 40
- Achieved up to 11% improvement in optimal solutions compared to standard Bayesian optimization for underwater acoustic metamaterial coating design

## Executive Summary
This work presents a SHAP-bounded Bayesian optimization framework for designing underwater acoustic metamaterials with improved sound absorption. The approach integrates SHapley Additive exPlanations (SHAP) with Bayesian optimization to identify influential design parameters and refine the search space, thereby accelerating optimization. Applied to two polyurethane materials with distinct hardness levels (PU80 and PU90), the method achieved up to 11% improvement in optimal solutions compared to standard Bayesian optimization, without increasing simulation iterations. SHAP analysis revealed distinct parameter sensitivities between the materials, enabling targeted search space refinement.

## Method Summary
The method combines Bayesian optimization with SHAP-based interpretability to accelerate design optimization of underwater acoustic metamaterials. A Deep Neural Network surrogate model predicts sound absorption from 10 geometric parameters. SHAP analysis identifies which parameters most influence the objective and whether their effects are consistently positive or negative. The algorithm automatically refines parameter bounds based on these trends, focusing the search on high-potential regions. The optimization runs on two polyurethane materials (PU80 and PU90) with different hardness levels, using weighted absorption coefficient as the objective function and incorporating manufacturability constraints.

## Key Results
- Achieved up to 11% improvement in optimal solutions compared to standard Bayesian optimization
- Required no additional simulation iterations beyond the standard approach
- Successfully identified distinct parameter sensitivities for PU80 and PU90 materials through SHAP analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SHAP analysis captures large-scale parameter trends that standard Gaussian Process Regression (GPR) kernels may smooth over or miss.
- **Mechanism:** While GPR models in Bayesian Optimization (BO) often assume a return to the mean in unexplored regions (flat large-scale trends), SHAP aggregates feature contributions across the dataset to identify consistent monotonic relationships (e.g., "increasing radius always improves absorption"). This allows the algorithm to bias the search based on global sensitivity rather than just local uncertainty.
- **Core assumption:** The objective function possesses discoverable, largely monotonic dependencies for key parameters that persist across the search space.

### Mechanism 2
- **Claim:** Algorithmic bound refinement transforms global exploration into localized exploitation, accelerating convergence.
- **Mechanism:** By identifying parameter ranges with strictly negative SHAP values (detrimental to the objective) and excluding them from the search space, the algorithm reduces the effective volume of the design space. This forces the acquisition function (Expected Improvement) to propose samples within the remaining high-potential regions, increasing sample efficiency.
- **Core assumption:** The initial random sampling (10x dimensionality) is sufficient to identify detrimental parameter ranges without accidentally excluding the global optimum.

### Mechanism 3
- **Claim:** The framework adapts to distinct material physics by detecting rheology-dependent parameter sensitivities.
- **Mechanism:** The method trains a Deep Neural Network (DNN) surrogate on simulation data specific to a material (e.g., PU80 vs. PU90). SHAP analysis then reveals that optimal geometric parameters differ based on the matrix hardness (e.g., PU90 prefers larger voids; PU80 is sensitive to vertical positioning), allowing the optimization to customize the search path for specific physical constraints.
- **Core assumption:** The surrogate model (DNN) accurately captures the complex non-linear interactions between material rheology and geometry.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** This is the core interpretability tool used to open the "black box" of the DNN surrogate. It quantifies how much each design parameter (e.g., void radius) contributes to the predicted sound absorption.
  - **Quick check question:** If a parameter has a SHAP value of +0.5 for a specific sample, does that increase or decrease the objective function value?

- **Concept: Bayesian Optimization (BO)**
  - **Why needed here:** BO is the base optimization engine used to navigate the expensive design space. It uses a surrogate model and an acquisition function to decide where to sample next.
  - **Quick check question:** What is the trade-off managed by the "Expected Improvement" acquisition function? (Answer: Exploration vs. Exploitation).

- **Concept: Acoustic Metamaterials & Rheology**
  - **Why needed here:** The domain context. The objective is to minimize sound reflection via embedded voids in a viscoelastic matrix. The "hardness" (PU80/PU90) dictates how sound waves propagate and attenuate.
  - **Quick check question:** Why can't a single optimized geometry apply to both PU80 and PU90 materials? (Answer: Different frequency-dependent viscoelastic behaviors).

## Architecture Onboarding

- **Component map:** Data Generator -> Surrogate Model (DNN) -> Explainer (SHAP) -> Refiner (Bound Logic) -> Optimizer (BO)
- **Critical path:** The **SHAP-to-Bound Feedback Loop**. The system samples → trains DNN → computes SHAP → tightens bounds → samples again. This loop is triggered every 50 iterations after the initial 100 random samples.
- **Design tradeoffs:**
  - **Interpretability vs. Lock-out Risk:** Aggressive bound tightening speeds up convergence but risks excluding the global optimum if the initial trend analysis is flawed.
  - **DNN vs. GPR:** The authors use a DNN for the SHAP explanation (better for capturing complex non-linearities for interpretation) but typically rely on GPR for the BO acquisition function (better uncertainty quantification).
- **Failure signatures:**
  - **Premature Convergence:** Optimization plateaus early with no improvement. *Fix:* Loosen the bound margin (e.g., from 10% to 20%) or increase initial random sampling size.
  - **Noisy SHAP Plots:** SHAP dependence plots look like clouds rather than lines/trends. *Fix:* Increase the training data for the surrogate model or reduce DNN complexity to prevent overfitting.
  - **Constraint Violation:** Refined bounds suggest geometries that are physically impossible (e.g., voids overlapping). *Fix:* Ensure geometric constraint checks (Page 12) are applied *after* SHAP bound refinement.
- **First 3 experiments:**
  1. **Baseline Validation:** Run standard BO vs. SHAP-bounded BO on a known analytic function (e.g., Hartmann 6D) to verify the acceleration effect without simulation costs.
  2. **Sensitivity Sweep:** Run the optimization on PU90 with varying "bound tightening margins" (e.g., 5%, 10%, 20%) to find the sweet spot between speed and robustness against lock-out.
  3. **Ablation Study:** Disable the SHAP refinement (using only standard domain reduction) to isolate the performance gain specifically attributed to the SHAP trend detection (comparing "blue" vs "pink/orange" curves in Fig 3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SHAP interaction values be effectively integrated into the optimization framework to handle parameters with inseparable coupling?
- **Basis in paper:** [explicit] The authors state, "Another potential expansion... would be to include two-parameter SHAP interaction values," noting that uni-variate analysis struggles when SHAP values bifurcate due to coupling.
- **Why unresolved:** The current algorithm relies on identifying domains with purely positive/negative SHAP values, which fails when parameters have complex, codependent effects on the objective function.
- **What evidence would resolve it:** An updated algorithm that utilizes interaction values to refine bounds based on parameter pairs, tested on objective functions with known high-order interactions.

### Open Question 2
- **Question:** Are dimensionality reduction techniques (e.g., PCA, autoencoders) redundant with SHAP-bounded domain reduction, or can they be combined for enhanced performance?
- **Basis in paper:** [explicit] The paper asks, "Further investigation is needed to determine the effectiveness of combining these two strategies. They could be redundant... Alternatively, dimensional reduction could amplify the SHAP-bounding algorithm."
- **Why unresolved:** Both methods leverage broad trends in the data, but it is unclear if transforming the parameter space via dimensionality reduction would obscure or clarify the SHAP trends needed for bound refinement.
- **What evidence would resolve it:** Benchmarking the SHAP-bounding algorithm on raw parameters versus PCA-transformed parameters to see if the latter leads to faster convergence.

### Open Question 3
- **Question:** Can interpretability metrics derived from SHAP analysis provide sufficient confidence to activate domain reduction earlier than the standard 10x dimensionality sampling threshold?
- **Basis in paper:** [explicit] The authors note, "It is an open question if interpretability tools could provide some of that reassurance, and allow earlier activation of intelligent search algorithms."
- **Why unresolved:** Standard practice requires 10x dimension samples to ensure robustness, but this delays optimization; currently, there is no metric to determine if a SHAP trend is "reliable" on sparse data.
- **What evidence would resolve it:** Identifying a heuristic for SHAP trend stability (e.g., variance across cross-validation folds) that correlates with successful bound refinement on small datasets.

### Open Question 4
- **Question:** Would using SHAP values to bias the acquisition function (soft constraint) yield better results than the current method of strictly enforcing boundary refinements?
- **Basis in paper:** [explicit] The conclusion suggests "Future work may consider using SHAP values to bias an acquisition function without strictly enforcing a boundary," which would mitigate the risk of accidentally excluding the global optimum.
- **Why unresolved:** The current method uses "hard" bounds based on SHAP positivity; a "soft" biasing approach requires significant implementation effort for specific acquisition functions and has not been tested.
- **What evidence would resolve it:** Comparing the convergence rates and final optima of hard-bound SHAP-BO against a SHAP-weighted acquisition function (e.g., modifying Expected Improvement) on the same metamaterial design problem.

## Limitations
- Limited validation across diverse material systems beyond the two polyurethane variants tested
- DNN surrogate architecture details remain unspecified (layer sizes, training epochs, convergence criteria)
- The manufacturability penalty function formula and constraint handling are not fully detailed

## Confidence
- **High confidence:** The core SHAP-bounded optimization mechanism and its ability to accelerate convergence compared to standard BO
- **Medium confidence:** The generalizability of the approach to other metamaterial systems, as only two closely-related materials were tested
- **Low confidence:** The specific parameter bounds and trends identified for PU80/PU90 materials due to limited simulation data

## Next Checks
1. Conduct ablation studies on standard analytic benchmark functions (e.g., Hartmann 6D) to isolate SHAP trend detection benefits from other optimization improvements
2. Test the framework across a broader material spectrum (different polymer types, varying viscoelastic properties) to assess true generalizability
3. Implement uncertainty quantification for the bound refinement process to measure risk of excluding optimal solutions in early iterations