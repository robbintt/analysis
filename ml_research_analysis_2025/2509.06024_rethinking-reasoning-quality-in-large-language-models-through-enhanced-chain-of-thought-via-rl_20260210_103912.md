---
ver: rpa2
title: Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought
  via RL
arxiv_id: '2509.06024'
source_url: https://arxiv.org/abs/2509.06024
tags:
- reasoning
- answer
- logical
- drer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DRER, a reinforcement learning reward framework
  designed to improve reasoning quality in large language models. DRER combines a
  Reasoning Quality Reward, which incentivizes reasoning chains that increase confidence
  in correct answers, and a Dynamic Length Advantage, which stabilizes training by
  adjusting rewards based on response length.
---

# Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL

## Quick Facts
- arXiv ID: 2509.06024
- Source URL: https://arxiv.org/abs/2509.06024
- Reference count: 40
- Key outcome: DRER framework improves reasoning quality in LLMs via log-likelihood-based rewards and dynamic length constraints, achieving GPT-o3-mini level performance on synthetic logic tasks

## Executive Summary
This paper introduces DRER (Dynamic Reward for Enhanced Reasoning), a reinforcement learning framework designed to improve reasoning quality in large language models. DRER combines a Reasoning Quality Reward that measures how much chain-of-thought reasoning increases confidence in correct answers, with a Dynamic Length Advantage that stabilizes training by penalizing responses that deviate from validation-derived length bounds. The authors create LogicTree, a synthetic deductive reasoning dataset, and demonstrate that a 7B model trained with DRER achieves GPT-o3-mini level performance in 400 training steps while using 75% fewer tokens.

## Method Summary
DRER post-trains Qwen2.5-7B-Instruct-1M using DAPO/GRPO with a composite reward combining task performance (format and answer scores) and reasoning quality. The Reasoning Quality Reward (Rq) measures the log-likelihood margin between CoT and No-CoT prompt variants, passed through tanh to obtain bounded rewards. The Dynamic Length Advantage applies multiplicative attenuation to responses whose length deviates from 5th/95th percentile bounds computed from validation data. The framework requires two forward passes per sample to compute log-probabilities for both prompt variants, with training conducted over 400 steps.

## Key Results
- 7B model achieves GPT-o3-mini level performance on LogicTree in 400 training steps
- 30% increase in CoT answer confidence compared to baselines
- 75% reduction in token usage while maintaining or improving accuracy
- Generalization to other logical reasoning and mathematical benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Quality Reward (Rq)
- Claim: CoT tokens that genuinely improve reasoning should increase the model's confidence in the correct answer, measurable via log-likelihood margin.
- Mechanism: For each training instance, compute Δ(x) = ℓCoT − ℓNoCoT, where ℓCoT is the average log-probability of ground-truth answer tokens conditioned on the generated CoT, and ℓNoCoT is the same without CoT. Pass through tanh to obtain bounded reward Rq = tanh(Δ(x)).
- Core assumption: Effective reasoning chains causally increase the likelihood the model assigns to correct answers.
- Evidence anchors: [abstract] confirms Rq raises likelihood of correct answers; [Section 3.1, Eq. 9-12] provides mathematical specification; [Section 4.4] shows WR group achieves +2.46 nats (GSM8K) and +1.8 nats (LogicTree).

### Mechanism 2: Dynamic Length Advantage
- Claim: Penalizing responses that deviate from validation-derived length bounds stabilizes training and reduces token waste.
- Mechanism: After each validation round, compute 5th and 95th percentile response lengths (Lmin, Lmax) per difficulty bucket. Apply multiplicative attenuation gi = exp(−max{0, Lmin − ℓi, ℓi − Lmax} / τ) to the advantage Ai.
- Core assumption: Optimal response length correlates with task difficulty and can be estimated from validation statistics.
- Evidence anchors: [abstract] confirms length decay stabilizes training; [Section 3.1, Eq. 14] defines attenuation coefficient; [Figures 6-7] show DRER reduces average response length while maintaining accuracy.

### Mechanism 3: Composite Reward Integration
- Claim: Combining task-level rewards with reasoning-quality rewards yields better convergence than task-only rewards.
- Mechanism: Total reward R = Rtask + λq·Rq, where Rtask = Sformat + Sanswer. Integrated into DAPO's asymmetric clipping and group-normalized advantages.
- Core assumption: The two reward components are complementary and scale-compatible.
- Evidence anchors: [Section 3.1, Eq. 13] defines composite reward; [Section 4.1] describes task reward components; related work addresses similar problems but doesn't validate this specific composition.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation of language generation**
  - Why needed here: DRER operates on token-level rewards within an RL framework treating generation as sequential decision-making.
  - Quick check question: Can you explain why PPO clips the importance sampling ratio and how GRPO removes the value function?

- Concept: **Log-likelihood and autoregressive probability**
  - Why needed here: Rq mechanism requires computing log-probabilities of answer tokens under the policy, conditioned on different prompt variants.
  - Quick check question: Given πθ(y|x) = ∏ P(yt|x, y<t), how would you compute ℓCoT for a 3-token answer?

- Concept: **Advantage estimation (GAE, group normalization)**
  - Why needed here: DRER modifies advantages via length attenuation; understanding baseline advantage computation is prerequisite.
  - Quick check question: In GRPO, how is ˆAi,t computed from group rewards {Ri}?

## Architecture Onboarding

- Component map: Prompt Generator -> Rollout Sampler -> Reward Computer -> Advantage Normalizer -> Policy Updater

- Critical path:
  1. Sample batch of (question, answer) pairs
  2. Generate CoT trajectories and No-CoT prompts with ground-truth answers
  3. Compute ℓCoT, ℓNoCoT via forward pass → Rq
  4. Compute Rtask via rule-based format/answer checks
  5. Retrieve Lmin(d), Lmax(d) from validation statistics
  6. Compute gi based on response length and bucket
  7. Normalize rewards within groups, apply gi attenuation
  8. Update policy via DAPO objective

- Design tradeoffs:
  - Computing ℓCoT/ℓNoCoT requires 2× forward passes, increasing memory/compute overhead
  - Length bounds Lmin/Lmax are bucket-specific; finer granularity improves precision but requires more validation data
  - λq and τ are hyperparameters; paper uses λq=1, τ=8 but doesn't ablate extensively

- Failure signatures:
  - **Reward collapse**: If all responses have similar Rq (e.g., near zero), advantage signal weakens → check Δ(x) distribution
  - **Length oscillation**: If Lmin/Lmax are unstable across validation rounds → increase validation set size or smooth updates
  - **Over-attenuation**: If gi consistently < 0.5 for correct responses → Lmin/Lmax bounds may be too tight for task

- First 3 experiments:
  1. **Ablate Rq alone**: Train with Rtask only vs. Rtask + Rq (λq=1) on LogicTree subset; measure accuracy and Δ(x) trend
  2. **Ablate Dynamic Length**: Compare fixed vs. dynamic Lmin/Lmax; track token usage and training stability (advantage variance)
  3. **Generalization probe**: Evaluate DRER-trained checkpoint on held-out logical datasets (ProntoQA, ZebraLogic) to verify transfer claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DRER scale to larger model architectures (70B+ parameters or MoE models) in terms of memory overhead and latency for token-level reward computation?
- Basis in paper: [explicit] "memory and latency overhead of token-level rewards on 70 B-scale or MoE models is unknown and may be prohibitive."
- Why unresolved: The computational cost of computing log-likelihoods requires forward passes through both CoT and No-CoT trajectories.
- What evidence would resolve it: Benchmarks of DRER training on 70B or MoE models, reporting memory usage, training time per step, and throughput compared to baseline RL methods.

### Open Question 2
- Question: Can the DRER framework effectively incentivize analogical, inductive, or traceable reasoning beyond deductive logic?
- Basis in paper: [explicit] "LogicTree is limited to the deductive reasoning paradigm, while more diverse forms such as analogical reasoning, inductive reasoning, or traceable reasoning have not yet been evaluated."
- Why unresolved: The Reasoning Quality Reward is grounded in formal logical entailment; its applicability to probabilistic or non-monotonic reasoning patterns is unclear.
- What evidence would resolve it: Experiments applying DRER to datasets requiring inductive generalization or analogical transfer, with analysis of whether confidence-based rewards appropriately shape reasoning behavior.

### Open Question 3
- Question: How well does DRER-trained reasoning align with human judgments of chain-of-thought quality and preference?
- Basis in paper: [explicit] "Training and evaluation rely on an automatic logic verifier and confidence scores; no human preference or chain-quality annotation is included."
- Why unresolved: Automatic metrics may miss nuances such as pedagogical clarity, appropriate abstraction levels, or explanation styles preferred by humans.
- What evidence would resolve it: Human evaluation studies comparing DRER-trained CoT outputs against baselines on criteria like clarity, helpfulness, and trustworthiness.

### Open Question 4
- Question: Do social biases or distributional artifacts from synthetic LogicTree sentences transfer to downstream deployments?
- Basis in paper: [explicit] "LogicTree sentences are synthetically generated; potential social biases or misuse risks in real-world deployments have not been systematically analysed."
- Why unresolved: Even synthetic corpora may encode biases from template design or source knowledge bases.
- What evidence would resolve it: Bias audits on DRER-trained models using real-world reasoning tasks and adversarial probes to detect stereotyped or unfair outputs.

## Limitations
- Rq relies on log-likelihood margins as proxy for reasoning effectiveness, which may not capture all valid reasoning approaches
- Dynamic Length Advantage assumes optimal response lengths correlate with difficulty and can be estimated from validation statistics
- Framework requires multiple forward passes per sample (CoT and No-CoT variants), substantially increasing computational overhead

## Confidence

*High Confidence*: Experimental results showing DRER outperforming baselines on LogicTree and demonstrating improved efficiency (75% token reduction) are well-supported by the data.

*Medium Confidence*: Claim that DRER achieves "GPT-o3-mini level performance" on LogicTree in 400 steps requires contextual interpretation - this comparison is task-specific and doesn't imply general capability parity.

*Low Confidence*: Assertion that DRER "effectively trains models to leverage CoT for reasoning" overstates the evidence - while framework improves confidence scores and efficiency, whether models truly understand logical reasoning versus pattern matching remains unaddressed.

## Next Checks

1. **Ablation Study Design**: Implement controlled ablations testing Rq alone versus Rtask only on LogicTree subsets, measuring both accuracy trends and Δ(x) distributions to verify the reasoning quality signal persists throughout training and correlates with genuine reasoning improvement rather than proxy optimization.

2. **Length Bound Robustness Test**: Systematically vary validation frequency and bucket granularity for Dynamic Length Advantage, measuring training stability (advantage variance) and token efficiency across different logical complexity levels to determine optimal configuration parameters and identify failure modes.

3. **Cross-Dataset Generalization Benchmark**: Evaluate the DRER-trained checkpoint on a broader suite of logical reasoning datasets including natural language inference, deductive reasoning with varying premise structures, and mathematical word problems to assess whether improvements transfer beyond the synthetic LogicTree distribution and identify capability boundaries.