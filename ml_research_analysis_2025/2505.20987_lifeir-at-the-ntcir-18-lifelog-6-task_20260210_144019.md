---
ver: rpa2
title: LifeIR at the NTCIR-18 Lifelog-6 Task
arxiv_id: '2505.20987'
source_url: https://arxiv.org/abs/2505.20987
tags:
- retrieval
- images
- lifelog
- query
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of retrieving relevant lifelog
  images from large-scale personal archives using textual queries, targeting the NTCIR-18
  Lifelog-6 task. The proposed multi-stage pipeline includes image filtering to remove
  blurred content, query rewriting using large language models to preserve detailed
  intent, event-based candidate set expansion leveraging temporal continuity, and
  reranking with a multimodal large language model (MLLM) for enhanced relevance judgment.
---

# LifeIR at the NTCIR-18 Lifelog-6 Task

## Quick Facts
- arXiv ID: 2505.20987
- Source URL: https://arxiv.org/abs/2505.20987
- Reference count: 32
- One-line primary result: Multi-stage pipeline achieves mAP=0.2652, P@10=0.3038, R@10=0.2617 on NTCIR-18 Lifelog-6 retrieval task

## Executive Summary
This paper presents a multi-stage pipeline for retrieving relevant lifelog images from large personal archives using textual queries. The approach addresses the challenge of known-item search in lifelogs by combining image filtering, query rewriting, temporal candidate expansion, and multimodal reranking. The system processes 725k images across 26 topics, achieving strong performance through progressive refinements that leverage both semantic understanding and temporal continuity patterns in lifelog data.

## Method Summary
The method implements a five-stage pipeline: (1) blur filtering removes low-quality images using edge weight summation; (2) CLIP-based retrieval computes cosine similarity between image and rewritten text embeddings; (3) ChatGPT query rewriting synthesizes scattered topic information into concise first-person descriptions within 30-word limits; (4) event-based candidate expansion leverages temporal continuity by including 80 preceding and 80 subsequent images around high-confidence hits, with multi-round vector refinement; (5) Qwen2-VL multimodal reranking performs posterior filtering using location metadata when available. The approach balances recall enhancement through temporal expansion with precision optimization via advanced MLLM reasoning.

## Key Results
- Achieved mAP=0.2652, P@10=0.3038, R@10=0.2617 in final submission
- Progressive performance gains across five submissions demonstrate effectiveness of staged approach
- Query rewriting and MLLM reranking identified as particularly significant contributors to retrieval accuracy
- Successfully handles multimodal and temporal characteristics specific to lifelog data

## Why This Works (Mechanism)

### Mechanism 1: Intent Clarification via LLM-based Query Rewriting
The authors utilize an LLM (ChatGPT) to synthesize scattered information from the topic (Title, Description, Narrative) into a consolidated first-person description. This addresses CLIP's contrastive learning bias towards short text fragments, ensuring the embedding captures the "fine-grained descriptors" necessary for known-item search. The core assumption is that the LLM can successfully extract and condense relevant visual semantics from the narrative without hallucinating non-existent details.

### Mechanism 2: Recall Enhancement via Temporal Continuity
This mechanism exploits the "Temporal Connection" of lifelogs by grouping images into "visual events" and expanding the candidate set to include images temporally adjacent to high-confidence hits. The system employs multi-round expansion where the query vector is updated with visual features of top-retrieved images. The core assumption is that relevant moments are part of a continuous sequence rather than isolated snapshots, and timestamps are reliable.

### Mechanism 3: Precision Optimization via MLLM Reranking
After initial retrieval and expansion, the system uses Qwen2-VL to perform "posterior filtering" that likely possesses stronger cross-modal reasoning capabilities than CLIP. This allows it to verify complex conditions (e.g., "taking a photo of a meal" vs just "seeing food"). The core assumption is that the MLLM's reasoning capability is sufficient to distinguish subtle relevance differences that vector similarity cannot.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - Why needed here: This is the foundational retrieval engine. Understanding that CLIP maps images and text to a shared vector space is crucial for grasping why query length and semantic density matter.
  - Quick check question: How does CLIP handle a query that is significantly longer or more complex than its pre-training data?

- **Concept: Temporal Data Characteristics in Lifelogs**
  - Why needed here: The paper explicitly defines "Temporal Connection" as a core characteristic. Understanding that lifelog data is a time-series, not a set of independent images, is required to justify the expansion strategy.
  - Quick check question: Why would a standard search engine (like Google Images) fail to find a sequence of events like "walking to the coffee shop"?

- **Concept: Cross-Modal Reasoning / Semantic Gap**
  - Why needed here: The core challenge is aligning a text description ("putting away a Christmas tree") with visual pixels. Understanding the "semantic gap" explains why a lightweight model (CLIP) needs help from a heavyweight model (MLLM).
  - Quick check question: Why is cosine similarity alone often insufficient to determine if an image matches a complex textual narrative?

## Architecture Onboarding

- **Component map:** Preprocessing (Blur filter -> Metadata extraction) -> Query Processing (LLM Rewriter -> Text Embedding) -> Retrieval (Image Embedding -> Similarity Search -> Event Grouping) -> Expansion (Temporal Neighbor Retrieval -> Multi-round Vector refinement) -> Reranking (MLLM verification -> Final Ranking)

- **Critical path:** The query rewriting stage is the first critical bottleneck; if the rewrite fails to capture intent, the subsequent expansion and reranking operate on noise.

- **Design tradeoffs:**
  - Recall vs. Noise: The temporal expansion (including 80 preceding/subsequent images) significantly increases recall but introduces massive noise, placing a heavy burden on the MLLM reranker.
  - Cost vs. Accuracy: Using an MLLM for reranking is computationally expensive compared to simple vector search.

- **Failure signatures:**
  - Abstract Concept Failure: The system fails on queries requiring reasoning about state changes (e.g., "Putting away the Christmas tree") where the object appears visually different than the training concept.
  - Timestamp Dependency: The temporal expansion breaks completely if image filenames lack standardized timestamps.

- **First 3 experiments:**
  1. Baseline Validation: Run CLIP retrieval with raw queries vs. rewritten queries to quantify the impact of the LLM rewriter on P@10.
  2. Expansion Ablation: Test retrieval with and without the "80-frame temporal window" to measure the recall gain vs. the precision drop.
  3. MLLM Thresholding: Test the MLLM reranker with different prompt strictness levels (e.g., with/without location metadata hints) to optimize the trade-off between filtering noise and discarding true positives.

## Open Questions the Paper Calls Out

### Open Question 1
How can the embedding model be upgraded to handle fine-grained visual-semantic reasoning for complex queries involving non-canonical object states? The authors conclude that "upgrading the embedding model to improve its fine-grained understanding" is necessary, noting that CLIP fails on queries requiring reasoning (e.g., "putting away the Christmas tree"). This is unresolved because the current lightweight contrastive learning objective lacks the capacity to interpret complex concepts like a folded tree.

### Open Question 2
Can the temporal expansion strategy be decoupled from standardized timestamp naming conventions to support datasets with unstructured metadata? Section 3.4 notes that the temporal expansion method "relies heavily on standardized timestamp naming conventions" and fails when this structure is absent. This is unresolved because the current system cannot locate adjacent images without specific filename-based ordering, reducing its generalizability to other lifelog datasets.

### Open Question 3
How can the event-based candidate expansion be refined to capture relevant information in non-dominant time segments? The authors acknowledge that the strategy "exclusively expands the most densely clustered time segments," potentially neglecting relevant information in other periods. This is unresolved because the algorithm currently prioritizes peak intervals, which may systematically miss isolated but relevant events that occur outside high-density clusters.

## Limitations
- Query rewriting effectiveness is subjective and dependent on prompt quality without standardized evaluation
- MLLM reranking scalability and computational cost for large candidate sets is not addressed
- Temporal expansion assumes strictly temporally continuous and well-named lifelog images

## Confidence
- **Pipeline effectiveness**: Medium confidence - reported staged improvements but incomplete ablation study
- **MLLM reasoning capability**: Low confidence - no direct comparison to simpler reranking methods
- **Query rewriting impact**: Low confidence - claim not empirically validated with quality analysis

## Next Checks
1. **Ablation study on pipeline stages**: Run the pipeline with individual stages disabled to quantify the marginal contribution of each component to final metrics.
2. **Query rewriting quality analysis**: Manually evaluate a sample of rewritten queries against original topics to assess if the LLM preserves critical details and adheres to the 30-word limit without introducing hallucinations.
3. **Temporal expansion robustness test**: Test the expansion strategy on a subset of images with irregular timestamps or non-continuous naming to evaluate sensitivity to dataset-specific assumptions.