---
ver: rpa2
title: 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models
  via Reinforcement Learning'
arxiv_id: '2510.22282'
source_url: https://arxiv.org/abs/2510.22282
tags:
- reasoning
- urban
- socio-economic
- data
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CityRiSE introduces a reinforcement learning framework to enhance\
  \ large vision-language models for urban socio-economic status prediction. By designing\
  \ verifiable reward functions\u2014keyword and regression rewards\u2014and constructing\
  \ auxiliary reasoning datasets, the model learns to produce interpretable, structured\
  \ reasoning chains."
---

# CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.22282
- Source URL: https://arxiv.org/abs/2510.22282
- Reference count: 40
- One-line primary result: CityRiSE achieves R² = 0.603 on in-domain tasks and maintains positive performance on unseen cities and indicators using pure RL reasoning

## Executive Summary
CityRiSE introduces a reinforcement learning framework to enhance large vision-language models for urban socio-economic status prediction. By designing verifiable reward functions—keyword and regression rewards—and constructing auxiliary reasoning datasets, the model learns to produce interpretable, structured reasoning chains. The framework achieves superior generalization across unseen cities and novel socio-economic indicators, outperforming state-of-the-art baselines. CityRiSE reaches up to R² = 0.603 on in-domain tasks and maintains positive performance on out-of-domain predictions, including unseen indicators like Life Expectancy (R² = 0.200) and House Price (R² = 0.218). This demonstrates both accuracy and interpretability, advancing generalist urban socio-economic sensing.

## Method Summary
CityRiSE employs Group Relative Policy Optimization (GRPO) to fine-tune Qwen2.5-VL-7B for predicting urban socio-economic indicators from satellite and street view imagery. The model receives two verifiable rewards: a Keyword Reward that reinforces mentions of urban-perceptual concepts (person, vehicle, greenery, road infrastructure, street furniture, building, location), and a Regression Reward using Huber loss with exponential transform for numerical accuracy. Training uses CityLens dataset (17 cities, 10 train cities, 5 indicators, 2,828 samples) plus auxiliary perceptual and general visual reasoning tasks. The output includes structured reasoning chains in `<think/>` tags plus numerical predictions in `<answer/>` tags, optimized through 4 epochs with batch size 8, learning rate 1e-6, and rollout N=5.

## Key Results
- Achieves R² = 0.603 on in-domain tasks (GDP, Population, House Price, Mental Health, Accessibility to Education)
- Maintains positive performance on unseen cities (R² = 0.286) and unseen indicators (R² = 0.305)
- Outperforms SFT-based UI-CoT (0.063 on unseen cities, 0.103 on unseen indicators) and UrbanMLLM (0.372 on unseen cities)
- Shows robustness across diverse urban contexts including Shanghai, San Francisco, Sao Paulo, Nairobi, and others

## Why This Works (Mechanism)

### Mechanism 1: Dual Verifiable Reward Signals for Goal-Directed Reasoning
Combining keyword-based semantic reward with regression-based accuracy reward provides complementary supervision that guides both interpretability and numerical precision. The Keyword Reward reinforces mentions of urban-perceptual concepts, encouraging grounded reasoning, while the Regression Reward uses Huber loss transformed exponentially, providing smooth gradients that distinguish near-correct from far-incorrect predictions.

### Mechanism 2: Emergent Reasoning via Pure RL Without Supervised CoT Templates
Pure reinforcement learning—without supervised chain-of-thought examples—induces more flexible, transferable reasoning patterns than SFT with fixed CoT annotations. GRPO generates N responses per input, computes group-relative advantages, and optimizes via rewards plus KL divergence, allowing the model to discover reasoning structures organically.

### Mechanism 3: Auxiliary Reasoning Tasks for Cross-Domain Skill Transfer
Training on complementary perceptual and abstract reasoning tasks builds transferable skills supporting generalization to unseen cities and novel indicators. Perceptual Urban Reasoning Data (spatial reasoning, geolocation, socio-economic ranking) provides domain-specific priors, while General Visual Reasoning Data (object counting, pattern completion) cultivates abstract reasoning applicable across tasks.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: This RL algorithm enables learning from rewards without requiring ground-truth labels during optimization, computing advantages relative to response groups
  - Quick check question: How does GRPO compute advantages differently from standard policy gradient methods?

- **LVLMs as Generalist Predictors**
  - Why needed here: Unlike task-specific regression models, LVLMs can be prompted to predict any indicator via language, enabling zero-shot extension to unseen indicators
  - Quick check question: Why can CityRiSE predict "Life Expectancy" at test time when SVL-GT cannot?

- **Huber Loss for Continuous Reward Shaping**
  - Why needed here: The Regression Reward uses Huber loss for stability—quadratic near correct values, linear for outliers—preventing gradient explosion
  - Quick check question: Why is Huber loss preferred over raw squared error for this reward formulation?

## Architecture Onboarding

- **Component map:**
  Input: 1 satellite image + 10 street view images + text prompt specifying target indicator
  Base Model: Qwen2.5-VL-7B
  Training: GRPO with Keyword Reward + Regression Reward
  Auxiliary Data: Perceptual Urban Reasoning (632 spatial + 350 geolocation + 699 ranking) + General Visual Reasoning (300 counting + 300 patterns)
  Output: Reasoning chain in `<think/>` tags + numerical prediction (1–10) in `<answer/>` tags

- **Critical path:**
  1. Prepare three training datasets with appropriate reward types
  2. Implement Keyword Reward (urban concept detection + location grounding) and Regression Reward (Huber-based)
  3. Run GRPO training: 4 epochs, batch size 8, learning rate 1e-6, rollout N=5
  4. Evaluate on in-domain, unseen cities (7 held-out), and unseen indicators (6 novel)

- **Design tradeoffs:**
  - Pure RL vs. SFT-first: RL improves generalization but may underperform SFT on pure in-domain memorization
  - Auxiliary data ratio: More auxiliary data may dilute primary task signal but improves transfer
  - Keyword weight λ: Too high risks superficial keyword mention; too low loses semantic guidance

- **Failure signatures:**
  - Model outputs only numbers without `<think/>` reasoning: Keyword Reward may not be activating; check prompt format
  - Performance collapse on unseen indicators: Verify General Visual Reasoning data is included and properly formatted
  - Training instability: Reduce KL coefficient β or learning rate; authors note occasional GRPO instability

- **First 3 experiments:**
  1. **Reward ablation reproduction:** Train without Keyword Reward, without Regression Reward, and without both; verify performance drops match Figure 2 patterns
  2. **City transfer test:** Evaluate on held-out cities (Shanghai, San Francisco, etc.); confirm R² remains positive across indicators (target: 0.15–0.37 range)
  3. **Reasoning trace analysis:** Sample predictions and verify `<think/>` blocks contain location grounding and multi-concept reasoning; compare with UI-CoT outputs for rigidity vs. flexibility

## Open Questions the Paper Calls Out

- **Improving abstract indicator prediction:** The authors note CityRiSE struggles with abstract targets like Accessibility to Health where relevant cues are difficult to infer directly from imagery, remaining an important direction for future work
- **Enhancing training stability:** The paper observes instability during GRPO training that occasionally leads to unexpected behaviors, presenting an avenue for improving training robustness and sample efficiency
- **Dynamic keyword learning:** The fixed six urban-perceptual keywords in the Keyword Reward may not capture all relevant visual cues across diverse indicators and cultural contexts
- **Discretization strategy effects:** The 10-bin discretization may introduce quantization error, and the trade-off between unified output spaces and prediction granularity remains unexplored

## Limitations
- Modest performance on truly novel indicators (R² = 0.200-0.218) suggests room for improvement in transfer capabilities
- Exact hyperparameter values for reward formulation (α, δ, λ, β, ε) are unspecified, affecting reproducibility
- Construction methodology for auxiliary datasets is unspecified, requiring assumptions about source data
- Struggles with abstract socio-economic indicators like Accessibility to Health where visual cues are indirect

## Confidence
- **High confidence**: Dual verifiable rewards mechanism is well-supported by ablation studies showing performance collapse when either reward is removed
- **Medium confidence**: Superior generalization across unseen cities and indicators is supported by empirical results, though performance on novel indicators remains limited
- **Medium confidence**: Pure RL induces more flexible reasoning than SFT with CoT templates is supported by cross-dataset comparisons, though mechanisms could be more explicit

## Next Checks
1. **Reward ablation replication**: Train CityRiSE variants without Keyword Reward, without Regression Reward, and without both; verify performance drops match the reported patterns (GDP R² from 0.39 to below -1.00 when both rewards removed)
2. **City transfer validation**: Evaluate the model on all seven held-out cities across all indicators; confirm R² remains positive and within the reported 0.15-0.37 range
3. **Reasoning trace audit**: Sample 50 predictions from both CityRiSE and UI-CoT; analyze `<think/>` blocks for geographic grounding, multi-concept integration, and flexibility; verify CityRiSE reasoning demonstrates greater adaptability compared to rigid CoT patterns