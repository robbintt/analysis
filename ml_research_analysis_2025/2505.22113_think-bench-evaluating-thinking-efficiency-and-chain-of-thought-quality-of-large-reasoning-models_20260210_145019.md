---
ver: rpa2
title: 'THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of
  Large Reasoning Models'
arxiv_id: '2505.22113'
source_url: https://arxiv.org/abs/2505.22113
tags:
- reasoning
- arxiv
- efficiency
- answer
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Think-Bench, a benchmark for evaluating the
  reasoning efficiency and chain-of-thought (CoT) quality of large reasoning models
  (LRMs). The benchmark includes tasks from mathematics, physics, and chemistry at
  simple and difficult levels, with annotations of key reasoning steps.
---

# THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models

## Quick Facts
- arXiv ID: 2505.22113
- Source URL: https://arxiv.org/abs/2505.22113
- Authors: Zhiyuan Li; Yi Chang; Yuan Wu
- Reference count: 40
- Primary result: Introduces benchmark for evaluating reasoning efficiency and CoT quality of LRMs

## Executive Summary
This paper introduces Think-Bench, a comprehensive benchmark for evaluating the reasoning efficiency and chain-of-thought (CoT) quality of large reasoning models (LRMs). The benchmark includes tasks from mathematics, physics, and chemistry at simple and difficult levels, with annotations of key reasoning steps. Six efficiency metrics are proposed, including total tokens, first correct tokens, efficiency ratio, reflection quality, reflection tokens, and thought number. The evaluation covers eleven models, including proprietary and open-source LRMs.

Results show that most LRMs exhibit overthinking on simple questions, generating excessive reasoning tokens and leading to low efficiency. While many LRMs demonstrate high CoT quality, several models suffer from inefficiency due to prolonged reasoning chains and frequent thought switching. The study highlights the need for dynamic reasoning mechanisms and early exit strategies to improve computational efficiency in LRMs.

## Method Summary
Think-Bench evaluates reasoning efficiency through six proposed metrics: total tokens, first correct tokens, efficiency ratio, reflection quality, reflection tokens, and thought number. The benchmark covers STEM domains (mathematics, physics, chemistry) with tasks at simple and difficult levels, annotated with key reasoning steps. Eleven LRMs are evaluated, including both proprietary and open-source models. The evaluation methodology focuses on measuring token usage patterns, reasoning depth, and quality of chain-of-thought generation across different task complexities.

## Key Results
- Most LRMs exhibit overthinking on simple questions, generating excessive reasoning tokens and achieving low efficiency
- High CoT quality observed across many LRMs, but several models suffer from prolonged reasoning chains and frequent thought switching
- Need identified for dynamic reasoning mechanisms and early exit strategies to improve computational efficiency
- Proprietary and open-source LRMs show varying performance patterns across efficiency metrics

## Why This Works (Mechanism)
The evaluation framework works by systematically measuring token efficiency and reasoning quality across multiple dimensions. By distinguishing between simple and difficult tasks, the benchmark reveals how models allocate computational resources differently based on task complexity. The proposed metrics capture both the quantity (token usage) and quality (reasoning coherence) of chain-of-thought generation, providing a comprehensive assessment of model efficiency.

## Foundational Learning

**Chain-of-Thought Reasoning**: Why needed - to understand how models break down complex problems; Quick check - examine step-by-step reasoning traces for logical coherence

**Token Efficiency Metrics**: Why needed - to quantify computational resource usage; Quick check - compare token counts across different model approaches

**Reflection Quality**: Why needed - to assess model's ability to evaluate its own reasoning; Quick check - analyze self-correction patterns in reasoning chains

**Task Complexity Scaling**: Why needed - to understand how models adapt reasoning depth to problem difficulty; Quick check - compare performance across simple vs. difficult tasks

**Reasoning Path Annotation**: Why needed - to provide ground truth for evaluating model reasoning; Quick check - verify alignment between annotated and generated reasoning steps

## Architecture Onboarding

Component Map: Input Task -> LRM Processing -> Chain-of-Thought Generation -> Token Output -> Efficiency Metrics

Critical Path: Task input flows through LRM to generate reasoning tokens, which are then evaluated against efficiency metrics and ground truth annotations to produce performance scores.

Design Tradeoffs: Models must balance depth of reasoning (potentially higher accuracy) against computational efficiency (token usage), with some models favoring exhaustive exploration while others prioritize conciseness.

Failure Signatures: Excessive token generation on simple tasks, frequent thought switching without convergence, low reflection quality indicating poor self-evaluation capabilities.

Three First Experiments:
1. Compare token efficiency across simple vs. difficult tasks for a single LRM
2. Analyze reflection quality patterns in models with high vs. low efficiency
3. Test early exit strategies on simple questions to measure efficiency improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Potential data leakage concerns due to lack of explicit data partitioning strategies between training and evaluation sets
- Limited domain scope focusing primarily on STEM domains without addressing other reasoning domains
- Proposed efficiency metrics lack validation against human annotation or alternative efficiency measures

## Confidence

| Claim Cluster | Confidence Level |
| --- | --- |
| LRMs exhibit overthinking on simple questions | High |
| Characterization of CoT quality | Medium |
| Generalizability of efficiency findings | Low |

## Next Checks
1. Conduct data leakage analysis using embedding-based similarity searches to verify test questions were not in training data
2. Expand benchmark to include non-STEM reasoning tasks (commonsense reasoning, logical inference) to assess generalizability
3. Validate proposed efficiency metrics against human-annotated reasoning traces and alternative measures (wall-clock time, computational cost)