---
ver: rpa2
title: High Dimensional Bayesian Optimization using Lasso Variable Selection
arxiv_id: '2504.01743'
source_url: https://arxiv.org/abs/2504.01743
tags:
- optimization
- variables
- function
- bayesian
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LassoBO, a Bayesian optimization algorithm
  designed for high-dimensional problems. The core idea is to iteratively identify
  important variables using a Lasso-based method on Gaussian process inverse length
  scales, and then restrict the acquisition function optimization to a subspace containing
  only these important variables, while imputing the rest via a mix of exploitation
  (using the best observed point) and exploration (random sampling).
---

# High Dimensional Bayesian Optimization using Lasso Variable Selection

## Quick Facts
- arXiv ID: 2504.01743
- Source URL: https://arxiv.org/abs/2504.01743
- Reference count: 40
- LassoBO achieves sublinear cumulative regret bounds and outperforms state-of-the-art methods on high-dimensional BO benchmarks

## Executive Summary
This paper proposes LassoBO, a Bayesian optimization algorithm designed for high-dimensional problems. The core idea is to iteratively identify important variables using a Lasso-based method on Gaussian process inverse length scales, and then restrict the acquisition function optimization to a subspace containing only these important variables, while imputing the rest via a mix of exploitation (using the best observed point) and exploration (random sampling). Theoretically, the method achieves a sublinear cumulative regret bound, which is a novel result for variable-selection-based high-dimensional BO. Empirically, LassoBO outperforms several state-of-the-art methods on synthetic benchmarks and real-world tasks such as Rover trajectory optimization, MuJoCo locomotion, and DNA classification, demonstrating strong performance in both high- and low-dimensional regimes.

## Method Summary
LassoBO uses Gaussian process regression with ARD kernels and Lasso regularization on inverse length scales to identify important variables in high-dimensional black-box optimization. The method iteratively fits a GP, optimizes a Lasso-penalized marginal likelihood to estimate variable importance, partitions dimensions into important and unimportant sets, and optimizes the acquisition function only over the important variables while imputing unimportant variables through random sampling and best-point exploitation. The algorithm uses GP-UCB acquisition, Adam optimization for the Lasso objective, and constructs a multi-subspace search space with M_t = ⌈³√t⌉ random subspaces for exploration.

## Key Results
- LassoBO achieves sublinear cumulative regret bound RT, which is a novel theoretical result for variable-selection-based high-dimensional BO
- Outperforms SAASBO and TuRBO on synthetic benchmarks including Hartmann, Ackley, and Levy functions extended to D=300
- Demonstrates strong performance on real-world tasks including Rover trajectory optimization, Hopper locomotion, and DNA classification with D=1024

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse length scales of GP kernels serve as provable proxies for variable importance in high-dimensional BO.
- Mechanism: The paper proves (Theorem 3.1) that partial derivatives ∂f/∂xi are bounded by √ρi with high probability. Larger ρi implies dimension i influences f more; smaller ρi implies less impact. This enables ranking variables by estimated ρi values rather than heuristics.
- Core assumption: The objective function f is a GP sample path with SE or Matérn kernel, and the function exhibits low effective dimensionality (few truly important variables).
- Evidence anchors:
  - [abstract]: "identifies important variables by estimating the length scales of Gaussian process kernels"
  - [section 3.1]: Theorem 3.1 establishes P(|∂f/∂xi| ≤ L√ρi) ≥ 1 - e^(-L²/2σ²k) for SE kernel
  - [corpus]: Weak direct support; related work on variable selection (e.g., Stability Selection via Variable Decorrelation) uses Lasso for selection but not via GP length scales
- Break condition: If the function lacks low effective dimensionality, or if insufficient observations prevent accurate ρ estimation (the paper notes this requires "large number of observations relative to function's dimensionality").

### Mechanism 2
- Claim: L1 regularization on inverse length scales induces sparsity, correctly identifying important variables with limited samples.
- Mechanism: Instead of standard maximum likelihood, the method optimizes Eq. (7): U_t(ψ) = -log p(y|X,K) + λΣ|ρi|. The L1 penalty drives unimportant ρi toward zero while preserving large values for truly important dimensions. Variables with ρi above the mean are classified as important.
- Core assumption: The true underlying function has few active dimensions (intrinsic sparsity in variable importance).
- Evidence anchors:
  - [abstract]: "LassoBO uses Lasso regularization to estimate variable importance"
  - [section 3.1]: Eq. (7) shows the penalized objective; Figure 5 demonstrates correlation between √ρi and true coefficients αi on Sum Squares function
  - [corpus]: An Adaptive Dropout Approach for High-Dimensional BO (arXiv:2504.11353) addresses similar high-D BO challenges but via dropout, not Lasso-based selection
- Break condition: When λ is misspecified (too high suppresses true important variables; too low fails to induce sparsity), or when observations are too few (<30 initial samples used in experiments).

### Mechanism 3
- Claim: Multi-subspace search with exploitative-explorative imputation achieves sublinear regret while reducing acquisition optimization cost.
- Mechanism: At iteration t, construct search space X_t = [X_{I_t}, X_{[D]\I_t}] where X_{I_t} = [0,1]^{|I_t|} (important, optimized) and X_{[D]\I_t} = Z_t ∪ {x_{t-1,+}_{[D]\I_t}} (unimportant, sampled). Z_t contains M_t random vectors for exploration; the best-point component provides exploitation. This reduces acquisition optimization to d_t << D dimensions.
- Core assumption: Unimportant variables have bounded impact on function value, formalized in Lemma 11.3's bound involving Σ_{i∈[D]\I_t} ρi.
- Evidence anchors:
  - [abstract]: "construct an effective search region consisting of multiple subspaces to optimize the acquisition function only on the important variables"
  - [section 4]: Theorem 4.2 and Corollary 4.3 prove RT achieves sublinear growth with M_t = ⌈ⁿ√t⌉
  - [corpus]: No direct corpus evidence for this specific multi-subspace construction strategy
- Break condition: If M_t is too small (insufficient exploration of unimportant space) or too large (dilutes exploitation), regret bounds degrade; Figure 9 shows performance sensitivity to M_t.

## Foundational Learning

- Concept: Gaussian Process regression with ARD kernels
  - Why needed here: LassoBO's variable selection depends on understanding how length scales ρi encode dimension-specific smoothness in SE/Matérn kernels. Without this, the connection between ρi and variable importance is opaque.
  - Quick check question: Given a GP with SE kernel k(x,x') = σ²k exp(-0.5 Σ ρi(xi - x'i)²), what happens to the function's sensitivity to dimension i if ρi → 0?

- Concept: Bayesian optimization regret analysis
  - Why needed here: The paper's main theoretical contribution is a sublinear regret bound. Understanding cumulative regret RT = Σ(f(x*) - f(x_t)) and information gain γT is necessary to interpret why the method is provably efficient.
  - Quick check question: Why does sublinear regret (lim T→∞ RT/T = 0) imply convergence to the optimum?

- Concept: Lasso/L1 regularization and sparsity
  - Why needed here: The core innovation applies L1 penalties to GP hyperparameters. Understanding why L1 (not L2) induces sparsity explains why this correctly identifies important variables.
  - Quick check question: Why does L1 regularization tend to produce exact zero coefficients while L2 does not?

## Architecture Onboarding

- Component map:
  1. **GP Surrogate Module**: Fits GP on all D dimensions using current data D_t with Lasso-penalized hyperparameter estimation (Eq. 7)
  2. **Variable Selection Module**: Computes ρ estimates, partitions dimensions into important I_t (ρi > mean) and unimportant [D]\I_t
  3. **Search Space Constructor**: Builds X_t with M_t random unimportant components + best-point imputation
  4. **Acquisition Optimizer**: Maximizes a_t(x) only over important dimensions (computational savings: O(d_t²) vs O(D²))
  5. **Evaluation Loop**: Queries black-box f(x_t), updates D_t and best point x_{t,+}

- Critical path:
  1. Initialize with D_0 ≥ 30 random observations (paper's experiments use 30)
  2. Each iteration: Fit GP → Optimize Eq. (7) with ADAM (100 steps, 5 best of 10 random init) → Classify variables → Build X_t → Optimize acquisition → Evaluate
  3. Key hyperparameters: λ = 10⁻³ (default), M_t = ⌈³√t⌉ (n=3), window size W ≈ 10 for smoothing ρ estimates

- Design tradeoffs:
  - **λ (Lasso penalty)**: Higher λ → more aggressive sparsity → risk missing true important variables. Lower λ → less sparsity → dilutes computational savings. Paper uses 10⁻³ universally.
  - **M_t (number of random subspaces)**: Small M_t → faster but may miss good unimportant variable settings. Large M_t → better exploration but slower. Paper proves M_t = ⌈ⁿ√t⌉ suffices for sublinear regret.
  - **Window size W for ρ estimation**: Affects stability of variable selection. W=10 recommended; too small causes noisy selection, too large delays adaptation.

- Failure signatures:
  - **Converges slowly or plateaus early**: Likely M_t too small (insufficient exploration) or λ too high (over-pruning variables)
  - **High variance across runs**: Function may have high aleatoric noise (like MuJoCo tasks) or insufficient initial samples
  - **Computational bottleneck**: If |I_t| stays large (near D), Lasso isn't inducing sparsity → check λ and data quality
  - **Selects wrong variables**: Compare selected I_t against ground truth in synthetic tests; may need more observations before selection stabilizes (paper shows convergence after ~180 evaluations on D=300 problems)

- First 3 experiments:
  1. **Synthetic validation**: Run on Levy function (D=100, d_e=15) with known ground truth important variables. Verify that estimated ρi correlate with true coefficients (replicate Figure 5) and that selected I_t converges to true active set. Log: number of selected variables per iteration, final regret vs. baselines.
  2. **Ablation on M_t**: Fix D=100, d_e=15, vary M_t ∈ {1, 3, 5, 10, 20}. Plot regret curves to replicate Figure 9. Confirm that M_t = ⌈³√t⌉ balances early exploitation vs. late convergence.
  3. **Scalability stress test**: Compare LassoBO vs. SAASBO and TuRBO on D ∈ {100, 300, 500} with fixed d_e=15. Measure: wall-clock time per iteration, memory usage, and final regret. Verify LassoBO's advantage grows with D (Figure 8 suggests this).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LassoBO perform when the assumption of low effective dimensionality is violated, specifically when the percentage of valid variables is high?
- Basis in paper: [explicit] Section 14 states that LassoBO "relies on the assumption of low effective dimensionality and might not work well if the percentage of valid variables is high."
- Why unresolved: The empirical evaluation focuses on problems where the effective dimension (d_e) is significantly smaller than the ambient dimension (D), leaving the high-valid-variable regime untested.
- What evidence would resolve it: Empirical benchmarks on synthetic or real-world functions where the ratio of important variables to total dimensions is high.

### Open Question 2
- Question: Can the theoretical guarantees of sublinear regret be maintained when the regularity assumption that f is a Gaussian process sample path does not hold?
- Basis in paper: [explicit] Section 14 notes that theoretical guarantees rely on the assumption that f is a GP sample, which "may not be true in some problems."
- Why unresolved: The theoretical analysis (specifically Assumption 4.1 and Theorem 4.2) relies strictly on this regularity condition without discussing robustness to model misspecification.
- What evidence would resolve it: A regret analysis under model misspecification or experiments on highly non-stationary/discontinuous objective functions.

### Open Question 3
- Question: Would an adaptive or more sophisticated threshold for variable selection improve performance over the "simple" average-value approach described?
- Basis in paper: [inferred] Section 3.1 mentions the "simple yet efficient approach" of classifying variables based on whether their inverse length scale exceeds the average value.
- Why unresolved: The paper employs a heuristic threshold without exploring if alternative classification strategies could tighten the regret bound or accelerate convergence.
- What evidence would resolve it: An ablation study comparing the average-value threshold against adaptive thresholds or top-k selection strategies.

## Limitations

- The theoretical guarantees rely heavily on the low effective dimensionality assumption, which may not hold for many real-world problems
- The choice of λ=10⁻³ is heuristic and may not generalize across different function classes
- The method's performance depends on sufficient initial samples (30+ recommended) before variable selection stabilizes

## Confidence

- **High:** The mechanism connecting inverse length scales to variable importance (Theorem 3.1) and the multi-subspace search construction are well-supported by the theoretical analysis and experimental evidence
- **Medium:** The empirical superiority over state-of-the-art methods (SAASBO, TuRBO) is demonstrated across multiple benchmarks, but the paper doesn't provide extensive ablation studies on hyperparameter sensitivity beyond λ and M_t
- **Medium:** While the sublinear regret bound is theoretically proven, its practical tightness and dependence on specific function characteristics (smoothness, sparsity) remain unclear

## Next Checks

1. **Ablation on Initialization Size:** Run LassoBO with varying initial sample sizes (10, 30, 50) on the Levy function to verify the claim that "large number of observations relative to function's dimensionality" is necessary for accurate variable selection
2. **Hyperparameter Sensitivity Analysis:** Systematically vary λ across 5 orders of magnitude (10⁻⁵ to 10⁰) on a synthetic benchmark with known ground truth to map out the selection stability landscape
3. **Non-stationary Function Test:** Evaluate LassoBO on a function where important variables change over iterations (e.g., piecewise function with different active sets in different regions) to assess whether the method can adapt to dynamic importance patterns