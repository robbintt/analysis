---
ver: rpa2
title: 'OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated
  Image-to-Image Translation'
arxiv_id: '2511.11162'
source_url: https://arxiv.org/abs/2511.11162
tags:
- translation
- image
- diffusion
- ot-ald
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses low efficiency and trajectory deviations in
  dual diffusion implicit bridge (DDIB) methods for image-to-image translation, caused
  by mismatched latent distributions between source and target domains. The authors
  propose OT-ALD, which uses optimal transport (OT) to align latent distributions
  by computing an OT map from the source to the target domain and using it as the
  starting point for the target domain's reverse diffusion process.
---

# OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation

## Quick Facts
- arXiv ID: 2511.11162
- Source URL: https://arxiv.org/abs/2511.11162
- Reference count: 40
- Key outcome: OT-ALD improves sampling efficiency by 20.29% and reduces FID score by 2.6 on average compared to the best baseline model for image-to-image translation.

## Executive Summary
This paper addresses low efficiency and trajectory deviations in dual diffusion implicit bridge (DDIB) methods for image-to-image translation, caused by mismatched latent distributions between source and target domains. The authors propose OT-ALD, which uses optimal transport (OT) to align latent distributions by computing an OT map from the source to the target domain and using it as the starting point for the target domain's reverse diffusion process. The approach eliminates the theoretical gap in DDIB-based methods while preserving cycle consistency and flexibility. Experiments on four translation tasks across three datasets show OT-ALD improves sampling efficiency by 20.29% and reduces FID score by 2.6 on average compared to the best baseline model.

## Method Summary
OT-ALD addresses latent distribution mismatch in DDIB-based image-to-image translation by computing an optimal transport (OT) map between source and target domain latents at a fixed diffusion time T. The method trains two independent diffusion models (DM^A and DM^B) for source and target domains, then samples latent distributions at time T from both. Using semi-discrete OT, it computes a Brenier potential that defines the OT map, which is applied to align the source latent distribution to the target's. The mapped latent then serves as the starting point for the target domain's reverse diffusion process. The approach preserves cycle consistency when noise scaling η=0 and reduces sensitivity to termination time T, enabling faster sampling while maintaining quality.

## Key Results
- OT-ALD achieves 20.29% average speedup compared to DDIB baselines across translation tasks
- OT-ALD reduces FID score by 2.6 on average compared to the best baseline model
- For Cat→Dog translation, OT-ALD achieves 45.15 s/image vs DDIB's 92.30 s/image with better FID

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finite-step diffusion termination creates a latent distribution mismatch that bounds translation accuracy.
- Mechanism: The contraction property of SDEs exponentially shrinks W₂(p^T_A, p^T_B) toward zero as T→∞, but practical finite T leaves W₂(p^T_A, p^T_B) ≠ 0. Theorem 1 shows this mismatch propagates to the output: W₂(p^B_0, q^B_0) ≤ I^B(T) · W₂(p^T_A, p^T_B), where I^B(T) grows with T. This creates a theoretical gap even with perfect score matching.
- Core assumption: Assumption 5-9 hold, including Lipschitz continuity of drift and score networks, and exponential tail decay of distributions.
- Evidence anchors:
  - [section] Theorem 1 and Figure 1(b) demonstrate the relationship between latent mismatch and output error bounds.
  - [section] Page 4 explicitly states: "whenever W₂(p^T_A, p^T_B) ≠ 0 holds, W₂(p^B_0, q^B_0) ≠ 0 must follow."
  - [corpus] Weak direct evidence; corpus focuses on OT applications rather than DDIB limitations.
- Break condition: If infinite diffusion steps were feasible, or if both domains shared identical training data distributions, this mechanism would not apply.

### Mechanism 2
- Claim: An OT map computed between latent distributions eliminates the mismatch, providing a corrected starting point for reverse diffusion.
- Mechanism: Given latent samples X^T_A and X^T_B from forward diffusion, compute Brenier potential u_h via semi-discrete OT (Eq. 7-8). The gradient ∇u_h gives the OT map M^{ot,T}_{A→B}. Using q^T_B = M^{ot,T}_{A→B}(p^T_A) as the initial distribution for DM^B's reverse process ensures q^T_B = p^T_B. Theorem 2 bounds the residual error by score matching error and OT approximation error.
- Core assumption: The target latent distribution can be approximated by discrete samples; the convex energy E(h) can be optimized to sufficient precision.
- Evidence anchors:
  - [abstract] "compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point."
  - [section] Algorithm 1 details the OT computation; Theorem 2 provides error bounds incorporating ∥u^{A→B}_T - u_h∥_{∞}^{1/2}.
  - [corpus] Paper 37102 and 84489 discuss OT for domain translation but not specifically latent alignment in diffusion.
- Break condition: If the number of target samples |I| is too small, or if optimization fails to converge (E(h) > τ), the OT approximation degrades.

### Mechanism 3
- Claim: Aligned latent distributions reduce sensitivity to diffusion termination time T, enabling faster sampling.
- Mechanism: DDIB requires large T to naturally shrink W₂(p^T_A, p^T_B) toward zero, incurring computational cost. OT-ALD explicitly aligns distributions, so smaller T suffices. Figure 1(c) and Table 3 show OT-ALD (T=500) achieves lower FID than baselines with T=1000, with 20.29% average speedup.
- Core assumption: The trade-off between T and output quality/diversity can be controlled; too few steps still harm quality regardless of alignment.
- Evidence anchors:
  - [section] Figure 3(b) demonstrates T's effect on fidelity vs. diversity.
  - [section] Table 3: OT-ALD achieves 45.15 s/image (Cat→Dog) vs. DDIB's 92.30 s/image, with better FID.
  - [corpus] Paper 61232 connects Schrödinger bridges (related to OT) to diffusion, supporting the stochastic dynamics angle.
- Break condition: Extremely small T (<100) may still produce artifacts; η=0 is required for sample-level cycle consistency (Theorem 3).

## Foundational Learning

- Concept: **Wasserstein Distance (W₂)**
  - Why needed here: Core metric quantifying distribution mismatch; Theorems 1-2 and all error bounds are expressed in W₂.
  - Quick check question: Can you explain why W₂(p^T_A, p^T_B) ≠ 0 at finite T implies translation error, even with perfect score networks?

- Concept: **Probability Flow ODE (PF-ODE)**
  - Why needed here: Diffusion processes are analyzed as deterministic ODEs with matching marginals; sample cycle consistency (Theorem 3) relies on PF-ODE invertibility.
  - Quick check question: How does PF-ODE enable cycle consistency when η=0, and why does stochastic sampling (η>0) break sample-level reversibility?

- Concept: **Brenier Potential and Semi-Discrete OT**
  - Why needed here: The OT map is computed as ∇u_h where u_h is the Brenier potential (Eq. 7), optimized via convex energy E(h) (Eq. 8).
  - Quick check question: Given target samples Y = {y_i}, can you sketch how the height vector h parameterizes the Brenier potential and how power cells W_i relate to the OT map?

## Architecture Onboarding

- Component map:
  - DM^A, DM^B: Independently trained diffusion models (score networks S^A_θ, S^B_θ) for source and target domains
  - OT Module: Computes M^{ot,T}_{A→B} via Algorithm 1 using latent samples from both DMs
  - Solver_η: Unified sampler (Eq. 4) supporting both forward and reverse processes with noise scaling η
  - Translation Pipeline: x^A_0 → Solver_η (forward, DM^A) → x^T_A → M^{ot,T}_{A→B} → x^T_B → Solver_η (reverse, DM^B) → x^B_0

- Critical path:
  1. Train DM^A and DM^B independently on source and target datasets
  2. For OT computation: sample X^T_A and X^T_B via forward diffusion, then optimize h using Adam on E(h) until convergence (Algorithm 1)
  3. At inference: run Algorithm 2 per image. OT map is precomputed; only forward/reverse diffusion runs online

- Design tradeoffs:
  - **T vs. Efficiency/Quality**: Smaller T speeds up translation but may reduce diversity. Paper uses T=500 as default; ablation (Figure 3b) shows the spectrum
  - **η vs. Determinism/Diversity**: η→0 yields faster, more deterministic outputs; η→1 increases diversity (Figure 3a). Sample cycle consistency requires η=0
  - **OT Sample Size |I|**: Larger |I| improves OT accuracy but increases precomputation cost. Paper uses N_mc = 10 × |I| Monte Carlo samples

- Failure signatures:
  - **OT non-convergence**: If E(h) doesn't decrease over 50 iterations, the paper doubles N_mc and reduces learning rate by 0.8×. Persistent non-convergence suggests insufficient or non-representative target samples
  - **Excessive artifacts at low T**: If T < 200, even with OT alignment, outputs may lack detail. Increase T or η
  - **Cycle inconsistency with η>0**: Expected behavior; only distribution-level consistency (Theorem 4) holds, not sample-level

- First 3 experiments:
  1. **Validate latent alignment**: For a fixed T (e.g., 500), compute W₂(p^T_A, p^T_B) for DDIB vs. W₂(M^{ot,T}_{A→B}(p^T_A), p^T_B) for OT-ALD using sampled latents. Expect near-zero for OT-ALD
  2. **Ablate T**: Run OT-ALD with T ∈ {100, 250, 500, 750, 1000} on one task (e.g., Cat→Dog). Plot FID and translation time vs. T. Confirm reduced T sensitivity compared to DDIB baseline
  3. **Test cycle consistency**: For η=0, translate A→B→A and measure ∥x^A_0 - x'^A_0∥₂. For η>0, measure distribution-level W₂(p^A_0, p'^A_0) empirically. Compare against DDIB

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical impact of latent mismatch on translation quality is inferred rather than directly measured across all settings
- OT approximation fidelity depends on sample size and may degrade in high-dimensional latent spaces
- Performance on complex, multimodal, or real-world domains remains untested

## Confidence

- **High confidence**: The mechanism linking finite-step diffusion to latent distribution mismatch (Mechanism 1) is mathematically rigorous and well-supported by Theorem 1 and the contraction property of SDEs
- **Medium confidence**: The OT alignment approach (Mechanism 2) is theoretically sound, but practical implementation details (sample size, optimization convergence) introduce variability not fully characterized in the paper
- **Medium confidence**: Efficiency gains (Mechanism 3) are demonstrated empirically with clear quantitative improvements (20.29% speedup, 2.6 FID reduction), but the extent of T reduction possible without quality loss varies by dataset and task

## Next Checks

1. **Latent alignment verification**: For each translation task, empirically measure W₂(p^T_A, p^T_B) for DDIB vs. W₂(M^{ot,T}_{A→B}(p^T_A), p^T_B) across multiple T values to confirm the OT map effectively eliminates the mismatch

2. **Ablation on OT parameters**: Systematically vary the number of target samples |I| (e.g., 100, 500, 1000) and monitor OT optimization convergence (E(h) vs. iterations) and final translation quality to identify when OT approximation breaks down

3. **Cycle consistency stress test**: For η=0, measure sample-level cycle consistency (||x^A_0 - x'^A_0||₂) across all tasks. For η>0, empirically estimate distribution-level W₂(p^A_0, p'^A_0) and compare against the theoretical expectation that only distribution-level consistency holds