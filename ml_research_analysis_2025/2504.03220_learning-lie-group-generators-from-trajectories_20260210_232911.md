---
ver: rpa2
title: Learning Lie Group Generators from Trajectories
arxiv_id: '2504.03220'
source_url: https://arxiv.org/abs/2504.03220
tags:
- group
- trajectory
- generator
- algebra
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inverse problem of recovering Lie algebra
  generators from sampled trajectories on matrix Lie groups. The method learns a mapping
  from sequences of normalized logarithmic displacements (computed from consecutive
  group elements) to the constant generator that produces the observed flow.
---

# Learning Lie Group Generators from Trajectories

## Quick Facts
- **arXiv ID:** 2504.03220
- **Source URL:** https://arxiv.org/abs/2504.03220
- **Reference count:** 1
- **Primary result:** Learning Lie algebra generators from trajectories via neural regression on normalized logarithmic displacements

## Executive Summary
This paper presents a method to recover Lie algebra generators from sampled trajectories on matrix Lie groups. The approach uses a simple feedforward neural network trained on sequences of normalized logarithmic displacements computed from consecutive group elements. The method is tested on synthetic data across multiple Lie groups including SE(2), SE(3), SO(3), and SL(2,R), achieving low reconstruction error consistently below 0.03 across dimensions. The results demonstrate that shallow neural architectures can effectively approximate the inverse exponential map for constant generators, with robustness to moderate noise and geometric complexity.

## Method Summary
The method learns a mapping from sequences of normalized logarithmic displacements (computed as $\delta_t = \log(g_t^{-1} g_{t+1})$) to the constant generator that produces the observed flow. A feedforward neural network takes as input flattened sequences of these normalized increments and outputs a vector in the Lie algebra. The architecture consists of two hidden layers with ReLU activation, with input dimension depending on trajectory length and group dimension. Training uses synthetic data generated by the exponential map, with noise injection to test robustness. The approach offloads the complex geometric computations to the analytic logarithm map, converting the problem into standard vector regression.

## Key Results
- Achieves reconstruction error consistently below 0.03 across tested Lie groups
- Demonstrates robustness to moderate noise levels in trajectory data
- Validates feasibility of data-driven generator recovery using shallow architectures
- Shows successful recovery across compact (SO(3), SE(3)) and non-compact (SL(2,R)) groups

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalized logarithmic displacements serve as a pre-linearized feature space, converting a complex inverse problem on manifolds into a tractable vector regression task.
- **Mechanism:** The method preprocesses trajectories by computing $\delta_t = \log(g_t^{-1} g_{t+1})$. For a constant generator flow, $\delta_t \approx \Delta t \cdot \xi$. By applying the matrix logarithm analytically before the neural network, the curvature of the Lie group is "unwrapped" into the tangent space (Lie algebra), allowing a standard MLP to approximate the mapping without learning the intrinsic geometry from scratch.
- **Core assumption:** The discrete trajectory locally approximates the continuous exponential flow such that the logarithm map is well-defined and numerically stable for adjacent steps.
- **Evidence anchors:** [abstract] ("regression from normalized sequences of discrete Lie algebra increments"), [section 3.2] ("represents frame-to-frame discrete displacements... zero-centered, unit-scaled"), [corpus] ["Neural Inertial Odometry from Lie Events"] (supports validity of Lie algebra representations for learning)
- **Break condition:** Fails if step sizes $\Delta t$ are so large that $g_t^{-1}g_{t+1}$ leaves the injectivity radius of the exponential map, causing logarithm ambiguities.

### Mechanism 2
- **Claim:** Temporal aggregation of displacement sequences provides implicit denoising, enabling recovery of the latent generator even when individual steps are perturbed.
- **Mechanism:** The network input is a flattened sequence $[\hat{\delta}_0, \dots, \hat{\delta}_{T-1}]$ rather than a single step. Under the noise model where $g_{t+1}$ is perturbed, the errors in individual $\delta_t$ are zero-mean. The MLP learns to weight these vectors (effectively smoothing them) to estimate the constant underlying $\xi$, suppressing Gaussian noise through redundant observations.
- **Core assumption:** Noise is zero-mean and independent across timesteps, and the generator $\xi$ remains constant throughout the trajectory.
- **Evidence anchors:** [section 3.1] ("Noise injection... perturbs the motion while preserving its underlying structure"), [section 5] ("effectiveness of temporal aggregation... as input features"), [corpus] Corpus evidence is weak for specific denoising bounds in this context; neighbors focus on generative/control dynamics.
- **Break condition:** Fails if the system exhibits time-varying generators (non-stationary dynamics) or systematic bias (drift), as the network expects a single constant output.

### Mechanism 3
- **Claim:** The inverse exponential map for constant generators lies in a low-complexity function class, solvable by shallow architectures without explicit inductive biases for group structure.
- **Mechanism:** Because the input features ($\delta_t$) are already Lie algebra elements, the target $\xi$ is effectively a scaled "mean" of the inputs. This linear or near-linear relationship (in the algebra) means deep recurrence or geometric convolutions are unnecessary; a simple feedforward structure suffices to approximate the mapping.
- **Core assumption:** The mapping from normalized increments to the generator does not require hierarchical feature extraction.
- **Evidence anchors:** [section 3.3] ("architecture is intentionally simple... no recurrence, convolution, or explicit sequence bias"), [section 5] ("inversion problem lies within a relatively low-complexity regime"), [corpus] ["Riemannian Direct Trajectory Optimization"] (implies Lie group structures often simplify optimization)
- **Break condition:** Complexity may increase sharply for non-compact or sensitive groups like SL(2,$\mathbb{R}$) where numerical stability is required.

## Foundational Learning

- **Concept: Matrix Lie Groups and Algebras**
  - **Why needed here:** The entire framework relies on distinguishing the group $G$ (the manifold of poses/matrices) from the algebra $\mathfrak{g}$ (the tangent space of velocities/generators).
  - **Quick check question:** Can you explain why $\log(g_t^{-1} g_{t+1})$ returns an element in the Lie algebra rather than the group?

- **Concept: The Exponential Map**
  - **Why needed here:** The forward problem (generating data) uses $\exp(t\xi)$ to create motion; understanding this is vital to grasping why inverting it is the goal.
  - **Quick check question:** If $\xi$ is a generator, what physical or geometric quantity does $\exp(\Delta t \cdot \xi)$ represent after a small time step $\Delta t$?

- **Concept: Discretization and Numerical Stability**
  - **Why needed here:** The paper notes SL(2,$\mathbb{R}$) has "sensitive determinants" and "floating-point noise."
  - **Quick check question:** Why might a matrix logarithm calculation fail or produce inconsistent results if the step size $\Delta t$ is too large?

## Architecture Onboarding

- **Component map:**
  1. **Trajectory Generator (Data Pipe):** Samples $\xi \to$ integrates via $\exp \to$ adds noise $\to$ outputs $\{g_t\}$
  2. **Preprocessor:** Computes $\delta_t = \log(g_t^{-1}g_{t+1})$, normalizes via dataset stats ($\mu, \sigma$), flattens to vector $x$
  3. **Encoder (MLP):** Input dim = $(T-1) \times \text{dim}(\mathfrak{g})$ $\to$ 2 Hidden Layers (ReLU) $\to$ Output dim = $\text{dim}(\mathfrak{g})$
  4. **Loss:** MSE between predicted vector and ground truth $\xi$

- **Critical path:** The **Preprocessor** is the most sensitive component. Unlike standard vision or text tasks, raw inputs here are matrices. The transformation $g \to \log(g)$ must be numerically robust (handling edge cases near identity or singularities). If this step is misconfigured, the MLP receives garbage.

- **Design tradeoffs:**
  - **Analytic vs. Learned Preprocessing:** The paper offloads the "hard" geometry to the analytic log map, trading end-to-end differentiability for numerical stability and simpler network design.
  - **Sequence Flattening:** Flattening temporal indices destroys explicit temporal ordering, implying the model relies on aggregate statistics (mean/variance of the sequence) rather than specific temporal patterns.

- **Failure signatures:**
  - **High MSE on SL(2,$\mathbb{R}$):** Indicates floating point instability in the log map or determinant drift.
  - **Saturation under high noise:** If $\sigma_{noise}$ overwhelms the signal $\Delta t \cdot \xi$, the model will output the dataset mean $\mu$ (collapse).
  - **Dimension mismatch:** Group-agnostic architecture means input dim must be manually tuned per group (e.g., SE(3) is 6D, SO(3) is 3D).

- **First 3 experiments:**
  1. **Sanity Check (SE(2) Noiseless):** Implement the pipeline for SE(2). Verify that the network can overfit a tiny dataset (MSE $\approx 0$) to confirm the log/exp math is correct.
  2. **Noise Robustness (SE(3)):** Inject Gaussian noise as per Section 3.1. Plot reconstruction error vs. noise standard deviation to find the breaking point.
  3. **Generalization to SL(2,$\mathbb{R}$):** Switch the group definition. Identify if "determinant drift" occurs and apply regularization if necessary, comparing convergence speed against the compact groups (SO(3)/SE(3)).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to recover time-varying generators $\xi(t)$ rather than constant ones?
- **Basis in paper:** [explicit] The conclusion states "Future work may explore extensions to time-varying generators..."
- **Why unresolved:** The current methodology assumes a fixed $\xi$ for the entire trajectory, allowing the network to regress a single vector from an aggregated sequence. Time-varying dynamics would fundamentally change the problem structure, likely requiring sequence-to-sequence modeling rather than simple regression.
- **What evidence would resolve it:** Demonstration of a model architecture (e.g., recurrent or temporal convolutional networks) successfully recovering a generator trajectory $\xi(t)$ from observations where the underlying velocity is non-constant.

### Open Question 2
- **Question:** Can the learned mapping generalize to real-world sensor trajectories without extensive synthetic pre-training?
- **Basis in paper:** [explicit] The author invites future work on "learning directly from observed sensor trajectories in real-world systems."
- **Why unresolved:** The current results are derived entirely from synthetic data generated via the exponential map with idealized Gaussian noise. Real-world data contains complex biases, outliers, and discretization errors not captured by the current noise injection model.
- **What evidence would resolve it:** Successful reconstruction of generators from physical sensor datasets (e.g., IMU or motion capture data) using the proposed architecture, potentially comparing sim-to-real transfer performance.

### Open Question 3
- **Question:** How does the method perform under non-uniform time sampling or drift in the sampling interval $\Delta t$?
- **Basis in paper:** [inferred] The Discussion notes that "performance may degrade... under drift in $\Delta t$," acknowledging a limitation in the current fixed-step assumption.
- **Why unresolved:** The input features rely on normalized logarithmic displacements computed with a fixed $\Delta t$. If the time step varies, the direct relationship between the displacement magnitude and the generator is distorted, which the current simple MLP may fail to correct without explicit time encoding.
- **What evidence would resolve it:** An experiment evaluating reconstruction error when $\Delta t$ is treated as a random variable for each trajectory, or an architectural modification that takes $\Delta t$ as an auxiliary input.

## Limitations
- The method is restricted to constant generators, limiting applicability to non-stationary dynamics
- Performance under systematic bias or time-varying generators remains untested
- Claims about no inductive bias needed are uncertain for non-compact groups like SL(2,R)
- Results are based entirely on synthetic data without validation on real-world trajectories

## Confidence
- **High:** The mechanism works for constant generators under low-to-moderate noise, as evidenced by consistently low MSE (<0.03) across groups
- **Medium:** Claims about noise robustness and sequence aggregation, since the paper doesn't explore systematic bias or temporal correlation structures
- **Low:** The assertion that no inductive bias is needed for non-compact groups like SL(2,R), where numerical conditioning is critical and unstabilized

## Next Checks
1. Test the method on real-world trajectory data (e.g., motion capture or odometry) to verify generalization beyond synthetic flows
2. Evaluate performance under systematic bias or non-constant generators to identify breaking conditions for the "constant Î¾" assumption
3. Benchmark against end-to-end differentiable approaches to quantify the trade-off between numerical stability and gradient-based optimization