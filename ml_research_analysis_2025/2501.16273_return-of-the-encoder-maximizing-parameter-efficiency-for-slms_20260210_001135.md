---
ver: rpa2
title: 'Return of the Encoder: Maximizing Parameter Efficiency for SLMs'
arxiv_id: '2501.16273'
source_url: https://arxiv.org/abs/2501.16273
tags:
- encoder-decoder
- decoder-only
- arxiv
- encoder
- efficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Encoder-decoder architectures offer 47% lower latency and 4.7x\
  \ higher throughput than decoder-only models for small language models (\u22641B\
  \ parameters) on edge devices. The key advantage stems from one-time input processing\
  \ and separation of understanding/generation phases, making them particularly effective\
  \ for asymmetric sequence tasks."
---

# Return of the Encoder: Maximizing Parameter Efficiency for SLMs

## Quick Facts
- arXiv ID: 2501.16273
- Source URL: https://arxiv.org/abs/2501.16273
- Authors: Mohamed Elfeki; Rui Liu; Chad Voegele
- Reference count: 14
- Primary result: Encoder-decoder architectures achieve 47% lower latency and 4.7x higher throughput than decoder-only models for small language models (≤1B parameters) on edge devices

## Executive Summary
Encoder-decoder architectures offer substantial efficiency advantages over decoder-only models for small language models (≤1B parameters) deployed on edge devices. By processing input once and maintaining pre-computed encoder representations, these models achieve 47% lower first-token latency and 4.7x higher throughput while using 11-16% less memory. A novel knowledge distillation framework enables these models to learn from large decoder-only teachers, achieving up to 6 average performance points improvement across diverse tasks. The architecture also extends effectively to vision-language tasks, with encoder-decoder outperforming decoder-only by 7-11% on multimodal benchmarks while maintaining parameter efficiency.

## Method Summary
The method involves two-stage training of encoder-decoder models with 2/3 of parameters allocated to the encoder and 1/3 to the decoder. Pretraining uses span corruption (15% noise, span length k=3) on 100B tokens from FineWeb-Edu with Muon optimizer, BF16 precision, and cosine learning rate decay. Downstream finetuning employs cross-entropy loss, optionally augmented with knowledge distillation from decoder-only teachers using a sequence alignment strategy that handles architectural differences. For vision-language tasks, the framework integrates CLIP ViT-L-336px with a partitioned high-resolution plus thumbnail approach and variance-based token compression.

## Key Results
- Encoder-decoder achieves 47% lower first-token latency and 4.7x higher throughput than decoder-only on edge devices for long sequences
- 2/3-1/3 encoder-decoder parameter split consistently outperforms balanced splits across asymmetric sequence tasks
- Knowledge distillation from decoder-only teachers improves performance by up to 6 average points across benchmarks
- VL-Encoder-Decoder outperforms VL-Decoder by 7-11% on vision-language tasks while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput than decoder-only models on edge devices for sequences with long inputs.
- **Mechanism:** The encoder processes input once into fixed representations. During generation, the decoder attends to these pre-computed encoder states via cross-attention, eliminating the need to maintain and recompute KV cache for input tokens at each generation step. Decoder-only models must perform quadratic attention over concatenated input-output sequences and grow KV cache for both.
- **Core assumption:** The encoder's compressed representation preserves sufficient information for downstream generation—a bottleneck that limits scaling to hundreds of billions of parameters but provides useful inductive bias at small scales.
- **Evidence anchors:**
  - [abstract] "These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases."
  - [section 3.1] "The encoder produces token-wise representations with fixed dimensionality... enabling efficient memory utilization during generation. Furthermore, cross-attention computations leverage these pre-computed encoder representations, eliminating the need for repeated input processing."
  - [corpus] Limited direct confirmation; related work "Encoder-Decoder or Decoder-Only?" discusses architectural tradeoffs but does not replicate the specific latency/throughput measurements.
- **Break condition:** When input complexity exceeds the encoder's compression capacity (very long documents with intricate cross-sentence dependencies), or when scaling beyond ~1B parameters where the information bottleneck constrains performance gains.

### Mechanism 2
- **Claim:** Allocating 2/3 of parameters to the encoder and 1/3 to the decoder consistently outperforms balanced or reversed splits for asymmetric sequence tasks.
- **Mechanism:** Bidirectional attention in the encoder must capture complex relationships and long-range dependencies across the entire input—requiring more capacity. The decoder's autoregressive generation leverages these rich representations and needs fewer layers for effective conditional generation.
- **Core assumption:** Task asymmetry (input comprehension vs. output generation) maps to architectural asymmetry; understanding is harder than conditional synthesis.
- **Evidence anchors:**
  - [section 3.1] "We find the 2/3-1/3 configuration consistently outperforms other splits, which we attribute to the inherent asymmetry in sequence-to-sequence tasks: the encoder must capture complex bidirectional relationships and long-range dependencies in the input."
  - [Table 1] Shows 2/3-1/3 KD achieving SQuAD 0.69/0.94 vs. 1/2-1/2 at 0.60/0.91 and 1/3-2/3 at 0.62/0.93.
  - [corpus] No direct corpus validation of the 2/3-1/3 split specifically.
- **Break condition:** When tasks are symmetric (input ≈ output distributions) or when generation complexity exceeds encoding complexity (extended chain-of-thought reasoning from brief prompts).

### Mechanism 3
- **Claim:** Cross-architecture knowledge distillation enables small encoder-decoder models to transfer capabilities from large decoder-only teachers, achieving up to +6 average performance points.
- **Mechanism:** Sequence alignment strategy pads and positions tokens so teacher (decoder-only) sees [PAD]∘x∘y∘[PAD] while student encoder processes x and student decoder generates from [BOS]∘y. Logits align via offset management; loss combines reverse KL-divergence (scaled by τ²) with cross-entropy.
- **Core assumption:** On-policy distillation using student-generated outputs provides effective knowledge transfer despite architectural differences.
- **Evidence anchors:**
  - [Algorithm 1] Complete distillation process with temperature τ, loss ratio α, and logit alignment.
  - [section 5.1] "Using student generations for KD (on-policy distillation) demonstrates competitive performance while offering practical advantages: faster training times and elimination of teacher generation caching requirements."
  - [corpus] "Encoder-Decoder Gemma" mentions adapting pretrained decoder-only LLMs to encoder-decoder but does not detail cross-architecture KD mechanisms.
- **Break condition:** When student capacity is insufficient to absorb teacher knowledge, or when architectural incompatibility prevents meaningful alignment (extremely divergent attention patterns).

## Foundational Learning

- **Concept: KV Cache**
  - **Why needed here:** Understanding encoder-decoder efficiency requires grasping why decoder-only models must cache key-value pairs for all prior tokens, growing memory linearly with sequence length.
  - **Quick check question:** Why does decoder-only KV cache scale with input length while encoder-decoder's memory footprint remains fixed after encoding?

- **Concept: Cross-Attention**
  - **Why needed here:** The efficiency gain hinges on cross-attention operating on pre-computed encoder outputs rather than reprocessing input at each generation step.
  - **Quick check question:** In cross-attention, what do the queries come from and what do keys/values come from? How does this differ from self-attention?

- **Concept: On-Policy vs. Off-Policy Distillation**
  - **Why needed here:** The paper uses student-generated outputs for KD training; understanding this distinction affects implementation choices.
  - **Quick check question:** What are the tradeoffs between using teacher-generated outputs vs. student-generated outputs for distillation training?

## Architecture Onboarding

- **Component map:**
  - **Encoder (2/3 params):** Bidirectional attention, RoPE with NTK scaling, produces fixed representations; handles understanding phase.
  - **Decoder (1/3 params):** Causal self-attention + cross-attention to encoder; handles generation phase.
  - **Cross-Attention Bridge:** Links each decoder layer to encoder final-layer outputs.
  - **Vision Encoder (VL extension):** CLIP ViT-L-336px + 2-layer MLP projection; partitioned high-res + thumbnail approach.
  - **Vision Token Compression:** Variance-based selection (67% reduction, -0.012 Rouge-L tradeoff).

- **Critical path:**
  1. Pre-training: Span corruption (15% noise, k=3) on 100B tokens, BF16, Muon optimizer, cosine decay.
  2. Fine-tuning: Choose Seq2Seq (cross-entropy) OR KD from decoder-only teacher (Phi-3.5Mini 3.3B).
  3. VL extension: Feature alignment (600K pairs) → OCR (200K) → Instruction following (700K) → Task fine-tuning.

- **Design tradeoffs:**
  - **Encoder/Decoder split:** 2/3-1/3 favors asymmetric tasks; 1/2-1/2 balanced; 1/3-2/3 underperforms.
  - **Vision token compression:** Variance-based (-0.012 Rouge-L, efficient) vs. learned weighting (-0.072, unreliable).
  - **KD mixing ratio (α):** Task-specific; SQuAD favors lower α (more Seq2Seq), other tasks benefit higher α.

- **Failure signatures:**
  - 1/3-2/3 split: Encoder under-capacity → poor input comprehension on long documents.
  - Learned token weighting: Training instability, degraded VL performance.
  - Decoder-only with KD at 1B: Underperforms even base encoder-decoder without KD (0.37 vs. 0.39).

- **First 3 experiments:**
  1. **Encoder/Decoder split ablation:** Train 330M models at 1/3-2/3, 1/2-1/2, 2/3-1/3 on SQuAD; validate 2/3-1/3 achieves highest Rouge-L (>0.67).
  2. **Efficiency measurement:** Profile 330M encoder-decoder vs. decoder-only on 4096-token inputs, batch 32; confirm 11-16% memory reduction and ~22% FLOP reduction.
  3. **Cross-architecture KD validation:** Distill 330M encoder-decoder from Phi-3.5Mini decoder-only teacher; expect +7% improvement over Seq2Seq baseline at 330M scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what precise model scale does the encoder-decoder information bottleneck become prohibitive, causing decoder-only architectures to become more advantageous?
- Basis in paper: [explicit] "While we limit our experiments to 1B scale, understanding the transition point where encoder-decoder advantages diminish could provide valuable insights into fundamental architectural trade-offs. We encourage the research community to investigate these limits at larger scales."
- Why unresolved: The authors only evaluated models up to 1B parameters; larger scales require substantially more computational resources and were outside the study's scope.
- What evidence would resolve it: Systematic comparison of encoder-decoder vs decoder-only architectures across scales from 1B to 100B+ parameters on standardized benchmarks.

### Open Question 2
- Question: What novel mechanisms for information flow between encoders and decoders (e.g., residual connections) could overcome scaling limitations while preserving efficiency benefits?
- Basis in paper: [explicit] "We should explore novel mechanisms for information flow between encoders and decoders, such as residual connections, to overcome scaling limitations."
- Why unresolved: The paper identifies this as a critical area requiring further investigation but does not implement or test any such mechanisms.
- What evidence would resolve it: Architectural ablations testing skip connections, dense cross-attention variants, or hybrid approaches across multiple scales.

### Open Question 3
- Question: Why do encoder-decoder advantages emerge primarily during post-training rather than pretraining, where all variants show comparable perplexity?
- Basis in paper: [inferred] The paper reports that "encoder-decoder advantages over decoder-only primarily emerge during post-training rather than pretraining," with pretraining showing no statistically significant differences in perplexity or benchmark performance.
- Why unresolved: The mechanism behind this post-training emergence is discussed but not empirically isolated or explained.
- What evidence would resolve it: Layer-wise analysis of representation dynamics during pretraining vs fine-tuning, probing task-specific vs general capabilities.

### Open Question 4
- Question: Can specialized cross-architecture knowledge distillation techniques be developed that combine scalable decoder-only training benefits with encoder-decoder inference efficiency?
- Basis in paper: [explicit] "Additionally, developing specialized knowledge distillation techniques could allow us to combine the benefits of scalable decoder-only training with the efficient inference offered by encoder-decoder architectures."
- Why unresolved: The current KD framework is preliminary; the authors note this as a direction requiring dedicated investigation beyond their current approach.
- What evidence would resolve it: Novel distillation methods evaluated against the current reverse-KL approach, measuring both transfer quality and retained inference efficiency.

## Limitations

- Efficiency claims depend heavily on specific hardware configurations and may not generalize across all edge deployment scenarios
- Knowledge distillation framework requires careful hyperparameter tuning that isn't fully specified
- Vision-language extension uses complex multi-stage training pipeline that may not scale well to other multimodal domains
- Architectural advantages appear most pronounced at 1B parameter scale, with scaling beyond this encountering potential bottlenecks
- Asymmetric task assumption underlying parameter split may not hold for all applications

## Confidence

**High Confidence Claims:**
- Encoder-decoder architectures provide measurable efficiency improvements (lower latency, higher throughput) on edge devices for small models
- The 2/3-1/3 parameter split consistently outperforms other configurations on asymmetric sequence tasks
- Knowledge distillation from decoder-only teachers improves encoder-decoder performance across multiple benchmarks

**Medium Confidence Claims:**
- The specific magnitude of efficiency gains (47% latency reduction, 4.7x throughput) generalizes across all edge deployments
- The cross-architecture KD framework scales effectively beyond the tested parameter ranges
- Vision-language extensions maintain efficiency benefits while matching or exceeding decoder-only performance

**Low Confidence Claims:**
- The architectural advantages persist at scales significantly beyond 1B parameters
- The efficiency benefits translate equally to all edge hardware configurations
- The asymmetric task assumption (encoding harder than decoding) applies universally

## Next Checks

1. **Hardware Architecture Validation**: Reproduce the latency and throughput measurements across diverse edge hardware (mobile NPUs, embedded GPUs, CPUs) with varying batch sizes and sequence lengths to verify the 47% latency reduction claim holds under different deployment scenarios.

2. **Scaling Limit Investigation**: Systematically scale encoder-decoder models from 1B to 8B parameters while measuring both performance and efficiency metrics, specifically testing whether the information bottleneck hypothesis holds and identifying the exact point where decoder-only models overtake encoder-decoder architectures.

3. **Task Symmetry Stress Test**: Design and evaluate symmetric sequence tasks (e.g., code translation, dialogue systems with long contexts) where input and output distributions are similar, measuring whether the 2/3-1/3 split remains optimal or if alternative architectures become preferable.