---
ver: rpa2
title: 'OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence'
arxiv_id: '2601.21083'
source_url: https://arxiv.org/abs/2601.21083
tags:
- containment
- calibration
- injection
- opensec
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OpenSec is a dual-control RL environment that evaluates incident
  response agents under adversarial prompt injection scenarios. It scores world-state-changing
  containment actions via execution-based metrics: time-to-first-containment (TTFC),
  blast radius (false positives per episode), and injection violation rates.'
---

# OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence

## Quick Facts
- arXiv ID: 2601.21083
- Source URL: https://arxiv.org/abs/2601.21083
- Reference count: 9
- One-line primary result: OpenSec reveals frontier IR agents over-trigger containment actions with 90-97% false positive rates, except Claude Sonnet 4.5 which shows partial calibration

## Executive Summary
OpenSec is a dual-control reinforcement learning environment designed to evaluate incident response agent calibration under adversarial prompt injection scenarios. The framework scores world-state-changing containment actions using execution-based metrics including time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Testing four frontier models on 40 standard-tier episodes reveals consistent over-triggering: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates, while Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP). The calibration gap appears to be about restraint rather than detection capability.

## Method Summary
OpenSec employs a dual-control RL environment where agents must decide when to execute containment actions in response to detected threats. The environment provides evidence with varying trust tiers and includes adversarial prompt injection scenarios designed to test agent calibration. Actions are scored based on execution outcomes rather than intent, with key metrics including time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Four frontier models (GPT-5.2, Gemini 3, DeepSeek, and Claude Sonnet 4.5) were evaluated across 40 standard-tier episodes each, with ground-truth threat identification serving as the reference point for calibration assessment.

## Key Results
- All four frontier models demonstrate systematic over-triggering in adversarial scenarios
- GPT-5.2, Gemini 3, and DeepSeek achieve 100% containment rates with 90-97% false positive rates
- Claude Sonnet 4.5 shows partial calibration with 85% containment and 72% false positive rate
- Models correctly identify ground-truth threats when acting, indicating the calibration gap is about restraint not detection
- EGAR and TTFC metrics successfully quantify calibration variation between models

## Why This Works (Mechanism)
OpenSec works by creating a controlled environment where agents face adversarial evidence that tests their ability to distinguish genuine threats from injected prompts. The dual-control architecture separates detection from execution, allowing precise measurement of when agents decide to act versus when they actually execute containment. By scoring actions based on execution outcomes rather than intent, the framework captures real-world consequences of miscalibration. The adversarial prompt injection serves as a stress test that reveals fundamental differences in how models balance caution against action.

## Foundational Learning
- **Containment Calibration**: Why needed - Measures the gap between threat detection and appropriate response; Quick check - Compare containment rates with ground-truth threat presence
- **Adversarial Prompt Injection**: Why needed - Tests model resistance to manipulation; Quick check - Measure false positive rates under varying injection intensities
- **Dual-Control Architecture**: Why needed - Separates decision-making from execution consequences; Quick check - Verify that execution scores differ from detection scores
- **Execution-Based Metrics**: Why needed - Captures real-world impact rather than intended behavior; Quick check - Compare intended vs. actual containment outcomes
- **Trust Tier Evidence Gating**: Why needed - Models must evaluate evidence reliability; Quick check - Test model performance across different trust tier combinations
- **Blast Radius Measurement**: Why needed - Quantifies collateral damage from over-containment; Quick check - Calculate false positives per episode as containment proxy

## Architecture Onboarding

Component Map: Evidence Processor -> Decision Module -> Action Executor -> Outcome Evaluator -> Reward Calculator

Critical Path: Evidence with trust tiers → Decision module evaluates containment necessity → Action executor attempts containment → Outcome evaluator measures TTFC and false positives → Reward calculator provides feedback for RL training

Design Tradeoffs: The framework trades computational complexity for precise measurement of real-world consequences. Using execution-based scoring rather than intent-based scoring provides more realistic evaluation but requires simulating full containment scenarios. The adversarial injection approach effectively reveals calibration issues but may not represent all real-world threat scenarios.

Failure Signatures: Over-triggering manifests as high containment rates with low precision scores, while under-triggering appears as low containment rates with high false negative rates. Model-specific failure patterns may indicate differences in training data, alignment procedures, or architectural biases toward action versus inaction.

First Experiments:
1. Run ablation tests with symmetric vs. asymmetric penalty structures to measure impact on containment behavior
2. Test model performance across different trust tier combinations to analyze evidence provenance sensitivity
3. Implement curriculum learning staging to evaluate whether gradual difficulty increases improve calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training methodologies or alignment approaches cause calibration differences between frontier models (e.g., Sonnet 4.5 vs. GPT-5.2)?
- Basis in paper: [explicit] "Why Sonnet and not GPT-5.2, Gemini, or DeepSeek? This is an open question, but the EGAR and TTFC metrics make the variation precisely measurable."
- Why unresolved: The paper measures calibration variation but cannot determine causal factors from evaluation alone.
- What evidence would resolve it: Comparative analysis of training corpora, alignment techniques, and safety fine-tuning procedures across frontier model families.

### Open Question 2
- Question: What training pipeline (SFT warmup, curriculum staging, verification gates) most effectively improves IR agent calibration?
- Basis in paper: [explicit] "Direct RL from multi-component reward is insufficient. Likely improvements: SFT warmup on successful trajectories, curriculum staging, explicit verification gates."
- Why unresolved: Preliminary RL training produced modified but not clearly improved calibration (70% FP vs. Sonnet's 45%).
- What evidence would resolve it: Ablation studies comparing SFT+RL, curriculum learning, and verification-gated approaches on the OpenSec environment.

### Open Question 3
- Question: How does symmetric versus asymmetric reward penalty structure affect agent containment behavior?
- Basis in paper: [explicit] "We did not ablate the penalty asymmetry in this work; symmetric penalties are a natural ablation for future runs."
- Why unresolved: The current design penalizes false positives more than inaction, but the effect of this asymmetry on learned behavior is untested.
- What evidence would resolve it: Controlled experiments comparing agents trained with symmetric vs. asymmetric penalty schemes on identical scenarios.

### Open Question 4
- Question: How does model behavior change as a function of evidence provenance quality (trust tiers)?
- Basis in paper: [explicit] "While EGAR uses trust tiers for evidence gating, analyzing model behavior as a function of evidence provenance quality remains future work."
- Why unresolved: Trust tier metadata exists but has not been analyzed as an independent variable affecting decision-making.
- What evidence would resolve it: Stratified evaluation across trust tier conditions with statistical analysis of containment decisions per provenance level.

## Limitations
- Evaluation uses fixed 40-episode standard-tier set, raising questions about statistical power and generalizability
- No ablation studies varying episode difficulty or injection intensity to isolate causes of over-triggering
- Claims about ground-truth detection accuracy when acting are not independently verified in summary
- Absence of confidence intervals or significance testing for metrics makes it difficult to assess measurement noise
- Single-containment-metric design oversimplifies complex incident response scenarios

## Confidence
- **High**: Observation of systematic over-triggering across frontier models
- **Medium**: Calibration gap interpretation (detection vs. restraint)
- **Low**: Claims about ground-truth detection accuracy when acting

## Next Checks
1. Run statistical power analysis with varying episode counts (10-200) to establish confidence intervals for FP rates and TTFC across models
2. Conduct ablation testing with different adversarial prompt intensities to isolate whether over-triggering is prompt-dependent or model-inherent
3. Implement multi-metric evaluation capturing containment quality (precision, recall, F1) beyond binary action/non-action decisions