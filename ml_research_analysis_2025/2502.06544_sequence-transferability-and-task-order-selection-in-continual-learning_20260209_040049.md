---
ver: rpa2
title: Sequence Transferability and Task Order Selection in Continual Learning
arxiv_id: '2502.06544'
source_url: https://arxiv.org/abs/2502.06544
tags:
- task
- learning
- transferability
- tasks
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding how task sequence
  characteristics affect continual learning (CL) performance and developing methods
  for task order selection. The authors propose two novel sequence transferability
  measures, Total Forward Transferability (TFT) and Total Reverse Transferability
  (TRT), which quantify the transferability of CL algorithms across task sequences
  using conventional transferability metrics.
---

# Sequence Transferability and Task Order Selection in Continual Learning

## Quick Facts
- arXiv ID: 2502.06544
- Source URL: https://arxiv.org/abs/2502.06544
- Reference count: 8
- Key outcome: This paper addresses the problem of understanding how task sequence characteristics affect continual learning (CL) performance and developing methods for task order selection.

## Executive Summary
This paper addresses the problem of understanding how task sequence characteristics affect continual learning (CL) performance and developing methods for task order selection. The authors propose two novel sequence transferability measures, Total Forward Transferability (TFT) and Total Reverse Transferability (TRT), which quantify the transferability of CL algorithms across task sequences using conventional transferability metrics. They show empirically that both measures correlate well with average accuracy across multiple CL algorithms and datasets. Based on these findings, they develop the Heuristic Continual Task Order Selection (HCTOS) algorithm for selecting optimal task orders in practical CL scenarios where tasks arrive in batches.

## Method Summary
The authors propose two sequence transferability measures (TFT and TRT) that aggregate pairwise transferability scores across task sequences, then use these to develop HCTOS for selecting task orders in continual learning. For each batch of tasks, HCTOS trains lightweight models on small subsets, computes pairwise transferability matrices, and greedily selects tasks with minimum total outgoing transferability. The approach is evaluated on Split CIFAR-100 and Split tiny-ImageNet using replay-based CL algorithms (A-GEM, ER, DER++, X-DER) with buffer sizes ranging from 360 to 3600.

## Key Results
- TFT and TRT measures show moderate-to-high Pearson correlations (>70% of settings) with average accuracy across multiple CL algorithms and datasets
- HCTOS outperforms random task selection in both single-batch and multi-batch settings
- HCTOS demonstrates robustness to sample size and choice of base transferability metric (LogME, LEEP, GBC, TransRate)
- The hard-to-easy task ordering heuristic leads to improved average accuracy on standard benchmarks

## Why This Works (Mechanism)

### Mechanism 1
TFT and TRT serve as predictive proxies for continual learning average accuracy by aggregating pairwise transferability scores across forward task transitions (TFT) and backward from final model to all prior tasks (TRT). TFT captures knowledge leverage from prior-to-current tasks; TRT captures retention/forgetting via final-model-to-earlier-task adaptability. The core assumption is that base transferability metrics (e.g., LogME) approximate true transfer effectiveness sufficiently well, and linear aggregation across transitions preserves meaningful signal.

### Mechanism 2
Selecting tasks that are "hardest to transfer from" (lowest outgoing transferability) early leads to higher subsequent transferability and improved sequence-level performance. HCTOS picks, at each iteration, the task minimizing total transferability to all remaining tasks. The intuition is that pre-training on "hard" sources tends to yield higher transfer to "easy" targets, and harder-first ordering improves downstream knowledge flow. This relies on the assumption that hard-to-easy transfer directionality holds across the task distribution.

### Mechanism 3
HCTOS is robust to limited data and choice of base transferability metric because it uses a lightweight model trained on small subsets to compute pairwise transferability, and the relative ordering of total outgoing scores is more stable than absolute values. This reduces sensitivity to metric and sample size variations. The ranking stability holds despite reduced data and metric variation because the heuristic (minimum outgoing transfer) is ordering-dominant rather than score-magnitude-dominant.

## Foundational Learning

- **Transferability metrics in transfer learning (LEEP, LogME, TransRate)**: Why needed here: They serve as the base component in TFT/TRT; without understanding how they estimate source→target transfer, you cannot interpret TFT/TRT values or debug failures. Quick check: Can you explain why LogME uses log marginal likelihood of labels given features rather than raw accuracy?

- **Forward transfer vs. backward transfer (and forgetting) in continual learning**: Why needed here: TFT operationalizes forward transfer; TRT operationalizes backward/forgetting behavior. Distinguishing them is essential to reason about stability–plasticity tradeoffs. Quick check: Given a 5-task sequence, how would you manually compute TFT and TRT from pairwise transferability scores?

- **Replay-based CL strategies and buffer size effects**: Why needed here: Empirical validation of TFT/TRT uses A-GEM, ER, DER++, X-DER; buffer size affects both accuracy and the correlation structure with TFT/TRT. Quick check: Why might TRT correlate more strongly with average accuracy than TFT in replay methods?

## Architecture Onboarding

- **Component map**: Pairwise transferability estimator (base metric; default LogME) -> Simple model trainer (lightweight ResNet on subset of each task) -> TFT/TRT aggregator (mean over forward or reverse pairs) -> HCTOS scheduler (per-batch greedy task picker using minimum outgoing transfer)

- **Critical path**: 1. For each new batch, train simple models on small subsets per task. 2. Compute all-pairs transferability matrix using the chosen base metric. 3. Greedily select task with minimum total outgoing transferability; append to ordered sequence; repeat until batch exhausted. 4. Feed ordered tasks to the main CL algorithm.

- **Design tradeoffs**: Base metric choice: LogME generally best but slower than simpler metrics; TransRate faster but potentially less stable. Sample size per task: Fewer samples speed up ordering but risk noisier estimates; paper shows reasonable robustness down to very small subsets. Simple model capacity vs. main CL model: Mismatch may cause ordering derived on proxy model to misalign with actual CL model dynamics.

- **Failure signatures**: TFT/TRT correlation with accuracy drops close to zero → base metric may be unsuitable for domain or input-space assumption violated. HCTOS underperforms random → pairwise estimates likely too noisy; increase per-class samples or switch base metric. Large performance variance across random seeds → ordering signal weak; consider ensembling base metrics.

- **First 3 experiments**: 1. Replicate TFT/TRT–accuracy correlation on Split CIFAR-100 with DER++ across buffer sizes [360, 900, 1800]; log Pearson correlations per setting. 2. Ablate sample size (1, 5, 10, 20 samples per class) for simple model training; compare HCTOS ordering quality vs. random baseline. 3. Swap base metric (LogME → LEEP, GBC, TransRate) on a fixed 4-batch setting; report average accuracy and robustness to metric choice.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can sequence transferability measures be developed that do not require running the actual CL algorithm to compute, similar to how traditional transferability measures work in transfer learning? Current TFT and TRT require training the CL model first, unlike traditional transferability metrics which can estimate transfer efficacy without running transfer learning.

- **Open Question 2**: How does HCTOS compare to more sophisticated task ordering strategies beyond random selection? The paper only compares HCTOS against random task ordering, but does not compare against other principled ordering methods that may exist in the literature.

- **Open Question 3**: Can the empirical correlation between TFT/TRT and average accuracy be explained theoretically rather than just observationally? The paper establishes correlations empirically but does not provide theoretical justification for why these measures should predict CL performance.

- **Open Question 4**: Does the HCTOS approach generalize effectively to non-replay-based CL methods and to modalities beyond image classification? Experiments focus primarily on replay-based methods and all benchmarks are image classification tasks.

## Limitations

- External validity: TFT/TRT correlation results are reported only within this paper's experimental setup; no independent replication exists in the corpus.
- Assumption sensitivity: Both TFT/TRT and HCTOS assume the base transferability metric (LogME) accurately captures cross-task transfer, and that the simple proxy model's transferability landscape mirrors the main CL model's behavior.
- Task overlap ambiguity: While shared classes between consecutive tasks in Split mutual-CIFAR-10 are mentioned, the exact class assignments and their impact on transferability measures are not specified.

## Confidence

- **High**: HCTOS outperforms random task selection in the reported experiments; empirical correlation between TFT/TRT and average accuracy is demonstrated.
- **Medium**: The proposed transferability measures (TFT/TRT) are predictive proxies for CL performance; hard-to-easy task ordering heuristic improves downstream transferability.
- **Low**: Claims about robustness to sample size and base metric choice are supported only by within-paper experiments without external validation.

## Next Checks

1. Replicate TFT/TRT correlation: Independently reproduce the Pearson correlation between TFT/TRT and average accuracy on Split CIFAR-100 with DER++ across buffer sizes [360, 900, 1800], logging results per setting.

2. Sample size ablation: Vary simple model training samples per class (1, 5, 10, 20) and measure HCTOS ordering quality and average accuracy versus random ordering.

3. Base metric robustness: Replace LogME with LEEP, GBC, and TransRate on a fixed 4-batch setting; compare average accuracy and consistency across metrics.