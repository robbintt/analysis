---
ver: rpa2
title: 'SELF-PERCEPT: Introspection Improves Large Language Models'' Detection of
  Multi-Person Mental Manipulation in Conversations'
arxiv_id: '2505.20679'
source_url: https://arxiv.org/abs/2505.20679
tags:
- manipulation
- person
- dataset
- dialogue
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SELF-PERCEPT, a two-stage prompting framework
  inspired by Self-Perception Theory, to detect mental manipulation in multi-turn,
  multi-person conversations. The authors create MultiManip, a balanced dataset of
  220 dialogues from reality shows, covering 11 manipulation techniques.
---

# SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations
## Quick Facts
- arXiv ID: 2505.20679
- Source URL: https://arxiv.org/abs/2505.20679
- Reference count: 18
- Two-stage prompting framework inspired by Self-Perception Theory detects mental manipulation in multi-turn, multi-person conversations with improved F1-score, precision, and accuracy over baseline methods.

## Executive Summary
SELF-PERCEPT is a novel two-stage prompting framework designed to detect mental manipulation in multi-person conversations by leveraging introspection inspired by Self-Perception Theory. The framework was evaluated on a new dataset, MultiManip, comprising 220 dialogues from reality shows across 11 manipulation techniques. Results show that SELF-PERCEPT significantly outperforms baseline methods (Zero-Shot, Few-Shot, CoT) on GPT-4o and Llama-3.1-8B, achieving higher F1-scores, precision, and accuracy. The method also reduces false positives through contextual behavioral analysis and offers improved interpretability via SHAP analysis. Future work will focus on dataset expansion, robustness, and real-world deployment considerations.

## Method Summary
SELF-PERCEPT employs a two-stage prompting framework that first identifies potentially manipulative utterances within a conversation, then applies introspection to assess the contextual behavioral cues indicating mental manipulation. The approach is inspired by Self-Perception Theory, encouraging the model to reason about its own outputs and the broader conversational context. Evaluations were conducted using GPT-4o and Llama-3.1-8B on the MultiManip dataset, which contains 220 dialogues from reality shows annotated for 11 manipulation techniques. The framework demonstrated superior performance over standard prompting baselines in detecting manipulation, with notable improvements in F1-score, precision, and accuracy, and reduced false positives.

## Key Results
- SELF-PERCEPT outperforms baseline methods (Zero-Shot, Few-Shot, CoT) on GPT-4o and Llama-3.1-8B in F1-score, precision, and accuracy for manipulation detection.
- On MultiManip, GPT-4o achieves 0.42 accuracy and 0.37 F1-score, surpassing baseline methods.
- SHAP analysis reveals that SELF-PERCEPT better identifies manipulative cues and reduces false positives through contextual behavioral analysis.

## Why This Works (Mechanism)
SELF-PERCEPT leverages introspection to enhance the detection of mental manipulation by encouraging the model to reason about its own outputs and the conversational context. The two-stage prompting approach first isolates potentially manipulative utterances, then applies introspective analysis to assess behavioral cues and contextual signals. This method reduces false positives and improves interpretability by focusing on relevant features of manipulation. The inspiration from Self-Perception Theory enables the model to better understand and contextualize manipulation, leading to more accurate detection.

## Foundational Learning
- **Self-Perception Theory**: Why needed—provides a theoretical basis for introspection in decision-making. Quick check—understand how self-observation influences behavioral inference.
- **Two-Stage Prompting**: Why needed—separates detection and analysis for improved accuracy. Quick check—validate each stage independently before integration.
- **Multi-Turn Dialogue Processing**: Why needed—manipulation often unfolds over multiple conversational turns. Quick check—test robustness across varied dialogue lengths and structures.
- **SHAP Analysis**: Why needed—offers interpretability for model decisions. Quick check—verify that identified features align with known manipulation cues.
- **Dataset Annotation for Manipulation**: Why needed—ground truth for training and evaluation. Quick check—ensure annotations are balanced and representative of diverse manipulation techniques.

## Architecture Onboarding
- **Component Map**: Input Dialogue -> Stage 1 (Utterance Identification) -> Stage 2 (Introspective Analysis) -> Output (Manipulation Detection)
- **Critical Path**: The two-stage prompting pipeline, where Stage 1 filters and identifies candidate manipulative utterances, and Stage 2 applies introspective reasoning to assess context and behavioral cues.
- **Design Tradeoffs**: Prioritizing accuracy and interpretability over computational efficiency; reliance on a relatively small, genre-specific dataset may limit generalizability.
- **Failure Signatures**: High false positives if contextual cues are misinterpreted; underperformance on dialogues outside the reality show domain; potential bias from dataset composition.
- **First Experiments**:
  1. Validate Stage 1 utterance identification accuracy on a subset of MultiManip.
  2. Test Stage 2 introspective analysis independently for interpretability and correctness.
  3. Benchmark computational latency for real-time deployment feasibility.

## Open Questions the Paper Calls Out
None.

## Limitations
- The MultiManip dataset is small (220 dialogues) and sourced from reality shows, which may limit generalizability to other conversational contexts.
- Evaluations are limited to two model variants (GPT-4o, Llama-3.1-8B), leaving uncertainty about performance on other LLM architectures.
- Computational overhead and latency implications for real-time deployment are not addressed.

## Confidence
- Performance claims (F1-score, precision, accuracy): Medium
- Generalizability across conversational domains: Low
- Computational efficiency for real-time use: Low
- Robustness to diverse manipulation scenarios: Low

## Next Checks
1. **Dataset Expansion and Diversity Testing**: Validate SELF-PERCEPT on a significantly larger and more diverse corpus of multi-person conversations spanning different domains (workplace, political, social media) to assess robustness and generalizability.
2. **Real-World Deployment Simulation**: Conduct latency and computational efficiency tests in simulated real-time deployment settings to evaluate practical feasibility and identify bottlenecks in the two-stage prompting pipeline.
3. **Bias and Ethical Impact Assessment**: Perform systematic audits for demographic and cultural biases in manipulation detection, including cross-cultural validation and privacy impact analysis for sensitive conversation types.