---
ver: rpa2
title: 'InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models'
arxiv_id: '2512.17851'
source_url: https://arxiv.org/abs/2512.17851
tags:
- spatial
- diffusion
- object
- attention
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "InfSplign addresses spatial misalignment in text-to-image diffusion\
  \ models by leveraging cross-attention maps during denoising to enforce object placement\
  \ and balanced representation. It introduces a training-free inference-time method\
  \ that applies a compound loss\u2014combining spatial alignment, object presence,\
  \ and balance terms\u2014to adjust noise in every reverse diffusion step."
---

# InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2512.17851
- Source URL: https://arxiv.org/abs/2512.17851
- Reference count: 40
- Primary result: Achieves up to 24.81% improvement over inference-time baselines and surpasses fine-tuning methods by 14.33% on spatial alignment benchmarks.

## Executive Summary
InfSplign is an inference-time method for improving spatial alignment in text-to-image diffusion models. The approach leverages cross-attention maps during denoising to enforce object placement and balanced representation. By introducing a compound loss function that combines spatial alignment, object presence, and balance terms, InfSplign guides the generation process toward spatially coherent images without requiring model retraining. The method operates by partitioning decoder attention maps into hierarchical levels and using them to compute centroids and variances, which inform the spatial guidance during each reverse diffusion step.

## Method Summary
InfSplign introduces a training-free inference-time method that applies a compound loss—combining spatial alignment, object presence, and balance terms—to adjust noise in every reverse diffusion step. The method partitions decoder attention maps into hierarchical levels and uses them to compute centroids and variances, guiding the generation process toward spatially coherent images. Evaluated on VISOR and T2I-CompBench, InfSplign achieves up to 24.81% improvement over inference-time baselines and surpasses fine-tuning methods by 14.33%, demonstrating superior spatial understanding and object fidelity.

## Key Results
- Achieves up to 24.81% improvement over inference-time baselines on spatial alignment benchmarks
- Surpasses fine-tuning methods by 14.33% in spatial accuracy metrics
- Demonstrates superior spatial understanding and object fidelity in qualitative and quantitative evaluations

## Why This Works (Mechanism)
InfSplign works by leveraging cross-attention maps during the denoising process of text-to-image diffusion models. The method partitions these attention maps hierarchically and computes spatial statistics (centroids and variances) for each object mentioned in the prompt. A compound loss function then guides the noise prediction at each sampling step, enforcing spatial alignment, ensuring object presence, and maintaining balance between objects. This inference-time guidance effectively corrects spatial misalignments without requiring model retraining.

## Foundational Learning
- **Cross-attention maps**: Capture the relationship between text tokens and image regions during denoising; needed to extract spatial information about objects; quick check: verify attention maps show expected object locations
- **Denoising diffusion probabilistic models**: Generate images through iterative noise prediction and removal; needed as the base framework for inference-time guidance; quick check: confirm stable reverse diffusion process
- **Loss function compounding**: Combines multiple objectives (alignment, presence, balance) into a single gradient; needed to balance competing spatial constraints; quick check: monitor gradient magnitudes and stability
- **Hierarchical attention partitioning**: Divides attention maps into levels for more granular spatial control; needed to capture multi-scale spatial relationships; quick check: validate hierarchical structure reflects object hierarchy
- **Centroid and variance computation**: Quantifies object locations and spatial extents from attention maps; needed to define spatial targets for guidance; quick check: ensure computed statistics match visual object positions

## Architecture Onboarding

**Component Map**: Text Prompt -> Cross-Attention Maps -> Hierarchical Partitioning -> Centroid/Variance Computation -> Compound Loss -> Noise Adjustment -> Image Generation

**Critical Path**: During each reverse diffusion step, cross-attention maps are extracted, partitioned hierarchically, and used to compute spatial statistics. These statistics inform a compound loss that adjusts the predicted noise, guiding the generation toward spatially aligned output.

**Design Tradeoffs**: The method trades computational overhead during sampling for improved spatial accuracy without retraining. High guidance weight (η=1000) is required to balance the spatial loss against the standard CFG term, potentially introducing fidelity tradeoffs.

**Failure Signatures**: 
- Base model fails to generate prerequisite objects (rare/unnatural combinations)
- Attention maps do not reliably encode object locations
- High guidance weight causes artifacts or oversaturation
- Pairwise balance formulation conflicts for scenes with three+ objects

**3 First Experiments**:
1. Ablation study isolating contributions of spatial alignment, object presence, and balance loss terms
2. Evaluation of method's robustness across diverse diffusion model architectures
3. Measurement of computational overhead and analysis of unintended effects on image style/fidelity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can inference-time guidance methods effectively enforce spatial alignment when the base diffusion model fails to generate the requisite objects due to rare or unnatural object combinations?
- **Basis in paper:** The authors explicitly state in the Limitations section that for uncommon object combinations, "prerequisite object presence is impaired," causing the spatial alignment correction to fail.
- **Why unresolved:** InfSplign relies on attention maps derived from the denoising process; if the base model does not attend to the object (failure to generate), the spatial loss cannot compute meaningful gradients to guide the layout.
- **What evidence would resolve it:** A mechanism that decouples object generation from spatial alignment, or a "presence-first" guidance step that ensures attention activation for all tokens before applying spatial constraints.

### Open Question 2
- **Question:** Does the high guidance weight (η=1000) required for InfSplign induce a trade-off between spatial accuracy and general image fidelity (e.g., FID, CLIP score) or introduce artifacts?
- **Basis in paper:** The implementation details set the guidance weight η to 1000 to balance magnitudes against the standard CFG term. While spatial benchmarks improve, the paper does not report standard fidelity metrics like FID.
- **Why unresolved:** High guidance scale factors are known to cause "oversaturation" or artifacts in diffusion outputs; the impact of adding a third, heavily weighted term to the noise prediction is not quantified in terms of image quality.
- **What evidence would resolve it:** Reporting Fréchet Inception Distance (FID) and CLIP scores comparing standard SD outputs against InfSplign outputs on a general dataset like MS-COCO.

### Open Question 3
- **Question:** Can the pairwise mathematical formulation of InfSplign be extended to complex scenes involving three or more objects with relative spatial dependencies?
- **Basis in paper:** The method represents prompts as triplets ⟨A, R, B⟩ and defines the balance loss specifically as the absolute difference between two objects (|σ²_A - σ²_B|).
- **Why unresolved:** The balance loss is currently computed between two variables; it is unclear if applying this pairwise to multiple object combinations would result in conflicting gradients or unstable optimization.
- **What evidence would resolve it:** Extending the formalism to N objects and evaluating performance on benchmarks requiring multi-object reasoning (e.g., "a cat to the left of a dog and above a car").

## Limitations
- Reliance on cross-attention maps assumes these maps reliably encode object locations, with limited empirical validation across diverse model architectures and text prompts
- Performance gains are benchmarked mainly against inference-time approaches and limited fine-tuning baselines, without comparison to state-of-the-art spatial control methods
- High guidance weight (η=1000) may introduce artifacts or tradeoffs in image fidelity, but this impact is not thoroughly quantified

## Confidence
- Spatial accuracy improvements: **High** - supported by quantitative benchmarks and human evaluation
- Generalization across prompts and models: **Medium** - evidence is present but limited in scope
- Absence of side effects (style/color fidelity): **Low** - not thoroughly investigated

## Next Checks
1. Conduct ablation studies to quantify the individual and combined contributions of the spatial alignment, object presence, and balance loss terms
2. Evaluate the method's robustness and performance on a broader set of diffusion model architectures and diverse, complex prompts beyond the current benchmarks
3. Measure and report the computational overhead introduced during inference, and assess any unintended effects on image style, color consistency, or prompt adherence