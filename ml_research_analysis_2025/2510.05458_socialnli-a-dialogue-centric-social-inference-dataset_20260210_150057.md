---
ver: rpa2
title: 'SocialNLI: A Dialogue-Centric Social Inference Dataset'
arxiv_id: '2510.05458'
source_url: https://arxiv.org/abs/2510.05458
tags:
- inference
- dialogue
- social
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SocialNLI, the first dialogue-centric social
  inference dataset, to evaluate models' ability to interpret complex social phenomena
  like sarcasm and irony from multi-speaker dialogue transcripts. The dataset contains
  5,320 inferences over 243 curated dialogues from the TV show Friends, paired with
  human plausibility scores and explanations.
---

# SocialNLI: A Dialogue-Centric Social Inference Dataset

## Quick Facts
- **arXiv ID**: 2510.05458
- **Source URL**: https://arxiv.org/abs/2510.05458
- **Reference count**: 34
- **Key outcome**: Introduces SocialNLI, a dialogue-centric social inference dataset with 5,320 inferences over 243 Friends dialogues, revealing that most LLMs struggle to align with human social reasoning despite reasonable calibration metrics.

## Executive Summary
This paper introduces SocialNLI, the first dialogue-centric social inference dataset designed to evaluate models' ability to interpret complex social phenomena like sarcasm and irony from multi-speaker dialogue transcripts. The dataset contains 5,320 inferences over 243 curated dialogues from the TV show Friends, paired with human plausibility scores and explanations. The authors employ a novel LLM-driven counterfactual reasoning approach, generating supporting and opposing arguments for each inference and computing posterior likelihoods. Evaluation on the human-scored subset shows that while GPT-4o exhibits some positive correlation with human judgments, most models struggle to align with human social reasoning, highlighting significant room for improvement in developing models with robust theory-of-mind capabilities.

## Method Summary
The authors created SocialNLI by curating 243 dialogues from the FriendsQA dataset, then generating 10 inferences per dialogue-question pair using GPT-4o with both chain-of-thought and non-CoT prompting. They employed a counterfactual reasoning approach where models generate supporting (A+) and opposing (A-) arguments for each inference, assign likelihoods to each argument, and compute posterior plausibility using the formula: P̂ = s+(1-s-)/[s+(1-s-) + (1-s+)s-]. The dataset includes 1,400 human-scored examples over 70 dialogues, with scores normalized to [0,1]. Primary evaluation uses Pearson correlation between model and human plausibility scores, with MAE serving as a calibration indicator.

## Key Results
- GPT-4o shows positive correlation (ρ=0.06) with human plausibility judgments on SocialNLI
- Most models (Claude-3.5, Qwen2.5-32B, Llama-3.1-70B) show little to no correlation with human scores
- Models achieve reasonable MAE scores by clustering predictions near dataset mean (0.82), indicating calibration without genuine understanding
- Generated argument quality shows 7-19% gaps compared to human baselines, impacting downstream plausibility scoring

## Why This Works (Mechanism)
The counterfactual reasoning approach works by forcing models to explicitly consider both supporting and opposing perspectives for each social inference, mimicking human reasoning processes. By generating arguments and assigning likelihoods, models must engage with the ambiguity inherent in social inference tasks rather than making binary decisions. The posterior computation formula naturally balances confidence in both directions, preventing overconfident predictions when evidence is mixed.

## Foundational Learning
- **Theory of Mind (ToM)**: Understanding that others have mental states different from one's own - needed to interpret sarcasm and irony in dialogue, quick check: can the model explain why a statement might be interpreted differently by different characters
- **Counterfactual reasoning**: Evaluating hypothetical scenarios by considering what would happen if certain conditions were different - needed to generate supporting and opposing arguments, quick check: can the model articulate alternative interpretations of ambiguous social cues
- **Plausibility scoring**: Assigning continuous values to represent degree of belief rather than binary judgments - needed to capture the graded nature of social inference, quick check: does the model's score distribution match human uncertainty patterns
- **Multi-speaker dialogue processing**: Tracking multiple conversational threads and speaker intentions simultaneously - needed to understand complex social dynamics, quick check: can the model correctly attribute sarcastic statements to their intended targets
- **Context window management**: Maintaining relevant dialogue context while generating arguments - needed for coherent inference generation, quick check: do generated arguments reference appropriate dialogue turns
- **Prompt engineering**: Crafting effective prompts for both inference generation and argument evaluation - needed to elicit desired reasoning patterns, quick check: does prompt format significantly affect argument quality

## Architecture Onboarding
**Component Map**: Dialogue Corpus -> Question Generation -> Inference Generation -> Argument Generation -> Likelihood Scoring -> Posterior Computation -> Human Evaluation
**Critical Path**: Inference Generation -> Counterfactual Reasoning -> Posterior Scoring
**Design Tradeoffs**: Hand-curated dialogues provide quality control but limit scalability; LLM-generated inferences enable large-scale data creation but introduce potential biases
**Failure Signatures**: Low correlation with human judgments despite reasonable MAE suggests models are calibrating to dataset mean rather than understanding social nuances
**First Experiments**: 1) Generate A+/A- arguments for a sample inference and evaluate quality manually, 2) Compute posterior scores using different likelihood combinations, 3) Compare model score distributions against human score distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Hand-selection process for 243 dialogues from FriendsQA is not fully specified, creating uncertainty about dataset coverage
- Counterfactual reasoning approach relies heavily on model-generated arguments, with 7-19% quality gaps compared to human baselines
- Most models show low correlation with human judgments but reasonable MAE, suggesting systematic biases in social reasoning approaches

## Confidence
**High Confidence**: Dataset construction methodology and counterfactual reasoning evaluation framework are clearly described and reproducible
**Medium Confidence**: Correlation results showing GPT-4o's positive correlation versus other models' struggles are well-supported
**Low Confidence**: Claims about models' "robust theory-of-mind capabilities" are difficult to verify given the dataset's limited scope (Friends TV show dialogues only)

## Next Checks
1. Conduct blind re-curation of 30 random dialogues from FriendsQA using explicit sarcasm/irony identification criteria to assess inter-annotator agreement
2. Evaluate models on SocialNLI using dialogues from a different source (e.g., different TV show or movie transcripts) to assess cross-domain generalization
3. Systematically vary the quality of generated supporting/opposing arguments (using human-written vs. model-generated) to quantify impact on final plausibility score correlations