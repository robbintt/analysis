---
ver: rpa2
title: 'CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback
  in Large Vision-Language Models'
arxiv_id: '2512.23453'
source_url: https://arxiv.org/abs/2512.23453
tags:
- visual
- decoding
- image
- large
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CoFi-Dec, a training-free decoding framework
  that reduces hallucinations in large vision-language models by integrating coarse-to-fine
  generative feedback. The method decomposes input images into coarse and fine-grained
  views, generates multi-scale textual hypotheses, synthesizes corresponding pseudo-images
  using a text-to-image model, and fuses the resulting token distributions via Wasserstein
  barycenter optimization.
---

# CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2512.23453
- Source URL: https://arxiv.org/abs/2512.23453
- Authors: Zongsheng Cao; Yangfan He; Anran Liu; Jun Xie; Feng Chen; Zepeng Wang
- Reference count: 40
- Key outcome: A training-free decoding framework that reduces hallucinations in large vision-language models by integrating coarse-to-fine generative feedback.

## Executive Summary
This paper introduces CoFi-Dec, a training-free decoding framework designed to reduce hallucinations in large vision-language models (VLMs) by leveraging coarse-to-fine generative feedback. The approach decomposes input images into coarse and fine-grained views, generates multi-scale textual hypotheses, synthesizes corresponding pseudo-images using a text-to-image model, and fuses the resulting token distributions via Wasserstein barycenter optimization. This multi-path decoding aligns global semantics with local visual details while allowing self-correction during generation. Extensive experiments on six hallucination-focused benchmarks demonstrate consistent performance gains, with CoFi-Dec achieving up to 90.33% accuracy on POPE, significant reductions in CHAIR hallucination scores, and improved overall scores on MME-Hallucination and MMBench. The framework is model-agnostic, requires no additional training, and offers a practical approach to improving visual grounding in multimodal generation.

## Method Summary
CoFi-Dec is a training-free decoding framework that addresses hallucinations in large vision-language models by integrating coarse-to-fine generative feedback. The method works by first decomposing the input image into coarse (global semantic) and fine (local visual detail) views. The VLM then generates textual hypotheses at both scales, which are converted into pseudo-images using a text-to-image model. These pseudo-images are fed back into the VLM as additional context, and the resulting token distributions are fused using Wasserstein barycenter optimization. This process enables the model to cross-check its own outputs against synthesized visual feedback, promoting self-correction and improved visual grounding. The framework is designed to be model-agnostic and does not require additional training, making it broadly applicable to existing VLMs.

## Key Results
- CoFi-Dec achieves up to 90.33% accuracy on the POPE hallucination benchmark.
- Significant reductions in CHAIR hallucination scores, indicating improved faithfulness to visual input.
- Consistent performance gains across six hallucination-focused benchmarks, including MME-Hallucination and MMBench.

## Why This Works (Mechanism)
The framework improves visual grounding by leveraging a multi-path decoding strategy that incorporates coarse-to-fine generative feedback. By decomposing images into global and local views, the method ensures that both high-level semantics and fine-grained visual details are considered during generation. Synthesizing pseudo-images from textual hypotheses and feeding them back as context allows the model to self-correct by comparing its outputs against synthesized visual evidence. The Wasserstein barycenter optimization fuses token distributions from multiple paths, balancing semantic alignment with visual fidelity. This approach mitigates the tendency of VLMs to hallucinate by grounding generation in multi-scale visual feedback.

## Foundational Learning
- **Coarse-to-fine image decomposition**: Splitting images into global and local views ensures both high-level semantics and fine-grained details are captured. Needed to provide multi-scale visual context for generation. Quick check: Verify decomposition preserves relevant visual information at both scales.
- **Text-to-image synthesis for feedback**: Converting generated text into pseudo-images allows the model to visually ground its own outputs. Needed to provide a feedback loop for self-correction. Quick check: Assess fidelity of synthesized pseudo-images to input text.
- **Wasserstein barycenter optimization**: Fusing token distributions from multiple decoding paths using optimal transport ensures balanced semantic and visual alignment. Needed to integrate coarse and fine feedback coherently. Quick check: Confirm fusion improves grounding without degrading fluency.

## Architecture Onboarding

**Component map:**
Image → Coarse/Fine Decomposition → VLM (multi-scale hypotheses) → Text-to-Image Model → Pseudo-Images → VLM (with feedback) → Token Distributions → Wasserstein Barycenter Fusion → Final Output

**Critical path:**
Image decomposition → Multi-scale text generation → Pseudo-image synthesis → Feedback fusion → Final token distribution

**Design tradeoffs:**
- Fixed decomposition vs. adaptive granularity: Fixed coarse/fine split is simple but may not optimize for all image types.
- Real-time feasibility: Text-to-image synthesis introduces latency, limiting real-time use.
- Model dependence: Relies on quality of external text-to-image model; poor synthesis degrades performance.

**Failure signatures:**
- Pseudo-images poorly aligned with input text lead to misleading feedback.
- Over-fusion of coarse and fine paths may dilute important local details.
- Excessive computational overhead during inference due to synthesis step.

**Three first experiments to run:**
1. Ablation: Remove pseudo-image synthesis and use only textual hypotheses to measure feedback contribution.
2. Latency analysis: Measure inference time overhead introduced by multi-path decoding.
3. Robustness test: Evaluate performance on out-of-distribution images not seen in benchmarks.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can adaptive granularity selection via tree-structured exploration optimize the balance between visual detail and computational efficiency?
- Basis in paper: [explicit] The conclusion states that future work includes "exploring adaptive granularity selection via tree-structured exploration."
- Why unresolved: The current framework uses a fixed decomposition into coarse and fine views, which may be redundant for simple images or insufficient for highly complex ones.
- What evidence would resolve it: A study comparing the fixed-path approach against a dynamic tree-search method, measuring accuracy on complex benchmarks versus computational cost.

### Open Question 2
- Question: Does incorporating learned feedback quality estimation improve the robustness of the self-correction loop?
- Basis in paper: [explicit] The authors propose "incorporating learned feedback quality estimation to further enhance self-correction capabilities" in the conclusion.
- Why unresolved: The current fusion mechanism assumes the synthesized pseudo-images provide useful grounding, but it lacks a mechanism to filter out low-quality or irrelevant generative feedback.
- What evidence would resolve it: Experiments demonstrating that weighting the Wasserstein fusion based on a quality estimator outperforms the standard unweighted fusion.

### Open Question 3
- Question: Can the computational overhead of the diffusion-based synthesis step be reduced to allow for real-time application?
- Basis in paper: [inferred] Table A2 reports a latency of 20.83 seconds (6.05× slower than baseline), highlighting a significant efficiency bottleneck.
- Why unresolved: The method relies on running a full text-to-image diffusion model (Stable Diffusion) during the inference loop, which is resource-intensive.
- What evidence would resolve it: An implementation using latent feature manipulation or one-step diffusion models that achieves comparable hallucination reduction with substantially lower latency.

## Limitations
- The approach relies heavily on the quality and faithfulness of the text-to-image model for pseudo-image synthesis; poor synthesis can propagate hallucinations.
- The fixed decomposition strategy may not generalize optimally across all image types or model architectures.
- Wasserstein barycenter optimization introduces additional computational overhead during inference, which is not fully quantified in terms of latency impact.

## Confidence
- High confidence in the methodological novelty of integrating coarse-to-fine visual decomposition with generative feedback
- High confidence in the experimental results showing consistent improvements across multiple hallucination benchmarks
- Medium confidence in the generalization of performance gains to real-world applications beyond controlled benchmarks
- Medium confidence in the scalability of the approach to very large VLMs or alternative image decomposition strategies

## Next Checks
1. Conduct ablation studies isolating the impact of text-to-image model quality by testing with multiple T2I models of varying fidelity.
2. Measure and report the additional inference latency introduced by the multi-path decoding process compared to standard decoding.
3. Evaluate performance on out-of-distribution images and tasks not represented in the benchmark suite to assess robustness.