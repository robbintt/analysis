---
ver: rpa2
title: '"This Suits You the Best": Query Focused Comparative Explainable Summarization'
arxiv_id: '2507.04733'
source_url: https://arxiv.org/abs/2507.04733
tags:
- qf-ces
- comparative
- evaluation
- query
- m-os
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Query-Focused Comparative Explainable Summarization
  (QF-CES), a novel task using LLMs to generate comparative summaries of top-3 recommended
  products based on user queries. To enable this, the authors create MS-Q2P, a dataset
  of 7,500 queries with 22,500 products, and CES-EVAL, a benchmark for evaluating
  QF-CES across five dimensions.
---

# "This Suits You the Best": Query Focused Comparative Explainable Summarization

## Quick Facts
- arXiv ID: 2507.04733
- Source URL: https://arxiv.org/abs/2507.04733
- Reference count: 16
- Key outcome: LLM-based QF-CES with M-OS intermediate step reduces latency by 40% and achieves 0.74 Spearman correlation with human judgments

## Executive Summary
This paper introduces Query-Focused Comparative Explainable Summarization (QF-CES), a novel task using LLMs to generate comparative summaries of top-3 recommended products based on user queries. To enable this, the authors create MS-Q2P, a dataset of 7,500 queries with 22,500 products, and CES-EVAL, a benchmark for evaluating QF-CES across five dimensions. They propose M-OS as an intermediate step, which reduces inference latency by 40% compared to direct input processing. Evaluations using QF-CES-PROMPT across five dimensions showed an average Spearman correlation of 0.74 with human judgments. GPT-4 outperformed other models, with Qwen2-7B-instruct as the best open-source model.

## Method Summary
The authors propose a two-stage pipeline using LLMs with specific inference parameters (temp=0.2, top_k=25, top_p=0.95, beams=3): 1) Multi-Source Opinion Summarization (M-OS) generates intermediate summaries for individual products using Mistral-7B-Instruct-v0.3, 2) QF-CES generation produces final comparative summaries from M-OS outputs using Qwen2-7B-instruct or GPT-4. The system is evaluated on 5 dimensions (Clarity, Faithfulness, Informativeness, Format Adherence, Query Relevance) using Spearman correlation with human judgments and latency reduction compared to Direct Input Approach (DIA).

## Key Results
- Average Spearman correlation of 0.74 with human judgments across five evaluation dimensions
- GPT-4 achieved highest scores (avg 4.55/5) while Qwen2-7B-instruct offered near-parity (avg 4.49/5) as best open-source model
- M-OS reduced inference latency by approximately 40% compared to direct input processing (9.99s vs 16.55s per summary)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: M-OS intermediate layer reduces inference latency by condensing multi-source product data before comparative generation
- Mechanism: Raw product data is first summarized into compact opinion summaries via an LLM ensemble, then these pre-computed M-OS outputs serve as input for QF-CES generation
- Core assumption: Condensed summaries retain sufficient signal for downstream comparative reasoning without critical information loss
- Evidence anchors: [abstract] "M-OS reduces inference latency approximately by 40%", [Section 6] "M-OS averaged 9.99 seconds per summary, compared to 16.55 seconds for DIA"

### Mechanism 2
- Claim: Structured, dimension-specific prompting enables LLMs to generate tabular comparative summaries with consistent format adherence
- Mechanism: QF-CES-PROMPT decomposes generation into role assignment, step-by-step attribute selection, tabular output schema enforcement, and final verdict generation
- Core assumption: LLMs follow multi-step instructions sufficiently to maintain structural consistency across diverse product categories
- Evidence anchors: [Table 4] Format adherence scores remain relatively high across models (3.58–4.61 on 5-point scale)

### Mechanism 3
- Claim: LLM-based evaluators using QF-CES-PROMPT achieve moderate-to-strong correlation with human judgments across five dimensions
- Mechanism: Each dimension has a dedicated prompt with scoring criteria (1–5 scale). LLM evaluators sample multiple outputs and compute weighted scores
- Core assumption: LLM evaluators can internalize evaluation criteria similarly to trained human annotators when provided explicit rubrics
- Evidence anchors: [abstract] "average Spearman correlation of 0.74 with human judgments", [Table 3] LLaMA-3.1-70B-Instruct achieves highest correlations

## Foundational Learning

- Concept: Opinion Summarization Fundamentals
  - Why needed here: M-OS synthesis requires understanding how to extract sentiment-aspect pairs from reviews and merge with objective product metadata without hallucination
  - Quick check question: Given 10 reviews for a product mentioning "battery life" positively and "screen brightness" negatively, can you produce a 2-sentence summary preserving both sentiments?

- Concept: LLM Prompt Engineering for Structured Output
  - Why needed here: QF-CES demands consistent tabular format and natural language verdict; prompt design directly controls output adherence
  - Quick check question: How would you modify a prompt to enforce a specific JSON schema for product comparison output?

- Concept: Reference-Free Evaluation Metrics
  - Why needed here: CES-EVAL uses LLM-as-judge without gold reference summaries; understanding correlation-based validation is essential for interpreting results
  - Quick check question: If an LLM evaluator shows ρ=0.5 with human judgments, is this sufficient for deployment? What threshold would you require?

## Architecture Onboarding

- Component map: User query + top-3 product IDs → MS-Q2P dataset lookup → M-OS Generator → QF-CES Generator → Evaluation Layer → Human Validation
- Critical path: M-OS generation quality → QF-CES informativeness and faithfulness scores
- Design tradeoffs:
  - Latency vs. Quality: M-OS adds pre-computation overhead but reduces per-query inference by 40%
  - Model Size vs. Cost: GPT-4 achieves best scores (avg 4.55/5) but Qwen2-7B-instruct (avg 4.49/5) offers near-parity at lower cost
  - Evaluator Choice: LLaMA-3.1-70B-Instruct shows higher human correlation than GPT-4o for evaluation
- Failure signatures:
  - Incomplete summaries: Products with extensive specifications cause LLMs to stall or truncate
  - Low query relevance: DIA approach scores 3.89/5 vs. M-OS-based Qwen2-7B at 4.63/5
  - Evaluator disagreement: Round-I Krippendorff's α=0.50 improves to 0.80 after discussion
- First 3 experiments:
  1. Generate M-OS for 20 diverse products, manually assess query-relevant attribute preservation
  2. Measure inference time for M-OS vs. DIA across 50 queries to confirm 40% reduction
  3. Run QF-CES-PROMPT evaluation on 10 CES-EVAL samples with both LLaMA-3.1-70B-Instruct and GPT-4o to check Spearman correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the QF-CES-PROMPT evaluation framework be effectively adapted for comparative summarization in domains outside of e-commerce?
- Basis in paper: [explicit] The conclusion states future work involves "exploring the applicability of QF-CES-PROMPT to other domains beyond e-commerce"
- Why unresolved: Current validation is restricted to product recommendations, leaving performance on different data structures unknown
- What evidence would resolve it: Successful application and high correlation with human judgment when evaluating comparative summaries in distinct domains

### Open Question 2
- Question: How can the summarization pipeline be stabilized to prevent incomplete outputs when processing products with extensive technical specifications?
- Basis in paper: [explicit] The Limitations section notes that "LLMs occasionally struggled with products having extensive specifications"
- Why unresolved: Current prompting strategy or context handling fails when faced with high-density technical data
- What evidence would resolve it: A modified generation strategy that consistently yields complete summaries for high-specification products

### Open Question 3
- Question: Does increasing the input review count beyond the current average of 10 significantly impact the faithfulness and aspect coverage of generated summaries?
- Basis in paper: [inferred] Authors identify the "limitation of only 10 reviews" and suggest future benchmarks should incorporate datasets with more reviews
- Why unresolved: Unclear if current models can maintain performance when scaled to larger, noisier corpus of user opinions
- What evidence would resolve it: Ablation studies measuring summary quality metrics while varying review count from 10 to 100+

## Limitations
- Proprietary MS-Q2P dataset prevents direct replication of reported results and specific 0.74 Spearman correlation
- LLM struggles with products having extensive specifications (>242 words average), leading to stalled or truncated outputs
- Annotator disagreement on informativeness and query relevance dimensions suggests potential ambiguity in evaluation criteria

## Confidence
- M-OS latency reduction mechanism (40% improvement): High confidence
- LLM evaluator correlation with human judgments (0.74 Spearman): Medium confidence
- GPT-4 superiority and Qwen2-7B-instruct performance parity: Medium confidence
- Format adherence consistency through structured prompting: Medium confidence

## Next Checks
1. Generate M-OS for 20 diverse products, manually assess whether query-relevant attributes are preserved; if >15% missing critical features, refine M-OS prompt or increase review sample size beyond 10
2. Measure inference time for M-OS vs. DIA across 50 queries with 3 repetitions each; confirm 40% reduction holds for your infrastructure and model configuration
3. Run QF-CES-PROMPT evaluation on 10 CES-EVAL samples with both LLaMA-3.1-70B-Instruct and GPT-4o; compute Spearman correlation with provided human scores; if ρ<0.6, review prompt wording for dimension alignment