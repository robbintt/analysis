---
ver: rpa2
title: Semantic search for 100M+ galaxy images using AI-generated captions
arxiv_id: '2512.11982'
source_url: https://arxiv.org/abs/2512.11982
tags:
- images
- image
- galaxy
- search
- astronomical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work demonstrates how Vision-Language Models (VLMs) can generate\
  \ informative, free-form descriptions of galaxy images that enable semantic search\
  \ at scale without requiring human labels. By using GPT-4.1-mini to caption millions\
  \ of unlabeled galaxy images and aligning a frozen astronomy foundation model with\
  \ these synthetic captions via contrastive learning, the authors create AION-Search\u2014\
  a zero-shot semantic retrieval system."
---

# Semantic search for 100M+ galaxy images using AI-generated captions

## Quick Facts
- arXiv ID: 2512.11982
- Source URL: https://arxiv.org/abs/2512.11982
- Reference count: 25
- Primary result: Vision-language models can generate informative captions enabling semantic search for 100M+ galaxy images without human labels.

## Executive Summary
This work demonstrates that current vision-language models (VLMs) can generate sufficiently informative descriptions of galaxy images to train a semantic search model without human labels. By using GPT-4.1-mini to caption millions of unlabeled galaxy images and aligning a frozen astronomy foundation model with these synthetic captions via contrastive learning, the authors create AION-Searchâ€”a zero-shot semantic retrieval system. AION-Search significantly outperforms similarity-based baselines on three scientific targets (spirals, mergers, lenses), with nDCG@10 scores of 0.941, 0.554, and 0.180 respectively. VLM-based re-ranking nearly doubles the number of rare gravitational lenses found in the top-100 results.

## Method Summary
The method trains a semantic search model by generating synthetic captions for unlabeled galaxy images using GPT-4.1-mini, then aligning a frozen astronomy foundation model (AION-1-Base) to these captions through contrastive learning. A trainable projector head maps the image embeddings to the text embedding space, minimizing InfoNCE loss against embeddings of the VLM-generated captions. The system uses random brightness-limited galaxy samples for training to maintain generality, and employs a separate VLM (GPT-4.1) for inference-time re-ranking of top candidates to improve precision on rare objects.

## Key Results
- AION-Search achieves nDCG@10 scores of 0.941, 0.554, and 0.180 for spiral galaxies, mergers, and lenses respectively
- VLM-based re-ranking nearly doubles the number of rare gravitational lenses found in top-100 results (13 vs 7)
- Training on single-sentence summaries improves performance by 0.08-0.18 nDCG compared to multi-paragraph captions
- The approach scales to 140 million images and generalizes to finding objects not seen during training

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Caption-Driven Alignment
VLMs generate sufficiently accurate textual descriptions of scientific images, enabling contrastive learning to align visual embeddings to semantic text space without human labels. The VLM (GPT-4.1-mini) converts images to free-form text, which a projector head maps to the text embedding space. The system minimizes InfoNCE loss, forcing the image embedding to correspond with the VLM's description. This works because VLMs extract scientifically relevant visual features better than random noise, even if they hallucinate specifics.

### Mechanism 2: Generalization via Random Sampling & Frozen Encoders
Training on randomly sampled, uncured images preserves the foundation model's generality, enabling zero-shot retrieval of rare phenomena not seen frequently during alignment. The training set uses a simple brightness cut rather than curated rare-object catalogs, and the AION image encoder weights are frozen during training. This maintains the foundation model's general visual features while learning to "translate" them into the text embedding space.

### Mechanism 3: Inference-Time Verification via Re-ranking
Using a high-capacity VLM to re-rank top-k results acts as a verification step, trading compute for precision to overcome embedding noise. The coarse search retrieves top-1000 candidates, which a separate VLM (GPT-4.1) scores against the query. N-sample averaging reduces stochasticity, and results are re-ordered by this score. This works because the initial retrieval is recall-sufficient and the VLM has better classification judgment than the single-vector embedding similarity.

## Foundational Learning

- **Concept: Contrastive Learning (CLIP)**
  - Why needed here: This is the engine that links images to text. You must understand that the model learns "this image goes with this text" by pushing away embeddings of mismatched pairs.
  - Quick check question: If you fed the model an image of a spiral galaxy and a caption saying "elliptical galaxy," how would the loss function punish the model?

- **Concept: Zero-Shot Generalization**
  - Why needed here: The key value prop is finding things (lenses) the model wasn't explicitly trained on.
  - Quick check question: Why does training on random galaxies help the model find rare gravitational lenses later?

- **Concept: Foundation Models (Frozen Weights)**
  - Why needed here: The architecture relies on AION as a fixed "eye." You need to know that you are only training the "translator" (projector), not the visual cortex itself.
  - Quick check question: What happens to the pre-existing visual capabilities of AION if you unfreeze its weights and train on a small, noisy dataset?

## Architecture Onboarding

- **Component map:** Galaxy cutout -> Lupton transformation -> RGB Image -> GPT-4.1-mini -> Text Description -> Summarizer -> Text Embedding. Image -> Frozen AION Encoder -> Trainable Projector -> Predicted Embedding. Loss vs. Text Embedding.

- **Critical path:** The prompt engineering for the VLM and the summary distillation are the highest leverage points. If the captions are low-quality or too verbose (noise), the alignment fails.

- **Design tradeoffs:**
  - Caption Length: Multi-paragraph vs. Single-sentence. Paper finds shorter summaries reduce noise and overfitting.
  - Cost vs. Accuracy: GPT-4.1-mini chosen for cost-efficiency over more accurate (but expensive) o3.
  - Generalization vs. Specificity: Training on random samples preserves generalization but limits peak performance on specific rare classes.

- **Failure signatures:**
  - VLM Hallucinations: Captions describe features not present (e.g., "dust lanes" in clean disks).
  - Domain Shift: VLM fails on imaging modalities it hasn't seen (e.g., unusual telescope filters).
  - Re-ranking Cost: Applying GPT-4.1 to thousands of results is expensive and slow at scale.

- **First 3 experiments:**
  1. Caption Benchmarking: Run the VLM prompt on a held-out set of 64 images with ground truth (Galaxy Zoo) to measure feature extraction accuracy vs. cost.
  2. Projection Ablation: Train the system using raw multi-paragraph captions vs. single-sentence summaries to validate the "shorter is better" claim.
  3. Retrieval Validation: Query for "gravitational lens" and measure nDCG@10 against a curated lens catalog, comparing AION-Search against pure AION visual similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agentic re-ranking with domain-specific tools (e.g., lens modeling) outperform N-sample averaging for discovery verification?
- Basis in paper: The authors suggest extending re-ranking through "agentic approaches that provide models with... computational tools" to scale test-time compute.
- Why unresolved: Current re-ranking relies on statistical averaging of scores rather than tool-based reasoning or physical modeling.
- What evidence would resolve it: Comparing retrieval precision between tool-augmented agents and standard VLMs on complex phenomena like gravitational lenses.

### Open Question 2
- Question: Does the VLM-to-retrieval pipeline transfer effectively to scientific domains lacking large-scale web-scraped text corpora?
- Basis in paper: The authors note VLM performance relies on pretraining with "online image-text discourse" and may be "weaker in scientific domains without comparable corpora."
- Why unresolved: It is unclear if synthetic captions are sufficient for training when the VLM lacks prior domain-specific knowledge.
- What evidence would resolve it: Evaluating the method on niche datasets (e.g., specialized microscopy) absent from VLM pretraining data.

### Open Question 3
- Question: Can AION-Search identify scientifically novel features, such as double tidal streams, that are currently uncurated and rare?
- Basis in paper: The authors identify "finding galaxies with two tidal streams" as a future application for constraining dark matter halos.
- Why unresolved: Current benchmarks validate against existing catalogs, not truly novel, uncataloged phenomena.
- What evidence would resolve it: Successful retrieval and expert confirmation of uncataloged galaxies exhibiting specific rare morphological features.

## Limitations

- The exact MLP projection head architecture and training hyperparameters are not specified, making exact reproduction difficult.
- VLM performance may be limited in scientific domains lacking large-scale web-scraped text corpora for pretraining.
- The computational cost of VLM-based re-ranking scales linearly with the number of candidates, potentially limiting practical deployment.

## Confidence

- **High:** VLM caption quality is sufficient for semantic alignment (supported by nDCG improvements with summary captions). Re-ranking significantly improves rare-object recall (doubling lens findings in top-100).
- **Medium:** Generalization to rare objects via frozen encoders (reasonable given foundation model design, but not directly validated beyond lenses). Cost-efficiency of GPT-4.1-mini vs. higher-capacity VLMs.
- **Low:** Exact hyperparameter values (batch size, epochs, learning rate details) and precise evaluation catalog cross-match procedures.

## Next Checks

1. Sample and manually evaluate 100 VLM-generated captions against Galaxy Zoo ground truth to quantify feature extraction accuracy and hallucination rate.
2. Conduct an ablation study comparing full-paragraph captions vs. single-sentence summaries on a held-out retrieval task to verify the noise-reduction claim.
3. Query AION-Search for "gravitational lens" and measure nDCG@10 against a curated HSC lens catalog cross-matched to Legacy Survey, benchmarking against pure AION visual similarity retrieval.