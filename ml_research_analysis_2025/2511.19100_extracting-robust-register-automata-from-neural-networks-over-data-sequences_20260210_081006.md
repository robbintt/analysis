---
ver: rpa2
title: Extracting Robust Register Automata from Neural Networks over Data Sequences
arxiv_id: '2511.19100'
source_url: https://arxiv.org/abs/2511.19100
tags:
- automata
- robustness
- neural
- register
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for extracting deterministic
  register automata (DRAs) from black-box neural networks operating on data sequences.
  The key challenge addressed is extending automata extraction beyond finite alphabets
  to continuous data domains, enabling interpretability and robustness certification
  for models like RNNs and transformers.
---

# Extracting Robust Register Automata from Neural Networks over Data Sequences

## Quick Facts
- arXiv ID: 2511.19100
- Source URL: https://arxiv.org/abs/2511.19100
- Reference count: 40
- Primary result: Framework extracts deterministic register automata (DRAs) from neural networks over continuous data sequences with polynomial-time robustness verification and statistical equivalence guarantees

## Executive Summary
This paper presents a novel framework for extracting interpretable deterministic register automata (DRAs) from black-box neural networks operating on data sequences, extending beyond traditional finite alphabets to continuous domains. The key innovation is a polynomial-time robustness checker that reduces local robustness verification to a shortest-path computation problem, combined with three complementary learning algorithms (SMT-based, search-based, and active learning) to synthesize DRAs. The framework provides statistical guarantees on network robustness by filtering spurious counterexamples and demonstrates strong performance across 18 benchmark languages with models including RNNs and transformers.

## Method Summary
The framework extracts DRAs by treating trained neural networks as oracles within automata learning algorithms. Three synthesis methods are employed: SMT-based synthesis using Z3 to encode DRA structure as logical constraints, local search that mutates candidate automata to fit samples, and active learning using RALib to query the network. Once a candidate DRA is synthesized, robustness is verified by reducing the problem to shortest-path computation through a composition with Register Automata with Accumulator (RAA) that tracks perturbation costs. The framework also includes an equivalence checker that uses statistical sampling (PAC bounds with n=369, epsilon=gamma=0.05) to filter spurious counterexamples and refine the extracted automaton.

## Key Results
- Polynomial-time robustness verification for DRAs with fixed register counts by reducing to shortest-path computation
- Three complementary DRA synthesis methods achieving 95%+ agreement accuracy across 18 benchmark languages
- Statistical robustness certification with confidence guarantees through PAC-style equivalence checking
- Successfully extracted DRAs from LSTM and Transformer models capturing complex data sequence behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The robustness of a DRA with a fixed number of registers can be verified in polynomial time
- Mechanism: Reduces local robustness checking to a single-source shortest path problem by composing the DRA with a Register Automata with Accumulator (RAA) that tracks perturbation costs, constructing a weighted graph where edge weights represent accumulated costs
- Core assumption: The number of registers k is fixed; complexity becomes PSPACE-complete if k is variable
- Evidence: [abstract] "reducing the problem to shortest-path computation"; [section 4] Theorem 1 and Lemma 4; neighbor papers focus on extraction rather than this specific reduction

### Mechanism 2
- Claim: Interpretable DRAs can be extracted from black-box neural networks over continuous domains
- Mechanism: Three strategies - passive SMT-based synthesis (logical constraints), passive local search (mutation), and active learning (RALib queries) - synthesize DRAs as finite-state approximations of network decision boundaries
- Core assumption: Network behavior is approximately regular and training samples are representative
- Evidence: [abstract] "combine it with passive and active learning algorithms"; [section 5] three complementary methods; consistent with finite automata extraction literature extended to register automata

### Mechanism 3
- Claim: Framework provides statistical guarantees by filtering spurious counterexamples
- Mechanism: When DRA checker finds robustness violations, queries neural network at those points; genuine counterexamples are retained while spurious ones (indicating DRA inaccuracy) are fed back to learner for refinement
- Core assumption: Sufficient i.i.d. samples satisfy Chernoff/Hoeffding bounds for statistical confidence
- Evidence: [section 5] Algorithm 2 lines 11-15; [section 3] Definition 1; specific to this paper's verification loop

## Foundational Learning

- **Register Automata (RA) vs. Finite Automata**: RAs extend finite automata with registers to store and compare numeric values, enabling modeling of continuous concepts like "strictly increasing" or "higher highs" that standard DFAs cannot handle. Quick check: How does a Register Automaton handle an input stream of rational numbers compared to a standard DFA?

- **Shortest Path Algorithms (Dijkstra/Bellman-Ford)**: Core theoretical contribution reduces robustness verification to finding lowest-cost path in a graph derived from automaton states. Quick check: In the reduction described, what does the "weight" of an edge in the graph represent in terms of the input sequence?

- **SMT (Satisfiability Modulo Theories) Solvers**: Primary method for synthesizing automata by encoding transition structure and sample consistency as logical formulas (Linear Real Arithmetic) solved by tools like Z3. Quick check: Why is the "guard" of a transition encoded as an interval constraint [low, high] rather than a simple boolean?

## Architecture Onboarding

- **Component map**: Oracle (Black-box Neural Network) -> Learner (Synthesis: SMT-based, Local Search, or Active) -> Verifier (Checker: RAA composition -> Weighted Graph -> Shortest Path) -> Refinement Loop (Compare counterexamples against Oracle)

- **Critical path**: The Synthesis (Learning) phase is the bottleneck; while robustness check is polynomial-time and fast, synthesis (especially SMT-based) can take minutes to hours depending on language complexity and number of registers

- **Design tradeoffs**: Active learning is fast but cannot synthesize uninterpreted constants in guards; passive methods are slower but more expressive; Local Search is faster than SMT but risks local optima

- **Failure signatures**: Divergence (Active: hypothesis tree expands infinitely); Timeout (SMT: search space too large); Inconclusive (spurious counterexamples that learner fails to eliminate)

- **First 3 experiments**:
  1. Unit Test the Checker: Build trivial DRA (e.g., "input must be > 5") and verify robustness checker correctly identifies boundary (5) as shortest path to label flip
  2. Baseline Extraction: Train simple LSTM on "Strictly Increasing" language (S1) and run Local Search learner to verify quick extraction of correct 1-DRA
  3. Robustness Certification: Use extracted DRA from step 2 to certify LSTM; perturb last letter of valid sequence and confirm system reports "Robust" or "Non-robust" based on distance to breaking value

## Open Questions the Paper Calls Out

- **Extending to parametric LRA automata**: The framework could be extended to learn and verify parametric register automata with Linear Real Arithmetic guards, though this would require handling NP-complete non-emptiness checking instead of the current polynomial-time approach

- **Adapting for probabilistic/nondeterministic settings**: Current methodology assumes deterministic neural networks and produces deterministic DRAs; extending to probabilistic or nondeterministic networks would require fundamental revision of robustness definitions and equivalence checking

- **Active learning with uninterpreted constants**: The active learning method cannot currently synthesize transition guards containing uninterpreted constants, unlike the SMT-based approach, requiring a different hypothesis construction strategy

- **Generalizing to richer temporal properties**: Current robustness certification focuses on label stability; verifying richer temporal properties like "sequence always increases" would require different integration of model checking into the synthesis loop

## Limitations

- Polynomial-time robustness checking critically depends on fixed register counts; becomes PSPACE-complete when register count scales with input length
- Extraction algorithms face practical scalability limits with SMT-based synthesis times increasing exponentially with state/register complexity
- Statistical guarantees depend entirely on representative sampling from distribution D, with insufficient boundary coverage potentially yielding false robustness certificates
- Active learning can diverge on non-regular tasks like certain Transformer models on languages L5/L6

## Confidence

- **High Confidence**: Polynomial-time reduction of DRA robustness to shortest-path computation (follows directly from theoretical construction and Theorem 1 proof); passive and active learning framework for DRA extraction (well-established in automata learning literature)
- **Medium Confidence**: Statistical robustness certification loop (methodology sound but practical effectiveness depends heavily on sample size adequacy and distribution coverage)
- **Low Confidence**: Practical scalability of entire pipeline for complex languages requiring many registers or states, particularly SMT-based learner on languages S9-S11 with 30-minute timeouts

## Next Checks

1. **Boundary Coverage Validation**: Systematically evaluate whether equivalence checker's sample distribution D adequately covers fragile boundary regions for each language, particularly testing languages where spurious counterexamples were observed

2. **Register Scalability Test**: Run robustness checker on synthetic DRAs with variable register counts (k=1,2,3,5,10) to empirically confirm exponential complexity growth and validate PSPACE-complete claim when k is unbounded

3. **Cross-Algorithm Comparison**: For challenging language like S6 (All Substrings) or S11 (All Subsequences), run all three learners (Active, SMT, Local Search) and compare extracted DRA accuracy, synthesis time, and subsequent robustness certification performance to quantify tradeoff between expressiveness and efficiency