---
ver: rpa2
title: An automatic patent literature retrieval system based on LLM-RAG
arxiv_id: '2508.14064'
source_url: https://arxiv.org/abs/2508.14064
tags:
- patent
- retrieval
- semantic
- generation
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an automatic patent literature retrieval system
  integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG)
  technology. The system addresses limitations of traditional keyword-based retrieval
  methods by enhancing semantic understanding and cross-domain matching through LLM-generated
  embeddings and context-aware response generation.
---

# An automatic patent literature retrieval system based on LLM-RAG

## Quick Facts
- arXiv ID: 2508.14064
- Source URL: https://arxiv.org/abs/2508.14064
- Reference count: 0
- Semantic matching accuracy: 80.5% with gpt-3.5-turbo-0125+RAG

## Executive Summary
This study proposes an automatic patent literature retrieval system integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology to address limitations of traditional keyword-based patent search methods. The system enhances semantic understanding and cross-domain matching through LLM-generated embeddings and context-aware response generation. Experiments on the Google Patents dataset (2006–2024) demonstrate significant performance improvements over baseline LLM methods, validating the effectiveness of the LLM-RAG integration for intelligent patent retrieval.

## Method Summary
The system employs a RAG-End2End architecture with gpt-3.5-turbo-0125 as generator, Faiss vector index for embedding storage, and Top-K=5 retrieval via MIPS. The framework processes patent records including application numbers, titles, abstracts, IPC/CPC classifications, and technical domains. Training uses stratified 8:1:1 splits on the Google Patents dataset, with batch_size=32, epochs=8, and AdamW optimizer at lr=2e-5. Embeddings are generated for patent texts using an unspecified embedding model, indexed in Faiss, and used to retrieve relevant patents during generation. The system is implemented with PyTorch 2.0, HuggingFace Transformers, and RAGSequenceForGeneration API, running on 4x RTX 4090 GPUs with 128GB RAM.

## Key Results
- Semantic matching accuracy of 80.5% achieved with gpt-3.5-turbo-0125+RAG configuration
- Recall of 92.1% demonstrated on Google Patents dataset
- Outperforms baseline LLM methods by 28 percentage points in accuracy

## Why This Works (Mechanism)
The LLM-RAG framework works by combining dense semantic embeddings from LLMs with context-aware generation to overcome keyword-based retrieval limitations. The retriever uses LLM-generated embeddings to capture semantic relationships between patents, while the generator produces context-aware responses that incorporate retrieved information. This end-to-end training approach allows the system to learn optimal embedding-generation coupling, improving both retrieval precision and response relevance compared to traditional keyword matching or standalone LLM approaches.

## Foundational Learning
- **RAG-End2End Architecture**: Combines dense retrieval with generation in a unified framework. Needed because traditional RAG pipelines often train retriever and generator separately, missing optimization opportunities. Quick check: Verify that both components are updated during training via loss backpropagation.
- **MIPS (Maximum Inner Product Search)**: Efficient vector similarity search method for high-dimensional embeddings. Needed to enable fast retrieval from millions of patent embeddings. Quick check: Confirm retrieval latency remains acceptable as index size scales.
- **Stratified Sampling**: Ensures balanced representation across technical domains in train/val/test splits. Needed because patent domains vary significantly in volume and terminology. Quick check: Verify domain distribution is preserved across all dataset splits.
- **Faiss Vector Index**: High-performance similarity search library optimized for dense vectors. Needed for scalable embedding storage and retrieval at scale. Quick check: Benchmark retrieval throughput with increasing index size.
- **Early Stopping**: Training regularization technique to prevent overfitting. Needed because patent domain vocabulary is large and diverse. Quick check: Monitor validation loss for convergence plateau before stopping.

## Architecture Onboarding

**Component Map**: Patent Data -> Embedding Generator -> Faiss Index <- MIPS Retriever <- gpt-3.5-turbo-0125 Generator

**Critical Path**: Query encoding → Embedding retrieval → Context augmentation → Response generation

**Design Tradeoffs**: The system prioritizes semantic matching accuracy over computational efficiency, using large embedding models and end-to-end training. This increases retrieval quality but requires significant computational resources (4x RTX 4090). The choice of gpt-3.5-turbo-0125 balances generation quality with inference cost compared to larger models like gpt-4.0.

**Failure Signatures**: 
- Poor retrieval quality indicates embedding model inadequacy or incorrect indexing configuration
- Low accuracy suggests misalignment between retriever and generator training objectives
- Domain bias manifests as systematically poor performance on underrepresented patent classes

**First 3 Experiments**:
1. Validate embedding generation by sampling patent embeddings and checking semantic coherence
2. Test retrieval quality with known queries to ensure Top-K results are relevant before RAG training
3. Evaluate end-to-end performance on small labeled dataset to confirm accuracy metrics are achievable

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The embedding model used is unspecified, creating uncertainty about the foundation of semantic matching claims
- No baseline comparison against established keyword-based patent retrieval systems
- Limited quantitative evidence for cross-domain matching capabilities beyond reported accuracy metrics

## Confidence

**High confidence**: The claim of 80.5% semantic matching accuracy with gpt-3.5-turbo-0125+RAG configuration is directly reported, though evaluation methodology remains unclear.

**Medium confidence**: The 28 percentage point improvement over baseline LLMs is stated, but lacks context about experimental conditions and whether differences stem from architectural choices versus implementation details.

**Low confidence**: Claims of superior cross-domain matching capabilities relative to keyword-based methods lack comparative experimental validation.

## Next Checks

1. Reconstruct the evaluation dataset by sampling patent records from the Google Patents dataset (2006–2024), creating labeled query-relevance pairs with human annotators to establish ground truth for semantic matching and cross-domain classification tasks.

2. Implement multiple embedding strategies (OpenAI ada-002, sentence-transformers models, and potential open-source alternatives) to isolate the impact of embedding quality on retrieval performance, comparing results against the claimed gpt-3.5-turbo-0125 configuration.

3. Conduct ablation experiments systematically disabling RAG components (retrieval only, generation only, frozen retriever during fine-tuning) to quantify the marginal contribution of each architectural element to the reported performance gains.