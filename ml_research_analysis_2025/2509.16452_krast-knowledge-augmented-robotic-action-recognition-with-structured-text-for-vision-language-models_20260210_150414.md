---
ver: rpa2
title: 'KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text
  for Vision-Language Models'
arxiv_id: '2509.16452'
source_url: https://arxiv.org/abs/2509.16452
tags:
- action
- activity
- which
- recognition
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces KRAST, a knowledge-augmented prompting strategy
  for vision-language models to improve vision-based human action recognition in indoor
  environments. The method leverages learnable prompts conditioned on structured textual
  descriptions of action classes, using hierarchical, semantic, and discriminative
  strategies to enhance prompt semantics.
---

# KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models

## Quick Facts
- arXiv ID: 2509.16452
- Source URL: https://arxiv.org/abs/2509.16452
- Authors: Son Hai Nguyen; Diwei Wang; Jinhyeok Jang; Hyewon Seo
- Reference count: 40
- Primary result: 95.22% top-1 accuracy on ETRI-Activity3D using only RGB video inputs

## Executive Summary
KRAST introduces a knowledge-augmented prompting strategy for vision-language models to improve vision-based human action recognition in indoor environments. The method leverages learnable prompts conditioned on structured textual descriptions of action classes, using hierarchical, semantic, and discriminative strategies to enhance prompt semantics. Experiments on the ETRI-Activity3D dataset show that KRAST achieves over 95% top-1 accuracy using only RGB video inputs, outperforming state-of-the-art approaches and demonstrating the effectiveness of knowledge-augmented prompts in distinguishing visually similar actions with minimal supervision.

## Method Summary
KRAST adapts a frozen CLIP backbone with learnable video and text prompts for 55-class human action recognition. Input videos are cropped to 224×224 person-centric regions using YOLOv11, resized from 1920×1080, and sampled at 32 frames per clip. Structured text descriptions for each action class are generated using ChatGPT and manually reviewed, then encoded with RoBERTa to initialize prompt vectors. The SegKPT strategy combines hierarchical, semantic, and discriminative text segments to create separable embeddings. Only prompt parameters are updated during training using contrastive loss with multi-class focal loss; the CLIP encoders remain frozen.

## Key Results
- Achieves 95.22% top-1 accuracy on ETRI-Activity3D cross-subject split
- SegKPT(S+H+D) outperforms CPT (87.14%), KeyPT, and baseline approaches
- 32 frames optimal; performance degrades with more frames due to noise
- Person-centric cropping with YOLOv11 improves signal-to-noise ratio for subtle indoor actions

## Why This Works (Mechanism)

### Mechanism 1
Structuring action descriptions into hierarchical, semantic, and discriminative segments creates a more separable feature space than generic prompts. By explicitly encoding differentiating attributes (e.g., "twisting" vs. "scrubbing") and hierarchical context (e.g., "personal care") into the text encoder, the model learns to pull visually similar actions apart in the embedding space. Core assumption: visual features contain subtle distinctions amplified when conditioned on fine-grained textual attributes. Break condition: if visual inputs are too low-resolution or occluded to show discriminative attributes, alignment may fail.

### Mechanism 2
Optimizing learnable prompts while keeping the VLM backbone frozen preserves generalizable visual features while adapting to the specific domain of indoor actions. The system updates lightweight prompt vectors to maximize alignment between video and text embeddings, forcing the frozen encoder to focus on action-relevant features without overwriting its pre-trained knowledge. Core assumption: the frozen CLIP backbone already possesses sufficient visual primitive detection capabilities. Break condition: if the target domain involves visual concepts entirely foreign to the pre-training dataset, frozen features may be insufficient.

### Mechanism 3
Person-centric cropping and specific temporal sampling (32 frames) maximize signal-to-noise ratio for subtle indoor actions. YOLO-based cropping removes background clutter, focusing compute on the subject. Uniform sampling of 32 frames captures sufficient motion dynamics for the VLM's attention mechanism without introducing temporal noise or redundancy. Core assumption: optimal frame count for indoor actions is lower than for long-term activities. Break condition: if the object detector fails to localize the person, the cropping mechanism will feed the model irrelevant background noise.

## Foundational Learning

- **Vision-Language Models (CLIP)**: Why needed: KRAST relies on the joint embedding space of CLIP to align video features with text descriptions. Quick check: How does contrastive pre-training allow a model to associate an image with a text caption without explicit classification labels?

- **Prompt Tuning (Soft Prompts)**: Why needed: The method adds learnable tokens to the input layer rather than fine-tuning weights. Quick check: What is the difference between "hard prompts" (discrete text) and "soft prompts" (continuous vectors) in terms of gradient flow?

- **Focal Loss**: Why needed: The dataset is imbalanced, and standard loss functions would bias the model toward high-frequency classes. Quick check: How does the focusing parameter ($\gamma$) in Focal Loss reduce the loss contribution from easy, well-classified examples?

## Architecture Onboarding

- **Component map**: Video Pipeline: Input → YOLOv11 (Crop) → Resize (456x256) → ViT Tokenizer → VitaVPL (Learnable Video Prompts) → Frozen CLIP Vision Encoder → Feature $F_V$. Text Pipeline: Action Class → Structured Description (H+S+D) → RoBERTa Embedding → MLP (Text Prompt Tuner) → Frozen CLIP Text Encoder → Feature $F_T$. Head: Cosine Similarity($F_V$, $F_T$) → Multi-class Focal Loss.

- **Critical path**: 1. Generate and validate structured text descriptions (Manual review of ChatGPT output is required). 2. Initialize prompts using these descriptions (SegKPT strategy). 3. Train prompts using the cross-subject split (IDs 3, 6, 9... for testing).

- **Design tradeoffs**: KeyPT vs. SegKPT: KeyPT is token-efficient but less descriptive. SegKPT achieves higher accuracy but requires careful prompt engineering to fit the 77-token limit. Frame Count: 32 frames is the optimal tradeoff; 8 frames lacks context, 70+ frames introduces noise and compute overhead.

- **Failure signatures**: Token Overflow: Descriptions exceeding 77 tokens will be truncated, losing discriminative details. Visual Similarity Collapse: Without the "Discriminative" (D) prompt segment, the model struggles to distinguish "washing hands" from "washing a towel." Overfitting: Continuous Prompt Tuning (CPT) without discrete knowledge priors may overfit to seen subjects.

- **First 3 experiments**: 1. Sanity Check (Frame Sampling): Train a baseline on 8 vs. 32 frames to verify the temporal resolution hypothesis. 2. Ablation (Prompt Strategy): Compare Baseline → +CPT → +KeyPT → +SegKPT(S+H+D) to isolate the value of structured text. 3. Inference Validation: Run the trained model on the specific cross-subject test set (IDs {3, 6, ...}) to ensure no subject leakage occurred during preprocessing.

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be extended to achieve robust cross-dataset generalization and domain adaptation in dynamic, unconstrained real-world environments? Basis: The Conclusion states that real-time deployment "necessitates tackling the challenges of dynamic environments through robust methods for cross-dataset generalization [and] domain adaptation." Unresolved because current experiments are restricted to ETRI-Activity3D and NTU RGB+D datasets. Evidence needed: Evaluation results showing the model maintaining high accuracy on entirely unseen environmental datasets without requiring fine-tuning.

### Open Question 2
Can the system effectively integrate continual learning to adapt to novel actions without forgetting previously learned knowledge? Basis: The Conclusion explicitly identifies the need for "continual learning to ensure the model can adapt to novel actions" for responsive human-robot interaction. Unresolved because the methodology trains on a fixed set of 55 classes and does not evaluate the model's plasticity when new action categories are introduced sequentially. Evidence needed: Demonstrating that prompt learners can be updated with new action classes incrementally while preserving performance on the original class set.

### Open Question 3
To what extent is performance dependent on manual curation of textual descriptions versus raw automated generation? Basis: The Methodology section notes that descriptions were "generated using a large language model (ChatGPT) and then reviewed and improved manually," suggesting a potential scalability bottleneck. Unresolved because the paper does not ablate the impact of human review. Evidence needed: An ablation study comparing model performance using raw LLM outputs versus the curated descriptions used in the final experiments.

## Limitations

- Architecture specification gaps: Exact VitaVPL video prompt architecture (token dimensions, layer configurations) is underspecified, creating ambiguity in faithful reproduction.
- Training hyperparameter omission: Key hyperparameters including batch size, learning rate, optimizer type, weight decay, and training epochs are not provided, significantly impacting reproducibility.
- YOLOv11 performance dependency: The method relies heavily on accurate person detection, but no validation of detection accuracy on the ETRI-Activity3D dataset is provided, creating a potential failure point.

## Confidence

**High Confidence:**
- KRAST achieves over 95% top-1 accuracy on ETRI-Activity3D with minimal supervision
- SegKPT(S+H+D) outperforms CPT, KeyPT, and baseline approaches
- 32 frames is optimal for this dataset (more frames degrade performance)

**Medium Confidence:**
- Structured text descriptions meaningfully improve action distinction
- Frozen CLIP backbone with prompt tuning is sufficient for this task
- Person-centric cropping improves signal-to-noise ratio

**Low Confidence:**
- Exact implementation details for VitaVPL and prompt combination
- Specific hyperparameter settings required for reproduction

## Next Checks

1. **Frame Sampling Validation**: Train a baseline model on 8 frames vs. 32 frames to empirically verify that 32 frames is indeed optimal and that 8 frames lack sufficient temporal context for indoor actions.

2. **Prompt Strategy Ablation**: Systematically compare Baseline → +CPT → +KeyPT → +SegKPT(S+H+D) on the same cross-subject split to isolate and quantify the contribution of structured text descriptions versus continuous prompt optimization.

3. **Detection Quality Audit**: Evaluate YOLOv11 person detection accuracy on the ETRI-Activity3D validation set, particularly for small subjects and cluttered backgrounds, to establish the reliability of the person-centric cropping pipeline.