---
ver: rpa2
title: 'ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings'
arxiv_id: '2511.07658'
source_url: https://arxiv.org/abs/2511.07658
tags:
- circuit
- design
- performance
- analog
- topologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZeroSim is a transformer-based framework for zero-shot performance
  evaluation of analog amplifier circuits. It uses a unified structural embedding
  strategy with global-aware tokens and hierarchical attention to generalize across
  topologies, combined with a topology-conditioned parameter mapping approach to maintain
  consistent representations regardless of parameter variations.
---

# ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings

## Quick Facts
- arXiv ID: 2511.07658
- Source URL: https://arxiv.org/abs/2511.07658
- Authors: Xiaomeng Yang; Jian Gao; Yanzhi Wang; Xuan Zhang
- Reference count: 39
- ZeroSim achieves 13× acceleration in RL-based sizing optimization compared to SPICE simulation

## Executive Summary
ZeroSim introduces a transformer-based framework for zero-shot performance evaluation of analog amplifier circuits. The system uses a unified structural embedding strategy with global-aware tokens and hierarchical attention to generalize across topologies, combined with topology-conditioned parameter mapping to maintain consistent representations regardless of parameter variations. Trained on 3.6 million instances covering over 60 amplifier topologies, ZeroSim delivers accurate predictions for unseen circuits without fine-tuning.

The framework demonstrates superior performance compared to MLP, GCN, DeepGEN, and GTN baselines, achieving MAPE of 0.143 and Acc@K of 0.645 while significantly accelerating reinforcement learning-based sizing optimization by 13× compared to traditional SPICE simulation methods.

## Method Summary
ZeroSim employs a transformer architecture that encodes both structural and parametric information of analog circuits through a unified embedding strategy. The approach uses global-aware tokens to capture circuit-level relationships and hierarchical attention mechanisms to process different abstraction levels. Topology-conditioned parameter mapping ensures consistent representations across varying circuit parameters. The model is trained on a large corpus of 3.6 million instances spanning over 60 amplifier topologies, enabling zero-shot generalization to unseen circuit designs without requiring fine-tuning.

## Key Results
- Achieves MAPE of 0.143 and Acc@K of 0.645 on performance prediction tasks
- Outperforms MLP, GCN, DeepGEN, and GTN baselines in zero-shot accuracy
- Accelerates RL-based sizing optimization by 13× compared to SPICE simulation
- Trained on 3.6 million instances covering 60+ amplifier topologies

## Why This Works (Mechanism)
The transformer architecture excels at capturing complex relationships in circuit structures through self-attention mechanisms. The unified structural embedding strategy allows the model to learn shared representations across different topologies, while hierarchical attention processes information at multiple abstraction levels. Topology-conditioned parameter mapping decouples structural and parametric variations, enabling the model to generalize to unseen parameter combinations within known topologies and even entirely new topologies.

## Foundational Learning
- **Transformer architectures**: Essential for capturing long-range dependencies in circuit structures; quick check: verify attention patterns align with known circuit behavior
- **Zero-shot learning**: Enables evaluation of unseen circuits without retraining; quick check: test on circuits outside training distribution
- **Graph neural networks**: Provide baseline comparisons for circuit representation learning; quick check: compare attention weights to GCN message passing
- **Reinforcement learning optimization**: Benefits from fast surrogate models for circuit sizing; quick check: measure RL convergence speed with different surrogate models
- **SPICE simulation**: Gold standard for circuit evaluation but computationally expensive; quick check: validate predicted values against SPICE for edge cases
- **Hierarchical attention**: Processes circuit information at multiple abstraction levels; quick check: visualize attention distributions across circuit hierarchies

## Architecture Onboarding
- **Component map**: Input → Structural Encoder → Parameter Encoder → Unified Embedding → Hierarchical Attention → Performance Prediction
- **Critical path**: Circuit topology encoding → Parameter integration → Attention-based feature refinement → Performance output
- **Design tradeoffs**: Unified embeddings vs. separate representations (flexibility vs. specialization), hierarchical vs. flat attention (computational cost vs. abstraction capability)
- **Failure signatures**: Poor performance on topologies with novel structural arrangements, degradation when parameter ranges exceed training distribution, attention collapse in highly regular circuits
- **First experiments**: 1) Validate embedding consistency across parameter variations, 2) Test zero-shot performance on held-out topologies, 3) Benchmark against SPICE on simple circuits

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot generalization may struggle with circuit topologies having significantly different structural characteristics than the 60+ amplifier types in the training set
- Performance metrics indicate reasonable accuracy but leave room for improvement in precision-critical applications
- The unified embedding strategy's effectiveness could diminish for circuits with novel component arrangements or unconventional topologies

## Confidence
- **High confidence**: Transformer architecture implementation, hierarchical attention mechanism, topology-conditioned parameter mapping approach
- **Medium confidence**: Zero-shot generalization performance across diverse amplifier topologies, unified embedding strategy's robustness to extreme parameter variations

## Next Checks
1. Test ZeroSim on circuit topologies with structural arrangements significantly different from the 60+ amplifier types in the training corpus to evaluate true zero-shot generalization limits
2. Conduct ablation studies comparing unified structural embeddings against separate topological and parametric embeddings on edge cases with extreme parameter variations
3. Validate the 13× speedup claim across different circuit complexity levels and on multiple SPICE simulators to ensure the acceleration is not simulator-dependent