---
ver: rpa2
title: 'MedPAO: A Protocol-Driven Agent for Structuring Medical Reports'
arxiv_id: '2510.04623'
source_url: https://arxiv.org/abs/2510.04623
tags:
- medical
- report
- agent
- concept
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedPAO introduces a protocol-driven agentic framework to structure
  medical reports, addressing the challenges of hallucination and rule-following in
  large language models. The system uses a Plan-Act-Observe loop to systematically
  decompose the report structuring task into verifiable steps guided by clinical protocols
  like ABCDEF.
---

# MedPAO: A Protocol-Driven Agent for Structuring Medical Reports

## Quick Facts
- arXiv ID: 2510.04623
- Source URL: https://arxiv.org/abs/2510.04623
- Reference count: 32
- F1-score of 0.96 on concept categorization; expert-rated 4.52/5

## Executive Summary
MedPAO introduces a protocol-driven agentic framework to structure medical reports, addressing the challenges of hallucination and rule-following in large language models. The system uses a Plan-Act-Observe loop to systematically decompose the report structuring task into verifiable steps guided by clinical protocols like ABCDEF. Specialized tools perform concept extraction, ontology mapping, filtering, categorization, and report generation. MedPAO achieves an F1-score of 0.96 on concept categorization and is rated 4.52/5 by expert radiologists, outperforming baseline LLM-only approaches.

## Method Summary
The method involves a six-tool pipeline using Deepseek-R1-70B for orchestration and a fine-tuned MedLlama-8B for concept extraction. The system follows a Plan-Act-Observe (PAO) loop where the LLM plans tool calls based on the ABCDEF protocol, executes extraction and ontology mapping via external APIs, filters and categorizes concepts, then generates structured reports. Training used 200 samples (40 expert-annotated + 160 synthetic) with LoRA fine-tuning on an Intel Gaudi2 AI accelerator. Evaluation was performed on MIMIC-CXR dataset using fuzzy matching and expert radiologist assessment.

## Key Results
- F1-score of 0.96 for concept categorization in ABCDEF protocol
- Expert radiologist rating of 4.52/5 for structured report quality
- Outperforms baseline LLM-only approaches on medical report structuring

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing report structuring into a protocol-grounded Plan-Act-Observe (PAO) loop reduces hallucination frequency compared to monolithic generation.
- **Mechanism:** The system forces the LLM to generate a step-by-step plan based on a clinical protocol (e.g., ABCDEF) before execution. By separating planning from generation and verifying each step (Observe), the LLM is constrained to a verifiable workflow rather than free-form text completion.
- **Core assumption:** The LLM possesses sufficient reasoning capability to decompose a task when explicitly prompted with a strict protocol structure.
- **Evidence anchors:** [abstract] "MedPAO decomposes the report structuring task into a transparent process managed by a Plan-Act-Observe (PAO) loop..."; [Section 1] "...operationalizes a clinical protocol as its core reasoning structure."; [corpus] ["Towards Engineering Multi-Agent LLMs"] supports protocol-driven approaches to mitigate agent failure modes.

### Mechanism 2
- **Claim:** Retrieving and filtering external ontologies acts as a semantic guardrail, improving categorization accuracy over parametric memory alone.
- **Mechanism:** Instead of relying on the LLM to "know" medical definitions, the system explicitly maps extracted terms to SNOMED-CT/RADLEX. It then uses the LLM to filter these into "Primary" (pathology) vs. "Secondary" (location/severity) concepts. This structured context grounds the subsequent categorization step.
- **Core assumption:** Mapping concepts to standard ontologies resolves lexical ambiguity (e.g., "cold" vs. "cold sore") better than context-free text processing.
- **Evidence anchors:** [Section 3.2 Tool 4] "...ontological information serves as critical contextual data, enabling more accurate and clinically relevant categorization..."; [Section 3.2 Tool 3] "...single concept may map to multiple ontologies... To automate this classification, we leverage the LLM engine."; [corpus] Weak direct evidence for the specific "Primary/Secondary" filtering mechanism in the provided corpus.

### Mechanism 3
- **Claim:** Fine-tuning a smaller model (MedLlama-8B) specifically for concept extraction yields higher precision than prompting larger, generalist models.
- **Mechanism:** The "Get Concept" tool uses a domain-specific fine-tuned model rather than the main Deepseek-R1 engine. This specialized model identifies source sentences and terms, effectively filtering noise before the reasoning-heavy steps begin.
- **Core assumption:** A LoRA-tuned 8B model trained on specific report-concept pairs generalizes better to new reports than a 70B+ generalist model for this specific extraction task.
- **Evidence anchors:** [Section 4.2] Table 1 shows "Our get concept tool" outperforming Deepseek-R1-70B and Qwen-7B on F1-scores (0.86 vs 0.87 macro / 0.91 vs 0.90 weighted); [Section 3.2 Tool 1] "...utilizes a fine-tuned MedLlama-8B model... ensuring traceability and reducing extraction errors."; [corpus] ["Structuring Radiology Reports: Challenging LLMs with Lightweight Models"] corroborates that lightweight models can challenge LLMs in this domain.

## Foundational Learning

- **Concept:** **Plan-Act-Observe (ReAct) Pattern**
  - **Why needed here:** This is the core agentic loop of MedPAO. Without understanding how "Observe" feeds back into "Plan" to verify tool outputs, the system looks like a simple script rather than an autonomous agent.
  - **Quick check question:** If a tool returns an empty result, does the agent stop immediately or replan based on the observation?

- **Concept:** **Medical Ontologies (SNOMED-CT / RADLEX)**
  - **Why needed here:** The paper relies on these standards to disambiguate medical terms. You cannot evaluate the "Ontology Filtering" tool without grasping that a single term like "mass" has specific hierarchical meanings in these systems.
  - **Quick check question:** Why is mapping a concept to an ontology ID safer than using the raw text string for LLM categorization?

- **Concept:** **LoRA (Low-Rank Adaptation) Fine-Tuning**
  - **Why needed here:** The "Get Concept" tool uses a LoRA-tuned model. Understanding this explains how the authors achieved high performance on concept extraction without retraining a massive model from scratch.
  - **Quick check question:** Does LoRA update all weights of the base Llama model, or just a small subset of adapters?

## Architecture Onboarding

- **Component map:** Controller: MCP Client + Deepseek-R1-70B; Specialist (Extraction): Fine-tuned MedLlama-8B; External Knowledge: BioPortal API; State: Local Cache
- **Critical path:** 1. Plan: Deepseek-R1 receives user prompt + tool list -> Returns tool call `get Concept`; 2. Act: MedLlama-8B extracts raw text concepts; 3. Act: BioPortal maps concepts to IDs; Deepseek filters them; 4. Act: Deepseek categorizes concepts into ABCDEF slots; 5. Act: Deepseek generates final structured report; 6. Observe: Deepseek checks if plan is complete; if yes, return result
- **Design tradeoffs:** Latency vs. Accuracy: The sequential pipeline (Table 5) takes ~280 seconds for a full report. This is too slow for real-time interactive use but acceptable for batch processing. Modularity vs. Complexity: Using separate tools (Extraction vs. Filtering) allows debugging but increases the surface area for API failures.
- **Failure signatures:** Hallucination Persistence: If "Tool 5: generate Report" ignores the categorized inputs and invents findings, the "Observe" step is failing to validate the output. Empty Loop: If the agent repeatedly calls `get Concept` or `check cache` without progressing, the "Plan" prompt may be missing the context of previous steps.
- **First 3 experiments:** 1. Ablation on Ontology: Disable the Ontology Mapping tool and run categorization directly on raw extracted concepts. Measure the drop in F1-score to quantify the value of semantic grounding. 2. Latency Optimization: Enable `check cache` (Tool 6) for a batch of 50 repeated reports to verify the claimed speedup (from 280s to 66s as per Table 5). 3. Extraction Robustness: Feed the agent a report with heavy negation (e.g., "No evidence of pneumonia") to see if the "Get Concept" tool correctly captures the negation or hallucinates the finding.

## Open Questions the Paper Calls Out
None

## Limitations
- System's reliance on ABCDEF protocol creates brittleness when clinical protocols evolve or differ across institutions
- Performance generalizability to other imaging modalities (MRI, CT) or clinical domains remains untested
- Evaluation relies heavily on expert human ratings without detailed inter-rater reliability metrics

## Confidence
- **High confidence:** The core PAO mechanism (Mechanism 1) and ontology-grounded categorization (Mechanism 2) are well-supported by the experimental results and literature. The F1-score of 0.96 for concept categorization is compelling evidence.
- **Medium confidence:** The fine-tuned concept extractor (Mechanism 3) shows good performance, but the ablation study comparing it against baseline LLMs is limited to a single dataset. The 280-second processing time per report suggests practical deployment constraints not fully addressed.
- **Low confidence:** The scalability claims and real-world deployment readiness are not substantiated beyond the controlled MIMIC-CXR environment.

## Next Checks
1. Cross-protocol validation: Test MedPAO with alternative clinical protocols (e.g., SOAP, HEADSS) to verify the PAO framework's adaptability beyond ABCDEF.
2. Real-time performance audit: Measure end-to-end latency on diverse report types and assess whether caching (Tool 6) provides consistent speedup across different usage patterns.
3. Error propagation analysis: Systematically inject controlled errors at each pipeline stage (extraction → ontology mapping → categorization → generation) to map how failures cascade and whether the "Observe" step effectively catches them.