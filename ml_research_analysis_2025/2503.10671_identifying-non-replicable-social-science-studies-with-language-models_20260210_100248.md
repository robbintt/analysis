---
ver: rpa2
title: Identifying Non-Replicable Social Science Studies with Language Models
arxiv_id: '2503.10671'
source_url: https://arxiv.org/abs/2503.10671
tags:
- llama
- qwen
- mistral
- effect
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  identify non-replicable findings in behavioral social science studies. Using 14
  studies from the Many Labs 2 replication project, the authors compare the replicability
  predictions from LLMs against human replication results.
---

# Identifying Non-Replicable Social Science Studies with Language Models

## Quick Facts
- arXiv ID: 2503.10671
- Source URL: https://arxiv.org/abs/2503.10671
- Reference count: 40
- Primary result: LLMs can predict non-replicable social science findings with F1 scores up to 77%

## Executive Summary
This study investigates whether large language models can identify non-replicable findings in behavioral social science studies. Using 14 studies from the Many Labs 2 replication project, the authors compare replicability predictions from LLMs against human replication results. They generate synthetic samples using open-source models (Llama 3 8B, Qwen 2 7B, Mistral 7B) and a proprietary model (GPT-4o) at different temperature settings to control variance in responses. The models achieve F1 scores of up to 77% with Mistral 7B, 67% with GPT-4o and Llama 3 8B, and 55% with Qwen 2 7B, indicating their potential to identify non-replicable studies. The study also finds that open-source models perform comparably to the proprietary model, and that low temperature settings lead to biased effect size estimations due to artificially reduced variance.

## Method Summary
The authors use LLMs to generate synthetic responses to behavioral social science experiments, then apply standard statistical tests to estimate effect sizes and significance. They test four models (GPT-4o, Llama 3 8B, Qwen 2 7B, Mistral 7B) at four temperature settings (0.1, 0.5, 1.0, 1.5) on 14 studies from Many Labs 2. Each model generates 1,000 synthetic responses per condition. The authors compare LLM predictions (significant same-direction effect = replicated) against human replication labels and compute F1 scores, precision, and recall. They find that low temperatures produce artificially inflated effect sizes due to variance reduction, while open-source models perform comparably to proprietary models.

## Key Results
- Mistral 7B achieves highest F1 score of 77% in predicting study replicability
- GPT-4o and Llama 3 8B achieve 67% F1 scores, Qwen 2 7B achieves 55% F1 score
- Low temperature settings (0.1, 0.5) produce artificially high effect sizes due to variance reduction
- Open-source models perform comparably to proprietary GPT-4o in replicability prediction

## Why This Works (Mechanism)

### Mechanism 1: Social Knowledge Encoded in Training Data
LLMs can predict behavioral experiment outcomes because their training data contains human behavioral patterns, allowing them to simulate "silicon samples" that reflect associations present in human populations. The models leverage learned statistical associations between experimental conditions and human behavioral responses encoded in their pre-training corpora. When prompted with experimental scenarios, they generate responses that reflect these underlying associations, which can then be analyzed using standard statistical methods to determine if an effect replicates. The associations learned by LLMs from text corpora align sufficiently with human behavioral tendencies to produce similar patterns of experimental effects.

### Mechanism 2: Low Variance ("Correct-Answer Effect") Biases Effect Size Estimation
Low sampling temperatures produce low response variance, leading to inflated or incalculable effect sizes due to near-zero standard deviations in synthetic samples. At low temperatures, LLMs repeatedly select the highest-probability tokens, reducing variance in generated responses. This "correct-answer effect" creates artificially narrow distributions that can artificially inflate effect size magnitudes, as the standard deviation in the denominator becomes very small. Temperature primarily affects variance without substantially shifting the underlying response distribution.

### Mechanism 3: Task Type and Sensitivity Impact Reliability
The reliability of LLMs for replicability prediction varies with the type of experimental task, particularly when tasks involve value-laden or sensitive topics that may trigger refusals or uneven distributions. Instruction-tuned LLMs (often via RLHF) may suppress or overwrite certain social, moral, or ethical views present in their training data. This leads to systematic biases in responses to value-laden questions, potentially causing refusals, skewed distributions, or responses that do not reflect human behavior. RLHF and instruction tuning do not completely erase relevant social associations but may distort them in predictable ways for sensitive topics.

## Foundational Learning

- **Concept: Sampling Temperature in LLMs**
  - Why needed here: Temperature is the primary control for response variance, which directly affects the statistical properties (mean, variance, effect size) of synthetic samples. Understanding its role is critical for designing valid LLM-based simulation experiments.
  - Quick check question: If you set temperature to 0.1 and observe a Cohen's d of 15.0, what is the likely cause and how would you adjust?

- **Concept: Effect Size (Cohen's d, Odds Ratio) and Statistical Significance**
  - Why needed here: The paper evaluates LLM performance by comparing effect sizes and significance from synthetic samples to human replication results. Interpreting these metrics is essential for understanding and reproducing the study's methodology.
  - Quick check question: Given two experimental conditions, if the mean difference is 2.0 and the pooled standard deviation is 0.5, what is Cohen's d? If you double the sample size, does the effect size change?

- **Concept: Replicability in Social Science (Many Labs 2)**
  - Why needed here: The paper uses the Many Labs 2 replication project as ground truth. Understanding what constitutes a "successful replication" (same direction, significant effect) is necessary to evaluate the LLM's predictive performance.
  - Quick check question: A replication finds an effect in the same direction as the original study but with a p-value of 0.08 (Î± = 0.05). Is this replication considered successful under the Many Labs 2 criterion?

## Architecture Onboarding

- **Component map:** Experimental scenario text -> Prompt formatting -> LLM generation -> Response parsing -> Statistical analysis -> Effect size computation -> Significance testing -> Performance evaluation
- **Critical path:** Extract and format study questions -> Generate synthetic samples at multiple temperatures -> Filter and validate responses -> Compute effect sizes and significance -> Aggregate predictions, compare to ground truth, calculate performance metrics
- **Design tradeoffs:** Temperature selection trades off noise vs. variance; model choice trades off transparency vs. instruction following; prompting strategy trades off efficiency vs. capability; filtering trades off data retention vs. quality
- **Failure signatures:** Zero/near-zero variance (all responses identical); high invalid token rate (>20% study discarded); model refusals on sensitive topics; direction flip (effect size changes sign across temperatures)
- **First 3 experiments:** Reproduce effect size vs. temperature relationship for one study using open-source model; compare response distributions between GPT-4o and Mistral 7B at temperature 1.0; test new study with existing human data to assess generalization

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LLMs accurately indicate the replicability of studies where effects are stratified by socio-demographic attributes (e.g., gender, age, ethnicity)? The paper states, "It remains an open question whether LLMs can or should be applied to studies where an effect of socio-demographic attributes is observed." This is unresolved because the current study excluded studies where socio-demographic characteristics were measured confounding variables. Evidence would come from applying the methodology to replication studies with strong demographic interaction effects.

- **Open Question 2:** Do LLM-based replication predictions generalize to a larger and more diverse sample of behavioral social science studies? The authors state that "due to the limited number of human replication studies, these results need to be further validated in a larger sample in future work." This is unresolved because findings rely on only 14 text-based studies from Many Labs 2, limiting statistical power. Evidence would come from testing the method on the full set of Many Labs studies or other large-scale replication projects.

- **Open Question 3:** Can alternative methods for increasing response variance, such as prefacing queries with random text, mitigate the "correct answer effect" more effectively than temperature adjustments? The authors note that "Other ways of increasing the variance of LLMs can be explored, such as prefacing queries with random text, but this is left to future research." This is unresolved because temperature adjustments created a trade-off: low temperatures caused low variance (bias), while high temperatures introduced noise (unusable data). Evidence would come from an ablation study comparing random-prompt prefacing versus standard temperature tuning.

- **Open Question 4:** How do non-instruction-tuned (base) models or encoder-only models compare to instruction-tuned LLMs in predicting study replicability? The Limitations section states, "Other types of models should be explored in future work, such as base models without tuning, encoder models, etc." This is unresolved because the study focused exclusively on instruction-tuned models to match proprietary models like GPT-4o, leaving other architectures untested. Evidence would come from replicating experiments using base versions of Llama or Mistral and encoder-based models adapted for generation.

## Limitations
- Small sample size of only 14 studies from Many Labs 2 limits statistical power and generalizability
- Certain sensitive topics led to model refusals or uneven response distributions that were difficult to remedy
- Temperature-variance relationship lacks direct external validation
- Use of single-letter response scales and specific prompt engineering may not represent optimal strategies

## Confidence

- **High Confidence:** Open-source models (particularly Mistral 7B) achieve comparable F1 scores to GPT-4o (77% vs 67%) given consistent methodology across models; low temperatures produce artificially inflated effect sizes due to variance reduction supported by empirical data showing extreme effect size magnitudes
- **Medium Confidence:** General claim that LLMs can predict non-replicable findings is supported but qualified by small sample size and potential domain-specific effects; performance differences between models show consistent patterns but need validation on additional studies
- **Low Confidence:** Mechanism by which social knowledge enables replicability prediction remains largely theoretical with limited direct evidence; related work shows LLMs simulate human behavior in economic games but direct evidence for replicability prediction is limited

## Next Checks
1. Systematically test the relationship between temperature, response variance, and effect size across multiple model families on a standardized task to confirm the "correct-answer effect" mechanism
2. Apply the methodology to studies from different subfields of social science (e.g., cognitive psychology, organizational behavior) to assess whether performance patterns hold across diverse experimental paradigms
3. Conduct an ablation study comparing different prompting strategies (e.g., chain-of-thought, few-shot examples) and response formats to determine whether the current single-letter approach represents best practice or whether alternative approaches could improve prediction accuracy