---
ver: rpa2
title: 'NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis
  and topic Classification toward multitask learning'
arxiv_id: '2506.23524'
source_url: https://arxiv.org/abs/2506.23524
tags:
- bert
- language
- dataset
- sentiment
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of Vietnamese datasets for educational
  sentiment and topic classification, particularly those capturing student slang and
  domain-specific discourse. The authors introduce NEU-ESC, a comprehensive dataset
  with over 32,000 samples from university forums, featuring richer class diversity,
  longer texts, and expanded vocabulary compared to existing resources.
---

# NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning

## Quick Facts
- arXiv ID: 2506.23524
- Source URL: https://arxiv.org/abs/2506.23524
- Reference count: 40
- Primary result: Introduced NEU-ESC dataset with 32,966 samples achieving 83.7% sentiment accuracy and 79.8% topic accuracy with multitask BERT models

## Executive Summary
This study addresses the lack of Vietnamese datasets for educational sentiment and topic classification, particularly those capturing student slang and domain-specific discourse. The authors introduce NEU-ESC, a comprehensive dataset with over 32,000 samples from university forums, featuring richer class diversity, longer texts, and expanded vocabulary compared to existing resources. They benchmark BERT-based models and explore multitask learning approaches, achieving state-of-the-art performance with up to 83.7% accuracy for sentiment classification and 79.8% for topic classification. The dataset and models outperform existing benchmarks and even competitive Large Language Models, particularly when using SMART loss regularization. NEU-ESC is publicly available and offers a valuable resource for Vietnamese NLP research in education.

## Method Summary
The authors curated NEU-ESC from university Facebook forums, preprocessing text to remove noise and expand Vietnamese slang acronyms. They fine-tuned BERT variants (PhoBERT, VisoBERT, XLM-R) using multitask learning with shared encoders and task-specific classification heads. The models were trained with combined cross-entropy losses for sentiment (4 classes) and topic (10 classes) classification, optionally including an MLM head scaled by average sentence length. SMART regularization was applied to improve generalization, particularly for the multitask setup. Models were evaluated on a 7:1:2 train-val-test split using accuracy and F1 scores.

## Key Results
- BERT-based multitask models with SMART regularization achieved 83.7% accuracy for sentiment classification and 79.8% for topic classification
- VisoBERT outperformed other BERT variants, particularly for topic classification, due to its pretraining on Vietnamese social media data
- Large Language Models (GPT-4o, Claude) underperformed BERT models, with topic classification accuracy as low as 42.31%
- Multitask learning with shared encoder improved performance over single-task fine-tuning by 0.1-0.6% across tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multitask learning with a shared BERT encoder improves generalization across sentiment and topic classification tasks compared to single-task fine-tuning.
- **Mechanism:** A unified encoder learns shared linguistic representations (syntax, semantics, domain-specific vocabulary) that transfer between tasks. Task-specific heads then specialize, while the shared layer benefits from gradient signals from both objectives, acting as implicit regularization.
- **Core assumption:** Sentiment and topic classification in educational discourse share underlying linguistic features (e.g., topic words correlate with sentiment polarity) that a single model can exploit.
- **Evidence anchors:** [abstract] "explore multitask learning using encoder-only language models (BERT), in which we showed that it achieves performance up to 83.7% and 79.8% accuracy"; [Section 4.2, Table 6] phoBERTbase-v2 accuracy increased by 0.1% (sentiment) and 0.6% (topic) in 2-task vs. single-task; XLM-R and VisoBERT show similar gains; [corpus] Neighbor papers on multitask sentiment-topic classification show mixed results; this mechanism is task-dependent, not universally proven
- **Break condition:** If tasks are conflicting (e.g., gradients oppose each other), shared representations may degrade. The paper notes PCGrad could address this but wasn't tested due to computational cost.

### Mechanism 2
- **Claim:** SMART loss regularization improves fine-tuning robustness and accuracy, particularly for multitask BERT models on limited labeled data.
- **Mechanism:** SMART adds a smoothness-inducing regularizer that penalizes large output changes under small input perturbations. This discourages overfitting to noisy or sparse training data, encouraging the model to learn smoother decision boundaries that generalize better.
- **Core assumption:** Overfitting during fine-tuning stems from "rough" loss landscapes; smoothness correlates with generalization.
- **Evidence anchors:** [Section 4.2] "When fine-tuning the multitask models using SMART loss, we observe a significant improvement in performance across both tasks. Notably, VisoBERT stands out... achieving accuracies of 83.58% for sentiment analysis and 79.80% for topic classification—its highest performance in all configurations."; [Section 2.3] Cites Jiang et al. [47] for SMART formulation; paper applies it without modification; [corpus] SMART regularization is not widely adopted in Vietnamese NLP; external validation for this domain is limited
- **Break condition:** Excessive regularization (high λs) may underfit, suppressing task-specific nuance. The paper does not report ablations on λs sensitivity.

### Mechanism 3
- **Claim:** Domain-specific curation of informal, slang-rich student discourse is necessary for educational sentiment models to generalize beyond formal corpora.
- **Mechanism:** Standard Vietnamese NLP datasets (e.g., UIT-VSFC) contain formal feedback with limited slang. NEU-ESC includes teen code, acronyms, and informal expressions via manual acronym mapping and noise removal, enabling models to learn robust representations of real-world student language.
- **Core assumption:** Informal language patterns in educational forums differ systematically from formal feedback; models trained only on formal data will misclassify slang-heavy inputs.
- **Evidence anchors:** [Section 3.1] "The use of teen code (slang, abbreviations, and phonetic spellings), as well as a significant number of typographical errors, was pervasive... we manually constructed a mapping dictionary that translates these acronyms into their full textual representations."; [Section 2.2] "while the UIT-VSFC dataset is relatively formal and lacks common slang, the others are not closely related to the education sector"; [corpus] Neighbor paper "Expanding Vietnamese SentiWordNet" similarly addresses slang, but for general-domain sentiment; education-specific slang handling remains underexplored
- **Break condition:** If slang evolves rapidly or varies across universities, static acronym dictionaries become stale. The paper notes plans for dataset updates but does not automate slang detection.

## Foundational Learning

- **Concept: Encoder-only Transformer (BERT)**
  - **Why needed here:** All experiments fine-tune BERT variants (PhoBERT, VisoBERT, XLM-R, vELECTRA). Understanding bidirectional self-attention, [CLS] token pooling, and MLM pretraining is prerequisite to interpreting results.
  - **Quick check question:** Can you explain why BERT uses a [CLS] token for classification tasks and how it differs from causal (decoder-only) attention?

- **Concept: Multitask Learning (Hard Parameter Sharing)**
  - **Why needed here:** The 2-head and 3-head architectures share a BERT encoder while splitting task-specific outputs. Gradients from each loss (classification, sentiment, MLM) are summed during backpropagation.
  - **Quick check question:** In hard parameter sharing, what is the risk of negative transfer between tasks, and how would you detect it during training?

- **Concept: Regularization via Smoothness Constraints**
  - **Why needed here:** SMART loss is central to the paper's best results. Understanding adversarial perturbation, Bregman proximal optimization, and the role of λs in balancing task loss vs. regularizer is critical for replication.
  - **Quick check question:** If SMART loss dominates training (λs too high), what symptom would you observe in validation accuracy curves?

## Architecture Onboarding

- **Component map:** Input text -> Tokenizer (WordPiece/subword) -> Shared BERT encoder -> Task-specific heads (sentiment: 4 classes, topic: 10 classes) -> Loss aggregation -> Output
- **Critical path:**
  1. Preprocess text: clean noise, map acronyms, tokenize
  2. Load pretrained BERT variant (VisoBERT or PhoBERT recommended per results)
  3. Attach 2-head classification layers; optionally add MLM head
  4. Fine-tune with cross-entropy loss; integrate SMART regularizer
  5. Evaluate on 7:1:2 train/val/test split; monitor accuracy and macro-F1

- **Design tradeoffs:**
  - **VisoBERT vs. PhoBERT:** VisoBERT pretrained on Vietnamese social media (best for slang); PhoBERT strongest on sentiment. Consider ensemble or task-specific selection.
  - **MLM head:** Adds language understanding regularization but doubles forward/backward passes (computationally expensive). Paper shows mixed gains.
  - **SMART vs. no SMART:** Consistent accuracy gains (~0.5–1.5%) but requires tuning λs and ε; adds complexity to training loop.
  - **LLMs vs. BERT:** LLMs (GPT-4o, Claude) underperform on topic classification (42.31% max accuracy) vs. BERT (79.80%); likely due to label space complexity and prompt limitations. BERT remains more efficient for this task.

- **Failure signatures:**
  - **Class imbalance:** Neutral (69%) and Other (44%) dominate; Toxic (2.6%) and Spam (1.2%) poorly predicted. Expect low recall on minority classes.
  - **Task confusion:** Confusion matrices show Negative/Positive often misclassified as Neutral; rare topics (Help & Share, Club & Events) confused with Other or Service.
  - **LLM failures:** LLMs struggle with 10-class topic space; zero-shot topic accuracy as low as 14.52% (GPT-4o). Indicates prompt-only approaches insufficient.

- **First 3 experiments:**
  1. **Baseline single-task:** Fine-tune VisoBERT and PhoBERT separately on sentiment and topic (BERT-Linear head). Record accuracy, macro-F1, weighted-F1 to establish benchmarks.
  2. **Multitask with SMART:** Implement 2-head architecture with shared VisoBERT encoder; add SMART regularizer (start with λs=1.0, ε=1e-6). Compare against single-task.
  3. **Ablation on MLM head:** Run 2-task + SMART both with and without MLM head. Measure accuracy gain vs. training time increase to assess cost-benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can expanding the dataset to include non-academic demographics and topics (e.g., career development, personal finance) resolve the model's difficulty in classifying underrepresented topics?
- **Basis in paper:** [explicit] The authors state in the Conclusion that the current scope is limited to "college life" and that future work involves expanding the dataset to "various age groups" and "career development" to capture a more accurate representation of real-world conversations.
- **Why unresolved:** The current dataset is imbalanced and specific to university students, causing the model to struggle with "semantically overlapping classes" and limiting generalizability.
- **What evidence would resolve it:** Improved classification accuracy on minority classes and better performance on external, non-university educational datasets after re-training on the expanded data.

### Open Question 2
- **Question:** Would gradient conflict resolution techniques (PCGrad) or parameter-efficient modules (Adapter Modules) yield superior performance compared to the SMART loss regularization used in this study?
- **Basis in paper:** [explicit] The Conclusion notes that PCGrad is a "great tool for resolving conflict" but was not used due to resource constraints. It also lists "ALICE" and "Adapter Modules" as potential future fine-tuning approaches.
- **Why unresolved:** The authors did not experiment with these specific architectural or optimization improvements due to a "lack of resources" and the high computational cost ($O(n^2)$) associated with PCGrad.
- **What evidence would resolve it:** Benchmarking the multitask BERT model with PCGrad or Adapters on the NEU-ESC dataset to compare accuracy and training efficiency against the SMART loss baseline.

### Open Question 3
- **Question:** Is fine-tuning smaller open-source Large Language Models (e.g., Qwen, LLaMA) more effective for this task than using proprietary models (GPT-4o, Claude) via prompt engineering?
- **Basis in paper:** [explicit] The Discussion section suggests that fine-tuning smaller LLMs might boost task-specific performance but requires deeper research to determine the optimal trade-off between computational demands and accuracy.
- **Why unresolved:** The authors observed that proprietary LLMs performed poorly (42.31% accuracy on topic classification) and hypothesized that fine-tuning smaller models might be the solution, but did not conduct the experiment.
- **What evidence would resolve it:** A comparative study evaluating the accuracy and inference cost of fine-tuned open-source LLMs against few-shot prompted proprietary models on the NEU-ESC test set.

## Limitations
- **Class imbalance sensitivity:** Severe skew toward Neutral sentiment (69.08%) and Other topic (44.00%) with underrepresented Toxic and Spam classes limits minority class performance.
- **Hyperparameter opacity:** Critical training details—learning rates, batch sizes, optimizer settings, SMART regularization parameters (λs, ε), and MLM masking ratios—are unspecified.
- **Dataset representativeness:** Corpus drawn from single Vietnamese university's Facebook forum; slang patterns may not generalize across institutions or regions.

## Confidence
- **High confidence:** BERT-based multitask models with SMART regularization outperform single-task and LLM baselines on NEU-ESC
- **Medium confidence:** Domain-specific slang handling improves model robustness
- **Low confidence:** Generalization to other Vietnamese educational forums or evolving slang without dataset updates

## Next Checks
1. **Class-wise performance audit:** Recompute per-class precision, recall, and F1 scores for all model configurations. Identify if high aggregate accuracy masks poor minority class detection, especially for Toxic and Spam.
2. **Hyperparameter sensitivity sweep:** Systematically vary SMART regularization strength (λs ∈ {0.1, 0.5, 1.0, 2.0}) and learning rates (1e-5, 2e-5, 3e-5) on validation set. Document accuracy curves and overfitting thresholds.
3. **Cross-institutional transferability test:** Apply the best-performing model to a small, independently collected dataset from a different Vietnamese university's forum. Measure accuracy drop to assess domain shift.