---
ver: rpa2
title: 'Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions'
arxiv_id: '2505.16311'
source_url: https://arxiv.org/abs/2505.16311
tags:
- reward
- regret
- where
- treatment
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces generator-mediated bandits (GAMBITTS), a
  new framework for online decision-making where actions influence rewards through
  stochastic, observed treatments rather than directly. The method is motivated by
  mobile health interventions using large language models to generate personalized
  text messages, but applies broadly to any setting where actions produce high-dimensional,
  stochastic outputs.
---

# Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions

## Quick Facts
- arXiv ID: 2505.16311
- Source URL: https://arxiv.org/abs/2505.16311
- Reference count: 40
- Primary result: Introduces GAMBITTS framework achieving lower regret than standard bandits when treatment embeddings are low-dimensional and well-specified

## Executive Summary
This paper introduces generator-mediated bandits (GAMBITTS), a new framework for online decision-making where actions influence rewards through stochastic, observed treatments rather than directly. The method is motivated by mobile health interventions using large language models to generate personalized text messages, but applies broadly to any setting where actions produce high-dimensional, stochastic outputs. GAMBITTS explicitly models both the treatment-generation process (how prompts produce messages) and the reward-prediction process (how messages affect outcomes), using Thompson sampling to balance exploration and exploitation.

## Method Summary
GAMBITTS models the system as a directed acyclic graph where actions (prompts) influence rewards through a stochastic treatment (LLM-generated text). The agent selects a prompt, observes the generated text and its embedding, and receives a reward signal. The framework uses Thompson sampling with two models: a treatment model estimating the distribution of embeddings given (action, context) pairs, and a reward model predicting outcomes from embeddings and context. Two variants exist: foGAMBITTS learns the treatment model online, while poGAMBITTS pre-trains it offline using simulation access to the generator.

## Key Results
- GAMBITTS achieves better regret bounds than standard bandits when the treatment embedding is low-dimensional and well-specified
- Performance benefits increase as the number of arms grows or outcome noise increases
- poGAMBITTS variant shows superior performance when offline simulation access to the generator is available
- Empirical results demonstrate consistent outperformance across multiple experimental conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly modeling the stochastic "treatment" generation process reduces the variance of policy learning compared to treating the system as a black-box bandit.
- **Mechanism:** Standard bandits map Action → Reward directly, conflating uncertainty from the generator (e.g., LLM randomness) and the environment (user response). GAMBITTS decomposes this into a treatment model ($P(Z|A,X)$) and a reward model ($E[Y|Z,X]$). By observing the realized treatment $Z$, the agent gains information about the reward structure independent of the action-to-treatment variance, allowing it to generalize across actions that produce similar treatments.
- **Core assumption:** The observed treatment embedding $Z$ captures sufficient information about the reward; the reward model $E[Y|Z,X]$ is identifiable and learnable.

### Mechanism 2
- **Claim:** Projecting high-dimensional generated content (text) into a low-dimensional embedding space allows the agent to generalize across prompts that produce semantically similar outputs.
- **Mechanism:** The framework posits that the effect of the high-dimensional generated text $G$ on reward $Y$ acts through a lower-dimensional latent embedding $Z^*$. By learning a working representation $Z = h(G)$, GAMBITTS can share statistical strength across different actions (prompts) if they yield similar embeddings, effectively reducing the effective action space size.
- **Core assumption:** The working embedding $h$ preserves the reward-relevant features of the text (i.e., $Y \perp Z^* | Z, X$ holds sufficiently well).

### Mechanism 3
- **Claim:** Pre-training the treatment model using offline simulation access to the generator significantly improves online sample efficiency.
- **Mechanism:** In the partially online variant (poGAMBITTS), the agent queries the LLM offline to estimate the distribution $P(Z|A,X)$ before interacting with the user. This removes the need to learn the "prompt-to-text" mapping online. The online phase then only needs to learn the "text-to-reward" mapping, isolating the user-specific signal from the generator's inherent randomness.
- **Core assumption:** The agent has "simulation access" to the generator (can query it without user interaction) and the offline query distribution matches the online context distribution.

## Foundational Learning

- **Concept:** **Thompson Sampling (Posterior Sampling)**
  - **Why needed here:** GAMBITTS is built on TS. You must understand how sampling from the posterior distribution of model parameters balances exploration (trying uncertain arms) and exploitation.
  - **Quick check question:** Can you explain why sampling a parameter $\theta_t$ from the posterior at each step $t$ induces implicit exploration?

- **Concept:** **Causal Mediation / Instrumental Variables**
  - **Why needed here:** The paper frames the problem as a mediation analysis where Action is the instrument, Treatment (Text) is the mediator, and Reward is the outcome. Understanding this directional flow is essential to distinguish between modeling the generator ($A \to Z$) and the outcome ($Z \to Y$).
  - **Quick check question:** In this framework, why is the observed text $G$ considered a "mediator" rather than a confounder?

- **Concept:** **Representation Learning (Sufficient Dimensionality Reduction)**
  - **Why needed here:** The algorithm relies on projecting raw text $G$ to an embedding $Z$. You need to understand that $Z$ must be a "sufficient statistic" for the reward; if $Z$ loses information relevant to $Y$, the bandit cannot learn effectively.
  - **Quick check question:** If your embedding $Z$ only captures the "length" of the message but the user cares about "tone," what specific failure mode in Section 6.2 would you expect to replicate?

## Architecture Onboarding

- **Component map:** Prompt Bank ($\mathcal{A}$) → Generator (LLM) → Embedder ($h$) → Reward Model ($m_2$) → Treatment Model ($f_1$)
- **Critical path:**
  1. Define the working embedding $h(G)$
  2. Select between foGAMBITTS (Online estimation of $f_1$) or poGAMBITTS (Offline pre-training of $f_1$)
  3. Implement the posterior update for the Reward Model $m_2$
  4. Execute the decision step: Sample $\theta \sim \text{Posterior} \to$ Compute $E[Y|a]$ for all $a \to$ Argmax
- **Design tradeoffs:**
  - fo vs. po: Use poGAMBITTS if you can query the LLM cheaply offline; it yields lower regret by isolating generator noise. Use foGAMBITTS if the generator is inaccessible offline or changes dynamically.
  - Linear vs. Ensemble: Linear models provide closed-form posteriors (faster, robust) but require well-specified embeddings. Ensemble models (MLP) handle misspecified embeddings better but are computationally heavier and require more tuning.
- **Failure signatures:**
  - Regret higher than Standard TS: Likely indicates the working embedding $Z$ is uncorrelated with the true reward mechanism (see Section 6.2).
  - High variance in poGAMBITTS: The offline sample size ($M_{off}$) for the treatment model is insufficient to estimate $P(Z|A,X)$.
- **First 3 experiments:**
  1. **Embedding Sanity Check:** Train a supervised model to predict reward from your embedding $Z$ using logged data. If predictive power is low, the bandit will fail regardless of the algorithm.
  2. **Baseline Comparison:** Run GAMBITTS against a Standard Contextual Thompson Sampler. Verify if GAMBITTS shows lower regret specifically when action space $K$ is large (e.g., $K > 20$).
  3. **Simulation Access Ablation:** Compare poGAMBITTS with varying levels of offline simulation samples (e.g., 15 vs. 500 samples per context) to determine the data budget required for a stable treatment model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the treatment embedding be learned online rather than pre-specified?
- **Basis in paper:** Explicit (Section 7).
- **Why unresolved:** Current performance depends heavily on the alignment between the working embedding and the true reward-relevant structure. The paper assumes a fixed embedding function.
- **What evidence would resolve it:** A modification of GAMBITTS that integrates online sufficient dimensionality reduction to learn the embedding structure concurrently with the policy.

### Open Question 2
- **Question:** How can the framework be extended to handle non-static generators that are fine-tuned based on observed outcomes?
- **Basis in paper:** Explicit (Section 7).
- **Why unresolved:** The theoretical analysis and algorithms assume a fixed generative model; fine-tuning introduces non-stationarity into the treatment distribution that invalidates current regret bounds.
- **What evidence would resolve it:** Regret bounds and algorithmic variants that explicitly model the temporal drift in the generator's parameters during deployment.

### Open Question 3
- **Question:** How can safety constraints, such as content filters, be incorporated into the generator-mediated bandit framework?
- **Basis in paper:** Explicit (Section 7).
- **Why unresolved:** The current framework assumes the generated response is always delivered, whereas practical deployment in sensitive domains often requires deterministic or stochastic checks prior to delivery.
- **What evidence would resolve it:** Theoretical analysis and algorithmic adaptations that account for an intermediary safety layer between generation and environment delivery.

## Limitations

- GAMBITTS' superiority depends critically on the assumption that the working embedding Z captures reward-relevant features of the generated treatment.
- The theoretical regret bounds rely on strong assumptions about low-dimensional embeddings and well-specified treatment models that may not hold in practice.
- The framework requires access to the generator (LLM) for either online or offline treatment modeling, which may not be feasible for all real-world applications.

## Confidence

- **High confidence:** The mechanism that explicitly modeling the treatment-generation process reduces variance when the embedding is well-specified.
- **Medium confidence:** The theoretical regret bounds, as they depend on assumptions about low-dimensional embeddings and the relationship between treatment and outcome noise.
- **Medium confidence:** The superiority of poGAMBITTS over foGAMBITTS, as this depends on the quality and representativeness of offline simulation samples.

## Next Checks

1. **Embedding Sensitivity Analysis:** Systematically vary the dimensionality and alignment of the working embedding Z to quantify the performance degradation threshold where GAMBITTS underperforms standard TS.

2. **Distribution Shift Robustness:** Test poGAMBITTS under generator drift by introducing controlled distribution shifts between offline training and online deployment to measure performance degradation.

3. **Scaling with Arm Count:** Validate the claim that GAMBITTS benefits increase with K by testing across a broader range of arm counts (e.g., K=10 to K=100) to identify the regime where the advantage becomes substantial.