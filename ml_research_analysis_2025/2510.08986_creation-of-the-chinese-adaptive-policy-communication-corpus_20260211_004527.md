---
ver: rpa2
title: Creation of the Chinese Adaptive Policy Communication Corpus
arxiv_id: '2510.08986'
source_url: https://arxiv.org/abs/2510.08986
tags:
- policy
- chinese
- corpus
- dataset
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAPC-CG, the first open dataset of Chinese
  policy directives annotated with a five-color taxonomy of clear and ambiguous language
  categories, spanning 1949-2023. The dataset covers 337,056 unique policy documents
  segmented into 3.3 million analyzable paragraphs.
---

# Creation of the Chinese Adaptive Policy Communication Corpus

## Quick Facts
- arXiv ID: 2510.08986
- Source URL: https://arxiv.org/abs/2510.08986
- Reference count: 24
- First open dataset of Chinese policy directives with five-color taxonomy annotation

## Executive Summary
This paper introduces CAPC-CG, the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories, spanning 1949-2023. The dataset covers 337,056 unique policy documents segmented into 3.3 million analyzable paragraphs. Using a hybrid human-LLM annotation pipeline, the authors trained expert annotators to achieve high inter-annotator agreement (κ = 0.86) on directive labels. They fine-tuned GPT-4o-mini on a gold-standard set of 6,000 paragraphs to classify paragraphs into five categories (Red, Black, Grey, Yellow, Charcoal) with strong performance (Level-1: κ = 0.841, Level-2: κ = 0.833). The corpus provides metadata-rich structure and baseline models for downstream NLP tasks in policy communication, enabling large-scale analysis of how central authorities use language to direct local implementation in China's adaptive governance system.

## Method Summary
The authors developed a two-level classification system for Chinese policy paragraphs. Level-1 classifies paragraphs as W (affirmative directive), R (prohibition), or N (no directive). Level-2, applied only to W paragraphs, further classifies them as B (authorize), C (flexible mandate), G (ambiguous), or Y (high-pressure mandate). The dataset contains 6,000 gold-standard annotated paragraphs (80/20 train/val split) plus a separate 1,900-sample test set. Raw documents span 1949–2023 and were segmented into 3.3M paragraphs. The authors fine-tuned GPT-4o-mini on the gold-standard data using two-round labeling (Level-1 first, then Level-2). Prompts for both levels are provided in Appendix C. Zero-shot baseline achieved κ=0.618; fine-tuning achieved κ=0.841 (Level-1) and κ=0.833 (Level-2).

## Key Results
- Achieved high inter-annotator agreement (κ = 0.86) on directive labels
- Fine-tuned GPT-4o-mini achieved Level-1 classification performance κ = 0.841 and Level-2 classification performance κ = 0.833
- Dataset covers 337,056 unique policy documents spanning 1949-2023
- Documents segmented into 3.3 million analyzable paragraphs
- Five-color taxonomy (Red, Black, Grey, Yellow, Charcoal) enables nuanced analysis of policy communication strategies

## Why This Works (Mechanism)
The approach works by combining human expertise with LLM capabilities through a hybrid annotation pipeline. Expert annotators first established gold-standard labels using detailed guidelines, achieving high agreement (κ = 0.86). This high-quality labeled data then trained the LLM to recognize the nuanced distinctions between directive types. The two-level classification system captures both the presence/absence of directives (Level-1) and their specificity/flexibility (Level-2). The five-color taxonomy maps directly to different implementation strategies: Red/Black for clear prohibitions/permissions, Yellow for specific mandates, Charcoal for high-pressure mandates, and Grey for ambiguous guidance. This structure enables the model to learn not just whether a directive exists, but how it is intended to guide implementation.

## Foundational Learning
- **Cohen's Kappa**: Measures inter-annotator agreement, accounting for chance agreement. Why needed: Provides reliable measure of annotation quality for subjective classification tasks. Quick check: Values > 0.8 indicate excellent agreement.
- **Directive classification taxonomy**: Five-color system (Red, Black, Grey, Yellow, Charcoal) mapping to implementation strategies. Why needed: Enables systematic analysis of how language choices affect policy implementation. Quick check: Ensure all classes are mutually exclusive and collectively exhaustive.
- **Two-level classification**: First identify directive presence, then classify type. Why needed: Reduces complexity by separating binary and multi-class tasks. Quick check: Verify that Level-2 only applies to Level-1 W (directive) cases.
- **Hybrid human-LLM annotation**: Combining expert human annotation with LLM fine-tuning. Why needed: Leverages human expertise for training data while enabling scalable classification. Quick check: Compare human vs. LLM agreement on validation set.
- **Chinese policy document segmentation**: Converting raw documents into analyzable paragraphs. Why needed: Enables fine-grained analysis of directive language. Quick check: Sample segmented paragraphs to verify quality.
- **Zero-shot vs. fine-tuned baselines**: Comparing LLM performance with and without training. Why needed: Demonstrates value of the annotated corpus. Quick check: Confirm fine-tuned model outperforms zero-shot baseline by substantial margin.

## Architecture Onboarding

**Component map**: Raw documents -> Document segmentation -> Paragraph extraction (3.3M) -> Gold-standard annotation (6,000) -> LLM fine-tuning (GPT-4o-mini) -> Classification model -> CAPC-CG corpus

**Critical path**: Raw documents → Document segmentation → Gold-standard annotation → LLM fine-tuning → Classification model → CAPC-CG corpus

**Design tradeoffs**: The authors chose a five-color taxonomy that balances granularity with practical usability. More categories would capture more nuance but reduce inter-annotator agreement and model performance. The two-level classification approach reduces cognitive load on both human annotators and the model. Using GPT-4o-mini (rather than larger models) balances performance with computational efficiency. The 80/20 train/val split provides sufficient validation data while maximizing training data.

**Failure signatures**: Poor class balance in Level-2 (B/C/G/Y) leads to degraded performance on minority classes. Over-reliance on keywords (e.g., "必须" → Y) causes misclassification when context differs. Incomplete segmentation may fragment or merge directives, affecting classification accuracy. Insufficient gold-standard data in certain policy domains may bias model performance.

**3 first experiments**:
1. Verify class balance across B/C/G/Y categories in Level-2 and analyze per-class F1 scores
2. Spot-check 50 predictions where keyword appears but label differs to test keyword-overfitting
3. Compare human vs. LLM agreement on a validation subset to confirm model reliability

## Open Questions the Paper Calls Out

**Open Question 1**: Do provincial and municipal governments interpret and implement central directives differently depending on whether they receive clear mandates (Yellow/Black) versus flexible guidance (Charcoal/Grey)?
- Basis in paper: [explicit] Authors state that future work "can collect directives issued by provincial and municipal governments to enable analysis of multi-level policy communication and diffusion."
- Why unresolved: CAPC-CG only covers central-level documents; no local-level correspondence or implementation data is included to trace how ambiguity versus clarity affects downstream interpretation.
- What evidence would resolve it: A matched corpus of local implementation documents paired with central directives, with annotation of local-level interpretation variance.

**Open Question 2**: Can NLP models be trained to reliably detect strategic ambiguity in policy texts beyond the five-category taxonomy, capturing gradations of hedging, conditional authorization, and delegated discretion?
- Basis in paper: [explicit] Authors note that "capturing the nuances of Chinese policy directives" is "a key challenge for the NLP community" and call for models "deeply attuned to the subtleties of political language."
- Why unresolved: Current models achieve κ ≈ 0.84, but the taxonomy collapses complex pragmatic phenomena into five discrete categories; finer-grained ambiguity detection remains untested.
- What evidence would resolve it: Annotations at sub-category granularity (e.g., degree of hedging, scope of delegation) and benchmarking against human expert judgments on edge cases.

**Open Question 3**: To what extent does the public-facing corpus systematically differ from internal or restricted policy discourse during politically sensitive periods (e.g., 1966–1976)?
- Basis in paper: [explicit] Authors acknowledge that "numerous documents from politically sensitive periods, such as the Cultural Revolution (1966-1976), are intentionally withheld" and that the corpus represents "China's public-facing policy landscape."
- Why unresolved: The archival gap means temporal analyses may conflate genuine policy shifts with publication biases; we cannot assess how representative the public record is.
- What evidence would resolve it: Comparison with declassified internal archives (where available) or cross-validation against contemporaneous secondary sources to estimate omission patterns.

**Open Question 4**: Under what conditions do central authorities prefer Grey or Charcoal directives over clear mandates (Yellow/Black), and does this correlate with policy domain uncertainty or regime priorities?
- Basis in paper: [explicit] Authors ask: "why and when Chinese central authorities choose to be ambiguous to encourage policy experimentation," positioning this as a core question for adaptive political economy.
- Why unresolved: The paper describes temporal and domain-level patterns (e.g., Charcoal dominates technology domains) but does not test causal mechanisms or develop predictive models of directive choice.
- What evidence would resolve it: Regression or causal inference models linking directive type frequencies to covariates such as policy domain risk, economic conditions, or leadership turnover, validated against qualitative case studies.

## Limitations

- Public dataset URL and release date not specified, creating uncertainty about data access
- Fine-tuning hyperparameters (epochs, batch size, learning rate multiplier) not detailed, affecting exact reproduction
- Mention of "GPT-4.1-mini-2025-04-14" appears to be an error since this model does not exist publicly
- Incomplete segmentation pipeline details create uncertainty about how 3.3 million paragraphs were derived
- Temporal analyses may be biased due to systematic omission of documents from politically sensitive periods

## Confidence

**Dataset construction**: Medium - Well-described annotation scheme with high inter-annotator agreement, but missing technical specifications for segmentation and uncertain public access
**Annotation methodology**: High - Clear guidelines, validation process, and κ = 0.86 inter-annotator agreement
**Model fine-tuning**: Low - Performance metrics provided but missing critical hyperparameters and potential model specification errors
**Taxonomy validity**: Medium - Five-color system well-justified but untested at finer granularity
**Generalizability**: Medium - Strong within Chinese policy context but uncertain performance on non-policy or non-Chinese texts

## Next Checks

1. Verify dataset availability and download the CAPC-CG corpus to confirm document count (337,056) and paragraph count (3.3M) match paper specifications
2. Test the fine-tuning procedure using GPT-4o-mini with the provided prompts to replicate the Level-1 and Level-2 classification performance metrics
3. Conduct a spot-check of 100 randomly sampled paragraphs from the gold-standard set to verify the five-color taxonomy labels align with the annotation guidelines in Appendix A