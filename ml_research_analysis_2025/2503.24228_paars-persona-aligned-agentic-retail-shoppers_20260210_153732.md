---
ver: rpa2
title: 'PAARS: Persona Aligned Agentic Retail Shoppers'
arxiv_id: '2503.24228'
source_url: https://arxiv.org/abs/2503.24228
tags:
- persona
- alignment
- human
- agents
- shopping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAARS, a framework for simulating human shoppers
  using LLM-powered agents equipped with personas derived from anonymized historical
  shopping data. The framework includes an alignment suite that measures both individual
  and group-level similarity between human and agent populations across tasks like
  query generation, item selection, and session generation.
---

# PAARS: Persona Aligned Agentic Retail Shoppers

## Quick Facts
- arXiv ID: 2503.24228
- Source URL: https://arxiv.org/abs/2503.24228
- Reference count: 40
- Primary result: Persona-driven LLM agents improve alignment with human shopping behavior across query generation, item selection, and session generation tasks, enabling scalable behavioral simulation for retail applications.

## Executive Summary
This paper introduces PAARS, a framework for simulating human shoppers using LLM-powered agents equipped with personas derived from anonymized historical shopping data. The framework includes an alignment suite that measures both individual and group-level similarity between human and agent populations across tasks like query generation, item selection, and session generation. Experiments show that personas improve alignment metrics—for example, query generation similarity increases from 0.59 to 0.69, and item selection accuracy rises from 25.46% to 47.26%. Group alignment also improves, with KL divergence reductions in all tasks. The authors demonstrate an initial application for agentic A/B testing, showing directional agreement with human results. PAARS enables scalable, privacy-preserving behavioral simulation for retail and other domains.

## Method Summary
PAARS uses anonymized shopping histories to create personas through a two-step prompting process: first extracting consumer profiles (age, interests) then inferring shopping preferences (price sensitivity, brand reliance). These personas condition LLM agents (Claude Sonnet 3.0) equipped with retail tools (search, view, cart, purchase) that interact with a textual simulated retail environment. The alignment suite evaluates agent behavior against held-out human test sets using individual metrics (cosine similarity, accuracy) and group metrics (KL divergence via KDE on sentence embeddings). The framework demonstrates improved alignment with personas and shows directional agreement with human A/B testing results, though effect magnitudes are exaggerated (10-30x).

## Key Results
- Query generation similarity improves from 0.59 to 0.69 with personas
- Item selection accuracy increases from 25.46% to 47.26% with personas
- Group alignment (KL divergence) improves across all tasks with personas
- Agentic A/B testing shows directional agreement with human results (2/3 tests)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Condition: If an LLM agent is prompted with a persona synthesized from a user's historical behavioral data, THEN its task performance will align more closely with that user's actions compared to a non-persona baseline.
- Mechanism: A two-step prompting process extracts a structured persona from raw shopping history. First, a consumer profile (age, interests, etc.) is synthesized; second, shopping preferences (price sensitivity, brand reliance) are inferred. This persona conditions the agent, constraining its output distribution to match the inferred user characteristics during tasks like query generation or item selection.
- Core assumption: The LLM can accurately infer stable, predictive user traits from behavioral history, and these traits causally influence shopping decisions in a way the model can emulate.
- Evidence anchors:
  - [abstract] "...creates synthetic shopping agents by automatically mining personas from anonymised historical shopping data...Experimental results demonstrate that using personas improves performance on the alignment suite..."
  - [section] Table 3 shows item selection accuracy improving from 25.46% (Base) to 47.26% (+ Persona). Section 3 details the two-step persona mining process.
  - [corpus] Corpus evidence for this specific two-step behavioral-to-persona pipeline is weak in direct neighbors. Related work like "Population-Aligned Persona Generation" addresses persona creation but not this exact mechanism.
- Break condition: The mechanism likely fails if user preferences are highly volatile, contradictory, or if the persona inference step produces noisy or stereotypical profiles, potentially reducing alignment for certain subgroups.

### Mechanism 2
- Claim: Condition: If group-level alignment metrics (e.g., KL divergence) are used to evaluate a population of agents, THEN they can provide a sufficient signal for applications like A/B testing simulation, even when individual-level alignment is imperfect.
- Mechanism: Group alignment measures the distributional similarity between the collective outputs of an agent population and a human population (e.g., embedding distributions of queries, rank distributions of clicked items). This is a "weaker condition" than individual alignment, which requires an agent to mimic its specific human counterpart. The paper argues that for aggregate tasks, correct population-level statistics are sufficient.
- Core assumption: Key population behaviors can be captured by the statistical distributions of selected metrics (e.g., click rank, query semantics), and minimizing divergence on these distributions correlates with downstream utility in tasks like A/B testing.
- Evidence anchors:
  - [abstract] "...introduces a novel alignment suite measuring distributional differences between humans and shopping agents at the group (i.e. population) level..."
  - [section] Section 4.1 explicitly contrasts group vs. individual alignment and argues group alignment is "more tractable" for large populations. Appendix B provides a dice-rolling example showing how a system with poor individual accuracy can have perfect group alignment.
  - [corpus] Weak corpus support for this specific group/individual distinction in provided neighbors.
- Break condition: The metric is insufficient if the chosen distributions (e.g., simple frequency counts) fail to capture complex behavioral dynamics, such as temporal sequences or inter-item dependencies, leading to a misleadingly high alignment score.

### Mechanism 3
- Claim: Condition: If persona-driven agents interact with a simulated retail environment reflecting an A/B test's control and treatment variants, THEN the simulated outcome (e.g., sales direction) will agree with the historical human test.
- Mechanism: The framework sets up parallel simulated environments (Control and Treatment) that differ only in the specific feature being tested (e.g., search ranking algorithms). Agents, conditioned by their personas, generate sessions in both. The aggregate metric (e.g., sales change) is computed and compared to the historical human result.
- Core assumption: The textual simulation captures the primary causal drivers of the A/B test outcome, and persona-equipped agents model the relevant human behavioral sensitivities to those drivers.
- Evidence anchors:
  - [abstract] "We showcase an initial application of our framework for automated agentic A/B testing and compare the findings to human results."
  - [section] Section 5 "A/B testing simulation" reports "directional agreement...for 2 A/B tests out of 3." It also notes a key limitation: "the magnitude of Sales change is bigger for the simulated environments (10-30x)."
  - [corpus] The paper "LLM Agent Meets Agentic AI" from the corpus supports the general concept of using agents for evaluation, though not specifically for A/B testing.
- Break condition: The mechanism fails if the simulation omits critical real-world factors (e.g., visual cues, latency, shipping options) or if agent behavior is inherently biased (e.g., a "purchasing intention" bias leading to exaggerated sales), leading to incorrect or scaled-incorrect results.

## Foundational Learning

- Concept: **KL Divergence for Distributional Alignment**
  - Why needed here: This is the core mathematical tool for quantifying how well the agent population's behavior matches the human population's at a group level.
  - Quick check question: If a model always predicts the single most common action in a population, will it achieve a low KL divergence against the true, diverse human population? (Hint: No, because the predicted distribution is a single spike, very different from the human one).

- Concept: **Persona-based Conditioning**
  - Why needed here: This is the primary intervention the paper explores for improving simulation fidelity, acting as the bridge between historical data and agent behavior.
  - Quick check question: What is the expected effect of providing a "budget-conscious student" persona prompt to an LLM on its output for a query generation task about laptops? (Hint: It should bias the query towards terms like "cheap," "refurbished," or "student discount").

- Concept: **Monte Carlo Estimation for High-Dimensional Distributions**
  - Why needed here: The paper uses this technique to estimate KL divergence in continuous, high-dimensional spaces like sentence embeddings, which is a common challenge in NLP evaluation.
  - Quick check question: Why can't you simply use the discrete histogram-based KL divergence formula directly on 384-dimensional sentence embeddings? (Hint: You would need an impractical number of bins to cover the space, and most would be empty).

## Architecture Onboarding

- Component map:
  1. Persona Mining Module: Takes anonymized shopping history (sessions, purchases) and outputs a structured persona (consumer profile + shopping preferences).
  2. Agent Core: An LLM (e.g., Claude Sonnet) prompted with the persona and equipped with retail tools (search, view, cart, purchase).
  3. Simulated Environment: A textual interface that mimics a retail website, returning search results and product details based on agent tool calls.
  4. Alignment Suite: A set of evaluation tasks (Query Generation, Item Selection, Session Generation) with specific individual and group-level metrics.
  5. Evaluation Pipelines: Compute metrics like semantic similarity, accuracy, and KL divergence to compare agent outputs against a held-out human test set.

- Critical path: The end-to-end flow is: Raw History → Persona Mining → Agent Instantiation → Task Execution in Simulated Environment → Output Collection → Group Alignment Computation. The most critical step for the paper's claims is the Persona Mining prompt design, as it determines the quality of the behavioral conditioning.

- Design tradeoffs:
  - Persona Complexity: A detailed persona (profile + preferences + history + reasoning) improves alignment but increases prompt length and cost. The paper shows incremental gains from adding components (Table 3).
  - Group vs. Individual Focus: Designing for group alignment relaxes constraints, making the problem more tractable but potentially masking poor individual mimicry.
  - Environment Fidelity: A textual-only simulation is scalable but lacks visual and experiential elements, which the authors identify as a key limitation.

- Failure signatures:
  - Exaggerated Effect Magnitude: In A/B tests, agents show 10-30x larger sales changes than humans. This likely indicates a "purchasing intention" bias in the session generation prompt.
  - Diversity Gap: Agents (even with personas) show lower Token-Type-Ratio (TTR) in queries and viewed products compared to humans, indicating simulated behavior is less diverse.
  - Poor Alignment on Low-Perplexity Queries: Query prediction degrades for highly unique or context-dependent queries, suggesting persona conditioning is insufficient for niche needs.

- First 3 experiments:
  1. Reproduce Persona Ablation: Re-run the item selection task using different persona components (None, Profile only, Preferences only, Full Persona) on a small sample. Verify that the Full Persona yields the highest accuracy and compare your numbers to Table 3.
  2. Implement Group Alignment for a Simplified Task: Create a small dataset of human "dice rolls" (a simple 5-sided distribution). Implement both a system that always predicts '3' and one that predicts randomly. Compute individual accuracy and KL divergence for both, reproducing the intuition from Appendix B.
  3. Profile a Single Persona: Take one example shopping history and manually run the persona mining prompts (from Appendix C). Inspect the generated profile and preferences for plausibility and hallucination. Use this persona to prompt an agent for a single query generation turn and compare the output with and without the persona.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework is demonstrated only on Amazon shopping data; generalization to other retail domains remains untested.
- The textual-only simulated environment lacks visual and experiential elements present in real shopping, contributing to exaggerated effect magnitudes in A/B testing.
- KL divergence estimation via KDE is sensitive to bandwidth selection, potentially introducing artifacts in alignment measurements.

## Confidence

- **High Confidence**: The core mechanism that persona conditioning improves individual task performance (query generation, item selection) is well-supported by ablation studies (Table 3). The concept of using distributional metrics for group alignment is mathematically sound.
- **Medium Confidence**: The group alignment framework provides a useful signal for aggregate tasks like A/B testing, but the magnitude errors (10-30x) indicate the simulation is not yet ready for precise quantitative predictions. The framework shows directional agreement, which is valuable but limited.
- **Low Confidence**: The long-term stability and predictive power of the mined personas is unknown. The paper does not test whether personas inferred from 6-month-old data remain accurate for current behavior, nor does it evaluate robustness to data sparsity or noise.

## Next Checks

1. **Cross-Domain Persona Transfer**: Apply the PAARS framework to a non-Amazon retail dataset (e.g., fashion or grocery e-commerce data) and re-evaluate alignment metrics. Document any drops in performance and analyze whether persona components need domain-specific adaptation.

2. **Visual Feature Integration**: Augment the simulated environment with basic visual cues (e.g., product categories, price ranges displayed as text) and measure the impact on agent behavior diversity (TTR) and A/B test magnitude accuracy. This tests whether the lack of visual information is a primary driver of current limitations.

3. **KL Divergence Robustness Test**: Systematically vary the KDE bandwidth parameter (e.g., 0.01, 0.1, 1.0) and sample size (e.g., 100, 1000, 5000) in the alignment suite. Report the variance in KL divergence scores and assess whether claimed improvements between persona conditions remain statistically significant across the parameter sweep.