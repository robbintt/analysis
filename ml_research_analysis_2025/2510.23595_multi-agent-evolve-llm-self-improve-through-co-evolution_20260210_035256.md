---
ver: rpa2
title: 'Multi-Agent Evolve: LLM Self-Improve through Co-evolution'
arxiv_id: '2510.23595'
source_url: https://arxiv.org/abs/2510.23595
tags:
- question
- answer
- judge
- questions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Multi-Agent Evolve (MAE) is a self-improving framework that uses\
  \ three interacting LLM roles\u2014Proposer, Solver, and Judge\u2014to generate\
  \ and solve questions without human-curated data. The Proposer generates questions,\
  \ the Solver answers them, and the Judge evaluates both, providing domain-agnostic\
  \ rewards."
---

# Multi-Agent Evolve: LLM Self-Improve through Co-evolution

## Quick Facts
- arXiv ID: 2510.23595
- Source URL: https://arxiv.org/abs/2510.23595
- Reference count: 40
- Primary result: MAE achieves 4.54% average improvement across math, coding, reasoning, and general knowledge benchmarks on Qwen2.5-3B-Instruct, outperforming base and SFT baselines.

## Executive Summary
Multi-Agent Evolve (MAE) is a self-improving framework that uses three interacting LLM roles—Proposer, Solver, and Judge—to generate and solve questions without human-curated data. The Proposer generates questions, the Solver answers them, and the Judge evaluates both, providing domain-agnostic rewards. The system employs difficulty-aware rewards, format rewards, and quality filtering to ensure stable training. On Qwen2.5-3B-Instruct, MAE achieves a 4.54% average improvement across math, coding, reasoning, and general knowledge benchmarks, outperforming both base and supervised fine-tuning baselines. Ablations confirm the necessity of each role and the effectiveness of quality filtering and format reward mechanisms.

## Method Summary
MAE implements a co-evolutionary loop where three LLM agents—Proposer, Solver, and Judge—interact to improve reasoning capabilities without human labels. The Proposer generates questions from a small seed set, the Solver attempts answers, and the Judge evaluates both question quality and answer correctness on a 1-10 scale. Rewards are computed using Task-Relative REINFORCE++ with per-role advantage normalization, incorporating format adherence, quality scores, and difficulty-adjusted feedback. Questions scoring below 0.7 quality are filtered out. The framework trains on Qwen2.5-3B-Instruct using AdamW (LR 1e-6, batch 128) for 300 steps, achieving consistent improvements across 22 benchmarks.

## Key Results
- MAE achieves 4.54% average improvement across 22 benchmarks compared to base Qwen2.5-3B-Instruct
- Outperforms supervised fine-tuning baseline on GSM8K (+1.97%), MATH (+5.96%), and HumanEval (+3.18%)
- Ablation studies confirm Judge quality filtering and format rewards are essential for stable co-evolution

## Why This Works (Mechanism)
The framework creates a closed-loop co-evolutionary system where each agent's output serves as training signal for another. The Judge provides normalized rewards that prevent score inflation while the difficulty-aware reward mechanism ensures the Solver is challenged appropriately. Quality filtering prevents the system from optimizing for easy or malformed questions. The Task-Relative REINFORCE++ algorithm stabilizes training by normalizing advantages per role, preventing any single agent from dominating the learning signal.

## Foundational Learning
- **Co-evolutionary dynamics**: Understanding how multiple agents can improve simultaneously through mutual feedback loops
  - *Why needed*: The core innovation relies on agents improving each other rather than isolated training
  - *Quick check*: Verify each agent's performance improves when paired with others versus alone
- **Advantage normalization in RL**: Task-Relative REINFORCE++ uses per-role normalization to stabilize learning
  - *Why needed*: Prevents reward scale differences between agents from destabilizing training
  - *Quick check*: Monitor variance of raw vs. normalized rewards during training
- **Quality filtering thresholds**: 0.7 cutoff prevents low-quality question propagation
  - *Why needed*: Without filtering, the system could optimize for easy or malformed questions
  - *Quick check*: Track percentage of questions passing filter over training steps

## Architecture Onboarding

**Component Map**: Proposer -> Judge -> (Filter) -> Solver -> Judge -> Reward Update

**Critical Path**: The synchronous training loop where Proposer generates questions, Judge evaluates them, Solver attempts answers, and Judge provides final rewards. Each component must complete before the next begins.

**Design Tradeoffs**: Synchronous execution ensures consistent reward computation but limits throughput. Domain-agnostic Judge enables broad applicability but may miss domain-specific nuances. Quality filtering ensures training signal quality but may reduce diversity.

**Failure Signatures**: 
- Proposer collapse: Rapid increase in difficulty score with no corresponding Solver improvement
- Judge format drift: High format reward penalties indicating inconsistent output parsing
- Solver stagnation: Low answer scores despite high question difficulty

**First 3 Experiments**:
1. Run single-step inference through all three agents to verify prompt parsing and tag extraction
2. Test Judge consistency by evaluating same question-answer pairs across 5 runs
3. Verify quality filtering by checking that questions scoring below 0.7 are correctly discarded

## Open Questions the Paper Calls Out
- **Scaling to larger backbones**: Whether MAE maintains stability and proportional gains on models larger than 3B parameters remains untested, though the framework is hypothesized to benefit from stronger reasoning capabilities.
- **Adding more roles**: The current three-agent architecture could potentially be expanded with specialized verifiers or refiners, but the impact of added complexity is unknown.
- **Integrating verifiable environments**: Combining LLM-based rewards with executable environments (e.g., code interpreters) could create a unified platform, but the interaction between soft and hard rewards is untested.

## Limitations
- Single Judge model dependency creates potential single-point-of-failure risk for evaluation consistency
- 0.7 quality threshold is not justified through ablation analysis, making it potentially arbitrary
- "No human-curated data" claim is partially qualified since a small human-curated seed set is required

## Confidence
- **High Confidence**: Overall framework design and reproducibility of the propose-solve-judge loop
- **Medium Confidence**: 4.54% average improvement claim due to dependence on single Judge model stability
- **Low Confidence**: Long-term stability and scalability beyond tested 300 steps and 3B parameter scale

## Next Checks
1. **Judge Stability Audit**: Run 5 independent evaluation passes on fixed question-answer sets, verifying score variation stays under 5% to confirm Judge reliability.
2. **Filtering Sensitivity Test**: Repeat training with quality thresholds at 0.6 and 0.8, measuring impact on final benchmark performance to assess threshold optimality.
3. **Role Isolation Test**: Train ablated variants (Proposer+Solver only, Solver+Judge only, all roles with random rewards) and compare final performance to isolate each role's contribution.