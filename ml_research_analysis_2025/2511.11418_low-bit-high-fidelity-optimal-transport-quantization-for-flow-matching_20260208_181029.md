---
ver: rpa2
title: 'Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching'
arxiv_id: '2511.11418'
source_url: https://arxiv.org/abs/2511.11418
tags:
- quantization
- generative
- uniform
- weight
- bitwidth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applies optimal transport-based quantization to compress
  flow matching generative models, minimizing the 2-Wasserstein distance between quantized
  and original weights. Theoretical analysis provides upper bounds on generative degradation,
  showing FID scales as \( 2^{-2b} \) with bit-width.
---

# Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching

## Quick Facts
- arXiv ID: 2511.11418
- Source URL: https://arxiv.org/abs/2511.11418
- Reference count: 34
- Primary result: OT quantization preserves FM visual quality down to 2-3 bits per parameter, where uniform, piecewise, and logarithmic schemes fail.

## Executive Summary
This work introduces optimal transport-based quantization for compressing flow matching generative models, minimizing the 2-Wasserstein distance between quantized and original weights. The method partitions per-layer weights into equal-probability-mass bins and uses bin means as centroids, achieving superior fidelity preservation at low bit-widths compared to uniform, piecewise, and logarithmic baselines. Theoretical analysis provides FID scaling bounds of 2^{-2b}, validated empirically across five datasets. OT quantization uniquely maintains latent variance stability and visual quality down to 2-3 bits, enabling practical deployment on edge devices.

## Method Summary
The approach applies post-training quantization to flow matching models by flattening per-layer weights, sorting them, partitioning into K=2^b equal-mass bins, and computing centroids as bin means. This equal-mass optimal transport scheme minimizes the 2-Wasserstein distance between original and quantized distributions. The quantized model stores a codebook of centroids and assignment indices per layer. Theoretical bounds show FID degrades as 2^{-2b} with bit-width, while empirical results demonstrate consistent SSIM/PSNR improvements and latent variance stability across MNIST, FashionMNIST, CIFAR-10, CelebA, and ImageNet datasets.

## Key Results
- OT quantization preserves visual quality down to 2-3 bits where uniform, piecewise, and logarithmic schemes fail
- FID scales as 2^{-2b} with bit-width, with OT having smaller front-constant than uniform quantization
- OT uniquely maintains latent variance distributions at 2-3 bits, preventing variance explosion seen in other methods

## Why This Works (Mechanism)

### Mechanism 1: Equal-Mass Quantization Minimizes Wasserstein Distance
Partitioning weights into equal-probability-mass bins concentrates representation resolution in high-density regions where most weights lie, while sparse tails share fewer codebook entries. This is provably optimal for 1D distributions under quadratic cost when weights follow approximately Gaussian or Laplace distributions.

### Mechanism 2: FID Scales as 2^{-2b} with Bit-Width
Theoretical derivation bounds trajectory error via Lipschitz assumptions on the velocity network, then connects Wasserstein-2 distance to FID under Gaussian-approximated feature distributions. The bit-width exponent emerges from quantization error propagation through the bound.

### Mechanism 3: Latent Space Stability Under Aggressive Compression
By minimizing distributional mismatch between original and quantized weights, OT reduces cumulative perturbation to the learned vector field, preventing nonlinear amplification through ODE integration steps that destabilize latent trajectories in other quantization methods.

## Foundational Learning

- **Optimal Transport and Wasserstein Distance**: Core mathematical framework; OT quantization minimizes W₂ between weight distributions. Quick check: For two 1D distributions, does sorting and pairing quantiles yield the W₂-optimal coupling?

- **Flow Matching (Continuous Normalizing Flows)**: Target architecture; generates samples by integrating a learned velocity field ODE from noise to data. Quick check: How does deterministic ODE-based sampling differ from diffusion's stochastic denoising?

- **Lipschitz Continuity and Gronwall's Inequality**: Proof technique for bounding trajectory error propagation; enables FID-bit-width derivation. Quick check: If ||f(x) - f(x')|| ≤ L||x - x'||, what does Gronwall's lemma guarantee about accumulated error over time?

## Architecture Onboarding

- **Component map**: Trained FM model → Per-layer weight extraction → Flatten to 1D distribution → Sort ascending → Equal-mass partition (K = 2^b groups) → Compute centroids (group means) → Assign each weight to nearest centroid → Store codebook + indices

- **Critical path**: Algorithm 1 lines 4–7 (sorting + equal-mass split + centroid computation) determine quantization quality; assignment step (lines 9–11) is straightforward nearest-neighbor.

- **Design tradeoffs**: Sorting overhead O(N log N) per layer vs. O(N) for uniform binning; acceptable for one-time PTQ but costly for very large layers. Equal-mass may under-represent tail weights that are critical for specific downstream tasks (untested).

- **Failure signatures**: Uniform/LogBase2: Abrupt PSNR/SSIM collapse below 5 bits on complex datasets (CelebA, ImageNet). Latent variance explosion (10³–10⁹× increase) indicating internal representation destabilization. OT: Graceful degradation; if visual coherence fails at 2 bits, check codebook utilization efficiency.

- **First 3 experiments**:
  1. Replicate MNIST results at 4-bit and 2-bit: train baseline FM, apply OT quantization, compare SSIM/PSNR against full-precision; verify FID reduction aligns with 2^{-2b} scaling
  2. Ablate weight distribution assumption: fit Gaussian vs. Laplace vs. empirical histogram; measure α(f_W)^3 constant impact on actual vs. theoretical FID gap
  3. Latent variance probe: extract intermediate representations at t∈{0.25, 0.5, 0.75, 1.0} for OT vs. uniform at 3-bit; plot variance trajectory to confirm stability claim

## Open Questions the Paper Calls Out

- **Open Question 1**: How does OT-quantization-induced latent space regularity affect downstream tasks such as sample diversity, mode coverage, and robustness to out-of-distribution data? The paper focuses on fidelity metrics but does not evaluate functional downstream performance of quantized models.

- **Open Question 2**: Does OT quantization generalize effectively to other generative architectures such as diffusion transformers and latent variable models? The theoretical bounds assume FM-specific ODE dynamics and Lipschitz conditions; whether the same 2⁻²ᵇ FID scaling holds for stochastic diffusion or latent architectures is untested.

- **Open Question 3**: What are the practical inference speed, power consumption, and memory bandwidth gains of OT-quantized FM models on actual edge hardware? The paper evaluates compression quality but provides no on-device benchmarks; actual hardware may introduce overhead from codebook lookups that offsets theoretical gains.

## Limitations

- Theoretical assumptions validity: FID-bit-width scaling relies on Assumptions 1-A through 1-E, particularly Gaussian-approximable feature distributions and state-Lipschitz velocity fields that real models may violate.

- Distribution assumptions: Equal-mass quantization assumes per-layer weight distributions are sub-Gaussian (Gaussian or Laplace). For highly multi-modal weight distributions or layers with outlier clusters critical for specific tasks, equal-mass binning might under-allocate resolution to small but important weight clusters.

- Integration stability: The claim that small weight errors compound nonlinearly through ODE integration steps is stated but not formally proven. The actual stability depends on the specific velocity field sensitivity and integration scheme.

## Confidence

- **High confidence**: Empirical results showing OT quantization's superiority over baselines at low bit-widths (2-3 bits), particularly the visual quality preservation and latent variance stability metrics across five datasets.

- **Medium confidence**: The theoretical FID scaling bound (2^{-2b}) and its practical tightness, as this depends on multiple assumptions about the model architecture and feature distributions that weren't fully validated across all datasets.

- **Medium confidence**: The mechanism explanation for latent space stability, as the connection between weight quantization errors and their propagation through ODE integration is stated but not rigorously established through formal analysis.

## Next Checks

1. **Distribution assumption validation**: For each dataset and layer type, empirically fit the weight distributions to Gaussian, Laplace, and empirical histograms. Measure how the constant α(f_W)^3 varies across these distributions and quantify the gap between theoretical and actual FID degradation.

2. **Integration stability analysis**: Conduct controlled experiments varying the number of integration steps (e.g., 10, 50, 100, 500 steps) for a fixed bit-width (e.g., 3-bit). Plot the divergence between OT and uniform quantization performance to test whether the claimed nonlinear error propagation manifests in practice.

3. **Multi-modal distribution stress test**: Construct synthetic weight distributions with multiple distinct clusters of varying sizes (e.g., bimodal with one small cluster containing 5% of weights). Apply OT quantization and measure whether the small cluster's weights are adequately preserved in the codebook, or if equal-mass allocation causes critical under-representation.