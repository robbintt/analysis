---
ver: rpa2
title: "Square$\u03C7$PO: Differentially Private and Robust $\u03C7^2$-Preference\
  \ Optimization in Offline Direct Alignment"
arxiv_id: '2505.21395'
source_url: https://arxiv.org/abs/2505.21395
tags:
- privacy
- arxiv
- preference
- learning
- square
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Square\u03C7PO, a simple modification to\
  \ the \u03C7PO algorithm that replaces the log-loss with a square loss over probabilities\
  \ for offline preference optimization in language models. The key innovation is\
  \ its ability to handle both privacy protection and label corruption simultaneously,\
  \ achieving optimal rates under local differential privacy (LDP) and first-time\
  \ guarantees under central differential privacy (DP) for general function approximations."
---

# Square$χ$PO: Differentially Private and Robust $χ^2$-Preference Optimization in Offline Direct Alignment

## Quick Facts
- **arXiv ID:** 2505.21395
- **Source URL:** https://arxiv.org/abs/2505.21395
- **Reference count:** 40
- **Primary result:** Square$χ$PO achieves optimal LDP rates and first-time DP guarantees for preference optimization while providing robustness against label corruption

## Executive Summary
Square$χ$PO introduces a novel modification to χPO that replaces the log-loss with square loss over probabilities, enabling simultaneous privacy protection and robustness to label corruption in offline preference optimization. The method achieves optimal rates under local differential privacy (LDP) and provides first-time guarantees under central differential privacy (DP) for general function approximations. A key theoretical contribution is the analysis of the interaction between privacy and corruption orders, revealing that when corruption follows privacy (LTC), the error bound includes an additional multiplicative factor, while the reverse order (CTL) shows additive effects. The approach extends naturally to general preference models beyond Bradley-Terry assumptions, maintaining state-of-the-art guarantees across all settings.

## Method Summary
Square$χ$PO modifies the original χPO algorithm by replacing the log-loss with square loss over probabilities in the preference optimization framework. This simple change enables the method to handle both differential privacy requirements and label corruption simultaneously. The algorithm maintains the favorable single-policy concentrability property while providing robustness against Huber corruption. The theoretical analysis establishes a clear separation between the order of privacy and corruption mechanisms, showing that the LTC order incurs an additional multiplicative factor in the error bound compared to the CTL order. The method extends naturally to general preference models beyond the Bradley-Terry assumption by leveraging spectral properties of the preference matrix.

## Key Results
- Achieves optimal rates under local differential privacy (LDP) and first-time guarantees under central differential privacy (DP)
- Provides robustness against Huber corruption while maintaining single-policy concentrability
- Reveals theoretical separation between privacy-corruption orders (LTC vs CTL) with multiplicative vs additive error effects
- Extends to general preference models beyond Bradley-Terry with state-of-the-art guarantees
- Outperforms original χPO on synthetic financial preference datasets in both CTL and LTC settings

## Why This Works (Mechanism)
The square loss modification in Square$χ$PO provides several key advantages over the original log-loss approach. First, square loss is more robust to outliers and corrupted labels, which is crucial when dealing with Huber corruption in preference data. Second, the mathematical properties of square loss enable tighter concentration bounds under differential privacy constraints. Third, the square loss formulation naturally extends to general preference models by working directly with probability distributions rather than ratios. The separation between LTC and CTL orders emerges from the different ways privacy and corruption mechanisms interact with the optimization landscape - when corruption follows privacy, the additional noise from privacy mechanisms gets amplified by the corruption process.

## Foundational Learning

**Differential Privacy (DP)** - A framework for preserving individual privacy in statistical analysis by adding calibrated noise to query results. *Why needed:* Essential for protecting user preferences in language model fine-tuning. *Quick check:* Verify that the algorithm's noise addition mechanism satisfies the privacy budget constraints.

**Preference Optimization** - Learning from pairwise comparisons rather than absolute ratings, typically using Bradley-Terry or Plackett-Luce models. *Why needed:* Language model alignment often relies on human preference data collected as pairwise comparisons. *Quick check:* Confirm the preference model assumptions match the data generation process.

**Huber Corruption** - A robust statistical framework that combines squared loss for small errors with absolute loss for large errors, providing protection against outliers. *Why needed:* Real preference data often contains noisy or adversarial labels that can degrade model performance. *Quick check:* Test the corruption model's ability to handle different noise distributions.

**Local vs Central DP** - Local DP adds noise at the data source level, while central DP adds noise at the aggregator level. *Why needed:* Different deployment scenarios require different privacy guarantees and have different performance implications. *Quick check:* Compare the utility-privacy tradeoff between local and central variants.

**Single-policy Concentrability** - A property ensuring that the optimal policy's value can be bounded by the average value across all policies in the function class. *Why needed:* Critical for establishing finite-sample convergence rates in offline reinforcement learning settings. *Quick check:* Verify that the concentrability coefficient remains bounded under the modified loss function.

## Architecture Onboarding

**Component map:** Data pairs -> Square loss computation -> Privacy mechanism (Laplace/Gaussian) -> Optimization (gradient descent) -> Preference model estimation

**Critical path:** The privacy mechanism is the critical component that determines both the privacy guarantees and the error bounds. The choice between local and central DP fundamentally affects the algorithm's performance characteristics.

**Design tradeoffs:** Square loss vs log-loss (robustness vs statistical efficiency), local vs central DP (user privacy vs computational efficiency), general vs specific preference models (flexibility vs tighter guarantees).

**Failure signatures:** Degradation in privacy guarantees when privacy budget is too small, increased error when corruption rate exceeds robustness thresholds, convergence issues when the preference matrix lacks full rank.

**First experiments:**
1. Compare Square$χ$PO vs χPO on synthetic data with varying corruption rates
2. Evaluate privacy-utility tradeoff across different privacy budgets (ε values)
3. Test extension to general preference models on non-Bradley-Terry datasets

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Assumes i.i.d. preference pairs, which may not hold in sequential or temporally correlated preference collection
- Analysis focuses on Huber corruption, potentially missing other realistic noise patterns in preference datasets
- Extension to general preference models relies on spectral properties that may not hold for all real-world preference distributions
- Theoretical guarantees may not fully capture practical challenges in deploying the method at scale

## Confidence
- **High confidence** in LDP guarantees and square loss modification analysis
- **Medium confidence** in CTL/LTC separation results due to specific assumption dependencies
- **Medium confidence** in general preference model extension pending empirical validation
- **Low confidence** in practical impact without real-world dataset experiments

## Next Checks
1. Empirical evaluation on established preference learning benchmarks (Yahoo! Music ratings or Copilot suggestions) to verify theoretical claims
2. Stress testing with non-i.i.d. preference pairs to assess robustness beyond theoretical assumptions
3. Ablation studies comparing square loss to log-loss under various corruption rates and privacy budgets to quantify practical benefits