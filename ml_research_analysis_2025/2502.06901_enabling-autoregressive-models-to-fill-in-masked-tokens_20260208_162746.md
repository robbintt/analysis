---
ver: rpa2
title: Enabling Autoregressive Models to Fill In Masked Tokens
arxiv_id: '2502.06901'
source_url: https://arxiv.org/abs/2502.06901
tags:
- maria
- arxiv
- masked
- infilling
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling autoregressive language
  models to perform masked infilling, a task they are inherently incapable of due
  to their unidirectional nature. The authors introduce MARIA (Masked and Autoregressive
  Infilling Architecture), a novel framework that combines pre-trained masked language
  models (MLM) and autoregressive (AR) models by training a linear decoder on their
  concatenated hidden states.
---

# Enabling Autoregressive Models to Fill In Masked Tokens

## Quick Facts
- **arXiv ID**: 2502.06901
- **Source URL**: https://arxiv.org/abs/2502.06901
- **Reference count**: 17
- **One-line primary result**: MARIA achieves lower perplexity than discrete diffusion models on masked infilling across all masking rates

## Executive Summary
This paper introduces MARIA (Masked and Autoregressive Infilling Architecture), a framework that enables autoregressive language models to perform masked infilling by combining frozen MLM and AR models through a trained linear decoder. The approach addresses the fundamental limitation that AR models cannot leverage bidirectional context for infilling tasks. Experiments show MARIA significantly outperforms baselines including ModernBERT and discrete diffusion models on perplexity metrics across various masking rates, while maintaining KV caching compatibility for efficient inference.

## Method Summary
MARIA combines a pre-trained masked language model (ModernBERT-Large) and autoregressive model (OLMo) by training a linear decoder on their concatenated hidden states. The MLM processes the masked sequence to provide bidirectional context, while the AR model processes the clean sequence autoregressively. A learned linear layer maps the concatenated [h_MLM; h_AR] states to vocabulary logits. The method uses product initialization (averaging parent model output weights) for faster convergence and maintains KV caching compatibility by computing MLM hidden states once upfront.

## Key Results
- MARIA 7B achieves perplexity scores of 2.82, 3.85, 5.94, 10.11, and 16.30 at masking rates 0.1, 0.3, 0.5, 0.7, and 0.9 respectively, outperforming all baselines
- MARIA exhibits higher throughput than discrete diffusion models and autoregressive decoding of ModernBERT, with MARIA 1B showing the best performance
- LLM-based ELO scoring confirms MARIA generates higher-quality text compared to baselines, with MARIA 7B and 1B achieving highest ratings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating MLM and AR hidden states enables bidirectional infilling while preserving autoregressive generation capability
- Mechanism: MLM provides bidirectional context through its hidden states while AR maintains sequential generation order. The linear decoder combines these complementary representations
- Core assumption: Frozen pretrained MLM and AR models have learned complementary representations that require only linear transformation to produce coherent predictions
- Evidence anchors: Abstract states the framework combines MLM and AR via linear decoder; section 3 shows equation πMARIA(x | · ) = σ(W3 [h1(x); h2(x)]); related work Mask-Enhanced Autoregressive Prediction integrates MLM similarly
- Break condition: If MLM and AR hidden states encode fundamentally incompatible information (different tokenization schemes or divergent semantic spaces), linear fusion will fail

### Mechanism 2
- Claim: Product initialization accelerates convergence by starting from a sensible mixture distribution
- Mechanism: W3 initialized as [W1/2; W2/2] produces logits averaging πAR and πMLM outputs, corresponding to geometric mean (product of experts) of distributions
- Core assumption: Optimal solution lies near manifold of combinations of two parent models' behaviors
- Evidence anchors: Section 3 states average of logits corresponds to multiplicative mixture; Figure 2 shows faster convergence with product initialization
- Break condition: If target distribution requires representations diverging significantly from both parent models, product initialization may anchor to poor local basin

### Mechanism 3
- Claim: KV caching remains compatible because AR model processes tokens sequentially while MLM provides static bidirectional context
- Mechanism: MLM forward pass computed once for all positions; AR iterates through masked positions left-to-right with KV cache updates. AR computations depend only on previous (cached) positions
- Core assumption: Computational overhead of single MLM forward pass is negligible compared to AR KV caching savings
- Evidence anchors: Section 3 Algorithm 1 shows MLM states computed once (lines 3-4), AR with caching (lines 8-9); section 4 Figure 3 shows MARIA 1B highest throughput
- Break condition: If sequence length becomes extremely long, single MLM forward pass (O(n²) without caching) may dominate runtime, reducing AR KV caching benefits

## Foundational Learning

- Concept: **Masked Language Modeling (MLM) vs. Autoregressive (AR) objectives**
  - Why needed here: MARIA explicitly combines these paradigms; understanding their inductive biases explains why fusion is non-trivial
  - Quick check question: Can you explain why an MLM can condition on future tokens during training but an AR model cannot?

- Concept: **KV Caching in Transformer Decoders**
  - Why needed here: Paper's efficiency claims hinge on preserving KV caching; understanding what breaks it (bidirectional attention) clarifies design constraint
  - Quick check question: Why does bidirectional attention in MLMs prevent KV cache reuse during autoregressive decoding?

- Concept: **Product of Experts (PoE)**
  - Why needed here: Weight initialization strategy justified via PoE theory; understanding multiplicative vs. additive ensembles clarifies why averaging logits makes sense
  - Quick check question: If you average two probability distributions additively vs. geometrically, which gives higher probability to samples that both models agree on?

## Architecture Onboarding

- Component map: Tokenizer -> MLM Encoder (ModernBERT) -> AR Decoder (OLMo) -> Linear Decoder (W3) -> Output
- Critical path:
  1. Tokenize input with shared tokenizer (GPT-2 based)
  2. Create masked copy of input for MLM
  3. Compute MLM hidden states for all positions (one forward pass)
  4. Initialize KV cache as empty
  5. For each masked position (left-to-right): run AR forward pass on tokens since previous masked position, update KV cache, concatenate AR + MLM hidden states at current position, decode via W3, sample token
  6. Return infilled sequence
- Design tradeoffs:
  - Tokenizer constraint: MLM and AR must share tokenizer; limits model pairings (paper uses ModernBERT + OLMo)
  - Memory overhead: Must store both MLM and AR hidden states; MARIA 1B has 3072-dim hidden states vs. 1024 for ModernBERT alone
  - Training efficiency: Only W3 trained (~billions of tokens sufficient); full fine-tuning would be more expensive but might yield better task-specific performance
- Failure signatures:
  - Tokenizer mismatch: If tokenizers differ, alignment of hidden states will be incorrect, causing incoherent predictions
  - Masking rate out-of-distribution: ModernBERT trained at 0.3 mask ratio; very high masking rates (>0.7) may produce poor MLM representations
  - Memory exhaustion on long sequences: Storing full MLM hidden states for long contexts can exceed GPU memory
- First 3 experiments:
  1. Validate linear layer convergence: Train W3 on small data subset (e.g., 1B tokens) with product vs. random initialization; verify loss curves match Figure 2 patterns
  2. Benchmark inference throughput: Implement Algorithm 1 and measure tokens/second at 0.5 masking rate across sequence lengths (128, 512, 2048); compare against baseline AR-only and MLM-only decoding
  3. Ablate MLM vs. AR contributions: At inference time, zero out either MLM or AR hidden states before W3; measure perplexity degradation to quantify each component's contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MARIA leverage domain-specific fine-tuned AR and MLM models to improve performance in non-natural language domains?
- Basis in paper: [Explicit] The authors state that future work involves using "fine-tuned versions of these models for domain-specific tasks" such as DNA sequences or code blocks
- Why unresolved: Current study only evaluates architecture on general natural language tasks using standard pre-trained checkpoints (OLMo and ModernBERT)
- What evidence would resolve it: Evaluation of MARIA models constructed from domain-specific backbones (e.g., code or biology) on specialized infilling benchmarks

### Open Question 2
- Question: Can modern memory optimization techniques like Paged Attention be integrated into the MARIA architecture to further increase inference throughput?
- Basis in paper: [Explicit] The conclusion suggests that "incorporating Paged Attention... would provide tremendous gains in throughput beyond the gains demonstrated in this paper"
- Why unresolved: Current implementation utilizes standard KV caching; potential additive benefits of advanced attention memory management have not been quantified
- What evidence would resolve it: System benchmarks comparing throughput and memory footprint of MARIA with and without Paged Attention enabled

### Open Question 3
- Question: Is it possible to adapt the MARIA architecture to combine models that utilize different tokenizers?
- Basis in paper: [Inferred] Implementation section notes the "key constraint" that models "must be trained with the same tokenizer," which limits component selection
- Why unresolved: Current concatenation of hidden states relies on sequence alignment, which breaks if token boundaries differ between AR and MLM models
- What evidence would resolve it: A mechanism (e.g., alignment layer or resampling) that allows MARIA to combine models with mismatched vocabularies without performance degradation

## Limitations

- Tokenizer Dependency: Requires MLM and AR models to share a tokenizer, constraining model pairing options and limiting broader applicability
- Memory Overhead: Must store both MLM and AR hidden states simultaneously, resulting in ~3× the hidden dimension compared to using a single model
- Out-of-Distribution Masking: ModernBERT trained with 0.3 masking ratio, yet MARIA evaluated at rates up to 0.9; MLM representations may degrade at extreme masking rates

## Confidence

- **High Confidence**: Core architectural claim that concatenating MLM and AR hidden states enables masked infilling while preserving autoregressive efficiency is well-supported by experimental results
- **Medium Confidence**: Efficiency claims regarding KV caching compatibility and throughput advantages are reasonably supported, though could benefit from more detailed breakdowns
- **Low Confidence**: Theoretical justification for product initialization via PoE reasoning is somewhat hand-wavy; Figure 2 shows faster convergence but lacks rigorous analysis

## Next Checks

1. **Memory Scaling Analysis**: Systematically measure MARIA's memory consumption across varying sequence lengths (128, 512, 1024, 2048 tokens) and compare against baseline models to identify sequence length threshold where memory overhead becomes prohibitive

2. **Extreme Masking Rate Evaluation**: Evaluate MARIA's performance at masking rates 0.1, 0.3, 0.5, 0.7, and 0.9, plus intermediate points at 0.2, 0.4, 0.6, and 0.8; analyze whether MLM representations degrade significantly beyond 0.3 training distribution, particularly at 0.8-0.9 masking rates

3. **Alternative Initialization Ablation**: Experiment with alternative weight initialization strategies including Xavier/He initialization, pretrained MLM/AR weight initialization, and learned initialization via small auxiliary network; quantify impact on convergence speed and final perplexity to determine whether product initialization is genuinely optimal or merely adequate