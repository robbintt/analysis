---
ver: rpa2
title: Reinforcement Learning Finetunes Small Subnetworks in Large Language Models
arxiv_id: '2505.11711'
source_url: https://arxiv.org/abs/2505.11711
tags:
- sparsity
- training
- wang
- updates
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reveals that reinforcement learning in large language
  models induces parameter update sparsity, where only 5-30% of parameters are actively
  updated, leaving the rest unchanged. This phenomenon occurs across seven RL algorithms
  (PPO, GRPO, DPO, etc.) and ten different LLM families without explicit sparsity-promoting
  techniques.
---

# Reinforcement Learning Finetunes Small Subnetworks in Large Language Models

## Quick Facts
- arXiv ID: 2505.11711
- Source URL: https://arxiv.org/abs/2505.11711
- Reference count: 40
- Only 5-30% of parameters are actively updated during RL finetuning, with the rest unchanged

## Executive Summary
This paper reveals that reinforcement learning in large language models naturally induces extreme parameter update sparsity, where only 5-30% of parameters are actively modified while the remaining 70-95% stay effectively unchanged. This phenomenon occurs across seven different RL algorithms (PPO, GRPO, DPO, etc.) and ten different LLM families without any explicit sparsity-promoting techniques. Remarkably, fine-tuning only the identified sparse subnetwork reproduces both the performance and near-identical parameter values of the full model, suggesting pretrained LLMs contain transferable subnetwork structures that are consistently selected across random seeds and algorithms.

## Method Summary
The study analyzes parameter update sparsity by comparing initial and final model weights across multiple RL algorithms and model families. Update sparsity is measured as the proportion of parameters with |Δ| ≤ 10⁻⁵ (bfloat16 tolerance). The researchers identify the effective subnetwork by tracking which parameters receive non-zero gradients during training, then verify that finetuning only this subnetwork reproduces the full-finetuned model's performance and parameter values. They systematically vary training conditions including in-distribution vs out-of-distribution data, KL regularization, and gradient clipping to understand the drivers of sparsity.

## Key Results
- RL algorithms produce 5-30% parameter update sparsity across different model families
- Finetuning only the identified subnetwork reproduces both performance and near-identical parameter values
- In-distribution training (via SFT pre-alignment) is the primary driver of sparsity
- Layer normalization parameters consistently receive zero updates across all configurations
- Update ranks remain nearly full (99.2-99.8%) despite sparse parameter selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on data near the policy distribution induces sparse parameter updates
- Mechanism: When gradients are computed on sequences the model already assigns high probability to, smaller updates are needed. Many fall below floating-point precision thresholds (e.g., <10⁻⁴⁰ in bfloat16), effectively becoming zero.
- Core assumption: Limited numerical precision in modern hardware causes very small gradients to be discarded, and these discarded updates have negligible impact on performance.

### Mechanism 2
- Claim: The sparse subnetwork identified post-training can be finetuned in isolation to reproduce the full-finetuned model
- Mechanism: Pretrained LLMs contain partially transferable subnetwork structures that are consistently selected across random seeds, data orderings, and even different RL algorithms. Updating only these parameters converges to nearly identical parameter values.
- Core assumption: The subnetwork structure exists in the pretrained model and is discoverable through the gradient update pattern.

### Mechanism 3
- Claim: Sparse updates maintain nearly full rank across parameter matrices
- Mechanism: RL selects a dispersed subset of parameters across nearly all layers that collectively span the full representational subspaces. Unlike LoRA's explicit low-rank constraint, natural sparsity preserves rank.
- Core assumption: The sparse selection is structured rather than random.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed: This paper extends LTH by showing naturally emerging subnetworks (not pruning-identified) can reproduce not just performance but exact parameter values
  - Quick check question: How does finding a subnetwork through gradient masking differ from finding one through iterative pruning?

- Concept: Update Sparsity vs Model Sparsity
  - Why needed: Paper measures sparsity of (θ₁ - θ₀), not sparsity of θ₁; a model can have sparse updates yet remain dense
  - Quick check question: If θ₀ is dense and update sparsity is 80%, is the final model θ₁ sparse?

- Concept: In-Distribution vs Out-of-Distribution Training
  - Why needed: Primary causal factor for sparsity; on-policy RL inherently trains in-distribution, off-policy methods need SFT pre-alignment
  - Quick check question: Why would PPO naturally produce more in-distribution training data than DPO without prior SFT?

## Architecture Onboarding

- Component map:
  - Effective Subnetwork (~20%): Parameters consistently updated across training
  - Cancelled-out Updates (~8%): Parameters receiving non-zero gradients that sum to zero
  - Untouched (~70%): Parameters never receiving gradient updates
  - Layer Normalization: Consistently excluded from updates across all configurations

- Critical path:
  1. Compute update magnitude: |θ_init - θ_full| for each parameter
  2. Apply tolerance threshold (10⁻⁵ for bfloat16) to create binary mask m
  3. Train with gradient masking: m ⊙ ∇_θ L(θ) at each step
  4. Verify: Compare θ_sub to θ_full parameters (>90% match expected)

- Design tradeoffs:
  - Tolerance threshold: 10⁻⁸ captures more updates but may include numerical noise; 10⁻⁵ is robust but conservative
  - Pre-identification vs online: Current method requires full training first; early identification remains open problem
  - Full-rank sparse vs low-rank dense: Unlike LoRA, preserves representational capacity but requires different efficiency strategies

- Failure signatures:
  - Dense updates (>50% non-zero): Likely training on out-of-distribution data without SFT alignment
  - Subnetwork isolation underperforming: Check for training instability or insufficient gradient steps
  - Low-rank updates: Unexpected per paper findings; investigate numerical issues

- First 3 experiments:
  1. Verify baseline sparsity: Load Llama-3.1-Tulu-3-8B-SFT and DPO checkpoints, compute update sparsity (target: ~81%)
  2. Subnetwork reproduction: Train DPO with gradient masking using pre-identified subnetwork, compare final parameters to full training (target: >90% match at 10⁻⁵ tolerance)
  3. In-distribution ablation: Train two DPO models—with and without prior SFT on same data—measure sparsity difference (target: >60 percentage point gap)

## Open Questions the Paper Calls Out

- Does the observed RL-induced sparsity generalize to multimodal or diffusion model architectures?
- Can the sparse subnetwork be identified early in training to maximize efficiency gains?
- What is the rigorous theoretical explanation for why in-distribution training drives sparsity?

## Limitations

- The sparsity phenomenon is demonstrated primarily on math-oriented tasks and preference learning scenarios
- The study focuses on 7B and 8B parameter models, with limited analysis of larger models
- The exact mechanism by which in-distribution training induces sparsity is hypothesized rather than definitively proven

## Confidence

**High Confidence**:
- RL algorithms produce 5-30% parameter update sparsity across different model families
- Finetuning only the identified subnetwork reproduces both performance and near-identical parameter values
- Layer normalization parameters consistently receive zero updates across all configurations
- In-distribution training (via SFT pre-alignment) is the primary driver of sparsity

**Medium Confidence**:
- Update sparsity is primarily attributed to numerical precision limits rather than intrinsic learning dynamics
- The sparse subnetwork spans nearly full-rank subspaces across parameter matrices
- KL regularization and gradient clipping have limited impact on sparsity magnitude

**Low Confidence**:
- The exact mechanism by which in-distribution training induces sparsity is hypothesized rather than definitively proven
- The discovered subnetwork represents the optimal sparse solution rather than one of many equivalent solutions
- The findings generalize to all LLM families and task domains beyond math and preference learning

## Next Checks

1. **Cross-Domain Sparsity Validation**: Train DPO on non-math tasks (e.g., creative writing, code generation, general QA) using the same model families and verify whether 5-30% sparsity persists. Compare update patterns between task domains to identify any systematic differences.

2. **Scaling Law Investigation**: Repeat the subnetwork isolation experiment on larger models (34B, 70B parameters) to determine if the ~20% effective subnetwork proportion holds. Track how update sparsity and subnetwork overlap scale with model size, particularly examining whether larger models show different sparsity distributions across layers.

3. **Out-of-Distribution Stress Test**: Systematically vary the distribution gap between SFT pre-training and DPO fine-tuning data. Measure how update sparsity degrades as data becomes increasingly out-of-distribution, and identify quantitative thresholds where sparsity falls below 50%. This would validate the in-distribution mechanism and establish practical bounds for subnetwork isolation methods.