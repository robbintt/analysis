---
ver: rpa2
title: In-Context Multi-Operator Learning with DeepOSets
arxiv_id: '2512.16074'
source_url: https://arxiv.org/abs/2512.16074
tags:
- learning
- in-context
- deeposets
- operator
- operators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a precise mathematical formulation of the in-context
  multi-operator learning problem and proves that a variant of the DeepOSets architecture
  is universally approximative for this problem. DeepOSets combines set learning via
  DeepSets with operator learning via DeepONets to create a non-autoregressive, non-attention-based
  architecture for learning multiple differential equation solution operators simultaneously.
---

# In-Context Multi-Operator Learning with DeepOSets

## Quick Facts
- **arXiv ID**: 2512.16074
- **Source URL**: https://arxiv.org/abs/2512.16074
- **Reference count**: 24
- **Primary result**: DeepOSets achieves accurate multi-operator learning without attention, with training times of minutes versus hours for transformers

## Executive Summary
DeepOSets is a novel architecture for in-context multi-operator learning that combines DeepSets for permutation-invariant set encoding with DeepONets for operator learning. The model takes in-context examples of parameter-solution pairs and predicts solutions for new partial differential equations without weight updates. Unlike transformer-based approaches, DeepOSets achieves linear complexity in context size and demonstrates robust generalization to context sizes larger than those seen during training. Experiments on 12 differential equation tasks show accurate predictions across forward and inverse initial/boundary value problems, with significant speed advantages over attention-based alternatives.

## Method Summary
DeepOSets architecture combines DeepSets and DeepONet components to learn multiple differential equation solution operators simultaneously. A shared MLP encoder (Φ) processes each parameter-solution example pair independently, with mean pooling aggregating embeddings into a latent vector. This pooled representation is concatenated with the query parameter and passed to the DeepONet branch network (ρ), which combines with the trunk network (Ψ) to produce mesh-free outputs. The model is trained to minimize MSE between predicted and ground truth solutions across diverse differential equation tasks, with training times of only minutes on consumer GPUs compared to hours for transformer-based alternatives.

## Key Results
- DeepOSets achieves accurate predictions across 12 differential equation tasks (forward and inverse initial/boundary value problems)
- Model shows robust generalization to context sizes up to 20x larger than training context (N=4 trained, tested up to N=20)
- MSE decreases monotonically as more in-context examples are provided
- Training time reduced from hours to minutes compared to transformer-based alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepOSets achieves in-context operator disambiguation through permutation-invariant set encoding combined with operator decomposition.
- Mechanism: The DeepSets encoder maps each parameter-solution example pair to an embedding, then pools them via mean aggregation. This pooled representation encodes which operator from the training distribution is currently in context. The pooled embedding is concatenated with the query parameter and passed to the DeepONet branch network, which combines with the trunk network to produce mesh-free outputs.
- Core assumption: Examples are drawn from the same operator being queried and are sufficiently dense in the input function space.
- Evidence anchors: Architecture formula showing Φ encodes pairs, pooling aggregates, ρ and Ψ decode; limited corpus evidence on this specific hybrid architecture.

### Mechanism 2
- Claim: Linear complexity in prompt length enables scaling to larger context sizes than quadratic attention mechanisms.
- Mechanism: DeepSets pooling processes m examples in O(m) time versus O(m²) for self-attention. Each example is processed independently through the shared MLP before aggregation, enabling zero-shot generalization to context sizes not seen during training.
- Core assumption: Mean pooling preserves sufficient information to distinguish operators; permutation invariance is a valid inductive bias.
- Evidence anchors: Model trained with N=4 examples successfully generalizes to N=3-20 with decreasing MSE; corpus evidence on ICL efficiency trade-offs exists.

### Mechanism 3
- Claim: Universal approximation for compact operator classes is achieved through partition-of-unity construction localized over the input function space.
- Mechanism: The proof constructs continuous functions Φ and ρ where Φ uses partition-of-unity functions to localize approximation over a grid of the input space. The branch network computes weighted averages of operator outputs for nearby examples, recombining them via the same partition functions.
- Core assumption: The operator class is compact in the operator norm topology; input space admits (δ,C)-discretizations.
- Evidence anchors: Theorem 5.1 proves universal uniform approximation; explicit construction of Φ and ρ using partition of unity; no corpus evidence on universal approximation for multi-operator ICL.

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed here: Core capability being studied—learning from prompt examples without weight updates. Without understanding ICL, the distinction from traditional operator learning is unclear.
  - Quick check question: Can you explain why ICL differs from fine-tuning, and why it requires architecture support for variable-length inputs?

- **Operator Learning (DeepONets)**
  - Why needed here: DeepOSets builds directly on DeepONet's branch/trunk decomposition. The branch network encodes input functions, the trunk encodes evaluation coordinates, and their dot product gives the output.
  - Quick check question: Given a DeepONet with branch ρ and trunk Ψ, how would you evaluate G(u)(x) for a new input function u at point x?

- **Set Learning (DeepSets)**
  - Why needed here: The DeepSets encoder provides permutation invariance and handles variable numbers of examples. Understanding that Φ processes each example independently before pooling is essential.
  - Quick check question: If you swap the order of two examples in the prompt, should the output change? Why or why not?

## Architecture Onboarding

- Component map:
  Input: m example pairs + query → Encoder Φ → Pooling → Branch ρ → Trunk Ψ → Output

- Critical path:
  1. Discretize all functions on fixed grids (100 points)
  2. Encode each (parameter, solution) pair through shared Φ
  3. Pool embeddings (mean pooling used)
  4. Concatenate pooled vector with discretized query parameter
  5. Pass through branch network; simultaneously pass query coordinates through trunk
  6. Dot product gives prediction at query coordinates

- Design tradeoffs:
  - **Pooling choice**: Mean pooling is simple but may lose fine-grained example relationships. Alternatives: max pooling, attention-based pooling (increases complexity)
  - **No attention**: Faster training/inference (minutes vs hours) but may limit expressiveness for complex operator relationships
  - **Fixed discretization grids**: Simplifies implementation but requires all examples on same grid

- Failure signatures:
  - MSE does not decrease with more examples → examples may not be from the query operator or are poorly distributed
  - High variance across runs → insufficient training diversity or latent dimension too small
  - Poor generalization to new operators → training operator class not compact/sufficiently representative
  - Training instability → check that pooling denominator is bounded away from zero

- First 3 experiments:
  1. **Ablation on context size**: Train with N=4 examples, test N∈{1,2,4,8,16,20}. Expect monotonic MSE decrease. Verifies ICL capability.
  2. **Permutation invariance test**: Shuffle example order in prompt; output should be identical. Validates DeepSets property.
  3. **Cross-operator generalization**: Train on 6 IVP tasks, test on held-out BVP tasks. Expect degraded but non-trivial performance. Reveals operator class coverage requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the universality guarantees and architecture of DeepOSets be extended to handle variable input grids for the parameter and solution functions?
- Basis: "We remark that one could additionally consider variable grids in $K_1$ and $K_2$, but we leave this to future work."
- Why unresolved: Current mathematical formulation and proof assume fixed discretization grids.
- What evidence would resolve it: Theoretical extension of Theorem 5.1 for variable grids, or architectural modification demonstrating empirical success on irregular meshes.

### Open Question 2
- Question: Does the DeepOSets architecture scale effectively to higher-dimensional spatial domains (2D/3D) and more complex PDE systems compared to the 1D cases tested?
- Basis: Experimental validation restricted to 1D ODEs and PDEs with small discretization sizes (100 points).
- Why unresolved: Unclear if linear complexity advantage suffers from curse of dimensionality or information bottleneck in high-dimensional spaces.
- What evidence would resolve it: Empirical benchmarks on 2D/3D Navier-Stokes or elasticity equations comparing performance and training time against transformer-based operator learners.

### Open Question 3
- Question: Is the $(\delta, C)$-discretization assumption strictly necessary for empirical convergence, or can DeepOSets generalize to non-uniform or sparse context distributions?
- Basis: Proof of continuity for aggregation function relies on density properties of $(\delta, C)$-discretization to bound denominators away from zero.
- Why unresolved: Real-world data may not satisfy balanced density requirements, potentially causing instability in local averaging mechanism.
- What evidence would resolve it: Ablation studies testing model performance on context sets deliberately sparse in certain regions of input space.

## Limitations
- Universal approximation proof relies on strong assumptions about operator class compactness and well-distributed examples
- Architecture's dependence on fixed discretization grids may limit applicability to problems requiring adaptive resolution
- Paper doesn't explore failure cases when examples span multiple operators or when operator class lacks compactness properties

## Confidence

- **High confidence**: Mechanism by which DeepSets pooling provides linear complexity and permutation invariance is well-established; experimental results showing improved training speed and context generalization are robust
- **Medium confidence**: Universal approximation claim is theoretically sound given proof assumptions, but real-world applicability depends on whether assumptions hold in practice
- **Low confidence**: Claim that mean pooling is sufficient for operator disambiguation in all cases; while experiments show this works for tested PDE tasks, more complex relationships might require attention mechanisms

## Next Checks

1. **Operator class robustness test**: Systematically evaluate performance when training examples come from mixed operator distributions versus pure distributions. Measure degradation in MSE to quantify robustness of pooling mechanism to contamination.

2. **Discretization sensitivity analysis**: Vary the fixed grid resolution (e.g., 50 vs 200 points) and measure impact on both training stability and generalization. This validates the claim that fixed discretization is a reasonable simplification.

3. **Compactness violation stress test**: Design operator classes that explicitly violate the compactness assumption (e.g., operators with unbounded variation) and measure whether the architecture still provides reasonable approximations or fails catastrophically.