---
ver: rpa2
title: Decomposing and Revising What Language Models Generate
arxiv_id: '2509.00765'
source_url: https://arxiv.org/abs/2509.00765
tags:
- evidence
- fides
- answer
- gemini
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIDES, a novel fact decomposition-based framework
  for attributed question answering with large language models. FIDES addresses the
  limitations of existing question decomposition approaches by using a two-stage faithful
  decomposition method that segments answers into individual sentences and further
  decomposes them into sub-facts for evidence retrieval.
---

# Decomposing and Revising What Language Models Generate

## Quick Facts
- **arXiv ID:** 2509.00765
- **Source URL:** https://arxiv.org/abs/2509.00765
- **Reference count:** 40
- **Primary result:** FIDES achieves over 14% improvement in average performance across GPT-3.5-turbo, Gemini, and Llama 70B series on six QA datasets.

## Executive Summary
This paper introduces FIDES, a novel framework for attributed question answering that addresses limitations in existing question decomposition approaches. The key innovation is a two-stage faithful decomposition method that first segments answers into individual sentences with explicit coreference resolution, then further decomposes them into atomic sub-facts for evidence retrieval. FIDES includes evidence aggregation and revision capabilities to ensure comprehensive coverage of facts in answers. The framework significantly outperforms state-of-the-art methods across six datasets, achieving over 14% improvement in average performance. The authors also propose a new metric, Attrauto-P, to evaluate evidence precision more comprehensively.

## Method Summary
FIDES implements a post-hoc attributed QA pipeline where a long-form answer is first generated, then decomposed into atomic sub-facts through explicit coreference resolution and claim decomposition. The framework uses Bing Search API with reranking to retrieve evidence for each sub-fact, then employs NLI-based verification to detect conflicts between evidence and generated facts. If conflicts are detected, a specialized editing model revises the answer. The system uses asymmetric model specialization, with GPT-3.5-turbo for decomposition tasks and Llama3 70B for editing. The pipeline includes aggregation to map evidence back to the original sentence structure and a new precision metric (Attrauto-P) to complement traditional recall-based evaluation.

## Key Results
- FIDES achieves over 14% improvement in average performance across GPT-3.5-turbo, Gemini, and Llama 70B series
- The framework outperforms state-of-the-art methods on six datasets including WebQSP, Mintaka, Natural Questions, StrategyQA, HotpotQA, and Musique
- FIDES demonstrates superior precision with the new Attrauto-P metric while maintaining strong recall performance
- Asymmetric model specialization shows GPT-3.5-turbo excels at decomposition while Llama3 70B provides stable editing without abnormal revisions

## Why This Works (Mechanism)

### Mechanism 1: Explicit Coreference Resolution (ECR)
The framework implements Explicit Coreference Resolution during the Sentence Segmentation phase, forcing LLMs to expand pronouns (e.g., "It is dangerous" to "Mount Rainier is dangerous") before generating search queries. This provides high-fidelity signals to the retrieval system by ensuring search queries contain explicit entities rather than ambiguous pronouns.

### Mechanism 2: Atomic Fact Decomposition
FIDES breaks segmented sentences into atomic facts rather than complex sentences or generated questions. This reduces the semantic load on the retriever, allowing it to find specific evidence snippets for distinct claims (e.g., location vs. danger status) that might exist in different documents.

### Mechanism 3: Asymmetric Model Specialization
The system assigns specialized models to distinct roles - GPT-3.5 excels at following decomposition instructions while Llama3 70B demonstrates superior stability in verifying and editing facts without introducing errors. This cross-model arbitration strategy improves overall pipeline accuracy.

## Foundational Learning

**Concept: Coreference Resolution**
- Why needed here: The SS module fails without this - a system cannot search for "It" effectively; it must search for the named entity.
- Quick check question: Given "He won the award in 2023," what entity must be substituted for "He" to make this a valid search query?

**Concept: Atomic Facts vs. Complex Claims**
- Why needed here: The CD stage relies on splitting compound sentences. Understanding the difference is vital for debugging why a retriever missed evidence.
- Quick check question: Is "The Eiffel Tower is in Paris and is made of iron" an atomic fact? If not, how would you split it?

**Concept: Post-hoc Retrieval vs. RAG**
- Why needed here: This paper addresses post-hoc attribution (fixing an answer after generation), distinct from standard RAG (generating an answer from retrieved context).
- Quick check question: In the FIDES workflow, does external evidence influence the creation or only the revision of the initial long-form answer?

## Architecture Onboarding

**Component map:** Generator -> FD (SS with ECR + CD) -> Retriever (Bing + Reranker) -> EV (NLI Verifier) -> FE (Editor) -> Aggregator

**Critical path:** The FD (Fact Decomposition) stage is critical - if Explicit Coreference Resolution fails here, the retrieval queries are corrupted and the entire downstream pipeline receives garbage input.

**Design tradeoffs:**
- Precision vs. Recall: The paper argues standard metrics focus on recall (Attr_auto-R), while they introduce precision (Attr_auto-P). Optimizing for precision may reduce retrieved evidence volume, potentially missing edge-case facts.
- Model Latency vs. Accuracy: Using two different models (GPT-3.5 for FD, Llama for Editing) increases operational complexity and latency compared to single-model solutions.

**Failure signatures:**
- "Abnormal Revisions": Instances where the Editor introduces errors or alters correct facts, lowering AR/AP scores after the Edit stage
- No Retrieval: If sub-facts are too generic or contain unresolved pronouns, the search engine returns 0 results

**First 3 experiments:**
1. **Unit Test ECR:** Run FD module on a dataset with high pronoun usage. Verify ss_output contains zero pronouns.
2. **Metric Correlation:** Calculate Attr_auto-P vs. Attr_auto-R on held-out set. Confirm FIDES improves Precision specifically.
3. **Module Ablation:** Swap Editor model (e.g., use GPT-4 instead of Llama3) to verify if "abnormal revision" rate changes.

## Open Questions the Paper Calls Out

**Open Question 1:** Can instruction-tuning of LLMs significantly reduce the rate of "abnormal revisions" observed in the FE module? The authors note the FE module has space for improvement through instruct-tuning, as current few-shot prompting fails to prevent hallucination or alteration of correct facts.

**Open Question 2:** What effective benchmarks can be developed to evaluate the EV module independent of final attribution score? The authors state the EV module "can not be directly evaluated due to lacking labeled datasets" and argue benchmarks are urgently required.

**Open Question 3:** How can the evidence retrieval stage be optimized to handle sub-optimal queries that result in irrelevant or partially supportive documents? The paper acknowledges using search engines may face irrelevant or partially support situations caused by sub-optimal retrieval.

## Limitations

- The specific few-shot prompts for Fact Decomposition and Factual Edit modules are not disclosed, making exact reproduction difficult
- Bing Search API results may drift over time, causing temporal variability in retrieved evidence
- The paper does not provide ablation studies to isolate the contribution of asymmetric model specialization from other factors

## Confidence

- **High Confidence:** The existence of the FIDES framework and its general workflow (SS → CD → ER → EV → FE) is well-documented. The Attrauto-P metric implementation is clearly specified.
- **Medium Confidence:** The claim that asymmetric model specialization is optimal is supported by Table 3 but lacks ablation studies.
- **Low Confidence:** The specific performance gains attributed to "Explicit Coreference Resolution" are difficult to verify without exact prompt templates.

## Next Checks

1. **Prompt Template Audit:** Request exact few-shot prompts for SS and CD modules from authors to validate pronoun resolution reliability.
2. **Temporal Retrieval Validation:** Run retrieval module on fixed queries from paper's evaluation and compare top-5 results with reported evidence to quantify variance over time.
3. **Module Ablation Study:** Conduct ablation experiment replacing FD module with baseline (no coreference resolution, no claim decomposition) to measure impact on attribution metrics.