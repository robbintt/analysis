---
ver: rpa2
title: Dialogue-Based Multi-Dimensional Relationship Extraction from Novels
arxiv_id: '2507.04852'
source_url: https://arxiv.org/abs/2507.04852
tags:
- relationship
- extraction
- character
- relation
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of extracting multi-dimensional
  character relationships from novels, which involve complex contexts and implicit
  expressions. The proposed CREDI method leverages dialogue structure information
  and integrates strategies such as relationship dimension separation, dialogue data
  construction, prompt optimization, example retrieval, and parameter-efficient fine-tuning
  to enhance extraction performance.
---

# Dialogue-Based Multi-Dimensional Relationship Extraction from Novels

## Quick Facts
- arXiv ID: 2507.04852
- Source URL: https://arxiv.org/abs/2507.04852
- Authors: Yuchen Yan; Hanjie Zhao; Senbin Zhu; Hongde Liu; Zhihong Zhang; Yuxiang Jia
- Reference count: 16
- Primary result: F1 scores of 0.79, 0.76, and 0.74 on three relation dimensions of the NCRE dataset

## Executive Summary
This study addresses the challenge of extracting multi-dimensional character relationships from novels, which involve complex contexts and implicit expressions. The proposed CREDI method leverages dialogue structure information and integrates strategies such as relationship dimension separation, dialogue data construction, prompt optimization, example retrieval, and parameter-efficient fine-tuning to enhance extraction performance. Experimental results show that CREDI significantly outperforms traditional baselines, achieving F1 scores of 0.79, 0.76, and 0.74 on the three relation dimensions of the NCRE dataset.

## Method Summary
CREDI employs a multi-dimensional approach to character relationship extraction, treating polarity, type, and hierarchy as independent classification tasks. The method reconstructs dialogue with explicit speaker-listener structure ("A said to B"), uses multilingual BERT and FAISS for semantic retrieval of training examples, and fine-tunes LLaMa 3.1 with LoRA adapters. This parameter-efficient approach adapts the model to the task while preserving pre-trained knowledge, with retrieval-based in-context learning providing semantic grounding for each prediction.

## Key Results
- CREDI achieves F1 scores of 0.79 (polarity), 0.76 (type), and 0.74 (hierarchy) on the NCRE dataset
- The method significantly outperforms traditional baselines like BERT_classification and GPT-3.5 few-shot approaches
- Ablation studies confirm the effectiveness of dialogue expansion and multi-dimensional separation strategies
- CREDI successfully generalizes to public datasets (CRECIL, FiRe, DialogRE) and can construct character relationship networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing fine-grained relationship labels into parallel coarse-grained dimensions improves extraction accuracy in complex literary contexts.
- Mechanism: The paper proposes separating relationships into three independent classification tasks (polarity, type, hierarchy) rather than predicting among many fine-grained labels. This reduces label confusion and allows the model to capture different semantic aspects simultaneously.
- Core assumption: Relationship dimensions are sufficiently independent that separate prediction is beneficial; annotations in the NCRE dataset reflect ground truth as perceived by human readers.
- Evidence anchors:
  - [abstract]: "incorporating relationship dimension separation... enhances extraction performance"
  - [section 4.3, Table 3]: Ablation shows single-dimension extraction drops from 0.77→0.66→0.68 across dimensions versus full CREDI at 0.79→0.76→0.74
  - [corpus]: Related work on dialogue relation extraction (DialogRE with 36 relation types, CRECIL with 30) suggests fine-grained labels increase task difficulty; corpus evidence for multi-dimensional decomposition specifically is limited.
- Break condition: If relationship dimensions are highly correlated (e.g., kinship strongly predicts hierarchy), separate prediction may introduce inconsistency without additional constraint mechanisms.

### Mechanism 2
- Claim: Reconstructing dialogue with explicit speaker-listener structure ("A said to B") improves model perception of character interactions.
- Mechanism: The "Expanded Dialogue" strategy extracts speaker and addressee from narrative text, reformatting exchanges to make interaction structure explicit. This addresses implicit expressions common in novels where dialogue attribution requires inference.
- Core assumption: Speaker-addressee information can be reliably extracted or is already annotated; explicit structure reduces ambiguity more than it introduces noise from extraction errors.
- Evidence anchors:
  - [section 4.1]: "extracting speaker and listener information from novel texts and reconstructing dialogues in the format 'A said to B'"
  - [section 4.3, Table 3]: Ablation comparing "Basic dialogue information" (0.73→0.70→0.67) versus full CREDI (0.79→0.76→0.74) shows ~4-6 point F1 improvement from expanded dialogue
  - [corpus]: No direct corpus comparison for dialogue expansion strategy; related CRECIL work assumes already-structured multi-party dialogue.
- Break condition: If original text contains ambiguous or multiple addressees, forced "A said to B" formatting may misrepresent actual interaction structure.

### Mechanism 3
- Claim: Retrieval-based in-context learning combined with LoRA fine-tuning provides semantic grounding and task adaptation without full model retraining.
- Mechanism: Multilingual BERT encodes training examples; FAISS retrieves Top-K similar instances as few-shot prompts. LoRA fine-tunes Query and Value matrices in LLaMa 3.1, adapting to the task while preserving pre-trained knowledge.
- Core assumption: Semantic similarity in embedding space correlates with useful demonstration examples; LoRA rank and target modules are sufficient for task adaptation.
- Evidence anchors:
  - [section 4.1]: "Using multilingual BERT to vectorize the training data and FAISS for Top-K similarity retrieval"
  - [section 4.3, Table 2]: CREDI achieves 0.79→0.76→0.74 versus GPT-3.5 few-shot at 0.69→0.35→0.31, suggesting retrieval + fine-tuning substantially outperforms few-shot prompting alone
  - [corpus]: Retrieval-augmented generation and parameter-efficient fine-tuning are established techniques; specific combination for literary relation extraction lacks external validation.
- Break condition: If retrieved examples exhibit selection bias or do not cover the distribution of test cases, in-context learning may reinforce errors rather than correct them.

## Foundational Learning

- Concept: **In-Context Learning with Retrieved Examples**
  - Why needed here: CREDI relies on semantic retrieval to select demonstration examples, requiring understanding of how similarity search and prompt construction affect LLM behavior.
  - Quick check question: Given a new dialogue about hostility between two characters, which training example would FAISS retrieve—a similar hostile exchange or a structurally similar but relationally different scene?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The method uses LoRA to adapt LLaMa 3.1 for relation extraction, targeting only Query and Value matrices rather than full weights.
  - Quick check question: If LoRA rank is set too low, what symptom would you expect—overfitting to training relations or underfitting with poor generalization?

- Concept: **Multi-Label vs. Multi-Dimensional Classification**
  - Why needed here: The paper frames relationship extraction as three parallel single-label classification tasks rather than one multi-label task, affecting loss function design and evaluation.
  - Quick check question: Why might predicting "negative" (polarity) + "affiliative" (type) + "senior" (hierarchy) separately cause inconsistencies that a joint model might avoid?

## Architecture Onboarding

- Component map:
  - Novel text → dialogue extraction → "Expanded Dialogue" formatting (A said to B)
  - mBERT encoder → FAISS index → Top-K similar training examples
  - Target characters + candidate labels + dialogue context + retrieved examples
  - LLaMa 3.1 with LoRA adapters on Query/Value matrices
  - Three independent classification outputs (polarity, type, hierarchy)

- Critical path:
  1. Annotate or extract speaker-addressee pairs from raw novel text
  2. Encode all training dialogues with mBERT and build FAISS index
  3. For each test instance, retrieve Top-K examples and construct structured prompt
  4. Forward through LoRA-adapted LLaMa 3.1
  5. Extract three dimension predictions independently

- Design tradeoffs:
  - Coarse vs. fine-grained labels: Fewer categories per dimension improves accuracy but loses nuance (3 polarity labels vs. 36 relation types in DialogRE)
  - Retrieval vs. random examples: Semantic similarity may help but requires embedding inference at test time
  - LoRA vs. full fine-tuning: Reduced computation but potentially lower ceiling on task adaptation

- Failure signatures:
  - Low F1 on "relationship type" dimension specifically (0.76 vs. 0.79 polarity): may indicate overlap between "kinship" and "affiliative" categories
  - GPT-3.5 few-shot instability (Table 2): sample selection bias in manually chosen examples
  - Traditional methods near-zero on CRECIL (BERT_classification: 0.04): fine-grained labels overwhelm classification head

- First 3 experiments:
  1. **Baseline replication**: Run BERT_classification and GPT-3.5 zero-shot on NCRE test split to verify reported F1 scores (0.65, 0.52 for polarity) before implementing full CREDI
  2. **Ablation on dialogue expansion**: Compare "Basic dialogue information" vs. "Expanded Dialogue" on a held-out subset to confirm the 6-point F1 delta reported in Table 3
  3. **Retrieval K sensitivity**: Vary Top-K (1, 3, 5, 10) and measure impact on F1 and inference latency to find operating point before deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CREDI framework be extended to capture the temporal dynamics of relationship shifts across a complete narrative arc rather than isolated dialogue instances?
- Basis in paper: [explicit] The authors state in the conclusion: "We also aim to develop more in-depth models for character relationship analysis, focusing on capturing the dynamic evolution of relationships throughout the narrative."
- Why unresolved: The current methodology treats relationship extraction as a classification task within specific dialogue units, lacking a temporal mechanism to track how these relations change over time.
- What evidence would resolve it: A longitudinal study or model extension that successfully maps relationship trajectories (e.g., from ally to enemy) across different chapters of a novel.

### Open Question 2
- Question: Is the "Generational Hierarchy" dimension transferable to modern or Western literary genres where strict seniority labeling is less prominent than in martial arts fiction?
- Basis in paper: [inferred] The authors note that the label schema was "tailored to the unique characteristics of Jin Yong’s martial arts fiction," specifically citing "Generational Hierarchy" (senior/peer/junior) as a core dimension.
- Why unresolved: It is unclear if this dimension provides signal or noise when applied to genres (e.g., modern romance, Western novels) that do not operate on strict generational clan structures.
- What evidence would resolve it: Experimental results applying the current three-dimensional schema to a diverse dataset of non-martial arts novels without schema modification.

### Open Question 3
- Question: To what extent does performance degrade when extracting relationships from narrative prose that lacks the explicit "A said to B" dialogue structures the model relies upon?
- Basis in paper: [inferred] The method relies heavily on an "Expanded Dialogue" strategy and ablation studies show that removing dialogue data construction drops F1 scores significantly (e.g., Polarity drops from 0.79 to 0.73).
- Why unresolved: Novels often convey relationships through descriptive action or internal monologue; the model's robustness to these non-dialogue modalities remains untested.
- What evidence would resolve it: A comparative evaluation of extraction performance on dialogue-heavy versus narration-only segments within the same literary work.

## Limitations
- The method relies heavily on dialogue structure and explicit speaker-addressee information, which may not be available or reliable in all literary texts
- Performance is validated primarily on Chinese martial arts novels, limiting generalizability to other languages and genres
- The three-dimensional separation strategy may introduce inconsistencies when relationship dimensions are correlated

## Confidence
- **High Confidence**: The effectiveness of multi-dimensional separation over fine-grained classification (supported by ablation results showing consistent F1 improvements across all three dimensions)
- **Medium Confidence**: The dialogue expansion strategy's contribution (based on single ablation comparison without external validation)
- **Medium Confidence**: Retrieval-based in-context learning superiority (demonstrated against GPT-3.5 few-shot but lacks comparison with alternative retrieval strategies)
- **Low Confidence**: Generalizability to other literary genres and languages (validation limited to three additional datasets without systematic cross-genre evaluation)

## Next Checks
1. **Cross-Novel Validation**: Test CREDI on at least three additional novels from different genres and time periods to verify robustness beyond "The Legend of the Condor Heroes"
2. **Correlation Analysis**: Measure inter-dimensional correlations in the NCRE dataset to quantify the risk of inconsistencies when predicting dimensions independently
3. **Speaker-Extraction Evaluation**: Implement and evaluate the accuracy of automatic speaker-addressee extraction on a held-out sample to quantify noise introduced by the "Expanded Dialogue" strategy