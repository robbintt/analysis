---
ver: rpa2
title: Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management
arxiv_id: '2510.06727'
source_url: https://arxiv.org/abs/2510.06727
tags:
- function
- context
- tool
- training
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning framework for training
  large language models to handle long-horizon multi-turn tool use by addressing the
  fundamental bottleneck of context length limits. The core idea is to integrate summarization-based
  context management into the reinforcement learning process, where the model periodically
  generates concise summaries of tool-use history to maintain compact context while
  retaining task-relevant information.
---

# Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management

## Quick Facts
- arXiv ID: 2510.06727
- Source URL: https://arxiv.org/abs/2510.06727
- Reference count: 40
- This paper introduces a reinforcement learning framework for training large language models to handle long-horizon multi-turn tool use by addressing the fundamental bottleneck of context length limits.

## Executive Summary
This paper addresses the challenge of training reinforcement learning agents for long-horizon multi-turn tool use tasks when context length limits constrain the agent's ability to maintain full history. The proposed solution, SUPO, integrates summarization-based context management directly into the reinforcement learning process, allowing the model to periodically generate concise summaries of tool-use history while retaining task-relevant information. This approach enables agents to scale beyond fixed context windows through joint optimization of tool-use behaviors and summarization strategies. Experiments demonstrate significant improvements in success rates compared to baselines, with up to 14% absolute gain, while maintaining or reducing working context length.

## Method Summary
SUPO implements a summarization-augmented MDP where trajectories are split into sub-trajectories at summarization points, enabling standard RL infrastructure to process long-horizon rollouts. The framework uses group-relative advantage estimation and overlong rollout masking to prevent summarization collapse and optimize multi-trajectory exploration. Training occurs with bounded context windows (4K tokens for CodeGym, 64K for BrowseComp-Plus) while achieving effective context lengths up to 192K through periodic summarization. The method builds on GRPO-style optimization with importance sampling ratios and clipping, using binary success rewards and excluding failed rollouts from gradient computation.

## Key Results
- Achieves up to 14% absolute improvement in success rates over baseline methods on interactive function calling and web searching tasks
- Maintains or reduces working context length while scaling to effective context lengths of 32K-192K
- Demonstrates improved performance when scaling test-time summarization beyond training limits (up to 2×)
- Shows 5-10% accuracy drop when using trajectory-relative instead of group-relative advantage estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Periodic summarization enables RL training beyond fixed context windows by maintaining bounded working context while preserving task-relevant information across long horizons.
- Mechanism: When context length exceeds threshold L, the system triggers LLM-generated summarization, discards raw history, and resets working context to (initial_prompt, summary). This creates bounded working context (L + 2L_A + L_O + |v_sum|) regardless of total trajectory length.
- Core assumption: The model can learn to generate task-relevant summaries that retain critical information for future decisions while discarding irrelevant details.
- Evidence anchors: [abstract] "periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window"; [Section 3.2, Proposition 3.1] Formal bound on working context length under M^sum_V.

### Mechanism 2
- Claim: Policy gradient decomposition enables standard RL infrastructure to optimize multi-trajectory rollouts without architectural modifications.
- Mechanism: Theorem 3.2 decomposes the gradient of a long-horizon rollout into sum of gradients from I+1 "complete trajectories," each beginning with (initial_prompt, previous_summary) and ending with current summary. Each sub-trajectory can be processed by existing single-trajectory RL code.
- Core assumption: The gradient contribution from each sub-trajectory can be computed independently and summed without introducing optimization instability.
- Evidence anchors: [Section 3.2, Theorem 3.2] Formal policy gradient representation under summarization-augmented MDP; [Section 4.2, Trajectory management] "SUPO can be easily built upon the existing infrastructure by directly treating each rollout j ∈ [G] as I_j + 1 single complete trajectories."

### Mechanism 3
- Claim: Overlong trajectory masking prevents collapse of summarization behavior by filtering failed rollouts before gradient computation.
- Mechanism: Binary mask 1{T ≤ H, I ≤ S} excludes rollouts that fail to produce final answer before reaching step limit H or summarization limit S from loss calculation, preventing reward signal from penalizing long-horizon exploration.
- Core assumption: Excluding failed rollouts doesn't introduce bias in gradient estimation for successful strategies.
- Evidence anchors: [Section 4.1, Eq. 2] Overlong mask indicator in SUPO objective; [Section 5.2.2, Figure 3] Ablation shows summarization pattern collapses without overlong mask; conditional success rate drops to 0.

## Foundational Learning

- Concept: **MDP formulation of LLM tool-use**
  - Why needed here: SUPO extends standard MDP (Section 3.1) to summarization-augmented MDP (Section 3.2). Understanding state transitions s_t → s_{t+1} via (action, observation) concatenation is prerequisite for grasping how summarization modifies transition dynamics.
  - Quick check question: In the standard MDP M_V, what determines the next state s_{t+1} given current state s_t and action a_t?

- Concept: **Policy gradient methods (PPO/GRPO)**
  - Why needed here: SUPO builds on GRPO-style optimization with importance sampling ratios ρ and clipping. The advantage estimator bA_j (Eq. 3) uses group-relative normalization, and Theorem 3.2 provides the gradient decomposition enabling application of standard RL infrastructure.
  - Quick check question: How does the advantage estimator in Eq. 3 differ from trajectory-relative advantage in Eq. 4, and why does the paper claim Eq. 3 performs better?

- Concept: **Context management in long-context agents**
  - Why needed here: The paper positions itself against compression-based [9,10,19] and external memory [2,13,17] approaches. Understanding the trade-offs helps appreciate why end-to-end RL optimization of summarization is proposed as an alternative.
  - Quick check question: What is the fundamental difference between SUPO's summarization approach and external memory systems like MemGPT or Memory-R1?

## Architecture Onboarding

- Component map:
  Rollout Generator (Algorithm 2) → Trajectory Manager [splits into I+1 sub-trajectories per rollout] → Advantage Calculator (Eq. 3) [group-relative across G rollouts] → Loss Computer (Eq. 2) [with overlong mask] → Policy Updater [standard GRPO-style backprop]

- Critical path: **Summarization trigger logic (Algorithm 2, lines 6-18)**
  - Checks |(s_t, a_t, o_t)| < L before appending; if exceeded and I < S, triggers summarization; discards last action-observation pair to control length (L + |v_sum| + L_A)
  - Incorrect implementation here causes context overflow or premature summarization

- Design tradeoffs:
  - **Working context L vs. summarization limit S**: Lower L → more frequent summarization → more compression risk; lower S → bounded compute but limits effective horizon
  - **Advantage estimation**: Group-relative (Eq. 3) favors successful long rollouts; trajectory-relative (Eq. 4) dilutes reward signal across sub-trajectories (ablation shows -5% to -10% accuracy drop)
  - **Overlong mask threshold**: Must balance excluding failed rollouts vs. maintaining batch diversity

- Failure signatures:
  - **Summarization collapse**: Conditional success rate → 0 during training; caused by missing overlong mask (Figure 3)
  - **Information loss in summaries**: Downstream trajectories fail to continue task; visible in Table 2 pre-SUPO summaries missing critical indices
  - **Context overflow**: RuntimeError during rollout; caused by incorrect threshold check or missing discard of last action-observation pair

- First 3 experiments:
  1. **Sweep summarization threshold L** at fixed effective context (e.g., 4K × 8 = 32K on CodeGym); plot accuracy vs. L to find optimal compression ratio
  2. **Ablate overlong mask** on BrowseComp-Plus; compare summarization rate and conditional success rate curves against Figure 3 to validate collapse mechanism
  3. **Test-time scaling**: Train with S=2, evaluate with S ∈ {2, 5, 11, 23}; replicate Figure 5 curve to verify generalization of summarization strategy beyond training limits

## Open Questions the Paper Calls Out

- Can SUPO benefit from training a dedicated critic model for advantage estimation instead of group-relative advantages?
  - Basis in paper: [explicit] Section 4.2 states, "one can utilize the new MDP framework... to further train a critic model... We leave this as a future work."
  - Why unresolved: The current implementation relies on group-relative advantage estimation (Eq. 3), which may introduce variance or bias compared to a learned value function in long-horizon tasks.
  - What evidence would resolve it: Empirical comparison of SUPO's convergence speed and performance when trained with a critic versus the baseline group-relative estimator.

- How can external memory modules be integrated into the SUPO framework to complement summarization-based context management?
  - Basis in paper: [explicit] The Conclusion identifies "integrating external memory modules" as a specific future direction.
  - Why unresolved: Summarization is lossy compression; the paper does not explore hybrid approaches where raw data is offloaded to external storage while summaries maintain high-level state.
  - What evidence would resolve it: Architectural modifications allowing the agent to read/write to an external buffer, evaluated on tasks requiring precise recall of specific historical details.

- Does a fixed summarization threshold $L$ limit performance compared to an adaptive compression strategy?
  - Basis in paper: [inferred] The paper sets a static threshold $L$ (Algorithm 2), but the optimal compression point likely varies with information density and task complexity.
  - Why unresolved: A static threshold may trigger unnecessary summarization in sparse contexts or delay it in dense contexts, potentially losing critical information.
  - What evidence would resolve it: Ablation studies comparing fixed thresholds against dynamic triggers based on semantic density or estimated value uncertainty.

## Limitations

- The generalizability of summarization strategies beyond training limits (Figure 5) requires careful experimental validation, as the paper shows improvement only for test-time summarization up to 2× training limits
- Implementation details for the CodeGym environment and BrowseComp-Plus corpus integration are not fully specified, requiring either reconstruction or author clarification
- The impact of different summarization prompt templates on summary quality and downstream performance remains unexplored

## Confidence

- **High confidence**: The fundamental claim that summarization-based context management enables training RL agents beyond fixed context limits, supported by formal MDP analysis (Theorem 3.2) and experimental results showing 14% absolute improvement
- **Medium confidence**: The claim about test-time scaling benefits, as results show diminishing returns beyond 2× training summarization limits
- **Medium confidence**: The assertion that group-relative advantage estimation (Eq. 3) consistently outperforms trajectory-relative alternatives, based on ablation studies showing 5-10% accuracy differences

## Next Checks

1. **Summarization quality vs. performance**: Systematically vary summarization threshold L at fixed effective context (e.g., 4K × 8 = 32K on CodeGym) and plot accuracy vs. L to identify optimal compression ratios and quantify summarization quality impact

2. **Overlong mask necessity**: Replicate the ablation from Figure 3 by training without overlong masking on BrowseComp-Plus, measuring summarization rate and conditional success rate curves to confirm the summarization collapse mechanism

3. **Cross-task generalization**: Train with S=2 and evaluate with S ∈ {2, 5, 11, 23} on both CodeGym and BrowseComp-Plus to verify that test-time scaling benefits observed in BrowseComp-Plus generalize to iterative function calling tasks