---
ver: rpa2
title: Exploring the Stratified Space Structure of an RL Game with the Volume Growth
  Transform
arxiv_id: '2507.22010'
source_url: https://arxiv.org/abs/2507.22010
tags:
- space
- dimension
- agent
- local
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work adapts the volume growth transform (VGT) method to analyze
  the latent space structure of a transformer-based PPO model trained to play a visual
  RL game. By estimating local dimension via log-volume growth curves, the study finds
  that the token embedding space is not a manifold but a stratified space with varying
  local dimensions.
---

# Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform

## Quick Facts
- **arXiv ID:** 2507.22010
- **Source URL:** https://arxiv.org/abs/2507.22010
- **Reference count:** 27
- **Key outcome:** This work adapts the volume growth transform (VGT) method to analyze the latent space structure of a transformer-based PPO model trained to play a visual RL game.

## Executive Summary
This paper challenges the "manifold hypothesis" in representation learning by analyzing the latent space of a transformer-based RL agent playing a visual game. Using the volume growth transform (VGT) method, the authors estimate local dimensions of token embeddings and discover that the space is better modeled as a stratified space with varying local dimensions (6-21) rather than a single smooth manifold. The distribution of local dimensions clusters around specific values, with spikes correlating to complex environmental states and near goal achievement. This geometric analysis reveals that RL agents' latent representations alternate between low-dimension sub-strategies and high-dimension goal-or complexity-driven states.

## Method Summary
The authors train a transformer-XL based PPO agent on a modified "Two-Coin" Searing Spotlights game from Memory Gym, where the agent collects two colored coins while avoiding dynamic spotlights. They extract 256-dimensional CNN embeddings from the first transformer block and apply the volume growth transform to estimate local dimensions. For each token, they compute log-volume vs log-radius relationships by counting neighbors within varying radii, then fit linear slopes to determine local dimension. The analysis focuses on volume-bounded ranges [50, 90] to improve stability. The method identifies stratified geometry through slope increases and analyzes dimension correlations with game complexity.

## Key Results
- Token embedding space is not a manifold but a stratified space with varying local dimensions (6-21)
- Dimension distribution shows clusters rather than uniform spread, indicating structured geometry
- Spikes in local dimension correlate with environmental complexity and sub-goal achievement
- Proof that arbitrary VGT curves can be realized by stratified spaces, validating the methodology

## Why This Works (Mechanism)

### Mechanism 1: Local Dimension Estimation via Log-Volume Scaling
The local dimension of a token embedding can be estimated by analyzing the scaling relationship between log-volume and log-radius, assuming the space locally approximates a manifold. In a smooth $n$-manifold, volume $v_x(r)$ scales as $Kr^n$. By taking logarithms ($\log v_x \approx \log K + n \log r$), the problem reduces to a linear regression where the slope represents the local dimension $n_x$. The authors estimate this by counting neighbors within radius $r$ and solving a least-squares problem. Core assumption: Tokens are in general position and the local geometry approximates a manifold sufficiently well for a linear fit to provide a meaningful slope. Break condition: If the log-log plot is non-linear or yields a poor linear fit, the local manifold assumption fails, indicating a stratified or singular point.

### Mechanism 2: Detecting Stratified Geometry via Slope Anomalies
A space is stratified rather than a smooth fiber bundle if volume growth curves exhibit slope increases or multi-modal dimension distributions. A fiber bundle model predicts that volume growth slopes can only decrease as a ball grows out of a "noise" fiber into a lower-dimensional base. Conversely, a stratified space allows for "flares"â€”lower-dimensional structures jutting off higher-dimensional bulks. If the slope increases, the fiber bundle hypothesis is rejected. Core assumption: The "fiber bundle hypothesis" is the null model for non-manifold geometry. Break condition: If all tokens showed monotonically decreasing slopes or a unimodal dimension distribution, the stratified space hypothesis would be rejected.

### Mechanism 3: Dimensional Spikes as Complexity Indicators
Spikes in local dimension along a trajectory correlate with high environmental complexity (multiple obstacles) or sub-goal achievement, rather than noise. The agent's representation compresses "routine" navigation into low-dimensional sub-strategies. When the agent faces a decision point or a goal state, the representation expands into higher-dimensional strata to encode the increased variance in possible futures or the reward signal. Core assumption: Dimensional variation reflects semantic/functional differences in the agent's policy state, not just pixel-level noise. Break condition: If high-dimensional tokens were found to correspond to visually simple, low-reward states, the complexity correlation would be invalidated.

## Foundational Learning

- **Concept: Stratified Spaces**
  - **Why needed here:** This is the central geometric model proposed to replace the "Manifold Hypothesis." You must understand that unlike a manifold (which has fixed dimension everywhere), a stratified space is a union of manifolds of varying dimensions (e.g., a sheet attached to a line).
  - **Quick check question:** If a point lies exactly where a 2D plane intersects a 1D line, is the local dimension 2, 1, or undefined?

- **Concept: The Manifold Hypothesis**
  - **Why needed here:** The paper frames its contribution as a "rebuke" to this hypothesis. Understanding that real data often fails to lie on a single smooth low-dimensional surface is key to interpreting the results.
  - **Quick check question:** Does the manifold hypothesis imply that the local dimension of a dataset should be constant or variable across different points?

- **Concept: Volume Growth Transform (VGT)**
  - **Why needed here:** This is the primary diagnostic tool used. It connects geometry (volume) to topology (dimension) via scaling laws.
  - **Quick check question:** In a 3-dimensional Euclidean space, does the volume of a ball scale with radius $r$ as $r^2$, $r^3$, or $r^4$?

## Architecture Onboarding

- **Component map:** Input (84x84 RGB images) -> CNN encoder (Cyan block) -> 256-dim embedding vector -> Transformer-XL (2 blocks, 4 heads) with recurrence -> Policy and Value heads -> Output (action distribution, expected return)
- **Critical path:** 1) Agent plays "Two-Coin" game, generating ~4500 observation tokens. 2) Extract 256-dim embeddings from the first transformer block. 3) Compute pairwise Euclidean distances between tokens. 4) For a target token, plot log(counts) vs. log(radius). 5) Fit linear slope over volume range [50, 90] to estimate local dimension.
- **Design tradeoffs:** The authors prefer bounding by volume (range [50, 90]) rather than radius (range [40, 60]). Volume bounding yields tighter dimension estimates and avoids "zero-dimension" errors in sparse regions, but may mix scales differently across dense vs. sparse clusters. Token selection focuses on the first transformer block; deeper layers might exhibit different geometric properties.
- **Failure signatures:** Zero-dimension tokens occur when a token has too few neighbors to fit a line, indicating sparse regions or isolated strategies. Non-linear growth curves ("kinks" or slope increases) indicate the token is on a stratified "flare," failing the fiber bundle model.
- **First 3 experiments:** 1) Generate synthetic data from a known 10-dimensional Gaussian manifold; verify VGT returns a unimodal distribution centered at 10. 2) Run the agent on a "No Obstacle" version of the game; verify that the local dimension distribution shifts lower and spike frequency decreases. 3) Measure local dimensions at the output of the CNN vs. the 2nd Transformer block to see if stratification is introduced by the attention mechanism or exists in the visual encoder.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do distinct strata in the latent space correspond directly to specific state-action trajectories or compressed symbolic representations of strategies?
- **Basis in paper:** The authors "conjecture that distinct strata in the latent space correspond to different state-action trajectories" and suggest this structure could map strata to symbolic representations.
- **Why unresolved:** The current analysis identifies the existence of strata and correlates dimension spikes with complexity, but does not decode specific strata into verifiable symbolic strategies or distinct trajectory classes.
- **What evidence would resolve it:** A successful mapping where specific regions of the latent space (strata) are shown to exclusively encode distinct, interpretable sub-policies (e.g., "fleeing" vs. "approaching") across multiple episodes.

### Open Question 2
- **Question:** Can adaptive training procedures utilizing local dimension estimates improve sample efficiency or agent robustness?
- **Basis in paper:** The authors suggest "an adaptive training procedure could make use of this information to select training examples to help the agent resolve future complex scenarios."
- **Why unresolved:** The paper analyzes a fully trained agent post-hoc and does not implement a feedback loop where the VGT dimension estimation actively influences the training data selection or reward shaping.
- **What evidence would resolve it:** An experiment showing that an RL agent trained with a curriculum prioritizing high-local-dimension states learns faster or achieves higher final rewards than a standard training regime.

### Open Question 3
- **Question:** Can the precise geometric structure of a stratified latent space be reconstructed from noisy point-cloud data beyond simple local dimension estimation?
- **Basis in paper:** The paper notes that "no method currently exists for characterizing precisely the stratified space structure of a noisy point-cloud sample" and that current formulas rely on "hard-to-estimate Lipschitz-Killing measures."
- **Why unresolved:** While the paper proves such spaces can realize volume growth curves, it relies on linear approximations for dimension estimation rather than recovering the full topological stratification (connectivity, boundaries, and singularities).
- **What evidence would resolve it:** The development of an algorithm capable of inferring the topological skeleton or stratified adjacency graph of the latent space directly from the token embeddings.

### Open Question 4
- **Question:** Is the correlation between high local dimension and decision complexity preserved in continuous control tasks or non-visual RL environments?
- **Basis in paper:** The study is restricted to a specific visual memory game and suggests the structure may be present in "many different applications," but restricts its own scope to image-based tokens.
- **Why unresolved:** The visual nature of the tokens (pixels) may inherently induce stratified structures (e.g., occlusion boundaries) that differ from the latent spaces of continuous vector-based states found in robotics.
- **What evidence would resolve it:** Application of the VGT method to non-visual RL benchmarks (e.g., MuJoCo) to verify if similar dimension spikes occur during high-entropy decision states.

## Limitations

- The VGT method assumes local neighborhoods can be meaningfully analyzed via log-volume scaling, which may not hold in regions with low token density or highly non-uniform geometry
- The correlation between dimension spikes and environmental complexity, while observed, may be an artifact of the specific architecture or training process rather than a fundamental property of RL representation learning
- The analysis focuses only on the first transformer block's output, leaving questions about how geometry evolves through deeper layers unanswered

## Confidence

- **High Confidence:** The stratified space characterization based on the presence of slope increases in VGT curves. This finding is directly supported by empirical observation and contradicts the fiber bundle hypothesis in a verifiable way.
- **Medium Confidence:** The claim that local dimension distribution (6-21) reveals meaningful structure about game complexity. While supported by correlation analysis, alternative explanations cannot be ruled out.
- **Low Confidence:** The theoretical proof that arbitrary VGT curves can be realized by stratified spaces. While mathematically interesting, this proof's relevance to the specific RL embedding geometry remains unclear.

## Next Checks

1. **Synthetic Manifold Validation:** Generate controlled synthetic data from known stratified spaces with varying local dimensions and verify that VGT correctly identifies the expected dimension distribution before applying to RL embeddings.

2. **Architectural Ablation Study:** Compare local dimension distributions across different transformer depths (CNN output, first block, second block) to determine whether stratification emerges from the attention mechanism or exists in the visual encoder.

3. **Complexity Manipulation Experiment:** Train agents on systematically varied game complexities (varying numbers of spotlights, coins, or obstacles) and measure whether the local dimension distribution shifts predictably with task difficulty.