---
ver: rpa2
title: 'BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation
  Models'
arxiv_id: '2510.20095'
source_url: https://arxiv.org/abs/2510.20095
tags:
- captions
- species
- visual
- caption
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating descriptive
  captions as additional supervision for biological multimodal foundation models.
  It proposes BioCAP, which uses synthetic captions generated by MLLMs guided by Wikipedia-derived
  visual information and taxon-tailored format examples to reduce hallucination and
  improve biological trait focus.
---

# BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models

## Quick Facts
- arXiv ID: 2510.20095
- Source URL: https://arxiv.org/abs/2510.20095
- Authors: Ziheng Zhang; Xinyue Ma; Arpita Chowdhury; Elizabeth G. Campolongo; Matthew J. Thompson; Net Zhang; Samuel Stevens; Hilmar Lapp; Tanya Berger-Wolf; Yu Su; Wei-Lun Chao; Jianyang Gu
- Reference count: 40
- One-line primary result: Achieves 8.8% improvement in species classification and 21.3% improvement in biological text-image retrieval by incorporating synthetic descriptive captions guided by Wikipedia and taxon-specific formats.

## Executive Summary
BioCAP addresses the challenge of incorporating descriptive captions as additional supervision for biological multimodal foundation models. The method generates synthetic captions using MLLMs guided by Wikipedia-derived visual information and taxon-tailored format examples to reduce hallucination and improve biological trait focus. Trained on TreeOfLife-10M with species names and captions as complementary supervision, BioCAP demonstrates strong performance in species classification and biological text-image retrieval tasks, outperforming baselines significantly.

## Method Summary
BioCAP trains a multimodal foundation model using both species labels and synthetic descriptive captions as complementary supervision. The approach begins with Wikipedia scraping to extract species-level morphological descriptions, supplemented by manually curated format examples for taxonomic classes. Synthetic captions are generated using InternVL3 38B with domain-specific prompts combining Wikipedia context and format examples. The model uses a dual-projector architecture with separate visual encoders for taxonomy labels and captions, initialized from OpenAI CLIP ViT-B/16 checkpoint. Training runs for 50 epochs on TreeOfLife-10M with AdamW optimizer and large batch sizes.

## Key Results
- Achieves 8.8% average improvement in zero-shot species classification over BioCLIP across 10 benchmarks
- Outperforms baselines by 21.3% in biological text-image retrieval tasks
- Human evaluation shows 85.0% win rate for full method versus 8.5% for trait-only captions
- Dual projector architecture consistently outperforms single projector across all evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1: Trait Alignment via Contrastive Learning
- Claim: Descriptive captions encourage image representations to align with diagnostic biological traits rather than spurious environmental factors
- Mechanism: Images and captions are modeled as noisy observations of a shared latent morphospace vector encoding true phenotypic traits. Contrastive alignment between these modalities amplifies trait-relevant factors while suppressing environmental noise
- Core assumption: Captions faithfully reflect visible, potentially diagnostic characters
- Evidence: Ablation shows "Base" prompt captions degrade performance below baseline, while full pipeline achieves 33.8% classification vs 27.0% for Base prompt
- Break condition: Hallucinated or instance-unspecific captions introduce contradictory signals

### Mechanism 2: Domain Context Reduces Hallucination
- Claim: Wikipedia-derived visual information and taxon-tailored format examples reduce MLLM hallucination and produce grounded, trait-focused captions
- Mechanism: Wikipedia provides species-level morphological vocabulary; format examples per taxonomic class guide attention to salient traits
- Core assumption: Wikipedia descriptions provide correct morphological vocabulary applicable to specific instances
- Evidence: Human evaluation shows 85.0% average win rate for full method vs 8.5% for Trait-only; Table 3 shows Trait+Example+Wiki achieves 33.8% classification
- Break condition: When Wikipedia lacks coverage (70.5% of taxa uncovered), only format examples apply

### Mechanism 3: Separate Projectors for Heterogeneous Supervision
- Claim: Separate visual projectors for taxonomy and caption supervision outperform unified projection
- Mechanism: Heterogeneous supervision signals (sparse symbolic labels vs dense descriptive captions) require decoupled alignment pathways
- Core assumption: Taxonomy and caption signals provide complementary rather than redundant information
- Evidence: Figure 3 shows dual proj consistently outperforms single proj across classification, retrieval, and INQUIRE tasks
- Break condition: Extreme class imbalance or caption sparsity may reduce benefit of separate projectors

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP-style alignment)**
  - Why needed here: BioCAP builds directly on OpenAI CLIP ViT-B/16 checkpoint; understanding InfoNCE loss, alignment/uniformity properties, and temperature scaling is prerequisite for interpreting §A analysis
  - Quick check question: Given normalized embeddings x and c, what does the cross-covariance Σxc represent, and why does it decompose into trait and nuisance terms?

- **Concept: Latent Morphospace (Biological)**
  - Why needed here: The theoretical justification assumes a latent trait vector z* encoding phenotypic characters; this biological concept underpins why caption-image alignment should improve classification
  - Quick check question: If an image captures pose ε but a caption captures color pattern, how does aligning them help recover z*?

- **Concept: MLLM Hallucination in Scientific Domains**
  - Why needed here: The core problem is that MLLMs hallucinate biological details when conditioned only on images; understanding why domain context mitigates this is essential
  - Quick check question: Why would a generic "describe this image" prompt produce worse downstream performance than no caption at all (Table 3: Base = 27.0% vs None = 30.2%)?

## Architecture Onboarding

- **Component map**: Wikipedia scraping pipeline → Format example curation → Caption generation → BioCAP training → Evaluation suite
- **Critical path**: Wikipedia coverage determines caption quality for 29.5% of taxa; caption quality directly controls classification gain; dual projector architecture required for heterogeneous supervision to not interfere
- **Design tradeoffs**: Caption generation cost (30 GPU-hours) vs simply training longer (200 epochs worse than 50 epochs + captions); Wikipedia coverage (52.3% of samples) vs format-example-only fallback; InternVL3 38B choice may emphasize different traits
- **Failure signatures**: Captions describe wrong object; hallucinated colors; retrieval collapse if single projector used with noisy captions
- **First 3 experiments**:
  1. Caption quality ablation: Train on TreeOfLife-1M with Base, Trait, Trait+Example, Trait+Example+Wiki prompts; validate on held-out species
  2. Projector architecture test: Compare single unified projector vs dual projectors on retrieval task
  3. Wikipedia coverage analysis: Partition test set by Wikipedia coverage; measure per-group classification gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of different MLLMs (e.g., InternVL3, GPT-4o, LLaVA) affect caption quality and downstream model performance?
- Basis: Limitations section explicitly states this as important future work
- Why unresolved: Study relies exclusively on InternVL3 38B for caption generation
- What evidence would resolve it: Systematic comparison of captions and model performance using multiple MLLMs on same image set

### Open Question 2
- Question: Does scaling the caption generation pipeline to datasets larger than TreeOfLife-10M (e.g., TreeOfLife-200M) continue to yield proportional improvements?
- Basis: Discussion mentions pipeline can be scaled up but doesn't empirically validate scaling behavior
- Why unresolved: Experiments conducted only on TreeOfLife-10M; scaling effects remain untested
- What evidence would resolve it: Training BioCAP on larger datasets and comparing performance gains per unit of additional data

### Open Question 3
- Question: How do synthetic captions compare to human expert-generated captions in terms of downstream model performance?
- Basis: Paper uses synthetic captions due to prohibitive cost of expert annotations but doesn't compare against expert-annotated subset
- Why unresolved: No experiment directly contrasts synthetic vs. expert captions
- What evidence would resolve it: Collect expert captions for subset of images, train separate models, and benchmark performance differences

### Open Question 4
- Question: Can the BioCAP approach generalize to other scientific domains lacking instance-level textual descriptions (e.g., astronomy, geology, material science)?
- Basis: Introduction states domains like astronomy, geology, and material science "often lack instance-level textual descriptions"
- Why unresolved: All experiments confined to organismal biology datasets
- What evidence would resolve it: Apply domain-specific context pipeline to dataset from another scientific domain and evaluate performance

## Limitations
- Wikipedia coverage is limited to 29.5% of taxa, with no analysis of how caption quality varies with coverage or taxonomic specificity
- The theoretical morphospace formulation lacks direct empirical validation through experiments testing trait dimension recovery
- Caption generation relies exclusively on InternVL3 38B, with no exploration of how different MLLMs affect performance

## Confidence

- **High confidence**: Empirical results showing BioCAP outperforms baselines on classification (8.8% gain) and retrieval (21.3% gain) are well-supported by presented experiments
- **Medium confidence**: Mechanism claims about trait alignment and hallucination reduction are supported by ablations and human evaluation, but theoretical justification lacks direct experimental validation
- **Low confidence**: Claim that Wikipedia-derived contexts are essential (vs. format examples alone) is supported by performance gains but not rigorously tested

## Next Checks

1. **Morphospace validation**: Analyze BioCAP representations to measure correlation between caption-aligned dimensions and known biological trait variables versus environmental factors
2. **Caption necessity test**: Train BioCAP variants using only format examples (no Wikipedia) on the 70.5% of taxa without coverage, measuring whether gains persist from format examples alone
3. **MLLM sensitivity analysis**: Repeat caption generation using different MLLMs (e.g., GPT-4V, Claude) to test whether performance gains depend on InternVL3's trait emphasis or generalize across models