---
ver: rpa2
title: Internalizing World Models via Self-Play Finetuning for Agentic RL
arxiv_id: '2510.15047'
source_url: https://arxiv.org/abs/2510.15047
tags:
- state
- pass
- world
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving Large Language Model
  (LLM) agents' performance in out-of-distribution (OOD) environments, where traditional
  reinforcement learning (RL) methods often fail to scale due to brittle exploration
  and limited generalization. The core method, SPA (Self-Play Agent), introduces a
  novel framework that equips LLM agents with an internal world model by decomposing
  it into state representation and transition modeling.
---

# Internalizing World Models via Self-Play Finetuning for Agentic RL

## Quick Facts
- arXiv ID: 2510.15047
- Source URL: https://arxiv.org/abs/2510.15047
- Authors: Shiqi Chen; Tongyao Zhu; Zian Wang; Jinghan Zhang; Kangrui Wang; Siyang Gao; Teng Xiao; Yee Whye Teh; Junxian He; Manling Li
- Reference count: 14
- Primary result: Two-stage self-play SFT followed by PPO outperforms online world-modeling baselines, achieving 59.8% Sokoban success rate and 70.9% FrozenLake score for Qwen2.5-1.5B-Instruct.

## Executive Summary
The paper addresses the challenge of improving Large Language Model (LLM) agents' performance in out-of-distribution (OOD) environments, where traditional reinforcement learning (RL) methods often fail to scale due to brittle exploration and limited generalization. The core method, SPA (Self-Play Agent), introduces a novel framework that equips LLM agents with an internal world model by decomposing it into state representation and transition modeling. SPA first uses self-play supervised fine-tuning (SFT) to learn the environment's dynamics through interaction, then leverages this learned model to simulate future states before policy optimization. This approach outperforms online world-modeling baselines and significantly boosts RL-based agent training performance. Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku demonstrate substantial improvements: SPA boosts Sokoban success rates from 25.6% to 59.8% and raises FrozenLake scores from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

## Method Summary
SPA employs a two-stage training pipeline: First, self-play SFT learns the environment's dynamics by collecting trajectories where the model predicts next states, then replacing those predictions with ground-truth states before training. This builds a dynamics prior without reward pressure. Second, PPO optimizes the policy for task rewards, initialized from the SFT checkpoint. The method decomposes the world model into state representation (converting raw observations to structured natural language with coordinates) and transition modeling (learning p(s'|s,a) via masked cross-entropy on observation/prediction/answer spans). This separation prevents the multi-objective interference seen in online world-modeling approaches that mix accuracy rewards with task rewards.

## Key Results
- SPA achieves 59.8% success rate on Sokoban 6×6 versus 25.6% baseline (Qwen2.5-1.5B-Instruct)
- SPA achieves 70.9% score on FrozenLake 4×4 versus 22.1% baseline (Qwen2.5-1.5B-Instruct)
- State estimation reduces Sokoban perplexity from 163.9 to 19.6, and FrozenLake from 187.1 to 15.4
- Easy-to-hard transfer works: models trained on 4×4 FrozenLake achieve 32.9% Pass@1 on 6×6, versus 0% for direct training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-play SFT builds a reusable dynamics prior that prevents reward overfitting during RL
- Mechanism: The model freely explores state/action space without exploitation pressure, generating diverse transition triples (s_t, a_t, s_{t+1}). Ground-truth states replace hallucinated predictions, and SFT trains the transition kernel p_θ(s_{t+1}|s_t, a_t). This coverage diversifies multi-step reasoning before RL narrows the policy.
- Core assumption: OOD failure stems from memorizing trajectories rather than learning environment dynamics
- Evidence anchors:
  - [abstract] "Self-Play supervised finetuning (SFT) stage to learn the world model by interacting with the environment, then uses it to simulate future states prior to policy optimization"
  - [section 2.2] Loss function optimizes masked cross-entropy over reasoning spans containing <observation> and <prediction>
  - [corpus] Related work on self-play (SWE-RL, Search Self-play) suggests self-play benefits agent capability, though theoretical foundations remain underexplored
- Break condition: If environment dynamics are non-Markovian or highly stochastic, single-step transition modeling may fail

### Mechanism 2
- Claim: Grounded state representations reduce distribution shift by converting unfamiliar symbolic states to interpretable natural language
- Mechanism: Raw states (e.g., `######\n#O XP#\n######`) have high perplexity (163.9 for Sokoban). Adding coordinate-based descriptions (e.g., "Player at (3,3); Box at (3,2)") reduces PPL to 19.6, making spatial relations explicit for LLM reasoning.
- Core assumption: Distribution shift—not lack of reasoning capacity—is the primary bottleneck for OOD generalization
- Evidence anchors:
  - [abstract] "Decomposing world model into state representation and transition modeling"
  - [Table 1] State Estimation reduces Sokoban PPL from 163.9→19.6, FrozenLake 187.1→15.4
  - [corpus] Weak direct evidence; neighbor papers focus on self-play/training, not state representation formats
- Break condition: If environments lack spatial structure or coordinates don't capture task-relevant information

### Mechanism 3
- Claim: Two-stage separation prevents multi-objective interference between world-model learning and policy optimization
- Mechanism: VAGEN (online world-modeling via reward shaping) mixes accuracy rewards with task rewards, causing brittleness. SPA confines world-model learning to SFT stage, then optimizes only task rewards during RL—avoiding gradient conflicts.
- Core assumption: Reward shaping for accuracy creates conflicting gradients that destabilize training
- Evidence anchors:
  - [abstract] "This simple initialization outperforms the online world-modeling baseline"
  - [section 3] "SPA consistently surpasses VAGEN... suggesting that the accuracy objective is brittle and should not be perturbed during RL training"
  - [corpus] Neighbor paper "Your Self-Play Algorithm is Secretly an Adversarial Imitator" provides theoretical lens linking self-play to imitation learning
- Break condition: If task rewards are too sparse to guide policy without intermediate accuracy signals

## Foundational Learning

- Concept: Model-Based RL / World Models
  - Why needed here: SPA imports classical model-based RL ideas (latent states + transition dynamics) into LLM agent training. Understanding Janner et al. (2019) or Dreamer helps contextualize why explicit dynamics models improve planning.
  - Quick check question: Can you explain why learning p(s'|s,a) helps with credit assignment in multi-step tasks?

- Concept: Distribution Shift / OOD Generalization
  - Why needed here: The paper diagnoses OOD failure via Pass@k degradation and perplexity analysis. Understanding train-test distribution mismatch clarifies why vanilla RL overfits.
  - Quick check question: Why would Pass@1 improve while Pass@k drops in OOD settings?

- Concept: Self-Play / Exploration-Exploitation Trade-off
  - Why needed here: SPA's core insight is "exploration before exploitation"—self-play SFT explores broadly, then RL exploits. This mirrors epsilon-greedy dynamics but at a stage level.
  - Quick check question: How does self-play differ from imitation learning when no expert demonstrations exist?

## Architecture Onboarding

- Component map: Environment wrapper -> State estimator -> Self-play data collector -> SFT trainer -> PPO trainer
- Critical path: Environment → Self-play data collection (T turns) → Ground-truth replacement → SFT on transitions → PPO on task rewards
- Design tradeoffs:
  - Longer SFT = better world model but delayed RL; paper shows 5 epochs optimal for 1.5B model
  - Filtered data (format-compliant only) improves stability but reduces coverage
  - Coordinate-based state representations help spatial tasks; unclear benefit for non-spatial domains
- Failure signatures:
  - Pass@k drops during training → world model not learned; check SFT loss on <prediction> spans
  - Random coordinates cause collapse → verify ground-truth injection in data pipeline
  - Instruction noncompliance in self-play → filter malformed samples or use stronger base model
- First 3 experiments:
  1. **Sanity check**: Train SPA on Sokoban with masked transition annotations (should show no improvement vs. baseline, per Figure 4)
  2. **State representation ablation**: Compare raw grids vs. coordinate-augmented states on perplexity and Pass@1 (should match Table 1 trends)
  3. **Transfer test**: Train world model on 4×4 FrozenLake, test on 6×6 (should show Easy2Hard transfer as in Figure 9)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SPA incorporate uncertainty-aware world models to improve training stability in stochastic environments?
- Basis in paper: [explicit] Section 7 states: "Limitations remain in stochastic settings, where training stability is fragile... Future work may incorporate uncertainty-aware transitions."
- Why unresolved: The current transition model $p_\theta(s_{t+1}|s_t, a_t)$ provides point predictions without uncertainty estimates, which may cause brittle behavior when environment stochasticity (e.g., FrozenLake's slippery ice) makes single predictions unreliable.
- What evidence would resolve it: Demonstrating that a probabilistic transition model with explicit uncertainty quantification (e.g., via ensembles or distributional outputs) improves Pass@k stability and asymptotic performance on FrozenLake compared to the current deterministic prediction approach.

### Open Question 2
- Question: How can world-model learning succeed when ground-truth state supervision is unavailable or partial?
- Basis in paper: [inferred] SPA relies on replacing hallucinated predictions with ground-truth states from environment interaction (Figure 3). The ablation in Section 4.1 shows using only self-belief states degrades RL performance. Real-world agentic settings often lack clean state abstractions.
- Why unresolved: The method depends on accurate ground-truth $(s_t, a_t, s_{t+1})$ triples. Whether agents can learn useful world models from noisy, partial, or inferred state information remains untested.
- What evidence would resolve it: Evaluating SPA variants on environments with partial observability or where state representations must be learned from raw observations without ground-truth coordinates.

### Open Question 3
- Question: What mechanisms could enable cross-game transfer of learned world models?
- Basis in paper: [explicit] Table 6 shows SPA trained on Sokoban achieves only 15.9% Pass@1 on FrozenLake (below baseline 17.2%), indicating "cross-game generalization is difficult." Section 5 notes transfer fails "across games with fundamentally different dynamics."
- Why unresolved: World models are environment-specific, and no analysis explains what shared structure (if any) could enable transfer, or whether meta-learning across multiple environments could yield generalizable dynamics priors.
- What evidence would resolve it: Demonstrating that multi-environment training or shared abstract state representations enable positive transfer, or characterizing what minimal shared structure is required for cross-game generalization.

### Open Question 4
- Question: How does SPA scale to richer modalities and more complex real-world agentic tasks?
- Basis in paper: [explicit] Section 7: "Future work may... scale to richer modalities." The paper evaluates only text-based grid worlds and Sudoku. Section 1 notes real-world environments involve "task-specific rules and stochasticity" that challenge grounding.
- Why unresolved: It is unclear whether the SFT-based world-modeling approach transfers to environments with visual observations, continuous action spaces, or longer horizons common in robotics or web automation.
- What evidence would resolve it: Applying SPA to multimodal agentic benchmarks (e.g., web navigation with screenshots, embodied tasks with pixel observations) and comparing against vanilla RL and VAGEN baselines.

## Limitations
- The method relies heavily on ground-truth state supervision for world-model learning, which may not be available in many real-world settings
- Cross-game transfer of learned world models shows limited success, with performance sometimes below baseline when transferring between fundamentally different environments
- Exact PPO hyperparameters, data collection scale, and environment implementations are underspecified, potentially affecting reproducibility

## Confidence

- **High Confidence**: The core claim that SPA's two-stage approach (self-play SFT → PPO) improves OOD generalization over online world-modeling baselines (VAGEN) is well-supported by experimental results showing substantial gains in Pass@1 and Pass@8 across Sokoban, FrozenLake, and Sudoku tasks.
- **Medium Confidence**: The mechanism that self-play SFT builds a reusable dynamics prior preventing reward overfitting is plausible but relies on assumed distribution shift as the primary bottleneck—this causal link isn't directly tested.
- **Medium Confidence**: The state representation claim (coordinates reduce PPL) is empirically supported but limited to spatial domains; benefits for non-spatial tasks remain unclear.

## Next Checks

1. **Hyperparameter Sensitivity**: Test SPA performance across different PPO clipping thresholds and GAE parameters to establish robustness to these unspecified settings.
2. **Non-Spatial Transfer**: Apply SPA to a non-spatial environment (e.g., text-based puzzle) to validate whether coordinate-based state representations and self-play SFT generalize beyond grid-worlds.
3. **Data Scale Dependence**: Vary the amount of self-play data used in SFT (e.g., 640 vs. 2560 samples) to quantify the relationship between data volume and downstream RL performance.