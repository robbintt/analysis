---
ver: rpa2
title: Sparse Offline Reinforcement Learning with Corruption Robustness
arxiv_id: '2512.24768'
source_url: https://arxiv.org/abs/2512.24768
tags:
- sparse
- offline
- learning
- coverage
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies corruption-robust offline reinforcement learning
  in high-dimensional sparse Markov decision processes (MDPs) where the feature dimension
  $d$ may exceed the number of samples $N$. The key challenge is that directly applying
  standard least-squares value iteration (LSVI) with sparsity constraints fails due
  to overly pessimistic pointwise bonuses that are incompatible with sparse support
  uncertainty.
---

# Sparse Offline Reinforcement Learning with Corruption Robustness

## Quick Facts
- arXiv ID: 2512.24768
- Source URL: https://arxiv.org/abs/2512.24768
- Authors: Nam Phuong Tran; Andi Nika; Goran Radanovic; Long Tran-Thanh; Debmalya Mandal
- Reference count: 40
- Primary result: First non-vacuous corruption-robust guarantees for sparse offline RL under single-policy concentrability

## Executive Summary
This paper addresses the challenge of learning in high-dimensional sparse Markov decision processes (MDPs) when the offline data may be corrupted. The key insight is that standard least-squares value iteration (LSVI) with pointwise pessimism fails in sparse settings because it cannot accommodate the uncertainty in sparse support. Instead, the authors propose actor-critic methods with sparse robust regression oracles that avoid pointwise pessimism. They establish theoretical guarantees showing suboptimality gaps of $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4})$ for computationally expensive oracles and $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4} + H^2\sqrt{\kappa s}\epsilon^{1/4})$ for efficient oracles, where $s$ is sparsity, $\kappa$ is condition number, and $\epsilon$ is corruption fraction. The work demonstrates that actor-critic methods are inherently better suited for sparse offline RL than LSVI-based approaches.

## Method Summary
The authors develop an actor-critic framework for sparse offline reinforcement learning that incorporates corruption robustness. The core innovation is the use of sparse robust regression oracles (SRLE) that operate on the Bellman residual rather than requiring pointwise pessimism. The algorithm maintains two key components: a sparse regression oracle for estimating Q-functions and an actor that updates policies based on these estimates. The method operates under single-policy concentrability coverage, which is weaker than uniform coverage assumptions used in prior work. The key technical insight is that by avoiding pointwise bonuses and instead using sparse regression to handle support uncertainty, the algorithm can achieve meaningful guarantees even when the feature dimension exceeds the number of samples.

## Key Results
- First non-vacuous corruption-robust guarantees for sparse offline RL under single-policy concentrability
- Suboptimality gap of $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4})$ with computationally expensive sparse robust regression oracles
- Suboptimality gap of $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4} + H^2\sqrt{\kappa s}\epsilon^{1/4})$ with efficient oracles
- Demonstrates superiority of actor-critic methods over LSVI in sparse regimes due to natural compatibility with sparsity structure

## Why This Works (Mechanism)
The mechanism works because actor-critic methods naturally accommodate the uncertainty in sparse support without requiring pointwise pessimism. In sparse MDPs, the feature dimension $d$ may exceed the number of samples $N$, making it impossible to estimate Q-functions pointwise with confidence. Traditional LSVI approaches fail because they rely on pessimistic bonuses that are overly conservative when the support is unknown. The proposed method instead uses sparse regression oracles that directly estimate the Bellman residual while enforcing sparsity constraints. This allows the algorithm to handle both the high-dimensional nature of the problem and the corruption in the data simultaneously, without the compounding pessimism that plagues LSVI-based approaches.

## Foundational Learning

### Sparse Regression in High Dimensions
- **Why needed**: When $d > N$, standard least-squares regression is ill-posed; sparsity constraints make the problem tractable
- **Quick check**: Verify that the sparse regression oracle achieves the stated error bounds $\tilde{O}(\sqrt{s}\epsilon)$ for the expensive version and $\tilde{O}(s\sqrt{\epsilon})$ for the efficient version

### Robust Statistics for Corruption
- **Why needed**: Offline data may contain arbitrary corruptions up to fraction $\epsilon$; robust estimators maintain accuracy despite contamination
- **Quick check**: Confirm that the robust regression oracle correctly identifies and downweights corrupted samples in the Bellman residual

### Single-Policy Concentrability
- **Why needed**: Weaker than uniform coverage; ensures that the offline data distribution is close to some behavior policy
- **Quick check**: Verify that the concentrability coefficient $\rho$ is bounded and doesn't scale poorly with problem complexity

## Architecture Onboarding

### Component Map
Sparse Regression Oracle -> Bellman Residual Estimation -> Policy Update -> Next State Distribution

### Critical Path
1. Compute Bellman residuals using current Q-function estimates
2. Apply sparse robust regression oracle to estimate Q-function
3. Update policy using estimated Q-function
4. Evaluate policy performance and check convergence

### Design Tradeoffs
- Computational efficiency vs statistical optimality: Expensive oracle achieves $\tilde{O}(\sqrt{s}\epsilon)$ error, efficient oracle achieves $\tilde{O}(s\sqrt{\epsilon})$
- Sparsity enforcement: $\ell_0$ constraint is statistically optimal but computationally intractable; convex relaxations may sacrifice guarantees
- Coverage assumption: Single-policy concentrability is weaker than uniform coverage but may still be restrictive in practice

### Failure Signatures
- If sparsity assumption violated: Algorithm performance degrades as it enforces unnecessary sparsity
- If corruption fraction $\epsilon$ too large: Robust regression fails to distinguish signal from noise
- If concentrability coefficient $\rho$ unbounded: Theoretical guarantees become vacuous

### Three First Experiments
1. Verify sparse regression oracle achieves stated error rates on synthetic corrupted data
2. Test actor-critic algorithm on a simple sparse MDP with known optimal policy
3. Compare performance of computationally expensive vs efficient oracles as corruption level varies

## Open Questions the Paper Calls Out

### Open Question 1
Can the $\ell_0$-constraint in the PessOpt optimization problem (Equation 14) be relaxed to a convex surrogate (e.g., $\ell_1$) while preserving the suboptimality guarantees under single-policy concentrability?

**Basis in paper**: The conclusion explicitly identifies this as an important direction, noting that the $\ell_0$ constraint is a primary bottleneck for achieving a polynomial-time algorithm.

**Why unresolved**: The authors note that without the sparsity constraint, the error scales poorly with dimension $d$, yet enforcing exact sparsity computationally is difficult.

**What evidence would resolve it**: An algorithm using a convex relaxation that provably achieves a suboptimality gap of $\tilde{O}(H^2\sqrt{\kappa s}\epsilon)$ in polynomial time.

### Open Question 2
Does there exist a computationally efficient sparse robust regression oracle (SRLE) that achieves the statistically optimal corruption rate of $O(\sqrt{s}\epsilon)$?

**Basis in paper**: Remark 4.2 states that the best efficient oracle (SRLE1) achieves a suboptimal $O(s\sqrt{\epsilon})$ rate, and asks if this can be improved.

**Why unresolved**: The authors observe a trade-off where statistically optimal oracles are computationally intractable, while efficient ones have looser error bounds.

**What evidence would resolve it**: The derivation of a polynomial-time estimator matching the $O(\sqrt{s}\epsilon)$ rate of the computationally expensive SRLE2.

### Open Question 3
What specific distributional relaxations on the data distribution $\nu$ (beyond uniform coverage) are required to render the sparse offline RL problem computationally tractable?

**Basis in paper**: Page 8 notes that without assumptions on $\nu$, solving the optimization is difficult, and the authors conjecture that "some relaxation of data distribution $\nu$ may be needed."

**Why unresolved**: The current analysis allows for general distributions under single-policy concentrability, which forces the reliance on hard sparsity constraints.

**What evidence would resolve it**: A theoretical demonstration that specific non-uniform structures in $\nu$ allow for efficient optimization without exact $\ell_0$ constraints.

## Limitations
- Relies on single-policy concentrability coverage, which may still be restrictive in practice
- Computationally expensive sparse robust regression oracle limits practical applicability
- Gap between computationally efficient and expensive oracle guarantees suggests room for algorithmic improvement
- No empirical validation on real-world benchmarks or synthetic experiments

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical guarantees under single-policy concentrability | High |
| Superiority of actor-critic over LSVI in sparse regimes | High |
| Suboptimality bounds for both oracle variants | High |
| Practical performance on real-world problems | Medium |

## Next Checks
1. Empirical evaluation comparing actor-critic versus LSVI-based approaches on benchmark sparse MDPs with varying corruption levels
2. Investigation of the practical performance gap between computationally efficient and expensive sparse robust regression oracles
3. Extension of theoretical analysis to more general concentrability assumptions or alternative coverage conditions