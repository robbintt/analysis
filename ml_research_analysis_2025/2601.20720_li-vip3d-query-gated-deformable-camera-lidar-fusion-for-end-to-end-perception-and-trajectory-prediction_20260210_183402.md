---
ver: rpa2
title: 'Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception
  and Trajectory Prediction'
arxiv_id: '2601.20720'
source_url: https://arxiv.org/abs/2601.20720
tags:
- lidar
- fusion
- end-to-end
- prediction
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Li-ViP3D++ introduces Query-Gated Deformable Fusion (QGDF), a fully
  differentiable fusion strategy for integrating RGB and LiDAR inputs within a query-based
  end-to-end perception-and-prediction framework. By aggregating image evidence through
  masked attention across cameras and feature levels, extracting LiDAR context via
  learned BEV offsets, and applying query-conditioned gating, the model improves detection
  quality and end-to-end trajectory prediction without sacrificing efficiency.
---

# Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction

## Quick Facts
- arXiv ID: 2601.20720
- Source URL: https://arxiv.org/abs/2601.20720
- Reference count: 32
- One-line primary result: Li-ViP3D++ achieves EPA of 0.335, mAP of 0.502, and reduces false positives to a ratio of 0.147 on nuScenes.

## Executive Summary
Li-ViP3D++ introduces Query-Gated Deformable Fusion (QGDF), a fully differentiable fusion strategy for integrating RGB and LiDAR inputs within a query-based end-to-end perception-and-prediction framework. By aggregating image evidence through masked attention across cameras and feature levels, extracting LiDAR context via learned BEV offsets, and applying query-conditioned gating, the model improves detection quality and end-to-end trajectory prediction without sacrificing efficiency. On nuScenes, Li-ViP3D++ achieves an EPA of 0.335, mAP of 0.502, and reduces false positives to a ratio of 0.147, while also running faster than its predecessor (139.82 ms vs. 145.91 ms). These results demonstrate that fully differentiable camera–LiDAR fusion in the query space can substantially improve robustness and calibration in end-to-end PnP models.

## Method Summary
Li-ViP3D++ builds on ViP3D by adding a Query-Gated Deformable Fusion (QGDF) module that fuses multi-view RGB and LiDAR features at the query level. Camera features are encoded by ResNet50 and sampled via masked attention; LiDAR is encoded via PointPillars and sampled using learned BEV offsets. A query-conditioned softmax gate dynamically weights modalities before residual fusion. Agent queries (with temporal memory) are decoded into 3D reference points, used for feature sampling, and passed through a trajectory decoder. The model is trained end-to-end with joint detection and trajectory prediction losses on nuScenes.

## Key Results
- EPA of 0.335 and mAP of 0.502 on nuScenes validation set.
- False positive ratio reduced to 0.147 (substantial improvement over baseline).
- Inference speed of 139.82 ms on RTX 4090 (faster than predecessor, slower than unimodal ViP3D).
- Trajectory prediction slightly degraded (minADE/minFDE up) but more conservative and better-calibrated detections.

## Why This Works (Mechanism)

### Mechanism 1: Query-Gated Deformable Fusion (QGDF)
- Claim: Fully differentiable query-space fusion of camera and LiDAR modalities improves end-to-end perception quality and reduces false positives.
- Mechanism: Agent queries serve as a sparse, learnable interface. Each query samples multi-view RGB features via masked attention and LiDAR BEV features via learned offsets, then a softmax-gated mechanism adaptively weights modalities per query before residual update.
- Core assumption: Object queries can faithfully encode agent state across modalities without dense BEV alignment overhead.
- Evidence anchors:
  - [abstract] "QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating"
  - [Section III-B] Equations 1–15 define the full QGDF pipeline: sampling, masking, gating, and residual update.
  - [corpus] Weak direct corpus support for this exact QGDF formulation; related work (UncAD, EgoFSD) addresses differentiable fusion but without query-gated offsets.
- Break condition: If queries are poorly initialized or reference points diverge, sampling degrades and gating receives uninformative features.

### Mechanism 2: Masked Attention Over Cameras and Feature Levels
- Claim: Sampling image features at query reference points and masking invalid projections reduces spurious attention to irrelevant views.
- Mechanism: For each query, project its 3D reference into all cameras, bilinear-sample across pyramid levels, compute a validity mask (front-of-camera + in-bounds), and apply masked softmax so invalid views receive zero probability.
- Core assumption: 3D reference points are sufficiently accurate to land near object centers in at least one view.
- Evidence anchors:
  - [Section III-B1] "The sampling procedure projects r into each camera... producing sampled features... validity mask M... masked softmax"
  - [abstract] "aggregates image evidence via masked attention across cameras and feature levels"
  - [corpus] Related sparse-query methods (e.g., DETR3D-style) use similar projection, but explicit multi-level masked aggregation is underexplored.
- Break condition: If camera calibration drifts or reference points are far from object centers, sampled features may be noisy or uninformative.

### Mechanism 3: Differentiable BEV Sampling with Learned Offsets
- Claim: Continuous, query-adaptive BEV sampling with bounded offsets replaces brittle discrete top-k selection and preserves gradient flow.
- Mechanism: Predict per-query 2D offsets via a lightweight FFN, apply tanh squashing and clipping to bound search radius, then bilinearly sample LiDAR BEV features. This enables the model to adjust sampling locations during training.
- Core assumption: BEV features encode sufficient local geometric context within the bounded offset range.
- Evidence anchors:
  - [Section III-B2] Equations 7–9: offset prediction, clipping, and grid_sample with bilinear interpolation.
  - [Section V-B] "monotonic trend... consistent with an adaptive fusion behavior where the model strengthens geometric conditioning when reliable 3D evidence is available"
  - [corpus] Weak direct evidence; corpus focuses on dense BEV fusion (BEVFusion variants), not query-offset sampling.
- Break condition: If BEV features are too coarse or LiDAR coverage is sparse, learned offsets may not find informative regions.

## Foundational Learning

- Concept: **Object/Agent Queries (DETR family)**
  - Why needed here: The entire Li-ViP3D++ architecture is built around agent queries as the interface between perception and prediction. Without understanding queries as learned sparse embeddings decoded into 3D reference points, the QGDF mechanism will not make sense.
  - Quick check question: Can you explain how a query embedding is transformed into a 3D reference point and then used to sample sensor features?

- Concept: **Deformable Attention / Learned Sampling Offsets**
  - Why needed here: QGDF uses learned BEV offsets rather than fixed sampling. Understanding deformable attention (predicting offsets, bounded sampling, gradient flow through bilinear interpolation) is essential.
  - Quick check question: Why does tanh + clip help stabilize learned offsets during training?

- Concept: **Gating Mechanisms (Softmax-based)**
  - Why needed here: Modality weighting uses softmax gates computed from concatenated features. Understanding why gating helps adaptive fusion—and how detached queries prevent circular dependencies—is important.
  - Quick check question: What would happen if you didn't detach Qt when computing fusion logits?

## Architecture Onboarding

- Component map: Camera Encoder -> QGDF (Image branch: project refs -> sample -> mask -> aggregate; LiDAR branch: predict offsets -> sample BEV) -> Gating (compute γt -> weighted fusion -> residual update) -> Enhanced queries -> Temporal cross-attention -> Trajectory decoder

- Critical path: Sensor encoders -> QGDF (per decoder layer) -> enhanced queries -> temporal cross-attention -> trajectory decoder. If QGDF outputs degraded features, all downstream tasks suffer.

- Design tradeoffs:
  - Detection vs. trajectory accuracy: Li-ViP3D++ improves EPA/mAP/FP ratio but slightly increases minADE/minFDE vs. Li-ViP3D. Paper attributes this to more conservative, better-calibrated detections.
  - Latency: QGDF is faster than original Li-ViP3D (139.82 ms vs. 145.91 ms) but slower than vision-only ViP3D (117.27 ms). LiDAR processing adds overhead.

- Failure signatures:
  - High FP ratio with low LiDAR weight: gating may not be learning to use LiDAR; check γt distribution.
  - Queries with large BEV offsets: may indicate reference point drift or BEV misalignment.
  - Sudden recall drop: validity mask may be overly aggressive; check camera calibration and projection bounds.

- First 3 experiments:
  1. **Ablate gating**: Set γt = [0.5, 0.5] fixed. Compare EPA/FP ratio to full model to isolate gating contribution.
  2. **Vary offset bounds**: Change clip range in Equation 8 (e.g., ±0.2 vs. ±0.8). Measure impact on minADE and FP ratio.
  3. **Modality dropout**: Run inference with LiDAR disabled (QL = 0). Compare detection metrics to assess LiDAR reliance and robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multimodal agent query interface maintain performance when the dependency on HD maps is removed?
- Basis in paper: [explicit] The authors explicitly state the need to "explore the benefits... by removing the dependence on HD maps," identifying them as a limiting factor in many applications.
- Why unresolved: The current architecture fuses agent queries with HD map context via VectorNet, and it is unclear if the sensor fusion alone is sufficient to sustain prediction accuracy without this semantic road structure.
- What evidence would resolve it: Ablation studies on nuScenes excluding the HD map input to quantify the degradation in trajectory error and end-to-end prediction accuracy (EPA).

### Open Question 2
- Question: How does the QGDF mechanism quantitatively perform under adverse environmental conditions such as severe weather or lighting variations?
- Basis in paper: [explicit] The conclusion proposes that the mechanism "should be evaluated to quantitatively determine the benefits it can bring under adverse conditions, such as lightning and weather variations."
- Why unresolved: The reported results are derived from the standard nuScenes validation set, which may not contain enough extreme weather samples to fully stress-test the modality gating and fusion robustness.
- What evidence would resolve it: Benchmarks on adverse-condition subsets (e.g., nuScenes-Canada or WOD fog/rain splits) analyzing the stability of the learned modality gating weights ($\gamma$).

### Open Question 3
- Question: To what extent can the robustness of multimodal agent queries reduce the requirement for high-quality input sensor data?
- Basis in paper: [explicit] The authors suggest future work should explore "reducing the computational cost... by reducing the need for high-quality input sensor data," hypothesizing that the interface is robust against noise.
- Why unresolved: The model was trained and evaluated using standard sensor configurations (32-beam LiDAR, multi-view RGB), leaving the performance impact of lower-resolution or sparser inputs untested.
- What evidence would resolve it: Experiments varying LiDAR sweep density (e.g., dropping beams) and image resolution to measure the trade-off between input quality and EPA/mAP.

### Open Question 4
- Question: Can compute-aware fusion strategies close the latency gap with unimodal models without sacrificing detection quality?
- Basis in paper: [inferred] The authors note that Li-ViP3D++ is still slower than the unimodal ViP3D (139.82 ms vs. 117.27 ms) and explicitly motivate "future work on compute-aware fusion (e.g., conditional modality usage...)".
- Why unresolved: The current implementation fuses all modalities for every query at every step; it is unknown if dynamic, query-specific modality skipping can recover the lost latency.
- What evidence would resolve it: Implementation of an early-exit or dynamic gating mechanism that achieves latency parity with ViP3D while retaining the gains in False Positive reduction.

## Limitations

- No ablation studies are provided to isolate the contribution of masking, gating, or learned offsets to the performance gains.
- The slight degradation in trajectory prediction accuracy (minADE/minFDE) is not fully explained, despite being attributed to more conservative detections.
- Key architectural and training hyperparameters (query count, embedding dimension, learning rate, optimizer, etc.) are not specified, hindering faithful reproduction.

## Confidence

- **High confidence**: Query-gated deformable fusion improves detection quality and reduces false positives (supported by quantitative EPA/mAP/FP ratio results).
- **Medium confidence**: Fully differentiable fusion via query-space sampling is the primary driver of gains; however, without ablations, the relative importance of gating, masking, and offsets remains uncertain.
- **Low confidence**: The slight drop in trajectory accuracy is solely due to more conservative, better-calibrated detections; alternative explanations (e.g., loss of fine-grained geometric detail) are possible.

## Next Checks

1. **Ablate gating**: Fix γt = [0.5, 0.5] and compare EPA/FP ratio to full model to isolate gating contribution.
2. **Vary offset bounds**: Modify clip range in Equation 8 (e.g., ±0.2 vs. ±0.8) and measure impact on minADE and FP ratio.
3. **Modality dropout**: Run inference with LiDAR disabled (QL = 0) to assess LiDAR reliance and robustness.