---
ver: rpa2
title: 'MoFE: Mixture of Frozen Experts Architecture'
arxiv_id: '2503.06491'
source_url: https://arxiv.org/abs/2503.06491
tags:
- expert
- mofe
- performance
- training
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MoFE (Mixture of Frozen Experts), a new architecture
  that combines parameter-efficient fine-tuning with the mixture of experts approach
  to improve training efficiency and model scalability. MoFE works by freezing the
  Feed Forward Network (FFN) layers in the MoE framework, significantly reducing trainable
  parameters while maintaining knowledge transfer from expert models.
---

# MoFE: Mixture of Frozen Experts Architecture

## Quick Facts
- arXiv ID: 2503.06491
- Source URL: https://arxiv.org/abs/2503.06491
- Authors: Jean Seo; Jaeyoon Kim; Hyopil Shin
- Reference count: 11
- Key outcome: MoFE achieves competitive performance with 0.34B trainable parameters vs. 14-26 hours full fine-tuning, outperforming LoRA/QLoRA/DoRA

## Executive Summary
MoFE (Mixture of Frozen Experts) introduces a novel architecture that combines parameter-efficient fine-tuning with mixture of experts by freezing Feed Forward Network (FFN) layers in MoE frameworks. This approach significantly reduces trainable parameters while maintaining knowledge transfer from expert models, achieving competitive performance on MMLU and MedMCQA benchmarks with substantially faster training times. The architecture demonstrates effective domain expertise transfer through compositional expert selection and reveals that post-pretraining before instruction-tuning actually harms performance when FFN layers are frozen.

## Method Summary
MoFE builds on the Mixtral MoE architecture by freezing all FFN blocks while keeping only embeddings, self-attention layers, and router trainable. Expert models are created by fine-tuning TinyLlama on domain-specific data, then merged using mergekit with top-2 expert gating. The router computes dot products between hidden states and expert-specific vectors to select experts per token. Training uses batch_size=4, lr=3e-5, gradient_accumulation=512, with frozen FFN blocks maintaining ~0.34B trainable parameters regardless of expert count.

## Key Results
- MoFE trains in 6 hours vs. 14-26 hours for full fine-tuning with 0.34B trainable parameters
- Outperforms LoRA, QLoRA, and DoRA on MMLU and MedMCQA benchmarks
- Domain expertise transfer shows improved performance with more domain-specific experts (0.3488 → 0.3636 on MedMCQA)
- Post-pretraining before instruction-tuning reduces performance by 37% (0.2589 vs 0.3529 on MedMCQA)

## Why This Works (Mechanism)

### Mechanism 1: Frozen FFN Reduces Trainable Parameters While Preserving Expert Knowledge
Freezing FFN blocks maintains domain expertise while reducing trainable parameters to ~0.34B regardless of expert count. This works because FFN layers encode domain-specific knowledge that remains useful without gradient updates, while only router and base model components (embeddings, self-attention) are updated during training.

### Mechanism 2: Domain Expertise Transfer via Compositional Expert Selection
The router selects top-k experts per token using dot products between hidden states and expert-specific vectors, enabling domain-specific frozen FFN weights to become accessible through learned routing. Performance improves as domain expert count increases (0.3488 → 0.3636 on MedMCQA with more medical experts).

### Mechanism 3: Post-Pretraining Harms Performance When FFN Is Frozen
Post-pretraining degrades performance because frozen FFN cannot integrate new knowledge, creating layer misalignment between updated attention/embeddings and frozen FFN blocks. Direct instruction-tuning (0.3529) outperforms post-pretraining → instruction-tuning (0.2589) by 37% on MedMCQA.

## Foundational Learning

- **Concept**: Mixture of Experts (MoE) Routing
  - Why needed here: MoFE builds on MoE architecture; understanding sparse expert selection is prerequisite
  - Quick check question: Can you explain how top-k gating routes tokens to a subset of experts per forward pass?

- **Concept**: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: MoFE is positioned as a PEFT alternative; comparing to LoRA/QLoRA/DoRA requires understanding low-rank adaptation
  - Quick check question: What is the key difference between adapter-based PEFT and freezing-based approaches?

- **Concept**: Transformer FFN Role
  - Why needed here: MoFE specifically freezes FFN layers; understanding their function (isotropy, knowledge storage) explains why freezing is viable
  - Quick check question: What role do FFN layers play in transformer architectures beyond non-linearity?

## Architecture Onboarding

- **Component map**: Base Model (embeddings + 22 self-attention layers, trainable) -> Expert Models (frozen FFN blocks) -> Router/Gate (linear layer computing expert scores via dot products with hidden states; selects top-2 experts per token)

- **Critical path**: 1) Create expert models by instruction-tuning TinyLlama on domain data OR use pretrained directly 2) Merge experts into MoE structure via mergekit using Mixtral architecture 3) Freeze all FFN blocks, set requires_grad=True only for attention, embeddings, router 4) Instruction-tune on target task with batch_size=4, lr=3e-5, gradient_accumulation=512

- **Design tradeoffs**: More experts → more total parameters but same trainable count → same training time; Domain-specific experts → better domain performance but requires expert model creation; General base model recommended for multi-domain tasks

- **Failure signatures**: Post-pretraining before instruction-tuning → ~37% performance drop (catastrophic); All same-domain experts without diversity → no multi-domain capability; Expecting linear scaling with expert count → non-monotonic relationship observed

- **First 3 experiments**: 1) Baseline replication: Build small MoFE (2 experts) with TinyLlama, instruction-tune on single-domain dataset, compare training time vs. LoRA 2) Expert composition ablation: Vary medical vs. general expert ratio, measure domain-specific task performance to verify knowledge transfer 3) Training strategy test: Compare direct instruction-tuning vs. post-pretraining → instruction-tuning on held-out dataset (e.g., PubMedQA) to confirm negative post-pretraining effect

## Open Questions the Paper Calls Out

### Open Question 1
Does MoFE's efficiency-performance trade-off scale consistently to larger base models (e.g., 7B, 70B parameters)? The paper acknowledges all experiments use TinyLlama (1.1B) and results may not generalize to larger models or all domains.

### Open Question 2
What mechanisms explain why performance does not correlate consistently with the number of frozen FFN blocks? Appendix A shows non-linear patterns but no theoretical or empirical explanation is provided.

### Open Question 3
Can alternative pre-training strategies be designed that integrate new knowledge without causing the misalignment observed with post-pretraining? The paper hypothesizes misalignment issues but does not explore modified pre-training approaches.

### Open Question 4
How sensitive is MoFE's expert selection and task performance to different routing mechanisms beyond the positive/negative prompt-based gating used? The routing mechanism's design space remains unexplored.

## Limitations
- Limited to TinyLlama (1.1B) model family; scalability to larger models unknown
- No implementation details provided for critical components (mergekit configuration, router initialization)
- Evaluation focused on MMLU and MedMCQA datasets only

## Confidence

**High confidence**: Core architectural contribution (freezing FFN layers to reduce trainable parameters) is clearly described with measurable training time reductions.

**Medium confidence**: Negative finding about post-pretraining harming performance is compelling but based on limited experimental evidence.

**Low confidence**: Assertion that frozen FFN layers preserve expert knowledge without adaptation lacks theoretical justification; router's ability to effectively route tokens to frozen experts across diverse domains is assumed rather than empirically validated.

## Next Checks

1. **Reproduce the post-pretraining ablation**: Implement both training strategies on held-out domain data to verify the 37% performance degradation finding across multiple datasets and model sizes.

2. **Router analysis and ablation**: Monitor expert selection distributions during training to verify balanced routing; perform ablations varying top-k selection and routing strategies.

3. **Cross-domain generalization study**: Test MoFE performance when experts from one domain are applied to tasks in different domains to validate domain-specific knowledge in frozen FFN layers.