---
ver: rpa2
title: 'FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable
  Automatic Fact-Checking'
arxiv_id: '2504.05229'
source_url: https://arxiv.org/abs/2504.05229
tags:
- explanation
- error
- claim
- evaluation
- actionability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FinGrAct, a fine-grained evaluation framework
  for assessing actionability in explainable Automatic Fact-Checking (AFC) explanations.
  The framework systematically measures actionability by detecting factual errors,
  generating corrections, and evaluating supporting sources, with optional web content
  retrieval.
---

# FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking

## Quick Facts
- arXiv ID: 2504.05229
- Source URL: https://arxiv.org/abs/2504.05229
- Reference count: 40
- Outperforms state-of-the-art evaluators with Pearson correlation of 0.46 and Kendall's tau of 0.409 with human judgments, while exhibiting the lowest ego-centric bias

## Executive Summary
FinGrAct introduces a fine-grained evaluation framework for assessing actionability in explainable Automatic Fact-Checking (AFC) explanations. The framework systematically measures actionability by decomposing claims into atomic subclaims, detecting factual errors, generating corrections, and evaluating supporting sources with optional web content retrieval. By breaking evaluation into discrete subtasks and incorporating real-time web content, FinGrAct achieves superior alignment with human judgments while demonstrating the lowest ego-centric bias among compared models. The method establishes a new standard for assessing the actionability of AFC explanations through structured LLM prompting and external source grounding.

## Method Summary
FinGrAct employs a divide-and-conquer approach to evaluate actionability in AFC explanations through structured LLM prompting. The framework first segments claims into atomic subclaims, then detects factual errors and generates corrections for each segment. It evaluates explanations using boolean judgments for error detection, correction quality, and source support, aggregating results into categorical scores. An optional URL Content Retriever (UCR) scrapes web pages, summarizes content using MiniLM-L6-v2, and injects condensed information into evaluation prompts to improve source assessment accuracy. The final Likert scores are computed by mapping categorical ratios to discrete categories and scaling by 5/6 multipliers.

## Key Results
- Achieves Pearson correlation of 0.46 and Kendall's tau of 0.409 with human judgments, outperforming all state-of-the-art evaluators
- Demonstrates the lowest ego-centric bias with only 8.4% of samples overestimated versus 48.7% for G-Eval
- Incorporating retrieved web content improves correlations across all models, with FinGrAct's Pearson correlation increasing by 6% and Kendall's tau by 1%

## Why This Works (Mechanism)

### Mechanism 1: Divide-and-Conquer Task Decomposition
Breaking evaluation into discrete subtasks improves alignment with human judgment compared to holistic scoring. Claims are decomposed into atomic subclaims via LLM prompting, with each subclaim independently evaluated for factual errors, proposed corrections, and source support. Boolean judgments are aggregated into categorical scores. The assumption is that granular, explicit evaluation criteria reduce subjectivity and ambiguity in LLM-as-judge outputs. This granular methodology likely contributes to achieving the highest correlation with human annotations, as demonstrated by fine-grained criteria improving alignment in related summarization tasks.

### Mechanism 2: External Content Retrieval for Source Grounding
Injecting real-time web content into prompts improves source evaluation accuracy beyond internal LLM knowledge. The URL Content Retriever scrapes HTML, strips tags, summarizes via MiniLM-L6-v2, and injects condensed content into the evaluation prompt. The LLM then judges link existence, relevance, and support. The assumption is that LLMs assess source quality more accurately when given actual page content rather than relying on parametric knowledge which may be outdated or absent. Incorporating retrieved web content further improved correlations, with FinGrAct's Pearson correlation improving by 6% and Kendall's tau by 1% across all evaluators.

### Mechanism 3: Structured Boolean-to-Categorical Scoring Reduces Ego-Centric Bias
Constraining LLM outputs to structured boolean decisions, then mapping to categorical scores, reduces self-preference bias. Each error, correction, and link is judged as Yes/No, with ratios categorized as None (0%), Partial (>0% and <100%), or Full (100%), then scaled to Likert scores. The assumption is that explicit rubrics and constrained outputs limit LLM's opportunity to inflate scores for its own generations. FinGrAct showed the least variance and the lowest ego-centric bias, with only 8.4% of samples overestimated versus 48.7% for G-Eval, demonstrating that structured rubrics are an effective mitigation for documented ego-centric bias in LLM-as-judge systems.

## Foundational Learning

- Concept: Atomic Claim Decomposition
  - Why needed here: The framework's first stage requires decomposing complex claims into verifiable subclaims before errors can be identified and corrected.
  - Quick check question: Given "The Moon is made of cheese and orbits Mars," can you identify two atomic subclaims and their factual status?

- Concept: Ego-Centric Bias in LLM Evaluation
  - Why needed here: Interpreting evaluation results requires understanding that LLMs systematically over-score outputs from their own model family.
  - Quick check question: In the paper, what percentage of samples did G-Eval overestimate by ≥2 points vs. FinGrAct?

- Concept: Three-Tier Categorical Scoring (None/Partial/Full)
  - Why needed here: The scoring algorithm maps continuous boolean ratios to discrete categories before Likert scaling.
  - Quick check question: If 2 of 3 errors are detected and corrected, what categorical score (0, 1, or 2) is assigned for error detection?

## Architecture Onboarding

- Component map:
  - Error Segmentation & Correction: GPT-4 prompted with claim + evidence → JSON list of {sentence, reason, correction}
  - Explanation Evaluator: Takes error list + explanation → boolean judgments per error/correction
  - UCR (optional): URLs → requests-based HTML scrape → tag stripping → MiniLM-L6-v2 summarization → prompt injection
  - Source Evaluator: Link content + corrections → {exist, relevant, supporting} booleans
  - Scoring Aggregator: Algorithm 1 → categorical (0/1/2) per dimension → sum → scale by 5/6 → Likert 0–5

- Critical path:
  1. Claim + Evidence → Error Segmentation (blocking; must produce JSON)
  2. Error list + Explanation → Explanation Evaluation
  3. (If UCR enabled) URLs → Source Evaluation
  4. All booleans → Scoring Aggregator → Final Likert score

- Design tradeoffs:
  - With UCR: Higher correlation but latency from web scraping; fails on image/JS-heavy pages
  - Without UCR: Faster inference but relies on stale/absent LLM knowledge for source credibility
  - Temperature=0: Reproducibility; higher temp introduces variance (G-Eval uses temp=1, n=20)

- Failure signatures:
  - Empty or malformed JSON from Error Segmentation → downstream evaluation cannot proceed
  - UCR returns empty text → Source Evaluator outputs "No" for support regardless of actual content
  - High run-to-run variance → check temperature; FinGrAct uses temp=0, baselines use temp=1

- First 3 experiments:
  1. Reproduce baseline correlations: Run FinGrAct (without UCR) on the 203-sample dataset; compute Pearson and Kendall vs. human annotations; verify ~0.46 and ~0.409.
  2. Ablate UCR component: Compare FinGrAct with-UCR vs. without-UCR on a held-out subset with known-textual URLs; quantify delta in correlation.
  3. Ego-centric bias probe: Have each evaluator score explanations generated by its underlying LLM (GPT-4 for FinGrAct/G-Eval; Mistral for Prometheus); measure overestimation rate (score ≥2 above human).

## Open Questions the Paper Calls Out

### Open Question 1
Does a multimodal URL Content Retriever (UCR) significantly improve evaluation accuracy for explanations relying on image-based or JavaScript-rendered web sources? The current text-scraping implementation returns no content for image-heavy pages, causing the evaluator to incorrectly assign low relevance scores. The authors explicitly state that the current UCR is text-only and fails on non-textual content, leading to misalignment with human annotators who can interpret images.

### Open Question 2
Can cost-effective fine-tuning strategies enhance the correlation of actionability evaluators with human judgments beyond the zero-shot performance observed in this study? The authors note that fine-tuning was not explored due to high computational costs, leaving the potential performance gains of a trained model unknown. It is unclear if the structural advantages of FinGrAct are maximized in a zero-shot setting or if they would scale further with model adaptation.

### Open Question 3
To what extent does cross-model bias (preference for specific LLMs' outputs) affect the reliability of actionability evaluations? The authors identify the analysis of cross-model biases, where LLMs may favor explanations generated by certain other models, as a specific direction for future work. The current study focused strictly on ego-centric bias and did not map preferential relationships between different models.

## Limitations

- The framework's reliance on web scraping through UCR introduces significant failure modes - the evaluation cannot proceed when URLs contain only images or JavaScript-rendered content, as no extractable text is available for source evaluation.
- The 203-sample dataset, while showing strong correlations, may not generalize to different AFC systems or explanation formats.
- The study uses GPT-4 and Mistral-based evaluators, so the ego-centric bias results may not extend to other LLM families.

## Confidence

- **High confidence**: The superiority of fine-grained task decomposition over holistic scoring is well-supported by both abstract claims and analysis section results showing highest correlation with human judgments.
- **Medium confidence**: The external content retrieval improvement is demonstrated with specific correlation improvements (6% Pearson, 1% Kendall), but the underlying assumption about LLM parametric knowledge limitations has weak direct corpus support.
- **Medium confidence**: The ego-centric bias reduction is strongly evidenced by specific percentages (8.4% overestimation vs 48.7% for G-Eval), though the broader literature connection is somewhat indirect.

## Next Checks

1. Test FinGrAct's generalizability by evaluating explanations from multiple AFC systems beyond the original 203-sample dataset, measuring correlation stability.
2. Conduct ablation studies removing individual subtasks (error segmentation, correction generation, source evaluation) to quantify each component's contribution to overall performance.
3. Evaluate edge cases where atomic subclaims become semantically incoherent due to excessive granularity, measuring impact on error detection precision.