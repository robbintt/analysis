---
ver: rpa2
title: 'FGGM: Fisher-Guided Gradient Masking for Continual Learning'
arxiv_id: '2601.18261'
source_url: https://arxiv.org/abs/2601.18261
tags:
- fggm
- learning
- migu
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  for large language models (LLMs). The authors propose Fisher-Guided Gradient Masking
  (FGGM), a framework that uses diagonal Fisher Information to strategically select
  parameters for updates.
---

# FGGM: Fisher-Guided Gradient Masking for Continual Learning

## Quick Facts
- **arXiv ID:** 2601.18261
- **Source URL:** https://arxiv.org/abs/2601.18261
- **Reference count:** 0
- **Key outcome:** 9.6% relative improvement in retaining general capabilities over SFT and 4.4% over MIGU on TRACE benchmark tasks.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for large language models (LLMs) through Fisher-Guided Gradient Masking (FGGM). The method uses diagonal Fisher Information to identify critical parameters and generates binary masks that selectively preserve these parameters during sequential fine-tuning. FGGM dynamically adapts to non-stationary parameter importance distributions across layers without requiring historical data, achieving strong performance on the TRACE benchmark and code generation tasks.

## Method Summary
FGGM operates in two phases: first, it computes diagonal Fisher Information Matrix using gradients of log-likelihood over new task data to quantify parameter importance. Second, it generates binary masks via adaptive quantile-based thresholding (α=0.7) that preserves the top 30% of parameters by Fisher score. The method applies input-dimension aggregation for weight matrices, summing Fisher values across input connections per output neuron to protect functionally cohesive parameter groups. During training, gradients are element-wise multiplied by these masks before parameter updates, restricting learning to high-Fisher parameters while freezing critical ones.

## Key Results
- 9.6% relative improvement in General capabilities retention over supervised fine-tuning (SFT)
- 4.4% improvement over MIGU on TRACE tasks
- Input-dimension aggregation shows significant advantage over output-dimension aggregation (General: 55.75 vs 53.16, TRACE-OP: 46.00 vs 43.15)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diagonal Fisher Information Matrix provides principled parameter importance estimation for selective masking.
- **Mechanism:** High Fisher scores indicate parameters whose perturbation significantly degrades task likelihood, enabling targeted preservation of critical weights.
- **Core assumption:** Parameter importance correlates with Fisher Information magnitude.
- **Evidence:** Abstract states FGGM "preserves critical parameters to balance stability and plasticity"; Section 2 explains Fisher scores quantify importance.
- **Break condition:** If Fisher Information doesn't correlate with true importance in non-convex landscapes.

### Mechanism 2
- **Claim:** Adaptive quantile-based thresholding handles non-stationary parameter importance across layers.
- **Mechanism:** Uses (1-α) quantile of empirical Fisher values per task to generate layer-specific masks.
- **Core assumption:** Top-(1-α) fraction by Fisher score contains most task-critical parameters.
- **Evidence:** Section 3.2 describes adaptive thresholding; Table 2 shows α=0.7 achieves best balance.
- **Break condition:** If critical parameters are distributed differently than assumed.

### Mechanism 3
- **Claim:** Input-dimension aggregation preserves functional unit coherence and improves noise resistance.
- **Mechanism:** Sums Fisher values across input connections per output neuron, treating each output neuron as functional unit.
- **Core assumption:** Output neurons represent meaningful computational units.
- **Evidence:** Section 3.3 explains aggregation protects cohesive parameter groups; Table 1 shows w/o IA degrades performance.
- **Break condition:** If output neurons don't correspond to semantically meaningful functions.

## Foundational Learning

- **Concept:** Fisher Information Matrix
  - Why needed: Core mathematical tool for parameter importance estimation
  - Quick check: High Fisher score parameter is 10x more important for current task likelihood than low Fisher score parameter.

- **Concept:** Catastrophic Forgetting in Sequential Fine-Tuning
  - Why needed: Problem FGGM solves by preserving critical parameters
  - Quick check: Standard SGD overwrites parameters critical for old tasks during new task training.

- **Concept:** Stability-Plasticity Trade-off
  - Why needed: FGGM's design goal balanced by masking rate α
  - Quick check: If observing good new-task learning but poor old-task retention, increase α.

## Architecture Onboarding

- **Component map:** Task data -> FIM Initialization -> Binary Mask Generation -> Gradient Masking -> Parameter Update

- **Critical path:**
  1. Receive new task data → compute per-sample gradients of log-likelihood
  2. Aggregate gradients into empirical diagonal FIM
  3. Apply input-dimension aggregation for weight matrices
  4. Compute adaptive threshold as (1-α) quantile
  5. Generate binary mask
  6. During training, mask gradients before parameter update

- **Design tradeoffs:**
  - Hard vs. soft masking: Binary masking provides strict stability but may over-constrain plasticity
  - Diagonal vs. full FIM: Diagonal approximation enables LLM scalability but ignores parameter correlations
  - Online vs. offline FIM: Offline reduces overhead but may introduce noise

- **Failure signatures:**
  - Excessive forgetting: α too low or insufficient FIM data samples
  - Poor new-task learning: α too high or incorrect Fisher score identification
  - Architecture mismatch: Input-dimension aggregation requires adaptation for non-standard layers

- **First 3 experiments:**
  1. Reproduce TRACE benchmark results with α=0.7 on Qwen2-1.5B
  2. Ablation: Compare w/o IA vs. Apply OA vs. full FGGM on single TRACE task
  3. Hyperparameter sweep: Test α ∈ {0.5, 0.6, 0.7, 0.8, 0.9} on code generation task

## Open Questions the Paper Calls Out

- **Question:** Can FGGM be adapted for multi-modal fine-tuning scenarios?
  - Basis: Conclusion mentions future work on multi-modal applicability
  - Why unresolved: Current validation limited to text-based LLMs
  - Evidence needed: Empirical results on vision-language continual learning benchmarks

- **Question:** Does diagonal Fisher approximation degrade performance vs. full approximations?
  - Basis: Section 2 notes diagonal approximation assumes parameter independence
  - Why unresolved: Computational savings vs. correlation information loss trade-off unquantified
  - Evidence needed: Comparative ablation with Kronecker-factored or block-diagonal approximations

- **Question:** Would soft-masking outperform binary hard-masking for high gradient conflict tasks?
  - Basis: Discussion notes hard-masking may overly restrict plasticity
  - Why unresolved: Theoretical benefits of soft-masking not experimentally validated
  - Evidence needed: Comparison of binary masks vs. continuous suppression weights on dissimilar tasks

## Limitations

- Reliance on diagonal Fisher Information Matrix assumes parameter independence, which may not hold for strongly coupled parameters
- Effectiveness of input-dimension aggregation lacks broader theoretical justification beyond empirical ablation
- α=0.7 hyperparameter selection lacks systematic analysis across different dataset sizes and task similarities
- Evaluation limited to Qwen2-1.5B and smaller models, leaving scalability questions for larger LLMs unanswered

## Confidence

- **High Confidence:** Core Fisher Information mechanism and general framework design
- **Medium Confidence:** Specific implementation details (input-dimension aggregation, α=0.7 threshold)
- **Low Confidence:** Claims about superiority of input-dimension aggregation and cross-architecture robustness

## Next Checks

1. Test FGGM on Qwen2-7B and larger models to assess scalability and optimal α threshold stability
2. Implement and compare output-dimension aggregation and attention-specific aggregation on TRACE tasks
3. Evaluate FGGM's performance across tasks with varying degrees of semantic overlap to understand task similarity effects on optimal α selection