---
ver: rpa2
title: 'UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency
  Prompting'
arxiv_id: '2506.05589'
source_url: https://arxiv.org/abs/2506.05589
tags:
- sentences
- patient
- essential
- question
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes the UTSA-NLP system for the ArchEHR-QA 2025
  shared task on answering clinical questions using electronic health records. The
  approach uses a two-step pipeline: first, identifying relevant sentences in the
  EHR using few-shot prompting with self-consistency and thresholding; second, generating
  concise, citation-supported responses using zero-shot prompting.'
---

# UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting

## Quick Facts
- arXiv ID: 2506.05589
- Source URL: https://arxiv.org/abs/2506.05589
- Reference count: 11
- Key outcome: UTSA-NLP system achieved 40.45 overall score using 8B model with self-consistency prompting for sentence classification

## Executive Summary
This paper describes the UTSA-NLP system for the ArchEHR-QA 2025 shared task on answering clinical questions using electronic health records. The approach uses a two-step pipeline: first, identifying relevant sentences in the EHR using few-shot prompting with self-consistency and thresholding; second, generating concise, citation-supported responses using zero-shot prompting. The sentence classification step achieved F1 scores of 52.97 (lenient) and 45.06 (strict), with the smaller 8B model outperforming the larger 70B model for this task. Error analysis revealed that class imbalance and ambiguity in the supplementary label were primary sources of misclassification.

## Method Summary
The system employs a two-step pipeline for clinical question answering. First, it classifies EHR sentences as essential, supplementary, or not-relevant using few-shot prompting with the LLaMA 3.1 8B model. Each sentence is evaluated through 20 self-consistent samples at temperature 1.0, with class thresholds applied (essential if ≥2/20 votes, supplementary if ≥1/20). Second, relevant sentences are used to generate ≤75-word answers with citations using the LLaMA 3.1 70B model. If the combined sentences exceed 75 words, the system summarizes them while preserving citation information in pipe-delimited format.

## Key Results
- Sentence classification achieved F1 scores of 52.97 (lenient) and 45.06 (strict) on development set
- 8B model outperformed 70B model for classification despite fewer parameters
- Overall system score of 40.45 on development set
- Error analysis revealed class imbalance and supplementary label ambiguity as primary failure modes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-consistency sampling with low thresholds mitigates the model's bias toward predicting "not-relevant" in imbalanced classification settings.
- **Mechanism:** By sampling 20 independent predictions at temperature 1.0 and applying per-class thresholds (essential if ≥2/20 votes; supplementary if ≥1/20), the system counteracts the pretrained model's tendency to default to the majority class. This effectively recalibrates decision boundaries without fine-tuning.
- **Core assumption:** The model's raw probability outputs contain useful signal that can be extracted through repeated sampling, even if single predictions are unreliable.
- **Evidence anchors:**
  - [abstract] "We use few-shot prompting, self-consistency, and thresholding to improve the sentence classification step"
  - [section 2.2] "Removing thresholding alone reduced overall performance to 26.34, while removing both thresholding and self-consistency resulted in a score of 29.31"
  - [corpus] No direct corpus evidence on self-consistency thresholds in medical QA; related ArchEHR-QA papers do not report comparable ablations.
- **Break condition:** If class distributions shift significantly (e.g., >50% essential sentences), the low thresholds may cause excessive false positives, degrading precision.

### Mechanism 2
- **Claim:** A smaller 8B model can outperform a larger 70B model for sentence-level relevance classification when paired with self-consistency sampling.
- **Mechanism:** The 8B model produces more consistent label distributions across samples, which thresholding can exploit. The 70B quantized model may exhibit greater variance or calibration issues that reduce aggregation benefits.
- **Core assumption:** The 70B model's underperformance is attributable to quantization effects or instruction-tuning differences, not fundamental capacity limitations.
- **Evidence anchors:**
  - [abstract] "a smaller 8B model performs better than a larger 70B model for identifying relevant information"
  - [section 3.3] "Despite having fewer parameters, the 8B model outperformed the 70B quantized model across almost every metric, especially in classification"
  - [corpus] Weak external validation—related ArchEHR-QA submissions do not systematically compare 8B vs. 70B configurations.
- **Break condition:** If full-precision 70B inference becomes feasible, or if task complexity increases (e.g., multi-document reasoning), the capacity advantage may reassert.

### Mechanism 3
- **Claim:** Sentence selection quality is the primary bottleneck for end-to-end QA performance; generation quality is comparatively easier to achieve.
- **Mechanism:** The two-step factorization isolates the harder subproblem (identifying relevant evidence) from the easier one (summarizing known-relevant content). Errors in selection propagate to generation; better inputs yield better outputs.
- **Core assumption:** The 75-word constraint does not introduce additional failure modes beyond what sentence selection already determines.
- **Evidence anchors:**
  - [abstract] "accurate sentence selection is critical for generating high-quality responses"
  - [section 3.2] Ground-truth sentence selection achieves 74.58 overall vs. 42.37 with predicted selections, showing large headroom.
  - [corpus] ArgHiTZ and Neural ArchEHR-QA submissions similarly adopt two-step architectures, consistent with this factorization assumption.
- **Break condition:** If generation requires synthesis beyond selected sentences (e.g., inference across facts), errors may arise independently of selection quality.

## Foundational Learning

- **Concept: Self-consistency decoding**
  - **Why needed here:** The core mechanism relies on aggregating multiple stochastic samples to improve classification reliability.
  - **Quick check question:** Can you explain why temperature >0 is necessary for self-consistency to provide benefit over greedy decoding?

- **Concept: Threshold-based decision calibration**
  - **Why needed here:** Thresholds convert vote counts into discrete labels, compensating for model bias toward majority classes.
  - **Quick check question:** Given a 3-class problem with 70% negative examples, would raising or lowering the positive-class threshold increase recall?

- **Concept: Class imbalance in classification**
  - **Why needed here:** The paper identifies "supplementary" as the hardest class due to its scarcity relative to "not-relevant."
  - **Quick check question:** In a confusion matrix with heavy class imbalance, which metric (precision, recall, F1) is most sensitive to false negatives on the minority class?

## Architecture Onboarding

- **Component map:** Clinical question + EHR sentences -> LLaMA 3.1 8B classification (20 samples + thresholding) -> essential/supplementary/not-relevant labels -> sentence filtering -> LLaMA 3.1 70B generation (≤75 words with citations) -> final answer

- **Critical path:**
  1. Replicate sentence classification ablation (8B with vs. without self-consistency/thresholding) on dev set
  2. Verify threshold values (2/20 for essential, 1/20 for supplementary) using the provided prompt template
  3. Compare strict vs. lenient evaluation modes to confirm the ~10-point overall score gap

- **Design tradeoffs:**
  - **Lenient vs. strict:** Lenient (essential+supplementary) yields higher recall but more false positives; strict (essential-only) improves precision but risks missing useful context.
  - **8B vs. 70B for classification:** 8B + self-consistency is cheaper and more reliable; 70B alone is faster but less accurate in this configuration.
  - **Word limit enforcement:** Pre-generation filtering preserves citations; post-generation summarization risks citation loss.

- **Failure signatures:**
  - **Lexical overlap bias:** Model marks sentences as essential based on keyword matching rather than semantic relevance to the question intent (e.g., procedure acronym present but rationale absent).
  - **Supplementary confusion:** High false negative rate (18/51 correct) indicates difficulty distinguishing useful-but-non-essential from not-relevant.
  - **Context blindness:** Independent sentence classification misses discourse-level cues; sentences referencing prior events are misclassified.

- **First 3 experiments:**
  1. **Threshold sweep:** Vary essential threshold from 1–5/20 and supplementary from 1–3/20; plot F1 vs. overall score to find optimal operating point.
  2. **Context window ablation:** Include 1–2 neighboring sentences in the classification prompt; measure impact on supplementary-class recall.
  3. **Full-precision 70B test:** If resources permit, run classification with unquantized 70B to isolate whether quantization explains the 8B advantage.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating document-level context or sequential modeling significantly improve sentence classification accuracy compared to independent classification?
  - Basis in paper: [explicit] "Future work should explore incorporating sentence context and document structure to improve classification... Sentences referring to previous or subsequent medical events could be misclassified due to this lack of discourse awareness."
  - Why unresolved: The current methodology treats each sentence independently, potentially missing critical dependencies in clinical narratives.
  - What evidence would resolve it: A comparative study on the ArchEHR-QA dataset between the current independent classifier and a context-aware model (e.g., using sliding windows or transformer attention over the full note).

- **Open Question 2:** Can adaptive or data-driven calibration methods outperform the manually tuned self-consistency thresholds used for identifying essential sentences?
  - Basis in paper: [explicit] "Future work could explore adaptive or data-driven methods to calibrate sentence selection confidence... threshold values were tuned manually and may not generalize well across datasets."
  - Why unresolved: The current thresholds (e.g., 2 out of 20 samples) are heuristics that may be brittle when data distributions shift.
  - What evidence would resolve it: Implementation of a learned confidence calibration layer that dynamically adjusts thresholds based on input features, evaluated against the fixed-threshold baseline.

- **Open Question 3:** Does the 8B model consistently outperform the 70B model on sentence classification when quantization artifacts and prompt sensitivity are removed?
  - Basis in paper: [inferred] "while the 8B model outperformed the 70B model... this may reflect the effects of quantization, prompt format sensitivity, or differences in instruction tuning. These variables were not systematically controlled."
  - Why unresolved: It is unclear if the smaller model's superior performance is genuine or a side effect of the 70B model's quantization (w416b) or prompt compatibility.
  - What evidence would resolve it: An ablation study running both models at full precision (or identical quantization) with optimized prompts for each to isolate the variable of model size.

## Limitations

- **Class imbalance sensitivity:** The system's performance heavily depends on the class distribution in the test set, which may differ from the dev set. The self-consistency thresholds are tuned for the dev set's distribution and may not generalize.
- **Context independence:** Sentence-level classification ignores document-level context, leading to misclassification when sentences rely on surrounding text for full meaning. The error analysis shows this limitation directly affects supplementary-class identification.
- **Quantization effects:** The 70B model uses w416b quantization, but the specific quantization method and its impact on generation quality are not detailed, introducing uncertainty about whether the reported performance is optimal or degraded.

## Confidence

- **High confidence:** The two-step pipeline architecture (classification → generation) is valid and improves over end-to-end approaches for this task. The ablation showing 26.34→42.37→74.58 score progression is clear and reproducible.
- **Medium confidence:** The 8B model outperforming 70B for classification is reported but may be specific to the quantized configuration used. The mechanism (self-consistency sampling benefits) is plausible but not extensively validated across different model sizes.
- **Medium confidence:** The identification of lexical overlap bias and supplementary-class confusion as primary error sources is supported by error analysis but not quantitatively measured against alternative explanations.

## Next Checks

1. **Threshold sensitivity analysis:** Systematically vary the essential (1-5/20) and supplementary (1-3/20) thresholds on the dev set to identify optimal operating points and quantify performance sensitivity to threshold changes.
2. **Context window ablation:** Modify the classification prompt to include 1-2 neighboring sentences for each target sentence, then measure impact on supplementary-class recall and overall F1 to quantify the context blindness limitation.
3. **Full-precision 70B comparison:** If computational resources permit, run the sentence classification task with unquantized 70B inference to isolate whether quantization artifacts explain the 8B model's superior performance.