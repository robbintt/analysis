---
ver: rpa2
title: Automated Bug Triaging using Instruction-Tuned Large Language Models
arxiv_id: '2508.21156'
source_url: https://arxiv.org/abs/2508.21156
tags:
- mozilla
- top-1
- eclipsejdt
- triaging
- developer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses automated bug triaging by proposing an instruction-tuned
  LLM with LoRA adapters and candidate-constrained decoding. The method fine-tunes
  DeepSeek-R1-Distill-Llama-8B on conversational JSONL data from EclipseJDT and Mozilla
  projects, ensuring only valid developers are assigned.
---

# Automated Bug Triaging using Instruction-Tuned Large Language Models

## Quick Facts
- **arXiv ID:** 2508.21156
- **Source URL:** https://arxiv.org/abs/2508.21156
- **Reference count:** 38
- **Primary result:** LLM-based bug triaging achieves strong Hit@10 performance (up to 0.753) for shortlist generation, though exact Top-1 remains challenging in large label spaces.

## Executive Summary
This work introduces an automated bug triaging system using instruction-tuned LLMs with LoRA adapters and candidate-constrained decoding. The method fine-tunes DeepSeek-R1-Distill-Llama-8B on conversational JSONL data from EclipseJDT and Mozilla projects, ensuring only valid developers are assigned. Evaluated on multi-year datasets aligned with prior graph-based work, it achieves Top-1/Hit@10 of 0.156/0.475 on EclipseJDT and 0.013/0.753 on Mozilla. Strong Hit@10 performance indicates practical utility for shortlist generation in real-world triage workflows, while exact Top-1 remains challenging in large label spaces. The approach requires no handcrafted features or graph construction, offering a lightweight, scalable solution.

## Method Summary
The approach fine-tunes an 8-billion parameter LLM (DeepSeek-R1-Distill-Llama-8B) using LoRA adapters on conversational JSONL data from bug tracking systems. Training data is formatted as System/User/Assistant conversations where the assistant's response is the developer assignee. Candidate-constrained decoding restricts inference to valid developer identifiers from the training set. The model is trained on multi-year EclipseJDT and Mozilla datasets with temporal splits matching prior graph-based baselines. Low-rank adaptation (rank=16) enables efficient fine-tuning on consumer hardware while preserving the base model's semantic capabilities.

## Key Results
- Top-1/Hit@10 accuracy of 0.156/0.475 on EclipseJDT and 0.013/0.753 on Mozilla
- Strong shortlisting performance (Hit@10) indicates practical utility for human-in-the-loop triage
- Text-only approach achieves competitive shortlisting vs. graph-based methods on text-heavy projects
- Exact Top-1 matching remains difficult in large label spaces with developer turnover

## Why This Works (Mechanism)

### Mechanism 1: Candidate-Constrained Decoding
Constraining the output vocabulary to valid developer identifiers prevents hallucination and improves shortlist precision. During inference, decoding is restricted to a predefined candidate set, forcing the model to rank only valid assignees. This assumes the correct assignee exists within the provided roster (closed-world assumption). The approach avoids generating plausible but non-existent email addresses or handles.

### Mechanism 2: Semantic Mapping via Instruction Tuning (LoRA)
Parameter-efficient fine-tuning (LoRA) adapts a general LLM to map bug semantics to specific developer identifiers without full model retraining. Low-Rank Adaptation injects trainable layers into the frozen 8B model, learning to associate bug content with assignee responses through conversational training. This assumes the base model possesses sufficient semantic capacity to distinguish bug types and that the LoRA rank is sufficient to capture the mapping logic.

### Mechanism 3: Shortlist Ranking in Large Label Spaces
While exact Top-1 matching is difficult in large label spaces, LLMs effectively narrow the search space, placing the correct developer in the Top-K recommendations. The model optimizes for the next token based on semantic similarity. In large, noisy datasets like Mozilla (37,371 developers), the signal for a unique exact match is weak, but semantic correlation remains strong enough to place the correct assignee in the top 10 possibilities 75% of the time.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**
  - Why needed: Enables efficient fine-tuning of 8B model on consumer hardware by training smaller adapter matrices alongside frozen base model
  - Quick check: Does LoRA update the weights of the base LLM, or does it train a smaller set of adapter matrices alongside the frozen base model?

- **Hit@K (Top-K Accuracy)**
  - Why needed: Measures shortlisting utility rather than exact automation success, shifting how triage success is measured
  - Quick check: If a model has a Hit@10 of 0.75 but a Top-1 of 0.01, is it more useful as a fully autonomous agent or a recommendation engine for a human triager?

- **Constrained Decoding**
  - Why needed: Prevents LLM hallucination by restricting outputs to valid developer identifiers
  - Quick check: How does the model ensure the generated output string matches a valid entry in the developer roster during the inference step?

## Architecture Onboarding

- **Component map:** Data Processor -> JSONL Prompt Construction -> LoRA Supervised Fine-Tuning -> Inference with Candidate Constraints -> Ranking/Post-processing
- **Critical path:** Data cleaning/filtering -> JSONL Prompt Construction -> LoRA SFT -> Inference with Candidate Constraints -> Ranking/Post-processing
- **Design tradeoffs:** Sacrifices structural/temporal precision of GNNs for ease of deployment and zero feature engineering; lowers Top-1 on structured projects but raises Hit@10 on text-heavy projects
- **Failure signatures:** Cold-Start Misassignment (defaults to high-frequency developers), Ambiguity Drift (relies on generic keywords when technical details missing)
- **First 3 experiments:**
  1. Reproduce the "Window Effect" by training on multi-year Mozilla data and evaluating on recent 6-month test split to verify Top-1 accuracy jump
  2. Compare constrained vs. unconstrained inference to quantify hallucination rate and validate candidate masking effectiveness
  3. Artificially cap candidate set to top 100 developers to measure impact of large label space on Top-1 score degradation

## Open Questions the Paper Calls Out

- **Hybrid Architecture Integration:** Can combining graph-derived structural priors with LLM-based text reasoning bridge the Top-1 performance gap in projects like EclipseJDT? Evidence would require evaluation of combined models showing improved Top-1 scores over standalone LLM or graph baselines.

- **Ranking Loss Optimization:** Does replacing standard SFT with pairwise or listwise ranking losses improve intra-list ordering quality compared to cross-entropy objective? Evidence would require comparison of MRR or NDCG between models trained with different loss functions.

- **Profile Embedding Integration:** To what extent does integrating developer profile embeddings (expertise, component ownership, temporal recency) mitigate cold-start failures for low-frequency contributors? Evidence would require ablation studies showing improved Hit@K for long-tail developers when profile embeddings are added.

- **Retrieval-Augmented Expansion:** Can retrieval-augmented candidate expansion effectively address the long-tail developer problem without extensive retraining? Evidence would require experiments demonstrating dynamically retrieved examples improve assignment accuracy for rare developer classes.

## Limitations
- Closed-world assumption limits effectiveness for new contributors not in training roster
- Text-only features cannot leverage structural code information or developer interaction patterns
- Multi-year training setup introduces label drift from developer turnover, suppressing Top-1 metrics
- Exact Top-1 matching remains challenging in large label spaces (37k+ developers in Mozilla)

## Confidence
- **High Confidence:** Shortlisting capability (Hit@10) and technical implementation details of LoRA fine-tuning and candidate-constrained decoding
- **Medium Confidence:** Practical utility argument as human-in-the-loop recommendation engine
- **Medium Confidence:** Claim that text-only modeling is "competitive" with graph-based methods

## Next Checks
1. **Label Drift Verification:** Train and evaluate separate models on fixed 6-month windows versus full multi-year dataset to quantify impact of developer turnover on Top-1 accuracy
2. **Constrained Decoding Validation:** Implement and compare multiple constrained decoding strategies (logit masking, constrained beam search, post-hoc filtering) to verify candidate constraint effectiveness
3. **Shortlist Quality Audit:** For cases where correct assignee is in Top-10 but not Top-1, conduct qualitative review of bug reports and recommendations to assess genuine usefulness for human triager