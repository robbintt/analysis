---
ver: rpa2
title: 'DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision
  Making with RL-Finetuned LLMs'
arxiv_id: '2601.14711'
source_url: https://arxiv.org/abs/2601.14711
tags:
- budget
- allocation
- few-shot
- data
- periods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot budget allocation
  in online advertising, where advertisers have personalized objectives but limited
  historical data. The authors propose DARA, a dual-phase LLM-based framework that
  separates the decision-making process into few-shot reasoning and fine-grained optimization.
---

# DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs

## Quick Facts
- arXiv ID: 2601.14711
- Source URL: https://arxiv.org/abs/2601.14711
- Reference count: 40
- Key outcome: DARA achieves up to 12.2% improvement in cumulative advertiser value under budget constraints through dual-phase LLM architecture with adaptive RL fine-tuning.

## Executive Summary
This paper addresses the challenge of few-shot budget allocation in online advertising, where advertisers have personalized objectives but limited historical data. The authors propose DARA, a dual-phase LLM-based framework that separates the decision-making process into few-shot reasoning and fine-grained optimization. DARA leverages a novel RL fine-tuning strategy called GRPO-Adaptive, which dynamically updates the reference model during training to enhance both reasoning and numerical precision. Extensive experiments on real-world and synthetic data demonstrate that DARA consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

## Method Summary
DARA is a dual-phase LLM architecture for few-shot budget allocation. The first phase uses a Few-shot Reasoner to generate initial allocation vectors from limited historical episodes via in-context prompting. The second phase employs a Fine-grained Optimizer that iteratively refines plans using a sliding window of recent feedback. Both components are trained using GRPO-Adaptive, an RL fine-tuning strategy that dynamically updates the reference model during training to prevent over-regularization to outdated baselines. The framework incorporates a composite reward design with variance minimization, constraint enforcement, and adaptive bonus shaping to guide the LLMs toward feasible, balanced allocations.

## Key Results
- DARA achieves up to 12.2% improvement over the strongest baseline in cumulative advertiser value under budget constraints
- Dual-phase architecture reduces marginal ROI variance across time periods compared to single-phase approaches
- GRPO-Adaptive with dynamic reference updates (M=60) outperforms static reference and no-KL variants in variance reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing budget allocation into few-shot reasoning and fine-grained optimization phases improves cumulative advertiser value under budget constraints.
- Mechanism: The Few-shot Reasoner generates initial allocation vectors from limited historical episodes via in-context prompting, while the Fine-grained Optimizer iteratively refines plans using a sliding window of recent feedback. This separates high-level pattern generalization from numerically sensitive adjustments.
- Core assumption: Early-stage allocation benefits primarily from cross-episode pattern recognition, while later stages require responsive, feedback-driven optimization—these cognitive demands are sufficiently distinct to warrant separate models.
- Evidence anchors: [abstract] dual-phase framework; [section 5.3] ablation shows single-phase LLM yields highest marginal ROI variance; [corpus] ABPlanner uses RL for budget planning but lacks explicit phase separation.

### Mechanism 2
- Claim: Periodically updating the KL reference model during GRPO fine-tuning enhances both reasoning coherence and numerical precision.
- Mechanism: GRPO-Adaptive replaces the static reference policy π_ref with a snapshot of the current policy every K iterations, preventing over-regularization to an outdated baseline while maintaining distributional constraint.
- Core assumption: As policy improves, anchoring KL divergence to a fixed initial policy becomes counterproductive—early-iteration outputs may be incoherent, and later improvements get penalized relative to obsolete references.
- Evidence anchors: [abstract] GRPO-Adaptive enhances reasoning and numerical precision; [section 4.1.1] Equation 10 shows KL divergence against periodically updated π_ref; [section 5.4.2] M=60 updates achieve lowest variance.

### Mechanism 3
- Claim: Composite reward design with variance minimization, constraint enforcement, and adaptive bonus shaping guides LLMs toward feasible, balanced allocations.
- Mechanism: R(b) = R_env + R_constraint + R_bonus, where R_env penalizes marginal ROI variance across periods, R_constraint enforces budget sum equality, and R_bonus encourages reallocating toward historically high-reward periods.
- Core assumption: LLMs lack inherent numerical sensitivity; explicit reward shaping is necessary to translate abstract ROI equalization goals into concrete gradient signals.
- Evidence anchors: [section 4.3.2] Equations 15-17 define the composite reward; [abstract] "reducing marginal ROI variance across time periods" is identified as a key outcome.

## Foundational Learning

- Concept: Concave Return Functions and Marginal ROI Equalization
  - Why needed here: The theoretical optimality condition (equal marginal ROI across periods) underpins the variance-minimization objective; without this, the reward design is unmotivated.
  - Quick check question: If marginal ROI is 0.8 in period A and 0.4 in period B, which period should receive additional budget to improve total return?

- Concept: KL Divergence as Policy Regularization
  - Why needed here: GRPO-Adaptive relies on KL constraints to prevent policy collapse; understanding why a reference model is needed—and why updating it helps—is essential for tuning M and β.
  - Quick check question: What happens to policy diversity if KL regularization is removed entirely during RL fine-tuning?

- Concept: Sliding Window Context for Online Adaptation
  - Why needed here: The Fine-grained Optimizer uses a fixed-size window of recent episodes; this controls context length and determines how quickly the model responds to distribution shifts.
  - Quick check question: If the sliding window size w is set to 1, what information does the optimizer have access to when refining allocations?

## Architecture Onboarding

- Component map:
  - Few-shot Reasoner (LLM #1): Input = historical episodes H; Output = initial allocation vector b^(1); Trained with GRPO-Adaptive on simulated environments
  - Fine-grained Optimizer (LLM #2): Input = sliding window W_s of recent (allocation, ROI) pairs; Output = refined allocation b^(s); Trained with GRPO-Adaptive with bonus reward term
  - Environment Wrapper: Provides marginal ROI feedback; supports both real-world data and synthetic polynomial functions

- Critical path:
  1. Encode H into prompt P_1 → Few-shot Reasoner → b^(1)
  2. Observe MROI feedback r^(1)
  3. Build sliding window W_2 = H_tail ∪ {(b^(1), r^(1))}
  4. Loop: Encode W_s → Fine-grained Optimizer → b^(s) → Observe r^(s) → Update W_{s+1}

- Design tradeoffs:
  - Single LLM vs. dual-phase: Single LLM is simpler but struggles with both generalization and precision (Section 5.3 confirms performance ceiling)
  - Static vs. adaptive KL reference: Static is easier to implement but slower convergence; adaptive requires tuning M but improves final performance
  - Real vs. synthetic training data: Real data is more realistic but limited; synthetic enables unlimited scenarios but may not capture real distribution tails

- Failure signatures:
  - High variance persisting after step 3-4: Likely Few-shot Reasoner not generalizing; check prompt encoding and historical episode quality
  - Allocation vectors violating budget constraint: R_constraint penalty weight insufficient or extraction regex failing
  - Sudden variance spike mid-training: KL reference update too frequent (M too small); increase to 60-90 iterations

- First 3 experiments:
  1. Ablate dual-phase: Run single LLM with identical prompts and GRPO-Adaptive; compare variance trajectory to dual-phase. Expect Section 5.3 replication.
  2. Sweep KL reference update frequency: Test M ∈ {30, 60, 90, ∞ (static)} on synthetic environment; plot variance vs. training episode. Validate Figure 6 findings.
  3. Reward component isolation: Train with R_env only, then add R_constraint, then add R_bonus; measure constraint violation rate and variance reduction at each stage. Confirm each term's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DARA's performance scale when the number of time periods increases substantially (e.g., 24-hourly periods or 100+ granular slots)?
- Basis in paper: [inferred] The sensitivity analysis only tests 2-10 periods, with 6 being optimal; real-world campaigns may require finer temporal granularity.
- Why unresolved: The authors state "the improvement is most pronounced when the number of periods is 6" but do not explore whether the dual-phase architecture degrades or benefits from substantially longer horizons.
- What evidence would resolve it: Experiments on 24, 48, or 100+ time periods, measuring both variance reduction and computational overhead.

### Open Question 2
- Question: What is the theoretical justification for the optimal GRPO-Adaptive reference update frequency (M=60)?
- Basis in paper: [explicit] "When the reference is updated too frequently (e.g., M=30), the gain over vanilla GRPO becomes marginal... updating the reference too slowly (e.g., M=90) delays alignment."
- Why unresolved: The choice is empirically determined through experimentation; no formal analysis connects M to environment characteristics, model size, or task complexity.
- What evidence would resolve it: Theoretical analysis linking update frequency to convergence rates, or systematic experiments across varying M values and task dimensions.

### Open Question 3
- Question: How sensitive is DARA to violations of the concave, differentiable return function assumption?
- Basis in paper: [explicit] "Each function v_t(b_t) is assumed to be differentiable, strictly increasing, and concave... capturing the realistic effect of diminishing returns."
- Why unresolved: Real advertising environments may exhibit non-concave returns (e.g., threshold effects, saturation spikes) or discontinuous cost functions not explored in the paper.
- What evidence would resolve it: Experiments with non-concave or discontinuous reward functions in the synthetic environment.

### Open Question 4
- Question: What is the computational latency of the dual-LLM architecture in real-time bidding scenarios?
- Basis in paper: [inferred] The paper emphasizes deployment in "online advertising" but provides no inference time analysis for the dual-phase framework with two LLMs.
- Why unresolved: RTB systems operate under millisecond constraints; the sequential Few-shot Reasoner → Fine-grained Optimizer pipeline may exceed acceptable latency thresholds.
- What evidence would resolve it: End-to-end latency benchmarks comparing single-LLM vs. dual-LLM inference under production-like constraints.

## Limitations
- Performance sensitivity to reference update frequency (M) lacks theoretical justification and systematic sensitivity analysis
- Real-time latency of dual-LLM architecture not evaluated for production RTB constraints
- Reward component weights appear tuned to synthetic environment without systematic sensitivity analysis

## Confidence
- High confidence: Dual-phase framework design and ablation results showing phase separation improves performance
- Medium confidence: GRPO-Adaptive claims, as optimal M=60 appears critical but lacks sensitivity analysis across diverse problem scales
- Low confidence: Specific reward component weights (α, δ, τ) without showing systematic sensitivity analysis

## Next Checks
1. **Cross-dataset generalization**: Evaluate DARA on real advertising platform data with varying campaign characteristics (different industries, budget scales, time horizons) to validate performance beyond synthetic environments.

2. **Reference update frequency sensitivity**: Systematically vary M from 10 to 200 iterations on the same dataset to map the performance landscape and identify optimal ranges for different problem scales.

3. **Real-time adaptation robustness**: Test the sliding window mechanism with non-stationary ROI distributions (sudden market shifts, seasonal effects) to verify stability guarantees under distribution drift.