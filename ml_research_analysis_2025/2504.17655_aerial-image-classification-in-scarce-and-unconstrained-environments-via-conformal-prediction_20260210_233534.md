---
ver: rpa2
title: Aerial Image Classification in Scarce and Unconstrained Environments via Conformal
  Prediction
arxiv_id: '2504.17655'
source_url: https://arxiv.org/abs/2504.17655
tags:
- prediction
- conformal
- sets
- calibration
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper conducts an empirical evaluation of conformal prediction\
  \ methods using the ERA dataset for aerial image classification in resource-scarce,\
  \ unconstrained environments. The study examines three nonconformity score functions\u2014\
  LAC, APS, and RAPS\u2014combined with three pretrained vision models (MobileNet,\
  \ DenseNet, ResNet) fine-tuned on limited labeled data."
---

# Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction

## Quick Facts
- arXiv ID: 2504.17655
- Source URL: https://arxiv.org/abs/2504.17655
- Reference count: 40
- Primary result: Conformal prediction produces reliable prediction sets in aerial image classification with minimal labeled data, with LAC yielding smallest sets and temperature scaling having model-dependent effects.

## Executive Summary
This paper evaluates conformal prediction methods for aerial image classification in resource-scarce, unconstrained environments using the ERA dataset. The study examines three nonconformity score functions (LAC, APS, RAPS) combined with three pretrained vision models (MobileNet, DenseNet, ResNet) fine-tuned on limited labeled data. Results demonstrate that conformal prediction can deliver reliable uncertainty estimates even with small datasets, with LAC producing the most informative prediction sets. Temperature scaling does not consistently reduce prediction set sizes, as its effect depends on the model's optimized parameter. The findings highlight the feasibility of applying conformal prediction in challenging real-world scenarios with minimal labeled data.

## Method Summary
The study fine-tunes pretrained vision models (MobileNetV2, DenseNet-121, ResNet-152) on limited labeled aerial imagery from the ERA dataset, using only the middle frame of video clips. Models are trained for 10 epochs with batch size 8 and learning rate 0.001, modifying only the final classifier layer. Conformal prediction methods (LAC, APS, RAPS) from the MAPIE library generate prediction sets on a held-out calibration set, with temperature scaling optionally applied to calibrate model probabilities. Performance is evaluated on test sets using coverage (targeting 1-α for α ∈ {0.1, 0.2}) and average prediction set size metrics.

## Key Results
- Conformal prediction produces reliable prediction sets even with small datasets (386 train, 261 calibration, 112 test samples)
- LAC yields the smallest and most informative prediction sets compared to APS and RAPS
- APS and RAPS achieve higher coverage but at the cost of larger prediction sets
- Temperature scaling does not consistently reduce prediction set sizes, with effects depending on the model's optimized parameter
- Even lightweight models like MobileNet can deliver valuable uncertainty estimates in resource-constrained scenarios

## Why This Works (Mechanism)
Conformal prediction transforms point predictions into prediction sets that provably cover the true label with a user-specified probability, even with limited data. By using nonconformity scores based on softmax probabilities, the method adapts to the model's confidence calibration, with LAC leveraging maximum class probability, APS using normalized nonconformity scores, and RAPS applying adaptive thresholding. Temperature scaling further refines probability estimates by optimizing a single parameter on calibration data, though its impact varies by architecture.

## Foundational Learning
- **Conformal prediction**: A framework for uncertainty quantification that produces prediction sets with guaranteed coverage. Why needed: Provides statistically valid uncertainty estimates in low-data regimes. Quick check: Verify coverage on calibration set meets target (1-α).
- **Nonconformity scores**: Measures of how different a prediction is from the rest of the calibration set. Why needed: Determines which labels belong in the prediction set. Quick check: Ensure scores are computed correctly for all calibration examples.
- **Prediction set size vs. coverage tradeoff**: Balancing informativeness with reliability. Why needed: Smaller sets are more useful but must maintain target coverage. Quick check: Monitor both metrics during evaluation.
- **Temperature scaling**: Post-processing technique to calibrate model probabilities. Why needed: Improves probability estimates before conformal prediction. Quick check: Validate calibration on held-out data.
- **ERA dataset structure**: Aerial video frames labeled with 7 disaster/construction classes. Why needed: Defines the classification task and data constraints. Quick check: Confirm class distribution and frame extraction.
- **Transfer learning with limited data**: Fine-tuning pretrained models on small datasets. Why needed: Enables good performance with minimal labeled examples. Quick check: Monitor training/validation accuracy to detect overfitting.

## Architecture Onboarding

**Component Map**
MobileNet/DenseNet/ResNet -> Fine-tuning -> Temperature Scaling (optional) -> Conformal Prediction (LAC/APS/RAPS) -> Evaluation

**Critical Path**
Data loading → Model fine-tuning → Calibration set processing → Conformal predictor fitting → Test set evaluation

**Design Tradeoffs**
- Model capacity vs. data scarcity: Larger models may overfit; lighter models may underfit
- Prediction set size vs. coverage: Smaller sets are more actionable but must meet coverage targets
- Temperature scaling vs. prediction set size: Can improve calibration but may increase set size for some architectures

**Failure Signatures**
- Empty prediction sets from LAC indicate overly strict thresholds
- Poor coverage suggests miscalibration or insufficient calibration data
- Large prediction sets may indicate model uncertainty or suboptimal temperature scaling

**First Experiments**
1. Fine-tune MobileNet on ERA dataset and evaluate baseline accuracy
2. Apply LAC conformal prediction and verify coverage on calibration set
3. Compare prediction set sizes across LAC, APS, and RAPS methods

## Open Questions the Paper Calls Out
None

## Limitations
- Exact train/calibration/test splits are unspecified, making precise numerical reproduction difficult
- Optimizer choice and learning rate scheduler details are not provided
- Temperature scaling's effectiveness varies by model architecture, with counterintuitive results possible
- Empty prediction sets from LAC handling is not explicitly specified in evaluation metrics

## Confidence

**High confidence**: Overall methodology and experimental framework are well-specified and reproducible
**Medium confidence**: Core results showing conformal prediction's effectiveness with limited data
**Low confidence**: Exact numerical values in tables due to unspecified data splits and training hyperparameters

## Next Checks
1. Verify that the 386/261/112 split is reproducible by fixing random seeds and checking class distributions
2. Compare coverage and prediction set sizes across all three conformal methods (LAC, APS, RAPS) to confirm LAC produces smaller, more informative sets
3. Test temperature scaling's effect on each model architecture separately to confirm the model-dependent behavior described in the paper