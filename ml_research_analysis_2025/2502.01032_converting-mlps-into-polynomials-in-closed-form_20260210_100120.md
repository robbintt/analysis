---
ver: rpa2
title: Converting MLPs into Polynomials in Closed Form
arxiv_id: '2502.01032'
source_url: https://arxiv.org/abs/2502.01032
tags:
- linear
- quadratic
- gaussian
- approximants
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to analytically derive polynomial approximations
  of neural networks under the assumption of Gaussian inputs. The core contribution
  is a closed-form least-squares solution for linear and quadratic approximations
  of MLPs and GLUs, enabling mechanistic interpretability through eigendecomposition
  of the coefficients.
---

# Converting MLPs into Polynomials in Closed Form
## Quick Facts
- arXiv ID: 2502.01036
- Source URL: https://arxiv.org/abs/2502.01036
- Reference count: 29
- Primary result: Method to analytically derive polynomial approximations of neural networks under Gaussian input assumptions

## Executive Summary
This paper presents a closed-form least-squares method for deriving polynomial approximations (linear and quadratic) of MLPs and GLUs under the assumption of Gaussian input distributions. The core innovation enables mechanistic interpretability through eigendecomposition of the resulting coefficients. The authors apply this technique to study distributional simplicity bias in neural networks, demonstrating that networks initially learn low-order statistics (linear, then quadratic) before progressing to higher-order features, as evidenced by phase transitions in approximation quality during training on MNIST.

## Method Summary
The authors develop a method to convert MLPs into polynomials in closed form by exploiting properties of jointly Gaussian variables to compute required integrals efficiently. For a given neural network, they analytically derive linear and quadratic polynomial approximations that minimize least-squares error under Gaussian input assumptions. The approach leverages Gaussian integration formulas to obtain closed-form expressions for the coefficients, enabling efficient computation without iterative optimization. This allows for direct analysis of the network's learned representations through eigendecomposition of the polynomial coefficients.

## Key Results
- Networks exhibit a phase transition during training where linear approximation quality degrades while quadratic quality remains stable on MNIST
- Quadratic approximations capture over 95% of variance in network outputs by the end of training
- Linear and quadratic approximations can generate effective adversarial attacks by projecting inputs onto the nullspace of the approximation

## Why This Works (Mechanism)
The method works by exploiting the mathematical properties of Gaussian distributions to analytically compute the moments required for polynomial approximation. When inputs are jointly Gaussian, the expectations needed to compute least-squares optimal polynomial coefficients can be expressed in closed form using Gaussian moment formulas. This allows the transformation of the network's nonlinear operations into a polynomial representation without requiring iterative numerical optimization.

## Foundational Learning
- **Gaussian Integration**: Understanding how to compute moments of products of Gaussian variables is essential for deriving the closed-form expressions
- **Least-Squares Approximation**: The method minimizes squared error between network outputs and polynomial approximations, requiring familiarity with normal equations
- **Eigendecomposition**: Used to analyze the learned representations through the polynomial coefficients, revealing the network's learned features
- **Polynomial Bases**: Knowledge of how to represent and manipulate polynomial expressions is necessary for implementing the approximation
- **MLP Forward Pass**: Understanding how MLPs compute outputs layer-by-layer is crucial for deriving the polynomial approximations

Quick check: Verify that the Gaussian moment formulas used in the derivation match standard results for expectations of products of Gaussian variables.

## Architecture Onboarding
Component map: Input -> Polynomial Approximation -> Eigendecomposition -> Analysis
Critical path: Gaussian input assumption → Moment computation → Coefficient derivation → Interpretation
Design tradeoffs: Assumes Gaussian inputs (limitation) vs. closed-form solution (advantage)
Failure signatures: Poor approximation quality when inputs deviate from Gaussian distribution
First experiments:
1. Apply the method to a simple MLP on synthetic Gaussian data to verify the approximation accuracy
2. Compute the linear and quadratic approximations of a trained MLP on MNIST and track their explained variance during training
3. Generate adversarial examples using the nullspace of the polynomial approximation and measure their effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes Gaussian input distributions, limiting applicability to real-world datasets with non-Gaussian inputs
- Quadratic approximation requires storing O(n²) coefficients, limiting scalability for high-dimensional inputs
- Demonstrated only on MNIST with specific architectures, requiring validation across diverse datasets and network types

## Confidence
High: The mathematical derivation using Gaussian integration formulas is sound and the closed-form solution is correctly computed
Medium: The distributional simplicity bias claim (low-to-high order learning) needs validation across more architectures and datasets
Low: The practical effectiveness of polynomial-based adversarial attacks compared to established methods remains unassessed

## Next Checks
1. Test the polynomial approximation method on non-Gaussian datasets (CIFAR, tabular data) to assess robustness beyond the MNIST assumption
2. Evaluate the low-to-high order learning hypothesis across multiple architectures (CNNs, ResNets) and datasets to determine generality
3. Compare quadratic approximation-based attacks against established methods (PGD, FGSM) in terms of success rate, computational cost, and transferability