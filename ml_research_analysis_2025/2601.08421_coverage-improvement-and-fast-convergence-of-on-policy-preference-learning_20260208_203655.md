---
ver: rpa2
title: Coverage Improvement and Fast Convergence of On-policy Preference Learning
arxiv_id: '2601.08421'
source_url: https://arxiv.org/abs/2601.08421
tags:
- on-policy
- learning
- preference
- policy
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper analyzes how the sampling policy\u2019s coverage evolves\
  \ during on-policy preference learning, proposing the \"coverage improvement principle\"\
  \ that sufficient batch size leads to uniform improvement in coverage and rapid\
  \ convergence. In the contextual bandit setting with Bradley-Terry preferences and\
  \ linear softmax policies, the authors show that on-policy DPO converges exponentially\
  \ in iterations for batch sizes above a coverage threshold, while offline methods\
  \ are constrained by slower minimax rates."
---

# Coverage Improvement and Fast Convergence of On-policy Preference Learning

## Quick Facts
- arXiv ID: 2601.08421
- Source URL: https://arxiv.org/abs/2601.08421
- Reference count: 40
- Key outcome: On-policy DPO achieves exponential convergence with total sample complexity of (ln 1/ε) times the coverage threshold, outperforming offline DPO's 1/ε² dependence.

## Executive Summary
This paper analyzes on-policy preference learning in contextual bandits, proposing the "coverage improvement principle" that sufficient batch size leads to uniform improvement in coverage and rapid convergence. The authors show that on-policy DPO converges exponentially in iterations for batch sizes above a coverage threshold, while offline methods are constrained by slower minimax rates. A hybrid sampler based on a novel preferential G-optimal design removes coverage dependence and achieves convergence in two rounds. Experiments confirm that on-policy methods outperform offline counterparts and achieve stable, monotonic performance gains.

## Method Summary
The paper analyzes on-policy DPO for contextual bandits with Bradley-Terry preferences and linear softmax policies. The method samples response pairs from the current policy, obtains preference labels, and updates the policy via DPO. The key innovation is showing that with sufficient batch size, each update improves coverage relative to the target policy, enabling exponential convergence. The authors also propose a hybrid sampler using preferential G-optimal design to eliminate coverage dependence, and develop reward distillation methods with faster noiseless rates.

## Key Results
- On-policy DPO achieves exponential convergence with total sample complexity of (ln 1/ε) times the coverage threshold
- Offline DPO requires 1/ε² multiplicative factor, demonstrating exponential separation in overhead
- Hybrid sampler based on preferential G-optimal design removes coverage dependence and converges in two rounds
- On-policy methods show stable, monotonic performance gains while offline methods degrade after first epoch

## Why This Works (Mechanism)

### Mechanism 1: Coverage Improvement Principle
- **Claim:** Sufficiently large batch sizes in on-policy DPO improve the coverage of the sampling policy relative to the target policy, enabling exponential convergence.
- **Mechanism:** Unlike offline methods constrained by a fixed reference policy, on-policy sampling updates the policy $\hat{\pi}_k$. Theorem 3.2 shows that if the batch size $n$ exceeds a generalized coverage threshold $C_p^*(R)$, the error radius $r_k$ shrinks such that the updated policy enters a "confidence region" with uniformly better coverage than the previous iteration.
- **Core assumption:** The batch size must satisfy $n \gtrsim C_p^*(R)$ (Corollary 3.3). Realizability (Assumption 2) ensures the target is reachable.
- **Evidence anchors:**
  - [abstract] "We propose and rigorously justify the coverage improvement principle: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better..."
  - [section 3] Theorem 3.2 establishes the exponential decay factor $\eta$ and the condition on $\xi_n$.
  - [corpus] SIMPLEMIX (arXiv:2505.02363) supports the idea that mixing strategies (related to coverage) impact performance.
- **Break condition:** If batch size $n$ is below the coverage threshold, the error radius may not shrink, stalling convergence.

### Mechanism 2: Exponential Convergence Overhead Separation
- **Claim:** On-policy DPO achieves a total sample complexity of $(\ln 1/\epsilon) \cdot C_p^*(R)$, whereas offline DPO requires $1/\epsilon^2 \cdot C_p^*(R)$.
- **Mechanism:** Offline learning suffers a "minimax rate" bottleneck because it cannot improve data quality. On-policy learning leverages the coverage improvement (Mechanism 1) to convert the $1/\epsilon^2$ dependence into a $\ln(1/\epsilon)$ dependence (iterations). This is formalized as "linear convergence in the number of rounds."
- **Core assumption:** The target policy $\pi^*$ must be realizable within the linear softmax class (Assumption 2).
- **Evidence anchors:**
  - [section 2.2] Theorem 2.2 provides the offline lower bound.
  - [section 3] Corollary 3.3 explicitly contrasts the sample complexities $n_{on} \ll n_{off}$.
  - [corpus] "Understanding the Performance Gap..." (arXiv:2505.19770) discusses the theoretical dichotomy between DPO and RLHF, aligning with the convergence rate analysis here.
- **Break condition:** If the preference oracle violates the Bradley-Terry model significantly, the theoretical separation may not hold.

### Mechanism 3: Hybrid Sampler via Preferential G-Optimal Design
- **Claim:** A hybrid sampler combining on-policy data with a fixed "preferential G-optimal design" $\pi_g$ removes dependence on coverage $C_p^*(R)$ and converges in two rounds.
- **Mechanism:** The algorithm constructs $\pi_g$ to minimize the maximum prediction variance for centered features $\phi_g$. By sampling from a mixture $\frac{1}{2}\hat{\pi}_k + \frac{1}{2}\pi_g$, the first round guarantees a good estimate due to $\pi_g$, and the second round leverages this improved policy to converge without requiring luck on the initial coverage.
- **Core assumption:** Features $\phi(x,a)$ are known and bounded; computation of the design $\pi_g$ is feasible.
- **Evidence anchors:**
  - [section 4] Algorithm 2 details the sampler; Theorem 4.4 proves convergence in two steps with complexity $e^{O(dR^2/\epsilon^2)}$.
  - [corpus] InCo-DPO (arXiv:2503.15880) highlights the balance between distribution shift and data quality, which this hybrid design explicitly manages.
- **Break condition:** If $|X|$ (prompt space) is massive, computing the conditional design $\pi_{cg}(x)$ for all $x$ becomes computationally prohibitive (Remark 4.3).

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - **Why needed here:** It provides the probabilistic link between rewards and binary preferences ($P(a_1 \succ a_2) = \sigma(r(x,a_1) - r(x,a_2))$). The DPO loss is derived as the negative log-likelihood under this model.
  - **Quick check question:** Can you write the mathematical definition of the preference probability given the reward function $r^*$?

- **Concept: Generalized Coverage ($C_{\pi \to \pi'}$)**
  - **Why needed here:** It quantifies the "exploration difficulty." The paper defines coverage via feature covariance inequality $V(\pi') \preceq C \cdot V(\pi)$. The threshold $C_p^*(R)$ is the critical batch size scaling factor.
  - **Quick check question:** Why does the coverage condition $V(\pi^*) \succeq \lambda I$ (Assumption 3) imply a lower bound on the batch size $n$?

- **Concept: KL-Regularized Reward Maximization**
  - **Why needed here:** It defines the "target policy" $\pi^* \propto \pi_0 \exp(\gamma^{-1}r^*)$. This links the alignment goal to the policy class used in the proofs (linear softmax).
  - **Quick check question:** How does the temperature parameter $\gamma$ affect the convergence bounds (Remark 3.5)?

## Architecture Onboarding

- **Component map:** Policy Network $\pi_\theta$ -> On-Policy Buffer -> DPO Loss Module -> Update $\theta$ -> New Policy $\hat{\pi}_{k+1}$
- **Critical path:** Sampling $D_k$ from $\hat{\pi}_k$ → Labeling via Oracle/LLM → Gradient Descent on $L_{DPO}$ → Update $\hat{\pi}_{k+1}$. The bottleneck is often the labeling step if using AI feedback.
- **Design tradeoffs:**
  - **Batch Size ($n$) vs. Iterations ($K$):** You need $n > C_p^*(R)$ to trigger exponential convergence. Using smaller batches might require significantly more iterations or fail to converge.
  - **Pure On-Policy vs. Hybrid Design:** Pure on-policy is simpler but depends on initial coverage. Hybrid (Algorithm 2) guarantees convergence in 2 rounds but requires computing a potentially complex design $\pi_g$.
- **Failure signatures:**
  - **REBEL Degeneracy:** If regularizing against the previous policy $\hat{\pi}_k$ instead of $\pi_0$, the policy may collapse to a one-hot distribution (Section 5.1).
  - **Coverage Stall:** If $n$ is too small, the "coverage improvement principle" fails, and performance plateaus like offline DPO.
- **First 3 experiments:**
  1. **TL;DR Summarization Baseline:** Run On-policy DPO vs. Off-policy DPO for 5 iterations to replicate the win-rate/reward score divergence (Figure 2).
  2. **Batch Size Ablation:** Test batch sizes around the theoretical threshold. Does convergence speed drop drastically below a certain $n$?
  3. **Distillation Stability Check:** Compare REBEL vs. Reward-Calibrated (RDRC) updates over 5 iterations to confirm that RDRC prevents the "degeneracy" drop in performance (Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an on-policy preference learning algorithm achieve the lower bound of O(d·C₁*(R)/n) for KL divergence when p=1, closing the Θ(d) gap between the upper and lower bounds?
- Basis in paper: [explicit] "We leave to future work whether an on-policy preference learning algorithm can match the lower bound when p=1, or under a different notion of coverage." (Remark 3.5)
- Why unresolved: The current analysis for on-policy DPO gives an upper bound with an extra factor of d compared to the minimax lower bound in Theorem 2.2 for p=1.
- What evidence would resolve it: An algorithm with proven sample complexity matching the lower bound, or a refined lower bound showing the current upper bound is tight.

### Open Question 2
- Question: Can the preferential G-optimal design in Eq.(7) be improved from O(d²) to O(d) variance guarantee, and can the dependency on |X| be reduced?
- Basis in paper: [explicit] "We leave open the question of whether Eq.(7) can be improved to O(d)... whether we can avoid the full dependency on |X$, perhaps under additional structural conditions on ϕ, is left to future work." (Remark 4.3)
- Why unresolved: The current construction via global G-optimal design yields d² suboptimality, and requires computing conditional designs for all x∈X.
- What evidence would resolve it: A modified design achieving O(d) guarantee, or proof that d² is necessary without additional structure on ϕ.

### Open Question 3
- Question: What are the convergence guarantees when the Bradley-Terry preference model is misspecified, such as when using LLMs to annotate preferences?
- Basis in paper: [inferred] The paper notes "the feedback oracle may also not be well-aligned with P* or even compatible with the Bradley-Terry assumption, e.g., when using LLMs to annotate preferences" (Section 5.2), but all main results assume Assumption 1.
- Why unresolved: The theoretical guarantees rely entirely on the Bradley-Terry assumption holding exactly.
- What evidence would resolve it: Convergence analysis with explicit misspecification error terms, or empirical characterization of Bradley-Terry violations in practice.

### Open Question 4
- Question: How does on-policy preference learning behave when batch sizes fall below the coverage threshold C_p*(R)?
- Basis in paper: [inferred] The coverage improvement principle requires "batch size exceeding a generalized coverage threshold" (Theorem 3.2, Eq.(6)), but the regime where this condition fails is uncharacterized.
- Why unresolved: The paper only proves linear convergence when n ≳ C_p*(R); behavior with smaller batches is not analyzed.
- What evidence would resolve it: Convergence rates for sub-threshold batch sizes, or a phase transition analysis showing failure modes.

## Limitations
- Theoretical analysis is confined to contextual bandit setting with Bradley-Terry preferences and linear softmax policies
- Computational burden of hybrid G-optimal design sampler may be prohibitive for large action spaces
- Does not address cost of oracle queries or trade-off with computation

## Confidence
- **High Confidence:** The exponential convergence advantage of on-policy over offline DPO (Mechanism 2) is supported by rigorous theorems (3.2, 3.3) and aligns with established bandit theory
- **Medium Confidence:** The practical effectiveness of the hybrid sampler (Algorithm 2) and the reward distillation methods (RDRC, PDFR) is demonstrated empirically, but the theory for these extensions is less developed
- **Medium Confidence:** The empirical superiority of on-policy methods over offline DPO in the TL;DR and chat experiments is demonstrated, but sample sizes and judge models introduce variability

## Next Checks
1. **Coverage Threshold Verification:** Systematically vary the batch size $n$ in the TL;DR experiment to empirically identify the threshold where on-policy convergence transitions from slow to exponential
2. **Reward Model Sensitivity:** Repeat the TL;DR experiment using a different reward model (e.g., ArmoRM instead of Pythia) to test the robustness of the on-policy convergence advantage
3. **Large-Scale Action Space Test:** Attempt a simplified implementation of Algorithm 2 on a toy problem to assess the feasibility of computing the preferential G-optimal design for high-dimensional action spaces