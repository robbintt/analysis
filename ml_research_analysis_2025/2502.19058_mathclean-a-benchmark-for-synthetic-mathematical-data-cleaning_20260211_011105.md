---
ver: rpa2
title: 'MathClean: A Benchmark for Synthetic Mathematical Data Cleaning'
arxiv_id: '2502.19058'
source_url: https://arxiv.org/abs/2502.19058
tags:
- error
- data
- answer
- mathematical
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MathClean, a benchmark for evaluating mathematical
  data cleaning models that addresses the challenge of ensuring correctness in synthetic
  math problems used for training large language models. The authors construct a dataset
  of 2,000 correct and 2,000 erroneous questions, plus 2,000 correct and erroneous
  answers derived from GSM8K and MATH, with detailed error-type annotations across
  four categories: expression errors, lack of conditions, contradictions, and unrealistic
  scenarios.'
---

# MathClean: A Benchmark for Synthetic Mathematical Data Cleaning

## Quick Facts
- arXiv ID: 2502.19058
- Source URL: https://arxiv.org/abs/2502.19058
- Reference count: 40
- Even state-of-the-art models achieve less than 80% accuracy on error detection and less than 70% on error type detection

## Executive Summary
MathClean introduces a benchmark for evaluating mathematical data cleaning models that ensures correctness in synthetic math problems used for training large language models. The authors construct a dataset of 2,000 correct and 2,000 erroneous questions, plus 2,000 correct and erroneous answers derived from GSM8K and MATH, with detailed error-type annotations across four categories: expression errors, lack of conditions, contradictions, and unrealistic scenarios. Comprehensive evaluations across 16 models (including GPT-o1, DeepSeek-R1, and open-source PRMs) show that even state-of-the-art models struggle significantly, achieving less than 80% accuracy on error detection and less than 70% on error type detection, highlighting MathClean as a challenging benchmark that reveals critical weaknesses in current mathematical reasoning models.

## Method Summary
MathClean employs Qwen2.5-72B-Instruct with 10 error augmentation prompts to systematically generate erroneous questions containing controlled, single-type errors from GSM8K and MATH seed data. The benchmark uses 16 diversity augmentation techniques to create novel problem formulations and prevent data contamination. Human annotators screen generated questions and answers, requiring >90% accuracy. Evaluation uses vLLM with temperature=0.7, top_p=0.95, and max_tokens=4000, measuring accuracy and F1 for error detection, and accuracy and macro-F1 for error type classification across 4 question error categories and 3 answer error categories.

## Key Results
- State-of-the-art models achieve less than 80% accuracy on error detection tasks
- Models score less than 70% on error type detection across all categories
- Performance drops significantly from error detection to error type classification, indicating shallow understanding of mathematical verification

## Why This Works (Mechanism)

### Mechanism 1: Targeted Error Augmentation via Structured Prompts
Systematic error injection produces diagnostic benchmark data that reveals model weaknesses in mathematical verification. Ten distinct error-type prompts guide Qwen2.5-72B-Instruct to generate questions containing controlled, single-type errors rather than random noise, creating a labeled taxonomy where each sample isolates a specific failure mode.

### Mechanism 2: Diversity Augmentation to Reduce Data Contamination
Sixteen question-rewriting strategies generate novel problem formulations that bypass benchmark memorization. Starting from GSM8K/MATH seed questions, the pipeline applies transformations to create syntactically and semantically distinct problems while preserving solvability, reducing contamination risk since models cannot rely on cached solutions.

### Mechanism 3: Hierarchical Task Structure (Detection → Classification)
Separating error detection (binary) from error-type classification (multi-class) reveals where models fail—spotting problems vs. understanding them. The benchmark evaluates two sequential tasks, with performance gaps between tasks indicating shallow error recognition without causal understanding.

## Foundational Learning

- **Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**: MathClean evaluates PRMs alongside general LLMs; understanding their step-level vs. outcome-level evaluation is essential for interpreting results. Given a multi-step math solution with correct final answer but flawed intermediate logic, would a PRM or ORM likely give a higher score?

- **Data Contamination in Benchmarks**: MathClean explicitly designs against contamination; recognizing how models can "cheat" via memorized training data helps contextualize why novel synthesis matters. If a model achieves 95% on MATH benchmark but was trained on MATH solutions, is this a valid capability signal?

- **Error Taxonomy in Mathematical Reasoning**: The 4-category (questions) and 3-category (answers) error classification scheme underpins the benchmark's diagnostic value; understanding these distinctions enables targeted model improvement. A problem asks for the number of students in groups of 32.5—what error category does this violate?

## Architecture Onboarding

- **Component map**: GSM8K/MATH seed data -> Qwen2.5-72B-Instruct with 10 error prompts -> 16 diversity rewriting techniques -> Model generation (CoT/PoT) -> Human annotation (>90% accuracy) -> vLLM evaluation harness

- **Critical path**: Seed selection → Error-type prompt design → Model generation → Human annotation & filtering → Decontamination → Benchmark release. Annotation quality is the bottleneck; budget for iterative review cycles.

- **Design tradeoffs**: Single-error-type constraint improves diagnostic clarity but reduces ecological validity. Human annotation ensures quality but limits scale (4,000 total samples). Temperature=0.7 balances diversity and consistency.

- **Failure signatures**: Models classifying correct questions as incorrect (over-sensitive) indicate instruction-following issues. High detection accuracy but random classification suggests surface-level pattern matching. PRMs underperforming general LLMs may indicate step-level training doesn't transfer to question-level validation.

- **First 3 experiments**: 1) Baseline sweep: Run all 16 evaluated models on both tasks with paper's exact prompts; reproduce reported numbers within ±2%. 2) Error ablation: Train/fine-tune a smaller model on subsets containing only specific error types; measure per-category generalization. 3) Contamination test: Check if any evaluated model's training corpus contains GSM8K/MATH test splits; compare performance on original vs. MathClean-rewritten versions.

## Open Questions the Paper Calls Out
None

## Limitations
- Single-error-type constraint improves diagnostic clarity but reduces ecological validity for real synthetic data containing multiple overlapping errors
- With only 4,000 total samples, the benchmark may lack statistical power for fine-grained analysis of rare error types
- Human annotation bottleneck creates scalability limits and potential subjectivity in error classification

## Confidence
- **High Confidence**: Core finding that state-of-the-art models struggle with error detection (<80%) and type classification (<70%) is well-supported by consistent performance drops across all evaluated models
- **Medium Confidence**: Claim that models fail to detect specific error types due to missing verification capabilities is plausible but requires further validation
- **Medium Confidence**: Assertion that 16 diversity augmentation techniques effectively reduce data contamination is reasonable but not empirically measured

## Next Checks
1. **Multi-Error Generation Test**: Generate a small subset of questions containing multiple error types and evaluate model performance to test whether single-error-type performance generalizes to realistic synthetic data.

2. **Annotator Agreement Analysis**: Measure inter-annotator agreement rates across all error types to quantify the subjectivity in classification and identify categories requiring definition refinement.

3. **Scaling Law Investigation**: Evaluate models of varying parameter counts (e.g., 1B, 7B, 70B) on MathClean to determine whether performance gaps persist across scales or if larger models show emergent error-detection capabilities.