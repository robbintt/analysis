---
ver: rpa2
title: 'GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation'
arxiv_id: '2505.12888'
source_url: https://arxiv.org/abs/2505.12888
tags:
- medical
- medication
- dialogue
- prompts
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dialogue-based medication
  recommendation in medical dialogue systems (MDS), where large language models (LLMs)
  often struggle to capture fine-grained medical information and may generate non-factual
  responses. The authors propose GAP (Graph-Assisted Prompts), a framework that constructs
  patient-centric graphs from dialogue to explicitly represent medical concepts and
  states.
---

# GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation

## Quick Facts
- arXiv ID: 2505.12888
- Source URL: https://arxiv.org/abs/2505.12888
- Reference count: 40
- Primary result: GAP achieves competitive medication recommendation accuracy on DialMed dataset by using graph-assisted prompts to capture fine-grained medical information

## Executive Summary
This paper addresses the challenge of dialogue-based medication recommendation in medical dialogue systems, where large language models often struggle to capture fine-grained medical information and may generate non-factual responses. The authors propose GAP (Graph-Assisted Prompts), a framework that constructs patient-centric graphs from dialogue to explicitly represent medical concepts and states. By integrating external medical knowledge graphs and generating context-relevant queries, GAP retrieves information from multiple sources to enhance response accuracy. Evaluated on a dialogue-based medication recommendation dataset and a dynamically diagnostic interviewing task, GAP demonstrates competitive performance compared to strong baselines, achieving improved accuracy in medication recommendations.

## Method Summary
GAP constructs patient-centric graphs from medical dialogues by extracting medical concepts (diseases, symptoms, medications) and their states (positive/negative, past history) using an LLM with instruction prompts. The extracted concepts are normalized and linked to external medical knowledge graphs (CMeKG, Disease-KB). The framework generates two types of prompts: neighborhood prompts from KG retrieval and path-based prompts derived from pre-defined schema paths that traverse the patient graph. These prompts are used to query multiple sources (KG, LLM, Internet) to retrieve relevant medical knowledge. The final response is generated by an LLM that takes the dialogue history, patient graph, and all retrieved prompts as input.

## Key Results
- GAP outperforms strong baselines on medication recommendation task with Jaccard and F1 metrics
- Ablation study shows neighborhood prompts contribute 19.13% F1 improvement and path-based prompts contribute 10.98% F1 improvement
- The framework demonstrates competitive performance on dynamically diagnostic interviewing task
- GAP achieves better safety by reducing hallucinations through multi-source knowledge retrieval

## Why This Works (Mechanism)

### Mechanism 1: Patient-Centric Graph Representation
Explicit patient-centric graph representation captures fine-grained medical information across dialogue turns that LLMs may otherwise overlook. The two-stage extraction process first identifies medical concepts from each utterance using LLMs with instruction prompts, then fills pre-defined state slots by extending to surrounding context. These are structured as a graph with a patient node connected to concept nodes via typed edges.

### Mechanism 2: External Knowledge Graph Integration
Neighborhood prompts retrieved from external medical knowledge graphs provide relevant, structured knowledge that improves medication accuracy and reduces hallucinations. Normalized concept nodes are linked to KG entities, forming a neighborhood graph. The system identifies the knowledge type needed for response, then retrieves top-k facts with matching relations to construct neighborhood prompts.

### Mechanism 3: Path-Based Compositional Queries
Path-based prompts derived from graph traversal enable compositional, patient-specific queries that retrieve critical safety knowledge from multiple sources. Medical professionals pre-define prominent schema paths. Paths matching the current patient graph are extracted and used as queries to KG verification, LLM self-prompting, and Internet search for missing/complex cases.

## Foundational Learning

- **Knowledge Graph Structure** (entities, relations, neighborhoods): The entire framework relies on constructing patient graphs and querying KG neighborhoods. You must understand directed triplets $(e_{head}, r, e_{tail})$ and neighborhood definitions $N_{G'}$. *Quick check*: Given a triple (aspirin, contraindicated_for, pregnancy), what is the neighborhood of the subgraph containing just the pregnancy node?

- **Retrieval-Augmented Generation (RAG)**: GAP extends RAG by using graph-derived queries rather than text-based retrieval. Understanding baseline RAG helps you see why graph-based queries improve specificity. *Quick check*: How does GAP's neighborhood prompt generation differ from standard document retrieval in vanilla RAG?

- **Slot-Filling / State Tracking in Dialogue**: The second extraction stage treats medical state identification as a slot-filling task with pre-defined slots (e.g., "past medical history: yes/no/unknown"). *Quick check*: For the utterance "I've had bronchitis before but it's gone now," what slot-values would the system extract for bronchitis?

## Architecture Onboarding

- **Component map**: Medical IE Module -> Graph Constructor -> Prompt Generator -> Multi-Source Retriever -> Response Generator
- **Critical path**: Extraction accuracy → Graph correctness → Prompt relevance → Recommendation safety
- **Design tradeoffs**: Graph complexity vs. retrieval efficiency; Multi-source retrieval vs. latency; Pre-defined schema vs. flexibility
- **Failure signatures**: 1) Extraction errors missing contraindicated conditions; 2) Normalization failures preventing KG linking; 3) Source conflicts between KG and Internet; 4) Path explosion causing context overflow
- **First 3 experiments**:
  1. Run extraction-only baseline: Verify the LLM correctly extracts concepts and states from sample dialogues. Manually check 20 cases against ground truth.
  2. Ablate single prompt types: Run with only NP, only PP, and neither. Verify F1 drops match paper (19.13% without NP, 10.98% without PP).
  3. Test on edge cases: Construct dialogues with contraindicated conditions. Verify KG verification and Internet retrieval catch conflicts before recommendation.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the framework against cascading errors when the initial LLM-based medical concept extraction misidentifies entities? The paper relies entirely on an LLM to extract concepts and slot-values before graph construction, meaning extraction errors would propagate directly to the final prompts. This is unresolved because the paper does not analyze sensitivity to noise in the upstream extraction phase.

### Open Question 2
How does the system resolve conflicts between static Knowledge Graph constraints and dynamic, potentially noisy Internet search results? Section 3.4 integrates KG Verification and Internet Access into path-based prompts but provides no mechanism for handling contradictory information between these heterogeneous sources.

### Open Question 3
Can the retrieval mechanism be optimized to close the performance gap with supervised models on fine-grained Jaccard scores? While GAP outperforms other LLM methods, it still trails the supervised DDN model on the "All Data" Jaccard score (39.28 vs. 42.62). The authors demonstrate competitive performance but leave the gap with supervised state-of-the-art methods as an implicit challenge.

## Limitations

- The exact medical entity extraction prompts and slot-filling templates are only partially shown, making faithful reproduction difficult
- Internet search integration method and engine are unspecified, creating potential reproducibility issues
- The framework assumes medical professionals can pre-define appropriate path schemas, which may not generalize across diverse clinical domains
- Performance is heavily dependent on the quality and coverage of external medical knowledge graphs that may not be publicly accessible

## Confidence

- **High Confidence**: The core claim that explicit graph representation captures fine-grained medical information and improves recommendation accuracy is well-supported by ablation studies
- **Medium Confidence**: The effectiveness of multi-source retrieval for reducing hallucinations is demonstrated, but implementation details and conflict resolution strategies are underspecified
- **Low Confidence**: Claims about generalizability to new medical domains or different foundation LLMs are not empirically tested

## Next Checks

1. **Extraction and Normalization Accuracy**: Manually validate the medical entity extraction and slot-filling on 20 sample dialogues against ground truth. Check for missed concepts, misclassified states, and normalization failures.

2. **Ablation Study Replication**: Replicate the ablation experiments by running with only neighborhood prompts, only path-based prompts, and neither. Verify the reported F1 drops and analyze which specific prompts are most critical for safety-critical cases.

3. **Edge Case Safety Testing**: Construct synthetic dialogues with known contraindications. Verify that KG verification and Internet retrieval correctly identify conflicts and prevent unsafe recommendations, checking for both false positives and negatives.