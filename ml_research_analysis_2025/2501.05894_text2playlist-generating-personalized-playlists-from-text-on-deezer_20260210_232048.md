---
ver: rpa2
title: 'Text2Playlist: Generating Personalized Playlists from Text on Deezer'
arxiv_id: '2501.05894'
source_url: https://arxiv.org/abs/2501.05894
tags:
- music
- deezer
- search
- text2playlist
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Text2Playlist, a system for generating personalized
  playlists from text queries on the Deezer music streaming platform. The system addresses
  the limitation of traditional search engines that are optimized for specific item
  retrieval rather than exploratory queries.
---

# Text2Playlist: Generating Personalized Playlists from Text on Deezer

## Quick Facts
- arXiv ID: 2501.05894
- Source URL: https://arxiv.org/abs/2501.05894
- Reference count: 40
- Primary result: 45% listen rate for AI-generated playlists vs 27% for manual playlists

## Executive Summary
Text2Playlist is a system deployed on Deezer that generates personalized music playlists from natural language text queries. Unlike traditional search engines optimized for specific item retrieval, Text2Playlist addresses exploratory queries like "Chill vibes on a rainy afternoon" by leveraging Large Language Models to extract both explicit and implicit tags, then retrieving and personalizing tracks using collaborative filtering and final LLM refinement. The system achieved a 45% listen rate compared to 27% for manual playlists in a 20% premium user rollout, with "Chill" and "Party" moods representing nearly half of all requests.

## Method Summary
The system implements a three-stage pipeline: (1) an LLM extracts explicit tags (e.g., "90s" decade) and infers implicit preferences (e.g., "Focus" mood for work context) from user queries; (2) Elasticsearch retrieves matching tracks using pre-existing catalog metadata, then re-ranks them by cosine similarity between user and track embeddings computed via collaborative filtering; (3) a second LLM pass refines the final track order based on query alignment and diversity rules (artist diversity, overall quality). The architecture uses Python on Kubernetes, Gemini Flash 1.5 for LLM processing, Cassandra for data storage, and a Golang + Faiss service for affinity scoring.

## Key Results
- 45% listen rate for generated playlists vs 27% for manual playlists
- "Chill" and "Party" moods represent nearly half of all requests
- System deployed to 20% of premium users on Deezer platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can extract both explicit and implicitly inferred tags from natural language queries to improve music retrieval.
- Mechanism: The LLM parses user text to identify explicitly stated constraints (e.g., "90s" decade) while inferring implicit preferences from context (e.g., "Focus" mood for a work scenario). These tags are then matched against Deezer's pre-existing catalog metadata from expert annotations and audio analysis models.
- Core assumption: Users articulate needs in natural language that maps reliably to structured tag categories, and LLM inference captures unstated preferences accurately.
- Evidence anchors:
  - [abstract] "Text2Playlist leverages generative AI, music information retrieval and recommendation systems"
  - [section 3.1] "For example, in the query 'I want music from the 90s for work', we can extract the explicit decade tag '90s'... but we can also infer they may prefer 'Focus' mood tracks"
  - [corpus] Related work on cross-modal music similarity (CrossMuSim) suggests text-to-music mapping is tractable, but corpus lacks direct validation of implicit tag inference accuracy.
- Break condition: If implicit tag inference frequently misinterprets user intent (e.g., inferring "Party" when user meant "Relaxed"), playlist relevance degrades and user trust erodes.

### Mechanism 2
- Claim: Reordering retrieved tracks by user-track embedding similarity increases engagement by prioritizing personally relevant content.
- Mechanism: Collaborative filtering models learn low-dimensional embedding vectors for users and tracks from usage data. After initial tag-based retrieval, cosine similarity between the target user's embedding and candidate track embeddings reorders the list—highest affinity tracks appear first.
- Core assumption: Historical listening patterns predict preferences even for exploratory, mood-based queries that may differ from typical consumption.
- Evidence anchors:
  - [abstract] "applies collaborative filtering for personalization"
  - [section 3.2] "By analyzing usage data on Deezer, they learn low-dimensional embedding vector representations... Then, they offer recommendations based on embedding similarity metrics"
  - [corpus] Content filtering review (arXiv:2507.02282) confirms CF is standard for music recommendation, though notes cold-start limitations.
- Break condition: If users seek discovery outside their historical patterns (e.g., exploring a new genre), CF reordering may over-narrow results, reducing serendipity.

### Mechanism 3
- Claim: A second LLM pass over retrieved tracks improves final playlist quality by enforcing semantic coherence and diversity rules.
- Mechanism: Inspired by two-stage recommender systems and RAG, track metadata and tags are converted to unstructured text. The LLM then re-prioritizes tracks based on query alignment and explicit rules (artist diversity, overall quality), producing the final ordered list.
- Core assumption: LLMs can make nuanced semantic judgments about track-query fit that retrieval scoring and CF similarity cannot capture alone.
- Evidence anchors:
  - [abstract] "refines the track list using another LLM pass"
  - [section 3.3] "Additional rules such as artists diversity and overall quality of the playlist are also mentioned in the LLM prompt to optimize the final user experience"
  - [corpus] Weak direct evidence; corpus lacks comparative studies of LLM-based playlist refinement vs. rule-based re-ranking.
- Break condition: If LLM refinement introduces latency without measurable quality gain, or if "quality" judgments conflict with user taste, the added complexity becomes unjustified.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The system grounds LLM output in actual catalog tracks rather than hallucinating music, ensuring playlists are playable.
  - Quick check question: Can you explain why retrieving tracks first and then having an LLM reorder them is safer than asking an LLM to generate a playlist directly?

- Concept: **Collaborative Filtering Embeddings**
  - Why needed here: Personalization depends on representing users and tracks in a shared vector space where proximity reflects preference compatibility.
  - Quick check question: If a new user has no listening history, what happens to the CF-based reordering step? (Hint: cold-start problem.)

- Concept: **Two-Stage Recommendation Architecture**
  - Why needed here: The system separates candidate generation (tag-based retrieval) from ranking (CF + LLM refinement), a pattern common in large-scale recommenders.
  - Quick check question: Why might you want the retrieval stage to over-fetch candidates even though the refinement stage will discard most of them?

## Architecture Onboarding

- Component map: User query -> LLM tag extraction -> Elasticsearch retrieval -> CF embedding similarity reordering -> LLM refinement -> Final playlist
- Critical path: User query → LLM tag extraction (explicit + implicit) → Elasticsearch retrieval → CF embedding similarity reordering → LLM refinement with diversity rules → Final playlist JSON
- Design tradeoffs:
  - **Cost vs. flexibility**: Gemini Flash pay-per-token model allows cost-effective gradual rollout (no idle costs), but may become expensive at full scale
  - **Latency vs. quality**: Multi-stage pipeline (LLM → retrieval → CF → LLM) adds latency; acceptable for playlist generation but would fail real-time search
  - **Standalone deployment**: Kubernetes isolation limits blast radius but requires separate data sync (daily Cassandra exports)
- Failure signatures:
  - **Empty retrieval**: Tags extracted are too specific or don't match catalog vocabulary (e.g., "lo-fi hip hop" vs. catalog tag "Lo-Fi")
  - **Cold-start collapse**: New users get meaningless CF ordering; fallback required
  - **LLM drift**: Refinement step ignores diversity rules or hallucinates track descriptions
  - **Latency spike**: LLM API throttling or Cassandra read timeouts under load
- First 3 experiments:
  1. **A/B test tag extraction accuracy**: Compare LLM-extracted tags vs. human-labeled ground truth on a sample of queries; measure precision/recall for explicit vs. implicit tags separately.
  2. **Ablate the CF reordering stage**: Serve playlists with random order vs. CF-ordered to isolate the engagement lift from personalization (proxy: skip rate on first 3 tracks).
  3. **Measure LLM refinement impact**: Compare listen rates for playlists with vs. without the second LLM pass; if lift is minimal, consider replacing with deterministic diversity rules.

## Open Questions the Paper Calls Out

- Can integrating lyrics-based analysis effectively increase the coverage and diversity of music tags beyond current expert and audio-based methods?
  - Basis in paper: [explicit] The conclusion asks, "could we use LLM or lyrics to enrich even more music representation?"
  - Why unresolved: The current system relies on existing metadata; the utility of generative or lyric-based tag expansion is proposed but not yet implemented or tested.
  - What evidence would resolve it: A comparative study measuring tag diversity and retrieval recall between the current system and a lyrics-augmented model.

- Can the system be refactored into a conversational tool to better handle user query reformulations?
  - Basis in paper: [explicit] Section 5 notes that "users often need to reformulate queries" and suggests refactoring Text2Playlist into a "conversational tool."
  - Why unresolved: The current implementation is a single-shot prompt; the feasibility and efficacy of a multi-turn dialogue for playlist refinement are unexplored.
  - What evidence would resolve it: User satisfaction metrics and success rates from a conversational prototype compared to the existing single-query interface.

- Does the final LLM-based refinement step yield significantly higher engagement than the initial Collaborative Filtering (CF) ranking alone?
  - Basis in paper: [inferred] Section 3.3 claims the refinement optimizes the user experience, but Section 4 reports only aggregate listen rates without an ablation study on the refinement layer.
  - Why unresolved: It is unclear if the latency and cost of the second LLM pass justify the specific value it adds over the personalized CF ordering.
  - What evidence would resolve it: A/B testing the full pipeline against a version omitting the final LLM refinement to isolate its impact on listen rates.

## Limitations
- LLM prompt design uncertainty: Exact prompts for tag extraction and refinement are undisclosed, significantly impacting output quality
- Implicit tag inference reliability: Limited validation of false positive rates for inferred preferences
- CF cold-start handling: No clear strategy for new users with limited interaction data

## Confidence
- 45% listen rate vs 27% baseline: High confidence
- LLM can reliably extract implicit tags: Medium confidence
- CF reordering improves engagement: Medium confidence

## Next Checks
1. **Implicit tag inference validation**: Run a controlled experiment where human annotators label both explicit and implicit tags in a sample of queries, then compare LLM-extracted tags against ground truth. Measure precision/recall separately for explicit and implicit tag categories to quantify inference reliability.

2. **CF cold-start fallback evaluation**: Deploy A/B testing that compares playlists for new users with (a) no CF reordering, (b) popularity-based fallback, and (c) minimal interaction-based CF. Measure skip rates and completion rates to identify optimal cold-start strategy.

3. **LLM refinement necessity test**: Implement an ablation study serving playlists with three variants: (a) retrieval-only, (b) retrieval + CF reordering, (c) full pipeline with LLM refinement. Compare listen rates and diversity metrics (artist repetition, genre spread) to isolate the incremental value of the refinement stage.