---
ver: rpa2
title: 'FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated
  Videos'
arxiv_id: '2504.10358'
source_url: https://arxiv.org/abs/2504.10358
tags:
- reasoning
- video
- arxiv
- entity-level
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FingER, a novel entity-level reasoning evaluation
  framework for AI-generated videos that addresses the challenge of assessing localized
  defects in increasingly sophisticated video generation models. The core innovation
  lies in decomposing overall video quality assessment into fine-grained entity-level
  questions across five dimensions (visual quality, text-to-video alignment, temporal
  consistency, factual consistency, and dynamic degree), which are then answered by
  a reasoning model.
---

# FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos

## Quick Facts
- arXiv ID: 2504.10358
- Source URL: https://arxiv.org/abs/2504.10358
- Reference count: 40
- Key outcome: Introduces entity-level reasoning evaluation framework achieving SOTA performance with 11.8% and 5.5% relative margins on GenAI-Bench and MonetBench using only one-tenth of training samples.

## Executive Summary
This paper introduces FingER, a novel entity-level reasoning evaluation framework for AI-generated videos that addresses the challenge of assessing localized defects in increasingly sophisticated video generation models. The core innovation lies in decomposing overall video quality assessment into fine-grained entity-level questions across five dimensions (visual quality, text-to-video alignment, temporal consistency, factual consistency, and dynamic degree), which are then answered by a reasoning model. The authors construct a high-quality dataset (FingER-Instruct-60k) containing 3.3k videos and 60k QA annotations with detailed reasons, and explore various training protocols including supervised fine-tuning and reinforcement learning. Their approach achieves state-of-the-art performance on public benchmarks (GenAI-Bench and MonetBench) with a relative margin of 11.8% and 5.5% respectively, using only one-tenth of the training samples compared to existing methods, demonstrating superior generalization capability for evaluating AI-generated videos.

## Method Summary
FingER is an entity-level video quality assessment framework that decomposes evaluation into structured QA pairs across five dimensions. The method uses GPT-4o to extract entities from text prompts and generate fine-grained questions, then employs a reasoning model (Qwen2.5-VL-7B fine-tuned) to answer these questions with detailed explanations. The framework employs a cold-start supervised fine-tuning approach followed by Group Relative Policy Optimization (GRPO) training. Scores are aggregated hierarchically from binary answers using token probability aggregation. The approach is trained on the FingER-Instruct-60k dataset with 3.3k videos and 60k QA pairs with verified reasoning annotations.

## Key Results
- Achieves SOTA performance on GenAI-Bench (11.8% relative margin) and MonetBench (5.5% relative margin)
- Entity-level evaluation achieves 81.23 SRCC vs. 30.68 SRCC for overall-level evaluation on FingER-test
- Training with explicit reasoning chains improves text alignment accuracy from 82.77% to 86.79% and factual consistency from 72.89% to 74.03%
- GRPO with cold-start initialization achieves 57.03 SRCC on GenAI-Bench vs. 56.68 SRCC for +Reason alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-level question decomposition improves evaluation accuracy over holistic scoring.
- Mechanism: Breaking complex video quality assessment into focused entity-specific questions (e.g., "Is the African girl's gait consistent?") reduces cognitive load on the model, allowing more reliable binary judgments that aggregate into robust scores.
- Core assumption: Models perform better on simpler, localized judgments than on complex holistic evaluations requiring simultaneous multi-aspect reasoning.
- Evidence anchors:
  - [abstract] "entity-level questions... focus on some specific entities... making answering or scoring much easier by MLLMs"
  - [Table 1] Entity-level evaluation achieves 81.23 SRCC vs. 30.68 for overall-level on FingER-test
  - [corpus] GRADEO paper confirms multi-step reasoning improves text-to-video evaluation, supporting decomposition approach
- Break condition: If entity-level questions become too numerous or granular, aggregation noise could outweigh individual accuracy gains.

### Mechanism 2
- Claim: Training with explicit reasoning chains enhances performance on dimensions requiring deep understanding.
- Mechanism: Supervised fine-tuning on answer+reason pairs teaches the model structured analytical thinking, improving predictions on complex dimensions (factual consistency, temporal consistency, text alignment) that require causal reasoning rather than surface pattern matching.
- Core assumption: Reasoning capability can be transferred from training data to held-out evaluation scenarios through supervised learning.
- Evidence anchors:
  - [abstract] "construct a FingER dataset... 60k fine-grained QA annotations, each with detailed reasons"
  - [Table 2] +Reason training improves text alignment Acc from 82.77% to 86.79%, factual consistency from 72.89% to 74.03%
  - [corpus] VLM-R³ paper demonstrates iterative visual reasoning refinement supports chain-of-thought in multimodal tasks
- Break condition: If reasoning annotations contain systematic errors or domain mismatches, models may learn spurious reasoning patterns.

### Mechanism 3
- Claim: GRPO with cold-start initialization outperforms pure reinforcement learning for reasoning tasks.
- Mechanism: Cold-start SFT provides a stable policy initialization with basic reasoning structure; GRPO then optimizes this policy using group-relative advantages, rewarding correct answers with valid reasoning formats while avoiding collapse into caption-style outputs.
- Core assumption: RL can refine but not instill reasoning capabilities from scratch in this domain.
- Evidence anchors:
  - [abstract] "reasoning model trained using Group Relative Policy Optimization (GRPO) with a cold-start strategy achieves the best performance"
  - [Section 3.3.3] "Zero-GRPO generates reasons that resemble captions rather than logical reasoning"
  - [Table 3] +GRPO achieves 57.03 SRCC on GenAI-Bench vs. 56.68 for +Reason alone
  - [corpus] No direct corpus evidence for GRPO specifically in video evaluation; related work focuses on DPO/RLHF
- Break condition: If reward signals (accuracy + format) are insufficiently discriminative or cold-start model is already near-optimal, GRPO provides marginal gains.

## Foundational Learning

- Concept: **Question Generation/Answering (QG/A) Framework**
  - Why needed here: FingER builds on DSG's approach of decomposing evaluation into structured QA pairs rather than direct scoring.
  - Quick check question: Can you explain why asking "Is the handshake natural?" is easier to evaluate than "Rate temporal consistency from 1-4"?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core training method for reasoning model; differs from standard PPO by computing advantages relative to group samples rather than individual baselines.
  - Quick check question: How does GRPO's group-based advantage calculation differ from single-sample advantage estimation in standard RL?

- Concept: **Token Probability Aggregation for Scoring**
  - Why needed here: FingER converts model logits over Yes/No token sets into calibrated quality scores via softmax and weighted summation.
  - Quick check question: Why aggregate probability over multiple Yes/No token variants rather than using a single token's logit?

## Architecture Onboarding

- Component map:
  - **Question Generator** (LLM-based): Extracts entities from prompt → generates 5-dimension entity-level questions
  - **Reasoning Model** (Qwen2.5-VL-7B fine-tuned): Takes video + questions → outputs `<reason>` + `<answer>` tags
  - **Score Aggregator**: Converts answer token probabilities → dimension scores → weighted overall score
  - **Training Pipeline**: SFT with reasoning → GRPO fine-tuning (accuracy + format rewards)

- Critical path:
  1. Entity extraction from text prompt (GPT-4o)
  2. Question generation per dimension (template + ICL examples)
  3. Human annotation of Yes/No answers
  4. MLLM reason generation with answer provided (human verification required)
  5. Cold-start SFT on answer+reason pairs (2 epochs)
  6. GRPO training (2k steps, group size 16)

- Design tradeoffs:
  - Resolution/fps vs. compute: 448×448 @ 2fps chosen; higher settings give marginal gains (Fig. 3)
  - Dataset size vs. quality: 3.3k videos with verified reasoning outperforms 10x larger unverified datasets
  - Zero-GRPO vs. cold-start: Zero-GRPO fails (caption-style outputs); cold-start essential for reasoning

- Failure signatures:
  - Zero-GRPO produces descriptive captions instead of analytical reasoning
  - Base model (no fine-tuning) achieves only 26.1% accuracy on visual quality dimension
  - GPT-4o misidentifies deformed hands, assigns high scores with misleading explanations (Fig. 1)
  - Entity-level without probability calculation shows inconsistent dimension correlations

- First 3 experiments:
  1. **Baseline sanity check**: Run Qwen2.5-VL zero-shot on FingER-test across all 5 dimensions with entity-level questions; expect 40-80% accuracy range (Table 1 pattern)
  2. **SFT comparison**: Train separate models with (a) answers only, (b) answers+reasoning; compare on factual consistency dimension to isolate reasoning benefit
  3. **GRPO ablation**: From same SFT checkpoint, run (a) Zero-GRPO, (b) GRPO with cold-start; verify Zero-GRPO produces caption-style reasoning while cold-start maintains analytical structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Zero-GRPO (reinforcement learning without cold-start supervision) fail to produce logical reasoning for video quality assessment, instead generating caption-like outputs?
- Basis in paper: [explicit] Page 7 states: "Zero-GRPO generates reasons that resemble captions rather than logical reasoning... the issue stems from the reasoning component."
- Why unresolved: The authors identify the failure but do not provide theoretical or empirical analysis of why pure RL fails to induce reasoning without supervised initialization.
- What evidence would resolve it: Ablation studies varying the amount/type of cold-start data, or analysis of the policy gradient dynamics during Zero-GRPO training to identify where learning diverges.

### Open Question 2
- Question: What are the fundamental mechanisms underlying the domain gap between AI-generated videos and natural videos that causes state-of-the-art MLLMs (including GPT-4o) to misidentify localized defects?
- Basis in paper: [explicit] Page 2 states: "This issue is primarily attributed to a domain gap between the training data used by MLLMs and the unique features of AI-generated videos. In essence, AI-generated videos can deceive MLLMs in certain latent feature spaces."
- Why unresolved: The paper identifies the gap and its symptoms but does not characterize the specific features or latent representations that cause the deception.
- What evidence would resolve it: Probing experiments on MLLM internal representations comparing AI-generated vs. natural videos, or feature attribution analysis to identify which visual features are misinterpreted.

### Open Question 3
- Question: How does the performance of the FingER framework scale with the size and diversity of the training dataset, given its current success with only 3.3k videos?
- Basis in paper: [inferred] The paper emphasizes achieving SOTA with "only one-tenth of the training samples" but does not investigate whether additional data could yield further improvements or what the minimum effective dataset size is.
- Why unresolved: The data efficiency is presented as a strength, but the scaling behavior and potential ceiling effects remain unexplored.
- What evidence would resolve it: Systematic experiments varying training set size (e.g., 1k, 3k, 10k, 30k videos) and measuring performance curves on both FingER-test and public benchmarks.

### Open Question 4
- Question: Can the entity-level reasoning framework generalize to other video understanding tasks beyond AI-generated video quality assessment, such as action recognition or video captioning?
- Basis in paper: [inferred] The framework decomposes video understanding into entity-level questions across multiple dimensions, which could theoretically apply to broader video tasks, but this is not tested.
- Why unresolved: The paper focuses exclusively on T2V quality evaluation; the transferability of the question generation and reasoning paradigm remains unexamined.
- What evidence would resolve it: Applying the FingER framework (with appropriate dimension/question modifications) to established video understanding benchmarks and comparing against task-specific baselines.

## Limitations
- The evaluation framework may not capture higher-level semantic failures or emergent properties from interactions between multiple entities
- Generalization claims are primarily validated on text-to-video generation tasks with limited testing on other video generation paradigms
- Dataset construction involves human verification of AI-generated reasoning, introducing potential subjectivity and scalability concerns

## Confidence

**High Confidence:** The entity-level evaluation mechanism is well-supported by quantitative evidence showing consistent improvements across all five dimensions (Table 1: 81.23 SRCC for entity-level vs. 30.68 for overall-level). The cold-start training strategy for GRPO is empirically validated through direct comparison showing Zero-GRPO produces caption-style outputs while cold-start maintains analytical reasoning structure.

**Medium Confidence:** The reasoning chain benefit shows measurable improvements (Table 2: 86.79% vs. 82.77% for text alignment accuracy) but the magnitude of gains varies across dimensions. The GRPO optimization claims are supported by results but lack direct comparison with alternative RL methods beyond Zero-GRPO, and the specific hyperparameters are not extensively ablated.

**Low Confidence:** The claim of superior generalization with one-tenth the training samples lacks direct empirical comparison with specific baselines. The long-term stability and consistency of the evaluation framework across different model generations and domains remains unproven.

## Next Checks
1. **Cross-domain robustness test**: Evaluate FingER on non-text-to-video generation tasks (e.g., image-to-video, video-to-video) to assess framework generalizability beyond the studied domain.
2. **Temporal consistency stress test**: Create synthetic videos with known temporal inconsistencies at varying granularities to validate whether entity-level decomposition accurately localizes and scores temporal defects compared to holistic approaches.
3. **Scalability validation**: Measure human annotation time and quality consistency across increasing dataset sizes (e.g., 10k, 50k, 100k samples) to identify practical limits of the current verification process and potential automation opportunities.