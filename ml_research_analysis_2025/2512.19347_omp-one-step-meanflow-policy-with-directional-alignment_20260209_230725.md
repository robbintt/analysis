---
ver: rpa2
title: 'OMP: One-step Meanflow Policy with Directional Alignment'
arxiv_id: '2512.19347'
source_url: https://arxiv.org/abs/2512.19347
tags:
- arxiv
- policy
- learning
- meanflow
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OMP resolves critical pathologies in applying MeanFlow to robotics:
  spectral bias that filters high-frequency dynamics, gradient starvation that prevents
  directional learning in low-velocity regimes, and prohibitive memory complexity
  from exact Jacobian computations. The framework introduces directional alignment
  to decouple velocity magnitude from direction, ensuring precise control even for
  fine manipulations, and implements a differential derivation equation to approximate
  Jacobian-vector products with reduced memory overhead.'
---

# OMP: One-step Meanflow Policy with Directional Alignment

## Quick Facts
- arXiv ID: 2512.19347
- Source URL: https://arxiv.org/abs/2512.19347
- Reference count: 25
- OMP achieves 82.3% average success rate, outperforming state-of-the-art MP1 by 3.4% and FlowPolicy by 10.7%

## Executive Summary
OMP addresses critical pathologies in applying MeanFlow to robotics: spectral bias that filters high-frequency dynamics, gradient starvation that prevents directional learning in low-velocity regimes, and prohibitive memory complexity from exact Jacobian computations. The framework introduces directional alignment to decouple velocity magnitude from direction, ensuring precise control even for fine manipulations, and implements a differential derivation equation to approximate Jacobian-vector products with reduced memory overhead. Experiments on Adroit, Meta-World, and real-world tasks demonstrate OMP achieves 82.3% average success rate, outperforming state-of-the-art MP1 by 3.4% and FlowPolicy by 10.7%, while maintaining single-step inference speed.

## Method Summary
OMP builds on the MeanFlow paradigm, modeling interval-averaged velocity u(z_t,r,t) rather than instantaneous velocity to enable single-step inference. The framework introduces directional alignment loss to decouple velocity magnitude from direction, preventing gradient starvation in low-velocity regimes. It also implements a differential derivation equation (DDE) to approximate Jacobian-vector products, reducing memory overhead from ~26.71GB to ~19.19GB. The composite loss combines MSE reconstruction, dispersive regularization, and directional alignment to balance coarse and fine control across different motion regimes. OMP is trained on point cloud inputs (512-1024 points) using a 3D encoder and evaluated on robot manipulation tasks with NFE=1 inference.

## Key Results
- Achieves 82.3% average success rate across Adroit, Meta-World, and real-world tasks
- Outperforms MP1 by 3.4% and FlowPolicy by 10.7% in success rate
- Reduces memory usage by ~28% through DDE approximation (19.19GB vs 26.71GB)
- Directional alignment proves essential for precision tasks (Pen: 58%→48% drop when removed)

## Why This Works (Mechanism)

### Mechanism 1: Directional Alignment Addresses Gradient Starvation and Spectral Bias
Decoupling direction from magnitude via cosine similarity enables robust learning in low-velocity regimes and circumvents spectral bias. The Directional Alignment loss (L_DA = -log((cosα+1)/2)) enforces alignment between predicted velocity u and true mean velocity v₀ independent of magnitude. When v₀ → 0, angular gradient remains non-zero (∂L_DA/∂α ∝ sinα), preventing the collapse to stationary policy that MSE causes. Direct alignment with v₀ bypasses the low-pass filtering effect of the MeanFlow integral target.

### Mechanism 2: Differential Derivation Equation (DDE) Reduces Memory Complexity
Finite-difference approximation of time derivatives decouples forward/backward passes, enabling training of larger models on standard hardware. The MeanFlow Identity requires computing du/dt = ∂u/∂t + ∇z_u · dz/dt (JVP). Exact JVP needs forward-mode AD nested inside reverse-mode AD, storing primal activations X, tangent vectors δX, and adjoint gradients. DDE replaces this with: du/dt ≈ [u(t+ε) - u(t-ε)]/(2ε), requiring only two forward passes without storing intermediate Jacobian graphs.

### Mechanism 3: Composite Loss Balances Coarse and Fine Control
Combining MSE reconstruction, dispersive regularization, and directional alignment creates complementary supervision across different motion regimes. L = L_mse + λ_Disp·L_Disp + λ_DA·L_DA handles three regimes: (1) ballistic transit where L_mse dominates with high-velocity targets, (2) latent space discrimination where L_Disp prevents mode collapse, (3) fine contact manipulation where L_DA provides directional supervision despite low magnitudes.

## Foundational Learning

- **Concept: MeanFlow Paradigm (Average Velocity vs Instantaneous Velocity)**
  - Why needed here: OMP builds directly on MeanFlow's core innovation—modeling interval-averaged velocity u(z_t,r,t) rather than instantaneous velocity v(z_t,t). Understanding that u = (1/(t-r))∫v(τ)dτ enables single-step inference by avoiding ODE solvers is essential.
  - Quick check question: Given a trajectory x(t) with instantaneous velocity v(t) = ẋ(t), what is the average velocity u(t) over interval [0,t]? Answer: u(t) = (x(t)-x(0))/t = (1/t)∫₀ᵗ v(τ)dτ

- **Concept: Spectral Bias in Neural Networks**
  - Why needed here: The paper's core theoretical contribution identifies that MeanFlow's averaging operator acts as a low-pass filter with 1/ω² power decay, attenuating high-frequency dynamics needed for precision control. Understanding frequency-domain neural network behavior explains why standard MeanFlow struggles with rapid adjustments.
  - Quick check question: If an instantaneous velocity signal v(t) has power spectral density S_v(ω), what is the PSD of its time-averaged version u(t)? Answer: S_u(ω) ∝ S_v(ω)/ω²

- **Concept: Automatic Differentiation Modes and Memory Trade-offs**
  - Why needed here: The memory bottleneck stems from computing gradients of JVP operations, which requires nesting forward-mode AD (for JVP) inside reverse-mode AD (for loss gradient). Understanding why this doubles activation storage (M_JVP ≈ 2·M_std) clarifies the DDE motivation.
  - Quick check question: Why does computing ∇_θ(∇z_u · v) require storing more intermediate values than standard backpropagation? Answer: Reverse-mode AD over a forward-mode AD graph requires storing both primal activations (for backward pass) and tangent vectors (for inner JVP), roughly doubling memory.

## Architecture Onboarding

- **Component map:**
  Point Cloud Input (512-1024 points, FPS sampled) -> 3D Encoder (PointNet++ or Transformer variant) -> Noise State z_T ~ N(0,I) + Context c -> MeanFlow Policy Network u_θ(z_t, r, t|c) [ predicts interval-averaged velocity ] -> Loss Computation: L_mse + λ_Disp·L_Disp + λ_DA·L_DA -> [Training] DDE approximation for du/dt OR [Exact] JVP computation -> [Inference] Single-step: action = z_T - t·u_θ(z_T, r, T|c)

- **Critical path:**
  1. Data loading: Sample expert demonstrations, extract point clouds, apply FPS
  2. Forward pass: Encode point cloud → compute u_θ at sampled timesteps (t, r, t±ε for DDE)
  3. Target construction: Compute MeanFlow target u_tgt using Eq. 4, with du/dt via DDE (Eq. 14) or exact JVP
  4. Loss computation: Evaluate composite L = L_mse + λ_Disp·L_Disp + λ_DA·L_DA
  5. Backward pass: Standard gradient computation (DDE) or nested AD (JVP)
  6. Evaluation: NFE=1 inference, success rate over 20 trials per checkpoint

- **Design tradeoffs:**
  - **JVP vs DDE:** JVP provides exact gradients but doubles memory (Table 3: 26.71GB vs 19.19GB for Horizon=16); DDE sacrifices ~1.5% average success (82.3% → 80.8%) for ~28% memory reduction. Choose DDE for large point clouds or long horizons; choose JVP when GPU memory permits and maximum precision is required.
  - **Point cloud size:** 512 points suffice for Adroit (simple objects); 1024 needed for Meta-World/real-world (complex scenes). Larger point clouds increase memory linearly with N.
  - **Horizon length:** Short horizons (Hp=4) favor JVP's precision; long horizons (Hp=16) benefit from DDE's stability against compounding errors (Table 5: DDE outperforms JVP on Pen task at Long setting: 70% vs 64%).

- **Failure signatures:**
  - **Removing L_DA:** Sharp decline in precision tasks (Pen: 58%→48%), high variance in training curves, convergence to near-stationary policies for fine manipulation
  - **Using only L_mse:** Gradient starvation in low-velocity regimes causes directional drift; angular error plateaus while magnitude collapses
  - **Exact JVP without sufficient memory:** OOM errors with batch size 128; forced reduction to smaller batches degrades training stability
  - **DDE with inappropriate ε:** Too large ε causes gradient error (performance degradation >3%); too small ε causes numerical instability

- **First 3 experiments:**
  1. **Reproduce ablation study (Table 1):** Train OMP variants (-L_DA, -L_Disp, full) on 3 Meta-World tasks (1 Easy, 1 Hard, 1 Very Hard) for 1000 epochs each. Verify that L_DA removal causes >5% drop on precision-heavy tasks. Expected: directional alignment proves essential for complex manipulation.
  2. **Validate memory-accuracy tradeoff (Tables 3, 5):** Compare OMP-JVP vs OMP-DDE on Place Bottle (real-world) with Horizon=16. Measure GPU memory and success rate. Expected: DDE saves ~7GB memory with <2% success degradation.
  3. **Test gradient starvation hypothesis (Figure 2b replication):** Create synthetic 2D regression task with targets at ρ*=1.0 and ρ*=0.01. Compare MSE vs L_DA optimization trajectories. Expected: MSE collapses magnitude before correcting direction for low-velocity targets; L_DA maintains angular gradient throughout.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DDE approximation error be systematically reduced through adaptive perturbation scheduling or higher-order finite difference schemes, rather than fixed ε?
- Basis in paper: [explicit] The authors note that DDE introduces "calculation errors, resulting in a moderate performance drop" (82.3% → 80.8%), and the performance gap widens on complex tasks (77.8% vs 70.6% on Very Hard tasks).
- Why unresolved: The paper uses a fixed perturbation constant ε but does not investigate whether task-adaptive or iteration-adaptive ε values could narrow the accuracy gap with exact JVP.
- What evidence would resolve it: Ablation studies varying ε across task complexity levels, or implementing higher-order finite difference approximations and comparing accuracy/memory trade-offs.

### Open Question 2
- Question: How does OMP scale to high-fidelity point clouds (>10K points) or high-resolution images that are necessary for industrial manipulation?
- Basis in paper: [explicit] The authors state: "In real-world applications requiring high-fidelity point clouds or high-resolution images, the memory overhead of the standard JVP would likely become prohibitive, making the DDE approximation a crucial implementation strategy."
- Why unresolved: Experiments only tested small point clouds (512-1024 points). The scalability claim for DDE remains unverified at scale.
- What evidence would resolve it: Benchmarks on larger point clouds or image-based policies, measuring memory usage and success rate degradation curves as input dimensionality increases.

### Open Question 3
- Question: Is cosine similarity the optimal directional alignment objective, or would geodesic distance on SO(3) or learned alignment metrics yield better precision?
- Basis in paper: [inferred] The paper introduces L_DA using cosine similarity (Equation 12) without comparing alternative alignment objectives. The theoretical motivation decouples magnitude from direction, but the specific functional form is not justified.
- Why unresolved: Different alignment metrics have different gradient properties near convergence; cosine similarity may have suboptimal behavior when vectors are nearly aligned.
- What evidence would resolve it: Comparative study of alternative directional losses on fine-manipulation tasks (e.g., Pen, Slip Ring), analyzing convergence speed and final angular error.

### Open Question 4
- Question: Can an adaptive JVP/DDE switching mechanism be developed to combine JVP's precision for short horizons with DDE's stability for long horizons?
- Basis in paper: [inferred] Table 5 shows JVP outperforms for Short/Medium action chunks while DDE excels at Long horizons, suggesting complementary strengths that could be unified.
- Why unresolved: The paper treats JVP and DDE as separate variants rather than exploring a unified approach that dynamically selects based on prediction horizon or task phase.
- What evidence would resolve it: A hybrid model that switches between JVP and DDE based on horizon length or velocity magnitude, evaluated across all chunk configurations.

## Limitations
- The spectral bias analysis relies on empirical observation rather than rigorous frequency-domain proof of the 1/ω² decay in MeanFlow targets.
- The DDE approximation error's impact on long-horizon tasks remains theoretically unquantified beyond the observed <2% success drop in Table 5.
- The exact weighting hyperparameters (λ_Disp, λ_DA) and DDE epsilon (ε) are not specified, requiring empirical tuning that may affect reproducibility.

## Confidence

- **High Confidence**: The composite loss formulation and its three-component breakdown (MSE + Dispersive + Directional Alignment) are well-specified and empirically validated through ablation studies.
- **Medium Confidence**: The memory reduction claims from DDE vs exact JVP are directly measured but depend on the unspecified ε parameter and may vary with hardware/implementation.
- **Low Confidence**: The theoretical grounding for directional alignment's effectiveness in low-velocity regimes is demonstrated empirically but lacks rigorous proof of gradient non-vanishing.

## Next Checks

1. **Spectral Bias Verification**: Implement synthetic 2D regression with high-frequency vs low-frequency target dynamics. Compare OMP vs standard MeanFlow performance on tasks requiring rapid directional changes.
2. **DDE Approximation Error Quantification**: Systematically vary ε in OMP-DDE across a range (1e-5 to 1e-3) on a precision task (Pen). Measure both success rate and angular error to characterize the tradeoff curve.
3. **Directional Alignment Necessity Proof**: Create a controlled manipulation task where optimal actions span from high-magnitude ballistic motions to near-zero fine adjustments. Train variants with only L_mse vs full OMP. Verify directional alignment prevents gradient starvation in the low-velocity regime.