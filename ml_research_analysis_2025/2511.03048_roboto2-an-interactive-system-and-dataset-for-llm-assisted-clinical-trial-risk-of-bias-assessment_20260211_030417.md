---
ver: rpa2
title: 'ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial
  Risk of Bias Assessment'
arxiv_id: '2511.03048'
source_url: https://arxiv.org/abs/2511.03048
tags:
- rob2
- risk
- evidence
- roboto2
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ROBOTO2 is a web-based platform for LLM-assisted risk of bias (ROB)
  assessment in clinical trials, addressing the bottleneck of manually applying the
  ROB2 tool, which takes over 30 minutes per trial. The system combines PDF parsing,
  retrieval-augmented LLM prompting, and interactive human review, allowing users
  to upload trial reports, receive preliminary answers with evidence for ROB2 signaling
  questions, and provide real-time feedback.
---

# ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment

## Quick Facts
- arXiv ID: 2511.03048
- Source URL: https://arxiv.org/abs/2511.03048
- Reference count: 40
- ROBOTO2 is a web-based platform for LLM-assisted risk of bias (ROB) assessment in clinical trials, addressing the bottleneck of manually applying the ROB2 tool, which takes over 30 minutes per trial.

## Executive Summary
ROBOTO2 is a web-based platform that combines PDF parsing, retrieval-augmented LLM prompting, and human-in-the-loop review to assist in ROB2 risk of bias assessment of clinical trial reports. The system addresses the bottleneck of manual ROB2 assessment, which takes over 30 minutes per trial, by providing preliminary answers with evidence for ROB2 signaling questions. Using ROBOTO2, a dataset of 521 pediatric clinical trial reports was annotated manually and via LLM assistance, with experiments showing moderate LLM accuracy (best micro-F1 0.71-0.82) but conservative risk judgments.

## Method Summary
The system preprocesses PDF clinical trial reports into structured JSON, embeds paragraphs using Sentence-Transformers, retrieves top-k paragraphs for each signaling question via cosine similarity, and prompts LLMs with the question, elaboration, and retrieved evidence to generate answers and rationales. Human experts review and can accept, edit, or override LLM suggestions. Four LLMs were evaluated (GPT-3.5-Turbo, GPT-4o, Claude 3.5-Sonnet, Llama-3.3-70B-Instruct) across different retrieval settings (k=1,3,5,10, full paper) on a dataset of 521 pediatric clinical trials with 8954 signaling questions.

## Key Results
- LLM-assisted annotation dataset of 521 pediatric clinical trials (8954 signaling questions, 1202 evidence passages)
- Micro-F1 accuracy ranges from 0.60 to 0.82 across four LLMs and retrieval settings
- Human annotators accepted model suggestions 57.6% of the time and edited 42.4% of answers
- Models over-selected "No Information" and "No/Probably No" answers, leading to conservative risk judgments
- Larger context windows and more retrieved evidence improved performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented context improves LLM accuracy on domain-specific signaling questions by grounding responses in actual trial report passages.
- Mechanism: Signaling questions are embedded using Sentence-Transformers; cosine similarity retrieves top-k paragraphs from the parsed PDF; retrieved evidence is injected into the LLM prompt as context for answer generation.
- Core assumption: Relevant evidence exists explicitly in the trial report text and can be located via semantic similarity.
- Evidence anchors: [abstract] "combines PDF parsing, retrieval-augmented LLM prompting, and human-in-the-loop review"; [section 4] "embed the signaling question...use cosine similarity to identify the top-k paragraphs to use as context"

### Mechanism 2
- Claim: Larger context windows and more retrieved passages improve model performance by reducing information omission.
- Mechanism: Models with larger context (GPT-4o, Claude 3.5-Sonnet, Llama-3.33-70B) can ingest full papers or more retrieved paragraphs; this mitigates recall limitations of sparse retrieval.
- Core assumption: The relevant information is somewhere in the document, and providing more context increases the probability of inclusion.
- Evidence anchors: [section 7] "Larger context windows and more retrieved evidence somewhat mitigate these tendencies"; [table 2] Full paper context achieves highest micro-F1 (0.71 for Claude 3.5-Sonnet) vs. k=1 retrieval (0.60)

### Mechanism 3
- Claim: Human-AI collaboration with exposed intermediate outputs enables faster review while maintaining quality.
- Mechanism: System displays LLM-predicted answers, explanations, and evidence passages; experts accept (57.6%), edit (42.4%), or override suggestions; feedback is logged.
- Core assumption: Experts can efficiently verify AI suggestions and will correct errors when detected.
- Evidence anchors: [abstract] "annotators accepting model suggestions 57.6% of the time and editing 42.4% of answers"; [section 4] "ROBOTO2 retains the original LLM responses...as well as the versions confirmed or edited by expert users"

## Foundational Learning

- Concept: **ROB2 Assessment Framework**
  - Why needed here: The entire system is structured around 5 domains, 22 signaling questions, and hierarchical risk logic (question→domain→overall). Understanding this hierarchy is prerequisite to interpreting system outputs.
  - Quick check question: Can you explain why "No Information" answers at the signaling question level cascade to more conservative domain-level judgments?

- Concept: **Embedding-Based Retrieval (RAG)**
  - Why needed here: The system uses Sentence-Transformers to embed paragraphs and questions, then retrieves via cosine similarity. Understanding retrieval quality (recall@k) directly impacts answer accuracy.
  - Quick check question: Why might S-BERT (recall@10=0.678) still miss relevant evidence, and how does full-paper context address this?

- Concept: **LLM Calibration and Over-Conservatism**
  - Why needed here: Models over-select "No Information" or "No/Probably No," leading to inflated high-risk judgments. This reflects safety alignment rather than task-specific calibration.
  - Quick check question: If a model predicts "No Information" for a question where evidence exists in the paper, is this a retrieval failure or an LLM reasoning failure?

## Architecture Onboarding

- Component map: PDF Preprocessing -> Embedding Index -> Retrieval Module -> LLM QA Module -> Risk Logic Engine -> Web Interface

- Critical path:
  1. Upload PDF → preprocess to JSON → embed paragraphs
  2. For each signaling question: embed query → retrieve top-k → prompt LLM → get answer + explanation
  3. Display to user with evidence passages; collect accept/edit/override feedback
  4. Apply ROB2 flowchart logic to compute domain and overall risk
  5. Export assessment with full audit trail (original LLM outputs + expert edits)

- Design tradeoffs:
  - k=3 retrieval vs. full paper: k=3 is faster and cheaper but lower recall; full paper (for models with large context) achieves best F1 but higher cost/latency
  - S-BERT vs. BM25: S-BERT has higher recall@k (0.455 vs. 0.272 at k=3) but adds embedding computation overhead
  - Zero-shot vs. few-shot prompting: Paper reports minimal gains from few-shot; zero-shot is simpler and equally effective
  - Human-in-the-loop vs. fully automated: Fully automated produces overly conservative judgments; human review is essential for quality

- Failure signatures:
  - Model over-selects "No Information": Safety alignment causes excessive caution; check if retrieved evidence is relevant before assuming reasoning failure
  - Retrieval misses evidence: Low recall@k (e.g., D5 has few annotated evidence passages); consider increasing k or using full paper
  - Domain-level judgments inflated: Conservative question answers compound; paper-level "high risk" judgments (101 by LLM vs. 47 by humans)
  - Anchoring bias: Users may accept incorrect suggestions; no automated detection (acknowledged limitation)

- First 3 experiments:
  1. Retrieval ablation: Run same LLM with k=1,3,5,10 and full paper; measure micro-F1 per domain to identify where retrieval matters most (expect D1 easy, D5 hard)
  2. Model comparison on same evidence: Provide oracle evidence to all 4 LLMs; isolate reasoning vs. retrieval failures by comparing oracle vs. retrieved performance gaps
  3. Human acceptance rate analysis: For the 276 LLM-assisted assessments, correlate acceptance rate with question type/domain; identify systematic patterns where models fail vs. succeed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does exposure to LLM-generated suggestions introduce anchoring bias in human reviewers, leading to different final label distributions compared to fully manual annotation?
- Basis in paper: [explicit] The authors state in Section 9 that they "leave the measurement of this bias in the ROB setting to future work," citing prior work showing annotators may be influenced by LLM outputs.
- Why unresolved: While the system tracks user edits, the study design did not include a control group where annotators were blinded to model outputs to isolate cognitive bias effects.
- What evidence would resolve it: A randomized controlled trial comparing the agreement rates and label distributions of reviewers who perform assessments with blinded vs. visible LLM suggestions.

### Open Question 2
- Question: Does the efficiency gained from LLM assistance translate into higher quality assessments through repurposed effort, or does it merely reduce time?
- Basis in paper: [explicit] Section 9 notes that the annotation team reported repurposing saved time to enhance evidence coverage, but "the impact... on the quality of the resulting assessments was not explicitly measured."
- Why unresolved: The current study measures annotator acceptance rates (57.6%) but does not quantitatively assess if the "repurposed effort" results in more accurate domain-level risk judgments.
- What evidence would resolve it: A study correlating the time saved per assessment with the depth of evidence retrieved and the accuracy of the final risk judgment against a gold standard.

### Open Question 3
- Question: Can advanced reasoning models improve performance on signaling questions that require interpreting numerical data or external clinical guidelines?
- Basis in paper: [explicit] Section 9 states that "Future work could explore whether these models [e.g., o3, DeepSeek-R1] improve current performance," specifically noting that current models struggled with domains requiring numerical or clinical context.
- Why unresolved: The current experiments were limited to standard instruct models (GPT-3.5, GPT-4o, Claude 3.5, Llama-3.3), which exhibited conservative error patterns in complex domains (D2, D3, D5).
- What evidence would resolve it: Benchmarking reasoning-focused models on the ROBOTO2 dataset to evaluate if they reduce the over-selection of "No Information" in domains requiring multi-step inference.

## Limitations
- Model predictions show conservative bias (over-selection of "No Information" and "No/Probably No"), potentially reflecting safety alignment rather than domain calibration
- Implementation details for cascading question logic remain underspecified, complicating exact reproduction
- Human-AI collaboration effectiveness may reflect anchoring bias, though this was not formally tested
- Dataset focuses on pediatric trials, limiting generalizability to adult populations

## Confidence
- **High**: Retrieval-augmented prompting improves accuracy over baseline; human-in-the-loop review reduces overly conservative automated judgments
- **Medium**: Specific F1 scores (0.71-0.82 micro-F1) and acceptance rates (57.6%) are reproducible with exact parameter replication
- **Low**: Claims about clinical decision impact and safety improvements require external validation beyond accuracy metrics

## Next Checks
1. Retrieval quality assessment: Compute recall@k for each domain using the 1202 gold evidence passages to identify where retrieval fails most (expected: D5 domain has lowest recall)
2. Oracle vs. retrieved performance gap: Provide ground-truth evidence passages to all four LLMs and measure the performance difference to isolate retrieval vs. reasoning failures
3. Anchoring bias quantification: For LLM-assisted annotations where experts edited answers, analyze whether initial LLM predictions influenced final judgments through systematic bias patterns