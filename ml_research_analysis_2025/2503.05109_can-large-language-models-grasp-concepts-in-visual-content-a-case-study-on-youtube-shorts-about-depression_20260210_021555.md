---
ver: rpa2
title: Can Large Language Models Grasp Concepts in Visual Content? A Case Study on
  YouTube Shorts about Depression
arxiv_id: '2503.05109'
source_url: https://arxiv.org/abs/2503.05109
tags:
- mllm
- image
- concepts
- visual
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how multimodal large language models (MLLMs)
  can assist in analyzing video content related to depression by interpreting abstract
  visual concepts like presenting style, interacting style, arousal, and diversity.
  Using LLaVA-1.6 Mistral 7B to analyze 725 keyframes from 142 YouTube Shorts, the
  research compares AI interpretations with human understanding across four prompt
  configurations varying in detail and operationalization.
---

# Can Large Language Models Grasp Concepts in Visual Content? A Case Study on YouTube Shorts about Depression

## Quick Facts
- **arXiv ID**: 2503.05109
- **Source URL**: https://arxiv.org/abs/2503.05109
- **Reference count**: 40
- **Primary result**: MLLMs perform well on straightforward concepts (arousal, diversity) but struggle with complex performative concepts (presenting, interacting) in YouTube Shorts about depression.

## Executive Summary
This study investigates how multimodal large language models (MLLMs) can analyze video content related to depression by interpreting abstract visual concepts. Using LLaVA-1.6 Mistral 7B to analyze 725 keyframes from 142 YouTube Shorts, researchers compared AI interpretations with human understanding across four prompt configurations varying in detail and operationalization. The findings reveal that while MLLMs effectively handle straightforward concepts like arousal and diversity, they struggle with more complex performative concepts like presenting and interacting styles, particularly when video genres include non-human elements or conflicting visual-textual information. Interestingly, greater prompt detail does not consistently improve alignment, highlighting the need for customized prompts and human-centered evaluation when using AI for video content analysis.

## Method Summary
The study extracted 725 keyframes from 142 depression-related YouTube Shorts using FFmpeg, selecting frames where structural similarity index (SSIM) difference exceeded 0.3 to ensure distinct visual content. LLaVA-1.6 Mistral 7B was used to analyze four abstract concepts (presenting style, interacting style, arousal, and diversity) using four different prompt configurations ranging from naive to detailed operationalization. A separate Llama-3.1-8B-Instruct model parsed the MLLM outputs into structured labels, which were then compared against human annotations from three coders with inter-coder reliability above 75%. The analysis measured alignment between AI and human interpretations using bootstrapping methods.

## Key Results
- MLLMs showed strong alignment with humans on arousal and diversity concepts but struggled with presenting and interacting styles
- Greater prompt detail did not consistently improve human-AI alignment and sometimes constrained the model's ability to recognize novel social dynamics
- The model exhibited modality priority bias, typically prioritizing textual over visual cues when signals conflicted
- Non-human video genres (cartoons, memes) resulted in lower alignment due to cultural context limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Static keyframe extraction via structural variance acts as a proxy for video content, enabling MLLMs to process temporal data despite limited context windows.
- **Mechanism**: The system filters frames where SSIM difference exceeds 0.3, isolating visually distinct moments that fit within the MLLM's processing limits.
- **Core assumption**: Abstract concepts can be inferred from isolated visual snapshots without continuous temporal audio-visual flow.
- **Evidence anchors**: Methodology 3.1 details the use of FFmpeg to extract frames with SSIM difference > 0.3; "TimeBlind" highlights temporal dynamics remain brittle for MLLMs.
- **Break condition**: Fails if the concept relies on motion or audio tone, as static frames discard this data.

### Mechanism 2
- **Claim**: Increasing prompt operationalization can inversely constrain model alignment by over-specifying criteria, preventing recognition of novel social dynamics.
- **Mechanism**: Detailed definitions create rigid feature-matching loops where the model prioritizes explicit prompt features over holistic context.
- **Core assumption**: MLLMs prioritize explicit instruction constraints over pre-trained latent understanding when generating explanations.
- **Evidence anchors**: Abstract states "Greater detail does not necessarily increase human-AI alignment... sometimes constrained the model's ability"; Section 4.2.1 shows misclassification of informal presentations due to missing explicit features.
- **Break condition**: Breaks if the concept is purely objective where specificity aids precision rather than limiting interpretation.

### Mechanism 3
- **Claim**: MLLMs exhibit modality priority bias where textual overlays override conflicting visual cues in short-form video content.
- **Mechanism**: The model leverages strong text-based pre-training to resolve semantic ambiguity, defaulting to text when it conflicts with visual static state.
- **Core assumption**: Text-Image synthesis in current MLLMs is not balanced; the model defaults to the "easier" text modality.
- **Evidence anchors**: Section 4.2.3 notes MLLMs "typically prioritize textual over visual cues"; "Audio-centric Video Understanding" notes audio is often treated as auxiliary.
- **Break condition**: Changes if text is illegible or visual cues are culturally specific symbols the model recognizes but cannot textually parse.

## Foundational Learning

- **Concept**: Structural Similarity Index (SSIM)
  - **Why needed here**: To understand how the authors reduced 150 videos to 725 distinct data points without redundancy.
  - **Quick check question**: How does using SSIM > 0.3 differ from extracting a frame every 5 seconds?

- **Concept**: Concept Operationalization
  - **Why needed here**: The core finding is that *how* you define a concept changes the model's accuracy.
  - **Quick check question**: Why might a "detailed" prompt cause a model to miss a "novel" interaction style?

- **Concept**: Human-AI Alignment (vs. Accuracy)
  - **Why needed here**: The paper measures "alignment" with human coders, not ground truth accuracy.
  - **Quick check question**: Why is "alignment" the preferred metric over "accuracy" when analyzing depression-related social content?

## Architecture Onboarding

- **Component map**: YouTube Shorts -> FFmpeg (SSIM filtering) -> LLaVA-1.6 Mistral 7B (4 prompt strategies) -> Llama-3.1-8B-Instruct (parsing) -> Human annotation comparison
- **Critical path**: The prompting strategy determines the model's ability to align with humans more than the visual input itself.
- **Design tradeoffs**:
  - Keyframes vs. Video: Sacrificing temporal context for computational tractability
  - Prompt Specificity: Trading flexibility for clarity, with strict definitions potentially breaking generalization
- **Failure signatures**:
  - False Negatives in "Presenting": Model claims "No slide detected" for informal vlogs due to detailed prompts
  - Genre Confusion: Cartoon/Meme inputs result in low alignment due to cultural context gaps
  - Hallucinated Constraints: Model generates explanations citing absence of prompt-listed features rather than analyzing image content
- **First 3 experiments**:
  1. Baseline Alignment Test: Run Naive vs. Detailed prompts on 50 frames for "Interacting" to reproduce alignment variance
  2. Genre Stress Test: Filter dataset for "Cartoons/Memes" to quantify genre bias against "Human Vlogs"
  3. Text-Visual Conflict Test: Create synthetic keyframes where text contradicts image to verify text cue dominance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does incorporating video temporality affect MLLM alignment with human interpretations of abstract concepts dependent on narrative context?
- **Basis in paper**: [explicit] Section 5.2 states future work could explore MLLMs that directly interpret videos or sequences of keyframes to provide more contextual information.
- **Why unresolved**: Study limited by computational constraints and model context windows, forcing use of static keyframes that miss concepts dependent on video's overall narrative.
- **What evidence would resolve it**: Comparative study measuring human-AI alignment scores on same dataset using temporal input versus static keyframe approach.

### Open Question 2
- **Question**: Can sophisticated methods for synthesizing multimodal inputs effectively resolve conflicts between visual and textual cues in short videos?
- **Basis in paper**: [explicit] Section 5.2 notes developing more sophisticated methods for synthesizing multimodal inputs is promising to address issues where visual signals conflict with text overlays.
- **Why unresolved**: Current workflow decoded videos into keyframes only, ignoring audio and transcripts, creating misinterpretations when model prioritized text over visual context.
- **What evidence would resolve it**: Evaluation of MLLM pipeline that integrates audio/transcripts with visuals, specifically testing instances with conflicting visual and textual information.

### Open Question 3
- **Question**: To what extent does implementing human-centered post-hoc auditing improve MLLM alignment for non-human video genres?
- **Basis in paper**: [explicit] Section 5.2 suggests future work can explore incorporating human-centered evaluation as standard step to address systematic misunderstandings of specific genres.
- **Why unresolved**: Study identified MLLM struggled with "non-human genres" and "cultural references" but did not test interventions to correct these specific biases.
- **What evidence would resolve it**: Iterative experiment where human feedback on misaligned non-human genre outputs refines prompts or fine-tunes model, followed by re-evaluation.

### Open Question 4
- **Question**: Do misalignment factors persist when using state-of-the-art proprietary models like GPT-4 compared to open-weight LLaVA-1.6 Mistral 7B?
- **Basis in paper**: [inferred] Section 5 notes findings may not be generalizable due to single model exploratory study; Section 5.1 references even state-of-the-art MLLMs like GPT4 face difficulties interpreting abstract concepts.
- **Why unresolved**: Study only tested one specific model architecture and size; unknown if struggles are limitations of specific model scale or inherent to current MLLM capabilities.
- **What evidence would resolve it**: Replication using GPT-4 or other frontier models on same dataset to compare alignment distributions across four concepts.

## Limitations
- Reliance on static keyframes discards temporal and audio information critical for understanding social concepts in video content
- Ground truth annotations represent only three human perspectives on inherently subjective social concepts
- Specific video IDs and complete annotation datasets remain unavailable, limiting reproducibility

## Confidence
- **High Confidence**: MLLMs perform better on straightforward concepts (arousal, diversity) than complex performative ones (presenting, interacting)
- **Medium Confidence**: Greater prompt detail does not consistently improve alignment, with mixed results across different concepts
- **Low Confidence**: MLLMs prioritize textual over visual cues in conflicting scenarios needs more systematic testing

## Next Checks
1. **Temporal Context Recovery Test**: Re-run analysis pipeline using short video clips (3-5 seconds) instead of keyframes to quantify performance gap
2. **Cross-Cultural Genre Validation**: Test model on depression-related videos from different cultural contexts to determine if genre confusion extends beyond cartoons/memes
3. **Controlled Text-Visual Conflict Experiment**: Create synthetic test frames with systematically varied text-visual conflicts to quantify threshold at which text cues override visual interpretation across concept types