---
ver: rpa2
title: Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions
arxiv_id: '2509.14165'
source_url: https://arxiv.org/abs/2509.14165
tags:
- token
- tokens
- vision
- pruning
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces STEP, a hybrid token-reduction framework that
  combines dynamic patch merging (dCTS) and early token pruning to enhance the efficiency
  of Vision Transformers (ViTs) for semantic segmentation. By adaptively merging similar
  patches into superpatches and halting high-confidence tokens early in the network,
  STEP significantly reduces computational cost and inference time while maintaining
  segmentation accuracy.
---

# Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions

## Quick Facts
- arXiv ID: 2509.14165
- Source URL: https://arxiv.org/abs/2509.14165
- Authors: Michal Szczepanski; Martyna Poreba; Karim Haroun
- Reference count: 40
- Primary result: dCTS alone achieves 2.5× reduction in tokens, 2.6× lower computational cost, and 3.4× higher throughput on high-res segmentation benchmarks

## Executive Summary
STEP introduces a hybrid token-reduction framework for Vision Transformers that combines dynamic patch merging (dCTS) and early token pruning to enhance efficiency in semantic segmentation at high resolutions. By adaptively merging similar patches into superpatches and halting high-confidence tokens early in the network, STEP significantly reduces computational cost and inference time while maintaining segmentation accuracy. The method scales well across different resolutions and model sizes, achieving up to 4× reduction in computational complexity with maximum accuracy drop of no more than 2.0%.

## Method Summary
STEP is a hybrid token-reduction framework combining dynamic patch merging (dCTS) and early token pruning for Vision Transformer-based semantic segmentation. The method uses a lightweight CNN policy network (EfficientNetLite0) to predict similarity scores for adjacent patches, merging them hierarchically from large to small windows to reduce tokens at the input level. Encoder blocks are divided into stages with auxiliary decoding heads that compute confidence scores for each token, halting those exceeding a threshold to bypass subsequent layers. The framework applies resolution-dependent thresholds (strict for large patches, loose for small) to balance accuracy and efficiency, validated on COCOStuff10k, ADE20K, and Cityscapes at resolutions up to 1024×1024.

## Key Results
- dCTS alone achieves 2.5× reduction in tokens, 2.6× lower computational cost, and 3.4× higher throughput compared to standard ViT backbones
- Full STEP framework reaches up to 4× reduction in computational complexity and 1.7× faster inference
- Maximum accuracy drop remains within 2.0% across all configurations and resolutions
- Up to 40% of tokens can be confidently predicted and halted before reaching the final encoder layer

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Patch Merging (dCTS)
Reducing tokens at the patch-level via dynamic merging preserves spatial structure better than late-stage pruning, lowering baseline computational cost. A lightweight CNN policy network predicts similarity scores for windows of adjacent patches. If the score exceeds a threshold, patches merge into "superpatches" hierarchically from large windows down to small to prevent conflicts. This works because semantic homogeneity in images correlates with local visual similarity, and merging these regions doesn't destroy critical boundary information.

### Mechanism 2: Confidence-Based Early Exiting
Not all tokens require the full receptive field of deep layers; "easy" tokens can be halted early to save computation. Encoder blocks are divided into stages with auxiliary decoding heads that compute confidence scores per token. If confidence exceeds a threshold, the token is masked and bypasses subsequent layers. This works because intermediate layers provide sufficient context for high-confidence predictions on a significant subset of tokens, as token difficulty evolves non-uniformly.

### Mechanism 3: Resolution-Dependent Thresholding
Fixed global thresholds for merging fail because the cost of error scales with patch size. The system applies stricter thresholds for merging large patch groups and lower thresholds for small groups. This works because the negative impact of merging dissimilar large regions is greater than the computational benefit, whereas small merge errors are tolerable.

## Foundational Learning

- **Quadratic Complexity of Self-Attention**: ViTs have $O(N^2)$ computational cost where $N$ is sequence length. Reducing $N$ via merging yields quadratic savings. Quick check: If you double input resolution, how much does computational cost increase in a standard ViT, and how does dCTS mitigate this?

- **Dense Prediction vs. Classification Pruning**: In classification, unimportant tokens can be discarded entirely. In segmentation, you must preserve spatial correspondence. STEP uses "halting" (soft pruning) rather than deletion to maintain the spatial map. Quick check: Why can't STEP simply delete tokens classified as "background" early in the network?

- **The Latency-FLOPs Gap**: Reduced FLOPs don't always equal faster inference. Understanding GPU parallelism and memory access patterns explains why masking mechanisms can become bottlenecks. Quick check: Why might a model with 50% fewer FLOPs actually run slower than baseline on a GPU?

## Architecture Onboarding

- **Component map**: High-res Image (1024×1024) -> dCTS Module (EfficientNetLite0 predicts merges -> Bilinear interpolation -> Linear Embedding) -> ViT Backbone (Transformer blocks + Auxiliary Heads) -> Token Masking (halted or continue) -> Final Decoder (SegViT head combines predictions)

- **Critical path**: The dCTS policy network must be lightweight; if it adds significant latency, speedup is negated. The masking logic in early-exit mechanism is identified as latency bottleneck.

- **Design tradeoffs**: Strict vs. loose thresholds (high merge thresholds preserve accuracy but reduce throughput gains), head placement (earlier heads increase theoretical savings but risk premature halting).

- **Failure signatures**: Throughput stagnation (check for CPU-GPU memory transfers in masking loop or irregular control flow), boundary artifacts (visual checks for "jagged" segmentation edges indicating over-aggressive merging).

- **First 3 experiments**: 1) Threshold sweep to find optimal τ for your dataset (reproduce Figure 4/Table 1), 2) Ablation on head placement (test Layer 18 vs [8,16] for GFLOPs vs FPS trade-off), 3) Scaling test at 512², 768², 1024² to verify efficiency gains scale with resolution.

## Open Questions the Paper Calls Out

- **Question**: How can the early-pruning mechanism be redesigned to eliminate GPU inefficiencies caused by dynamic masking operations?
- **Question**: What adaptive strategies can determine the optimal placement of auxiliary heads for early-exit mechanisms in Vision Transformers?
- **Question**: Can dynamic, content-aware thresholding for patch merging prevent the premature discarding of relevant tokens in high-resolution regimes?

## Limitations
- Architectural transparency gaps prevent faithful reproduction (EfficientNetLite0 input/output format and Token Unsharing mechanism unspecified)
- Dataset generalization concerns (fixed thresholds may not work for domains with different semantic distributions)
- Latency vs. FLOPs disconnect (masking operations introduce GPU inefficiencies that reduce practical speedup)

## Confidence
- **High Confidence**: Hierarchical patch merging (dCTS) achieving 2.5× token reduction with ≤2.0% mIoU drop is well-supported
- **Medium Confidence**: Early token pruning mechanism showing up to 40% tokens halted early is supported but relies on less-detailed auxiliary heads
- **Low Confidence**: Resolution-dependent thresholding claim lacks ablation studies showing what happens when thresholds are reversed or dynamically adjusted

## Next Checks
- Test published τ configurations on datasets with different semantic characteristics (medical segmentation or satellite imagery) to validate generalizability
- Implement GPU-optimized token masking operations using tensor-based masking instead of loop-based control flow
- Conduct systematic ablation studies on dCTS thresholds, testing reversed thresholds and dynamic adjustment based on local edge density