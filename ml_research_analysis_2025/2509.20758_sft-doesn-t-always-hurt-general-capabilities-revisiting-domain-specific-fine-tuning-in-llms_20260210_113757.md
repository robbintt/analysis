---
ver: rpa2
title: 'SFT Doesn''t Always Hurt General Capabilities: Revisiting Domain-Specific
  Fine-Tuning in LLMs'
arxiv_id: '2509.20758'
source_url: https://arxiv.org/abs/2509.20758
tags:
- performance
- learning
- general
- tokens
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the common belief that supervised fine-tuning
  (SFT) on domain-specific datasets degrades general capabilities of large language
  models (LLMs). The authors show empirically that using a smaller learning rate can
  substantially mitigate this degradation while preserving strong domain performance.
---

# SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs

## Quick Facts
- arXiv ID: 2509.20758
- Source URL: https://arxiv.org/abs/2509.20758
- Reference count: 40
- Smaller learning rates (1e-6) achieve favorable trade-offs between domain gains and general capability preservation compared to larger rates (2e-5).

## Executive Summary
This paper challenges the common belief that supervised fine-tuning (SFT) on domain-specific datasets inevitably degrades general capabilities of large language models. Through extensive empirical experiments across multiple models and domains, the authors demonstrate that using smaller learning rates can substantially mitigate general performance degradation while preserving strong domain performance. They introduce Token-Adaptive Loss Reweighting (TALR), a novel technique that dynamically downweights hard tokens during training, which further improves the balance between domain adaptation and general capability preservation. Theoretical analysis supports these findings, showing that smaller per-step updates lead to tighter bounds on general performance degradation.

## Method Summary
The authors systematically investigate SFT's impact on general capabilities through learning rate sweeps and introduce TALR, which reweights tokens based on their difficulty using dynamic temperature scheduling. The method computes token weights as w_i ∝ p_i^(1/τ) where τ is the median batch loss, with a minimum weight floor of 0.01 and stop-gradient applied to weights. Experiments span multiple models (Qwen, Gemma, DeepSeek) and domains (medical reasoning, e-commerce classification, mathematical problem-solving) using standard SFT, TALR, and several baselines including L2 regularization, LoRA, model averaging, and KL regularization. The key insight is that most tokens in domain data are already well-modeled by pretrained models, and TALR creates an implicit curriculum by gradually incorporating harder tokens.

## Key Results
- Using learning rate 1e-6 yields significantly better trade-offs between domain and general performance compared to 2e-5 across all tested datasets and models
- TALR outperforms standard SFT and baselines (L2-Reg, LoRA, Wise-FT, FLOW, KL-Reg) in balancing domain gains with general capability preservation
- Dynamic temperature scheduling in TALR is critical—fixed τ=1 degrades domain performance by ~0.2+ accuracy points
- The benefits hold across diverse domains including medical reasoning (MedCalc), e-commerce classification (ESCI), and mathematical problem-solving (MetaMathQA)

## Why This Works (Mechanism)

### Mechanism 1: Smaller Learning Rate Reduces General Performance Degradation Upper Bound
Using smaller learning rates tightens the theoretical upper bound on general-performance degradation while maintaining comparable domain performance. Under exponential tilting approximation, per-step update size λ scales with learning rate, and Theorem B.2 shows that for fixed domain improvement, the general-performance degradation bound decreases as learning rate decreases. This works when KL divergence per step is bounded (KL(qt || qt+1) ≤ ε), though if learning rate is too small (e.g., 5e-7 in MetaMathQA), domain performance suffers due to insufficient gradient signal.

### Mechanism 2: Token-Level Difficulty Distribution Enables Curriculum Learning
Most tokens in SFT training data are already well-modeled by pretrained LLMs; hard tokens are sparse but disproportionately drive general capability degradation. TALR adaptively downweights low-probability tokens, creating implicit curriculum—easier tokens dominate early training, harder tokens are gradually incorporated as they become easier. This operates when token-level shift is sparse (most tokens well-modeled, only small subset requires adjustment), though the curriculum effect diminishes if all tokens uniformly have low probability (completely foreign domain).

### Mechanism 3: Dynamic Temperature Prevents Learning Collapse on Hard Tokens
Batch-dependent τ (median of average token loss) prevents both excessive downweighting of hard tokens and insufficient protection against forgetting. Larger τ when batch contains more hard tokens prevents weights from vanishing; smaller τ when overall loss is low acts as hard clipping. The closed-form solution w_i ∝ p_θ(x_i)^(1/τ) works when token difficulty distribution varies across batches and training stages, though extreme outliers in token loss distribution could make median non-representative.

## Foundational Learning

- Concept: **KL Divergence as Code Length Discrepancy**
  - Why needed here: Theoretical analysis views LLMs as compressors where KL(P||Q) represents expected code length difference. Essential for understanding why learning rate affects the forgetting-adaptation trade-off at a formal level.
  - Quick check question: Can you explain why ΔL(P) = KL(P||Q₂) − KL(P||Q₁) represents performance change when switching models?

- Concept: **Exponential Tilting Approximation**
  - Why needed here: Paper approximates fine-tuning updates via exponential tilting, enabling tractable analysis. Understanding this approximation clarifies what theorems prove vs. assume.
  - Quick check question: Given current distribution q_t and target p̃, how does q̃_{t+1}(a) ∝ q_t(a)^(1−λ)·p̃(a)^λ differ from actual gradient descent?

- Concept: **Catastrophic Forgetting in Data-Oblivious Settings**
  - Why needed here: Paper operates in data-oblivious setting (no access to pretraining data). Understanding why this is harder than data-dependent continual learning clarifies the contribution.
  - Quick check question: Why might rehearsal methods (storing original data subset) be impractical for adapting LLMs to proprietary domains?

## Architecture Onboarding

- Component map:
  ```
  TALR Training Loop: Forward pass -> Loss computation -> Temperature calculation -> Weight calculation -> Clipping -> Stop gradient -> Reweighted loss
  ```

- Critical path:
  1. **Learning rate selection**: Start with 1e-6 for favorable trade-off; increase only if domain performance insufficient.
  2. **Temperature scheduling**: Use dynamic τ = median(batch token losses)—do not fix τ.
  3. **Weight floor**: Apply w_min = 0.01 cutoff to prevent hard tokens from vanishing.
  4. **Checkpoint selection**: Select based on target domain performance (not general benchmarks).

- Design tradeoffs:
  | Choice | Pros | Cons |
  |--------|------|------|
  | Smaller LR (1e-6) | Better general capability preservation | May need more epochs |
  | Larger LR (5e-6) + TALR | Can push domain performance further | Doesn't fully eliminate degradation |
  | Label-only (vs CoT) | Larger safe learning rate range | Loses reasoning supervision |
  | Fixed τ | Simpler implementation | Risk of learning collapse |

- Failure signatures:
  - **Domain performance plateaus early**: LR likely too small (try 5e-6).
  - **General performance drops sharply**: LR too large without TALR, or τ not dynamic.
  - **Hard tokens never learned**: w_min too aggressive or τ too small.
  - **No improvement over standard SFT**: Weight computation missing stop-gradient.

- First 3 experiments:
  1. **Baseline learning rate sweep**: Train standard SFT at lr ∈ {1e-6, 5e-6, 2e-5} on domain data. Plot domain vs. general performance; verify Figure 1 pattern (smaller LR → upper-right favorable trade-off).
  2. **TALR vs. standard SFT at aggressive LR**: At lr=5e-6, compare standard SFT vs. TALR. Verify TALR achieves better general performance while maintaining domain gains per Table 2.
  3. **Ablate temperature strategy**: Compare TALR with dynamic τ vs. fixed τ=1 on held-out validation. Expect ~0.2+ absolute improvement in domain accuracy per Section D.3.

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical scope is limited to specific dataset characteristics (MedCalc with 10k samples, ESCI with 49k samples) which may not generalize to domains with fundamentally different token distributions or task structures
- Theoretical assumptions may not hold universally—the exponential tilting approximation requires bounded KL divergence per update, and real fine-tuning involves complex optimization dynamics that violate these simplifications
- Baseline comparison concerns exist since several methods (L2-Reg, Wise-FT, FLOW, KL-Reg) lack detailed hyperparameter specifications, potentially making TALR's superiority partly reflect implementation advantages

## Confidence
**High confidence (8-10/10):** Smaller learning rates (1e-6) consistently yield better trade-offs between domain and general performance than larger rates (2e-5) across multiple datasets and models.

**Medium confidence (6-7/10):** TALR provides additional improvements over standard SFT at moderate learning rates (5e-6), though magnitude varies by dataset and model.

**Low confidence (3-4/10):** The theoretical analysis fully explains the empirical observations—the exponential tilting framework provides qualitative insight but quantitative bounds are likely loose.

## Next Checks
**Validation Check 1: Learning Rate Breakpoint Detection** Systematically identify the maximum learning rate that preserves general capabilities for each dataset by performing a finer-grained sweep (e.g., {1e-6, 2e-6, 5e-6, 1e-5, 2e-5}) to establish whether the "small learning rate" threshold varies significantly across domains.

**Validation Check 2: Direct Token-Level Analysis** Measure actual token probability distributions before and after fine-tuning to verify the sparse token-shift assumption by computing token entropy/uncertainty on validation data before SFT and tracking which tokens' probabilities change most during training.

**Validation Check 3: Cross-Domain Generalization Test** Fine-tune on one domain (e.g., MedCalc) and evaluate on completely different domain data (e.g., ESCI test set) to measure pure capability preservation versus catastrophic forgetting, isolating the forgetting effect from adaptation to new task distributions.