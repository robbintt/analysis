---
ver: rpa2
title: Soft Tokens, Hard Truths
arxiv_id: '2509.19170'
source_url: https://arxiv.org/abs/2509.19170
tags:
- soft
- hard
- fuzzy
- training
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first reinforcement learning framework\
  \ for training continuous Chain-of-Thought reasoning in LLMs with minimal computational\
  \ overhead and without requiring ground-truth discrete CoTs. The core method uses\
  \ \"soft\" and \"fuzzy\" tokens\u2014probability-weighted mixtures of embeddings\
  \ with added noise\u2014to enable exploration during RL training, allowing the model\
  \ to implicitly follow multiple reasoning paths."
---

# Soft Tokens, Hard Truths

## Quick Facts
- **arXiv ID:** 2509.19170
- **Source URL:** https://arxiv.org/abs/2509.19170
- **Reference count:** 40
- **Primary result:** First RL framework for continuous CoT reasoning achieving pass@1 parity with discrete training and superior pass@32 performance

## Executive Summary
This paper introduces a reinforcement learning framework for training continuous Chain-of-Thought reasoning in LLMs with minimal computational overhead. The method uses "soft" tokens—probability-weighted mixtures of embeddings with added noise—to enable exploration during RL training without requiring ground-truth discrete CoTs. Across mathematical reasoning benchmarks, the approach achieves pass@1 parity with discrete token training while surpassing it for pass@32, demonstrating greater CoT diversity. Additionally, it preserves the base model's out-of-distribution performance better than hard token fine-tuning, providing a softer touch on the model's capabilities.

## Method Summary
The method trains LLMs to perform continuous Chain-of-Thought reasoning using reinforcement learning with "soft" tokens. Instead of sampling discrete tokens from the softmax distribution, the model computes a weighted embedding mixture h_t^0 = p_{t-1}E (where E is the embedding matrix), then adds Gaussian noise: h̃_t^0 = p_{t-1}E + σN(0,I). This creates a distribution over continuous CoT trajectories that can be optimized via Reinforce-style policy gradients. The log-probability of a noisy embedding has a closed-form Gaussian expression, making gradient computation straightforward. The training uses RLOO (Reinforce with Leave-One-Out baseline) with 32 samples per prompt and a reward of 100 for correct answers, 10 for extractable-but-wrong answers, and 0 otherwise. Model selection uses greedy validation performance on a held-out set.

## Key Results
- Soft and fuzzy training achieve pass@1 parity with hard training while surpassing it for pass@32 (3-5 percentage point improvement)
- Soft/fuzzy training preserves out-of-distribution performance (HellaSwag, ARC, MMLU) while hard training degrades it
- Optimal deployment uses continuous CoT training with discrete (hard) token inference
- Llama-8B-Instruct shows dramatic generalization improvements (20.2%→44.7% on MATH-500) with soft training

## Why This Works (Mechanism)

### Mechanism 1
Injecting Gaussian noise into probability-weighted embedding mixtures enables tractable RL exploration in continuous token space. At each CoT step, instead of sampling a discrete token from the softmax distribution, the model computes a weighted embedding mixture h_t^0 = p_{t-1}E, then adds noise: h̃_t^0 = p_{t-1}E + σN(0,I). This stochastic perturbation creates a distribution over continuous CoT trajectories that can be optimized via Reinforce-style policy gradients. The log-probability of a noisy embedding has a closed-form Gaussian expression (Eq. 10), making gradient computation straightforward. The core assumption is that the noise scale σ is small enough relative to embedding norms that perturbed embeddings remain in a semantically meaningful region of the continuous space the transformer was trained on. Break condition: If noise scale exceeds ~1.0× the RMS embedding norm, validation performance collapses.

### Mechanism 2
Training with continuous tokens preserves model diversity and out-of-distribution robustness better than hard token RL fine-tuning. Hard token training with discrete sampling reduces next-token entropy during CoT generation, causing overconfidence and narrower reasoning paths. Soft/fuzzy training maintains entropy profiles closer to the base model (Figure 4), which correlates with better pass@32 performance (more diverse valid solutions) and lower negative log-likelihood on out-of-domain benchmarks (Table 2). The continuous mixture representation may act as implicit regularization against catastrophic specialization. Break condition: The benefit appears model-dependent; Llama-8B-Instruct shows dramatic generalization improvements while Qwen shows smaller gaps.

### Mechanism 3
Optimal deployment uses continuous CoT training with discrete (hard) token inference. Continuous training learns more robust internal representations of reasoning by exploring superpositions of reasoning paths during optimization. At inference time, discrete decoding on this better-trained model achieves the best pass@1 and pass@32. Soft/fuzzy inference (using continuous tokens at test time) provides no additional benefit and often underperforms hard inference (Tables 3-5 in Appendix E). Break condition: If your use case specifically requires diverse sampling at inference (e.g., generating multiple candidate solutions), the higher pass@32 from soft/fuzzy training still requires hard sampling to realize.

## Foundational Learning

- **Concept: Reinforce with Leave-One-Out (RLOO) Baseline**
  - Why needed here: The paper uses RLOO to reduce variance in policy gradient estimates. Without a baseline, Reinforce has high variance; RLOO computes a per-prompt baseline by averaging rewards from other samples in the same batch.
  - Quick check question: Given G=32 samples per prompt with rewards [0, 0, 100, 0, ...], what is the LOO baseline for the sample that got reward 100?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The entire method operates on the CoT phase—intermediate "thinking" tokens before the final answer. Understanding that CoT extends reasoning capacity through sequential decomposition is prerequisite.
  - Quick check question: Why might a model with CoT solve problems that a direct-answer model cannot, even with the same total compute?

- **Concept: Policy Gradient Log-Probability Decomposition**
  - Why needed here: The method requires computing log π(h̃^0) for continuous embeddings. Understanding that sequential decisions factor as sum of conditional log-probs (Eq. 9) enables the Gaussian log-prob calculation.
  - Quick check question: For a trajectory of T continuous tokens, how does the log-probability decompose, and why does the Gaussian form (Eq. 10) matter for gradient flow?

## Architecture Onboarding

- **Component map:**
Input Tokens → Embedding Layer (E) → [NOISE INJECTION POINT] → Transformer Stack (T) → Output Embeddings (h^L) → Decode Layer (W_d) → Softmax → Next-token Probs (p_t)

During CoT phase:
p_t → Weighted mixture h^0_{t+1} = p_t @ E → Add noise → h̃^0_{t+1} → Feed to transformer → Repeat until stopping criterion

- **Critical path:**
1. Implement the soft/fuzzy token generation loop (modify generation code to accumulate p_t @ E instead of sampling)
2. Add Gaussian noise injection at embedding level with configurable scale σ
3. Implement the Reinforce loss with log π(h̃^0) computed via Eq. 10
4. Ensure early stopping criterion monitors a "shadow" greedy sequence for "The final answer is:" trigger

- **Design tradeoffs:**
- **Temperature τ:** τ=0.5 (soft) vs τ→0 (fuzzy). Fuzzy is closer to discrete tokens; soft maintains more mixture information. Paper finds both work; fuzzy may be more stable.
- **Noise scale σ:** Paper recommends 0.33× RMS embedding norm. Higher values (≤1.0) work; ≥3.0 causes collapse.
- **CoT length:** Training uses 128-512 tokens; longer CoTs are now feasible since BPTT isn't required.
- **Assumption:** RLOO batch size (G=32 samples per prompt) affects baseline quality; smaller G increases variance.

- **Failure signatures:**
- Validation accuracy plateaus early or decreases → noise scale too high (check σ/embedding_norm ratio)
- Pass@1 much worse than base model → temperature too high during training or learning rate issues
- Model ignores stopping criterion → shadow greedy sequence not implemented correctly for soft/fuzzy mode
- Out-of-domain NLL degrades significantly → you're doing hard training, not soft/fuzzy

- **First 3 experiments:**
1. **Sanity check:** Train Llama-3B-Instruct on GSM8K with fuzzy tokens (τ=0.0001, σ=0.33×RMS) for 500 steps. Compare greedy pass@1 on GSM8K validation against hard-token baseline. Expect similar performance.
2. **Diversity probe:** On the checkpoint from #1, compute pass@32 with hard sampling. Compare to hard-token baseline. Expect ~3-5 percentage point improvement if the mechanism is working.
3. **OOD test:** Evaluate the checkpoint on ARC/HellaSwag. Compare NLL of correct answers against base model and hard-token baseline. Soft/fuzzy should preserve NLL; hard may degrade it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions, if any, does soft inference provide measurable benefits over hard inference for models trained with soft/fuzzy methods?
- Basis in paper: The authors state: "we do not confirm previously reported benefits of soft inference on hard (normal) training (Zhang et al., 2025)" and "hard inference on all models achieves the best performance."
- Why unresolved: The paper directly contradicts prior claims from Soft Thinking (Zhang et al., 2025) but does not explain the discrepancy. The theoretical motivation for soft inference remains, yet empirical support is absent.
- What evidence would resolve it: A systematic study varying model architectures, training regimes, and task types to identify conditions where soft inference helps, or a formal analysis showing why soft inference cannot improve upon hard decoding in the RL setting.

### Open Question 2
- Question: Can the theoretical expressivity advantages of continuous CoTs (proven for graph reachability) be realized as pass@1 improvements on practical reasoning benchmarks?
- Basis in paper: The paper cites Zhu et al. (2025a) proving "continuous tokens have much greater expressivity and can solve specific problems more efficiently" but finds only "pass@1 parity" in practice.
- Why unresolved: The gap between theoretical efficiency gains (O(n) vs O(n²) for graph reachability) and empirical results (no pass@1 improvement) suggests either the benchmarks do not require the expressivity advantage, or current training methods fail to exploit it.
- What evidence would resolve it: Identifying benchmark tasks where continuous CoT training yields pass@1 gains over hard training, or proving bounds on when expressivity advantages manifest.

### Open Question 3
- Question: What explains the divergent entropy profiles between Llama and Qwen base models under temperature sampling during CoT generation?
- Basis in paper: The authors observe "entropy blowup" in Llama base models during hard sampling that is "not present on Qwen" but state "Analyzing base models is not our main topic, but we still report how this entropy profile changes."
- Why unresolved: The architectural or pretraining differences causing this behavior are not investigated, yet the phenomenon may affect how different model families respond to soft/fuzzy training.
- What evidence would resolve it: Controlled experiments swapping attention mechanisms, training objectives, or data distributions between model families to isolate the cause of entropy divergence.

## Limitations

- **Experimental scope uncertainty**: The paper evaluates primarily on mathematical reasoning benchmarks with a single model family, leaving non-mathematical reasoning tasks and open-ended problem solving untested.
- **Noise scale sensitivity**: The optimal noise scale (0.33×RMS) is determined empirically for Llama-3.2 models, but sensitivity across different model scales and domains is not characterized.
- **Generalization mechanism ambiguity**: While the paper shows soft/fuzzy training preserves entropy and OOD performance, the causal mechanism linking these factors is not definitively established.

## Confidence

- **High confidence**: The core technical contribution (noise-injected continuous token RL with tractable gradient computation) is well-supported by mathematical derivation and implementation details.
- **Medium confidence**: The claim that soft/fuzzy training better preserves out-of-distribution performance is supported by NLL comparisons, but effect size varies significantly by model.
- **Low confidence**: The recommendation for discrete inference on continuously-trained models contradicts prior work on Soft Thinking, though empirical results support this approach.

## Next Checks

1. **Cross-domain generalization test**: Evaluate the soft/fuzzy trained Llama-8B-Instruct on non-mathematical reasoning tasks (e.g., strategy game play, code generation with complex logic, or commonsense reasoning benchmarks).

2. **Ablation on noise scale sensitivity**: Systematically vary σ from 0.1× to 1.0× RMS(embedding norm) and measure the tradeoff between training stability, pass@1, pass@32, and OOD NLL preservation.

3. **Comparison with continuous pretraining**: Compare soft/fuzzy RL training against models that undergo continuous token pretraining (using Soft Thinking-style objectives) before RL fine-tuning.