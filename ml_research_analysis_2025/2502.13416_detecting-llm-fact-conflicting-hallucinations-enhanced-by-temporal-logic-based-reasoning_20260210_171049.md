---
ver: rpa2
title: Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based
  Reasoning
arxiv_id: '2502.13416'
source_url: https://arxiv.org/abs/2502.13416
tags:
- knowledge
- reasoning
- llms
- test
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting fact-conflicting
  hallucinations (FCH) in large language models (LLMs), where outputs contradict established
  facts. The authors propose DROWZEE, an automated end-to-end metamorphic testing
  framework that utilizes temporal logic to systematically detect FCH.
---

# Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning

## Quick Facts
- **arXiv ID**: 2502.13416
- **Source URL**: https://arxiv.org/abs/2502.13416
- **Reference count**: 40
- **Primary result**: DROWZEE detects 24.7-59.8% non-temporal and 16.7-39.2% temporal hallucinations across nine LLMs and nine knowledge domains

## Executive Summary
This paper introduces DROWZEE, an automated metamorphic testing framework for detecting fact-conflicting hallucinations (FCH) in large language models. The framework leverages temporal logic to systematically generate complex test cases that reveal when LLM outputs contradict established facts. By crawling knowledge from sources like Wikipedia and applying logical reasoning rules, DROWZEE creates diverse test scenarios that stress-test LLM knowledge consistency. The approach demonstrates that LLMs struggle significantly with out-of-distribution knowledge and logical reasoning, achieving detection rates ranging from 24.7% to 59.8% for non-temporal hallucinations and 16.7% to 39.2% for temporal hallucinations across multiple domains and models.

## Method Summary
DROWZEE operates as an end-to-end metamorphic testing framework that combines knowledge extraction, temporal logic generation, and semantic validation. The system first crawls factual knowledge from sources such as Wikipedia, then applies temporal logic rules to transform these facts into complex test cases that preserve logical relationships while introducing variations. These metamorphic test cases are fed to LLMs, and their outputs are validated against ground truth using semantic-aware oracles. The framework systematically tests nine LLMs across nine knowledge domains, revealing significant hallucination rates that vary by domain type and logical complexity. The approach specifically targets fact-conflicting hallucinations where outputs directly contradict established knowledge.

## Key Results
- DROWZEE detects 24.7-59.8% non-temporal fact-conflicting hallucinations across nine LLMs
- Temporal-related hallucination detection rates range from 16.7-39.2% across tested models
- Performance varies significantly across knowledge domains, with some domains showing higher vulnerability to FCH
- LLMs demonstrate particular weakness in handling out-of-distribution knowledge and complex logical reasoning scenarios

## Why This Works (Mechanism)
DROWZEE works by exploiting the systematic nature of knowledge contradictions through metamorphic testing. The framework generates test cases that preserve logical relationships while introducing controlled variations, allowing detection of when LLMs produce outputs that violate established facts. Temporal logic enables the creation of complex, multi-step reasoning scenarios that stress-test both factual knowledge and temporal consistency. The semantic-aware oracle provides precise validation by comparing LLM outputs against ground truth extracted from reliable sources, enabling automated detection without human intervention.

## Foundational Learning

**Temporal Logic**: Formal system for reasoning about time-dependent propositions
- Why needed: Enables creation of complex test cases that preserve logical relationships while varying temporal aspects
- Quick check: Can generate valid logical transformations that maintain truth conditions while changing temporal structure

**Metamorphic Testing**: Software testing technique that verifies outputs through relationship-based validation
- Why needed: Allows detection of hallucinations without requiring exhaustive ground truth for all possible inputs
- Quick check: Can transform test inputs while preserving expected output relationships

**Semantic-aware Oracles**: Validation systems that understand meaning rather than just surface patterns
- Why needed: Enables accurate detection of fact-conflicting hallucinations beyond simple string matching
- Quick check: Can correctly identify semantic contradictions between LLM outputs and ground truth

**Knowledge Crawling**: Automated extraction of factual information from reliable sources
- Why needed: Provides diverse, real-world knowledge base for generating test cases
- Quick check: Can extract consistent, verifiable facts across multiple domains

## Architecture Onboarding

**Component Map**: Wikipedia/Knowledge Source -> Fact Extractor -> Temporal Logic Generator -> LLM Input Processor -> Semantic Oracle -> Results Analyzer

**Critical Path**: Knowledge extraction → Temporal logic generation → LLM inference → Semantic validation → Results aggregation

**Design Tradeoffs**: 
- Breadth vs depth of knowledge crawling: More domains increase coverage but reduce depth per domain
- Logical complexity vs detection efficiency: More complex temporal rules improve detection but increase computational cost
- Wikipedia dependency: Provides rich knowledge but introduces potential bias and quality variation

**Failure Signatures**: 
- High false positive rates indicate oracle sensitivity issues
- Low detection rates suggest inadequate temporal logic coverage
- Domain-specific failures reveal knowledge gaps in particular areas

**3 First Experiments**:
1. Test single LLM on single domain with basic temporal transformations to establish baseline detection
2. Vary logical complexity of transformations to find detection threshold for each model
3. Compare detection rates across knowledge sources to validate Wikipedia independence

## Open Questions the Paper Calls Out
None

## Limitations
- Knowledge crawling from Wikipedia may introduce biases and quality variations affecting detection accuracy
- Temporal logic generation may not capture all real-world hallucination patterns across diverse domains
- Semantic oracle effectiveness depends on ground truth quality and may struggle with ambiguous or context-dependent facts

## Confidence

**Detection Rates**: High confidence
- 24.7-59.8% (non-temporal) and 16.7-39.2% (temporal) hallucination rates are directly measured through systematic testing

**Framework Generalizability**: Medium confidence
- Validated across nine knowledge domains but limited to specific areas tested

**Scalability**: Low confidence
- Framework adaptation required for arbitrary knowledge domains beyond tested scenarios

## Next Checks

1. Test DROWZEE on knowledge domains outside the nine evaluated to assess framework adaptability and robustness

2. Compare detection performance using multiple knowledge sources (not just Wikipedia) to validate source-independence

3. Evaluate the framework's ability to detect hallucinations in real-world deployment scenarios with dynamic, evolving knowledge bases rather than static crawled data