---
ver: rpa2
title: An Active Inference Model of Covert and Overt Visual Attention
arxiv_id: '2505.03856'
source_url: https://arxiv.org/abs/2505.03856
tags:
- attention
- sensory
- visual
- target
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an active inference model of covert and overt
  visual attention that dynamically optimizes sensory precisions to minimize free-energy.
  The model uses belief updating through attractor goals and error updates, with a
  bottom-up attention module regulating attention via dynamic sensory precisions based
  on environmental beliefs and sensory input.
---

# An Active Inference Model of Covert and Overt Visual Attention

## Quick Facts
- arXiv ID: 2505.03856
- Source URL: https://arxiv.org/abs/2505.03856
- Reference count: 37
- Key outcome: Active inference model with precision optimization successfully captures covert and overt visual attention dynamics including Posner cueing effects and inhibition of return-like behavior

## Executive Summary
This paper presents an active inference model that dynamically optimizes sensory precisions to regulate both covert (attention shifts without eye movements) and overt (saccadic eye movements) visual attention. The model uses a variational autoencoder to encode visual input and computes precision-weighted prediction errors that drive belief updating and action selection. Tested in simulated Posner cueing and target focus tasks, the model reproduces human-like attention patterns including faster responses to valid cues, inhibition of return effects, and faster involuntary than voluntary saccades.

## Method Summary
The model implements active inference with precision-weighted prediction errors, using a disentangled VAE for visual encoding and dynamic RBF-based precision matrices to modulate attention. Belief updates combine bottom-up sensory errors and top-down attractor goals, while action signals emerge from precision gradients. The model was tested in ROS/Gazebo with 32×32×3 RGB images, measuring reaction times across exogenous/endogenous and valid/invalid cue conditions, with additional overt attention experiments comparing reflexive versus intentional saccades.

## Key Results
- Exogenous and valid cues produced faster reaction times compared to endogenous and invalid cues, matching human attention studies
- Model exhibited inhibition of return-like behavior where previously attended locations became suppressed after specific cue-target onset asynchrony intervals
- Involuntary, reflexive saccades occurred faster than intentional ones but at the expense of adaptability
- Reaction times increased with target eccentricity, demonstrating location-based attention effects

## Why This Works (Mechanism)

### Mechanism 1: Precision-Weighted Prediction Errors Drive Attention Allocation
Attention emerges from dynamic optimization of sensory precisions, which weight how strongly prediction errors influence belief updating. The model computes sensory precision Πs using a radial basis function (RBF) centered on the covert attention belief [μamp, μu, μv]. Precision decreases with distance from this center, mimicking foveation. Higher precision at a location amplifies prediction errors from that region, causing beliefs to update more strongly toward salient stimuli there.

### Mechanism 2: Belief Updates Combine Bottom-Up Error Gradients and Top-Down Attractors
Covert attention shifts arise from free-energy gradients with respect to beliefs, combining likelihood errors (bottom-up) and intention errors (top-down). The belief update includes terms from sensory prediction errors and from system dynamics/flexible intentions. Exogenous cues drive updates via VAE decoder gradients; endogenous cues require interpreting symbolic input through higher-level intentions.

### Mechanism 3: Action Signals Emerge from Precision Gradients with Respect to Sensory Input
Overt saccades arise from free-energy minimization with respect to action, where bottom-up precision gradients create reflexive orienting. The action update includes precision-gradient terms that depend only on sensory input. These produce involuntary saccades toward salient features. Top-down action uses proprioceptive prediction errors driven by intentional attractors.

## Foundational Learning

- **Concept: Variational Free-Energy and the Evidence Lower Bound**
  - Why needed: The entire model is framed as minimizing free-energy F = −L(q) = DKL[q(z)||p(z|s)] − ln p(s)
  - Quick check: Can you explain why minimizing free-energy bounds surprise, and how the KL divergence term regularizes the approximate posterior?

- **Concept: Precision as Inverse Variance in Gaussian Models**
  - Why needed: Precision matrices Π weight prediction errors. Higher precision = lower uncertainty = stronger influence on updates
  - Quick check: If sensory precision at location A is 10× higher than at location B, how does this affect the relative contribution of prediction errors from A vs. B to belief updates?

- **Concept: Disentangled Variational Autoencoders**
  - Why needed: The VAE decoder serves as the generative model g(μ) for visual predictions
  - Quick check: What does it mean for a VAE latent space to be "disentangled," and why does this matter for grounding beliefs in interpretable variables?

## Architecture Onboarding

- **Component map:** Visual input → VAE encoder → visual belief update → covert attention belief → precision RBF → Πs → belief and action updates
- **Critical path:** 1) Visual input → VAE encoder → visual belief update via likelihood error 2) Belief → precision RBF → Πs → modulates all error gradients 3) Precision gradients → action signals → camera orientation changes 4) Symbolic cue → interoceptive channel → flexible intentions → top-down attractors
- **Design tradeoffs:** RBF precision shape chosen to push covert attention toward high-error regions; diagonal precision assumption simplifies computation but ignores pixel correlations; color-based saliency tied to red objects; single covert focus cannot track multiple simultaneous attention loci
- **Failure signatures:** Reaction times fail to vary with cue validity → precision not properly modulating error gradients; beliefs oscillate or diverge → learning rate too high or precision values exploding; no IOR-like behavior emerging → CTOA intervals too short or precision dynamics insufficient
- **First 3 experiments:** 1) Validate precision modulation: Run Posner task with fixed precision; confirm reaction time differences disappear or invert 2) Ablate bottom-up precision gradients: Remove precision-gradient terms from action update; verify reflexive saccades slow or fail while intentional saccades remain 3) Test eccentricity scaling: Vary target distance systematically; confirm reaction times increase with eccentricity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the emergent inhibition of return (IOR)-like behavior in the model accurately replicate the mechanisms and time course of biological IOR?
- Basis: "Although not explicitly modeled, this behavior is similar to an attentional mechanism of inhibition of return (IOR)... Since this was not explicitly modeled, this model behavior will be examined in future work."
- Why unresolved: IOR-like suppression emerged spontaneously at longer CTOA intervals without being designed into the model, and the underlying computational mechanism remains unanalyzed
- What evidence would resolve it: Systematic ablation studies identifying which model components are necessary for IOR emergence, combined with comparison of the model's temporal dynamics to human IOR experimental data

### Open Question 2
- Question: Can the model scale to multiple simultaneous targets while preserving the observed distinction between object-based and location-based attention effects?
- Basis: "Bottom-up overt orienting can only effectively orient to one point in the visual area... We leave multiple-object overt attention for future work."
- Why unresolved: The current single-target architecture cannot address how competitive selection occurs among multiple salient objects or how object-based vs location-based attention interact
- What evidence would resolve it: Extending the model with multiple intentions/targets and testing on multi-object visual search tasks, comparing reaction time patterns to human data on object-based and location-based attention

### Open Question 3
- Question: How would incorporating learning processes for precisions and generative model parameters affect attentional development and adaptation?
- Basis: "In this paper we only consider perception and action, and leave the learning processes of attention for future work."
- Why unresolved: The model uses fixed VAE weights and empirically-chosen precision function parameters, whereas biological attention systems adapt through experience
- What evidence would resolve it: Implementing online precision learning and testing whether attentional performance improves with experience in novel environments or with new object categories

## Limitations

- Key hyperparameters (learning rates, integration step sizes, noise covariances) are not specified in the paper, making exact reproduction difficult
- The bottom-up action mechanism assumes a direct color-centroid-to-camera-angles mapping rather than a learned inverse model, limiting generalization
- The model uses a single RBF-based precision spotlight, preventing simulation of divided or multiple-object attention scenarios

## Confidence

- High confidence: The core free-energy formulation and precision-weighted prediction error mechanism are mathematically sound and align with established active inference theory
- Medium confidence: The behavioral results are plausible but rely on unspecified numerical parameters that could significantly affect outcomes
- Low confidence: The generalizability of the model beyond the specific experimental conditions remains untested

## Next Checks

1. **Parameter sensitivity analysis:** Systematically vary the learning rate, temporal order count, and noise covariances to determine their impact on reaction time patterns and belief convergence
2. **Cross-task transfer validation:** Test whether a model trained on Posner cueing can generalize to novel visual search tasks with different target-distractor configurations
3. **Neural plausibility mapping:** Compare the model's precision modulation dynamics to neurophysiological recordings of gain modulation in visual cortex during attention tasks