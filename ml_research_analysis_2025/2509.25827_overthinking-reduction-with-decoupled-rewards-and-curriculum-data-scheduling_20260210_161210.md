---
ver: rpa2
title: Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling
arxiv_id: '2509.25827'
source_url: https://arxiv.org/abs/2509.25827
tags:
- tokens
- reasoning
- decs
- which
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DECS addresses the overthinking problem in large reasoning models
  by introducing a decoupled token-level reward mechanism that distinguishes essential
  reasoning steps from redundant tokens. The method employs a lightweight NRP detector
  to identify minimal reasoning prefixes and assigns lower rewards to tokens beyond
  this prefix, while using curriculum scheduling to balance exploration.
---

# Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling

## Quick Facts
- arXiv ID: 2509.25827
- Source URL: https://arxiv.org/abs/2509.25827
- Authors: Shuyang Jiang; Yusheng Liao; Ya Zhang; Yanfeng Wang; Yu Wang
- Reference count: 40
- Key outcome: DECS reduces reasoning tokens by >50% across 7 benchmarks while maintaining or improving performance

## Executive Summary
DECS addresses the overthinking problem in large reasoning models by introducing a decoupled token-level reward mechanism that distinguishes essential reasoning steps from redundant tokens. The method employs a lightweight NRP detector to identify minimal reasoning prefixes and assigns lower rewards to tokens beyond this prefix, while using curriculum scheduling to balance exploration. Experimental results show DECS achieves superior efficiency without compromising reasoning quality, reducing token counts by over 50% while maintaining or improving accuracy across multiple benchmarks.

## Method Summary
DECS implements a three-stage approach: first, it trains a 1.5B parameter NRP detector to identify the minimal reasoning prefix that contains sufficient information to derive correct answers; second, it applies decoupled token-level rewards where NRP tokens receive maximum positive reward and redundant tokens receive progressively lower rewards; third, it uses curriculum prompt scheduling to dynamically adjust the ratio of easy prompts based on batch redundancy patterns. The method builds on GRPO/REINFORCE++ frameworks and uses veRL for implementation, achieving >50% token reduction without accuracy degradation.

## Key Results
- >50% token reduction across seven benchmarks (AIME2024/2025, AMC23, MATH500, OlympiadBench, GPQA-Diamond, LiveCodeBench-v6)
- Maintained or improved pass@1 accuracy compared to baseline DeepSeek-R1-Distill models
- Reduced reasoning token count from 10.7K to 4.8K on AIME2025 while improving accuracy from 28.0% to 33.3%
- Achieved 2.2× efficiency improvement on MATH500 with 91.8% accuracy vs baseline 91.6%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupled token-level rewards prevent misalignment between sequence-level penalties and token-level optimization.
- Mechanism: DECS assigns differentiated rewards based on token position relative to NRP, with NRP tokens receiving maximum positive reward while redundant tokens receive progressively lower rewards, ensuring negative advantages regardless of sequence length.
- Core assumption: NRP boundary can be reliably detected and tokens beyond this point contribute no additional reasoning value.
- Evidence anchors: Abstract states "decoupled token-level reward mechanism that surgically distinguishes and penalizes redundant tokens"; Section 4.2 defines decoupled reward function; DRPO (arXiv:2510.04474) validates decoupled reward approaches.

### Mechanism 2
- Claim: NRP detector enables precise identification of minimal sufficient reasoning.
- Mechanism: Lightweight 1.5B model segments reasoning using discourse markers and judges whether each chunk contains sufficient information to derive correct answer.
- Core assumption: Discourse markers reliably segment reasoning into semantically coherent units that generalize from training data.
- Evidence anchors: Section 4.1 describes judge model training; Appendix E.6 reports >99% accuracy on development set; limited corpus support for this specific detection approach.

### Mechanism 3
- Claim: Curriculum prompt scheduling maintains exploration capacity by adaptively limiting easy prompts.
- Mechanism: Scheduler dynamically adjusts easy prompt ratio (κm) based on batch NRP ratio, increasing κ when redundancy is high and adjusting to maintain condition κ·σL < C.
- Core assumption: Easy prompts produce uniformly correct responses with varying lengths, creating negative advantages that penalize high-entropy tokens.
- Evidence anchors: Section 4.3 defines curriculum update rule; Section 3.2 establishes mathematical condition; limited corpus support for curriculum approaches.

## Foundational Learning

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: DECS builds on GRPO's advantage estimation; understanding how advantages are computed per-group is essential to see why sequence-level penalties corrupt token-level gradients.
  - Quick check question: Given 4 rollouts for a prompt with rewards [1.0, 0.8, 0.6, 0.4], what is the advantage of the first rollout?

- Concept: **High-entropy tokens in reasoning**
  - Why needed here: These tokens (e.g., "wait", "however", "alternatively") are identified as critical for exploration but vulnerable to suppression under length penalties.
  - Quick check question: Why would a model's tendency to generate "Wait, let me reconsider" be harmed by penalizing long correct sequences?

- Concept: **Autoregressive generation and prefix sufficiency**
  - Why needed here: NRP concept depends on understanding that once correct answer is derivable, subsequent tokens are generated purely from autoregressive continuation without adding value.
  - Quick check question: If a model has derived "x = 5" and the answer is 5, what tokens would constitute redundancy vs. NRP?

## Architecture Onboarding

- Component map:
  Base Policy -> NRP Detector -> Reward Engine -> Curriculum Scheduler -> Training Framework

- Critical path:
  1. Generate G rollouts per prompt (default: 16)
  2. Segment reasoning using discourse markers
  3. Query NRP detector on each chunk until "yes" judgment
  4. Assign token-level rewards per Equation 9
  5. Compute advantages per Equation 10
  6. Update curriculum ratio κ based on batch NRP statistics
  7. Apply PPO surrogate loss with token-level advantages

- Design tradeoffs:
  - NRP detector size vs. accuracy: 1.5B model adds ~5% training overhead but achieves >99% accuracy
  - β hyperparameter: Controls curriculum adaptation speed; too aggressive (0.5+) causes instability
  - Reward gap (r+ − r0): Larger gaps enforce stronger redundancy penalization but may over-constrain

- Failure signatures:
  - Performance degradation with efficiency gains: Indicates κ too high or NRP detection cutting essential tokens
  - High residual redundancy (PNRP < 80%): Suggests reward gap insufficient or curriculum not adapting
  - Training instability: Check β value and ensure κ clipping correctly to [0, κ0]
  - Detector overhead >10%: vLLM serving may be misconfigured

- First 3 experiments:
  1. Validate NRP detector in isolation: Run detector on held-out reasoning traces, manually verify first "yes" judgment aligns with minimal sufficient prefix
  2. Ablate curriculum scheduling: Train with decoupled rewards but fixed κ (e.g., κ = 0)
  3. Stress test reward sensitivity: Vary r+ ∈ [1.05, 1.2] with fixed r0 = 1.0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can NRP detection be integrated directly into the reasoning policy via internal signals (confidence or entropy) rather than relying on a separate auxiliary model?
- Basis in paper: Appendix C states "Integrating NRP detection directly into the policy, e.g., via confidence or entropy signals, is a promising future direction."
- Why unresolved: Current implementation requires separate 1.5B parameter judge model, adding 3-5% training overhead
- What evidence would resolve it: Demonstration of entropy or confidence-based stopping criterion achieving comparable PNRP scores without auxiliary model supervision

### Open Question 2
- Question: How does DECS performance scale to models significantly larger than 7B parameters?
- Basis in paper: Appendix C: "we evaluate DECS on models up to 7B due to resource constraints... we expect it to scale smoothly to larger architectures"
- Why unresolved: Empirical validation limited by computational resources; theoretical expectations remain untested at larger scales
- What evidence would resolve it: Evaluation on 70B+ models showing consistent >50% token reduction without accuracy degradation

### Open Question 3
- Question: To what extent does math-specialized NRP detector generalize to non-mathematical reasoning domains without domain-specific fine-tuning?
- Basis in paper: Section 5.2 states "the NRP detector is specialized for math reasoning and the training data only cover the math corpus" yet shows strong OOD results
- Why unresolved: Strong performance on GPQA-D and LiveCodeBench was unexpected given math-only detector training
- What evidence would resolve it: Ablation studies comparing math-only vs. multi-domain NRP detector training across diverse reasoning task families

### Open Question 4
- Question: Would finer-grained hyperparameter search for curriculum scheduling coefficient β yield superior efficiency-accuracy trade-offs?
- Basis in paper: Appendix F.1: "a more fine-grained search value like 0.25 may bring a better trade-off, but we leave it for future research"
- Why unresolved: Grid search limited to {0.0, 0.1, 0.2, 0.3, 0.5}; intermediate values and adaptive schemes remain unexplored
- What evidence would resolve it: Systematic search over β ∈ [0.15, 0.30] with continuous interpolation

## Limitations

- The NRP detection mechanism relies on discourse marker segmentation without empirical validation that these markers consistently align with actual logical reasoning boundaries
- The curriculum scheduling mechanism's effectiveness depends on the assumption that easy prompts produce uniformly correct responses with varying lengths, which may not hold across all problem distributions
- The claim that high-entropy tokens are specifically vulnerable to suppression under length penalties requires more direct evidence

## Confidence

- **High Confidence**: Decoupled reward mechanism's mathematical formulation and empirical results showing >50% token reduction while maintaining/improving performance
- **Medium Confidence**: NRP detector design and reported accuracy are credible, but reliance on discourse markers introduces generalization uncertainty
- **Low Confidence**: Claim about high-entropy token suppression requires more direct evidence

## Next Checks

1. **NRP Boundary Validation**: Manually annotate NRP boundaries in 100 held-out reasoning traces using expert judges, then compare against NRP detector predictions to quantify false positive/negative rates and their correlation with performance drops

2. **Token Type Analysis**: Analyze distribution of high-entropy tokens (discourse markers, re-evaluation phrases) in reasoning traces before and after DECS training, stratified by reward configuration, to directly test suppression hypothesis

3. **Curriculum Stability Testing**: Run curriculum scheduling with extreme β values (0.5 and 0.05) to identify stability threshold and measure how quickly κ adapts to changing redundancy patterns across training epochs