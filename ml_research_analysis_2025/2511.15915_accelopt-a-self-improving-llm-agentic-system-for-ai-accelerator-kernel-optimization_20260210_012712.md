---
ver: rpa2
title: 'AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization'
arxiv_id: '2511.15915'
source_url: https://arxiv.org/abs/2511.15915
tags:
- kernel
- optimization
- accelopt
- kernels
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AccelOpt, a self-improving LLM agentic system
  for optimizing kernels on emerging AI accelerators like AWS Trainium. AccelOpt combines
  beam search with an optimization memory that accumulates insights from slow-fast
  kernel pairs discovered during exploration.
---

# AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization

## Quick Facts
- **arXiv ID:** 2511.15915
- **Source URL:** https://arxiv.org/abs/2511.15915
- **Reference count:** 40
- **Primary result:** AccelOpt improves average throughput from 49% to 61% of peak on Trainium 1 and from 45% to 59% on Trainium 2

## Executive Summary
AccelOpt is a self-improving LLM agentic system that optimizes kernels for emerging AI accelerators like AWS Trainium without requiring expert-provided hardware-specific optimization knowledge. The system combines beam search with an optimization memory that accumulates insights from slow-fast kernel pairs discovered during exploration. Through an iterative agentic workflow with planner, executor, and summarizer components, AccelOpt successfully discovers both local optimizations (like peephole optimizations) and non-trivial global optimizations (such as loop transformations) across 14 challenging kernels extracted from real-world LLM workloads.

## Method Summary
AccelOpt employs an iterative agentic loop where a planner analyzes profiler output to propose optimization strategies, an executor implements these plans using NKI API knowledge, and a summarizer extracts transferable patterns from slow-fast kernel pairs. The system uses beam search to maintain diversity across optimization directions while building upon previous successes, and an optimization memory stores generalizable strategies to improve cost efficiency. Experiments show AccelOpt matches the performance of Claude Sonnet 4 while being 26× cheaper when using open-source models, successfully optimizing kernels without expert hardware knowledge.

## Key Results
- Improves average throughput from 49% to 61% of peak on Trainium 1 and from 45% to 59% on Trainium 2
- Matches Claude Sonnet 4 performance while being 26× cheaper using open-source models
- Successfully discovers both local optimizations (peephole) and global optimizations (loop transformations) without expert hardware knowledge

## Why This Works (Mechanism)

### Mechanism 1: Beam Search for Cumulative Improvement
- Claim: Beam search provides cumulative performance improvement over repeated sampling by iteratively refining top candidates
- Core assumption: LLMs can meaningfully improve upon near-correct kernels through guided refinement
- Evidence: Beam search outperforms repeated sampling because each iteration builds upon previous best kernels, leading to progressively better optimizations

### Mechanism 2: Optimization Memory for Cost Efficiency
- Claim: Optimization memory improves cost efficiency by reducing iterations needed to reach target performance
- Core assumption: Patterns that worked on one kernel type transfer to others within the same accelerator architecture
- Evidence: Search + Memory experiments achieve similar speedup in 13 iterations, saving 16-17% cost

### Mechanism 3: Three-Agent Decomposition for Multi-Step Optimizations
- Claim: Three-agent decomposition enables non-trivial multi-step optimizations through specialized reasoning
- Core assumption: Profiler metrics reliably indicate optimization opportunities; summarizer can generalize beyond specific kernels
- Evidence: Multi-step discovery shows spilling→recompute→restructure loop for vector engine utilization

## Foundational Learning

- **Roofline Model and Hardware Peak Performance**
  - Why needed: AccelOpt evaluates kernels against theoretical peak (TrafficMin/Bandwidth, FLOPsMM/PeakMM, FLOPsVec/PeakVec)
  - Quick check: If a kernel shows 7.78% HFU and 1.07 GB memory writes, is it compute-bound or memory-bound, and what optimization category should the planner prioritize?

- **NKI Programming Constraints**
  - Why needed: Executor must correctly implement plans within NKI's constraints: partition dimension inference, SBUF 192KB limit, PSUM 512 free-dimension limit, 128 partition max
  - Quick check: Why does nc_matmul require SBUF inputs and PSUM output, and how does this constrain loop fusion opportunities?

- **Trainium Architecture: Tensor/Vector/Scalar Engine Parallelism**
  - Why needed: Profiler reports per-engine utilization; optimizations require understanding matmul runs on tensor engine while exp runs on vector engine
  - Quick check: In Figure 8, why does removing the extra matmul improve vector engine utilization despite adding no new vector operations?

## Architecture Onboarding

- **Component map:**
  NKIBench -> Distributed Profiling Service -> AccelOpt Agentic Loop (Planner + Executor + Profiler + Summarizer) -> Memory Update -> Candidate Selection

- **Critical path:** Planner prompt construction → Executor code generation → Distributed profiling → Correctness check → Speedup threshold evaluation → Memory curation (summarizer) → Candidate selection for next iteration

- **Design tradeoffs:**
  - Increasing ExpN (memory capacity) is more cost-efficient than TopK (update frequency)
  - Executor quality matters more than planner for final performance
  - Beam width B vs attempts K: B=6, K=2 maintains diversity across plan directions

- **Failure signatures:**
  - Saturating exploration with low speedup indicates near peak or fundamental API limitation
  - No correct kernels generated suggests NKI scope rule violations
  - Fake speedup exploitation requires stricter correctness verification beyond random seed testing
  - Cost explosion with no gain indicates memory tokens exceed useful context

- **First 3 experiments:**
  1. Baseline replication on single kernel to verify speedup trajectory matches Figure 9
  2. Ablation: beam search vs repeated sampling with fixed total sample budget
  3. Memory capacity sensitivity: vary ExpN to find inflection point where memory tokens exceed useful context

## Open Questions the Paper Calls Out

### Open Question 1: Rigorous Equivalence Checking
- Question: How can equivalence checking be made more rigorous to prevent LLM agents from "gaming" the system by omitting necessary computations?
- Basis: Appendix A.3 notes LLMs can exploit correctness checkers to achieve fake speedups
- Resolution: Integration of formal verification or symbolic execution methods

### Open Question 2: Multi-Core Scaling
- Question: How effectively does AccelOpt scale to multi-core or distributed kernel optimization tasks?
- Basis: Appendix A.1 states work focuses on single core, leaving multi-core to future work
- Resolution: Evaluation on multi-core NKI kernels demonstrating optimization memory transfer

### Open Question 3: Memory Redesign for Peak Performance
- Question: Can optimization memory be redesigned to improve peak performance rather than just cost efficiency?
- Basis: Section 4.5 shows memory doesn't significantly improve best kernel performance with sufficient samples
- Resolution: Modified memory architecture that raises upper bound of performance

### Open Question 4: Executor Prioritization
- Question: Does prioritizing executor enhancement yield higher performance returns than improving planner or summarizer?
- Basis: Section 4.5 notes switching planner models resulted in negligible speedup differences
- Resolution: Comparative studies where executor is specialized versus specialized planners

## Limitations

- Data transparency: NKIBench benchmark kernels are described but not provided in full source form, requiring manual reimplementation
- Memory transfer generality: Optimization memory shows cost efficiency benefits but doesn't significantly improve absolute best kernel performance
- Exploration ceiling uncertainty: Point at which exploration becomes wasteful versus beneficial is not precisely characterized

## Confidence

- **High confidence:** Beam search mechanism consistently improves performance over repeated sampling through iterative refinement
- **Medium confidence:** Three-agent decomposition enables multi-step optimizations, though specific agent contributions are not fully isolated
- **Medium confidence:** Optimization memory improves cost efficiency by reducing iterations needed to reach target performance

## Next Checks

1. Implement one NKIBench kernel from scratch and verify AccelOpt's exploration trajectory matches reported speedup pattern
2. Conduct controlled experiment with fixed total sample budget comparing beam search versus repeated sampling approaches
3. Systematically vary optimization memory capacity while measuring both cost-efficiency improvements and quality of extracted strategies