---
ver: rpa2
title: Demographically-Inspired Query Variants Using an LLM
arxiv_id: '2508.17644'
source_url: https://arxiv.org/abs/2508.17644
tags:
- query
- variants
- queries
- profiles
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes using an LLM to generate query variants that
  reflect diverse user profiles, addressing the gap in existing test collections that
  lack representation of varied user demographics. The authors generate query variants
  for three methods: persona-based, user group-based, and textual transformation-based.'
---

# Demographically-Inspired Query Variants Using an LLM

## Quick Facts
- arXiv ID: 2508.17644
- Source URL: https://arxiv.org/abs/2508.17644
- Authors: Marwah Alaofi; Nicola Ferro; Paul Thomas; Falk Scholer; Mark Sanderson
- Reference count: 40
- Key outcome: LLM-generated query variants reveal significant performance disparities across user demographics in IR system evaluation

## Executive Summary
This paper addresses the limitation of traditional IR test collections that use single queries to represent diverse user populations. The authors propose using an LLM (GPT-4) to generate demographically-inspired query variants that reflect different user profiles characterized by language proficiency, domain expertise, and other properties. They validate these variants through human assessment for semantic similarity and profile alignment, then evaluate 15 IR systems to reveal significant performance differences across user profiles. The method enables more nuanced system evaluation by highlighting how different user demographics experience varying levels of system effectiveness.

## Method Summary
The method uses GPT-4 with temperature=1 to generate three query variants per seed query-profile combination. Seed queries come from TREC DL21 (53) and DL22 (76) collections, and profiles span three methods: 6 personas, 8 user groups, and 4 textual transformations. Variants are validated through human assessment (10% sample) for semantic similarity and profile alignment, with automatic feature computation (length, readability, lexical diversity). The full variant set is evaluated using 15 IR systems (lexical, neural, hybrid) with GPT-4o-generated relevance labels. Performance is analyzed via NDCG@10 and three-way ANOVA to isolate effects of topic, system, and profile.

## Key Results
- LLM-generated variants are lexically different (Jaccard scores 0.35-0.44) but semantically similar (0.67-1.00) to seed queries
- Profile alignment varies by method: personas (0.79), user groups (0.78), textual transformations (0.76)
- Child, Non-native, and Novice profiles show significantly lower NDCG@10 scores compared to seeds and Expert profiles
- Profile effects explain 21-33% of variance in system performance (ω²=0.21-0.33)

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated query variants can simulate demographically distinct user queries while preserving semantic intent. GPT-4 is prompted with seed queries combined with transformation profiles, producing variants that reflect profile-specific formulation patterns. The core assumption is that the LLM has internalized patterns of how different demographics formulate queries. Evidence shows semantic similarity scores of 0.67-1.00, substantially exceeding the 25% chance baseline. Break condition occurs when profile descriptions are too vague or outside the LLM's training distribution.

### Mechanism 2
Human assessment validates that variants maintain semantic similarity while aligning with intended profiles. Two annotators independently assess 10% samples, requiring consensus for positive classification. Profile alignment is further validated through computed features showing statistically significant differences matching expectations. Semantic similarity scores range from 0.67-1.00 across profiles. Break condition occurs when seed queries are ambiguous or have multiple interpretations, increasing annotator disagreement.

### Mechanism 3
Query variants reveal statistically significant performance disparities across user profiles that are masked by single-query evaluation. Variants are run through 15 IR systems with GPT-4o-generated relevance labels validated against NIST judgments (κ=0.46-0.50). Three-way ANOVA isolates effects of topic, system, and profile, showing large profile effect sizes (ω²=0.21-0.33). Child, Non-native, and Novice profiles show significantly lower NDCG@10 scores. Break condition occurs if relevance labels are biased toward certain query formulations.

## Foundational Learning

- **Concept: Test Collections and the Single-Query Assumption**
  - Why needed here: The paper's core contribution challenges the assumption that one query adequately represents diverse users. Understanding how test collections work is essential to grasp why query variants matter.
  - Quick check question: Why might the same information need produce different queries from a domain expert vs. a novice?

- **Concept: Prompt Engineering with Structured Profiles**
  - Why needed here: The method relies on crafting detailed profile descriptions that the LLM can interpret. The prompt template and profile descriptions are the interface between user characteristics and variant generation.
  - Quick check question: What profile attributes would you include to simulate queries from a hearing-impaired user who relies on caption search?

- **Concept: ANOVA and Effect Size Interpretation**
  - Why needed here: The paper uses three-way ANOVA with ω² effect sizes to quantify how much variance in system performance is attributable to profiles vs. systems vs. topics.
  - Quick check question: A System×Profile interaction with ω²=0.01 (small) means what for the claim that systems serve users unequally?

## Architecture Onboarding

- **Component map:**
  Seed Queries (DL21/DL22) -> Profile Definitions (Personas, Groups, Textual Rules) -> GPT-4 Variant Generation (temp=1, 3 variants per profile) -> Validation Layer (Jaccard Index, Human Assessment, Feature Computation) -> Retrieval Layer: 15 IR systems -> Relevance Labeling: GPT-4o with backstories -> Analysis: NDCG@10 -> ANOVA -> Tukey HSD

- **Critical path:**
  1. Define profiles with sufficient specificity (background + influencing factors)
  2. Generate variants with temperature high enough for diversity but check quality
  3. Validate semantic similarity before running full evaluation
  4. Use backstories not seeds for relevance labeling to avoid seed-query bias
  5. Interpret profile effects through marginal means and confidence intervals

- **Design tradeoffs:**
  - Temperature=1 vs. lower: Higher temperature increases lexical diversity but risks semantic drift
  - 10% sampling vs. full assessment: Sampling reduces cost but may miss edge cases
  - Profile granularity: Personas capture interactions between traits but complicate attribution
  - LLM relevance labels vs. human: Enables large-scale evaluation but introduces potential bias

- **Failure signatures:**
  - Variants that pass semantic similarity but fail profile alignment suggest profile descriptions are not actionable
  - High annotator disagreement (>15%) on specific profiles suggests subjective or overlapping profile definitions
  - System rankings that invert dramatically across profiles indicate measuring fundamentally different tasks
  - GPT-4o labels that consistently favor one query style suggest label bias

- **First 3 experiments:**
  1. Replicate with one new profile: Define a profile not in the paper (e.g., dyslexic user), generate variants, assess semantic similarity, and compare NDCG@10 marginal means
  2. Ablate temperature: Generate variants at temperatures 0.3, 0.7, 1.0, 1.3 and measure trade-offs between lexical diversity and semantic drift
  3. Profile-specific relevance test: Select 5 queries where Child and Expert variants differ substantially, have human assessors judge relevance, and compare whether "relevant to child" and "relevant to expert" diverge

## Open Questions the Paper Calls Out
1. How does the evaluation of IR systems change when relevance judgments are generated specifically from the perspective of the issuing user profile rather than a standardized ground truth?
2. To what extent do LLM-generated query variants align with the actual query formulations of real users belonging to the target demographics?
3. Can IR systems be fine-tuned using demographically-inspired query variants to reduce the performance gap for disadvantaged user groups?

## Limitations
- Reliance on LLM-generated relevance labels without demographic-specific human validation introduces potential label bias
- Profile descriptions used to prompt GPT-4 are not fully specified, making exact replication challenging
- 10% human assessment sampling may miss edge cases where variants drift semantically or misalign with profiles

## Confidence
- **High:** Semantic similarity and profile alignment validation (human assessment shows 0.67-1.00 semantic similarity scores and significant feature differences)
- **Medium:** Profile effect sizes and performance disparities (ANOVA shows ω²=0.21-0.33 for profile effects, but LLM label bias cannot be ruled out)
- **Low:** Generalizability to real-world IR systems (tested on 15 research systems with curated relevance labels, not production environments)

## Next Checks
1. **Demographic relevance validation:** Have human assessors from representative demographic groups judge relevance for 20 Child vs. Expert variant pairs to test whether "relevant to child" diverges from "relevant to expert"
2. **Temperature sensitivity analysis:** Generate variants at temperatures 0.3, 0.7, 1.0, 1.3 for a 10-query subset and measure trade-offs between lexical diversity and semantic drift
3. **Cross-corpus replication:** Apply the method to a non-biomedical collection (e.g., news or social media queries) to test whether profile effects persist across domains