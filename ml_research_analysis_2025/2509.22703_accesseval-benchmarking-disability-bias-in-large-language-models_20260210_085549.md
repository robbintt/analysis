---
ver: rpa2
title: 'AccessEval: Benchmarking Disability Bias in Large Language Models'
arxiv_id: '2509.22703'
source_url: https://arxiv.org/abs/2509.22703
tags:
- disability
- bias
- response
- impairments
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AccessEval introduces a benchmark for evaluating disability bias
  in LLMs by comparing responses to neutral and disability-aware queries across six
  domains and nine disability types. Using metrics for sentiment, social perception,
  and factual accuracy, the study finds that disability-aware queries consistently
  yield more negative tones, increased stereotyping, and higher factual errors than
  neutral queries.
---

# AccessEval: Benchmarking Disability Bias in Large Language Models

## Quick Facts
- arXiv ID: 2509.22703
- Source URL: https://arxiv.org/abs/2509.22703
- Reference count: 40
- Primary result: Disability-aware queries yield more negative tone, stereotyping, and factual errors than neutral queries

## Executive Summary
AccessEval introduces a benchmark for evaluating disability bias in large language models by comparing responses to neutral and disability-aware queries across six real-world domains and nine disability types. Using metrics for sentiment, social perception, and factual accuracy, the study finds that disability-aware queries consistently yield more negative tones, increased stereotyping, and higher factual errors than neutral queries. Bias varies by domain and disability type, with mobility, speech, and hearing impairments most affected, while finance and healthcare domains show the strongest bias. Larger models improve factual accuracy but do not reduce bias in tone or social perception, indicating that scaling alone is insufficient.

## Method Summary
The AccessEval benchmark uses a dataset of 234 neutral queries paired with 2,106 disability-aware queries across 6 domains and 9 disability types. Researchers evaluated 21 closed- and open-source LLMs using zero-shot prompting with temperature=0.1 and max_tokens=1048. Model responses were assessed using three metrics: VADER sentiment scores, Regard social perception classification, and LLM Judge factual accuracy (validated against human annotations with Spearman's ρ > 0.75). Performance degradation was calculated as the percentage of disability-aware query responses scoring at least 5% worse than their paired neutral queries.

## Key Results
- Disability-aware queries consistently yield more negative sentiment, increased stereotyping, and higher factual errors than neutral queries
- Bias varies by domain and disability type, with mobility, speech, and hearing impairments most affected
- Larger models improve factual accuracy but do not reduce bias in tone or social perception
- LLM Judge scores correlate strongly with human evaluations (ρ > 0.75), validating automated bias assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comparing model responses to paired neutral and disability-aware queries isolates disability-specific bias by controlling for query difficulty
- Mechanism: Each disability-aware query (DQ) has a structurally matched neutral query (NQ); the difference in response quality across sentiment, social perception, and factual accuracy reveals systematic degradation attributable to disability context rather than query complexity
- Core assumption: The NQ-DQ pairs are semantically equivalent except for the disability context, and any performance difference is due to bias rather than inherent difficulty
- Evidence anchors: [abstract] "benchmark evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9 disability types using paired Neutral and Disability-Aware Queries"; [section 3.1] "To quantify bias in disability-related queries, we compare model responses to disability-aware queries (DQ) against corresponding neutral queries (NQ)"

### Mechanism 2
- Claim: Multi-metric evaluation (VADER sentiment, Regard social perception, LLM Judge factual accuracy) captures distinct, non-correlated dimensions of bias
- Mechanism: Each metric probes a different failure mode: tone/sentiment (VADER), stereotyping/social perception (Regard), and factual correctness/completeness (LLM Judge); their lack of correlation indicates bias manifests independently across dimensions
- Core assumption: These three metrics collectively cover the major ways disability bias appears in LLM outputs, and they are sufficiently distinct to avoid redundancy
- Evidence anchors: [abstract] "evaluated model outputs with metrics for sentiment, social perception, and factual accuracy"; [section 5.8] "three evaluation metrics... do not correlate, indicating that bias manifests in distinct ways rather than as a single trend"

### Mechanism 3
- Claim: LLM Judge, validated against human annotations (ρ > 0.75), provides a scalable, reliable automated bias assessment tool
- Mechanism: A strong LLM (Qwen2.5-72B) evaluates response quality using structured criteria (relevance, completeness, accuracy, clarity); human annotation on a subset establishes statistical correlation, validating the judge as a proxy for human evaluation
- Core assumption: LLM Judge scores generalize from the annotated subset to the full dataset, and the judge model does not introduce its own systematic biases
- Evidence anchors: [abstract] "LLM Judge scores correlate strongly with human evaluations (ρ > 0.75), validating its use as an automated bias assessment tool"; [section 5.6] "All LLM Judge scores show a strong correlation with human judgments, with Spearman's (ρ >0.75)"

## Foundational Learning

- Concept: Disability bias in LLMs
  - Why needed here: AccessEval operationalizes this as systematic response degradation when disability context is introduced, distinct from other demographic biases
  - Quick check question: How does disability bias differ from gender or racial bias in typical LLM outputs?

- Concept: Sentiment and social perception metrics (VADER, Regard)
  - Why needed here: These quantify tone and stereotyping, two of the three key bias dimensions AccessEval measures
  - Quick check question: Why might a response have neutral sentiment but still be biased in social perception?

- Concept: Statistical significance testing (t-tests, ANOVA)
  - Why needed here: Rigorous benchmarking requires confirming that observed NQ-DQ differences are systematic, not random noise
  - Quick check question: What does a low p-value (<0.05) in a paired t-test tell you about NQ vs. DQ response quality?

## Architecture Onboarding

- Component map: Dataset construction (persona generation → query pairs → validation) → Model inference (21 LLMs, zero-shot) → Evaluation (VADER, Regard, LLM Judge) → Analysis (degradation metrics, statistical tests, domain/disability breakdown)

- Critical path: Ensuring NQ-DQ semantic equivalence; selecting and validating LLM Judge; defining meaningful degradation thresholds (5% chosen empirically)

- Design tradeoffs: Synthetic data enables scale but may not reflect real user phrasing; single-turn evaluation misses multi-turn dynamics; LLM Judge reduces annotation cost but inherits model biases

- Failure signatures: High degradation in finance/healthcare domains; misapplied assistive technologies (e.g., screen readers recommended for hearing impairments); hallucinated programs (e.g., non-existent "mental health credit cards")

- First 3 experiments:
  1. Replicate benchmark on 2-3 models (e.g., LLaMA-3.1-8B, GPT-4o) across 2 domains to verify degradation patterns
  2. Test disability-aware prompting guardrails (as in Appendix D.2) to measure bias reduction
  3. Validate LLM Judge correlation with human ratings on a small annotated subset to ensure tool reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does disability bias manifest and evolve during multi-turn conversational interactions compared to the single-turn queries evaluated?
- Basis in paper: [explicit] Section 7 (Limitations) states, "We assess bias in single-turn, but LLM behavior can change over multi-turn conversations. Evaluating how well models switch context... is an important area."
- Why unresolved: The current AccessEval framework is restricted to single-turn prompt-response pairs and does not measure context retention or accumulated bias
- What evidence would resolve it: A longitudinal evaluation using multi-turn dialogue datasets specifically designed with disability contexts

### Open Question 2
- Question: Can techniques like Reinforcement Learning from Human Feedback (RLHF) effectively mitigate disability bias where model scaling has failed?
- Basis in paper: [explicit] Appendix F notes, "The effectiveness of reinforcement learning with human feedback (RLHF) and adversarial fine-tuning for disability fairness remains uncertain and warrants further study."
- Why unresolved: The study concludes that increasing model size improves accuracy but not sentiment or social perception, leaving the efficacy of specific debiasing training objectives unknown
- What evidence would resolve it: Comparative benchmarking of base models against RLHF-fine-tuned variants on the AccessEval dataset

### Open Question 3
- Question: Does the use of synthetic query generation accurately capture the nuances of real-world user queries regarding disability?
- Basis in paper: [explicit] Section 7 acknowledges, "Our dataset relies on synthetic data generated using LLM. Future work should explore user generated data collection methods."
- Why unresolved: Synthetic data may fail to capture the linguistic nuances, cultural variations, or specific phrasing used by individuals with disabilities in natural settings
- What evidence would resolve it: A comparative analysis between model performance on synthetic AccessEval prompts versus a dataset crowdsourced from actual users with disabilities

## Limitations

- The synthetic nature of NQ-DQ pairs may not capture the full range of real-world disability-related query phrasing and context
- LLM Judge validation was performed on a subset (1200 responses) and may not fully represent the broader dataset distribution
- The study only evaluates single-turn interactions, missing potential bias mitigation or amplification in multi-turn conversations

## Confidence

- **High Confidence**: The finding that larger models do not consistently reduce bias in tone and social perception dimensions; this is well-supported by the comparative analysis across model sizes
- **Medium Confidence**: The claim that mobility, speech, and hearing impairments face the most bias; while statistically significant, the sample size for some disability types may be limited
- **Medium Confidence**: The assertion that scaling alone is insufficient for bias reduction; though supported by the data, the study does not explore other architectural modifications that might help

## Next Checks

1. Replicate the benchmark on additional model families (e.g., Gemini, Grok) to test generalizability of the bias patterns across different architectures
2. Conduct human evaluation on a stratified sample of responses across all domains and disability types to validate the LLM Judge's performance on edge cases
3. Test the same NQ-DQ pairs with and without disability-aware prompting guardrails to quantify potential bias mitigation strategies