---
ver: rpa2
title: 'iLSU-T: an Open Dataset for Uruguayan Sign Language Translation'
arxiv_id: '2507.21104'
source_url: https://arxiv.org/abs/2507.21104
tags:
- language
- sign
- translation
- video
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces iLSU-T, the first large-scale dataset for
  Uruguayan Sign Language (LSU) translation, containing over 185 hours of interpreted
  sign language videos from public TV broadcasting with audio and text transcriptions.
  The dataset covers diverse topics and includes 18 professional interpreters, with
  a vocabulary of nearly 38k words.
---

# iLSU-T: an Open Dataset for Uruguayan Sign Language Translation

## Quick Facts
- arXiv ID: 2507.21104
- Source URL: https://arxiv.org/abs/2507.21104
- Reference count: 40
- First large-scale dataset for Uruguayan Sign Language translation with over 185 hours of interpreted sign language videos

## Executive Summary
This paper introduces iLSU-T, the first large-scale dataset for Uruguayan Sign Language (LSU) translation, containing over 185 hours of interpreted sign language videos from public TV broadcasting with audio and text transcriptions. The dataset covers diverse topics and includes 18 professional interpreters, with a vocabulary of nearly 38k words. A processing pipeline was developed to segment videos, recognize signers, generate captions, and manually align text with sign language content. Three state-of-the-art sign language translation methods were evaluated on iLSU-T, achieving BLEU-4 scores up to 3.82 and ROUGE-L scores up to 16.05 on the largest subset. Results highlight the need for more localized datasets and the challenges in aligning text and sign language content, with future work focusing on improving methods and enriching annotations.

## Method Summary
The iLSU-T dataset was created by processing over 185 hours of interpreted sign language videos from Uruguayan public television. A comprehensive processing pipeline was developed that includes video segmentation to isolate individual segments, signer recognition to identify different interpreters, caption generation for automatic text transcription, and manual alignment of text with sign language content. The dataset covers diverse topics including news, parliamentary sessions, and educational content, featuring 18 professional interpreters. The vocabulary encompasses nearly 38,000 unique words, making it a substantial resource for LSU translation research.

## Key Results
- iLSU-T contains over 185 hours of interpreted sign language videos from public TV broadcasting
- The dataset covers diverse topics and includes 18 professional interpreters with a vocabulary of nearly 38k words
- State-of-the-art translation methods achieved BLEU-4 scores up to 3.82 and ROUGE-L scores up to 16.05 on the largest subset

## Why This Works (Mechanism)
The dataset's effectiveness stems from its real-world broadcast content featuring professional interpreters, which provides authentic and diverse sign language usage. The multi-step processing pipeline ensures high-quality segmentation and alignment between visual sign content and corresponding text. By covering various topics and including multiple interpreters, the dataset captures the full range of LSU vocabulary and expression styles needed for robust translation models.

## Foundational Learning
- Sign Language Translation: The process of converting sign language videos into spoken/written language - needed for developing accessibility tools for the deaf community; quick check: verify model outputs match reference translations
- Video Segmentation: Dividing continuous video into meaningful segments - needed to isolate individual signs or utterances; quick check: ensure segment boundaries align with natural pauses
- Signer Recognition: Identifying different interpreters in video content - needed for tracking individual signing styles and building personalized models; quick check: confirm correct attribution of segments to interpreters
- Manual Text Alignment: Synchronizing written text with corresponding sign language content - needed for supervised learning of translation mappings; quick check: verify alignment accuracy through spot-checking
- Multi-modal Feature Extraction: Processing visual, temporal, and textual information together - needed for capturing the full context of sign language expressions; quick check: ensure all modalities are properly synchronized

## Architecture Onboarding

Component Map: Video Input -> Preprocessing Pipeline -> Feature Extraction (I3D) -> Translation Model -> Text Output

Critical Path: Video segmentation and signer recognition feed into feature extraction, which provides input to translation models that generate the final text output.

Design Tradeoffs: The authors chose RGB-only visual features using I3D networks for simplicity, but this ignores important skeletal and facial information that could improve translation accuracy.

Failure Signatures: Low BLEU/ROUGE scores indicate poor alignment between sign language content and text, while inconsistent signer recognition suggests segmentation issues.

First Experiments:
1. Test baseline translation performance using only RoI visual features
2. Evaluate the impact of different text alignment strategies on translation accuracy
3. Compare translation results across different topic domains to identify content-specific challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating skeleton-based data through one-stream or multi-modal approaches significantly outperform current RGB-only translation methods on the iLSU-T dataset?
- Basis in paper: [explicit] The authors state in Section VII that they will "explore strategies based on skeleton data, either using one-stream or multi-modal approaches" to address the limitations of methods based exclusively on visual features.
- Why unresolved: The current baselines rely entirely on global visual features extracted from the Region of Interest (RoI) using I3D networks, without distinguishing body parts.
- What evidence would resolve it: Comparative experiments on iLSU-T using architectures that process skeletal joint data alongside or instead of RGB features.

### Open Question 2
- Question: To what extent does the duplication of text phrases between training and test sets inflate performance metrics, and how does strict separation affect model generalization?
- Basis in paper: [explicit] Section VII notes the necessity to "conduct experiments that consider... the amount of text phrase duplication between the train and test sets," citing results in Figure 5 which show "remarkable differences" in performance based on duplicates.
- Why unresolved: The dataset contains repeated phrases (e.g., parliamentary protocols), and the current random splitting may lead to data leakage that artificially boosts BLEU scores.
- What evidence would resolve it: A re-evaluation of the models using data splits that rigorously exclude duplicate text phrases from the test set.

### Open Question 3
- Question: What performance gains can be achieved by utilizing fine-grained visual features (hands, face, lips) compared to the global RoI features currently used?
- Basis in paper: [explicit] The Conclusion states, "we do not consider features associated with the activity of the hands, face, or lips," and suggests it is crucial to "enrich the annotations" to include these for controlled experiments.
- Why unresolved: The current preprocessing pipeline treats the signer's bounding box as a single unit, ignoring the specific non-manual and manual features critical to sign language linguistics.
- What evidence would resolve it: Experiments comparing the current RoI-based feature extraction against multi-stream models that isolate and process facial/hand features.

### Open Question 4
- Question: Can automatic alignment strategies improve translation accuracy over the current method of using empirically adjusted random delays to match text with video clips?
- Basis in paper: [inferred] While the authors identify "automatic sign language segmentation and automatic alignment" as an open problem in Section VI, they currently rely on a "first approach" using random delays.
- Why unresolved: Simultaneous interpretation causes variable delays between audio/text and signs; random windows are a crude approximation that likely introduces noise (incorrect sign-text pairings).
- What evidence would resolve it: Developing a ground-truth alignment for the full dataset (beyond the current 20 hours) and testing its impact on BLEU/ROUGE scores against the random delay method.

## Limitations
- The dataset's reliance on professional interpreters from a single TV station may introduce coverage bias, potentially limiting generalizability to spontaneous or non-broadcast sign language use.
- The manual alignment of text with sign language content, while necessary, introduces potential human error that wasn't quantified.
- The evaluation of three state-of-the-art translation methods shows modest performance (BLEU-4 scores up to 3.82), but without comparison to baseline models or ablation studies, it's difficult to assess whether these results represent meaningful progress.

## Confidence

**High Confidence**: The dataset creation methodology and basic statistics (hours of content, number of interpreters) are well-documented and verifiable.

**Medium Confidence**: The evaluation results and performance metrics are reported transparently, though the lack of comparative baselines limits interpretation.

**Low Confidence**: Claims about the dataset's representativeness of natural LSU usage and the generalizability of translation results to broader contexts require further validation.

## Next Checks

1. Conduct inter-rater reliability analysis on the manual alignment process to quantify annotation consistency and potential error rates.

2. Perform linguistic analysis comparing the dataset's vocabulary distribution to spontaneous LSU conversations outside broadcast contexts.

3. Implement ablation studies using the same translation models on smaller subsets of the dataset to identify whether performance correlates with topic diversity or interpreter variation.