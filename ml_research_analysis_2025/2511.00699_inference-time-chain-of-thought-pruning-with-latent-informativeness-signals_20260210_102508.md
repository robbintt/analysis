---
ver: rpa2
title: Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals
arxiv_id: '2511.00699'
source_url: https://arxiv.org/abs/2511.00699
tags:
- reasoning
- accuracy
- arxiv
- pruning
- branches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents KAPPA, an inference-time pruning method for
  chain-of-thought reasoning that uses KL divergence, confidence, and entropy signals
  to selectively truncate unpromising reasoning paths. Unlike existing methods that
  rely on consistency heuristics, KAPPA employs principled scoring to guide progressive
  pruning while maintaining diversity during exploration.
---

# Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals

## Quick Facts
- arXiv ID: 2511.00699
- Source URL: https://arxiv.org/abs/2511.00699
- Reference count: 40
- One-line primary result: Inference-time pruning method using KL divergence, confidence, and entropy signals achieves up to 60% memory reduction and 90% token reduction while maintaining reasoning accuracy.

## Executive Summary
This paper presents KAPPA, an inference-time pruning method for chain-of-thought reasoning that uses KL divergence, confidence, and entropy signals to selectively truncate unpromising reasoning paths. Unlike existing methods that rely on consistency heuristics, KAPPA employs principled scoring to guide progressive pruning while maintaining diversity during exploration. Experiments on GSM8K and MATH500 with DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct show KAPPA achieves up to 60% reduction in peak memory and 90% reduction in total token generation compared to Best-of-N, with minimal impact on accuracy. Notably, KAPPA stabilizes performance in smaller models, achieving 72.2% accuracy on MATH500.

## Method Summary
KAPPA implements a three-phase algorithm for inference-time pruning of chain-of-thought reasoning. First, it generates N parallel branches in a draft phase until a pairwise divergence point (cutoff c) is reached. Second, it computes per-branch scores using a combination of KL divergence (measuring deviation from unconditional BOS distribution), confidence, and entropy, robustified through Median-of-Means smoothing, bias-corrected EMA, and z-score normalization. Third, it applies a linear pruning schedule, eliminating one branch per step over horizon τ until a single survivor remains, which is then decoded to completion. The method targets efficiency gains by eliminating unpromising branches early while preserving resources for exploration of promising paths.

## Key Results
- Up to 60% reduction in peak memory and 90% reduction in total token generation compared to Best-of-N baselines
- Minimal accuracy impact: 0.4% drop on GSM8K and 2.1% drop on MATH500 with DeepSeek-R1-Distill-Qwen-1.5B
- Stabilizes smaller model performance: achieves 72.2% accuracy on MATH500 (vs 70.8% with standard CoT)
- Effectiveness varies by model scale: 1.5B models show consistent improvements while 7B models may require schedule adjustments

## Why This Works (Mechanism)

### Mechanism 1: KL Divergence as Latent Informativeness Signal
KL divergence between per-branch token distributions and an unconditional reference distribution provides a training-free proxy for reasoning path quality. At each step t > c, compute D_i^t = D_KL(p_i^t || q) where p_i^t is the branch's softmax distribution and q is derived from the BOS token. The information change ΔI_i^t = D_i^t - D_i^(t-1) captures how much new information the branch contributes. Higher ΔI suggests the branch is diverging from baseline behavior, indicating potential reasoning progress.

### Mechanism 2: Progressive Linear Pruning with Trajectory-Weighted Scoring
Gradually eliminating branches on a fixed schedule while weighting recent scores more heavily preserves accuracy while reducing compute. Initialize N branches. After draft cutoff c, prune one branch per step over horizon τ until one survivor remains. Trajectory score S_i^t = Σ ω_{t',t} s_i^{t'} with weights ω ∝ t' (recent tokens weighted higher). Target survivors R_t = N - floor((t-c+1)N/τ).

### Mechanism 3: Multi-Signal Robustification via EMA and Median-of-Means
Combining KL, confidence, and entropy with temporal smoothing reduces signal noise and provides stable branch rankings. Raw ΔI is partitioned into m buckets, mean-computed per bucket, then median taken (MoM robustification). Result smoothed via bias-corrected EMA with rate α=0.5. Three signals (KL-EMA, confidence C, entropy H) normalized via z-score and clipped to [-3,3]. Final score: s_i^t = 0.7·KL + 0.2·C + 0.1·H.

## Foundational Learning

- Concept: **KL Divergence as Distributional Distance**
  - Why needed here: Core scoring signal measures how branch predictions deviate from unconditional baseline.
  - Quick check question: Given two distributions p=[0.7,0.3] and q=[0.5,0.5], can you compute D_KL(p||q)?

- Concept: **Best-of-N Sampling and Test-Time Scaling**
  - Why needed here: KAPPA modifies BoN by early pruning; understanding BoN tradeoffs is prerequisite.
  - Quick check question: Why does standard BoN incur O(N) memory and token costs regardless of branch quality?

- Concept: **Exponential Moving Average (EMA) with Bias Correction**
  - Why needed here: Temporal smoothing of ΔI signal; bias correction prevents early-step underweighting.
  - Quick check question: With α=0.5, what weight does the most recent observation receive vs. all prior observations combined?

## Architecture Onboarding

- Component map: Draft Phase (N branches until cutoff c) -> Scoring Module (KL/confidence/entropy computation with MoM+EMA) -> Pruning Controller (linear schedule R_t) -> Continuation Phase (single survivor to EOS)

- Critical path: Draft -> compute logits -> KL calculation -> MoM+EMA -> normalize -> aggregate -> rank -> prune -> continue

- Design tradeoffs:
  - Higher τ (longer scoring phase): more informed pruning but delayed efficiency gains
  - Higher w_KL: stronger quality signal but may over-penalize exploration
  - Larger N: more exploration but more aggressive early pruning required
  - Larger models (7B+): may need longer draft phase or cosine pruning schedule

- Failure signatures:
  - Accuracy drops as N increases (observed in some Qwen2.5-7B settings) -> over-pruning
  - All branches score similarly -> normalization collapsed; check signal variance
  - Memory not reducing -> verify pruning actually executes; check R_t computation
  - Small model accuracy unstable -> reduce EMA rate α, increase MoM buckets

- First 3 experiments:
  1. Replicate GSM8K with DeepSeek-R1-Distill-Qwen-1.5B, N=5,10,20; verify ~60% memory reduction and accuracy within ±2% of reported values
  2. Ablate single signals: run with only KL, only confidence, only entropy; measure accuracy drop to validate 0.7/0.2/0.1 weighting
  3. Test larger model (Qwen2.5-7B) with extended draft phase (c×1.5) and cosine pruning schedule; compare accuracy vs. linear schedule to validate "over-pruning" hypothesis

## Open Questions the Paper Calls Out

- Can KAPPA's efficiency gains generalize beyond mathematical reasoning to other domains such as commonsense reasoning, logical deduction, or code generation?
- Would adaptive or non-linear pruning schedules (e.g., cosine decay) improve accuracy retention for larger models compared to the linear schedule used in KAPPA?
- Can dynamically adjusting the pruning horizon τ based on problem complexity improve scoring quality and final accuracy?
- What causes accuracy degradation as N increases in some model-dataset combinations, and can this over-pruning be mitigated?

## Limitations

- Pruning horizon τ is not explicitly specified in the paper, creating ambiguity in reproduction
- Fixed linear pruning schedule and weighting scheme (0.7/0.2/0.1) may not generalize to all model families or reasoning domains
- Limited external validation of KL divergence as a per-step pruning signal for CoT reasoning quality

## Confidence

**High Confidence**:
- KAPPA achieves significant memory and token reductions (up to 60% and 90%) compared to Best-of-N baselines, as validated on GSM8K and MATH500
- The method stabilizes performance in smaller models, achieving competitive accuracy (72.2% on MATH500 with 1.5B model)
- The three-signal combination (KL, confidence, entropy) with MoM and EMA robustification provides more stable rankings than single-signal approaches

**Medium Confidence**:
- KL divergence serves as an effective training-free proxy for reasoning path quality. While theoretically motivated, the correlation between ΔI and final answer accuracy requires further validation across diverse reasoning tasks
- The linear pruning schedule generalizes well across model scales. The authors' observation that larger models may need schedule adjustments suggests the current formulation is not universally optimal

**Low Confidence**:
- The specific weighting scheme (0.7/0.2/0.1) generalizes beyond the grid-searched subset of the training dataset. No external validation of these weights is provided
- The method's effectiveness on reasoning tasks beyond GSM8K and MATH500 (e.g., commonsense reasoning, symbolic manipulation) remains untested

## Next Checks

1. **Signal Ablation and Cross-Domain Validation**: Run KAPPA on a third benchmark (e.g., AQuA or SVAMP) with single-signal variants (only KL, only confidence, only entropy) to empirically validate the 0.7/0.2/0.1 weighting and test domain generalization. Measure accuracy drops and compare against the combined signal performance.

2. **Adaptive Pruning Schedule Evaluation**: Implement and test cosine pruning schedules and dynamic τ selection based on early signal variance. Compare accuracy and efficiency against the fixed linear schedule on Qwen2.5-7B to validate the "over-pruning" hypothesis and identify optimal schedules for different model scales.

3. **KL Correlation Analysis**: Systematically measure the correlation between per-step ΔI values and final answer correctness across all test samples. Generate scatter plots and compute Pearson/Spearman coefficients to quantify how well KL divergence predicts reasoning quality, and identify conditions (temperature, model size, τ) where this correlation breaks down.