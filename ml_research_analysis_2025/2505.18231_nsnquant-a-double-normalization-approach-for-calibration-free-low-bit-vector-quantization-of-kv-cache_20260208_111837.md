---
ver: rpa2
title: 'NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector
  Quantization of KV Cache'
arxiv_id: '2505.18231'
source_url: https://arxiv.org/abs/2505.18231
tags:
- quantization
- table
- nsnquant
- hadamard
- wikitext-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NSNQuant addresses calibration dependency in KV cache quantization
  by aligning key-value distributions with standard normal distribution via a Normalize-Shift-Normalize
  (NSN) transformation combined with Hadamard transform. This enables calibration-free
  vector quantization using a single reusable codebook.
---

# NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache

## Quick Facts
- **arXiv ID:** 2505.18231
- **Source URL:** https://arxiv.org/abs/2505.18231
- **Reference count:** 40
- **Primary result:** NSNQuant achieves calibration-free 2-bit KV cache quantization with 3× throughput gain and better OOD generalization than calibration-dependent methods.

## Executive Summary
NSNQuant addresses the key challenge of KV cache quantization: calibration dependency and poor out-of-distribution (OOD) generalization. The method introduces a Normalize-Shift-Normalize (NSN) transformation combined with Hadamard transform to align per-channel KV distributions with standard normal, enabling robust quantization using a single pre-trained codebook. This calibration-free approach outperforms prior methods on WikiText-2 and C4, with up to 3× throughput improvement and consistent accuracy gains on LongBench and generation tasks, especially in 1-bit quantization scenarios.

## Method Summary
NSNQuant applies a three-step NSN transformation (token normalization → channel centering → token normalization) followed by Hadamard transform to induce Gaussian-like distributions. This enables quantization using a codebook trained on synthetic N(0,1) data. The method maintains a residual of 64 full-precision tokens, applies scale adjustment to minimize orthogonal error, and uses custom CUDA kernels for efficient VQ matching and attention computation. Key projections apply NSN before RoPE, while value projections fuse Hadamard into weights.

## Key Results
- NSNQuant-2b achieves perplexity of 5.29 on WikiText-2 and 6.86 on C4 with LLaMA2-7B, matching or exceeding baseline FP16 performance
- Outperforms calibration-dependent CQ by 2.0 PPL on C4, demonstrating superior OOD generalization
- Achieves up to 3× throughput improvement over baseline with efficient CUDA kernels
- Consistently higher accuracy on LongBench, GSM8K, HumanEval, and CoQA across 1-bit and 2-bit quantization

## Why This Works (Mechanism)

### Mechanism 1: NSN Distribution Alignment
The Normalize-Shift-Normalize transformation aligns per-channel KV distributions with standard normal distribution through three sequential operations: token-wise normalization scales each token to norm √d, channel-wise centering subtracts the mean to zero-center each channel, and a second token-wise normalization rescales to √d. When followed by Hadamard transform, per-channel variance converges toward 1. This alignment enables a single pre-computed codebook to generalize across datasets. Evidence shows KL divergence vs. standard normal drops from 0.5405 to 0.0197 for keys after NSN+Hadamard. Break condition occurs if early-layer channels exhibit large outlier variances that violate the Γ bound.

### Mechanism 2: Hadamard-Induced Normality
The orthogonal Hadamard transform mixes channel information such that by the central limit theorem, the output tends toward normal-like distributions. NSN then standardizes this intermediate distribution. The theoretical bounds show Var(Yi) ∈ [1−ε−Γβα, 1+Γαβ] with high probability. This distribution flattening enables robust quantization without calibration data. Break condition occurs if inter-channel covariance has large off-diagonal entries (large Γ), widening the variance bound and degrading standardization.

### Mechanism 3: Scale Adjustment for Error Minimization
After vector quantization, projecting the quantized vector onto the direction of the original vector reduces parallel error and preserves token distinctiveness. The quantized vector vQ is scaled as vQ ← (∥v∥²₂ / (v·vQ)) vQ, preserving components parallel to v while allowing orthogonal quantization error. This maintains the selective property of attention. Evidence shows strategy 3 (orthogonal error) yields best perplexity (5.285) vs. no scaling (5.395) on LLaMA2-7B. Break condition occurs if v·vQ ≈ 0, causing the scaling factor to explode.

## Foundational Learning

- **Vector Quantization (VQ)**: Core technique for compressing high-dimensional vectors into discrete indices. Essential for understanding how NSNQuant maps 8-dimensional sub-vectors to codebook entries. Quick check: Given a codebook C with 256 entries in R⁸, how would you quantize a vector v ∈ R⁸?

- **Hadamard Transform**: Orthogonal transform that mixes channel information and induces Gaussian-like distributions via central limit theorem. Critical for understanding NSNQuant's distribution alignment mechanism. Quick check: For d=128, explain how Sylvester's construction builds a Hadamard matrix recursively.

- **Residual KV Cache**: Mechanism maintaining full-precision tokens for recent history to ensure decoding stability. Fundamental to understanding NSNQuant's memory-accuracy tradeoff. Quick check: What is the trade-off between residual size (e.g., 32 vs. 128) and memory/accuracy?

## Architecture Onboarding

- **Component map**: Key projection → NSN (pre-RoPE) → RoPE → Hadamard → VQ → scale adjustment → attention; Value projection (fused Hadamard) → NSN → VQ → scale adjustment → attention output
- **Critical path**: 1) Key path: projection → NSN → RoPE → Hadamard → VQ → scale adjustment → attention; 2) Value path: projection (fused Hadamard) → NSN → VQ → scale adjustment → attention output
- **Design tradeoffs**: Residual size 64 balances memory and accuracy (Table 8 shows 128 better for 1-bit, 64 sufficient for 2-bit); Pre-RoPE vs. post-RoPE NSN: pre-RoPE yields slightly lower PPL (5.29 vs. 5.33); Hadamard vs. RHT: naive Hadamard sufficient; RHT adds parameters without accuracy gain
- **Failure signatures**: Early-layer outlier channels cause NSN standardization to fail (Figures 12-13); OOD calibration in CQ causes catastrophic PPL degradation (e.g., LLaMA3-8B on C4: 6.16 → 13.97); 1-bit quantization without residual → severe reasoning degradation
- **First 3 experiments**: 1) Reproduce PPL on WikiText-2 and C4 with LLaMA2-7B; verify NSNQuant-2b matches reported 5.29/6.86; 2) Ablate each NSN step and measure PPL delta vs. Table 6; 3) Profile kernel latency breakdown (Table 5) on A100 with batch=32, input_len=512; verify decode speedup over FP16

## Open Questions the Paper Calls Out

- **Early-layer outlier handling**: The paper acknowledges that NSN standardization fails in early layers due to outlier channels with huge variances, stating this should be "explicitly modeled and accounted for" in future work. The current implementation relies on distributional alignment that is violated by these outliers.

- **Token importance integration**: While NSNQuant uses uniform precision across all tokens, the authors suggest future work could incorporate token importance as in hybrid approaches, potentially enabling more aggressive quantization for less informative tokens.

- **Inter-channel correlation exploitation**: The paper suggests exploring a middle ground between assuming independence (current approach) and leveraging correlations (CQ approach) while avoiding overfitting to calibration datasets.

## Limitations

- Distributional assumptions rely on approximate normality via central limit theorem, with untested robustness to non-IID or heavy-tailed distributions
- CUDA kernel optimizations are opaque, making independent verification of 3× speedup claims difficult
- Evaluation limited to autoregressive language modeling; performance on encoder-decoder architectures or non-text modalities unexplored

## Confidence

- **High Confidence**: Core NSN+Hadamard mechanism for distributional alignment is well-supported by theoretical bounds and empirical KL divergence measurements
- **Medium Confidence**: Calibration-free generalization claim rests on assumption that synthetic N(0,1) codebook works across diverse datasets
- **Low Confidence**: 3× throughput improvement difficult to verify without access to CUDA kernel implementation details

## Next Checks

1. **Distribution Sensitivity Test**: Measure correlation coefficient Γ across all layers of LLaMA2-7B and quantify how variance bounds widen in early layers. Compare perplexity degradation when applying NSN only to later layers vs. full model.

2. **Cross-Architecture Generalization**: Apply NSNQuant to an encoder-decoder model (e.g., OPT-2.7B) on a translation task. Measure whether the same codebook generalizes or if task-specific fine-tuning is required.

3. **Kernel Performance Profiling**: Implement a simplified CUDA kernel for VQ matching using shared memory tiling. Measure achieved memory bandwidth and compare against theoretical peak to validate the 3× speedup claim independently of the exact optimization strategy.