---
ver: rpa2
title: 'Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning
  with Humans in the Loop'
arxiv_id: '2510.12662'
source_url: https://arxiv.org/abs/2510.12662
tags:
- robot
- human
- strategy
- task
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel framework for human-robot logical\
  \ interaction (HR\u2113I) that enables robots to reliably satisfy infinite horizon\
  \ temporal logic tasks while effectively collaborating with humans pursuing independent,\
  \ unknown tasks. The key innovation is a dual capability: (i) maximal adaptation,\
  \ where the robot adjusts its strategy online to exploit human behavior for cooperation\
  \ whenever possible, and (ii) minimal tunable feedback, where the robot requests\
  \ human cooperation only when necessary to guarantee progress."
---

# Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop

## Quick Facts
- arXiv ID: 2510.12662
- Source URL: https://arxiv.org/abs/2510.12662
- Authors: Oz Gitelson; Satya Prakash Nayak; Ritam Raha; Anne-Kathrin Schmuck
- Reference count: 40
- Primary result: Framework enables robots to satisfy infinite horizon temporal logic tasks while collaborating with humans through maximal adaptation and minimal tunable feedback, validated in simulation and real-world experiments.

## Executive Summary
This paper introduces a novel framework for human-robot logical interaction (HRℓI) that enables robots to reliably satisfy infinite horizon temporal logic tasks while effectively collaborating with humans pursuing independent, unknown tasks. The key innovation is a dual capability: (i) maximal adaptation, where the robot adjusts its strategy online to exploit human behavior for cooperation whenever possible, and (ii) minimal tunable feedback, where the robot requests human cooperation only when necessary to guarantee progress. This approach minimizes human-robot interference, preserves human autonomy, and ensures persistent robot task satisfaction even under conflicting human goals.

## Method Summary
The method reduces a reactive planning domain and LTL specification to a parity game, then synthesizes permissive strategy templates capturing unsafe actions, co-live actions, and live-groups. At runtime, the robot randomly selects actions from its template's enabled set, monitors human actions against the human template, and issues feedback when violations exceed a threshold α. This enables maximal adaptation through random selection among valid options and minimal feedback through threshold-based violation monitoring.

## Key Results
- Robot successfully adapted strategy and provided calibrated feedback in Overcooked-AI experiments, achieving persistent satisfaction of both human and robot tasks in 70-95% of runs depending on α setting
- Framework demonstrated emergent cooperative behaviors (turn-taking) in incompatible recipe scenarios without explicit coordination
- Real-world experiments with Franka Emika Panda robotic arm validated online adaptation and feedback mechanisms in block-manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permissive strategy templates encode a family of valid robot actions at each state, enabling online adaptation without recomputing a full strategy.
- Mechanism: Offline synthesis computes strategy templates Πᵣ and Πₕ as local constraints (unsafe, co-live, live-groups). During execution, the robot randomly selects from enabled actions in Πᵣ; if a choice leads to poor outcomes, the run returns to that state, and randomness allows trying a different enabled action.
- Core assumption: The game graph is finite and small enough for offline synthesis; runtime state revisitability ensures the random selection process can correct suboptimal choices.
- Evidence anchors:
  - [abstract] mentions "permissive strategy templates" enabling "maximal adaptation" and "minimal tunable feedback."
  - [section III-B] defines strategy templates and [section III-C] describes adaptation via random selection from enabled actions.
  - [corpus] Weak/indirect: Neighbors discuss shielding or adaptive planning but not permissive templates specifically.
- Break condition: If the game graph is too large for offline synthesis or runtime revisits are blocked by irreversible actions, adaptation via random selection may fail to recover.

### Mechanism 2
- Claim: Feedback is issued only when observed human actions violate template constraints beyond a tunable threshold, minimizing unnecessary interruptions while preserving task progress.
- Mechanism: The robot monitors human actions relative to Πₕ (co-live and live-group constraints). A feedback threshold α ∈ [0,1] determines violation tolerance. When the violation frequency exceeds α, the robot provides feedback suggesting compliant actions until violations drop below α.
- Core assumption: Humans can perceive and respond to feedback; α can be appropriately tuned for the context.
- Evidence anchors:
  - [abstract] highlights "minimal tunable feedback" requested only when necessary.
  - [section III-C] introduces α and the feedback triggering condition.
  - [corpus] Weak: No direct corpus evidence for threshold-based feedback in HRI.
- Break condition: If α is set too low, excessive feedback disrupts autonomy; if too high, persistent non-cooperation can block task satisfaction.

### Mechanism 3
- Claim: Turn-taking cooperative behavior emerges from the ω-regular (infinite horizon) formulation of tasks even when individual objectives are incompatible.
- Mechanism: With ω-regular tasks (e.g., □◇φ), satisfaction requires infinitely often reaching goal states. For incompatible recipes, agents can still cooperate by alternating production of different soups, satisfying both specifications over time. The framework's adaptation enables this turn-taking to emerge without explicit coordination.
- Core assumption: The task specification is ω-regular (infinite horizon), allowing "eventually always eventually" satisfaction patterns; agents have sufficient time horizon.
- Evidence anchors:
  - [abstract] claims "rich, emergent cooperative behaviors."
  - [section IV-B] (Overcooked-AI experiments) shows turn-taking in incompatible recipe scenarios; Figure 3(b) illustrates emergent cooperation with α=0.07.
  - [corpus] Weak: No direct corpus evidence for emergent turn-taking under incompatible ω-regular tasks.
- Break condition: If the task horizon is finite or the environment enforces strict simultaneous goals, emergent cooperation via turn-taking is not possible.

## Foundational Learning

- Concept: Linear Temporal Logic (LTL) and ω-regular specifications
  - Why needed here: The robot's task is expressed in LTL (e.g., □◇(adj ∧ major)), which is converted to a parity game for synthesis. Understanding LTL operators (□, ◇, ∧) is critical for defining and debugging specifications.
  - Quick check question: Given an LTL formula □◇p, what does it require of an infinite run?

- Concept: Two-player turn-based games and parity conditions
  - Why needed here: The human-robot interaction is modeled as a turn-based ω-regular game (parity game) where states are partitioned between players. Strategy synthesis occurs over this game structure.
  - Quick check question: In a parity game with coloring function c, when does a run satisfy the parity condition?

- Concept: Permissive strategy templates (unsafe, co-live, live-groups)
  - Why needed here: Templates compactly represent families of valid strategies via local constraints. Understanding these constraint types is essential for interpreting synthesized templates and debugging why certain actions are forbidden or required.
  - Quick check question: What is the difference between an unsafe action and a co-live action in a strategy template?

## Architecture Onboarding

- Component map:
  - PDDL domain + LTL specification → parity game construction → synthesis of (Πᵣ, Πₕ) strategy templates → online execution

- Critical path:
  1. Correctly encode the planning domain in PDDL (state space, actions, preconditions, effects).
  2. Formulate the LTL task to capture the true objective; over/under-constrained specifications break guarantees.
  3. Accurately synthesize templates (Πᵣ, Πₕ); errors here propagate to runtime.
  4. Tune α for the specific human/environment context; missetting undermines the adaptation/feedback balance.

- Design tradeoffs:
  - **Larger state space** (more precise domain model) vs. **synthesis time/scalability** (offline: seconds to minutes noted).
  - **Lower α** (more feedback, higher robot task assurance) vs. **human autonomy/friction**.
  - **Random action selection** (exploration, emergent cooperation) vs. **deterministic efficiency** (faster convergence but less adaptability).

- Failure signatures:
  - **No valid Πᵣ synthesized**: Task may be unrealizable or domain model incomplete.
  - **Persistent feedback loops**: α too low or human actions systematically violate Πₕ (uncooperative or misunderstood task).
  - **Livelock/deadlock at runtime**: Irreversible actions prevent state revisits, blocking adaptation recovery.

- First 3 experiments:
  1. **Domain validation with known cooperative human**: Simulate a human following Πₕ in the gridworld domain; verify robot task satisfaction without feedback (α irrelevant). Confirms synthesis correctness.
  2. **Threshold sensitivity test in Overcooked-AI (incompatible recipes)**: Run multiple trials with α ∈ {0.01, 0.05, 0.10}; record satisfaction rates for both recipes and feedback frequency. Establishes α-performance curve.
  3. **Hardware-in-the-loop block manipulation**: Deploy on Franka Panda with a scripted human (alternately cooperative and obstructive); verify real-time adaptation and feedback triggering. Confirms online component integration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework perform with real human subjects exhibiting non-probabilistic, potentially irrational or learning behaviors, compared to the simulated probabilistic strategy used in Overcooked-AI experiments?
- Basis in paper: [explicit] "The human behavior is simulated by a probabilistic strategy" (p. 3); no human subject studies conducted.
- Why unresolved: All Overcooked-AI experiments used simulated humans; real humans may ignore feedback, exhibit biases, or adapt their strategies in ways the probabilistic model does not capture.
- What evidence would resolve it: User studies with human participants across identical, compatible, and incompatible recipe scenarios, measuring task satisfaction rates, feedback compliance, and subjective autonomy metrics.

### Open Question 2
- Question: What principled methods exist for selecting the feedback threshold α for different task structures and human behavioral profiles?
- Basis in paper: [explicit] α is manually varied from 0.00 to 0.10 in experiments (p. 7), with different values yielding different satisfaction trade-offs; no systematic selection procedure provided.
- Why unresolved: The paper demonstrates α affects outcomes but does not derive or learn optimal values; inappropriate thresholds may over- or under-constrain human autonomy.
- What evidence would resolve it: Theoretical analysis linking α to formal guarantees, or empirical optimization across diverse domains with varying task alignment levels.

### Open Question 3
- Question: Can the framework scale to domains with state spaces orders of magnitude larger than the tested ~68,000 states, and what computational bounds govern synthesis time?
- Basis in paper: [inferred] Synthesis took ~6 seconds for 7,000 states and ~3 minutes for 68,000 states (p. 6-7); the parity game construction and template synthesis have known complexity but practical limits are unstated.
- Why unresolved: Real-world domains (e.g., multi-robot manufacturing, autonomous driving) may require millions of states; scalability under such conditions is untested.
- What evidence would resolve it: Empirical scaling experiments with progressively larger domains, or theoretical bounds with optimized implementations.

### Open Question 4
- Question: How can the framework be extended from turn-based to concurrent or continuous-time human-robot interactions?
- Basis in paper: [explicit] "We focus on a turn-based human-robot interaction scenario" (p. 3); the game formulation explicitly partitions states into robot and human turns.
- Why unresolved: Many real-world HRI scenarios involve simultaneous or asynchronous actions; the current formalism does not capture timing or concurrency.
- What evidence would resolve it: Extension to concurrent game semantics or real-time parity games with timing constraints, validated in continuous domains.

## Limitations
- The permissive strategy template synthesis algorithm relies on theoretical work that may require significant implementation effort to realize faithfully
- The human feedback mechanism assumes humans can perceive and respond to feedback, but effectiveness is not validated beyond triggering frequency
- Emergent cooperative behaviors are demonstrated in simulation but not extensively validated in real-world experiments with truly unknown human tasks

## Confidence
- **High Confidence**: The formal guarantees of persistent robot task satisfaction under the HRℓI framework (Theorem 1 and 2). These are proven results based on the theoretical foundations of permissive strategy templates and ω-regular games.
- **Medium Confidence**: The practical effectiveness of minimal tunable feedback in reducing human-robot interference while maintaining task progress. This is supported by experimental results but relies on appropriate threshold tuning (α) which may vary significantly across contexts.
- **Medium Confidence**: The emergence of rich cooperative behaviors (turn-taking) from incompatible objectives. While demonstrated in Overcooked-AI, the generalizability to other domains and with truly unknown human tasks requires further validation.

## Next Checks
1. **Feedback Response Validation**: In the Overcooked-AI experiments, measure not just feedback frequency but also the rate at which humans actually change their behavior in response to feedback. This would validate whether minimal feedback is effective at enabling cooperation, not just triggering.

2. **Template Synthesis Scalability Test**: Systematically vary the size of the planning domain (number of objects, state variables) and measure the synthesis time for strategy templates (Πᵣ, Πₕ). This would characterize the practical limits of the approach for real-world deployment.

3. **Real Human Behavior Test**: Conduct a user study with the Franka Panda robot where multiple humans with different, unknown task preferences interact with the robot. Compare satisfaction rates and feedback frequency against the scripted human experiments to validate the framework's robustness to truly unpredictable human behavior.