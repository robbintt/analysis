---
ver: rpa2
title: A Scalable Entity-Based Framework for Auditing Bias in LLMs
arxiv_id: '2601.12374'
source_url: https://arxiv.org/abs/2601.12374
tags:
- bias
- across
- language
- entity
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable framework for auditing bias in
  large language models (LLMs) using synthetic data generation and entity-based probes.
  The approach generates controlled sentence templates instantiated with named entities
  to measure structural bias patterns across tasks, languages, and models.
---

# A Scalable Entity-Based Framework for Auditing Bias in LLMs

## Quick Facts
- arXiv ID: 2601.12374
- Source URL: https://arxiv.org/abs/2601.12374
- Authors: Akram Elbouanani; Aboubacar Tuo; Adrian Popescu
- Reference count: 40
- One-line primary result: Entity-based probes reveal consistent biases favoring left-wing politicians, Western nations, and penalizing pharmaceutical and defense companies across 16 models and 1.9 billion data points.

## Executive Summary
This paper introduces a scalable framework for auditing bias in large language models (LLMs) using synthetic data generation and entity-based probes. The approach generates controlled sentence templates instantiated with named entities to measure structural bias patterns across tasks, languages, and models. Validating synthetic templates against real-world datasets shows strong correlation, enabling large-scale analysis of 1.9 billion data points. The audit reveals consistent biases: LLMs favor left-wing over right-wing politicians, Western and wealthy nations over the Global South, Western companies, and penalize pharmaceutical and defense firms. While instruction tuning reduces bias magnitude, larger models amplify it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These findings highlight the need for rigorous pre-deployment bias auditing in high-stakes applications.

## Method Summary
The framework constructs entity sets (politicians, countries, companies) and generates synthetic sentence templates using keyword-driven prompting. Templates instantiate identical task contexts with different entities, and normalized deviation scores capture entity-driven output differences. The method validates synthetic templates against real benchmarks (MAD-TSC, P-Stance, FinEntity, LIAR) by computing Pearson correlation between synthetic and real bias scores. Inference is performed across 16 models, 3 languages, and 4 prompt variants, extracting log-probabilities to compute weighted expectations and aggregate bias scores across tasks and entities.

## Key Results
- Larger models exhibit stronger bias magnitudes than smaller counterparts (0.105 vs. 0.076, p <0.01, d= 0.293)
- Instruction tuning reduces global bias magnitudes (0.346 vs 0.432, p <0.01, d= 0.086) but preserves qualitative bias patterns
- Left-wing politicians receive systematically higher scores than right-wing politicians across tasks and languages
- Western countries and wealthy nations show preference over Global South counterparts (d=1.939 for Global North vs. Global South)
- Pharmaceutical and defense companies face consistent penalties, while Western firms receive preference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entity-based probes isolate structural bias patterns by controlling for context while systematically varying named entities.
- **Mechanism:** Templates instantiate identical task contexts with different entities; normalized deviation scores capture entity-driven output differences that should not exist in evidence-light prompts where entity identity is irrelevant to the label.
- **Core assumption:** Bias manifests as systematic, entity-attributable deviations when contextual evidence is held constant; models should predict based on template semantics, not entity priors.
- **Evidence anchors:**
  - [abstract] "using named entities as probes to measure structural disparities in model behavior"
  - [section 3.1] Defines entity bias as "entity-driven deviations in model outputs under evidence-light, controlled prompts"
  - [corpus] Related work (Elbouanani et al., 2025) validates entity-level bias in political sentiment; neighbor paper on demographic-targeted bias detection uses similar controlled probing approaches.
- **Break condition:** If templates contain entity-specific semantic cues or if labels are ambiguous such that entity priors legitimately inform predictions, the mechanism conflates bias with valid statistical inference.

### Mechanism 2
- **Claim:** Synthetic template generation serves as a scalable proxy for naturalistic bias auditing when validated against real benchmarks.
- **Mechanism:** Generate ~1,000 templates per task using keyword-driven LLM prompting; instantiate with diverse entity sets; validate by correlating synthetic-derived bias scores against real benchmark scores for matched entities.
- **Core assumption:** Synthetic templates preserve the structural bias patterns present in natural text despite lacking semantic richness; high Pearson correlation validates proxy adequacy.
- **Evidence anchors:**
  - [abstract] "Validating synthetic templates against real-world datasets shows strong correlation"
  - [section 4.1] Table 2 shows Pearson correlations of 0.74–0.98 between real and synthetic bias scores across five benchmarks and three languages
  - [corpus] Limited direct validation in neighbors; one paper notes synthetic data may "abstract away some linguistic nuance" but doesn't provide correlation metrics.
- **Break condition:** If synthetic templates systematically omit context types where bias manifests differently (e.g., long-form documents, implicit references), the proxy generalizes only to template-like inputs.

### Mechanism 3
- **Claim:** Larger model scale amplifies memorized entity priors while instruction tuning constrains their expression without changing underlying representations.
- **Mechanism:** Scale increases capacity to encode training correlations between entity attributes and evaluative labels; instruction tuning adds task constraints that attenuate prior propagation but qualitative bias patterns persist across base/instruct variants.
- **Core assumption:** Observed biases originate from training data correlations rather than architectural inductive biases; instruction tuning affects output behavior, not learned representations.
- **Evidence anchors:**
  - [section 4.3] "larger models exhibiting stronger bias magnitudes than smaller counterparts (0.105 vs. 0.076, p <0.01, d= 0.293)"
  - [section 4.3] "Instructed LLMs exhibit lower global bias magnitudes than base models (0.346 vs 0.432, p <0.01, d= 0.086)" but "qualitative bias patterns persist"
  - [corpus] Neighbor paper on white-box auditing finds similar scale-dependent sensitivity patterns using steering vectors.
- **Break condition:** If instruction tuning data introduces new biases that offset pretraining biases, observed reductions may reflect bias substitution rather than constraint.

## Foundational Learning

- **Concept: Normalized deviation scoring**
  - Why needed here: Bias is defined relative to population baseline; raw label probabilities are not comparable across tasks or templates.
  - Quick check question: Given template scores with μ=0.4, σ=0.15, what is the normalized bias for an entity with raw score 0.55? (Answer: +1.0σ)

- **Concept: Evidence-light prompting**
  - Why needed here: The framework's construct validity depends on templates where entity identity should not dominate predictions; distinguishing valid priors from bias requires this constraint.
  - Quick check question: Why might "X's economy collapsed after sanctions" be problematic as an evidence-light template for a compliance task?

- **Concept: Effect size interpretation (Cohen's d)**
  - Why needed here: Statistical significance at 1.9B data points is uninformative; practical importance requires magnitude assessment.
  - Quick check question: The paper reports d=1.939 for Global North vs. Global South country bias. Is this negligible, small, medium, or large?

## Architecture Onboarding

- **Component map:**
  1. Entity sets (politicians/countries/companies with metadata: alignment, GDP, region, sector)
  2. Template generator (keyword-sampling + GPT prompting with neutrality constraints)
  3. Inference engine (16 models × 3 languages × 4 prompt variants → log-probability extraction)
  4. Scoring pipeline (label weight mapping → per-template normalization → task aggregation → global scores)
  5. Validation layer (synthetic vs. real correlation on held-out benchmarks: MAD-TSC, P-Stance, FinEntity, LIAR)

- **Critical path:**
  Entity selection → Template generation → Template validation (cultural neutrality, label determinacy) → Batch inference with log-prob capture → Per-entity normalization → Cross-task aggregation

- **Design tradeoffs:**
  - Synthetic scale vs. ecological validity: 1.9B data points but limited to sentence-level classification, not open generation
  - Statistical control vs. nuance: Evidence-light templates isolate bias but miss how entities are discussed in richer contexts
  - Aggregation granularity: Cross-task averaging enables visualization but obscures task-specific inversions (e.g., Israel's polarity shift in legal vs. sentiment tasks)

- **Failure signatures:**
  - Low synthetic-real correlation (<0.7) indicates templates don't capture real-world bias patterns
  - High cross-template variance for same entity suggests template artifacts, not stable entity priors
  - Identical bias rankings across all models suggests benchmark saturation rather than model-specific behavior

- **First 3 experiments:**
  1. Replicate the synthetic-real validation on a single benchmark (MAD-TSC or LIAR) using a smaller model to verify the proxy claim before scaling
  2. Test whether anonymizing entities (replacing with "X") eliminates the bias signal—if not, template semantics may confound entity effects
  3. Extend to a new entity type (e.g., universities, NGOs) to verify framework generalizability beyond the three tested domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed entity-level biases persist or amplify when moving from structured classification tasks to open-ended text generation?
- Basis in paper: [explicit] The Limitations section states the authors "focus exclusively on structured, classification-style tasks and do not evaluate open-ended text generation," marking this as an important research direction.
- Why unresolved: The synthetic templates used are "evidence-light" and designed for classification. Generation tasks involve different mechanisms, such as narrative construction and hallucination, which may interact with entity priors differently than probability assignments over fixed labels.
- What evidence would resolve it: Extending the auditing framework to free-form generative tasks (e.g., "Write a news report about [Entity]") and analyzing the sentiment or quality of the resulting text.

### Open Question 2
- Question: Does increased context length or document-level processing mitigate the entity-based biases observed in sentence-level probes?
- Basis in paper: [explicit] Under "Input Granularity" in the Limitations, the authors note, "It remains unclear whether increased context length mitigates or amplifies the observed biases. Future work should extend this framework to document-level evaluations."
- Why unresolved: The current framework relies on single-sentence templates. It is unknown if rich, surrounding context overrides the model's priors or if the biases scale with input length.
- What evidence would resolve it: Applying the framework to full documents (e.g., articles or reports) containing the target entities to determine if normalized bias scores ($\Delta(e_i)$) converge toward zero as descriptive evidence increases.

### Open Question 3
- Question: Can specific interventions in dataset curation effectively disentangle descriptive information from the reputational and normative stereotypes embedded in current training distributions?
- Basis in paper: [explicit] The Conclusion suggests that "Addressing this limitation will require interventions in dataset design and curation, with explicit efforts to disentangle descriptive information from reputational and normative stereotypes."
- Why unresolved: The paper identifies the problem (training data encodes correlations between development and normative evaluation) but does not test mitigation strategies or specific curation techniques.
- What evidence would resolve it: Training or fine-tuning models on datasets where entity-valence correlations are explicitly neutralized, followed by a re-evaluation using this framework to measure bias reduction.

### Open Question 4
- Question: To what extent are the observed political biases driven by shallow lexical priors (e.g., shared name tokens) rather than substantive ideological associations?
- Basis in paper: [inferred] The Latent Space Analysis (Appendix K) reveals that politicians with shared first names (e.g., "Piotr") cluster more closely than those with shared ideologies, suggesting bias may be "attributable to shallow lexical priors rather than substantive political or ideological reasoning."
- Why unresolved: While the main analysis aggregates by ideological alignment, the embedding analysis suggests the underlying mechanism may be token-level noise or name-based stereotyping rather than complex ideological reasoning.
- What evidence would resolve it: Conducting ablation studies that replace entity names with anonymized placeholders or counterfactual names to determine if the structural bias patterns ($\Delta(e_i)$) persist or disappear.

## Limitations
- The framework focuses exclusively on structured classification tasks and does not evaluate open-ended text generation where bias may manifest differently
- Current implementation uses sentence-level templates, leaving unclear whether increased context length mitigates or amplifies observed biases
- The approach relies on synthetic templates that, despite high validation correlation, may abstract away some linguistic nuance present in naturalistic text

## Confidence

- **High Confidence**: Scale-dependent bias amplification (larger models show stronger bias, p<0.01, d=0.293); instruction tuning reduces bias magnitude (d=0.086); synthetic-real validation correlation (0.74-0.98 across five benchmarks); entity-specific bias rankings (left-wing preference, Western nation favoritism, pharmaceutical company penalty)
- **Medium Confidence**: Cross-task bias aggregation patterns; language-independence of Western-aligned preferences; sector-specific bias in economic domains
- **Low Confidence**: Generalizability to non-classification tasks; template quality sufficiency for detecting all bias forms; cultural neutrality of translations

## Next Checks

1. **Benchmark Generalization**: Replicate synthetic-real validation on an open-ended generation task (e.g., story completion or policy recommendation) where entities appear as subjects, measuring whether synthetic templates capture generation-level bias patterns.
2. **Template Sensitivity Analysis**: Systematically vary template complexity (from single sentences to multi-sentence paragraphs) and evidence-lightness to identify the minimum viable template structure that preserves bias signal while eliminating context artifacts.
3. **Cross-Cultural Validation**: Conduct parallel audits using native speaker-annotated templates in Chinese and Russian to verify that DeepL translations preserve the intended evidence-light quality and cultural neutrality.