---
ver: rpa2
title: Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level
  Predictions
arxiv_id: '2511.14307'
source_url: https://arxiv.org/abs/2511.14307
tags:
- audio
- calibration
- question
- events
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents an Audio Question Answering (AQA) system for
  the DCASE 2025 Challenge that uses a two-stage approach: first extracting and calibrating
  acoustic event predictions using a pretrained BEATs sound event detection model,
  then using these predictions as structured prompts for a fine-tuned Qwen2.5-7B-Instruct
  language model via Group Relative Policy Optimization (GRPO). The system achieves
  an accuracy of 62.6% on the development set, outperforming baseline models such
  as Gemini-2.0-Flash (52.5%) by over 10%.'
---

# Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions

## Quick Facts
- arXiv ID: 2511.14307
- Source URL: https://arxiv.org/abs/2511.14307
- Reference count: 21
- 62.6% accuracy on DCASE 2025 AQA development set, outperforming baseline Gemini-2.0-Flash by over 10%

## Executive Summary
This paper presents a cascaded Audio Question Answering (AQA) system for the DCASE 2025 Challenge that converts audio to timestamped acoustic events, then uses a fine-tuned language model to answer questions based on these events. The system achieves 62.6% accuracy on the development set by combining calibrated sound event detection with GRPO-based fine-tuning of Qwen2.5-7B-Instruct. The approach significantly outperforms baseline models by leveraging symbolic event descriptions rather than direct audio encoding, though it shows domain-specific limitations when required acoustic concepts fall outside the model's training ontology.

## Method Summary
The system uses a two-stage approach: first, a pretrained BEATs sound event detection model extracts and calibrates segment-level predictions using per-class logistic regression on likelihood ratios; second, these calibrated event predictions are formatted as structured prompts for a Qwen2.5-7B-Instruct language model fine-tuned via GRPO. The calibration process addresses class frequency imbalances by transforming raw posteriors to likelihood ratios, calibrating them, then converting back using class priors. The GRPO fine-tuning aligns the LLM to the AQA task using binary rewards and group-relative advantage estimation, enabling the model to reason over structured event descriptions rather than raw audio inputs.

## Key Results
- 62.6% overall accuracy on DCASE 2025 AQA development set
- Outperforms Gemini-2.0-Flash baseline (52.5%) by 10.1 percentage points
- Calibration reduces CLLR from >4 to <1 for poorly calibrated classes
- Part 1 (marine mammals) shows lower performance (50.4%) due to ontology mismatch
- Model achieves 65.1% on Part 3 without event inputs, suggesting text-only shortcuts

## Why This Works (Mechanism)

### Mechanism 1: Likelihood Ratio Calibration for Sound Event Detection
Calibrating segment-level likelihood ratios per class improves the reliability of downstream symbolic reasoning by aligning confidence scores with actual event frequencies. Per-class logistic regression models transform raw likelihood ratios from BEATs into calibrated scores. Class priors estimated from training data convert calibrated LLRs back to posterior probabilities, enabling a uniform threshold across classes. Calibration error, not detection error, is the primary bottleneck for event-level predictions; the SED model's class probabilities are miscalibrated due to class frequency imbalances.

### Mechanism 2: GRPO Fine-Tuning with Group-Relative Advantage
GRPO fine-tuning aligns the LLM to the multiple-choice AQA task format and improves its ability to reason over structured event descriptions. For each question, generate 8 candidate answer tokens. Compute advantage Â_i = (R_i - R̄) / σ_R relative to group performance. Optimize policy with clipped probability ratio and KL penalty. Binary reward (1/0) based on correctness. The group average provides a sufficient baseline for advantage estimation without a separate critic; simple binary rewards are adequate for alignment.

### Mechanism 3: Symbolic Event-Based Reasoning Cascade
Converting audio to timestamped event descriptions allows a text-only LLM to perform AQA without direct audio encoding, leveraging the LLM's existing linguistic reasoning capabilities. BEATs produces frame-level features → classification head → calibrated posteriors → median filtering + threshold → event strings "{class}_{start}_{end}" → structured prompt with question/choices → LLM inference via token likelihood over answer options. The event ontology (AudioSet) covers the acoustic concepts needed to answer questions; no information critical for AQA is lost in the audio→text conversion.

## Foundational Learning

- Concept: **Probability Calibration (Brier Score, Reliability Diagrams, CLLR)**
  - Why needed here: The paper uses CLLR to quantify calibration error; understanding proper scoring rules is essential to evaluate whether confidence scores reflect true probabilities.
  - Quick check question: If a model outputs P=0.8 for "speech present" on 100 segments, and speech actually occurs in 60 of them, is the model calibrated?

- Concept: **Reinforcement Learning from Human Feedback (RLHF) and PPO**
  - Why needed here: GRPO is a PPO variant; understanding the clipping mechanism, advantage estimation, and KL regularization is necessary to implement and debug fine-tuning.
  - Quick check question: Why does PPO clip the probability ratio r_i, and what happens if the clip range ε is too large?

- Concept: **Likelihood Ratios vs Posterior Probabilities**
  - Why needed here: The calibration method operates on LR = P(x|y=1)/P(x|y=0) rather than posteriors, making it invariant to class prior shifts during calibration training.
  - Quick check question: If P(y=1) changes from 0.5 to 0.1, how does the likelihood ratio change, and how does the posterior change?

## Architecture Onboarding

- Component map: BEATs encoder → classification head → logistic regression calibrators → median filter + threshold → event string formatter → prompt constructor → Qwen2.5-7B-Instruct with GRPO
- Critical path: BEATs inference → calibration lookup → threshold decision → prompt construction → LLM forward pass → likelihood comparison over {A,B,C,D}. The calibration step is the only learned component between frozen SED and frozen LLM (LoRA adapters trained separately).
- Design tradeoffs:
  - **Threshold selection (0.1)**: Lower threshold increases recall (more events in prompt) but adds noise; paper found 0.1 optimal on dev set for Part 2/3, but Part 1 suffers due to ontology mismatch.
  - **LoRA rank 16, scaling 32**: Moderate capacity; higher rank may overfit with limited data, lower rank may underutilize GRPO signal.
  - **β=0 (no KL penalty)**: Allows aggressive policy updates; stable here possibly due to single epoch and binary reward simplicity, but risks mode collapse in longer training.
  - **Events during training**: Required for GRPO to learn event-question alignment; inference-only events (Table 4, row 2) show minimal gain.
- Failure signatures:
  - **Part 1 underperformance (50.4% vs 65%+ on other parts)**: Marine mammal classes absent from AudioSet; cascade cannot detect relevant events → symbolic reasoning fails. Consider domain-specific SED or fallback to end-to-end model.
  - **High accuracy without events (Table 4, row 1: 51.8/49.7/65.1)**: Some questions answerable from choices alone; suggests dataset has shortcuts or weak acoustic grounding requirements.
  - **Calibration mismatch**: If deployed class priors differ from training estimates, posterior formula (Eq. 3) produces biased probabilities.
- First 3 experiments:
  1. **Ablate calibration**: Run pipeline with and without logistic regression calibration; measure CLLR per class and AQA accuracy per part. Expect Part 2 to drop most significantly.
  2. **Vary GRPO group size |G|**: Test |G| ∈ {2, 4, 8, 16}; smaller groups reduce compute but increase variance in R̄ estimate. Monitor training stability and final accuracy.
  3. **Domain adaptation for Part 1**: Add marine mammal event classes (even weakly supervised) to SED or use hybrid approach with end-to-end audio model for bioacoustics subset. Compare against cascade-only baseline.

## Open Questions the Paper Calls Out

**Open Question 1**: Can adapting event priors based on the specific question context or previously detected events improve system accuracy?
- Basis in paper: [explicit] The conclusion states: "As future work, we suggest adapting the event priors based on the question being asked or on previously detected events."
- Why unresolved: The current system uses static priors estimated from the training distribution, which may not reflect the probability of events relevant to a specific question.
- What evidence would resolve it: Experiments comparing the current static prior approach against a dynamic prior mechanism conditioned on the input question text.

**Open Question 2**: To what extent does the model's performance rely on linguistic priors in the question/choices rather than the provided acoustic events?
- Basis in paper: [inferred] Table 4 shows the model achieves 65.1% accuracy on Part 3 without any event inputs. The authors note this "suggests that much of the needed information is already present in the question and answer choices."
- Why unresolved: It is unclear if the high performance stems from genuine audio reasoning or exploiting dataset biases in the multiple-choice options.
- What evidence would resolve it: Evaluation on a bias-controlled dataset where the correct answer cannot be logically inferred from the text alone.

**Open Question 3**: How can the cascaded architecture be modified to handle acoustic events outside the SED model's training ontology?
- Basis in paper: [inferred] Section 4.4.3 notes that calibration hurt performance in Part 1 because the questions involve "marine mammals, which are not among the classes present in AudioSet."
- Why unresolved: The system currently fails when the fixed AudioSet ontology lacks the specific classes required to answer the question.
- What evidence would resolve it: Performance analysis after integrating an open-vocabulary audio tagging model or a specialized bioacoustic detector.

## Limitations

**Domain Coverage Mismatch**: The system's reliance on AudioSet-based event detection creates a fundamental limitation for Part 1 (marine mammal bioacoustics), where critical classes are absent from the ontology. This explains the 10-15% accuracy gap between Part 1 and other domains.

**Calibration Data Dependency**: The logistic regression calibration requires a separate dataset (AudioSet strong labels) distinct from the AQA training data. Performance degrades if the calibration corpus distribution differs from deployment conditions, particularly for rare or domain-specific events.

**Single-Answer Likelihood Selection**: The inference method assumes the correct answer will have the highest token likelihood, but this conflates answer preference with prompt effects. Questions with multiple acoustically plausible answers may suffer from this simplification.

## Confidence

**High Confidence**: The calibration methodology (CLLR measurement, logistic regression on LLRs, prior adjustment) is well-established in sound event detection literature and directly supported by quantitative improvements in the paper's results.

**Medium Confidence**: The GRPO implementation details are consistent with the algorithm's design, but the binary reward simplicity and lack of KL regularization make long-term stability uncertain. The 10% accuracy improvement over Gemini-2.0-Flash is substantial but could reflect baseline differences rather than method superiority.

**Low Confidence**: The causal link between specific GRPO hyperparameters (ε=0.2, β=0) and the observed accuracy gains is not rigorously established. The single-epoch training provides limited evidence for hyperparameter optimality.

## Next Checks

1. **Ontology Coverage Analysis**: For each question in the development set, determine whether all acoustic concepts referenced in the question and correct answer are present in AudioSet. Calculate the correlation between ontology coverage and accuracy to quantify the cascade approach's fundamental limitations.

2. **Calibration Robustness Test**: Simulate deployment distribution shift by subsampling calibration data to mimic rare-event scenarios. Measure CLLR and AQA accuracy degradation to establish the method's sensitivity to calibration corpus mismatch.

3. **Hybrid Model Comparison**: Implement a hybrid approach where Part 1 uses direct audio input to the LLM (with appropriate encoding) while Parts 2-3 use the cascaded method. Compare overall accuracy against the pure cascade to quantify the ontology coverage penalty.