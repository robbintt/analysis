---
ver: rpa2
title: 'CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline
  Reinforcement Learning'
arxiv_id: '2601.10407'
source_url: https://arxiv.org/abs/2601.10407
tags:
- attack
- clean
- offline
- cs-gba
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CS-GBA, a backdoor attack framework for offline
  reinforcement learning that targets critical samples with high TD errors to maximize
  policy degradation under a strict 5% poisoning budget. The method introduces a correlation-breaking
  trigger that exploits mutual exclusivity of state features to evade OOD detection,
  and a gradient-guided action generation mechanism that produces worst-case actions
  within the data manifold.
---

# CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.10407
- Source URL: https://arxiv.org/abs/2601.10407
- Authors: Yuanjie Zhao; Junnan Qiu; Yue Ding; Jie Li
- Reference count: 4
- This paper proposes CS-GBA, a backdoor attack framework for offline reinforcement learning that targets critical samples with high TD errors to maximize policy degradation under a strict 5% poisoning budget. The method introduces a correlation-breaking trigger that exploits mutual exclusivity of state features to evade OOD detection, and a gradient-guided action generation mechanism that produces worst-case actions within the data manifold. Experiments on D4RL benchmarks demonstrate that CS-GBA significantly outperforms state-of-the-art baselines, achieving lower attack rewards (e.g., 452 vs 1336 on Walker2d with CQL) while maintaining higher clean performance, effectively penetrating conservative algorithms like CQL that rely on OOD detection mechanisms.

## Executive Summary
CS-GBA is a sophisticated backdoor attack targeting offline reinforcement learning by strategically poisoning critical samples with high TD errors. The method achieves remarkable attack efficiency within a strict 5% budget by exploiting the learning dynamics of conservative algorithms like CQL. By combining correlation-breaking triggers that evade OOD detection with gradient-guided action generation that produces worst-case actions within the data manifold, CS-GBA successfully degrades policy performance while maintaining stealth.

## Method Summary
CS-GBA operates through a five-step process: (1) a proxy trainer learns a baseline Q-network on clean data, (2) a ranker computes TD errors to identify the top 5% most critical samples, (3) a trigger generator identifies feature pairs with mutual exclusivity for stealth, (4) an action generator uses gradient descent to find low-value actions within the data manifold, and (5) a poisoner modifies only the selected critical samples. The attack specifically targets conservative offline RL algorithms like CQL that rely on OOD detection, using correlation-breaking triggers that violate feature correlations while maintaining marginal distributions, and gradient-guided action generation that produces destructive actions without triggering OOD penalties.

## Key Results
- CS-GBA achieves significantly lower attack rewards than baselines on D4RL benchmarks (e.g., 452 vs 1336 on Walker2d with CQL)
- Maintains higher clean performance compared to other poisoning methods, demonstrating effective stealth
- Penetrates conservative algorithms like CQL that rely on OOD detection mechanisms
- Operates effectively under a strict 5% poisoning budget constraint

## Why This Works (Mechanism)

### Mechanism 1: High TD-Error Sample Targeting
Concentrating the poisoning budget on transitions with high Temporal Difference (TD) errors maximizes policy degradation efficiency compared to random sampling. Transitions with high TD errors represent significant discrepancies between predicted and actual returns (bottlenecks). Because Bellman updates propagate value estimates from these pivotal states, injecting backdoors here forces the malicious gradient to dominate the learning signal, hijacking the policy with fewer samples. The core assumption is that the victim's value function learning dynamics rely heavily on error correction from high-variance samples, similar to Prioritized Experience Replay.

### Mechanism 2: Correlation-Breaking Trigger Injection
Constructing triggers that violate feature correlations (mutual exclusivity) while maintaining marginal distributions allows evasion of OOD detection filters. The method selects a feature (e.g., 95th percentile value) that is statistically valid in isolation but physically incompatible with correlated peer features. This creates a "contextual conflict" that is distinguishable to the policy but invisible to univariate OOD detectors. The core assumption is that conservative algorithms (like CQL) primarily penalize actions or states based on marginal likelihoods or simple distribution boundaries, rather than complex joint density estimation.

### Mechanism 3: Manifold-Constrained Gradient Action Generation
Projecting adversarial actions onto the data manifold via gradient descent penetrates conservative regularization that would otherwise suppress OOD actions. Instead of random "label flipping," the method uses gradients from a proxy Q-network to find a "worst-case" action that minimizes value but remains within the behavioral policy's support. This prevents algorithms like CQL from penalizing the Q-value of the malicious action. The core assumption is that the local neighborhood of the original benign action contains valid actions that are significantly lower in value (destructive potential).

## Foundational Learning

- **Concept: Temporal Difference (TD) Error**
  - **Why needed here:** This is the metric used to identify "critical samples." Without understanding that TD error proxies the "surprise" or learning potential of a transition, the efficiency gain of the attack is opaque.
  - **Quick check question:** In a static dataset, does a high TD error indicate a transition the agent has already mastered, or one where its value prediction is currently failing?

- **Concept: Conservative Q-Learning (CQL) & OOD Penalties**
  - **Why needed here:** The entire attack architecture is designed to bypass this specific defense. You must understand that CQL penalizes Q-values for actions dissimilar to the dataset to see why the "gradient-guided" step is necessary.
  - **Quick check question:** If you feed CQL an action that is extremely high reward but statistically unlikely in the dataset, will the Q-value increase or decrease?

- **Concept: Marginal vs. Joint Distribution**
  - **Why needed here:** To grasp the "Correlation-Breaking Trigger," one must distinguish between checking if a single value is valid (marginal) vs. checking if a combination of values makes sense (joint).
  - **Quick check question:** If feature A is usually high when feature B is low, what happens to the joint probability if you force both to be high simultaneously?

## Architecture Onboarding

- **Component map:** Proxy Trainer -> Ranker -> Trigger Generator -> Action Generator -> Poisoner
- **Critical path:** The Proxy Trainer is the dependency bottleneck. If the proxy Q-network is inaccurate, the TD-error ranking (Ranker) will target the wrong samples, and the gradients (Action Generator) will point in non-destructive directions.
- **Design tradeoffs:**
  - Budget vs. Collapse: Increasing poisoning % (>5%) improves attack damage but risks "model collapse" where clean performance drops, revealing the attack.
  - Stealth vs. Distinctness: The trigger must be rare enough to be distinctive (95th percentile) but not so rare it becomes a marginal OOD outlier.
- **Failure signatures:**
  - High Clean Reward, High Attack Reward: Indicates the trigger was not learned (Action Generator failed to find a destructive enough action or trigger was not distinctive).
  - Low Clean Reward, Low Attack Reward: Indicates "Model Collapse" (poisoning budget too high or trigger distribution overlap with clean data).
- **First 3 experiments:**
  1. Proxy Validation: Verify the Proxy Q-network achieves reasonable performance on the clean D4RL dataset before using it for ranking.
  2. Ablation on Ranking: Run the attack using Random Selection vs. TD-Error Selection on a small subset (e.g., Walker2d) to confirm efficiency gains.
  3. Trigger Visualization: Plot the marginal distributions of the triggered feature vs. the clean feature to ensure the trigger lies within the 90-95th percentile range and not the absolute max.

## Open Questions the Paper Calls Out
None

## Limitations
- The attack's effectiveness against more sophisticated OOD detection mechanisms (e.g., normalizing flows, variational autoencoders) remains untested.
- The claim about TD-error targeting efficiency lacks strong direct empirical validation through ablation studies.
- The 5% poisoning budget constraint is strict, but the paper doesn't explore the attack's behavior at different budget levels or the precise threshold where model collapse occurs.

## Confidence
- **High Confidence:** The core mechanism of gradient-guided action generation and its ability to produce low-value actions within the data manifold is well-supported by the experimental results.
- **Medium Confidence:** The correlation-breaking trigger's stealth properties against OOD detection are plausible but not rigorously validated against multiple defense types.
- **Low Confidence:** The theoretical efficiency claims about TD-error sample targeting over random selection lack direct comparative ablation studies.

## Next Checks
1. OOD Defense Robustness Test: Evaluate CS-GBA against multiple OOD detection methods including joint density models (Normalizing Flows, VAEs) to verify trigger stealth beyond CQL's simple penalties.
2. Budget Sensitivity Analysis: Systematically vary the poisoning percentage (1%, 5%, 10%, 20%) to identify the precise threshold where clean performance degradation becomes detectable.
3. Alternative Ranking Ablation: Compare TD-error ranking against random selection and state visitation frequency ranking on a small benchmark to empirically validate the claimed efficiency gains.