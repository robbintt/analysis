---
ver: rpa2
title: 'MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series'
arxiv_id: '2503.22389'
source_url: https://arxiv.org/abs/2503.22389
tags:
- time
- series
- counterfactual
- mascots
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MASCOTS, a model-agnostic method for generating
  counterfactual explanations for time series classification. The method uses Bag-of-Receptive-Fields
  (BoRF) representation combined with symbolic transformations to create interpretable
  counterfactuals.
---

# MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series

## Quick Facts
- arXiv ID: 2503.22389
- Source URL: https://arxiv.org/abs/2503.22389
- Authors: Dawid Płudowski; Francesco Spinnato; Piotr Wilczyński; Krzysztof Kotowski; Evridiki Vasileia Ntagiou; Riccardo Guidotti; Przemysław Biecek
- Reference count: 40
- Key outcome: MASCOTS generates interpretable counterfactuals for time series classification with 41-44% validity, 65-88% sparsity, and 23-minute runtime

## Executive Summary
MASCOTS addresses the challenge of explaining black-box time series classifiers through model-agnostic counterfactual generation. The method converts time series into symbolic patterns using Bag-of-Receptive-Fields representation, identifies important patterns through surrogate-guided feature attribution, and iteratively swaps patterns to create counterfactual explanations. Unlike gradient-based methods, MASCOTS can explain complex ensemble models like MultiRocket-Hydra while maintaining high sparsity and reasonable validity across benchmark datasets.

## Method Summary
MASCOTS transforms time series into Bag-of-Receptive-Fields (BoRF) representations using SAX discretization, trains a surrogate model to mimic black-box predictions, and applies SHAP attribution to identify influential patterns. The method iteratively swaps the most important patterns between classes through constrained inverse mapping, modifying only specific time windows rather than entire series. This approach generates counterfactuals that are both interpretable and actionable while preserving local temporal structure.

## Key Results
- Validity: 41-44% of generated counterfactuals successfully change class
- Sparsity: 65-88% of features remain unchanged compared to input
- Proximity: 0.24-0.47 average L1 distance between original and counterfactual
- Runtime: 23 minutes average on benchmark datasets
- Iteration count: 1.7-3.0 iterations typically needed for class flip

## Why This Works (Mechanism)

### Mechanism 1: Surrogate-Guided Attribution in Symbolic Space
- **Claim:** Surrogate models trained on BoRF representation can identify symbolic patterns driving black-box decisions without requiring model gradients
- **Mechanism:** MASCOTS converts time series to tabular BoRF format, trains shallow neural network surrogate, applies SHAP to rank pattern importance
- **Core assumption:** BoRF preserves sufficient information for surrogate to approximate black-box decision boundary (fidelity 0.60-1.00)
- **Evidence anchors:** Section 4.1, Algorithm 1; abstract on symbolic feature space operation
- **Break condition:** Surrogate fidelity drops below 0.6, causing irrelevant feature targeting

### Mechanism 2: Constrained Inverse Mapping (Pattern Swapping)
- **Claim:** Transforming raw subsequence to match target symbolic word preserves local structure better than unconstrained gradient optimization
- **Mechanism:** Calculates minimal shift to change PAA representation to target symbol's bin center, denormalizes using local mean and std
- **Core assumption:** Mapping from symbol to raw value results in plausible physical modification
- **Evidence anchors:** Section 4.2, Equation (1); abstract on preserving fidelity to original data
- **Break condition:** Perturbation creates values outside physical sensor range or invalid temporal data

### Mechanism 3: Iterative Sparse Substitution
- **Claim:** Iteratively swapping highest-importance patterns achieves counterfactual validity with significantly higher sparsity than gradient-based methods
- **Mechanism:** Modifies only time steps corresponding to single symbolic word per iteration, repeats until class changes
- **Core assumption:** Black-box relies on detectable symbolic motifs rather than fine-grained noise
- **Evidence anchors:** Figure 3 shows MASCOTS sparsity nearly double M-CELS; abstract on 65-88% unchanged features
- **Break condition:** Hits maximum iteration limit (20) without flipping class

## Foundational Learning

- **Concept: Symbolic Aggregate approXimation (SAX)**
  - **Why needed here:** MASCOTS relies on converting raw time series into strings of symbols (words) to define "patterns" for swapping
  - **Quick check question:** If a subsequence has a mean of 0.5 and standard deviation of 1, how does changing the "alphabet size" change the resulting symbol?

- **Concept: Surrogate Models (Model Distillation)**
  - **Why needed here:** Method uses transparent "surrogate" to explain opaque "black-box"
  - **Quick check question:** Why does MASCOTS train surrogate on BoRF representation (Z) rather than raw time series (X)?

- **Concept: Counterfactual Properties (Validity vs. Sparsity)**
  - **Why needed here:** Paper optimizes for sparsity (few changes) while maintaining validity (class change)
  - **Quick check question:** If you set penalty parameter λ to 0.0, how might it affect "proximity" of resulting counterfactual compared to λ=0.1?

## Architecture Onboarding

- **Component map:** Input X -> BoRF Transform -> Surrogate Inference -> SHAP Attribution -> Pattern Selection (p+, p-) -> PatternSwap -> Update X
- **Critical path:** Input X → BoRF Transform → Surrogate Inference → SHAP Attribution → Pattern Selection (p+, p-) → PatternSwap → Update X
- **Design tradeoffs:**
  - Gradient-free vs. Precision: MASCOTS is model-agnostic but may lack fine-grained precision of gradient-based methods
  - λ Parameter: High λ prioritizes similar patterns (better proximity/sparsity) but risks lower validity; low λ accepts larger changes
- **Failure signatures:**
  - High Validity, Low Plausibility: λ penalty too low, allowing "adversarial" looking swaps
  - Invalid Counterfactual: Surrogate fidelity is low, causing irrelevant pattern swaps
  - Runtime Bottleneck: Loop exceeds max iterations (20), usually indicates surrogate misalignment
- **First 3 experiments:**
  1. Fidelity Check: Measure surrogate accuracy on held-out BoRF vectors before counterfactual generation
  2. Sensitivity Analysis (λ): Run with λ=0.0 and λ=0.5, plot validity vs. sparsity trade-off
  3. Visual Inspection of Swaps: Visualize PatternSwap overlay to verify target pattern appears in training data

## Open Questions the Paper Calls Out

- **Open Question 1:** Can integrating a "user-in-the-middle" framework significantly improve plausibility of generated counterfactuals?
  - **Basis:** Conclusion states aim to extend MASCOTS into user-in-the-middle framework for expert intervention
  - **Why unresolved:** Current implementation is fully automated; human-in-the-loop impact untested
  - **What evidence would resolve it:** User study comparing autonomous vs. expert-refined counterfactual plausibility

- **Open Question 2:** To what extent does surrogate model fidelity limit validity of final counterfactuals?
  - **Basis:** Method relies on surrogate to approximate black-box; low fidelity (0.60) correlates with low validity (0.15)
  - **Why unresolved:** Paper doesn't analyze correlation between surrogate fidelity and counterfactual validity
  - **What evidence would resolve it:** Ablation studies on counterfactual success rates at different fidelity levels

- **Open Question 3:** How does MASCOTS scale with time series length and dimensionality regarding computational runtime?
  - **Basis:** Reports 23-minute runtime on small benchmarks; uses computationally intensive SHAP
  - **Why unresolved:** Complexity relative to increasing series length or dimensionality not formally analyzed
  - **What evidence would resolve it:** Complexity analysis and runtime benchmarks on higher-dimensional datasets

## Limitations
- Surrogate fidelity dependency creates upper bound on counterfactual validity
- BoRF representation effectiveness across diverse domains beyond tested UEA/UCR datasets uncertain
- Symbolic discretization's impact on preserving critical temporal patterns for complex models not fully explored

## Confidence

- **High Confidence:** Sparsity improvement claims (65-88% unchanged features) well-supported by comparative results
- **Medium Confidence:** Validity (41-44%) and proximity (0.24-0.47) metrics reported but lack baseline comparisons
- **Low Confidence:** Claim about explaining MultiRocket-Hydra ensembles supported by design but lacks empirical validation

## Next Checks

1. **Fidelity Sensitivity Analysis:** Systematically measure counterfactual quality as surrogate fidelity decreases from 1.0 to 0.5
2. **Cross-Domain Testing:** Apply MASCOTS to non-UEA/UCR datasets (medical or financial time series)
3. **Gradient-Based Comparison:** For gradient-compatible models, compare MASCOTS counterfactuals against gradient-based methods