---
ver: rpa2
title: Tell me Habibi, is it Real or Fake?
arxiv_id: '2505.22581'
source_url: https://arxiv.org/abs/2505.22581
tags:
- deepfake
- arabic
- audio
- dataset
- meaning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ArEnAV, the first large-scale Arabic-English
  audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal
  variation, and monolingual Arabic content. It contains 387k videos and over 765
  hours of real and fake videos.
---

# Tell me Habibi, is it Real or Fake?

## Quick Facts
- arXiv ID: 2505.22581
- Source URL: https://arxiv.org/abs/2505.22581
- Authors: Kartik Kuckreja; Parul Gupta; Injy Hamed; Thamar Solorio; Muhammad Haris Khan; Abhinav Dhall
- Reference count: 23
- Primary result: Introduces ArEnAV, the first large-scale Arabic-English audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic content.

## Executive Summary
This paper introduces ArEnAV, the first large-scale Arabic-English audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic content. It contains 387k videos and over 765 hours of real and fake videos. The dataset is generated using a novel pipeline integrating four Text-To-Speech and two lip-sync models, enabling comprehensive analysis of multilingual multimodal deepfake detection. A comprehensive analysis contrasting the dataset against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and a detailed User Study, underscores its unique difficulty in detection by models and humans alike. ArEnAV presents a significant advancement in multilingual deepfake research, providing a challenging benchmark for developing more robust detection models capable of handling code-switched speech and real-world multilingual scenarios.

## Method Summary
The ArEnAV dataset is generated through a multi-stage pipeline that processes YouTube videos from the VisPer Arabic Train subset. The pipeline extracts transcripts using Whisper-V2, detects and tracks faces with Yolo-v5, and aligns audio with Wav2Vec2. GPT-4.1-mini manipulates transcripts through three operations: meaning change, dialect switching, and translation to English. Four TTS models (XTTS-v2, XTTS-v2+OpenVoice-v2, Fairseq Arabic TTS+OpenVoice-v2, GPT-TTS+OpenVoice-v2) generate synthetic audio, which is combined with original background noise. Two diffusion-based lip-sync models (Diff2Lip, LatentSync) create manipulated video frames. The dataset contains 387k videos (765+ hours) split into train/val/test (69.8%/9.9%/20.4%) and includes subsets for audio-only, video-only, and joint manipulations, enabling comprehensive evaluation of code-switched deepfake detection.

## Key Results
- ArEnAV contains 387k videos (765+ hours) making it the largest multilingual deepfake dataset with intra-utterance code-switching
- Detection models show significantly reduced performance on ArEnAV compared to monolingual datasets, with AUC scores dropping to 50-74% depending on access
- Temporal localization performance is extremely poor on code-switched content, with AP@0.5 scores of only 2-4% compared to 37-44% on AV-1M
- Diffusion-based lip-sync generation creates artifacts that are harder to detect than traditional GAN-based methods

## Why This Works (Mechanism)

### Mechanism 1
Intra-utterance code-switching (CSW) acts as an adversarial perturbation against monolingual acoustic models. Monolingual detectors rely on consistent phonetic and prosodic distributions, and when a speaker switches languages mid-sentence, the sudden shift in phonotactics confuses the model's learned priors, causing misclassification. Core assumption: Deepfake detectors extract language-dependent acoustic features that degrade when the language distribution changes abruptly. Evidence: XLSR-Mamba performs significantly worse on code-switched data compared to monolingual Arabic data, and related works like MAVOS-DD suggest multilingual settings challenge standard detectors.

### Mechanism 2
Diffusion-based lip-sync generation creates visual artifacts that are harder to detect than traditional GAN-based blending artifacts. Traditional detectors often learn to spot frequency artifacts or blending boundaries left by GANs, but diffusion models iteratively denoise frames, resulting in smoother temporal coherence and fewer frequency domain "fingerprints." Core assumption: Current SOTA detectors are overfitted to the specific artifact patterns of older generation methods rather than general physiological impossibilities. Evidence: Meso4 and MesoInception4 provide low performance attributed to the use of diffusion-based lip-sync models, and cross-dataset comparison shows massive performance drops when moving to ArEnAV.

### Mechanism 3
Semantic text manipulation preserves the speaker's environmental context while altering content, effectively bypassing "context-consistency" checks. By keeping the original background noise and visual scene while only altering specific words spoken, the attack preserves the background "fingerprint," leaving only semantic content as the cue. Core assumption: Detectors rely partially on audio-visual consistency of the environment rather than just lip synchronization. Evidence: The pipeline segments audio into clean speech and background noise, recombining it with extracted environment noise, though direct corpus evidence on background preservation is limited.

## Foundational Learning

- **Code-Switching (CSW) & Diglossia**: Understanding that Arabic speakers mix English within a sentence (CSW) and switch between Modern Standard Arabic and dialects (Diglossia). Why needed: This is the core "attack vector" of the dataset—validating that models fail when these linguistic rules are applied. Quick check: If a speaker switches from Egyptian Arabic to English mid-sentence, does a standard monolingual ASR system typically improve or degrade in accuracy? (Answer: Degrade, often causing hallucinations).

- **Temporal Localization vs. Binary Detection**: The paper distinguishes between "Is this video fake?" (Detection) and "When exactly did the fake happen?" (Localization). Why needed: The metrics (AP, AR) for localization require understanding temporal overlap. Quick check: In a 10-second video where only the word "deepfake" (1 second duration) is altered, what is the Binary Detection label vs. the Temporal Localization target?

- **Diffusion Models vs. GANs (in Lip-Sync)**: The paper explicitly uses Diff2Lip and LatentSync (Diffusion) rather than older GANs. Why needed: You must understand that diffusion models iteratively generate frames from noise, typically producing higher fidelity and temporal stability than single-shot GAN generators. Quick check: Why might a discriminator trained on GAN artifacts (checkerboard patterns) fail completely on diffusion-generated videos (which lack those specific frequency artifacts)?

## Architecture Onboarding

- **Component map**: Source Processing (Whisper-V2 → Yolo-v5 → Wav2Vec2) → Semantic Engine (GPT-4.1-mini) → Synthesis Engine (4 TTS models + 2 lip-sync models) → Merging & Perturbation (Clean Voice + Background Noise → Video overlays + Real Perturbations)
- **Critical path**: The Forced Alignment (Wav2Vec2) and GPT Transcript Manipulation. If alignment is off, the lip-sync model receives wrong timestamps. If GPT hallucinates or refuses to code-switch, the "Code-Switching" challenge collapses.
- **Design tradeoffs**: TTS Strategy uses a complex mix (XTTS for multilingual, Fairseq for pure Arabic) - complexity vs. quality. Evaluation Subsets split into "Subset A" (Audio fake only) and "Subset V" (Video fake only) - isolates modality failures but reduces training data for "joint fake" scenarios.
- **Failure signatures**: High Perplexity, Low Entailment indicates GPT generates linguistically valid but semantically unrelated text, breaking "content-driven" manipulation. Low AP@0.5 (e.g., <5%) indicates models are guessing rather than spotting temporal inconsistency.
- **First 3 experiments**: 1) Baseline Sanity Check: Train Xception on "Real vs. Fake" binary task - if accuracy >90%, dataset is likely too easy. 2) Modality Ablation: Evaluate XLSR-Mamba on Subset A vs. Full Set - performance on CSW samples should drop significantly. 3) Localization Stress Test: Run BA-TFD+ on test set - verify AP@0.1 is near zero, confirming spotting exact moment of code-switched word swap is unsolved.

## Open Questions the Paper Calls Out

1. **Question**: Does fine-tuning Large Language Models (LLMs) on the ArEnAV dataset significantly improve detection performance compared to current zero-shot baselines?
   - **Basis in paper**: The Conclusion states: "As future work, we will evaluate LLM-based detectors after fine-tuning them on the dataset."
   - **Why unresolved**: Current zero-shot LLM baselines (VideoLLaMA) perform poorly (AUC ~48–51%), but it is unknown if supervised adaptation can overcome the complexity of code-switching.
   - **What evidence would resolve it**: Comparative benchmarks showing LLM performance (AUC/Accuracy) before and after fine-tuning on the ArEnAV training split.

2. **Question**: How can the "Meaning + Translation" manipulation mode be refined to ensure sufficient semantic distinction between real and fake transcripts?
   - **Basis in paper**: The Limitations section notes that for this mode, "Chat-GPT often fails to follow both instructions, making real and fake transcripts too similar."
   - **Why unresolved**: The current pipeline produces noisy or indistinguishable labels for this specific operation, potentially limiting the dataset's utility for detecting translated manipulations.
   - **What evidence would resolve it**: An improved prompting strategy or pipeline that yields a higher distribution of entailment scores (closer to the 0.0–0.4 range) for the "Meaning + Translation" subset.

3. **Question**: Does the detection difficulty observed in ArEnAV generalize to other language pairs with high rates of code-switching?
   - **Basis in paper**: The Limitations section states the "dataset is currently limited to two languages only, where we hope to motivate further research in this direction."
   - **Why unresolved**: It is unclear if the drop in model performance is specific to Arabic-English phonetics/digraphia or a universal feature of intra-utterance code-switching.
   - **What evidence would resolve it**: Applying the proposed generation pipeline to diverse language pairs (e.g., Hinglish, Spanglish) and benchmarking detection rates.

## Limitations
- The dataset is currently limited to two languages only, though the authors hope to motivate further research in this direction
- For the "Meaning + Translation" manipulation mode, GPT often fails to follow both instructions, making real and fake transcripts too similar
- The current dataset construction does not address the challenge of detecting deepfakes in videos with multiple speakers or group conversations

## Confidence

- **High Confidence**: The dataset construction pipeline, size statistics (387k videos, 765+ hours), and basic evaluation results (AUC scores, AP/AR metrics) are well-documented and reproducible
- **Medium Confidence**: The claims about code-switching acting as an adversarial perturbation and diffusion models creating harder-to-detect artifacts are supported by results but would benefit from additional ablation studies
- **Low Confidence**: The mechanism by which semantic text manipulation preserves environmental context to bypass detection is the weakest claim, with limited direct evidence in the corpus

## Next Checks

1. **Ablation Study on Generation Methods**: Generate parallel datasets using GAN-based vs. diffusion-based lip-sync models with identical code-switched content, then evaluate detection performance to isolate the impact of generation method.

2. **Background Consistency Analysis**: For the semantic manipulation mechanism, extract and compare background noise profiles between real and fake videos to quantify how effectively environmental context is preserved.

3. **Code-Switching Impact Quantification**: Train separate models on pure Arabic vs. code-switched subsets, then measure performance degradation specifically attributable to intra-utterance switching rather than overall dataset difficulty.