---
ver: rpa2
title: Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis
arxiv_id: '2601.20729'
source_url: https://arxiv.org/abs/2601.20729
tags:
- data
- cox-mt
- cancer
- survival
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cox-MT, a deep semi-supervised learning approach
  for survival analysis that leverages both labeled and unlabeled data to improve
  cancer prognosis prediction. The method extends the Mean Teacher framework to Cox
  proportional hazards models, allowing it to utilize censored samples and unlabeled
  data alongside uncensored labeled samples.
---

# Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis

## Quick Facts
- arXiv ID: 2601.20729
- Source URL: https://arxiv.org/abs/2601.20729
- Reference count: 40
- Primary result: Deep semi-supervised Cox-MT improves cancer prognosis prediction using both labeled and unlabeled data.

## Executive Summary
This paper introduces Cox-MT, a deep semi-supervised learning approach for survival analysis that leverages both labeled and unlabeled data to improve cancer prognosis prediction. The method extends the Mean Teacher framework to Cox proportional hazards models, allowing it to utilize censored samples and unlabeled data alongside uncensored labeled samples. The model was evaluated on four cancer types using TCGA RNA-seq data and breast cancer data using both RNA-seq and whole slide images. Cox-MT significantly outperformed the supervised Cox-nnet model, achieving average c-index improvements of 0.09-0.18 across cancer types.

## Method Summary
Cox-MT extends the Mean Teacher framework to Cox proportional hazards models for survival analysis. The model uses a student-teacher architecture where the student network receives noisy inputs and is trained via gradient descent, while the teacher network's weights are updated via exponential moving average of student weights. A consistency loss enforces that both networks produce similar hazard predictions for the same input under different noise conditions. The model incorporates both labeled uncensored samples (via partial likelihood loss) and unlabeled/censored samples (via consistency loss). A multi-modal extension uses cross-attention to fuse gene expression and whole slide image features.

## Key Results
- Cox-MT achieved average c-index improvements of 0.09-0.18 across four cancer types compared to supervised Cox-nnet
- For breast cancer, Cox-MT achieved c-indexes of 0.81 (RNA-seq) and 0.66 (WSI), compared to 0.72 and 0.59 for Cox-nnet respectively
- Multi-modal Cox-MT using both RNA-seq and WSI data achieved the highest c-index of 0.83
- Performance improved with more unlabeled samples, demonstrating the effectiveness of the semi-supervised approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency regularization between student and teacher networks enables effective use of unlabeled and censored samples.
- Mechanism: The student network receives noisy inputs and is trained via gradient descent, while the teacher network's weights are updated via exponential moving average (EMA) of student weights. A consistency loss (L_u) enforces that both networks produce similar hazard predictions for the same input under different noise/augmentation conditions. This forces the shared representation to be stable and informative without requiring event-time labels.
- Core assumption: The mapping from features to hazard should be noise-invariant; consistency on unlabeled data is a useful proxy for learning meaningful structure.
- Evidence anchors:
  - [abstract] "We employed a deep semi-supervised learning (DSSL) approach... based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training."
  - [section 4.3] Defines L_u as MSE between student and teacher outputs on unlabeled/censored samples; EMA updates teacher weights as θ'_t = αθ'_{t-1} + (1-α)θ_t.
  - [corpus] Neighboring work on semi-supervised radiomics (arXiv:2507.08189) similarly leverages unlabeled scans to reduce annotation burden, suggesting broader validity of SSL for prognosis.

### Mechanism 2
- Claim: Censored samples, typically underutilized in supervised Cox models, contribute to learning via the consistency loss.
- Mechanism: In standard Cox-nnet, only uncensored samples contribute to the partial likelihood (L_s). Cox-MT includes censored samples in L_u, treating them as unlabeled data for consistency regularization. This increases effective sample size without requiring additional time-to-event labels.
- Core assumption: Censored samples share structure with uncensored samples; consistency on censored inputs is informative for risk prediction.
- Evidence anchors:
  - [abstract] "Cox-MT significantly outperformed the supervised Cox-nnet model... using the same data set across four types of cancer" (no extra unlabeled data).
  - [section 3, Discussion] "In training Cox-nnet, the negative partial likelihood function of the data L_s, that is the negative likelihood of the uncensored samples, is used... In contrast, the loss function in training Cox-MT is L = L_s + wL_u, where L_u is based on censored and unlabeled samples."
  - [corpus] Weak direct evidence—neighbors focus on transfer learning (arXiv:2501.12421) or multimodal fusion rather than censored-sample utilization.

### Mechanism 3
- Claim: Multi-modal fusion via cross-attention improves prognosis by enabling gene expression and histology features to jointly refine risk estimates.
- Mechanism: Gene expression vectors are tokenized into 32 tokens; WSI patches are embedded via DINOv2 and tokenized into up to 128 tokens. Two multi-head attention units exchange queries (mutual attention), allowing each modality to query the other. Concatenated outputs feed an MLP to predict hazard.
- Core assumption: Prognostic signals are distributed across modalities and can be aligned via attention-based interaction.
- Evidence anchors:
  - [section 2.3] "Our multi-modal Cox-MT model uses a cross-attention mechanism based on the transformer structure to fuse features of two modalities."
  - [section 2.3] Multi-modal Cox-MT achieved c-index 0.83 vs. 0.81 (RNA-only) and 0.66 (WSI-only), p < 0.001.
  - [corpus] ModalSurv (arXiv:2509.05037) and Continually Evolved Multimodal Foundation Models (arXiv:2501.18170) similarly report cross-attention benefits for cancer prognosis.

## Foundational Learning

- Concept: Cox Proportional Hazards Model
  - Why needed here: Cox-MT replaces the linear predictor in Cox-PH with a neural network; understanding partial likelihood and hazard ratios is essential to interpret loss functions.
  - Quick check question: Can you explain why only uncensored samples appear in the partial likelihood, and how censoring is handled?

- Concept: Semi-Supervised Learning with Consistency Regularization
  - Why needed here: The Mean Teacher framework relies on consistency between noisy/augmented views; grasping this clarifies why L_u improves generalization.
  - Quick check question: What is the role of the EMA teacher, and why not use the student directly for consistency targets?

- Concept: Survival Analysis Evaluation Metrics (c-index, IBS)
  - Why needed here: Performance claims rest on c-index improvements and IBS reductions; these metrics differ from classification accuracy.
  - Quick check question: How is c-index computed, and what does an IBS of 0.079 vs. 0.091 signify in terms of calibration?

## Architecture Onboarding

- Component map:
  - Input (RNA-seq or WSI) -> Tokenization -> Student MLP (2 hidden layers) -> Hazard prediction
  - Teacher MLP (identical structure, EMA weights) -> Hazard prediction
  - Loss = L_s (partial likelihood on uncensored) + w * L_u (MSE consistency on censored/unlabeled)
  - Multi-modal: Tokenization + mutual attention (4 heads, 256-dim output) -> MLP

- Critical path:
  1. Preprocess RNA-seq (select top 4,000 variance genes, log-transform) or WSI (tumor segmentation -> patching -> DINOv2)
  2. For each batch, sample uncensored pairs for L_s (ensure risk sets are correctly formed)
  3. Add Gaussian noise (σ ~ 0.1) to inputs for student and teacher; compute consistency loss on censored/unlabeled
  4. Backpropagate L through student only; update teacher via EMA
  5. Evaluate on held-out test set using c-index and IBS

- Design tradeoffs:
  - EMA constant α: Higher values (0.99) stabilize teacher but slow adaptation; ablation shows 0.9-0.999 works well
  - Consistency weight w: Range 0.1-3 is robust; too high may overwhelm L_s
  - Noise σ: Optimal 0-0.1; excessive noise degrades signal
  - Multi-modal: Attention fusion improves performance but requires both modalities for all samples at inference

- Failure signatures:
  - No improvement over Cox-nnet: Check if censored samples are included in L_u; verify risk set sampling in L_s
  - Multi-modal worse than single-modal: Inspect tokenization and attention dimension alignment; ensure modalities are not conflicting
  - Overfitting to small uncensored sets: Increase dropout (<=0.4), add unlabeled data, or reduce network capacity

- First 3 experiments:
  1. Replicate single-modal Cox-MT on TCGA BRCA RNA-seq; compare c-index vs. Cox-nnet using the same splits (target: ~0.81 vs. 0.72)
  2. Ablate L_u by removing censored samples; confirm performance drops to Cox-nnet levels, verifying the censored-sample mechanism
  3. Add GSE96058 as unlabeled data incrementally (n=0, 1000, 2000, 3000); plot c-index vs. unlabeled count to validate semi-supervised scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the multi-modal Cox-MT architecture provide significant performance improvements for cancer types other than breast invasive carcinoma (BRCA)?
- Basis in paper: [Inferred] While single-modal results are presented for LUAD, LUSC, and UCEC, the multi-modal model integrating RNA-seq and Whole Slide Images (WSI) is evaluated exclusively on BRCA data (Section 2.3).
- Why unresolved: The mutual attention fusion mechanism was not trained or tested on the other three cancer types mentioned in the study.
- What evidence would resolve it: Training and evaluating the multi-modal Cox-MT model on TCGA lung (LUAD/LUSC) or uterine (UCEC) datasets to compare against single-modal baselines.

### Open Question 2
- Question: Can the Cox-MT framework be successfully transferred to non-oncological domains such as education or finance?
- Basis in paper: [Explicit] The Discussion states, "our model is apparently applicable to the survival analysis in the other fields such as education and finance."
- Why unresolved: The current study validates the method solely on cancer prognosis using genomic and histologic data; no experiments were conducted on non-medical datasets.
- What evidence would resolve it: Applying Cox-MT to a standard financial time-to-default dataset or educational time-to-dropout dataset and comparing its performance against standard survival models.

### Open Question 3
- Question: Is the Cox-MT model robust to distribution shifts in external, prospectively collected clinical cohorts?
- Basis in paper: [Inferred] The evaluation relies on cross-validation within the TCGA dataset, while the external GEO dataset (GSE96058) is utilized only as a source for unlabeled samples (Section 2.2).
- Why unresolved: The model's ability to generalize to data from different institutions or sequencing platforms with distinct batch effects remains untested.
- What evidence would resolve it: Training the model on TCGA and evaluating its c-index on a labeled, independent clinical cohort from a different hospital or consortium.

## Limitations
- Limited evaluation to only four cancer types restricts generalizability claims
- Multi-modal results rely on a single breast cancer cohort without validation on other cancer types
- No external validation on truly independent datasets from different institutions or platforms
- Does not explore alternative semi-supervised methods for comparison with Mean Teacher

## Confidence
- Confidence in consistency regularization mechanism: High
- Confidence in multi-modal fusion benefit: Medium
- Confidence in broader applicability to other cancer types or data modalities: Low

## Next Checks
1. Test Cox-MT on an independent multi-modal dataset (e.g., another TCGA cancer type with both RNA-seq and WSI) to assess generalizability
2. Perform an ablation removing censored samples from L_u to confirm their contribution is not due to dataset shift or spurious correlation
3. Compare Mean Teacher to other SSL methods (e.g., FixMatch) on the same survival tasks to evaluate whether consistency regularization is the optimal semi-supervised strategy for this domain