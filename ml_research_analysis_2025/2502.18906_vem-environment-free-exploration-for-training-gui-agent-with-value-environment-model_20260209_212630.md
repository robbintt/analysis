---
ver: rpa2
title: 'VEM: Environment-Free Exploration for Training GUI Agent with Value Environment
  Model'
arxiv_id: '2502.18906'
source_url: https://arxiv.org/abs/2502.18906
tags:
- action
- task
- policy
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VEM, a framework that addresses challenges
  in training GUI agents through environment-free reinforcement learning. The core
  idea is to decouple value estimation from policy optimization by leveraging a pretrained
  Value Environment Model (VEM) that directly predicts state-action values from offline
  data, avoiding costly environmental interactions and compounding errors.
---

# VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model

## Quick Facts
- arXiv ID: 2502.18906
- Source URL: https://arxiv.org/abs/2502.18906
- Reference count: 40
- This paper introduces VEM, a framework that addresses challenges in training GUI agents through environment-free reinforcement learning.

## Executive Summary
This paper introduces VEM, a framework that addresses challenges in training GUI agents through environment-free reinforcement learning. The core idea is to decouple value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM) that directly predicts state-action values from offline data, avoiding costly environmental interactions and compounding errors. The VEM is trained using binary labels from GPT-4o annotations and then frozen to guide policy exploration. Evaluated on Android-in-the-Wild benchmarks, VEM achieves 28.0% task success on General tasks and 21.0% on Webshopping tasks in offline settings, outperforming environment-free baselines by 12-28%. In online deployment, it reaches 42.4% task success on General tasks, matching environment-based methods while eliminating interaction costs. The approach demonstrates that semantic-aware value estimation can achieve comparable performance to online-trained methods, advancing environment-free GUI automation.

## Method Summary
The method involves two stages: first, training a Value Environment Model (VEM) by fine-tuning Qwen2VL with GPT-4o-annotated binary labels on offline data; second, optimizing a policy (Auto-GUI) using PPO to maximize the frozen VEM's predicted values. The approach avoids environment interaction by leveraging semantic reasoning over GUI elements rather than next-state prediction, and uses a small dataset (500 trajectories) to train both the value model and policy. The framework achieves comparable performance to environment-based methods while eliminating the need for costly interaction data.

## Key Results
- VEM achieves 28.0% task success on General tasks and 21.0% on Webshopping tasks in offline settings
- In online deployment, VEM reaches 42.4% task success on General tasks
- Outperforms environment-free baselines by 12-28% on AITW benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling value estimation from policy optimization enables stable offline training without environment interaction.
- **Mechanism:** A Value Environment Model (VEM) is pretrained to predict Q(s,a) from offline data, then frozen. The policy learns by maximizing expected VEM values via PPO, avoiding on-policy rollouts and environment stochasticity.
- **Core assumption:** The frozen VEM approximates optimal Q-values sufficiently on the dataset support (Assumption: Qθ ≈ Q* within ε on D).
- **Evidence anchors:**
  - [abstract]: "decouple value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM)"
  - [Section 3.3]: "By freezing Qθ as a fixed state-action value estimator, we transform policy learning into a stable optimization problem"
  - [corpus]: Weak direct corpus support; related work GUI-Xplore addresses generalization via exploration but not VEM-style decoupling.
- **Break condition:** If distribution shift causes the policy to select actions outside dataset support, value estimates become unreliable (‖π̂ − β‖ large).

### Mechanism 2
- **Claim:** Semantic reasoning over GUI elements provides more robust value signals than next-state prediction.
- **Mechanism:** VEM answers "Does this action advance the goal?" rather than predicting pixels. This bypasses compounding errors in sequential state prediction and remains valid across layout changes.
- **Core assumption:** Binary annotations from GPT-4o capture task-progress signals that generalize across UI variations.
- **Evidence anchors:**
  - [abstract]: "focusing on semantic reasoning (e.g., 'Does this action advance the user's goal?')"
  - [Section 1]: "avoids the compounding errors of next-state prediction by focusing on value estimation"
  - [corpus]: No direct corpus comparison of semantic vs. predictive value models found.
- **Break condition:** If semantic cues are insufficient to distinguish high-value actions (e.g., visually similar but functionally distinct buttons), VEM predictions degrade.

### Mechanism 3
- **Claim:** GPT-4o-annotated binary labels provide sufficient supervision for value model training.
- **Mechanism:** Each (state, action) pair receives a binary label ℓ ∈ {0,1} via chain-of-thought prompting. VEM is trained via MSE regression: min E[(Qθ(s,a) − ℓ)²].
- **Core assumption:** GPT-4o's task understanding generalizes to held-out GUI contexts; binary granularity captures enough signal.
- **Evidence anchors:**
  - [Section 3.2]: "GPT-4o annotations achieve 90% human consistency"
  - [Section 4.3]: VEM achieves F1 of 78–80% on annotation tasks
  - [corpus]: No corpus papers validate GPT-4o annotation quality for RL value models.
- **Break condition:** If annotation noise is systematic (e.g., consistent over-optimism), policy may exploit spurious high-value actions.

## Foundational Learning

- **Concept: Q-Function and Value-Based RL**
  - **Why needed here:** VEM is a learned Q-function; understanding Bellman equations, value backup, and off-policy evaluation is essential.
  - **Quick check question:** Can you explain why maximizing Q(s,a) offline differs from on-policy TD learning?

- **Concept: Offline RL and Distribution Shift**
  - **Why needed here:** The framework operates entirely offline; distribution shift (π deviating from β) bounds performance per Theorem 3.1.
  - **Quick check question:** What happens if the learned policy queries Q-values for state-action pairs absent from the training dataset?

- **Concept: Vision-Language Models for GUIs**
  - **Why needed here:** Both VEM (Qwen2VL) and policy (Auto-GUI) are VLMs; familiarity with multimodal grounding and coordinate representations is required.
  - **Quick check question:** How does Qwen2VL encode screenshot + task description + action history as input?

## Architecture Onboarding

- **Component map:** Annotation Pipeline → VEM Training → Policy Optimization
- **Critical path:**
  1. Verify annotation quality (GPT-4o prompts, human consistency spot-check)
  2. Train VEM to convergence (monitor MSE; target F1 > 0.75)
  3. Freeze VEM; train policy with PPO (monitor Q-value loss stability as in Figure 4)
- **Design tradeoffs:**
  - Binary labels vs. continuous values: simpler supervision but limited granularity
  - Frozen VEM vs. joint updates: stability vs. potential value-policy misalignment
  - Small dataset (500 trajectories) vs. scalability: sample-efficient but may limit coverage
- **Failure signatures:**
  - Policy exploits high-Q actions not actually beneficial (annotation bias)
  - VEM overfits to training app layouts, fails on new GUIs
  - Training instability: erratic Q-loss indicates learning rate or batch size issues
- **First 3 experiments:**
  1. **VEM validation:** Measure annotation consistency and VEM F1/accuracy on held-out data (replicate Table 2)
  2. **Ablation:** Train policy with random value labels vs. GPT-4o labels to isolate annotation signal
  3. **Distribution shift probe:** Evaluate policy on in-distribution vs. out-of-distribution GUI tasks; compare success rate gaps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can self-supervised training methods effectively replace GPT-4o annotations for the Value Environment Model (VEM) to reduce labeling costs and improve scalability?
- **Basis:** [explicit] The Conclusion states: "In the future, we plan to explore self-supervised approaches for training the value model, aiming to reduce labeling overhead and further improve scalability."
- **Why unresolved:** The current framework relies on supervised fine-tuning using expensive GPT-4o generated binary labels. The authors have not yet validated if intrinsic motivation or self-supervised objectives can replicate the quality of these external annotations.
- **Evidence:** Experiments comparing policy performance when VEM is trained via self-supervised objectives (e.g., contrastive learning on interaction sequences) versus the current GPT-4o labeled method.

### Open Question 2
- **Question:** Does the simplification of value targets to binary labels (0 or 1) limit the precision of credit assignment compared to continuous or multi-step value predictions?
- **Basis:** [inferred] Section 3.2 describes the labels as "coarse supervision signals" that approximate long-term value through immediate assessments. The authors acknowledge this simplification but do not test if the lack of granular value distinction hinders learning in complex, multi-stage tasks.
- **Why unresolved:** While binary labels simplify training, they fail to distinguish between actions that are slightly beneficial versus highly optimal, potentially flattening the learning gradient for the policy model.
- **Evidence:** An ablation study training the VEM with continuous labels (e.g., normalized distance to task completion or discounted return) and comparing the resulting policy success rates against the binary baseline.

### Open Question 3
- **Question:** How robust is the frozen VEM to distribution shift when the policy explores actions that deviate significantly from the static offline dataset?
- **Basis:** [inferred] The method relies on a frozen VEM to guide policy exploration (Section 3.3) and assumes the policy remains within the dataset's support (Theorem 3.1). However, the paper lacks analysis on how the VEM hallucinates or fails when evaluating out-of-distribution (OOD) actions proposed by a diverging policy.
- **Why unresolved:** The theoretical bound suggests performance drops if the policy strays from the dataset distribution ($\|\bar{\pi} - \beta\|$), but the empirical failure modes of the frozen VEM in OOD scenarios are not demonstrated.
- **Evidence:** An evaluation measuring the correlation between VEM confidence scores and actual task success rates for actions with low probability in the original training dataset.

## Limitations
- Performance remains significantly below human levels on GUI tasks
- Reliance on expensive GPT-4o annotations limits scalability
- Limited analysis of VEM robustness to distribution shift and out-of-distribution actions

## Confidence
- Decoupling value estimation from policy optimization: **High**
- Semantic reasoning over GUI elements: **Medium**
- GPT-4o annotation quality for RL: **Medium**
- Long-term robustness of frozen value models: **Low**

## Next Checks
1. Conduct a distribution shift analysis by evaluating the policy on held-out app categories and measuring the correlation between dataset coverage and success rates
2. Perform an ablation study comparing VEM-guided exploration against random exploration with identical computational budget to quantify the value of semantic reasoning
3. Test policy performance when VEM predictions are deliberately corrupted (e.g., label flipping) to measure sensitivity to annotation noise