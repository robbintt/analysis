---
ver: rpa2
title: 'STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing
  Question Quality Estimation'
arxiv_id: '2504.05693'
source_url: https://arxiv.org/abs/2504.05693
tags:
- question
- human
- evaluation
- baseline
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces STRIVE, a structured iterative method for
  automated question quality estimation using multiple large language models. The
  approach generates multiple strength-weakness pairs for each question, selects the
  best pairs, and iteratively refines the evaluation through two Think & Improve modules
  until convergence.
---

# STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation

## Quick Facts
- arXiv ID: 2504.05693
- Source URL: https://arxiv.org/abs/2504.05693
- Reference count: 35
- Primary result: Iterative refinement with two LLM modules improves automated question quality estimation correlation with human judgments from 0.26-0.41 to 0.42-0.62 for relevance and 0.41-0.41 to 0.56-0.62 for appropriateness

## Executive Summary
STRIVE introduces a structured iterative method for automated question quality estimation using multiple large language models. The approach generates multiple strength-weakness pairs for each question at different temperature settings, selects the best pairs through LLM judging, and iteratively refines the evaluation through two Think & Improve modules until convergence. Tested on EduProbe and SciQ datasets, STRIVE shows improved correlation with human judgments compared to baseline methods, with Pearson correlation increases from 0.26-0.41 to 0.42-0.62 for relevance and 0.41-0.41 to 0.56-0.62 for appropriateness. Error analysis demonstrates that 63-71% of relevance scores and 71-74% of appropriateness scores exactly match human expert ratings using STRIVE, compared to 40-45% and 45-60% respectively with baseline approaches.

## Method Summary
STRIVE uses two independent Think & Improve modules (TM1 and TM2) that iteratively exchange evaluations until convergence. Each module generates 10 strength-weakness pairs for a given question at different temperature values, uses a judge LLM to select the best pair, and produces metric scores. TM1's output feeds TM2, which generates variations and returns its best evaluation to TM1. This process continues until both modules produce identical scores for two consecutive iterations across all five quality metrics (grammaticality, relevance, appropriateness, complexity, novelty). The method is tested on 1000 <Context, Question> pairs from both EduProbe and SciQ datasets, comparing Pearson correlation with human expert scores against a single-pass baseline.

## Key Results
- Pearson correlation improvements: Relevance increases from 0.26-0.41 to 0.42-0.62; Appropriateness increases from 0.41-0.41 to 0.56-0.62
- Exact match percentage improvements: Relevance matches increase from 40-45% to 63-71%; Appropriateness matches increase from 45-60% to 71-74%
- Performance comparison: GPT-4 outperforms Gemini-Pro and Llama3 across all metrics and datasets
- Consistency: Results validated across two distinct educational datasets (EduProbe and SciQ) covering multiple subjects and grade levels

## Why This Works (Mechanism)

### Mechanism 1: Temperature-induced diversity generation
Generating multiple evaluations at varying temperature values captures broader quality signals than single-pass evaluation. For each question, 10 <strength, weakness> pairs are generated using different temperature settings (0.3-1.2), creating evaluation diversity that reduces the risk of single-generation bias. This assumes temperature variation produces semantically meaningful differences rather than surface-level noise.

### Mechanism 2: LLM-as-judge selection filtering
Using an LLM as a judge to select the best evaluation from candidate sets improves alignment with human quality judgments. After generating 10 candidate evaluations, a second LLM call selects the most appropriate strength and weakness, then produces final metric scores. This acts as an internal quality filter based on the assumption that LLM selection reliably identifies higher-quality evaluations that correlate with human expert preferences.

### Mechanism 3: Iterative convergence between independent modules
Iterative exchange between two independent Think modules drives evaluation scores toward convergence with human-aligned stability. TM1 generates initial evaluations and passes best strength/flaw to TM2. TM2 generates variations, selects its best, and returns feedback to TM1. This alternates until TM1 and TM2 produce identical scores for two consecutive iterations, under the assumption that convergence indicates evaluation stability and improved alignment with ground truth.

## Foundational Learning

- **Temperature sampling in language models**:
  - Why needed here: Understanding how temperature affects output diversity is critical for interpreting why 10 generations at different temperatures might capture evaluative nuance.
  - Quick check question: Can you explain why temperature=0.2 produces more deterministic outputs than temperature=0.8?

- **Convergence criteria in iterative systems**:
  - Why needed here: The method terminates when two modules agree for two consecutive iterations; understanding convergence prevents infinite loops or premature stopping.
  - Quick check question: What are two failure modes for an iterative algorithm with a convergence stopping condition?

- **Correlation metrics for evaluation alignment**:
  - Why needed here: The paper uses Pearson correlation to measure alignment between LLM scores and human expert scores.
  - Quick check question: What does a Pearson correlation of 0.62 between LLM and human scores indicate about their relationship?

## Architecture Onboarding

- **Component map**: Question+Context → TM1 (generate 10 pairs → judge selects best → score) → TM2 (generate 10 variations → judge selects best → score) → convergence check → iterate or return

- **Critical path**: Question+Context → TM1 (generate 10 pairs → judge selects best → score) → TM2 (generate 10 variations → judge selects best → score) → convergence check → iterate or return

- **Design tradeoffs**:
  - Number of generations (10): Higher values increase coverage but raise API costs linearly
  - Convergence threshold (exact match, 2 iterations): Stricter than tolerance-based convergence; may cause longer runs
  - Model choice: GPT-4 outperforms Gemini-Pro and Llama3, but cost/performance tradeoff varies by deployment context

- **Failure signatures**:
  - Non-convergence: If TM1 and TM2 oscillate without reaching identical scores, the loop may not terminate (implement max-iteration cap)
  - Metric-specific degradation: Tables show Novelty consistently has lowest human-LLM match; this metric may be inherently harder to automate
  - Cost explosion: Each iteration requires 20+ LLM calls (10 generations × 2 modules plus judge calls)

- **First 3 experiments**:
  1. **Baseline replication**: Run single-pass evaluation on 50 <Context, Question> pairs, compute Pearson correlation with provided human scores to verify baseline numbers.
  2. **Ablation on generation count**: Test with 5 and 15 generations instead of 10 to determine if quality gains saturate or improve with more candidates.
  3. **Convergence sensitivity**: Replace exact-match convergence with tolerance-based (e.g., scores within 0.1) and measure impact on correlation and iteration count.

## Open Questions the Paper Calls Out

- **Computational overhead comparison**: How does the computational overhead and latency of STRIVE's iterative refinement compare to single-pass baselines, and can the process be optimized for real-time educational settings? The paper focuses on correlation accuracy without reporting computational efficiency, latency, or API costs associated with the iterative loops.

- **Novelty metric performance**: Why does the STRIVE approach demonstrate significantly lower agreement with human experts on "Novelty" compared to metrics like "Grammaticality," and can the iteration mechanism be adapted to handle subjective metrics better? While the paper notes the lower performance, it does not investigate whether iterative convergence creates bias toward "safe" evaluations, reducing novelty detection.

- **Convergence criterion validation**: Does the convergence criterion of "identical scores for two consecutive iterations" risk premature termination or "model collapse" where the LLMs agree on a consistent but incorrect evaluation? The paper assumes convergence equals accuracy but does not explore whether modules simply align on shared errors without external grounding.

## Limitations

- Prompt templates and exact temperature values are not fully specified, creating reproducibility challenges
- The method shows notably weaker performance on the "Novelty" metric compared to other quality dimensions
- The approach requires significantly more computational resources than single-pass baselines due to iterative refinement

## Confidence

**High confidence**: The reported correlation improvements (Pearson increases from 0.26-0.41 to 0.42-0.62 for relevance and 0.41-0.41 to 0.56-0.62 for appropriateness) are statistically robust and consistently observed across both EduProbe and SciQ datasets.

**Medium confidence**: The mechanism explanations for why temperature variation and iterative refinement produce better results are plausible but not empirically validated. The assumption that convergence between two modules indicates quality rather than systematic bias remains untested.

**Low confidence**: The claim that STRIVE "excels at capturing relevance and appropriateness metrics" lacks comparative analysis against alternative approaches beyond the single baseline provided. Cost-benefit analysis of the iterative approach versus simpler alternatives is absent.

## Next Checks

1. **Ablation study on generation count**: Test STRIVE with 5, 10, and 15 candidate generations per module to determine if the improvement curve plateaus, suggesting diminishing returns beyond a certain diversity threshold.

2. **Alternative convergence criteria**: Replace exact score matching with tolerance-based convergence (±0.1 on all metrics) and measure impact on both correlation scores and iteration counts to assess whether the strict criterion is necessary or optimal.

3. **External validation on new dataset**: Apply STRIVE to a held-out test set from a different educational domain or grade level to verify whether the reported improvements generalize beyond the EduProbe and SciQ datasets used in training and validation.