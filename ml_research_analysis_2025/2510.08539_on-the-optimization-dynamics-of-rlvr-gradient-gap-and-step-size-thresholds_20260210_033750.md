---
ver: rpa2
title: 'On the optimization dynamics of RLVR: Gradient gap and step size thresholds'
arxiv_id: '2510.08539'
source_url: https://arxiv.org/abs/2510.08539
tags:
- step
- gradient
- policy
- size
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a theoretical framework for understanding the
  convergence dynamics of RLVR in LLM post-training. The key contribution is the introduction
  of the Gradient Gap, which characterizes the direction of improvement from low-reward
  to high-reward regions in the response space.
---

# On the optimization dynamics of RLVR: Gradient gap and step size thresholds

## Quick Facts
- **arXiv ID:** 2510.08539
- **Source URL:** https://arxiv.org/abs/2510.08539
- **Reference count:** 40
- **Primary result:** Proves convergence in RLVR depends on Gradient Gap alignment and derives sharp step-size thresholds; validates predictions through bandit simulations

## Executive Summary
This paper develops a theoretical framework for understanding convergence dynamics in RLVR (Reinforcement Learning from Verifiable Rewards) for LLM post-training. The key contribution is the introduction of the "Gradient Gap" - the vector pointing from incorrect to correct response gradients - which characterizes the direction of improvement in response space. The authors prove that successful optimization requires aligning updates with this gap and derive a sharp step-size threshold: below it, learning converges; above it, performance collapses. The theory explains why length normalization stabilizes training and why fixed step sizes can cause stagnation, with predictions validated through controlled bandit simulations.

## Method Summary
The paper analyzes RLVR optimization dynamics through a theoretical framework focusing on the Gradient Gap between high-reward and low-reward response regions. Using contextual bandit experiments with linear reward structures, the authors validate their theoretical predictions about step-size thresholds and convergence behavior. The method involves exact REINFORCE gradient updates with step size η = 0.1 on a synthetic bandit task where contexts x ∈ [0,1]^d (d=10) select among N=100 arms with linear scores. Training contexts are selected with intermediate difficulty (J ∈ [0.2, 0.8]), and performance is evaluated across 500 random contexts to measure the relationship between cumulative gradient gap and value function improvement.

## Key Results
- Proves convergence in RLVR depends critically on aligning update directions with the Gradient Gap between high-reward and low-reward response regions
- Derives a sharp step-size threshold: below it, learning converges; above it, performance collapses rather than learning slower
- Shows effective step size must scale inversely with response length (1/T scaling) and adapt to success rate for stability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Convergence in RLVR is governed by the alignment between the update direction and the "Gradient Gap" (the vector pointing from incorrect to correct response gradients).
- **Mechanism:** The authors define the Gradient Gap as $g^+_q - g^-_q$, representing the expected difference in log-probability gradients between high-reward ($O^+$) and low-reward ($O^-$) response regions. Optimization succeeds when the update vector $w_k$ aligns with this gap (positive inner product), increasing the log-odds of correct responses.
- **Core assumption:** The policy score function $\nabla_\theta \log \pi_\theta$ is Lipschitz continuous and bounded (Assumption 1).
- **Evidence anchors:**
  - [abstract] Introduces "Gradient Gap" as formalizing the direction of improvement.
  - [section 3.1] Defines Gap Alignment $\Delta\mu_q(k)$ and its relation to the objective.
  - [corpus] "Stabilizing Knowledge..." discusses constraints in RLVR, implying the difficulty of maintaining this alignment without specific intervention.
- **Break condition:** If the cumulative alignment $M(K)$ remains bounded or the update direction is orthogonal/opposite to the Gradient Gap, learning stagnates (Theorem 1a).

### Mechanism 2
- **Claim:** A sharp step-size threshold exists; crossing it causes performance collapse rather than slower learning.
- **Mechanism:** The authors derive a critical step size $\eta_k$ proportional to the Gap Alignment. If the step size is too large, the second-order variance term (specifically variance in the negative response space) dominates the first-order gradient gain. This causes "overshooting," where the probability of correct answers strictly decreases at every step.
- **Core assumption:** Regularity of the token policy score (Assumption 2) and sub-exponential response length (Assumption 3).
- **Evidence anchors:**
  - [abstract] States "below it, learning converges; above it, performance collapses."
  - [theorem 2] Proves that an overly large step size leads to $J_q(k) \to 0$ even with positive alignment.
  - [corpus] "Gradient Descent with Provably Tuned Learning-rate Schedules" supports the general necessity of precise learning rate scaling.
- **Break condition:** Violating the condition $\eta_k \lesssim \frac{\Delta\mu_q}{L + G^2}$ triggers immediate performance degradation.

### Mechanism 3
- **Claim:** Effective step size must scale inversely with response length ($T$) and adapt to the success rate ($J_q$).
- **Mechanism:** Longer responses accumulate more token-level gradient variance, requiring a smaller step size ($1/T$ scaling) to maintain stability. Similarly, as the success rate approaches 1, the dynamics change; length normalization (as in GRPO) stabilizes training by automatically adjusting the effective step size.
- **Core assumption:** The log-probability score is a sum of token-wise scores (autoregressive structure).
- **Evidence anchors:**
  - [section 4] Shows critical step size depends on $T_{\infty}$ and $T_{\psi_1}$.
  - [section 4] Analyzes GRPO vs. Dr. GRPO, showing GRPO's length normalization aligns with theoretical stability requirements.
  - [corpus] "Revisiting Entropy..." highlights entropy collapse in RLVR, which relates to the shrinking valid step-size window as training progresses.
- **Break condition:** Using a fixed step size for tasks with varying sequence lengths or difficulties (success rates) leads to instability or stagnation.

## Foundational Learning

- **Concept:** **Log-Odds Ratio Optimization**
  - **Why needed here:** The paper's core proof tracks the change in $\log(J_q/(1-J_q))$ rather than the raw probability. Understanding how gradients affect log-odds is essential to grasp why the "Gradient Gap" drives improvement.
  - **Quick check question:** Why is the log-odds parameterization more sensitive to gradient updates than the raw probability when $J_q$ is near 0 or 1?

- **Concept:** **Score Function Regularity (Lipschitz Continuity)**
  - **Why needed here:** The theoretical bounds on step size rely entirely on the assumption that the gradient of the log-probability ($\nabla \log \pi$) doesn't change too abruptly (Lipschitz) and isn't infinite (Bounded).
  - **Quick check question:** If the policy network had a massive Lipschitz constant $L$, how would that change the safe learning rate?

- **Concept:** **Variance of the Score Function**
  - **Why needed here:** The "overshooting" failure mode is driven by the variance of the score function in the negative (incorrect) response space. High variance requires smaller steps to avoid "bang-bang" oscillation.
  - **Quick check question:** In a bandit setting, why would a high-variance incorrect arm be more dangerous to optimize than a low-variance one?

## Architecture Onboarding

- **Component map:** Policy ($\pi_\theta$) -> Response ($\vec{o}$) -> Reward Model ($r^*$) -> Gradient Estimator -> Optimizer
- **Critical path:**
  1. Sample responses and classify into $O^+$ (success) and $O^-$ (failure).
  2. Estimate the **Gradient Gap** ($g^+ - g^-$) to determine the ideal update direction.
  3. Calculate the **Gap Alignment** ($\Delta\mu$) to determine the valid step-size magnitude.
  4. Update weights $\theta$ only if $\Delta\mu > 0$ and step size $\eta$ is below the derived threshold.

- **Design tradeoffs:**
  - **GRPO vs. Dr. GRPO:** GRPO normalizes by length ($1/|\vec{o}|$), aligning with the paper's stability scaling. Dr. GRPO removes this, risking overshooting on long sequences but avoiding potential bias from normalization.
  - **Fixed vs. Adaptive LR:** Fixed learning rates are theoretically shown to cause stagnation (never reaching 100% success); adaptive rates scaling with variance/length are preferred.

- **Failure signatures:**
  - **Stagnation:** Success rate plateaus strictly below 100% (e.g., 80-90%).
  - **Collapse:** Success rate drops monotonically to 0 despite "correct" gradient signs (due to large step sizes).

- **First 3 experiments:**
  1. **Replicate the Contextual Bandit:** Train a softmax policy on a multi-armed bandit to verify the "logistic relationship" between cumulative gradient gap and value function (Figure 1).
  2. **Step Size Ablation:** Run the optimization on a synthetic task while varying $\eta$. Plot the "sharp threshold" where performance flips from convergence to collapse to validate Theorem 2.
  3. **Length Normalization Check:** Compare GRPO (length-normalized) vs. vanilla REINFORCE on a task with variable sequence lengths to confirm the $1/T$ scaling stability prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the optimization dynamics of RLVR change when training over a diverse batch of prompts rather than a single prompt?
- **Basis in paper:** [explicit] The discussion section explicitly states the analysis is restricted to a single prompt and that "a single update direction $w_k$ may align well with some prompts but poorly with others" in batch settings.
- **Why unresolved:** The current theory assumes a unified update direction for a specific prompt's gradient gap, but batch updates force a compromise that may cause misalignment or overshooting for individual prompts within the batch.
- **What evidence would resolve it:** A theoretical extension deriving convergence bounds for a distribution of prompts, or empirical results showing the variance of gap alignment across a batch correlates with training instability.

### Open Question 2
- **Question:** Can prompt-adaptive update mechanisms (adjusting direction or step size) mitigate the instability caused by batch heterogeneity?
- **Basis in paper:** [explicit] The authors list "developing prompt-adaptive updates that adjust direction or scale based on batch heterogeneity" as a primary direction for future work.
- **Why unresolved:** Current algorithms like GRPO generally apply a global step size and update direction, which the paper suggests may be suboptimal when the optimal update varies significantly across prompts.
- **What evidence would resolve it:** The development and empirical validation of an RLVR variant that scales step sizes based on per-prompt statistics (like the Gradient Gap magnitude) and demonstrates faster convergence or higher final accuracy than uniform scaling.

### Open Question 3
- **Question:** Do the derived step-size thresholds and length scaling predictions ($ \eta \propto 1/|\vec{o}| $) hold empirically in large-scale Transformer models?
- **Basis in paper:** [inferred] The paper validates its theory using "controlled bandit simulations" (Section 5) rather than full-scale LLMs, and relies on specific regularity assumptions (Assumptions 1 and 2) regarding the score function.
- **Why unresolved:** While the bandit experiments support the theory, the dynamics in high-dimensional Transformer parameter spaces and real-world token distributions may violate the regularity assumptions or introduce factors (like attention patterns) not captured by the bandit model.
- **What evidence would resolve it:** Experiments on standard RLVR benchmarks (e.g., math or code generation) showing that tuning the step size inversely to response length stabilizes training and prevents collapse, matching the theoretical predictions.

## Limitations

- The theoretical framework relies on strong assumptions about Lipschitz continuity and boundedness of the policy score function that may not hold in practical LLM settings
- The analysis assumes stationary reward distributions while real LLM training involves evolving prompts and data distributions
- The theory predicts irreversible performance collapse, but empirical observations sometimes show recovery behaviors not captured by the model

## Confidence

**High Confidence (Mechanistic Validity):** The core mathematical derivations connecting gradient gap alignment to convergence, and the proof that step-size thresholds exist, appear sound within the theoretical framework. The relationship between log-odds optimization and probability improvement is well-established.

**Medium Confidence (Practical Applicability):** The specific scaling relationships (inverse with response length, dependence on success rate) are theoretically derived but may be less sharp in practice due to the simplifying assumptions. The bandit simulations provide initial validation but may not fully capture LLM complexities.

**Low Confidence (Failure Mode Completeness):** The analysis focuses on two failure modes (stagnation and collapse) but may miss other practical failure patterns like mode collapse, reward hacking, or slow convergence due to poor exploration.

## Next Checks

1. **Lipschitz Constant Sensitivity Analysis:** Systematically vary the assumed Lipschitz constant $L$ in the bandit simulations to quantify how sensitive the convergence threshold is to this critical parameter. This would validate whether the theoretical bounds are practically useful or overly conservative.

2. **Multi-Token Correlation Effects:** Extend the theoretical framework to account for token-level correlations in LLM responses. Design experiments where token dependencies are explicitly controlled (e.g., through synthetic autoregressive models) to test whether the variance-based step-size scaling remains accurate.

3. **Dynamic Success Rate Regimes:** Create experiments where the success rate $J_q$ evolves during training (e.g., through curriculum learning or varying task difficulty) to test the theory's predictions about step-size adaptation requirements across different optimization phases.