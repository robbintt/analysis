---
ver: rpa2
title: 'Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents
  Generation'
arxiv_id: '2601.02128'
source_url: https://arxiv.org/abs/2601.02128
tags:
- segmentation
- topic
- hierarchical
- transcripts
- inproc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address hierarchical topic segmentation of speech transcripts,
  a problem where spoken content lacks the structural organization found in written
  text. Their method, ToC-LLM, uses LoRA fine-tuned large language models to generate
  multi-level tables of contents directly from transcripts, producing nested topic
  structures.
---

# Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation

## Quick Facts
- **arXiv ID:** 2601.02128
- **Source URL:** https://arxiv.org/abs/2601.02128
- **Reference count:** 0
- **Primary result:** LoRA fine-tuned LLM generates multi-level ToCs from transcripts; achieves F1=67.34 and B=0.55 on VIDEOAULA.

## Executive Summary
This paper tackles hierarchical topic segmentation of speech transcripts, where spoken content lacks the structural organization found in written text. The authors propose ToC-LLM, a method that uses LoRA fine-tuned large language models to generate multi-level tables of contents directly from transcripts, producing nested topic structures. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show that their approach outperforms established linear baselines in both flat and hierarchical segmentation tasks.

## Method Summary
The approach fine-tunes LoRA adapters on large language models (Mistral Nemo 12B, Qwen2.5-7B) to generate hierarchical ToCs from speech transcripts. Input preprocessing includes ASR transcription (Whisper large-v2), forced alignment (Montreal Forced Aligner) for pause extraction, and sentence segmentation (SpaCy). The model outputs dotted-numbered outlines (e.g., 2.2.1) with sentence indices. Acoustic cues (inter-sentence pause durations) are optionally included as text annotations. Training uses leave-one-speaker-out validation on three datasets: AMI (English meetings), VIDEOAULA (Portuguese lectures), and LECTUREDE (German lectures).

## Key Results
- Fine-tuned ToC-NEMO achieves F1=67.34 and B=0.55 on VIDEOAULA, outperforming linear baselines.
- LoRA fine-tuning improves F1 from 12.27 (zero-shot) to 64.26 on VIDEOAULA.
- Acoustic pause cues improve performance only when model is fine-tuned to interpret them.
- Hierarchical B_hier metric captures segmentation quality across all levels.

## Why This Works (Mechanism)

### Mechanism 1: LoRA Fine-Tuning for Task-Specific Adaptation
- **Core assumption:** Pre-trained LLM has sufficient language understanding; only task alignment is missing.
- **Evidence:** TOC-NEMO zero-shot F1=12.27 vs. fine-tuned F1=64.26 on VIDEOAULA; fine-tuned TOC-NEMO+PAUSE outperforms all baselines.
- **Break condition:** If target domain differs radically from pre-training distribution, zero-shot may remain inadequate.

### Mechanism 2: Structured Output Schema Enforces Hierarchy
- **Core assumption:** LLMs have internalized hierarchical document conventions from pre-training on structured text.
- **Evidence:** TOC-NEMO consistently outperforms SEGMENTLLM NEMO (flat output) across all datasets despite using the same base model.
- **Break condition:** If transcript length exceeds context window or topic structure is ambiguous, the LLM may produce inconsistent depth or skip levels.

### Mechanism 3: Acoustic Cues Require Supervised Integration
- **Core assumption:** Pause duration correlates with topic boundaries in spoken content.
- **Evidence:** Zero-shot TOC-NEMO+PAUSE degrades vs. TOC-NEMO on AMI and VIDEOAULA; fine-tuned TOC-NEMO+PAUSE improves across all datasets.
- **Break condition:** If forced alignment is poor or ASR errors misalign sentence boundaries, pause features become misleading.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed: Enables efficient fine-tuning of large LLMs (12B parameters) on modest hardware while preserving base capabilities.
  - Quick check: What rank (r) and alpha values are used, and how do they affect the tradeoff between adaptation capacity and overfitting risk?

- **Forced Alignment (Montreal Forced Aligner)**
  - Why needed: Synchronizes ASR-generated transcripts with audio to extract precise inter-sentence pause durations for acoustic cue injection.
  - Quick check: How does alignment quality degrade with ASR errors, and what is the fallback when alignment fails?

- **Boundary Similarity (B-score)**
  - Why needed: Evaluation metric accounting for near-miss boundary predictions; extended in paper to hierarchical settings (B_hier).
  - Quick check: How does B-score differ from F1 in handling boundaries that are off by one or two sentences?

## Architecture Onboarding

- **Component map:**
  - Audio -> Whisper large-v2 ASR -> Montreal Forced Aligner -> SpaCy sentence segmentation -> Pause extraction -> ToC-LLM (Mistral Nemo/Qwen2.5-7B with LoRA) -> Dotted-numbered ToC with sentence indices

- **Critical path:**
  1. Preprocess audio → transcript → alignment → pause annotations
  2. Format input with sentence indices and optional pause tokens
  3. Fine-tune LoRA adapters on domain-specific ToC annotations
  4. Generate ToC, parse dotted numbers and indices into hierarchical segments
  5. Compute B_hier by aligning reference and hypothesis levels

- **Design tradeoffs:**
  - 4-bit quantization reduces memory but may affect precision on subtle boundary decisions
  - ToC output format enforces structure but requires parsing; errors in numbering propagate to segmentation
  - Pause features help when fine-tuned but add pipeline complexity (alignment dependency)
  - Leave-one-speaker-out validation is robust but increases experiment duration

- **Failure signatures:**
  - Flat outputs (e.g., all segments at level 1) suggest LoRA did not converge or prompt format mismatch
  - Inconsistent depth across similar documents indicates context window pressure or prompt instability
  - B_hier significantly lower than linear B suggests hierarchy is not being captured (check output parsing)

- **First 3 experiments:**
  1. **Baseline comparison:** Run zero-shot TOC-NEMO vs. fine-tuned TOC-NEMO on held-out set; verify Table 1 replication (expected: ~5x F1 improvement with fine-tuning).
  2. **Ablation on pause features:** Compare fine-tuned TOC-NEMO with and without pause annotations; expect degradation without pauses (ΔF1 ≈ -3 on VIDEOAULA based on Table 1).
  3. **Hierarchy depth analysis:** Compute level-specific B-scores (as in Figure 2); verify that deeper reference levels (e.g., level 4-5) match finer hypothesis levels. If not, investigate whether model is under-generating depth.

## Open Questions the Paper Calls Out

- **How can the substantial performance gap between zero-shot prompting and LoRA fine-tuning for hierarchical segmentation be minimized without relying on supervised data from the target domain?**
  - The conclusion states that zero-shot models show "substantial performance gaps, pointing to the need for more effective solutions that do not rely on supervised data from the target domain or language."

- **To what extent does the quality of the upstream Automatic Speech Recognition (ASR) transcription affect the reliability of the generated Table of Contents?**
  - The method relies on Whisper large-v2 to generate transcripts, but the evaluation does not isolate the impact of ASR errors on the LLM's ability to detect topic boundaries.

- **Can the integration of richer prosodic and speaker-diarization features further enhance hierarchical boundary detection beyond inter-sentence pause durations?**
  - The authors limit acoustic integration to "inter-sentence pause durations," despite acknowledging that other works utilize pitch, volume, and speaker changes.

## Limitations
- LECTUREDE dataset is internal and unavailable for public reproduction, limiting external validation of multilingual claims.
- Key LoRA hyperparameters and training configurations are not specified, making exact replication difficult.
- Cross-lingual generalization claims are based on limited sample sizes (34 and 96 transcripts), making broader applicability uncertain.

## Confidence

- **High Confidence:** The core mechanism of LoRA fine-tuning improving hierarchical segmentation over zero-shot baselines is well-supported by the 5× F1 improvement on VIDEOAULA and consistent results across all three datasets.

- **Medium Confidence:** The effectiveness of acoustic cues (pause durations) is supported but requires fine-tuning for proper integration. The degradation observed in zero-shot with pauses suggests the mechanism is conditional rather than universal.

- **Low Confidence:** Cross-lingual generalization claims are based on two non-English datasets with limited sample sizes, making broader multilingual applicability uncertain.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary LoRA rank and alpha values to determine optimal configuration and assess overfitting risk, as these parameters critically impact adaptation capacity.

2. **Zero-shot vs. Fine-tuned Ablation:** Run controlled experiments comparing zero-shot, fine-tuned without pauses, and fine-tuned with pauses on held-out AMI data to quantify the exact contribution of each component.

3. **Boundary Persistence Verification:** Implement automated checks for hierarchical consistency—verify that every coarse boundary in level l exists in all finer levels l+1, as required by the B_hier metric's assumptions.