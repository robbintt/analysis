---
ver: rpa2
title: 'Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers
  to Leak Privacy'
arxiv_id: '2506.19486'
source_url: https://arxiv.org/abs/2506.19486
tags:
- other
- forgetting
- classes
- class
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a membership recall attack (MRA) framework
  to recover forgotten class memberships from unlearned machine learning models (ULMs)
  without access to the original trained model. The core idea is to use the ULM as
  a noisy labeler in a teacher-student knowledge distillation architecture, translating
  the problem into learning with noisy labels.
---

# Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers to Leak Privacy

## Quick Facts
- **arXiv ID**: 2506.19486
- **Source URL**: https://arxiv.org/abs/2506.19486
- **Reference count**: 40
- **Key outcome**: MRA framework recovers forgotten class memberships from unlearned models using noisy labeler assumption, achieving up to 0.880 accuracy improvement on forgetting data.

## Executive Summary
This paper demonstrates that unlearned machine learning models (ULMs) retain sufficient discriminative information to enable membership recall attacks, despite being designed to "forget" specific data. The authors propose a Membership Recall Attack (MRA) framework that treats ULMs as noisy labelers in a teacher-student knowledge distillation architecture. By translating the problem into learning with noisy labels, MRA can recover forgotten class memberships without access to the original trained model. Extensive experiments across four datasets and various unlearning methods show significant accuracy improvements on forgotten instances, revealing critical privacy vulnerabilities in current machine unlearning approaches.

## Method Summary
The MRA framework operates by leveraging unlearned models as noisy teachers in a knowledge distillation setup. The attack proceeds in two phases: initial denoising knowledge distillation to warm up the student model, followed by iterative confident sample selection and model updates. In the closed-source case, only the student model is updated, while the open-source case allows updates to both teacher and student. The method uses mixup regularization and confidence-based sample selection to progressively reduce noise in pseudo-labels. The framework is designed to work across various unlearning methods and model architectures, with performance evaluated on CIFAR-10, CIFAR-100, Pet-37, and Flower-102 datasets.

## Key Results
- MRA achieves significant accuracy improvements on forgetting data, up to 0.880 recall accuracy on Pet-37 with L1-SP unlearning
- Precise unlearning methods (SalUn, UNSC) paradoxically enable better recall than over-unlearning methods due to preserved model utility
- Open-source co-training strategy outperforms closed-source approach, demonstrating the importance of iterative model updates
- The attack successfully generalizes across multiple datasets and model architectures including EfficientNet, ResNet, and ConvNeXt

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating an unlearned model as a "noisy labeler" allows recovery of forgotten class memberships via learning with noisy labels (LNL) techniques.
- **Mechanism:** After unlearning, the model retains residual discriminative information for forgotten classes but produces noisy or low-confidence predictions. By using this model as a teacher in a knowledge distillation setup and applying label-denoising strategies (e.g., mixup, confident sample selection), the student model learns to correct the noise and recover the original class information.
- **Core assumption:** Unlearning methods leave behind partial decision boundaries or feature representations that, while suppressed, still provide signal when aggregated across many samples.
- **Evidence anchors:** [abstract] "Specifically, we implement a Membership Recall Attack (MRA) framework with a teacher-student knowledge distillation architecture, where ULMs serve as noisy labelers..." [section 4.3] "Therefore, we use the ULM M(ΘU) to serve as a noisy labeling teacher for knowledge distillation... MRA can be further translated into a Learning with Noisy Labels (LNL) problem..."
- **Break condition:** If unlearning truly eliminates all class-relevant features (e.g., via complete retraining or provable removal), the noisy labeler assumption fails.

### Mechanism 2
- **Claim:** Iterative co-training between the unlearned model (teacher) and a student model amplifies recovery signal through confident sample selection.
- **Mechanism:** In the open-source case, both models are updated iteratively. High-confidence agreement samples (joint probability of teacher and student predictions) are selected to form a clean subset for training. This bootstrapping process refines both models, progressively reducing noise in the recovered labels.
- **Core assumption:** True class memberships have higher consistency between teacher and student predictions compared to random errors, even when individual predictions are noisy.
- **Evidence anchors:** [section 4.3] "Then, it is translated into a Learning with Noisy Labels (LNL) problem for inferring the correct labels of the forgetting instances... we selected samples with high confidence agreement..." [Table 4] Ablation shows DST+STU+TCH (full co-training) achieves significantly higher accuracy than DST alone.
- **Break condition:** If noise is systematic (e.g., teacher always mislabels a class the same way), confident agreement may reinforce errors.

### Mechanism 3
- **Claim:** Precise unlearning methods paradoxically increase recall vulnerability by preserving more model utility.
- **Mechanism:** Methods like SalUn and UNSC avoid over-unlearning and maintain high accuracy on retained data. This preserves the model's general discriminative capability, which can be repurposed for recalling forgotten classes. Over-unlearning methods (e.g., L1-SP) degrade this capability, making recall harder.
- **Core assumption:** The model's representational quality on retained data correlates with its ability to separate forgotten classes if probed correctly.
- **Evidence anchors:** [section 5.2] "As a result, SalUn and UNSC can provide less noisy pseudo labels for knowledge distillation, leading to better recall from ULMs." [Figures 3-4] Precise methods show higher recall accuracy improvements.
- **Break condition:** If precise unlearning also removes class-specific features (not just boundary), recall may still fail.

## Foundational Learning

- **Concept: Machine Unlearning (MU)**
  - Why needed here: The entire attack targets MU methods. You must understand how unlearning modifies models and what "forgotten" means (e.g., reduced accuracy on forget set).
  - Quick check question: Can you explain the difference between exact unlearning (retraining) and approximate unlearning (e.g., gradient ascent)?

- **Concept: Knowledge Distillation**
  - Why needed here: The MRA framework uses distillation to transfer knowledge from a noisy teacher (ULM) to a student.
  - Quick check question: How does a student model learn from a teacher's soft predictions (logits)?

- **Concept: Learning with Noisy Labels (LNL)**
  - Why needed here: The core problem framing—recovering true labels from noisy pseudo-labels—is an LNL problem.
  - Quick check question: What are common strategies for training models with corrupted labels (e.g., loss correction, sample selection)?

## Architecture Onboarding

- **Component map:** Dᴜ → ULM(Θᵤ) → Denoising KD → STM(Θₛ) → Confident Sample Selection → STM(Θₛ) + ULM(Θᵤ) → RCM(Θᵣ)

- **Critical path:** Obtain ULM → Denoising KD (warmup) → Iterate: Confident selection → update models → Repeat → RCM predicts on Dₚ

- **Design tradeoffs:**
  - **Closed-source vs. open-source:** Open-source allows ULM updates → higher recall but requires parameter access. Closed-source uses only STM.
  - **Dataset Dₚ composition:** Must include some forgotten samples (Xᵤ ⊆ Xᶠ) and potentially test data (Xₜₛ). Balance matters for noise estimation.
  - **Hyperparameters (τ, αₓ, αᵧ):** τ controls confidence threshold; too high → few samples, too low → more noise.

- **Failure signatures:**
  - **No improvement over ULM:** Likely insufficient warmup or too aggressive denoising early on.
  - **Degraded performance on retained classes:** Over-reliance on forget set samples during training.
  - **High variance across runs:** Confidence selection too strict; consider increasing τ or using ensemble.

- **First 3 experiments:**
  1. **Baseline sanity check:** Run MRA on a TRM (no unlearning). RCM should match TRM accuracy (attack should not degrade normal models).
  2. **Closed-source on CIFAR-10:** Use EfficientNet, SalUn unlearning. Measure accuracy on Dᶠ before (ULM) and after (RCM) attack. Target: >40% improvement as in paper.
  3. **Ablation on open-source:** Compare DST vs. DST+STU vs. DST+STU+TCH on Pet-37 with UNSC. Observe incremental gains as per Table 4.

## Open Questions the Paper Calls Out
None

## Limitations
- Attack assumes access to some forgotten samples in the prediction set, which may not hold in all real-world scenarios.
- Performance depends heavily on hyperparameter tuning (confidence threshold τ, mixup parameters), not extensively explored across datasets.
- Results may not generalize to deeper architectures (ViT, MLP-Mixer) or unlearning methods not tested in the paper.

## Confidence

- **High confidence**: The core mechanism of using unlearned models as noisy labelers for knowledge distillation is well-supported by experimental results and theoretical framing as an LNL problem.
- **Medium confidence**: The claim that precise unlearning methods paradoxically increase vulnerability is supported by empirical results but requires more theoretical justification.
- **Medium confidence**: The co-training amplification mechanism is demonstrated empirically but has limited theoretical grounding in the paper.

## Next Checks

1. **Cross-architecture validation**: Test MRA on ViT and MLP-Mixer architectures to assess robustness beyond CNN-based models, measuring recall accuracy on forgetting data.
2. **Provable unlearning comparison**: Compare MRA effectiveness against exact unlearning methods (complete retraining) versus approximate methods, quantifying the noise threshold below which attack fails.
3. **Sample efficiency analysis**: Systematically vary the proportion of forgotten samples in Dₚ (Xᵤ ⊆ Xᶠ) to determine minimum requirements for successful recall, reporting recall accuracy curves across different ratios.