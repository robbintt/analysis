---
ver: rpa2
title: 'ADDQ: Adaptive Distributional Double Q-Learning'
arxiv_id: '2506.19478'
source_url: https://arxiv.org/abs/2506.19478
tags:
- distributional
- addq
- q-learning
- double
- overestimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of overestimation bias in Q-learning,
  which slows down convergence in reinforcement learning. The authors propose ADDQ,
  an adaptive distributional double Q-learning method that leverages distributional
  reinforcement learning to locally mitigate overestimation.
---

# ADDQ: Adaptive Distributional Double Q-Learning

## Quick Facts
- arXiv ID: 2506.19478
- Source URL: https://arxiv.org/abs/2506.19478
- Reference count: 40
- Primary result: Addresses overestimation bias in Q-learning by adaptively choosing between Q-learning and double Q-learning updates based on local sample variance from distributional RL

## Executive Summary
ADDQ addresses the problem of overestimation bias in Q-learning by leveraging distributional reinforcement learning to estimate the sample variance of return distributions. The method dynamically chooses between Q-learning and double Q-learning updates by comparing sample variances of return distributions across actions, effectively mitigating overestimation locally. The approach requires only minor modifications to existing distributional RL code while providing improved stability and performance over standard Q-learning and double Q-learning in tabular, Atari, and MuJoCo environments.

## Method Summary
ADDQ uses two independent Q-networks and their target networks within a distributional RL framework (C51 or QRDQN). It computes the sample variance of return distributions for each action, normalizes it relative to other actions in the same state, and uses this to adaptively weight between Q-learning and double Q-learning targets. The target distribution is a convex combination of the two estimators based on a coefficient β derived from relative variance thresholds. The algorithm randomly selects which network to update per step and requires approximately 2× the memory of standard DRL.

## Key Results
- Outperforms standard Q-learning and double Q-learning in tabular MDPs, Atari, and MuJoCo environments
- Demonstrates improved stability over Q-learning while achieving higher scores than double Q-learning in different metrics
- Shows that adaptive interpolation between estimators mitigates the failure modes of using either algorithm in isolation

## Why This Works (Mechanism)

### Mechanism 1: Variance as a Proxy for Overestimation Risk
The sample variance of return distributions serves as a local indicator of estimation uncertainty and potential overestimation bias. High variance implies high uncertainty, which amplifies maximization bias. The distributional parameterization accurately captures return distribution spread, and this spread correlates linearly with overestimation risk.

### Mechanism 2: Relative Variance Weighting
Uniform overestimation is less harmful than skewed overestimation. Comparing variance across actions in the same state allows for local bias correction. The relative sample variance normalizes uncertainty of taking action $a$ against average uncertainty of other actions, identifying high-risk sources of skewed bias.

### Mechanism 3: Adaptive Interpolation between Estimators
Dynamically blending Q-learning and Double Q-learning targets mitigates failure modes of using either algorithm in isolation. The target value is a weighted mixture where high relative variance reduces Q-learning weight in favor of double Q-learning's underestimation bias.

## Foundational Learning

- **Distributional Reinforcement Learning (DRL)**: Unlike standard RL learning scalar Q-values, ADDQ requires return distribution variance. You must understand how C51 (categorical) or QRDQN (quantile) represent probability distributions over returns to extract variance. *Quick check*: Can you calculate the variance of a distribution represented by 51 weighted atoms?

- **Overestimation Bias in Q-Learning**: The $\max$ operator in Bellman updates systematically pushes value estimates upward because $\mathbb{E}[\max(X_1, X_2)] > \max(\mathbb{E}[X_1], \mathbb{E}[X_2])$. *Quick check*: Why does function approximation exacerbate overestimation bias compared to tabular settings?

- **Double Q-Learning (DQL) & Decoupling**: ADDQ uses DQL as a component. You must understand how maintaining two independent estimators and using one to select greedy action while the other evaluates it breaks the correlation causing overestimation. *Quick check*: How does DQL differ from a simple ensemble average?

## Architecture Onboarding

- **Component map**: Observation -> Variance Calculator -> β-Selector -> Target Mixer -> Loss -> Update

- **Critical path**: 
  1. Sample transition $(s, a, r, s')$
  2. Find best action $a^* = \text{argmax}_a Q(s', a)$
  3. Compute sample variance $S^2$ for current/next state action
  4. Calculate $\beta$ based on relative variance thresholds
  5. Construct mixed target $\hat{\eta} = \beta \cdot \eta_{\text{target}, A} + (1-\beta) \cdot \eta_{\text{target}, B}$
  6. Minimize distance between current distribution and mixed target

- **Design tradeoffs**: Requires 2× critic memory/resources; hyperparameter sensitivity to β thresholds; slightly more complex target mixing than scalar expectations

- **Failure signatures**: 
  - High variance noise causing β oscillation
  - Division by zero when $S^2_s \approx 0$
  - Underperformance on MuJoCo compared to clipped double Q (expected)

- **First 3 experiments**:
  1. Implement "Two-sided bandit MDP" to verify ADDQ identifies high-variance traps
  2. In Atari (Pong), compare neutral β schedule against fixed β=0 (pure DQL) and β=1 (pure QL)
  3. Visualize return distributions $\eta_A$ and $\eta_B$ during training to ensure stable variance calculation

## Open Questions the Paper Calls Out

- **Open Question 1**: Can static hyperparameter thresholds for β be replaced with adaptive, learnable mechanisms? The current manual intervals may not be optimal across diverse environments.

- **Open Question 2**: Can ADDQ performance in continuous control be improved by integrating variance-based adaptive weighting into advanced algorithms like REDQ or TQC?

- **Open Question 3**: Can theoretical analysis of overestimation bias be extended beyond bandit MDP using Gaussian processes?

## Limitations
- Theoretical analysis limited to tabular categorical setting; function approximation lacks formal guarantees
- Adaptive weight thresholds are heuristic choices without theoretical grounding for optimality
- Underperforms clipping-based methods (TD3/SAC) in continuous control despite improvements over standard Q-learning/DQL

## Confidence
- High: Variance correlates with overestimation risk in DRL (supported by Proposition 2.1 and ablation results)
- Medium: Adaptive interpolation effective over fixed Q-learning/DQL baselines (supported by experimental results)
- Low: Generalization to continuous control without modification (ADDQ underperforms Clipped DQL on MuJoCo)

## Next Checks
1. Test ADDQ in continuous control environments with varying reward noise scales to verify variance thresholds remain effective
2. Compare performance against ensemble-based methods like TQC that use explicit truncation
3. Evaluate sensitivity of β thresholds by systematically varying them across an environment suite