---
ver: rpa2
title: 'NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction'
arxiv_id: '2511.09971'
source_url: https://arxiv.org/abs/2511.09971
tags:
- 'false'
- gemini
- numerical
- claim
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces NumPert, a systematic framework for evaluating\
  \ large language models (LLMs) on numerical reasoning in veracity prediction tasks.\
  \ By applying controlled perturbations\u2014such as numeration, approximation, range,\
  \ masking, random replacement, and negative numbers\u2014to claims and evidence\
  \ pairs, the study tests model robustness across six perturbation types and three\
  \ prompting regimes (zero-shot, two-shot, and perturbation-aware prompts)."
---

# NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction

## Quick Facts
- arXiv ID: 2511.09971
- Source URL: https://arxiv.org/abs/2511.09971
- Authors: Peter Røysland Aarnes; Vinay Setty
- Reference count: 27
- Primary result: All tested LLMs suffer up to 62% accuracy drops on numerical perturbations, with masking and negative numbers being most challenging.

## Executive Summary
This paper introduces NumPert, a systematic framework for evaluating large language models (LLMs) on numerical reasoning in veracity prediction tasks. By applying controlled perturbations—such as numeration, approximation, range, masking, random replacement, and negative numbers—to claims and evidence pairs, the study tests model robustness across six perturbation types and three prompting regimes (zero-shot, two-shot, and perturbation-aware prompts). The results show that all state-of-the-art models suffer accuracy drops of up to 62% under certain perturbations, with masking and negative numbers being the most challenging. Larger models generally perform better, but no model is consistently robust. Notably, perturbation-aware prompts significantly recover performance, especially for reasoning-oriented models, by providing targeted demonstrations. The analysis also reveals that misclassification correlates with longer prompt and reasoning tokens, suggesting "overthinking" as a key failure mode. Overall, NumPert demonstrates that numerical robustness in long-context fact-checking remains an open challenge for current LLMs, and highlights the potential of calibration techniques to improve resilience.

## Method Summary
The NumPert framework evaluates LLMs on numerical reasoning in veracity prediction by applying six controlled perturbations to claim-evidence pairs from the QuanTemp dataset. Perturbations include numeration, approximation, range, masking, random replacement, and negative numbers. The study tests three prompting regimes: zero-shot, two-shot, and perturbation-aware prompts (PAP) that provide examples of label-flipping demonstrations. Models are evaluated on their ability to maintain accuracy when numerical values are altered, with special attention to reasoning-oriented models. The framework includes token-length analysis to identify "overthinking" patterns that correlate with misclassification.

## Key Results
- All tested LLMs show substantial accuracy drops (up to 62%) under numerical perturbations, particularly masking and negative numbers
- Larger models generally perform better but no model achieves consistent robustness across all perturbation types
- Perturbation-aware prompts (PAP) significantly improve performance, especially for reasoning-oriented models
- Misclassification correlates with longer reasoning tokens, suggesting "overthinking" as a key failure mode
- Standard two-shot prompting often degrades performance compared to zero-shot due to harmful anchoring effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing targeted demonstrations of perturbed claims (PAP) improves robustness to numerical attacks, particularly for reasoning-oriented models.
- **Mechanism:** In-context learning calibrates the model's attention to numerical discrepancies by exposing it to examples where small numerical edits flip the label, shifting from semantic similarity matching to strict verification logic.
- **Core assumption:** The model possesses sufficient reasoning capacity to generalize the specific "trap" demonstrated in the prompt to unseen claims.
- **Evidence anchors:** [Section 5.1.2] Results show "introducing a single label-flipping demonstration... (PAP) substantially boosts performance... The most striking gains appear in reasoning-oriented models." [Table 2] Qwen3-32BT accuracy on Masking improves from ~12% (Two-shot) to ~45% (PAP).

### Mechanism 2
- **Claim:** Excessive reasoning length ("overthinking") correlates with higher misclassification rates.
- **Mechanism:** When uncertain, models generate speculative reasoning chains that rationalize incorrect answers rather than halting on direct evidence mismatch, creating a feedback loop where the model "talks itself into" a wrong verdict.
- **Core assumption:** Valid reasoning is typically concise and direct, whereas confused reasoning is verbose and exploratory.
- **Evidence anchors:** [Section 6] "Misclassifications correlate with longer prompt and reasoning tokens... with inflated reasoning chains being a strong marker of misclassification." [Table 7] Qwen3-32BT misclassifications show a ~120% increase in reasoning tokens compared to correct predictions (876 vs 397).

### Mechanism 3
- **Claim:** Models rely on surface-level heuristics (e.g., semantic similarity) rather than numerical rigor, causing specific failure modes in Masking and Negative Numbers.
- **Mechanism:** Models treat masked numbers ("#######") as placeholders to be imputed from evidence rather than logical contradictions. Similarly, negative signs are often dismissed as typos rather than value inversions.
- **Core assumption:** The model's pre-training prioritizes textual fluency and context completion over strict logical consistency.
- **Evidence anchors:** [Section 6.1] Error analysis notes: "masked numbers were often treated as placeholders... leading the model to 'complete' the claim" and "models often interpreted the negative sign (–) as a typo." [Abstract] "Masking and negative numbers being the most challenging" (up to 62% accuracy drops).

## Foundational Learning

- **Concept: In-Context Learning (Few-Shot vs. Zero-Shot)**
  - **Why needed here:** The study reveals a counter-intuitive result: standard two-shot prompting often *degrades* performance compared to zero-shot due to "harmful anchoring effects." Understanding how to structure demonstrations (PAP) is critical.
  - **Quick check question:** Why does adding random examples hurt performance in numerical tasks, and how does Perturbation-Aware Prompting (PAP) solve this?

- **Concept: Model Robustness vs. Accuracy**
  - **Why needed here:** High baseline accuracy (e.g., 91% on unperturbed claims) masks total fragility. A model can be "accurate" but lack "robustness" (e.g., dropping to 8% on masked claims).
  - **Quick check question:** If a model has 90% accuracy on standard claims but 20% on perturbed claims, is it safe for deployment? Why or why not?

- **Concept: Chain-of-Thought (CoT) and "Overthinking"**
  - **Why needed here:** The paper challenges the assumption that more reasoning is always better. It distinguishes between necessary reasoning and inefficient "overthinking" that correlates with errors.
  - **Quick check question:** How can token length serve as a proxy for model confidence or confusion in reasoning models?

## Architecture Onboarding

- **Component map:** QuanTemp dataset -> Perturbation Engine -> Prompt Constructor -> Model Inference -> Evaluator
- **Critical path:** The definition of the **Label-Flipping Probes** (True → False) is the most sensitive component. Incorrectly generating a "False" claim that is actually true (or vice versa) invalidates the robustness measurement. Manual verification is required here.
- **Design tradeoffs:**
  - **Zero-shot vs. PAP:** Zero-shot is cheaper and often performs better than standard few-shot, but PAP provides the highest robustness recovery at the cost of higher token usage and prompt engineering complexity.
  - **Thinking Models:** "Thinking" variants (e.g., GPT-o3, Gemini 2.5FT) show better recovery with PAP but generate significantly more invalid outputs and are more prone to "overthinking."
- **Failure signatures:**
  - **The "Typo" Fallacy:** Model ignores negative signs (interpreting "-4%" as "4%").
  - **The "Imputation" Error:** Model predicts "True" for masked numbers by filling them in from evidence rather than flagging the mismatch.
  - **Verbose Decay:** Reasoning tokens exceed ~800 (for Qwen) or ~600 (for Gemini) without a conclusion, signaling a high probability of error.
- **First 3 experiments:**
  1. **Baseline Vulnerability Test:** Run Zero-shot on QuanTemp claims to establish the performance cliff between "Original" and "Masked" claims.
  2. **PAP Recovery Test:** Compare standard Two-shot vs. PAP prompts on a reasoning model (e.g., Qwen3-32B) to quantify the recovery margin for Neg-Num perturbations.
  3. **Token Analysis:** Plot reasoning token length vs. prediction accuracy to verify the "overthinking" correlation for your specific model configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does perturbing the numerical values in the *evidence* text, rather than just the claim, affect model robustness?
- **Basis in paper:** [explicit] The Conclusion states that "perturbing the evidence side of claim–evidence pairs" is a primary direction for future work.
- **Why unresolved:** The current study only applies controlled perturbations to the claims, treating the evidence as static ground truth.
- **Evidence:** Evaluation results using the NumPert framework where numerical noise or modifications are injected into the retrieved evidence documents.

### Open Question 2
- **Question:** Can models maintain robustness when perturbations flip the label from False to True (counterfactual correction)?
- **Basis in paper:** [explicit] Section 3.2 notes that the authors "do not perturb False to True" and leave exploring this direction for future work.
- **Why unresolved:** The current methodology is restricted to True-to-False or preserving False labels, omitting scenarios where a false claim is numerically adjusted to become true.
- **Evidence:** Performance metrics on a dataset containing claims that have been numerically corrected to align with the provided evidence.

### Open Question 3
- **Question:** Does explicitly constraining reasoning chain length mitigate the "overthinking" failure mode without sacrificing accuracy?
- **Basis in paper:** [inferred] The error analysis identifies "overthinking" (unnecessarily long reasoning tokens) as a key correlate of misclassification.
- **Why unresolved:** The paper identifies the correlation between verbosity and error but does not test interventions to limit this behavior.
- **Evidence:** Experiments comparing standard Chain-of-Thought prompting against length-constrained reasoning on the NumPert dataset.

## Limitations

- The study focuses exclusively on English numerical claims, limiting generalizability to other languages or multi-modal numerical reasoning
- The QuanTemp dataset, while carefully constructed, remains relatively small (260 True, 604 False claims), which may affect the statistical significance of certain perturbation effects
- The manual verification of perturbed claims introduces potential subjectivity in defining "label-flipping" transformations

## Confidence

- **High Confidence:** The core finding that all tested models show substantial accuracy drops (up to 62%) under specific perturbations like masking and negative numbers is well-supported by the systematic experimental design and multiple model comparisons.
- **Medium Confidence:** The mechanism explaining "overthinking" as a cause of misclassification is supported by token-length correlations but lacks deeper causal analysis of why models generate verbose reasoning chains in error-prone cases.
- **Medium Confidence:** The PAP recovery mechanism is demonstrated effectively for reasoning models, but the explanation for why standard two-shot prompting often degrades performance (rather than helping) could benefit from more detailed investigation of anchoring effects.

## Next Checks

1. **Cross-Lingual Validation:** Test NumPert perturbations on a multilingual fact-checking dataset to verify whether the identified failure modes (masking, negative numbers, etc.) persist across different language models and tokenization systems.

2. **Fine-Tuning Impact Study:** Evaluate whether fine-tuning LLMs on augmented datasets containing perturbed examples improves robustness, or whether the models simply memorize surface patterns without developing genuine numerical reasoning capabilities.

3. **Human-AI Agreement Analysis:** Conduct a controlled study comparing human fact-checkers' performance on the same perturbed claims to determine whether the observed model failures reflect genuine numerical reasoning difficulties or artifacts of the evaluation methodology.