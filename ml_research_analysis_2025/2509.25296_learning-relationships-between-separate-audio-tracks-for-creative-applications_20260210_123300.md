---
ver: rpa2
title: Learning Relationships Between Separate Audio Tracks for Creative Applications
arxiv_id: '2509.25296'
source_url: https://arxiv.org/abs/2509.25296
tags:
- musical
- audio
- module
- symbolic
- relationships
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel approach for learning symbolic relationships
  between separate audio tracks in musical agents. By employing Wav2Vec 2.0 for perception
  and Transformer-based models for decision-making, the system learns corpus-level
  musical relationships from multi-stem audio data.
---

# Learning Relationships Between Separate Audio Tracks for Creative Applications

## Quick Facts
- **arXiv ID:** 2509.25296
- **Source URL:** https://arxiv.org/abs/2509.25296
- **Reference count:** 8
- **Primary result:** Wav2Vec 2.0-based perception and Transformer-based decision module learn symbolic relationships between separate audio tracks, achieving 10-20% True Positive Percentage.

## Executive Summary
This paper introduces a novel approach for learning symbolic relationships between separate audio tracks in musical agents. The system employs Wav2Vec 2.0 for audio perception, Vector Quantization for symbolic abstraction, and Transformer-based models for decision-making. By training on multi-stem audio data, the model learns corpus-level musical relationships without requiring paired examples. The approach is evaluated through a re-generation task where the system predicts coherent symbolic sequences from paired tracks, demonstrating its ability to capture general musical relationships between stems.

## Method Summary
The system consists of three modules: Perception, Decision, and Action. The Perception module uses Wav2Vec 2.0 to encode audio into feature vectors, which are temporally condensed through average pooling and projected into a quantized space via K-Means clustering. The Decision module employs a Transformer decoder to predict symbolic sequences from the quantized input. During training, random pairs of stems from the same mix are used to learn probabilistic relationships. The Action module, implemented with the Dicy2 concatenative synthesis agent, generates audio from the predicted symbolic specifications. The system is evaluated on three datasets (FolkSongs, XStrings, MoisesDB) with varying vocabulary sizes (16, 64, 256 tokens).

## Key Results
- The model achieves True Positive Percentages ranging from 10% to 20% across three datasets, significantly outperforming random baselines
- Constrained generation improves performance, particularly for larger vocabularies (A256), by restricting predictions to physically present symbols
- Longer segmentation windows (500ms) generally yield better performance than shorter ones (250ms) for certain datasets
- Entropy and Longest Common Prefix metrics show the model learns to anticipate future musical events beyond immediate input

## Why This Works (Mechanism)

### Mechanism 1: Sub-sampled Symbolic Abstraction
The system learns high-level musical relationships by reducing complex audio streams into discrete symbolic sequences. Wav2Vec 2.0 projects raw audio into latent space, average pooling temporally condenses it, and Vector Quantization assigns single tokens to each segment. This forces the decision module to operate on "musical equivalences" rather than raw signal fluctuations, assuming the codebook effectively clusters audio segments into musically meaningful classes.

### Mechanism 2: Corpus-Level Conditional Autoregression
The decision module learns general "stylistic" rules for responding to input rather than memorizing specific track-to-track mappings. By using a Transformer decoder to predict B's tokens given A's tokens from random pairs of stems, the model learns a probability distribution that maximizes the likelihood of coherent pairings across the entire dataset. This assumes the dataset contains consistent, underlying patterns of interaction that generalize across different specific songs.

### Mechanism 3: Constrained Generation for Realizability
Restricting the model's output vocabulary to symbols physically present in the synthesis corpus significantly improves the coherence of generated audio response. During inference, the system masks the Transformer's output logits, forcing re-sampling from valid tokens when predictions don't exist in the current synthesis corpus. This aligns symbolic specification with the concatenative synthesis engine's capabilities, assuming correct musical responses can be adequately approximated by rearranging existing corpus segments.

## Foundational Learning

- **Wav2Vec 2.0 (Self-Supervised Audio Learning)**
  - **Why needed here:** The Perception module requires robust conversion of raw audio waveforms into meaningful feature vectors without training from scratch on limited musical data
  - **Quick check question:** Can you explain how contrastive learning in Wav2Vec 2.0 creates a latent space that groups similar audio features (like timbre or pitch) together?

- **Vector Quantization (VQ) & Codebooks**
  - **Why needed here:** The architecture relies on bridging continuous audio domain and discrete symbolic domain; VQ creates the "musical alphabet"
  - **Quick check question:** How does increasing the codebook size (K) affect the granularity of symbolic representation and difficulty of downstream prediction task?

- **Autoregressive Generation with Transformers**
  - **Why needed here:** The Decision module generates response track token-by-token; understanding causal mask and conditioning is vital for re-generation task
  - **Quick check question:** In a decoder-only Transformer, how is the guiding sequence (Track A) distinguished from the generated sequence (Track B) during inference?

## Architecture Onboarding

- **Component map:** wav2vec_mus (Encoder) -> Average Pooling (Condenser) -> K-Means (VQ) -> Transformer Decoder (Decision) -> Top-P Sampling -> Constrained Masking (optional) -> Dicy2 Agent (Action)

- **Critical path:** The VQ Mapping. K-Means clustering determines the "musical alphabet." Poor clusters (e.g., grouping silence with attack transients) prevent the Transformer from learning meaningful relationships. Verify cluster quality before training the decision module.

- **Design tradeoffs:**
  - **Vocabulary Size (K):** Low K (e.g., 16) is easier to model but loses detail; High K (e.g., 256) is expressive but harder to predict without "Constrained Generation"
  - **Segmentation Window:** Short (250ms) captures fine-grained interaction (good for free improv); Long (500ms) captures broader harmonic movement (better for pop/pulsed music)

- **Failure signatures:**
  - **Repetitive Loops:** Model overfits to common tokens in VQ output, generating infinite loops of same segment
  - **Silent Generation:** Transformer predicts token not found in Action module's corpus, resulting in silence or failed synthesis
  - **Low LCP:** Model fails to anticipate future structural changes, reacting only to immediate input

- **First 3 experiments:**
  1. **Perception Validation:** Train K-Means on validation set. Check if resulting tokens cluster coherently (visualize using PCA—do high-energy and low-energy segments separate?)
  2. **Baseline TPP:** Train Transformer on one dataset (e.g., MoisesDB) and report True Positive Percentage against random sampling baseline
  3. **Ablation on Constraints:** Compare generation quality (TPP and Entropy) with and without "Constrained Generation" on large vocabulary (A256)

## Open Questions the Paper Calls Out

None

## Limitations

- **Limited Vocabulary Expressiveness:** VQ-based tokenization may miss important musical dimensions like nuanced dynamics or articulation, particularly problematic for genres requiring fine-grained control
- **Dataset-Specific Generalizability:** Model's ability to learn "general musical relationships" may be style-specific rather than representing genuine musical understanding
- **Evaluation Metric Limitations:** True Positive Percentage metric may reward conservative predictions that don't necessarily produce musically interesting results

## Confidence

- **High Confidence:** The Decision module can learn relationships between tracks in a corpus-specific manner (TPP 10-20% across multiple datasets, outperforming random baselines)
- **Medium Confidence:** Constrained generation enhances performance particularly for larger vocabularies, but comes at cost of diversity
- **Low Confidence:** The system can "learn general musical relationships" from multi-stem data—extent to which learned relationships represent genuine musical understanding versus statistical pattern matching remains uncertain

## Next Checks

1. **Cross-Dataset Transferability Test:** Train the model on one dataset (e.g., MoisesDB) and evaluate its performance on entirely different musical genres or cultural traditions not present in training data to validate whether relationships are truly general or merely corpus-specific patterns.

2. **Human Perceptual Validation:** Conduct listening tests where musicians and non-musicians rate coherence and creativity of generated responses, comparing constrained versus unconstrained generation across different vocabulary sizes to address gap between statistical metrics and actual musical quality.

3. **Feature Ablation Analysis:** Systematically remove different audio features (timbre, rhythm, harmony) from input representation and measure impact on TPP and LCP to identify which musical dimensions the model actually relies on for relationship learning, validating effectiveness of VQ tokenization approach.