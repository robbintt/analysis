---
ver: rpa2
title: Trainable Quantum Neural Network for Multiclass Image Classification with the
  Power of Pre-trained Tree Tensor Networks
arxiv_id: '2504.14995'
source_url: https://arxiv.org/abs/2504.14995
tags:
- quantum
- tensor
- latexit
- training
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of embedding tensor network
  (TN) classifiers into quantum neural networks (QNNs) for multiclass image classification.
  Key obstacles include high-order quantum gates required for large bond dimensions
  and mid-circuit postselection with exponentially low success rates.
---

# Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks

## Quick Facts
- arXiv ID: 2504.14995
- Source URL: https://arxiv.org/abs/2504.14995
- Reference count: 40
- Primary result: Successfully trained FTN-classifiers on MNIST and CIFAR-10, then converted them into postselection-free quantum FTN-classifiers while maintaining or improving training accuracy.

## Executive Summary
This paper addresses the challenge of embedding tensor network (TN) classifiers into quantum neural networks (QNNs) for multiclass image classification. The authors propose forest tensor network (FTN) classifiers, which aggregate multiple small-bond-dimension tree tensor networks (TTNs) to handle multiclass classification without requiring large quantum gates. They extend the adiabatic encoding framework to remove the need for mid-circuit postselection. Numerical experiments on MNIST and CIFAR-10 demonstrate successful training of FTN classifiers and their conversion into postselection-free quantum FTN classifiers, achieving over 99% training accuracy on MNIST and over 96% training accuracy on CIFAR-10 while maintaining or improving performance.

## Method Summary
The method involves training classical FTN-classifiers consisting of d small-bond-dimension TTNs with cyclic shifts, then converting them to quantum FTN-classifiers through isometrization and unitary embedding. The adiabatic encoding framework gradually increases a weight parameter w from 0 to 1, enabling smooth transition from postselection-based to postselection-free quantum circuits. At each step, unitary gates are re-optimized using Riemannian SGD until a loss threshold is met, allowing the final quantum model to operate without postselection while maintaining classification performance.

## Key Results
- Successfully trained FTN-classifiers achieving >99% training accuracy on MNIST (16×16 grayscale, χ=2)
- Achieved >96% training accuracy on CIFAR-10 (8×8 RGB, χ=8) using adiabatic encoding
- Converted classical FTN classifiers to postselection-free quantum FTN classifiers while maintaining or improving training performance
- Demonstrated that adiabatic encoding maintains classification performance while eliminating postselection overhead

## Why This Works (Mechanism)

### Mechanism 1: Forest Tensor Network Aggregation
Aggregating multiple small-bond-dimension TTN classifiers achieves multiclass classification without requiring high-order quantum gates. The FTN-classifier uses d TTN classifiers (d = number of classes) each with small bond dimension χ = 2^k. Each TTN receives cyclically shifted input images, creating different pixel connectivity patterns and extracting diverse features. A final fully connected layer aggregates outputs for classification.

### Mechanism 2: Adiabatic Encoding for Postselection Removal
Gradually increasing a weight parameter w from 0 to 1 enables smooth transition from postselection-based to postselection-free quantum circuits while maintaining classification performance. Weighted quantum states assign classical weights W_m to mid-circuit measurement outcomes. At each step, unitary gates are re-optimized, allowing the loss to recover after each jump caused by w increment.

### Mechanism 3: Canonical Decomposition and Isometric Embedding
TTN classifiers can be exactly embedded into quantum circuits with postselection by first converting to canonical form with isometries, then embedding isometries into unitary operators via SVD-based decomposition. Each χ × χ² isometry is embedded into a 2χ² × 2χ² unitary by adding orthonormal rows. The non-isometric top tensor uses block-encoding with an ancilla qubit.

## Foundational Learning

- Concept: **Tensor Network Notation and Bond Dimensions**
  - Why needed here: Understanding how tensors connect via "legs" with dimension χ, how contractions reduce dimensionality, and why bond dimension controls both expressiveness and quantum gate size.
  - Quick check question: Given a 3-leg tensor with dimensions χ×χ×χ contracted with another on one leg, what is the resulting tensor's shape?

- Concept: **Isometries vs. Unitaries**
  - Why needed here: Understanding why postselection is needed to extract isometry behavior from a unitary.
  - Quick check question: If V is a 4×8 isometry, what must be added to construct an 8×8 unitary?

- Concept: **Postselection and Exponential Overhead**
  - Why needed here: Understanding why mid-circuit postselection is impractical (success probability decreases exponentially with circuit depth/system size) motivates the entire adiabatic encoding framework.
  - Quick check question: If 15 tensor embeddings each require postselection on 1 qubit with success probability 0.5, what is the total circuit success probability?

## Architecture Onboarding

- Component map:
  Input Image (L×L×k channels) → Cyclic Shift Layer (d different shifts) → d × TTN Feature Extractors (χ=2^k) → Non-linear Map ψ(y) = y²/Σy²_i → Fully Connected Layer (d·2^k → d classes) → Softmax → Classification

  Quantum embedding path: Classical TTN → Canonical form (SVD) → Unitary gates + Postselection → Adiabatic encoding (w: 0→1) → Postselection-free qFTN

- Critical path:
  1. Train FTN-classifier classically using Adam optimizer
  2. Convert each TTN to canonical form via bottom-to-top SVD
  3. Embed isometries into unitaries; block-encode top tensor with ancilla
  4. Set w=0, increment by ∆w at each step
  5. At each w: optimize unitaries using Riemannian SGD until loss < threshold
  6. Repeat until w=1 (fully postselection-free)

- Design tradeoffs:
  - Small χ (simpler quantum gates, better hardware compatibility) vs. model expressiveness (compensated by using d TTNs)
  - Larger ∆w (faster encoding) vs. loss recoverability (may cause unrecoverable jumps)
  - Number of TTNs fixed to d (paper choice) vs. computational budget (no ablation provided)

- Failure signatures:
  - Loss jumps after w increment and never recovers → ∆w too large or learning rate too small
  - Barren plateaus during direct qFTN training from scratch → initialization problem; must use pre-trained FTN
  - Large train-test gap (CIFAR-10: 96% train, 56% test) → overfitting; needs regularization/augmentation

- First 3 experiments:
  1. Replicate MNIST experiments: 16×16 downsampled images, χ=2, d=10. Verify training accuracy >99% and confirm adiabatic encoding maintains performance at w=1.
  2. Ablation study on TTN count: Test FTN with d/2 or d/4 TTNs to quantify expressiveness-compute tradeoff.
  3. Binary classification comparison: Run MNIST 0 vs 1 with both FTN (χ=2, d=2) and single TTN (χ=4) to determine when aggregation is unnecessary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the incorporation of regularization, data augmentation, or advanced optimization techniques close the generalization gap observed in the qFTN-classifier for complex datasets like CIFAR-10?
- Basis in paper: The authors observe a significant gap between training and test accuracy on CIFAR-10 and explicitly state that investigating such techniques to improve generalization is "an important direction for future research."
- Why unresolved: The current study prioritized demonstrating trainability and expressive power over generalization performance.
- What evidence would resolve it: Improved test loss and accuracy metrics on CIFAR-10 after applying specific regularization or data augmentation strategies without degrading training performance.

### Open Question 2
- Question: Is it possible to extend the qFTN-classifier beyond classical simulatability to achieve a quantum advantage by appending additional quantum gates?
- Basis in paper: The paper describes the "ultimate ambition" of using the qFTN merely as an initialization to be extended with further quantum gates, noting this remains an "open challenge for future exploration."
- Why unresolved: The current work only validates the embedding of the pre-trained classical model and does not explore the "further optimization" phase using non-simulatable quantum resources.
- What evidence would resolve it: Demonstrating that a qFTN-classifier, when expanded with additional parametrized layers, can solve classification tasks that classical tensor networks cannot.

### Open Question 3
- Question: How robust is the proposed adiabatic encoding framework when deployed on physical quantum hardware subject to noise and decoherence?
- Basis in paper: The authors identify verifying the performance of the approach on physical quantum hardware as a "crucial next step."
- Why unresolved: All results in the paper are derived from classical numerical simulations which assume ideal conditions, ignoring hardware noise.
- What evidence would resolve it: Successful execution of the training and inference pipeline on a Noisy Intermediate-Scale Quantum (NISQ) device with performance comparable to the simulated results.

## Limitations

- Scalability concerns: Results are demonstrated on downscaled MNIST (16×16) and CIFAR-10 (8×8), but scalability to full-resolution images or larger class counts is unproven.
- Generalization gap: Significant overfitting observed in CIFAR-10 experiments with 90% training accuracy but only 45% test accuracy for FTN, and 96% vs 56% for qFTN.
- Hyperparameter sensitivity: Adiabatic encoding relies on careful selection of increment size ∆w and loss threshold without systematic sensitivity analysis.

## Confidence

- **High confidence**: The core theoretical framework connecting tensor networks to quantum circuits through isometrization and block-encoding is mathematically sound.
- **Medium confidence**: The adiabatic encoding framework successfully removes postselection in the demonstrated experiments, maintaining or improving training accuracy.
- **Low confidence**: Claims about hardware efficiency and near-term applicability are not directly validated with realistic noise models.

## Next Checks

1. **Generalization validation**: Apply standard regularization techniques (dropout, data augmentation, early stopping) to the CIFAR-10 experiments and measure whether test accuracy improves beyond the current 56% while maintaining the postselection-free advantage.

2. **Scaling analysis**: Test the approach on larger image resolutions (e.g., 32×32 CIFAR-10) and measure how training/test accuracy, quantum circuit depth, and training time scale with image size and class count.

3. **Hardware compatibility verification**: Implement the qFTN circuits on actual quantum hardware or high-fidelity simulators with realistic noise models to verify that the postselection-free circuits maintain accuracy under decoherence and gate errors.