---
ver: rpa2
title: Can Multi-modal (reasoning) LLMs detect document manipulation?
arxiv_id: '2508.11021'
source_url: https://arxiv.org/abs/2508.11021
tags:
- detection
- document
- forgery
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of state-of-the-art multi-modal
  large language models (LLMs) in detecting forged documents using a dataset of real
  transactional receipts. We benchmark leading models including OpenAI O1/4o, Gemini
  Flash, Llama 4, Qwen 2/2.5 VL, Mistral Pixtral, and Claude 3.5/3.7 Sonnet against
  traditional SVM and CNN-based methods.
---

# Can Multi-modal (reasoning) LLMs detect document manipulation?

## Quick Facts
- arXiv ID: 2508.11021
- Source URL: https://arxiv.org/abs/2508.11021
- Reference count: 31
- Top LLMs achieve AUC 0.57-0.71 for zero-shot document forgery detection, outperforming traditional SVM/CNN baselines

## Executive Summary
This study evaluates whether state-of-the-art multi-modal large language models can detect forged documents using a dataset of real transactional receipts. Through systematic prompt optimization and analysis of model reasoning processes, the research benchmarks leading models including OpenAI O1/4o, Gemini Flash, Llama 4, Qwen 2/5 VL, Mistral Pixtral, and Claude 3.5/3.7 Sonnet against traditional SVM and CNN-based methods. Results show that top-performing LLMs achieve moderate AUC scores (0.57-0.71) for zero-shot generalization, outperforming traditional methods that struggled with overfitting or random guessing performance. Notably, model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical.

## Method Summary
The study evaluates multi-modal LLMs on binary classification of receipt images as authentic or forged using zero-shot inference with structured prompts. Traditional baselines include SVM (flattened pixel features with linear kernel and grid search) and CNN (OH-JPEG/PQL features from DCT compression artifacts). The RDDFD dataset contains forged and authentic transactional receipts. Evaluation metrics include AUC-ROC and F1 score, with confidence calibration analysis and error categorization.

## Key Results
- Top-performing LLMs achieve AUC 0.57-0.71 for zero-shot document forgery detection
- Traditional SVM and CNN baselines perform near random guessing or exhibit severe overfitting
- GPT-O1 demonstrates strongest performance (AUC 0.71) through structured reasoning capabilities
- Model size and reasoning capabilities show limited correlation with detection accuracy

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal LLMs can leverage cross-modal reasoning to detect document forgery through visual-textual inconsistency identification. Models analyze both layout/font characteristics (visual) and calculation/content coherence (textual/logical), integrating these streams to identify tampering patterns that single-modal approaches miss. Core assumption: Forgery indicators manifest across modalities and pre-trained multi-modal representations encode sufficient pattern recognition for zero-shot detection. Evidence: GPT-O1 demonstrated capabilities in mathematical verification, contextual logic, content consistency, and visual anomaly detection. Break condition: If forgery types leave no cross-modal traces, multi-modal advantage diminishes.

### Mechanism 2
Explicit reasoning augmentation (chain-of-thought) improves forgery detection by enabling systematic verification steps. "Thinking" models decompose analysis into sequential checks (layout→content→calculations→context), reducing hallucination through structured evaluation rather than immediate classification. Core assumption: Step-by-step verification aligns with how document authenticity is assessed. Evidence: Gemini-2.5-Flash improved from 0.55 to 0.59 with thinking; Gemini-2.5-Pro from 0.51 to 0.63. Break condition: If reasoning steps are not grounded in domain-specific forgery patterns, additional computation yields diminishing returns.

### Mechanism 3
Zero-shot detection relies on pre-trained visual-document understanding that transfers imperfectly to specialized forgery patterns. Models trained on general image-text corpora recognize some forgery patterns (font inconsistencies, layout anomalies) but lack exposure to domain-specific manipulation techniques, yielding moderate rather than high performance. Core assumption: General multi-modal pretraining captures partial forgery-relevant features; specialized training would improve detection. Evidence: Model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical. Break condition: Pre-training corpora lack sufficient document forgery examples; performance ceiling remains low without fine-tuning.

## Foundational Learning

- Concept: **AUC (Area Under ROC Curve)**
  - Why needed here: Paper reports AUC 0.47-0.71; need to interpret that 0.5 = random guessing, 0.71 = moderate discrimination
  - Quick check question: If a model achieves AUC 0.65, what's the probability it ranks a randomly chosen positive higher than a randomly chosen negative? (Answer: 65%)

- Concept: **Zero-shot vs. Fine-tuned Transfer**
  - Why needed here: Study evaluates zero-shot specifically; understanding why fine-tuning is hypothesized as critical explains performance ceiling
  - Quick check question: Why would a model with general document understanding fail on forgery detection without task-specific training? (Answer: Forgery patterns are niche; general pretraining doesn't encode forensic artifact recognition)

- Concept: **Multi-modal Fusion in Vision-Language Models**
  - Why needed here: Models combine visual (font, layout) and textual (calculations, consistency) signals; understanding fusion explains why some manipulations are detected and others missed
  - Quick check question: If a forged receipt has correct calculations but misaligned fonts, which processing stream should flag it? (Answer: Visual pathway; if fusion is weak, textual coherence may override visual anomaly)

## Architecture Onboarding

- Component map: Receipt images → Multi-modal LLM (GPT-O1, Gemini, Claude, etc.) + structured prompt → JSON output extraction → AUC calculation
- Critical path: Prompt engineering determines structured reasoning vs. shallow classification → Output parsing (string → probability extraction) introduces failure mode if model refuses or outputs non-numeric responses → Threshold selection for binary decision based on probability scores
- Design tradeoffs: Zero-shot vs. fine-tuning (zero-shot enables rapid deployment but caps AUC ~0.71); Reasoning tokens vs. cost (thinking models improve performance but increase latency/cost); Ensemble vs. single model (ensemble of top performers not explored)
- Failure signatures: Underconfidence (GPT-O1 assigns 0.02-0.15 scores to correct predictions); False negatives from mathematical tunnel-vision; False positives from strict interpretation; Refusal/non-numeric output
- First 3 experiments: Prompt ablation (test simplified vs. full structured prompt); Confidence calibration (apply temperature scaling to GPT-O1 outputs); Ensemble baseline (combine top-3 models via probability averaging)

## Open Questions the Paper Calls Out

1. **To what extent does task-specific supervised fine-tuning improve detection AUC scores compared to the zero-shot baseline of 0.71?**
   - Basis: The abstract and conclusion repeatedly state that "model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical."
   - Why unresolved: The study exclusively evaluates zero-shot generalization capabilities and does not experiment with training or adapting model weights.
   - What evidence would resolve it: A comparative experiment benchmarking the performance of models like Llama or Qwen before and after LoRA or full fine-tuning on the receipt dataset.

2. **What specific visual artifacts or logical inconsistencies trigger hallucinations or false negatives in reasoning models like GPT-O1?**
   - Basis: The conclusion calls for "further exploration into failure modes and model hallucination," noting that GPT-O1 produced 29 false negatives often clustered at very low confidence scores.
   - Why unresolved: The paper quantifies the error rates but does not provide a detailed feature-attribution analysis to explain *why* specific legitimate receipts were flagged or forged receipts were missed.
   - What evidence would resolve it: An ablation study modifying specific variables (font, alignment, calculation logic) in the input images to identify the precise triggers for model errors.

3. **Do the observed detection capabilities for receipts transfer effectively to other document types, such as identity cards or legal contracts?**
   - Basis: The methodology is restricted to the "FindIt Again" / RDDFD dataset of transactional receipts, while Table 1 lists numerous other forgery datasets that were excluded from evaluation.
   - Why unresolved: The paper does not test if the "mathematical verification" and "visual anomaly detection" skills are domain-general or specific to the structured layout of receipts.
   - What evidence would resolve it: Benchmarking the top-performing models (GPT-O1, Gemini) on diverse document formats outside the transactional domain.

## Limitations

- Dataset accessibility: The RDDFD dataset used for evaluation is not publicly available, making independent validation impossible without contacting original authors
- Zero-shot constraint: All experiments evaluate zero-shot performance only; the paper acknowledges but does not demonstrate how task-specific fine-tuning would improve results
- Confidence calibration issues: Several models show severe underconfidence, with correct predictions receiving near-zero scores, making threshold-based deployment unreliable

## Confidence

- Moderate confidence: Multi-modal LLMs outperform traditional ML baselines in document forgery detection (AUC 0.57-0.71 vs 0.5)
- Low confidence: Zero-shot detection suffices for production deployment (paper explicitly states fine-tuning is needed for reliable detection)
- Moderate confidence: Model size and reasoning capabilities do not correlate with detection accuracy (limited evidence from single model per family)
- High confidence: Structured reasoning prompts improve performance (documented improvements from Gemini thinking experiments)

## Next Checks

1. **Dataset replication**: Request access to RDDFD dataset and reproduce the exact train/test split to verify reported AUC scores (0.57-0.71 range)
2. **Confidence calibration**: Apply temperature scaling to GPT-O1 outputs to improve threshold reliability and test if corrected scores enable practical deployment
3. **Ensemble baseline**: Implement probability averaging across top-3 performers (GPT-O1, Gemini-2.5-Pro-Think, Llama-4) to measure if ensemble performance exceeds individual ceilings