---
ver: rpa2
title: Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech
  Detection
arxiv_id: '2506.16476'
source_url: https://arxiv.org/abs/2506.16476
tags:
- datasets
- samples
- hate
- dataset
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting implicit hate speech,
  which is often overlooked in existing harmful speech datasets. The authors propose
  a method to improve the generalizability of generic harmful speech datasets for
  implicit hate detection by leveraging influential sample identification, reannotation,
  and augmentation using Llama-3 70B and GPT-4o.
---

# Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection

## Quick Facts
- **arXiv ID**: 2506.16476
- **Source URL**: https://arxiv.org/abs/2506.16476
- **Reference count**: 13
- **Primary result**: +12.9-point F1 score improvement on specialized implicit hate datasets

## Executive Summary
This paper addresses the challenge of detecting implicit hate speech, which is often overlooked in existing harmful speech datasets. The authors propose a method to improve the generalizability of generic harmful speech datasets for implicit hate detection by leveraging influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. They hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. The method involves identifying influential samples responsible for misclassifications, reannotating them using GPT-4o, and augmenting the dataset with paraphrased implicit hate speech samples generated by Llama-3 70B. Experimental results demonstrate the effectiveness of the approach, achieving significant improvements in implicit hate detection while maintaining performance on explicit harmful speech.

## Method Summary
The approach identifies influential training samples responsible for misclassifying implicit hate speech using embedding similarity to a trusted benchmark (TSD). These samples are then corrected through either removal, GPT-4o reannotation, or augmentation with Llama-3 70B-generated paraphrases. The method addresses the core hypothesis that implicit hate already exists in generic datasets but was missed by annotators due to subjective interpretation. By focusing on samples that cause systematic errors, the approach improves model generalization to specialized implicit hate datasets while preserving explicit harm detection capabilities.

## Key Results
- Achieved +12.9-point F1 score improvement on specialized implicit hate datasets (IHC, OLID_IH, THOS_IH)
- For Davidson dataset: 12.1-point F1 improvement using GPT-4o reannotation; 13.4-point F1 improvement using Llama-3 70B augmentation
- For HateXplain dataset: 17.3-point F1 improvement using Llama-3 70B augmentation
- Maintained performance on explicit harmful speech detection while significantly enhancing implicit hate recognition

## Why This Works (Mechanism)

### Mechanism 1: Influential Sample Identification via Embedding Similarity
- **Claim:** Training samples most similar (in embedding space) to misclassified benchmark examples are responsible for systematic errors and can be corrected to improve generalization.
- **Mechanism:** A Trusted Samples Dataset (TSD) of 500 expert-curated samples serves as a diagnostic benchmark. When the trained model misclassifies a TSD sample, the training samples with highest cosine similarity to that misclassified example are flagged as "influential." These are hypothesized to be either mislabeled or creating confused decision boundaries.
- **Core assumption:** Implicit hate already exists in generic harmful speech datasets but annotators failed to recognize it due to subjective interpretation and task complexity.
- **Evidence anchors:**
  - [abstract]: "we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized and labeled by annotators"
  - [Page 1]: Lexicon analysis showed 14-71.7% of positive samples across four datasets contained no offensive language, suggesting latent implicit hate
  - [corpus]: Related work on annotator disagreement supports noise hypothesis, but no corpus evidence directly validates influential sample methods for implicit hate specifically
- **Break condition:** If TSD itself contains labeling errors or doesn't represent implicit hate patterns, influential sample identification amplifies noise rather than reducing it.

### Mechanism 2: GPT-4o Reannotation for Label Correction
- **Claim:** Large language models can correct mislabeled influential samples at scale where human reannotation is impractical.
- **Mechanism:** Influential samples are submitted to GPT-4o for re-labeling. When GPT-4o's annotation differs from the original, the ground truth label is updated. This addresses crowdsourced annotation variability.
- **Core assumption:** GPT-4o's harmful speech classification is sufficiently reliable to override human annotations, particularly for implicit hate detection.
- **Evidence anchors:**
  - [Page 4]: "the authors found that the GPT family (GPT3.5-turbo and GPT4) were the best closest performing LLMs to the human baseline"
  - [Page 8 - Limitations]: "LLMs have not yet reached the level of human experts in accurately identifying harmful content, particularly in its implicit form"
  - [corpus]: Weak/missing - no corpus papers validate GPT-4o superiority for implicit hate; referenced studies show limitations with sarcasm and individual-targeted hate
- **Break condition:** If GPT-4o systematically misclassifies sarcasm, opinions, or individual-targeted implicit hate, reannotation introduces systematic bias.

### Mechanism 3: Targeted Llama-3 70B Augmentation
- **Claim:** Paraphrasing explicit harmful speech into implicit variants—when limited to influential samples—improves model recognition of veiled hate patterns.
- **Mechanism:** Llama-3 70B rewrites explicit harmful speech as implicit hate. Critically, augmentation is constrained to influential samples only (not the full dataset), preventing convergence failures observed with high proportions of deeply implicit synthetic data.
- **Core assumption:** LLM-generated paraphrases authentically represent how humans naturally express implicit hate, and smaller models (BERT) can learn these patterns from synthetic examples.
- **Evidence anchors:**
  - [Page 4]: "Llama-3 70B demonstrated a substantial understanding of the task and consistently produced accurate paraphrases" while "Llama-3 8B struggled...often repeating the provided system and user prompts"
  - [Page 4]: "models often face difficulties when training on datasets with a high proportion of deeply implicit samples" — limiting to influential samples yielded improvements
  - [corpus]: No direct corpus validation for Llama-3 paraphrasing quality on implicit hate
- **Break condition:** If paraphrased samples don't reflect real-world implicit hate expression patterns, models learn synthetic artifacts that don't transfer to authentic content.

## Foundational Learning

- **Concept: Implicit vs. Explicit Hate Speech Distinction**
  - Why needed here: The entire methodology depends on recognizing that implicit hate lacks lexical markers (slurs, offensive terms) but conveys harmful intent through veiled means. The paper's lexicon analysis revealed 14-71.7% of harmful samples contained no offensive language.
  - Quick check question: Why would a model trained predominantly on explicit hate (containing slurs) fail on statements like "RACE should stay in their own neighborhoods"?

- **Concept: Influence Functions and Sample-Level Attribution**
  - Why needed here: Unlike loss-based influence methods (TracIn, gradient-based), this approach uses embedding-space similarity to identify training samples responsible for misclassifications. Understanding this distinction is critical for debugging the pipeline.
  - Quick check question: Given a misclassified test sample, how would you identify which training samples most contributed to that error using this paper's approach versus TracIn?

- **Concept: Cross-Dataset Evaluation Semantics**
  - Why needed here: The paper emphasizes Recall over F1 when testing generic-trained models on specialized datasets because generic models predict explicit hate as positive, but specialized datasets label explicit hate as negative (focusing only on implicit). This creates systematic false positives.
  - Quick check question: Why does testing a generic harmful speech model on an implicit-hate-specific dataset systematically depress F1 scores even if the model correctly identifies implicit hate?

## Architecture Onboarding

- **Component map:** TSD (Trusted Samples Dataset) -> BERT-base classifier -> Influential Sample Finder (cosine similarity) -> GPT-4o Reannotator or Llama-3 70B Augmenter -> Retrained BERT

- **Critical path:**
  1. Train BERT on generic dataset → evaluate on TSD
  2. Identify misclassified TSD samples (set E)
  3. Find top-x training samples most similar to each error via embedding cosine similarity
  4. Apply configuration: (a) drop, (b) GPT-4o reannotate, or (c) augment with Llama-3
  5. Retrain; repeat loops until optimal (observed: 3-16 loops across datasets)
  6. Evaluate on specialized implicit hate datasets (IHC, OLID_IH, THOS_IH)

- **Design tradeoffs:**
  - **Drop influential samples:** Simple, effective for Waseem/Davidson/Founta on implicit detection (Recall); risks losing signal
  - **GPT-4o reannotation:** Preserves data volume; introduces LLM bias risk; best F1 for Davidson
  - **Llama-3 70B augmentation:** Best F1 for HateXplain; convergence risk with deeply implicit samples
  - Manual loop selection currently required—no automated stopping criterion
  - Reannotating test data improves Waseem/Founta but degrades Davidson/HateXplain (noise correction vs. over-correction)

- **Failure signatures:**
  - BERT fails to converge when deeply implicit samples exceed threshold (observed with full-dataset augmentation)
  - Llama-3 8B repeats prompts—use 70B minimum for paraphrasing
  - High false positive rate when testing generic-trained models on specialized datasets (expected; use Recall for fair comparison)
  - GPT-4o may misclassify individual-targeted hate and sarcasm (acknowledged limitation)

- **First 3 experiments:**
  1. **Baseline establishment:** Train BERT-base on each generic dataset (Waseem, Davidson, Founta, HateXplain), evaluate on TSD and all three specialized datasets (IHC, OLID_IH, THOS_IH), record F1 and Recall across 5 random seeds with 500 balanced samples per test.
  2. **Drop influential samples (simplest intervention):** Implement TSD evaluation, identify top-10 influential samples per misclassified TSD example (top-20 for Founta due to size), remove from training, retrain for 5 loops, track Recall improvement on specialized datasets. Compare per-dataset loop counts against Table 1 targets.
  3. **Full pipeline on HateXplain:** Apply all three configurations sequentially (drop → reannotate → augment) to HateXplain (showed strongest specialized F1 improvement), measure trade-off between explicit harm maintenance (generic test) vs. implicit detection gains (specialized test). Identify which configuration best balances both objectives.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the selection of the optimal training version be automated to eliminate the reliance on manual observation in the influential sample identification pipeline?
  - **Basis in paper:** [explicit] The authors note in the Limitations section that selecting the best-performing version of the dataset is currently based on "manual observation" rather than a systematic approach.
  - **Why unresolved:** The current methodology requires human intervention to determine the optimal loop for stopping training and data removal, preventing a fully automated pipeline.
  - **What evidence would resolve it:** The development of a metric or heuristic that reliably signals the point of diminishing returns for influential sample removal without manual inspection.

- **Open Question 2:** Can LLM-based reannotation achieve parity with human experts in identifying implicit hate speech, particularly regarding sarcasm and content targeting individuals?
  - **Basis in paper:** [explicit] The Limitations section states that LLMs have "not yet reached the level of human experts" and may mislabel critical data, especially regarding implicit forms and sarcasm.
  - **Why unresolved:** The paper relies on GPT-4o for reannotation, yet acknowledges its potential for error compared to human judgment, creating a trade-off between scalability and label reliability.
  - **What evidence would resolve it:** A comparative analysis showing that LLM reannotation improves annotation consistency to a level statistically indistinguishable from expert human annotators on implicit hate samples.

- **Open Question 3:** Does the drop in performance on some datasets (like Davidson and HateXplain) during reannotation tests suggest that certain datasets require additional training loops to correct noise?
  - **Basis in paper:** [explicit] The authors state on page 7 that the results "suggest that HateXplain and Davidson require additional training loops... to identify and correct more influential samples."
  - **Why unresolved:** The number of loops applied in the study varied (3 to 16), but it remains unclear if performance plateaus or if specific datasets possess fundamental characteristics that limit the effectiveness of this specific reannotation method.
  - **What evidence would resolve it:** A convergence analysis running the pipeline for a significantly higher number of loops on these specific datasets to observe if the F1 score eventually exceeds the baseline.

## Limitations

- **LLM reliability concerns:** The paper acknowledges that LLMs have "not yet reached the level of human experts" in identifying implicit hate, particularly regarding sarcasm and individual-targeted content, yet relies on GPT-4o for reannotation.
- **Manual loop selection:** The optimal stopping point for training loops is currently determined through manual observation rather than an automated criterion, limiting reproducibility.
- **Paraphrase authenticity validation:** No corpus validation confirms that Llama-3 70B-generated implicit paraphrases authentically represent real-world implicit hate expression patterns versus synthetic artifacts.

## Confidence

- **High confidence**: The baseline observation that generic harmful speech datasets contain implicit hate (14-71.7% without offensive language) is well-supported by lexicon analysis and consistent with known annotator disagreement literature.
- **Medium confidence**: The mechanism of using influential samples for targeted reannotation/augmentation is theoretically sound but lacks direct corpus validation for implicit hate specifically.
- **Low confidence**: The reliability of GPT-4o and Llama-3 70B for implicit hate detection, particularly their ability to accurately identify and generate implicit hate expressions without introducing systematic bias.

## Next Checks

1. **Corpus validation of LLM performance**: Test GPT-4o's implicit hate detection accuracy on a held-out expert-labeled subset of each generic dataset, comparing against human annotator consensus to establish reliability baseline.
2. **Alternative influence method comparison**: Implement TracIn or gradient-based influence methods alongside the embedding similarity approach to determine if influential sample identification is robust across attribution methods.
3. **Paraphrase authenticity validation**: Conduct human evaluation of Llama-3 70B-generated implicit paraphrases against naturally occurring implicit hate examples to verify they authentically represent real-world expression patterns rather than synthetic artifacts.