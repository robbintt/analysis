---
ver: rpa2
title: 'Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for
  In-Context Planning'
arxiv_id: '2510.24690'
source_url: https://arxiv.org/abs/2510.24690
tags:
- tool
- wang
- zhang
- graph
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a graph-based framework that leverages tool
  dependencies and domain knowledge to enhance exemplar artifact generation for in-context
  planning. The approach constructs a tool knowledge graph from tool schemas using
  a DeepResearch-inspired analysis, then fuses it with a domain knowledge graph derived
  from internal documents and SOPs.
---

# Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning

## Quick Facts
- arXiv ID: 2510.24690
- Source URL: https://arxiv.org/abs/2510.24690
- Reference count: 28
- Key outcome: Graph-based framework fusing tool dependencies and domain knowledge achieves >80% precision/recall on dependency checking and up to 77% binary match accuracy on exemplar plan generation.

## Executive Summary
This work addresses the cold-start problem in In-Context Planning (ICP) by constructing exemplar plans that bridge tool dependencies and domain knowledge. The framework builds a tool knowledge graph from tool schemas using LLM analysis and fuses it with a domain knowledge graph derived from internal documents and SOPs. Through a dense-sparse integration framework combining structural tool dependencies with procedural knowledge, the approach generates more relevant exemplars for ICP tasks. Experiments demonstrate that the fused graph structure, particularly when enhanced with Personalized PageRank, significantly improves both dependency detection and exemplar plan generation quality.

## Method Summary
The framework constructs a tool knowledge graph by analyzing pairwise tool schemas (descriptions, arguments, outputs) using LLMs to identify dependencies, then validates these with LLM-as-judge filtering. Simultaneously, it derives a domain knowledge graph from internal documents and SOPs using GraphRAG. These graphs are fused in Neptune, creating a unified structure where tools link to relevant documentation. For a given query, the system retrieves top-K tool triplets and documents via embedding search, then runs Personalized PageRank from these seed nodes to produce a query-specific subgraph. This subgraph guides LLM exemplar plan generation, which is stored in a vector database for ICP use.

## Key Results
- Dependency checking achieves over 80% precision and recall using LLM analysis of tool schemas
- Exemplar plan generation reaches up to 77% binary match accuracy and 1.62 LLM-as-a-judge score
- Removing Personalized PageRank reduces binary match accuracy by 9 percentage points, demonstrating its importance for retrieval

## Why This Works (Mechanism)

### Mechanism 1
LLMs can accurately identify tool dependencies by analyzing pairwise schema information (descriptions, arguments, output payloads). The framework sends pairs of tool schemas to an LLM with a dependency-checking prompt. An LLM-as-judge validates identified dependencies, filtering spurious connections. This produces a directed tool knowledge graph where edges represent `_can_use_this_tool_output` relationships. Core assumption: Tool schemas contain sufficient signal for LLMs to infer meaningful input-output dependencies without execution traces.

### Mechanism 2
Fusing a tool dependency graph with a domain knowledge graph improves exemplar plan relevance. GraphRAG extracts entities and relationships from internal documents/SOPs, creating a domain KG. The tool graph (structural dependencies) is fused with this domain KG (procedural knowledge) in Neptune, creating a unified graph where tools link to relevant documentation. Core assumption: Domain documents contain procedural knowledge that aligns with and disambiguates tool usage patterns.

### Mechanism 3
Personalized PageRank (PPR) over a fused graph recovers relevant tools and documents that pure embedding retrieval misses. For a query, top-K tool triplets and documents are retrieved via embedding search. These serve as seed nodes for PPR, which propagates importance through graph edges, surfacing tools like `BacklogCheck` that embedding-only approaches overlook. The resulting subgraph guides LLM exemplar generation. Core assumption: Graph connectivity encodes complementary relevance signals to semantic similarity.

## Foundational Learning

- **Knowledge Graph Construction (GraphRAG)**
  - Why needed here: You must understand how unstructured documents become entities/relations before grasping fusion.
  - Quick check question: Given an SOP paragraph, can you sketch the entities and relations it would yield?

- **Personalized PageRank**
  - Why needed here: Core retrieval mechanism; distinguishes this from pure embedding-based RAG.
  - Quick check question: If seed nodes are {A, B} and A→C→D, which nodes receive highest PPR scores?

- **In-Context Planning (ICP) with Exemplars**
  - Why needed here: The entire framework optimizes exemplar quality for cold-start ICP.
  - Quick check question: Why would irrelevant exemplars degrade LLM plan quality more than no exemplars?

## Architecture Onboarding

- Component map: Tool Schema Parser -> LLM Dependency Checker -> GraphRAG Module -> Neptune Graph Fusion -> Embedding Retriever -> Personalized PageRank -> LLM Generator

- Critical path: Tool schema quality → dependency extraction accuracy → graph fusion completeness → PPR subgraph relevance → exemplar quality → downstream ICP performance

- Design tradeoffs:
  - Pairwise LLM checks are thorough but O(n²) in tool count; consider clustering first for large toolsets.
  - PPR adds retrieval latency but improves recall; tune damping factor and iteration count.
  - LLM-as-judge for dependency validation adds cost but reduces false positives.

- Failure signatures:
  - Low precision in dependency checking → dense but incorrect graph → PPR surfaces wrong tools
  - Missing SOP coverage for tools → orphan tool nodes → PPR cannot reach procedural context
  - Embedding model mismatch to domain terminology → poor seed selection → irrelevant subgraphs

- First 3 experiments:
  1. Dependency extraction validation: Run the LLM checker on 50 tool pairs with human-annotated ground truth; measure precision/recall before enabling LLM-as-judge filtering.
  2. PPR ablation on synthetic queries: Create 20 queries with known relevant tools; compare embedding-only vs. embedding+PPR retrieval recall.
  3. End-to-end exemplar quality test: Generate exemplars for 30 real production queries; use LLM-as-judge to score plan coverage against held-out ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
How would the framework perform on a purpose-built benchmark specifically designed for tool dependency detection, rather than on repurposed TOOLBENCH data? Basis: The conclusion states the absence of real benchmarks for detecting tool dependencies remains a limitation. Unresolved because current evaluation repurposes TOOLBENCH queries and ground-truth plans to derive 1,500 valid tool dependencies, but this dataset was not designed for dependency detection tasks.

### Open Question 2
How does framework performance scale when deployed across thousands of tools spanning multiple business domains? Basis: The introduction mentions business assistants operating over "hundreds of in-domain tools," but experiments only validate on a filtered subset of TOOLBENCH APIs. Unresolved because scalability to real-world enterprise complexity remains untested, particularly for cross-domain tool interactions.

### Open Question 3
What is the relative contribution of the dense (domain knowledge) versus sparse (tool graph) components to final plan generation quality? Basis: The paper notes performance may be largely determined by the quality of the subgraph returned by Personalized PageRank, but does not isolate each component's contribution. Unresolved because ablation only removes Personalized PageRank, not the dense vs. sparse elements themselves.

## Limitations
- Pairwise LLM dependency checking scales quadratically with tool count, creating computational bottlenecks
- Framework performance depends heavily on quality and coverage of SOP documents - gaps cannot be compensated by tool schema analysis
- Experiments conducted on filtered subset of ToolBench queries (1,500 dependencies) may not represent full real-world complexity

## Confidence
- High Confidence: LLM-based dependency extraction from schemas and PPR-based retrieval improvement are well-supported by ablation results
- Medium Confidence: Fusion of tool and domain knowledge graphs improving exemplar relevance is quantitatively supported but needs qualitative ICP performance validation
- Low Confidence: Framework scalability to production environments and robustness with ambiguous tool schemas remain untested

## Next Checks
1. Scalability Test: Measure dependency extraction precision/recall and runtime when scaling from 50 to 500 tools, implementing clustering optimization if needed
2. Robustness Validation: Evaluate framework performance when tool schemas contain ambiguous descriptions or missing output information, comparing against human-annotated dependencies
3. End-to-End ICP Impact: Conduct user study measuring how exemplar quality improvements translate to actual ICP task completion rates and user satisfaction in realistic planning scenarios