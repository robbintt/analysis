---
ver: rpa2
title: Generalized Fitted Q-Iteration with Clustered Data
arxiv_id: '2510.03912'
source_url: https://arxiv.org/abs/2510.03912
tags:
- learning
- data
- policy
- optimal
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a generalized fitted Q-iteration (GFQI) algorithm
  that incorporates generalized estimating equations to handle intra-cluster correlations
  in reinforcement learning. The method addresses the challenge of correlated data
  commonly found in healthcare applications.
---

# Generalized Fitted Q-Iteration with Clustered Data

## Quick Facts
- arXiv ID: 2510.03912
- Source URL: https://arxiv.org/abs/2510.03912
- Reference count: 40
- Key outcome: GFQI achieves 50-80% regret reduction over standard FQI in clustered healthcare data by incorporating GEE

## Executive Summary
This paper addresses the challenge of correlated data in offline reinforcement learning by developing a generalized fitted Q-iteration (GFQI) algorithm that incorporates generalized estimating equations. The method is specifically designed for clustered data where subjects within the same group (e.g., medical interns at the same institution) exhibit intra-cluster correlations. Theoretical analysis shows that GFQI achieves optimal performance when the correlation structure is correctly specified and maintains consistency under mis-specification. Empirical results demonstrate that GFQI significantly outperforms standard FQI and compares favorably against deep neural network-based approaches like CQL and DDQN in semi-synthetic studies using real-world mobile health data.

## Method Summary
GFQI extends standard fitted Q-iteration by incorporating generalized estimating equations to handle clustered data with intra-cluster correlations. The algorithm iteratively solves estimating equations based on temporal difference errors while accounting for correlation structure. At each iteration, it estimates a greedy policy, computes transformed features that remove future dependency, estimates the working correlation matrix (using exchangeable structure), and solves a weighted least squares problem. The method uses linear function approximation with polynomial features (degree 1-4 selected via cross-validation) and can be viewed as a generalization of variance-weighted TD learning. The key innovation is the use of inverse variance weighting combined with correlation modeling to reduce estimation variance in clustered settings.

## Key Results
- GFQI achieves approximately 50% reduction in regret under weak correlations and up to 80% reduction under strong correlations compared to standard FQI
- The method maintains consistency even when the working correlation structure is mis-specified, though efficiency gains require correct specification
- GFQI outperforms deep neural network-based approaches (CQL and DDQN) in semi-synthetic studies using real-world mobile health data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighting state-action features by inverse TD error variance reduces estimation variance.
- **Mechanism:** The algorithm constructs Φ*(A,S) = [ϕ*(A(1),S(1)), ..., ϕ*(A(M),S(M))]V^(-1), where each subject's feature is inversely weighted by σ²(A,S)—the conditional variance of the temporal difference error. This gives less influence to high-variance observations.
- **Core assumption:** The TD error variance σ² can be consistently estimated from data (Assumption: estimators converge fast enough).
- **Evidence anchors:**
  - [Section 5.1] "inversely weighting each state-action feature by the conditional variance of the TD error σ²"
  - [Theorem 1] "bβ reaches the minimal asymptotic MSE... if Φ = Φ*"
  - [corpus] Related work on variance-aware weighted regression (Min et al. 2021) uses similar inverse-variance weighting but without correlation adjustment
- **Break condition:** If TD error variance estimates are highly unstable (e.g., sparse state-action regions), weighting may amplify noise rather than reduce it.

### Mechanism 2
- **Claim:** Modeling intra-cluster correlation via a working correlation matrix C improves efficiency when clusters share unobserved factors.
- **Mechanism:** The covariance matrix V is decomposed as V = BCB, where B is diagonal (individual variances) and C is an exchangeable correlation structure. This captures that subjects in the same cluster (e.g., medical interns at the same institution) have correlated residuals due to shared environment.
- **Core assumption:** The working correlation structure approximates the true correlation; C need not be exact for consistency, only for efficiency gains.
- **Evidence anchors:**
  - [Section 2.1] "When the working correlation structure is correctly specified, the resulting estimator achieves a smaller asymptotic variance"
  - [Section 5.1] "even when this correlation structure is mis-specified, the proposed estimator maintains consistency"
  - [corpus] Weak direct evidence; related MARL work (arxiv:2502.11260) addresses multi-agent settings but not clustered data
- **Break condition:** If within-cluster correlations are highly heterogeneous or non-exchangeable (e.g., hierarchical within subgroups), the exchangeable assumption yields limited efficiency gains.

### Mechanism 3
- **Claim:** Using transformed features ϕ*(A,S) = ϕ(A,S) - γE[ϕ(π*(S'),S')|A,S] reduces variance compared to raw features ϕ.
- **Mechanism:** This transformation removes the "future-dependent" component from the current feature, decorrelating the estimating equation. It derives from the optimal estimating equation theory in GEE/TD learning.
- **Core assumption:** The conditional expectation E[ϕ(π*(S'),S')|A,S] can be estimated via supervised learning at each iteration.
- **Evidence anchors:**
  - [Section 4, Equation 4] Defines ϕ* explicitly
  - [Section 5.1] "using ϕ* instead of ϕ itself to construct the state-action feature" is identified as one of three key efficiency steps
  - [corpus] GTD (Ueno et al. 2011) uses similar optimal feature construction for i.i.d. settings
- **Break condition:** If supervised learning for the conditional expectation is poor (e.g., insufficient data per action-state region), feature transformation may introduce bias.

## Foundational Learning

- **Concept: Generalized Estimating Equations (GEE)**
  - Why needed here: GFQI's core innovation is adapting GEE to RL. Without understanding GEE's working correlation and robustness properties, the algorithm's theoretical guarantees are opaque.
  - Quick check question: Can you explain why GEE estimators remain consistent even when the working correlation is mis-specified?

- **Concept: Bellman Optimality Equation and Temporal Difference Error**
  - Why needed here: GFQI solves estimating equations based on TD errors. Understanding δ(A,S,R,S') = R + γ max_a Q(a,S') - Q(A,S) is essential.
  - Quick check question: What is the interpretation of setting E[δ] = 0 at the population level?

- **Concept: Bias-Variance Decomposition in Policy Regret**
  - Why needed here: Theorem 2 decomposes regret into a leading variance term and higher-order bias. Interpreting regret bounds requires this foundation.
  - Quick check question: Why does the leading term in regret scale as O(N^(-1)) while bias decays as O(N^(-3/2))?

## Architecture Onboarding

- **Component map:**
  - Data ingestion -> Cluster organization by M subjects with T timesteps
  - Feature engineering -> Compute ϕ(A,S) and estimate ϕ*(A,S) via supervised learning
  - TD error computation -> Calculate δ(m) for each subject at each timestep
  - Covariance estimation -> Estimate V = BCB using exchangeable correlation; estimate ρ from TD errors
  - Iterative Q-update -> Solve Φ* δ(β^(k), β^(k-1)) = 0 at each iteration

- **Critical path:**
  1. Initialize β^(0) = 0
  2. At iteration k: estimate π*(greedy policy), then ϕ*, then V, then solve for β^(k+1)
  3. Check convergence (e.g., ||β^(k+1) - β^(k)|| < tolerance)

- **Design tradeoffs:**
  - **Exchangeable vs. independence working correlation:** Exchangeable improves efficiency under clustering but adds computational cost for V^(-1); independence reduces to AGTD (faster but higher variance)
  - **Linear vs. neural Q-function:** Paper uses linear ϕ for interpretability and theoretical guarantees; neural (CQL/DDQN) offers flexibility but paper shows GFQI can outperform them when correlations are strong
  - **Iteration count K:** Must satisfy K >> log(N)/log(γ^(-1)) for convergence (Assumption 6); too few iterations → biased Q

- **Failure signatures:**
  - **Non-convergence:** β oscillates → likely unstable V^(-1) (check for near-singular covariance)
  - **Regret higher than FQI:** Possible causes: (a) wrong cluster assignments, (b) exchangeable assumption badly violated, (c) poor estimation of ϕ*
  - **Covariance estimation errors:** If cluster size M is small, ρ estimates are noisy → fall back to independence working correlation

- **First 3 experiments:**
  1. **Synthetic validation:** Generate clustered MDP data with known correlation structure (vary ψ ∈ {1,3,5,7,9}); confirm GFQI regret reduction matches theoretical predictions (~50% at ψ=1, ~80% at ψ=9)
  2. **Ablation on working correlation:** Compare GFQI (exchangeable) vs. AGTD (independence) vs. standard FQI; quantify efficiency gain from correlation modeling alone
  3. **Robustness to mis-specification:** Deliberately use incorrect cluster assignments or non-exchangeable true correlation; verify consistency (regret still converges to 0 as N increases, but efficiency degraded)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed GFQI framework be extended to non-linear function approximation, such as deep neural networks, while preserving the semi-parametric efficiency guarantees established for linear models?
- Basis in paper: [inferred] The theoretical optimality (Theorem 1) relies on linear function approximation (Assumptions 2 and 3) and linear basis functions $\phi$, whereas the empirical results compare the method against deep neural network baselines (CQL and DDQN).
- Why unresolved: The derivation of the optimal estimating equation and the resulting minimal asymptotic variance depend on the specific properties of linear model classes, leaving the efficiency of non-linear extensions unproven.
- What evidence would resolve it: A theoretical extension of Theorem 1 providing efficiency bounds for non-linear function classes, or empirical results showing GFQI combined with deep networks consistently outperforms deep baselines.

### Open Question 2
- Question: Can the regret bound be refined to depend on the number of subjects per cluster ($M$) by imposing specific structural assumptions on the intra-cluster correlations?
- Basis in paper: [explicit] The proof of Theorem 2 remarks that the regret bound depends on the number of clusters ($N$) but not the cluster size ($M$), specifically noting this is because "we do not impose any assumption on the within-cluster correlations."
- Why unresolved: Without structural assumptions on the correlation, the effective sample size is theoretically limited by $N$, ignoring potential statistical gains from increasing $M$ within clusters.
- What evidence would resolve it: A modified regret analysis that includes $M$ in the convergence rate under specific correlation constraints (e.g., sparse or decaying correlations).

### Open Question 3
- Question: How does the performance of GFQI vary when using alternative working correlation structures (e.g., autoregressive) compared to the standard exchangeable structure?
- Basis in paper: [inferred] While the methodology allows for any working correlation matrix $C$, the numerical studies exclusively implement and test the exchangeable structure.
- Why unresolved: It is unclear if the robustness to misspecification holds equally well for complex temporal correlations within clusters as it does for the exchangeable misspecifications implied by the parameter $\psi$ in the simulations.
- What evidence would resolve it: Simulation results evaluating GFQI performance on data generated with autoregressive (AR) or unstructured correlation patterns using different working correlation specifications.

## Limitations
- Theoretical optimality requires correct specification of working correlation structure, which may not hold in complex real-world clustering patterns
- Linear Q-function approximation limits scalability to high-dimensional state spaces
- Performance depends on accurate estimation of variance and correlation parameters, which can be challenging with limited data

## Confidence
- **High confidence**: Theoretical consistency results under mis-specified correlation, synthetic experiment outcomes showing regret reduction
- **Medium confidence**: Empirical results on semi-synthetic IHS data (real data access restricted), comparison with neural methods (CQL/DDQN)
- **Low confidence**: Performance in settings with non-exchangeable correlation structures, scalability to very large state-action spaces

## Next Checks
1. Test GFQI with intentionally mis-specified cluster assignments to quantify robustness degradation
2. Implement GFQI with alternative correlation structures (e.g., autoregressive) on synthetic data
3. Scale GFQI to high-dimensional states using kernelized features or neural approximators