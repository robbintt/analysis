---
ver: rpa2
title: 'Large Language Models for Cancer Communication: Evaluating Linguistic Quality,
  Safety, and Accessibility in Generative AI'
arxiv_id: '2505.10472'
source_url: https://arxiv.org/abs/2505.10472
tags:
- llms
- cancer
- medical
- health
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates eight large language models (LLMs) for generating
  breast and cervical cancer information, focusing on linguistic quality, safety,
  and accessibility. A mixed-methods framework combining quantitative metrics and
  expert qualitative ratings was applied across three dimensions: linguistic quality
  (accuracy, coherence, jargon, reasoning), safety & trustworthiness (toxicity, bias,
  harm, trust), and communication accessibility & affectiveness (readability, empathy,
  clarity, actionability).'
---

# Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI

## Quick Facts
- arXiv ID: 2505.10472
- Source URL: https://arxiv.org/abs/2505.10472
- Reference count: 23
- Primary result: General-purpose models (Llama3, Gemma) outperform specialized medical models in linguistic quality and affectiveness, while medical models achieve better readability but exhibit higher toxicity and bias.

## Executive Summary
This study evaluates eight large language models (LLMs) for generating breast and cervical cancer information, focusing on linguistic quality, safety, and accessibility. A mixed-methods framework combining quantitative metrics and expert qualitative ratings was applied across three dimensions: linguistic quality (accuracy, coherence, jargon, reasoning), safety & trustworthiness (toxicity, bias, harm, trust), and communication accessibility & affectiveness (readability, empathy, clarity, actionability). General-purpose models (Llama3, Gemma) outperformed specialized medical models in linguistic quality and affectiveness, while medical models (MedAlpaca, BioMistral) achieved better readability. However, medical models exhibited higher toxicity and bias, reducing safety. Llama3 consistently ranked highest in safety, coherence, and empathy, while Alpaca and MedAlpaca showed the lowest toxicity and gender bias. The results highlight the trade-off between domain knowledge and safety, underscoring the need for hybrid neurosymbolic approaches to improve clinical robustness in AI-driven health communication.

## Method Summary
The study evaluated 8 open-source LLMs (≤8B parameters) using a three-dimensional framework across 50 curated cancer-related questions. Quantitative metrics included BLEURT, BERTScore, ROUGE for linguistic quality; Perspective API toxicity, GenBit gender bias, and in-context impersonation for safety; and 6 readability indices plus PAIR reflection score for accessibility. Expert qualitative ratings on 15 criteria were collected for 400 responses. Welch's ANOVA with Games-Howell post-hoc and Hedges' g effect sizes were used for statistical analysis. The dataset was constructed from 4,643 QA pairs filtered from 5 public datasets using cancer-related keywords.

## Key Results
- Llama3 achieved highest scores across all linguistic criteria, particularly in reasoning (2.94) and accuracy (2.92)
- Medical models (MedAlpaca, BioMistral, Meditron) showed better readability scores but higher toxicity and bias levels
- Llama3 ranked highest in safety, coherence, and empathy metrics, while Alpaca and MedAlpaca showed lowest toxicity and gender bias
- A clear trade-off emerged between domain knowledge and safety, with medical fine-tuning potentially degrading safety guardrails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical domain fine-tuning can degrade safety guardrails established during general pre-training, creating a knowledge-safety trade-off.
- Mechanism: Specialized medical LLMs are fine-tuned on domain corpora that may contain terminology, framing patterns, or historical biases not aligned with patient-facing communication norms, shifting behavior toward domain accuracy while eroding RLHF-aligned safety behaviors.
- Core assumption: Safety behaviors learned during general instruction-tuning do not fully persist through domain-specific fine-tuning.
- Evidence anchors: Medical LLMs exhibited higher toxicity and bias; fine-tuning strategies prioritize domain knowledge at expense of safety.

### Mechanism 2
- Claim: General-purpose models with diverse training data produce more coherent, empathetic, and linguistically fluent responses than narrow medical models.
- Mechanism: Models like Llama3 and Gemma, trained on broad conversational and instructional data, develop stronger pragmatic language capabilities—including emotional attunement, contextual reasoning, and natural discourse structure.
- Core assumption: Linguistic quality and affectiveness depend on diverse conversational exposure, not just domain knowledge.
- Evidence anchors: Llama3 scored highest in reasoning, accuracy, clarity, empathy, and compassion; related work found domain fine-tuning can reduce emotional appropriateness.

### Mechanism 3
- Claim: Medical model outputs achieve better readability scores because they use simpler sentence structures, but this simplicity can correlate with reduced reasoning depth and increased factual errors.
- Mechanism: Medical LLMs generate shorter, less complex sentences (lower grade-level requirements), which improves readability indices, but this simplified output may reflect truncated reasoning chains or over-reliance on template-like responses.
- Core assumption: Readability metrics capture surface-level complexity but not semantic accuracy or logical completeness.
- Evidence anchors: MedAlpaca achieved higher Flesch Reading Ease but also higher hallucination scores; medical models scored lowest on understanding/reasoning.

## Foundational Learning

- Concept: **RLHF (Reinforcement Learning from Human Feedback) and its fragility under fine-tuning**
  - Why needed here: Understanding why medical fine-tuning degrades safety requires knowing that safety behaviors are often instilled via RLHF, which can be overwritten during subsequent domain adaptation.
  - Quick check question: Can you explain why continued fine-tuning on a new domain might cause a model to "forget" previously learned safety behaviors?

- Concept: **N-gram vs. embedding-based text similarity metrics**
  - Why needed here: The paper uses both ROUGE (n-gram overlap) and BERTScore (embedding similarity)—understanding the difference is essential for interpreting why models perform differently across metrics.
  - Quick check question: Why might a response with high semantic similarity but low n-gram overlap still be considered high quality?

- Concept: **Readability indices and their limitations for medical text**
  - Why needed here: The paper uses six readability formulas; understanding what they actually measure helps interpret the accessibility findings critically.
  - Quick check question: Why might a text score well on Flesch-Kincaid but still be incomprehensible to a patient with low health literacy?

## Architecture Onboarding

- Component map:
  Input layer (cancer questions) -> Model layer (8 LLMs) -> Evaluation layer (3 assessment tracks) -> Human validation layer (expert ratings)

- Critical path:
  1. Curate domain-specific question set (4,643 cases filtered to cancer-relevant)
  2. Generate responses from each model using standardized prompts
  3. Run automated metrics on all outputs
  4. Sample 50 questions × 8 models = 400 responses for expert review
  5. Apply Welch's ANOVA + Games-Howell post-hoc for statistical significance
  6. Compute Hedges' g for effect size and direction

- Design tradeoffs:
  - Model size constraint (≤8B) ensures reproducibility but excludes state-of-the-art proprietary models
  - Readability vs. depth: Medical models optimize for accessibility at potential cost of nuance
  - Automated vs. human evaluation: Quantitative metrics enable scale; expert ratings capture clinical validity

- Failure signatures:
  - High hallucination score with high BLEURT: fluent but factually ungrounded responses
  - Low readability score with high empathy rating: emotionally appropriate but inaccessible
  - High domain accuracy with high toxicity: medical correctness delivered in harmful framing

- First 3 experiments:
  1. **Safety-aware medical fine-tuning**: Fine-tune a base model on medical data with explicit constraints preserving toxicity/bias metrics
  2. **Readability-controlled generation**: Implement prompt engineering to reduce Llama3's grade level while measuring impact on empathy and accuracy
  3. **Hybrid retrieval-augmented approach**: Combine general-purpose model's affective generation with medical model's domain knowledge via retrieval-augmented generation

## Open Questions the Paper Calls Out

- Can hybrid neurosymbolic approaches effectively resolve the trade-off between the high linguistic safety of general-purpose models and the domain-specific accuracy of medical LLMs?
- To what extent do cultural context and patient-specific health literacy moderate the perceived empathy and usability of LLM-generated cancer communication?
- Does the fine-tuning process on specialized medical corpora inherently degrade safety alignment, leading to the increased toxicity and bias observed in medical LLMs?

## Limitations

- Prompt design and generation parameters were not disclosed, creating potential irreproducibility
- Expert ratings based on only 400 samples may not capture full response variability
- Automated safety metrics may not fully capture clinical harm or complex bias patterns
- Models limited to 8B parameters excludes potentially superior proprietary systems

## Confidence

- **High confidence**: General-purpose models outperform medical models on linguistic quality and affectiveness; medical models show higher toxicity/bias while achieving better readability
- **Medium confidence**: The knowledge-safety trade-off mechanism; effectiveness of the three-dimensional evaluation framework
- **Low confidence**: Specific effect sizes and statistical significance without access to raw data and exact computation methods

## Next Checks

1. Replicate the evaluation using identical prompts and generation parameters across all 8 models to verify the reported safety-quality trade-off pattern
2. Expand expert evaluation sample size to 100+ questions to assess statistical robustness of qualitative ratings
3. Test the proposed hybrid neurosymbolic approach by implementing a retrieval-augmented generation system combining general-purpose and medical models