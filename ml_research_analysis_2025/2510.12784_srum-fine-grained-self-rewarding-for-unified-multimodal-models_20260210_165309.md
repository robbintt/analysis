---
ver: rpa2
title: 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models'
arxiv_id: '2510.12784'
source_url: https://arxiv.org/abs/2510.12784
tags:
- arxiv
- generation
- srum
- understanding
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRUM, a self-rewarding post-training framework
  that bridges the gap between understanding and generation capabilities in Unified
  Multimodal Models (UMMs). SRUM leverages the model's own understanding module as
  an internal evaluator, providing corrective feedback to its generation module without
  requiring additional human-labeled data.
---

# SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models

## Quick Facts
- arXiv ID: 2510.12784
- Source URL: https://arxiv.org/abs/2510.12784
- Authors: Weiyang Jin; Yuwei Niu; Jiaqi Liao; Chengqi Duan; Aoxue Li; Shenghua Gao; Xihui Liu
- Reference count: 36
- Primary result: Achieves SOTA on T2I-CompBench (88.37) and T2I-ReasonBench (46.75) by leveraging a UMM's own understanding module to reward its generation module

## Executive Summary
SRUM introduces a self-rewarding post-training framework that bridges the understanding-generation gap in Unified Multimodal Models. The key innovation is using the model's own understanding module as an internal evaluator to provide corrective feedback to its generation module, eliminating the need for human-labeled data. By decomposing rewards into global (layout) and local (object-level) components, SRUM provides richer gradient signals that significantly improve compositional generation and reasoning capabilities. Extensive experiments demonstrate state-of-the-art performance across multiple benchmarks, validating the effectiveness of this self-rewarding approach.

## Method Summary
SRUM operates in two stages: first, it generates candidate images with bounding boxes using the model's CoT "think" mode; second, it applies reward-weighted training using a global-local dual reward system. The understanding module scores generated images with global composition scores (normalized to [0,1]) and per-region local scores ([-1,1]), which are converted to dense reward maps. The training objective combines velocity prediction loss weighted by these rewards with a reference constraint term to prevent reward hacking. The framework is implemented on Bagel and BLIP3o, using external SAM for bounding boxes in Bagel and native grounding in BLIP3o.

## Key Results
- Boosts T2I-CompBench performance from 82.18 to 88.37 (state-of-the-art)
- Improves T2I-ReasonBench from 43.82 to 46.75
- Generalizes well across in-domain and out-of-domain settings
- Ablation studies confirm importance of both global and local reward components

## Why This Works (Mechanism)

### Mechanism 1: Internal Cross-Module Critique
The understanding module serves as an internal evaluator, providing corrective feedback signals without external supervision. This creates a closed-loop where understanding capabilities are distilled back into generation, based on the observation that models can correctly judge alignment between prompts and images but struggle to generate faithful images.

### Mechanism 2: Multi-Scale Dense Reward Decomposition
Decomposing rewards into global (layout) and local (object-level) components provides richer gradient signals than single holistic scores. Global rewards ensure compositional coherence while local rewards provide region-specific feedback, with the product α·R modulating per-region velocity prediction loss.

### Mechanism 3: Reference Constraint as Regularizer
A reference loss term prevents reward hacking by penalizing deviation from the artifact-free latent. Combined with the reward term via λ_c=0.5, this maintains global coherence while allowing local refinement, ensuring the model doesn't optimize for the reward signal at the expense of output quality.

## Foundational Learning

- **Understanding-Generation Gap in UMMs**: The framework assumes understanding capability exceeds generation capability in current UMMs. Quick check: Can you explain why a model that correctly identifies "the red banana is on top of the yellow apple" might still generate an image where positions are reversed?

- **Flow-Based Velocity Prediction (Rectified Flow)**: The loss operates on velocity predictions v_θ rather than direct image predictions. Quick check: What does the term (ε - x_0^gt) represent in the loss equation, and why is it the target velocity?

- **Dense vs Sparse Reward Signals**: The paper contrasts dense reward maps with sparse scalar rewards. Quick check: Why would a sparse reward (correct/incorrect) provide less useful training signal than a dense map with per-region scores?

## Architecture Onboarding

- **Component map**: Generation Module -> Grounding Pipeline -> Understanding Module as Judge -> Reward Aggregation -> Training Objective
- **Critical path**: 1) Generate candidates with bounding boxes, 2) Score with understanding module (global + local), 3) Convert to reward map, 4) Apply reward-weighted velocity loss with reference constraint, 5) Iterate without refreshing training data
- **Design tradeoffs**: External grounding (SAM) vs native grounding based on benchmark performance; dense reward granularity balancing feedback quality vs grounding error risk; λ_c=0.5 optimizes constraint strength
- **Failure signatures**: Reward hacking (monitor L_ref term), grounding misalignment (visible as inconsistent regional quality), understanding module drift (monitor MME/MMBench scores)
- **First 3 experiments**: 1) Reproduce main results on Bagel (82→88 on T2I-CompBench), 2) Ablate global vs local rewards to confirm dual-reward synergy, 3) Test out-of-domain generalization (43→46 on T2I-ReasonBench)

## Open Questions the Paper Calls Out

- Can a fully closed-loop training system be constructed where the understanding module autonomously generates questions and answers to further refine the generation module?
- How can the local reward mechanism be adapted to prevent performance degradation in low-level visual attributes like texture and color?
- Is the reliance on external grounding models (e.g., SAM) a strict requirement for high-performance local rewarding, or can native grounding capabilities suffice?
- Does the "self-rewarding" mechanism saturate as the generation module approaches the performance level of the understanding module?

## Limitations

- Grounding reliability is highly dataset-dependent, with SAM potentially failing on abstract or complex scenes
- Understanding module calibration risks "Clever Hans" behavior where models use spurious correlations
- Velocity space assumptions lack explicit validation of the relationship between semantic fidelity and latent velocity magnitude

## Confidence

- **High Confidence**: Claims about SRUM improving benchmark scores and the existence of understanding-generation gaps
- **Medium Confidence**: Claims about dual-reward mechanism optimality and reward hacking prevention
- **Low Confidence**: Claims about precise latent-level reward interactions and general applicability to all UMM architectures

## Next Checks

1. **Grounding stress test**: Systematically evaluate SRUM with corrupted or missing bounding boxes to quantify sensitivity to grounding errors
2. **Understanding module drift monitoring**: Track understanding module performance throughout SRUM training to detect degradation from repeated self-evaluation
3. **Latent space validation**: Analyze latent velocity distributions before and after SRUM training to confirm updates remain within "safe" regions