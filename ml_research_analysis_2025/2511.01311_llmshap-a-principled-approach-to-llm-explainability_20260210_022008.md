---
ver: rpa2
title: 'llmSHAP: A Principled Approach to LLM Explainability'
arxiv_id: '2511.01311'
source_url: https://arxiv.org/abs/2511.01311
tags:
- shapley
- value
- attribution
- feature
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of applying Shapley value-based\
  \ feature attribution methods to explain large language model (LLM) decisions, where\
  \ the inherent stochasticity of LLM inference violates some classical Shapley axioms.\
  \ The core method idea is llmSHAP, which introduces several variants of Shapley\
  \ value computation for LLMs: the standard approach (\u03D5S), a cache-based deterministic\
  \ variant (\u03D5CS), a sliding-window approximation (\u03D5SW), and a leave-one-out\
  \ counterfactual method (\u03D5C)."
---

# llmSHAP: A Principled Approach to LLM Explainability

## Quick Facts
- arXiv ID: 2511.01311
- Source URL: https://arxiv.org/abs/2511.01311
- Reference count: 13
- Primary result: Introduces llmSHAP framework with four Shapley value variants for LLM explainability, analyzing their axiomatic compliance and computational trade-offs.

## Executive Summary
This paper addresses the challenge of applying Shapley value-based feature attribution methods to explain large language model (LLM) decisions, where the inherent stochasticity of LLM inference violates classical Shapley axioms. The authors propose llmSHAP, which introduces four variants of Shapley value computation: the standard approach (ϕS), a cache-based deterministic variant (ϕCS), a sliding-window approximation (ϕSW), and a leave-one-out counterfactual method (ϕC). Through theoretical analysis and empirical evaluation, the paper demonstrates how these variants satisfy (or violate) Shapley axioms and their computational complexity trade-offs.

## Method Summary
The llmSHAP framework wraps LLM inference calls to compute feature attributions using four variants. The standard Shapley value (ϕS) independently queries the LLM for each coalition, violating efficiency under stochasticity. The cache-based method (ϕCS) caches LLM outputs per coalition to enforce determinism and satisfy all Shapley axioms. The sliding-window approximation (ϕSW) restricts computation to local feature interactions for efficiency but violates symmetry and efficiency. The counterfactual method (ϕC) uses leave-one-out differences for fast computation while violating efficiency and symmetry. The payoff function uses cosine similarity between response embeddings, and the framework is evaluated on a disease-symptom dataset using GPT-4.1-mini.

## Key Results
- The cache-based method (ϕCS) satisfies all Shapley axioms but may distort perceived stochasticity of LLM inference.
- The standard method (ϕS) satisfies symmetry and null player axioms but violates efficiency under stochasticity.
- The sliding-window (ϕSW) and counterfactual (ϕC) methods violate efficiency and symmetry but are computationally faster.
- Empirically, ϕCS maintains stable similarity to the gold standard Shapley value across feature counts, while ϕC and ϕSW trade some accuracy for faster computation.

## Why This Works (Mechanism)

### Mechanism 1: Cache-Based Deterministic Value Restoration (ϕCS)
- Claim: Caching LLM outputs per coalition enforces a deterministic value function, thereby restoring full Shapley axiom compliance.
- Mechanism: The standard Shapley value assumes a deterministic value function $v(S)$. LLM inference $h(S)$ is stochastic, which breaks axiom guarantees. The $\phi_{CS}$ variant introduces an order-invariant cache. When a coalition $S$ is queried, the wrapper returns the cached result if it exists; otherwise, it calls the LLM, stores the result, and returns it. This ensures $h(S)$ is constant across all evaluations for a single run. By fixing the value function, the system satisfies the conditions for the classical Shapley proofs (Efficiency, Symmetry, Null Player) to hold.
- Core assumption: A single stochastic sample from the LLM for each coalition, once cached, is sufficiently representative for the attribution analysis.
- Evidence anchors:
  - [abstract] "...the cache-based method (ϕCS) satisfies all Shapley axioms..."
  - [section] Proposition 4.1 states $\phi_{CS}$ is exactly the Shapley value computed on the (now deterministic) characteristic function $h$, so original proofs apply. Table 1 confirms axiom compliance.
- Break condition: The cache is cleared mid-computation, or the cache key fails to handle order-invariance, causing repeated stochastic draws for the same semantic coalition.

### Mechanism 2: Efficiency Violation from Stochastic Redraws (ϕS)
- Claim: Independent stochastic draws for the same coalition during Shapley computation prevent marginal contributions from cancelling out, violating the Efficiency axiom.
- Mechanism: The Efficiency axiom ($\sum \phi_i(v) = v(X)$) relies on a telescoping sum where each intermediate coalition $S$ appears with positive and negative signs that cancel. Under stochastic inference ($\phi_S$), evaluating the term $h(S)$ in one context and $h(S)$ in another results in two independent random samples from the distribution $D(S)$. These values ($A_{with}$ and $A_{without}$) are not guaranteed to be equal, leaving residual terms in the sum and breaking the efficiency guarantee.
- Core assumption: The stochasticity is non-trivial (e.g., temperature > 0).
- Evidence anchors:
  - [abstract] "...the standard approach (ϕS)...violates efficiency under stochasticity."
  - [section] Proposition 4.2 proves that under independent redraws, the sum of attributions does not equal the total payoff. Figure 2 illustrates the divergent draws.
- Break condition: The LLM inference is made deterministic (temperature = 0), causing identical outputs for the same coalition and restoring efficiency.

### Mechanism 3: Computational-Axiomatic Trade-off in Sliding Window (ϕSW)
- Claim: Restricting Shapley computation to a local sliding window of features reduces complexity from exponential to linear/polynomial but deliberately violates Efficiency and Symmetry axioms.
- Mechanism: To avoid the $O(2^n)$ cost of evaluating all coalitions, $\phi_{SW}$ computes Shapley values only within a small window $W$ of size $w$, treating out-of-window features as constant context. This restricts coalition sampling to $O(2^w n)$. However, this transforms the underlying "game." **Symmetry** fails because features may appear symmetric locally but not globally (Proposition 4.5 shows features $a$ and $c$ receiving $1/2$ vs $1/4$). **Efficiency** fails because the sum of averaged local attributions does not equate to the global grand coalition payoff.
- Core assumption: Feature interactions are primarily local (e.g., adjacent tokens).
- Evidence anchors:
  - [abstract] "...sliding-window approximation (ϕSW)...violates efficiency and symmetry but are computationally faster."
  - [section] Proposition 4.5 provides a counterexample proving violations. Section 5 confirms computational complexity drops to $O(2^w n)$.
- Break condition: The window size $w$ is set equal to the total number of features $n$, reverting to the full, axiom-compliant (but expensive) computation.

## Foundational Learning

- Concept: **Shapley Value Axioms (Efficiency, Symmetry, Null Player)**
  - Why needed here: The paper's core contribution is mapping which implementation variants satisfy or violate these specific axioms. Understanding them is required to interpret the trade-offs (e.g., sacrificing Efficiency for speed).
  - Quick check question: "If a feature contributes zero to every possible coalition, what must its Shapley value be? If the sum of all feature attributions does not equal the model's total output, which axiom is violated?"

- Concept: **Stochastic LLM Inference**
  - Why needed here: The violation of classical Shapley guarantees is entirely driven by the non-deterministic nature of LLM decoding (temperature, top-p). One must grasp why $h(S)$ is not a constant function.
  - Quick check question: "If you prompt an LLM twice with the exact same input and temperature > 0, are you guaranteed the exact same output embedding? Why does this prevent the cancellation required for Efficiency?"

- Concept: **Feature Ablation & Coalitions**
  - Why needed here: Shapley values are calculated by ablating features (removing them from the prompt) to form coalitions. The paper assumes features (tokens/sentences) are pre-defined and can be subsetted.
  - Quick check question: "In the leave-one-out counterfactual method ($\phi_C$), how is the importance of a feature $x$ calculated relative to the full feature set $X$?"

## Architecture Onboarding

- Component map: Inference Wrapper -> LLM API -> Cache -> Attribution Engine -> Attribution Vector

- Critical path:
    1.  **Feature Parsing**: Convert input prompt into a set of discrete features (e.g., sentences).
    2.  **Coalition Generation**: Based on the selected variant, generate all required subsets (coalitions) of features.
    3.  **Cached Inference**: For each coalition, query the Inference Wrapper. If the coalition result is not in the cache, call the LLM, post-process the output (e.g., cosine similarity to a baseline), and cache the resulting scalar.
    4.  **Aggregation**: Apply the respective formula (weighted average for Shapley, difference for counterfactual) to the scalar payoffs to produce the final attributions.

- Design tradeoffs:
    -   **Axiom Compliance vs. Fidelity**: Use $\phi_{CS}$ for guaranteed axioms but a potentially "distorted" deterministic view of the model. Use $\phi_{S}$ for a truer stochastic view but with broken efficiency guarantees.
    -   **Computation vs. Completeness**: Use $\phi_{CS}$ for exact Shapley values ($O(2^n)$ cost, exponential). Use $\phi_{C}$ or $\phi_{SW}$ for fast, linear-time approximations that ignore feature interactions outside their scope.
    -   **Global vs. Local**: $\phi_{SW}$ prioritizes local feature interactions (faster, breaks Symmetry) vs. $\phi_{CS}$ which considers the global interaction space (slower, maintains Symmetry).

- Failure signatures:
    -   **Efficiency Gap**: For $\phi_{S}$, the sum of feature attributions significantly diverges from the total payoff (`h(X) - h(Empty)`).
    -   **Cache Storm**: A bug in the cache key (e.g., failing to normalize feature order) causes identical coalitions to be re-computed, destroying performance.
    -   **Instability**: Running $\phi_{S}$ multiple times on the same input yields high-variance attribution vectors.

- First 3 experiments:
    1.  **Axiom Compliance Test**: Run $\phi_{CS}$ and verify $\sum \phi_i(x) \approx h(X) - h(\emptyset)$ (Efficiency). Run $\phi_{S}$ and confirm the gap is non-zero.
    2.  **Scaling Benchmark**: Measure wall-clock time for $\phi_{CS}, \phi_{SW}, \phi_{C}$ across feature counts (4-10). Confirm exponential growth for $\phi_{CS}$ and linear for others (Figure 4).
    3.  **Similarity Analysis**: Compute cosine similarity between the attribution vectors of the fast methods ($\phi_{C}, \phi_{SW}$) and the "gold standard" ($\phi_{S}$). Confirm $\phi_{CS}$ maintains highest similarity (Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the trade-offs between computational efficiency and axiom satisfaction impact the human interpretability and trustworthiness of the explanations?
- Basis in paper: [explicit] The authors explicitly "encourage future work that investigates how the above trade-offs affect the crucially important human-computer interaction aspect of LLM explainability."
- Why unresolved: The paper provides a theoretical and empirical analysis of the technical trade-offs but does not conduct user studies to validate human perception.
- What evidence would resolve it: User studies measuring trust and understanding when participants are exposed to explanations from $\phi_{CS}$ (deterministic) versus $\phi_S$ (stochastic).

### Open Question 2
- Question: Can llmSHAP be effectively adapted to attribute importance to internal reasoning steps (Chain-of-Thought) in reasoning-oriented LLMs?
- Basis in paper: [explicit] The conclusion suggests that one could view internal reasoning steps as higher-level "features" to determine how much each contributes to the final outcome.
- Why unresolved: The current framework evaluates static input features; applying it to dynamic, sequentially generated reasoning traces introduces dependencies not addressed by the current coalition definitions.
- What evidence would resolve it: An extension of the llmSHAP framework applied to CoT traces, validating whether the axioms hold when "features" are intermediate reasoning tokens.

### Open Question 3
- Question: How does the specific choice of temperature and sampling strategy influence the stability of the standard Shapley value ($\phi_S$) and the magnitude of axiom violations?
- Basis in paper: [explicit] The methodology states that "investigating optimal temperature settings is out of scope," noting the analysis relies on a fixed temperature of 0.2.
- Why unresolved: While the paper proves efficiency is violated under stochasticity, the relationship between the degree of stochasticity (temperature) and the error magnitude remains unquantified.
- What evidence would resolve it: An ablation study measuring the variance of attributions and the efficiency gap in $\phi_S$ across a range of temperature settings (e.g., 0.0 to 1.0).

## Limitations
- The paper assumes a linear payoff function (cosine similarity), which may not capture all attribution scenarios.
- The leave-one-out counterfactual method ($\phi_C$) assumes features are independently removable, which may not hold for structured text.
- Scalability of the cache-based method ($\phi_{CS}$) to large feature sets remains an open question.

## Confidence
- High: Theoretical analysis of axiom violations is rigorous and well-supported.
- Medium: Empirical runtime benchmarks depend on external API performance.
- Medium: Similarity comparisons are sensitive to the choice of baseline attribution method.

## Next Checks
1. **Axiom Robustness Test:** Verify that efficiency violations persist across different temperature settings and LLM models.
2. **Scalability Benchmark:** Measure runtime and memory usage of $\phi_{CS}$ for feature counts beyond $n=10$ to identify practical limits.
3. **Payoff Function Generalization:** Test the attribution methods with alternative payoff functions (e.g., log-likelihood) to assess sensitivity to the choice of metric.