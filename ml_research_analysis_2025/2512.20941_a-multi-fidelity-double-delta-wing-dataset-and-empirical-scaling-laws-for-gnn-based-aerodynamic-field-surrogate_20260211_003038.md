---
ver: rpa2
title: A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based
  Aerodynamic Field Surrogate
arxiv_id: '2512.20941'
source_url: https://arxiv.org/abs/2512.20941
tags:
- training
- https
- dataset
- design
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the need for open-source, multi-fidelity\
  \ datasets and empirical scaling laws for graph neural network (GNN) based aerodynamic\
  \ field surrogate models. We present an open-source dataset for double-delta wings,\
  \ comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack\
  \ from 11\xB0 to 19\xB0 at Ma=0.3 using both Vortex Lattice Method (VLM) and RANS\
  \ solvers."
---

# A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate

## Quick Facts
- **arXiv ID:** 2512.20941
- **Source URL:** https://arxiv.org/abs/2512.20941
- **Reference count:** 40
- **Key outcome:** Open-source double-delta wing dataset with 2448 flow snapshots and empirical scaling laws showing test error decreases with data size following a power law (exponent -0.6122)

## Executive Summary
This study addresses the need for open-source, multi-fidelity datasets and empirical scaling laws for graph neural network (GNN) based aerodynamic field surrogate models. The authors present an open-source dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11° to 19° at Ma=0.3 using both Vortex Lattice Method (VLM) and RANS solvers. Using this dataset, they conduct an empirical scaling study of the MF-VortexNet surrogate model by constructing six training datasets with sizes ranging from 40 to 1280 snapshots. The results show that test error decreases with data size following a power-law distribution with exponent -0.6122, indicating efficient data utilization.

## Method Summary
The paper presents a multi-fidelity approach using MF-VortexNet, a GNN architecture that maps low-fidelity VLM pressure fields to high-fidelity RANS CFD pressure fields on a lifting-surface lattice. The dataset consists of 272 double-delta wing geometries sampled using nested Saltelli sequences in a 6-dimensional design space, evaluated at 5 different angles of attack. The GNN uses a U-Net-like encoder-decoder structure with multi-head graph attention blocks to capture residual errors from non-linear vortical structures. The scaling study fixes computational budget while varying dataset size to isolate data scaling effects, training models on 40 to 1280 snapshots and measuring test error on a held-out set.

## Key Results
- Test error decreases with training dataset size following a power law with exponent -0.6122
- The scaling law suggests optimal sampling density of approximately eight samples per dimension in a d-dimensional design space
- Larger models show more efficient data utilization but may be limited by fixed computational budgets
- The multi-fidelity approach achieves high accuracy by learning corrections from VLM rather than predicting high-fidelity fields from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The surrogate model (MF-VortexNet) achieves high accuracy with relatively sparse data by learning a correction mapping from a low-fidelity baseline (VLM) rather than predicting high-fidelity fields from scratch.
- **Mechanism:** The GNN encodes geometric descriptors and flow conditions alongside the low-fidelity pressure field. It uses a U-Net-like structure with graph-attention blocks to identify the residual error (non-linear vortical effects missed by VLM) and predicts a corrected "quasi-CFD" pressure distribution.
- **Core assumption:** The low-fidelity solver (VLM) provides a physically relevant structural baseline (potential flow) that the neural network only needs to perturb, rather than learning the entire fluid dynamics mapping blindly.
- **Evidence anchors:** [abstract] "MF-VortexNet surrogate model... maps low-fidelity pressure fields computed by VLM to high-fidelity pressure fields." [section II.D] "MF-VortexNet employs a U-Net-like encoder-decoder... to construct latent representations that capture pressure loading induced by nonlinear vortical structures."

### Mechanism 2
- **Claim:** Test error decreases with training dataset size following a power-law distribution, provided the computational budget is sufficient to train the model to convergence.
- **Mechanism:** Increasing the sampling density in the 6-dimensional design space reduces the "distance" between unseen test geometries and the training manifold, allowing the model to interpolate effectively.
- **Core assumption:** The model is trained in the "power-law region" and has not entered the "asymptotic error region" caused by a fixed computational budget (under-training).
- **Evidence anchors:** [abstract] "Test error decreases with data size with a power-law exponent of -0.6122." [section IV.B] "We observe an approximately linear correlation between β and N on a log-log scale... indicating faster convergence and more efficient utilization of training data."

### Mechanism 3
- **Claim:** Using a graph representation allows the surrogate to generalize across varying wing geometries without the mesh-mapping constraints of CNNs.
- **Mechanism:** The geometry and solution are represented as nodes and edges on a lattice. This unstructured representation allows the model to ingest varying planforms (e.g., trapezoidal vs. delta) directly, as the graph topology adapts to the geometry.
- **Core assumption:** The discretization (30x32 lattice) is sufficient to resolve the critical pressure gradients and vortex structures required for accurate aerodynamic coefficient calculation.
- **Evidence anchors:** [section I] "Point-cloud-based models naturally encode both geometric and solution information... eliminating the need for mesh mapping." [section IV.A.1] "We define D as the total number of graphs in the dataset... aligned with the downstream design application."

## Foundational Learning

- **Concept: Vortex Lattice Method (VLM)**
  - **Why needed here:** This is the "low-fidelity" input to the GNN. You must understand that VLM assumes potential flow and attached vorticity, meaning it fails to predict vortex lift and separation at high angles of attack (AOA).
  - **Quick check question:** Why does VLM accuracy degrade above 10° AOA for delta wings, and what physical feature does it miss?

- **Concept: Design of Experiments (DOE) - Saltelli Sampling**
  - **Why needed here:** The dataset uses a specific nested sampling strategy. Understanding this is critical for expanding the dataset or performing sensitivity analysis.
  - **Quick check question:** Why is a "nested" sequence preferred over random sampling when the cost of simulation (CFD) is high?

- **Concept: Neural Scaling Laws**
  - **Why needed here:** The paper's core contribution is quantifying performance vs. data size.
  - **Quick check question:** In the context of this paper, what causes the "asymptotic error region"—insufficient data or insufficient training compute?

## Architecture Onboarding

- **Component map:** Geometry Generator (SUAVE/OpenVSP) -> Low-Fidelity Solver (VLM) -> High-Fidelity Solver (SU2 RANS) -> Data Pre-processor -> Surrogate (MF-VortexNet) -> Optimizer
- **Critical path:** The CFD simulation is the bottleneck. Generating the Level 6 dataset required ~310,835 CPU-hours. Training the GNN is orders of magnitude faster but requires careful hyperparameter tuning (Optuna) to ensure convergence.
- **Design tradeoffs:**
  - Fixed vs. Scaled Compute: The paper fixes the training budget ($C^*$) to isolate data scaling effects. In production, you should scale compute with data to avoid the "asymptotic error" plateau seen in the paper.
  - Model Size vs. Data Efficiency: Larger models ($N$) seem to use data more efficiently (higher $\beta$ exponent) but are harder to optimize under tight budgets.
- **Failure signatures:**
  - CFD Divergence: Highly cambered wings at high AOA (>16°) failed to converge with standard JST schemes; required switching to AUSM scheme (Page 12).
  - Optimization Collapse: The "Large" model (2.4M params) showed worse performance than "Medium" under fixed budget, likely due to optimizer getting stuck in local minima (Page 18).
- **First 3 experiments:**
  1. Baseline Reproduction: Train the "Small" model (0.5M params) on the Level 3 dataset (32 geometries) to verify you can reproduce the reported MSE range (~0.05-0.06).
  2. Compute Budget Ablation: Retrain the "Large" model on Level 6 data with a 2x or 4x increased computational budget (training steps) to test the hypothesis that the scaling exponent ($\beta$) improves when not compute-constrained.
  3. Fidelity Ablation: Train a baseline GNN *without* the VLM input features (using only geometry) on a small subset (Level 2) to quantify the performance gain specifically attributed to the multi-fidelity mechanism.

## Open Questions the Paper Calls Out

- **Question:** How do the empirical scaling laws for GNN-based surrogates change when the computational budget is scaled jointly with dataset size and model size?
  - **Basis in paper:** [explicit] The conclusion states that the current scaling-law analysis is "incomplete without systematically varying the computational budget" and outlines a goal to "jointly scale the computational budget, model size, and dataset size."
  - **Why unresolved:** The current study fixed the maximum computational budget ($C^*$) to isolate the effects of data scaling, leaving the interaction between compute and model size unexplored.
  - **What evidence would resolve it:** A series of experiments where the training budget increases proportionally with model parameters and dataset size, showing whether the power-law exponent ($\beta$) remains constant or shifts.

- **Question:** Does the performance anomaly observed in the largest model (2.4M parameters) result from insufficient computational budget limiting optimizer convergence?
  - **Basis in paper:** [explicit] The authors note that the largest model deviated from the expected log-linear scaling trend (Figure 9) and explicitly state, "The exact cause of this deviation requires further investigation," speculating it is "optimization-limited."
  - **Why unresolved:** The fixed computational budget and stochastic subsampling scheme may have prevented the large model from fully utilizing the training data, mimicking under-training.
  - **What evidence would resolve it:** Re-training the largest model with a significantly increased number of weight-update steps and adaptive hyper-parameters to see if it aligns with the scaling trends of smaller models.

- **Question:** How does incorporating higher-fidelity simulation data, such as Detached Eddy Simulation (DES), for massively separated flows affect the multi-fidelity surrogate's accuracy and scaling behavior?
  - **Basis in paper:** [explicit] The discussion section notes that for roughly 2.3% of snapshots involving critical separation, RANS solvers failed to converge. The authors state, "Future extensions of the dataset may consider using higher-fidelity methods, such as detached eddy simulation."
  - **Why unresolved:** The current dataset relies solely on RANS (high-fidelity) and VLM (low-fidelity) data, potentially limiting the model's ability to learn physics in highly unsteady regimes.
  - **What evidence would resolve it:** Generating DES data for the failed geometric configurations and evaluating if the MF-VortexNet model improves its prediction of vortex breakdown and separation regions.

## Limitations

- The power-law exponent and optimal sampling density may not generalize beyond the studied conditions (subsonic, moderate AOA, 6-dimensional space)
- Fixed computational budget prevents full convergence for larger models, potentially underestimating their true performance
- Dataset is limited to double-delta wing geometries and may not transfer to other aerodynamic configurations

## Confidence

- **High confidence:** The multi-fidelity mechanism of using VLM as a baseline for the GNN is well-supported by the architecture description and validation results. The dataset construction methodology and the basic scaling trend (error decreases with data size) are clearly demonstrated.
- **Medium confidence:** The specific power-law exponent (-0.6122) and the optimal sampling density estimate (8 samples per dimension) are statistically derived but may not generalize beyond the studied conditions.
- **Low confidence:** Extrapolation of scaling laws to significantly different problem dimensions, flow regimes, or computational constraints is not supported by the presented evidence.

## Next Checks

1. **Compute budget validation:** Retrain the Large model (2.4M parameters) on the Level 6 dataset with 4x the computational budget (8000 update steps) to test whether the apparent underperformance compared to Medium model is due to under-training rather than model capacity limitations.

2. **Dimensionality scaling test:** Apply the same experimental protocol to a higher-dimensional aerodynamic design problem (e.g., 9-12 dimensions) to verify whether the 8-samples-per-dimension rule holds or requires adjustment.

3. **Fidelity pairing ablation:** Train equivalent GNN architectures using different low-fidelity baselines (e.g., linear panel method vs. VLM) on the same double-delta wing dataset to quantify the sensitivity of the scaling law to the quality of the baseline solver.