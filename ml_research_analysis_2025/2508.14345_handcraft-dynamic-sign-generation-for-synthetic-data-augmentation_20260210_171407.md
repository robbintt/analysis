---
ver: rpa2
title: 'HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation'
arxiv_id: '2508.14345'
source_url: https://arxiv.org/abs/2508.14345
tags:
- data
- sign
- synthetic
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CMLPe, a lightweight conditional motion
  generator for sign language data that creates diverse, semantically consistent synthetic
  samples. The approach combines CMLPe with a two-phase pretraining strategy: first
  training on synthetic data, then fine-tuning on real sign language data.'
---

# HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation

## Quick Facts
- **arXiv ID**: 2508.14345
- **Source URL**: https://arxiv.org/abs/2508.14345
- **Reference count**: 40
- **Primary result**: Lightweight conditional motion generator (CMLPe) with synthetic pretraining achieves state-of-the-art results on LSFB (55.0%) and DiSPLaY (97.3%) sign language datasets

## Executive Summary
This paper introduces CMLPe, a lightweight conditional motion generator for sign language data that creates diverse, semantically consistent synthetic samples. The approach combines CMLPe with a two-phase pretraining strategy: first training on synthetic data, then fine-tuning on real sign language data. The method uses Transformer-SL and Mamba-SL classifiers to evaluate performance across three sign language datasets (LSFB, INCLUDE, and DiSPLaY). Results show consistent improvements in recognition accuracy, with synthetic pretraining outperforming traditional augmentation methods in some cases. The approach establishes new state-of-the-art results for the LSFB (55.0%) and DiSPLaY (97.3%) datasets while demonstrating complementary benefits when combined with traditional data augmentation.

## Method Summary
The method uses a conditional MLP-based architecture (CMLPe) with Discrete Cosine Transform (DCT) to encode temporal information, processes sequences through conditional MLP blocks with Adaptive Layer Normalization (adaLN), and uses a dual-generator approach (forward + reversed) to synthesize complete sign sequences from class labels with noise injection for diversity. Synthetic pretraining (~10% of training iterations) establishes weight initialization that captures general sign motion patterns, enabling better generalization when fine-tuned on limited real data distributions. The approach leverages pose-based representations that retain sufficient discriminative spatial-temporal information while removing task-irrelevant visual noise, making synthetic generation computationally efficient compared to full image generation.

## Key Results
- CMLPe achieves low motion prediction errors: 14.1 MPJPE (LSFB), 8.0 MPJPE (INCLUDE), 7.3 MPJPE (DiSPLaY)
- Synthetic pretraining consistently improves accuracy: LSFB (+1.2-1.3%), INCLUDE (+0.7-3.1%), DiSPLaY (+2.7-9.0%)
- New state-of-the-art results: LSFB (55.0%) and DiSPLaY (97.3%)
- Synthetic data and traditional augmentation show complementary benefits, especially for Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1: Conditional MLP Architecture for Efficient Pose Generation
A lightweight MLP-based architecture with adaptive conditioning can generate semantically consistent sign language pose sequences at low computational cost. CMLPe applies Discrete Cosine Transform (DCT) to encode temporal information, processes sequences through conditional MLP blocks with Adaptive Layer Normalization (adaLN), and uses a dual-generator approach (forward + reversed) to synthesize complete sign sequences from class labels with noise injection for diversity. Core assumption: Pose-based representations retain sufficient discriminative spatial-temporal information while removing task-irrelevant visual noise.

### Mechanism 2: Synthetic-to-Real Transfer Learning
Pretraining SLR models on synthetically generated data before fine-tuning on real samples consistently improves recognition accuracy. Synthetic pretraining (~10% of training iterations) establishes weight initialization that captures general sign motion patterns, enabling better generalization when fine-tuned on limited real data distributions. Core assumption: Generated samples sufficiently approximate the real data distribution to transfer useful representations.

### Mechanism 3: Complementary Regularization Effects
Synthetic data generation and traditional augmentation provide additive benefits when combined, particularly for Transformer architectures. Synthetic samples introduce out-of-distribution diversity while traditional augmentation (rotation, scaling) provides in-distribution variations—they regularize through different pathways. Core assumption: The two approaches capture complementary sources of variation without redundancy.

## Foundational Learning

- **Concept: Discrete Cosine Transform (DCT) for Temporal Encoding**
  - Why needed here: Compacts temporal dynamics of pose sequences into frequency domain for efficient processing by both generator and classifier networks
  - Quick check question: Given a 32-frame pose sequence, what does the low-frequency DCT coefficient represent versus high-frequency coefficients?

- **Concept: Adaptive Layer Normalization (adaLN) for Conditional Generation**
  - Why needed here: Enables class-conditional generation by modulating intermediate features based on sign label embeddings with learned scale, shift, and gate parameters
  - Quick check question: How does adding noise to the class embedding before adaLN enable generating multiple variations of the same sign class?

- **Concept: State Space Models (SSM/Mamba) for Sequence Modeling**
  - Why needed here: Provides linear-time sequential processing alternative to quadratic-attention transformers, particularly effective for temporal pose sequences
  - Quick check question: Why must the classification token be placed at the sequence end for Mamba-SL but at the beginning for Transformer-SL?

## Architecture Onboarding

- **Component map**: CMLPe Generator: DCT → Linear(IN) → Transpose → K×CMB blocks (adaLN + FC) → Transpose → Linear(OUT) → IDCT → Residual add
  Dual-Generator System: Forward CMLPe (first half) + Reversed CMLPe (second half) → Concatenate for full sequences
  Transformer-SL: DCT+Linear embed → [CLS] + positional encoding → Encoder blocks → MLP classifier
  Mamba-SL: DCT+Linear embed → Token sequence + [CLS] at end → SSM blocks → MLP classifier

- **Critical path**: Data Prep: MediaPipe extraction → normalize → Savitzky-Golay filter → 32-frame windows → DCT
  Generator Training: Train paired CMLPe models on value→target prediction (M=16, T=16)
  Synthetic Generation: Class-balanced sampling, noise injection (scale=0.1), dual-generator output
  SLR Pretraining: 10% of training steps on synthetic data only
  SLR Fine-tuning: Remaining 90% on real data with optional rotation/scaling augmentation

- **Design tradeoffs**: Pose vs. video input: Lower dimensionality (60 landmarks vs. pixels) but loses appearance/texture information
  Window size (32 frames): Captures most signs but truncates longer sequences; zero-padding for shorter ones
  Synthetic pretraining ratio: 10% balances regularization benefit vs. compute; higher ratios not tested
  Transformer-SL vs. Mamba-SL: Transformer benefits more from combined augmentation; Mamba shows larger gains from synthetic alone (esp. DiSPLaY +9.0%)

- **Failure signatures**: High MPJPE gap between forward and reversed generators (LSFB: 14.1 vs 8.8) indicates padding artifacts with short sequences
  Performance degradation when adding traditional augmentation to synthetic-pretrained Mamba-SL suggests over-regularization
  Class imbalance: Without oversampling, models overfit to majority classes (LSFB has 1-1514 samples/class range)

- **First 3 experiments**: Baseline reproduction: Train CMLPe on LSFB with 6 blocks, embedding=32, noise=0.1; evaluate MPJPE on held-out samples
  Pretraining ablation: Compare 0%, 5%, 10%, 20% synthetic pretraining ratios on Mamba-SL for one dataset
  Architecture depth: Vary CMB blocks (4/6/8) and embedding dimension (16/32/64) to trade off quality vs. speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can leveraging data from multiple sign language or gesture datasets for unsupervised learning reduce the dependency on in-distribution data for synthetic generation?
- Basis in paper: Section 6 (Conclusions & Future Work) states the SLG model could be improved by leveraging data from multiple datasets for unsupervised learning to reduce the need for in-distribution data
- Why unresolved: The current CMLPe model is trained exclusively on the target dataset distribution for each specific language (LSFB, INCLUDE, DiSPLaY)
- What evidence would resolve it: Demonstrating that a model pre-trained on diverse, unlabeled sign language corpora requires fewer target-domain samples to achieve comparable generation fidelity

### Open Question 2
- Question: Would a sequential generation approach yield significantly higher quality samples than the current dual-generator CMLPe method?
- Basis in paper: Section 6 proposes a sequential approach for generation that exchanges computational cost for better quality samples, replacing the current dual CMLPe and reversed-CMLPe setup
- Why unresolved: The current method splits generation into two distinct segments (forward and reverse), which may introduce temporal discontinuities or artifacts at the splice point
- What evidence would resolve it: A comparative study showing that autoregressive or sequential generation results in lower motion prediction errors (MPJPE) or higher recognition accuracy in downstream tasks

### Open Question 3
- Question: Why does traditional data augmentation degrade performance in Mamba-SL when combined with synthetic pretraining, while it benefits Transformer-SL?
- Basis in paper: Table 9 shows Mamba-SL accuracy drops from 54.5% to 54.1% when augmentation is added to synthetic pretraining, whereas Transformer-SL improves from 54.1% to 55.0%
- Why unresolved: The paper speculates that synthetic data provides sufficient regularization for Mamba-SL but does not investigate the architectural sensitivities that cause this performance divergence
- What evidence would resolve it: Ablation studies analyzing the interaction between Mamba's state space model updates and standard geometric augmentations (rotation/scaling) during fine-tuning

## Limitations

- **Pretraining ratio optimization**: Only 10% synthetic pretraining ratio tested, leaving unclear whether this is optimal or how performance scales with different ratios
- **Architecture specification gaps**: "Simplified SSM blocks" in Mamba-SL and adaLN parameter mapping from label embeddings lack full specification
- **Distribution shift concerns**: Limited analysis of whether synthetic samples truly capture the diversity and edge cases of real sign language data

## Confidence

- **High confidence**: CMLPe architecture design, dual-generator approach, and synthetic pretraining methodology
- **Medium confidence**: Performance improvements on benchmark datasets
- **Medium confidence**: Complementary regularization claims

## Next Checks

1. **Pretraining ratio sensitivity analysis**: Systematically evaluate 5%, 10%, 20%, and 30% synthetic pretraining ratios on each dataset to identify optimal settings and generalization patterns

2. **Real vs. synthetic distribution comparison**: Use statistical metrics (KL divergence, FID-style metrics for pose sequences) to quantify how closely synthetic samples match real data distributions across different sign classes

3. **Long-sequence performance evaluation**: Test CMLPe with extended sequence lengths (64+ frames) to assess whether padding artifacts observed in LSFB persist and how they affect recognition accuracy