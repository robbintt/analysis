---
ver: rpa2
title: Advancing and Benchmarking Personalized Tool Invocation for LLMs
arxiv_id: '2505.04072'
source_url: https://arxiv.org/abs/2505.04072
tags:
- tool
- user
- invocation
- query
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces personalized tool invocation for LLMs, addressing
  the gap where existing methods ignore user-specific constraints in tool selection
  and parameter extraction. The authors define two key tasks: Tool Preference (selecting
  tools based on user preferences among functionally similar options) and Profile-dependent
  Query (inferring missing tool parameters from user profiles).'
---

# Advancing and Benchmarking Personalized Tool Invocation for LLMs

## Quick Facts
- arXiv ID: 2505.04072
- Source URL: https://arxiv.org/abs/2505.04072
- Reference count: 20
- Primary result: Fine-tuning on synthesized personalized tool data achieves 73.7% platform accuracy and 82.9% overall tool invocation accuracy without degrading general abilities

## Executive Summary
This paper addresses a critical gap in LLM tool invocation: existing methods ignore user-specific constraints when selecting tools and extracting parameters. The authors define two key tasks—Tool Preference (selecting tools based on user preferences among similar options) and Profile-dependent Query (inferring missing tool parameters from user profiles). They introduce PTool, an automated data synthesis framework that generates diverse tools, constructs realistic user profiles, and simulates user behaviors to create training data. A benchmark PTBench with 1,083 human-verified samples across five scenarios demonstrates that fine-tuning models on their synthesized dataset significantly improves personalized tool invocation capabilities while also enhancing general tool use without degrading other abilities.

## Method Summary
The PTool framework employs a multi-stage synthesis pipeline: first generating diverse tools through depth-first expansion, then constructing user profiles via bottom-up feature clustering, and finally simulating realistic user behaviors through role-playing. This creates query-solution pairs where tool selection must align with implicit user traits. The authors fine-tune Qwen2.5-7B-Instruct using LoRA with rank=8, alpha=16, learning rate=1e-4, cosine scheduler, warmup=0.1, and 1 epoch. Training data consists of 7,096 queries across five scenarios with 15 platforms, 360 APIs, and 74 user profiles. Evaluation uses the PTBench benchmark measuring platform accuracy (tool preference), parameter-value accuracy, and overall accuracy for complete invocation correctness.

## Key Results
- Best model achieves 73.7% platform accuracy and 82.9% overall tool invocation accuracy on PTBench
- Personalized fine-tuning enhances general tool invocation, improving BFCL non-live performance by 3.7% without degrading other abilities
- Ablation studies confirm the importance of user history for tool preference and basic features for profile-dependent queries
- The approach demonstrates superior performance compared to baseline models on personalized tool invocation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-stage data synthesis pipeline creates training data that teaches models to infer user preferences for tool selection.
- Mechanism: The PTool framework first generates diverse tools via depth-first expansion, then constructs user profiles through bottom-up feature clustering, and finally simulates realistic user behaviors through role-playing. This process creates query-solution pairs where tool selection must align with implicit user traits, providing a supervised learning signal for preference-aware tool invocation.
- Core assumption: LLM-generated synthetic data quality is sufficiently high to transfer personalized reasoning capabilities to smaller models via fine-tuning.
- Evidence anchors:
  - [abstract]: "They propose PTool, an automated data synthesis framework that generates diverse tools, constructs realistic user profiles, and simulates user behaviors to create training data."
  - [section 4.2]: "To simulate authentic behavioral traits, we employ an LLM-based role-playing approach, where the model generates user actions on various platforms based on their profile and platform characteristics."
  - [corpus]: Related work PEToolLLM (arXiv:2502.18980) also addresses personalized tool learning, suggesting the task formulation is a recognized problem, but this paper's specific multi-stage synthesis mechanism is its primary contribution.

### Mechanism 2
- Claim: Structured user profile representation enables the model to resolve ambiguous queries by conditioning tool invocation on historical user data.
- Mechanism: By explicitly separating "basic features" and "implicit preferences" in the prompt, and training on data where missing parameters are intentionally filled from these profiles, the model learns to attend to user context as a source of tool parameters. The ablation study confirms this; removing history hurts tool preference, and removing basic features hurts profile-dependent queries.
- Core assumption: Improvements stem from learning to map profile features to tool arguments, not solely from learning specific tool APIs.
- Evidence anchors:
  - [section 3, Definition 3.2]: "Profile-dependent Query... there exists value α ∈ A, α ∈ Pu and α /∈ q, then the query q is called profile-dependent query."
  - [section 5.3]: "The results exhibit that the model shows poor performance in the tool preference task when lacking user history information in training or evaluation."
  - [corpus]: Related work "Do LLMs Recognize Your Preferences?" (arXiv:2502.09597) highlights that LLMs generally struggle with preference inference, making this explicit conditioning mechanism a plausible driver of the observed performance gains.

### Mechanism 3
- Claim: Fine-tuning on synthesized, personalized tool-calling data improves general tool invocation without degrading other model abilities.
- Mechanism: Training on diverse APIs and user scenarios teaches general tool-use syntax and logic. Because synthesized data is broad, gains transfer to general benchmarks like BFCL. Evidence suggests personalized data adds reasoning layers without breaking core competencies.
- Core assumption: The synthesized API distribution is sufficiently representative of or transferable to tasks in general benchmarks like BFCL.
- Evidence anchors:
  - [abstract]: "...while also enhancing general tool invocation without degrading other abilities."
  - [section 5.5, General Capabilities]: "Nonetheless, our model gains a notable improvement on BFCL non-live... These findings suggest that our approach effectively enhances personalized functional calling capabilities without compromising the underlying LLM's other abilities."
  - [corpus]: Related work "Alignment for Efficient Tool Calling of Large Language Models" (arXiv:2503.06708) discusses trade-offs in tool use. The evidence here counters potential negative trade-offs (catastrophic forgetting), showing neutral or positive effects.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) on synthesized data**
  - Why needed here: The core method involves training a base model on the PTool-generated dataset to instill personalized tool invocation capability.
  - Quick check question: What is the primary risk when training on purely LLM-generated synthetic data?

- **Concept: User Profile Representation**
  - Why needed here: Understanding how user traits are structured and fed into the model is essential, as this structure is what the model conditions its outputs on.
  - Quick check question: How should you structure a user profile to maximize its utility for a model inferring missing tool parameters?

- **Concept: Tool Invocation / Function Calling**
  - Why needed here: This is the base task. The paper extends it with personalization, assuming a model can already perform basic tool invocation.
  - Quick check question: What is the critical difference between extracting a parameter from a query versus inferring it from a user's historical profile?

## Architecture Onboarding

- **Component map**: PTool Data Synthesis Framework (Tool Generator -> User Profile Constructor -> Query/Solution Generator) -> Fine-tuned LLM -> User Profile Store
- **Critical path**: The most critical path is the data synthesis pipeline (Section 4). If this pipeline generates flawed or low-diversity data, the fine-tuned model will fail to generalize. The second critical path is the inference prompt format, which must perfectly match the training data format, including the user profile block.
- **Design tradeoffs**: A key tradeoff is between data realism and diversity. Role-playing simulation creates realistic behaviors but may inherit teacher LLM biases. Another tradeoff is between complex, deep user profiles and the model's context window/attention capacity.
- **Failure signatures**:
    1. **Hallucination of tools/parameters**: The model invents tools or parameters not in the candidate list. The paper's rule-based checker is designed to catch this.
    2. **Ignoring user profile**: The model defaults to the most popular tool or fails to fill profile-dependent parameters.
    3. **Format errors**: The model outputs natural language instead of the required JSON-like tool invocation format.
- **First 3 experiments**:
  1. **Run inference on PTBench samples**: Load the fine-tuned model and run examples from the benchmark to observe how it handles both Tool Preference and Profile-dependent Query tasks. Manually inspect output format.
  2. **Ablate user profile components**: Following Section 5.3, create test sets with incomplete profiles (e.g., no history) and measure performance drops on platform accuracy to validate learned dependency on specific profile features.
  3. **Compare against a baseline**: Run the same test set through the base model and the fine-tuned model. Quantify improvement in both "platform accuracy" (preference) and "overall accuracy" (combined ability).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does personalized tool invocation performance generalize to domains beyond the five tested scenarios (shopping, food delivery, entertainment, work, travel)?
- Basis in paper: [explicit] The authors state in the Limitations section: "the current coverage of scenarios is limited, as we primarily focus on the five most commonly encountered scenarios in daily life. However, this does not encompass the full spectrum of everyday needs."
- Why unresolved: The benchmark only covers five scenarios, and it remains unclear whether the synthesized training data approach transfers to specialized domains (e.g., healthcare, legal, financial services) with different tool complexity and user preference patterns.
- What evidence would resolve it: Extension of PTBench to additional domains and evaluation of models trained on the current synthesized dataset on these new domains.

### Open Question 2
- Question: How does model performance compare when using real user profiles versus LLM-synthesized profiles?
- Basis in paper: [inferred] The entire framework relies on GPT-4-turbo to synthesize user profiles, behaviors, and queries. The paper does not validate whether simulated preferences match actual human decision-making patterns.
- Why unresolved: Role-playing simulation may produce unrealistic or biased user behaviors that do not reflect true human preferences, potentially inflating benchmark performance compared to real-world deployment.
- What evidence would resolve it: A study comparing model performance on PTBench against performance using profiles and queries collected from real users in a live system.

### Open Question 3
- Question: Can personalized tool invocation be achieved while preserving user privacy through profile obfuscation or federated approaches?
- Basis in paper: [inferred] The framework requires full access to detailed user profiles including behavioral history, basic features, and implicit preferences, with no discussion of privacy constraints.
- Why unresolved: Real-world deployment would face privacy regulations (GDPR, CCPA) and user concerns about sharing comprehensive profile data with LLMs for inference.
- What evidence would resolve it: Experiments demonstrating whether models can achieve comparable performance using anonymized, partial, or encrypted user profile representations.

### Open Question 4
- Question: How does the model handle multi-turn tool invocations where sequential decisions depend on previous personalized choices?
- Basis in paper: [inferred] The task formulation addresses single-query scenarios. The paper does not examine whether personalized preferences remain consistent or adapt across a session of related tool calls.
- Why unresolved: Real users often perform complex workflows requiring multiple interdependent tool invocations, where early personalized choices constrain later options.
- What evidence would resolve it: Extension of PTBench to include multi-turn conversation scenarios and evaluation of preference consistency across sequential invocations.

## Limitations
- Complete reliance on synthetic data for training with no evaluation on real-world personalized tool usage scenarios
- Human verification limited to the test set (1,083 samples) rather than the training data, raising questions about synthetic data quality
- Heavy focus on accuracy metrics without assessing computational overhead, latency, or cost implications
- User profile construction method may not capture full complexity and variability of real user preferences and behaviors

## Confidence

- **High Confidence**: The core finding that fine-tuning on synthesized data improves personalized tool invocation performance on the PTBench benchmark (73.7% platform accuracy, 82.9% overall accuracy). This is directly measured and reproducible.
- **Medium Confidence**: The claim that the approach "enhances general tool invocation without degrading other abilities" based on BFCL benchmark improvements. While measured, the effect size is modest (3.7% improvement) and the mechanism for this transfer is not fully explained.
- **Medium Confidence**: The effectiveness of the PTool synthesis framework in creating diverse and realistic training data. The paper provides process descriptions but limited qualitative analysis of the synthetic data quality.

## Next Checks

1. **Real-world deployment validation**: Deploy the fine-tuned model on actual user tool usage data from production systems and measure performance degradation or improvement compared to baseline approaches, focusing on both preference accuracy and parameter inference.

2. **Ablation of profile components**: Systematically remove different parts of the user profile (basic features vs. historical preferences) during inference to quantify their individual contributions to performance and identify which profile elements are most critical for the model's decision-making.

3. **Cross-domain generalization test**: Evaluate the model on tool invocation tasks outside the five scenarios in PTBench (e.g., healthcare, education, finance) to assess whether the learned personalization generalizes beyond the training domain distribution or overfits to the specific scenarios.