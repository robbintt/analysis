---
ver: rpa2
title: 'Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs'
arxiv_id: '2509.00084'
source_url: https://arxiv.org/abs/2509.00084
tags:
- candidate
- dataset
- self-refinement
- correct
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generative Self-Refinement (GSR), a parallel
  test-time scaling method where a single LLM generates multiple candidate solutions
  and then refines them into a superior final answer. The method addresses the limitation
  of existing approaches like majority voting and Best-of-N, which are bounded by
  candidate quality.
---

# Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs

## Quick Facts
- arXiv ID: 2509.00084
- Source URL: https://arxiv.org/abs/2509.00084
- Authors: Qibin Wang; Pu Zhao; Shaohan Huang; Fangkai Yang; Lu Wang; Furu Wei; Qingwei Lin; Saravan Rajmohan; Dongmei Zhang
- Reference count: 40
- Primary result: Introduces GSR-7B achieving 73.6% average accuracy across 5 math benchmarks, outperforming selection and fusion methods

## Executive Summary
This paper introduces Generative Self-Refinement (GSR), a parallel test-time scaling method where a single LLM generates multiple candidate solutions and then refines them into a superior final answer. The method addresses the limitation of existing approaches like majority voting and Best-of-N, which are bounded by candidate quality. GSR trains the model on dual objectives—direct solving and self-refinement—using a hybrid dataset constructed from a large math corpus. Experiments show GSR-7B achieves state-of-the-art performance on five mathematical benchmarks, with 73.6% average accuracy, outperforming both selective and fusion methods. Notably, GSR-7B maintains robustness when all candidates are incorrect (up to 9% accuracy) and generalizes to out-of-distribution tasks, demonstrating its model-agnostic enhancement capability.

## Method Summary
GSR is a parallel test-time scaling method that trains a single LLM to both generate candidate solutions and refine them into superior answers. The method uses hybrid training on dual objectives (direct solving and self-refinement) with a dataset of 368K samples constructed from OpenMathReasoning corpus. The student model (Qwen2.5-7B-Instruct) generates k=4-6 diverse candidates at high temperature, then self-refines them using an augmented prompt that includes the problem and candidates with explicit uncertainty framing. The teacher model (QwQ-32B) provides targets for both direct-solving and self-refinement tasks. The model is fine-tuned for 3 epochs using AdamW optimizer with cosine learning rate decay, minimizing a combined loss of direct-solving and self-refinement objectives.

## Key Results
- GSR-7B achieves 73.6% average accuracy across five mathematical benchmarks (AIME24, AIME25, AMC22-23, MATH500, OlympiadBench)
- With all candidates incorrect (Nc=0), GSR maintains 5.9% accuracy on AIME24 while baselines achieve 0%
- GSR generalizes to out-of-distribution tasks, improving 7.2% on K&K logic puzzles compared to base model's -7.6% degradation
- Cross-scale training shows refinement skill transfers: 14B (+20.1% relative) and 32B (+13.7% relative) models trained on 7B-generated candidates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-refinement is a learned skill, not an emergent capability from prompting alone.
- Mechanism: Hybrid training on dual objectives (direct-solving + self-refinement) creates complementary abilities. The model learns to evaluate candidate quality and synthesize superior solutions, rather than merely selecting or copying.
- Core assumption: Error patterns and valid reasoning fragments in incorrect candidates contain recoverable signal.
- Evidence anchors:
  - [abstract] "LLMs struggle to perform refinement effectively when prompted directly"
  - [section 4.4] Ablation shows Direct-Only training severely hampers self-refinement; Refinement-Only training improves pass@1 but underperforms hybrid
  - [corpus] SSR paper confirms coarse self-verification limits effectiveness; SETS shows self-correction requires structured approaches
- Break condition: If candidates contain no recoverable reasoning fragments (pure noise), refinement degrades to random generation.

### Mechanism 2
- Claim: Generative refinement transcends candidate quality bounds that limit selection methods.
- Mechanism: Instead of selecting from candidates (bounded by best candidate), the model constructs new solutions by analyzing connections between problem and candidates, identifying flaws, and reasoning independently.
- Core assumption: The model can diagnose errors in candidates without being misled by them.
- Evidence anchors:
  - [section 4.3] With Nc=0 (all candidates wrong), GSR achieves 5.9% accuracy on AIME24 vs. 0% for all baselines
  - [section 4.3] With Nc=1, GSR achieves 60.2% vs. 18.8% (maj@4), 38.3% (fusion), 56.3% (best BoN)
  - [corpus] Weak direct evidence; related work focuses on sequential refinement, not parallel synthesis
- Break condition: When context noise from many candidates exceeds model's attention capacity, performance saturates (observed at k>10 candidates in section 4.6).

### Mechanism 3
- Claim: Self-refinement skill generalizes across model scales and domains, indicating decoupling from specific error patterns.
- Mechanism: Training on candidates from 7B model transfers to 14B and 32B models, suggesting the learned skill is evaluating and synthesizing solutions rather than correcting model-specific errors.
- Core assumption: Reasoning evaluation is a transferable meta-skill independent of the candidate generator.
- Evidence anchors:
  - [section 4.5] Same dataset (7B candidates) trains effective refinement in 14B (+20.1% relative) and 32B (+13.7% relative) models
  - [section 4.7] GSR-7B trained on math shows 7.2% improvement on K&K logic puzzles vs. base model's -7.6% degradation
  - [corpus] No direct corpus support; related papers don't address cross-scale transfer of refinement skills
- Break condition: If training distribution is too narrow (single domain, single generator), skill may not generalize—current evidence limited to math→logic and 7B→larger scales.

## Foundational Learning

- **Test-Time Scaling (TTS)**:
  - Why needed here: GSR is a parallel TTS method; understanding TTS paradigms (parallel vs. sequential vs. hybrid) contextualizes the approach.
  - Quick check question: Can you explain why majority voting fails when all candidates are incorrect?

- **Supervised Fine-Tuning with Distillation**:
  - Why needed here: The hybrid training pipeline uses teacher-student distillation; understanding loss composition (L_direct + L_self-R) is essential.
  - Quick check question: Why does the student model generate candidates while the teacher provides targets?

- **Chain-of-Thought Reasoning**:
  - Why needed here: GSR parses CoT outputs to extract summaries; understanding thinking vs. summary components is critical for context management.
  - Quick check question: Why does the paper discard thinking tokens and only use summary tokens for refinement?

## Architecture Onboarding

- **Component map**:
  - Candidate Generator (student model, temp=1.0, k=4-6) -> Augmented Prompt Constructor -> Refiner (same model) -> Final Answer

- **Critical path**:
  1. Generate k candidates with high temperature for diversity
  2. Construct augmented prompt with randomized candidate order and explicit uncertainty instruction
  3. Model analyzes connections, diagnoses errors, synthesizes solution
  4. Training: minimize L_direct + L_self-R on hybrid dataset (50/50 split recommended)

- **Design tradeoffs**:
  - More candidates increase input noise vs. provide more recoverable fragments (saturation at k>10)
  - Refinement-only training improves pass@1 but underperforms hybrid on self-refinement
  - Longer summaries improve context but increase compute; paper uses ~1000 tokens max

- **Failure signatures**:
  - Prompting without training: selfRef@4 comparable to majority voting (no improvement)
  - Direct-only training: self-refinement severely hampered
  - Too many candidates (k>10): attention disruption, performance decline
  - High-accuracy regimes (pass@1 >90%): selection methods may match or exceed refinement

- **First 3 experiments**:
  1. **Baseline validation**: Compare selfRef@4 on base model vs. hybrid-trained model to confirm training necessity (replicate Table 4 ablation)
  2. **Nc=0 stress test**: Measure accuracy when all candidates are incorrect across difficulty levels to validate error recovery claim
  3. **Cross-domain transfer**: Test math-trained model on logic/code tasks to assess generalization boundaries

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Data Dependency: Method relies heavily on synthetic training data from QwQ-32B, creating potential bias propagation
- Context Management Trade-offs: Discarding thinking tokens may lose valuable intermediate reasoning steps
- Model Scale Generalization: Cross-scale transfer claims based on limited evidence (7B→14B→32B, math→logic only)

## Confidence

**High Confidence**: Self-refinement requires explicit training rather than prompt engineering (strong ablation support, robust mathematical formulation)

**Medium Confidence**: GSR outperforms selection methods and shows some cross-domain generalization (compelling evidence but limited domain testing)

**Low Confidence**: Refinement is a transferable meta-skill independent of candidate generator (limited evidence, narrow transfer scope)

## Next Checks

1. **Candidate Quality Sensitivity Analysis**: Systematically vary candidate quality distribution and measure GSR performance relative to selection methods to test robustness claims

2. **Full vs. Partial Context Evaluation**: Compare GSR performance using full CoT reasoning versus summary-only processing to validate context reduction strategy

3. **Domain Transfer Stress Test**: Evaluate GSR on out-of-distribution mathematical domains and reasoning modalities to test generalizability beyond math→logic transfer