---
ver: rpa2
title: 'The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in
  VLMs'
arxiv_id: '2509.13379'
source_url: https://arxiv.org/abs/2509.13379
tags:
- uncertainty
- size
- across
- vlms
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks uncertainty quantification in Vision-Language
  Models (VLMs) using conformal prediction across 18 state-of-the-art models on six
  multimodal datasets. The core method employs three scoring functions (LAC, APS,
  MS) to generate prediction sets with 90% coverage guarantees.
---

# The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs

## Quick Facts
- arXiv ID: 2509.13379
- Source URL: https://arxiv.org/abs/2509.13379
- Reference count: 35
- Primary result: Conformal prediction framework benchmarks uncertainty in 18 VLMs, showing larger models produce more calibrated uncertainty with smaller prediction sets.

## Executive Summary
This paper introduces a conformal prediction framework for benchmarking uncertainty quantification in Vision-Language Models (VLMs) across 18 state-of-the-art architectures. The method evaluates VLMs on six multimodal datasets using three scoring functions (LAC, APS, Marginal Score) to generate prediction sets with 90% coverage guarantees. The study reveals that larger VLMs consistently produce more calibrated uncertainty estimates with smaller prediction sets, establishing a correlation between model scale, accuracy, and uncertainty efficiency. The framework also extends to closed-source models via instruction-guided likelihood proxies, though with reduced statistical guarantees.

## Method Summary
The framework applies conformal prediction to VLM uncertainty quantification by computing non-conformity scores from model output probabilities. For each multiple-choice task, the method extracts token-level log-probabilities for answer options (A-E, up to A-J for MMMU-Pro), applies three scoring functions (Least Ambiguous Classifier, Adaptive Prediction Sets, Marginal Score), and generates prediction sets containing the true label with probability ≥90%. The evaluation uses a 50/50 calibration-test split across six multimodal datasets, measuring Set Size (lower is better), accuracy, and coverage rate. For closed-source models, an instruction-guided proxy prompts models to self-report likelihood estimates in JSON format.

## Key Results
- Larger VLMs produce smaller prediction sets at fixed coverage rates, demonstrating better calibrated uncertainty.
- LAC scoring yields the most compact sets, while APS is more robust for high-ambiguity tasks like MathVision.
- Model accuracy strongly correlates inversely with set size, indicating uncertainty improves with performance.
- MMMU-Pro and MathVision show higher entropy distributions requiring adaptive scoring functions.
- Instruction-guided proxy method achieves Pearson correlation up to 0.47 with ground truth for closed-source models.

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Free Coverage Guarantee
Conformal prediction constructs prediction sets containing the ground truth with user-defined probability (1-α) without distributional assumptions. Using a calibration set, it establishes a threshold based on non-conformity scores, ensuring valid confidence sets during inference. The core assumption is exchangeability between calibration and test data.

### Mechanism 2: Scale-Driven Uncertainty Calibration
Larger VLMs produce more efficient prediction sets at fixed coverage rates by sharpening probability mass on correct answers. This lowers non-conformity scores for ground truth relative to distractors, reducing average Set Size. The core assumption is that accuracy gains from scaling correlate with semantic understanding rather than surface-level pattern matching.

### Mechanism 3: Task-Ambiguity Adaptive Scoring
Different scoring functions handle varying task ambiguity profiles. LAC penalizes low confidence in top class, working best when probability mass is concentrated. APS sums cumulative probabilities, expanding sets adaptively when distributions are diffuse. The core assumption is that task difficulty manifests as higher entropy in output distributions.

## Foundational Learning

- **Exchangeability in Statistics**: Conformal prediction relies on identical distribution between calibration and test data for valid coverage guarantees. If you calibrate on cartoon images but test on X-rays, does the conformal guarantee hold? (Answer: No, due to distribution shift).

- **Non-conformity Score**: This metric defines "strangeness" by ranking how unusual a prediction is compared to calibration data. If a model outputs probabilities [0.4, 0.3, 0.3] for classes A, B, C, does LAC score for class A equal 0.4 or 0.6? (Answer: 0.6, because s_LAC = 1 - prob).

- **Calibration vs. Accuracy**: A model can be highly accurate but poorly calibrated (e.g., 99% confident when wrong). If a weather model predicts "Rain" 90% of the time and it rains on 90% of those days, is the model accurate, calibrated, or both? (Answer: It is calibrated. Accuracy refers to raw correctness).

## Architecture Onboarding

- **Component map**: Input (Image+Text) -> VLM Core -> Probability Extractor -> Scoring Layer -> Conformal Layer -> Output (Set C(X))

- **Critical path**: Extraction of valid token-level log-probabilities from VLM output. This is the primary constraint for open-source vs. closed-source models.

- **Design tradeoffs**: LAC vs. APS (LAC yields smaller sets but may fail on high-entropy tasks; APS is robust but produces larger sets). Direct logprobs vs. Proxy (Proxies allow closed-source evaluation but introduce noise and lack formal guarantees).

- **Failure signatures**: Empty Sets (using LAC on very uncertain tasks), Coverage Collapse (insufficient calibration set size), Large Set Sizes (>3) (model is effectively guessing).

- **First 3 experiments**: 1) Verify coverage on held-out ScienceQA validation split. 2) Compare LAC vs. APS on MathVision using same model. 3) Validate GPT-4o-mini proxy against direct logprob on 100 samples.

## Open Questions the Paper Calls Out

- How can conformal prediction frameworks be extended to open-ended generative tasks where the output space is unbounded?
- To what extent do instruction-guided likelihood proxies preserve the statistical validity of conformal prediction for closed-source models?
- How can scaling laws of uncertainty be leveraged to create dynamic calibration mechanisms for safer VLM deployments?

## Limitations

- Conformal guarantees rely on exchangeability assumptions that may fail under significant distribution shift.
- Instruction-guided proxy method introduces noise (Pearson correlation ~0.47) without formal coverage guarantees.
- Evaluation focuses exclusively on multiple-choice QA tasks, leaving open-ended generation unexplored.

## Confidence

**High Confidence**: Larger models producing smaller prediction sets, inverse relationship between accuracy and set size, overall framework benchmarking ability across 18 models and six datasets.

**Medium Confidence**: Task-specific scoring function recommendations and coverage guarantee robustness under distribution shift.

**Low Confidence**: Proxy method's ability to approximate true uncertainty for closed-source models and framework performance on non-QA multimodal tasks.

## Next Checks

1. Evaluate same VLM on same dataset with systematic covariate shift to measure coverage guarantee degradation.

2. Apply conformal framework to text-to-image or visual storytelling task to validate coverage guarantees beyond multiple-choice QA.

3. Systematically compare prediction sets from instruction-guided proxy against ground-truth log-prob sets, quantifying noise impact on decision-making.