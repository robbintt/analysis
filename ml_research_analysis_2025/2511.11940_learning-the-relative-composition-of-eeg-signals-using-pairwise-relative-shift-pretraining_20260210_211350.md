---
ver: rpa2
title: Learning the relative composition of EEG signals using pairwise relative shift
  pretraining
arxiv_id: '2511.11940'
source_url: https://arxiv.org/abs/2511.11940
tags:
- pretraining
- learning
- each
- sleep
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARS (Pairwise Relative Shift) pretraining,
  a novel self-supervised learning method for EEG representation learning that predicts
  relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based
  approaches that capture local patterns, PARS learns long-range temporal dependencies
  and relative composition inherent in neural signals.
---

# Learning the relative composition of EEG signals using pairwise relative shift pretraining

## Quick Facts
- arXiv ID: 2511.11940
- Source URL: https://arxiv.org/abs/2511.11940
- Reference count: 22
- PARS (Pairwise Relative Shift) pretraining consistently outperforms existing methods on multiple EEG decoding tasks

## Executive Summary
This paper introduces PARS (Pairwise Relative Shift) pretraining, a novel self-supervised learning method for EEG representation learning that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based approaches that capture local patterns, PARS learns long-range temporal dependencies and relative composition inherent in neural signals. The method samples N patches from EEG sequences, tokenizes them, and uses a transformer encoder with masked positional embeddings to predict relative normalized time shifts between patch pairs. A cross-attention decoder reconstructs an anti-symmetric distance matrix containing these temporal relationships.

Extensive experiments across multiple EEG decoding tasks demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies. On clinical sleep staging, PARS shows superior label efficiency, particularly in low-data regimes. In transfer learning settings across four diverse EEG datasets (wearable sleep staging, abnormal EEG detection, seizure detection, and motor imagery classification), PARS achieves state-of-the-art performance, outperforming masked autoencoders and other position prediction methods in three out of four tasks. The results establish PARS as a new paradigm for self-supervised EEG representation learning that effectively captures temporal structure critical for downstream decoding tasks.

## Method Summary
PARS pretraining trains transformers to predict relative temporal shifts between randomly sampled patches from EEG sequences. The method samples N patches from 30-second EEG windows, applies masked positional embeddings (γ_pos=0.8), and uses a transformer encoder to embed these patches. A cross-attention decoder then predicts the relative normalized time shifts between all patch pairs, forming an anti-symmetric distance matrix. The pretext task encourages learning long-range temporal dependencies rather than local pattern reconstruction. During fine-tuning, the decoder is replaced with a spatial cross-attention head for multi-channel EEG processing.

## Key Results
- PARS achieves superior label efficiency on clinical sleep staging, especially in low-data regimes
- In transfer learning across four diverse EEG datasets, PARS outperforms MAE and other SSL methods in three out of four tasks
- Cross-attention decoder with masked positional embeddings significantly improves performance over alternative architectures
- Random patch sampling creates more challenging pretext tasks than fixed patch sampling, leading to stronger representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predicting relative temporal shifts between EEG patches forces learning of long-range dependencies rather than local pattern reconstruction.
- Mechanism: The pretext task requires the encoder to embed patches such that their pairwise temporal relationships can be decoded. This necessitates capturing global temporal structure, as local features alone are insufficient to determine whether patch A occurred before or after patch B in a sequence.
- Core assumption: Temporal composition (the relative ordering and spacing of neural events) carries semantically meaningful information for downstream EEG tasks.
- Evidence anchors:
  - [abstract] "Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals."
  - [section 2.2] The task estimates θ_{j,k} = (t_j - t_k) / T_s, requiring comparison across arbitrarily spaced patches.
  - [corpus] Related PART method (Ayoughi et al. 2025) demonstrates similar benefits for spatial composition in images, suggesting transferability of relative-position pretext tasks.
- Break condition: If EEG tasks primarily required only local spectral/temporal features (e.g., simple artifact detection), relative position learning would provide minimal advantage over MAE.

### Mechanism 2
- Claim: Masked positional embeddings prevent trivial solutions and force content-based temporal reasoning.
- Mechanism: Position masking (γ_pos = 0.8) means 80% of tokens receive a learnable "mask" token instead of sinusoidal positional embeddings. The decoder must infer temporal relationships from signal content, not from leaked position information. Pairs involving unmasked positions are excluded from loss computation.
- Core assumption: There exists learnable structure in EEG content that correlates with temporal position within a recording session.
- Evidence anchors:
  - [section 2.1] "a learnable position mask token is added to N_m tokens, and the standard sinusoidal PE is added to the remaining (N - N_m) tokens"
  - [section 2.2] "pairs of patches without PE masking are excluded to avoid the leak of positional information"
  - [appendix A.4, Figure 6a] Higher γ_pos increases task difficulty and improves downstream performance, up to memory constraints.
  - [corpus] DropPos (Wang et al. 2023) uses similar masked position prediction but for absolute positions; no direct corpus evidence on relative vs. absolute comparison for EEG.
- Break condition: If position masking ratio is too low, the task becomes trivially solvable; if too high, insufficient signal for learning.

### Mechanism 3
- Claim: Cross-attention decoder enables information sharing across all patch pairs during relative shift prediction.
- Mechanism: Rather than processing each patch pair independently (as an MLP would), the cross-attention decoder allows each pairwise prediction to attend over all patch embeddings. This provides context about the broader temporal structure when estimating any single pairwise relationship.
- Core assumption: Access to the full set of patches improves individual pairwise predictions compared to isolated pair processing.
- Evidence anchors:
  - [section 2.3] "The cross-attention decoder shares information from all sampled patches amongst the pairwise embeddings being evaluated."
  - [appendix A.4, Figure 6c] Cross-attention decoder outperforms pairwise MLP across all γ_pos values tested.
  - [corpus] No direct corpus comparison; this appears to be a methodological contribution specific to PARS.
- Break condition: If computational budget prohibits cross-attention (O(N²) pairs), simpler decoders may be necessary with performance tradeoff.

## Foundational Learning

- Concept: **Self-supervised pretext tasks**
  - Why needed here: PARS is a pretext task—the model learns representations by solving a task where labels come from the data itself (temporal positions), not human annotations. Understanding this paradigm is essential for grasping why pretraining helps downstream tasks.
  - Quick check question: Can you explain why predicting temporal shifts doesn't require labeled data?

- Concept: **Transformer positional embeddings**
  - Why needed here: The method deliberately masks positional embeddings during pretraining. You need to understand what positional embeddings normally do (inject sequence order into order-invariant transformers) to see why masking them creates a meaningful learning signal.
  - Quick check question: What would happen if a transformer had no positional embeddings at all?

- Concept: **Anti-symmetric matrices**
  - Why needed here: The target matrix θ is anti-symmetric (θ_{j,k} = -θ_{k,j}) because relative time shifts are directional. This mathematical structure is core to how the task is formulated.
  - Quick check question: If patch A is 5 seconds after patch B, what is θ_{B,A} if θ_{A,B} = 0.15?

## Architecture Onboarding

- Component map:
  Input EEG (30-sec) → Random patch sampling (N=40, 1-sec each) → Tokenizer (linear projection, dim=512) → Masked positional embedding (γ_pos=0.8 masked, rest sinusoidal) → Transformer encoder (8 blocks, 8 heads, FFN hidden=512) → Pairwise embedding construction: concatenate [y_j, y_k] for all pairs → Cross-attention decoder → Linear head → Predicted θ matrix (N_m × N_m) → MSE loss vs. ground-truth relative shifts

- Critical path:
  1. Pretraining: Train encoder + cross-attention decoder on combined YSYW + TUEG datasets (1.5M examples, 1000 epochs)
  2. Fine-tuning: Replace decoder with spatial cross-attention for multi-channel EEG; freeze nothing, fine-tune all layers
  3. Key hyperparameters: N=40 patches, γ_pos=0.8, batch size 512, lr=1e-4

- Design tradeoffs:
  - **Random vs. fixed patch sampling**: Random sampling creates harder pretext task (more variation in inter-patch distances) → stronger representations but less predictable compute
  - **Cross-attention vs. MLP decoder**: Cross-attention uses global context → better performance but O(N²) memory vs. O(N²) forward passes for MLP
  - **Higher γ_pos**: More difficult task → better representations, but combinatorial explosion in pair count limits by GPU memory

- Failure signatures:
  - **Trivial loss convergence**: γ_pos too low; model solving task via position leakage, not learning useful representations
  - **OOM during pretraining**: N or γ_pos too high; reduce N or use gradient checkpointing
  - **Poor transfer to new datasets**: Pretraining data may not cover target domain's channel configuration/sampling rate; check preprocessing compatibility
  - **Overfitting during fine-tuning**: Spatial token dropout (0.5 probability) should be enabled; reduce if still overfitting

- First 3 experiments:
  1. **Reproduce ablation on patch count (N)**: Pretrain with N={20, 30, 40}, fine-tune on YSYW subset (100 subjects), report Cohen's Kappa. Validates that your implementation matches paper before investing in full pretraining.
  2. **Verify random sampling benefit**: Compare random vs. fixed patch sampling at γ_pos=0.6. Should see ~5-10% Kappa improvement with random sampling per Figure 6b.
  3. **Test label efficiency curve**: Fine-tune PARS-pretrained model on {10, 25, 50, 100, 200} YSYW subjects. Compare against from-scratch baseline to verify pretraining benefit is most pronounced in low-data regimes (per Figure 3).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can hybrid pretraining approaches that combine PARS with masked reconstruction (MAE) outperform either method alone across EEG decoding tasks?
  - Basis in paper: [explicit] "Further investigation is required to determine whether these pretext tasks can be combined to learn even more powerful representations that can outperform MAE and PARS alone."
  - Why unresolved: The paper shows MAE and PARS have complementary strengths (local vs. global features), but only evaluates them separately. MAE matched PARS on seizure detection, suggesting task-specific advantages.
  - What evidence would resolve it: Ablation experiments with joint pretraining objectives, measuring whether combined representations improve performance on tasks where each method currently excels.

- **Open Question 2**: What mechanisms can improve robustness to noisy or degraded EEG channels during fine-tuning?
  - Basis in paper: [explicit] "Better rejection of noisy channels will be investigated in future work," prompted by poor performance on Subject 5 in EESM17 due to electrode contact deterioration.
  - Why unresolved: Current spatial token dropout randomly drops channels at 0.5 probability but doesn't explicitly detect or reject noisy channels based on signal quality.
  - What evidence would resolve it: Integration of signal quality metrics or learned channel attention mechanisms, evaluated on datasets with known channel degradation.

- **Open Question 3**: How does PARS pretraining generalize to EEG tasks requiring fine-grained temporal resolution beyond the 1-second patch scale?
  - Basis in paper: [inferred] PARS uses 1-second patches and 30-second pretraining windows, but downstream tasks like seizure detection require detecting brief events. The similar PARS-MAE performance on TUSZ may indicate limits of coarse temporal learning.
  - Why unresolved: No ablation studies on patch size or analysis of what temporal scales PARS captures.
  - What evidence would resolve it: Systematic evaluation across varying patch sizes and pretraining window lengths on tasks with different temporal granularity requirements.

## Limitations

- Cross-attention decoder architecture details remain underspecified, including layer count, head count, and hidden dimensions
- Study uses relatively controlled EEG datasets with consistent sampling rates and channel configurations, lacking evaluation on substantial domain shifts
- Does not report pretraining validation curves or loss convergence patterns to assess whether 1000 epochs was optimal

## Confidence

- **High confidence**: The empirical finding that PARS outperforms MAE and other SSL methods across multiple downstream tasks, given the consistent improvements shown in Tables 2 and 3 and the ablation studies in Figure 6.
- **Medium confidence**: The claim that PARS is particularly effective in low-data regimes, as this is based on single dataset (YSYW) experiments without cross-dataset validation of label efficiency.
- **Medium confidence**: The assertion that masked positional embeddings are critical for learning temporal composition rather than trivial position memorization, supported by ablations but without alternative position masking strategies for comparison.

## Next Checks

1. **Decoder architecture ablation**: Systematically vary the cross-attention decoder depth (1-3 layers) and width (hidden dimensions 256, 512, 1024) to determine the minimal sufficient architecture and whether current specifications are over-parameterized.

2. **Domain shift robustness test**: Pretrain on YSYW + TUEG, then fine-tune on a clinically distinct dataset (e.g., epilepsy monitoring units or neonatal EEG) to measure performance degradation and identify whether relative temporal composition transfers across acquisition protocols and pathologies.

3. **Temporal resolution sensitivity**: Repeat pretraining with different patch durations (0.5s, 2s, 5s) and patch counts (20, 60) to determine whether the 1-second patch size is optimal or whether different temporal granularities better capture meaningful EEG composition for different downstream tasks.