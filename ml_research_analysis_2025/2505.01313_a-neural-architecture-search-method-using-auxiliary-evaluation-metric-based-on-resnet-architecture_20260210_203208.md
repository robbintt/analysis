---
ver: rpa2
title: A Neural Architecture Search Method using Auxiliary Evaluation Metric based
  on ResNet Architecture
arxiv_id: '2505.01313'
source_url: https://arxiv.org/abs/2505.01313
tags:
- neural
- conv
- search
- network
- resnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MO-ResNet, a neural architecture search method
  that leverages ResNet as a backbone and incorporates an auxiliary evaluation metric
  based on validation loss. The approach extends the search space of previous GA-NAS
  algorithms by allowing variable-length gene codes and introducing new genetic operators
  for convolutional, pooling, and fully connected layers.
---

# A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture

## Quick Facts
- arXiv ID: 2505.01313
- Source URL: https://arxiv.org/abs/2505.01313
- Authors: Shang Wang; Huanrong Tang; Jianquan Ouyang
- Reference count: 40
- Primary result: MO-ResNet achieves better or comparable performance to hand-designed models and existing NAS techniques on MNIST, Fashion-MNIST, and CIFAR-100 with fewer parameters.

## Executive Summary
This paper introduces MO-ResNet, a neural architecture search method that leverages ResNet as a backbone and incorporates an auxiliary evaluation metric based on validation loss. The approach extends the search space of previous GA-NAS algorithms by allowing variable-length gene codes and introducing new genetic operators for convolutional, pooling, and fully connected layers. Unlike single-objective methods, MO-ResNet uses multi-objective optimization to balance both accuracy and loss, expanding the search space and improving the likelihood of finding competitive architectures. Experiments on MNIST, Fashion-MNIST, and CIFAR-100 show that MO-ResNet achieves better or comparable performance to hand-designed models and existing NAS techniques, often with fewer parameters. The inclusion of loss as a secondary metric enhances the discovery of well-balanced architectures. Transfer learning results on ImageNet further demonstrate the method's effectiveness. Code is publicly available.

## Method Summary
MO-ResNet employs a multi-objective evolutionary algorithm to search for optimal neural network architectures based on ResNet. The method uses variable-length gene codes to represent architectures and introduces specialized genetic operators for convolutional, pooling, and fully connected layers. The search process maintains an elite population (EP) set and applies Pareto-based selection to balance accuracy and validation loss objectives. During each iteration, architectures are generated, evaluated through full training, and selected based on their performance across both objectives. The algorithm iteratively refines the population until convergence, resulting in a set of Pareto-optimal architectures that can be deployed directly or used for transfer learning.

## Key Results
- MO-ResNet achieves superior or comparable performance to hand-designed models on MNIST, Fashion-MNIST, and CIFAR-100
- The method consistently discovers architectures with fewer parameters than baseline models while maintaining competitive accuracy
- Transfer learning experiments on ImageNet demonstrate that architectures found on CIFAR-100 can effectively generalize to larger datasets
- The auxiliary validation loss metric significantly improves the quality of discovered architectures compared to single-objective optimization

## Why This Works (Mechanism)
MO-ResNet works by expanding the search space through variable-length gene representations and incorporating validation loss as a secondary optimization objective. This multi-objective approach prevents the algorithm from converging prematurely to suboptimal solutions that might achieve high accuracy but poor generalization. The evolutionary process maintains diversity in the population by preserving multiple Pareto-optimal solutions, allowing exploration of different architectural trade-offs between accuracy and loss. By using ResNet as the backbone, the method benefits from proven architectural patterns while exploring novel variations through genetic operations. The full training evaluation of each candidate ensures reliable performance estimates, though this comes at the cost of computational efficiency.

## Foundational Learning
- **Multi-objective optimization**: Balancing multiple conflicting objectives (accuracy vs. loss) rather than optimizing a single metric - needed to find architectures that generalize well, not just those that overfit training data
- **Evolutionary algorithms**: Using genetic operations (crossover, mutation) to explore architecture space - needed to systematically search large design spaces without exhaustive enumeration
- **Pareto optimality**: Maintaining a set of non-dominated solutions rather than a single best architecture - needed to capture the trade-off between accuracy and loss for different application requirements
- **Variable-length gene representation**: Allowing architectures of different complexities to be encoded in the same search space - needed to explore both shallow and deep network architectures
- **Elite population strategy**: Preserving top-performing architectures across generations - needed to maintain high-quality solutions while continuing exploration
- **Transfer learning validation**: Testing discovered architectures on different datasets - needed to verify that the search process finds generalizable solutions rather than dataset-specific overfits

## Architecture Onboarding

**Component Map:**
Initial Population -> Evaluation (Full Training) -> Pareto Selection -> Genetic Operations -> Elite Population Update -> Convergence Check

**Critical Path:**
The critical path is the evaluation phase where each individual architecture undergoes full training to determine its fitness. This step dominates the computational cost and directly influences the quality of the Pareto front.

**Design Tradeoffs:**
- Accuracy vs. loss: The bi-objective optimization balances model performance with generalization capability
- Exploration vs. exploitation: The elite population strategy maintains good solutions while genetic operations explore new architectural variations
- Computational cost vs. search quality: Full training provides accurate fitness estimates but requires significant GPU resources
- Architecture complexity vs. performance: Variable-length representation allows discovery of both simple and complex architectures

**Failure Signatures:**
- Premature convergence to local optima if the population diversity is not maintained
- Overfitting to validation loss if the auxiliary metric dominates the optimization
- Computational infeasibility if the population size or evaluation budget is too large
- Poor generalization if the search is conducted on a dataset too different from target applications

**First Experiments:**
1. Run MO-ResNet on MNIST with a small population size (20 individuals) and 5 generations to verify basic functionality
2. Compare the Pareto front obtained with and without the auxiliary loss objective on Fashion-MNIST
3. Evaluate the transferability of top-3 architectures from CIFAR-100 to ImageNet using the transfer learning protocol

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of hardware-aware metrics (e.g., FLOPs, latency, or memory usage) as optimization objectives improve the practicality of the discovered architectures compared to optimizing solely for accuracy and loss?
- Basis in paper: [explicit] The Conclusion states, "Future work could consider more neural network evaluation metrics and design more combinations of objective functions to find better neural network architectures."
- Why unresolved: The current study is restricted to a bi-objective optimization (accuracy and validation loss) and does not evaluate the trade-offs introduced by explicitly targeting computational efficiency during the search.
- What evidence would resolve it: Comparative experiments where MO-ResNet is run with a third objective (e.g., FLOPs), demonstrating the resulting Pareto fronts and the trade-off between accuracy and computational cost.

### Open Question 2
- Question: How does the transferability of architectures found on the CIFAR-100 dataset correlate with their ranking in the Pareto set when applied to large-scale datasets like ImageNet?
- Basis in paper: [explicit] Section 3.3 notes, "It may be possible to try to migrate more of the network structures obtained from training on the CIFAR-100 dataset in the future work."
- Why unresolved: The paper only evaluates the transfer learning performance of the single "optimal" architecture, leaving the generalization capability of other competitive individuals in the EP set untested.
- What evidence would resolve it: A study applying the transfer learning protocol (modifying initial convolution layers) to multiple architectures from the final EP set to determine if lower error on CIFAR-100 consistently predicts better transfer performance on ImageNet.

### Open Question 3
- Question: Can the computational efficiency of the MO-ResNet search process be significantly reduced by integrating weight-sharing mechanisms or performance predictors?
- Basis in paper: [inferred] Appendix E (Runtime Analysis) reports that the search on CIFAR-100 requires approximately 37â€“39 GPU days because steps 3 and 7 involve fully training each individual neural network.
- Why unresolved: The methodology relies on standard evolutionary evaluation which is computationally expensive; the paper does not explore acceleration techniques common in modern NAS (e.g., one-shot methods).
- What evidence would resolve it: An ablation study replacing the full training of individuals with a shared-weight supernet training strategy, showing whether comparable architecture rankings can be achieved with substantially lower GPU hours.

## Limitations
- Experiments limited to MNIST, Fashion-MNIST, and CIFAR-100 datasets, with only single-architecture transfer to ImageNet
- Computational efficiency not directly compared with other NAS methods in terms of search time and resource requirements
- Transfer learning validation based on a single architecture rather than demonstrating effectiveness across multiple transferred architectures

## Confidence

**Confidence Labels:**
- High confidence: The multi-objective optimization approach using validation loss as an auxiliary metric is theoretically sound and well-explained.
- Medium confidence: The performance improvements over hand-designed models and existing NAS techniques, as these results are dataset-specific and may not generalize.
- Low confidence: The scalability claims for large-scale datasets and real-world applications, given the limited experimental scope.

## Next Checks

1. Evaluate MO-ResNet on additional diverse datasets including large-scale image classification tasks and non-vision domains to verify generalizability.
2. Conduct direct computational cost comparisons with other state-of-the-art NAS methods, measuring both search time and GPU memory requirements.
3. Test the robustness of discovered architectures by performing multiple independent runs and analyzing the variance in performance outcomes.