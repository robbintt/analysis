---
ver: rpa2
title: Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender
  Bias in SpeechLLMs
arxiv_id: '2510.01254'
source_url: https://arxiv.org/abs/2510.01254
tags:
- bias
- mcqa
- long-form
- speech
- sage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether bias mitigation in SpeechLLMs trained
  on MCQA benchmarks generalises to long-form, realistic tasks. The authors fine-tune
  three SpeechLLMs (Qwen2-Audio, LTU-AS, LLaMA-Omni) using LoRA adapters to induce
  stereotypical, anti-stereotypical, or neutral behaviours on two MCQA datasets (Spoken
  StereoSet and SAGE).
---

# Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs

## Quick Facts
- arXiv ID: 2510.01254
- Source URL: https://arxiv.org/abs/2510.01254
- Reference count: 0
- Primary result: MCQA bias mitigation does not reliably transfer to long-form tasks in SpeechLLMs

## Executive Summary
This paper investigates whether bias mitigation in SpeechLLMs trained on MCQA benchmarks generalizes to long-form, realistic tasks. The authors fine-tune three SpeechLLMs (Qwen2-Audio, LTU-AS, LLaMA-Omni) using LoRA adapters to induce stereotypical, anti-stereotypical, or neutral behaviors on two MCQA datasets (Spoken StereoSet and SAGE). Cross-benchmark tests show partial behavior transfer but inconsistent reduction of undesired biases. More critically, long-form evaluation (career advice, therapy, interview screening, story generation) reveals minimal reliable generalization: fine-tuned models show modest intended changes in some bias dimensions but often fail to transfer or even reverse behaviors. Human validation confirms low alignment between MCQA and long-form bias mitigation. The authors conclude that MCQA benchmarks alone are insufficient for assessing real-world gender bias in SpeechLLMs and introduce the SAGE evaluation suite for more realistic, multi-dimensional bias assessment.

## Method Summary
The study fine-tunes three SpeechLLMs (Qwen2-Audio-7B-Instruct, LTU-AS, LLaMA-Omni) using LoRA adapters on all attention (q/k/v/o proj) and FFN (gate/up/down proj) matrices of the LLM backbone. Five fine-tune variants are created per model: two stereotypical, two anti-stereotypical, and one neutral, using Spoken StereoSet gender subset (2847 samples) and SAGE MCQA suite (1000 samples). Models are trained until convergence on held-out sets with ranks r=4,8 and temperature 0.7. Cross-benchmark MCQA tests evaluate transfer between datasets, while long-form outputs are generated for therapy, career advice, interview screening, and story generation tasks using 20 TTS voices (10 male, 10 female). Automated scoring uses an LLM judge (gemini-2.5-flash-lite-preview-06-17) on 1-5 scales across dimensions, with human validation on a subset.

## Key Results
- MCQA fine-tuning shows partial behavior transfer between benchmarks but inconsistent reduction of undesired biases
- Long-form evaluation reveals minimal reliable generalization: fine-tuned models show modest intended changes but often fail to transfer or reverse behaviors
- Human validation confirms low alignment between MCQA and long-form bias mitigation
- Stereotypical fine-tuning on SSS transfers better to SAGE than vice versa, but neither reduces stereotypical responses effectively
- LLaMA-Omni exhibits unexpected refusal behavior after neutral fine-tuning, refusing MCQA options more frequently

## Why This Works (Mechanism)
The paper demonstrates that LoRA-based fine-tuning on MCQA benchmarks fails to consistently transfer bias mitigation behaviors to long-form generation tasks in SpeechLLMs. This occurs because the fine-tuning only modifies the LLM backbone while leaving the gender-related representations generated by the speech encoder unchanged. The cross-benchmark evaluation reveals that while some behavioral patterns transfer between MCQA datasets, the intended bias reductions are inconsistent. In long-form tasks, the models' responses show that the fine-tuned behaviors either disappear or reverse, suggesting that MCQA benchmarks capture only a narrow slice of gender bias and do not reflect the complexity of real-world interactions.

## Foundational Learning
- LoRA adapters: Parameter-efficient fine-tuning method that modifies only specific projection matrices while freezing most model weights - needed to efficiently test multiple bias configurations without full fine-tuning
- Cross-benchmark evaluation: Testing model behavior across different datasets to assess generalization - needed to determine if bias mitigation transfers beyond the training domain
- Long-form generation assessment: Evaluating model outputs in realistic, extended conversational contexts - needed to measure practical bias impact beyond multiple-choice scenarios
- TTS voice generation: Using synthetic voices to control gender presentation - needed to systematically study gender bias without confounding variables from natural speech variation
- LLM judge scoring: Using language models to evaluate bias dimensions automatically - needed to scale assessment across many generated responses
- Mann-Whitney U test: Non-parametric statistical test for comparing distributions - needed to assess significance of behavioral differences

## Architecture Onboarding
- Component map: Speech encoder -> LLM backbone -> LoRA adapters -> Output generation
- Critical path: Voice input → Speech encoder → LLM backbone → LoRA-modified attention/FFN → Generated response
- Design tradeoffs: Full fine-tuning vs. LoRA efficiency, synthetic vs. natural voices, automated vs. human evaluation
- Failure signatures: Refusal to answer MCQA options, inconsistent cross-benchmark transfer, behavior reversal in long-form tasks
- First experiment 1: Test LoRA fine-tuning convergence on held-out MCQA set with different ranks (r=4 vs r=8)
- First experiment 2: Evaluate cross-benchmark transfer accuracy between SSS and SAGE for each fine-tune variant
- First experiment 3: Generate and score 10 long-form responses per task/variant to check basic functionality

## Open Questions the Paper Calls Out
- What bias mitigation approaches can reliably transfer from MCQA benchmarks to long-form, realistic SpeechLLM tasks? The paper demonstrates that LoRA-based fine-tuning on MCQAs fails to consistently reduce bias in long-form outputs; no alternative method is tested.
- Do findings from TTS-generated voices generalize to natural human speech with its acoustic variability? The study uses commercial TTS voices that may not capture prosody, accent, or speaking style variations present in real user speech.
- How does gender bias interact with other social dimensions (e.g., race, age, accent) in SpeechLLMs? The study isolates gender as a single axis; intersectional effects remain unexplored despite voice carrying multiple identity signals simultaneously.
- Does modifying the speech encoder representations (not just the LLM backbone) improve bias mitigation transfer? LoRA adapters were applied only to LLM projection matrices, explicitly leaving the gender-related representations generated by the speech encoder unchanged.

## Limitations
- Training hyperparameters (learning rate, batch size, epochs, convergence criteria, LoRA alpha/dropout) are not fully specified
- Human validation subset is relatively small (60 samples across 3 annotators)
- Study focuses exclusively on gender bias in English, leaving open questions about generalization to other bias types and languages
- Exact LLM judge prompts and SAGE MCQA scenario templates are not provided

## Confidence
- High confidence: The observation that MCQA fine-tuning shows limited reliable generalization to long-form tasks, as supported by both automated and human validation
- Medium confidence: The conclusion that MCQA benchmarks alone are insufficient for assessing real-world gender bias in SpeechLLMs, given the controlled experimental setup and cross-benchmark tests
- Low confidence: The specific claim that anti-stereotypical fine-tuning can reverse behaviors (e.g., increasing emotional validation in therapy for female voices), due to the small sample size and potential judge bias in long-form evaluation

## Next Checks
1. Re-run long-form evaluation with a larger human validation set (e.g., 10 annotators, 100+ samples) to strengthen statistical conclusions
2. Test cross-benchmark transfer with additional SpeechLLM architectures and bias types (e.g., racial, age-related) to assess generalizability beyond gender
3. Compare results using alternative LLM judges (e.g., GPT-4, Claude) to rule out judge-specific biases in automated scoring