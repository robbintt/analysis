---
ver: rpa2
title: '"If we misunderstand the client, we misspend 100 hours": Exploring conversational
  AI and response types for information elicitation'
arxiv_id: '2506.11610'
source_url: https://arxiv.org/abs/2506.11610
tags:
- design
- designers
- questions
- were
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how digital systems, particularly those
  integrating conversational AI and choice-based response formats, can support information
  elicitation and foster mutual understanding in early-stage client-designer collaboration.
  Through interviews with 10 design companies, a prototype elicitation tool was developed
  and evaluated with 50 mock clients in a 2x2 factorial design (AI vs.
---

# "If we misunderstand the client, we misspend 100 hours": Exploring conversational AI and response types for information elicitation

## Quick Facts
- arXiv ID: 2506.11610
- Source URL: https://arxiv.org/abs/2506.11610
- Reference count: 40
- Primary result: Conversational AI and choice-based responses improve client response clarity in early-stage design elicitation, though they reduce perceived dependability

## Executive Summary
This study investigates how digital systems integrating conversational AI and structured response formats can support information elicitation and mutual understanding in early-stage client-designer collaboration. Through interviews with 10 design companies and a prototype evaluation with 50 mock clients in a 2x2 factorial design, the research examines the impact of AI assistance and response type (free-text vs. choice-based) on elicitation quality. The findings reveal that while conversational AI and choice-based responses led to lower dependability scores on the User Experience Questionnaire, they significantly improved the clarity of client responses. Designers emphasized the value of such systems for client preparation but stressed the critical need for AI outputs to be verifiable and editable.

## Method Summary
The research employed a mixed-methods approach involving interviews with 10 design companies to understand current elicitation challenges, followed by prototype development and evaluation. The prototype was tested with 50 mock clients using a 2x2 factorial design (AI vs. no AI, free-text vs. choice-based responses). The study measured outcomes using quantitative metrics from the User Experience Questionnaire and qualitative feedback from participating designers. This approach allowed for both statistical analysis of elicitation effectiveness and rich contextual insights from design professionals.

## Key Results
- Conversational AI and choice-based responses improved the clarity of client responses compared to free-text and no-AI conditions
- These features reduced dependability scores on the User Experience Questionnaire
- Designers valued the system for client preparation but emphasized the need for AI outputs to be verifiable and editable

## Why This Works (Mechanism)
The study's mechanism centers on how conversational AI can scaffold client thinking and guide responses toward more structured, actionable information, while choice-based responses reduce ambiguity by constraining the response space. The AI provides contextual prompts and clarifications that help clients articulate requirements more precisely, while choice formats prevent vague or incomplete answers. This combination addresses the common problem of misaligned expectations in early design phases, where poor elicitation can lead to significant project rework.

## Foundational Learning
- **Information Elicitation**: The process of gathering requirements and understanding from clients in early design phases. Why needed: Poor elicitation leads to misaligned expectations and costly project revisions. Quick check: Are elicitation tools improving the specificity and actionability of client responses?
- **Conversational AI in Design**: AI systems that can engage in dialogue to extract and clarify requirements. Why needed: Standard questionnaires often fail to capture nuanced client needs. Quick check: Does the AI adapt its questioning based on client responses?
- **Choice-Based vs. Free-Text Responses**: Structured vs. open-ended response formats. Why needed: Different clients have different preferences and capabilities for expressing requirements. Quick check: Which response type yields more complete and clear information for a given client type?
- **User Experience Questionnaire (UEQ)**: A standardized tool for measuring user experience across multiple dimensions. Why needed: Provides objective metrics for comparing different elicitation approaches. Quick check: How do different elicitation features affect user satisfaction and perceived dependability?
- **Prototype Evaluation with Mock Clients**: Testing design tools with simulated rather than actual clients. Why needed: Allows controlled experimentation but may not capture real-world complexities. Quick check: How well do mock client results generalize to actual client interactions?
- **2x2 Factorial Design**: A research design testing two independent variables each at two levels. Why needed: Allows examination of both individual and interaction effects of elicitation features. Quick check: Are the effects of AI and response type independent or synergistic?

## Architecture Onboarding
**Component Map**: Client Interface -> Response Processor -> AI Engine (optional) -> Designer Dashboard -> Feedback Loop

**Critical Path**: The elicitation process flows from client interface through response processing, with optional AI augmentation, to designer review and feedback integration.

**Design Tradeoffs**: The study highlights the tension between AI assistance (which improves clarity) and user control (which affects dependability). Choice-based responses improve structure but may limit expressiveness. The system must balance guidance with flexibility.

**Failure Signatures**: Poor client engagement, ambiguous responses despite AI assistance, designer distrust of AI outputs, and system complexity overwhelming clients are key failure modes.

**First Experiments**:
1. Test AI verification and editing capabilities to improve dependability scores
2. Compare mock client results with actual client interactions across different design disciplines
3. Evaluate the long-term impact of structured elicitation on project success rates

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The study used mock clients rather than real clients, potentially missing complexities of actual client-designer relationships
- Sample size of 10 design companies and 50 mock clients limits generalizability to broader design contexts
- The research focused on early-stage elicitation, leaving unclear how findings apply to later project phases

## Confidence
- **Medium** confidence that conversational AI and choice-based responses improve response clarity
- **Medium** confidence that these features reduce dependability scores
- **Medium** confidence in generalizability due to mock client limitations

## Next Checks
1. Conduct a longitudinal field study with actual clients and design projects to validate findings in real-world contexts
2. Test the prototype with a more diverse range of design disciplines and project types to assess generalizability
3. Investigate the impact of AI verification and editing capabilities on both response quality and user trust through controlled experiments