---
ver: rpa2
title: LLMs model how humans induce logically structured rules
arxiv_id: '2507.03876'
source_url: https://arxiv.org/abs/2507.03876
tags:
- rules
- human
- rule
- logical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  serve as models of human logical reasoning, a long-standing question in cognitive
  science. Using a logical rule learning paradigm, researchers tested various LLMs
  on their ability to induce and apply rules over logical concepts.
---

# LLMs model how humans induce logically structured rules

## Quick Facts
- arXiv ID: 2507.03876
- Source URL: https://arxiv.org/abs/2507.03876
- Reference count: 40
- Primary result: LLMs achieve human-level accuracy on logical rule induction and can match human learning trajectories better than Bayesian models

## Executive Summary
This study investigates whether large language models can serve as models of human logical reasoning. Using a logical rule learning paradigm, researchers tested various LLMs on their ability to induce and apply rules over logical concepts. The findings show that several LLMs, particularly GPT-4 and Gemma-7B, achieved human-level accuracy on both propositional and first-order logic rules. Experiment 3 demonstrated that a tuned LLM matched human learning trajectories better than existing Bayesian models, with high correlation in error patterns and learning curves. The results suggest that LLMs can model human logical reasoning at least as well as symbolic models, offering a new computational theory of human cognition that may differ from classical logic in its primitives and inference procedures.

## Method Summary
The study used a logical rule learning paradigm where models receive sets of labeled objects (text descriptions of size, color, shape) and must infer the underlying logical rule to classify new objects. Experiment 1 evaluated off-the-shelf LLMs on classification accuracy. Experiment 2 examined rule articulation consistency by having models verbalize their inferred rules. Experiment 3 fine-tuned Gemma-7B using QLoRA on human response distributions to match learning trajectories. Experiment 4 tested generalization to held-out rules. The evaluation used accuracy metrics, R² correlation with human trajectories, and consistency between verbalized rules and classifications.

## Key Results
- Multiple LLMs achieved human-level accuracy (0.932 ± 0.024) on propositional and FOL rules without explicit logical primitives
- GPT-4's verbalized rules showed 96.3% consistency with its classifications but were more verbose than Bayesian models
- Fine-tuned Gemma-7B explained 84.8% of variance in human learning trajectories, outperforming models tuned on correct labels
- Tuning generalized to held-out rules sharing logical components, though transfer to novel primitives was limited

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs acquire logic-like representations from language pretraining without explicit logical primitives.
- **Mechanism**: Scale-driven emergence of compositional reasoning capabilities from generic next-token prediction over large corpora.
- **Core assumption**: High accuracy + consistent rule articulation implies logical representation rather than surface pattern matching.
- **Evidence anchors**: Abstract states LLMs "provide at least as good a fit to human behavior as models that implement a Bayesian probabilistic language of thought"; related work shows LLMs have different inductive biases than humans in concept tasks.
- **Break condition**: If LLM performance collapses on novel logical combinations not resembling training distribution, emergence claim weakens.

### Mechanism 2
- **Claim**: LLMs maintain consistent internal representations linking verbalized rules to classification behavior.
- **Mechanism**: Both classification outputs and rule explanations are generated from shared latent representations.
- **Core assumption**: Verbalized rules reflect actual computational process, not post-hoc rationalization.
- **Evidence anchors**: Section 3.2 shows 96.3% consistency between classification responses and self-reported rules; high likelihood of elicited hypotheses suggests actual rule application.
- **Break condition**: If future work shows LLMs can be prompted to produce arbitrary rule explanations while maintaining identical classifications, shared-representation claim fails.

### Mechanism 3
- **Claim**: Fine-tuning on human response distributions calibrates LLM hypothesis search to match human learning dynamics.
- **Mechanism**: Training on human probability distributions shapes the model's uncertainty patterns and hypothesis ordering.
- **Core assumption**: Tuning process reweights existing learned concepts rather than introducing fundamentally new representations.
- **Evidence anchors**: Section 4.2 shows tuned LLM explains 84.8% of variance in human responses vs. 57.6% for model tuned on correct labels.
- **Break condition**: If tuning fails to generalize to rules with novel logical components, claim that it teaches "concepts" rather than surface heuristics weakens.

## Foundational Learning

- **Concept: Bayesian probabilistic Language of Thought (pLoT)**
  - **Why needed here**: Baseline framework LLMs are compared against. Understanding pLoT is essential to interpret what it means that LLMs "match or exceed" this approach.
  - **Quick check question**: Can you explain why a pLoT model requires pre-specified logical primitives while LLMs do not?

- **Concept: Logical rule learning paradigm**
  - **Why needed here**: Experimental methodology structure (incremental evidence, held-out lists, learning trajectories) determines what can be measured and compared.
  - **Quick check question**: Why does measuring learning trajectories provide more information than measuring only final accuracy?

- **Concept: Propositional vs. First-Order Logic (FOL) rules**
  - **Why needed here**: LLMs perform differently on these rule types. Understanding the distinction is crucial for interpreting results and designing experiments.
  - **Quick check question**: What is an example of a rule that requires FOL but not propositional logic to express?

## Architecture Onboarding

- **Component map**: Stimuli generation -> LLM inference engine -> Prompt formatting -> Tuning pipeline (for Exp 3-4) -> Evaluation framework
- **Critical path**: Start with Experiment 1 methodology (basic classification accuracy without tuning) -> Progress to Experiment 2 (rule elicitation and consistency checking) -> Advance to Experiment 3 (trajectory correlation with tuning) -> Validate with Experiment 4 (generalization to held-out rules)
- **Design tradeoffs**: GPT-4 vs. Gemma-7B for tuning (GPT-4 higher raw accuracy, Gemma-7B chosen for tuning due to open weights); Chat vs. completion formats (different prompt structures, completion models had higher exclusion rates); Tuning on human responses vs. correct labels (human-response tuning produces better trajectory matching but lower accuracy); Context length limitations (GPT-2 variants could only process 14 sets vs. 25 for other models)
- **Failure signatures**: FOL rule struggles (LLMs avoid quantifier language, preferring verbose propositional approximations); XOR rule failures (LLMs consistently fail on XOR rules where Bayesian models succeed); Verbosity in elicited rules (LLM rules concatenate features rather than finding compact representations); Limited transfer to novel primitives (tuning on 112 rules doesn't improve performance on majority/minority concepts not in training set)
- **First 3 experiments**:
  1. **Replicate Experiment 1 on a single rule type**: Test a pretrained LLM (e.g., Gemma-7B) on propositional rules only, measuring last-quarter accuracy against the human baseline range (0.932 ± 0.024).
  2. **Implement rule elicitation for consistency check**: Following Experiment 2 methodology, prompt the model to articulate its rule before classifying. Convert natural language rules to executable Python code and measure consistency between stated rule and actual classifications. Target: >90% consistency for propositional rules.
  3. **Fine-tune on human response distributions**: Using QLoRA, tune a model on the training lists from PTG16 using human probability distributions as targets. Evaluate R² correlation on held-out lists. Compare to both pretrained baseline and model tuned on correct labels. Key metric: variance explained in human responses (target >80%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do humans induce rules using complex logical primitives like 'xor' and 'iff', or do they merely approximate these rules using simpler operators?
- Basis in paper: Section 3.2 notes that because LLMs struggle with 'xor' rules, it raises an "important open question for further research on humans" regarding whether humans possess these complex primitives.
- Why unresolved: Humans are generally unable to articulate the specific rules they use in these tasks, and prior studies did not analyze participant reports in sufficient detail.
- What evidence would resolve it: Experimental paradigms specifically designed to distinguish between the application of complex primitives versus concatenations of simpler Boolean operators in human subjects.

### Open Question 2
- Question: Do LLMs differ from Bayesian pLoT models primarily in their logical primitives, their inference procedures, or both?
- Basis in paper: Section 6.2 states that the results are "inconclusive" regarding whether the LLM's improved fit is due to more human-like hypothesis-updating (inference) or more human-like operators (primitives).
- Why unresolved: High correlation in learning trajectories can arise from different computational mechanisms, and verbalized rules may not faithfully reflect internal representations.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., circuit analysis) mapping the internal representations of LLMs to logical concepts and inference steps during the task.

### Open Question 3
- Question: Can newer, larger open-source models match human learning trajectories without specific fine-tuning on human response data?
- Basis in paper: The General Discussion mentions that newer models like Gemma 2 or Llama 3 have been released, and the authors "leave it to future work to test" if these models can achieve similar correspondence without tuning.
- Why unresolved: The current study successfully fit human data by fine-tuning a smaller model (Gemma-7B); it is unknown if scale alone provides a sufficient prior for human-like learning curves.
- What evidence would resolve it: Replicating the correlation analysis (Experiment 3) using off-the-shelf versions of newer, larger models to see if they naturally produce human-like error patterns.

## Limitations

- Behavioral accuracy and consistency metrics cannot definitively establish whether LLMs use logical representations or sophisticated pattern matching
- Fine-tuning on human response distributions produces better human trajectory matching but lower accuracy than tuning on ground truth
- Limited generalization to rules with novel logical components not present in training data
- The scope of generalization may be limited to specific rule learning paradigms rather than general logical reasoning

## Confidence

**High Confidence Claims:**
- LLMs achieve human-level accuracy on logical rule learning tasks
- Fine-tuned LLMs can match human learning trajectories better than Bayesian models
- GPT-4 can articulate rules with high consistency to its classifications

**Medium Confidence Claims:**
- LLMs acquire logic-like representations through scale-driven emergence rather than innate primitives
- Fine-tuning on human response distributions calibrates hypothesis search to match human dynamics
- LLMs provide "at least as good a fit" to human behavior as Bayesian models

**Low Confidence Claims:**
- LLMs model human logical reasoning "at least as well as" symbolic models in general
- LLMs could serve as a new computational theory of human cognition
- The mechanisms differ from classical logic in their primitives and inference procedures

## Next Checks

1. **Mechanistic interpretability validation**: Apply circuit analysis or feature visualization techniques to examine whether the LLM's internal representations contain compositional logical operators that correspond to the rules being induced.

2. **Novel primitive generalization test**: Design an experiment where the LLM must induce rules using logical components never seen during tuning (e.g., temporal logic, modal operators) to demonstrate acquisition of abstract logical reasoning capabilities.

3. **Adversarial prompting intervention**: Systematically perturb the rule articulation prompt while keeping the classification context identical to test whether rule explanations remain consistent with classifications, validating the shared-representation hypothesis.