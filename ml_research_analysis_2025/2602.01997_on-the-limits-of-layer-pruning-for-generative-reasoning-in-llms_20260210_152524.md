---
ver: rpa2
title: On the Limits of Layer Pruning for Generative Reasoning in LLMs
arxiv_id: '2602.01997'
source_url: https://arxiv.org/abs/2602.01997
tags:
- pruning
- generative
- layer
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Layer pruning is effective for compressing large language models
  on classification tasks, but severely degrades performance on generative reasoning
  tasks such as math and coding. Through systematic ablation studies, we find that
  layer pruning disrupts core algorithmic capabilities, including arithmetic computation
  and syntax tracking, which are essential for multi-step reasoning.
---

# On the Limits of Layer Pruning for Generative Reasoning in LLMs

## Quick Facts
- **arXiv ID**: 2602.01997
- **Source URL**: https://arxiv.org/abs/2602.01997
- **Reference count**: 40
- **Primary result**: Layer pruning severely degrades generative reasoning while maintaining classification performance, with algorithmic capabilities like arithmetic and syntax tracking being particularly vulnerable.

## Executive Summary
Layer pruning effectively compresses large language models for classification tasks but causes severe degradation for generative reasoning tasks like math and coding. Through systematic ablation studies across multiple model architectures, we find that pruning disrupts core algorithmic capabilities including arithmetic computation and syntax tracking, which are essential for multi-step reasoning. Even with our proposed Self-Generated Responses (SGR) approach for post-pruning recovery, generative task performance remains fundamentally limited compared to classification. The persistent gap under highly favorable conditions indicates that layer pruning irreversibly damages algorithmic circuits that are difficult to reconstruct without large-scale data or compute.

## Method Summary
The study systematically evaluates layer pruning across three model families (Llama-3.1-8B, Qwen2.5-7B, Mistral-7B, Gemma2-2B) using three pruning strategies: Block Influence, Reverse Order, and Iterative. Models are finetuned using QLoRA with learning rate 2e-4, batch size 8, and sequence length 8192. The SGR approach generates training data by having the unpruned base model respond to prompts from datasets like Alpaca-cleaned and Dolci. Performance is evaluated on classification benchmarks (HellaSwag, PIQA, MMLU, WinoGrande, OpenBookQA, ARC-E/C) and generative benchmarks (GSM8K, HumanEval+, MBPP+, XSUM), measuring retention relative to baseline, arithmetic accuracy, syntax errors, and text degeneration metrics.

## Key Results
- Classification task retention ranges from 0.70-0.84 across models, while generative reasoning retention drops to 0.14-0.33
- Pruning deeper layers in Qwen and mid-depth layers in Llama causes arithmetic failures, with models failing simple calculations like "32 × 6 = 364"
- Self-Generated Responses improve pruned model recovery by 20-30 percentage points on generative benchmarks compared to standard finetuning
- The classification-generative gap persists even at moderate pruning ratios (10-15%), indicating fundamental limitations

## Why This Works (Mechanism)

### Mechanism 1
Layer pruning disproportionately damages algorithmic circuits (arithmetic, syntax tracking) compared to surface-level text generation, and this damage is difficult to reverse under constrained post-training. Pruning removes entire transformer blocks containing specialized sub-circuits for multi-step algorithmic operations. Classification tasks rely on shallower or more redundant sub-networks, while generative reasoning depends on deeper, non-redundant structures for sequential computation. The core assumption is that algorithmic capabilities like arithmetic and parenthesis tracking are implemented in distributed circuits spanning specific layers that cannot be easily rerouted after pruning.

### Mechanism 2
Self-Generated Responses (SGR) provide superior supervision for pruned model recovery compared to external open-source datasets because the base model's outputs preserve internal representations and distributional patterns. The unpruned base model generates responses already aligned with its learned representations, reducing distribution shift when training the pruned model. This implicit knowledge distillation occurs without requiring a separate teacher model. The core assumption is that pruned models learn to approximate base model behavior more effectively when training targets come from the same model family rather than heterogeneous external sources.

### Mechanism 3
The gap between classification and generative task recovery reflects fundamentally different dependencies on model depth, with generative reasoning requiring deeper computational chains. Classification tasks evaluate via log-likelihood comparison over fixed candidates, computable with shallower processing. Generative reasoning requires sequential token production with maintained state (arithmetic partial results, parenthesis counts), demanding deeper or more specialized layers. The core assumption is that depth in transformers serves a functional role for iterative computation that cannot be fully compensated by remaining layers after pruning.

## Foundational Learning

- **Layer pruning vs. other compression techniques**: Why needed - Layer pruning removes entire computational units, qualitatively different from quantization or sparsification. Quick check - What type of compression removes entire computational units rather than individual parameters?

- **Algorithmic capabilities as specialized circuits**: Why needed - The central claim depends on understanding arithmetic and syntax tracking as specialized neural circuits spanning particular layers. Quick check - Why would removing layer 23 in Qwen specifically increase parenthesis-matching errors while other layers don't?

- **Perplexity as training signal quality metric**: Why needed - The paper uses perplexity curves to demonstrate SGR provides better optimization targets than external datasets. Quick check - Lower perplexity during training indicates what about the quality of the supervision signal?

## Architecture Onboarding

- **Component map**: Base model -> Pruning module -> SGR generator -> Recovery trainer -> Evaluation suite
- **Critical path**: 1) Select pruning ratio and method (BI recommended for generative tasks at moderate ratios) 2) Generate SGR training data from base model using target domain prompts 3) Apply QLoRA finetuning with learning rate ~2e-4, sequence length 8192 4) Evaluate on both classification and generative benchmarks to quantify recovery gap
- **Design tradeoffs**: Higher pruning ratio yields more compression but irreversible generative reasoning loss; SGR vs. external data requires base model inference time but yields 15-30pp improvement; Iterative vs. BI pruning - Iterative is model-specific and doesn't generalize; BI is more robust across architectures
- **Failure signatures**: Arithmetic errors in intermediate CoT steps (e.g., "32 × 6 = 364"); unbalanced parentheses in code generation (e.g., `arr[n - 1 - i)`); sharp performance drop at specific layer indices (non-uniform sensitivity); elevated 4-gram repetition and Self-BLEU scores indicating degeneration
- **First 3 experiments**: 1) Single-layer ablation sweep: Remove each layer individually, evaluate on GSM8K and HumanEval+ to identify sensitive layers for your specific model 2) Arithmetic probe test: Use single-token arithmetic task to isolate computational capability from generation ability before/after pruning 3) SGR vs. external data comparison: Train pruned model on Dolci prompts with self-generated responses vs. original Dolci responses; measure perplexity curves and downstream task retention

## Open Questions the Paper Calls Out

### Open Question 1
Can algorithmic circuits disrupted by layer pruning be fully reconstructed with pretraining-scale data and compute, or is the damage fundamentally irreversible? The study focuses on realistic post-training constraints without access to large-scale resources; the upper bound of recovery potential remains untested. Experiments applying knowledge distillation or continued pretraining with billions of tokens to pruned models would resolve this question.

### Open Question 2
What architectural or functional properties determine whether a specific layer is critical for generative reasoning versus redundant? The paper observes model- and task-dependent effects where layer sensitivity varies across model families, but doesn't identify underlying causes. Mechanistic interpretability studies mapping arithmetic and syntax-tracking circuits to specific layers across architectures would resolve this question.

### Open Question 3
Do other atomic capabilities beyond arithmetic and parenthesis tracking underlie the generative reasoning gap, and can they be identified and preserved? The authors identify arithmetic and syntax as important algorithmic capabilities but acknowledge these may represent only a subset. Targeted ablation studies probing additional algorithmic primitives would resolve this question.

## Limitations
- The paper doesn't establish whether arithmetic and syntax tracking failures represent truly specialized "algorithmic circuits" versus degraded general capabilities
- SGR superiority is demonstrated against specific datasets but not established as a general principle across diverse high-quality supervision sources
- The study doesn't test whether architectural modifications could compensate for depth reduction, leaving open the question of fundamental irreversibility

## Confidence

**High Confidence**: Classification vs. generative recovery gap exists and is substantial - The empirical evidence across multiple models and benchmarks is robust and well-supported.

**Medium Confidence**: Layer pruning specifically damages algorithmic circuits (arithmetic, syntax tracking) rather than general language capabilities - While failure cases are documented, the mechanism explaining why these specific capabilities are uniquely vulnerable requires further validation.

**Low Confidence**: SGR provides fundamentally better supervision than external datasets - The paper demonstrates SGR works better than the specific datasets tested, but doesn't establish this as a general principle.

## Next Checks

1. **Algorithmic circuit isolation test**: Design experiments that separate arithmetic computation from generation ability by testing whether models can correctly compute intermediate values when given as input versus when they must generate the computation steps themselves.

2. **Cross-dataset SGR validation**: Compare SGR performance against multiple high-quality external datasets to determine if the advantage is specific to the SGR mechanism or reflects dataset quality differences.

3. **Architecture compensation study**: Test whether architectural modifications like increased attention heads, wider layers, or modified attention mechanisms in pruned models can recover generative reasoning performance, which would challenge the claim that depth loss is fundamentally irreversible.