---
ver: rpa2
title: 'Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters
  via Segmented and Distributed Prompt Processing'
arxiv_id: '2503.21598'
source_url: https://arxiv.org/abs/2503.21598
tags:
- prompt
- response
- evaluation
- processing
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a jailbreaking framework that segments malicious
  prompts into smaller, seemingly benign components processed in parallel across multiple
  LLMs, achieving a 73.2% success rate in generating malicious code across 10 cybersecurity
  categories. The framework employs iterative refinements, transforming abstract function
  descriptions into runnable implementations through systematic distributed processing.
---

# Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing

## Quick Facts
- arXiv ID: 2503.21598
- Source URL: https://arxiv.org/abs/2503.21598
- Reference count: 40
- 73.2% success rate in generating malicious code across 10 cybersecurity categories using distributed prompt processing

## Executive Summary
This paper introduces a jailbreaking framework that segments malicious prompts into smaller, seemingly benign components processed in parallel across multiple LLMs, achieving a 73.2% success rate in generating malicious code across 10 cybersecurity categories. The framework employs iterative refinements, transforming abstract function descriptions into runnable implementations through systematic distributed processing. A key innovation is the LLM jury evaluation system, which revealed that traditional single-LLM judge assessments significantly overestimate success rates (93.8% vs 73.2%). The distributed architecture improves performance by 12% compared to non-distributed approaches, with effectiveness correlating positively with abstraction levels—demonstrating superior performance for high-level code-based attacks versus hardware-focused implementations.

## Method Summary
The framework processes malicious prompts through a four-module pipeline: (1) Prompt Segmentation breaks harmful requests into benign function descriptions, (2) Parallel Processing transforms these into runnable code through iterative refinement, (3) Response Aggregation combines and documents the final program, and (4) Jury Evaluation uses three LLMs to validate output quality against five criteria. The approach leverages the observation that LLMs exhibit higher compliance when refining existing content versus generating harmful code from scratch, and that distributing intent across multiple models reduces safety filter effectiveness.

## Key Results
- 73.2% success rate in generating functional malicious code across 10 cybersecurity categories
- 12% performance improvement using distributed architecture versus non-distributed approaches
- Single-LLM judges overestimate success rates by 20% (93.8% vs 73.2%), validating the jury evaluation system
- Python bias in outputs limits effectiveness for hardware-focused attacks requiring low-level languages

## Why This Works (Mechanism)

### Mechanism 1: Intent Segmentation
Bypassing safety filters by decomposing malicious intent into semantically benign sub-tasks that individually fail to trigger refusals. The Prompt Segmentation Module translates harmful requests into high-level function descriptions processed in parallel, exploiting safety filters' reliance on isolated prompt evaluation rather than cumulative multi-turn intent.

### Mechanism 2: Iterative Refinement
Leverages higher LLM compliance rates when refining existing content compared to direct generation. The three-step transformation pipeline (Description → Pseudocode → Runnable Code) gradually lowers abstraction levels, exploiting compliance bias where contextually framed refinement tasks override safety fine-tuning more effectively than generative tasks.

### Mechanism 3: Multi-Model Jury Evaluation
Reduces overestimation of attack success common in single-model judging by using three diverse LLMs for consensus-based quality assessment. The ensemble approach corrects for individual model biases, revealing that single judges accept incomplete implementations and placeholders that the jury system rejects.

## Foundational Learning

**Concept: Reinforcement Learning with Human Feedback (RLHF) & Safety Alignment**
- Why needed here: Understanding what is being bypassed—safety filters are typically products of RLHF that pattern-match for "harmful intent"
- Quick check question: How does splitting a prompt into "function descriptions" change the semantic vector processed by the safety classifier compared to the full prompt?

**Concept: Chain-of-Thought (CoT) & Intermediate Reasoning**
- Why needed here: The iterative refinement mechanism exploits the model's ability to follow logic steps, similar to CoT but used adversarially to drift from benign to harmful outputs
- Quick check question: Why might an LLM refuse a direct request but comply with a step-by-step request that leads to the same outcome?

**Concept: Ensemble Methods (Law of Large Numbers)**
- Why needed here: The LLM Jury relies on ensemble theory—aggregated judgments reduce variance and individual bias
- Quick check question: Why is a "majority vote" among 3 diverse models likely more robust than using the single "smartest" model for evaluation?

## Architecture Onboarding

**Component map:**
1. **Prompt Segmentation (Input):** GPT-4o-mini breaks malicious intent → N benign function specs
2. **Parallel Processing (Execution):** GPT-4o processes specs concurrently via Pseudocode → Code → Refinement
3. **Response Aggregation (Assembly):** Gemini 1.5 Pro stitches functions into runnable program and adds documentation
4. **Jury Evaluation (QA):** Claude 3.5 Sonnet, Gemini 1.5 Pro, GPT-4o-mini vote on 5 quality criteria

**Critical path:** The Prompt Segmentation Module—the paper identifies Acceptance Rate (AR) and Diversion Rate (DR) here as critical bottlenecks. If the segmenter fails to produce N descriptions or drifts from the topic, the entire distributed pipeline collapses.

**Design tradeoffs:**
- Latency vs. Success: Distributed architecture adds significant latency (~76s) and API costs (63 unique LLM steps) but yields 12% higher Success Rate
- Strictness vs. Yield: Jury Evaluation reduces reported success rate (93.8% → 73.2%) but ensures "successful" outputs are actually functional runnable code

**Failure signatures:**
- High Diversion Rate: Segmenter produces off-topic functions, leading to useless aggregation
- Abstract Logic Persistence: Aggregation fails to resolve placeholders, violating "Specificity" criterion
- Jury Deadlocks: Ambiguous evaluation criteria could lead to inconsistent binary classifications

**First 3 experiments:**
1. **Ablation Replication:** Run 500 CySecBench prompts with "Parallel Processing" module disabled to verify 12% performance drop
2. **Judge vs. Jury Validation:** Manually inspect 50 samples rated "Successful" by Single-Judge but "Failed" by Jury to confirm specific "incomplete implementations"
3. **Model Substitution Stress Test:** Swap GPT-4o-mini in Segmentation module with open-source model to test robustness of "educational context" framing

## Open Questions the Paper Calls Out

**Open Question 1:** Will newer or alternative LLM architectures exhibit greater resistance to distributed prompt processing attacks compared to currently evaluated models?
- Basis in paper: Authors explicitly state intent to assess new models for enhanced resistance
- Why unresolved: Study limited to specific proprietary models available at time of writing
- What evidence would resolve it: Replication using subsequently released models (e.g., GPT-5) to compare Success Rates against 73.2% benchmark

**Open Question 2:** Can cross-session context analysis successfully mitigate distributed jailbreaking without triggering excessive false positives?
- Basis in paper: Framework reduces detection likelihood by distributing segments, implying current single-prompt defenses are insufficient
- Why unresolved: Research focuses on attack vector efficacy, not defensive mechanisms
- What evidence would resolve it: Study implementing security layer that tracks semantic context across concurrent API calls

**Open Question 3:** Does Python bias in outputs significantly limit framework's efficacy for attack vectors requiring low-level languages?
- Basis in paper: Discussion notes linguistic homogeneity toward Python and corresponding drop in Success Rates for hardware-focused categories
- Why unresolved: Experiments relied on LLMs' default generation tendencies without enforcing specific low-level language constraints
- What evidence would resolve it: Experiment enforcing C or Assembly language in prompt segmentation phase

## Limitations
- Framework effectiveness heavily relies on specific proprietary LLM behaviors, with uncertain performance across different model families
- 73.2% success rate achieved only on cybersecurity-focused prompts from CySecBench, with uncertain generalizability to other malicious intent categories
- Jury evaluation system addresses single-judge bias but may oversimplify nuanced quality assessments, with some generated code exhibiting minor logical inconsistencies

## Confidence
- **High Confidence**: Distributed segmentation demonstrably bypasses safety filters (validated through jury comparison showing 93.8% vs 73.2% discrepancy)
- **Medium Confidence**: Correlation between abstraction levels and effectiveness is well-supported within cybersecurity domain but may not extend to other categories
- **Low Confidence**: Framework's robustness across different LLM architectures remains unverified, and paper doesn't address potential adversarial defenses

## Next Checks
1. **Model Architecture Stress Test**: Substitute proprietary models with open-source alternatives (Llama, Mistral) while maintaining identical prompts and parameters to quantify performance degradation
2. **Cross-Domain Applicability**: Test framework on non-cybersecurity malicious prompts (social engineering, fraud) to validate whether abstraction-level correlation holds across different malicious intent types
3. **Detection Evasion Analysis**: Evaluate whether distributed processing pattern creates detectable signatures that could trigger enhanced safety mechanisms, measuring framework's sustainability against adaptive defenses