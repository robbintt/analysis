---
ver: rpa2
title: On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving
  Software
arxiv_id: '2504.02141'
source_url: https://arxiv.org/abs/2504.02141
tags:
- code
- generated
- software
- test
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simulation-guided pipeline for LLM-based
  code generation tailored to safety-critical autonomous driving systems. The core
  method employs an iterative feedback loop where generated Python code is evaluated
  in realistic traffic scenarios via the esmini simulator, and test reports are fed
  back to the LLM for refinement.
---

# On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software

## Quick Facts
- arXiv ID: 2504.02141
- Source URL: https://arxiv.org/abs/2504.02141
- Reference count: 40
- Primary result: Simulation-guided feedback improves GPT-4's ACC code generation success rate from 30% to 35%, with first successful CAEM baseline after 26 iterations

## Executive Summary
This paper introduces a simulation-guided pipeline for LLM-based code generation tailored to safety-critical autonomous driving systems. The core method employs an iterative feedback loop where generated Python code is evaluated in realistic traffic scenarios via the esmini simulator, and test reports are fed back to the LLM for refinement. The pipeline was tested with multiple models (Codellama, DeepSeek, CodeGemma, Mistral, and GPT-4) for Adaptive Cruise Control and Unsupervised Collision Avoidance by Evasive Manoeuvre. GPT-4 showed superior performance, with success rates increasing from 30% to 35% for ACC after correction, and producing the first fully successful baseline for CAEM after 26 iterations. Open-source models struggled with more complex tasks, and none successfully generated functional code for CAEM. An interview study with 11 ADS experts validated the tool's potential to reduce development workload while emphasizing the need for human oversight and robust test scenarios.

## Method Summary
The pipeline implements an iterative feedback loop for LLM code generation, where Python controllers are tested in esmini simulation against 7 traffic scenarios. A rule-based report generator translates numerical simulation logs into natural language test reports, which are fed back to the LLM via structured correction prompts. The process iterates through fresh code generation and feedback-based refinement until all safety requirements are met or iteration limits are reached. The method was evaluated across 5 LLMs for two ADS functions, with GPT-4 showing the most success in both initial generation and iterative improvement phases.

## Key Results
- GPT-4 improved ACC code success rate from 30% to 35% after correction feedback
- First successful CAEM baseline achieved after 26 iterations with GPT-4
- Open-source models (CodeLlama, DeepSeek, CodeGemma, Mistral) failed to generate functional CAEM code due to token limitations
- Average improvement of 9.2% in passed test cases across 14 correction iterations for ACC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative simulation-guided feedback improves LLM code generation success rates for safety-critical ADS functions
- Mechanism: LLM generates initial Python controller → esmini simulator tests against multiple traffic scenarios (TC1-TC7) → Rule-based feedback generator converts numerical logs to natural language test report → LLM receives correction prompt with failed test descriptions → Refined code generated and re-tested → Loop continues until all tests pass or iteration limit reached
- Core assumption: Simulation environment provides sufficiently realistic feedback to guide LLM toward functional solutions
- Evidence anchors:
  - [abstract] "GPT-4 showed superior performance, with success rates increasing from 30% to 35% for ACC after correction, and producing the first fully successful baseline for CAEM after 26 iterations"
  - [section 7.1] "the success rate for ACC increased from 30% (6 out of 20) in the first version to 35% (5 out of 14) in the correction loop"
  - [corpus] Similar iterative testing approaches found in neighbor papers on ADS testing, though limited direct comparison data available
- Break condition: Open-source models showed no improvement from correction loop due to token limitations, indicating mechanism effectiveness depends on model capacity

### Mechanism 2
- Claim: Natural language test reports enable more effective LLM self-correction than raw simulation data
- Mechanism: esmini generates tabular log files with vehicle positions/velocities per timestep → Python script translates numerical logs into structured natural language ("Ego was involved in a collision at time: 13.3 seconds with a speed of 33.33 m/s") → LLM processes actionable feedback in correction prompt → Targets specific safety violations (SR1-SR3)
- Core assumption: LLMs cannot reliably interpret raw numerical simulation data but can reason effectively about natural language descriptions of failures
- Evidence anchors:
  - [section 6.2] "Since the LLM is not able to reliably interpret these log files, a report generator was developed to translate the numerical logs into natural language"
  - [section 6.1] Correction prompt template includes "Test Results" with natural language descriptions of passed and failed test cases
  - [corpus] No direct corpus evidence for this specific translation mechanism
- Break condition: Assumption requires validation across different LLM architectures; may not generalize to future models with better numerical reasoning

### Mechanism 3
- Claim: Combining fresh code generation with feedback-based refinement produces higher success rates than either approach alone
- Mechanism: Pipeline initiates fresh generation (C₁, C₃, C₅...) → Each initial version potentially refined through feedback (C₂, C₄, C₆...) → Automatic baseline selection compares new versions against current best → Best-performing code becomes new baseline → Process continues horizontally (new approaches) and vertically (refinements) until golden baseline achieved
- Core assumption: Multiple independent solution attempts combined with targeted refinement increases probability of finding working solutions
- Evidence anchors:
  - [section 7.1] "In the 13th fresh start of the pipeline (t₁₃), the initial code version (C₂₅) passed 4 test cases. Through the feedback mechanism the next version (C₂₆) reached the mature stage and successfully passed all test cases"
  - [section 7.1] "average improvement of passed tests cases (P) of 9.2% of all 14 corrections and 37% considering only the improvements (5 corrections)"
  - [corpus] No corpus evidence on this hybrid approach strategy
- Break condition: Regressions occurred in 6 instances where correction attempts introduced non-executable code or runtime errors

## Foundational Learning

- Concept: **Software-in-the-Loop (SiL) Testing**
  - Why needed here: Understanding how simulation-based verification works as a preliminary validation step before hardware integration and human code review
  - Quick check question: Can you explain why SiL simulation provides faster feedback than physical testing while still catching safety violations?

- Concept: **Operational Design Domain (ODD)**
  - Why needed here: ADS functions must operate within defined ODD boundaries; understanding how test scenarios (TC1-TC7) map to ODD coverage is critical
  - Quick check question: How do the test cases in this paper constrain the ODD for CAEM versus ACC functions?

- Concept: **ISO 26262 Functional Safety Requirements**
  - Why needed here: Generated code must satisfy safety requirements (SR1-SR3) before deployment; understanding safety-critical software development processes
  - Quick check question: Why can't LLM-generated code bypass traditional verification and validation processes in automotive systems?

## Architecture Onboarding

- Component map: LLM Agent (code generation) → Python Controller → esmini Simulator (traffic scenario execution) → Log Files → Report Generator (numerical-to-text translation) → Natural Language Test Report → Feedback Loop back to LLM → Baseline Selector → User Interface (code + report delivery)

- Critical path: Prompt engineering (Specification/Correction templates) → Code extraction → Simulation execution (7 test cases) → Safety criteria evaluation (collision detection, lane boundaries, unintended behavior) → Report generation → LLM correction loop → Baseline comparison → Final delivery

- Design tradeoffs: (1) Token limits in open-source models prevent effective correction prompts vs. proprietary model dependency; (2) Python abstraction level enables rapid testing vs. production code requires C/C++; (3) Automated feedback reduces human effort vs. risk of converging on local optima without human oversight

- Failure signatures: (1) Non-compilable code (syntax errors, missing code blocks) - flagged automatically; (2) Runtime errors during specific scenarios - detected by simulation crash; (3) Safety violations (collisions, unintended lane changes) - caught by test criteria but may persist across iterations; (4) Regressions in correction loop - baseline selector prevents adoption

- First 3 experiments:
  1. Replicate ACC baseline with GPT-4: Set up pipeline with existing test cases (TC1-TC7), measure initial success rate vs. post-correction success rate, compare with reported 30%→35% improvement
  2. Test token limitation hypothesis: Run CodeGemma:7b with abbreviated correction prompts to test if reduced context causes failure in CAEM generation
  3. Validate report translation necessity: Ablation study comparing LLM performance with numerical logs vs. natural language reports for identical simulation failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating retrieval-based techniques to include certified legacy code and company-specific processes improve the quality and safety compliance of the generated code?
- Basis in paper: [explicit] The Conclusion states future work should explore "combining the proposed prototype with retrieval techniques... to use certified legacy codes and integrate company-specific processes."
- Why unresolved: The current prototype generates code based solely on natural language prompts without access to external knowledge bases or existing approved code libraries.
- What evidence would resolve it: An evaluation of a RAG-enhanced pipeline comparing the compliance and bug rates of generated code against the current baseline.

### Open Question 2
- Question: Can formal methods be effectively integrated into the pipeline to validate LLM-generated code for higher software reliability?
- Basis in paper: [explicit] The Conclusion notes that "employing formal methods... as proposed by P8, could improve software reliability."
- Why unresolved: The current prototype relies on simulation-based testing (SiL) and rule-based feedback, which cannot mathematically guarantee the absence of specific hazard classes.
- What evidence would resolve it: A comparative study measuring the detection rate of edge-case hazards in a pipeline augmented with formal verification versus the simulation-only approach.

### Open Question 3
- Question: Does the performance of the simulation-guided pipeline generalize to lower-level programming languages like C++ typically used in production ADS?
- Basis in paper: [inferred] The methodology selected Python for its "simple syntax" and "high-level constructs" at the function abstraction level, but real-world ADS often requires C++.
- Why unresolved: The paper acknowledges differences in LLM performance across languages (e.g., Copilot quality for Java vs. C), suggesting Python results may not directly transfer to the constraints of C++.
- What evidence would resolve it: Experimental results applying the identical pipeline and feedback mechanism to generate C++ code for the ACC and CAEM functions.

## Limitations

- Open-source LLMs failed to generate functional CAEM code due to token limitations, restricting accessibility
- Simplified Python abstraction may not translate to production C/C++ code requirements
- 7 test scenarios may not provide comprehensive ODD coverage for all driving conditions

## Confidence

- High confidence: GPT-4 superiority in ACC generation (30%→35% improvement) and CAEM breakthrough (26 iterations)
- Medium confidence: Effectiveness of natural language test reports for LLM comprehension
- Low confidence: Generalizability to other ADS functions beyond ACC and CAEM

## Next Checks

1. Test the pipeline with expanded test scenarios (30+ cases) to validate ODD coverage claims and identify potential blind spots in safety validation
2. Implement a controlled ablation study comparing LLM performance with numerical logs versus natural language reports across multiple model architectures
3. Evaluate the approach on production-level C/C++ code generation with integration into existing automotive development toolchains