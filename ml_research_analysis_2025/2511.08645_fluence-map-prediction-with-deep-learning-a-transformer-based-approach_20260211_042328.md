---
ver: rpa2
title: 'Fluence Map Prediction with Deep Learning: A Transformer-based Approach'
arxiv_id: '2511.08645'
source_url: https://arxiv.org/abs/2511.08645
tags:
- dose
- fluence
- clinical
- learning
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a deep learning framework for automated fluence
  map prediction in intensity-modulated radiation therapy (IMRT) for prostate cancer.
  The proposed 3D Swin-UNETR model directly predicts nine volumetric fluence maps
  from CT images and anatomical contours using hierarchical self-attention to capture
  both local anatomical structures and long-range spatial dependencies.
---

# Fluence Map Prediction with Deep Learning: A Transformer-based Approach

## Quick Facts
- arXiv ID: 2511.08645
- Source URL: https://arxiv.org/abs/2511.08645
- Reference count: 21
- Authors: Ujunwa Mgboh; Rafi Sultan; Dongxiao Zhu; Joshua Kim
- Primary result: 3D Swin-UNETR model predicts 9 volumetric fluence maps with R²=0.95±0.02, gamma passing rate of 85±10% (3%/3mm)

## Executive Summary
This study presents a deep learning framework for automated fluence map prediction in intensity-modulated radiation therapy (IMRT) for prostate cancer. The proposed 3D Swin-UNETR model directly predicts nine volumetric fluence maps from CT images and anatomical contours using hierarchical self-attention to capture both local anatomical structures and long-range spatial dependencies. Trained on 79 prostate IMRT cases and tested on 20 cases, the model achieved high dosimetric consistency with clinical reference plans, requiring only 3.97±0.25 seconds per patient and demonstrating potential as a clinically scalable, inverse-free solution for automated radiotherapy planning.

## Method Summary
The method employs a 3D Swin-UNETR architecture that takes CT images and anatomical contours as input to directly predict nine volumetric fluence maps for IMRT planning. The model uses hierarchical self-attention through a Swin Transformer encoder with shifted-window mechanism to capture both local anatomical details and long-range spatial relationships. A UNETR-style decoder with skip connections preserves voxel-level spatial fidelity during multi-scale feature fusion. The model was trained on 79 prostate IMRT cases from a single institution, resized to 128×128 resolution, and evaluated on 20 held-out cases using MSE loss and clinical metrics including gamma analysis and DVH parameters.

## Key Results
- Model achieved average R² of 0.95±0.02 and MAE of 0.035±0.008 on fluence map predictions
- Gamma passing rate of 85±10% (3%/3mm) indicates high dosimetric agreement with clinical plans
- No statistically significant differences in DVH parameters between predicted and clinical plans
- Prediction time of 3.97±0.25 seconds per patient demonstrates clinical scalability

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Self-Attention for Long-Range Dependencies
Hierarchical self-attention enables simultaneous capture of local anatomical structures and long-range spatial dependencies across the full 3D planning volume. The Swin Transformer backbone uses shifted windowing to partition volumetric input into localized windows for efficient attention computation, then shifts these windows between layers. This allows cross-window connections without quadratic complexity, letting the model relate distant anatomical regions (e.g., bladder-rectum-prostate relationships) that influence beam modulation patterns.

### Mechanism 2: End-to-End Anatomy-to-Fluence Mapping
The end-to-end mapping from anatomy to fluence eliminates error propagation from intermediate dose predictions. Rather than predicting dose first then inverting to fluence (error-prone inverse problem), the network directly learns the forward mapping from (CT + contours) → fluence. Ground-truth fluence maps from clinical plans provide direct supervision via MSE loss across all 9 beam channels simultaneously.

### Mechanism 3: Skip Connections for Spatial Fidelity
Skip connections preserve voxel-level spatial fidelity during multi-scale decoding. The UNet-style decoder receives features at multiple resolutions from the Swin encoder via residual skip connections. This allows the decoder to recover fine-grained anatomical boundaries while still benefiting from the encoder's global context, maintaining alignment between input contours and output fluence patterns.

## Foundational Learning

- **Self-Attention and Windowed Attention**: Understanding how Swin Transformer partitions input into windows and shifts them between layers explains why the model can capture long-range dependencies efficiently. Quick check: Can you explain why standard self-attention has O(n²) complexity and how window-based attention reduces this?

- **IMRT Fluence Maps and Inverse Planning**: Fluence maps represent beam intensity modulations; traditional planning solves an inverse optimization to find these. This model bypasses that iterative process entirely. Quick check: What does a fluence map represent physically, and why is the inverse problem (dose → fluence) ill-posed?

- **DVH Metrics and Gamma Analysis**: Clinical validation uses dose-volume histograms (D95, V70, etc.) and gamma passing rates to assess whether predicted plans are clinically acceptable. Quick check: What does a gamma passing rate of 85% at 3%/3mm indicate about dose distribution agreement?

## Architecture Onboarding

- **Component map**: Input (CT + contour masks) → Swin Transformer Encoder → Skip connections → CNN Decoder → Output (9 fluence maps) → Eclipse TPS → Dose recalculation → DVH evaluation

- **Critical path**: The Swin encoder's shifted-window attention blocks → skip connections → decoder upsampling. If attention windows don't adequately bridge anatomical regions, global context is lost; if skip connections fail, spatial precision degrades.

- **Design tradeoffs**: 128×128 downsampling sacrifices resolution for memory efficiency; fine MLC leaf details may be under-resolved. Single-institution training data limits generalizability (acknowledged in paper). MSE loss enforces voxel-wise accuracy but doesn't directly optimize clinical DVH metrics.

- **Failure signatures**: Gamma pass rate significantly below 85% → spatial misalignment or anatomical generalization failure. Statistically significant DVH deviations → model not capturing organ-sparing constraints. High variance across test cases → overfitting to training distribution.

- **First 3 experiments**: 
  1. Reproduce baseline: Train on provided 79/20 split with R² and gamma metrics; verify reported values within confidence intervals.
  2. Ablate skip connections: Remove or reduce skip pathways and measure MAE and gamma degradation to quantify their contribution.
  3. Cross-institutional holdout: If additional data available, test on external institution cases to assess generalizability claims.

## Open Questions the Paper Calls Out
None

## Limitations
- Model generalizability remains constrained by single-institution training data and fixed beam geometry assumptions
- 128×128 voxel resolution may undersample fine MLC leaf modulation patterns critical for complex cases
- Cross-institutional validation was not performed, limiting assessment of robustness to different IMRT delivery systems

## Confidence

- **High confidence**: Direct fluence prediction mechanism, skip connection preservation of spatial fidelity, MSE-based training with ground-truth supervision
- **Medium confidence**: Long-range dependency capture through shifted-window attention, end-to-end anatomy-to-fluence mapping eliminating dose-fluence error propagation, clinical plan equivalence based on DVH statistics
- **Low confidence**: Generalizability to external institutions, performance on anatomies outside training distribution, handling of complex MLC modulation patterns given resolution constraints

## Next Checks
1. Cross-institutional validation using independent prostate IMRT datasets to assess model robustness and generalizability claims
2. High-resolution testing with 256×256 or native resolution inputs to evaluate MLC modulation pattern fidelity and determine if resolution limits clinical acceptability
3. Ablation study removing shifted-window attention mechanism to quantify contribution of long-range dependency capture versus local attention alone