---
ver: rpa2
title: To Vaccinate or not to Vaccinate? Analyzing $\mathbb{X}$ Power over the Pandemic
arxiv_id: '2503.02563'
source_url: https://arxiv.org/abs/2503.02563
tags:
- tweets
- covid-19
- s-svdd
- data
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes public sentiment towards COVID-19 vaccination
  by processing approximately 400,000 tweets using natural language processing and
  sentiment analysis. The authors applied the VADER sentiment analysis tool to classify
  tweets as positive, negative, or neutral, and used One-Class Classifiers (OCCs)
  to distinguish between vaccine supporters and deniers.
---

# To Vaccinate or not to Vaccinate? Analyzing $\mathbb{X}$ Power over the Pandemic

## Quick Facts
- **arXiv ID**: 2503.02563
- **Source URL**: https://arxiv.org/abs/2503.02563
- **Reference count**: 31
- **Primary result**: S-SVDD classifiers achieved highest Geometric Mean (0.81 for positive, 0.79 for negative) on vaccine sentiment classification from 400K tweets

## Executive Summary
This study analyzes public sentiment toward COVID-19 vaccination using approximately 400,000 tweets processed through natural language processing and sentiment analysis. The authors apply VADER sentiment analysis to classify tweets as positive, negative, or neutral, then use One-Class Classifiers (OCCs) to distinguish vaccine supporters from deniers. While global sentiment was predominantly positive (45.1%), country-level analysis revealed significant variation, with the UK showing more positive sentiment than India, the US, and South Africa. The study demonstrates that subspace-based S-SVDD classifiers outperform traditional OCC methods, particularly the NS-SVDDψ2-max variant for positive tweets and S-SVDDψ1-min for negative tweets.

## Method Summary
The authors collected approximately 400,000 tweets from 2021 using Tweepy API with COVID-19 vaccine-related keywords. They applied VADER sentiment analysis to classify tweets into positive, negative, or neutral categories, then manually labeled 200 tweets (100 positive, 100 negative) by two domain experts. For OCC training, they implemented SVDD, ESVDD, OCSVM, and various S-SVDD variants with gradient descent and Newton-based optimization. The models were evaluated using 70/30 train-test splits with 5-fold cross-validation, selecting hyperparameters based on Geometric Mean (GM) performance.

## Key Results
- Global sentiment distribution: 45.1% positive, 30% negative, 24.9% neutral
- Country-level variation: UK showed highest positive sentiment compared to India, US, and South Africa
- S-SVDD classifiers outperformed traditional OCCs with GM values of 0.81 (positive class) and 0.79 (negative class)
- NS-SVDDψ2-max and S-SVDDψ1-min variants achieved the best performance for positive and negative classes respectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VADER sentiment analysis can classify tweet polarity without training data by leveraging rule-based heuristics attuned to social media language patterns.
- **Mechanism:** VADER employs a bag-of-words approach combined with intensity-modifying heuristics (e.g., "really," "very") to compute compound polarity scores ranging from -1 to +1, where neutral = 0, positive > 0, and negative < 0.
- **Core assumption:** Social media text follows predictable lexical patterns where sentiment-bearing words and modifiers correlate reliably with expressed opinions.
- **Evidence anchors:**
  - [abstract] "We applied the VADER sentiment analysis tool to classify tweets as positive, negative, or neutral"
  - [section 4] "VADER is a library that is pre-trained using rule-based values. It is particularly attuned to sentiments expressed in social media... performs very well on social media text without requiring any training data."
  - [corpus] Related papers confirm VADER's widespread use for COVID-19 tweet sentiment analysis (Crisis Messaging Journeys paper, SenWave dataset paper).
- **Break condition:** When tweets contain heavy sarcasm, domain-specific jargon, or multilingual code-switching, rule-based scoring may misclassify. The paper excludes non-English characters but does not address sarcasm explicitly.

### Mechanism 2
- **Claim:** Subspace-based One-Class Classifiers (S-SVDD) outperform traditional OCCs by learning optimized lower-dimensional projections that better capture class variance.
- **Mechanism:** S-SVDD minimizes a hypersphere's volume in a projected subspace defined by matrix Q, while a regularization term ψ captures class variance. The projection matrix is refined iteratively via gradient descent or Newton's method, allowing the model to adapt the feature space to the target class distribution.
- **Core assumption:** The target class (positive or negative tweets) has an underlying structure that can be captured more effectively in a learned subspace than in the original feature space.
- **Evidence anchors:**
  - [abstract] "S-SVDD classifiers, particularly NS-SVDDψ2-max for positive tweets and S-SVDDψ1-min for negative tweets, outperformed other OCC methods with the highest Geometric Mean (GM) values of 0.81 and 0.79 respectively"
  - [section 3] "S-SVDD incorporates a regularization term (ψ) that effectively captures class variance and adapts to the data distribution, providing a more refined boundary"
  - [corpus] Limited direct corpus validation for S-SVDD specifically on tweet data; related papers focus on sentiment analysis methods but not subspace OCC variants.
- **Break condition:** With only 100 training samples per class, overfitting risk is substantial. The paper uses 5-fold cross-validation but acknowledges standard deviations up to 0.16 on some metrics, indicating variability across splits.

### Mechanism 3
- **Claim:** Newton-based optimization (NS-SVDD) can outperform gradient-based S-SVDD by incorporating second-order Hessian information for faster convergence on certain class distributions.
- **Mechanism:** Newton's method uses curvature information (Hessian matrix) to adjust step sizes dynamically, potentially reaching better optima in fewer iterations compared to fixed learning rate gradient descent.
- **Core assumption:** The objective function landscape has sufficient curvature structure for second-order information to provide meaningful optimization advantages.
- **Evidence anchors:**
  - [section 2] "Unlike gradient descent, Newton's method incorporates second-order information through Hessian matrix, enabling different optimization of objective"
  - [section 5] "NS-SVDDψ2-max demonstrates superior performance compared to the other linear OCC... achieving a GM value of 0.81" for positive tweets
  - [corpus] No corpus validation; this appears to be a methodological contribution with limited external replication evidence.
- **Break condition:** For non-convex or poorly conditioned objective functions, Newton's method may diverge or converge to inferior local optima. The paper uses fixed 10 iterations with 0.1 learning rate without convergence analysis.

## Foundational Learning

- **Concept: One-Class Classification (OCC)**
  - **Why needed here:** Traditional binary classifiers require labeled examples from both classes. OCC enables learning from only the target class (e.g., "positive tweets") when negative examples are scarce, expensive to label, or ill-defined.
  - **Quick check question:** Given 100 labeled positive tweets and no negative examples, how would you train a classifier to distinguish supporters from deniers? What boundary would you learn?

- **Concept: Support Vector Data Description (SVDD)**
  - **Why needed here:** SVDD is the foundation for S-SVDD. It finds the minimum-volume hypersphere enclosing target class data, with slack variables allowing some outliers. Understanding this helps interpret why adding subspace learning (the "S" in S-SVDD) improves performance.
  - **Quick check question:** If you enclose all positive tweets in a hypersphere, what happens when a new tweet falls inside vs. outside the boundary? How do slack variables change this?

- **Concept: Geometric Mean (GM) as Evaluation Metric**
  - **Why needed here:** The paper selects hyperparameters based on GM = √(tpr × tnr). GM balances sensitivity and specificity, which is critical when class distributions may be skewed or when both false positives and false negatives carry cost.
  - **Quick check question:** Why use GM instead of accuracy when evaluating one-class classifiers? If tpr = 0.99 and tnr = 0.10, what is the GM, and what does this reveal about the classifier?

## Architecture Onboarding

- **Component map:**
  Data Ingestion (Tweepy API) -> Preprocessing Pipeline (cleaning, stemming/lemmatization) -> Sentiment Scoring (VADER/TextBlob) -> Manual Annotation (200 tweets) -> Feature Extraction -> OCC Training (SVDD variants) -> Evaluation (70/30 splits, 5-fold CV)

- **Critical path:**
  1. Tweet collection quality (keyword/hashtag coverage) determines data relevance
  2. Preprocessing consistency affects VADER scoring reliability
  3. Manual labeling quality (inter-annotator agreement not reported) determines OCC ground truth
  4. Hyperparameter selection (C, d, β, σ) via GM determines final model performance

- **Design tradeoffs:**
  - **VADER vs. TextBlob**: Paper uses both but reports VADER results. VADER is optimized for social media; TextBlob is more general. Trade-off: VADER's heuristics may miss domain-specific expressions.
  - **Linear vs. Non-linear OCC**: Non-linear (kernel) methods capture complex boundaries but risk overfitting with 100 samples. Paper reports both; best results vary by class.
  - **Manual labeling scale**: 200 tweets enables rapid experimentation but limits statistical power. Paper acknowledges this is a pilot-scale study.

- **Failure signatures:**
  - **Class confusion**: High tpr but low tnr indicates boundary too loose (many false positives). Check hypersphere radius and regularization weight.
  - **High variance across splits**: Standard deviation > 0.10 on GM suggests unstable model. Increase training data or reduce model complexity.
  - **OCSVM underperformance**: GM = 0.41-0.43 indicates hyperplane-based separation unsuitable for this data distribution. Use SVDD-based methods instead.

- **First 3 experiments:**
  1. **Reproduce VADER sentiment distribution**: Collect 1,000 tweets using paper's hashtags, apply VADER preprocessing pipeline, compute polarity distribution. Compare to paper's 45.1% positive, 30% negative, 24.9% neutral.
  2. **Ablation on regularization term ψ**: Train S-SVDDψ1 through ψ4 on paper's manually labeled data (or new annotation). Isolate which ψ variant contributes most to GM improvement over baseline SVDD.
  3. **Sample size sensitivity**: Train NS-SVDDψ2-max with 50, 100, 150, and 200 positive samples. Plot GM vs. sample size to identify minimum viable training set for stable performance.

## Open Questions the Paper Calls Out

- **Question 1:** How does the performance of S-SVDD classifiers scale when trained on significantly larger labeled datasets compared to the 200 manually labeled tweets used in this study?
  - **Basis in paper:** The authors acknowledge manually labeling only 100 positive and 100 negative tweets (Page 7) to train the One-Class Classifiers, representing a very small fraction of the 400,000 collected tweets.
  - **Why unresolved:** It is unclear if the high Geometric Mean (GM) scores (0.81 and 0.79) achieved by the S-SVDD variants are robust or if they would degrade/improve with a more representative training sample.
  - **What evidence would resolve it:** Benchmarking the S-SVDD models using larger training sets (e.g., 1,000 or 10,000 labeled samples) to observe the trajectory of performance metrics.

- **Question 2:** To what extent does the exclusion of non-English tweets bias the country-level sentiment analysis, particularly for multilingual nations like India and South Africa?
  - **Basis in paper:** The methodology states that non-English characters were excluded during preprocessing (Page 6), yet the country-level analysis includes non-native English speaking countries (Page 9).
  - **Why unresolved:** The paper does not quantify the volume of discarded data for these specific countries, leaving a potential gap between the reported sentiment and the actual public perception in those regions.
  - **What evidence would resolve it:** A comparison of sentiment distributions between English-only tweets and native-language tweets for the analyzed countries.

- **Question 3:** How stable are the S-SVDD decision boundaries over time as public discourse shifts from initial vaccine roll-out to booster shots or variant-specific concerns?
  - **Basis in paper:** The data was collected from 2021 (Page 6), and the authors suggest the work can assist in "timely management" of future campaigns (Page 12).
  - **Why unresolved:** The concept of a "vaccine supporter" or "denier" is dynamic; the classifier models a static snapshot, and it is unknown if the "positive class" boundary drifts as topics change.
  - **What evidence would resolve it:** Temporal validation experiments testing the 2021-trained models on tweets from later phases of the pandemic to measure concept drift.

## Limitations

- The study's OCC results are based on only 100 manually labeled examples per class, creating substantial risk of overfitting and high variance in performance estimates
- Feature extraction methodology for OCC training is unspecified, making exact reproduction difficult and potentially affecting comparative performance
- The superiority of Newton-based NS-SVDD over gradient-based S-SVDD lacks external validation across different datasets or domains

## Confidence

- **High confidence**: VADER sentiment classification methodology and global sentiment distribution (45.1% positive, 30% negative, 24.9% neutral)
- **Medium confidence**: OCC performance rankings (S-SVDD > SVDD/ESVDD > OCSVM) due to limited labeled data
- **Low confidence**: NS-SVDD optimization claims without convergence analysis or external dataset validation

## Next Checks

1. Collect and preprocess a new sample of 1,000 vaccine-related tweets; reproduce VADER sentiment distribution and verify preprocessing pipeline consistency
2. Implement all OCC variants (SVDD, ESVDD, OCSVM, S-SVDD, NS-SVDD) on the original manually labeled data; report hyperparameter sensitivity and cross-validation stability
3. Test OCC performance sensitivity to training set size (50, 100, 150, 200 samples per class) to establish minimum viable sample requirements