---
ver: rpa2
title: Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks
arxiv_id: '2505.05375'
source_url: https://arxiv.org/abs/2505.05375
tags:
- adaptation
- learning
- threshold
- neural
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Threshold Modulation (TM), a novel online
  test-time adaptation framework specifically designed for spiking neural networks
  (SNNs) deployed on neuromorphic chips. The key innovation is dynamically adjusting
  neuronal firing thresholds through normalization-inspired techniques, enabling models
  to adapt to distribution shifts without requiring source data or labels, while maintaining
  compatibility with neuromorphic hardware constraints.
---

# Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks

## Quick Facts
- arXiv ID: 2505.05375
- Source URL: https://arxiv.org/abs/2505.05375
- Reference count: 40
- Primary result: Dynamic threshold adjustment via normalization reduces running error by 20-30% on corrupted datasets with only 3% energy overhead

## Executive Summary
This paper introduces Threshold Modulation (TM), an online test-time adaptation framework for spiking neural networks (SNNs) deployed on neuromorphic chips. TM dynamically adjusts neuronal firing thresholds through normalization-inspired techniques, enabling models to adapt to distribution shifts without requiring source data or labels. The method achieves significant performance improvements on benchmark datasets while maintaining compatibility with neuromorphic hardware constraints and adding minimal computational overhead.

## Method Summary
TM operates by calibrating membrane potential statistics during inference using exponential moving averages of mean and variance per channel. The firing threshold is re-parameterized to absorb batch normalization parameters, eliminating the need for BN layers at test time. The framework supports two variants: TM-NORM (statistics-only) and TM-ENT (with entropy minimization for affine parameter updates). Pre-training uses Membrane Potential Batch Normalization (MPBN), which is fused into thresholds during deployment, making the approach suitable for resource-constrained neuromorphic hardware.

## Key Results
- Running error rates reduced by 20-30% across various corruption types on CIFAR-10/100-C and ImageNet-C
- Energy consumption increased by only 3% compared to baseline inference, versus 12% for traditional normalization calibration
- TM-NORM often matches or exceeds TM-ENT performance while being ~7.5× more energy-efficient
- Effective adaptation demonstrated across digit recognition transfer tasks (SVHN → MNIST/MNIST-M/USPS)

## Why This Works (Mechanism)

### Mechanism 1
Distribution shifts manifest as shifts in membrane potential statistics. TM normalizes these statistics at test time using running mean and variance estimates, correcting covariate shift by dynamically recalibrating what constitutes a "high" membrane potential. This alignment restores performance degraded by corrupted or shifted inputs.

### Mechanism 2
TM converts batch normalization into a threshold operation through re-parameterization. During deployment, MPBN layers are fused into per-channel thresholds, eliminating the need for separate BN layers that cannot be easily implemented on neuromorphic chips. This equivalence holds for single time steps and empirically extends to multi-step inference.

### Mechanism 3
Entropy minimization on affine parameters provides marginal gains at substantial computational cost. While TM-ENT can sharpen predictions through gradient updates, the simpler TM-NORM variant achieves comparable or better results with far less energy consumption, making it preferable for edge deployment.

## Foundational Learning

- **Leaky Integrate-and-Fire (LIF) Neuron Dynamics**: TM operates within LIF neurons; understanding charging (ht = Xt + 1/τ · ut-1), firing, and reset is prerequisite. Quick check: If τ increases, does the membrane potential retain more or less of its prior state? (Answer: more)

- **Batch Normalization Statistics**: TM transfers the "statistics calibration" idea from BN layers to neuronal thresholds; understanding how μ and σ² are estimated and why they matter is essential. Quick check: Why does covariate shift in inputs propagate to internal activations? (Answer: Layer outputs depend on input distribution; if inputs shift, pre-activation statistics shift)

- **Test-Time Adaptation without Labels**: The paper assumes no access to source data or target labels; adaptation must be self-supervised (entropy minimization) or purely statistical (BN calibration / threshold modulation). Quick check: Why can't we just fine-tune on target data? (Answer: No labels, no source data, and on-chip constraints prohibit storing gradients or large batches)

## Architecture Onboarding

- **Component map**: Pre-training with MPBN → Fuse BN into thresholds → Deployment with TM module → Optional entropy backpropagation
- **Critical path**: Pre-train SNN with MPBN → Fuse MPBN into thresholds → At test time, compute ht statistics → Update μ̂, σ̂² → Compute fVth → Fire/reset as usual → (Optional) Backprop entropy loss to update γ, β
- **Design tradeoffs**: TM-NORM vs TM-ENT (energy efficiency vs marginal gains), r=0 vs r=1 (strict equivalence vs practical simplicity), ρ₀ and ω (adaptation speed vs stability)
- **Failure signatures**: Unstable thresholds (oscillation in fVth), model collapse (affine parameters diverge), no improvement (distribution shift not captured in membrane potential statistics)
- **First 3 experiments**: 1) Sanity check on clean CIFAR-10 (verify no degradation vs source), 2) Single corruption type (Gaussian noise on CIFAR-10-C, plot running accuracy), 3) Ablation of ρ₀ (test with ρ₀ ∈ {0.5, 0.9, 1.0} on fog corruption)

## Open Questions the Paper Calls Out

- **Integration without MPBN**: How can TM be integrated into SNNs without relying on the Membrane Potential Batch Normalization module? The current implementation depends on MPBN for threshold re-parameterization, but alternative integration methods remain unexplored.

- **Continuous online adaptation**: How does TM perform under continuous online adaptation scenarios with streaming distribution shifts? Current experiments use fixed corrupted datasets, while real-world deployment involves continuously evolving distributions requiring anti-forgetting mechanisms.

- **Hardware deployment validation**: What is the actual energy consumption and latency overhead when deployed on physical neuromorphic hardware? Current estimates rely on theoretical MACs/ACs/MULs counts rather than measurements from actual chips like Loihi or Darwin3.

- **Cross-modal generalization**: Can TM effectively adapt SNNs to distribution shifts beyond image classification, such as event-based vision, speech, or control tasks? All experiments are limited to static image corruption benchmarks, leaving temporal task performance unexplored.

## Limitations

- Theoretical equivalence between MPBN and threshold modulation is only strictly proven for single time steps (T=1); multi-step inference with r=0 lacks formal validation
- Energy consumption measurements rely on computational counts rather than actual hardware measurements, potentially missing neuromorphic-specific optimizations
- TM-ENT's marginal gains come at substantial computational cost, with limited evidence of superiority over TM-NORM across diverse distribution shifts

## Confidence

- **High**: Threshold modulation effectively reduces running error on benchmark corruption datasets; the normalization-inspired approach is technically sound
- **Medium**: The equivalence re-parameterization is correct for T=1 but extrapolation to multi-step inference requires empirical validation
- **Medium**: Energy efficiency claims are based on computational counts; actual neuromorphic hardware measurements would strengthen the evidence

## Next Checks

1. Verify threshold stability across heterogeneous corruption batches by measuring variance of fVth over time; identify potential oscillation patterns
2. Test TM-ENT with lower learning rates (e.g., 0.000125) and shorter adaptation windows to quantify the model collapse risk more precisely
3. Implement the full T>1 case with r=1 (normalize non-firing potentials) and compare against r=0 to validate the practical equivalence claim for corruption benchmarks