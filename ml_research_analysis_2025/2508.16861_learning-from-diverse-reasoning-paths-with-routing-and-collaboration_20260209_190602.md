---
ver: rpa2
title: Learning from Diverse Reasoning Paths with Routing and Collaboration
arxiv_id: '2508.16861'
source_url: https://arxiv.org/abs/2508.16861
tags:
- reasoning
- arxiv
- preprint
- distillation
- paths
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of knowledge distillation from
  large language models (LLMs) to smaller, more efficient student models, specifically
  focusing on improving the student's reasoning abilities. Traditional distillation
  methods struggle to capture the full reasoning process of the teacher model due
  to token-level supervision limitations.
---

# Learning from Diverse Reasoning Paths with Routing and Collaboration

## Quick Facts
- arXiv ID: 2508.16861
- Source URL: https://arxiv.org/abs/2508.16861
- Reference count: 28
- Primary result: Proposed QR-Distill framework improves student performance by up to 41.44% on average compared to zero-shot prompting and by up to 13.36% compared to multi-path distillation baselines.

## Executive Summary
This paper addresses the challenge of distilling reasoning capabilities from large language models (LLMs) to smaller, more efficient student models. Traditional distillation methods struggle to capture the full reasoning process due to token-level supervision limitations. The authors propose Quality-filtered Routing with Cooperative Distillation (QR-Distill), a framework that filters out incorrect reasoning paths, dynamically routes the remaining high-quality paths to the most suitable students based on their learning state, and enables mutual knowledge transfer among students through a collaborative peer-teaching mechanism.

## Method Summary
QR-Distill generates diverse reasoning paths using a teacher LLM (Gemini-1.5-Pro) with six prompt styles (Vanilla, CoT, ToT, Program, Backward, Fact Retrieval). Paths undergo two-stage quality filtering: first removing paths with incorrect final answers, then using an LLM-as-judge to discard spurious intermediate steps. A RoBERTa-base encoder + MLP router assigns filtered paths to students (Mistral-7B-Instruct, Gemma-7B-Instruct) using Gumbel-Softmax for differentiable routing. Students train on assigned paths using a combined loss: SFT (QLoRA) + Entropy Regularization + Mutual-Student Distillation, where students project hidden states into a shared space and align to an ensemble representation.

## Key Results
- QR-Distill improves student performance by up to 41.44% on average compared to zero-shot prompting
- QR-Distill improves student performance by up to 13.36% compared to multi-path distillation baselines without routing
- QR-Distill shows improved sample efficiency, achieving comparable performance with only 30% of the training data in some cases

## Why This Works (Mechanism)

### Mechanism 1: Quality Filtering Reduces Harmful Supervision Signals
The LLM-as-judge component removes paths with incorrect final answers and spurious intermediate steps, ensuring only high-quality reasoning traces are used for training. This prevents misleading supervision signals from corrupting student learning. Break condition: If the LLM-judge misclassifies valid reasoning as spurious or fails to catch subtle hallucinations, filtering could remove useful data or retain harmful samples.

### Mechanism 2: Conditional Routing Matches Paths to Student Compatibility
The trainable MLP router uses Gumbel-Softmax to produce differentiable, discrete assignments of paths to students based on their current learning state and path suitability. Different student architectures benefit differentially from various reasoning styles (e.g., program-based vs. tree-of-thought). Break condition: If the router fails to learn meaningful compatibility or overfits to certain paths, the benefits of adaptive routing would be lost.

### Mechanism 3: Mutual-Student Distillation Closes Knowledge Gaps
Students project their hidden states into a shared space, compute competence scores to form a soft ensemble representation, and align their representations to this ensemble via mean-squared error loss. This enables knowledge transfer between peers, compensating for gaps in teacher coverage and reducing reasoning-style bias. Break condition: If students converge to similar representations without diversity, or if the ensemble amplifies shared errors, mutual distillation may not provide diverse insights and could degrade performance.

## Foundational Learning

- **Concept: Knowledge Distillation (Black-box)**
  - Why needed here: Transfer reasoning ability from a black-box LLM teacher (only API/text access) to smaller student models without logit-level access
  - Quick check question: Can you explain why token-level supervision from teacher outputs is limited compared to full logit-based distillation?

- **Concept: Chain-of-Thought (CoT) and Multi-Path Reasoning**
  - Why needed here: Multiple, diverse reasoning paths per query provide richer supervision than a single chain
  - Quick check question: Why might a single reasoning path per query be insufficient to capture a teacher's full problem-solving process?

- **Concept: Gumbel-Softmax for Differentiable Discrete Routing**
  - Why needed here: The conditional router uses Gumbel-Softmax to make discrete path-to-student assignments while remaining differentiable for end-to-end training
  - Quick check question: How does Gumbel-Softmax allow backpropagation through discrete routing decisions?

## Architecture Onboarding

- **Component map:** Teacher (Gemini-1.5-Pro-001) -> LLM-as-Judge (J) -> Encoder (RoBERTa-base) -> Router (MLP + Gumbel-Softmax) -> Students (Mistral-7B-Instruct, Gemma-7B-Instruct) -> Mutual Distillation (ensemble representation)

- **Critical path:** 1. Teacher generates diverse reasoning paths. 2. Paths undergo quality filtering (incorrect answers + spurious steps). 3. Router assigns remaining paths to students. 4. Students train on assigned paths (SFT loss) and engage in mutual representation distillation.

- **Design tradeoffs:** Number of reasoning paths per query (more paths increase diversity but raise computational cost and potential noise); Router complexity (more complex router could capture finer compatibility but risks overfitting and longer training); Number of students (more students could enhance mutual distillation but require more resources).

- **Failure signatures:** Router collapse (assigns all paths to one student or makes near-random assignments); Mutual distillation degradation (students' performance drops if ensemble representation is noisy or amplifies errors); Filter over-pruning (overly aggressive filtering removes useful reasoning paths, limiting training data).

- **First 3 experiments:** 1. Ablation of Components: Remove quality filtering, routing, and mutual distillation one at a time to measure each component's contribution. 2. Routing Analysis Visualization: Track routing decisions across different datasets and student architectures to verify path-student compatibility patterns. 3. Sample Efficiency Test: Train QR-Distill and SFT baseline on varying percentages (e.g., 30%, 60%, 100%) of training data to compare data efficiency.

## Open Questions the Paper Calls Out

1. Does increasing the number of collaborative student models beyond two yield further performance gains, or does it introduce diminishing returns or optimization conflicts? The authors note computational resource constraints limited experiments to two students, but increasing the number holds huge potential for further gains.

2. Does integrating reasoning paths generated by multiple diverse teacher models (e.g., GPT-4 alongside Gemini) improve student generalization capabilities? The framework currently relies on a single teacher source (Gemini-1.5-Pro-001), potentially limiting the diversity of the initial reasoning path pool.

3. Can exploring a wider set of reasoning prompt templates or automatic prompt optimization further enrich the training signals for the distillation framework? The path generation relies on a fixed set of categories, potentially missing other valid reasoning styles beneficial for specific student architectures.

## Limitations
- The quality-filtering mechanism's effectiveness hinges critically on the reliability of the LLM-as-judge component, which is not specified in detail
- The routing mechanism's complexity scales with the number of students and reasoning paths, raising questions about computational efficiency and potential overfitting in larger configurations
- The mutual distillation component assumes students can provide diverse, complementary knowledge, but if students converge to similar representations, this mechanism could amplify shared errors rather than reduce them

## Confidence
- **High confidence**: The empirical results showing QR-Distill outperforming baselines across multiple reasoning benchmarks, and the basic effectiveness of removing incorrect reasoning paths from training data
- **Medium confidence**: The claims about conditional routing providing benefits beyond simple random assignment, and the effectiveness of mutual-student distillation in closing knowledge gaps, due to limited ablation studies and weak corpus support for the peer-teaching mechanism
- **Low confidence**: The scalability claims beyond the tested 2-student configuration and the specific impact of the LLM-as-judge quality filtering given the lack of detail about the judging prompts and potential biases

## Next Checks
1. **Router Assignment Analysis**: Conduct detailed analysis of routing assignments across varying student counts and reasoning path types to verify that the router learns meaningful compatibility patterns rather than making random or collapsed assignments.

2. **Filter Robustness Test**: Systematically evaluate the impact of different quality filtering strategies (e.g., varying judge criteria, filtering thresholds) on student performance to determine the sensitivity of the method to filtering quality and potential over-pruning of useful data.

3. **Mutual Distillation Diversity Check**: Measure the diversity of student representations throughout training and analyze whether mutual distillation maintains or increases diversity, or if students converge to similar representations that could amplify errors rather than provide complementary knowledge.