---
ver: rpa2
title: Multimodal Deep Learning for Subtype Classification in Breast Cancer Using
  Histopathological Images and Gene Expression Data
arxiv_id: '2503.02849'
source_url: https://arxiv.org/abs/2503.02849
tags:
- cancer
- classification
- fusion
- gene
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a deep multimodal learning framework for breast
  cancer subtype classification, integrating histopathological images and gene expression
  data. The approach uses a ResNet-50 model for image feature extraction and fully
  connected layers for gene expression processing, with a cross-attention fusion mechanism
  to enhance modality interaction.
---

# Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data

## Quick Facts
- arXiv ID: 2503.02849
- Source URL: https://arxiv.org/abs/2503.02849
- Reference count: 17
- Primary result: Multimodal integration achieved F1=0.9379 and PR-AUC=0.9948, outperforming unimodal approaches

## Executive Summary
This study presents a deep multimodal learning framework that integrates histopathological images and gene expression data for breast cancer subtype classification. The approach uses a ResNet-50 model for image feature extraction and fully connected layers for gene expression processing, with a cross-attention fusion mechanism to enhance modality interaction. Using five-fold cross-validation, the multimodal integration achieved superior performance compared to unimodal approaches, demonstrating that combining histopathological and genomic features significantly improves predictive accuracy for breast cancer subtype classification.

## Method Summary
The method extracts 35×256×256 patches per patient from whole-slide images using Otsu's thresholding, then processes them through a ResNet-50 feature extractor. Gene expression data undergoes log transformation, Z-score normalization, and IQR outlier removal before encoding through fully connected layers. Three fusion strategies are compared: concatenation, late fusion, and cross-attention with multi-head attention. The model is trained using Adam optimizer with binary cross-entropy loss, with color jitter and rotation augmentations applied to images.

## Key Results
- Multimodal integration achieved F1-score of 0.9379 and PR-AUC of 0.9948 using cross-attention fusion
- Concatenation fusion model achieved F1-score of 0.8960 and PR-AUC of 0.9684
- Late fusion approach yielded lower performance, confirming feature-level integration superiority
- Unimodal gene expression achieved F1=0.8197 while histopathology alone achieved F1=0.1780

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention fusion outperforms simpler fusion strategies by dynamically weighting modality-specific features. The multi-head attention mechanism allows the model to learn which features from histopathology vs. gene expression are most informative for each sample, creating aligned cross-modal representations rather than static concatenation. This works because features from both modalities contain complementary information that benefits from sample-specific weighting rather than fixed fusion.

### Mechanism 2
Multimodal integration compensates for weak unimodal performance through complementary information. Gene expression provides strong molecular signal (F1=0.8197) while histopathology alone is weak (F1=0.1780), but combining them yields F1=0.9379—suggesting images provide contextual features that resolve genomic ambiguities. This works because histopathological images contain morphological patterns correlated with molecular subtypes that, while insufficient alone, disambiguate gene expression predictions.

### Mechanism 3
Feature-level fusion outperforms decision-level fusion for this classification task. Concatenation and cross-attention fuse learned representations before classification, allowing joint feature learning; late fusion combines independent predictions, preventing cross-modal feature interaction. This works because joint representation learning enables discovery of cross-modal correlations that decision-level aggregation cannot capture.

## Foundational Learning

- **Multi-head cross-attention**: Core fusion mechanism; understanding Query/Key/Value interactions explains how image and gene features align dynamically. Quick check: Can you explain why cross-attention (different modalities as Q and K) differs from self-attention (same modality)?

- **Transfer learning with CNN feature extractors**: ResNet-50 is pretrained on ImageNet; understanding what transfers (edges, textures) vs. what doesn't (histopathology-specific patterns) informs fine-tuning strategy. Quick check: Why might ImageNet-pretrained features still help for histopathology despite domain shift?

- **Gene expression preprocessing (log transform, Z-score normalization)**: Raw expression values span orders of magnitude; improper normalization destabilizes training and attention weighting. Quick check: What problem does log transformation solve for RNA-seq-like data?

## Architecture Onboarding

- **Component map**: WSIPatches → ResNet-50 (ImageNet pretrained) → 2048-dim features per patch → aggregation → image embedding; Expression vector → log transform + Z-score → FC layers → gene embedding; Cross-attention (image as Q, gene as K,V or bidirectional) → fused representation; FC layers → sigmoid → binary subtype prediction

- **Critical path**: Align patient IDs across modalities (missing modalities break training); Preprocess gene expression (IQR outlier removal, normalization) before FC encoding; Ensure image patches pass Otsu's thresholding (low-content patches degrade features); Apply cross-attention before classification head

- **Design tradeoffs**: Concatenation vs. cross-attention: Simpler implementation vs. +4% F1 gain; Image patch count (35 per patient): More patches increase compute but may capture heterogeneity; Pretrained vs. frozen ResNet: Fine-tuning improves features but risks overfitting on small datasets

- **Failure signatures**: Histopathology unimodal F1≈0.18: Image features alone insufficient—expected, not a bug; Overfitting on gene modality: If gene pathway trains faster, use differential learning rates or gradient clipping; Class imbalance in BRCA.Basal/Her2: Confusion matrix shows moderate misclassification; consider weighted loss or focal loss; NaN losses: Check for remaining outliers in gene expression after IQR filtering

- **First 3 experiments**: Baseline unimodal reproduction: Train gene-only and image-only models to verify F1≈0.82 and F1≈0.18 before multimodal integration; Fusion strategy ablation: Compare concatenation, late fusion, and cross-attention head-to-head on same folds to confirm reported ranking; Attention visualization: Extract attention weights from cross-attention layer to identify which gene expression features attend to which image regions—critical for interpretability claims

## Open Questions the Paper Calls Out

- **Generalization to external cohorts**: The framework's effectiveness on independent clinical cohorts remains untested despite current dataset size constraints.

- **Transformer-based fusion models**: Whether transformer architectures could outperform the current cross-attention mechanism in subtype classification accuracy.

- **BRCA.Basal/Her2 subtype optimization**: What specific architectural or data balancing modifications can improve recall for the BRCA.Basal/Her2 subtype.

- **Interpretability for clinical decision-making**: How interpretability techniques can elucidate biological interactions between histopathological and genomic features to support clinical adoption.

## Limitations
- Dataset size constraints limit model generalizability and require external validation on independent cohorts
- Underspecified architectural details including gene expression input dimensions, FC layer configurations, and cross-attention hyperparameters
- Multimodal dataset availability and patient-level alignment requirements present practical reproducibility barriers

## Confidence
- **High confidence**: Multimodal advantage claim (F1=0.9379 vs unimodal baselines), supported by direct experimental comparisons and ablation results
- **Medium confidence**: Cross-attention superiority over concatenation, as architectural specifics differ beyond fusion strategy alone
- **Low confidence**: Generalizability beyond specific breast cancer subtypes studied, given dataset specificity and limited external validation

## Next Checks
1. Replicate unimodal baselines (gene-only F1≈0.82, image-only F1≈0.18) to verify data preprocessing pipeline correctness
2. Implement ablation comparing all three fusion strategies (concatenation, late, cross-attention) under identical training conditions
3. Perform attention weight visualization to validate interpretability claims and identify which gene features attend to specific histopathological patterns