---
ver: rpa2
title: 'ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory'
arxiv_id: '2601.21545'
source_url: https://arxiv.org/abs/2601.21545
tags:
- tier
- shard
- retrieval
- memory
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShardMemo addresses the challenge of scalable memory access in
  agentic LLM systems by introducing a tiered architecture with scope-correct and
  budgeted retrieval over sharded storage. It enforces scope-before-routing by masking
  ineligible shards prior to routing, casts shard probing as masked mixture-of-experts
  routing under a hard probe cap, and uses cost-aware gating over heterogeneous shard
  families.
---

# ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory

## Quick Facts
- **arXiv ID:** 2601.21545
- **Source URL:** https://arxiv.org/abs/2601.21545
- **Reference count:** 11
- **Primary result:** ShardMemo improves F1 by +5.11 to +6.82 over GAM across question categories, with 20.5% fewer VecScan operations and 19ms p95 latency reduction.

## Executive Summary
ShardMemo addresses the challenge of scalable memory access in agentic LLM systems by introducing a tiered architecture with scope-correct and budgeted retrieval over sharded storage. It enforces scope-before-routing by masking ineligible shards prior to routing, casts shard probing as masked mixture-of-experts routing under a hard probe cap, and uses cost-aware gating over heterogeneous shard families. When evidence-to-shard supervision is available, it trains the router with a multi-positive set-likelihood objective. The system achieves significant improvements in both retrieval accuracy and efficiency compared to existing methods.

## Method Summary
ShardMemo implements a three-tier architecture: Tier A for working memory, Tier B for sharded evidence with local ANN indexes, and Tier C for versioned skill libraries. The core innovation is a masked MoE router that enforces scope-before-routing by applying eligibility constraints before routing decisions. The router uses cost-aware gating to prioritize low-cost shard families and performs budgeted sparse probing via Top-B_probe or adaptive Top-P selection. When evidence-to-shard labels are available, the router is trained with a multi-positive set-likelihood objective. The system handles heterogeneous shard families (profiles, observations, sessions) with different access costs and utility profiles.

## Key Results
- Improves F1 by +5.11 to +6.82 over GAM across question categories on LoCoMo
- Under B_probe=3 budget, improves over cosine-to-prototype by +6.87 F1 while reducing VecScan by 20.5% (521→414)
- Achieves 63.41/61.88/57.95 F1 on HotpotQA at 56K/224K/448K tokens
- On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval)

## Why This Works (Mechanism)

### Mechanism 1: Scope-Before-Routing via Hard Masking
The system applies a scope predicate ψ^t_B to identify eligible shards S_t before routing. Ineligible shards are masked by setting logits to -∞ before the softmax, ensuring they are never selected for probing. This guarantees scope correctness and reduces retrieval work compared to post-hoc filtering.

### Mechanism 2: Cost-Aware Gating over Heterogeneous Families
The router adjusts scores by a cost penalty s_{t,j} ← s_{t,j} - α c_{t,j}, prioritizing low-cost shards (profiles/summaries) over expensive ones (session traces). This improves latency without significantly sacrificing accuracy by escalating to expensive shards only when semantic scores justify the cost.

### Mechanism 3: Budgeted Sparse Probing (MoE Casting)
Shard selection is cast as a Top-B_probe MoE problem, balancing recall against fixed retrieval budgets. Instead of scanning all shards, the router selects a sparse probe set P_t (size ≤ B_probe) using Top-B or adaptive Top-P, executing local ANN searches only on selected shards.

## Foundational Learning

- **Concept:** Mixture-of-Experts (MoE) Routing
  - **Why needed:** The core innovation treats shards as "experts" selected by a sparse gate
  - **Quick check:** How does a Top-K gate differ from a standard dense layer in terms of computational sparsity?

- **Concept:** Approximate Nearest Neighbor (ANN) Indexing
  - **Why needed:** Each shard uses a local ANN index, trading exact recall for speed
  - **Quick check:** Why is ANN preferred over exact KNN for high-dimensional vector search in production?

- **Concept:** Multi-label Set Likelihood
  - **Why needed:** The router is trained using a multi-positive objective because queries can validly retrieve from multiple shards
  - **Quick check:** In the loss function -log Σ p_{t,j}, why sum probabilities before taking the log rather than summing individual log-losses?

## Architecture Onboarding

- **Component map:** Tier Gate → (if B) Embed Query → Apply Scope Mask → Cost-Aware Router → Select Top-B_probe Shards → Parallel Local ANN → Global Merge/Filter
- **Critical path:** Query → Tier Gate → (if B) Embed Query → Apply Scope Mask → Cost-Aware Router → Select Top-B_probe Shards → Parallel Local ANN → Global Merge/Filter
- **Design tradeoffs:** Fixed Top-B provides predictability vs. Adaptive Top-P varying probe count based on confidence; requires evidence-to-shard labels for optimal training
- **Failure signatures:** High latency + Low recall suggests masking logic failure or insufficient probe budget; scope leakage indicates post-hoc filtering failure
- **First 3 experiments:**
  1. Run untrained cosine-similarity routing vs. Masked MoE router on held-out logs to measure supervision gap
  2. Vary B_probe ∈ {1, 2, 4, 8} to find F1 vs. Latency knee point for specific latency SLO
  3. Disable scope-masking and measure increase in VecScan and p95 latency to quantify efficiency gain

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the ShardMemo router adapt to workload drift over time without requiring continuous evidence→shard supervision? The paper acknowledges workload drift as a motivation but doesn't propose adaptation mechanisms beyond fixed shard maps with supervised training.

- **Open Question 2:** What is the optimal strategy for dynamic shard management (split, merge, rebalance) under ShardMemo's masked routing framework? The paper notes real systems require dynamic reorganization but excludes it from evaluation, leaving effectiveness during transitions unclear.

- **Open Question 3:** Can ShardMemo's tiered routing generalize to settings where evidence→shard supervision is unavailable or sparse? The reliance on LoCoMo's supervision raises questions about applicability to domains lacking explicit shard-to-evidence mappings.

## Limitations
- Router architecture details (f_θ) and shard summary representation (σ_j) are underspecified
- Training hyperparameters and cost estimation methodology lack precision
- Adaptive Top-P mechanism's entropy threshold selection is not fully detailed

## Confidence
- **High Confidence:** Scope-before-routing mechanism effectiveness (direct performance improvements shown in ablation)
- **Medium Confidence:** Cost-aware gating benefits (Ablation shows impact but specific cost coefficient α tuning unclear)
- **Medium Confidence:** Budgeted probing trade-offs (Clear saturation pattern in Figure 2, but optimal B_probe varies by task)

## Next Checks
1. Implement and test the untrained MoE baseline (Table 4, Row 2) to verify the "supervision gap" claim
2. Disable masking and measure post-hoc filtering costs to quantify the efficiency benefit of hard masking
3. Run B_probe sweep {1, 2, 4, 8} to identify the knee point for your specific latency constraints