---
ver: rpa2
title: 'HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource
  African Language'
arxiv_id: '2509.16256'
source_url: https://arxiv.org/abs/2509.16256
tags:
- sentiment
- dataset
- learning
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces HausaMovieReview, a benchmark dataset of
  5,000 YouTube comments for sentiment analysis in Hausa. The dataset was annotated
  by three independent annotators, achieving a Fleiss' Kappa score of 0.85.
---

# HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language

## Quick Facts
- arXiv ID: 2509.16256
- Source URL: https://arxiv.org/abs/2509.16256
- Reference count: 26
- Decision Tree outperforms transformer models (89.72% vs 79.7% accuracy) on 5,000 Hausa YouTube comments

## Executive Summary
This study introduces HausaMovieReview, a benchmark dataset of 5,000 YouTube comments for sentiment analysis in Hausa. The dataset was annotated by three independent annotators, achieving a Fleiss' Kappa score of 0.85. A comparative analysis of classical (Logistic Regression, Decision Tree, KNN) and transformer models (BERT, RoBERTa) was conducted. The Decision Tree classifier outperformed all other models, achieving 89.72% accuracy and 89.60% F1-score. Transformer models achieved lower performance (BERT: 79.7% accuracy, RoBERTa: 76.6% accuracy). The results demonstrate that effective feature engineering with classical models can achieve state-of-the-art performance in low-resource contexts.

## Method Summary
The study created a dataset of 5,000 Hausa YouTube comments from 13 episodes of the LABARINA series, annotated by three native Hausa speakers using majority voting with Fleiss' Kappa=0.85 agreement. Classical models used TF-IDF vectorization with preprocessing (lowercase, punctuation removal) and 10-fold cross-validation. Transformers (BERT-base-uncased, RoBERTa) were fine-tuned with learning rate 2×10⁻⁵, batch size 16, 3 epochs, and AdamW optimizer. Performance was evaluated using accuracy, F1-score (macro), precision, recall, and AUC metrics.

## Key Results
- Decision Tree achieved 89.72% accuracy and 89.60% F1-score, outperforming all other models
- KNN achieved only 63.47% accuracy, suggesting high-dimensional TF-IDF features are poorly suited for distance-based methods
- BERT achieved 79.7% accuracy, RoBERTa achieved 76.6% accuracy, with both showing overfitting after early training steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-annotator consensus labeling produces reliable ground truth for low-resource language sentiment tasks.
- Mechanism: Three independent native Hausa speakers annotate each comment; majority voting resolves disagreements. Fleiss' Kappa (0.85) quantifies inter-annotator agreement, filtering ambiguous cases.
- Core assumption: Native speakers with domain knowledge (Kannywood) can consistently interpret sentiment despite code-switching.
- Evidence anchors:
  - [abstract] "annotated by three independent annotators, demonstrating a robust agreement with a Fleiss' Kappa score of 0.85"
  - [section 3.2] "majority-vote mechanism... If two or more annotators agreed on a sentiment, that sentiment was assigned as the final label"
  - [corpus] Neighbor paper "HausaNLP at SemEval-2025" confirms annotation challenges persist for Hausa emotion detection tasks
- Break condition: Fleiss' Kappa drops below 0.6, indicating unreliable labels; dataset expansion without maintaining annotator quality.

### Mechanism 2
- Claim: TF-IDF features with Decision Tree classifiers can outperform transformer fine-tuning on small, domain-specific datasets.
- Mechanism: TF-IDF weights terms by document frequency, creating sparse feature vectors. Decision Trees partition feature space hierarchically, capturing non-linear decision boundaries without requiring large training corpora.
- Core assumption: Sentiment signals in Kannywood comments are concentrated in discriminative keywords/phrases rather than complex contextual patterns.
- Evidence anchors:
  - [abstract] "Decision Tree classifier... significantly outperformed the deep learning models" with 89.72% accuracy vs BERT's 79.7%
  - [section 5] "It appears to have effectively identified key features within the TF-IDF vectorized data that are highly predictive of sentiment"
  - [corpus] "Investigating the Impact of Language-Adaptive Fine-Tuning" suggests transformers need language-specific pretraining for Hausa
- Break condition: Dataset expands beyond ~10,000 samples or requires cross-domain generalization—transformers should then be re-evaluated.

### Mechanism 3
- Claim: Pretrained transformers without language-adaptive pretraining underperform on low-resource African languages with code-switching.
- Mechanism: BERT/RoBERTa were pretrained primarily on English; fine-tuning on 5,000 Hausa comments may cause overfitting without adequate semantic transfer. Validation loss fluctuated after Step 40.
- Core assumption: The semantic representations learned from English Wikipedia-scale data transfer poorly to Hausa-English code-switched informal text.
- Evidence anchors:
  - [section 4] "both models showed a rapid increase in validation accuracy... followed by a plateauing... suggesting that they began to overfit"
  - [section 5] "Transformer models... require vast amounts of data to fully realize their predictive power"
  - [corpus] Neighbor papers recommend AfriBERTa and language-adaptive fine-tuning specifically for African languages
- Break condition: Using Hausa-pretrained models (AfriBERTa, mDeBERTaV3) should improve transformer performance—confirmed in neighbor literature.

## Foundational Learning

- Concept: **TF-IDF Vectorization**
  - Why needed here: Converts raw Hausa text into numerical features for classical ML models; captures term importance relative to corpus.
  - Quick check question: Can you explain why a word frequent in one document but rare across the corpus receives a higher TF-IDF weight?

- Concept: **Inter-Annotator Agreement (Fleiss' Kappa)**
  - Why needed here: Validates dataset quality; Kappa > 0.8 indicates "almost perfect" agreement per Landis-Koch scale.
  - Quick check question: Why is accuracy alone insufficient for measuring annotation reliability between multiple annotators?

- Concept: **10-Fold Cross-Validation**
  - Why needed here: With only 5,000 samples, train/test splits risk high variance; cross-validation provides robust performance estimates.
  - Quick check question: How does 10-fold CV reduce overfitting risk compared to a single holdout split?

## Architecture Onboarding

- Component map:
  ```
  YouTube Comments → Preprocessing (lowercase, punctuation removal)
                  → TF-IDF Vectorizer → Decision Tree / LR / KNN
                  → 10-Fold CV → Metrics (Accuracy, F1, AUC)

  Raw Text → BERT/RoBERTa Tokenizer → Fine-tuning (AdamW, lr=2e-5, epochs=3)
           → Validation → Test Evaluation
  ```

- Critical path: Data annotation quality (Fleiss' Kappa) → Feature engineering (TF-IDF) → Model selection based on dataset size constraints.

- Design tradeoffs:
  - Classical models: Faster training, interpretable, work well on small data—but may not scale to complex linguistic patterns.
  - Transformers: Capture context but require more data and compute; English-pretrained models lack Hausa semantic knowledge.

- Failure signatures:
  - BERT/RoBERTa validation loss increases after early training steps = overfitting on small dataset.
  - KNN accuracy ~63% = high-dimensional sparse TF-IDF features poorly suited for distance-based methods.
  - Large gap between training and validation metrics = insufficient regularization or data augmentation needed.

- First 3 experiments:
  1. **Replicate Decision Tree baseline**: Apply TF-IDF + Decision Tree with 10-fold CV; verify ~89% accuracy on provided dataset.
  2. **Ablate feature engineering**: Test raw counts vs TF-IDF vs n-grams (1-2) to identify which features drive Decision Tree performance.
  3. **Swap transformer backbone**: Replace bert-base-uncased with AfriBERTa or mDeBERTaV3; compare fine-tuning results to assess language-adaptive pretraining impact.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific features or decision boundaries allow classical models like Decision Trees to outperform transformer models on the HausaMovieReview dataset?
  - Basis in paper: [explicit] The authors explicitly call for a "Deeper Investigation into Classical ML Performance" to understand why these models performed exceptionally well.
  - Why unresolved: The paper notes the result is counterintuitive but only speculates that data size and overfitting are causes, without conducting feature importance analysis.
  - What evidence would resolve it: A detailed ablation study or feature importance analysis (e.g., SHAP values) identifying which TF-IDF features drive the Decision Tree's predictions.

- **Open Question 2**: Can larger, Africa-centric pre-trained models (e.g., Afro-XLMR-Large, AfriBERTa) surpass the performance of the current classical baselines?
  - Basis in paper: [explicit] The "Future Work" section suggests fine-tuning larger, multilingual pre-trained models specifically designed for African languages.
  - Why unresolved: The current study relied on standard BERT and RoBERTa, which may lack sufficient representation of Hausa linguistic nuances.
  - What evidence would resolve it: Fine-tuning the suggested Africa-centric models on the HausaMovieReview dataset and comparing their F1-scores against the 89.60% baseline of the Decision Tree.

- **Open Question 3**: How robust is the Decision Tree model when applied to sentiment analysis tasks in other Hausa domains outside of movie reviews?
  - Basis in paper: [inferred] The authors acknowledge the limitation that the "dataset is restricted to Kannywood movie comments and may not be generalizable to other domains."
  - Why unresolved: The vocabulary and sentiment expression in the "LABARANA" series may be domain-specific, potentially limiting the model's utility in news or general social media contexts.
  - What evidence would resolve it: Cross-domain validation testing the trained model on a different Hausa dataset, such as political news comments or general social media posts.

## Limitations

- Small dataset size (5,000 samples) constrains generalizability and may favor classical models over transformers
- English-pretrained transformers (BERT, RoBERTa) lack Hausa-specific semantic representations, creating unfair comparison
- Minimal hyperparameter tuning across all models, potentially leaving performance on the table

## Confidence

- **High Confidence**: Dataset creation methodology (multi-annotator consensus with Fleiss' Kappa=0.85) and comparative performance ranking on this specific dataset
- **Medium Confidence**: Claim that classical models can achieve state-of-the-art performance in low-resource contexts—dataset-specific result
- **Low Confidence**: Assertion that transformers inherently underperform on low-resource African languages without language-adaptive pretraining

## Next Checks

1. **Dataset scaling experiment**: Train all models (Decision Tree, BERT, and AfriBERTa) on incrementally larger subsets (1K, 2K, 5K, 10K samples) to identify the crossover point where transformers surpass classical models, if any.

2. **Language-adapted transformer comparison**: Replace bert-base-uncased with AfriBERTa and mDeBERTaV3, fine-tuning with identical hyperparameters to directly test whether language-specific pretraining resolves the performance gap.

3. **Hyperparameter optimization**: Conduct systematic grid searches for Decision Tree (max_depth, criterion), KNN (k-values), and transformer learning rates to establish whether current results reflect true model capabilities or suboptimal configurations.