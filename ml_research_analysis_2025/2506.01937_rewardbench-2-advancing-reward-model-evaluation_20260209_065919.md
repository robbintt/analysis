---
ver: rpa2
title: 'RewardBench 2: Advancing Reward Model Evaluation'
arxiv_id: '2506.01937'
source_url: https://arxiv.org/abs/2506.01937
tags:
- reward
- tulu
- arxiv
- training
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RewardBench 2, a new multi-skill reward modeling
  benchmark designed to evaluate reward models across six challenging domains using
  unseen human prompts. The benchmark features a 4-way classification format with
  a 25% random baseline, making it more challenging than prior benchmarks where models
  score about 20 points lower on average.
---

# RewardBench 2: Advancing Reward Model Evaluation

## Quick Facts
- arXiv ID: 2506.01937
- Source URL: https://arxiv.org/abs/2506.01937
- Reference count: 40
- Primary result: New multi-skill reward modeling benchmark using unseen human prompts with 4-way classification format and strong downstream correlation (0.87)

## Executive Summary
RewardBench 2 introduces a challenging new benchmark for evaluating reward models across six domains using unseen human prompts from real-world usage. The benchmark features a 4-way classification format with a 25% random baseline, making it more difficult than prior benchmarks where models score about 20 points lower on average. The authors train and evaluate over 100 reward models, finding that Llama 3.1-based models perform best and that training for multiple epochs can be beneficial. The benchmark shows strong correlation with downstream performance in best-of-N sampling across diverse tasks and provides useful signals for RLHF training, though optimal reward models depend on policy model lineage and training setup.

## Method Summary
RewardBench 2 evaluates reward models on 1,865 prompts across six domains (Factuality, Precise Instruction Following, Math, Safety, Focus, Ties) using 4-way classification where models must identify the chosen completion from three rejected ones. The benchmark sources new human prompts from WildChat to avoid contamination with downstream evaluations, using a 25% random baseline. The authors train over 100 reward models using Bradley-Terry preference modeling with base models including Llama 3.1 8B and Tulu 3 8B, testing various training configurations and data sources. Downstream validation uses best-of-N sampling on tasks like GSM8K, MATH, and IFEval, with RLHF experiments using PPO to examine policy-RM lineage matching effects.

## Key Results
- Models score about 20 points lower on RewardBench 2 compared to the first RewardBench due to the more challenging 4-way classification format
- Llama 3.1-based reward models perform best, while Qwen 2.5-based models underperform despite competitive performance on the original RewardBench
- Reward models trained for multiple epochs (up to 2) can outperform single-epoch models, contrary to accepted best practice
- Strong correlation (0.87) between RewardBench 2 scores and downstream performance in best-of-N sampling across diverse tasks
- For RLHF with PPO, optimal reward models depend on policy model lineage, with self-preference effects causing performance degradation when RM and policy have different base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 4-way classification format increases benchmark discrimination power by lowering the random baseline from 50% to 25%, creating more headroom to separate strong models.
- Mechanism: Adding distractor completions increases task difficulty without changing the underlying preference modeling objective, forcing reward models to demonstrate finer-grained discrimination rather than relying on coarse binary judgments.
- Core assumption: The three rejected completions are genuinely distinguishable from the chosen completion by a well-calibrated reward model (verified through multi-stage filtering: programmatic, LM-based, and manual).
- Evidence anchors:
  - [abstract] "models score about 20 points on average lower on REWARD BENCH 2 compared to the first REWARD BENCH"
  - [section 3] "Accuracy on REWARD BENCH 2 is judged by selecting the correct response from 4 completions per prompt... the random baseline is 25% accuracy, versus 50% for many related works"
  - [corpus] No directly comparable 4-way benchmarks found in corpus; Multimodal RewardBench and similar extensions focus on modality coverage rather than classification format.
- Break condition: If rejected completions are not genuinely lower quality (e.g., if they're adversarially similar), the benchmark may measure noise sensitivity rather than preference alignment.

### Mechanism 2
- Claim: Using unseen human prompts from real-world usage enables valid downstream correlation claims by eliminating train-test contamination between benchmark and evaluation datasets.
- Mechanism: Sourcing prompts independently from downstream evaluations ensures that correlation with downstream performance reflects generalization capability, not memorization of benchmark prompts during RM training.
- Core assumption: Reward models in the wild have not been trained or evaluated on the held-out WildChat prompts used in this benchmark.
- Evidence anchors:
  - [abstract] "Compared to most other benchmarks, REWARD BENCH 2 sources new human prompts instead of existing prompts from downstream evaluations"
  - [section 3] "We compared our prompts against twenty widely-used downstream evaluations with the Tulu 3 decontamination toolkit and ensured there was no overlap"
  - [corpus] Related benchmarks do not explicitly emphasize unseen prompt sourcing as a contamination control mechanism.
- Break condition: If RM training datasets have incorporated WildChat or similar in-the-wild interaction logs, contamination could inflate correlation estimates.

### Mechanism 3
- Claim: For RLHF with PPO, the reward model's base model lineage must match the policy model's lineage; otherwise, downstream performance degrades even when benchmark scores are high.
- Mechanism: Reward models develop a systematic preference for completions from their base model's output distribution, and distributional mismatch between RM training prompts and PPO training prompts causes optimization to proceed on an off-policy reward signal.
- Core assumption: The self-preference effect generalizes across model families and training data sources, and PPO performance depends on in-distribution reward signal quality.
- Evidence anchors:
  - [abstract] "optimal reward models depend on policy model lineage and training setup"
  - [section 5.2] "when there is a misalignment between the policy and either the RM's base model... downstream performance drops significantly... top scoring RMs on REWARD BENCH 2 often do not help the policy improve"
  - [appendix D] "reward models rank the outputs of their own base model... higher than other reward models do" with statistical significance
- Break condition: If the policy model is updated significantly during PPO (distribution shift), the initial RM may become misaligned even with matched lineage, requiring RM retraining.

## Foundational Learning

- Concept: **Bradley-Terry Preference Modeling**
  - Why needed here: Understanding how reward models convert pairwise preferences into scalar scores; the paper uses this formulation as the canonical RM training objective.
  - Quick check question: Given scores r(A)=2.5 and r(B)=1.0, what is the probability that A is preferred over B under Bradley-Terry? (Answer: exp(2.5)/(exp(2.5)+exp(1.0)) ≈ 0.82)

- Concept: **Best-of-N Sampling vs. RLHF (PPO)**
  - Why needed here: The paper evaluates RM utility in two distinct downstream scenarios—inference-time selection and policy optimization—which have different failure modes and correlation patterns with benchmark scores.
  - Quick check question: Why might a reward model perform well for BoN sampling but poorly for PPO training? (Answer: PPO requires in-distribution reward signal and compatible output distributions; BoN only requires relative ranking quality among candidates.)

- Concept: **Reward Model Overfitting and Multi-Epoch Training**
  - Why needed here: The paper challenges the common practice of single-epoch RM training, finding that 2 epochs can improve performance; understanding the overfitting regime is critical for practical deployment.
  - Quick check question: What are the risks of training a reward model for multiple epochs, and what signals would indicate overfitting? (Answer: Overfitting to training preferences; indicated by training accuracy increasing while benchmark/downstream performance plateaus or degrades.)

## Architecture Onboarding

- Component map: WildChat prompts → Domain classifiers → Per-domain completion pools → Filtering (LM-as-judge, verifiers, manual) → 4-completion instances (1 chosen, 3 rejected) → Reward model scoring → Accuracy aggregation per domain → Unweighted average across 6 domains

- Critical path:
  1. **Prompt decontamination**: Verify no overlap with downstream evaluations using the Tulu 3 decontamination toolkit before using any prompt.
  2. **Completion quality control**: For each domain, ensure chosen/rejected labels are objectively verifiable (verifier functions for IF, majority voting for Math, multi-LLM agreement for Factuality).
  3. **RM-policy lineage matching**: For PPO experiments, confirm the RM's base model shares tokenizer and output distribution characteristics with the policy model.

- Design tradeoffs:
  - **4-way vs. 2-way classification**: 4-way increases discrimination but requires more completion generation and quality verification effort; paper shows ~20-point score reduction with 4-way format.
  - **Accuracy vs. preference-based metrics**: Paper opts for accuracy (objective correctness) over human/LM-as-judge preference correlation to avoid subjectivity; trade-off is reduced coverage of stylistic preference alignment.
  - **Combined vs. single-source training data**: Combining Tulu and Skywork preference mixes improves average performance but may dilute domain-specific gains (e.g., Skywork data helps safety/focus more; Tulu data helps factuality).

- Failure signatures:
  - **PPO performance saturation**: If all decent RMs (scores 50-70) produce similar PPO performance, check for policy-RM lineage mismatch or training prompt distribution mismatch.
  - **BoN correlation drop**: If benchmark scores don't correlate with BoN downstream performance, verify that generator model produces sufficiently varied quality (too-strong generators reduce discrimination).
  - **Domain-specific anomalies**: Safety subset may show high variance due to subjective refusal boundaries; Ties subset requires careful calibration scoring beyond simple accuracy.

- First 3 experiments:
  1. **Baseline RM evaluation**: Run your existing reward model(s) on RewardBench 2 and compare per-domain breakdown to identify weak domains (expect ~20-point drop vs. RewardBench 1).
  2. **BoN correlation check**: For your top 3 RMs, run Best-of-16 sampling with a mid-tier generator on 2-3 downstream tasks; verify correlation with benchmark scores matches paper's ~0.87 average.
  3. **Lineage ablation (if using PPO)**: Train identical PPO runs with (a) same-lineage RM and (b) different-lineage RM with similar benchmark score; quantify performance gap to determine lineage effect size in your setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can accuracy-based benchmarks be adapted to predict downstream PPO performance when the reward model and policy model have different lineages?
- Basis in paper: The conclusion states that for RLHF, "simply taking the highest scoring reward model on a benchmark will not ensure a good post RLHF model" if the reward model is not based on the same lineage as the policy.
- Why unresolved: The paper identifies the "off-policy" degradation effect but does not propose a specific evaluation metric or protocol to quantify this risk before training begins.
- What evidence would resolve it: A benchmark metric that incorporates the policy model's lineage or a calibration method that restores correlation for off-policy reward models.

### Open Question 2
- Question: Under what specific data conditions does training reward models for multiple epochs improve performance rather than causing overfitting?
- Basis in paper: The authors find that "contrary to the accepted best practice, training for more than one epoch can be beneficial," but note that recent works lack explicit ablations explaining why.
- Why unresolved: The paper observes the result empirically in controlled settings but leaves the interaction between epoch count, learning rate, and data composition as an open area for optimization.
- What evidence would resolve it: Ablation studies mapping dataset size and diversity against optimal epoch counts to define the "overfitting boundary."

### Open Question 3
- Question: Can generative reward models overcome the "lack of granularity" identified in the paper to reliably distinguish between similar candidate completions?
- Basis in paper: The evaluation details note that generative models acting as judges "typically lack granularity in their judgments" compared to classifier-based reward models, often rating distinct candidates identically.
- Why unresolved: The paper evaluates generative models using standard prompts; it is unknown if chain-of-thought prompting or specialized training could provide the necessary resolution.
- What evidence would resolve it: A generative model achieving parity with top classifiers on the fine-grained "Precise Instruction Following" subset.

## Limitations

- Limited validation of downstream correlation claims across diverse model families beyond Tulu 3 and Skywork ecosystems
- Single-objective focus on accuracy may miss important dimensions of reward model quality like stylistic preference alignment
- Lineage dependency complexity adds practical constraints without clear guidance on handling policy distribution shifts during RLHF training

## Confidence

**High confidence**: The 4-way classification format's effectiveness in creating a more challenging benchmark (supported by the 20-point score reduction evidence); the importance of unseen prompt sourcing for valid downstream correlation; the self-preference effect and lineage matching requirement for PPO.

**Medium confidence**: The strong correlation (0.87) with downstream BoN performance across diverse tasks; the finding that 2 epochs of training can be beneficial; the relative performance ordering of Llama 3.1 vs. other base models.

**Low confidence**: The exact optimal RM configuration for any given PPO setup; the generalizability of the Ties subset scoring mechanism; the benchmark's sensitivity to different reward model architectures beyond the evaluated Llama 3.1 and Tulu 3 variants.

## Next Checks

1. **Downstream validation across model families**: Test the benchmark's predictive power by evaluating a diverse set of reward models (including different base model families like GPT, Mistral, or newer open models) on both RewardBench 2 and multiple downstream RLHF tasks. Quantify whether the 0.87 correlation holds across model families or if it's specific to the Tulu 3/Skywork ecosystem.

2. **PPO ablation study with explicit lineage matching**: Design a controlled experiment where you systematically vary (a) RM base model lineage relative to policy, (b) training data distribution between RM and PPO, and (c) PPO training duration. Measure the interaction effects to quantify how much each factor contributes to final policy quality, providing clearer guidance than the current qualitative finding.

3. **Robustness to generator strength variation**: The paper notes that generator model strength affects BoN downstream performance. Conduct experiments varying generator quality (weak, medium, strong) and measure how RewardBench 2 scores correlate with downstream performance in each regime. This would reveal whether the benchmark maintains predictive validity across different inference scenarios.