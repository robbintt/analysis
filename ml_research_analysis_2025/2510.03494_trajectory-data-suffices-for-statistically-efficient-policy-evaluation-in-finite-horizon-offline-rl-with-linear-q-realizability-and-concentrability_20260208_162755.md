---
ver: rpa2
title: "Trajectory Data Suffices for Statistically Efficient Policy Evaluation in\
  \ Finite-Horizon Offline RL with Linear $q^\u03C0$-Realizability and Concentrability"
arxiv_id: '2510.03494'
source_url: https://arxiv.org/abs/2510.03494
tags:
- policy
- lemma
- traj
- function
- since
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of statistically efficient offline
  policy evaluation in finite-horizon Markov decision processes with linear q-function
  realizability and concentrability. Prior work showed that policy evaluation is statistically
  inefficient under these assumptions unless trajectory data is available.
---

# Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability

## Quick Facts
- **arXiv ID**: 2510.03494
- **Source URL**: https://arxiv.org/abs/2510.03494
- **Reference count**: 5
- **Primary result**: First polynomial sample complexity algorithm for offline policy evaluation under linear q^π-realizability and concentrability using trajectory data

## Executive Summary
This paper resolves a fundamental open problem in offline reinforcement learning: achieving statistically efficient policy evaluation when the true Q-function is linearly realizable but the Bellman completeness property fails. Prior work showed that policy evaluation is statistically intractable under these assumptions unless trajectory data is available. The authors present LIN-q^π-FQE, an algorithm that achieves polynomial sample complexity by leveraging trajectory data to construct "skippy" Bellman operators that restore completeness. The key insight is that while linear realizability alone is insufficient for standard FQE, the additional structure from trajectory data enables the construction of a modified MDP where Bellman completeness is recovered.

## Method Summary
The algorithm operates by constructing a set of approximate Q-functions based on different modifications of the MDP, then selecting the one minimizing an empirical estimate of the expected advantage. The modifications involve "skipping" problematic states where the range of Q-values across actions is small, following the behavior policy instead. This skipping operation is implemented implicitly in the Bellman update using trajectory data. The algorithm proceeds in three steps: (1) construct candidate Q-function sets for various modifications, (2) select the Q-function minimizing the empirical advantage between evaluation and skippy policies, and (3) return the value estimate at the initial state. The sample complexity is shown to be Õ(C₀⁵H⁷d³/ε² + L_φ²) trajectories.

## Key Results
- First polynomial sample complexity algorithm for offline policy evaluation under linear q^π-realizability and concentrability
- Sample complexity of Õ(C₀⁵H⁷d³/ε² + L_φ²) trajectories
- Improvement in policy optimization sample complexity by a factor of C₀d through tighter analysis
- Theoretical validation that trajectory data is necessary and sufficient for efficient policy evaluation under these assumptions

## Why This Works (Mechanism)

### Mechanism 1: Recovering Completeness via "Skippy" Operators
Linear q^π-realizability alone does not ensure standard Bellman completeness, causing FQE to fail. Trajectory data enables the construction of "skippy" Bellman operators that restore completeness. The algorithm identifies states where Q-value ranges are small, then defines a modified MDP where these states are "skipped" by following the behavior policy. This skipping is implicit in the Bellman update using trajectory data, ensuring Q-functions remain in the linear class. The key assumption is that the dataset consists of full trajectories rather than independent transitions.

### Mechanism 2: Bounding Error via Advantage Minimization
The correct Q-function estimate can be identified among candidates without knowing the optimal state-skipping strategy by minimizing the empirical expected advantage. Since the optimal modification is unknown, the algorithm generates candidate Q-functions for various modifications and selects the one minimizing the difference between evaluation and skippy policies. For the correct modification, this difference is provably small because policies only differ in "low range" states. The behavior policy must be known to define the skippy policy.

### Mechanism 3: Tighter Analysis via Ridge Regression Concentration
The sample complexity for policy optimization can be improved by a factor of C₀d through refined analysis of ridge regression concentration. The analysis leverages the structure of the empirical covariance matrix and applies tighter concentration inequalities (Hsu et al. 2012). By correctly ordering Jensen's inequality with concentrability assumptions, the dependency on dimension d and concentrability C₀ is reduced.

## Foundational Learning

- **Concept: Bellman Completeness vs. Realizability**
  - **Why needed here**: The paper's primary motivation is that standard FQE fails under realizability alone. Understanding the gap between "the function class contains the true Q-function" (realizability) and "the class is closed under the Bellman operator" (completeness) is essential.
  - **Quick check question**: Can you explain why a function class might contain the optimal Q-function q* but not the result of applying the Bellman update to a slightly incorrect estimate q?

- **Concept: Concentrability Coefficient (C₀)**
  - **Why needed here**: This assumption bounds the distribution shift between the behavior policy and the evaluation policy. All error bounds scale polynomially with C₀.
  - **Quick check question**: If the behavior policy πb rarely visits a state s that is critical for the evaluation policy πe, how does the value of C₀ reflect this?

- **Concept: Performance Difference Lemma**
  - **Why needed here**: This is the mathematical bridge used to connect the selection criterion (advantage) to the final policy value error.
  - **Quick check question**: How does the difference in values between two policies relate to the sum of their advantage functions along a trajectory?

## Architecture Onboarding

- **Component map**: Data Buffer -> Candidate Set Constructor -> Advantage Estimator -> Value Selector
- **Critical path**: The construction of the set G_eval (Section 4.5). This set must be loose enough to contain the "correct" modification G* but tight enough to exclude spurious solutions. This relies on the parameter α balancing modification error and function class size.
- **Design tradeoffs**:
  - Statistical vs. Computational Efficiency: The algorithm is statistically efficient (polynomial sample complexity) but computationally heavy because it requires solving least-squares problems over a potentially large candidate set of modifications G.
  - Threshold α: A small α reduces "skipping" error (Hα) but increases the size of the function class, potentially increasing variance.
- **Failure signatures**:
  - High C₀ values: If the behavior data has poor coverage, the error scales as C₀^(5/2), causing variance to explode.
  - Misspecified Realizability: If the true q^π is non-linear, the "skippy" completeness proof likely fails, and there is no guarantee a good candidate exists.
- **First 3 experiments**:
  1. Validation of Skippy Mechanism: Construct a linear q^π-realizable MDP that is not Bellman complete. Verify that standard FQE diverges while LIN-q^π-FQE converges.
  2. Scaling with Dimension: Vary the feature dimension d while keeping state space |S| fixed. Plot error vs. samples to confirm O(d³) scaling.
  3. Sensitivity to Trajectory Integrity: Corrupt the trajectory structure (e.g., shuffle steps) to verify that the algorithm fails without temporal dependencies.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Algorithms 1 and 2 be modified to be computationally efficient while maintaining statistical efficiency? The authors explicitly list this as the first direction for future work, noting that current algorithms involve constructing and searching over sets of approximate q-functions, which may be computationally prohibitive.

- **Open Question 2**: Is statistically efficient offline policy optimization possible when only the optimal policy's value function is linearly realizable (Obj π Lin-Realizable)? This is marked as an open problem in Table 1, as current work achieves efficiency under the stronger assumption that q-functions of all memoryless policies are realizable.

- **Open Question 3**: Can the assumption that the behavior policy πb is known be removed for the policy evaluation task? The current LIN-q^π-FQE learner relies on knowing πb to define the skippy policies used in advantage estimation, and removing this assumption is listed as a specific future direction.

- **Open Question 4**: Can the sample complexity dependence on dimension d be improved? The current bounds feature a d³ dependence, and the authors suggest that techniques like loss-based analysis with Bernstein's inequality could potentially tighten this to O(d²) or O(d).

## Limitations

- Computational tractability of G-enumeration: The paper provides statistical guarantees but does not specify a tractable algorithm for searching over the space of modifications G, relying on exhaustive enumeration which may be computationally prohibitive.
- Realizability assumption strength: The algorithm assumes exact linear q^π-realizability, a strong assumption that may not hold for many real-world problems, with no robustness guarantees provided.
- Dependence on known πb: The algorithm requires knowing the behavior policy πb to construct skippy policies, which may not be realistic in many offline settings where the behavior policy is unknown or only implicitly defined.

## Confidence

- **High confidence**: The statistical efficiency claim (polynomial sample complexity) and the core mechanism of using trajectory data to restore Bellman completeness through skippy operators. The mathematical proofs appear sound given the stated assumptions.
- **Medium confidence**: The improvement in sample complexity for policy optimization (factor of C₀d improvement). While the analysis is detailed, the practical impact depends on the magnitude of C₀ in real applications.
- **Low confidence**: The practical implementation details for constructing G_eval and G_opt efficiently. The paper provides theoretical bounds but lacks guidance on tractable algorithms for real-world use.

## Next Checks

1. Implement on linear MDP that violates Bellman completeness: Construct a simple linear MDP where standard FQE fails due to lack of Bellman completeness. Verify that LIN-q^π-FQE succeeds while standard methods fail.

2. Test scaling with concentrability: Generate datasets with varying levels of concentrability C₀ and measure how the error scales. Verify the theoretical scaling of C₀^(5/2) predicted by the sample complexity bounds.

3. Validate advantage-based selection: Create scenarios where multiple candidate modifications G are plausible. Verify that the selection criterion (minimizing empirical advantage) consistently chooses the modification that leads to the most accurate value estimate.