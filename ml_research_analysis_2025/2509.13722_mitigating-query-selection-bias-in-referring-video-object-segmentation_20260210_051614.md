---
ver: rpa2
title: Mitigating Query Selection Bias in Referring Video Object Segmentation
arxiv_id: '2509.13722'
source_url: https://arxiv.org/abs/2509.13722
tags:
- object
- query
- video
- segmentation
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of query selection bias in Referring
  Video Object Segmentation (RVOS), where static text-initialized queries fail to
  distinguish between target and distractor objects with similar appearance or motion.
  To address this, the authors propose Triple Query Former (TQF), which factorizes
  the referring query into three specialized components: an appearance query for static
  attributes, an intra-frame query for spatial relations, and an inter-frame query
  for temporal association.'
---

# Mitigating Query Selection Bias in Referring Video Object Segmentation

## Quick Facts
- arXiv ID: 2509.13722
- Source URL: https://arxiv.org/abs/2509.13722
- Reference count: 40
- Key outcome: Introduces TQF framework achieving state-of-the-art RVOS performance with 65.8% J&F on Ref-YouTube-VOS and 50.1% on MeViS

## Executive Summary
This paper addresses the problem of query selection bias in Referring Video Object Segmentation (RVOS), where static text-initialized queries struggle to distinguish between target and distractor objects with similar appearance or motion. The authors propose Triple Query Former (TQF), a framework that factorizes the referring query into three specialized components: appearance, intra-frame, and inter-frame queries. Each query type is dynamically constructed by integrating linguistic cues with visual guidance. The framework also introduces two motion-aware aggregation modules to enhance object token representations through spatial relation modeling and cross-frame motion alignment.

## Method Summary
The Triple Query Former (TQF) framework tackles query selection bias by decomposing the referring query into three specialized components. The appearance query captures static visual attributes, the intra-frame query models spatial relations within individual frames, and the inter-frame query handles temporal associations across frames. Each query is dynamically constructed by integrating textual descriptions with visual information from the video. Additionally, two motion-aware aggregation modules are introduced: Intra-frame Interaction Aggregation enhances object tokens through spatial relation modeling, while Inter-frame Motion Aggregation aligns object representations across frames based on motion patterns.

## Key Results
- TQF achieves 65.8% J&F on Ref-YouTube-VOS, surpassing prior methods by over 2%
- TQF achieves 50.1% on MeViS dataset, improving over previous methods by 3.7%
- Ablation studies confirm the effectiveness of query design and motion-aware aggregation modules

## Why This Works (Mechanism)
The framework works by addressing the fundamental limitation of static text queries in RVOS. By factorizing the query into three specialized components, each query type can focus on specific aspects of the segmentation task. The appearance query handles static visual attributes, the intra-frame query captures spatial relationships within frames, and the inter-frame query manages temporal consistency. The dynamic construction of each query by integrating linguistic cues with visual guidance allows for more accurate object localization and tracking. The motion-aware aggregation modules further enhance object representations by explicitly modeling spatial relations and cross-frame motion patterns.

## Foundational Learning

1. **Referring Video Object Segmentation (RVOS)**: The task of segmenting objects in videos based on natural language queries. Needed because traditional video object segmentation methods don't incorporate linguistic guidance. Quick check: Can the method segment objects based on complex natural language descriptions?

2. **Query Selection Bias**: The tendency of static text queries to fail when multiple objects share similar appearance or motion. Needed because this bias limits the performance of language-guided segmentation methods. Quick check: Does the method handle cases where multiple objects match the query description?

3. **Modular Query Decomposition**: Breaking down complex queries into specialized components that handle different aspects of the task. Needed because single monolithic queries struggle with the multi-faceted nature of RVOS. Quick check: Does each query component focus on a distinct aspect of the segmentation task?

4. **Motion-Aware Aggregation**: Techniques that use motion information to improve object representation across frames. Needed because temporal consistency is crucial for video segmentation tasks. Quick check: Does the method maintain temporal coherence across video frames?

## Architecture Onboarding

**Component Map**: TQF -> Query Factorization -> Dynamic Query Construction -> Motion-Aware Aggregation -> Segmentation Output

**Critical Path**: The critical path involves: (1) Input text query and video frames, (2) Query factorization into appearance, intra-frame, and inter-frame components, (3) Dynamic construction of each query type by integrating linguistic and visual information, (4) Motion-aware aggregation to enhance object representations, (5) Final segmentation output.

**Design Tradeoffs**: The modular approach increases model complexity but provides better handling of diverse query types. The motion-aware modules add computational overhead but significantly improve temporal consistency. The dynamic query construction requires additional parameters but enables more accurate object localization.

**Failure Signatures**: The method may struggle with extremely long videos due to computational constraints in the inter-frame aggregation module. Ambiguous queries describing multiple similar objects might still cause confusion even with the specialized query components. Complex spatial relationships that span multiple frames might not be fully captured by the current aggregation approach.

**First Experiments**: 1) Test on simple RVOS tasks with clear visual distinctions to establish baseline performance, 2) Evaluate on videos with multiple similar objects to test query discrimination, 3) Assess temporal consistency on videos with significant motion changes.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation relies primarily on standard benchmarks with limited analysis of generalization to diverse scenarios
- The impact of motion-aware aggregation modules on long-range temporal consistency remains unclear
- Performance on out-of-domain datasets or videos with extreme conditions is not explicitly tested

## Confidence

**High**: The architectural design of TQF and its modular query decomposition is well-justified and logically consistent with prior work.

**Medium**: The reported performance gains are substantial, but the lack of detailed analysis on failure cases or edge scenarios reduces confidence in robustness claims.

**Medium**: The motion-aware aggregation modules are described clearly, but their scalability to longer video sequences or higher frame rates is not explicitly tested.

## Next Checks
1. Evaluate TQF on datasets with longer temporal spans (e.g., >100 frames) to assess the scalability of the inter-frame motion aggregation module.
2. Conduct a failure case analysis on ambiguous queries where multiple objects share similar appearance or motion to quantify the robustness of the query factorization.
3. Test the framework on out-of-domain datasets (e.g., surgical videos or aerial imagery) to validate generalization beyond standard RVOS benchmarks.