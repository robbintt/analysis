---
ver: rpa2
title: World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents
arxiv_id: '2512.12548'
source_url: https://arxiv.org/abs/2512.12548
tags:
- agent
- learning
- patch
- foraging
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that artificial foragers equipped with
  learned world models naturally converge to patch-leaving strategies aligned with
  the Marginal Value Theorem (MVT). Using a model-based reinforcement learning agent
  (DreamerV2) that acquires a predictive representation of its environment, we show
  that anticipatory capabilities, rather than reward maximization alone, drive efficient
  patch-leaving behavior.
---

# World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents

## Quick Facts
- **arXiv ID**: 2512.12548
- **Source URL**: https://arxiv.org/abs/2512.12548
- **Reference count**: 37
- **Primary result**: Model-based RL agents with learned world models converge to MVT-aligned patch-leaving strategies, outperforming model-free methods in sparse-reward foraging tasks.

## Executive Summary
This study demonstrates that artificial foragers equipped with learned world models naturally converge to patch-leaving strategies aligned with the Marginal Value Theorem (MVT). Using a model-based reinforcement learning agent (DreamerV2) that acquires a predictive representation of its environment, we show that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. The agent's learned world model enables it to anticipate resource depletion and make near-optimal foraging decisions. Compared with standard model-free RL agents (PPO and R2D2), the model-based agent exhibits decision patterns similar to biological foragers, with its step counts closely matching MVT predictions across different inter-patch distances. These findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI systems.

## Method Summary
The study trains three reinforcement learning algorithms—DreamerV2 (model-based), PPO, and R2D2 (model-free)—on a patch-foraging task in the Melting Pot Commons Harvest environment. The task involves navigating between resource patches with exponentially decaying rewards and deciding when to leave each patch. DreamerV2 learns a Recurrent State-Space Model (RSSM) to predict environment dynamics and uses imagination-based planning to evaluate future trajectories. Performance is evaluated by comparing patch residence times against MVT predictions across four different inter-patch distances (3, 5, 7, and 9 tiles). Agents are trained for 1.5 million steps with 25 test runs per scenario.

## Key Results
- DreamerV2 achieves MVT-aligned patch-leaving behavior, with step counts within interquartile range of optimal predictions
- Model-free agents (PPO and R2D2) fail to approximate MVT, showing significant deviation in patch residence times
- Latent representations learned by DreamerV2's RSSM encode interpretable ecological concepts like resource scarcity and spatial distance
- The world model enables anticipatory planning that outperforms direct reward maximization strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learned world models enable anticipatory planning that approximates optimal patch-leaving decisions without explicit MVT programming.
- Mechanism: The DreamerV2 agent learns a Recurrent State-Space Model (RSSM) that compresses environmental observations into latent representations, then uses these to simulate ("dream") future trajectories. This imagination-based planning allows the agent to anticipate resource depletion and evaluate the expected value of staying versus leaving, naturally converging to MVT-aligned behavior.
- Core assumption: The latent dynamics learned by RSSM capture sufficient structure of the true environment to support accurate multi-step prediction.
- Evidence anchors:
  - [abstract] "Using a model-based reinforcement learning agent (DreamerV2) that acquires a predictive representation of its environment, we show that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior."
  - [section 3.4.1] DreamerV2 demonstrates accelerated learning, outperforming PPO and R2D2 within 200 episodes.
  - [section 4.1] "The agent is capable of imagining both the inherent dynamics of the patch, such as the color change... and the spatial configuration of the foraging scenario."
- Break condition: If the environment dynamics become highly stochastic or non-stationary, the learned world model may fail to generalize, degrading planning quality.

### Mechanism 2
- Claim: Model-free agents fail to approximate MVT because they lack explicit predictive representations for credit assignment across the long time horizons required for patch-foraging decisions.
- Mechanism: PPO and R2D2 learn policies directly from experience replay without building an internal model. In patch foraging, the optimal decision depends on anticipating *future* marginal returns, not just past rewards. Model-free methods must infer this through slow trial-and-error value propagation.
- Core assumption: The gap between model-free and model-based performance stems from representational limitations rather than hyperparameter choices (authors note no tuning was performed).
- Evidence anchors:
  - [section 3.4.2] PPO "shows a performance that is significantly divergent when the MVT is used as a baseline... the MVT value does not even fall within the interquartile range."
  - [section 3.4.1] R2D2 "shows no signs of learning" and was excluded from analysis.
  - [corpus] Related work on learning to reset in target search problems (arxiv:2503.11330) confirms that strategic decision points (like patch-leaving) benefit from mechanisms beyond pure reward maximization.
- Break condition: If reward signals are dense and informative at each step, model-free methods may close the gap.

### Mechanism 3
- Claim: The world model learns disentangled latent representations that encode ecologically relevant concepts—patch distance, resource scarcity, and spatial layout—enabling interpretable policy emergence.
- Mechanism: The RSSM encodes observations into a 32×32 discrete latent space. t-SNE projections of these latents cluster by state value and environmental features, showing the model has learned structured representations of key foraging variables without supervision.
- Core assumption: The observed clustering reflects meaningful conceptual abstraction rather than artifacts of the visualization method.
- Evidence anchors:
  - [section 4.2] "Figure 6 is a testament to the learning process behind future planning, as the agent can represent concepts such as distance or resource scarcity in a patch."
  - [section 3.3] Describes the t-SNE analysis showing state-value-correlated clustering in latent space.
  - [corpus] Weak corpus support—no directly comparable latent structure analyses in neighbor papers.
- Break condition: If observation space lacks sufficient signal for the task (e.g., no visual cues about patch state), latent structure may not emerge.

## Foundational Learning

- Concept: **Marginal Value Theorem (MVT)**
  - Why needed here: MVT provides the optimality baseline against which all agent behaviors are evaluated. Without understanding the marginal value equation (energy intake rate = accumulated reward / travel time + patch time), you cannot interpret whether agents are behaving optimally.
  - Quick check question: Can you explain why increasing inter-patch distance should increase optimal patch residence time under MVT?

- Concept: **Model-Based vs Model-Free Reinforcement Learning**
  - Why needed here: The paper's core claim hinges on this distinction. Model-free agents (PPO, R2D2) learn value functions directly; model-based agents (DreamerV2) learn environment dynamics first, then plan.
  - Quick check question: In a sparse-reward environment, which approach would you expect to learn faster and why?

- Concept: **Recurrent State-Space Models (RSSM) and Latent Planning**
  - Why needed here: DreamerV2's architecture centers on RSSM—the mechanism by which it learns world dynamics and performs imagination-based planning. Understanding this is essential for reproducing or extending the work.
  - Quick check question: How does an RSSM differ from a standard RNN in terms of stochasticity and representation learning?

## Architecture Onboarding

- Component map:
  - Environment observations (64×64 partial view) -> RSSM encoder -> Latent state z_t + recurrent state h_t -> Actor network -> Actions
  - Actor network -> Critic network -> Reward prediction -> World model training
  - Latent state z_t + action a_t + recurrent state h_t -> RSSM dynamics model -> Next latent state z_{t+1} + reward r_{t+1} + done signal

- Critical path:
  1. Agent observes environment (64×64 partial view) -> RSSM encodes to latent z_t + recurrent state h_t
  2. World model trained to predict (z_{t+1}, r_{t+1}, done) from (z_t, h_t, a_t)
  3. Actor imagines H=15 step trajectories in latent space using learned dynamics
  4. Critic evaluates imagined trajectories -> policy gradient update
  5. Every 100 steps: slow critic update for training stability

- Design tradeoffs:
  - **Imagination horizon (H=15)**: Longer horizons improve planning but increase computational cost and compounding model errors. Paper uses 15; sensitivity not tested.
  - **Discrete vs continuous latents**: RSSM uses discrete latents (32 classes)—better for structured environments but may limit expressiveness.
  - **No hyperparameter tuning**: Authors explicitly note results are without tuning. Performance gaps vs PPO may narrow with tuning.

- Failure signatures:
  - Agent stays in patch too long: Check if world model is underestimating reward decay rate (λ).
  - Agent exhibits wandering behavior (like PPO in Figure 4): World model may not be learning spatial structure; increase training steps or check observation preprocessing.
  - High variance in policy evaluation: Stochastic policy sampling is introducing noise; consider deterministic evaluation mode.

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train DreamerV2 and PPO on the provided Melting Pot patch-foraging environment with N=30, λ=0.01. Compare step counts against MVT predictions for x̄ = 3, 5, 7, 9 distances. Target: DreamerV2 within ±1 quartile of MVT; PPO outside interquartile range.
  2. **Ablate imagination horizon**: Train DreamerV2 with H ∈ {5, 10, 15, 25}. Plot alignment with MVT vs horizon. Hypothesis: Shorter horizons degrade MVT alignment; longer horizons may show diminishing returns or instability.
  3. **Test generalization to novel patch distances**: Train on x̄ ∈ {3, 5, 7, 9}, evaluate on x̄ ∈ {4, 6, 8}. Assess whether world model generalizes or overfits to training distances.

## Open Questions the Paper Calls Out

- Question: Does the emergence of MVT-aligned behavior persist in multi-agent environments, and do specific social roles emerge within the agent society?
- Basis in paper: [explicit] The Conclusion states: "Future studies in this type of scenario involve extending this research to a multi-agent foraging environment... to observe the emergence of behaviors comparable to social foraging theories. Here, roles within the agent society could be examined."
- Why unresolved: The current study is strictly single-agent. It is unknown if the optimality of the learned world model holds when agents must compete for resources or coordinate, nor if complex social behaviors (like producer-scrounger dynamics) naturally arise from these architectures.
- What evidence would resolve it: Training multiple DreamerV2 agents simultaneously in the same patch environment and analyzing the resulting policies for signs of role differentiation and adherence to social foraging optimality models.

## Limitations

- The study lacks hyperparameter optimization across algorithms, potentially inflating the performance gap between DreamerV2 and model-free methods
- Generalization analysis is limited to interpolation within trained distance ranges rather than true extrapolation to novel foraging scenarios
- Ecological validity of the simplified patch-foraging task may not capture the complexity of real biological foraging with stochastic resource dynamics and multi-agent competition

## Confidence

- **High confidence**: DreamerV2 achieves MVT-aligned patch-leaving behavior and outperforms model-free baselines in this specific task
- **Medium confidence**: The world model's anticipatory planning is the primary mechanism driving optimal behavior
- **Medium confidence**: Latent representations capture interpretable ecological concepts

## Next Checks

1. **Ablation study on world model components**: Train DreamerV2 variants with (a) fixed world model (no learning), (b) reduced imagination horizon (H=5), and (c) continuous vs discrete latents. Compare MVT alignment to isolate the contribution of learned predictive capabilities.

2. **Cross-environment generalization test**: Evaluate trained agents on modified foraging environments with (a) different reward decay functions (linear, polynomial), (b) stochastic resource regeneration, and (c) varying observation resolutions. This tests whether world models provide robust generalization beyond the training distribution.

3. **Model-free performance optimization**: Systematically tune PPO hyperparameters (learning rate, network depth, entropy regularization) and implement reward shaping to provide denser intermediate signals. Compare optimized PPO against baseline DreamerV2 to determine if the performance gap is primarily architectural or optimization-related.