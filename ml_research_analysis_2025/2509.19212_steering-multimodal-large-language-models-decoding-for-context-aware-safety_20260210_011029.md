---
ver: rpa2
title: Steering Multimodal Large Language Models Decoding for Context-Aware Safety
arxiv_id: '2509.19212'
source_url: https://arxiv.org/abs/2509.19212
tags:
- safety
- safecode
- visual
- arxiv
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SafeCoDe, a lightweight decoding framework
  that addresses context-aware safety alignment in multimodal large language models
  (MLLMs). The method operates in two stages: contrastive decoding initialization,
  which highlights tokens sensitive to visual context by contrasting real and Gaussian-noised
  images, and global-aware token modulation, which integrates scene-level reasoning
  to dynamically adjust token probabilities for safety-relevant tokens.'
---

# Steering Multimodal Large Language Models Decoding for Context-Aware Safety

## Quick Facts
- **arXiv ID:** 2509.19212
- **Source URL:** https://arxiv.org/abs/2509.19212
- **Reference count:** 40
- **Primary result:** SafeCoDe achieves up to 13.5% higher accuracy on MSSBench and 22% lower rejection rates on MOSSBench while preserving general task performance

## Executive Summary
SafeCoDe introduces a lightweight, inference-time decoding framework that addresses context-aware safety alignment in multimodal large language models (MLLMs). The method operates in two stages: contrastive decoding initialization, which highlights tokens sensitive to visual context by contrasting real and Gaussian-noised images, and global-aware token modulation, which integrates scene-level reasoning to dynamically adjust token probabilities for safety-relevant tokens. SafeCoDe effectively mitigates both oversensitivity (unnecessary refusals of benign queries) and undersensitivity (missed detection of visually grounded risks) by grounding decisions in visual context rather than relying solely on textual priors. Across diverse MLLM backbones and safety benchmarks, SafeCoDe achieves significant improvements while preserving general task performance.

## Method Summary
SafeCoDe is a two-stage inference-time decoding framework that improves context-aware safety alignment in MLLMs. Stage 1 uses contrastive decoding to highlight tokens sensitive to visual context by subtracting logits from a Gaussian-noised image from real-image logits. Stage 2 employs global-aware token modulation that integrates scene-level reasoning from an MLLM judge (GPT-4o or Qwen-2.5-3B-Instruct) to dynamically adjust refusal token probabilities based on a binary safety verdict. The modulation is applied only during early decoding steps (typically steps 2-5) to seed an appropriate safety stance while preserving fluency in later generation. The approach is model-agnostic and demonstrates strong generalizability across different MLLM backbones.

## Key Results
- Achieves up to 13.5% higher accuracy on MSSBench compared to baseline models
- Reduces rejection rates by 22% on MOSSBench while maintaining safety
- Shows strong generalizability to general safety risks and jailbreak scenarios across diverse MLLM backbones

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Decoding for Visual Grounding
Contrasting logits between real and Gaussian-noised images surfaces tokens that depend on visual context rather than textual priors. The difference highlights tokens whose likelihoods depend on meaningful visual content, addressing the unimodal bias where MLLMs anchor refusals on statistical co-occurrence patterns in text while ignoring visual grounding.

### Mechanism 2: Global Safety Verdict for Bidirectional Modulation
Scene-level reasoning from an auxiliary MLLM judge enables context-appropriate refusal behavior in both directions—suppressing unnecessary refusals and enabling necessary ones. A binary verdict s ∈ {safe, unsafe} determines whether refusal token logits are boosted or suppressed, capturing user intent relative to global scene context.

### Mechanism 3: Early-Step Modulation for Safety-Helpfulness Tradeoff
Intervening only during the first few decoding steps (t=2–5) seeds an appropriate safety stance while preserving fluency in later generation. Since refusal phrases are typically prefix tokens, early intervention captures these without over-regularizing the full response.

## Foundational Learning

- **Unimodal Bias in MLLMs**
  - Why needed: Understanding that MLLMs over-rely on textual priors (refusing based on keywords rather than visual context) explains why contrastive decoding is necessary.
  - Quick check question: Given an image-text pair where the image fundamentally changes safety implications (e.g., "How do I swing harder?" in an office vs. a baseball field), does the model's refusal rate actually change?

- **Contrastive Decoding Fundamentals**
  - Why needed: The core technique requires understanding how logit subtraction amplifies differences between inputs.
  - Quick check question: If you subtract logits from a semantically-degraded input from logits from the original input, which features survive in the difference—low-level patterns or high-level semantics?

- **Refusal Token Vocabulary**
  - Why needed: Targeted modulation requires knowing which tokens constitute refusal prefixes (e.g., "I'm sorry", "I cannot", "I apologize").
  - Quick check question: What are the first 3–5 tokens in 90% of your model's refusal responses? Are they consistent or varied?

## Architecture Onboarding

- **Component map:** Image → Captioner → Caption → MLLM Judge → Safety Verdict → Logit Modulation → Contrastive Logits → Modulated Logits → Token Sampling
- **Critical path:**
  1. Pre-generate caption C from image v using Captioner
  2. Query MLLM Judge with (Q, C, v) → verdict s
  3. At each decoding step t (2 ≤ t ≤ 5):
     - Compute contrastive logits: z_cd = z_t(v) − α·z_t(~v)
     - Identify if candidate token x ∈ I_r (refusal space)
     - Adjust: +λ_boost if s=unsafe, −λ_supp if s=safe
     - Sample from softmax(˜ℓ_t)

- **Design tradeoffs:**
  - Judge quality vs. latency: GPT-4o provides accurate verdicts but adds API latency; Qwen-2.5-3B is faster but shows utility degradation on safe cases
  - Modulation window: Steps 1–3 may miss refusal tokens; steps 1–10 may over-regularize
  - λ magnitude: High values force binary behavior; low values produce weak signal

- **Failure signatures:**
  - Oversensitivity persists → contrastive α may be too low; visual grounding insufficient
  - Undersensitivity persists → global verdict may be incorrect; λ_boost too weak
  - Degraded fluency → modulation window too long or λ too aggressive

- **First 3 experiments:**
  1. **Blank-image baseline:** Run contextual safety benchmarks with images replaced by blanks; confirm near-identical performance indicates textual bias requiring Stage 1
  2. **Judge ablation:** Replace GPT-4o judge with Qwen-2.5-3B; measure tradeoff between safety gains and utility loss on benign queries
  3. **Modulation window sweep:** Test t∈{1–3, 2–5, 1–7} on MSSBench/MOSSBench; identify Pareto-optimal window for safety-helpfulness balance

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can SafeCoDe be effectively adapted for fully black-box MLLMs (e.g., GPT-4, Gemini) where internal logits are inaccessible?
- **Basis in paper:** Appendix B states that SafeCoDe currently assumes access to internal logit outputs for token-level modulation, restricting applicability to open-source models.
- **Why unresolved:** The core mechanism relies on direct logit subtraction and modulation, operations typically blocked by proprietary model APIs.
- **Evidence to resolve it:** A successful deployment using surrogate models or approximate probability distributions that maintains safety alignment performance parity.

### Open Question 2
**Question:** Can the token modulation strategy be refined to generate helpful safe alternatives rather than "hard" refusals?
- **Basis in paper:** Appendix B highlights that while SafeCoDe improves precision, it still issues "flat refusals" and suggests future work should focus on "softer and more helpful refusals" offering safe suggestions.
- **Why unresolved:** The current logic boosts probabilities for specific refusal tokens but lacks a mechanism to generate context-specific guidance for safer alternative actions.
- **Evidence to resolve it:** A modified modulation scheme that selectively boosts safe-completion tokens over refusal tokens, resulting in higher helpfulness scores.

### Open Question 3
**Question:** Does adaptive tuning of the modulation window based on input complexity yield better safety-utility trade-offs than the fixed heuristic (steps 2–5)?
- **Basis in paper:** Appendix B notes that the early-stage modulation design is "heuristically set" and "may benefit from adaptive step length tuning based on input complexity or visual ambiguity."
- **Why unresolved:** A fixed step window risks over-regularizing simple benign queries or failing to sufficiently steer generation in complex scenarios.
- **Evidence to resolve it:** A dynamic scheduler that adjusts modulation duration based on a visual ambiguity metric, demonstrating superior utility preservation.

## Limitations
- **External judge dependency:** Relies on an external MLLM judge (GPT-4o) for safety verdicts, creating potential privacy concerns and limiting generalizability across domains or languages
- **Gaussian noise specification:** Vague specification of noise parameters raises questions about whether noise truly preserves low-level visual structure while removing semantic content
- **Modest general safety gains:** Performance improvements on general safety benchmarks (MM-SafetyBench, FigStep, Hades) are modest (2-8%) compared to contextual safety tasks

## Confidence

**High Confidence (Evidence Strongly Supports):**
- The textual bias problem in MLLMs is well-established (Section 1.1, Table 1 showing near-identical performance with real vs blank images)
- The two-stage architecture is clearly specified and implemented (Section 3)
- Safety-helpfulness tradeoffs are empirically demonstrated across multiple benchmarks (Tables 2-4)

**Medium Confidence (Evidence Supports but With Caveats):**
- Contrastive decoding effectively surfaces visual context-dependent tokens (Section 3.2, but relies on ASCD paper for core technique)
- Early-step modulation preserves fluency while improving safety (Section 2.2, but timing heuristic lacks ablation studies)
- Judge verdict integration improves both oversensitivity and undersensitivity (Section 3.3, but Qwen-2.5 judge shows utility degradation)

**Low Confidence (Evidence Weak or Unverified):**
- The method generalizes to novel jailbreak scenarios (Appendix H, but only tested on publicly available examples)
- Performance improvements are robust across all MLLM backbones (Figure 4, but Qwen-2.5 shows inconsistent gains)
- The tradeoff between safety gains and utility loss is optimal (Figure 5, but only shows results for LLaVA-1.6)

## Next Checks
1. **Judge Generalization Test:** Replace GPT-4o with an open-source MLLM judge (e.g., Qwen2.5-7B-Instruct) across all benchmarks to quantify performance degradation and identify domains where the approach breaks down.

2. **Noise Parameter Sensitivity:** Systematically vary Gaussian noise parameters (mean, variance, spatial correlation) and measure their impact on contrastive signal strength and safety performance to determine robustness to implementation details.

3. **Temporal Modality Analysis:** Track safety performance as a function of decoding step for both modulated and unmodulated tokens to verify that refusal decisions are indeed made early and that later tokens don't override the safety stance.