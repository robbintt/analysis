---
ver: rpa2
title: 'Translation via Annotation: A Computational Study of Translating Classical
  Chinese into Japanese'
arxiv_id: '2511.05239'
source_url: https://arxiv.org/abs/2511.05239
tags:
- chinese
- characters
- classical
- japanese
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a computational approach to translating Classical
  Chinese into Japanese using a modern NLP framework. It models the traditional Kundoku
  annotation system as sequence tagging tasks, where Kaeriten marks dictate character
  reordering and Okurigana marks add grammatical information.
---

# Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese

## Quick Facts
- arXiv ID: 2511.05239
- Source URL: https://arxiv.org/abs/2511.05239
- Reference count: 32
- A computational approach to translating Classical Chinese into Japanese using the traditional Kundoku annotation system

## Executive Summary
This paper presents a novel computational approach to translating Classical Chinese into Japanese by modeling the traditional Kundoku annotation system as sequence tagging tasks. The authors address the low-resource challenge of this translation task by constructing a new dataset from digitized open-source translations and introducing an LLM-based annotation pipeline validated by pushdown automata. Their method achieves comparable machine translation performance to existing models while significantly outperforming large language models on character ordering accuracy. The study demonstrates that incorporating auxiliary Chinese NLP tasks enhances model performance, with optimal results achieved using two auxiliary tasks, providing a valuable supplement to LLMs for generating accurate Kundoku annotations.

## Method Summary
The authors model the Kundoku annotation system as sequence tagging tasks, where Kaeriten marks dictate character reordering and Okurigana marks add grammatical information. To address the low-resource challenge, they construct a new dataset from digitized open-source translations and introduce an LLM-based annotation pipeline validated by a pushdown automaton that ensures annotation correctness. The approach incorporates auxiliary Chinese NLP tasks to enhance model performance, with optimal results achieved using two auxiliary tasks. The method is evaluated against existing models and large language models (LLMs), focusing on machine translation performance and character ordering accuracy.

## Key Results
- Achieves comparable machine translation performance to existing models
- Outperforms LLMs on character ordering accuracy, particularly for Kaeriten marks
- Optimal performance achieved using two auxiliary Chinese NLP tasks
- LLM-based annotation pipeline validated by pushdown automaton ensures annotation correctness

## Why This Works (Mechanism)
The approach works by leveraging the structured nature of the Kundoku annotation system, which breaks down the complex translation task into manageable sequence tagging components. By modeling Kaeriten marks (character reordering) and Okurigana marks (grammatical information addition) as separate tagging tasks, the system can handle the structural differences between Classical Chinese and Japanese more effectively than direct translation approaches. The use of auxiliary Chinese NLP tasks provides additional linguistic context that helps the model better understand the source text's meaning and structure, leading to improved translation quality.

## Foundational Learning
- Kundoku annotation system: Traditional Japanese method for reading Classical Chinese texts; needed because it provides the structural framework for translation; quick check: verify understanding of Kaeriten and Okurigana mark functions
- Pushdown automaton validation: Computational model for validating annotation correctness; needed to ensure LLM-generated annotations follow proper structure; quick check: confirm pushdown automaton correctly validates Kaeriten ordering rules
- Sequence tagging tasks: NLP technique for labeling sequential data; needed to model the annotation process as manageable subtasks; quick check: verify sequence tagging accuracy on annotated examples
- Auxiliary Chinese NLP tasks: Additional language processing tasks to enhance understanding; needed to provide contextual information for better translation; quick check: assess impact of different auxiliary task combinations
- Low-resource translation: Challenges of translating between languages with limited parallel data; needed to understand why traditional approaches struggle; quick check: verify dataset size and quality metrics

## Architecture Onboarding

**Component Map**
LLM annotation generator -> Pushdown automaton validator -> Sequence tagging model -> Auxiliary task integration -> Translation output

**Critical Path**
The critical path flows from LLM-generated annotations through pushdown automaton validation to the sequence tagging model, with auxiliary task integration providing parallel enhancement. The translation output depends on successful completion of all preceding steps.

**Design Tradeoffs**
- Using LLMs for annotation generation provides scalability but introduces potential error propagation
- Pushdown automaton validation ensures structural correctness but may not catch semantic errors
- Incorporating auxiliary tasks improves performance but increases model complexity
- Dataset construction from open-source translations ensures data availability but may introduce stylistic biases

**Failure Signatures**
- Incorrect Kaeriten annotations that pass pushdown validation but result in poor character ordering
- Auxiliary task integration that creates conflicting signals for the sequence tagging model
- Dataset bias that causes poor generalization to texts from different classical Chinese periods or genres

**3 First Experiments**
1. Test pushdown automaton validation on a diverse set of manually annotated examples to verify detection accuracy
2. Evaluate sequence tagging performance with different numbers of auxiliary tasks to confirm optimal configuration
3. Compare character ordering accuracy on a held-out test set with varying levels of annotation complexity

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on newly constructed dataset from digitized open-source translations may introduce annotation consistency concerns
- Evaluation framework focuses primarily on machine translation performance and character ordering accuracy without comprehensive assessment of literary quality
- LLM-based annotation generation may propagate systematic errors despite pushdown automaton validation

## Confidence

**High confidence**: The experimental methodology for evaluating machine translation performance against established baselines is sound, and the findings about comparable MT performance while outperforming LLMs on character ordering are well-supported by the reported metrics.

**Medium confidence**: The claim about optimal results with two auxiliary Chinese NLP tasks is supported by ablation studies, but the specific task selection and combination could benefit from more extensive hyperparameter tuning across different classical text genres.

**Medium confidence**: The assertion that the method serves as a valuable supplement to LLMs for Kundoku annotation generation is plausible given the reported Kaeriten annotation improvements, but real-world annotation quality would require human expert evaluation beyond the automated pushdown automaton validation.

## Next Checks
1. Conduct a human evaluation study with classical Chinese and Japanese scholars to assess the literary quality and accuracy of the generated Kundoku annotations compared to traditional manual annotations.

2. Test the model's performance across different classical Chinese genres (philosophical, historical, poetic) to determine if the approach generalizes beyond the current dataset's composition.

3. Implement a long-term stability test by evaluating model performance on texts from different historical periods of classical Chinese to assess temporal robustness of the annotation system.