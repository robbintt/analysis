---
ver: rpa2
title: 'EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation'
arxiv_id: '2509.19770'
source_url: https://arxiv.org/abs/2509.19770
tags:
- data
- translation
- en2x
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underperformance of large language models
  (LLMs) in direct non-English (x2x) translation, where models exhibit significantly
  lower quality compared to English-centric (en2x) translation despite strong overall
  multilingual capabilities. The core method, EnAnchored-X2X, leverages models' established
  English-to-x (en2x) translation strengths by extending English parallel corpora
  into omnidirectional datasets through English-Anchored x2x Translation (EAxT), where
  both source text and its English reference are provided during generation.
---

# EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation

## Quick Facts
- arXiv ID: 2509.19770
- Source URL: https://arxiv.org/abs/2509.19770
- Reference count: 27
- Direct x2x translation quality improves by +7 BLEURT points across 72 language pairs using English-anchored optimization

## Executive Summary
This paper addresses a fundamental limitation in multilingual translation: large language models significantly underperform in direct non-English to non-English (x2x) translation compared to English-centric translation, despite strong overall multilingual capabilities. The proposed EnAnchored-X2X method leverages models' established English-to-x (en2x) translation strengths by extending English parallel corpora into omnidirectional datasets through English-Anchored x2x Translation (EAxT), where both source text and its English reference are provided during generation. Combined with a preference-based optimization framework using synthetic data, the approach achieves substantial improvements across three distinct 7B-parameter base models, demonstrating that English anchoring can effectively bootstrap x2x translation quality without requiring expensive human-annotated data.

## Method Summary
The EnAnchored-X2X method consists of three stages: First, English-Anchored x2x Translation (EAxT) generates high-quality x2x translations by providing both the source text and its English reference to the model during inference. Second, English-Anchored x2x Evaluation (EAxE) trains a reward model on en2x pairs, then uses the English reference as a proxy to score x2x candidates via r(en, translation). Third, preference-based optimization constructs pairs from best/worst-scoring candidates and trains with DPO. The approach generates synthetic data from TowerBlocks en-x parallel data (~150k sentences across 9 languages), creates ~1M x2x translation candidates, and filters to ~140k-250k preference pairs for final training.

## Key Results
- Llama2-7B model shows +7 BLEURT points improvement across 72 x2x directions
- Improvements sustain in en2x performance despite focusing solely on x2x optimization
- Method generalizes across three distinct 7B-parameter base models (Llama2, TowerBase, Qwen2.5)
- Outperforms baselines including pivot translation and fine-tuning on human-annotated FLORES data

## Why This Works (Mechanism)
LLMs struggle with direct x2x translation due to limited parallel data and distributional shift between training and inference. The English-anchored approach exploits the model's strong en2x capabilities by providing English references during generation, which serves as a strong prior that guides the model toward more accurate translations. The English-referenced evaluation proxy converts the challenging x2x evaluation problem into an en2x problem where the model has established competence, enabling effective data filtering and preference learning.

## Foundational Learning
- **English-anchored translation**: Generating x2x translations using both source and English reference as prompts. Why needed: Provides strong prior guidance from the model's strongest translation direction. Quick check: Compare translation quality with vs without English reference.
- **Reward model proxy evaluation**: Using en2x-trained reward model to score x2x translations via r(en, translation). Why needed: Enables effective filtering of synthetic x2x data using established evaluation capability. Quick check: Correlate proxy scores with reference-based BLEURT on holdout set.
- **Preference-based optimization**: Constructing training pairs from best/worst candidates and fine-tuning with DPO. Why needed: Leverages relative preferences rather than absolute quality for more stable training. Quick check: Verify preference pairs show meaningful quality differences.
- **Synthetic data generation**: Creating large-scale x2x training data from limited parallel corpora. Why needed: Addresses data scarcity for direct x2x translation. Quick check: Validate generated data quality before preference construction.

## Architecture Onboarding

**Component Map**
TowerBlocks en-x parallel data -> EAxT generation -> EAxE reward model scoring -> Preference pair construction -> DPO fine-tuning -> Improved x2x translation model

**Critical Path**
The critical path is EAxT generation → EAxE scoring → Preference pair construction, as each stage depends on the previous one and collectively determines the quality of the final fine-tuning data.

**Design Tradeoffs**
The method trades computational cost of generating synthetic data against data annotation costs. Using English references during generation introduces potential distributional shifts but provides strong guidance. The proxy evaluation approach may not perfectly capture x2x quality but enables effective data filtering at scale.

**Failure Signatures**
- Direct x2x generation without English reference yields significantly lower quality (Table 1 shows Direct=78.46 vs EAxT=82.26 GPT4 scores)
- DPO underperforms SFT at small data scales (Figure 3 indicates need for ≥32k preference pairs)
- Reward model scoring fails to correlate with reference-based BLEURT on holdout sets

**3 First Experiments**
1. Test EAxT generation with different prompt templates to verify the claimed improvement from Direct to EAxT generation
2. Train EAxE reward model on en2x data and validate proxy scoring against reference-based BLEURT on holdout set
3. Construct preference pairs from EAxE scoring and verify meaningful quality differences before DPO training

## Open Questions the Paper Calls Out
None

## Limitations
- Exact EAxT prompt templates remain unspecified, with only partial information about 21 variants tested
- Preference data filtering process is opaque, with approximately 80% of pairs discarded based on variable margin thresholds
- Lack of detailed ablations showing whether improvements are consistent across all three tested model architectures

## Confidence
High: The overall methodology and experimental setup are clearly described with specific datasets, model sizes, and evaluation metrics provided.
Medium: Translation quality improvements and generalization across different base models are supported by reported metrics, though lack of public datasets limits independent verification.
Low: Exact prompt templates, specific margin thresholds, and complete ablation studies remain unknown, preventing full reproducibility.

## Next Checks
1. Reconstruct the EAxT prompt template using partial information provided and test generation quality with vs without English reference to verify claimed improvement from Direct (78.46) to EAxT (82.26) GPT4 scores.
2. Implement the EAxE evaluation pipeline and test correlation between reward model scores and reference-based BLEURT scores on held-out en2x validation set.
3. Train preference reward model on synthetic en2x data as described and test its ability to discriminate between high and low-quality x2x translations before full DPO fine-tuning.