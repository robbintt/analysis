---
ver: rpa2
title: Learning with Expert Abstractions for Efficient Multi-Task Continuous Control
arxiv_id: '2503.14809'
source_url: https://arxiv.org/abs/2503.14809
tags:
- learning
- policy
- reward
- abstractions
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Goal-Conditioned Reward Shaping (GCRS), a
  hierarchical reinforcement learning approach that leverages expert-defined abstractions
  to enable efficient learning in continuous multi-task environments with sparse rewards.
  The method dynamically plans over an expert-provided abstract model to generate
  subgoals, which are then used to condition a goal-conditioned policy.
---

# Learning with Expert Abstractions for Efficient Multi-Task Continuous Control

## Quick Facts
- **arXiv ID**: 2503.14809
- **Source URL**: https://arxiv.org/abs/2503.14809
- **Reference count**: 17
- **Primary result**: GCRS achieves over 90% success rate on harder evaluation tasks and 60% success on zero-shot generalization to new object colors in the ObjectDelivery task, outperforming baseline methods across all tested metrics.

## Executive Summary
This paper introduces Goal-Conditioned Reward Shaping (GCRS), a hierarchical reinforcement learning approach that leverages expert-defined abstractions to enable efficient learning in continuous multi-task environments with sparse rewards. The method dynamically plans over an expert-provided abstract model to generate subgoals, which are then used to condition a goal-conditioned policy. To address the challenges of sparse rewards, GCRS employs potential-based reward shaping using the optimal state values from the abstract model. The approach is evaluated on procedurally generated continuous control environments, demonstrating superior sample efficiency, task completion rates, scalability, and zero-shot generalization compared to existing hierarchical RL methods.

## Method Summary
GCRS is a hierarchical reinforcement learning approach that integrates expert abstractions into the learning process. It operates by planning over an expert-provided abstract model to generate subgoals, which are then used to condition a goal-conditioned policy. The method addresses the challenge of sparse rewards through potential-based reward shaping, using optimal state values from the abstract model. This combination of hierarchical planning and reward shaping enables efficient learning in continuous multi-task environments where traditional RL approaches struggle due to the difficulty of exploration and credit assignment.

## Key Results
- GCRS achieves over 90% success rate on harder evaluation tasks
- Demonstrates 60% success rate on zero-shot generalization to new object colors in ObjectDelivery task
- Outperforms baseline hierarchical RL methods across all tested metrics including sample efficiency, task completion rates, and scalability

## Why This Works (Mechanism)
The method works by decomposing the problem into two levels: a high-level planner that operates on an abstract model to generate subgoals, and a low-level goal-conditioned policy that learns to achieve these subgoals. The expert abstraction provides a compressed representation of the environment that captures essential state transitions, enabling more efficient planning. The potential-based reward shaping uses the optimal state values from this abstract model to provide dense reward signals that guide the low-level policy toward the subgoals, effectively mitigating the exploration challenges posed by sparse rewards in the original task.

## Foundational Learning
1. **Hierarchical Reinforcement Learning** - needed because single-level policies struggle with long-horizon tasks; quick check: can the agent decompose complex tasks into manageable sub-tasks?
2. **Goal-Conditioned Policies** - needed because conditioning on subgoals enables more efficient learning than trying to learn a monolithic policy; quick check: can the policy generalize across different goal specifications?
3. **Reward Shaping** - needed because sparse rewards make credit assignment difficult; quick check: does the shaping preserve the optimal policy while providing more informative gradients?
4. **Expert Abstractions** - needed because manually designed abstractions can capture domain knowledge that speeds learning; quick check: is the abstract model a valid abstraction of the true environment dynamics?
5. **Potential-Based Reward Shaping** - needed because this form of shaping guarantees policy invariance; quick check: does the shaping function satisfy the potential-based conditions?
6. **Continuous Control** - needed because the method must handle the high-dimensional action spaces typical of robotic control tasks; quick check: can the policy produce smooth, continuous actions?

## Architecture Onboarding

**Component Map**: Expert Abstract Model -> High-Level Planner -> Subgoal Generator -> Goal-Conditioned Policy -> Environment

**Critical Path**: The high-level planner generates subgoals from the abstract model, which are then used to condition the goal-conditioned policy. The policy's performance is evaluated in the environment, with rewards shaped using the optimal state values from the abstract model. This creates a feedback loop where the abstract model guides learning through both subgoal generation and reward shaping.

**Design Tradeoffs**: The method trades the need for expert knowledge (in the form of an abstract model) for improved sample efficiency and learning stability. This makes it particularly suitable for domains where expert knowledge is available but data collection is expensive. The hierarchical structure also introduces additional complexity in training and coordination between levels.

**Failure Signatures**: Poor performance may indicate issues with the expert abstraction (if it doesn't capture critical transitions), inadequate shaping (if the potential function doesn't provide sufficient guidance), or misalignment between the abstract and concrete state spaces (if subgoals generated by the planner are not achievable by the low-level policy).

**3 First Experiments**:
1. Verify that the goal-conditioned policy can learn to achieve simple, single-step subgoals from the abstract model
2. Test the reward shaping component in isolation to confirm it provides denser gradients without changing the optimal policy
3. Evaluate the complete hierarchical system on a simple navigation task to confirm the planning and execution pipeline works end-to-end

## Open Questions the Paper Calls Out
None

## Limitations
- Method's effectiveness is contingent on the quality and completeness of the expert-provided abstract model
- Assumes access to an expert who can provide the abstraction, which may not be available in all real-world scenarios
- Scalability to significantly more complex tasks with larger state spaces remains an open question

## Confidence

**High Confidence**:
- Superior sample efficiency and task completion rates compared to baseline hierarchical RL methods
- Achievement of over 90% success rate on harder evaluation tasks

**Medium Confidence**:
- Zero-shot generalization capabilities, particularly the 60% success rate on novel object colors
- Outperformance across all tested metrics, given the limited set of baselines and environments tested

## Next Checks
1. Evaluate GCRS on more complex, real-world robotic control tasks with higher-dimensional state spaces to assess scalability
2. Test the method's robustness to varying qualities of expert abstractions by systematically degrading the abstract model
3. Conduct ablation studies to quantify the individual contributions of the goal-conditioned policy, reward shaping, and abstract model planning components