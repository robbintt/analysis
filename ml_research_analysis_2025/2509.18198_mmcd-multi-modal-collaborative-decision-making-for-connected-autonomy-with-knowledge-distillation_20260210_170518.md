---
ver: rpa2
title: 'MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with
  Knowledge Distillation'
arxiv_id: '2509.18198'
source_url: https://arxiv.org/abs/2509.18198
tags:
- data
- vehicles
- vehicle
- lidar
- connected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust decision-making in autonomous
  driving under challenging conditions where sensor failures or missing connected
  vehicles can occur. The authors propose a novel Multi-Modal Collaborative Decision-making
  (MMCD) framework that fuses observations from ego and collaborative vehicles across
  multiple modalities (RGB and LiDAR) to enhance decision-making.
---

# MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation

## Quick Facts
- arXiv ID: 2509.18198
- Source URL: https://arxiv.org/abs/2509.18198
- Reference count: 40
- Primary result: Multi-modal fusion with knowledge distillation improves driving safety by up to 20.7% in accident-prone scenarios while maintaining performance with missing modalities

## Executive Summary
This paper addresses robust decision-making for connected autonomous vehicles facing sensor failures or missing collaborative vehicles. The proposed MMCD framework fuses RGB and LiDAR observations from ego and collaborative vehicles using cross-attention aggregation. To handle modality unavailability during testing, the authors introduce cross-modal knowledge distillation, training a teacher model on multi-modal data and a student model on single modalities that learns from the teacher's soft targets. Experiments demonstrate significant safety improvements over baselines while maintaining robustness when certain data modalities are missing.

## Method Summary
MMCD employs a teacher-student architecture where the teacher combines RGB and LiDAR features from multiple vehicles through cross-attention aggregation, while the student learns to operate with reduced modalities. The RGB encoder uses ResNet-18 with self-attention producing 256-dim features, while the LiDAR encoder employs Point Transformer producing 128-dim features. Features are fused via concatenation and processed by a 3-layer MLP for binary braking decisions. Knowledge distillation transfers information from the multi-modal teacher to single-modal students using KL-divergence loss combined with standard binary cross-entropy.

## Key Results
- Multi-modal fusion (RGB+LiDAR) improves accident detection rate by 8-11% over single modalities in collaborative scenarios
- Knowledge distillation enables RGB-only student to achieve performance within 2-3% of full multi-modal teacher
- Cross-attention aggregation outperforms feature concatenation by ~1.9% in decision accuracy
- Communication overhead is minimal: 66.5KB for full multi-modal sharing vs 4.9KB for RGB-only

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Knowledge Distillation for Missing Modality Robustness
A student model trained only on RGB can approximate multi-modal performance when guided by a teacher trained on RGB+LiDAR. The teacher produces soft targets (probability distributions) that encode richer information than hard labels. The student learns to match these soft targets via KL-divergence loss, forcing it to learn implicit depth and spatial cues from RGB. The loss function balances distillation and task losses: $L_S = (1-\alpha)L_{BCE}(y,S) + \alpha t^2 L_{KD}(S,T)$ with $\alpha=0.5$. Break condition: If LiDAR and RGB features are highly redundant or the teacher produces overconfident outputs, distillation transfers noise rather than useful knowledge.

### Mechanism 2: Cross-Attention Aggregation for Collaborative Feature Fusion
Cross-attention between ego and collaborator features improves decision-making by selectively weighting the most relevant collaborator information. Ego vehicle features form the query ($Q$), collaborator features form keys ($K$) and values ($V$). Attention weights computed via scaled dot-product determine each collaborator's relevance. The final RGB embedding combines ego features and attention-weighted collaborator features. Break condition: If all collaborators are equally relevant or attention learns spurious correlations, performance degrades under distribution shift.

### Mechanism 3: Multi-Modal Feature Concatenation for Joint Decision Making
Fusing RGB and LiDAR embeddings provides complementary cues (appearance + geometry) for superior accident detection. RGB captures semantic/contextual cues; LiDAR captures precise geometric cues. These are concatenated and passed to a 3-layer MLP to predict braking probability. Break condition: If one modality consistently dominates or modalities are misaligned, concatenation fuses conflicting cues, potentially degrading performance.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student Framework)**: Transfers knowledge from multi-modal teacher to single-modal student for robustness when modalities are missing. Quick check: Why do soft targets (probability distributions) provide more information than hard labels (one-hot class indices)?

- **Cross-Attention Mechanism**: Aggregates features from multiple collaborators by learning which are most relevant to the ego vehicle's decision. Quick check: In the attention equation $A = \text{softmax}(QK^T/\sqrt{d})$, what do $Q$, $K$, and $V$ represent in this specific collaborative driving context?

- **Multi-Modal Fusion (RGB + LiDAR)**: Combines complementary strengths of RGB (semantic richness) and LiDAR (geometric precision) for more robust perception and decision-making. Quick check: What specific type of information does LiDAR provide that RGB cameras struggle with, particularly in accident-prone driving scenarios?

## Architecture Onboarding

- **Component map**: RGB Encoder (ResNet-18 + self-attention + 3 conv layers → 256-dim) -> RGB Aggregator (Cross-Attention → Weighted sum) -> LiDAR Encoder (Point Transformer → 128-dim) -> Decision MLP (3-layer MLP takes concatenated features → Binary output)

- **Critical path**: 1) Multi-modal data collection from CARLA/AutoCast simulation, 2) Train Teacher Model (RGB+LiDAR) with BCE loss, 3) Train Student Model (RGB-only) with KD loss from Teacher + BCE loss, 4) Deploy Student or Teacher depending on available modalities

- **Design tradeoffs**: Complexity vs. Robustness (Teacher more accurate but requires LiDAR; Student less accurate but works with RGB-only), Communication vs. Performance (feature embeddings cheaper than raw data but may lose information), Attention vs. Concatenation (cross-attention improves performance but adds computational overhead)

- **Failure signatures**: Significant performance drop from Case C to Case D indicates KD is not effectively transferring knowledge, Student performs worse than Case A suggests negative transfer, Performance degrades with more collaborators suggests attention mechanism is learning to ignore collaborators

- **First 3 experiments**: 1) Reproduce KD Ablation: Train Student with and without KD loss, compare Case D vs Case A, 2) Attention vs Concatenation Ablation: Replace cross-attention with simple concatenation, compare performance, 3) Modality Ablation: Evaluate all four cases on new challenging scenario to test generalization

## Open Questions the Paper Calls Out

- **State Estimation Under Noise**: Future work could explore state estimation strategies of vehicles under noises and uncertainties, as the current framework assumes accurate vehicle position detection.

- **Additional Sensor Modalities**: Exploring integration of other sensor modalities, such as radar or infrared, could further enhance decision-making capabilities beyond the current RGB and LiDAR focus.

- **Continuous Control Generalization**: The binary decision-making formulation may not generalize to continuous control tasks required for complex navigation, as the policy output is specifically defined as braking probability.

- **Real-World Deployment**: The framework's robustness when deployed in physical real-world environments versus simulation environments used for training remains untested, with potential performance degradation due to the Sim-to-Real gap.

## Limitations

- Critical architectural parameters remain unspecified including MLP dimensions, Point Transformer configuration, and LiDAR preprocessing specifics
- Dataset construction details are incomplete regarding data collection protocol and sensor calibration between vehicles
- Limited ablation studies prevent comprehensive understanding of design choice impacts

## Confidence

**High Confidence**:
- Cross-modal knowledge distillation improves RGB-only performance when LiDAR is available at training but missing at test
- Multi-modal fusion (RGB+LiDAR) outperforms single modalities
- Cross-attention aggregation improves over simple concatenation

**Medium Confidence**:
- Generalizability across different accident scenarios
- Communication efficiency claim without detailed bandwidth analysis

**Low Confidence**:
- Performance under real-world conditions (tested only in simulation)
- Behavior with more than 4 collaborative vehicles (tested only up to 4)

## Next Checks

1. **Temperature Sensitivity Analysis**: Systematically vary the distillation temperature parameter t from 1.0 to 5.0 in increments of 0.5 and measure impact on student performance

2. **Cross-Scenario Generalization Test**: Evaluate the complete MMCD system on a fourth, previously unseen accident scenario in CARLA to test whether the 20.7% improvement generalizes

3. **Real-Time Communication Overhead Validation**: Implement the feature-sharing protocol and measure actual communication latency, bandwidth usage, and processing overhead on a realistic vehicular network simulation