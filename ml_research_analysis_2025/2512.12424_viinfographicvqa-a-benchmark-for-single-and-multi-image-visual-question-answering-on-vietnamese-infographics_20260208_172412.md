---
ver: rpa2
title: 'ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering
  on Vietnamese Infographics'
arxiv_id: '2512.12424'
source_url: https://arxiv.org/abs/2512.12424
tags:
- question
- multi-image
- reasoning
- images
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViInfographicVQA, the first Vietnamese benchmark
  for visual question answering on infographics. It includes over 6,747 real-world
  infographics and 20,409 human-verified question-answer pairs covering topics such
  as economics, healthcare, and education.
---

# ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics

## Quick Facts
- **arXiv ID:** 2512.12424
- **Source URL:** https://arxiv.org/abs/2512.12424
- **Reference count:** 11
- **Key outcome:** First Vietnamese VQA benchmark with 6,747 infographics and 20,409 QA pairs, evaluating both Single and Multi-image reasoning tasks.

## Executive Summary
This paper introduces ViInfographicVQA, the first Vietnamese benchmark for visual question answering on infographics. It includes over 6,747 real-world infographics and 20,409 human-verified question-answer pairs covering topics such as economics, healthcare, and education. The benchmark supports both Single-image and Multi-image reasoning tasks, the latter requiring synthesis across multiple related infographics. To create the dataset, the authors used a two-stage pipeline involving VLM-assisted element extraction and question generation, followed by human verification. They evaluated recent vision-language models on the benchmark, finding that while models perform reasonably on extractive, Single-image tasks, their performance drops substantially on Multi-image and non-extractive reasoning. Fine-tuning improved results, especially for cross-image tasks. The study highlights the challenges of layout-aware and cross-image reasoning in low-resource language settings and provides a foundation for future research in this area.

## Method Summary
The authors created ViInfographicVQA using a two-stage pipeline: first, VLMs extracted text and visual elements from 6,747 Vietnamese infographics; second, these elements were used to generate question-answer pairs which were then human-verified. The dataset contains 20,409 QA pairs spanning economics, healthcare, and education. For evaluation, they fine-tuned Qwen2.5-VL-7B using QLoRA (r=8, alpha=32, lr=2e-4) on combined Single and Multi-image data for 5 epochs, achieving the best overall performance on the benchmark's Average Normalized Levenshtein Similarity metric.

## Key Results
- Models perform reasonably on extractive Single-image tasks but struggle significantly on Multi-image and non-extractive reasoning
- Fine-tuning improves results, especially for cross-image tasks
- Multi-image tasks show 12-32 ANLS point drop compared to Single-image tasks
- Best performance achieved through joint training on both Single and Multi-image subsets

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering for Cross-Image Contextualization
Organizing infographics into semantically related sets enables evaluation of complex cross-image reasoning that purely random image groupings cannot support. The authors compute multimodal representations (OCR + visual cues) to cluster images by topic (e.g., weather, economics). Questions are then generated specifically to require evidence from multiple images within these clusters, forcing models to synthesize information across distinct visual contexts rather than retrieving from a single source. Core assumption: Models capable of reasoning should be able to identify and aggregate semantically related data points spread across different visual layouts.

### Mechanism 2: Set-Level Data Isolation to Prevent Leakage
Assigning entire multi-image groups exclusively to a single split (train or test) is necessary to prevent artificial inflation of performance metrics. In multi-image tasks, if Image A appears in a training group and Image B (visually similar or related) appears in a test group, a model might learn spurious correlations. By ensuring that all images in a specific semantic cluster belong to the same split, the benchmark forces the model to generalize reasoning capabilities rather than memorizing visual templates or specific facts. Core assumption: Valid reasoning generalization requires testing on entirely unseen semantic clusters or image groups.

### Mechanism 3: Heterogeneous Supervision for Reasoning Transfer
Joint training on both single-image (extraction) and multi-image (synthesis) tasks improves overall performance more effectively than training on either alone. Fine-tuning on single-image data refines the model's ability to perform OCR and localize specific details (grounding). Fine-tuning on multi-image data teaches the model to compare and aggregate these details. Combining them allows the model to learn robust grounding and apply it in complex scenarios, preventing the "catastrophic forgetting" of basic extraction skills when learning complex reasoning. Core assumption: The visual-textual alignment skills learned in single-image tasks are prerequisites that transfer to multi-image synthesis.

## Foundational Learning

- **Concept: Average Normalized Levenshtein Similarity (ANLS)**
  - **Why needed here:** Standard exact-match accuracy is too harsh for OCR-based VQA, as it fails if a model extracts "31C" but the ground truth is "31°C". ANLS handles this fuzzy matching.
  - **Quick check question:** If a model predicts "Ha Noi" and the ground truth is "Hanoi", would ANLS score this as 0 or a value close to 1? (Answer: Close to 1, provided the normalized distance is below the 0.5 threshold).

- **Concept: Layout-Aware Reasoning**
  - **Why needed here:** Infographics convey meaning through spatial arrangement (e.g., a bar chart's height or a map's color key). Models must treat layout as a signal, not noise.
  - **Quick check question:** Why does a standard CNN trained on natural images often fail to read a table in an infographic? (Answer: It lacks the inductive bias or attention mechanism to parse grid-like structures or text-heavy regions as semantic units).

- **Concept: Cross-Image Attention / Context Integration**
  - **Why needed here:** For Multi-image VQA, the model cannot just process Image A and Image B independently; it must attend to relevant regions in B while reasoning about A.
  - **Quick check question:** In a multi-image model, what is the risk of processing images sequentially without a global context vector? (Answer: The model might "forget" details from Image A by the time it processes Image B, or fail to correlate specific details between them).

## Architecture Onboarding

- **Component map:** Input -> Image Encoder (ViT) + Text Tokenizer -> Projector -> LLM Backbone -> Task Heads -> Text generation
- **Critical path:** 1. Ingestion: Load 1-N images. 2. Encoding: Pass images through Vision Encoder. 3. Projection: Map visual features to LLM tokens. 4. Context Building: Concatenate question tokens + image tokens. 5. Generation: LLM produces answer.
- **Design tradeoffs:** Resolution vs. Context (high-res infographics need more tokens, reducing space for multiple images); VLM Choice (InternVL/Ovis might have better OCR/native resolution support than standard CLIP-based models); Zero-shot vs. Fine-tuning (zero-shot is cheaper but fails on complex arithmetic).
- **Failure signatures:** Hallucination (answering "Yes" to a non-extractive question that requires a number); OCR Miss (failing to read rotated text or small legends); Cross-Image Confusion (attributing a fact from Image 1 to Image 2); ANLS Drop (significant score drop from Single to Multi-image tasks indicates failure in context integration).
- **First 3 experiments:** 1. Zero-Shot Baseline: Evaluate a strong VLM on test set without modification. 2. Ablation on Supervision: Fine-tune on Single-image only, then evaluate on Multi-image to measure cross-task transfer. 3. Context Limit Test: Evaluate Multi-image performance while limiting visual tokens to simulate "context pressure".

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can vision-language model architectures be augmented to explicitly handle information integration across documents, specifically to reduce the performance gap between Single-image and Multi-image reasoning?
- **Basis in paper:** The Discussion notes that current models "lack explicit mechanisms for layout-aware reasoning and information integration across documents," which the authors identify as a key bottleneck given the 12–32 ANLS point drop in Multi-image tasks.
- **Why unresolved:** Standard VLM pipelines rely on "shallow OCR-language fusion" which fails to effectively synthesize evidence from semantically related but visually distinct infographics.
- **What evidence would resolve it:** A novel model architecture incorporating memory or alignment modules that achieves parity between Single-image and Multi-image ANLS scores on the ViInfographicVQA benchmark.

### Open Question 2
- **Question:** What specific evaluation protocols can effectively complement ANLS to measure numerical exactness and grounding faithfulness for non-extractive VQA tasks?
- **Basis in paper:** The authors explicitly state that future evaluations should "complement ANLS with task-specific measures (e.g., exact/relative numeric accuracy, unit normalization), faithfulness/attribution checks."
- **Why unresolved:** The current ANLS metric tolerates minor textual deviations, masking errors in semantic correctness and numerical precision, particularly for arithmetic operations.
- **What evidence would resolve it:** The proposal and validation of a new metric that shows stronger correlation with human judgment than ANLS on the "Non-extractive" and "Non-span" categories.

### Open Question 3
- **Question:** Can tool-augmented reasoning methods (e.g., external calculators or Python interpreters) significantly outperform standard fine-tuning on the dataset's numerical and non-extractive reasoning categories?
- **Basis in paper:** The Conclusion encourages future exploration into "tool-augmented arithmetic and table/chart reasoning" to address the struggles models face with non-extractive questions.
- **Why unresolved:** Fine-tuning yields only moderate improvements on non-extractive tasks, suggesting that parametric memory is insufficient for precise numerical synthesis.
- **What evidence would resolve it:** A tool-augmented VLM baseline that achieves a significantly higher ANLS on the "Non-extractive" category compared to the current fine-tuned Qwen2.5-VL-7B baseline (63.93%).

## Limitations

- **Language Generalization Gap:** The benchmark focuses exclusively on Vietnamese infographics, creating an inherent limitation for cross-linguistic generalization.
- **Multi-Image Task Complexity Calibration:** The current setup may not fully capture the spectrum of real-world multi-image analysis scenarios involving temporal sequences or hierarchical relationships.
- **Evaluation Metric Constraints:** ANLS may not fully capture semantic correctness in cases where numerical precision or categorical distinctions matter.

## Confidence

- **High Confidence:** Dataset construction methodology and split isolation strategy are well-documented and reproducible.
- **Medium Confidence:** Performance claims regarding Single-image versus Multi-image tasks are supported empirically but may vary with different VLMs or fine-tuning approaches.
- **Low Confidence:** Claims about low-resource VQA challenges are somewhat speculative as evaluation is limited to Vietnamese.

## Next Checks

1. **Cross-Linguistic Transferability Test:** Evaluate the benchmark's core challenges using a parallel dataset in a different language script to assess whether difficulties are language-specific or universal.
2. **Intermediate Complexity Benchmark:** Develop and evaluate tasks bridging Single-image extraction and full Multi-image synthesis to better understand reasoning progression.
3. **Metric Sensitivity Analysis:** Conduct controlled experiments varying the ANLS threshold and comparing results with alternative metrics to determine how metric choice affects performance gap perception.