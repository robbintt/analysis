---
ver: rpa2
title: 'LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated
  via Semi-Automatic Clustering'
arxiv_id: '2504.02060'
source_url: https://arxiv.org/abs/2504.02060
tags:
- being
- hand
- dataset
- lifelog
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LSC-ADL, a novel lifelog dataset enriched with
  Activity of Daily Living (ADL) annotations derived from the LSC dataset. The dataset
  was generated using a semi-automatic clustering approach combining the HDBSCAN algorithm
  with human-in-the-loop verification to ensure annotation accuracy.
---

# LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated via Semi-Automatic Clustering

## Quick Facts
- arXiv ID: 2504.02060
- Source URL: https://arxiv.org/abs/2504.02060
- Reference count: 40
- Primary result: Novel lifelog dataset with 35 ADL classes generated via semi-automatic clustering with human verification

## Executive Summary
This paper introduces LSC-ADL, a novel lifelog dataset enriched with Activity of Daily Living (ADL) annotations derived from the LSC dataset. The dataset was generated using a semi-automatic clustering approach combining the HDBSCAN algorithm with human-in-the-loop verification to ensure annotation accuracy. ADL labels were selected through a systematic pipeline that incorporated insights from existing datasets and used GPT-based techniques to align labels with the lifelogger's daily activities. The resulting dataset contains 35 distinct ADL classes and addresses the challenge of incorporating activity-level information into lifelog retrieval systems. By capturing temporal relationships between frames, LSC-ADL offers a more semantically rich understanding of daily activities compared to traditional visual feature-based approaches.

## Method Summary
The method employs a 4-stage iterative pipeline: (1) Golden corpus initialization using GPT to generate egocentric descriptions and SnapSeek retrieval to obtain ~300 seed samples per ADL class, (2) Label suggestion via cosine similarity between cluster representative vectors and predicted labels with threshold filtering, (3) Human approval where 20 annotators verify suggested labels using predefined rules, and (4) Cluster update using HDBSCAN to re-cluster within each class and update representative vectors. This process repeats for two iterations, resulting in ~525,000 annotated images (75% of LSC dataset). The approach leverages multi-cluster representation within each ADL class to capture scenario variations and maintain semantic richness.

## Key Results
- Successfully generated ADL annotations for 525,000 images (75% of LSC dataset)
- Created 35 distinct ADL classes capturing diverse daily activities
- Dataset exhibits long-tailed distribution with 80% of samples in 10 classes
- Semi-automatic pipeline balances automation with human verification for accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-cluster representation within each ADL class may improve label prediction accuracy over single-vector approaches.
- Mechanism: HDBSCAN groups images by density, creating multiple representative vectors per class that capture scenario variations (e.g., "using a computer" at desk vs. in meeting). Instead of one centroid, each class maintains several cluster centroids for similarity matching.
- Core assumption: Same activity manifests in visually distinct but semantically equivalent scenarios that should be modeled separately.
- Evidence anchors:
  - [abstract] "semi-automatic approach featuring the HDBSCAN algorithm for intra-class clustering"
  - [Section 3.3, Stage 4] "each cluster within a class might represent a different scenario of that activity... we utilize multiple average vectors corresponding to different clusters within each class"
  - [corpus] Weak direct evidence; related ADL work focuses on multi-modal fusion rather than intra-class clustering strategies.

### Mechanism 2
- Claim: Iterative human-in-the-loop verification progressively expands annotated coverage while maintaining quality.
- Mechanism: Approved labels expand cluster membership → re-clustering updates representative vectors → next prediction round covers more variance. Two full cycles balance growth vs. error propagation.
- Core assumption: Human annotators apply consistent labeling criteria, and errors don't accumulate across iterations.
- Evidence anchors:
  - [abstract] "human-in-the-loop verification to ensure annotation accuracy"
  - [Section 3.3, Stage 3] "20 participants, each tasked with confirming or rejecting the suggested labels... rule set is continuously refined based on annotator feedback"
  - [corpus] Related HAR/ADL datasets (CASAS, ARAS) rely on expert annotation but lack iterative pipeline comparisons.

### Mechanism 3
- Claim: LLM-generated egocentric descriptions can bootstrap relevant image retrieval for cold-start annotation.
- Mechanism: GPT generates scenario descriptions from ADL class names → SnapSeek retrieves visually similar images → creates ~300 seed samples per class without manual hunting.
- Core assumption: LLM-generated descriptions adequately capture egocentric visual patterns for the target lifelogger's context.
- Evidence anchors:
  - [abstract] "ADL labels were selected through a systematic pipeline that incorporated insights from existing datasets and used GPT-based techniques"
  - [Section 3.2] "utilize Llama-3.2-11b-Vision to produce egocentric captions... employ the SnapSeek retrieval system to retrieve relevant images"
  - [corpus] OpenLifelogQA and related lifelog QA work shows LLM potential for lifelog understanding; direct evidence for retrieval bootstrapping remains limited.

## Foundational Learning

- **Concept: HDBSCAN density-based clustering**
  - Why needed here: Core algorithm for discovering variable-density clusters without pre-specifying cluster count.
  - Quick check question: Why would k-means fail on intra-class activity clustering where scenarios have unequal sample sizes?

- **Concept: Egocentric vision characteristics**
  - Why needed here: First-person viewpoint introduces unique challenges (hand occlusion, motion blur, head movement) affecting feature extraction.
  - Quick check question: What biases might an egocentric dataset introduce compared to third-person activity datasets?

- **Concept: Long-tailed class distribution handling**
  - Why needed here: 80% of samples in 10 classes; 11 classes have <1,000 samples—standard classifiers may ignore tail classes.
  - Quick check question: What evaluation metrics would reveal performance gaps between head and tail classes?

## Architecture Onboarding

- **Component map:**
  Label Selection Pipeline: Raw labels (4 datasets) → LLaMA captions (20 days × 15 images) → GPT categorization → 35 ADL classes
  Annotation Pipeline (4-stage loop):
  1. Golden Corpus: Decompose-Expand prompts → SnapSeek retrieval → ~300 seeds/class
  2. Label Suggestion: Cosine similarity to cluster representatives + threshold filtering
  3. Human Approval: 20 annotators, rule-based verification
  4. Cluster Update: HDBSCAN re-clustering → multi-vector class representation
  Output: ~525,000 annotated images (75% of LSC dataset)

- **Critical path:**
  1. Verify golden corpus seeds are high-precision (manual spot-check)
  2. Monitor similarity threshold vs. coverage tradeoff per iteration
  3. Track inter-annotator agreement to catch rule ambiguity early
  4. Measure class distribution drift after each iteration

- **Design tradeoffs:**
  - 35 classes: Broad coverage vs. label ambiguity at boundaries
  - 2 iterations: Coverage gain vs. error propagation risk
  - Threshold-based filtering: Precision vs. recall (unlabeled tail images)

- **Failure signatures:**
  - Class imbalance widening after iterations (head classes grow faster)
  - Annotator disagreement spiking on specific classes (rule ambiguity)
  - Clusters merging across semantically distinct activities

- **First 3 experiments:**
  1. **Seed quality audit:** Manually verify 50 random seeds per class for precision; identify low-quality classes early.
  2. **Threshold sweep:** Run Stage 2 with threshold values [0.3, 0.5, 0.7] on a held-out day; plot precision/recall curves.
  3. **Inter-annotator agreement study:** Have 5 annotators label the same 200-image subset; compute Cohen's kappa per class.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does the integration of explicit ADL annotations significantly improve the accuracy of lifelog retrieval systems compared to visual-only baselines?
  - Basis in paper: [explicit] The authors state the dataset is released to "encourage lifelog researchers to validate our hypothesis that this metadata could enhance the lifelog retrieval systems' performance."
  - Why unresolved: The paper details the dataset creation methodology but does not publish benchmark results comparing retrieval performance with and without the new ADL annotations.
  - Evidence to resolve it: Comparative experiments on the LSC dataset measuring Mean Average Precision (mAP) for standard queries using baseline visual features versus pipelines enriched with LSC-ADL labels.

- **Open Question 2**
  - Question: To what extent do structured ADL labels enhance the explainability of retrieval results for end-users?
  - Basis in paper: [explicit] The abstract claims the annotations are generated to "enhance retrieval explainability," and the conclusion posits that the dataset provides a "more semantically rich representation."
  - Why unresolved: The paper asserts the potential for explainability but provides no user studies or qualitative analysis demonstrating that users find ADL-based results more interpretable than standard retrieval outputs.
  - Evidence to resolve it: A user study where participants rate their understanding of "why" specific moments were retrieved, comparing results explained by ADL labels versus those explained by visual concepts alone.

- **Open Question 3**
  - Question: How can activity recognition models effectively mitigate the impact of the extreme long-tailed distribution observed in the dataset?
  - Basis in paper: [inferred] The Exploratory Data Analysis (Section 4) reveals a severe imbalance where 80% of data belongs to just 10 classes, while 11 classes have fewer than 1,000 samples.
  - Why unresolved: While the paper highlights the personal nature of the data causing this distribution, it does not propose or evaluate specific algorithms (e.g., few-shot learning, re-sampling) to handle the sparsity of the tail classes during training.
  - Evidence to resolve it: An evaluation of classification models on the LSC-ADL dataset specifically reporting per-class F1-scores or accuracy on the sparse tail classes to identify effective balancing strategies.

## Limitations

- Unknown visual feature extractor architecture and weights critical for similarity computation
- Unspecified HDBSCAN hyperparameters (min_cluster_size, min_samples, metric) limiting reproducibility
- Lack of inter-annotator agreement data and analysis of labeling rule evolution across iterations
- No reported precision/recall metrics for the golden corpus or overall annotation quality

## Confidence

- **High Confidence**: The dataset creation methodology (semi-automatic clustering with human verification) is clearly described and logically structured. The resulting dataset contains 35 ADL classes with ~525K annotated images, and the long-tailed distribution pattern is consistent with activity recognition datasets.
- **Medium Confidence**: The claim that multi-cluster representation captures scenario variations is plausible given HDBSCAN's density-based nature, but lacks direct empirical validation in this specific context. The iterative pipeline's effectiveness in expanding coverage while maintaining quality is reasonable but unproven without reported metrics.
- **Low Confidence**: The effectiveness of GPT-based seed retrieval for cold-start annotation in this egocentric context is the weakest claim, with minimal supporting evidence from the corpus and no reported precision/recall metrics for the golden corpus.

## Next Checks

1. **Annotation Quality Audit**: Manually verify 50 random seeds per ADL class to establish baseline precision, then compute inter-annotator agreement on 200 images using multiple annotators to identify rule ambiguities or systematic labeling errors.

2. **Coverage vs. Accuracy Tradeoff**: Run the full 4-stage pipeline with three different similarity thresholds (0.3, 0.5, 0.7) on a held-out day, measuring both per-class annotation coverage and estimated accuracy through spot-checking, to identify optimal threshold settings.

3. **Temporal Pattern Validation**: Extract temporal sequences from the annotated dataset and compare activity transition patterns against known daily routines to verify that the annotations capture realistic ADL progressions rather than isolated visual similarities.