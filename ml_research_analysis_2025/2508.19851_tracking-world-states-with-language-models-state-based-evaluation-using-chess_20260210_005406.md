---
ver: rpa2
title: 'Tracking World States with Language Models: State-Based Evaluation Using Chess'
arxiv_id: '2508.19851'
source_url: https://arxiv.org/abs/2508.19851
tags:
- state
- states
- metrics
- language
- chess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel, model-agnostic evaluation framework
  for assessing whether language models track structured world states by analyzing
  the downstream legal move distributions (state affordances) in chess. Unlike conventional
  string-based metrics such as exact match or edit distance, which fail to capture
  the strategic and semantic richness of chess, the proposed method evaluates the
  similarity between predicted and actual game states based on the sets of valid action
  sequences they enable.
---

# Tracking World States with Language Models: State-Based Evaluation Using Chess

## Quick Facts
- **arXiv ID:** 2508.19851
- **Source URL:** https://arxiv.org/abs/2508.19851
- **Reference count:** 22
- **Primary result:** State-based metrics (pm, rm) capture semantic fidelity in LLM chess state tracking better than string-based metrics like edit distance

## Executive Summary
This paper introduces a novel evaluation framework for assessing whether language models track structured world states by analyzing downstream legal move distributions (state affordances) in chess. Unlike conventional string-based metrics such as exact match or edit distance, which fail to capture the strategic and semantic richness of chess, the proposed method evaluates similarity between predicted and actual game states based on valid action sequences they enable. Experimental results demonstrate that these metrics capture deficiencies in state-tracking and highlight the limitations of LLMs in maintaining coherent internal models over long sequences.

## Method Summary
The framework uses chess as a testbed for state-tracking evaluation. Given a sequence of chess moves (PGN), an LLM reconstructs the board state (FEN). The evaluation compares predicted vs. true states via valid action sequence overlap rather than string similarity. Primary metrics are pm (precision) and rm (recall)—the probability that action sequences valid under one state remain valid under the other. The method uses uniform branch sampling: from a state, sample legal moves uniformly for m steps to generate trajectories, then check if trajectories from predicted state are accepted by true state's FSA (and vice versa). Two estimators are provided: naive and intermediate probability estimator, with the latter reducing sample complexity from exponential to quadratic.

## Key Results
- State-based metrics (pm, rm) capture deficiencies in LLM state-tracking that edit distance and exact match miss
- Intermediate probability estimator reduces sample complexity from exponential O(e^λm) to quadratic O(m²)
- Metrics correlate with edit distance at short sequences but diverge significantly as game length increases
- LLMs show decreasing state-tracking fidelity over longer game sequences

## Why This Works (Mechanism)

### Mechanism 1: Affordance-Based Semantic Fidelity
If two states enable similar sets of valid action sequences, they are semantically similar regardless of surface-level string differences. Rather than comparing predicted state strings directly, the framework samples action trajectories from both the predicted state and true state, then measures whether sequences valid under one remain valid under the other.

### Mechanism 2: Intermediate Probability Estimation Reduces Sample Complexity
Estimating stepwise conditional validity and multiplying them reduces sample complexity from exponential O(e^λm) to quadratic O(m²). Instead of sampling complete m-length sequences (where validity probability decays as p^m), estimate vi = E[1_valid at step i] at each depth incrementally.

### Mechanism 3: Model-Agnostic Behavioral Evaluation via State Reconstruction
If a model can reconstruct states from action sequences such that reconstructed states preserve valid continuations, this provides evidence of implicit world model internalization—without requiring internal activation access. Prompt the LLM with a game transcript (PGN), request state output (FEN), then evaluate whether the predicted FEN yields similar legal move sets to the ground truth.

## Foundational Learning

- **Finite State Automata (FSA)**
  - Why needed: The entire framework formalizes chess as an FSA (S*, Σ, δ, S0) where states transition via legal actions
  - Quick check: For an FSA with transition function δ, what happens when action a ∉ ΣS is applied to state S?

- **Monte Carlo Estimation with Variance Reduction**
  - Why needed: The metrics require sampling from exponentially large action spaces; the intermediate estimator is a variance-reduction technique
  - Quick check: Why does naive sampling require N ≈ 1/pm samples to achieve signal-to-noise ratio ≈ 1?

- **Precision/Recall for Set Comparison**
  - Why needed: pm and rm are precision/recall formulations over sets of valid action sequences
  - Quick check: If pm = 0.6 and rm = 0.3, what does this say about the relationship between predicted and true states?

## Architecture Onboarding

- **Component map**: PGN input -> LLM FEN prediction -> Parser validation -> Affordance sampling (both states) -> Metric computation -> Correlation analysis
- **Critical path**: PGN input → LLM FEN prediction → Parser validation → Affordance sampling (both states) → Metric computation → Correlation analysis
- **Design tradeoffs**:
  - Depth m: Higher m captures richer semantics but cost grows quadratically; paper uses m=4
  - Sample size N: Larger N reduces variance; paper uses N=500
  - Prompt engineering: Affects reconstruction quality substantially; introduces confound between model capability and prompt suitability
- **Failure signatures**:
  1. pm ≈ 0 at m=1: Prompt/parser failure or LLM outputs invalid FEN
  2. Legal next move probability ≈ 10^-4 (random baseline): Model has no chess knowledge
  3. Kendall's τ between pm and edit distance drops sharply with game length: Metrics diverge as state-tracking degrades
  4. High variance across runs with same inputs: N insufficient for depth m
- **First 3 experiments**:
  1. Validate metric behavior: Replicate Figure 3 showing Kendall's τ between pm and edit distance as function of game length (5-50 moves)
  2. Prompt sensitivity ablation: Test 3-5 prompt variants on fixed 100-game subset to quantify prompt-induced variance
  3. Computational scaling profile: Measure wall-clock time and memory for m ∈ {2, 4, 6, 8} with N=500

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do prompting strategies, such as chain-of-thought or self-verification, mitigate the sensitivity of state reconstruction in LLMs? The authors state that "systematic studies on prompt robustness, prompt tuning, or self-verification techniques could help mitigate" the sensitivity to prompting strategies identified as a major limitation.

### Open Question 2
Can importance sampling or guided rollouts significantly reduce the computational cost of estimating state metrics compared to uniform branch sampling? The discussion notes the "possibility of using importance sampling or guided rollouts based on fitness functions or model confidence to further improve sampling efficiency."

### Open Question 3
Does the framework generalize to open-ended domains with latent or ambiguous state representations, such as dialog tracking or robotic planning? The conclusion proposes "extending the framework to other structured domains such as program synthesis, dialog tracking, and robotic planning," while the discussion notes the current assumption of reliable action models may not hold in less formalized domains.

## Limitations
- Prompt sensitivity for state reconstruction is a significant confounder that may artificially depress pm/rm scores
- Computational scaling remains expensive for large m, limiting practical depth despite quadratic improvement
- The intermediate probability estimator assumes stepwise validity independence, which may not hold in highly correlated branching domains

## Confidence

- **High confidence**: The core mechanism of using affordance distributions instead of string metrics is well-justified and empirically supported by the dramatic divergence between pm/rm and edit distance as games progress
- **Medium confidence**: The claim that state reconstruction success implies internalized world models is plausible but confounded by prompt engineering effects
- **Low confidence**: The robustness of these metrics to non-chess domains (e.g., text-based games, physical reasoning) remains untested

## Next Checks
1. **Prompt sensitivity ablation**: Test 3-5 prompt variants on fixed 100-game subset to quantify variance induced by prompt engineering versus true state-tracking capability
2. **Domain generalization probe**: Apply the framework to a simple text-based adventure game to test whether affordance metrics capture state-tracking where string metrics fail
3. **Correlation stress test**: Systematically vary game complexity (piece count, branching factor) to measure how pm/rm degradation correlates with edit distance across different game types beyond standard chess