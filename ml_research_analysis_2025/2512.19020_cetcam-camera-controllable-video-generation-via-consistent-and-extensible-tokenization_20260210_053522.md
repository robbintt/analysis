---
ver: rpa2
title: 'CETCAM: Camera-Controllable Video Generation via Consistent and Extensible
  Tokenization'
arxiv_id: '2512.19020'
source_url: https://arxiv.org/abs/2512.19020
tags:
- camera
- video
- control
- generation
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CETCAM, a camera-controllable video generation\
  \ framework that eliminates the need for camera pose annotations by leveraging geometry\
  \ foundation models to estimate depth and camera parameters, converting them into\
  \ unified geometry-aware tokens integrated into a pretrained video diffusion backbone.\
  \ CETCAM is trained in two progressive phases\u2014first on diverse raw videos for\
  \ robust camera controllability, then on curated high-fidelity data for visual quality\u2014\
  achieving state-of-the-art geometric consistency, temporal stability, and visual\
  \ realism across multiple benchmarks."
---

# CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization

## Quick Facts
- **arXiv ID**: 2512.19020
- **Source URL**: https://arxiv.org/abs/2512.19020
- **Reference count**: 40
- **Primary result**: CETCAM achieves state-of-the-art geometric consistency and visual realism for camera-controllable video generation without requiring camera pose annotations.

## Executive Summary
CETCAM introduces a camera-controllable video generation framework that eliminates the need for camera pose annotations by leveraging geometry foundation models to estimate depth and camera parameters. The method converts these estimates into unified geometry-aware tokens integrated into a pretrained video diffusion backbone. Trained in two progressive phases—first on diverse raw videos for robust camera controllability, then on curated high-fidelity data for visual quality—CETCAM achieves state-of-the-art geometric consistency, temporal stability, and visual realism across multiple benchmarks. It also demonstrates strong extensibility to additional control modalities like inpainting and layout, maintaining high performance and adaptability.

## Method Summary
CETCAM uses a frozen VGGT model to estimate depth maps and camera parameters from video frames, which are then converted into geometry-aware tokens through point cloud reprojection. These tokens are processed through CETCAM context blocks and added to frozen Wan DiT blocks via zero-initialized linear layers, enabling training-free preservation of base model capabilities. The model is trained in two phases: Phase 1 on ~100K filtered diverse videos for robust camera controllability, and Phase 2 on ~3K high-quality curated videos for visual quality. The method employs flow matching loss and uses FSDP + Sequence Parallelism for training efficiency.

## Key Results
- Outperforms prior methods with overall VBench score improvements of +3.09%, +2.70%, and +2.88% on different benchmarks
- Achieves over 6% higher human evaluation scores compared to baselines
- Reduces pose errors by over 15% with lower ATE/RRE/RPE metrics
- Demonstrates strong extensibility to additional control modalities while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Unified geometry estimation eliminates train-test discrepancy in camera conditioning.
- **Mechanism**: VGGT jointly predicts depth maps and camera parameters from raw video during training, ensuring depth-pose alignment. At inference, the same VGGT estimates depth from a single reference frame, maintaining geometric consistency.
- **Core assumption**: The pre-trained VGGT model provides sufficiently accurate depth and pose estimates across diverse video domains.
- **Evidence anchors**: Abstract states CETCAM leverages VGGT to estimate depth and camera parameters; Section 3.1 explains how this estimation ensures geometric alignment across frames and uniform predictions across diverse training datasets.

### Mechanism 2
- **Claim**: Point cloud reprojection creates explicit geometry-aware conditioning without 3D scene reconstruction.
- **Mechanism**: First-frame pixels are back-projected to 3D using predicted depth and intrinsics, then reprojected to target frames using predicted extrinsics, producing renderings and visibility masks that expose occluded regions to the diffusion model.
- **Core assumption**: Single-view depth is sufficient for meaningful reprojection; temporal consistency emerges from diffusion rather than multi-view fusion.
- **Evidence anchors**: Section 3.1 describes constructing binary visibility masks based on point correspondences and occlusion checks; Figure 3 shows visual comparison where CETCAM renderings follow camera motion accurately while Uni3C exhibits distortions.

### Mechanism 3
- **Claim**: Zero-initialized context blocks enable training-free preservation of base model capabilities.
- **Mechanism**: CETCAM context blocks process camera tokens and add features to frozen Wan DiT blocks via zero-initialized linear layers, starting training equivalent to the base model and gradually injecting camera conditioning without disrupting pre-trained knowledge.
- **Core assumption**: Additive feature fusion preserves base model's generative quality better than concatenation or full fine-tuning.
- **Evidence anchors**: Assumption based on methodology description; requires verification through ablation studies comparing different integration strategies.

## Foundational Learning
CETCAM leverages pre-trained foundation models (VGGT for geometry estimation and Wan DiT for video generation) to bootstrap camera-controllable video generation without requiring additional pose annotations. The method demonstrates that geometry foundation models can effectively bridge the gap between 2D video generation and 3D-aware control by providing unified depth and pose estimates.

## Architecture Onboarding
The architecture integrates geometry-aware tokens into an existing video diffusion model through a plug-and-play approach. CETCAM context blocks are added to frozen Wan DiT blocks via zero-initialized linear layers, allowing the model to preserve its pre-trained capabilities while incorporating camera conditioning. This design enables efficient training and maintains the base model's generative quality.

## Open Questions the Paper Calls Out
- The paper does not explicitly identify open questions or future work directions.
- Potential areas for exploration include scaling to longer videos, improving performance in complex scene scenarios, and extending to more diverse camera motion patterns.

## Limitations
- CETCAM's performance depends on the accuracy of the pre-trained VGGT model, which may struggle with highly dynamic scenes or complex camera motions.
- The method requires a diverse dataset of raw videos for training Phase 1, which may be challenging to curate at scale.
- Extension to additional control modalities, while demonstrated, may introduce additional complexity and computational overhead.

## Confidence
- The paper provides clear methodology and results, demonstrating state-of-the-art performance in camera-controllable video generation.
- The use of pre-trained foundation models and the two-phase training approach are well-justified and supported by experimental results.
- However, some claims regarding the superiority of zero-initialized context blocks over alternative integration strategies require further validation through ablation studies.

## Next Checks
- Verify the accuracy and robustness of VGGT depth and pose estimates across diverse video domains.
- Conduct ablation studies comparing zero-initialized context blocks with alternative integration strategies (e.g., concatenation, full fine-tuning).
- Evaluate the model's performance on longer videos and in complex scene scenarios to assess scalability and generalization.