---
ver: rpa2
title: Explainable AI-Based Interface System for Weather Forecasting Model
arxiv_id: '2504.00795'
source_url: https://arxiv.org/abs/2504.00795
tags:
- performance
- rainfall
- user
- leadtime
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study develops a user-centered explainable AI (XAI) interface
  system for a precipitation forecasting model. Through user interviews with meteorologists,
  three explanation requirements were identified: model performance across rainfall
  types, reasoning behind predictions, and confidence calibration.'
---

# Explainable AI-Based Interface System for Weather Forecasting Model

## Quick Facts
- **arXiv ID:** 2504.00795
- **Source URL:** https://arxiv.org/abs/2504.00795
- **Reference count:** 40
- **Primary result:** User-centered XAI interface improves decision utility and trust for precipitation forecasting

## Executive Summary
This study develops a user-centered explainable AI (XAI) interface system for a precipitation forecasting model. Through user interviews with meteorologists, three explanation requirements were identified: model performance across rainfall types, reasoning behind predictions, and confidence calibration. Appropriate XAI methods were mapped to each requirement—performance diagrams for rainfall type evaluation, feature attribution for output reasoning, and temperature scaling for confidence calibration. The interface was designed based on user feedback and tested qualitatively. Results showed that explanations increased decision utility and trust, with users preferring intuitive over technical explanations. The study demonstrates the value of user-centric XAI development in operational meteorology and provides a foundation for improving AI system usability in practice.

## Method Summary
The system combines a UNet2-based precipitation forecasting model with three XAI components. First, a rainfall type classifier categorizes events using Self-Organizing Maps. Second, Integrated Gradients provides feature attribution for prediction reasoning. Third, Local Temperature Scaling calibrates model confidence. The interface presents these explanations through progressive disclosure: performance diagrams for different rainfall types, confidence toggles, and simplified attribution heatmaps. User requirements were gathered through interviews with operational meteorologists at the National Institute of Meteorological Sciences, then mapped to specific XAI methods through an iterative design process.

## Key Results
- Explanations increased decision utility and user trust compared to black-box predictions
- Users preferred intuitive explanations over technically faithful but complex attributions
- Performance diagrams revealed model biases across different rainfall types
- Temperature scaling reduced Expected Calibration Error without affecting prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping specific XAI methods to distinct user-identified requirements increases decision utility more effectively than applying generic explanation techniques.
- **Mechanism:** The system bridges the gap between "black-box" outputs and user needs by identifying three specific explanatory requirements (performance, reasoning, confidence) via interviews, then selectively applying tailored algorithms (performance diagrams, Integrated Gradients, temperature scaling) to address each independently.
- **Core assumption:** Users can explicitly articulate their explanatory needs during the design phase, and these needs remain stable during operations.
- **Evidence anchors:**
  - [abstract] "Appropriate XAI methods are mapped to each requirement... results indicate that the explanations increase decision utility."
  - [section 3.1] "Based on the discussion, the user requirements can be stated as follows... Appropriate XAI methods are selected to address each need."
  - [corpus] Conceptual alignment found in *From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support*, which similarly advocates for domain-specific XAI integration.
- **Break condition:** If user requirements are vague, contradictory, or change post-deployment, the static mapping of methods to needs fails to deliver utility.

### Mechanism 2
- **Claim:** Probability calibration via Local Temperature Scaling (LTS) creates actionable trust by aligning model confidence with true likelihoods, correcting the natural overconfidence of DNNs.
- **Mechanism:** Deep learning models trained on negative log-likelihood often output extreme probabilities (overconfidence). LTS introduces spatially varying temperature parameters to scale logits, minimizing Expected Calibration Error (ECE) without altering the final class prediction (F1 score).
- **Core assumption:** The validation dataset used to optimize the temperature mapping is representative of the operational distribution of weather events.
- **Evidence anchors:**
  - [abstract] "...confidence calibration... explanations increase decision utility and user trust."
  - [section 3.4] "The optimized LTS network improves the ECE scores... while maintaining the modified F1 scores."
  - [corpus] Weak direct corpus evidence for LTS specifically in this context; mechanism relies primarily on paper's internal validation.
- **Break condition:** If the calibration mapping overfits to the validation set (distribution shift), the displayed "confidence" becomes misleading, degrading trust.

### Mechanism 3
- **Claim:** Visualizing feature attribution (Integrated Gradients) allows domain experts to verify physical consistency, but technical complexity can overwhelm users unless simplified.
- **Mechanism:** Gradients quantify the contribution of input radar sequences to the output prediction. The paper evaluates these using "incremental deletion" (quantitative) and expert review (qualitative). However, the interface study reveals a divergence: while the method is quantitatively faithful, users found it "hard to understand."
- **Core assumption:** The physical dynamics of rainfall (e.g., westerlies, convection) are captured by the model features and recoverable via gradient-based attribution.
- **Evidence anchors:**
  - [section 3.3] "Integrated gradient method outperforms the other methods [in incremental deletion]."
  - [section 4] "Users found the explanations to be difficult to understand in the output reasoning explanatory module... induced by the effort required."
  - [corpus] *Theory of Mind for Explainable Human-Robot Interaction* supports the need for aligning system outputs with user mental models.
- **Break condition:** If the receptive field of the model (approx. 150km radius) is insufficient to capture upstream weather phenomena, attribution maps highlight artifacts rather than causes.

## Foundational Learning

- **Concept: Feature Attribution (Integrated Gradients)**
  - **Why needed here:** To satisfy the "Output Reasoning" requirement. You must understand how to implement and interpret gradient-based saliency maps to identify which radar pixels contributed to a specific rainfall prediction.
  - **Quick check question:** If the model predicts "Heavy Rain," does the attribution map highlight the incoming storm front or random noise?

- **Concept: Probability Calibration (Temperature Scaling)**
  - **Why needed here:** To satisfy the "Confidence of Output" requirement. You need to distinguish between a model's raw softmax output and a calibrated probability that reflects true accuracy.
  - **Quick check question:** A model outputs 90% confidence but is only correct 60% of the time. How does temperature scaling correct this?

- **Concept: Performance Diagrams (POD, SR, CSI)**
  - **Why needed here:** To satisfy the "Model Performance" requirement for imbalanced datasets. Standard accuracy is insufficient; you must learn how bias, probability of detection (POD), and critical success index (CSI) interact.
  - **Quick check question:** How does the performance diagram reveal if a model is "over-forecasting" (high bias) versus "under-forecasting"?

## Architecture Onboarding

- **Component map:** Inference Core (UNet2 -> Raw Logits) -> Calibration Layer (Local Temperature Scaling -> Calibrated Confidence) -> Explanation Layer (Rainfall Classifier + Integrated Gradients Module) -> Interface (Progressive disclosure UI)
- **Critical path:** The *User Feedback Loop* is the critical path. As noted in Section 3.1 and 3.5, the system architecture is not defined by the model weights alone but by the iterative definition of requirements (NIMS interviews) -> implementation -> usability testing.
- **Design tradeoffs:**
  - **Faithfulness vs. Interpretability:** The paper explicitly trades technical accuracy for user comprehension. The "Output Reasoning" module was quantitatively accurate (high fidelity) but qualitatively rejected for being too complex.
  - **Receptive Field vs. Lead Time:** The model has a ~150km effective receptive field. The architecture inherently loses reliability for lead times > 3 hours because the weather moves out of view (Section 3.3).
- **Failure signatures:**
  - **High ECE on Novel Events:** If calibration fails, confidence scores will remain high even for incorrect predictions on new rainfall types.
  - **West-Bias Artifacts:** Attribution maps showing high relevance at the edge of radar range during specific wind patterns (Section 3.3) indicate the model is "guessing" based on boundary conditions rather than physics.
  - **Cognitive Overload:** If the UI exposes raw attribution maps without simplification, user trust decreases (Section 4).
- **First 3 experiments:**
  1. **Receptive Field Validation:** Compute the Effective Receptive Field (ERF) on a sample batch. Verify that the ERF radius (~150km) aligns with the average wind speed * lead time to ensure physical feasibility.
  2. **Calibration Stress Test:** Measure Expected Calibration Error (ECE) specifically on "rare" rainfall types (e.g., Isolated Thunderstorms) to ensure LTS didn't overfit to "Normal" weather.
  3. **Ablation on Explanation Modalities:** Run a simplified A/B test with forecasters: show raw attribution vs. the "Intuitive" version to quantify the difference in decision latency and trust.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can explanation complexity be effectively measured and visualized in the meteorological domain to accommodate the spatiotemporal context of weather data?
- **Basis in paper:** [Explicit] The Discussion section notes that users found feature attribution "hard to understand" despite high fidelity, stating that "user-centric XAI performance may need to reflect qualities of explanation besides faithfulness." The authors explicitly suggest that "proxy variables in the weather domain... may require different metrics of complexity" beyond standard Shannon entropy.
- **Why unresolved:** Current complexity metrics do not account for the unique spatiotemporal characteristics of meteorological data, leading to a disconnect between algorithmic faithfulness and human interpretability.
- **What evidence would resolve it:** The development and validation of a domain-specific complexity metric that correlates strongly with forecaster comprehension and decision-making speed in user studies.

### Open Question 2
- **Question:** Does incorporating multi-modal input data (e.g., thermodynamic profiles) enhance the physical meaningfulness of feature attribution explanations compared to uni-modal radar inputs?
- **Basis in paper:** [Explicit] The Discussion section identifies a limitation where the "model’s reliance on uni-modal input features" restricts attribution to horizontal movement. It states, "This issue may be addressed by using multi-modal data... it would be ideal to include additional features that can provide this information."
- **Why unresolved:** The current model uses only radar data, which represents final outcomes rather than physical mechanisms (e.g., instability, vertical motion), limiting the "reasoning" explanation to horizontal patterns.
- **What evidence would resolve it:** A comparative study where attribution maps generated by multi-modal models are evaluated by domain experts for physical consistency with known meteorological mechanisms (e.g., convective initiation).

### Open Question 3
- **Question:** To what extent does high-level interactive dialogue improve the practical utility of XAI systems for forecasters compared to static interface dashboards?
- **Basis in paper:** [Explicit] The Discussion section classifies the current system as a "shallow-level user-interactive XAI (UXAI) system that becomes static." It explicitly lists "providing high-level interaction" to support "explanations in response to feedback from the users such as interactive dialogue" as a "potential area for future work."
- **Why unresolved:** Forecasting requires rapid decision-making; static explanations may not address specific, evolving user queries during an extreme weather event, potentially reducing the system's operational adoption.
- **What evidence would resolve it:** User studies comparing task completion times and trust levels between the current static prototype and a conversational/interactive agent interface during simulated forecasting scenarios.

### Open Question 4
- **Question:** Does increasing the sample size of labeled rainfall types improve the reliability of the rainfall type classifier sufficiently for operational performance diagnostics?
- **Basis in paper:** [Explicit] Section 3.2 states, "the classifier shows limited performance due to a lack of samples with rainfall type labels." The Discussion reiterates, "For actual implementation, it would be necessary to train the classifier with a larger dataset."
- **Why unresolved:** The current classifier, limited by small sample sizes (e.g., only 24 samples for some types), cannot robustly diagnose model performance biases across all critical weather scenarios.
- **What evidence would resolve it:** Re-training the classifier with a comprehensive dataset and demonstrating statistically significant improvements in classification accuracy and the stability of performance diagrams across rainfall types.

## Limitations

- The qualitative evaluation with meteorologists lacks quantitative metrics for trust and decision utility
- System performance on rare rainfall events and geographical generalization remains unclear
- Calibration effectiveness is only validated within the same dataset distribution, raising concerns about distribution shift
- User feedback was collected during design rather than post-deployment, limiting ecological validity

## Confidence

- **High Confidence:** The effectiveness of mapping specific XAI methods to user-identified requirements (Mechanism 1) - well-supported by interview data and qualitative results.
- **Medium Confidence:** The calibration improvements via Local Temperature Scaling (Mechanism 2) - supported by ECE metrics but limited to internal validation.
- **Medium Confidence:** The fidelity of Integrated Gradients explanations (Mechanism 3) - quantitatively validated but rejected by users for complexity.

## Next Checks

1. Conduct a longitudinal field study with operational meteorologists to measure actual decision-making improvements and trust evolution over time.
2. Perform cross-validation on geographically distinct radar datasets to assess generalization of both model performance and explanation quality.
3. Implement A/B testing with simplified vs. technical explanation interfaces to quantify the impact of explanation complexity on decision latency and accuracy.