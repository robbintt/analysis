---
ver: rpa2
title: 'Intelligent Legal Assistant: An Interactive Clarification System for Legal
  Question Answering'
arxiv_id: '2502.07904'
source_url: https://arxiv.org/abs/2502.07904
tags:
- legal
- system
- question
- user
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an interactive legal question-answering
  system that addresses the challenge of incomplete user queries in legal contexts.
  The system uses a three-stage approach: first, it detects missing information in
  user questions using a fine-tuned Llama-3.1-8B model; second, it generates clarifying
  questions and options using reinforcement learning (DDPG) combined with large language
  models; and third, it produces comprehensive legal responses integrating user feedback
  with relevant legal documents.'
---

# Intelligent Legal Assistant: An Interactive Clarification System for Legal Question Answering

## Quick Facts
- arXiv ID: 2502.07904
- Source URL: https://arxiv.org/abs/2502.07904
- Reference count: 4
- Primary result: 90% user preference over GPT-4o, AI Lawyer, and Callidus AI in human evaluations

## Executive Summary
This paper presents an interactive legal question-answering system designed to address incomplete user queries in legal contexts. The system implements a three-stage approach that first detects missing information, then generates clarifying questions, and finally produces comprehensive legal responses. Through human evaluations with 50 participants, the system demonstrated strong performance with scores of 4.8 out of 5 for both accuracy and satisfaction, while showing a 90% preference rate compared to established commercial systems.

## Method Summary
The system employs a three-stage approach to interactive legal question answering. First, it uses a fine-tuned Llama-3.1-8B model to detect missing information in user queries. Second, it generates clarifying questions and options through reinforcement learning (DDPG) combined with large language models. Finally, it produces comprehensive legal responses by integrating user feedback with relevant legal documents. The system achieved high human evaluation scores and demonstrated strong preference over competing commercial solutions.

## Key Results
- Human evaluation score of 4.8/5 for both accuracy and satisfaction
- 90% user preference over competing systems including GPT-4o, AI Lawyer, and Callidus AI
- Effective handling of incomplete legal queries through interactive clarification

## Why This Works (Mechanism)
The system addresses a fundamental challenge in legal question answering where users often submit incomplete queries that lack necessary context or specific details. By implementing an interactive clarification process, the system can gather missing information before generating responses, leading to more accurate and relevant legal advice. The three-stage approach ensures systematic handling of incomplete queries while maintaining user engagement throughout the clarification process.

## Foundational Learning
- Reinforcement Learning with DDPG: Used for optimizing clarifying question generation; needed to balance exploration of different clarification paths with exploitation of proven effective questions; quick check: verify reward function captures user satisfaction
- Fine-tuning Large Language Models: Llama-3.1-8B model is fine-tuned for missing information detection; needed to adapt general language understanding to legal domain specifics; quick check: measure performance on held-out legal queries
- Interactive Dialogue Systems: Multi-turn conversation management for gathering user feedback; needed to maintain context and coherence across clarification exchanges; quick check: evaluate dialogue flow quality and coherence

## Architecture Onboarding

Component Map:
User Query -> Missing Info Detection -> Clarifying Question Generation -> User Feedback Integration -> Legal Response Generation

Critical Path:
User Query -> Fine-tuned Llama-3.1-8B Missing Info Detection -> DDPG-based Clarifying Question Generation -> Legal Document Retrieval -> Response Generation

Design Tradeoffs:
- Balance between number of clarification questions and user fatigue
- Trade-off between response time and thoroughness of clarification
- Model complexity vs. inference speed for real-time interaction

Failure Signatures:
- Excessive clarification rounds leading to user abandonment
- Generated questions that don't effectively narrow down user intent
- Integration of user feedback that leads to contradictory or ambiguous responses

First Experiments:
1. Test missing information detection accuracy on diverse legal query types
2. Evaluate clarifying question effectiveness through A/B testing with human evaluators
3. Measure response accuracy improvement with different numbers of clarification rounds

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Effectiveness heavily dependent on quality and comprehensiveness of legal document corpus
- Human evaluation limited to 50 users, potentially missing edge cases
- Lack of detailed statistical analysis and baseline comparisons beyond mentioned commercial systems

## Confidence
- System Architecture: Medium
- Reinforcement Learning Implementation: Medium
- Human Evaluation Results: Medium-High

## Next Checks
1. Conduct a larger-scale evaluation with diverse legal domains and query types to validate generalizability
2. Perform error analysis on clarification questions that led to incorrect answers to identify failure patterns
3. Test the system's robustness with adversarial queries designed to expose potential weaknesses in the clarification and response generation pipeline