---
ver: rpa2
title: A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs
arxiv_id: '2511.06455'
source_url: https://arxiv.org/abs/2511.06455
tags:
- data
- mapping
- semantic
- knowledge
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semantic multi-agent system that uses LLM
  agents to automatically map relational database schemas to Schema.org concepts and
  integrate them into knowledge graphs. The approach addresses the challenge of integrating
  siloed enterprise databases by introducing a semantic layer that bridges structural
  differences.
---

# A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs

## Quick Facts
- arXiv ID: 2511.06455
- Source URL: https://arxiv.org/abs/2511.06455
- Reference count: 14
- Over 90% mapping accuracy achieved across multiple domains using LLM agents

## Executive Summary
This paper introduces a semantic multi-agent system that automatically maps relational database schemas to Schema.org concepts and integrates them into knowledge graphs. The system uses three specialized LLM agents—mapping, relation, and validator—to transform structured database metadata into semantically enriched knowledge graph representations. Evaluated on Spider dataset databases across five domains, the approach achieves over 90% accuracy in most cases, with execution times scaling from 122 to 342 seconds depending on dataset size.

## Method Summary
The system extracts table metadata from relational databases including column names, sample values, and statistics, then processes this through three GPT-4o-mini agents in sequence. The Mapping agent uses retrieval-augmented generation with a custom graph-vector store of Schema.org terms to map columns to ontology concepts. The Relation agent identifies primary and foreign key relationships to establish entity connections. The Validator agent reviews and corrects both mappings and relations, outputting confidence levels (HIGH/MEDIUM/LOW) averaged across processing steps.

## Key Results
- Achieved over 90% mapping accuracy across five domains (retail, movies, apartments, automotive, delivery)
- Highest accuracy of 93.54% in the apartments domain, lowest at 78.72% in retail
- HIGH confidence mappings correlated with 95-100% accuracy, while LOW confidence dropped to 0-55%
- Execution time ranged from 122 to 342 seconds depending on number of tables and columns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation with ontology subgraphs improves mapping accuracy over pure LLM reasoning.
- Mechanism: The system constructs a graph-vector store where each Schema.org term is enriched with a one-hop subgraph before embedding. During mapping, semantically similar terms are retrieved based on table/column names and sample values, providing grounded context that constrains the LLM's output to valid ontology terms.
- Core assumption: The target ontology (Schema.org) covers the semantic space of enterprise database schemas sufficiently well.
- Evidence anchors:
  - [abstract] "utilizing a system comprising multiple LLM agents that map tables and columns to Schema.org terms"
  - [section 3.2] "The most similar terms are retrieved from the custom graph-vector store... in the subsequent step"
  - [corpus] Retrieval-Augmented Generation of Ontologies from Relational Databases (arXiv:2506.01232) reports similar RAG-based ontology generation from databases

### Mechanism 2
- Claim: Role-specialized agents with sequential handoffs reduce error accumulation compared to a single monolithic agent.
- Mechanism: Three GPT-4o-mini agents operate in sequence: (1) Mapping agent maps columns to Schema.org terms using retrieved context, (2) Relation agent identifies primary/foreign keys to link entities across tables, (3) Validator agent inspects and corrects both mappings and relations. Each agent receives structured input from predecessors and outputs both results and confidence levels.
- Core assumption: Errors from earlier agents are detectable and correctable by downstream agents without introducing new errors.
- Evidence anchors:
  - [section 3.3] "Validator agent: Validates and corrects the mapping and relations produced by the two previous agents"
  - [section 4] HIGH confidence mappings achieved 95-100% accuracy across domains, suggesting the validation layer effectively filters low-quality outputs
  - [corpus] AgenticData (arXiv:2508.05002) demonstrates agentic workflows for heterogeneous data analytics with specialized roles

### Mechanism 3
- Claim: Confidence-based output categorization enables selective human review at accuracy-critical boundaries.
- Mechanism: Each agent outputs a confidence variable (HIGH/MEDIUM/LOW) averaged across processing steps. Results show HIGH confidence correlates with 95-100% accuracy, while LOW confidence drops to 0-55% across domains, providing a clear signal for where human intervention is most valuable.
- Core assumption: The LLM's internal confidence calibration aligns with actual mapping correctness.
- Evidence anchors:
  - [section 4, Table 2] "It is evident that the mappings with certainty equal to HIGH are, in general, the most correct ones"
  - [section 3.3] "All of the agents in the system output a confidence variable... The categories are averaged from all steps"
  - [corpus] No direct corpus evidence on confidence calibration in semantic mapping systems; this is a gap requiring further validation

## Foundational Learning

- Concept: **Knowledge Graphs and Ontologies**
  - Why needed here: The system maps relational schemas to Schema.org, requiring understanding of classes, properties, domain/range restrictions, and how ontologies encode semantics differently from relational foreign keys.
  - Quick check question: Can you explain why Schema.org's `domain` and `range` for a property constrain which classes it can meaningfully connect?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Mapping accuracy depends on retrieving relevant Schema.org terms from the graph-vector store before the LLM generates mappings; understanding embedding similarity and retrieval ranking is essential.
  - Quick check question: What would happen to mapping quality if the embedding model failed to capture semantic similarity between "unit_price" and Schema.org's "price" property?

- Concept: **Relational Schema Semantics (Primary/Foreign Keys)**
  - Why needed here: The Relation agent must infer entity relationships from key structures; misidentifying foreign keys breaks the knowledge graph's ability to represent cross-table entities.
  - Quick check question: Given a table with columns `order_id, customer_id, product_id, quantity`, which columns likely represent foreign key relationships to other entities?

## Architecture Onboarding

- Component map:
  ```
  Database → Schema Extractor → Table metadata (name, columns, samples, stats)
                                      ↓
                            Graph-Vector Store (Schema.org embeddings)
                                      ↓
                            Mapping Agent (GPT-4o-mini) → Column-to-term mappings
                                      ↓
                            Relation Agent → Primary/foreign key identification
                                      ↓
                            Validator Agent → Corrected mappings + confidence
                                      ↓
                            Knowledge Graph (RDF/property graph)
  ```

- Critical path: The Mapping Agent's retrieval step is the accuracy bottleneck—if incorrect Schema.org terms are retrieved, downstream agents inherit the error. Validate retrieval quality first.

- Design tradeoffs:
  - Schema.org vs. custom ontologies: Schema.org provides broad coverage but may lack domain-specific terms; custom ontologies require manual construction.
  - Single LLM vs. multi-agent: Multi-agent adds orchestration complexity but enables specialization; single agent is simpler but may conflate mapping and validation.
  - Confidence threshold for auto-acceptance: Higher threshold reduces errors but increases manual review load.

- Failure signatures:
  - LOW confidence on common columns (e.g., "id", "name") suggests retrieval failure or ambiguous schema context.
  - Validator removes most relations → Relation agent may lack sufficient foreign key signals.
  - Execution time grows non-linearly with table count → check for redundant retrieval calls.

- First 3 experiments:
  1. Run the system on a single-domain database from Spider (e.g., apartments) and manually validate 20 random column mappings against ground truth to establish baseline accuracy.
  2. Ablate the graph-vector store by providing the Mapping Agent only term names (not subgraph context) to measure retrieval contribution to accuracy.
  3. Test confidence calibration: Compare predicted HIGH/MEDIUM/LOW against actual accuracy on a held-out domain to determine if confidence thresholds need adjustment.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the multi-agent system be extended to support custom domain ontologies beyond Schema.org while maintaining mapping accuracy?
  - Basis in paper: [explicit] "Future work includes extending the system to support custom domain ontologies..."
  - Why unresolved: The current implementation relies solely on Schema.org, which covers general concepts but may lack specialized terminology for niche enterprise domains.
  - What evidence would resolve it: Evaluation of the system using domain-specific ontologies (e.g., medical, legal, manufacturing) with comparable accuracy metrics.

- **Open Question 2**: What scaling strategies can enable integration of significantly larger and more heterogeneous datasets without proportional increases in execution time?
  - Basis in paper: [explicit] "Future work includes...investigating scaling strategies for integrating larger and more heterogeneous datasets."
  - Why unresolved: Current execution time scales with database size (122-342 seconds for 3-12 tables); unclear how performance degrades with enterprise-scale databases containing hundreds of tables.
  - What evidence would resolve it: Benchmarks on databases with 50+ tables showing sub-linear time complexity through parallelization, caching, or hierarchical mapping approaches.

- **Open Question 3**: What domain characteristics cause the significant accuracy variance observed across sectors (78.72% Retail vs. 93.54% Apartments)?
  - Basis in paper: [inferred] Table 2 shows 14.82 percentage point spread between best and worst domains, yet the paper provides no analysis of causal factors.
  - Why unresolved: Understanding failure modes in lower-performing domains could guide targeted improvements; Schema.org term coverage may vary by domain specificity.
  - What evidence would resolve it: Correlation analysis between accuracy and factors like Schema.org term availability, column naming conventions, or domain complexity.

- **Open Question 4**: Can fine-tuning the LLM agents on schema-mapping tasks improve reasoning capabilities and reduce the accuracy gap between HIGH and LOW confidence predictions?
  - Basis in paper: [explicit] "Future work includes...enhancing the reasoning capabilities of the agents through fine-tuning..."
  - Why unresolved: Current LOW confidence predictions show dramatically lower accuracy (0-55.56% across domains) compared to HIGH confidence (85.71-100%), suggesting reasoning failures on ambiguous cases.
  - What evidence would resolve it: Comparative evaluation before/after fine-tuning showing improved LOW-confidence accuracy and better-calibrated confidence scores.

## Limitations
- Schema.org coverage may be insufficient for domain-specific enterprise terminology
- Confidence calibration reliability for ambiguous cases remains unverified
- Execution time scalability beyond Spider dataset subset is unknown

## Confidence
- **High confidence**: The multi-agent architecture with sequential handoffs reduces error accumulation compared to single-agent approaches
- **Medium confidence**: Retrieval-augmented generation with ontology subgraphs improves mapping accuracy over pure LLM reasoning
- **Medium confidence**: Confidence-based output categorization enables effective human review allocation

## Next Checks
1. **Domain coverage validation**: Test the system on enterprise databases containing domain-specific terms (e.g., medical or legal schemas) absent from Schema.org to quantify the coverage gap and identify when custom ontologies are needed.
2. **Confidence calibration analysis**: Evaluate confidence score reliability by comparing predicted HIGH/MEDIUM/LOW against actual accuracy on a held-out domain, calculating calibration curves and identifying systematic miscalibration patterns.
3. **RAG ablation study**: Measure mapping accuracy with and without the graph-vector store retrieval step on the same database subset to quantify the contribution of contextual retrieval to overall performance.