---
ver: rpa2
title: 'Performance of leading large language models in May 2025 in Membership of
  the Royal College of General Practitioners-style examination questions: a cross-sectional
  analysis'
arxiv_id: '2506.02987'
source_url: https://arxiv.org/abs/2506.02987
tags:
- correct
- questions
- clinical
- llms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluated four leading large language models\u2014o3,\
  \ Claude Opus 4, Grok-3, and Gemini 2.5 Pro\u2014on 100 MRCGP-style multiple-choice\
  \ questions from the RCGP GP SelfTest. Each model was prompted to answer as a UK\
  \ GP and scored against official correct answers."
---

# Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis

## Quick Facts
- arXiv ID: 2506.02987
- Source URL: https://arxiv.org/abs/2506.02987
- Reference count: 0
- Four leading LLMs achieved 95-99% accuracy on MRCGP-style questions, far exceeding the 73% peer benchmark

## Executive Summary
This study evaluated four leading large language models—o3, Claude Opus 4, Grok-3, and Gemini 2.5 Pro—on 100 MRCGP-style multiple-choice questions from the RCGP GP SelfTest. Each model was prompted to answer as a UK GP and scored against official correct answers. The models achieved 99.0%, 95.0%, 95.0%, and 95.0% accuracy respectively, far exceeding the average peer score of 73.0%. o3 performed best with only one incorrect answer, while the others had five each. The findings suggest that reasoning models, especially those trained on primary care data, can substantially support clinical decision-making in primary care, though caution is needed given their confident presentation of incorrect answers.

## Method Summary
A cross-sectional study conducted on 25 May 2025 evaluated four reasoning-class LLMs (o3, Claude Opus 4, Grok-3, Gemini 2.5 Pro) on 100 randomly selected MRCGP-style multiple-choice questions from RCGP GP SelfTest. Each model was prompted once with "Answer the following questions as if you were a GP in the United Kingdom" and given full question context including images and tables where applicable. Responses were scored against official answers, with accuracy compared to a 73% benchmark from previous human GP performance on the same questions.

## Key Results
- o3 achieved 99.0% accuracy (1 incorrect answer)
- Claude Opus 4, Grok-3, and Gemini 2.5 Pro each achieved 95.0% accuracy (5 incorrect answers each)
- All models substantially outperformed the 73.0% average peer score from GPs/GP registrars
- Incorrect answers were attributed to factual errors rather than reasoning failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning models outperform foundational LLMs on clinical multiple-choice questions requiring multi-step inference.
- Mechanism: Reasoning models undergo chain-of-thought fine-tuning that explicitly generates intermediate reasoning steps before final answer selection, improving logical coherence and transparency compared to next-token prediction alone.
- Core assumption: The MRCGP questions used require multi-step clinical reasoning rather than pure factual recall (not empirically validated in the paper).
- Evidence anchors:
  - [abstract] "These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care"
  - [section] "reasoning models have undergone reasoning-centric fine-tuning (such as chain-of-thought prompting) and 'think' through multiple intermediate steps before delivering a final answer"
  - [corpus] Weak direct evidence; related work MatSciBench shows similar reasoning benchmark patterns in materials science, suggesting domain transfer of reasoning capabilities.
- Break condition: If questions primarily test recall rather than reasoning, the chain-of-thought advantage would diminish substantially.

### Mechanism 2
- Claim: Live internet access enables incorporation of current clinical guidelines into model responses.
- Mechanism: All tested models can query external sources in real-time, allowing retrieval of updated clinical protocols (e.g., flying restriction guidelines, antibiotic duration recommendations) that may have changed since training data cutoff.
- Core assumption: The paywalled GP SelfTest questions were not directly retrievable via web search, so models were synthesizing general guideline knowledge rather than copying answers.
- Evidence anchors:
  - [section] "All these models have direct live internet access to inform their responses"
  - [section] "Another valuable feature...is direct live internet access, which allows incorporation of the latest clinical guidelines"
  - [corpus] No direct corpus evidence on internet-access mechanisms for clinical knowledge.
- Break condition: If web access were disabled, performance might drop on questions involving recent guideline changes.

### Mechanism 3
- Claim: Factual errors in model training data propagate through correct reasoning chains to produce incorrect answers.
- Mechanism: Models apply logically valid inference patterns to incorrect premises (e.g., misremembered dermatology guideline specifics), resulting in systematic errors despite sound reasoning structure.
- Core assumption: The distinction between "reasoning errors" and "factual errors" is meaningfully separable in clinical domains.
- Evidence anchors:
  - [section] "incorrect answers were largely due to factual errors which were mistakenly incorporated into the models' reasoning rather than due to errors in the reasoning itself"
  - [section] "Gemini 2.5 Pro made a factual error about a dermatology clinical guideline...with which it used correct reasoning to arrive at an incorrect answer"
  - [corpus] No corpus evidence addressing factual error propagation mechanisms.
- Break condition: If reasoning and factual knowledge are deeply entangled in model weights, targeted factual correction may not improve performance predictably.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Understanding what distinguishes "reasoning models" from foundational LLMs is essential for interpreting the performance differences observed.
  - Quick check question: What output does a reasoning model produce before delivering its final answer that a foundational model typically does not?

- Concept: Uncertainty quantification in LLMs
  - Why needed here: The paper identifies systematic overconfidence as a key limitation; understanding uncertainty representation is critical for safe clinical deployment.
  - Quick check question: Why might a model state an incorrect answer with the same confidence as a correct one?

- Concept: Cross-sectional study design limitations
  - Why needed here: Single-timepoint, single-attempt evaluations provide limited evidence about reproducibility and robustness.
  - Quick check question: What threats to validity arise from each model attempting each question only once?

## Architecture Onboarding

- Component map: Question extraction (GP SelfTest paywalled source) -> Prompt construction (GP persona instruction + full question context including images/tables) -> Model inference (o3, Claude Opus 4, Grok-3, Gemini 2.5 Pro) -> Response collection -> Binary scoring against official answer key -> Aggregate percentage calculation

- Critical path: Random question selection via Lucky Dip -> Context window population (text + images) -> Single-attempt inference -> Accuracy comparison against peer benchmark (73%)

- Design tradeoffs: 100 questions provides statistical signal but limits generalizability; 94 text-only questions underrepresents multimodal capabilities; single attempts per question prevents variance estimation; paywalled source reduces contamination risk but limits reproducibility

- Failure signatures:
  - Factual hallucination: Model invents plausible-sounding guideline details (e.g., wrong antibiotic duration)
  - Confidence miscalibration: Equal stated confidence for correct and incorrect answers
  - Shared failure modes: Multiple models failing on same questions (Questions 27 and 99) suggests systematic knowledge gaps

- First 3 experiments:
  1. Extend to 500+ questions with stratified sampling across question types (text, images, tables, graphs) to characterize multimodal performance distribution
  2. Run 5 attempts per question per model to quantify answer stability and confidence calibration across runs
  3. Test with internet access disabled to isolate reasoning capabilities from retrieval augmentation effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of reasoning models differ when analyzing clinical images and graphs compared to text-only questions?
- Basis in paper: [explicit] The author suggests future research should incorporate "a larger number of examination questions with non-textual data such as tables of results, graphs, and clinical images" to better assess analytical breadth.
- Why unresolved: This study relied mostly on text (94/100 questions), containing no graphs and only three clinical images, leaving visual reasoning capabilities largely untested.
- What evidence would resolve it: A comparative analysis of model accuracy on a dataset balanced between text-based and image-based clinical vignettes.

### Open Question 2
- Question: Can reasoning models be calibrated to accurately express confidence or uncertainty when generating incorrect clinical responses?
- Basis in paper: [explicit] The authors note it is "concerning" that all models "stated correct and incorrect answers with equal confidence," highlighting a suboptimal ability to recognise uncertainty.
- Why unresolved: High accuracy is dangerous in clinical settings if the model cannot signal low confidence in its own hallucinations or factual errors.
- What evidence would resolve it: Studies measuring the correlation between model-provided confidence scores and the objective validity of their clinical reasoning.

### Open Question 3
- Question: Does high performance on structured multiple-choice questions predict accuracy in managing the unstructured information of real-world primary care?
- Basis in paper: [inferred] The paper cautions that the "unstructured nature of information presentation in real-world primary care... is not reflected in the precise packages of information presented in MRCGP-style questions."
- Why unresolved: Success on clean, packaged exam questions may not translate to the "messier" reality of patient histories where relevant data must be filtered from noise.
- What evidence would resolve it: A validation study testing model performance on unscripted clinical vignettes or de-identified consultation transcripts.

## Limitations
- Single-attempt evaluation prevents quantification of answer stability and confidence calibration across multiple runs
- 100-question sample size limits generalizability across the full breadth of primary care scenarios
- Focus on 94 text-only questions underrepresents multimodal capabilities essential for clinical practice

## Confidence
- **High Confidence**: The relative ranking of models (o3 > Claude Opus 4 = Grok-3 = Gemini 2.5 Pro) and the substantial performance gap between LLMs (99-95%) and human peers (73%) are robust findings supported by the data structure.
- **Medium Confidence**: The mechanism explanation for reasoning model superiority (chain-of-thought fine-tuning improving multi-step inference) is plausible but not empirically validated within this study's design.
- **Low Confidence**: Claims about factual error propagation through reasoning chains lack direct empirical support; the study observed this pattern but did not systematically test whether correcting factual errors would improve reasoning outcomes.

## Next Checks
1. **Stability Analysis**: Run each model through the same 100 questions five times with identical prompts and settings, measuring answer consistency rates and confidence calibration across attempts to quantify performance variance and identify systematic versus random errors.
2. **Internet Access Isolation**: Disable live internet access for all models and re-run the evaluation to determine the contribution of real-time retrieval augmentation versus intrinsic reasoning capabilities to the observed performance levels.
3. **Question Type Stratification**: Expand the evaluation to 500+ questions stratified by format (text-only, image-based, table-based, graph-based) to characterize multimodal performance distributions and identify specific clinical reasoning domains where LLM performance may degrade.