---
ver: rpa2
title: 'Online Continual Learning: A Systematic Literature Review of Approaches, Challenges,
  and Benchmarks'
arxiv_id: '2501.04897'
source_url: https://arxiv.org/abs/2501.04897
tags:
- learning
- arxiv
- online
- data
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive Systematic Literature
  Review (SLR) on Online Continual Learning (OCL), analyzing 81 approaches and over
  500 components. The study identifies three main strategy types: Replay-based, Architecture-based,
  and Regularization-based, with replay-based methods being the most prevalent.'
---

# Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks

## Quick Facts
- **arXiv ID:** 2501.04897
- **Source URL:** https://arxiv.org/abs/2501.04897
- **Reference count:** 40
- **Primary result:** First comprehensive SLR analyzing 81 OCL approaches, identifying replay-based as most prevalent strategy.

## Executive Summary
This systematic literature review comprehensively analyzes 81 online continual learning (OCL) approaches, categorizing them into replay-based, architecture-based, and regularization-based strategies. The study identifies key challenges including catastrophic forgetting, computational overhead, and scalability, while highlighting dominant datasets (CIFAR, ImageNet) and tools (PyTorch, Avalanche). With over 500 components extracted, the review provides a structured synthesis of the field and outlines promising future directions including hybrid memory architectures and federated learning integration.

## Method Summary
The review employed Newman's SLR methodology across four major databases using a specific search query combining catastrophic forgetting, task incremental learning, streaming data, and memory efficiency terms with online continual learning variants. Initial 2,061 papers were filtered through semantic similarity assessment using SBERT embeddings against keyword sets, then quality assessment questions narrowed the selection to 81 high-quality papers. The analysis extracted approaches, components, features, and datasets following a defined entity coding scheme.

## Key Results
- Replay-based methods dominate OCL landscape (62 of 81 approaches)
- ResNet18 variants used in 62/81 approaches, with SGD as dominant optimizer
- CIFAR and ImageNet are most widely used benchmark datasets
- Three main strategy types identified: Replay-based, Architecture-based, Regularization-based
- Key challenges include catastrophic forgetting, computational overhead, and scalability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replay-based methods mitigate catastrophic forgetting by interleaving past samples with incoming data during model updates.
- **Mechanism:** A fixed-size episodic memory buffer stores representative samples from previous tasks. During training on new data (batch t), the model performs gradient updates on both the current batch and a sampled subset from memory. This dual exposure constrains weight updates to preserve performance on past tasks while adapting to new distributions.
- **Core assumption:** The memory budget is sufficient to store minimally representative samples across all observed tasks, and the sampling strategy maintains diversity without overfitting.
- **Evidence anchors:**
  - [abstract] "...replay-based methods being the most prevalent... key challenges addressed include catastrophic forgetting, computational overhead, and scalability."
  - [section 3.2] "Replay-based methods: The central idea behind this strategy is to use a portion of previously encountered input data for replay, ensuring that the model retains what it has learned... Experience Replay (ER) trains the model on a combination of new input data and samples stored in episodic memory."
  - [corpus] Related work confirms replay efficacy but notes privacy constraints in some domains (see "Replay-free Online Continual Learning with Self-Supervised MultiPatches" â€” arXiv:2502.09140).

### Mechanism 2
- **Claim:** Architecture-based methods reduce interference by isolating task-specific parameters through dynamic network expansion or parameter freezing.
- **Mechanism:** When new task data arrives, the model either (a) expands capacity by adding new neurons/sub-networks, or (b) masks/freezes weights critical to prior tasks while training a subset of parameters. This structural separation prevents gradient updates from overwriting previously learned representations.
- **Core assumption:** Task identity or reliable task boundary detection is available (at least during training) to allocate or isolate parameters correctly; computational budget permits model growth.
- **Evidence anchors:**
  - [section 3.2] "Architecture-based methods aim to address catastrophic forgetting by modifying the model's structure... assigning sub-networks for new tasks... This allows the model to retain previous knowledge while accommodating new learning."
  - [section 3.2] "Techniques such as pruning redundant weights, using dynamically expandable networks (DEN), and employing Progressive Neural Networks (PNNs) are commonly used."
  - [corpus] Limited direct corpus validation for OCL-specific architectural methods; most neighbors focus on replay or hybrid approaches.

### Mechanism 3
- **Claim:** Regularization-based methods preserve prior knowledge by penalizing changes to important weights during optimization.
- **Mechanism:** A regularization term is added to the loss function that constrains updates to weights identified as critical for previous tasks. Importance is typically estimated via Fisher information or knowledge distillation from a teacher model. This biases gradient descent toward directions that minimize interference.
- **Core assumption:** Importance estimates are accurate and stable across task sequences; the model has sufficient capacity to encode all tasks within a fixed parameter budget.
- **Evidence anchors:**
  - [section 3.2] "Regularization-based methods focus on minimizing changes to weights that are essential for retaining past knowledge... often struggle to effectively mitigate catastrophic forgetting, especially in online settings and with long task sequences."
  - [section 3.3.4] "Knowledge Distillation (KD)... ensures that older information is not lost during updates... reduces memory usage by storing predictions instead of raw data."
  - [corpus] Weak direct validation in neighbors; most recent corpus focuses on replay or prompt-based approaches.

## Foundational Learning

- **Concept:** **Catastrophic Forgetting**
  - **Why needed here:** The central problem OCL solves. Without mitigation, sequential gradient updates overwrite representations needed for earlier tasks, causing performance collapse on those tasks.
  - **Quick check question:** After training on task B, does your model retain >90% of its task A accuracy?

- **Concept:** **Stability-Plasticity Trade-off**
  - **Why needed here:** Governs the balance between retaining old knowledge (stability) and adapting to new data (plasticity). OCL methods must explicitly manage this tension.
  - **Quick check question:** Is your method tuning a hyperparameter (e.g., regularization weight, memory ratio) that directly controls this trade-off?

- **Concept:** **Class-Incremental vs. Task-Incremental Settings**
  - **Why needed here:** Evaluation protocols differ. In Class-IL (most common in the review), task identity is unavailable at inference, requiring a unified classifier. In Task-IL, task identity is given, simplifying evaluation but reducing realism.
  - **Quick check question:** Does your evaluation protocol match the intended deployment scenario (task identity available or not)?

## Architecture Onboarding

- **Component map:** Backbone (ResNet18-Slim) -> Optimizer (SGD) -> Memory buffer (episodic with reservoir sampling) -> Classifier (NCM or softmax) -> Loss (cross-entropy + distillation/contrastive terms)

- **Critical path:**
  1. Define OCL setting: single-pass data, disjoint label spaces, no revisit
  2. Choose strategy: replay (if memory available), hybrid (replay + regularization), or exemplar-free (if privacy-constrained)
  3. Select backbone (ResNet18-Slim or Reduced) and optimizer (SGD with appropriate learning rate schedule)
  4. Implement buffer management (reservoir sampling for uniform coverage; gradient-based for harder examples)
  5. Integrate evaluation metrics: Average Accuracy, Forgetting Measure, Backward Transfer

- **Design tradeoffs:**
  - Memory vs. accuracy: Larger buffers reduce forgetting but increase compute and may violate privacy
  - Plasticity vs. stability: Higher learning rates improve adaptation but increase forgetting; regularization counters this at the cost of slower learning
  - Simplicity vs. performance: ER baselines are strong; complex methods (e.g., generative replay) add overhead without guaranteed gains
  - **Assumption:** Hybrid approaches (replay + regularization) may offer the best trade-off, but this is inferred, not directly proven in the paper

- **Failure signatures:**
  - Sudden accuracy drop on early tasks after a new task: indicates buffer imbalance or regularization failure
  - Stagnant learning accuracy on new tasks: excessive stability (over-regularized) or insufficient model capacity
  - High variance across runs: non-IID data streams causing buffer bias or unstable optimization

- **First 3 experiments:**
  1. **Establish baseline:** Implement ER with reservoir sampling on Split CIFAR-10 using ResNet18-Slim, SGD, batch size 10, memory size 200. Report Average Accuracy and Forgetting Measure.
  2. **Ablate memory strategies:** Compare reservoir sampling vs. gradient-based selection vs. ring buffer, holding all else constant.
  3. **Probe stability-plasticity:** Sweep buffer ratio (0%, 10%, 20%, 50% of batch) and learning rate (0.01, 0.1, 0.3) to characterize the trade-off frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hybrid memory architectures that combine sparse retrieval techniques with generative replay improve memory efficiency and computational performance in resource-constrained OCL environments?
- **Basis in paper:** [explicit] The authors suggest in Section 4.5.2 that future work could focus on "hybrid memory architectures that combine sparse retrieval techniques with generative replay methods" to balance performance and resource constraints.
- **Why unresolved:** While generative models like those in the CONAN framework reduce memory usage, the paper notes that their "real-time applicability still requires further refinement" for high-frequency data streams.
- **What evidence would resolve it:** Empirical results from large-scale benchmarks (e.g., financial market predictions) showing that a hybrid approach reduces memory footprint and latency compared to pure replay or generative methods alone.

### Open Question 2
- **Question:** How can self-supervised learning be effectively leveraged to learn shared representations across multimodal data streams (e.g., audio and video) in online settings to address data scarcity?
- **Basis in paper:** [explicit] Section 4.5.4 states that "A promising direction... is to explore self-supervised learning, where unsupervised pretraining across modalities could address challenges like data scarcity."
- **Why unresolved:** The review highlights that "Most OCL literature has predominantly focused on visual data, leaving non-visual and multimodal tasks underexplored."
- **What evidence would resolve it:** The development of an OCL framework that uses SSL to successfully align audio, video, and text modalities in real-time, demonstrating improved generalization over unimodal baselines.

### Open Question 3
- **Question:** How can OCL strategies be integrated into federated learning systems to handle evolving data distributions (e.g., new disease patterns in hospitals) while preserving privacy?
- **Basis in paper:** [explicit] In Section 4.5.1, the authors propose that "integrating OCL strategies into federated learning systems presents an exciting avenue" and could provide insights for robust distributed frameworks.
- **Why unresolved:** The paper identifies "domain-agnostic solutions" and "privacy" as key challenges, noting that the synergy between OCL and federated learning requires efficient and scalable solutions.
- **What evidence would resolve it:** A successful implementation of a distributed OCL system where clients (e.g., hospitals) adapt to new data classes without sharing raw data, validated on a distributed medical dataset.

### Open Question 4
- **Question:** Can unsupervised task discovery combined with uncertainty estimation enable OCL models to automatically identify new tasks in dynamic streams with noisy labels?
- **Basis in paper:** [explicit] Section 4.5.5 suggests that future research could explore "unsupervised task discovery to automatically identify new tasks" and "integrating uncertainty estimation" to manage task ambiguity.
- **Why unresolved:** The paper notes that existing selective sampling methods often "struggle with detecting and mitigating noise" in complex, unlabeled streams where task boundaries are unclear.
- **What evidence would resolve it:** An algorithm capable of segmenting a continuous, noisy data stream into distinct tasks with high accuracy and maintaining performance without explicit task identifiers.

## Limitations
- Database coverage limited to four major sources, excluding arXiv preprints beyond 2022
- 81-paper selection may not fully represent niche OCL domains
- Quantitative claims about method prevalence may be biased by publication trends

## Confidence
- **High:** Replay-based methods are most prevalent (well-supported by paper's quantitative breakdown)
- **Medium:** Regularization-based method efficacy (weaker direct validation in neighboring literature)
- **Medium:** Architecture-based method descriptions (limited corpus validation in neighbors)

## Next Checks
1. Verify the exact search query and database coverage produce comparable initial paper pools
2. Replicate the semantic similarity filtering with SBERT to confirm the 170-paper Phase 2 selection
3. Reassess the quality assessment questions on a subset of papers to confirm the 81-paper final selection criteria