---
ver: rpa2
title: Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion
  in Federated Fine-Tuning
arxiv_id: '2506.05977'
source_url: https://arxiv.org/abs/2506.05977
tags:
- data
- training
- fedbe
- blocks
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in federated fine-tuning
  of large language models, a critical issue worsened by data heterogeneity and resource
  constraints in distributed environments. To address this, the authors propose FedBE,
  a novel framework that uses gradient-guided transformer block expansion to isolate
  new task knowledge from pre-trained representations and dynamically allocates expanded
  blocks to clients based on their data distribution and computational capacity.
---

# Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning

## Quick Facts
- **arXiv ID**: 2506.05977
- **Source URL**: https://arxiv.org/abs/2506.05977
- **Reference count**: 40
- **Primary result**: FedBE achieves 12-74% higher accuracy retention on general tasks and 1.9-3.1× faster convergence than existing federated fine-tuning methods

## Executive Summary
This paper addresses catastrophic forgetting in federated fine-tuning of large language models, a critical challenge in distributed learning environments with heterogeneous data and resource constraints. The authors propose FedBE, a novel framework that uses gradient-guided transformer block expansion to isolate new task knowledge from pre-trained representations. By dynamically allocating expanded blocks to clients based on their data distribution and computational capacity, FedBE enables structured knowledge separation and personalized adaptation while preserving generalization. Experiments demonstrate significant improvements in accuracy retention and convergence speed compared to existing methods.

## Method Summary
FedBE expands pre-trained models by appending trainable transformer blocks after selected frozen backbone layers, with zero-initialization ensuring preservation of original representations. The server performs brief proxy training to compute layer-wise gradient norms, using a composite score that balances gradient magnitude against spatial distribution to select top-k expansion positions. Clients receive dynamically allocated block subsets based on data heterogeneity (measured via Dirichlet concentration parameter α) and computational capacity, with low-α clients receiving higher-layer blocks for task-specific features and high-α clients receiving lower-layer blocks for general semantics. Only expansion blocks are updated locally and uploaded, with the backbone remaining frozen throughout training.

## Key Results
- Achieves 12-74% higher accuracy retention on general tasks compared to baseline methods
- Accelerates model convergence by 1.9-3.1× during federated fine-tuning
- Demonstrates strong resistance to catastrophic forgetting while maintaining downstream task performance
- Shows effectiveness across different levels of data heterogeneity (α=0.1, 1.0, 10.0)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structurally separating pre-trained knowledge from task-specific knowledge via block expansion mitigates catastrophic forgetting.
- Mechanism: FedBE appends trainable transformer blocks after selected frozen backbone layers. Zero-initialization ensures expanded blocks produce no output at initialization, preserving original representations. Gradient updates are confined to expansion blocks, preventing overwriting of pre-trained weights.
- Core assumption: Pre-trained representations encode general knowledge that can be preserved if gradients are blocked from modifying them.
- Evidence anchors:
  - [abstract] "structurally separating newly learned task-specific knowledge from the original pre-trained representations"
  - [section 3.2] "the parameters of ϕexpand_l are initialized by copying the parameters from ϕ_l followed by zero-initialization to ensure non-interference with the original model's behavior"
  - [corpus] Related work on "Mitigating Catastrophic Forgetting in Continual Learning through Model Growth" supports architectural separation strategies.

### Mechanism 2
- Claim: Gradient magnitude indicates layer sensitivity to downstream tasks and predicts effective expansion positions.
- Mechanism: The server trains briefly on a proxy dataset and computes L2 gradient norms per layer. A composite score S(l) = g(l)/max(g) + λ·d(l,Ek)/L balances gradient magnitude against spatial distribution penalty, selecting top-k layers for expansion.
- Core assumption: Layers with larger gradient norms during proxy training are task-critical and benefit most from additional capacity.
- Evidence anchors:
  - [section 3.2] "monitors the gradient norm at different blocks... estimates block-wise sensitivity to the task and selects the top-k blocks"
  - [section 2.3] "layers with larger gradient norms typically play a more critical role in information processing"

### Mechanism 3
- Claim: Client-aware block allocation improves convergence by matching block depth to data heterogeneity and computational capacity.
- Mechanism: Clients with uniform data distributions (high α in Dirichlet) receive lower-layer blocks for general semantic features; clients with skewed distributions (low α) receive higher-layer blocks for abstract task-specific features. Block count is dynamically adjusted based on reported training time.
- Core assumption: Lower transformer layers capture general features requiring diverse data; higher layers capture task-specific abstractions adaptable from concentrated data.
- Evidence anchors:
  - [section 3.3] "clients with lower data heterogeneity... expanded layers closer to the input end are prioritized"
  - [section 3.1] Fig. 1 illustrates differentiated block assignment across three client types

## Foundational Learning

- Concept: **Catastrophic forgetting in sequential learning**
  - Why needed here: Understanding that neural networks overwrite previous task representations when optimizing for new tasks is essential for grasping why structural separation matters.
  - Quick check question: Can you explain why regularization methods like EWC may fail when Fisher information is estimated from fragmented federated data?

- Concept: **Transformer layer functional specialization**
  - Why needed here: FedBE's allocation strategy assumes lower layers capture general semantics while higher layers encode task-specific abstractions—this informs block assignment logic.
  - Quick check question: Which layers of BERT would you expect to be most sensitive when fine-tuning for a domain-specific classification task?

- Concept: **Non-IID data partitioning via Dirichlet distribution**
  - Why needed here: The paper quantifies client heterogeneity using Dirichlet concentration parameter α; understanding this metric is necessary for interpreting allocation decisions.
  - Quick check question: If client A has α=0.1 and client B has α=10.0, which client has more skewed class distribution?

## Architecture Onboarding

- Component map:
  - Server -> Proxy training -> Gradient norm computation -> Layer selection -> Block initialization -> Client allocation -> Aggregation
  - Frozen backbone -> Original pre-trained parameters (never updated)
  - Expansion blocks -> Trainable transformer modules appended after selected backbone layers; zero-initialized weights/biases
  - Allocation module -> Computes priority scores based on heterogeneity, training history, and resource

- Critical path:
  1. Server trains on proxy dataset → monitors gradients → selects k expansion positions
  2. Server initializes expansion blocks with zero weights
  3. Per round: server assigns block subsets to each client via priority scoring
  4. Clients train only assigned blocks locally
  5. Clients upload only expansion block parameters (backbone stays frozen)
  6. Server aggregates via weighted averaging

- Design tradeoffs:
  - More expansion blocks → better forgetting mitigation but higher compute/memory
  - Larger λ → more uniform spatial distribution but weaker gradient-based selection
  - Aggressive dynamic allocation → better heterogeneity matching but more server coordination overhead

- Failure signatures:
  - Accuracy on general tasks drops significantly → expansion blocks may be contaminating backbone (check initialization)
  - Slow convergence on skewed clients → block allocation may be inverted (low-α clients receiving lower layers)
  - Training time variance increases → dynamic adjustment formula may be too sensitive

- First 3 experiments:
  1. **Sanity check**: Verify zero-initialized expansion blocks produce identical outputs to frozen backbone on sample inputs before training begins.
  2. **Gradient analysis**: Replicate Figure 4 comparison by training with expansion blocks at bottom/top/uniform/gradient-guided positions on MRPC; expect gradient-guided to achieve highest downstream accuracy with lowest general task degradation.
  3. **Allocation validation**: Simulate 10-client heterogeneous setting with known Dirichlet α values; confirm low-α clients receive higher-layer blocks and achieve faster per-round convergence than uniform assignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can parameter compression techniques be integrated with FedBE to further reduce the memory and communication overhead for highly resource-constrained edge devices?
- Basis in paper: [explicit] The conclusion states, "Future work will explore parameter compression and cross-modal adaptation to extend applicability to edge devices."
- Why unresolved: The current framework relies on expanding the architecture with full transformer blocks, which increases the parameter count. While more efficient than full fine-tuning, the added parameters may still be too heavy for the most limited edge devices (e.g., IoT sensors).
- What evidence would resolve it: A modified FedBE framework utilizing quantization or pruning on the expanded blocks, demonstrating reduced memory footprints and communication costs without degrading the mitigation of catastrophic forgetting.

### Open Question 2
- Question: To what extent does the domain similarity between the server's proxy dataset and the clients' downstream tasks affect the accuracy of the gradient-guided block selection?
- Basis in paper: [inferred] Section 3.1 describes the server using a "small-scale proxy dataset" to calculate gradient norms for block selection, but provides no analysis on how the choice of this data influences selection quality.
- Why unresolved: The method assumes the proxy data is sufficient to identify important blocks. If the proxy data distribution diverges significantly from the heterogeneous client data, the gradient norms may fail to identify the optimal blocks for expansion, potentially reducing the model's generalization ability.
- What evidence would resolve it: An ablation study comparing block selection patterns and final model performance using proxy datasets of varying sizes and domain alignments (e.g., generic text vs. domain-specific text).

### Open Question 3
- Question: Does FedBE maintain its convergence and efficiency advantages when applied to modern Large Language Models (LLMs) with parameter sizes exceeding 1 billion?
- Basis in paper: [inferred] The paper title references LLMs, but the experimental validation is restricted to RoBERTa-base (125M) and BERT-large (340M).
- Why unresolved: While the authors claim applicability to LLMs, the computational overhead of replicating transformer blocks for models like LLaMA-7B or LLaMA-13B may impose memory constraints that exceed the capabilities of the edge devices (Jetson TX2/NX) used in the study.
- What evidence would resolve it: Experimental results demonstrating FedBE's training time, memory usage, and accuracy retention on models with 1B+ parameters within a federated environment.

## Limitations

- Limited theoretical analysis of why zero-initialized expansion blocks specifically prevent catastrophic forgetting beyond architectural separation
- Evaluation restricted to text classification tasks with RoBERTa-base and BERT-large, leaving generalizability to other model architectures and task types unproven
- 50-round training limit may not capture long-term forgetting dynamics in extended federated scenarios
- Computational overhead of dynamic block allocation relative to static baselines not quantified

## Confidence

**High confidence**: The core architectural insight that structurally separating pre-trained knowledge from task-specific knowledge via block expansion effectively mitigates catastrophic forgetting is well-supported by the empirical results showing 12-74% higher accuracy retention.

**Medium confidence**: The gradient-guided layer selection mechanism is plausible and shows promising results, but its sensitivity to proxy dataset quality and distribution mismatch has not been thoroughly tested.

**Low confidence**: Claims about convergence speedup (1.9-3.1×) and training time efficiency improvements require more rigorous benchmarking against competitive baselines under identical computational budgets and hardware constraints.

## Next Checks

1. **Proxy dataset sensitivity analysis**: Systematically vary the proxy dataset size and distribution (including mismatched distributions) to quantify how gradient-guided layer selection degrades when the proxy diverges from client data distributions.

2. **Architectural ablation study**: Compare FedBE's zero-initialized expansion blocks against alternative architectural separation strategies (e.g., adapters, low-rank decomposition) under identical training budgets to isolate the specific contribution of the block expansion mechanism.

3. **Long-term forgetting dynamics**: Extend federated training beyond 50 rounds with periodic evaluation on both original pre-trained tasks and newly learned tasks to measure whether the forgetting mitigation effect persists or degrades over time.