---
ver: rpa2
title: 'Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator'
arxiv_id: '2505.20738'
source_url: https://arxiv.org/abs/2505.20738
tags:
- benchmark
- self-bias
- benchmarks
- evaluation
- silencer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and mitigates self-bias in LLM-as-Benchmark-Generator
  methods, where models perform better on benchmarks they generate themselves. The
  bias arises from question domain, language style, and wrong labels.
---

# Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator

## Quick Facts
- arXiv ID: 2505.20738
- Source URL: https://arxiv.org/abs/2505.20738
- Authors: Peiwen Yuan; Yiwei Li; Shaoxiong Feng; Xinglin Wang; Yueqi Zhang; Jiayi Shi; Chuyi Tan; Boyuan Pan; Yao Hu; Kan Li
- Reference count: 33
- Primary result: Reduces self-bias in LLM-generated benchmarks, increasing evaluation consistency from 0.655 to 0.833 Pearson correlation with human-annotated benchmarks.

## Executive Summary
This paper identifies and mitigates self-bias in LLM-as-Benchmark-Generator methods, where models perform better on benchmarks they generate themselves. The bias arises from question domain, language style, and wrong labels. To address this, the authors propose SILENCER, a framework that reduces these sub-biases at the sample level using Attribute Integration, Cross Paraphrase, and Label Calibration, then ensembles benchmarks with bias-aware weighting. Experiments show SILENCER significantly improves evaluation consistency with human-annotated benchmarks, increasing Pearson correlation from 0.655 to 0.833 on average, and suppresses self-bias to near zero. The approach is generalizable across tasks, generators, and methods, with ablation studies confirming the effectiveness of each component.

## Method Summary
SILENCER is a framework that mitigates self-bias in LLM-as-Benchmark-Generator methods through sample-level and benchmark-level interventions. Sample-level mitigation includes Attribute Integration (pooling attributes from all generators), Cross Paraphrase (having different models rewrite questions), and Label Calibration (using predictions from other generators to recalibrate labels). The benchmark-level component uses a Bias-Neutralizing Ensemble algorithm that iteratively weights generator outputs based on their consistency with the emerging consensus. The framework was validated across three tasks (MATH, MMLU-Pro, HellaSwag) using seven diverse generators, demonstrating significant reduction in self-bias and improved correlation with human-annotated benchmarks.

## Key Results
- Reduces self-bias to near-zero across all tested tasks and generators
- Increases Pearson correlation with human benchmarks from 0.655 to 0.833 on average
- Wrong Label Bias contributes 67-80% of total self-bias, with SILENCER reducing this by 85.3% on average
- Cross-paraphrasing and attribute integration increase entropy (diversity) from 9.749 to 10.078
- Reweighted ensemble achieves 0.9386 correlation vs 0.9216 for naive ensemble

## Why This Works (Mechanism)

### Mechanism 1: Label Error Alignment Disruption
The paper mathematically demonstrates that self-labeling inflates expected accuracy because models produce labels aligned with their own incorrect reasoning patterns. SILENCER disrupts this via Label Calibration, where a generator re-evaluates its initial label by reviewing predictions and rationales from other generators. This forces the generator to break its "error consistency" loop. Core assumption: models have heterogeneous error profiles; if all models make the exact same reasoning errors, cross-viewing predictions will not correct the label. Evidence shows Wrong Label Bias contributes 67-80% of total self-bias. Break condition: if the generator pool is homogeneous and shares consistent failure modes.

### Mechanism 2: Iterative Proxy-Consistency Weighting
SILENCER constructs reliable benchmarks without human ground truth by iteratively ensembling generator outputs, weighting each generator inversely to its deviation from the emerging consensus. The Bias-Neutralizing Ensemble algorithm initializes weights uniformly, treats the weighted ensemble as a proxy for the human benchmark, and updates weights using ReLU(Pearson correlation) + epsilon. This utilizes the "wisdom of the crowd" to suppress individual biases. Core assumption: the crowd is closer to the truth than any single biased individual (Condorcet's Jury Theorem application). Evidence shows consistency (0.861) outperforms accuracy (0.343) as a proxy for bias quality. Break condition: if the majority of generators possess a strong, shared bias, the consensus will converge on the wrong standard.

### Mechanism 3: Stylistic and Domain Detoxification
SILENCER removes the generator's "fingerprint" from question text and domain distribution through Attribute Integration (pooling attributes from all generators) and Cross Paraphrase (having Model B rewrite Model A's question). This ensures the evaluator is tested on reasoning, not pattern matching of its own output style. Core assumption: a model's performance is partially dependent on familiarity with specific linguistic tokens or topic distributions. Evidence shows entropy increases from 9.749 to 10.078 after applying SILENCER. Break condition: if paraphrasing introduces semantic drift, changing question difficulty or meaning.

## Foundational Learning

- **Concept: Self-Bias vs. Self-Preference**
  - *Why needed:* Distinguishing preference (liking one's own style) from alignment (making the same mistake in generation and evaluation) is crucial. This paper focuses on error alignment (Wrong Label Bias).
  - *Quick check:* If a model generates a question with an incorrect ground truth label, and later answers that question using its own faulty logic to match that label, is that self-preference or error alignment? (Answer: Error alignment)

- **Concept: Convergence in Iterative Algorithms**
  - *Why needed:* The ensemble algorithm relies on converging to a fixed point where weights stabilize.
  - *Quick check:* In the SILENCER algorithm, what condition ensures weights don't oscillate forever? (Answer: The inclusion of δ > 0 and properties of the correlation function ensure contraction)

- **Concept: The Dunning-Kruger Effect in LLMs**
  - *Why needed:* The paper investigates using "Accuracy" as a weight and finds it negatively correlated with quality (0.343), akin to Dunning-Kruger—less competent models may be overconfident.
  - *Quick check:* Why is raw accuracy a poor metric for assigning weight in this specific ensemble? (Answer: Because models with high self-bias might artificially inflate their own accuracy scores)

## Architecture Onboarding

- **Component map:** Task Description + Seed Samples -> Orchestrator (manages T Generator LLMs) -> Sample-wise Pipeline (Attribute Pooling -> Question Generation -> Cross-Paraphrase -> Label Calibration) -> Benchmark-wise Pipeline (Performance Matrix Calculation -> Iterative Weight Update -> Sampling & Assembly)

- **Critical path:** Label Calibration is the bottleneck, requiring generating predictions from all T models for every sample to calibrate one label, incurring ~30% higher token costs.

- **Design tradeoffs:**
  - *Cost vs. Bias:* Framework adds overhead (Attribute integration, Paraphrasing, Multi-prediction calibration). Paper argues evaluation accuracy is worth cost, but massive benchmarks become expensive.
  - *Homogeneity vs. Robustness:* Using T=7 diverse models (GPT, Claude, Qwen, etc.) is better than T=7 similar models. Must maintain heterogeneous model garden.

- **Failure signatures:**
  - *Weight Collapse:* If algorithm assigns near-zero weight to all but one model, ensemble benefit is lost. Check for dominant models or data contamination.
  - *Negative Bias:* If relative performance R(M|Mhuman) is abnormally high, suspect data contamination in reference human benchmark.

- **First 3 experiments:**
  1. **Sanity Check (Data Contamination):** Before running SILENCER, run the "ConStat" check on reference models against human benchmark to ensure no contamination artifacts.
  2. **Label Calibration Ablation:** Run pipeline with only Label Calibration enabled (B_l contributes >67% of bias, highest ROI).
  3. **Stress Test Generator Count (T):** Run with T=3 vs T=7. Verify Reweighted Ensemble weight estimation accuracy increases with T (Table 4 shows jump from 0.453 to 0.948 correlation).

## Open Questions the Paper Calls Out
None

## Limitations
- Label Calibration incurs ~30% higher token costs due to requiring predictions from all T generators for each sample
- Two reference models (DeepSeek-Distill-Qwen-32B, QwQ-32B) show negative evaluation bias suggesting data contamination
- Validation limited to three tasks (MATH, MMLU-Pro, HellaSwag) and specific set of seven generators

## Confidence
- **High Confidence:** Existence of self-bias in LLM-as-Benchmark-Generator methods is well-established and independently verified. Mathematical proof that self-labeling inflates expected accuracy is sound.
- **Medium Confidence:** Three sub-bias decomposition (Bq, Bs, Bl) and their relative contributions are well-supported by data. Attribution of improvements to each component relies on ablation studies with limited sample sizes.
- **Low Confidence:** Iterative ensemble weighting algorithm's convergence properties and sensitivity to initialization are not thoroughly analyzed. Claim that consistency is best proxy for bias quality is based on correlation without establishing causation.

## Next Checks
1. **Cross-domain validation:** Apply SILENCER to generate and evaluate benchmarks for tasks outside mathematical/cognitive reasoning domain (e.g., creative writing evaluation, code completion) to test generalizability claims.
2. **Robustness to generator homogeneity:** Test framework with generator pools of varying diversity (all GPT variants vs. heterogeneous mix) to validate claim that heterogeneous error profiles are essential for label calibration effectiveness.
3. **Cost-effectiveness analysis:** Measure relationship between benchmark size, token overhead, and evaluation consistency gains across multiple tasks to determine practical cost-benefit thresholds for deployment.