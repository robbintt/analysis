---
ver: rpa2
title: On the Optimality of the Median-of-Means Estimator under Adversarial Contamination
arxiv_id: '2510.07867'
source_url: https://arxiv.org/abs/2510.07867
tags:
- distributions
- theorem
- contamination
- class
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes the (minimax) optimality of the Median-of-Means\
  \ (MoM) estimator under adversarial contamination for multiple classes of distributions.\
  \ The key finding is that MoM achieves the optimal asymptotic bias of \u221A\u03B1\
  \ in the class of distributions with finite variance and \u03B1^{r/(1+r)} in the\
  \ class of distributions with infinite variance and finite (1+r)-th moment."
---

# On the Optimality of the Median-of-Means Estimator under Adversarial Contamination

## Quick Facts
- **arXiv ID**: 2510.07867
- **Source URL**: https://arxiv.org/abs/2510.07867
- **Reference count**: 40
- **Primary result**: Establishes minimax optimality of Median-of-Means estimator under adversarial contamination for distributions with finite variance and infinite variance with finite (1+r)-th moment

## Executive Summary
This paper provides a comprehensive analysis of the Median-of-Means (MoM) estimator under adversarial contamination, establishing its (minimax) optimality for multiple distribution classes. The authors prove that MoM achieves the optimal asymptotic bias of √α for distributions with finite variance and α^{r/(1+r)} for distributions with infinite variance and finite (1+r)-th moment. Notably, MoM is sub-optimal for light-tailed distributions, achieving an asymptotic bias of α^{2/3} instead of the optimal α^{1/2}log(1/α). The optimality is particularly pronounced for symmetric distributions, where MoM achieves the optimal asymptotic bias of α, including for Gaussian distributions. These theoretical results are supported by numerical experiments that align with the derived bounds.

## Method Summary
The paper analyzes the Median-of-Means estimator under adversarial contamination where an attacker can corrupt αn samples. The MoM estimator partitions n samples into k blocks, computes block means, and returns their median. The adversarial contamination model assumes the attacker can observe clean data before deciding which samples to corrupt. The paper provides explicit formulas for selecting the number of blocks k based on the distribution class to achieve optimal asymptotic bias. For finite variance distributions, k ≈ γαn is optimal, while for symmetric distributions, k ≈ βn can achieve the optimal bias of α. The analysis uses concentration inequalities and quantile function bounds to establish both upper and lower bounds on estimation error.

## Key Results
- MoM achieves optimal asymptotic bias of √α for distributions with finite variance
- MoM achieves optimal asymptotic bias of α^{r/(1+r)} for distributions with infinite variance and finite (1+r)-th moment
- MoM is sub-optimal for light-tailed distributions, achieving α^{2/3} instead of optimal α^{1/2}log(1/α)
- For symmetric distributions, MoM achieves optimal asymptotic bias of α, including for Gaussian distributions
- The paper provides matching lower bounds proving the sub-optimality for light-tailed distributions

## Why This Works (Mechanism)

### Mechanism 1: Majority-Vote Resilience via Block Partitioning
The Median-of-Means estimator provides robustness by localizing adversarial corruption to a minority of data blocks and filtering them out via a median operation. The algorithm partitions n samples into k blocks. With αn corrupted samples, if k > 2αn, the corrupted samples are distributed across fewer than half the blocks. The median of block means effectively ignores corrupted blocks and aggregates the clean majority. This mechanism fails if α ≥ k/2n or if block size m is too small for Law of Large Numbers to stabilize clean block means.

### Mechanism 2: Quantile-Driven Asymptotic Bias
The estimation error is fundamentally driven by the behavior of the distribution's quantile function Q_m around the median, determining the "asymptotic bias" floor. Theorem 3.1 demonstrates that MoM's error is bounded by the quantile function of block averages. For heavy-tailed distributions with finite variance, the quantiles behave such that error scales with √α. For symmetric distributions, the quantiles behave linearly, allowing error to scale optimally with α. This mechanism relies on the class of distribution being known and breaks when applied to asymmetric data.

### Mechanism 3: Sub-optimality in Light-Tailed Regimes
MoM is provably sub-optimal for light-tailed (sub-exponential) distributions because it cannot leverage tail decay to overcome the "median-of-means" structure's inherent noise. While light tails allow for tighter concentration, the MoM estimator acts as a "multiple-δ" estimator where confidence parameter constraints limit the rate. The paper proves a lower bound of α^{2/3} for sub-exponential distributions, whereas the optimal rate is α^{1/2}. This mechanism exploits asymmetry where the median differs from the mean.

## Foundational Learning

- **Concept: Adversarial Contamination vs. Additive Noise**
  - **Why needed here:** The paper distinguishes between simple additive noise and the "adversarial" model where an attacker inspects data, removes "good" samples, and adds "bad" ones. Understanding this distinction is critical to grasping why MoM's breakdown point matters.
  - **Quick check question:** Does the contamination model allow the attacker to see the clean data before deciding which samples to corrupt? (Answer: Yes, per Section 2.1).

- **Concept: Asymptotic Bias**
  - **Why needed here:** The paper analyzes the "plateau" of error that persists regardless of sample size n due to contamination α. Engineers must understand that increasing n does not reduce this specific error component.
  - **Quick check question:** If n → ∞ and α is fixed, does the error of an optimal estimator go to zero? (Answer: No, it plateaus at the asymptotic bias).

- **Concept: Finite vs. Infinite Variance (Heavy Tails)**
  - **Why needed here:** The optimality of MoM varies drastically depending on whether the distribution has finite variance (P_2) or only finite (1+r)-th moments (P_{1+r}).
  - **Quick check question:** Which class of distributions allows MoM to achieve an optimal asymptotic bias of α^{r/(1+r)}? (Answer: Infinite variance distributions with finite (1+r)-th moment).

## Architecture Onboarding

- **Component map:** Input samples -> Block Partitioner -> Block Aggregators (compute means) -> Median Selector -> Output estimate
- **Critical path:** The selection of hyperparameter k (number of blocks). This single integer determines the trade-off between robustness (high k) and accuracy of block means (low k). The paper provides explicit formulas for k based on the target distribution class.
- **Design tradeoffs:**
  - Heavy Tails (Finite/Infinite Variance): Selecting k ≈ γαn ensures optimality
  - Light Tails (Sub-Exponential): Cannot select k to achieve full optimality; architecture itself is the bottleneck
  - Symmetric Data: Can choose large k (e.g., k ≈ βn) to drive bias down to α, provided symmetry assumption holds
- **Failure signatures:**
  - Plateau at α^{2/3}: If implementing MoM on light-tailed data and observing bias convergence to this order
  - Crash on Excess Contamination: If αn ≥ k/2, the median will select a corrupted block mean, causing catastrophic failure
- **First 3 experiments:**
  1. Verify Finite Variance Optimality: Generate samples from Pareto distribution, contaminate α=0.1 of data adversarially, and plot estimation error vs. n to observe √α plateau
  2. Verify Sub-Exponential Sub-optimality: Use Half-Normal distribution, run MoM with varying k, confirm no choice breaks α^{2/3} error barrier
  3. Symmetric vs. Asymmetric Comparison: Compare error rates on Student's t distribution vs. skewed Exponential distribution to demonstrate performance gain from symmetry

## Open Questions the Paper Calls Out

- **Can the number of blocks k be selected adaptively to achieve optimal error rates across different distribution classes without prior knowledge of the class?**
  - Basis in paper: The paper states that choosing k to achieve optimal rates for heavy-tailed distributions "does not lead to improved rates" for specific distributions, necessitating adjustment based on the class.
  - Why unresolved: The paper provides optimal strategies for k conditioned on the specific class (P_2, P_{1+r}, P_{SE}) but does not propose a unified adaptive strategy.
  - What evidence would resolve it: An algorithm that selects k automatically and achieves optimal minimax rates for all classes simultaneously.

- **What is the precise order of the asymptotic bias for the trimmed mean estimator in the class of symmetric distributions?**
  - Basis in paper: Table 1 leaves the result for the trimmed mean on symmetric distributions empty. Appendix C notes that for symmetric distributions, "its [trimmed mean] error has not been characterized."
  - Why unresolved: The paper proves MoM is optimal for symmetric distributions (order α) but provides only empirical evidence suggesting the trimmed mean may be sub-optimal in this class.
  - What evidence would resolve it: Formal theoretical bounds establishing the asymptotic bias of the trimmed mean for symmetric distributions.

- **Can the leading constants in the error bounds for MoM under adversarial contamination be reduced to match those of the i.i.d. scenario?**
  - Basis in paper: Section 3.1 notes that the leading constants derived in the paper are significantly larger than those known for the i.i.d. scenario (e.g., √2 for the Lee-Valiant estimator), as the focus was on optimal rates.
  - Why unresolved: The current proof techniques prioritize order-optimality over minimizing multiplicative constants.
  - What evidence would resolve it: A refined analysis of MoM that establishes bounds with leading constants comparable to i.i.d. settings.

## Limitations

- The optimality results depend critically on precise distributional assumptions (symmetry, tail behavior) that may not hold in practice
- The adversarial model assumes the attacker can observe clean data before corruption, which may overestimate real-world threat models
- Numerical experiments are limited to synthetic data with controlled contamination patterns (replacing top αn samples with minimum values)

## Confidence

- **High confidence**: Theoretical minimax lower bounds for symmetric distributions and the finite/infinite variance cases
- **Medium confidence**: The sub-optimality result for light-tailed distributions (α^(2/3) vs optimal α^(1/2))
- **Medium confidence**: Numerical validation across all distribution classes

## Next Checks

1. **Robustness to asymmetric contamination patterns**: Test MoM performance when the adversary uses alternative contamination strategies beyond replacing top samples with minimum values (e.g., random replacements, clustering attacks)

2. **Real-world data evaluation**: Apply MoM to benchmark datasets with naturally occurring outliers (e.g., financial returns, sensor measurements) rather than synthetic contamination to assess practical utility

3. **Baseline comparison completeness**: Implement and compare against the Trimmed Mean and M-estimator baselines referenced from [24, 25] to verify the claimed performance gaps are consistent across all competing methods