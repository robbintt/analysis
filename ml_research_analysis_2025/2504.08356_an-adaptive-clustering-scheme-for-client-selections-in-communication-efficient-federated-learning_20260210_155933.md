---
ver: rpa2
title: An Adaptive Clustering Scheme for Client Selections in Communication-Efficient
  Federated Learning
arxiv_id: '2504.08356'
source_url: https://arxiv.org/abs/2504.08356
tags:
- learning
- number
- clusters
- training
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adaptive clustering method for client selection
  in federated learning to reduce communication costs. The key idea is to dynamically
  adjust the number of clusters based on loss reduction ratios, selecting representative
  clients from each cluster for training.
---

# An Adaptive Clustering Scheme for Client Selections in Communication-Efficient Federated Learning

## Quick Facts
- arXiv ID: 2504.08356
- Source URL: https://arxiv.org/abs/2504.08356
- Reference count: 10
- One-line primary result: Achieves 95.95-96.2% accuracy on non-IID MNIST with nearly 50% communication cost reduction compared to FedAvg

## Executive Summary
This paper proposes an adaptive clustering method for client selection in federated learning to reduce communication costs. The key idea is to dynamically adjust the number of clusters based on loss reduction ratios, selecting representative clients from each cluster for training. Two stabilization techniques—an SA-like probabilistic approach and an experience-based method—are introduced to maintain cluster stability. Experiments on a non-IID MNIST dataset with 8 clients show that the proposed method achieves 95.95-96.2% accuracy while reducing communication costs by nearly 50% compared to traditional FedAvg, without sacrificing model performance.

## Method Summary
The method combines hierarchical clustering with TCP-like dynamic adjustment and stabilization mechanisms. After initial warmup rounds, the server computes pairwise client model similarities and builds a clustering dendrogram. A control policy monitors average loss reduction ratios between rounds: when reduction exceeds a threshold, the number of clusters decreases (starting at 1, increasing exponentially); otherwise, it doubles. One representative client per cluster is selected for training each round. Two stabilization variants prevent premature or oscillating cluster changes: a simulated annealing-style probabilistic approach that accepts status quo with decreasing probability, and an experience-based method that uses historical performance to weight cluster count decisions.

## Key Results
- Achieves 95.95-96.2% accuracy on non-IID MNIST with 8 clients
- Reduces communication costs by nearly 50% compared to traditional FedAvg
- Maintains model performance while significantly reducing transmission count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically adjusting the number of client clusters based on training progress reduces communication overhead while preserving model accuracy.
- Mechanism: A TCP-like control policy monitors average loss reduction ratios between rounds. When the reduction ratio exceeds a threshold w, the number of clusters p decreases by d (starting at 1, increasing exponentially), reducing participating clients. If reduction is insufficient, p reverts to 2*p. Hierarchical clustering groups clients by model similarity, and one representative per cluster trains each round.
- Core assumption: Clients with similar data distributions produce similar local model updates, so selecting one representative per cluster preserves gradient information.
- Evidence anchors:
  - [abstract] "dynamically adjust the number of clusters to find the most ideal grouping results... reduce the number of users participating in the training"
  - [section II] "We refer to the concept of TCP flow control and propose a dynamic adjustment method to determine the number of clusters"
  - [corpus] Related work on clustered FL (arXiv:2507.07320) supports clustering for communication efficiency, though with different adjustment mechanisms.
- Break condition: If local model similarity does not correlate with data similarity (e.g., adversarial or highly heterogeneous clients), representative selection loses critical gradient information and convergence degrades.

### Mechanism 2
- Claim: Simulated annealing-style probabilistic stabilization prevents premature or oscillating cluster count changes.
- Mechanism: Even when the loss reduction ratio falls below threshold, the system retains the current cluster count with some probability rather than immediately increasing clusters. This probability decreases over time (analogous to temperature cooling in SA), allowing early exploration and later exploitation of stable configurations.
- Core assumption: Temporary loss plateaus are normal in training; overreacting by immediately increasing clusters wastes communication budget.
- Evidence anchors:
  - [abstract] "an SA-like probabilistic approach... to maintain cluster stability"
  - [section II] "We set a probability to let the number of clusters remain unchanged when the loss reduction ratio is below the threshold"
  - [corpus] No direct corpus evidence for SA-style stabilization in FL; this appears novel to this work.
- Break condition: If the annealing schedule is misconfigured (probability drops too fast or too slow), the system either freezes at suboptimal cluster counts or oscillates excessively.

### Mechanism 3
- Claim: Experience-based stabilization uses historical performance to bias cluster count decisions.
- Mechanism: The system tracks whether previous rounds at each cluster count p produced "good" (satisfactory loss reduction) or "bad" experiences. The probability of maintaining the current p is weighted by this history, reinforcing effective configurations.
- Core assumption: Past performance at a given cluster count predicts future effectiveness for similar data distributions.
- Evidence anchors:
  - [abstract] "experience-based method... introduced to maintain cluster stability"
  - [section II] "the probability is determined by the good/bad experiences when previously the clustering setting stayed in this number"
  - [corpus] Weak corpus support; related work mentions adaptive participant selection but not explicit experience-based state tracking.
- Break condition: If client data distribution shifts significantly over time, historical experience becomes misleading and locks the system into outdated cluster counts.

## Foundational Learning

- Concept: **Agglomerative Hierarchical Clustering**
  - Why needed here: The system must iteratively merge similar clients without knowing the optimal cluster count in advance. Agglomerative clustering provides a dendrogram, enabling dynamic threshold adjustment to produce any desired number of clusters.
  - Quick check question: Can you explain why hierarchical clustering is preferred over k-means when the target cluster count is unknown?

- Concept: **Non-IID Data in Federated Learning**
  - Why needed here: Client data heterogeneity drives the need for clustering—similar clients can share representation, but dissimilar clients must be in separate clusters to preserve gradient diversity. The paper's MNIST experiment partitions labels to create extreme non-IID conditions.
  - Quick check question: What happens to FedAvg convergence if all clients have identical data distributions vs. highly skewed distributions?

- Concept: **Simulated Annealing**
  - Why needed here: The SA-like stabilization mechanism borrows the temperature/probability concept from optimization. Understanding SA helps explain why probabilistic acceptance of "bad" moves (not increasing clusters) aids exploration.
  - Quick check question: In simulated annealing, what happens to the acceptance probability of worse solutions as temperature decreases?

## Architecture Onboarding

- Component map: Clients -> Server Aggregator -> Similarity Computation -> Hierarchical Clustering -> Adaptive Control Policy -> Participant Selector -> Clients
- Critical path:
  1. Initial rounds: All clients train and upload (p = n)
  2. After warmup: Server computes pairwise similarities and builds clustering dendrogram
  3. Each round: Control policy evaluates loss reduction → adjusts p → clustering module cuts dendrogram at appropriate threshold → one representative per cluster trains → repeat
- Design tradeoffs:
  - Aggressive cluster reduction (high w threshold, fast d growth): Lower communication, but risks missing optimal cluster count and degrading accuracy
  - Conservative stabilization (high annealing probability): More stable clustering, but slower adaptation to data shifts
  - Representative selection strategy (random vs. loss-based): Paper uses random; alternatives may improve convergence but add complexity
- Failure signatures:
  - Accuracy plateaus significantly below FedAvg baseline: Cluster count may be too low; check if p is stuck at 1-2
  - Cluster count oscillates wildly every few rounds: Stabilization mechanism underconfigured; increase annealing probability or experience weight
  - Communication cost doesn't decrease: Control policy may never trigger cluster reduction; check loss threshold w and reduction ratio computation
- First 3 experiments:
  1. **Baseline replication**: Run FedAvg on non-IID MNIST with 8 clients for 200 rounds; confirm ~96% accuracy and 1600 transmission count
  2. **Ablation on stabilization**: Compare TCP-only vs. SA-like vs. experience-based stabilization; measure accuracy, transmission count, and cluster count trajectories
  3. **Cluster count validation**: In the 8-client, 4-label-pair setup, verify that adaptive clustering converges to p ≈ 4; if it doesn't, diagnose whether similarity metric or control policy is misconfigured

## Open Questions the Paper Calls Out
- None explicitly called out in the paper

## Limitations
- Experimental validation limited to only 8 clients on MNIST, which may not reflect real-world FL scale
- Lack of implementation details for stabilization mechanisms (SA probability formula, experience calculation) hinders precise reproduction
- No evaluation on more complex datasets or tasks with volatile loss landscapes

## Confidence
- **High Confidence**: The core clustering mechanism and communication cost reduction claims are well-supported by the described methodology and align with established FL clustering literature
- **Medium Confidence**: The stabilization mechanisms (SA-like and experience-based) are conceptually sound but lack implementation specifics needed for faithful reproduction
- **Low Confidence**: Generalization to non-MNIST datasets and larger client pools is not validated, and the effectiveness of the stabilization mechanisms in highly dynamic environments remains unproven

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary the loss reduction threshold w and SA probability cooling schedule to determine their impact on convergence speed and final accuracy
2. **Scalability Testing**: Replicate experiments with 20+ clients and a more complex dataset (e.g., CIFAR-10) to assess whether adaptive clustering maintains communication efficiency gains
3. **Stabilization Ablation**: Implement and compare all three stabilization variants (TCP-only, SA-like, experience-based) under identical conditions to quantify their individual contributions to cluster stability