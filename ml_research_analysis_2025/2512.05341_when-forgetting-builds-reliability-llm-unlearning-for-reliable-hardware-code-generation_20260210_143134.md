---
ver: rpa2
title: 'When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code
  Generation'
arxiv_id: '2512.05341'
source_url: https://arxiv.org/abs/2512.05341
tags:
- code
- unlearning
- generation
- hardware
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first domain-specific unlearning framework\
  \ for LLM-based hardware code generation. It combines syntax-preserving unlearning\u2014\
  which protects structural tokens in RTL code\u2014with a fine-grained floor-aware\
  \ selective loss (FiFSL) that enables precise and accelerated forgetting."
---

# When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation

## Quick Facts
- arXiv ID: 2512.05341
- Source URL: https://arxiv.org/abs/2512.05341
- Authors: Yiwen Liang; Qiufeng Li; Shikai Wang; Weidong Cao
- Reference count: 40
- One-line primary result: Domain-specific unlearning framework for LLM-based hardware code generation that preserves syntax while enabling 3× larger forget sets than conventional methods

## Executive Summary
This paper introduces the first domain-specific unlearning framework for LLM-based hardware code generation, addressing the critical challenge of removing memorized problematic content while preserving code generation capability. The framework combines syntax-preserving unlearning with a fine-grained floor-aware selective loss (FiFSL) to achieve effective forgetting without compromising RTL syntactic integrity. Experiments demonstrate superior performance compared to conventional unlearning methods, supporting larger forget sets while maintaining high model utility on hardware code generation tasks.

## Method Summary
The method employs syntax-preserving unlearning that protects structural tokens (reserved keywords like module, assign, wire, always) from loss computation through binary masking, while FiFSL applies margin-based penalty with floor gating to accelerate convergence. The framework fine-tunes on RTLCoder dataset, applies syntax masks to exclude structural tokens from forgetting, and uses a selective loss that only updates unforgotten samples above a threshold. This dual constraint enables effective unlearning while maintaining RTL compilability and generation accuracy.

## Key Results
- Supports up to 3× larger forget sets (30%) than conventional methods without utility collapse
- Achieves high Pass@1 accuracy (53-72%) at 30% forget sets vs. 1-28% for competitors
- Requires only single training epoch for convergence, significantly faster than baseline methods
- Maintains strong BLEU/chrF scores while reducing MinK++ to target ~0.5

## Why This Works (Mechanism)

### Mechanism 1
Protecting structural tokens from unlearning preserves RTL code compilability while semantic content is forgotten. Binary masking excludes reserved keywords and skip-tag spans from loss computation via $m_t = 0$ for protected tokens, so gradients never update their representations. Only task-specific semantic tokens receive forgetting pressure. Core assumption: RTL syntax tokens constitute a "universal syntactic backbone" that is domain-invariant and should not be unlearned.

### Mechanism 2
Gating already-forgotten samples out of gradient computation accelerates convergence and prevents over-forgetting collapse. FiFSL applies margin-based penalty $\phi_i = \frac{2}{\beta}\text{softplus}(-\beta(L_i - \gamma))$ and only samples with $\phi_i > L_{min}$ contribute gradients. As training proceeds, active sample count $N_{act}$ decreases, concentrating optimization on hard-to-forget cases. Core assumption: Samples below the floor threshold $L_{min}$ have been sufficiently forgotten and further updates provide diminishing returns.

### Mechanism 3
Combining syntax-preservation with selective loss enables 3× larger forget sets than conventional methods without utility degradation. Syntax masking ensures RTL compilability is never compromised while FiFSL focuses gradient budget on unforgotten semantic tokens. This dual constraint prevents the instability seen in GA and the reference-dependency issues in NPO/SimNPO. Core assumption: Conventional unlearning methods fail on hardware code because they treat syntax tokens as removable.

## Foundational Learning

- **Gradient Ascent and NPO-based Unlearning**: The paper builds on and compares against GA (which maximizes forget-set loss) and SimNPO (which adds bounded penalty). Understanding why these fail (instability, collateral drift) motivates the FiFSL design. Quick check: Why does maximizing cross-entropy on forget samples cause "collateral drift on the retain distribution"?

- **RTL/HDL Syntax Constraints**: Unlike natural language, Verilog requires strict syntax for synthesizability. Keywords like `always`, `assign`, `module` are not content but structural scaffolding that cannot be corrupted. Quick check: What happens if an unlearning method treats `always @(posedge clk)` as forgettable semantic content?

- **Token-level vs. Sample-level Loss Objectives**: FiFSL operates at token level (syntax masking) and sample level (floor gating). The interaction requires understanding how per-token gradients aggregate to sample losses before selective gating. Quick check: How does normalizing by $|V_i|$ (valid token count) ensure comparability across sequences of different lengths?

## Architecture Onboarding

- **Component map**: Input preprocessing -> keyword detection + skip-tag insertion -> binary mask $m_t$ per token -> forward pass (masked cross-entropy, per-sample normalization, margin-shifted softplus, floor gating) -> backward pass (gradient rescaling by $\alpha_i \cdot (-2\sigma(-\beta(L_i - \gamma)))$ per Eq. 8, zeroing masked tokens and inactive samples)

- **Critical path**: 1. Identify forget set $D_f$ (proprietary IP, contaminated benchmarks, unsafe patterns) 2. Apply syntax-preserving masking (reserved keywords $K$ + skip-tag spans) 3. Compute FiFSL with hyperparameters $\beta=2.5$, $\gamma=0$, $L_{min}=0.35$ 4. Train until MinK++ converges near 0.5 (typically 1 epoch)

- **Design tradeoffs**: Forget set size vs. utility (larger forget sets increase forgetting pressure but risk utility collapse), Margin $\gamma$ vs. stability (higher $\gamma$ increases forgetting pressure but may cause gradient instability), Floor $L_{min}$ vs. convergence speed (lower floor means more samples remain active longer, slowing convergence but potentially improving thoroughness)

- **Failure signatures**: Syntax collapse (generated code fails compilation - check if keyword masking is applied correctly), Utility collapse (Pass@1 drops sharply - $L_{min}$ may be too low or forget set too large for base model), Incomplete forgetting (PrivLeak/MinK++ remain high - increase epochs or adjust $\beta$ sharpness), Over-forgetting (validation loss diverges - reduce learning rate from $2 \times 10^{-6}$)

- **First 3 experiments**: 1. Baseline comparison: Run GA, SimNPO, and proposed method on same forget set (10% of training) with same epoch budget, compare MinK++ vs. Pass@1 trade-off curve to verify Figure 3 reproduction 2. Ablation on syntax masking: Disable keyword/skip-tag masking and measure syntactic validity rate (compilation success) on generated RTL to quantify syntax-preservation contribution 3. Floor threshold sweep: Vary $L_{min}$ from 0.1 to 0.5 on a 20% forget set and plot convergence epoch count vs. final utility to identify optimal floor for target model architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Syntax Preservation Scope: The paper does not provide a complete enumeration of protected patterns beyond reserved keywords, potentially limiting effectiveness on domain-specific macros or preprocessor directives
- Generalization Across Model Architectures: 3× larger forget set capability demonstrated primarily on code-optimized LLMs, weaker performance on general-purpose models suggests limited scalability
- Evaluation Domain Restriction: All validation uses RTLCoder-generated data and VerilogEval benchmarks, effectiveness on other hardware description languages remains untested

## Confidence

**High Confidence** (Evidence directly supports claims):
- Syntax-preserving masking effectively protects structural integrity: Demonstrated through compilation success rates and maintained BLEU/chrF scores
- FiFSL converges in single epoch: Clear convergence patterns in Figure 3 with MinK++ approaching target 0.5
- Superior utility preservation vs. GA/SimNPO: Table II shows 53-72% Pass@1 vs. 1-28% for competitors at 30% forget sets

**Medium Confidence** (Evidence supports but with limitations):
- 3× larger forget set capability: Demonstrated on code-LLMs but not general-purpose models; may not generalize to larger models or different domains
- Floor-aware acceleration: Mechanistic explanation is sound but requires more ablation studies to confirm threshold sensitivity

**Low Confidence** (Claims under-supported by evidence):
- Complete protection against membership inference attacks: MinK++ reduction is shown but not compared against state-of-the-art attack methods
- Universal applicability to all hardware code: Only tested on Verilog subset; method may not extend to SystemC or hardware-software co-design scenarios

## Next Checks

**Check 1: Syntax Preservation Ablation**
Disable all syntax-preserving masking (both keyword and skip-tag protection) and run unlearning on identical forget sets. Measure syntactic validity rate (compilation success), Pass@1 accuracy, and MinK++ reduction. Expected outcome: Significant drop in compilation success rate while maintaining similar MinK++ reduction, quantifying the contribution of syntax preservation to utility preservation.

**Check 2: General-Purpose Model Scaling**
Apply proposed method to Llama-3-8B-Instruct and Mistral-7B-Instruct with varying forget set sizes (10%, 20%, 30%). Track Pass@1 accuracy, MinK++ convergence rate, and wall-clock training time per epoch. Expected outcome: Identify maximum forget set size before utility collapse, compare convergence speed vs. GA/SimNPO, and determine if FiFSL's acceleration benefit persists.

**Check 3: Cross-Domain Generalization**
Apply syntax-preserving unlearning to Python or C++ code generation tasks using similar datasets and forget set compositions. Measure syntax validity (parse success), functional correctness (test pass rate), and forget quality (MinK++ reduction). Expected outcome: Validate whether syntax-preserving principles transfer to other programming languages and whether FiFSL's floor-aware mechanism provides similar acceleration benefits outside RTL domain.