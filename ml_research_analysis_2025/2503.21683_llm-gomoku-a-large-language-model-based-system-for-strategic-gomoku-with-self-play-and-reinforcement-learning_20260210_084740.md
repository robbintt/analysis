---
ver: rpa2
title: 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with
  Self-Play and Reinforcement Learning'
arxiv_id: '2503.21683'
source_url: https://arxiv.org/abs/2503.21683
tags:
- gomoku
- game
- language
- position
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM-Gomoku, a large language model-based system
  for strategic Gomoku gameplay using self-play and reinforcement learning. The system
  enables LLMs to read the board, understand rules, select strategies, and evaluate
  positions through a carefully designed prompt template and parallel position evaluation
  framework.
---

# LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.21683
- Source URL: https://arxiv.org/abs/2503.21683
- Authors: Hui Wang
- Reference count: 9
- One-line primary result: LLM-Gomoku achieves 5× speedup in move generation and improves average survival steps against AlphaZero from 7 to 12 through self-play reinforcement learning

## Executive Summary
This paper presents LLM-Gomoku, a system that enables large language models to play Gomoku strategically through a carefully designed prompt template, parallel position evaluation, and self-play reinforcement learning. The system addresses the challenge of illegal move generation and slow inference by implementing local position evaluation with parallel LLM scoring, achieving a 5× speedup from 150 to 28 seconds per move. Through training on 1,046 self-play games with per-turn win probability rewards, the model's playing strength improved significantly, increasing average survival steps against AlphaZero from 7 to 12.

## Method Summary
LLM-Gomoku uses a structured prompt template that encodes the 15×15 board state, Gomoku rules, selected strategy, and analytical logic to ground LLM reasoning in concrete game states. The system implements parallel position evaluation using Ray to score candidate moves and their neighbors simultaneously, eliminating illegal positions while achieving significant speedup. A Deep Q-Network learns to select strategy-analytical logic combinations based on per-turn win probability rewards from an Evaluator Agent, with state-action-reward tuples stored in a database for training. The architecture simulates human learning processes while addressing the unique challenges of spatial reasoning and move legality in LLM-based game playing.

## Key Results
- Eliminated illegal position generation through local position evaluation with neighbor expansion
- Achieved 5× speedup from 150 to 28 seconds per move through parallel LLM scoring
- Improved average survival steps against AlphaZero from 7 to 12 after training on 1,046 self-play games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured prompt templates enable LLMs to reason about spatial game states through natural language encoding.
- Mechanism: The system encodes the 15×15 board as a text array (player pieces as `{self.player_id}`, opponent as `{-self.player_id}`, empty as `0`), injects Gomoku rules, selected strategy, and analytical logic into a unified prompt. This grounds the LLM's reasoning in concrete game state rather than abstract reasoning.
- Core assumption: LLMs can perform spatial reasoning when board states are serialized to text with explicit positional markers.
- Evidence anchors:
  - [abstract] "enabling LLMs to read the board, understand rules, select strategies, and evaluate positions through a carefully designed prompt template"
  - [section 3.1] "This template aims to replicate the thought process of human players during a game by incorporating key elements such as the current state of the board, the basic rules of Gomoku, chess strategies, and analytical logic"
  - [corpus] Liga and Pasetto (2023) documented "the critical role of prompt engineering in shaping LLMs' game strategy processing and spatial reasoning capabilities"
- Break condition: If board serialization format doesn't preserve spatial relationships, or if prompt exceeds context window for late-game states with many pieces.

### Mechanism 2
- Claim: Local position evaluation with parallel LLM calls eliminates illegal move generation while achieving 5× speedup.
- Mechanism: Instead of relying solely on LLM output, the system evaluates the candidate move position and its first-order neighbors (expanding to second-order if none legal). Each legal position receives an LLM-generated score (0-10 integer), and the highest-scoring legal position is selected. Ray enables parallel scoring across 24 CPU cores.
- Core assumption: LLMs can reliably score position quality even if they occasionally generate illegal coordinates.
- Evidence anchors:
  - [abstract] "parallel position evaluation framework...5x speed increase from 150 to 28 seconds per move"
  - [section 3.3] "This method evaluates each legal position among the candidate move positions and their local neighbors, precisely selecting the highest-scoring legal position as the final move"
  - [corpus] Weak direct corpus evidence for this specific technique; related work focuses on different evaluation approaches.
- Break condition: If parallel API calls hit rate limits, or if LLM scoring is inconsistent across parallel calls (temperature > 0 without fixed seed).

### Mechanism 3
- Claim: Self-play with per-turn reward signals enables strategy selection policy improvement via DQN.
- Mechanism: A Deep Q-Network (3-layer MLP, input 225 board positions, output 529 strategy+logic combinations) learns action values. A dedicated "Evaluator Agent" provides per-turn win probabilities as intermediate rewards, avoiding the slow feedback loop of game-outcome-only rewards. Self-play generates state-action-reward tuples stored in a database for training.
- Core assumption: Per-turn win probability estimates from an LLM evaluator correlate with actual game outcomes and provide useful learning signal.
- Evidence anchors:
  - [abstract] "After training on 1,046 self-play games, the model's Gomoku-playing capabilities were notably enhanced"
  - [section 3.5] "we designed a dedicated Agent for evaluating the game situation. This Agent assesses the two players in the current game, provides the win rate for each player, and uses these win rates as rewards"
  - [corpus] AlphaGo, AlphaZero literature (Silver et al.) validates self-play RL for game learning, though this paper uses per-turn rewards rather than outcome-only.
- Break condition: If evaluator win probability is poorly calibrated, or if strategy selection space (529 combinations) is too large for effective exploration given limited games.

## Foundational Learning

- Concept: **Deep Q-Networks (DQN)**
  - Why needed here: The paper uses DQN to learn which strategy+logic combination to select given a board state. Understanding Q(s,a), replay buffers, and reward shaping is essential.
  - Quick check question: Can you explain why per-turn rewards might accelerate learning compared to sparse game-outcome rewards?

- Concept: **Prompt Engineering for Structured Reasoning**
  - Why needed here: The system's performance hinges on encoding spatial game state into text that preserves positional relationships.
  - Quick check question: How would you serialize a 15×15 board to ensure the LLM can identify "three in a row" patterns?

- Concept: **Parallel Computing with Ray**
  - Why needed here: The 5× speedup comes from distributing LLM API calls across workers. Understanding Ray's actor model helps debug scaling issues.
  - Quick check question: What happens to your parallel evaluation if one LLM call hangs or times out?

## Architecture Onboarding

- Component map:
  - Prompt Constructor -> LLM -> Local Position Evaluator -> Parallel Framework (Ray) -> Strategy Database -> DQN Module -> Self-Play Engine -> Evaluator Agent -> State-Action-Reward Database

- Critical path:
  1. Board state → Prompt Constructor → LLM receives state/rules
  2. DQN selects strategy+logic combination given current state
  3. Local Position Evaluator generates candidate positions
  4. Parallel Framework scores all candidates simultaneously
  5. Highest-scoring legal position selected as move
  6. Evaluator Agent provides win probability → reward signal
  7. State-action-reward tuple stored → DQN updated

- Design tradeoffs:
  - Single strategy per turn vs. multi-strategy ensemble (paper acknowledges this limits reasoning depth)
  - Local neighbor evaluation vs. full-board evaluation (faster but may miss distant threats)
  - Per-turn rewards vs. outcome-only rewards (faster learning but depends on evaluator quality)
  - API-based LLM vs. local model (easier setup but rate limits and latency)

- Failure signatures:
  - Infinite loop on illegal positions: Local position evaluator not expanding to higher-order neighbors
  - API timeout crashes: Missing exception handling in parallel calls
  - Training data loss: Database not persisting state-action-reward tuples
  - Poor strategy selection: DQN exploration insufficient for 529-action space
  - Inconsistent scoring: Parallel LLM calls with non-zero temperature

- First 3 experiments:
  1. **Baseline validation**: Run Zero-shot, Few-shot, and Chain-of-Thought prompts on 20 Gomoku positions; verify the paper's finding that these methods produce illegal moves. This validates your prompt engineering is the bottleneck.
  2. **Local position evaluator unit test**: Create a board with 5 pieces, run the evaluator, confirm it only returns legal positions and correctly identifies the highest-scoring one. Use temperature=0 for reproducibility.
  3. **Self-play smoke test with reduced scale**: Run 10 self-play games with a random strategy selector (no DQN), verify the state-action-reward database populates correctly and can resume after a simulated crash. This de-risks the training pipeline before committing to 1,000+ games.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can combining multiple strategy-analytical logic pairs during each move decision significantly improve playing strength compared to selecting a single pair?
- Basis in paper: [explicit] Discussion section states the study "only selects one strategy and one analytical logic for thinking each time, which to some extent limits the comprehensiveness and depth of the model's game analysis."
- Why unresolved: The current architecture's single-pair selection was a simplification to manage reasoning complexity; the trade-off between computational cost and decision quality from multi-pair combinations remains untested.
- What evidence would resolve it: A controlled experiment comparing win rates and average survival steps against AlphaZero between single-pair and multi-pair configurations, with statistical significance testing across sufficient games.

### Open Question 2
- Question: Can using AlphaZero's move evaluations as expert guidance signals substantially accelerate LLM capability improvement during training?
- Basis in paper: [explicit] Discussion section proposes to "use the results of AlphaZero to guide the language model to think in the direction of the most correct move, thereby accelerating the speed of model capability improvement."
- Why unresolved: While AlphaZero provides strong positional evaluations, integrating these into LLM reasoning pipelines without undermining the model's language-based strategic understanding presents architectural challenges not yet addressed.
- What evidence would resolve it: Comparing learning curves (e.g., Elo or survival steps over training episodes) between standard self-play and AlphaZero-guided variants, measuring sample efficiency gains.

### Open Question 3
- Question: How does playing strength scale with the number of self-play training games beyond 1,046, and is there a performance plateau?
- Basis in paper: [inferred] The study trained on exactly 1,046 games with notable improvement, but no ablation study examined scaling behavior or convergence limits of the DQN-based approach.
- Why unresolved: Without scaling experiments, it is unclear whether the observed improvements would continue linearly, saturate, or whether 1,046 games are sufficient for near-optimal policy learning in this architecture.
- What evidence would resolve it: A systematic training scaling experiment (e.g., 250, 500, 1000, 2000, 4000 games) with consistent evaluation against AlphaZero to identify plateau points.

## Limitations
- The specific content of the 52 chess strategies and 9 analytical logics is not detailed, making it difficult to verify the full scope of the strategy selection space
- The exact prompt used by the Evaluator Agent to generate per-turn win probability estimates is not provided, raising questions about the reliability of reward signals
- DQN hyperparameters (hidden layer sizes, learning rate, batch size, replay buffer size) are unspecified, which could significantly impact learning outcomes

## Confidence
- **High Confidence**: The parallel position evaluation framework and its 5× speedup claim, as the mechanism is clearly described and experimentally validated
- **Medium Confidence**: The effectiveness of combining LLMs with reinforcement learning for Gomoku, based on the reported improvement from 7 to 12 average survival steps against AlphaZero
- **Low Confidence**: The generalizability of the approach to other board games, as the paper focuses exclusively on Gomoku-specific prompt engineering and strategy sets

## Next Checks
1. **Evaluator Agent Reliability Test**: Run the Evaluator Agent on 50 game positions with known outcomes and measure correlation between predicted win probabilities and actual results. This validates the reward signal quality for DQN training.
2. **Strategy Space Exploration Analysis**: Analyze the DQN's learned strategy selection patterns across 100 self-play games to determine if the model is exploring the full 529-action space or getting stuck in local optima.
3. **Cross-Game Generalization Experiment**: Adapt the prompt template and strategy set for Connect Four or Tic-Tac-Toe, then measure performance changes. This tests whether the approach generalizes beyond Gomoku-specific encodings.