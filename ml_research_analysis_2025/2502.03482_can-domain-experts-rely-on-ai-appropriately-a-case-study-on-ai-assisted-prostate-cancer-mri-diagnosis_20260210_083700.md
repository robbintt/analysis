---
ver: rpa2
title: Can Domain Experts Rely on AI Appropriately? A Case Study on AI-Assisted Prostate
  Cancer MRI Diagnosis
arxiv_id: '2502.03482'
source_url: https://arxiv.org/abs/2502.03482
tags:
- human
- performance
- cases
- radiologists
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how AI assistance affects decision-making
  accuracy among board-certified radiologists diagnosing prostate cancer from MRI
  scans. The research compares three conditions: human-only diagnosis, human-AI collaboration
  after independent diagnosis, and human-AI collaboration with upfront AI predictions
  and performance feedback.'
---

# Can Domain Experts Rely on AI Appropriately? A Case Study on AI-Assisted Prostate Cancer MRI Diagnosis

## Quick Facts
- arXiv ID: 2502.03482
- Source URL: https://arxiv.org/abs/2502.03482
- Reference count: 40
- Primary result: Human-AI teams outperform humans alone but underperform AI alone due to under-reliance, though ensemble decisions can outperform AI

## Executive Summary
This study investigates how AI assistance affects decision-making accuracy among board-certified radiologists diagnosing prostate cancer from MRI scans. The research compares three conditions: human-only diagnosis, human-AI collaboration after independent diagnosis, and human-AI collaboration with upfront AI predictions and performance feedback. Across 175 patient cases, the findings reveal that while human-AI teams consistently outperform humans alone, they still underperform AI alone due to under-reliance. Providing performance feedback and upfront AI assistance slightly increases AI adoption but does not significantly improve overall diagnostic accuracy. However, the ensemble of human-AI team decisions can outperform AI alone, suggesting promising directions for complementary human-AI collaboration.

## Method Summary
The study involved 20 board-certified radiologists (10 per study) diagnosing 175 prostate cancer MRI cases. Two studies were conducted: Study 1 compared human-only, human-then-AI, and AI-only conditions with independent initial diagnosis; Study 2 introduced upfront AI presentation with performance feedback from Study 1. The AI model used was a 3D nnU-Net trained on 1,211 cases with T2W/ADC/DWI MRI sequences. Diagnoses were evaluated using AUROC, accuracy, sensitivity, and specificity metrics.

## Key Results
- Human-AI teams (75.5% agreement accuracy) consistently outperformed humans alone (63.0% accuracy) but underperformed AI alone (69.1% accuracy) due to under-reliance
- Radiologists changed diagnosis in only 20.4% of cases when AI disagreed with their initial assessment
- Ensemble aggregation of human-AI decisions achieved 72.5% accuracy, outperforming AI alone
- Performance feedback did not significantly improve human-AI team accuracy despite engagement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Under-reliance on AI is the primary barrier preventing human-AI teams from matching or exceeding AI-only performance.
- **Mechanism:** When radiologists form independent diagnoses first (Study 1), they change their decision after seeing contradicting AI predictions in only ~20% of disagreement cases, even when their own accuracy on those cases is low (~44%). Anchoring to initial judgment inhibits appropriate AI adoption.
- **Core assumption:** The under-reliance pattern is partially caused by the sequence of forming an independent opinion first.
- **Evidence anchors:**
  - [abstract] "while human-AI teams consistently outperform humans alone, they still underperform the AI due to under-reliance"
  - [section 4.2] "radiologists change their diagnosis in only 4.6 (20.4%) of cases" when AI disagrees, "suggesting a significant barrier to incorporating AI assistance"
  - [corpus] "Self-Confidence Shaping" paper addresses calibrating human self-confidence to improve appropriate AI reliance
- **Break condition:** If AI confidence scores were displayed alongside predictions, or if AI were shown before independent diagnosis consistently, under-reliance may decrease.

### Mechanism 2
- **Claim:** Upfront AI presentation (before independent diagnosis) increases AI adoption compared to post-diagnosis AI revelation.
- **Mechanism:** In Study 2, AI predictions were shown directly without requiring an initial independent diagnosis, combined with performance feedback from Study 1. This increased human-AI agreement from 75.5% to 78.4% and "follow AI" accuracy reached 87.3% vs 35.3% when overruling.
- **Core assumption:** Removing the anchoring effect of an independent first judgment allows more objective consideration of AI input.
- **Evidence anchors:**
  - [abstract] "showing AI decisions in advance nudges people to follow AI more"
  - [section 4.2] "performance feedback and upfront AI assistance leads to higher rate of human-AI agreement (78.4% 'follow AI' vs. 75.5% final human-AI agreement from study 1)"
  - [corpus] Limited direct corpus evidence on timing effects; related work focuses on confidence calibration rather than presentation sequence
- **Break condition:** If upfront AI reduces human critical evaluation entirely (automation bias), it may improve adoption but harm complementary performance opportunities.

### Mechanism 3
- **Claim:** Performance feedback alone does not significantly improve human-AI team accuracy, even when experts are shown explicit metrics of AI superiority.
- **Mechanism:** Radiologists received detailed individual performance metrics (accuracy, sensitivity, specificity) showing AI and human-AI performance from Study 1. Despite attention checks confirming engagement, no statistically significant improvement in AUROC or accuracy was observed in Study 2's common-50 subset compared to Study 1.
- **Core assumption:** Explicit performance information should theoretically enable better trust calibration.
- **Evidence anchors:**
  - [abstract] "Providing clinicians with performance feedback did not significantly improve the performance of human-AI teams"
  - [section 4.2] "none of the metrics showed statistical significance... performance feedback did not lead to significant improvements in the Human+AI accuracy"
  - [corpus] Limited corpus evidence on feedback intervention efficacy; this represents a gap in related literature
- **Break condition:** If feedback were case-specific (explaining when AI is likely correct vs. incorrect) rather than aggregate, it may be more actionable.

## Foundational Learning

- **Concept: Under-reliance vs. Over-reliance in AI-assisted decision-making**
  - Why needed here: The paper identifies under-reliance as the core failure mode preventing complementary performance. Distinguishing these is essential for designing interventions.
  - Quick check question: If a human agrees with AI 95% of the time but AI accuracy is 70%, is this under-reliance, over-reliance, or neither?

- **Concept: Complementary Performance**
  - Why needed here: The stated goal is human+AI > human AND human+AI > AI. The paper finds this is achievable via ensemble but not individual decisions.
  - Quick check question: If human accuracy is 63%, AI accuracy is 69%, and human+AI accuracy is 66%, has complementary performance been achieved?

- **Concept: AUROC (Area Under Receiver Operating Characteristic Curve)**
  - Why needed here: Primary evaluation metric. Paper reports AI AUROC of 0.730 (Study 1) and 0.790 (Study 2) vs. human 0.674.
  - Quick check question: If a model has AUROC of 0.75, what is the probability it ranks a randomly chosen positive case higher than a randomly chosen negative case?

## Architecture Onboarding

- **Component map:**
  - nnU-Net AI model (trained on 1,211 cases, T2W/ADC/DWI MRI sequences) -> Web-based diagnostic interface (View Panel, Control Panel, Annotation Panel) -> Performance feedback module (accuracy, sensitivity, specificity metrics) -> Ensemble aggregation layer (majority vote with confidence-weighted tie-breaking)

- **Critical path:**
  1. Case loading -> 2. Human diagnosis (with or without prior AI input) -> 3. AI prediction display -> 4. Final decision -> 5. Metric calculation -> 6. Feedback synthesis

- **Design tradeoffs:**
  - Study 1 workflow (independent-first) preserves human judgment integrity but induces anchoring bias
  - Study 2 workflow (AI-first) increases adoption but risks automation bias
  - Aggregate feedback is scalable but not case-contextualized

- **Failure signatures:**
  - Low decision-change rate (~20%) when AI contradicts human diagnosis
  - Human-AI agreement accuracy (87.3%) far exceeding disagreement accuracy (35.3%)
  - Ensemble outperforming individual human-AI suggests untapped complementary information

- **First 3 experiments:**
  1. A/B test AI confidence display: Measure if showing calibrated confidence scores (e.g., "AI: 92% confident") changes adoption rates and final accuracy.
  2. Case-specific feedback intervention: Instead of aggregate metrics, provide post-hoc explanations of why AI was correct on specific cases where human disagreed—test if this improves calibration in subsequent sessions.
  3. Hybrid workflow test: Require humans to view AI prediction first but mandate a justification when they overrule—measure if this balances adoption with critical evaluation.

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 20 radiologists (10 per study) limits generalizability across the broader radiology community
- Controlled experimental setting may not fully capture real-world clinical pressures, time constraints, and decision-making contexts
- AI model trained on single institution's data raises questions about external validity across different imaging protocols and patient populations

## Confidence
**High Confidence:** The finding that human-AI teams underperform AI alone due to under-reliance is well-supported by multiple metrics across both studies. The ensemble approach outperforming individual human-AI decisions is also robust.

**Medium Confidence:** The claim that upfront AI presentation increases adoption rates is supported but based on a small sample and single intervention. The lack of significant improvement from performance feedback is also moderately supported but requires replication.

**Low Confidence:** The generalizability of these findings to other medical specialties, AI models, and clinical workflows remains uncertain given the specific context and sample characteristics.

## Next Checks
1. **External Validation Across Institutions:** Replicate the study with radiologists from multiple institutions using diverse patient populations and imaging protocols to test generalizability of the under-reliance phenomenon.

2. **AI Confidence Visualization Intervention:** Test whether displaying calibrated AI confidence scores alongside predictions increases appropriate adoption rates and improves complementary performance compared to prediction-only displays.

3. **Real-World Clinical Integration Study:** Implement the human-AI collaboration framework in actual clinical workflow with time pressure and compare performance metrics to the controlled experimental setting to assess ecological validity.