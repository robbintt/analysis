---
ver: rpa2
title: Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving
arxiv_id: '2503.21505'
source_url: https://arxiv.org/abs/2503.21505
tags:
- traffic
- vehicle
- lane
- driving
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VLADBench introduces a fine-grained benchmark for evaluating large\
  \ vision-language models in autonomous driving, addressing limitations in existing\
  \ coarse-grained datasets. It spans 5 key domains\u2014Traffic Knowledge Understanding,\
  \ General Element Recognition, Traffic Graph Generation, Target Attribute Comprehension,\
  \ and Ego Decision-Making and Planning\u2014with 29 detailed tasks."
---

# Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving

## Quick Facts
- **arXiv ID:** 2503.21505
- **Source URL:** https://arxiv.org/abs/2503.21505
- **Reference count:** 40
- **Key outcome:** VLADBench introduces a fine-grained benchmark for evaluating large vision-language models in autonomous driving, addressing limitations in existing coarse-grained datasets. It spans 5 key domains—Traffic Knowledge Understanding, General Element Recognition, Traffic Graph Generation, Target Attribute Comprehension, and Ego Decision-Making and Planning—with 29 detailed tasks. The dataset comprises 2,000 static and 3,000 dynamic scenes with 12,000 close-form questions. Evaluations of 20 VLM models, including open-source, closed-source, and domain-specific models, reveal that even top-performing models like Qwen2.5-VL-72B achieve less than 60% accuracy, highlighting significant performance gaps. The study also demonstrates that domain-specific data enhances specialized capabilities but may reduce generalization, and that synergy among domains improves overall performance.

## Executive Summary
This paper introduces VLADBench, a fine-grained benchmark designed to evaluate Large Vision-Language Models (VLMs) in autonomous driving scenarios. Existing benchmarks suffer from coarse-grained tasks and open-form question answering, which can mask critical reasoning failures. VLADBench addresses these issues by decomposing driving into 5 hierarchical domains and 29 granular tasks, using close-form (multiple-choice) questions with semantically similar distractors to enforce precise reasoning. The evaluation of 20 diverse VLM models reveals significant performance gaps, with top models achieving less than 60% accuracy. The study also explores the impact of domain-specific training, finding that while it boosts performance in targeted tasks, it can reduce generalization across domains. The benchmark provides a robust foundation for advancing VLM capabilities in autonomous driving.

## Method Summary
VLADBench is a fine-grained benchmark for evaluating VLMs in autonomous driving, spanning 5 domains (Traffic Knowledge Understanding, General Element Recognition, Traffic Graph Generation, Target Attribute Comprehension, Ego Decision-Making and Planning) with 29 tasks. The dataset includes 2,000 static and 3,000 dynamic scenes with 12,000 close-form questions. A domain-specific VLM (InternVL2-4B) is fine-tuned on 1.4M AD-specific QAs and 1.3M general QAs using full-parameter training (2 epochs, batch size 1, learning rate 1e-5). The evaluation uses accuracy and instruction compliance metrics, with results showing significant performance gaps across models and domains.

## Key Results
- Even top-performing models like Qwen2.5-VL-72B achieve less than 60% accuracy on VLADBench tasks.
- Domain-specific training enhances performance in targeted domains but reduces generalization across others.
- Larger vision encoders are more impactful than scaling the LLM for autonomous driving tasks.
- Models struggle with complex reasoning tasks like Traffic Graph Generation and Target Attribute Comprehension.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Cognitive Decomposition
- **Claim:** Structuring evaluation into granular domains isolates specific cognitive failures that coarse benchmarks miss.
- **Mechanism:** By decomposing driving into 5 domains and 29 distinct tasks, the benchmark prevents high scores in general recognition from masking incompetence in complex reasoning.
- **Core assumption:** Driving competence requires distinct cognitive capabilities (knowledge, perception, relation, prediction, planning) that can be evaluated independently.
- **Evidence anchors:**
  - [abstract] "The elaborate VLADBench spans 5 key domains... broken down into 11 secondary aspects and 29 tertiary tasks."
  - [Section 4.2.1] Shows significant performance gaps (e.g., 80% on Right of Way vs. <25% on Traffic Signals for small models).
  - [corpus] Related work (e.g., RoboDriveVLM) supports the need for robust evaluation across specific risks rather than general driving ability.
- **Break condition:** If tasks are too dependent (e.g., failing perception guarantees failing planning), the isolation property of the benchmark degrades.

### Mechanism 2: Close-Form Objective Grounding
- **Claim:** Using close-form (multiple-choice) questions with semantically similar distractors provides a stricter, more objective signal of reasoning capability than open-form VQA.
- **Mechanism:** The benchmark constructs a database of answers for each task and selects incorrect choices that are structurally or semantically similar to the truth. This forces the model to possess precise knowledge rather than generating plausible but vague hallucinations.
- **Core assumption:** Language metrics (BLEU, ROUGE) poorly correlate with human judgment in safety-critical contexts.
- **Evidence anchors:**
  - [Section 3.2] "We try to achieve a precise evaluation in terms of the no tolerance for evaluation error."
  - [Section 1] Critiques existing benchmarks for "coarse-grained tasks" and "open-form visual question answering."
- **Break condition:** If the "distractors" are too easy to distinguish, or if the model successfully "games" the multiple-choice format without true visual understanding.

### Mechanism 3: Vision-Encoder Bottlenecking
- **Claim:** Performance in autonomous driving tasks is more constrained by the visual acuity and resolution of the vision encoder than by the parameter count of the LLM.
- **Mechanism:** Driving requires detecting small, dense, and distant visual signals (signs, lights). A larger/dynamic-resolution vision encoder preserves this information before it enters the language reasoning stage, whereas scaling the LLM amplifies reasoning on already-lossy visual tokens.
- **Core assumption:** The visual token quality is the primary limiting factor for downstream reasoning in current VLM architectures.
- **Evidence anchors:**
  - [Section 4.3] "A larger vision encoder is more impactful than scaling up the language model for AD context."
  - [Section 4.2.1] Notes that InternVL2-76B (large vision encoder) excels in aspects where LV-72B (small vision encoder, large LLM) fails.
  - [corpus] ReasonDrive supports the need for efficient visual processing in reasoning tasks.
- **Break condition:** If the reasoning tasks become significantly more complex (e.g., multi-step logic puzzles unrelated to direct visual cues), the LLM scale might become the bottleneck again.

## Foundational Learning

- **Concept: Traffic Graph Generation (TGG)**
  - **Why needed here:** This is a key differentiator of VLADBench. It tests whether the VLM understands the *relationships* between elements (e.g., "which light applies to which lane"), moving beyond simple object detection to scene topology.
  - **Quick check question:** Can a model identify that a specific traffic light controls the ego-vehicle's lane versus an adjacent turn lane?

- **Concept: Generalization vs. Specialization Trade-off**
  - **Why needed here:** The paper highlights a critical risk: training on Domain-Specific (DS) data improves specific tasks but degrades performance on unseen tasks or general knowledge (catastrophic forgetting).
  - **Quick check question:** If a model is fine-tuned exclusively on "Traffic Knowledge," does its ability to predict "Target Intentions" degrade?

- **Concept: Spatio-Temporal Reasoning**
  - **Why needed here:** Essential for the TAC (Target Attribute Comprehension) and EDP (Ego Decision Planning) domains. The model must infer occluded information or predict future states based on a sequence of frames, not just a static image.
  - **Quick check question:** Can the model predict if a pedestrian intends to cross the street based on their trajectory over the last 3 seconds?

## Architecture Onboarding

- **Component map:**
  - Multi-source data (12 datasets) -> Manual selection of 2K static/3K dynamic scenes -> VLM (Vision Encoder + Projector + LLM) -> Accuracy metrics on 29 tasks.
- **Critical path:**
  1. Visual Acuity: Vision encoder processes high-res frame.
  2. Instruction Following: Model parses the close-form prompt and strict output requirements.
  3. Reasoning: Model applies domain logic (e.g., rules of the road) to visual features.
  4. Selection: Model selects the precise answer from the distractor list.
- **Design tradeoffs:**
  - **Close-form vs. Open-form:** Chosen strict accuracy (Close-form) to avoid metric ambiguity, sacrificing the ability to evaluate open-ended nuance or explanation style.
  - **Domain-Specific (DS) Training:** High performance in one domain (e.g., TKU) correlates with performance drops in others (e.g., TAC) if data ratios aren't balanced (Section 4.2.2).
- **Failure signatures:**
  - **Hallucinated Relations:** In TGG tasks, models often invent relationships between signs and lanes that don't exist.
  - **Temporal Amnesia:** Failure in "Intention Judgment" tasks due to inability to process frame sequences effectively.
  - **Instruction Non-Compliance:** Generating explanations instead of selecting from the choice list (handled by rule-based filter in Section 3.2).
- **First 3 experiments:**
  1. **Baseline Evaluation:** Run InternVL2-4B (or equivalent small VLM) on the full 12K QA set to establish a holistic score and identify the weakest of the 5 domains.
  2. **Domain Ablation:** Train the VLM on only one domain's data (e.g., TAC) and evaluate across *all* 5 domains to measure the generalization penalty (negative gain).
  3. **Encoder Scaling:** Swap the vision encoder for a larger model (keeping the LLM frozen) and measure the delta in "Traffic Graph Generation" vs. "Meta Decision-Making" to validate the visual bottleneck hypothesis.

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the integration of multi-view inputs improve the 3D spatial perception and evaluation of VLMs compared to the single perspective view used in VLADBench?
  - **Basis in paper:** [Explicit] Page 8 states the limitation: "The current benchmark focuses on evaluating the understanding... from the perspective view. Future research will incorporate multi-view inputs to further assess the 3D spatial perception capabilities of these models."
  - **Why unresolved:** The current evaluation relies on single-view images or videos, which limits the ability to assess the model's understanding of depth and 3D spatial relationships critical for autonomous driving.
  - **What evidence would resolve it:** An extension of VLADBench including surround-view data and a comparative study showing performance deltas on 3D spatial reasoning tasks versus the current 2D baseline.

- **Open Question 2:** How does the scalability of domain-specific VLMs (scaling beyond the 4B parameter baseline) impact their performance and generalization across the five key domains?
  - **Basis in paper:** [Explicit] Page 8 lists "Exploring the scalability of domain-specific models" as a crucial direction for future research because the study "start[s] from a small-scale VLM... and train[s] the DS models."
  - **Why unresolved:** The paper establishes a baseline using a 4B model, leaving the potential gains from training larger specialized backbones (e.g., 7B or 70B) unverified.
  - **What evidence would resolve it:** Training larger parameter models (e.g., InternVL2-8B or 78B) on the same 1.4M domain-specific QA pairs and comparing the accuracy and generalization against the 4B baseline.

- **Open Question 3:** What specific data sampling strategies or ratio adjustments between domains (TKU, GER, TGG, TAC, EDP) are required to prevent the "negative gains" observed in cross-domain training?
  - **Basis in paper:** [Explicit] Page 8 notes that "Adjusting the data ratio between different domains may lead to better training results" and "Optimizing data sampling strategies are the crucial directions" to mitigate the bias issues shown in Figure 4.
  - **Why unresolved:** The paper demonstrates that training on one domain (e.g., GER) can negatively impact performance in another (e.g., TGG), but it does not identify the specific sampling ratios needed to mitigate this interference.
  - **What evidence would resolve it:** An ablation study varying the mixing ratios of the 1.4M QA training dataset, identifying a curriculum or sampling weight distribution that maximizes total utility without sacrificing cross-domain generalization.

## Limitations
- The dataset construction methodology, particularly the use of GPT-4o for generating 109k structured QAs, introduces potential bias toward the linguistic patterns and reasoning style of that model.
- The close-form evaluation approach, while addressing hallucination issues, may not capture the full spectrum of VLM capabilities needed for autonomous driving.
- The fixed "Tips" for perceptual priors in Traffic Graph Generation tasks could lead to performance differences based on instruction following rather than actual visual understanding.
- The paper does not specify the random seed used for model initialization or data shuffling, which could affect reproducibility.

## Confidence
- **High Confidence:** The hierarchical domain decomposition mechanism is well-supported by experimental evidence showing significant performance gaps between domains. The claim that larger vision encoders outperform larger LLMs for AD tasks is strongly supported by the InternVL2-76B vs. Qwen2.5-VL-72B comparisons.
- **Medium Confidence:** The close-form objective grounding approach shows promise but may overestimate real-world capability since the distractor selection process could be gamed. The generalization vs. specialization tradeoff is observed but the underlying mechanism (catastrophic forgetting vs. representational shift) requires further investigation.
- **Low Confidence:** The vision-encoder bottleneck hypothesis, while supported by current results, may not hold as reasoning tasks become more complex or when evaluating against non-AD-specific benchmarks. The specific claim that 60% accuracy represents a "significant performance gap" lacks comparison to human-level performance on the same tasks.

## Next Checks
1. **Human Performance Baseline:** Conduct the same VLADBench evaluation with human annotators to establish a reference point for what constitutes "good" performance, particularly for the Traffic Graph Generation and Target Attribute Comprehension domains.
2. **Distractor Sensitivity Analysis:** Systematically vary the semantic similarity of incorrect choices in the close-form questions to determine if models are exploiting pattern recognition in distractor construction rather than genuine visual understanding.
3. **Cross-Domain Transfer Study:** Fine-tune a model on domain-specific data for one domain (e.g., Traffic Knowledge Understanding) and measure performance changes across all five domains, with particular attention to the nature of degradation (is it catastrophic forgetting or gradual performance decay?).