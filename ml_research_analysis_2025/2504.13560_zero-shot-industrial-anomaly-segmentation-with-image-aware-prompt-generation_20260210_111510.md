---
ver: rpa2
title: Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation
arxiv_id: '2504.13560'
source_url: https://arxiv.org/abs/2504.13560
tags:
- anomaly
- segmentation
- prompts
- image
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation

## Quick Facts
- arXiv ID: 2504.13560
- Source URL: https://arxiv.org/abs/2504.13560
- Authors: SoYoung Park; Hyewon Lee; Mingyu Choi; Seunghoon Han; Jong-Ryul Lee; Sungsu Lim; Tae-Ho Kim
- Reference count: 30
- Key outcome: Image-Aware Prompt (IAP) generation improves zero-shot anomaly segmentation accuracy by 7.6% F1-max and 3.9% AP over prior methods

## Executive Summary
This paper addresses the challenge of zero-shot industrial anomaly segmentation, where the goal is to detect and segment defects in images without any training data on the target anomalies. The proposed method, Image-Aware Prompt Generation (IAP), uses an image tagging model to identify object attributes from reference images, then employs an LLM to generate context-aware anomaly prompts. These prompts are fed to an open-vocabulary detector to localize potential anomalies, which are then segmented using SAM. The approach significantly outperforms existing zero-shot methods across seven industrial datasets.

## Method Summary
The method is a training-free two-stage pipeline. First, preprocessing uses an image tagging model (RAM) to extract object tags from normal images, then generates context-aware prompts via an LLM (LLaMA-3-8B). A size threshold is computed for filtering. Second, anomaly segmentation uses Grounding DINO to detect anomaly regions based on the prompts, filters detections by size, and generates masks with SAM. The system computes weighted anomaly scores from the masks. It's evaluated on seven industrial datasets (MVTec AD, MPDD, BTAD, KSDD1, MTD, DTD-Synthetic, DAGM2007) using Average Precision (AP) and maximum F1 score (F1-max).

## Key Results
- IAP-AS achieves 7.6% higher F1-max and 3.9% higher AP than prior zero-shot methods
- Ablation study shows prompt generation contributes 8.2% F1-max improvement
- Size filtering improves precision by removing false object detections
- Strong performance across both object and texture datasets

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Semantic Expansion
The pipeline uses image tagging to identify object types, then queries an LLM to generate specific anomaly nouns relevant to that material. These nouns are passed to the open-vocabulary detector. The core assumption is that the LLM can generate visually grounding terms that exist within the pre-trained feature space of the detection model. Evidence shows IAP-AS extracts object attributes from images to generate context-aware prompts, improving adaptability. Break condition: if image tagging misidentifies texture, LLM generates irrelevant terms causing detection failure.

### Mechanism 2: Object-Level Suppression via Size Filtering
Filtering detections based on relative size improves precision by removing the entire object to isolate defects. The method calculates a size threshold and filters out bounding boxes larger than this limit, retaining only smaller, localized anomaly regions. Core assumption: anomalies are strictly smaller than the object itself. Evidence: ablation study shows adding filtering boosts F1-max from 33.08% to 40.13%. Break condition: fails if defect is structural or spans large area, as it would be filtered out.

### Mechanism 3: Proxy-Based Segmentation Cascade
Decoupling detection and segmentation allows zero-shot generalization by leveraging distinct pre-trained foundations. The system cascades a text-to-box model (Grounding DINO) with a box-to-mask model (SAM). Detection provides "where" based on semantic text prompts, and SAM provides "what" based on visual features within the box. Core assumption: detection's bounding boxes are sufficiently tight for SAM to generate meaningful masks. Evidence: Grounding DINO localizes potential anomalous regions passed to SAM for detailed segmentation masks. Break condition: if Grounding DINO hallucinates a box in empty space, SAM will still attempt to segment it, potentially generating false positive mask from noise.

## Foundational Learning

- **Open-Vocabulary Object Detection (e.g., Grounding DINO)**: Unlike standard detectors trained on fixed classes, this model localizes objects based on arbitrary text input. Quick check: Can the model detect "a scratch on a metal surface" if "scratch" wasn't in original training vocabulary? (Hint: It leverages aligned text-image embeddings).
- **Prompt Engineering for Vision Models**: Quality of segmentation depends entirely on text prompt. The paper automates prompt engineering using LLM to find "magic words" that activate detector's features. Quick check: Why might "flaw" perform worse than "a discolored patch on the fabric"?
- **Zero-Shot Learning Paradigm**: Understands constraints—model must generalize to unseen defects without fine-tuning on target data. Quick check: Does system require labeled anomaly images to learn concept of defect? (No).

## Architecture Onboarding

- **Component map**: Input (Industrial Image + Reference Tags) -> Prompt Gen (LLM) -> Contextual Nouns -> Detector (Grounding DINO) -> Bounding Boxes -> Filter -> Filtered Boxes -> Segmenter (SAM) -> Masks -> Compute weighted anomaly score
- **Critical path**: The Prompt Generation step. If LLM outputs non-visual nouns or irrelevant defects, downstream detector will fail to localize anything. Specific phrasing of LLM prompt is the control lever for performance.
- **Design tradeoffs**: Latency vs. Accuracy: pipeline runs three heavy models sequentially (LLM + DINO + SAM), likely too slow for high-FPS real-time streaming but suitable for batch inspection. Recall vs. Precision: size filtering aggressively trades recall of large defects for precision on small defects.
- **Failure signatures**: Empty Output: caused by text prompts too specific or out-of-distribution for Grounding DINO. Whole Object Detection: caused by incorrect size thresholding or prompts describing object rather than defect.
- **First 3 experiments**: 1) Prompt Ablation: run pipeline with fixed prompts vs. IAP prompts to quantify lift from LLM component. 2) Threshold Sensitivity: vary box size threshold on validation set to find "break point" where valid defects start getting filtered out. 3) Tagging Robustness: feed "wrong" object tags into LLM to verify failure mode is graceful.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains heavily depend on quality of LLM-generated prompts, but paper doesn't fully validate what happens when LLM generates irrelevant or non-visual terms
- Size filtering mechanism may incorrectly discard large but valid defects, particularly in texture datasets where anomalies often span significant areas
- Paper doesn't address computational efficiency—three-model cascade likely creates prohibitive latency for real-time industrial applications

## Confidence
- **High confidence**: Core mechanism of image-aware prompt generation demonstrably improves zero-shot anomaly detection by bridging semantic gaps that fixed prompts cannot address
- **Medium confidence**: Size filtering approach shows clear performance gains in ablation studies, but assumption that anomalies are always smaller than objects is violated in real-world scenarios
- **Medium confidence**: Proxy-based cascade architecture is sound, but paper lacks rigorous analysis of failure modes when components misalign

## Next Checks
1. **Prompt Robustness Test**: Systematically corrupt image tagging output with wrong object types and measure degradation in detection performance to establish pipeline's failure threshold
2. **Size Filter Sensitivity Analysis**: Conduct fine-grained ablation study varying size threshold from 0.5 to 1.5 to identify exactly where valid large defects are being incorrectly filtered out
3. **Cross-Dataset Generalization**: Apply best-performing prompt set from one dataset category (e.g., textures) to a different category (e.g., objects) without retraining to test true zero-shot capability