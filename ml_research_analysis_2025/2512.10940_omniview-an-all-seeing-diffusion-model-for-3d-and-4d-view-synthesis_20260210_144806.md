---
ver: rpa2
title: 'OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis'
arxiv_id: '2512.10940'
source_url: https://arxiv.org/abs/2512.10940
tags:
- camera
- video
- arxiv
- view
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniView, a unified diffusion model for 4D
  view synthesis that handles multiple camera control tasks (static/dynamic novel
  view synthesis, text-to-video, image-to-video) within a single framework. The key
  innovation is a disentangled representation of space, time, and view conditions
  using separate RoPE mechanisms for camera embeddings and video tokens.
---

# OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis

## Quick Facts
- arXiv ID: 2512.10940
- Source URL: https://arxiv.org/abs/2512.10940
- Authors: Xiang Fan; Sharath Girish; Vivek Ramanujan; Chaoyang Wang; Ashkan Mirzaei; Petr Sushko; Aliaksandr Siarohin; Sergey Tulyakov; Ranjay Krishna
- Reference count: 40
- Primary result: Unified diffusion model achieving state-of-the-art performance across multiple 4D view synthesis tasks with improved generalization

## Executive Summary
OmniView introduces a unified diffusion transformer framework for 4D view synthesis that handles multiple camera control tasks within a single architecture. The key innovation is a disentangled representation of space, time, and view conditions using separate RoPE mechanisms for camera embeddings and video tokens. This design enables flexible combinations of inputs and improves generalization across tasks, achieving up to 33% improvement in SSIM for multiview NVS on LLFF, 60% on Neural 3D Video, 20% on RE-10K, and 4x reduction in camera trajectory errors for text-conditioned video generation.

## Method Summary
OmniView builds upon a Wan 2.1 1.1B DiT base with 3D VAE compression, adding a camera encoder that processes Plücker ray maps. The model uses token concatenation to combine context and target video latents, with disentangled RoPE: 2D RoPE (t=0) for camera tokens and 3D RoPE for video tokens. Multi-task training across heterogeneous datasets (RE10K, DL3DV, Stereo4D, SynCamMaster, ReCamMaster) enables learning shared geometric priors. The architecture uses separate QK projections for camera tokens and trains with rectified flow objective.

## Key Results
- Achieves up to 33% improvement in SSIM for multiview novel view synthesis on LLFF
- Improves Neural 3D Video reconstruction by 60% over existing approaches
- Reduces camera trajectory errors by 4x for text-conditioned video generation
- Generalizes to input configurations not seen during training, including more views and longer sequences

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Camera-Temporal Positional Encoding
Separating camera geometry encoding from temporal position encoding via independent RoPE mechanisms improves generalization to novel camera trajectories. Video tokens receive standard 3D RoPE (x, y, t) while camera tokens receive 2D RoPE only (x, y, t=0 constant). This eliminates cross-terms between camera and temporal embeddings that occur with additive fusion, reducing overfitting to training trajectory distributions.

### Mechanism 2: Token Concatenation for Variable-Configuration Input
Concatenating context and target video tokens as a unified DiT input sequence enables arbitrary combinations of views, frames, and timestamps without architectural modification. Context tokens (clean, from input views) concatenate with target tokens (noised, to denoise), and DiT self-attention jointly reasons over all tokens with 3D RoPE providing positional disambiguation.

### Mechanism 3: Multi-Task Training with Heterogeneous 3D/4D Data
Joint training across diverse task configurations on mixed datasets yields shared geometric priors that transfer to unseen configurations. Per-iteration random task sampling from RE10K, DL3DV, Stereo4D, SynCamMaster, and ReCamMaster enables the shared camera encoder to learn unified pose representation across tasks.

## Foundational Learning

- **Diffusion Transformers (DiT):** Base architecture for video generation; patchifies latents into tokens, applies transformer blocks with self-attention. Why needed: Handles variable-length sequences better than U-Net approaches. Quick check: Can you explain how DiT differs from U-Net diffusion models in handling variable-length sequences?

- **Rotary Positional Embeddings (RoPE):** Provides relative positional encoding for tokens; core mechanism that OmniView modifies for disentanglement. Why needed: Enables extrapolation to longer sequences better than absolute positional encodings. Quick check: Why does RoPE enable extrapolation to longer sequences better than absolute positional encodings?

- **Plücker Ray Coordinates:** 6-DOF camera representation (ray direction + origin) used as input to camera encoder; encodes per-pixel viewing geometry. Why needed: Provides complete geometric information for camera pose conditioning. Quick check: What advantages do Plücker rays offer over standard pose matrices for conditioning visual generation?

## Architecture Onboarding

- **Component map:** Video frames -> VAE encode -> patchify -> concatenate with camera tokens -> DiT stack (disentangled RoPE at each block) -> VAE decode -> output frames
- **Critical path:** 3D VAE compresses video to latents; camera encoder processes Plücker ray patches; patchifier flattens latents to tokens; DiT applies self-attention (video+camera), text cross-attention, FFN; output projects back to latent space
- **Design tradeoffs:** Channel concatenation vs addition (+parameters but removes entangled cross-terms); separate QK projections for cameras (+representation capacity vs complexity); multi-task training (broader generalization vs potential task interference); fixed t=0 for camera RoPE (temporal invariance vs potential loss of time-varying camera dynamics)
- **Failure signatures:** Camera drift over time (insufficient disentanglement); blurry novel views (weak multi-view training signal); trajectory deviation in T2V (camera encoder not learning proper pose-to-generation mapping); inconsistent geometry across views (attention not properly aggregating multi-view context)
- **First 3 experiments:** 1) RoPE ablation on single task: Train on monocular video NVS only, compare 3D vs 2D RoPE for camera tokens; 2) Zero-shot multi-view dynamic: Evaluate on N3DV with 3+ input views without any multi-view dynamic training; 3) Trajectory error comparison: T2V on RE10K against AC3D baseline; measure RotErr/TrErr to validate 4x improvement claim

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions but presents several limitations including computational costs, efficiency trade-offs, and potential task interference in multi-task training.

## Limitations
- Computational and memory costs of the unified model versus deploying multiple specialized models are not analyzed
- No systematic evaluation on extreme configurations beyond training distribution (10+ input views, 50+ frame sequences)
- Potential task interference in multi-task training with conflicting geometric priors

## Confidence
- **High Confidence:** Core architectural innovation (disentangled RoPE) is well-specified with clear mathematical formulation and empirical validation
- **Medium Confidence:** Performance improvements on standard benchmarks are substantial but rely on specific baseline comparisons
- **Low Confidence:** Claims about mechanism preventing overfitting are largely theoretical with limited direct evidence

## Next Checks
1. **Cross-Dataset Generalization Test:** Evaluate the pre-trained model on a held-out dataset with camera trajectories significantly different from training distribution to measure disentangled RoPE performance under distribution shift.

2. **Ablation of RoPE Dimensionality:** Systematically vary the dimensionality of camera RoPE (1D, 2D, 3D with different temporal encodings) while keeping concatenation fixed to isolate optimal configuration.

3. **Long-Video Scaling Analysis:** Generate videos with 2× or 4× the number of frames used in training to quantify quadratic complexity limits and measure attention patterns and quality degradation.