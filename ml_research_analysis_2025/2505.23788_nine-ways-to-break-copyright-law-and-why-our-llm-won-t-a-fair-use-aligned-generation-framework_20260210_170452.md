---
ver: rpa2
title: 'Nine Ways to Break Copyright Law and Why Our LLM Won''t: A Fair Use Aligned
  Generation Framework'
arxiv_id: '2505.23788'
source_url: https://arxiv.org/abs/2505.23788
tags:
- copyright
- your
- work
- fair
- could
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FUA-LLM addresses the risk of copyright infringement in large language
  model outputs by developing a legally-grounded framework that aligns generated content
  with U.S. fair use doctrine.
---

# Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework

## Quick Facts
- **arXiv ID:** 2505.23788
- **Source URL:** https://arxiv.org/abs/2505.23788
- **Reference count:** 40
- **One-line primary result:** FUA-LLM reduces copyright-infringing LLM outputs by up to 20% while preserving utility through fair use-aligned fine-tuning

## Executive Summary
FUA-LLM introduces a legally-grounded framework to align LLM outputs with U.S. fair use doctrine, addressing the growing risk of copyright infringement in generated content. The approach combines Supervised Fine-Tuning (SFT) on 252 U.S. court proceedings with Direct Preference Optimization (DPO) on FairUseDB, an expert-curated dataset of 18,000 preference pairs covering nine infringement scenarios. Extensive experiments demonstrate that FUA-LLM significantly reduces problematic outputs while maintaining practical utility, outperforming state-of-the-art baselines. The framework also introduces novel evaluation metrics to accurately assess both legal compliance and response utility.

## Method Summary
FUA-LLM employs a two-step fine-tuning pipeline: first, SFT on 252 U.S. federal court proceedings (Dfu) to provide legal grounding, then DPO on FairUseDB (Dpref) to align generation with fair use principles. The framework uses expert-curated preference pairs to teach the model to prefer legally compliant alternatives over refusals or infringing content. At inference, an agentic deployment strategy routes potentially sensitive prompts through topic analysis and web search agents before applying the legally-aligned model, balancing compliance with utility.

## Key Results
- FUA-LLM reduces copyright-infringing outputs by up to 20% compared to baseline models
- Weighted Penalty Utility scores improve from 0.21 (baseline) to 0.27, while maintaining high compliance (0.94 vs. 0.92)
- Compliance-Aware Harmonic Mean (CAH) scores show balanced improvement in both safety and helpfulness across all nine infringement categories
- Expert evaluation confirms FUA-LLM's superior performance in generating legally compliant yet practically useful responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on expert-curated preference pairs teaches models to prefer legally compliant responses over infringing ones.
- **Mechanism:** Direct Preference Optimization (DPO) directly optimizes the log-ratio of preferred vs. rejected response probabilities without requiring a separate reward model. The model internalizes the legal judgment embedded in expert-labeled pairs (x, y_w, y_l).
- **Core assumption:** Legal experts can reliably encode fair use principles into preference pairs that generalize beyond the training distribution.
- **Evidence anchors:** [abstract] "apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal"; [section 3.1] "The final FairUseDB dataset consists of 9,000 expert-labeled preference triples (x, y_w, y_l), encoding nuanced legal judgment for LLM generation"
- **Break condition:** If preference pairs fail to capture the nuance of fair use (e.g., context-dependent factors), the model may over-refuse or under-protect on novel prompt types.

### Mechanism 2
- **Claim:** Supervised fine-tuning on fair use court proceedings provides foundational legal reasoning before preference alignment.
- **Mechanism:** SFT on Dfu (252 U.S. federal court proceedings) exposes the model to real legal reasoning chains, creating an initialization that already understands how courts analyze fair use factors before DPO sharpens behavioral preferences.
- **Core assumption:** Exposure to legal reasoning text transfers to improved generation behavior, not just better legal QA.
- **Evidence anchors:** [section 3.2] "Step 1: Supervised Fine-Tuning on Dfu... This provides a legally informed initialization for subsequent preference learning"; [section 3.1] "Dfu thus serves to demonstrate how courts have analyzed and ruled on fair use in diverse contexts, providing an initial legal foundation"
- **Break condition:** If SFT causes the model to adopt legalistic tone without behavioral change, or overfits to specific case patterns, DPO alignment may be insufficient to correct.

### Mechanism 3
- **Claim:** Agentic deployment preserves general utility by routing only flagged queries to the legally-aligned model.
- **Mechanism:** A Topic Analysis Agent detects potentially copyrighted content, a Web Search Agent verifies copyright status, and only confirmed-sensitive queries are routed to FUA-LLM—avoiding unnecessary legal guardrails on benign requests.
- **Core assumption:** The topic detection and web verification pipeline has acceptably low false positive and false negative rates.
- **Evidence anchors:** [section 3.3] "This modular deployment strategy ensures FUA-LLM's legal capabilities are applied selectively and efficiently, balancing compliance with performance"; [section 6] "The agentic deployment pipeline... adds latency compared to a unified inference model. This overhead, incurred only for potentially sensitive prompts, is a trade-off"
- **Break condition:** If topic detection misses novel infringement categories or web search returns unreliable copyright status, problematic outputs may bypass the aligned model.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: The core training method; requires understanding how preference pairs shape model behavior without explicit reward modeling.
  - Quick check question: Given a preference pair (prompt, preferred_response, rejected_response), what objective does DPO optimize?

- **Concept: U.S. Fair Use Doctrine (17 U.S.C. §107)**
  - Why needed here: The legal framework being operationalized; the four factors (purpose, nature, amount, market effect) structure the dataset design.
  - Quick check question: What are the four factors courts consider when determining fair use?

- **Concept: Harmonic Mean for Multi-Objective Evaluation**
  - Why needed here: The CAH metric uses harmonic mean to penalize imbalanced performance—requires understanding why arithmetic mean fails here.
  - Quick check question: If Safety=0.9 and Helpfulness=0.1, why does harmonic mean give a lower score than arithmetic mean?

## Architecture Onboarding

- **Component map:** Dfu (court cases) → SFT checkpoint → Dpref (preference pairs) → DPO → FUA-LLM; User prompt → Topic Analysis Agent → (if flagged) Web Search Agent → (if confirmed) FUA-LLM or base model
- **Critical path:** DPO training on FairUseDB is the bottleneck—requires legal expert annotation (9,000 prompts × 2 responses each, manually evaluated).
- **Design tradeoffs:**
  - SFT before DPO vs. DPO-only: SFT provides legal grounding but adds training cost
  - Agentic routing vs. unified model: Selective routing preserves utility but adds latency and deployment complexity
  - Refusal vs. transformative alternatives: Framework optimizes for helpful alternatives but requires more sophisticated model behavior
- **Failure signatures:**
  - Over-refusal: High CAH but low raw utility scores indicates model has learned caution without helpfulness
  - Category-specific degradation: Check per-category CAH in Table 4—sharp drops indicate incomplete coverage
  - Prefix probing vulnerability: Compare Prefix vs. Direct probing results in Table 3—large gaps indicate brittleness
- **First 3 experiments:**
  1. Replicate the DPO training on FairUseDB with a different base model (e.g., LLaMA 3.1 8B) and compare CAH scores against the reported Qwen 2.5 7B results.
  2. Ablate the SFT phase: train DPO directly on the base model without Dfu pre-training and measure utility/compliance degradation.
  3. Test the agentic routing pipeline: inject adversarial prompts designed to bypass topic detection (e.g., paraphrased infringement requests) and measure false negative rate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a single unified model achieve both high utility and strict legal compliance without requiring the multi-model agentic deployment architecture?
- **Basis in paper:** [explicit] The Limitations section states: "Achieving both high utility and legal compliance in a single, unified model remains an open challenge."
- **Why unresolved:** The current design separates concerns via a topic analysis agent, web search agent, and routing mechanism, which increases deployment complexity and resource usage.
- **What evidence would resolve it:** Demonstrating a single model fine-tuned end-to-end that matches FUA-LLM's CAH and Utility scores without any external agents or routing.

### Open Question 2
- **Question:** Does FUA-LLM's legal alignment transfer effectively to other jurisdictions beyond U.S. copyright law?
- **Basis in paper:** [explicit] The Conclusion states: "Future work will extend the framework to other jurisdictions and broader forms of intellectual-property risk."
- **Why unresolved:** FairUseDB and court proceedings were curated exclusively from U.S. sources and Section 107 doctrine; other jurisdictions have different fair dealing or exception frameworks.
- **What evidence would resolve it:** Training on jurisdiction-specific legal data (e.g., EU, UK, Indian copyright law) and evaluating CAH scores across jurisdiction-specific test sets.

### Open Question 3
- **Question:** Can inference overhead from the agentic pipeline be reduced to enable real-time deployment without sacrificing compliance?
- **Basis in paper:** [explicit] The Limitations section identifies: "Reducing this cost through agent optimization, asynchronous execution, and efficient caching is a key avenue for future work."
- **Why unresolved:** The topic analysis and web search agents add latency that may be unacceptable for latency-sensitive applications.
- **What evidence would resolve it:** Benchmarking latency improvements from proposed optimizations while maintaining CAH scores within acceptable variance of the original system.

## Limitations
- The framework's effectiveness depends heavily on the quality and coverage of FairUseDB, which may not capture all nuanced fair use scenarios or evolving copyright interpretations.
- The agentic deployment pipeline introduces additional complexity and potential failure points—the topic analysis and web search agents' performance directly impacts the system's coverage.
- The study focuses exclusively on U.S. fair use doctrine, limiting generalizability to other jurisdictions with different copyright frameworks.

## Confidence

**High Confidence:** The core technical approach (SFT → DPO pipeline) is well-established in the literature. The experimental methodology (controlled comparisons with baselines, ablation studies, multiple evaluation metrics) is sound. The general trend that the framework improves legal compliance while preserving utility is consistently demonstrated across experiments.

**Medium Confidence:** The specific quantitative improvements (e.g., "up to 20% reduction in problematic outputs") depend on the chosen evaluation framework and automated metrics, which may not fully capture real-world performance. The effectiveness of the agentic deployment strategy in practical applications remains to be validated beyond the experimental setup.

**Low Confidence:** The generalizability of FairUseDB to all potential copyright infringement scenarios is uncertain, as the dataset was curated for specific scenarios and may not cover edge cases. The long-term stability of the model's behavior after deployment is not addressed.

## Next Checks

1. **Cross-Jurisdictional Validation:** Evaluate FUA-LLM's performance on copyright scenarios governed by European copyright law or other international frameworks to assess generalizability beyond U.S. fair use doctrine.

2. **Real-World Deployment Pilot:** Implement the agentic deployment pipeline in a production environment with actual user traffic to measure false positive/negative rates of the topic analysis and web search agents, and assess the practical utility-comprehensiveness trade-off.

3. **Adversarial Robustness Testing:** Systematically probe FUA-LLM with adversarial prompts designed to bypass topic detection or exploit gaps in FairUseDB coverage (e.g., novel paraphrasing techniques, zero-shot infringement requests) to identify brittleness in the framework's legal compliance guarantees.