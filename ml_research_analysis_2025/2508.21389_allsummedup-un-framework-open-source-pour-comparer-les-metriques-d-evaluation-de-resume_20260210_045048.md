---
ver: rpa2
title: 'AllSummedUp: un framework open-source pour comparer les metriques d''evaluation
  de resume'
arxiv_id: '2508.21389'
source_url: https://arxiv.org/abs/2508.21389
tags:
- triques
- pour
- dans
- valuation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines reproducibility challenges in automatic summarization
  evaluation, comparing six metrics including classical approaches like ROUGE and
  LLM-based methods (G-Eval, SEval-Ex). Experiments reveal significant discrepancies
  between reported and observed performance, highlighting a trade-off: metrics with
  highest human alignment are computationally intensive and less stable across runs.'
---

# AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume

## Quick Facts
- arXiv ID: 2508.21389
- Source URL: https://arxiv.org/abs/2508.21389
- Authors: Tanguy Herserant; Vincent Guigue
- Reference count: 0
- Primary result: LLM-based summarization metrics show higher human alignment but are computationally expensive and less reproducible than classical approaches

## Executive Summary
This paper investigates reproducibility challenges in automatic summarization evaluation by comparing six metrics including ROUGE, BERTScore, BARTScore, QuestEval, UniEval, G-Eval, and SEval-Ex. The study reveals significant discrepancies between reported and observed metric performance, with LLM-based metrics showing higher human alignment but also greater computational demands and cross-run instability. A unified open-source framework called AllSummedUp is introduced to enable fair comparison across heterogeneous evaluation metrics on standardized datasets like SummEval.

## Method Summary
The study evaluates six summarization metrics on the SummEval dataset (1600 summaries from 16 systems, annotated on coherence, consistency, fluency, and relevance). Metrics were implemented locally using HuggingFace models for traditional approaches and Ollama for LLM-based metrics (Gemma-3-27b and Qwen-2.5-72b). The framework standardizes preprocessing, computation, and output formatting through a unified TextMetric abstract class interface. Spearman correlation with human judgments and execution time were measured for each metric, with comparisons made to literature-reported values.

## Key Results
- LLM-based metrics (G-Eval, SEval-Ex) achieve highest human alignment but require hours vs. seconds for classical metrics
- Significant reproducibility gaps exist between reported and observed metric performance, particularly for BERTScore and BARTScore
- G-Eval shows correlation swings of -0.45 on fluency across runs despite fixed seeds, highlighting LLM stochasticity
- Computational time varies dramatically: UniEval (297s full dataset) vs. G-Eval (hours) for similar correlation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A unified abstract interface enables fair, reproducible comparison across heterogeneous evaluation metrics.
- Mechanism: The framework standardizes preprocessing, computation, and output formatting through a `TextMetric` abstract class. All metrics implement the same workflow, reducing implementation-specific variability that otherwise confounds comparisons.
- Core assumption: Variability in reported metric performance stems partially from inconsistent experimental setups rather than metric properties alone.
- Evidence anchors:
  - [abstract] "We introduce a unified, open-source framework... designed to support fair and transparent comparison of evaluation metrics."
  - [section 3.1] "Cette architecture garantit que toutes les métriques suivent le même flux de travail et produisent des sorties comparables."
  - [corpus] Related work on Spanish/Basque summarization metrics (arXiv:2503.17039) similarly emphasizes standardized evaluation, though corpus evidence for unified frameworks specifically is limited.
- Break condition: If metrics require fundamentally incompatible inputs (e.g., reference-free vs. reference-based), the interface abstraction cannot fully equalize comparison conditions.

### Mechanism 2
- Claim: Metrics with higher human alignment systematically require more computation and exhibit lower cross-run stability.
- Mechanism: LLM-based metrics (G-Eval, SEval-Ex, QuestEval) achieve higher Spearman correlations by leveraging semantic understanding, but this comes from autoregressive decoding and large model dependencies. Their stochasticity—even with fixed seeds—introduces variance across runs.
- Core assumption: The paper assumes default LLM settings (temperature=1, top_p=0.95) are representative of typical usage, though these amplify variability.
- Evidence anchors:
  - [abstract] "metrics with the highest alignment with human judgments tend to be computationally intensive and less stable across runs."
  - [section 4.2.1] G-Eval shows correlation swings of -0.45 on fluency; SEval-Ex shows +0.12 shift on relevance vs. literature.
  - [corpus] Evaluating LLMs for summarization (arXiv:2502.19339) corroborates that LLM-based evaluation introduces instability concerns.
- Break condition: If future LLMs achieve deterministic outputs (e.g., via constrained decoding), the stability penalty may diminish.

### Mechanism 3
- Claim: Reported metric-human correlations often fail to replicate due to undocumented model versions, prompt variations, and preprocessing choices.
- Mechanism: Small changes—prompt formatting, tokenizer versions, model weights—cascade into measurable correlation shifts. The paper documents specific gaps: ROUGE replication failed due to unclear reference handling; BERTScore diverged by -0.12 on coherence.
- Core assumption: Assumption: Original papers did not intentionally omit details; gaps reflect community-wide documentation norms.
- Evidence anchors:
  - [section 4.2.1] "Nous n'avons pas pu répliquer exactement les résultats de ROUGE en raison d'un manque de détails"
  - [section 2.3] Cites Salinas & Morstatter's "butterfly effect"—adding a single space alters LLM outputs.
  - [corpus] arXiv:2510.07061 (Indian language metrics) similarly finds reproducibility gaps when transferring English-validated metrics.
- Break condition: If a metric has no learned components (pure n-gram overlap like ROUGE-1), documentation gaps are less impactful—though still present as shown.

## Foundational Learning

- **Spearman vs. Pearson vs. Kendall correlation**
  - Why needed here: The paper explicitly chooses Spearman for robustness to non-linearity and outliers, but notes literature inconsistency (Pearson for QuestEval, Kendall for SummEval original).
  - Quick check question: Given scores [1,2,3,100] and [1,2,4,5], which correlation measure would be least affected by the outlier?

- **Reproducibility vs. Repeatability (metrology distinction)**
  - Why needed here: The paper invokes this distinction—repeatability is same conditions, reproducibility is across slight variations. NLP often conflates them.
  - Quick check question: If you rerun a metric with the same seed and get different results due to hardware nondeterminism, is this a repeatability or reproducibility failure?

- **LLM decoding stochasticity**
  - Why needed here: The paper uses temperature=1, top_p=0.95, top_k=64. Understanding how these interact explains the observed variance.
  - Quick check question: If temperature were set to 0, would G-Eval become deterministic? What about hardware-level nondeterminism?

## Architecture Onboarding

- **Component map:**
  - `TextMetric` (abstract class) -> `TextEvaluator` (orchestrator) -> `Report` (output)

- **Critical path:**
  1. Implement TextMetric for your target metric (inherit, override methods)
  2. Register with TextEvaluator alongside other metrics
  3. Run on SummEval (1600 summaries, 16 systems, 4 human-rated dimensions)
  4. Compare Spearman correlations and execution time

- **Design tradeoffs:**
  - Local LLM execution via Ollama avoids API dependency but requires GPU VRAM (tested on A6000 48GB)
  - Open-source model substitution (Gemma/Qwen instead of GPT-4) improves reproducibility but may shift correlations vs. original papers
  - Single-dataset focus (SummEval) limits generalization; framework supports adding datasets

- **Failure signatures:**
  - Correlation values diverge >0.1 from literature → check model version, prompt template, reference handling
  - Execution time >10 hours → likely running LLM-based metric on full dataset; consider sampling
  - Negative correlation on expected dimension → prompt may be misformatted or model not following instructions

- **First 3 experiments:**
  1. Run ROUGE and BARTScore on SummEval subset (100 examples) to verify baseline fast metrics; confirm output format matches framework.
  2. Run UniEval on same subset; compare correlation and time. This is the "middle ground" metric (297s full dataset, stable correlations).
  3. Run G-Eval with Gemma-3-27b on subset (not full dataset); measure correlation variance across 3 runs with same seed to quantify stochasticity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed discrepancies in metric reproducibility and the performance-frugality trade-off persist across languages and domains outside the SummEval benchmark?
- Basis in paper: [explicit] Section 5 states, "Des expérimentations sur d’autres corpus seraient nécessaires pour généraliser nos conclusions," noting the study relies exclusively on SummEval.
- Why unresolved: The validation of the proposed framework and the generalizability of the identified structural trade-offs are limited by the single-dataset scope.
- What evidence would resolve it: Applying the AllSummedUp framework to multilingual or domain-specific datasets (e.g., scientific or biomedical texts) to verify if the correlations and reproducibility issues hold.

### Open Question 2
- Question: To what extent does substituting proprietary models (e.g., GPT-4) with open-source alternatives (e.g., Gemma, Qwen) alter the fidelity of metric evaluation compared to original benchmarks?
- Basis in paper: [explicit] Section 5 notes the authors prioritized open-source alternatives, "quitte à perdre en fidélité par rapport aux conditions d’évaluation originales."
- Why unresolved: The study could not quantify the exact performance gap caused by using proxy models for metrics like G-Eval, which was originally designed for GPT-4.
- What evidence would resolve it: A comparative study running metrics like G-Eval on the same dataset using both the original proprietary APIs and the open-source proxies to isolate the "fidelity gap."

### Open Question 3
- Question: Can rigorous standardization of experimental parameters (specifically prompt formatting and random seeds) effectively mitigate the stochastic instability observed in LLM-based evaluators?
- Basis in paper: [inferred] The conclusion advocates for publishing parameters like seeds and prompts to ensure reliability, yet Section 2.3 suggests that even minor prompt changes (the "butterfly effect") cause significant variability.
- Why unresolved: The paper identifies the instability and recommends documentation, but it does not empirically verify if these documentation steps are sufficient to eliminate the reproducibility issues inherent in LLMs.
- What evidence would resolve it: An ablation study measuring the variance of metric scores when using fixed seeds versus default settings across multiple identical runs.

## Limitations
- Single dataset focus (SummEval) limits generalizability across languages and domains
- Open-source model substitutions may reduce fidelity compared to original proprietary benchmarks
- Does not address downstream impact of metric choice on model development decisions

## Confidence
- **High Confidence**: The observation that LLM-based metrics exhibit higher human alignment but also greater computational cost and cross-run instability is well-supported by controlled experiments and documented variance.
- **Medium Confidence**: Claims about documentation gaps and reproducibility challenges are grounded in specific comparison failures (e.g., ROUGE, BERTScore) but may not capture the full scope of unreported variability across the field.
- **Low Confidence**: Generalizability of findings to other languages, datasets, or newer metrics (e.g., post-2024 LLM-based methods) is not established due to the study's narrow scope.

## Next Checks
1. Apply the framework to multilingual summarization datasets (e.g., MLSum, WikiLingua) to test cross-lingual robustness of metric rankings
2. Systematically vary HuggingFace model versions and LLM parameters (temperature, top_p) for each metric to quantify stability ranges
3. Train summarization models using different metrics as rewards (e.g., ROUGE vs. G-Eval) and evaluate whether metric choice affects output quality or alignment with human preferences