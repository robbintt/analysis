---
ver: rpa2
title: In-Context Learning (and Unlearning) of Length Biases
arxiv_id: '2502.06653'
source_url: https://arxiv.org/abs/2502.06653
tags:
- figure
- blue
- length
- long
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can learn
  statistical length biases during in-context learning. The authors demonstrate empirically
  that models do learn length biases in the context window, with performance varying
  across different model sizes, numbers of examples, and class length differences.
---

# In-Context Learning (and Unlearning) of Length Biases

## Quick Facts
- arXiv ID: 2502.06653
- Source URL: https://arxiv.org/abs/2502.06653
- Reference count: 40
- Primary result: Large language models learn statistical length biases during in-context learning, with bias effects scaling with demonstration count and being mitigatable through balanced sampling.

## Executive Summary
This paper investigates whether large language models can learn statistical length biases during in-context learning (ICL). The authors demonstrate empirically that models across different sizes learn length biases in the context window, with performance varying based on demonstration count, model size, and class length differences. The results show that increased numbers of examples can exacerbate learned biases, and models across a range of sizes can acquire these biases. Importantly, the authors show that ICL can be used to debias fine-tuned models that exhibit length biases, suggesting that balanced data sampling is critical to minimize the likelihood of learning biases in-context.

## Method Summary
The authors conduct controlled experiments by sampling in-context demonstrations with systematic length differences between classes, then measuring model performance across length-bins of the validation set. They use datasets like HANS and PAWS-XEN, varying demonstration count (k=2 to 32), model sizes (8B, 2.7B parameters), and length difference magnitudes. The methodology involves creating "worst-case scenario" demonstrations where one class systematically has longer inputs than another, then evaluating whether accuracy varies systematically across input length bins for each class. They also test whether ICL can override finetuning-induced length biases by applying balanced or random sampling during inference.

## Key Results
- Models learn length biases in the context window, with performance varying across model sizes, numbers of examples, and class length differences
- Increased numbers of examples can exacerbate learned biases, with effects typically emerging around 8 examples and strengthening with more demonstrations
- In-context learning can be used to debias fine-tuned models that exhibit length biases, suggesting balanced data sampling is critical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs acquire length-to-label correlations from demonstration distributions in the context window, treating input length as a discriminatory feature.
- Mechan: When demonstrations are sampled such that one class systematically has longer inputs than another, the model exploits this correlation as a shortcut feature, improving accuracy on test inputs matching the demonstrated length pattern while degrading on mismatched inputs.
- Core assumption: Statistical data biases that affect fine-tuning similarly affect in-context learning through pattern recognition in attention mechanisms.
- Evidence anchors:
  - [abstract] "We demonstrate that models do learn length biases in the context window for their predictions"
  - [section 4.1] "We aim to introduce a distributional bias in the input lengths with respect to class... This effectively produces a 'worst-case scenario' in maximizing the distance between the classes"
  - [corpus] Weak direct support; related work on fairness in ICL exists (Tabular Foundation Models) but does not address length bias specifically.
- Break condition: If demonstrations are randomly sampled (balanced length distribution), the bias effect disappears.

### Mechanism 2
- Claim: Increasing the number of in-context examples amplifies learned length biases.
- Mechan: More demonstrations provide more evidence for the spurious length-label correlation, strengthening the model's reliance on length as a predictive feature. The effect typically emerges around k=8 examples and intensifies with k=16-32.
- Core assumption: The model aggregates statistical patterns across demonstrations rather than learning individual exemplar-to-label mappings independently.
- Evidence anchors:
  - [abstract] "increased numbers of examples can exacerbate learned biases"
  - [section 5.2] "models can generally begin learning biases around 8 in-context examples, with the effect typically strengthening with increased numbers of examples"
  - [corpus] No direct corpus evidence on this specific scaling behavior.
- Break condition: If class length distributions in demonstrations are balanced, more examples do not induce bias.

### Mechanism 3
- Claim: In-context demonstrations can override finetuning-induced length biases, effectively "unlearning" them without parameter updates.
- Mechan: Balanced or randomly sampled ICL demonstrations introduce competing length-label correlations that counteract the bias encoded in model weights, shifting predictions toward length-neutral behavior at inference time.
- Core assumption: In-context signals can override or suppress learned weight-based biases when they contradict each other.
- Evidence anchors:
  - [abstract] "learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning)"
  - [section 6] "random sampling was able to counteract the bias, essentially 'unlearning' the finetuned bias. This suggests that balanced data sampling is critical"
  - [corpus] "DRAGON: Guard LLM Unlearning in Context" supports ICL-based unlearning mechanisms broadly.
- Break condition: If finetuning bias is extremely strong or demonstrations are too few/short, ICL may fail to override.

## Foundational Learning

- Concept: **Statistical Data Bias vs. Task-Informative Features**
  - Why needed here: The paper distinguishes length as an artifact of data collection (exploitable bias) vs. length as genuine linguistic signal. Misidentifying which is present leads to incorrect conclusions about model behavior.
  - Quick check question: Does length correlate with the label in your training data? If yes, is that correlation meaningful for the task or an annotation artifact?

- Concept: **In-Context Learning (ICL) Paradigm**
  - Why needed here: Understanding ICL as learning from concatenated input-output pairs at inference without parameter updates is essential to grasp why biases can be introduced/removed dynamically.
  - Quick check question: Can you modify model behavior at inference time by changing only the prompt? If you need weight updates, it's not ICL.

- Concept: **Length Binning for Bias Detection**
  - Why needed here: The paper's evaluation methodology bins test data by length and measures per-class accuracy to reveal whether the model favors certain length-label combinations.
  - Quick check question: If you plot accuracy vs. input length for each class, do you see systematic divergence?

## Architecture Onboarding

- Component map:
  - Demonstration sampler -> Prompt constructor -> LLM backbone -> Evaluation binning

- Critical path:
  1. Sample demonstrations with controlled length distribution (e.g., top-k/2 long for y1, bottom-k/2 short for y2)
  2. Construct prompt with task instruction + demonstrations + test input
  3. Run inference on binned validation set
  4. Compute per-class accuracy across length bins to quantify bias

- Design tradeoffs:
  - More demonstrations (k=32) → stronger ICL performance but higher bias risk
  - Smaller length differences between classes → reduced bias effect but may not reflect real data
  - Random vs. biased sampling → random is safer but may not match deployment data distribution

- Failure signatures:
  - Accuracy varies drastically across length bins for the same class (diagnostic of length bias)
  - Increasing k improves performance on bias-aligned inputs but degrades on misaligned ones
  - Debiasing via opposite-length demonstrations fails (model learns ICL bias instead of overriding finetuning bias)

- First 3 experiments:
  1. Replicate bias detection: Sample k=16 demonstrations with extreme length differences per class; measure accuracy across 6 length bins on HANS or PAWS-XEN.
  2. Ablate example count: Repeat with k∈{2,4,8,16,24,32} to identify when bias emerges (expect ~k=8).
  3. Debias finetuned model: Finetune with biased length sampling, then apply random-sampling ICL at inference; verify bias reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed susceptibility and unlearning of length biases generalize to models larger than 8 billion parameters?
- Basis in paper: [explicit] The authors explicitly list the 8B parameter cap as a limitation, stating "we believe our results scale" but acknowledging they did not test larger models due to resource constraints.
- Why unresolved: It is unverified if larger, more capable models exhibit the same vulnerability to in-context length biases or if they possess emergent robustness.
- What evidence would resolve it: Replicating the bias induction and intervention experiments on models with 70B+ parameters or proprietary frontier models.

### Open Question 2
- Question: Does knowledge extraction via in-context learning override knowledge encoded during fine-tuning?
- Basis in paper: [explicit] The authors suggest this mechanism to explain their debiasing results but state "further study is warranted on whether knowledge-extraction from ICL overrides knowledge-gain during finetuning."
- Why unresolved: While the paper shows ICL can debias models, it does not determine if the model ignores the finetuned weights or if the context actively suppresses them.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., probing classifiers) comparing internal activations during biased vs. debiased inference.

### Open Question 3
- Question: Can in-context learning interventions mitigate other statistical biases (e.g., lexical or syntactic) besides length?
- Basis in paper: [inferred] The introduction notes that the impact of other statistical biases remains "under-explored," and the methodology is restricted to length biases.
- Why unresolved: The "unlearning" phenomenon is only demonstrated for length; it is unclear if balanced sampling works for complex features like syntactic heuristics.
- What evidence would resolve it: Applying the paper's intervention methodology to datasets known for non-length artifacts, such as lexical overlap biases in HANS.

## Limitations
- Limited model diversity: The study primarily uses smaller LLaMA 3 and GPT-Neo variants (8B, 2.7B), with uncertainty about how results generalize to frontier models
- Controlled experimental conditions: Artificially constructed length differences may not capture all manifestations of length bias in real-world deployment scenarios
- Unverified mechanism specificity: The paper doesn't conclusively prove that length bias learning is distinct from general statistical pattern learning

## Confidence
- High confidence: That LLMs learn length biases from demonstration distributions in ICL
- Medium confidence: That increased demonstration count amplifies learned length biases
- Medium confidence: That ICL can override finetuning-induced length biases

## Next Checks
1. **Scale validation experiment**: Test the same methodology across a wider range of model sizes, particularly including frontier models (GPT-4, Claude, Gemini) to verify that length bias susceptibility is consistent across the model size spectrum.

2. **Real-world bias transfer test**: Apply the experimental methodology to naturally occurring datasets with length-label correlations (e.g., customer service logs where longer inputs correlate with certain issue types) to validate that artificial length differences generalize to realistic deployment scenarios.

3. **Mechanism specificity probe**: Design experiments that introduce multiple spurious correlations simultaneously (length, lexical features, syntactic patterns) to determine whether length biases are learned differently from other statistical shortcuts, and whether ICL debiasing generalizes across different types of learned biases.