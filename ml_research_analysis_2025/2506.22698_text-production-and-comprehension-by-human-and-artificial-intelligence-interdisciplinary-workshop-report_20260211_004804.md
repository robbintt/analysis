---
ver: rpa2
title: 'Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary
  Workshop Report'
arxiv_id: '2506.22698'
source_url: https://arxiv.org/abs/2506.22698
tags:
- language
- human
- llms
- cognitive
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The NSF-funded workshop "Text Production and Comprehension by Human
  and Artificial Intelligence" explored the intersection of AI language models and
  human cognition in text processing. Bringing together experts in cognitive psychology,
  linguistics, and AI-based NLP, the workshop examined how LLMs can inform our understanding
  of human language processing and how human cognition adapts to AI collaboration.
---

# Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report

## Quick Facts
- arXiv ID: 2506.22698
- Source URL: https://arxiv.org/abs/2506.22698
- Authors: Emily Dux Speltz
- Reference count: 3
- Primary result: NSF-funded workshop explored how AI language models inform human language processing understanding and human-AI collaboration

## Executive Summary
The NSF-funded workshop "Text Production and Comprehension by Human and Artificial Intelligence" brought together cognitive psychologists, linguists, and AI researchers to examine the intersection of human and artificial intelligence in text processing. Participants explored how large language models (LLMs) can inform hypotheses about human language learning, how human cognition adapts to AI collaboration, and how AI tools can enhance language instruction. The workshop emphasized the need for interdisciplinary research to develop integrated models of human-AI language processing and to understand the long-term effects of AI collaboration on human cognitive skills.

## Method Summary
The workshop employed interdisciplinary presentations and discussions combining empirical studies on human-AI collaboration with theoretical frameworks from cognitive science and linguistics. Methods included analysis of RLHF fine-tuning effects on LLM-human alignment, neuroimaging studies using fMRI/MEG to predict brain activity during language tasks, and evaluation of AI-augmented educational feedback systems. The Poetry Turing Test was used to assess LLM-generated vs. human-authored text, while text simplification evaluation examined GPT-4's performance with human oversight requirements.

## Key Results
- LLMs offer valuable hypotheses about human language learning mechanisms, particularly through discriminative learning and uncertainty reduction rather than compositional meaning transfer
- Human-AI collaboration shows promise but introduces cognitive challenges requiring careful alignment of representations between humans and LLMs
- LLM-based tools demonstrate significant potential for improving language instruction and assessment when combined with human review and RAG calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs may offer hypotheses about human language learning mechanisms, particularly through discriminative learning and uncertainty reduction rather than compositional meaning transfer.
- Mechanism: LLMs learn statistical patterns from vast text corpora. When their learning patterns (e.g., handling "bursty" word distributions, managing linguistic unpredictability) align with observed human processing constraints, they generate testable hypotheses about human cognition that would be difficult to derive from traditional experimental methods alone.
- Core assumption: Human language processing operates on similar statistical/discriminative principles as LLMs, rather than primarily on explicit rule-based compositional semantics.
- Evidence anchors:
  - [section] Page 10-11: Ramscar's framework proposes "discriminative learning approach based on uncertainty reduction rather than compositional meaning transfer" and notes this "aligns more closely with how LLMs learn."
  - [section] Page 5: Notes that coherent text can be produced "without the writer stopping to think," suggesting implicit, probabilistic processing that "may, in important ways, parallel what happens within NLG systems like ChatGPT."
  - [corpus] "Advancing Cognitive Science with LLMs" (FMR 0.60) supports using LLMs as cognitive science tools, but corpus lacks direct validation of discriminative learning as human mechanism.
- Break condition: Hypotheses derived from LLMs may not generalize when human language use requires cultural knowledge, emotional context, or embodied experience that statistical text patterns cannot capture. Mahowald's distinction (formal vs. functional competence) highlights this boundary.

### Mechanism 2
- Claim: Effective human-AI collaboration in language tasks depends on representation alignment between human and LLM, which RLHF fine-tuning can improve.
- Mechanism: When LLMs are fine-tuned with human feedback, their outputs become more aligned with human response patterns—including capturing sample-level variance and approximating human performance patterns even in difficult areas. This alignment enables more intuitive collaboration, similar to representation alignment in human-human dialogue.
- Core assumption: The cognitive processes enabling successful human-human dialogue (shared representations, alignment) transfer to human-AI interaction when LLM outputs approximate human response distributions.
- Evidence anchors:
  - [abstract] States "increasing alignment between LLM behavior and human language processing when models are fine-tuned with human feedback."
  - [section] Page 13: Contreras Kallens found "RLHF fine-tuning significantly alters LLM behavior, making it more aligned with human responses... capturing sample-level variance and better approximating human performance patterns."
  - [section] Page 12: Pickering proposes "successful collaboration between humans and LLMs depends on an alignment of representations, similar to human-to-human dialogue."
  - [corpus] "The Prompting Brain" (FMR 0.53) investigates neural markers of expertise in LLM interaction, suggesting cognitive adaptation occurs, but corpus lacks direct evidence on representation alignment mechanisms.
- Break condition: Alignment achieved through RLHF may be superficial—matching response patterns without shared underlying representations. When tasks require deep pragmatic understanding or real-world grounding, apparent alignment may break down. Mahowald's caution about uneven functional competence applies.

### Mechanism 3
- Claim: RAG-calibrated AI combined with human review can provide more extensive and targeted formative feedback than either humans or AI alone in language-intensive learning domains.
- Mechanism: RAG systems retrieve relevant external knowledge to ground LLM outputs, reducing hallucinations. Combined with human peer review, this creates a hybrid feedback loop where AI provides scale and specificity while humans provide judgment and accountability—each compensating for the other's limitations.
- Core assumption: Detailed formative feedback improves learning outcomes in writing/argumentation, and AI can generate such feedback at scale when properly grounded.
- Evidence anchors:
  - [section] Page 16-17: Cope et al. "reported using RAG-calibrated AI in combination with human peer review to provide feedback on student projects, with that combination resulting in more extensive and finely targeted formative suggestions than would be feasible for human instructors or peer reviewers alone."
  - [section] Page 16: Reznitskaya and Chukharev proposed LLMs implementing "more comprehensive frameworks like the Rational Force Model (RFM) and the Argumentation Rating Tool (ART)" for nuanced argument evaluation.
  - [corpus] Weak direct evidence—no corpus papers validate RAG + human review efficacy for learning outcomes.
- Break condition: System fails when RAG retrieval is irrelevant, when AI cannot assess student authorship (substitution rather than augmentation), or when feedback overwhelms learner cognitive capacity.

## Foundational Learning

- Concept: **Formal vs. Functional Competence (Mahowald)**
  - Why needed here: Critical for understanding what LLMs can and cannot model about human language. LLMs may show strong formal competence (linguistic rules) but uneven functional competence (real-world language use). This distinction prevents overclaiming LLM-human parallels.
  - Quick check question: Can you explain why an LLM might generate grammatically perfect text that nonetheless fails pragmatically in a real-world context?

- Concept: **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: RLHF is the primary mechanism by which LLMs become more "aligned" with human responses. Understanding this helps evaluate claims about human-LLM similarity and collaboration potential.
  - Quick check question: How does RLHF change an LLM's output distribution, and what are the limitations of this alignment?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is presented as a key technique for grounding LLM outputs in educational contexts, reducing hallucinations while maintaining generative capabilities.
  - Quick check question: What problem does RAG solve compared to using an LLM alone, and what new failure modes might it introduce?

## Architecture Onboarding

- Component map:
  Human Writer ─┬─> Text Production ─> Draft
                │                        │
  LLM Assistant ─┴─> Augmentation ──────┘
                        │
                        v
                [Representation Alignment Layer]
                        │
                        v
  RAG System ──> Knowledge Retrieval ─> Grounded Feedback
                        │
                        v
                Human Peer Review ─> Final Feedback to Writer

- Critical path:
  1. Establish feedback quality metrics (before deployment)
  2. Implement RAG calibration with domain-specific knowledge bases
  3. Design human-AI workflow with clear division of labor
  4. Add safeguards for student authorship verification
  5. Deploy with monitoring for bias, hallucination, and learning outcomes

- Design tradeoffs:
  - **Scale vs. depth**: AI enables extensive feedback but may miss nuanced issues human reviewers catch
  - **Automation vs. skill development**: Over-reliance on AI feedback may hinder development of self-evaluation skills
  - **Alignment vs. authenticity**: RLHF makes AI more human-like but may also amplify human biases

- Failure signatures:
  - AI generates plausible but incorrect feedback (hallucination not caught by RAG)
  - Students substitute AI output for own work (authorship failure)
  - Feedback quantity overwhelms cognitive processing capacity
  - Cultural/linguistic biases in AI outputs go uncorrected
  - Long-term decline in independent writing ability (monitor via longitudinal assessment)

- First 3 experiments:
  1. **Baseline comparison**: Measure learning outcomes (writing quality, argumentation skills) for (a) AI-only feedback, (b) human-only feedback, (c) RAG + human hybrid, (d) no feedback control. Use blinded evaluation.
  2. **Authorship detection**: Develop and test methods for distinguishing human-authored vs. AI-assisted vs. AI-generated text in educational submissions.
  3. **Longitudinal tracking**: Monitor cohort over one academic term+ to assess whether AI collaboration enhances or diminishes independent writing skills and critical evaluation ability. Include pre/post measures of (a) writing fluency without AI, (b) ability to critique AI-generated text, (c) domain-specific knowledge retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can LLMs serve as valid cognitive models for human language processing given the disparity between their formal linguistic competence and functional real-world competence?
- Basis in paper: [explicit] The paper highlights Kyle Mahowald’s distinction between formal and functional competence, noting that while LLMs master rules, their context-dependent usage remains uneven, complicating direct parallels to human cognition.
- Why unresolved: There is insufficient data on whether LLMs' statistical learning mechanisms accurately mirror the nuanced, context-dependent nature of human communication or merely simulate surface-level linguistic rules.
- What evidence would resolve it: Comparative studies isolating formal versus functional language tasks in both humans and LLMs, specifically analyzing where their processing trajectories diverge.

### Open Question 2
- Question: What new cognitive strategies do humans develop to leverage AI capabilities while compensating for LLM limitations during text production?
- Basis in paper: [explicit] The report suggests that successful collaboration requires "alignment of representations" and speculates that humans may develop a "novel form of augmented cognition" involving unique strategies for interaction.
- Why unresolved: Current understanding focuses on the output quality rather than the real-time cognitive adaptations and "mental meshing" occurring within the human writer during AI-assisted tasks.
- What evidence would resolve it: Real-time neuroimaging (fMRI/MEG) and eye-tracking data collected during human-AI collaborative writing sessions to map cognitive load and strategy shifts.

### Open Question 3
- Question: What are the long-term cognitive and literacy impacts of sustained human-AI collaboration on critical thinking and creativity?
- Basis in paper: [explicit] The authors explicitly advocate for "long-term longitudinal studies to assess the impact of sustained human-AI collaboration on these cognitive abilities" and note concerns about potential skill atrophy.
- Why unresolved: Existing research is short-term; the field lacks data on whether prolonged reliance on AI degrades domain-specific knowledge and critical evaluation skills or enhances them through effective augmentation.
- What evidence would resolve it: Longitudinal studies tracking individuals' independent problem-solving abilities, literacy metrics, and critical thinking skills over multiple years of AI tool usage.

## Limitations

- Limited empirical evidence for long-term cognitive effects of AI collaboration on human literacy and critical thinking skills
- Unclear whether LLM learning patterns accurately reflect human cognitive processes or merely simulate surface-level linguistic patterns
- Insufficient validation of RAG + human review systems for improving learning outcomes in educational contexts

## Confidence

- **Medium Confidence**: Claims about LLMs offering hypotheses for human language learning mechanisms
- **Medium Confidence**: Assertions about effective human-AI collaboration through representation alignment
- **Low Confidence**: Claims about RAG-calibrated AI plus human review providing superior formative feedback

## Next Checks

1. **Randomized Controlled Trial**: Conduct a semester-long study comparing four conditions—AI-only feedback, human-only feedback, RAG + human hybrid, and control—measuring not just immediate writing quality but also development of independent evaluation skills and long-term retention of domain knowledge.

2. **Authorship Detection System Development**: Design and validate methods for reliably detecting AI-assisted vs. AI-generated text in educational contexts, addressing the critical challenge of ensuring student learning rather than substitution.

3. **Cognitive Load and Skill Development Monitoring**: Implement pre/post assessments measuring writing fluency without AI assistance, critical evaluation of AI-generated text, and domain-specific knowledge retention across multiple cohorts using AI collaboration tools.