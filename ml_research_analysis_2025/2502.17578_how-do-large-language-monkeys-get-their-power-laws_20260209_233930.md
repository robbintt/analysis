---
ver: rpa2
title: How Do Large Language Monkeys Get Their Power (Laws)?
arxiv_id: '2502.17578'
source_url: https://arxiv.org/abs/2502.17578
tags:
- power
- scaling
- language
- laws
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Monkeys, a framework for scaling inference compute
  via repeated sampling, exhibits power law scaling in negative log average success
  rate with number of attempts. However, the underlying mathematical mechanism remains
  unclear, as individual problems show exponential scaling.
---

# How Do Large Language Monkeys Get Their Power (Laws)?
## Quick Facts
- arXiv ID: 2502.17578
- Source URL: https://arxiv.org/abs/2502.17578
- Reference count: 40
- Large Language Monkeys framework shows power law scaling in repeated sampling inference compute via heavy-tailed distribution of single-attempt success probabilities

## Executive Summary
This paper investigates why Large Language Monkeys (LLMs) exhibit power law scaling in negative log average success rate with number of attempts, despite individual problems showing exponential scaling. The authors demonstrate that power law scaling emerges from the heavy-tailed distribution of single-attempt success probabilities across tasks. Specifically, a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law, even as each problem scales exponentially on its own. The work provides both theoretical understanding and practical forecasting methods for inference compute scaling.

## Method Summary
The authors develop a theoretical framework analyzing the distribution of single-attempt success probabilities across tasks. They show that when this distribution has heavy tails, repeated sampling leads to power law scaling in aggregate success rates. The paper introduces a forecasting method that estimates the power law exponent from the distributional structure rather than requiring extensive empirical testing. This approach requires significantly less inference compute while maintaining accuracy. The theoretical derivations are validated through synthetic experiments and analysis of real-world task distributions.

## Key Results
- Power law scaling in Large Language Monkeys emerges from heavy-tailed distributions of single-attempt success probabilities
- A small fraction of extremely hard tasks dominates the aggregate scaling behavior
- The proposed forecasting method achieves significantly lower relative error with orders of magnitude less inference compute
- The distributional perspective explains deviations from power law scaling when they occur

## Why This Works (Mechanism)
The power law emerges because the distribution of single-attempt success probabilities across tasks has heavy tails. When you repeatedly sample from tasks with such a distribution, the aggregate behavior exhibits power law scaling even though each individual task scales exponentially. The key insight is that a small number of tasks with very low success probabilities (the heavy tail) collectively determine the overall scaling behavior. This is similar to how extreme events in heavy-tailed distributions can dominate aggregate statistics. The mathematical mechanism involves the interaction between exponential decay within each task and the heavy-tailed structure across tasks, creating the observed power law relationship.

## Foundational Learning
1. **Heavy-tailed distributions** - Distributions where extreme values occur more frequently than in normal distributions. *Why needed:* Understanding how rare but significant events affect aggregate behavior. *Quick check:* Verify that P(X > x) decays slower than exponential for large x.

2. **Power law scaling** - A relationship where one quantity varies as a power of another. *Why needed:* The paper's core finding about how success rates scale with attempts. *Quick check:* Plot log(success rate) vs log(attempts) to check for linearity.

3. **Repeated sampling inference** - The process of making multiple attempts at solving problems to improve overall success rates. *Why needed:* The computational strategy being analyzed. *Quick check:* Confirm that each attempt is independent and identically distributed.

4. **Exponential vs power law decay** - Different mathematical forms describing how probabilities decrease with attempts. *Why needed:* Understanding why individual problems show exponential decay while aggregate shows power law. *Quick check:* Compare exponential decay P(t) = e^(-λt) with power law decay P(t) = t^(-α).

## Architecture Onboarding
**Component map:** Distribution of single-attempt success probabilities → Heavy-tailed structure → Power law emergence in repeated sampling → Forecasting method

**Critical path:** The heavy-tailed distribution of success probabilities is the critical component. Without heavy tails, power law scaling does not emerge. The forecasting method depends entirely on accurately characterizing this distribution.

**Design tradeoffs:** The paper trades detailed task-level modeling for a distributional approach that captures aggregate behavior. This simplification enables tractable analysis but may miss task-specific nuances. The forecasting method trades some accuracy for computational efficiency.

**Failure signatures:** If the distribution of success probabilities is not heavy-tailed, power law scaling will not emerge. If tasks are not independent, the theoretical framework breaks down. If the heavy tail is not properly characterized, forecasting accuracy degrades significantly.

**First experiments:**
1. Generate synthetic task distributions with varying tail heaviness and verify power law emergence conditions
2. Apply the forecasting method to real task distributions and compare predicted vs actual power law exponents
3. Test how different sampling strategies (temperature, top-k) affect the heavy-tailed structure

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation across diverse real-world tasks and model architectures
- The exponential scaling assumption for individual problems may not hold universally
- Does not explore how different inference-time optimization strategies affect distributional structure
- Theoretical focus with limited exploration of practical implementation details

## Confidence
**High confidence:** The mathematical proof that power law scaling emerges from heavy-tailed distributions is rigorous and well-supported. The demonstration that a small fraction of hard tasks can dominate aggregate scaling behavior is convincing.

**Medium confidence:** The empirical validation of theoretical predictions is promising but limited to a relatively small set of tasks and models. The forecasting method's performance claims would benefit from broader testing across different model families.

**Low confidence:** The generality of the exponential scaling assumption for individual problems across all possible task types and model architectures is not thoroughly established.

## Next Checks
1. Test the forecasting method across a wider range of model families (different architectures, training approaches) and task types to assess generalizability.

2. Investigate how different sampling strategies (temperature, top-k, beam search) affect the emergence of power law scaling and the heavy-tailed distribution structure.

3. Conduct experiments with synthetic task distributions to systematically explore the conditions under which power law scaling emerges versus when it breaks down.