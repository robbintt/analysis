---
ver: rpa2
title: We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They
  Go Wrong
arxiv_id: '2509.22510'
source_url: https://arxiv.org/abs/2509.22510
tags:
- ambs
- alignment
- steering
- llama-2-7b
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of aligning large language models\
  \ (LLMs) along multiple objectives\u2014helpfulness, harmlessness, and honesty (HHH)\u2014\
  while avoiding catastrophic forgetting and inference fragmentation. The core method,\
  \ Adaptive Multi-Branch Steering (AMBS), uses a two-stage 1-to-N transformer framework:\
  \ Stage I computes shared hidden states once, and Stage II applies branch-specific\
  \ steering vectors updated via a policy-reference mechanism to achieve objective-specific\
  \ control."
---

# We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong

## Quick Facts
- **arXiv ID**: 2509.22510
- **Source URL**: https://arxiv.org/abs/2509.22510
- **Reference count**: 9
- **Primary result**: AMBS improves average alignment scores by +32.4% and reduces unsafe outputs by 11.0% on DeepSeek-7B

## Executive Summary
This paper introduces Adaptive Multi-Branch Steering (AMBS), a method for aligning large language models along three objectives—helpfulness, harmlessness, and honesty—while avoiding catastrophic forgetting and inference fragmentation. The approach uses a two-stage 1-to-N transformer architecture where a shared backbone computes hidden states once, and branch-specific steering vectors guide objective-specific outputs. Empirical results demonstrate significant improvements over baseline methods, achieving higher alignment scores and reduced unsafe outputs while maintaining computational efficiency through shared computation.

## Method Summary
AMBS employs a two-stage 1-to-N transformer framework for multi-objective alignment. In Stage I, a shared backbone processes input through L layers to generate post-attention hidden states h^L. In Stage II, these states are cloned into N=3 branches, and learnable steering vectors v^(n) are added via broadcast addition to guide each branch toward a specific objective. The steering vectors are updated through a policy-reference mechanism: a policy network f_θ (trained online) and a frozen reference network f_ϕ (pretrained MLP) compute outputs that are optimized using cosine similarity loss. The method uses AdamW optimizer with specific hyperparameters, processes datasets including Alpaca, BeaverTails, and TruthfulQA, and evaluates performance using Win Rate, Safety Score, and Truthfulness-Informativeness metrics.

## Key Results
- AMBS achieves +32.4% improvement in average alignment scores compared to naive 1-to-N baseline
- Reduces unsafe outputs by 11.0% on DeepSeek-7B while maintaining computational efficiency
- Maintains competitive performance with state-of-the-art methods while avoiding inference fragmentation

## Why This Works (Mechanism)
The method works by computing shared hidden states once through the backbone, then applying objective-specific steering vectors to guide each branch toward its target alignment. The policy-reference mechanism ensures that steering vectors are updated based on the difference between online policy outputs and frozen reference outputs, preventing catastrophic forgetting. By maintaining a shared base computation, the approach avoids the inference fragmentation that occurs when models are fine-tuned separately for each objective.

## Foundational Learning
- **1-to-N Transformer Architecture**: A single input generates N parallel outputs through branching; needed for multi-objective alignment without separate models; quick check: verify branch count matches objective count
- **Steering Vectors**: Learnable parameters added to hidden states to guide objective-specific behavior; needed to control branch outputs without retraining; quick check: ensure vectors are properly broadcast across sequence dimensions
- **Policy-Reference Mechanism**: Online policy network compared against frozen reference to update steering; needed to prevent catastrophic forgetting; quick check: verify cosine similarity between policy and reference outputs
- **Cosine Loss Optimization**: Uses 1 - cos(y^+, y^-) to align policy and reference outputs; needed for smooth steering vector updates; quick check: monitor loss values during training
- **Broadcast Addition**: Element-wise addition of steering vectors across all tokens in sequence; needed for efficient parameter sharing; quick check: confirm vector dimensions match hidden state dimensions
- **Post-Attention States**: Hidden states after attention layers in transformer; needed as steering input point; quick check: verify extraction occurs after L-th attention layer

## Architecture Onboarding

**Component Map**: Input -> Stage I (shared backbone) -> h^L -> Stage II (N branches with steering) -> N objective outputs

**Critical Path**: Input tokenization → shared backbone computation → post-attention state extraction → branch cloning → steering vector addition → objective-specific outputs

**Design Tradeoffs**: Shared backbone reduces computation but requires careful steering design; separate branches enable objective specialization but risk fragmentation; cosine loss balances adaptation with stability

**Failure Signatures**: High branch divergence indicates poor steering; catastrophic forgetting shows as policy drift from reference; under-steering produces similar outputs across branches

**First 3 Experiments**:
1. Implement basic 1-to-N transformer with random steering vectors and verify parallel outputs
2. Add cosine loss optimization and test on small synthetic dataset
3. Evaluate branch similarity using t-SNE visualization of post-attention states

## Open Questions the Paper Calls Out
- Does AMBS maintain efficiency and alignment consistency when scaled to models larger than 7B parameters or extended beyond three HHH axes?
- How does the specific representational geometry of a base model influence the reduction of inference fragmentation?
- Can a mechanism be developed to automatically select the optimal alignment branch for ambiguous queries without manual specification?

## Limitations
- Reference model pretraining details are underspecified, including data sources and training procedures
- Results are limited to 7B-scale models, leaving scalability to larger models unverified
- Evaluation relies on proxy metrics rather than human judgment, raising questions about real-world applicability

## Confidence
**High Confidence**: Core architecture and training procedure are clearly specified with reproducible results
**Medium Confidence**: Claims about avoiding catastrophic forgetting are supported but could benefit from longer-term validation
**Low Confidence**: Competitiveness with state-of-the-art methods is difficult to verify due to differences in experimental setups across studies

## Next Checks
1. Implement and train the reference model f_ϕ using publicly available alignment datasets to verify cosine loss effectiveness
2. Systematically test different steering vector initialization strategies to determine impact on training stability
3. Evaluate AMBS on LLaMA-2-7B and Mistral-7B using the same three datasets to confirm cross-model generalization