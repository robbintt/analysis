---
ver: rpa2
title: 'FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality
  Awareness'
arxiv_id: '2504.07801'
source_url: https://arxiv.org/abs/2504.07801
tags:
- fairness
- recommendations
- systems
- prompt
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairEval is a novel evaluation framework that assesses fairness
  in LLM-based recommender systems by integrating both demographic attributes and
  personality traits. Unlike prior approaches, FairEval evaluates fairness through
  prompt-based analysis, testing how LLMs respond to variations in user identity and
  personality while maintaining core preferences.
---

# FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality Awareness

## Quick Facts
- **arXiv ID:** 2504.07801
- **Source URL:** https://arxiv.org/abs/2504.07801
- **Reference count:** 40
- **One-line primary result:** FairEval reveals significant fairness disparities in LLM recommendations, with SNSR values up to 34.79% and personality-based inconsistencies, especially in models like Gemini 1.5 Flash.

## Executive Summary
FairEval is a novel evaluation framework that assesses fairness in LLM-based recommender systems by integrating both demographic attributes and personality traits. Unlike prior approaches, FairEval evaluates fairness through prompt-based analysis, testing how LLMs respond to variations in user identity and personality while maintaining core preferences. The framework employs multiple fairness metrics, including PAFS@25 (Personality-Aware Fairness Score), SNSR, and SNSV, across domains like movies and music using models such as ChatGPT 4o and Gemini 1.5 Flash. Results show significant fairness disparities, with SNSR values reaching up to 34.79% and personality-based inconsistencies, especially in models like Gemini 1.5 Flash. FairEval demonstrates that fairness evaluations must account for personality traits and prompt robustness, offering actionable insights for designing more equitable and reliable AI-driven recommendation systems.

## Method Summary
FairEval evaluates fairness in LLM-based recommender systems by comparing recommendation outputs from neutral prompts (containing only user preferences) with sensitive prompts (containing demographic attributes and personality traits). The framework generates three prompt types: neutral, sensitive I (demographic attributes), and sensitive II (intersectional attributes). It queries ChatGPT 4o and Gemini 1.5 Flash APIs with structured prompts for 1,000 movie directors and 1,000 music artists, extracting top-25 recommendations. Similarity metrics (Jaccard@25, SERP*@25, PRAG*@25) are computed between neutral and sensitive outputs, aggregated into fairness metrics (PAFS@25, SNSR, SNSV) to quantify bias and consistency across user groups.

## Key Results
- SNSR values reached up to 34.79%, with religion and race showing notably high SNSR values exceeding 0.12 under PRAG*@25 for both models.
- PAFS@25 values varied significantly across personality traits, indicating inconsistent treatment based on psychological profiles.
- Gemini 1.5 Flash exhibited higher fairness disparities than ChatGPT 4o across multiple metrics and domains.

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Prompt Comparison for Bias Detection
FairEval generates neutral prompts (baseline preferences) and sensitive prompts (with demographic attributes) to reveal systematic biases by computing similarity between recommendations. Lower similarity indicates stronger identity-driven deviation from stated preferences.

### Mechanism 2: Personality-Aware Fairness Score (PAFS) for Psychographic Consistency
PAFS quantifies recommendation stability across personality-conditioned prompts, capturing fairness dimensions overlooked by demographic-only metrics. Higher values indicate consistent treatment across personality types.

### Mechanism 3: SNSR/SNSV Disparity Metrics for Group-Level Unfairness
SNSR (max-min similarity across groups) and SNSV (variance of similarity scores) identify which attributes cause the largest fairness gaps. Higher values indicate systematic bias or inconsistent treatment across demographic groups.

## Foundational Learning

- **Concept: Set-based similarity metrics (Jaccard, SERP, PRAG)**
  - **Why needed here:** FairEval relies on comparing recommendation lists using these metrics; understanding their differences is essential for interpreting results.
  - **Quick check question:** Given two recommendation lists with 60% item overlap but different rankings, which metric (Jaccard vs. PRAG) would show higher similarity and why?

- **Concept: Prompt sensitivity in LLMs**
  - **Why needed here:** FairEval's core premise is that LLM recommendations vary with prompt phrasing; understanding this instability is prerequisite to evaluating fairness.
  - **Quick check question:** If an LLM recommends different movies for "I love sci-fi" vs. "I'm an Asian male who loves sci-fi," is this evidence of bias or expected contextual adaptation?

- **Concept: Counterfactual fairness evaluation**
  - **Why needed here:** FairEval's methodology is fundamentally counterfactual—comparing what recommendations would be if only user identity changed.
  - **Quick check question:** What is the minimal set of changes needed to convert a neutral prompt to a sensitive prompt while maintaining preference validity?

## Architecture Onboarding

- **Component map:** Prompt Generator -> LLM Interface -> Recommendation Parser -> Similarity Engine -> Fairness Calculator -> Robustness Tester
- **Critical path:** Prompt Generator → LLM Interface → Recommendation Parser → Similarity Engine → Fairness Calculator. Errors in parsing propagate downstream and corrupt all metrics.
- **Design tradeoffs:**
  - Prompt template rigidity vs. naturalness: Standardized templates enable controlled comparison but may not reflect real-world usage patterns.
  - Metric selection: Jaccard is interpretable but ignores ranking; PRAG captures rank alignment but is computationally heavier; PAFS addresses personality but assumes personality-conditioned prompts are valid fairness probes.
  - Model coverage: Testing more LLMs increases generalizability but raises API costs and evaluation time.
- **Failure signatures:**
  - Low similarity across all prompts: May indicate LLM stochasticity rather than bias; run multiple samples to distinguish.
  - Inconsistent parsing: LLM adds commentary instead of pure item lists; enforce strict output format via prompt instructions.
  - SNSR/SNSV near zero: Either the model is fair, or prompt variations are not sufficiently diverse to trigger bias; verify prompt coverage.
- **First 3 experiments:**
  1. Baseline replication: Replicate Table 1 results for ChatGPT 4o movie recommendations using provided prompt templates; validate SNSR/SNSV calculations match reported values.
  2. Domain transfer: Apply FairEval to a new domain (e.g., book recommendations) using identical prompt structures; compare fairness profiles to movie/music results.
  3. Perturbation stress test: Introduce controlled typos in sensitive attribute terms following Figure 5 methodology; measure whether fairness metrics degrade and by how much for each model.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent do the fairness disparities identified in GPT-4o and Gemini 1.5 Flash generalize to open-source models (e.g., LLaMA) or other proprietary architectures (e.g., Claude, DeepSeek)?
- **Open Question 2:** Which specific fairness-aware prompt optimization strategies can effectively mitigate the personality-linked biases detected by FairEval?
- **Open Question 3:** How does FairEval's prompt-based personality simulation compare to or integrate with established psychological frameworks like the "Big Five" model?

## Limitations
- Findings are constrained by narrow domain focus (movies/music) and limited model scope (two LLMs).
- Reliance on synthetic prompts may not capture real-world recommendation scenarios with nuanced user queries.
- High variance in some metrics raises questions about whether observed disparities reflect systematic bias or measurement noise.

## Confidence
- **High confidence:** Framework's methodological validity for measuring prompt-based recommendation divergence, and observation that fairness varies significantly across models and attributes.
- **Medium confidence:** Interpretation of SNSR/SNSV values as evidence of systematic bias, given potential confounding factors like model stochasticity and prompt sensitivity.
- **Low confidence:** Generalizability of findings to other recommendation domains and whether PAFS@25 meaningfully captures personality-aware fairness versus mere recommendation instability.

## Next Checks
1. **Replication with additional domains:** Apply FairEval to non-media recommendation domains (e.g., news articles, products) to test whether fairness disparities persist across contexts.
2. **Ablation study on prompt sensitivity:** Systematically vary prompt phrasing while holding preferences constant to quantify how much similarity variance is due to language versus identity effects.
3. **Bias mitigation evaluation:** Test whether explicit fairness instructions to LLMs (e.g., "recommend without demographic bias") reduce SNSR/SNSV values, validating whether observed disparities are mitigable.