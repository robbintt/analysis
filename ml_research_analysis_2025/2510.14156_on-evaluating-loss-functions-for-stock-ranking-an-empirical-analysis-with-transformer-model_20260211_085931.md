---
ver: rpa2
title: 'On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With
  Transformer Model'
arxiv_id: '2510.14156'
source_url: https://arxiv.org/abs/2510.14156
tags:
- loss
- stock
- ranking
- functions
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how different loss functions affect a Transformer\
  \ model\u2019s ability to rank stocks for portfolio selection. The authors systematically\
  \ compare pointwise, pairwise, and listwise losses on S&P 500 data using a PortfolioMASTER\
  \ model."
---

# On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model

## Quick Facts
- arXiv ID: 2510.14156
- Source URL: https://arxiv.org/abs/2510.14156
- Authors: Jan Kwiatkowski; Jarosław A. Chudziak
- Reference count: 12
- Primary result: Ranking-specific losses (Margin, ListNet, BPR) outperform MSE baseline in risk-adjusted returns (Sharpe ratios up to 0.75) for Transformer-based stock ranking

## Executive Summary
This paper systematically evaluates how different loss functions affect a Transformer model's ability to rank stocks for portfolio selection. The authors compare pointwise, pairwise, and listwise losses on S&P 500 data using a PortfolioMASTER model. Their experiments reveal that ranking-specific losses like Margin, ListNet, and BPR outperform the standard MSE baseline in risk-adjusted returns (Sharpe ratios up to 0.75) and drawdown control. Surprisingly, predictive ranking metrics (e.g., Spearman IC) did not always align with portfolio performance, highlighting the importance of loss function design in translating signals into profitable trades.

## Method Summary
The study uses a PortfolioMASTER architecture with alternating temporal and spatial self-attention blocks to predict next-day returns for 110 S&P 500 stocks. The model takes 20-day lookback windows of returns and turnover as inputs, predicting scores for top-k=5 stock selection with daily rebalancing. Eight loss functions are compared: MSE, Hinge, Margin, BPR, RankNet, WHR1, WHR2, and ListNet. Portfolio performance is evaluated using Sharpe ratio (4.3% risk-free rate), Annualized Return, and Maximum Drawdown, while predictive ranking is measured by Spearman IC, ICIR, and Precision@5.

## Key Results
- Margin loss achieved highest Annualized Return (16.23%) and Sharpe Ratio (0.7529)
- ListNet delivered lowest Annualized Volatility (15.79%) while maintaining strong returns (16.00%) and Sharpe (0.7407)
- BPR loss showed lowest Maximum Drawdown (-15.77%)
- Spearman IC scores hovered around 0.073-0.077, but highest IC did not correspond to highest Sharpe ratio

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pairwise margin enforcement improves top-k portfolio selection by encouraging confident distinctions between stocks.
- **Mechanism:** The Margin loss explicitly penalizes pairs where the predicted score difference falls within a margin `m` of the true ranking. This forces the model to push predicted scores further apart for correctly ordered pairs, potentially reducing ambiguity at the decision boundary where top-k selection occurs.
- **Core assumption:** Confident separation in predicted scores translates to more reliable top-k stock identification.
- **Evidence anchors:**
  - [abstract] "ranking-specific losses like Margin, ListNet, and BPR outperform the standard MSE baseline in risk-adjusted returns (Sharpe ratios up to 0.75)"
  - [section 5] "Margin loss achieved the highest Annualized Return (16.23%) and Sharpe Ratio (0.7529)... The Margin loss, by enforcing a margin for correctly ranked pairs, might encourage the model to make more confident distinctions between stocks"
  - [corpus] Neighbor paper "Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions" proposes profit-guided losses, suggesting alignment between loss design and trading objectives is an active research direction, though no direct comparison to margin-based approaches is available.
- **Break condition:** If the margin hyperparameter `m` is set too large, the model may overfit to confident separations that don't generalize, or if the asset universe has many stocks with nearly identical returns where confident separation is not possible.

### Mechanism 2
- **Claim:** Listwise optimization captures global ranking structure that benefits portfolio-level risk-return tradeoffs.
- **Mechanism:** ListNet applies softmax transformation to both predictions and targets with temperature τ, then minimizes cross-entropy between these probability distributions. This explicitly models the entire rank distribution rather than individual pairs, allowing the loss to account for relative importance across the full cross-section of stocks.
- **Core assumption:** The softmax probability distribution over stock returns encodes meaningful information about relative investment attractiveness.
- **Evidence anchors:**
  - [abstract] "ListNet... outperform the standard MSE baseline in risk-adjusted returns"
  - [section 5] "ListNet also performed strongly, with an AR of 16.00% and SR of 0.7407, along with the lowest Annualized Volatility (15.79%)... ListNet, by considering the entire list, may capture more global ranking patterns that are beneficial for portfolio construction"
  - [section 5] "The high test set loss (MSE) for ListNet is expected, as it does not directly optimize for the accuracy of return value predictions but rather for the quality of the entire ranked list"
  - [corpus] Evidence is limited; neighbor papers focus on reinforcement learning or sentiment-based approaches rather than listwise ranking losses specifically.
- **Break condition:** If temperature τ is poorly calibrated (too low → overconfident gradients on noisy signals; too high → uninformative uniform distribution), or if the cross-sectional return distribution has extreme outliers that dominate the softmax.

### Mechanism 3
- **Claim:** Predictive ranking quality (IC Spearman) does not guarantee portfolio performance; loss function design mediates the translation of signals to trades.
- **Mechanism:** The loss function shapes which errors the model prioritizes during training. Ranking metrics like Spearman IC treat all rank positions equally, but portfolio construction heavily weights the top-k boundary. Losses that emphasize correct ordering at the top of the distribution (Margin, BPR) or global list structure (ListNet) may better align training gradients with portfolio objectives even when overall ranking correlation is similar.
- **Core assumption:** Portfolio performance depends more on correct identification of extreme performers (top/bottom) than on average ranking quality across all positions.
- **Evidence anchors:**
  - [abstract] "predictive ranking metrics (e.g., Spearman IC) did not always align with portfolio performance"
  - [section 5] "RankNet achieved the highest IC Spearman (0.0767), yet its portfolio AR and SR were only moderate. Conversely, Margin and ListNet, which excelled in portfolio metrics, did not show markedly superior ICs scores"
  - [section 5] "IC Spearman hovered around 0.073-0.077, and P@5 was consistently near 0.358-0.359... this suggests that while a certain level of predictive ranking ability is necessary, the specific design of the loss function plays a crucial role"
  - [corpus] Weak direct evidence; neighbor papers do not explicitly investigate the IC-to-portfolio performance gap.
- **Break condition:** If portfolio construction uses a different selection rule (e.g., weighting by predicted return magnitude rather than rank), or if transaction costs dominate returns such that small differences in top-k quality are irrelevant.

## Foundational Learning

- **Concept: Learning-to-Rank (LTR) Paradigms (Pointwise, Pairwise, Listwise)**
  - **Why needed here:** The paper's central contribution is benchmarking these three paradigms for financial returns. Without understanding that pointwise optimizes individual predictions, pairwise optimizes relative ordering, and listwise optimizes global rank structure, the experimental results cannot be interpreted.
  - **Quick check question:** Given three stocks with returns [5%, 3%, -2%], would a pairwise loss penalize a model that predicts [4%, 3%, -1%] differently than a pointwise MSE loss?

- **Concept: Sharpe Ratio and Risk-Adjusted Returns**
  - **Why needed here:** The paper uses Sharpe ratio as the primary portfolio evaluation metric. Understanding that Sharpe = (return - risk_free) / volatility is essential to interpret why ListNet's lower volatility contributed to its strong Sharpe despite slightly lower returns than Margin.
  - **Quick check question:** If Model A achieves 20% return with 25% volatility and Model B achieves 15% return with 15% volatility (risk-free = 4.3%), which has the higher Sharpe ratio?

- **Concept: Transformer Attention for Cross-Sectional and Temporal Modeling**
  - **Why needed here:** The PortfolioMASTER architecture uses alternating temporal and spatial self-attention. Understanding that temporal attention captures each stock's history while spatial attention captures cross-stock relationships is necessary to reason about how loss functions interact with learned representations.
  - **Quick check question:** In a spatio-temporal Transformer for 110 stocks over 20 timesteps, what does the spatial attention layer compute at each timestep?

## Architecture Onboarding

- **Component map:**
  Input Layer (daily return + turnover) -> Projection (linear embedding) -> Positional Encoding -> Spatio-Temporal Encoder (alternating temporal and spatial self-attention) -> Temporal Aggregation (attention-based pooling) -> Output Head (linear decoder) -> Loss Computation (one of 8 loss functions) -> Portfolio Construction (top-k selection)

- **Critical path:**
  1. Input normalization (scalers fit on training set only)
  2. Loss function selection and hyperparameter tuning (λ, margin m, scale α, or temperature τ depending on loss)
  3. Model hyperparameter tuning (d_model, d_ff, dropout, learning rate)
  4. Training with early stopping on validation loss
  5. Top-k portfolio simulation on test set

- **Design tradeoffs:**
  - **Lookback window T=20:** Captures ~1 month of history; may miss longer-term regimes but reduces computational cost
  - **Top-k=5 selection:** Concentrated portfolio; higher alpha potential but higher idiosyncratic risk
  - **Equal weighting within top-k:** Ignores confidence signal; could be replaced with prediction-magnitude weighting
  - **Combined losses (MSE + pairwise):** λ parameter trades off absolute prediction accuracy vs. relative ordering; not explored in ablation
  - **110 stocks (10 per sector):** Sector-balanced universe reduces sector concentration risk but may exclude high-performing small-caps

- **Failure signatures:**
  - **High test MSE with good Sharpe:** Expected for ListNet (optimizes ranking, not point prediction) — not a failure
  - **High IC Spearman but low Sharpe:** Model captures average ranking but makes errors at top-k boundary — consider loss functions that emphasize top ranks (WHR1/WHR2, BPR)
  - **High drawdown despite good Sharpe:** Model may be systematically wrong during volatility spikes — examine BPR which showed lowest MDD (-15.77%)
  - **Training instability with pairwise losses:** Large number of pairs (N²) can cause gradient variance — check normalization by |P_valid|

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train PortfolioMASTER with MSE vs. Margin loss on S&P 500 subset; verify that Margin achieves higher Sharpe ratio. Key hyperparameters: λ=0.5 for combined loss, margin m from {0.01, 0.05, 0.1}.
  2. **IC-to-portfolio ablation:** For each loss function, plot IC Spearman vs. Sharpe ratio across multiple random seeds. Confirm the paper's finding that highest IC does not yield highest Sharpe.
  3. **Top-k sensitivity analysis:** Re-evaluate Margin and ListNet with k={3, 5, 10, 20}. Test whether Margin's advantage persists across portfolio concentrations or if ListNet's global optimization becomes more valuable at larger k.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the superiority of ranking-specific losses be validated on the full S&P 500 index rather than a subset?
  - **Basis in paper:** [explicit] The authors state future work should validate findings on a "larger dataset, such as the full list of S&P 500 stocks."
  - **Why unresolved:** The current study limited the universe to 110 stocks (top 10 per sector) to manage complexity.
  - **What evidence would resolve it:** Consistent performance of Margin/ListNet losses when applied to the entire 500-stock universe.

- **Open Question 2:** How can the weighting parameter ($\lambda$) in combined loss functions be optimized automatically?
  - **Basis in paper:** [explicit] Future work includes "developing methods for the automated tuning of weights in combined loss functions."
  - **Why unresolved:** Current implementation relies on static grid search for the balance between MSE and pairwise components.
  - **What evidence would resolve it:** An adaptive algorithm that dynamically adjusts $\lambda$ to outperform static weighting.

- **Open Question 3:** Do these loss functions maintain their advantage under different market conditions or portfolio rules?
  - **Basis in paper:** [explicit] The authors suggest testing under "different market conditions or with different rules for building portfolios."
  - **Why unresolved:** The study focused on a specific 10-year period and a single top-k equal-weighted strategy.
  - **What evidence would resolve it:** Robust performance stability across distinct bull/bear cycles or long-short strategies.

## Limitations
- The study is constrained to a single market (S&P 500) and fixed top-k equal-weighted strategy, limiting generalizability to other asset classes and portfolio rules.
- The specific advantage of Margin and ListNet losses may depend on the chosen evaluation period and could vary with different market regimes.
- The Transformer architecture's contribution relative to simpler ranking models is not isolated, making it difficult to attribute gains specifically to the loss functions versus the model design.

## Confidence
- **IC-to-portfolio gap findings:** High confidence - multiple loss functions consistently show this pattern across experiments
- **Specific advantage of Margin/ListNet:** Medium confidence - may depend on evaluation period and market regime
- **Generalizability to other markets:** Low confidence - study is limited to S&P 500 subset with fixed methodology

## Next Checks
1. **Regime sensitivity test**: Re-run the analysis splitting the 10-year period into distinct market regimes (bull/bear/volatile) to verify whether Margin/ListNet advantages persist across different conditions.
2. **Cross-market generalization**: Apply the same experimental framework to non-US markets (e.g., emerging markets or different asset classes) to test the robustness of loss function rankings.
3. **Profit-guided loss comparison**: Implement the profit-guided loss functions referenced in the corpus literature and directly compare their portfolio performance against the ranking losses tested in this study.