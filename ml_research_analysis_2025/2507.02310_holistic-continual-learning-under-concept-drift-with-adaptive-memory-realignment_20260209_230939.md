---
ver: rpa2
title: Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment
arxiv_id: '2507.02310'
source_url: https://arxiv.org/abs/2507.02310
tags:
- drift
- learning
- concept
- classes
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a holistic continual learning framework that
  explicitly addresses concept drift in data streams. Unlike traditional approaches
  that assume static class distributions, the authors develop Adaptive Memory Realignment
  (AMR) - a lightweight mechanism that detects distributional shifts in recurring
  classes and selectively updates memory buffers by replacing outdated samples with
  newly collected instances.
---

# Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment

## Quick Facts
- arXiv ID: 2507.02310
- Source URL: https://arxiv.org/abs/2507.02310
- Authors: Alif Ashrafee; Jedrzej Kozal; Michal Wozniak; Bartosz Krawczyk
- Reference count: 40
- One-line primary result: AMR achieves accuracy comparable to Full Relearning while reducing computational cost and labeled sample requirements by orders of magnitude

## Executive Summary
This paper introduces Adaptive Memory Realignment (AMR), a holistic continual learning framework that explicitly addresses concept drift in data streams. Unlike traditional approaches assuming static class distributions, AMR detects distributional shifts in recurring classes and selectively updates memory buffers by replacing outdated samples with newly collected instances. The method is theoretically grounded, showing that gradient misalignment occurs when models train on outdated representations, and AMR optimally realigns gradients without requiring full retraining. Extensive experiments on four vision benchmarks demonstrate AMR's effectiveness across various buffer sizes and drift scenarios.

## Method Summary
AMR is a lightweight mechanism that augments rehearsal-based continual learning methods by detecting and responding to concept drift in recurring classes. The approach uses a Kolmogorov-Smirnov test on predictive entropy distributions to identify drifted classes, then selectively flushes outdated buffer samples and repopulates them with current instances. AMR is implemented on top of the Mammoth continual learning library using a ResNet-18 backbone and alibi-detect for drift detection with a pre-trained ImageNet ResNet-18. The method is evaluated on Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and Tiny-ImageNet-CD benchmarks with induced concept drift through image corruption.

## Key Results
- AMR maintains 92.48% accuracy on CIFAR10-CD versus Full Relearning's 90.76%, with significantly lower resource consumption
- The method consistently outperforms baselines across buffer sizes, achieving comparable accuracy to Full Relearning with reduced compute
- AMR demonstrates effectiveness across multiple benchmarks (Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, Tiny-ImageNet-CD) and drift scenarios

## Why This Works (Mechanism)

### Mechanism 1: Gradient Misalignment Correction via Targeted Buffer Replacement
- Claim: Replacing outdated samples of drifted classes in the replay buffer with new instances from the drifted distribution restores gradient alignment during training, preventing interference between old and new concept representations.
- Mechanism: When concept drift occurs, gradients from stale buffer samples diverge from those of the current distribution, causing inefficient parameter updates. AMR detects drifted classes and selectively flushes outdated buffer slots, repopulating them with updated samples to ensure gradient alignment.
- Core assumption: KS test on predictive entropy reliably detects distributional shifts, and new samples are representative of the drifted distribution.
- Evidence anchors: Theorem 1 establishes gradient interference, with alignment quantified; Section III-E1 proves gradients from old and new distributions interfere.

### Mechanism 2: Uncertainty-Based Drift Detection via KS Test
- Claim: Predictive entropy computed from class logits provides a distribution-sensitive signal that, when compared between reference and test samples using the Kolmogorov-Smirnov test, enables timely detection of representation-level concept drift.
- Mechanism: For each recurring class, AMR computes uncertainty U(x) = H(softmax(fθ(x))) for both buffer samples and incoming test samples. The KS statistic measures the maximum distance between their empirical CDFs, flagging drift if it exceeds threshold δ.
- Core assumption: Predictive entropy captures meaningful distributional shifts in feature space, and the KS threshold is appropriately calibrated.
- Evidence anchors: Section III-C formalizes the KS-based detection criterion; Figure 3 visualizes feature distribution collapse under drift.

### Mechanism 3: Buffer Size-Dependent Tradeoff Between Stability and Adaptation
- Claim: Larger replay buffers exacerbate the negative impact of concept drift by retaining more outdated representations, while smaller buffers partially mitigate drift through natural forgetting but suffer from reduced overall retention.
- Mechanism: Reservoir sampling naturally replaces buffer samples slowly. Larger buffers retain more stale samples per class, reinforcing outdated features during rehearsal. AMR's targeted replacement circumvents this by flushing all outdated class samples regardless of buffer size.
- Core assumption: The buffer is partitioned approximately equally across classes, and drifted classes reappear with sufficient samples to repopulate their buffer slots.
- Evidence anchors: Theorem 2 proves reservoir sampling's low probability of replacing all outdated samples; Tables I and II show larger buffers with AMR achieve comparable accuracy to FR.

## Foundational Learning

- **Rehearsal-based Continual Learning**
  - Why needed here: AMR is explicitly designed to augment rehearsal methods by modifying buffer management. Understanding how replay mitigates catastrophic forgetting is prerequisite.
  - Quick check question: Can you explain how storing past samples in a memory buffer and interleaving them with current task data helps retain knowledge?

- **Concept Drift in Data Streams**
  - Why needed here: The paper formalizes concept drift as distribution shift in recurring classes. Understanding drift types and detection is essential.
  - Quick check question: What is the difference between covariate shift and real concept drift, and how might each affect a classifier's decision boundary?

- **Statistical Hypothesis Testing for Drift Detection**
  - Why needed here: AMR uses the Kolmogorov-Smirnov test on uncertainty distributions. Familiarity with ECDFs, test statistics, and significance thresholds is required.
  - Quick check question: Given two empirical distributions, how does the KS statistic quantify their difference, and what does a p-value below a threshold indicate about the null hypothesis?

## Architecture Onboarding

- **Component map:**
  Backbone Model (fθ) -> Replay Buffer (M) -> Drift Detector Module -> AMR Buffer Manager -> Training Loop

- **Critical path:**
  1. At each task Ti, receive data Di containing new classes and potentially drifted recurring classes.
  2. For each recurring class, compute Uref from buffer and Utest from Di; run KS test.
  3. If drift detected, invoke AMR to flush and resample buffer slots for that class.
  4. Train model on Di ∪ M with rehearsal loss.
  5. Update buffer via reservoir sampling for new classes; AMR already handled drifted classes.

- **Design tradeoffs:**
  - Drift threshold (δ): Lower values increase sensitivity but may cause more false positives and buffer churn.
  - Buffer capacity (|M|): Larger buffers improve retention but require more AMR replacements under drift.
  - Detector backbone: Using a pretrained ImageNet ResNet-18 provides stable uncertainty estimates but may not align perfectly with the trained model's representations.

- **Failure signatures:**
  - Excessive buffer churn: Rapid drift detections causing frequent flushes → performance drop on non-drifted classes.
  - Stagnant performance on recurring classes: Drift not detected → accuracy on drifted classes remains low after adaptation.
  - Empty buffer slots: Insufficient new samples for drifted classes → buffer underflow warnings during training.

- **First 3 experiments:**
  1. Validate gradient alignment empirically: Measure cosine similarity between gradients from old buffer samples and new task samples for drifted classes, before and after AMR replacement.
  2. Ablate drift detector: Replace KS test with random drift labeling or fixed threshold; compare final accuracy and forgetting.
  3. Scale buffer size and drift frequency: Run AMR on CIFAR100-CD with buffer sizes {500, 2000, 5000} and drift counts {2, 4}; analyze FAA vs. buffer size.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims about gradient misalignment lack empirical validation of the gradient alignment metric during training.
- The KS-based drift detector's sensitivity to noise and calibration across datasets remains unverified.
- The paper does not address computational overhead of running the frozen ResNet-18 backbone for drift detection on every sample.

## Confidence

- **High Confidence:** The core mechanism of targeted buffer replacement (AMR) and its integration with rehearsal baselines is well-specified and theoretically grounded.
- **Medium Confidence:** The empirical results showing AMR's performance gains over baselines, though strong, rely on a specific concept drift implementation that may not generalize.
- **Low Confidence:** The robustness of the KS-based drift detector across different drift types, datasets, and noise levels, as well as the buffer size interaction effects in highly imbalanced settings.

## Next Checks

1. **Gradient Alignment Validation:** Measure cosine similarity between gradients from old buffer samples and new task samples for drifted classes, before and after AMR replacement, to empirically confirm Theorem 1.

2. **Drift Detector Ablation:** Replace the KS test with random drift labeling or a fixed threshold; compare final accuracy and forgetting to quantify the detector's contribution.

3. **Cross-Dataset Drift Robustness:** Evaluate AMR on CIFAR10-CD with varying drift severities (1-5) and types (Gaussian noise, rotation) to assess the KS detector's calibration and AMR's stability.