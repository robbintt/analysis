---
ver: rpa2
title: On Corruption-Robustness in Performative Reinforcement Learning
arxiv_id: '2505.05609'
source_url: https://arxiv.org/abs/2505.05609
tags:
- robust
- have
- gradient
- where
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses corruption-robustness in performative reinforcement\
  \ learning, where the reward and transition functions depend on the deployed policy.\
  \ The main contribution is extending prior performative RL approaches to handle\
  \ corrupted data under Huber's \u03F5-contamination model."
---

# On Corruption-Robustness in Performative Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.05609
- **Source URL:** https://arxiv.org/abs/2505.05609
- **Reference count:** 40
- **Primary result:** Robust gradient estimation in performative RL converges to an approximately stable policy with error linear in $\sqrt{\epsilon}$.

## Executive Summary
This paper introduces a framework for corruption-robust performative reinforcement learning where both rewards and transitions depend on the deployed policy. The authors extend standard performative RL approaches to handle Huber's $\epsilon$-contamination model, where a fraction $\epsilon$ of the data is adversarially corrupted. They propose a repeated retraining approach using robust convex-concave optimization with a novel coordinate-wise robust mean estimator for gradient estimation. The theoretical analysis shows convergence to an approximately stable policy with approximation error scaling as $\sqrt{\epsilon}$, and experiments on a gridworld environment demonstrate significant improvements over naive averaging under data corruption.

## Method Summary
The paper addresses corruption-robustness in performative RL by extending standard performative RL approaches to handle corrupted data under Huber's $\epsilon$-contamination model. The main contribution is a repeated retraining approach based on robust convex-concave optimization and a novel coordinate-wise robust mean estimator for gradient estimation. The algorithm operates by collecting trajectories from the current policy, applying robust gradient estimation to handle corrupted data, and using Robust OFTRL to update occupancy measures and dual variables. Under sensitivity assumptions, this process converges to an approximately stable policy with error linear in $\sqrt{\epsilon}$.

## Key Results
- Coordinate-wise robust gradient estimation filters unbounded adversarial noise while preserving the signal required for policy updates
- Robust OFTRL converges to an approximate saddle point despite inexact gradient oracles
- Repeated retraining converges to an approximately stable policy via a contraction mapping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Coordinate-wise robust gradient estimation filters unbounded adversarial noise while preserving the signal required for policy updates.
- **Mechanism:** The algorithm computes gradients w.r.t. the occupancy measure ($g_d$) and dual variables ($g_h$). For $g_d$, instead of averaging all samples, it applies a coordinate-wise median filter followed by averaging the closest $(1-\epsilon)$ fraction of data points. Because the contamination fraction $\epsilon < 0.5$, the median is guaranteed to originate from the uncorrupted distribution, bounding the estimation error even if the noise magnitude is infinite.
- **Core assumption:** The corruption level $\epsilon < 0.5$ and clean gradient samples are bounded.
- **Evidence anchors:** Theorem 3 establishes that the estimation error scales with $\sqrt{\epsilon}$ and sample complexity, rather than the noise magnitude.

### Mechanism 2
- **Claim:** Optimistic Follow the Regularized Leader (OFTRL) converges to an approximate saddle point despite inexact gradient oracles.
- **Mechanism:** The framework models RL as a convex-concave min-max problem (Lagrangian). Standard OFTRL updates parameters by predicting the next gradient. This paper introduces a "Robust OFTRL" variant that incorporates the error terms ($\zeta$) from the robust estimator. By bounding the cumulative sum of these gradient errors, the algorithm maintains an $O(1/T)$ convergence rate to a neighborhood of the saddle point, where the neighborhood size depends on the gradient estimation quality.
- **Core assumption:** The objective function is smooth and the domains for occupancy measures ($d$) and dual variables ($h$) are bounded.
- **Evidence anchors:** Theorem 1 bounds the duality gap, showing explicit additive terms for the gradient errors.

### Mechanism 3
- **Claim:** Repeated retraining converges to an approximately stable policy via a contraction mapping.
- **Mechanism:** In performative RL, deploying a policy changes the environment. Under sensitivity assumptions (Lipschitz continuity of rewards/transitions w.r.t. occupancy measure), the policy optimization operator is a contraction. The paper proves that using the Robust OFTRL output as an "approximate operator" still yields a contraction, provided regularization $\lambda$ is sufficiently high. This forces the sequence of policies to converge to a specific region (approximate stability) rather than diverging due to the coupled performative shift and data corruption.
- **Core assumption:** $\tilde{\epsilon}$-sensitivity (Lipschitz conditions on reward/transition) and high regularization $\lambda$.
- **Evidence anchors:** Theorem 5 provides the distance bound to the stable policy $\tilde{d}_S$, relying on the contraction property derived in prior work but extended for the approximate case.

## Foundational Learning

- **Concept:** Performative Stability vs. Optimality
  - **Why needed here:** The paper targets *stability* (finding a policy that is optimal for the environment it induces) rather than *optimality* (finding the best policy globally). You must understand that convergence here means the system stops shifting, not necessarily that it achieves the highest possible theoretical reward.
  - **Quick check question:** Does the algorithm seek the policy with the highest possible return, or a policy where the environment stops changing in response to the policy?

- **Concept:** Huber's $\epsilon$-Contamination Model
  - **Why needed here:** This defines the threat model. It assumes a fixed fraction $\epsilon$ of data is garbage/adversarial, while the rest is clean. Understanding this is critical because the robust estimator relies specifically on the structure that *less than half* the data is corrupt.
  - **Quick check question:** If an attacker can corrupt 60% of the data, will the coordinate-wise median estimator still function correctly?

- **Concept:** Lagrangian Relaxation in RL
  - **Why needed here:** The paper solves the RL problem via a convex-concave saddle point problem using Lagrangian multipliers ($h$) to enforce Bellman flow constraints. One needs to grasp that we are optimizing two sets of variables (occupancy $d$ and duals $h$) simultaneously.
  - **Quick check question:** In the min-max formulation, is the agent minimizing or maximizing with respect to the dual variables $h$?

## Architecture Onboarding

- **Component map:** Data Generator -> Adversarial Corruptor -> Robust Gradient Estimator -> Robust OFTRL Solver -> Policy Extractor
- **Critical path:** The **Robust Gradient Estimator** is the bottleneck for robustness. If the batch size per OFTRL iteration ($m/2T$) is too small, the statistical error of the median estimator dominates, and the optimization fails to converge.
- **Design tradeoffs:**
  - **Batch Splitting:** The paper splits data into $2T$ batches. Larger $T$ (more optimization steps) improves convergence rate but reduces samples per batch, increasing gradient variance.
  - **Regularization $\lambda$:** High $\lambda$ ensures contraction (convergence) but may bias the final policy toward uniformity, reducing final performance.
- **Failure signatures:**
  - **Oscillation:** If the corruption magnitude $Z$ is extremely high and filtering fails, gradient norms explode.
  - **Divergence:** If $\epsilon \ge 0.5$, the median filter may select corrupted data as the "center," causing the policy to optimize for the adversarial noise rather than the true environment.
- **First 3 experiments:**
  1. **Varying Noise Magnitude ($Z$):** Replicate Figure 1a vs 1b. Fix $\epsilon=0.01$ and compare naive averaging vs. robust estimation as $Z$ increases. Expectation: Naive error scales with $Z$; Robust stays flat.
  2. **Varying Corruption Fraction ($\epsilon$):** Replicate Figure 1c vs 1d. Fix $Z$ and vary $\epsilon$. Expectation: Both algorithms degrade, but Robust degrades gracefully (error $\propto \sqrt{\epsilon}$) while Naive diverges rapidly.
  3. **Sample Complexity Test:** Run the algorithm with varying total sample sizes $m$ to verify the $\sqrt{\tilde{m}}$ dependence in the estimation error bound (Theorem 3).

## Open Questions the Paper Calls Out

- **Open Question 1:** Is the approximation error scaling of $O(\sqrt{\epsilon})$ tight, or can a different algorithm achieve a linear dependence on $\epsilon$?
  - **Basis in paper:** Conclusion section: "One of the most interesting future research directions is to investigate the tightness of the approximation error."
  - **Why unresolved:** The authors prove an upper bound linear in $\sqrt{\epsilon}$ but do not provide a matching lower bound to confirm optimality.
  - **What evidence would resolve it:** A formal lower bound proof showing $\sqrt{\epsilon}$ is necessary, or a modified algorithm proven to converge with error $O(\epsilon)$.

- **Open Question 2:** Can the corruption-robust performative RL framework be extended to settings with function approximation?
  - **Basis in paper:** Conclusion section: "Extending this work to RL settings with function approximation is another important avenue for future research."
  - **Why unresolved:** The current analysis relies on coordinate-wise robust estimators suitable for the tabular setting, which do not directly apply to high-dimensional function approximation.
  - **What evidence would resolve it:** Theoretical convergence guarantees for a robust repeated retraining algorithm using generic function classes.

- **Open Question 3:** Does the Lagrangian of the regularized RL objective satisfy the Saddle-Point Metric Subregularity (SP-MS) condition?
  - **Basis in paper:** Appendix section "Special Classes of Smooth Convex-Concave Objectives": "...we leave this as an open question."
  - **Why unresolved:** If the SP-MS condition holds, it would imply exponential last-iterate convergence for specific algorithms like Robust OMDA, but proving this property for the specific Lagrangian structure remains undone.
  - **What evidence would resolve it:** A formal proof verifying that the Lagrangian satisfies SP-MS with $\beta=0$.

- **Open Question 4:** How does the algorithm's convergence change if the state-action pairs $(s, a)$ are also subject to adversarial corruption?
  - **Basis in paper:** The paper notes the assumption that $s_i$ and $a_i$ are uncorrupted is "made for technical simplicity" and moves the discussion of avoiding this assumption to the appendix.
  - **Why unresolved:** The primary theoretical results rely on the current contamination model; the suggested sketch to handle corrupted state-actions lacks the rigorous convergence analysis provided for the main setting.
  - **What evidence would resolve it:** A complete convergence proof under a contamination model that includes state-action pairs.

## Limitations

- The theoretical guarantees hinge critically on the assumption that $\epsilon < 0.5$ and that the gradient noise is bounded, with behavior in the high-corruption regime ($\epsilon \geq 0.5$) not addressed.
- The gridworld experiments use a simplified corruption model; real-world RL environments may have more complex noise structures that could break the assumptions.
- The convergence to an approximately stable policy (not optimal policy) means the final performance may be suboptimal compared to uncorrupted settings.

## Confidence

- **High Confidence:** The theoretical framework for robust OFTRL and its convergence to an approximate saddle point is well-established. Theorem 1's proof structure follows standard optimization theory.
- **Medium Confidence:** The coordinate-wise robust gradient estimation analysis (Theorem 3) is sound for $\epsilon < 0.5$, but the practical performance depends heavily on batch size and dimensionality. The claim that the method scales gracefully with $\sqrt{\epsilon}$ is supported by experiments but needs more systematic validation.
- **Medium Confidence:** The repeated retraining convergence (Theorem 5) relies on the contraction mapping argument from prior work. While the extension to the approximate case is reasonable, the sensitivity to the regularization parameter $\lambda$ is not thoroughly explored experimentally.

## Next Checks

1. **Regime Test:** Systematically evaluate the algorithm with $\epsilon \in \{0.1, 0.3, 0.45, 0.49, 0.51\}$ to observe the transition point where the robust estimator fails. Measure both convergence rate and final policy performance.
2. **Batch Size Sensitivity:** Vary the inner loop batch size $m/2T$ across a wide range (e.g., $10, 100, 1000$) and measure the gradient estimation error. Plot error vs. batch size to confirm the theoretical $\sqrt{m}$ scaling.
3. **Adversarial Noise Test:** Replace the Gaussian corruption with an adversarial noise model where the corrupted samples are selected to maximize gradient error. Assess whether the robust estimator maintains its guarantees under this worst-case scenario.