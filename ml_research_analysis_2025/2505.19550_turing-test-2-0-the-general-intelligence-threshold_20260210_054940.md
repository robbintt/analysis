---
ver: rpa2
title: 'Turing Test 2.0: The General Intelligence Threshold'
arxiv_id: '2505.19550'
source_url: https://arxiv.org/abs/2505.19550
tags:
- test
- system
- information
- intelligence
- turing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new definition of general intelligence (G.I.)
  and introduces the General Intelligence Threshold (G.I.T.) as a concrete metric
  to determine if a system has achieved A.G.I. The core method is based on distinguishing
  between functional information (F.I.) and non-functional information (N.F.I.) within
  a system, with G.I.
---

# Turing Test 2.0: The General Intelligence Threshold

## Quick Facts
- arXiv ID: 2505.19550
- Source URL: https://arxiv.org/abs/2505.19550
- Reference count: 0
- Primary result: Modern LLMs cannot generate new Functional Information from Non-Functional Information to achieve General Intelligence

## Executive Summary
This paper proposes a new definition of general intelligence (G.I.) based on the ability to generate new Functional Information (F.I.) from Non-Functional Information (N.F.I.) without external input. The paper introduces the Turing Test 2.0 framework with three rules for constructing tests that can reliably detect if a system has achieved A.G.I. The framework is tested against popular LLMs using two tasks: generating images of clocks showing 6:30 and hexagon-shaped stop signs. All tested LLMs failed to consistently generate correct images even after multiple attempts and feedback, demonstrating they cannot generate new F.I. from their own responses.

## Method Summary
The paper proposes evaluating whether AI systems can generate new Functional Information (F.I.) from Non-Functional Information (N.F.I.) using the Turing Test 2.0 framework. The method involves: (1) identifying a task where the required information exists in N.F.I. but not in F.I., (2) constructing tests where the system must bridge this gap without external input beyond pass/fail feedback, and (3) evaluating consistent generation across multiple trials. The paper tests four LLMs (ChatGPT, xAI, Gemini, and MetaAI) on image generation tasks requiring synthesis of knowledge not explicitly present in training data.

## Key Results
- All tested LLMs failed to consistently generate images of clocks showing 6:30
- All tested LLMs failed to generate hexagonal stop signs
- LLMs could describe the required knowledge verbally but could not execute it functionally
- The inability to generate new F.I. from N.F.I. demonstrates current models lack G.I.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** General Intelligence (G.I.) is defined as the capability to generate new Functional Information (F.I.) from Non-Functional Information (N.F.I.) without external input.
- **Mechanism:** A system operates in a "generating state" where it processes N.F.I. (data it possesses but cannot yet utilize for a task) and extracts new patterns or rules, converting them into F.I. This new F.I. permanently increases the system's functional capacity, allowing it to solve tasks that were previously impossible.
- **Core assumption:** Intelligence necessitates an increase in the total Functional Information of a system; relying solely on information transferred from a "teacher" (training data) constitutes information processing, not generation.
- **Evidence anchors:**
  - [abstract] Defines AGI as the ability to "generate new functional information (FI) from non-functional information (NFI)... enabling the completion of previously impossible tasks."
  - [section 2] Visualizes the "generating state" (Figure 3) where a system extracts new F.I. ($i$) from N.F.I. ($N$) to gain new functionality.
  - [corpus] "Optimisation Is Not What You Need" supports the distinction by arguing that current optimization-based AI differs fundamentally from cognition.
- **Break condition:** If a system relies exclusively on a "training state" (transferring existing F.I. from a teacher) or "static state," it has not achieved G.I.

### Mechanism 2
- **Claim:** Current Large Language Models (LLMs) fail the General Intelligence Threshold (G.I.T.) because they cannot consistently translate their accessible N.F.I. (verbal descriptions) into executable F.I. (functional generation).
- **Mechanism:** The paper leverages the "Chinese Room Argument" to demonstrate that LLMs may possess N.F.I. (e.g., they can describe how a clock works) but lack the internal mechanism to convert this knowledge into functional output (e.g., generating an image of a clock showing a specific time) if that specific pattern was absent from the original F.I. (training data).
- **Core assumption:** The inability to perform a task based on verbally demonstrated knowledge proves the knowledge is "non-functional" to the model, implying it cannot "understand" or "generate" new F.I. from its own representations.
- **Evidence anchors:**
  - [section 3.2] Notes that while LLMs could describe clock mechanics (N.F.I.), they failed to generate images of clocks at non-standard times (F.I.), failing to pass the G.I.T.
  - [section 1.3] Uses the Chinese Room argument to categorize information the system manipulates blindly as N.F.I. vs. instructions it understands as F.I.
  - [corpus] "Common Sense Is All You Need" highlights the lack of basic cognitive "common sense" in current models, aligning with the paper's findings on functional gaps.
- **Break condition:** If a model successfully completes a task requiring the synthesis of knowledge not explicitly present in training data, it may have passed the threshold (though the paper notes current models fail this).

### Mechanism 3
- **Claim:** The Turing Test 2.0 framework creates a verifiable threshold by isolating a "gap" between what a system knows (N.F.I.) and what it can do (F.I.).
- **Mechanism:** The test constructs a task ($T$) such that the necessary information ($i$) is provably within the system's N.F.I. (rule i) but provably absent from its F.I. (rule ii). The system must bridge this gap without external feedback (rule iii) to pass.
- **Core assumption:** It is possible to effectively bound the F.I. of a system (training data) to ensure the specific task solution was not pre-existing.
- **Evidence anchors:**
  - [section 2.3] Defines the three rules of the Turing Test 2.0: $i \subseteq N$, $i \cap F = \emptyset$, and no external info other than fail-pass feedback.
  - [abstract] Proposes the framework to "construct tests that can reliably detect if a system has achieved AGI."
  - [corpus] "Brain-Model Evaluations Need the NeuroAI Turing Test" suggests a need for tests looking beyond behavior to internal mechanisms, consistent with the 2.0 approach.
- **Break condition:** If the specific task information ($i$) is found to have been in the training data (F.I.), the test is invalid (false positive).

## Foundational Learning

- **Concept:** **Functional Information (F.I.) vs. Non-Functional Information (N.F.I.)**
  - **Why needed here:** This is the foundational taxonomy of the paper. You cannot evaluate the General Intelligence Threshold without distinguishing between data a system *has* (N.F.I.) and data a system can *use to act* (F.I.).
  - **Quick check question:** If a system has a textbook on chemistry in its database but cannot perform a chemical reaction, is the textbook F.I. or N.F.I.?

- **Concept:** **The Generating State**
  - **Why needed here:** This is the proposed definition of AGI. Engineers must understand that AGI is not defined by passing tests (static/training states) but by the autonomous *creation* of new functional capabilities (generating state).
  - **Quick check question:** Does adding more training data move a system into the "generating state," or just the "training state"?

- **Concept:** **Model Collapse (Information Stagnation)**
  - **Why needed here:** The paper references this phenomenon (from Section 3.4 and related work) to argue why recursive training without generation degrades intelligence. It explains the physical limit of systems that lack G.I.
  - **Quick check question:** Why does total F.I. decrease over time if a system only learns from itself without external reality or generation capabilities?

## Architecture Onboarding

- **Component map:** System ($M$) -> F.I. Set ($F$) -> N.F.I. Set ($N$) -> The Generator
- **Critical path:** Identify Task $T$ → Verify information for $T$ is in $N$ (System can describe it) → Verify information for $T$ is NOT in $F$ (System cannot yet do it) → Run test → Check for consistent generation of $F$
- **Design tradeoffs:**
  - *Test Validity vs. Feasibility:* Designing tests where you are 100% certain the solution is absent from $F$ (training data) is difficult with massive, opaque models (Section 3.1 methodology)
  - *Advanced Tests:* The "Single Discipline" test restricts training data for certainty but limits scope. The "Generational" test (recursive training) is rigorous but computationally expensive and risks model collapse
- **Failure signatures:**
  - **The Hallucination:** Generating noise or random outputs because the link between $N$ and $F$ is broken
  - **The Chinese Room:** Accurately describing the solution process verbally (N.F.I. present) but outputting an incorrect visual/functional result (F.I. absent)
  - **Inconsistency:** Solving the task once (lucky guess) but failing to retain the F.I. for future attempts (Section 2.3)
- **First 3 experiments:**
  1. **The Clock Test:** Request the model to generate an image of an analog clock showing "6:30" (a rare configuration in training data) to verify if it can convert verbal knowledge of clock mechanics into visual F.I.
  2. **The Hexagonal Stop Sign:** Ask the model to generate a hexagonal stop sign to test if it can modify a strongly embedded concept (octagon) based on abstract geometric rules it verbally understands
  3. **The Generational Test:** Train a "Gen 2" model exclusively on data generated by a "Gen 1" model; observe if information stagnation (model collapse) occurs, proving the absence of the generating state

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an AI model trained exclusively on a bounded historical dataset (e.g., music prior to 1800) generate novel Functional Information (FI) distinct from that era (e.g., Jazz) without external input?
- **Basis in paper:** [explicit] The author proposes the "Single Discipline Test" in Section 3.4 as an advanced method to verify AGI, contrasting it with current models that likely rely on massive, uncurated datasets.
- **Why unresolved:** The paper establishes the framework but does not execute this experiment. It remains a theoretical proposal for future evaluation to bypass the limitations of current web-scraped training data.
- **What evidence would resolve it:** An experiment where an AI, trained only on data from a specific historical period, produces an output that creates a new functional paradigm (novel FI) not present in its training set.

### Open Question 2
- **Question:** Can recursive generative training (Generational Test) result in a net increase of Functional Information across generations rather than model collapse?
- **Basis in paper:** [explicit] The author describes the "Generational Test" in Section 3.4, noting that current attempts at recursive training lead to information stagnation (model collapse), but leaving the possibility open for future models to succeed.
- **Why unresolved:** Current literature cited in the paper (e.g., Shumailov et al.) shows that recursive training degrades information. No model has yet demonstrated the ability to expand its FI set autonomously over generations.
- **What evidence would resolve it:** A lineage of models ($G_1$ through $G_n$) where $G_n$ can solve a task that $G_1$ could not, using only data generated by the previous generation.

### Open Question 3
- **Question:** How can the condition that new information ($i$) is absent from the initial Functional Information set ($F$) be rigorously verified for standard tasks without relying on probabilistic assumptions?
- **Basis in paper:** [inferred] The author acknowledges in Section 3.1 and 3.2 that verifying rule #2 ($i \cap F = \emptyset$) is difficult and relies on assumptions about "rare" images (e.g., hexagonal signs) or aesthetic biases (clocks at 10:10) in public data.
- **Why unresolved:** The paper relies on heuristic evidence (web search results) to argue that specific configurations are missing from training data. There is no formal method to audit a closed-source model's training distribution to prove the absence of specific functional data.
- **What evidence would resolve it:** A formal auditing method or a synthetic training environment where the precise composition of $F$ is known and constrained, thereby validating the "absence" of the test information $i$ deterministically.

## Limitations
- The binary pass/fail metric may oversimplify the continuous nature of capability development
- Verifying that test information is absent from training data is difficult with massive, opaque datasets
- The framework assumes functional/non-functional information can be cleanly partitioned in these systems
- The methodology for confirming absence of test patterns from training data has inherent limitations

## Confidence
- **High confidence:** The paper's conceptual framework distinguishing between functional and non-functional information, and its use of the Chinese Room argument to illustrate the gap between verbal knowledge and functional execution
- **Medium confidence:** The empirical finding that tested LLMs failed specific image generation tasks, as the methodology for confirming absence of test patterns from training data has inherent limitations
- **Low confidence:** The broader conclusion that current LLMs fundamentally cannot achieve G.I., as this extrapolates from specific task failures to a general capability limitation without addressing potential architectural or methodological advances

## Next Checks
1. **Cross-validation with alternative test paradigms:** Apply the Turing Test 2.0 framework to non-image tasks (e.g., novel mathematical reasoning, creative problem-solving) to verify if failure patterns generalize beyond visual generation
2. **Training data verification study:** Conduct a systematic analysis of whether the specific clock positions and hexagonal stop sign patterns actually exist in the training corpora of tested models, using dataset inspection tools
3. **Incremental capability assessment:** Design a series of progressively more complex tasks that build from simple N.F.I. → F.I. transformations to assess whether LLMs show any capacity for generating functional information, even if currently limited