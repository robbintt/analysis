---
ver: rpa2
title: Shapelets-Enriched Selective Forecasting using Time Series Foundation Models
arxiv_id: '2601.11821'
source_url: https://arxiv.org/abs/2601.11821
tags:
- time
- series
- shapelets
- forecasting
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limitation of time series foundation\
  \ models (TSFMs) in providing reliable predictions for specific data segments characterized\
  \ by unique or abrupt trends. The authors propose a selective forecasting framework\
  \ that identifies and discards unreliable predictions using shapelets\u2014representative\
  \ time series subsequences learned from high-error regions of validation data."
---

# Shapelets-Enriched Selective Forecasting using Time Series Foundation Models

## Quick Facts
- arXiv ID: 2601.11821
- Source URL: https://arxiv.org/abs/2601.11821
- Reference count: 8
- Reduces MSE by 22.17% (zero-shot) and 22.62% (fine-tuned) using shapelet-guided selective discarding

## Executive Summary
This paper addresses a key limitation of time series foundation models (TSFMs): their inability to reliably predict segments with unique or abrupt trends. The authors propose a selective forecasting framework that learns discriminative shapelets from high-error validation samples and uses distance-based similarity to identify and discard unreliable test predictions. By discarding the top 20% most similar samples to these shapelets, the approach achieves significant MSE reductions while maintaining interpretability through visualization of the learned shapelets.

## Method Summary
The framework operates by first predicting on validation data using a pre-trained TSFM (TTM-Base), then computing per-sample MSE and identifying high-error samples using a threshold τ = mean_error + 2×std. Shift-invariant dictionary learning is applied to these high-error validation samples to extract shapelets that capture error-correlated patterns. For test samples, z-normalized Euclidean distances to all shapelets are computed, and the closest dp% (default 20%) are discarded based on minimum distance ranking. MSE is then calculated on the remaining predictions and compared against baselines including random selection.

## Key Results
- Achieves 22.17% average MSE reduction for zero-shot models and 22.62% for fine-tuned models
- Outperforms random selection baselines by up to 21.43% in error reduction
- Demonstrates effectiveness across six benchmark datasets (ETTh1, ETTh2, ETTm1, ETTm2, Exchange Rate, Traffic)
- Shows monotonic MSE decrease as discard percentage increases from 10% to 50%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-error regions in TSFM predictions exhibit learnable local patterns that can be extracted as shapelets.
- Mechanism: Shift-invariant dictionary learning identifies representative subsequences from validation samples where model error exceeds threshold τ = mean_error + δ×std. These shapelets capture patterns correlated with unreliable predictions.
- Core assumption: Patterns causing high prediction error on validation data generalize to similar patterns in test data (distribution stability assumption).
- Evidence anchors:
  - [abstract] "We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset."
  - [section] "Shapelets are then learned from these curated high error-causing validation samples." (Page 2)
  - [corpus] Limited direct corpus support; related work (Less is More) discusses TSFM specialization gaps but not shapelet-based error detection.
- Break condition: Distribution shift between validation and test sets renders learned shapelets unrepresentative; authors note this limitation: "test samples leading to high error peaks at the start and end of January 2018 do not get discarded... highly likely due to distribution shift" (Page 8).

### Mechanism 2
- Claim: Distance-based similarity matching between test samples and learned shapelets enables identification of high-risk predictions.
- Mechanism: Z-normalized Euclidean distance computation yields a distance matrix; samples with smallest distances to any shapelet are flagged for discard based on user-defined drop percentage.
- Core assumption: Lower distance to error-correlated shapelets implies higher prediction risk.
- Evidence anchors:
  - [abstract] "Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions."
  - [section] "We then use z-normalized euclidean distance-based similarity matching on the test data samples" (Page 3); Figure 4 shows distance=4.46 for discarded sample vs. 7.44 for retained sample.
  - [corpus] No direct corpus precedent for shapelet-based selective forecasting in TSFMs.
- Break condition: Shapelets are too generic (low specificity) or too narrow (overfit to validation quirks), causing false positives/negatives in selection.

### Mechanism 3
- Claim: Selective discarding of predictions reduces aggregate MSE proportionally to drop percentage while providing interpretable rationale.
- Mechanism: Discarded samples contribute zero error to aggregate metric; MSE decreases monotonically as drop percentage increases (Table 4 shows consistent reduction from 10% to 50% drops).
- Core assumption: The utility of coverage loss is outweighed by error reduction; user can tolerate missing predictions for flagged segments.
- Evidence anchors:
  - [abstract] "reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model."
  - [section] Table 2 shows MSE reduction across 4/6 datasets; Appendix Table 4 confirms monotonic decrease with increasing drop percentage.
  - [corpus] Weak corpus support; neighbor papers focus on TSFM fine-tuning and pruning, not selective inference.
- Break condition: Application requires full coverage (e.g., safety-critical systems); discarding predictions is unacceptable regardless of error reduction.

## Foundational Learning

- Concept: **Shapelets (Time Series Subsequence Primitives)**
  - Why needed here: Core mechanism for capturing discriminative local patterns; requires understanding that shapelets are maximally representative subsequences, not full sequences.
  - Quick check question: Given a time series of length 500, can a shapelet of length 20 appear at position 50 and position 300? (Yes—shift-invariance is key.)

- Concept: **Shift-Invariant Dictionary Learning**
  - Why needed here: Enables extraction of recurring patterns regardless of temporal position; differs from standard dictionary learning which assumes aligned inputs.
  - Quick check question: Why would standard dictionary learning fail for time series patterns? (It requires fixed alignment; time series patterns can occur at any offset.)

- Concept: **Selective Prediction / Reject Option**
  - Why needed here: Framework paradigm allowing models to abstain; shifts evaluation from pure accuracy to accuracy-coverage tradeoffs.
  - Quick check question: If a model discards 20% of predictions, what additional metric must be reported alongside MSE? (Coverage rate or effective sample size.)

## Architecture Onboarding

- Component map: Pre-trained TSFM (TTM) -> Error Threshold Module -> Shapelet Learning Module -> Distance Computation Module -> Sample Selection Module -> Final predictions
- Critical path: Validation prediction → Error threshold filtering → Shapelet extraction → Test distance computation → Selective discard → Final predictions
- Design tradeoffs:
  - Higher δ: Fewer shapelets (more specific), risk missing error patterns
  - Lower δ: More shapelets (more diverse), potential false positives
  - Higher dp: Greater MSE reduction, lower coverage
  - Shapelet count K and length q: Grid-searched via 3-fold CV; no explicit sensitivity analysis provided
- Failure signatures:
  - High variance in MSE across random seeds → Shapelet learning instability
  - Discarded samples have lower actual error than retained samples → Inverse correlation break
  - Shapelets visually resemble noise → Dictionary learning convergence failure
- First 3 experiments:
  1. **Baseline validation**: Reproduce zero-shot and fine-tuned TTM MSE on all 6 datasets (Table 2 first two columns); verify error distribution on validation split.
  2. **Shapelet learning sanity check**: Extract shapelets with δ=2; visualize top 5 shapelets and manually verify they capture interpretable patterns (Figure 3 style).
  3. **Random selection baseline**: Implement random discard at 20%; compare against shapelet-guided discard to quantify informed selection benefit (Table 2 difference between columns 3-4 vs. 5-6).

## Open Questions the Paper Calls Out
None

## Limitations
- Critical dependence on distribution stability between validation and test sets; shapelets may fail to generalize under temporal shifts
- Hyperparameter sensitivity (K, q, λ, δ, dp) without systematic sensitivity analysis or ablation studies
- Limited comparison against alternative selective inference methods that could leverage TSFM's internal uncertainty measures

## Confidence
- **High**: The core claim that shapelet-guided selective discarding reduces MSE by 22.17-22.62% on average; empirical results in Table 2 and Appendix Table 4 are consistent and reproducible.
- **Medium**: The interpretability of shapelets as discriminative error patterns; while visually supported (Figure 3), the corpus lacks precedent for this specific application.
- **Low**: The generality of the framework across domains; only six datasets are tested, and distribution shift failures suggest sensitivity to temporal alignment and stationarity.

## Next Checks
1. **Distribution shift sensitivity test**: Intentionally split validation and test sets with temporal gaps; measure MSE reduction and identify failure modes (e.g., zero discarded samples or no MSE improvement).
2. **Ablation on shapelet hyperparameters**: Systematically vary K, q, λ, and dp; report MSE vs. coverage trade-off curves to identify optimal configurations and overfitting risks.
3. **Alternative selective methods benchmark**: Implement selective inference using TSFM's internal uncertainty scores (e.g., prediction variance) and compare MSE reduction against shapelet-based selection under identical coverage constraints.