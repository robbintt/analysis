---
ver: rpa2
title: 'Any4D: Unified Feed-Forward Metric 4D Reconstruction'
arxiv_id: '2512.10935'
source_url: https://arxiv.org/abs/2512.10935
tags:
- scene
- flow
- motion
- dynamic
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Any4D, a feed-forward transformer that produces
  dense metric-scale 4D reconstruction from multi-modal inputs. It predicts per-pixel
  motion and geometry in metric coordinates using a factored representation of allocentric
  (camera extrinsics, scene flow) and egocentric (depth, intrinsics) factors.
---

# Any4D: Unified Feed-Forward Metric 4D Reconstruction

## Quick Facts
- arXiv ID: 2512.10935
- Source URL: https://arxiv.org/abs/2512.10935
- Reference count: 40
- Any4D achieves 2-3× lower EPE and up to 15× faster runtime than state-of-the-art 4D reconstruction methods

## Executive Summary
Any4D introduces a feed-forward transformer architecture that produces dense metric-scale 4D reconstruction from multi-modal inputs in a single pass. The model predicts per-pixel motion and geometry in metric coordinates using a factored representation that separates allocentric factors (camera extrinsics, scene flow) from egocentric factors (depth, intrinsics). This design enables training on heterogeneous datasets with partial supervision while preserving metric scale. Any4D outperforms state-of-the-art methods on 3D tracking benchmarks, achieves top results on dense scene flow and video depth tasks, and demonstrates significant speed advantages for real-time applications.

## Method Summary
Any4D uses a 12-block alternating-attention transformer that processes N frames simultaneously, predicting both geometry and motion in metric coordinates. The architecture employs a factored representation where egocentric factors (ray directions, depth) are represented in local camera coordinates, while allocentric factors (camera poses, scene flow) are represented in a global world frame anchored to the first view. A global metric scale factor is predicted separately and applied to all outputs. The model supports RGB-D, IMU, and radar Doppler inputs through modality-specific encoders that aggregate via summation. Training uses multi-modal conditioning with dropout and scale-invariant losses, with 10× upweighting on dynamic regions for scene flow supervision.

## Key Results
- Achieves 2-3× lower End Point Error (EPE) than SpatialTrackerV2 on 3D tracking benchmarks
- Outperforms prior art by 5.5 points on scene flow APD and 1.6 points on τ metric
- Demonstrates 15× faster runtime by processing N frames in a single forward pass versus pairwise approaches
- Achieves state-of-the-art results on video depth tasks with Abs Rel of 0.081 on MegaDepth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing 4D reconstruction into egocentric and allocentric factors enables training on heterogeneous datasets with partial supervision while preserving metric scale.
- **Mechanism**: The model predicts: (1) egocentric factors (ray directions R_i, scale-normalized depth D_i) in local camera coordinates, and (2) allocentric factors (camera poses T_i, scale-normalized scene flow F_i) in a consistent world frame anchored to the first view. A global metric scale factor s is regressed separately. Metric pointmaps are recovered via composition: G_i = s · T_i · R_i · D_i (Eq. 2). Scene flow M_i = s · F_i (Eq. 3).
- **Core assumption**: Scale ambiguity can be isolated as a single global factor, allowing geometry and motion to be supervised independently on datasets with partial annotations.
- **Evidence anchors**:
  - [abstract]: "per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates"
  - [section 3]: "this parameterization of motion and geometry is optimal for model performance compared to other parameterizations"
  - [corpus]: MapAnything uses similar factored representation for metric 3D reconstruction with flexible geometric inputs

### Mechanism 2
- **Claim**: Directly predicting allocentric (world-frame) scene flow produces better 4D reconstructions than alternative motion parameterizations.
- **Mechanism**: The Motion DPT head directly regresses allocentric scene flow F_i representing 3D motion from the reference frame (view-0) to all other frames in a global coordinate system. This avoids error propagation from deriving flow via egocentric flow or differencing pointmaps.
- **Core assumption**: A consistent world coordinate frame enables more accurate cross-frame motion reasoning than camera-relative representations.
- **Evidence anchors**:
  - [section 3.1]: "scene flow represents motion of points in the reference view-0 to all other views"
  - [section 4, Table 5]: Allocentric scene flow achieves 87.51% τ on Kubric-4D static camera vs. 21.84% for 3D points-after-motion representation

### Mechanism 3
- **Claim**: Stochastic dropout of modalities during training enables flexible inference with any sensor subset while maintaining performance gains when all modalities are available.
- **Mechanism**: Multi-modal conditioning is applied with probability 0.7 per batch; individual modalities (depth, rays, poses, doppler) are independently dropped with probability 0.5. All encodings are aggregated via summation into F_view ∈ R^{1024×H/14×W/14}. Doppler velocity is simulated as the radial component of egocentric scene flow during offline preprocessing.
- **Core assumption**: Summation of learned embeddings preserves modality-specific information while enabling graceful degradation and cross-modal synergy.
- **Evidence anchors**:
  - [section 3.2]: "multi-modal conditioning is applied with a probability of 0.7...individual modalities (depth, rays, poses, and doppler) are independently removed with a probability of 0.5"
  - [section 4, Table 4]: Adding geometry improves APD from 21.33 to 80.18 on Kubric-4D; best performance with all modalities (APD 81.72)

## Foundational Learning

- **Concept: Scene Flow vs Optical Flow**
  - **Why needed here**: Scene flow is the dense 3D motion vector field for every surface point; optical flow is its 2D perspective projection. Any4D directly predicts allocentric scene flow, which is more informative for 4D reconstruction but requires learning 3D motion from 2D observations.
  - **Quick check question**: Given optical flow (u,v) and depth d for a pixel, plus camera intrinsics K and pose change T, can you compute egocentric scene flow? What additional transformation is needed for allocentric scene flow?

- **Concept: Metric Scale Recovery in SfM**
  - **Why needed here**: Traditional SfM/SLAM recovers up-to-scale reconstructions due to depth-scale ambiguity. Any4D explicitly predicts a global metric scale factor s, enabling physical-world coordinates. Understanding why scale is ambiguous and how it can be resolved is critical.
  - **Quick check question**: If a pointmap is normalized such that the mean Euclidean distance of valid points to the world origin is 1 (as in Eq. 7-10), and ground truth has mean distance 5.2 meters, what is the correct scale factor? How would you supervise this during training?

- **Concept: Allocentric vs Egocentric Coordinate Frames**
  - **Why needed here**: Any4D uses a mixed representation: egocentric factors (depth, intrinsics) in camera space, allocentric factors (poses, scene flow) in world space. Understanding this distinction is essential for composing outputs correctly via Eq. 2-4.
  - **Quick check question**: If a camera translates forward by 1m in a static scene, what is the egocentric scene flow for a point at depth 5m? What is the allocentric scene flow? Which one does Any4D predict?

## Architecture Onboarding

- **Component map**: RGB images → DINOv2 encoder → 1024-dim patch tokens + other modal encodings (summed) → transformer with cross-view alternating attention → token outputs per view → DPT heads decode per-pixel factors → compose via Eq. 2-4 with predicted scale → metric pointmaps and scene flow

- **Critical path**: RGB images → DINOv2 encoder → 1024-dim patch tokens + other modal encodings (summed) → transformer with cross-view alternating attention → token outputs per view → DPT heads decode per-pixel factors → compose via Eq. 2-4 with predicted scale → metric pointmaps and scene flow

- **Design tradeoffs**:
  1. **Single N-frame pass vs pairwise/iterative**: 15× faster than SpatialTrackerV2, but requires 4-view training for generalization to more views (Fig. S1 shows 2-view training degrades at high N)
  2. **Direct allocentric flow vs derived from geometry**: Cleaner object boundaries and better APD (Table 5), but requires objects visible in reference frame
  3. **Summation fusion vs concatenation**: Parameter-efficient and handles variable modalities, but may limit cross-modal interaction modeling
  4. **DPT heads vs simpler decoders**: Dense prediction capability, but adds computational overhead

- **Failure signatures**:
  1. **Large camera motion with no background overlap**: Scene flow degrades (Fig. S4)
  2. **Objects entering after reference frame**: No motion prediction for those pixels (explicit limitation)
  3. **Scene motion dominating image space**: Common failure mode per Fig. S4
  4. **2-view training used for N>4 inference**: EPE increases sharply (Fig. S1)

- **First 3 experiments**:
  1. **Ablate motion representation**: Train three variants (allocentric flow, egocentric flow, 3D points-after-motion) on Kubric-4D and evaluate on Kubric-4D static camera and LSFOdyssey. Report EPE, APD, τ. Expected: allocentric flow wins on scene flow τ (87.51% vs 85.37% vs 21.84%) and dynamic point APD per Table 5.
  2. **Multi-modal conditioning contribution**: Evaluate four configurations (images-only, images+geometry, images+doppler, all modalities) on Kubric-4D static and LSFOdyssey. Expected: geometry provides largest APD gain (~60 points), doppler improves scene flow τ by ~1 point (Table 4).
  3. **Training view count ablation**: Train separate models with 2-view and 4-view sampling, then evaluate with N∈{2,4,8,16,32,64} frames at inference. Plot scene flow EPE vs N. Expected: 2-view model degrades at high N; 4-view model remains stable (Fig. S1 pattern).

## Open Questions the Paper Calls Out

- **Question**: How can the model be adapted to handle dynamic objects that enter the scene after the initial reference frame without relying on a sliding-window or permutation-invariant architecture?
- **Basis in paper**: [explicit] The authors note that the model "always calculate[s] scene-flow from the reference (first) view... necessitating that the object of interest should be present at the start of the video."
- **Why unresolved**: The current architecture fixes the coordinate frame based on the first view, inherently ignoring 3D motion for objects that appear later in the sequence.
- **What evidence would resolve it**: A modification allowing the model to establish new reference frames dynamically or process frames in a permutation-invariant manner, evaluated on sequences where targets appear mid-video.

- **Question**: How does the model's metric reconstruction accuracy degrade when subjected to real-world sensor noise from IMUs or Radar, compared to the perfectly simulated inputs used during training?
- **Basis in paper**: [explicit] The authors state they "assume perfectly simulated multi-modal input and do not account for sensor noise - which is hardly true for real-world deployment."
- **Why unresolved**: The training pipeline uses idealized radial velocities and poses; the transformer's robustness to the stochastic noise distributions found in consumer-grade hardware remains untested.
- **What evidence would resolve it**: Evaluation on hardware-in-the-loop benchmarks containing unfiltered IMU drift or radar speckle noise, or an ablation study injecting realistic noise profiles into the training data.

## Limitations

- Performance relies on simulated Doppler radar data that may not generalize to real radar sensor characteristics including noise, clutter, and multi-path effects
- 15× speedup versus SpatialTrackerV2 is based on a specific pairwise baseline; comparison to other 4D reconstruction methods is limited
- Multi-view generalization (N>4) depends on 4-view training, but this scaling behavior may vary with different scene dynamics or camera trajectories

## Confidence

- **High confidence** in the factored metric-scale representation mechanism (Mechanism 1) due to direct empirical evidence and clear theoretical motivation
- **Medium confidence** in allocentric scene flow superiority (Mechanism 2) - the quantitative gap is significant, but limited direct comparison to egocentric alternatives in the literature
- **Medium confidence** in multi-modal conditioning benefits (Mechanism 3) - strong quantitative improvements, but performance depends on perfect input simulation assumptions

## Next Checks

1. **Cross-dataset generalization test**: Train Any4D on 3 datasets (e.g., ScanNet++, Kubric-4D, Dynamic Replica), then evaluate on a held-out dataset (e.g., PointOdyssey) to verify factorization enables partial supervision without catastrophic forgetting

2. **Real radar validation**: Replace simulated Doppler with real radar data from Waymo-DriveTrack, measuring whether scene flow performance degrades relative to the simulated version

3. **Iterative refinement ablation**: Add a lightweight refinement step (e.g., 1-2 iterations of bundle adjustment on predicted pointmaps and poses) and measure whether EPE/APD improve, testing the feed-forward assumption's optimality