---
ver: rpa2
title: Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs
arxiv_id: '2505.11227'
source_url: https://arxiv.org/abs/2505.11227
tags:
- reasoning
- arxiv
- process
- deepseek-r1
- self-prm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether explicit process reward models
  (PRMs) are necessary for developing reasoning capabilities in large language models
  (LLMs). The authors show that reinforcement learning (RL) training alone can foster
  strong PRM capabilities, challenging the conventional wisdom that process supervision
  is required.
---

# Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs

## Quick Facts
- arXiv ID: 2505.11227
- Source URL: https://arxiv.org/abs/2505.11227
- Authors: Zhangying Feng; Qianglong Chen; Ning Lu; Yongqian Li; Siqi Cheng; Shuangmu Peng; Duyu Tang; Shengcai Liu; Zhirui Zhang
- Reference count: 40
- Primary result: RL training alone can foster strong PRM capabilities, challenging the need for explicit process supervision

## Executive Summary
This paper investigates whether explicit process reward models (PRMs) are necessary for developing reasoning capabilities in large language models (LLMs). Through systematic experiments on math reasoning benchmarks, the authors demonstrate that RL-trained models like DeepSeek-R1 and QwQ-32B exhibit robust process-level judgment abilities without any PRM-labeled data, often outperforming models explicitly trained on PRM annotations. They introduce Self-PRM, a framework where models use their own internal reward signals to rerank generated solutions, achieving consistent performance gains—particularly at higher sample sizes. However, analysis reveals that Self-PRM suffers from low precision on difficult problems, often misclassifying flawed solutions as valid. The findings suggest that pure RL not only improves problem-solving skills but also inherently develops process supervision capabilities, making PRM supervision potentially unnecessary for complex reasoning tasks.

## Method Summary
The paper trains LLMs using reinforcement learning (RL) with outcome-based rewards on math reasoning tasks, then evaluates the emergent process-level judgment capabilities without any explicit PRM supervision. The authors introduce PROCESSBENCH, a comprehensive evaluation suite covering four datasets (GSM8K, MATH, OlympiadBench, OmniMath) to assess error detection and process-level judgment capabilities. They systematically compare RL-trained models against explicitly PRM-trained models on process judgment tasks, and evaluate Self-PRM—a framework where the model uses its own internal reward signal to rerank candidate solutions. The methodology includes chi-square tests to establish statistical correlation between problem-solving success and judgment accuracy, and controlled experiments varying sample sizes (k=8,16,32,64) to assess Self-PRM effectiveness.

## Key Results
- RL-trained models (DeepSeek-R1, QwQ-32B) achieve superior PRM capability (F1 scores 83.5-83.7) compared to explicitly PRM-trained models (78.3)
- Self-PRM consistently improves accuracy over majority voting, with gains increasing at higher sample sizes (k=32,64)
- External PRMs underperform on strong RL models, providing no improvement over simple majority voting
- Self-PRM precision drops below 10% on difficult problems where the model initially fails

## Why This Works (Mechanism)

### Mechanism 1: Co-Evolution of Problem-Solving and Process Judgment via RL
Reinforcement learning on outcome-based rewards implicitly develops process-level evaluation capabilities alongside problem-solving accuracy. During RL optimization with rule-based rewards, models internalize valid reasoning structures as intermediate representations that support both generation and evaluation. The learning curves show F1 scores on process judgment often improve earlier than accuracy, suggesting the model learns to recognize valid reasoning patterns before fully mastering solution strategies.

### Mechanism 2: Self-PRM Introspective Reranking
A model's internal reward signal better aligns with its own reasoning behavior than externally-trained PRMs, enabling more effective candidate selection at inference time. Self-PRM prompts the model to evaluate its own generated solutions, producing scores that reflect the model's learned reasoning preferences. These internal signals are used to rerank sampled outputs (Best-of-N selection), improving selection accuracy compared to external PRMs that may misalign with the model's reasoning patterns.

### Mechanism 3: RL Models Surpass Explicit PRM Training
Models trained purely with RL on outcome rewards achieve stronger process-level judgment than models explicitly trained on PRM-annotated data. RL training with outcome supervision forces the model to develop robust internal representations of valid reasoning chains. In contrast, discriminative PRMs trained on annotated data may overfit to annotation patterns or suffer from distribution shift when applied to strong reasoners.

## Foundational Learning

- **Markov Decision Process (MDP) formulation for LLM RL**: The paper frames RL training as learning sequential decisions where states are partial sequences, actions are tokens, and rewards come from outcome verification. Understanding this formulation is essential to grasp how outcome rewards propagate to process learning.
  - Quick check: Can you explain why outcome-based rewards can provide learning signal for intermediate reasoning steps in an LLM?

- **Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**: The central claim depends on distinguishing evaluation at the step level (PRM) from evaluation at the final answer level (ORM). PRMs score intermediate reasoning steps; ORMs score only final outputs.
  - Quick check: Given a math solution with 5 reasoning steps where step 3 contains an error but the final answer is correct, how would a PRM versus an ORM evaluate this?

- **Best-of-N (BoN) sampling with reranking**: Self-PRM operates by sampling N solutions and using internal scores to select the best. Understanding BoN is critical to interpreting the k=8,16,32,64 experiments.
  - Quick check: If you sample 32 solutions and your reranker has 10% precision at identifying correct solutions, how many truly correct solutions would you expect in your selected output on average?

## Architecture Onboarding

- **Component map**: Base LLM → RL Training (DAPO/GRPO/PPO) → Rule-based reward → Updated model → Self-PRM Inference (Sample N solutions → Generate scores → Rerank → Return best)
- **Critical path**: Train base model with outcome-only RL → Evaluate emerging PRM capability via PROCESSBENCH F1 → Deploy Self-PRM at inference: sample k solutions, prompt model to score each, select highest
- **Design tradeoffs**: External PRMs add infrastructure complexity and misalign with strong models; Self-PRM requires no external model but has low precision on hard problems; higher sample size k improves Self-PRM gains but increases compute cost linearly
- **Failure signatures**: Low precision on difficult problems (<10%), external PRM underperformance, stagnant F1 despite accuracy gains
- **First 3 experiments**: (1) Replicate RL training trajectory: Train Qwen2.5-7B-Base with DAPO, logging both accuracy and PROCESSBENCH F1 every 20 steps; (2) Self-PRM ablation by difficulty: Stratify AIME24/25 problems by initial pass@1 success rate, measure Self-PRM precision separately for easy vs. hard problems; (3) Compare Self-REF across model types: Apply Self-REF supervision to instruction-tuned vs. RL-trained models

## Open Questions the Paper Calls Out

1. **Underlying mechanism of PRM emergence**: What is the underlying mechanism by which pure RL training implicitly induces PRM capabilities in LLMs? The paper establishes empirical correlation but doesn't explain why RL optimizes both capabilities simultaneously.

2. **Improving Self-PRM precision**: Can Self-PRM precision on difficult problems be improved beyond the observed <10% without explicit process supervision? While continued RL scaling is suggested, no concrete method is proposed.

3. **Generalization beyond math**: Do the findings generalize beyond mathematical reasoning to other domains requiring multi-step reasoning? The paper acknowledges this limitation, noting that mathematical problems have clear correctness criteria not present in all domains.

## Limitations

- Self-PRM precision drops below 10% on difficult problems, limiting practical applicability for hard reasoning tasks
- All experiments focus on mathematical reasoning, leaving generalization to other domains untested
- The paper doesn't quantify how PRM annotation quality affects the relative performance of RL-trained vs. PRM-trained models

## Confidence

**High Confidence:**
- RL-trained models achieve superior PRM capability compared to explicitly PRM-trained models on PROCESSBENCH
- Self-PRM improves performance over majority voting, especially at higher sample sizes
- Statistical correlation exists between problem-solving success and process judgment accuracy

**Medium Confidence:**
- Co-evolution mechanism explains the emergence of PRM capability during RL training
- RL outcome rewards provide sufficient signal for learning intermediate reasoning structures
- External PRMs underperform on strong RL models due to misalignment

**Low Confidence:**
- Self-PRM is sufficient for reliable improvement on difficult reasoning tasks
- The mechanism generalizes beyond mathematical reasoning domains
- Current PRM annotation quality is the limiting factor for explicit PRM training

## Next Checks

1. **Cross-Domain Transfer Test**: Apply the same methodology to code generation or logical reasoning tasks with different reward structures to test if RL implicitly develops PRM capability in domains with less structured intermediate steps.

2. **Annotation Quality Sensitivity Analysis**: Systematically vary PRM annotation quality (synthetic noise, human vs. model labels) and retrain Qwen2.5-Math-PRM-72B to measure how annotation quality affects the gap between RL-trained and PRM-trained models.

3. **Self-PRM Calibration and Filtering**: Implement precision-aware selection in Self-PRM—only use self-scored solutions where the model expresses high confidence—to measure whether filtering improves performance on difficult problems where raw Self-PRM precision is low.