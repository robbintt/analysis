---
ver: rpa2
title: 'Uncovering Bias Paths with LLM-guided Causal Discovery: An Active Learning
  and Dynamic Scoring Approach'
arxiv_id: '2506.12227'
source_url: https://arxiv.org/abs/2506.12227
tags:
- causal
- fairness
- education
- variable
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of uncovering fairness-relevant causal
  pathways in noisy, complex datasets where standard methods struggle. It introduces
  a hybrid LLM-guided causal discovery framework that combines active learning with
  dynamic scoring to efficiently identify how sensitive attributes influence outcomes.
---

# Uncovering Bias Paths with LLM-guided Causal Discovery: An Active Learning and Dynamic Scoring Approach

## Quick Facts
- arXiv ID: 2506.12227
- Source URL: https://arxiv.org/abs/2506.12227
- Authors: Khadija Zanna; Akane Sano
- Reference count: 31
- One-line primary result: Hybrid LLM-guided causal discovery framework outperforms baselines in structural accuracy and fairness-path recovery on semi-synthetic and clinical networks

## Executive Summary
This work introduces a hybrid LLM-guided causal discovery framework that combines active learning with dynamic scoring to efficiently uncover fairness-relevant causal pathways in complex datasets. The approach prioritizes variable pairs for querying using a composite score of mutual information, partial correlation, and LLM confidence, adapting to graph size and complexity. Evaluated on semi-synthetic Adult dataset and clinical networks, it achieves top F1 scores and higher normalized bias detection compared to baselines.

## Method Summary
The method uses a breadth-first search strategy enhanced with active learning, where variable pairs are prioritized for querying based on a dynamic composite score. This score combines statistical metrics (mutual information and partial correlation) with LLM confidence derived from token log-probabilities and a history penalty. The process iteratively queries the LLM for the highest-scoring pair, adding an edge only if it doesn't create a cycle. The framework adapts from LLM-heavy exploration in small graphs to statistics-heavy verification in large ones, with hyperparameters optimized via Bayesian optimization.

## Key Results
- Outperforms baselines in structural accuracy and fairness-path recovery with top F1 scores
- Achieves higher normalized bias detection (C_bias) compared to traditional causal discovery methods
- Dynamic scoring and active querying significantly improve efficiency and robustness across different graph sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prioritizing variable pairs using a composite score improves discovery efficiency and structural accuracy over exhaustive or fixed-order search.
- **Mechanism:** The system calculates a dynamic score $S(x, y)$ for unqueried pairs by weighing three components: statistical dependency (Mutual Information + Partial Correlation), LLM confidence (derived from token log-probabilities), and a history penalty (to discourage redundant queries).
- **Core assumption:** Useful causal signals are distributed across both statistical dependencies in data and semantic relationships in metadata, and these can be linearly combined to rank query utility.
- **Evidence anchors:** [Section 3.1] "Variable pairs are prioritized for querying using a composite score based on mutual information, partial correlation, and LLM confidence..."; [Section 7.3] "Correlation analysis... shows anti-correlation among scoring weights, indicating competitive trade-offs."

### Mechanism 2
- **Claim:** Active learning (AL) via a breadth-first search (BFS) strategy reduces query complexity while maintaining robustness against noise.
- **Mechanism:** Instead of querying all pairs, the algorithm starts with root variables and iteratively expands. It queries the LLM only for the highest-scoring pair $(x^*, y^*)$ per iteration, adding an edge only if it does not create a cycle.
- **Core assumption:** The causal structure is sparse and acyclic (a DAG), and local discovery (BFS) suffices to recover global structure without exhaustive $O(n^2)$ comparisons.
- **Evidence anchors:** [Abstract] "...framework that extends a breadth-first search (BFS) strategy with active learning..."; [Section 3.1] "The process terminates after a query limit or when scores fall below a threshold."

### Mechanism 3
- **Claim:** Hybridizing semantic priors with statistical refinement allows the system to adapt from "exploration" (LLM-heavy) to "verification" (Stats-heavy) as the graph grows.
- **Mechanism:** In early stages or small graphs, semantic cues guide the search. As iterations proceed or in large graphs (like Neuropathic), statistical metrics (MI/PCorr) dominate the scoring, grounding the LLM's "hallucinations" or biases.
- **Core assumption:** LLMs encode valid domain knowledge about variable relationships but require statistical grounding to filter false positives.
- **Evidence anchors:** [Section 7.3] "In small graphs, deeper querying matters more; in large ones... MI/PCorr dominate."; [Section 7 Discussion] "Dynamic scoring balances LLM semantic priors with empirical signals, adapting to graph complexity..."

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) & Cycle Checks**
  - **Why needed here:** The system outputs a DAG; understanding that edges must not form loops ($A \rightarrow B \rightarrow A$) is critical to the "validity" of the causal model.
  - **Quick check question:** If adding edge $X \rightarrow Y$ creates a path from $Y$ back to $X$, should the algorithm accept it? (Answer: No).

- **Concept: Mutual Information (MI) & Partial Correlation (PCorr)**
  - **Why needed here:** These form the `StatScore`. MI captures non-linear dependencies, while PCorr captures linear relationships conditional on other variables.
  - **Quick check question:** If two variables are highly correlated solely due to a third confounding variable, would high MI alone distinguish this from direct causation? (Answer: Not necessarily; hence the need for PCorr).

- **Concept: LLM Token Log-Probabilities**
  - **Why needed here:** The `LLMConf` score is not just "Yes/No", but the model's certainty (softmax probability) in generating that token.
  - **Quick check question:** How does the system quantify "uncertainty" from the LLM? (Answer: By averaging the log-probabilities of the response tokens).

## Architecture Onboarding

- **Component map:** Input (Dataset + Variable Metadata) -> Scoring Engine (computes $S(x,y)$) -> Query Manager (selects $\text{argmax}(S)$, prompts LLM) -> Graph Builder (updates adjacency matrix, enforces acyclicity) -> Output (Adjacency Matrix $A$ and Normalized Bias score $C_{bias}$)

- **Critical path:** The loop inside `Query Manager` -> `LLM` -> `Graph Builder`. The latency and cost are dominated by the LLM API calls.

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** High `max_iterations` improves F1 but increases API costs linearly.
  - **Precision vs. Recall:** The paper notes the proposed method is "conservative" (high precision, lower recall) compared to "LLM Pairwise" (high recall, low precision).
  - **Sensitivity:** Hyperparameters ($w_{stat}, w_{conf}, temp$) require Bayesian optimization (Section 6) and are sensitive to graph size.

- **Failure signatures:**
  - **API Drift:** GPT-4 updates change baseline performance (noted in Section 7.1 regarding reproducibility).
  - **Metadata Ambiguity:** Vague variable descriptions lead to low `LLMConf` or hallucinated edges.
  - **Token Limits:** Large graphs (e.g., 221 nodes) may exceed context windows or budget.

- **First 3 experiments:**
  1. **Sanity Check (Small Graph):** Run on the "Child" network (20 nodes). Verify that the BFS strategy produces a valid DAG and compare F1 against a baseline PC algorithm.
  2. **Hyperparameter Sweep:** Run Bayesian optimization on the Adult-based synthetic dataset. Plot the correlation between `w_stat` and F1 score to verify if statistical weighting dominates in noisy conditions.
  3. **Fairness Path Verification:** Inject a known spurious path (e.g., `age -> income`) in the synthetic data. Check if the `StatScore` successfully penalizes this path despite potential semantic confusion.

## Open Questions the Paper Calls Out
None

## Limitations
- **Prompt Sensitivity:** The LLM-guided discovery heavily depends on the quality and format of metadata descriptions and the exact prompt templates, which are not fully specified.
- **API Drift and Reproducibility:** The paper notes discrepancies with baseline results due to updates in GPT-4's behavior, highlighting a fundamental fragility in LLM-based methods.
- **Graph-Specific Sensitivity:** The system's performance is highly sensitive to hyperparameters (weights and thresholds) that are dataset and graph-size dependent, requiring careful optimization for each new application.

## Confidence

- **High Confidence:** The core mechanism of using a composite score (statistical + LLM confidence + history penalty) for prioritizing variable pairs is well-specified and logically sound.
- **Medium Confidence:** The hybrid exploration-verification dynamic is plausible given the results, but the exact conditions and thresholds for this transition are not fully detailed.
- **Low Confidence:** The precise implementation of the LLM confidence score extraction from token log-probabilities, the full prompt templates, and the exact conditioning procedure for Partial Correlation are critical unknowns.

## Next Checks

1. **API Version Sensitivity Test:** Run the proposed method on the Adult synthetic dataset using two different versions of the GPT-4 API (if available) or with significantly different temperature settings. Compare the F1 scores, edge counts, and C_bias values to quantify the "API drift" risk.

2. **Metadata Quality Impact Test:** Systematically degrade the quality of the variable descriptions in the Adult dataset (make them vague, contradictory, or overly technical). Run the proposed method and measure the degradation in LLMConf scores, the number of edges added, and the final F1 and C_bias to test sensitivity to the semantic signal path.

3. **Cycle Rejection Stress Test:** During the BFS process on the Neuropathic network, log the number of times an edge addition is rejected due to a cycle. Analyze if there is a pattern (e.g., rejections clustered early or late, or associated with specific variable pairs) to assess the prioritization strategy's robustness.