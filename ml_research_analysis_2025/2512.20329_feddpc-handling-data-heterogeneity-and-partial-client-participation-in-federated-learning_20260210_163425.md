---
ver: rpa2
title: 'FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated
  Learning'
arxiv_id: '2512.20329'
source_url: https://arxiv.org/abs/2512.20329
tags:
- local
- global
- update
- learning
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FedDPC, a federated learning method designed
  to address the challenges of data heterogeneity and partial client participation,
  which cause variances in local and global model updates. FedDPC employs two key
  techniques: orthogonal projection of local updates onto the previous global update
  and adaptive scaling of these projected updates.'
---

# FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning

## Quick Facts
- **arXiv ID:** 2512.20329
- **Source URL:** https://arxiv.org/abs/2512.20329
- **Reference count:** 34
- **Primary result:** FedDPC achieves 55.44% accuracy on CIFAR-10 with Dirichlet parameter α=0.2, outperforming FedCM's 52.12%

## Executive Summary
FedDPC introduces a novel federated learning method that addresses the challenges of data heterogeneity and partial client participation through orthogonal projection and adaptive scaling of local updates. The method projects each client's local update onto the previous global update, then scales the orthogonal residual based on its relevance to the global direction. This approach reduces variance in both local and global updates while accelerating convergence. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet demonstrate that FedDPC outperforms existing state-of-the-art FL algorithms, particularly in settings with heterogeneous data and limited client participation.

## Method Summary
FedDPC operates through a two-stage server-side update mechanism. First, each client computes its local gradient update over multiple SGD steps. The server then projects each local update onto the previous global update, computing the orthogonal residual. This residual is scaled by a factor that increases with its relevance to the previous global direction. The server aggregates all scaled residuals to form the new global update. The method requires only server-side state (previous global update) and two hyperparameters: server learning rate and adaptive scaling coefficient λ. Experiments use Dirichlet-distributed data partitions across 100 clients with 10% participation per round.

## Key Results
- FedDPC achieves 55.44% accuracy on CIFAR-10 with α=0.2, compared to 52.12% for FedCM
- On CIFAR-100 with α=0.6, FedDPC reaches 33.02% accuracy versus 31.24% for FedCM
- FedDPC shows faster training loss reduction across all tested datasets and heterogeneity levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Orthogonal projection of local updates onto the previous global update reduces variance in both local and global updates caused by data heterogeneity and partial client participation.
- **Mechanism:** For each client's local update Δⱼᵗ, FedDPC computes the orthogonal projection onto the previous global update Δᵗ⁻¹, then uses the residual Δⱼᵗ' = Δⱼᵗ - Proj_Δᵗ⁻¹(Δⱼᵗ) as the modified local update. This residual is orthogonal to Δᵗ⁻¹, ensuring that local changes are optimally aligned with respect to the previous round's global direction.
- **Core assumption:** Updates that deviate less from the previous global trajectory cause less client drift and objective inconsistency.
- **Evidence anchors:** [abstract] "FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates."
- **Break condition:** If the previous global update Δᵗ⁻¹ approaches zero (near convergence), the projection denominator becomes unstable.

### Mechanism 2
- **Claim:** Adaptive scaling of the residual updates based on their angle to the original local update accelerates training convergence.
- **Mechanism:** Each residual Δⱼᵗ' is scaled by factor (λ + ||Δⱼᵗ'||/||Δⱼᵗ||). Since the residual is orthogonal to the projection, this ratio equals cosec(θ) where θ is the angle between original and projected update.
- **Core assumption:** Updates with higher relevance to the global direction should contribute more to aggregation.
- **Evidence anchors:** [abstract] "To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation."
- **Break condition:** If λ is negative (especially λ < -1), the scaling factor can become negative or approach zero, inverting or nullifying updates.

### Mechanism 3
- **Claim:** Server-side aggregation of orthogonalized residuals controls global update variance across communication rounds, mitigating partial client participation bias.
- **Mechanism:** After projecting and scaling, FedDPC aggregates all modified updates: Δᵗ = (1/|C|) Σ Δⱼᵗ'. Since each residual is orthogonal to Δᵗ⁻¹, the aggregated global update also maintains this property.
- **Core assumption:** Orthogonality at the individual update level propagates to stability at the global level.
- **Evidence anchors:** [section 4.1] "This approach also helps to control the variance of the global update, as the new global update, the average of the local updates, remains orthogonal to the previous global update."
- **Break condition:** If client updates are highly diverse, the average of orthogonal residuals may not effectively capture the global optimization direction.

## Foundational Learning

- **Concept: Orthogonal Projection in Vector Spaces**
  - Why needed here: FedDPC's core operation is projecting gradient updates onto previous global updates.
  - Quick check question: Given vectors a = [3, 4] and b = [1, 0], compute the projection of a onto b and the residual. Is the residual orthogonal to b?

- **Concept: Client Drift in Federated Learning**
  - Why needed here: The paper's motivation is that heterogeneous data causes local models to optimize at different objectives, causing the global model to drift.
  - Quick check question: In FedAvg with non-IID data, why does the global model converge to a point different from the true global objective optimum?

- **Concept: Variance Reduction in Stochastic Optimization**
  - Why needed here: FedDPC frames data heterogeneity and partial participation as variance problems in local and global updates.
  - Quick check question: How does SCAFFOLD's control variate approach differ from FedDPC's orthogonal projection approach for reducing update variance?

## Architecture Onboarding

- **Component map:** Server broadcasts global model -> Clients compute local updates -> Server receives updates -> Server projects onto previous global update -> Server computes orthogonal residuals -> Server applies adaptive scaling -> Server aggregates residuals -> Server updates global model

- **Critical path:** The orthogonal projection (step 3a-b on server) is the key innovation. The previous global update Δ^{t-1} must be stored server-side.

- **Design tradeoffs:**
  - Server computation: O(4k'd + d) per round vs. O(k'd) for FedAvg, where k' is participating clients
  - No client-side state required (unlike SCAFFOLD, FedDC), reducing client memory overhead
  - Two hyperparameters: server learning rate ηg and adaptive scaling λ

- **Failure signatures:**
  - Negative λ values cause training collapse (Figure 7 shows near-zero accuracy for λ=-0.5)
  - If initial Δ⁰ is not properly zeroed, first-round projections are undefined

- **First 3 experiments:**
  1. Reproduce baseline comparison: Run FedDPC vs. FedAvg on CIFAR-10 with 100 clients, 10% participation, Dirichlet α=0.2, LeNet5, λ=1, ηl=0.1, ηg=1. Target: ~55% test accuracy at ~370 rounds.
  2. Ablation study: Run FedDPC with (a) projection only (no adaptive scaling), (b) no projection (equivalent to FedAvg with two-sided learning rates).
  3. Hyperparameter sweep: Test λ ∈ {0.1, 0.5, 1, 2, 3} on CIFAR-10 to verify the paper's claim that 0.1 < λ ≤ 2 works well.

## Open Questions the Paper Calls Out

- **Question:** How can FedDPC be effectively adapted for Vertical Federated Learning (VFL)?
- **Basis in paper:** [explicit] The conclusion states, "In future, we plan to extend this work for vertical federated learning."
- **Why unresolved:** The current algorithm is designed for horizontal FL where clients share the same feature space.
- **Evidence to resolve:** A reformulation of the FedDPC projection mechanism for vertical partitioning and empirical results on a standard VFL dataset.

- **Question:** Does FedDPC provide formal convergence guarantees for non-convex objectives?
- **Basis in paper:** [inferred] The paper demonstrates empirical superiority on deep learning tasks but lacks a theoretical convergence analysis section.
- **Why unresolved:** While the method claims to control variance, the mathematical assurance that the algorithm converges to a stationary point is not established.
- **Evidence to resolve:** A mathematical proof of convergence bounds under standard assumptions.

- **Question:** Can the server-side computational overhead be reduced to support massive client participation?
- **Basis in paper:** [inferred] Section 4.4 explicitly notes that the "increased server computation costs" (complexity $O(4k'd + d)$) may challenge the server if the number of participating clients ($k'$) is high.
- **Why unresolved:** The algorithm requires the server to compute projection and adaptive scaling for every participating client sequentially.
- **Evidence to resolve:** An analysis of server latency with significantly larger $k'$ values or the proposal of an approximation technique to lower the linear dependency on $k'$.

## Limitations

- The adaptive scaling hyperparameter λ shows poor performance for negative values, but the theoretical justification for the cosec-based weighting remains unclear.
- The convergence analysis relies on gradient Lipschitz continuity assumptions that may not hold for deep networks.
- The algorithm requires the server to process each participating client sequentially, which could become a bottleneck with massive client participation.

## Confidence

- **High confidence:** Experimental results showing FedDPC outperforming baselines on standard benchmarks (CIFAR-10, CIFAR-100, Tiny ImageNet)
- **Medium confidence:** The mechanism by which orthogonal projection reduces variance in local updates, supported by vector space theory but lacking independent validation
- **Low confidence:** The theoretical convergence proof, which depends on assumptions about gradient smoothness and bounded client updates not verified in deep learning contexts

## Next Checks

1. **Ablation isolation:** Run experiments with (a) orthogonal projection only, (b) adaptive scaling only, (c) both components to quantify individual contributions to performance gains
2. **Convergence behavior:** Monitor the angle between consecutive global updates to verify the claimed orthogonality property at the global level
3. **Scaling sensitivity:** Systematically test λ values across [0.1, 0.5, 1, 2, 3] on CIFAR-10 to validate the claimed robustness range and identify performance degradation points