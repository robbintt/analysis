---
ver: rpa2
title: High-Probability Bounds For Heterogeneous Local Differential Privacy
arxiv_id: '2510.11895'
source_url: https://arxiv.org/abs/2510.11895
tags:
- privacy
- algorithm
- bound
- theorem
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies statistical estimation under local differential\
  \ privacy (LDP) when users have heterogeneous privacy levels and high-probability\
  \ error guarantees are required. The authors develop new algorithms for single-dimensional\
  \ and multi-dimensional mean estimation problems, as well as distribution learning,\
  \ that provide finite-sample \u2113\u2082-error bounds holding with probability\
  \ at least 1-\u03B2."
---

# High-Probability Bounds For Heterogeneous Local Differential Privacy

## Quick Facts
- arXiv ID: 2510.11895
- Source URL: https://arxiv.org/abs/2510.11895
- Reference count: 40
- Authors develop finite-sample ℓ₂-error bounds for statistical estimation under local differential privacy with heterogeneous user privacy budgets

## Executive Summary
This paper studies statistical estimation under local differential privacy when users have different privacy budgets. The authors develop novel algorithms for mean estimation (both single and multi-dimensional) and distribution learning that provide finite-sample ℓ₂-error bounds holding with probability at least 1-β. Their key insight is that standard high-probability amplification techniques fail in heterogeneous settings, so they design weighted aggregation methods that achieve optimal error rates. The work establishes matching upper and lower bounds for all three problems, demonstrating the optimality of their algorithms.

## Method Summary
The paper develops statistical estimation algorithms under local differential privacy (LDP) with heterogeneous user privacy levels (εᵢ). For single-dimensional mean estimation, users apply Laplace or randomized response mechanisms and the server computes a weighted average with weights proportional to εᵢ². For multi-dimensional estimation over ℓ₂ balls, the Duchi et al. [2013] mechanism is used with similar weighting. For distribution learning, a JL projection combined with randomized response achieves communication efficiency. All algorithms provide high-probability ℓ₂ or ℓ∞ error bounds that match minimax lower bounds.

## Key Results
- Single-dimensional mean estimation: Error bounded by O(min{log(1/β)∑ᵢ εᵢ², 1}), proven tight
- Multi-dimensional estimation over ℓ₂ ball: Error O(r²(d+log(1/β))∑ᵢ εᵢ²), first optimal high-probability bound
- Distribution learning: ℓ∞-error O(√(log(d/β)∑ᵢ εᵢ²)) via JL projection method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Privacy-weighted aggregation achieves optimal error rates under heterogeneous privacy budgets
- Mechanism: Each user i with privacy parameter ε_i applies a local randomizer (Laplace or randomized response). The server computes a weighted average ̂θ = Σᵢ wᵢYᵢ where weights wᵢ ∝ εᵢ²/(Σⱼεⱼ²). This down-weights noisier contributions from users with stronger privacy requirements (smaller εᵢ).
- Core assumption: εᵢ ≤ 1 for all users; privacy parameters are known to the server and independent of user data values
- Evidence anchors: [abstract] finite sample upper bounds; [section 2.1, Algorithm 1] closed-form weights; [corpus] limited validation
- Break condition: Fails if εᵢ values are unknown, incorrectly specified, or correlated with user data

### Mechanism 2
- Claim: Exploiting hemisphere mixture structure yields additive (d + log(1/β)) dependence instead of multiplicative (d·log(1/β))
- Mechanism: The Duchi et al. LDP randomizer outputs points uniformly drawn from hemispheres of a sphere. By applying Lévy's lemma, one establishes that ⟨ℓ, Yᵢ - Xᵢ⟩ is subgaussian with variance proxy O(r²/εᵢ²) independent of dimension d.
- Core assumption: Data lies in bounded ℓ₂ ball B_d(r); the privacy mechanism produces unbiased estimates E[Yᵢ] = Xᵢ
- Evidence anchors: [section 3.1, Proof Sketch] concentration of measure; [section C.1, Lemma 3.1] Lévy's lemma application; [corpus] no independent validation
- Break condition: Fails if data is unbounded or if privacy mechanism does not produce unbiased estimates

### Mechanism 3
- Claim: JL projection combined with weighted aggregation enables communication-efficient distribution learning
- Mechanism: Users compress data via random JL projection Φ, select one coordinate, privatize via randomized response, and send one bit. Server reconstructs estimates via ̂p(v) = ⟨z̄, Φe_v⟩ using the same ε-weighted aggregation for heterogeneous budgets.
- Core assumption: Shared randomness (public coin) for JL matrix Φ; domain size d is known; β is specified globally
- Evidence anchors: [section 4, Algorithm 5] modified algorithm; [section 4, Theorem 4.1] achieves ℓ∞ error bound; [corpus] Bassily-Smith (2015) validates homogeneous case
- Break condition: Fails without shared randomness; communication savings lost if all d coordinates needed

## Foundational Learning

- Concept: **Local Differential Privacy (LDP)**
  - Why needed here: The entire framework assumes each user independently perturbs data before transmission. Without understanding ε-LDP (Pr[C(X)∈W] ≤ e^ε Pr[C(X')∈W]), the weighting scheme and error bounds are unmotivated.
  - Quick check question: Can you explain why smaller ε requires more noise in the Laplace mechanism?

- Concept: **Subgaussian random vectors and norm concentration**
  - Why needed here: High-probability bounds require proving that ‖Σᵢwᵢ(Yᵢ - Xᵢ)‖₂ concentrates. The paper establishes subgaussianity via Lévy's lemma rather than generic bounded-vector bounds.
  - Quick check question: Why does the variance proxy of a subgaussian vector determine the log(1/β) term in tail bounds?

- Concept: **Minimax quantiles (high-probability lower bounds)**
  - Why needed here: Standard expected-error lower bounds don't capture tail behavior. The paper uses minimax quantiles from Ma et al. (2024) to prove matching lower bounds with β dependence.
  - Quick check question: How does a minimax quantile differ from minimax risk in expectation?

## Architecture Onboarding

- Component map:
  [Users i=1..n] --(εᵢ-LDP channel: Laplace/RR/Duchi)--> [Yᵢ (privatized)] --> [Server] --> [Weighted Aggregator] --> ̂θ
  For distribution learning: Users additionally apply JL compression (Φ) and coordinate selection before privatization.

- Critical path:
  1. Correctly assign εᵢ to each user (privacy budget collection)
  2. Compute weights wᵢ = εᵢ²/(Σⱼεⱼ²) at server
  3. Apply appropriate randomizer per dimensionality (Algorithm 1/2 for 1D, Algorithm 3/4 for multi-D, Algorithm 5 for distributions)
  4. Aggregate with weights; bound holds with probability ≥ 1-β

- Design tradeoffs:
  - **Laplace vs. Randomized Response (1D):** Laplace requires continuous communication; RR achieves same bound with 1 bit but only for binary data
  - **Generic concentration vs. structure-aware (multi-D):** Vector Bernstein gives d·log(1/β); Lévy-based analysis gives d + log(1/β) but requires mechanism-specific proof
  - **Public vs. private coin (distribution learning):** JL-based method requires shared randomness; private-coin alternatives exist but may have worse sample complexity

- Failure signatures:
  - Error scales as log(1/β)·(Σᵢεᵢ²)⁻¹ but observed error is much larger → check if εᵢ values are being misreported or weights computed incorrectly
  - Multi-D error has d·log(1/β) dependence instead of d + log(1/β) → verify Duchi mechanism implementation; check if unbiasedness holds (E[Yᵢ] should equal Xᵢ)
  - Distribution learning ℓ∞ error exceeds O(√(log(d/β)/Σᵢεᵢ²)) → verify JL matrix Φ is shared correctly; check coordinate selection randomness

- First 3 experiments:
  1. **Validate single-D weighted aggregation:** Synthetic data with n=1000 users, heterogeneous εᵢ ∈ {0.1, 0.5, 1.0}, compare weighted vs. unweighted mean estimation error across 1000 trials; verify error bound holds for β=0.05
  2. **Test multi-D concentration improvement:** Generate d-dimensional data (d=100, r=1), compare error scaling of Algorithm 3 vs. naive Vector-Bernstein-based aggregation as β varies from 0.01 to 0.2; plot error vs. log(1/β)
  3. **Distribution learning communication-accuracy tradeoff:** Implement Algorithm 5 with domain d=1000, vary JL projection dimension m; measure ℓ∞ error vs. bits transmitted per user; verify bound holds for heterogeneous εᵢ distribution

## Open Questions the Paper Calls Out
- Can the high-probability weighted aggregation techniques be extended to other statistical estimation tasks, such as frequency estimation?
- How do the optimal error rates and weighted aggregation strategies change in the low-privacy regime where user parameters εᵢ may significantly exceed 1?
- Can the optimal high-probability bounds for multi-dimensional mean estimation be achieved with a communication cost that is sub-linear in the dimension d?
- Do the high-probability error guarantees hold for distributions with unbounded support or heavy tails, where data is not strictly bounded by a ball of radius r?

## Limitations
- The analysis relies on approximations valid for small εᵢ and may not extend to the low-privacy regime where εᵢ > 1
- The multi-dimensional solution requires transmitting high-dimensional vectors, lacking communication efficiency
- All theorems explicitly assume bounded data support, which may not hold for real-world distributions with heavy tails

## Confidence
- **High confidence**: The core weighted aggregation mechanism is well-established in the LDP literature
- **Medium confidence**: The high-probability analysis for multi-dimensional estimation using hemisphere concentration is mathematically coherent but lacks independent validation
- **Medium confidence**: The distribution learning extension follows naturally from JL compression but introduces additional assumptions that may limit practical applicability

## Next Checks
1. **Monte Carlo verification of high-probability bounds**: Implement Algorithm 1 and Algorithm 3, run 1000 trials for various (n, εᵢ, β) configurations, and empirically verify that the β-quantiles of errors match theoretical O(·) predictions
2. **Implementation audit of hemisphere sampling**: For Algorithm 4, implement the hemisphere sampling procedure from Duchi et al. [2013] and verify unbiasedness empirically across dimensions d ∈ {10, 100, 1000}
3. **Communication-accuracy tradeoff analysis**: For Algorithm 5, implement the JL-based distribution learning with varying m (compression dimension) and heterogeneous εᵢ distributions, measuring achieved ℓ∞ error vs. bits per user