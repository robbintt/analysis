---
ver: rpa2
title: 'Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written
  and Algorithmic Adversarial Prompts'
arxiv_id: '2510.15973'
source_url: https://arxiv.org/abs/2510.15973
tags:
- attack
- security
- evaluation
- attacks
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a comprehensive security assessment of four
  LLMs (Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4) against four distinct attack
  categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG), and
  Tree-of-Attacks-with-Pruning (TAP). Using 1,200 stratified prompts from the SALAD-Bench
  dataset across six harm categories, the study reveals significant variations in
  model robustness, with Llama-2 achieving the highest overall security (3.4% average
  attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0% average
  attack success rate).'
---

# Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts

## Quick Facts
- **arXiv ID:** 2510.15973
- **Source URL:** https://arxiv.org/abs/2510.15973
- **Reference count:** 7
- **Primary result:** Comprehensive security assessment of four LLMs against four attack types using 1,200 stratified prompts from SALAD-Bench dataset, revealing significant variations in model robustness with Llama-2 achieving highest security (3.4% ASR) while Phi-2 exhibits greatest vulnerability (7.0% ASR).

## Executive Summary
This paper conducts a systematic security assessment of four large language models (Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4) against four distinct adversarial attack categories. Using the SALAD-Bench dataset with 1,200 stratified prompts across six harm categories, the study evaluates both direct attacks and transferability across models. The findings reveal substantial variations in model robustness, with optimization-based attacks showing high transferability (up to 17% success when transferred to other models) and significant differences in vulnerability across harm categories, particularly for malicious use prompts. These results highlight critical cross-model vulnerabilities and provide actionable insights for developing targeted defense mechanisms.

## Method Summary
The study evaluates four LLMs using the SALAD-Bench dataset (21K harmful questions) stratified to 200 prompts per attack type across six harm categories. Four attack methods are employed: human-written prompts from JailbreakChat.com and Reddit sources, AutoDAN (genetic algorithm with population=50, generations=20), Greedy Coordinate Gradient (GCG) optimized on Llama-2 then transferred (suffix length=20, steps=500, lr=0.01), and Tree-of-Attacks-with-Pruning (TAP) using GPT-3.5-Turbo as attacker/judge with tree depth=10. GPT-4 serves as the automated evaluator with human validation via Label Matching Rate. Statistical analysis employs Friedman tests and Wilcoxon signed-rank tests with Bonferroni correction to assess significance across models and harm categories.

## Key Results
- Llama-2 achieved the highest overall security with 3.4% average attack success rate, while Phi-2 showed greatest vulnerability at 7.0% average ASR
- GCG and TAP attacks demonstrated substantial transferability, achieving up to 17% success when transferred to other models, suggesting universal vulnerabilities across architectures
- Malicious use prompts showed the highest vulnerability across all models (10.71% average ASR), with statistically significant differences across harm categories (p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimization-based attacks generated for one model architecture exploit shared vulnerabilities that transfer effectively to other architectures.
- **Mechanism:** Adversarial suffixes identify "universal" perturbation features in semantic embedding space that trigger misalignment in safety classifiers across different models.
- **Core assumption:** High transfer success rates indicate structurally similar safety alignment failures or blind spots common to transformer-based safety filters.
- **Evidence anchors:** Abstract states GCG and TAP achieve substantially higher success rates when transferred (up to 17% for GPT-4); section 4.2.1 notes high transferability suggests fundamental weaknesses shared across model architectures.
- **Break condition:** If safety alignment methodologies diverge fundamentally (e.g., from RLHF to constitutional AI), transferability rates may drop or shift to different attack vectors.

### Mechanism 2
- **Claim:** Model scale and commercial capability do not strictly correlate with security robustness; specific safety training methodologies can yield superior security even in smaller, open-source models.
- **Mechanism:** Targeted safety fine-tuning (e.g., RLHF on adversarial prompts) creates more robust refusal boundaries than general capability scaling.
- **Core assumption:** Lower ASR in Llama-2 results from its specific "Chat" safety training methodology, not optimization artifacts.
- **Evidence anchors:** Abstract shows Llama-2 achieving highest security (3.4% ASR) while Phi-2 exhibits greatest vulnerability; section 5.1.1 indicates open-source models can achieve competitive or superior security compared to commercial alternatives.
- **Break condition:** If attacks are optimized specifically for the stronger model (Llama-2) rather than transferred, the defense advantage might erode.

### Mechanism 3
- **Claim:** Vulnerability is heterogeneous across harm categories, with "Malicious Use" prompts significantly bypassing defenses more often than others.
- **Mechanism:** Safety training data may be denser for easily identifiable toxic content but sparser for complex, functional malicious requests, creating weak spots in the refusal boundary.
- **Core assumption:** Statistical significance (p < 0.001) reflects true structural gaps in model alignment rather than noise in dataset labels.
- **Evidence anchors:** Abstract identifies significant differences in vulnerability across harm categories with malicious use showing highest attack success rates; section 4.4.2 shows GPT-4 demonstrates highest vulnerability to Malicious Use prompts (21.42% ASR).
- **Break condition:** If "Malicious Use" evaluation relies on subtle functional outputs rather than explicit text, the metric may capture evaluation insensitivity rather than true model failure.

## Foundational Learning

- **Concept: Attack Success Rate (ASR) vs. Transfer ASR (T-ASR)**
  - **Why needed here:** To distinguish between a model's inherent vulnerability and its susceptibility to attacks designed for other ecosystems.
  - **Quick check question:** If Model A has 0% ASR and Model B has 5% ASR, but attacks from A transfer to B with 20% success, which model is "safer" for the ecosystem?

- **Concept: White-box vs. Black-box Optimization**
  - **Why needed here:** To understand why GCG (white-box, gradient-based) and TAP (black-box, query-based) yield different transfer profiles.
  - **Quick check question:** Why would a black-box attack (TAP) potentially transfer better or worse than a gradient-based attack (GCG) despite having less information?

- **Concept: Stratified Sampling in Safety Evaluation**
  - **Why needed here:** To ensure reported "average" robustness isn't dominated by easy-to-defend categories while hiding failures in rare but critical categories.
  - **Quick check question:** If a dataset has 90% toxicity prompts and 10% malicious use prompts, how would a simple random sample skew the perceived security of a model?

## Architecture Onboarding

- **Component map:** SALAD-Bench (stratified 1,200 prompts across 6 harm categories) -> Attack Engines (GCG/AutoDAN optimizers + TAP multi-LLM tree search) -> Targets (Phi-2, Llama-2, GPT-3.5, GPT-4) -> Evaluator (GPT-4 automated judge) -> Validation (human meta-evaluation via Label Matching Rate)

- **Critical path:** The validity of the entire evaluation hinges on the Evaluator Validation (LMR). If the GPT-4 judge has low agreement with humans on specific attack types (e.g., TAP had 60% LMR), the reported ASR for those attacks is noisy.

- **Design tradeoffs:**
  - **Cost vs. Sample Size:** Limited API credits ($2,500) restricted sample sizes to n=7 per harm category for detailed breakdowns, reducing statistical power.
  - **Optimization Target:** Attacks were optimized on Llama-2 (white-box constraints), meaning results for GPT models are purely transfer tests, not direct optimization attacks.

- **Failure signatures:**
  - **High Transfer, Low Direct Success:** If a model resists direct optimization but fails on transfer attacks, it indicates overfitting defenses rather than robust alignment.
  - **Evaluator Drift:** Low Label Matching Rate (LMR) for specific models (e.g., Phi-2 at 62.5%) suggests the automated judge struggles to interpret the smaller model's outputs.

- **First 3 experiments:**
  1. **Baseline Check:** Run the Human-Written attacks against all 4 models to verify ASR alignment with the paper's Table 1 (e.g., confirm Llama-2 < GPT-4).
  2. **Transfer Probe:** Generate GCG suffixes on Llama-2 (or use open-source datasets) and test against GPT-3.5/4 to verify the ~15-17% transfer vulnerability.
  3. **Evaluator Calibration:** Manually label 20 random responses from the "Malicious Use" category and compare against the GPT-4 evaluator to check for the 20-100% LMR variance reported.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific defensive strategies are effective against the transferability of optimization-based attacks (like GCG and TAP) across different model architectures?
- **Basis in paper:** [explicit] The authors explicitly call for "Defensive Strategy Development" in Section 6.3 to address transferability risks and category-specific weaknesses.
- **Why unresolved:** The study focused on quantifying vulnerabilities rather than developing or testing defense mechanisms.
- **What evidence would resolve it:** Comparative studies showing reduced Transfer Attack Success Rates (T-ASR) in models employing the proposed defensive training techniques.

### Open Question 2
- **Question:** How can the reliability of automated safety evaluations be improved to reduce the variance in consistency observed with LLM-as-a-judge approaches?
- **Basis in paper:** [explicit] Section 6.3 identifies "Evaluation Methodology Improvement" as a critical direction, noting the substantial variation in Label Matching Rates (20-100%) found in Section 4.3.
- **Why unresolved:** The current reliance on models like GPT-4 for evaluation introduces biases and inconsistencies that current prompts cannot fully mitigate.
- **What evidence would resolve it:** Development of a hybrid human-AI evaluation framework that demonstrates statistically higher inter-rater reliability than the current automated methods.

### Open Question 3
- **Question:** Do the identified vulnerability patterns regarding "Malicious Use" and "Information Safety" hold true when evaluated with larger sample sizes and emerging attack methods?
- **Basis in paper:** [inferred] Section 8 acknowledges that "Evaluation Sample Sizes" were small (7 prompts per category) and "Attack Method Coverage" was limited, impacting statistical power.
- **Why unresolved:** The constraints limited the generalizability of the statistical significance regarding specific harm categories.
- **What evidence would resolve it:** Replication of the study using a larger stratified sample (e.g., >50 per category) and a broader set of adversarial techniques.

## Limitations
- **Evaluator validity:** The automated GPT-4 evaluator shows high variance in Label Matching Rate (LMR), ranging from 20-100% across models and attack types, with particularly low values (60% for TAP attacks) suggesting potential evaluator bias.
- **Transfer attack generalizability:** Transferability results rely on attacks optimized on Llama-2 and transferred to other models, creating uncertainty about whether observed transfer rates reflect true cross-model vulnerabilities or artifacts of the optimization starting point.
- **Resource constraints:** Limited API credits ($2,500) restricted sample sizes to n=7 per harm category for detailed breakdowns, potentially reducing statistical power for category-specific vulnerability analysis.

## Confidence

- **High confidence:** The overall finding that Llama-2 achieves superior security (3.4% ASR) compared to Phi-2 (7.0% ASR) and other models is well-supported by multiple evaluation methods and statistical tests (p < 0.001 for cross-model differences).
- **Medium confidence:** The claim about GCG and TAP achieving up to 17% transfer success rates is supported by the data but constrained by the single optimization target (Llama-2) and evaluator reliability concerns.
- **Medium confidence:** The assertion that malicious use prompts show highest vulnerability (10.71% ASR) is statistically significant but may be influenced by evaluator calibration issues and the specific SALAD-Bench categorization.

## Next Checks
1. **Evaluator calibration validation:** Manually label 50 random responses from each attack type and compute inter-rater agreement with the GPT-4 evaluator to quantify systematic biases and establish confidence intervals for reported ASR metrics.
2. **Cross-optimization transferability test:** Generate attacks optimized on GPT-4 (rather than Llama-2) and measure transfer success rates to Phi-2 and Llama-2 to determine if transfer patterns are symmetric or depend on the optimization target.
3. **Statistical power enhancement:** Increase sample sizes to n=20 per harm category for the most vulnerable categories (Malicious Use, Misinformation) to strengthen the statistical significance of category-specific vulnerability differences.