---
ver: rpa2
title: 'Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual
  Object Tracking'
arxiv_id: '2506.23783'
source_url: https://arxiv.org/abs/2506.23783
tags:
- tracking
- prompt
- event
- vision
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust visual object tracking
  by combining RGB and event camera data, leveraging the computational efficiency
  of Vision Mamba networks. The authors propose Mamba-FETrack V2, a unified framework
  that uses a lightweight Prompt Generator to create modality-specific learnable prompts
  from RGB and event embeddings.
---

# Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking

## Quick Facts
- **arXiv ID:** 2506.23783
- **Source URL:** https://arxiv.org/abs/2506.23783
- **Reference count:** 40
- **Primary result:** Achieves 53.8% Success Rate and 68.2% Precision on FELT V2 with only 30M parameters and 29 FPS

## Executive Summary
Mamba-FETrack V2 introduces a Vision Mamba-based framework for RGB-event visual object tracking that leverages cross-modal prompts for efficient fusion. The method uses a lightweight Prompt Generator to create modality-specific learnable prompts from RGB and event embeddings, which guide cross-modal interaction within a compact FEMamba backbone. Extensive experiments demonstrate state-of-the-art performance on FE108, FELT V2, and COESOT datasets while maintaining computational efficiency with only 30M parameters and 29 FPS inference speed.

## Method Summary
The method employs a Vision Mamba backbone (Vim-S) with prompt-guided cross-modal fusion. A Prompt Generator creates modality-specific prompts using a shared pool and Gumbel-Softmax routing, which are then added to the state space model's C-matrix for cross-modal interaction. The framework processes RGB and event frames through patch embedding and position encoding, followed by parallel branches in the FEMamba backbone. The tracking head performs classification, offset, and bounding box prediction, with optional template updating based on confidence scores.

## Key Results
- Achieves 53.8% Success Rate and 68.2% Precision on FELT V2 dataset
- Maintains 30M parameters and 29 FPS inference speed on RTX 4090
- Outperforms Transformer-based alternatives (ViT-B: 52.3 SR, ~90M parameters) while using 3× fewer parameters
- Demonstrates robustness across FE108, FELT V2, and COESOT benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Routing-based prompt selection enables modality-adaptive feature guidance without manual fusion rules. The Prompt Generator uses a shared pool constructed via element-wise multiplication of modality-specific bases, then applies Gumbel-Softmax to produce differentiable one-hot routing matrices that select relevant prompts per token. This allows the model to dynamically weigh which prompt embeddings influence each spatial location based on input content. Core assumption: The learned routing distributions meaningfully capture modality-specific saliency patterns rather than collapsing to uniform selection.

### Mechanism 2
Cross-modal state space integration via C-matrix additive fusion transfers complementary information efficiently. Within each SSM block, the output matrix C from one modality is augmented by the prompt of the counterpart modality (C' = C + Pe for RGB branch, C' = C + Pr for event branch). This allows event-derived motion cues to modulate RGB feature readout and vice versa, without additional fusion networks. Core assumption: Modifying C preserves gradient stability better than perturbing state transition dynamics (Ā, B̄), and the additive prompt carries sufficient cross-modal signal.

### Mechanism 3
Vision Mamba's linear complexity enables competitive tracking with smaller parameter budgets than Transformer alternatives. The bidirectional SSM processes token sequences with O(N) complexity via selective scanning, avoiding quadratic attention. This permits deeper or wider feature extraction within memory constraints, as evidenced by Vim-S achieving 53.8 SR with 30M parameters versus ViT-B's 52.3 SR with ~90M+ parameters. Core assumption: The sequential scan operation captures sufficient global context for tracking despite lacking explicit pairwise attention.

## Foundational Learning

**State Space Models and discretization**: The FEMamba backbone relies on continuous-to-discrete conversion via Zero-Order Hold (ZOH) to process visual tokens. Understanding h'(t) = Āh(t) + B̄x(t) is essential for debugging gradient flow and modifying fusion positions. Quick check: Can you explain why the Δ (delta) parameter controls the discretization step size and how it affects temporal modeling capacity?

**Event camera data representation**: Converting asynchronous event streams {x, y, t, p} to dense event frames for patch embedding is a non-trivial preprocessing choice affecting motion cue preservation. Quick check: What information is potentially lost when stacking events within an RGB exposure window into a single frame representation?

**Prompt learning and routing mechanisms**: The Prompt Generator's routing network uses MLP + LogSoftmax + Gumbel-Softmax; understanding this pipeline is critical for extending to new modalities or debugging routing collapse. Quick check: Why does Gumbel-Softmax enable backpropagation through discrete selection, and what temperature schedule prevents gradient vanishing?

## Architecture Onboarding

**Component map**: RGB/template (128×128) + Event/template → Patch Embedding + Position Encoding → Prompt Generator (Shared pool → Routing MLP → Gumbel-Softmax → Pr, Pe) → FEMamba Backbone (parallel RGB/event branches with C-matrix fusion) → Tracking Head (Conv-BN-ReLU stack → Classification + Offset + Bounding box) → Template Update (optional, score > 0.5)

**Critical path**: Patch embedding quality determines token representational capacity → Routing matrix sparsity controls prompt specificity → C-matrix fusion timing affects cross-modal interaction depth → Template update threshold balances adaptivity vs. drift risk

**Design tradeoffs**: Prompt pool size T vs. routing granularity (larger T enables finer specialization but increases routing complexity) → Fusion position (C vs. B vs. Δ) (C-fusion is empirically best but may limit state-space modulation) → Vim-S vs. ViT-B (Vim-S offers 3× parameter reduction but 29 FPS vs. ViT variants at 40+ FPS due to limited Mamba GPU optimization)

**Failure signatures**: Routing collapse (if Or/Oe become uniform across tokens, prompts lose modality specificity → check routing entropy during training) → Modality imbalance (if one modality dominates, cross-modal fusion degrades → monitor prompt norms per modality) → Template drift (overly aggressive dynamic updates cause identity switches → validate score head calibration)

**First 3 experiments**: Ablate routing mechanism (replace Gumbel-Softmax with hard argmax or soft attention to isolate routing's contribution to SR gains) → Fusion position sweep (test C-matrix fusion vs. B-matrix vs. multi-position fusion to validate C is optimal for gradient stability) → Modality dropout robustness (train with random modality masking to test cross-modal prompt generalization)

## Open Questions the Paper Calls Out

**Open Question 1**: Can more lightweight visual Mamba architectures be developed to improve inference speed beyond 29 FPS while maintaining tracking accuracy? The authors note current Mamba sequential nature and lack of optimized GPU acceleration limit speed to 29 FPS.

**Open Question 2**: How can adaptive fusion strategies be designed to tailor RGB-Event integration for specific challenging scenarios? The current general approach may not optimally adjust for extreme conditions like low light or fast motion.

**Open Question 3**: Can generative models effectively synthesize features for the weaker modality to resolve modality imbalance in bimodal fusion? The current framework may suffer from one modality dominating the representation.

## Limitations
- Model architecture specificity lacks exact hidden dimensions and layer counts for precise reproduction
- Training configuration gaps omit total training epochs and learning rate scheduling strategy
- Modality fusion generalization not validated on datasets with extreme event density variation or pure RGB scenarios

## Confidence
**High Confidence**: Computational efficiency gains from Vision Mamba's linear complexity; basic tracking pipeline functionality
**Medium Confidence**: Cross-modal fusion effectiveness at C-matrix position; Prompt routing mechanism's adaptive benefits
**Low Confidence**: Generalization to pure RGB tracking scenarios; Template update mechanism's stability

## Next Checks
1. **Routing Mechanism Ablation**: Replace Gumbel-Softmax with hard argmax (no gradients) or soft attention to isolate routing's contribution to the 1.5% SR improvement over baseline
2. **Fusion Position Validation**: Implement B-matrix and Δ-position fusion variants to verify that C-matrix fusion is truly optimal for gradient stability, as claimed
3. **Modality Dropout Robustness**: Train with random RGB/event masking to test whether cross-modal prompts generalize across modality availability or overfit to paired inputs