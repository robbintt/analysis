---
ver: rpa2
title: Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for
  Autonomous Vehicles
arxiv_id: '2601.11781'
source_url: https://arxiv.org/abs/2601.11781
tags:
- learning
- rail
- risk
- safety
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RAIL is a unified runtime framework that fuses heterogeneous risk
  cues into a single Intrusion Risk Score and translates elevated concern into graded,
  interpretable control adjustments blended with the nominal policy. By coupling a
  contextual bandit shield selector with risk-prioritized replay, RAIL achieves strong
  safety and performance: in MetaDrive, it attains a Test Return of 360.65, a Test
  Success Rate of 0.85, and only 29.07 training safety violations, outperforming RL,
  safe RL, offline/imitation learning, and prior HITL baselines.'
---

# Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles

## Quick Facts
- arXiv ID: 2601.11781
- Source URL: https://arxiv.org/abs/2601.11781
- Reference count: 31
- One-line primary result: RAIL achieves strong safety and performance in MetaDrive (Test Return 360.65, Test Success Rate 0.85, 29.07 training safety violations) and improves resilience under cyber-physical attacks (CAN injection Success Rate 0.68, LiDAR spoofing 0.80).

## Executive Summary
RAIL is a unified runtime framework that fuses heterogeneous risk cues into a single Intrusion Risk Score and translates elevated concern into graded, interpretable control adjustments blended with the nominal policy. By coupling a contextual bandit shield selector with risk-prioritized replay, RAIL achieves strong safety and performance: in MetaDrive, it attains a Test Return of 360.65, a Test Success Rate of 0.85, and only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under cyber-physical attacks, RAIL improves Success Rate to 0.68 (CAN injection) and 0.80 (LiDAR spoofing), reduces Disengagement Rate under Attack to 0.37 and 0.03, and lowers Attack Success Rate to 0.34 and 0.11. In CARLA, it reaches a Test Return of 1609.70 and a Test Success Rate of 0.41 with only 8K steps. RAIL's key insight is that detection and control can be tightly integrated into a single interpretable loop, enabling real-time risk mitigation while continuously learning from human oversight.

## Method Summary
RAIL uses SAC as its policy optimization backbone, augmented with a risk-aware control loop. Three risk cues—curvature actuation integrity, time-to-collision proximity, and LiDAR observation-shift consistency—are normalized to [0,1] and fused via a weighted Noisy-OR into an Intrusion Risk Score (IRS). When IRS exceeds threshold τ=0.3, a contextual bandit selects from a small library of shields (e.g., steering guard, brake bias, speed cap) to blend with the nominal action. A risk-prioritized replay buffer upweights high-IRS and human-corrected transitions to steer learning. Training runs for 30K steps in MetaDrive and 8K steps in CARLA with SAC hyperparameters (LR 1e-4, batch 1024, horizon 1000, 10 Hz).

## Key Results
- RAIL attains a Test Return of 360.65 and Test Success Rate of 0.85 in MetaDrive with only 29.07 training safety violations.
- Under CAN injection, RAIL improves Success Rate to 0.68 and reduces Attack Success Rate to 0.34.
- Under LiDAR spoofing, RAIL improves Success Rate to 0.80 and reduces Attack Success Rate to 0.11.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The weighted Noisy-OR fusion of heterogeneous risk cues provides sensitivity to single-cue anomalies while resisting false negatives that plague additive scoring.
- Mechanism: Three cues—curvature actuation integrity, time-to-collision proximity, and LiDAR observation-shift—are normalized to [0,1] and fused via `IRS_t = 1 - ∏(1 - w_i * r_i)`. This multiplicative structure models the probability that at least one cue indicates danger; a single high-confidence spike (e.g., TTC collapse from phantom obstacle) elevates IRS even if other cues are quiescent.
- Core assumption: Risk cues are partially independent and capture complementary failure modes (actuation corruption, imminent collision, sensor spoofing).
- Evidence anchors:
  - [abstract] "fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR"
  - [section III.B] "Unlike additive scoring, this structure avoids risk underestimation by accounting for inter-cue redundancy and non-linear compounding"
  - [corpus] Weak direct support; corpus focuses on intrusion detection transformers and federated learning rather than probabilistic risk fusion
- Break condition: If cues become highly correlated (e.g., TTC and curvature both driven by same underlying fault), Noisy-OR may overcount risk; threshold tuning would be required.

### Mechanism 2
- Claim: Contextual bandit arbitration over cue-specific shields enables graded, interpretable control adjustments that improve online without full policy retraining.
- Mechanism: Each shield k maintains a linear score z_k,t = θ_k^T c_t over the cue vector c_t. Softmax selection retains exploration; delayed success feedback (no takeover within horizon Δ minus effort penalty) drives lightweight online updates to θ_k. The executor blends: u_t = (1-α_t)a_t + α_t * S_k_t(a_t, s_t), where α_t scales with smoothed IRS.
- Core assumption: Shield effectiveness is context-dependent but learnable from sparse binary success signals; the shield library is sufficiently expressive.
- Evidence anchors:
  - [abstract] "A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online"
  - [section III.C] "The design yields (i) locality—only the risky channel is modified, (ii) monotonicity—shield authority grows with risk, and (iii) auditability"
  - [corpus] IRSDA mentions agent-orchestrated intrusion response but does not validate contextual bandits for real-time vehicle control
- Break condition: If the shield library lacks coverage for a novel attack mode, bandit will cycle among ineffective options; human override becomes the fallback.

### Mechanism 3
- Claim: Risk-prioritized replay concentrates gradient updates on safety-critical transitions, improving sample efficiency and reducing training violations.
- Mechanism: Transitions store annotations (IRS_t, dominant cue i*, shield k_t, takeover flag). Sampling priority combines TD error, IRS magnitude, and takeover flag. High-IRS and human-corrected transitions are upweighted while diversity is preserved via importance sampling corrections.
- Core assumption: Safety-critical transitions are underrepresented in nominal experience but contain disproportionate learning signal for constraint satisfaction.
- Evidence anchors:
  - [abstract] "RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning"
  - [section III.D] "Training alternates brief interaction bursts to refresh bandit context with off-policy SAC updates from the prioritized buffer; in practice, human-takeover steps receive the highest priority"
  - [corpus] Not addressed in corpus; corpus papers focus on detection rather than replay prioritization
- Break condition: If the priority exponent is too aggressive, nominal-behavior coverage degrades and the policy may forget task performance.

## Foundational Learning

- Concept: Soft Actor-Critic (SAC) off-policy RL
  - Why needed here: RAIL uses SAC as its policy optimization backbone; understanding entropy regularization, dual critics, and off-policy learning is required to modify the training loop.
  - Quick check question: Can you explain why SAC's entropy term helps exploration in continuous control, and how off-policy learning differs from PPO's on-policy updates?

- Concept: Contextual bandits with linear payoffs
  - Why needed here: Shield selection is cast as a contextual bandit; you must understand softmax action selection, delayed reward assignment, and online weight updates.
  - Quick check question: Given a context vector c_t and arm weights θ_k, how would you compute selection probabilities and update weights after receiving a reward?

- Concept: Noisy-OR probabilistic models
  - Why needed here: IRS fusion assumes Noisy-OR semantics; understanding why multiplicative fusion avoids risk underestimation vs. additive scoring is critical.
  - Quick check question: If three cues have risks r = [0.1, 0.05, 0.8] and equal weights w = 0.33, what is the IRS under Noisy-OR vs. weighted average?

## Architecture Onboarding

- Component map: Observation → 3 cue extractors (curvature, TTC, OOD) → IRS calculator (weighted Noisy-OR) → threshold check → contextual bandit selector → shield library (steering guard, brake bias, speed cap) → action blender → executor → log transition with annotations → risk-prioritized replay buffer → SAC optimizer

- Critical path: Observation → 3 cue extractors → IRS → (if IRS > τ) bandit selects shield → blend with nominal action → execute → log transition with annotations → prioritize in buffer → SAC update

- Design tradeoffs:
  - **Noisy-OR vs. additive fusion**: Noisy-OR is more sensitive to single-cue spikes but may overcount correlated cues; additive is smoother but can miss isolated anomalies.
  - **Shield library size**: Small library preserves interpretability but may lack coverage; large library increases bandit learning burden.
  - **Priority exponent**: Higher values focus learning on safety-critical transitions but risk catastrophic forgetting of nominal behavior.

- Failure signatures:
  - **Chattering**: Rapid shield switching; check softmax temperature annealing and rate limiter on α_t
  - **Conservative freeze**: IRS stuck high; check cue normalization and threshold calibration
  - **Bandit collapse**: Single shield dominates; check reward signal (success feedback may be too sparse)
  - **Replay starvation**: Nominal transitions rarely sampled; reduce priority exponent or increase diversity buffer fraction

- First 3 experiments:
  1. **Cue ablation**: Remove each cue (curvature, TTC, OOD) individually and measure SR, SV, DR. Table IV shows TTC is most critical (SR drops to 0.67 without it).
  2. **Threshold sweep**: Vary τ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} and plot tradeoff between false positive rate (unnecessary shielding) and attack success rate under CAN injection.
  3. **Bandit vs. fixed shield**: Compare contextual bandit selection against fixed shield assignment; Table IV shows fixed shields raise SV to 1.10 and DR to 0.018.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive thresholds and shield parameterization provide formal safety guarantees rather than heuristic risk mitigation?
  - Basis in paper: [explicit] The authors state the intent to "study adaptive thresholds and shield parameterization with formal safety envelopes."
  - Why unresolved: The current implementation relies on a fixed threshold (τ=0.3) and learned weights without providing formal proofs of safety bounds.
  - What evidence would resolve it: Theoretical verification or empirical results showing guaranteed constraint satisfaction under defined uncertainty bounds.

- **Open Question 2**: How does RAIL scale to multi-agent traffic environments where vehicles share risk information cooperatively?
  - Basis in paper: [explicit] Future work aims to "scale to multi-agent traffic with cooperative risk sharing."
  - Why unresolved: The current evaluation is restricted to a single ego vehicle navigating environments with non-cooperative background traffic.
  - What evidence would resolve it: Performance metrics (Success Rate, Return) in multi-agent scenarios demonstrating maintained safety with shared communication.

- **Open Question 3**: Does RAIL maintain real-time performance and safety robustness when transferred to hardware-in-the-loop platforms?
  - Basis in paper: [explicit] The paper lists plans to "transfer to hardware-in-the-loop tests."
  - Why unresolved: Results are derived entirely from software simulations (MetaDrive, CARLA), which may not capture real-world sensor latencies or actuator dynamics.
  - What evidence would resolve it: Successful deployment on physical driving rigs or hardware-in-the-loop systems with comparable safety violation rates.

## Limitations
- Shield transformations and dual reward coefficients are unspecified, requiring assumptions that may not match the original implementation.
- Bandit hyperparameters (temperature schedule, success horizon) and risk-prioritized replay weights are missing, preventing exact reproduction.
- CARLA attack injection specifics (LiDAR spoofing pattern, CAN injection timing) are vague, limiting replication of attack resilience results.

## Confidence
- **High**: Noisy-OR IRS fusion improves sensitivity to single-cue anomalies vs. additive scoring; contextual bandit arbitration improves online shield selection vs. fixed shield.
- **Medium**: Risk-prioritized replay concentrates learning on safety-critical transitions; tight integration of detection and control enables real-time mitigation.
- **Low**: Shield library expressiveness and generalizability; human takeover simulation fidelity; CARLA attack success rates.

## Next Checks
1. **Shield library ablation**: Remove each shield type (steering guard, brake bias, speed cap) individually and measure impact on Success Rate and Safety Violations under CAN injection and LiDAR spoofing.
2. **Replay prioritization sensitivity**: Sweep priority exponent α ∈ {0.0, 0.5, 1.0, 2.0} and plot tradeoff between Success Rate and Training Safety Violations.
3. **Human takeover simulation**: Replace simulated takeovers with random takeover triggers and measure degradation in Success Rate and increase in Training Safety Violations.