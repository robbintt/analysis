---
ver: rpa2
title: Benchmarking Prompt Sensitivity in Large Language Models
arxiv_id: '2502.06065'
source_url: https://arxiv.org/abs/2502.06065
tags:
- prompt
- variations
- sensitivity
- https
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompt Sensitivity Prediction as a novel
  task to evaluate how small variations in prompts affect LLM response accuracy. The
  authors created PromptSET, a dataset of 114K prompt variations derived from TriviaQA
  and HotpotQA, and annotated their answerability by LLMs.
---

# Benchmarking Prompt Sensitivity in Large Language Models

## Quick Facts
- arXiv ID: 2502.06065
- Source URL: https://arxiv.org/abs/2502.06065
- Reference count: 40
- Primary result: Novel task to predict LLM response accuracy to prompt variations; BERT-PE performs best but struggles on complex reasoning tasks

## Executive Summary
This paper introduces Prompt Sensitivity Prediction as a novel task to evaluate how small variations in prompts affect LLM response accuracy. The authors created PromptSET, a dataset of 114K prompt variations derived from TriviaQA and HotpotQA, and annotated their answerability by LLMs. They benchmarked existing methods including LLM self-evaluation, text classification, and query performance prediction techniques. Results show that while supervised QPP methods (e.g., BERT-PE) perform reasonably well on TriviaQA, all baselines struggle significantly with prompt sensitivity prediction, particularly on HotpotQA. The study highlights the challenges in predicting LLM response quality to prompt variations and demonstrates that prompt reformulation can sometimes convert unanswerable prompts into answerable ones.

## Method Summary
The authors created PromptSET by sampling 11,469 questions from TriviaQA and HotpotQA (4-40 words each), generating 9 variations per prompt using LLaMA 3.1 or Mistral-nemo, and evaluating each variation's answerability using target LLMs. They implemented three baseline approaches: LLM self-evaluation, BERT-based text classification, and query performance prediction (QPP) methods including specificity metrics (CC, DC, IEF, PageRank) and supervised BERT-PE. The dataset was split 70/30 for training/testing, and all methods were evaluated on binary classification accuracy of predicting whether variations would yield correct answers.

## Key Results
- Supervised QPP methods (BERT-PE) outperform unsupervised specificity metrics and LLM self-evaluation on TriviaQA
- All baselines struggle significantly with prompt sensitivity prediction on HotpotQA, particularly for multi-hop reasoning tasks
- BERT-PE achieves 0.651 F1 on TriviaQA but only 0.318 F1 on HotpotQA
- Systematic prompt reformulation can convert some unanswerable prompts into answerable ones through variation generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt sensitivity prediction accuracy is likely correlated with the semantic proximity of a variation to the original prompt.
- **Mechanism:** The paper hypothesizes that LLMs respond more accurately to variations that closely resemble the original prompt due to potential memorization or alignment with training data. Conversely, as variations drift semantically or syntactically, the model's ability to map the input to the correct internal representation degrades.
- **Core assumption:** The model's performance on the "original" prompt represents a ground-truth capability that degrades with distance, rather than the variation simply being a harder or different task.
- **Evidence anchors:**
  - [section 3]: "We observe that when a variation closely resembles the original prompt, it is more likely to generate both correct responses... This suggests that the model may have encountered this data points before."
  - [figure 1b/1c]: Shows higher frequency of correct responses at higher similarity buckets.
  - [corpus]: Indirect support from *Re-Evaluating Code LLM Benchmarks Under Semantic Mutation*, which examines robustness under semantic changes.
- **Break condition:** If the "original" prompt is actually sub-optimal (adversarial or poorly phrased) and the variation improves clarity, the correlation between similarity and accuracy would invert.

### Mechanism 2
- **Claim:** Supervised Query Performance Prediction (QPP) methods (specifically BERT-based estimators) transfer more effectively to prompt sensitivity prediction than unsupervised specificity metrics.
- **Mechanism:** BERT-PE leverages contextualized embeddings to learn a mapping between query representation and performance. While unsupervised metrics (e.g., clarity, specificity) rely on statistical properties of the text, supervised methods can learn implicit patterns of "answerability" from the training data (TriviaQA), making them more adaptable to the generative setting.
- **Core assumption:** The features that predict document retrieval difficulty are sufficiently similar to the features that predict generative answerability.
- **Evidence anchors:**
  - [abstract]: "...while supervised QPP methods (e.g., BERT-PE) perform reasonably well on TriviaQA, all baselines struggle significantly... particularly on HotpotQA."
  - [table 2]: BERT-PE consistently outperforms unsupervised QPP (IEF, PageRank) and LLM self-evaluation on TriviaQA.
  - [corpus]: Weak/No direct corpus evidence for BERT-PE specifically in this context; transfer is a hypothesis of the paper.
- **Break condition:** Performance degrades significantly on multi-hop reasoning tasks (HotpotQA), suggesting the learned features do not generalize to complex reasoning requirements.

### Mechanism 3
- **Claim:** Systematic prompt reformulation serves as a mechanism to recover answerability for initially failed prompts.
- **Mechanism:** By generating a distribution of variations (exploring the latent space around the original query), the system increases the probability of finding a formulation that triggers the correct retrieval path or reasoning chain in the LLM, effectively converting "unanswerable" inputs into "answerable" ones.
- **Core assumption:** The failure to answer the original prompt is often a function of phrasing or representation rather than a fundamental lack of model knowledge.
- **Evidence anchors:**
  - [section 3]: "...in the red cases, at least one of the two LLMs succeeded in answering a variation correctly... highlight the potential of prompt reformulation as a strategy."
  - [figure 2]: Visualizes cases where original prompts failed but variations succeeded.
- **Break condition:** If the information need is entirely outside the model's training corpus, reformulation will fail (hallucination or refusal).

## Foundational Learning

- **Concept:** **Pre-Retrieval Query Performance Prediction (QPP)**
  - **Why needed here:** The paper adapts QPP—traditionally used to predict search engine result quality—to predict LLM generative quality. Understanding how metrics like "specificity" or "clarity" estimate query difficulty is essential to interpreting the baseline results.
  - **Quick check question:** Can a pre-retrieval metric predict an LLM's accuracy without seeing the generated response? (The paper suggests only supervised methods do this well).

- **Concept:** **Semantic Similarity vs. Robustness**
  - **Why needed here:** The paper relies on generating variations that preserve semantic intent ($I_p == I_{p'}$) while testing sensitivity. One must distinguish between a "valid paraphrase" and an "adversarial perturbation."
  - **Quick check question:** If a variation has low cosine similarity to the original prompt but the same answer, is it a sensitivity issue or a robustness feature?

- **Concept:** **Dataset Contamination / Memorization**
  - **Why needed here:** The paper hypothesizes that high performance on original prompts (and similar variations) is due to the model having "seen" the data points before.
  - **Quick check question:** Why might a model answer "What is the capital of France?" correctly but fail on "Tell me the name of France's capital city?" (Context: Contamination vs. Generalization).

## Architecture Onboarding

- **Component map:**
  1.  **Source Selector:** Ingests TriviaQA/HotpotQA (Gold Standard Q&A).
  2.  **Variation Engine:** LLaMA 3.1 / Mistral generate $N$ paraphrases (PromptSET).
  3.  **Filter:** Validates $Sim(p, p') > \tau$ and checks for hallucination.
  4.  **Evaluator:** Target LLM generates answers for all variations.
  5.  **Labeler:** Binary classification (Answerable/Unanswerable) based on exact match/semantic alignment with Gold Standard.
  6.  **Predictor (Baselines):** BERT-PE, Text Classifiers, Self-Evaluation.

- **Critical path:** The **Variation Engine**. If the paraphrases hallucinate new entities or drift semantically, the "sensitivity" being measured is actually "intent drift," invalidating the benchmark.

- **Design tradeoffs:**
  - **Open Source (LLaMA/Mistral) vs. Closed Source (GPT-4):** Paper chooses Open Source for reproducibility, potentially limiting the "upper bound" of capability compared to SOTA commercial models.
  - **Exact Match Evaluation:** Using strict matching for labeling answerability is robust for factoid questions (TriviaQA) but may penalize valid semantic variations in the answer text.

- **Failure signatures:**
  - **High F1 on TriviaQA, Low on HotpotQA:** Indicates the predictor relies on surface-level pattern matching rather than reasoning complexity estimation.
  - **Low Specificity Metric Variance:** Unsupervised QPP metrics (DC, IEF) fail because the lexical specificity of a paraphrase is often too similar to the original to be discriminative.

- **First 3 experiments:**
  1.  **Reproduce the Similarity Correlation:** Plot the accuracy of LLaMA-3.1 on PromptSET variations against MiniLM cosine similarity to the original prompt to verify the "memorization bias" hypothesis.
  2.  **Baseline the Baseline:** Train a simple BERT classifier on the *training* split of PromptSET (as done in the paper) to establish the "Supervised Text Classification" upper bound before trying new architectures.
  3.  **Reformulation Recovery Test:** Take the subset of prompts where LLaMA failed (0% accuracy on original) and measure the "Recovery Rate"—the percentage of these prompts where at least 1 of the 9 variations yielded a correct answer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What novel methods specifically designed for prompt sensitivity prediction can substantially outperform current baselines (BERT-PE, LLM self-evaluation, text classification) across both simple factoid and complex multi-hop reasoning tasks?
- Basis in paper: [explicit] The authors conclude that "existing methods do not fully capture the complexities of prompt sensitivity" and explicitly call for "further research into prompt variation sensitivity, particularly in developing methods to help users generate more reliable prompts."
- Why unresolved: All tested baselines struggle significantly, especially on HotpotQA where even the best method (BERT-PE) achieves only 0.318 F1, compared to 0.651 on TriviaQA.
- What evidence would resolve it: New methods achieving consistently high F1 scores (>0.70) across both dataset types, particularly improving performance on complex multi-hop questions.

### Open Question 2
- Question: How does prompt sensitivity generalize across different LLM architectures, parameter scales, and training paradigms beyond the two tested models?
- Basis in paper: [inferred] The study only evaluates two open-source models (LLaMA 3.1 8B and Mistral-nemo), leaving untested how findings transfer to other model families, sizes, or proprietary systems.
- Why unresolved: The paper provides no evidence of whether sensitivity patterns are model-specific or universal, yet shows LLaMA and Mistral sometimes disagree on which variations are answerable.
- What evidence would resolve it: Benchmarking PromptSET across diverse models (GPT, Claude, smaller/larger parameter variants) showing whether sensitivity patterns correlate across architectures.

### Open Question 3
- Question: Can prompt sensitivity prediction methods be developed that generalize to genuinely novel prompt formulations beyond the training distribution?
- Basis in paper: [explicit] The authors note that "when a variation closely resembles the original prompt, it is more likely to generate both correct responses and accurate predictions of answerability," suggesting "a strong bias toward its training data and reduced generalizability to less familiar or novel prompt formulations."
- Why unresolved: Current methods appear to exploit similarity to known prompts rather than capturing fundamental properties of what makes prompts answerable.
- What evidence would resolve it: Methods that maintain prediction accuracy on low-similarity variations or entirely new prompt structures not seen during training.

## Limitations

- The use of exact match criteria for answerability labeling may artificially penalize semantically correct variations
- The study only evaluates two LLM families (LLaMA 3.1 and Mistral), limiting generalizability to other model architectures
- The PromptSET dataset derives from only two QA datasets, potentially introducing domain-specific biases
- The paper does not account for potential temporal variations in LLM performance across different inference runs

## Confidence

- **High Confidence:** The observation that supervised QPP methods (BERT-PE) outperform unsupervised specificity metrics on TriviaQA. This is directly supported by the empirical results in Table 2 with clear performance differentials.
- **Medium Confidence:** The hypothesis that prompt sensitivity prediction accuracy correlates with semantic proximity to the original prompt. While the paper shows this correlation exists, the causal mechanism (memorization vs. generalization) remains speculative without ablation studies.
- **Low Confidence:** The claim that prompt reformulation can reliably recover answerability for initially failed prompts. The paper presents anecdotal evidence from figure 2 but lacks statistical quantification of recovery rates or systematic analysis of which reformulation types work best.

## Next Checks

1. **Recovery Rate Quantification:** Systematically measure the percentage of initially unanswerable prompts that become answerable through variation reformulation, stratified by original prompt difficulty and variation type. This would validate the practical utility of the reformulation mechanism.

2. **Cross-Model Generalization Test:** Evaluate the trained BERT classifier on PromptSET variations generated by different LLMs (e.g., test on LLaMA-generated variations while training on Mistral-generated ones). This would reveal whether the model learns variation patterns or dataset-specific artifacts.

3. **Robustness to Labeling Criteria:** Repeat the main experiments using semantic similarity thresholds (e.g., cosine similarity > 0.8 between generated and ground truth answers) instead of exact match, to assess sensitivity to the answerability labeling approach.