---
ver: rpa2
title: Benchmarking Optimizers for Large Language Model Pretraining
arxiv_id: '2509.01440'
source_url: https://arxiv.org/abs/2509.01440
tags:
- learning
- rate
- tokens
- decay
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of 11 optimization
  methods for large language model pretraining, systematically varying model size,
  batch size, and training duration. Through extensive hyperparameter tuning across
  dense and MoE architectures (124M-720M parameters), the study reveals that AdEMAMix
  consistently achieves state-of-the-art performance, while sign-based methods (Signum,
  Lion) and MARS benefit significantly from larger batch sizes.
---

# Benchmarking Optimizers for Large Language Model Pretraining

## Quick Facts
- **arXiv ID:** 2509.01440
- **Source URL:** https://arxiv.org/abs/2509.01440
- **Reference count:** 40
- **Key outcome:** AdEMAMix consistently achieves state-of-the-art performance across dense and MoE architectures (124M-720M parameters), while sign-based methods benefit significantly from larger batch sizes.

## Executive Summary
This paper presents a comprehensive benchmark of 11 optimization methods for large language model pretraining, systematically varying model size, batch size, and training duration. Through extensive hyperparameter tuning, the study reveals that AdEMAMix consistently achieves state-of-the-art performance, while sign-based methods (Signum, Lion) and MARS benefit significantly from larger batch sizes. Key findings include the critical importance of proper weight decay implementation, the sensitivity of schedule-free methods to gradient clipping, and the need to re-tune optimizer hyperparameters when scaling training duration. The authors open-source their benchmarking toolkit and provide actionable insights for practitioners, challenging the long-held dominance of AdamW and establishing a foundation for future optimizer development in large-scale language model training.

## Method Summary
The study benchmarks 11 optimization methods on Llama-like architectures ranging from 124M to 720M parameters, trained on a 100B token subset of FineWeb using the GPT-2 tokenizer. The benchmark covers extensive hyperparameter sweeps including learning rates, betas, weight decay, and training durations from 2.1B to 16.8B tokens. Training uses mixed precision (bfloat16) with float32 optimizer states, and validation loss is measured without z-loss regularization. The methodology includes systematic testing of batch sizes, weight decay implementations, and learning rate schedules across optimizer families.

## Key Results
- AdEMAMix consistently outperforms other optimizers, achieving state-of-the-art validation loss across all tested configurations
- Sign-based optimizers (Signum, Lion) and MARS show substantial performance gains when batch size increases, approaching or exceeding AdamW performance
- Proper weight decay implementation (decoupled vs. coupled) has a critical impact on final model performance
- Training duration significantly affects optimal hyperparameter values, particularly requiring increased β₂ for AdamW-like methods in longer runs

## Why This Works (Mechanism)

### Mechanism 1: Batch Size Scaling for Sign-based Methods
Sign-based optimizers discard gradient magnitude, retaining only direction. With small batches, high gradient variance means the sign can flip erratically, producing noisy updates that slow convergence. Larger batches reduce variance through averaging, making the sign direction more stable and informative. This explains why Signum, Lion, and MARS underperform with small batches but approach or exceed AdamW performance when batch size increases sufficiently.

### Mechanism 2: Weight Decay Implementation Criticality
Decoupled weight decay (applied after optimizer update) substantially outperforms coupled L2 regularization (added to loss). This direct shrinkage of weights independent of gradient statistics prevents the optimizer's adaptive scaling from interfering with regularization. For short runs, high decay (0.5) accelerates convergence by aggressively controlling model norm; for long runs, moderate decay (0.1) provides stable regularization without excessive shrinkage that harms final performance.

### Mechanism 3: Training-Duration-Dependent Momentum Tuning
Second-moment momentum buffers accumulate historical gradient information. For longer training, smaller β values cause the buffer to "forget" older useful information too quickly, while larger β values retain more history, stabilizing updates over extended optimization trajectories. This explains why increasing β₂ for longer training substantially benefits AdEMAMix and other AdamW-like methods.

## Foundational Learning

- **Optimizer State Variables** (momentum buffers, variance estimates)
  - Why needed here: To understand why β parameters matter and how different optimizers store information differently (AdamW: 2 buffers, Signum: 1 buffer, AdEMAMix: 3 buffers)
  - Quick check question: Can you explain what information is lost when β₂ decreases from 0.999 to 0.95 in AdamW?

- **Gradient Variance vs. Batch Size**
  - Why needed here: To predict which optimizers will benefit from larger batches and why sign-based methods are particularly sensitive
  - Quick check question: If you double the batch size, what happens to the variance of gradient estimates, and how does this affect sign-based updates?

- **Learning Rate Schedules vs. Optimizer-Internal Adaptation**
  - Why needed here: The paper shows scheduler choice interacts with optimizer choice (e.g., WSD works well with Muon, cosine with most others), requiring understanding of external vs. internal learning rate modulation
  - Quick check question: How does Prodigy's effective learning rate (γeff) differ from a manually scheduled learning rate?

## Architecture Onboarding

- **Component map:** Optimizer core -> State management -> Hyperparameter groups -> Parameter type handling -> Scheduler integration

- **Critical path:**
  1. Select optimizer based on batch size and model scale constraints
  2. Set up proper weight decay implementation (decoupled, verify framework behavior)
  3. Tune learning rate and beta parameters for target training duration
  4. Configure learning rate scheduler appropriate to optimizer
  5. Implement gradient clipping with optimizer-specific thresholds

- **Design tradeoffs:**
  - AdEMAMix vs. AdamW: AdEMAMix consistently better but requires 3 buffers (50% more memory)
  - Signum/Lion vs. AdamW: Memory efficient (1 buffer) but requires large batches; may diverge with high learning rates
  - SOAP: Strong at Chinchilla-optimal duration but slows down with model size; complex preconditioner implementation
  - Schedule-free methods: Reduce tuning burden but sensitive to β parameters and gradient clipping

- **Failure signatures:**
  - Signum/Lion: Divergence or slow convergence with small batches or high learning rates (γ > 1e-3)
  - Sophia: Divergence after ~7× Chinchilla optimal duration (instability in Hessian approximation)
  - SF-AdamW: Loss spikes without gradient clipping (contrary to original paper's claim)
  - Muon: Poor performance without weight decay on 2D parameters (use D-Muon variant)
  - MARS: Underperformance at small batch sizes (needs >1M tokens/batch for best results)

- **First 3 experiments:**
  1. Baseline comparison: Train 124M model with AdamW, AdEMAMix, and Signum at batch size 256×512 for 2.1B tokens. Measure validation loss, wall-clock time, and memory usage.
  2. Batch size sensitivity: Take Signum and Lion from experiment 1, repeat at batch size 32×512 and 512×512 for same token count.
  3. Duration re-tuning: Take best hyperparameters from 2.1B token runs, extend to 16.8B tokens without re-tuning, then re-tune β₂ values and measure gap.

## Open Questions the Paper Calls Out

- **Do the relative rankings of optimizers based on validation loss transfer to downstream task performance?** The study acknowledges that bridging the gap between loss minimization and downstream task performance is important but leaves this investigation to future research.

- **Can the observed performance gains of novel optimizers over AdamW be sustained at production scales (billions of parameters and trillions of tokens)?** The authors note that training behavior may change further at practical scales and their experiments are limited to models under 1B parameters.

- **Does the optimal final learning rate (γend) depend on the specific optimizer family (e.g., sign-based vs. Adam-like)?** While the study established the importance of decaying γ beyond standard norms, it did not fully map this interaction across all optimizer architectures.

## Limitations

- The study's narrow architectural scope focuses exclusively on Llama-like architectures with SwiGLU activations and RMSNorm, constraining generalizability to other model families.

- All experiments use dense and standard MoE architectures, potentially missing behaviors in advanced MoE variants or multi-modal models.

- While hyperparameter tuning is extensive, the study uses a fixed 100B token FineWeb subset, which may not represent all pretraining data distributions.

## Confidence

- **AdEMAMix consistently achieves SOTA performance:** High confidence - supported by extensive head-to-head comparisons across multiple model sizes and training durations
- **Sign-based methods benefit from larger batch sizes:** High confidence - the mechanism is theoretically sound and empirically validated
- **Weight decay implementation critically affects performance:** High confidence - the decoupled vs. coupled distinction is well-established
- **Training duration affects optimal hyperparameter values:** Medium confidence - while empirical results show trends, the mechanism is less clearly established

## Next Checks

1. **Cross-architecture validation:** Implement the same benchmark using BERT-like architectures (full attention, LayerNorm) to verify whether optimizer rankings hold across architectural families.

2. **Production-scale validation:** Extend the benchmark to 1-10B parameter models with industrial-scale batch sizes (100M-1B tokens per batch) to confirm scaling behavior predictions.

3. **Data distribution sensitivity:** Repeat key experiments using different pretraining datasets (C4, The Pile, custom domain-specific corpora) to quantify how sensitive optimizer performance is to data distribution.