---
ver: rpa2
title: Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially
  Observable Urban Environments
arxiv_id: '2510.23899'
source_url: https://arxiv.org/abs/2510.23899
tags:
- evacuee
- fire
- human
- panic
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a coordinated multi-UAV system for real-time
  human guidance during urban fire evacuations under uncertainty. It models evacuee
  behavior using a psychologically grounded agent-based model that captures panic-induced
  deviations from optimal routes, and formulates UAV coordination as a POMDP to handle
  partial observability and dynamic fire spread.
---

# Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments

## Quick Facts
- arXiv ID: 2510.23899
- Source URL: https://arxiv.org/abs/2510.23899
- Reference count: 40
- Multi-UAV system guides evacuees to safety with up to 70% faster evacuation times

## Executive Summary
This paper introduces a coordinated multi-UAV system for real-time human guidance during urban fire evacuations under uncertainty. It models evacuee behavior using a psychologically grounded agent-based model that captures panic-induced deviations from optimal routes, and formulates UAV coordination as a POMDP to handle partial observability and dynamic fire spread. Two heterogeneous UAVs—a high-level planner for broad situational awareness and a low-level interceptor for close-range guidance—are trained using centralized multi-agent reinforcement learning with recurrent policies and PPO. Simulation results show that the UAV team can locate and guide evacuees in about 17.5 timesteps, achieving up to 70% improvement in evacuation time and maintaining strong performance (over 40% success rate) even in environments that differ substantially from training conditions.

## Method Summary
The framework trains two heterogeneous UAV agents—High-Level Rescuer (HLR) for surveillance and Low-Level Rescuer (LLR) for interception—using centralized multi-agent PPO with recurrent policies. The evacuee follows an agent-based model with panic parameter γ(t) blending optimal and herd behaviors. UAVs maintain implicit belief states via LSTM encoding of observation histories, sharing information to coordinate search and guidance. Training uses reward shaping to incentivize visibility, approach, and capture within a partially observable urban grid with stochastic fire spread.

## Key Results
- UAV team achieves 70% improvement in evacuation time compared to panic-only baseline
- Maintains over 40% success rate even with 10-cell position randomization from training conditions
- Average capture time of 17.5 timesteps across varied fire origins and evacuee positions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Heterogeneous role specialization enables efficient search-and-intercept coordination under partial observability.
- **Mechanism:** The HLR operates at altitude with wide field-of-view to estimate evacuee location probabilistically, while the LLR navigates close-range to physically intercept and guide. Shared observations between agents allow the HLR's broad awareness to direct the LLR's interception capability. Centralized training with a joint policy aligns incentives despite asymmetric observation and action spaces.
- **Core assumption:** Communication between UAVs is reliable and low-latency; occlusion types are correctly classified for each agent.
- **Evidence anchors:**
  - [abstract] "two heterogeneous UAV agents, a high-level rescuer (HLR) and a low-level rescuer (LLR), coordinate through shared observations and complementary capabilities"
  - [Section III] Defines HLR with "wide field of view" for tracking, LLR as "only agent capable of physically intercepting"
  - [corpus] Weak direct evidence; related UAV coordination work (AeroResQ, Coverage-Recon) addresses multi-drone collaboration but not heterogeneous search-intercept roles
- **Break condition:** Communication failure between UAVs; HLR cannot transfer location estimates; LLR cannot navigate to indicated positions.

### Mechanism 2
- **Claim:** Panic-parameterized agent-based modeling captures deviation from rational routes, enabling anticipatory UAV guidance.
- **Mechanism:** The evacuee's velocity is computed as a weighted blend of optimal (goal-directed) and herd (panic-induced) components: v(t) = (1 − γ(t)) · v_optimal + γ(t) · v_herd. Panic γ(t) is updated based on distance-to-exit, velocity misalignment with neighbors, nearby fire visibility, and observed discomfort in others. This produces non-optimal trajectories that UAVs must predict and intercept.
- **Core assumption:** The four panic factors (δ₁–δ₄) and their weighting sufficiently approximate real human stress responses; social forces can be modeled exogenously without explicit agent-to-agent interactions.
- **Evidence anchors:**
  - [abstract] "panic dynamically affects decision-making and movement in response to environmental stimuli"
  - [Section IV-A] Formal panic update equation and velocity blending; references Trivedi and Rao [10] for empirical grounding
  - [corpus] No direct validation of this specific panic model; VR Fire safety training paper addresses simulation but not panic dynamics
- **Break condition:** Real evacuee behavior deviates significantly from the modeled panic dynamics (e.g., freezing not captured, or group cohesion stronger than modeled).

### Mechanism 3
- **Claim:** Recurrent PPO policies maintain implicit belief states over evacuee location, enabling robust search under uncertainty.
- **Mechanism:** Since the evacuee's initial position is unknown and observations are partial, each UAV maintains an observation-action history h_t encoded via LSTM. The joint policy π(h_t) → a_t is trained with PPO to maximize expected cumulative reward, balancing exploration (locating evacuee) and exploitation (approaching once seen). Reward shaping incentivizes FOV coverage, proximity improvement, and capture.
- **Core assumption:** LSTM can learn sufficient belief-state representations; reward coefficients generalize across environment variations.
- **Evidence anchors:**
  - [abstract] "PPO algorithm with recurrent policies to enable robust decision-making in partially observable settings"
  - [Section IV-B] POMDP tuple definition; LSTM encoding of observation histories; reward function promoting visibility and capture
  - [corpus] Learning to Communicate in Multi-Agent RL addresses POMDP settings but uses explicit communication, not implicit recurrent belief states
- **Break condition:** History window insufficient for long-horizon inference; reward hacking (e.g., cycling to maximize FOV rewards without progressing toward capture).

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The environment state (evacuee location, fire spread, panic level) is not fully observable. Agents must act on partial observations and maintain belief distributions.
  - **Quick check question:** Can you explain why a POMDP requires belief-state tracking, unlike a standard MDP?

- **Concept: Proximal Policy Optimization (PPO) with Recurrence**
  - **Why needed here:** Training stable policies in multi-agent settings with partial observability requires constrained policy updates and memory of past observations.
  - **Quick check question:** What role does the clip range (ε = 0.2) play in preventing destabilizing policy updates?

- **Concept: Agent-Based Modeling (ABM) for Human Behavior**
  - **Why needed here:** Panic-induced deviations from rational behavior are emergent properties; ABM allows modeling individual decision rules that produce non-deterministic trajectories.
  - **Quick check question:** How does the panic parameter γ(t) change the balance between goal-directed and herd behavior?

## Architecture Onboarding

- **Component map:**
  Environment (Grid World) -> Fire Spread Module (stochastic propagation) -> Evacuee Agent (ABM with panic model) -> UAV Agents (HLR + LLR) -> Observation Encoder (FOV-based partial observations) -> Shared History Buffer (LSTM-encoded) -> Joint Policy Network (PPO-optimized)

- **Critical path:**
  1. Environment initializes with randomized fire origin, evacuee start, and safe zone
  2. HLR and LLR receive partial observations (no evacuee location initially)
  3. Joint policy selects polar-coordinate actions for each UAV
  4. A* pathfinding translates actions to discrete movements
  5. If evacuee enters any FOV, position is shared; LLR pursues
  6. Capture triggers guidance; evacuee follows A* path to safety

- **Design tradeoffs:**
  - Centralized vs. decentralized execution: Centralized simplifies coordination but requires reliable inter-UAV communication
  - Single vs. multiple evacuees: Current M = N + 1 assumption; scaling to M ≠ N requires policy retraining
  - Discrete grid abstraction vs. continuous space: Grid simplifies A* and observation masking but limits real-world fidelity

- **Failure signatures:**
  - Win rate drops sharply when position randomization radius r > 8 (Figure 4: ~45% at r=10)
  - Persistent panic without UAV interception nearly doubles evacuation time (Figure 2)
  - Policies may overfit to specific fire-origin patterns if training diversity is insufficient

- **First 3 experiments:**
  1. **Baseline validation:** Run 100 episodes with no UAV assistance; measure evacuee steps-to-safety across panic levels (replicate Figure 2 baseline curves)
  2. **Ablation on panic model:** Set γ(t) = 0 (rational evacuee) vs. γ(t) = 1 (maximum panic); compare capture times and UAV behavior patterns
  3. **Generalization stress test:** Train on fixed fire origins, test on fully randomized origins (r = 10); measure win rate degradation curve to identify policy brittleness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the framework scale to multi-evacuee scenarios with N humans and M rescuers, particularly when M ≠ N + 1?
- **Basis in paper:** [explicit] "Other promising directions are scaling to multi-human scenarios with N evacuees and M rescuers. While the current framework assumes M = N + 1, future extensions could explore more general team configurations and coordination strategies for M ̸= N."
- **Why unresolved:** The current framework only validates single-evacuee scenarios; multi-evacuee coordination introduces priority assignment, resource allocation, and potential conflicts in guidance decisions.
- **What evidence would resolve it:** Simulation results showing win rates, capture times, and coordination behavior in scenarios with 2–10 evacuees across varying rescuer team sizes.

### Open Question 2
- **Question:** How does the learned policy perform under imperfect or delayed communication between UAVs?
- **Basis in paper:** [explicit] "A key limitation of our framework lies in several modeling assumptions, including perfect communication between UAVs..."
- **Why unresolved:** The centralized training assumes shared observations are always available, but real disaster environments may have intermittent connectivity, latency, or packet loss affecting coordination.
- **What evidence would resolve it:** Ablation studies with simulated communication delays, dropout rates, or bandwidth constraints measuring degradation in capture rate and time-to-rescue.

### Open Question 3
- **Question:** To what extent does the panic model generalize to real human evacuation behavior?
- **Basis in paper:** [explicit] "While our panic model is grounded in empirical literature... we recognize the broader complexity of human behavior in emergency contexts. Future work could explore alternative modeling approaches, including rule-based social dynamics or data-driven behavioral models."
- **Why unresolved:** The panic model uses simplified heuristics (δ1–δ4) with sampled distributions; it may not capture cultural factors, group dynamics, or individual variability observed in real evacuations.
- **What evidence would resolve it:** Comparison between simulated trajectories and human subject experimental data, or integration of learned behavior models from real evacuation datasets.

### Open Question 4
- **Question:** How does policy performance degrade when deployed on physical UAVs with real-world sensing and actuation constraints?
- **Basis in paper:** [inferred] The paper uses a discrete 2D grid abstraction and assumes perfect state estimation; no real-world validation is mentioned despite the humanitarian deployment motivation.
- **Why unresolved:** Sim-to-real transfer introduces sensor noise, battery constraints, wind disturbances, and navigation errors not present in the grid-world simulation.
- **What evidence would resolve it:** Hardware-in-the-loop experiments or outdoor flight tests measuring capture success rates under realistic environmental conditions.

## Limitations
- No empirical validation of panic model against real human evacuation behavior
- No real-world deployment evidence; performance in physical environments untested
- Assumes perfect inter-UAV communication; breaks under communication failure

## Confidence
- High confidence: POMDP formulation and PPO training setup are technically sound
- Medium confidence: Heterogeneous UAV role specialization is logically justified but lacks direct empirical comparison
- Low confidence: Panic model parameters are partially specified and behavioral realism is inferred rather than validated

## Next Checks
1. Conduct a controlled experiment comparing evacuee trajectories under the panic model vs. real human movement data from fire drills or VR studies
2. Implement a communication failure scenario to test policy robustness when HLR-LLR information sharing is disrupted
3. Scale the simulation to multiple evacuees (M > N + 1) and evaluate whether the current policy generalizes or requires redesign