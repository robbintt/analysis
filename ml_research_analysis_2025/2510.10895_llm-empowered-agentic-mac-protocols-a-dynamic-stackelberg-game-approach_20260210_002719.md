---
ver: rpa2
title: 'LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach'
arxiv_id: '2510.10895'
source_url: https://arxiv.org/abs/2510.10895
tags:
- uni00000013
- uni00000011
- policy
- network
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing adaptive and generalizable
  Medium Access Control (MAC) protocols for dynamic wireless networks, where traditional
  multi-agent reinforcement learning (MARL) methods struggle with generalization to
  varying numbers of users. The core method introduces an LLM-empowered multi-agent
  reinforcement learning framework that models the MAC protocol emergence as a dynamic
  multi-follower Stackelberg game (MFSG).
---

# LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach

## Quick Facts
- **arXiv ID:** 2510.10895
- **Source URL:** https://arxiv.org/abs/2510.10895
- **Reference count:** 40
- **Primary result:** 77.6% greater throughput and 65.2% fairness improvement over conventional baselines using LLM-driven multi-agent reinforcement learning for adaptive MAC protocols.

## Executive Summary
This paper introduces an LLM-empowered multi-agent reinforcement learning framework for designing adaptive Medium Access Control (MAC) protocols in dynamic wireless networks. The framework models uplink transmission scheduling as a dynamic multi-follower Stackelberg game, where a base station acts as the leader and varying user equipments act as followers. Large language models serve as decision-making policies, naturally handling variable-length input sequences while preserving exploratory learning. The approach achieves significant performance gains and excellent generalization to fluctuating numbers of users without requiring retraining.

## Method Summary
The framework formulates uplink transmission scheduling as a dynamic multi-follower Stackelberg game where the base station (leader) broadcasts scheduling intent via Device Control Messages (DCMs), and user equipments (followers) respond with User Control Messages (UCMs) based on local observations. LLMs act as policy networks for both BS and UEs, with Protocol Action Grammar (PAG) constraining outputs to valid protocol tokens. Proximal Policy Optimization (PPO) trains the LLM policies, with critics implemented as MLPs. The approach handles variable UE counts by treating them as changes in prompt length rather than requiring architectural modifications.

## Key Results
- Achieves 77.6% greater throughput compared to conventional baselines
- Improves fairness by 65.2% while maintaining high performance
- Generalizes excellently to fluctuating numbers of users without retraining
- Proves existence of Stackelberg equilibrium and establishes convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Multi-Follower Stackelberg Game (MFSG)
The hierarchical leader-follower game captures asymmetric BS-UE relationships and guides policy convergence. The BS broadcasts macroscopic scheduling intent via DCMs; UEs interpret intent and articulate best responses via UCMs. This sequential decision structure ensures UEs exploit real-time local observations while BS maintains network-wide coordination. The mechanism relies on weak coupling near equilibrium, where inter-player second-order effects are negligible.

### Mechanism 2: LLM as Variable-Length Policy Network
LLMs serve as policy networks enabling zero-shot generalization to fluctuating UE counts. Numerical observations convert to text prompts; UE count changes alter prompt length rather than architecture. Output distributions computed via autoregressive token probabilities. The pre-trained LLM's generative priors transfer to protocol-action spaces through fine-tuning via PPO rather than requiring domain-specific datasets.

### Mechanism 3: Protocol Action Grammar (PAG)
PAG constrains LLM output vocabulary to valid protocol tokens, ensuring executability and improving training efficiency. Vocabulary restricts BS output to RBG allocation tokens and UE output to transmission bitmap tokens. Log-probabilities normalized via softmax over valid actions only. The mechanism assumes valid protocol tokens form a small discrete set without eliminating optimal solutions.

## Foundational Learning

- **Stackelberg Games and Stackelberg Equilibrium**
  - Why needed here: Provides theoretical framework for hierarchical BS-UE interactions; paper proves existence of Expected Stackelberg Equilibrium (Theorem 1) and local convergence (Theorem 2)
  - Quick check question: Can you explain why the leader must anticipate followers' best responses when optimizing its policy?

- **Proximal Policy Optimization (PPO)**
  - Why needed here: Core training algorithm for LLM policy networks; uses clipped surrogate objectives and GAE for stable updates
  - Quick check question: Given the objective in Eq. (10), what role does the clipping parameter ε play in preventing policy degradation?

- **Tokenization and Autoregressive Generation**
  - Why needed here: LLM policies generate actions as token sequences; understanding probability factorization (Eq. 12) is essential for computing policy gradients
  - Quick check question: How does Eq. (12) compute the joint probability of a DCM sequence, and why does this matter for policy gradient computation?

## Architecture Onboarding

- **Component map:**
  - Environment (OFDMA UDTS simulator) -> Base Station (LLM actor + MLP critic) -> User Equipments (shared LLM actor + shared critic) -> Protocol Action Grammar constraint -> Transmission execution

- **Critical path:**
  1. Environment resets → Initial observations $\tilde{o}_{0,b}, \{\tilde{o}_{i,0,u}\}$
  2. Leader generates DCM via LLM (Alg 1, line 7)
  3. Followers observe DCM + local state, generate actions (line 8)
  4. Environment executes transmissions → rewards $F_{t,b}, F_{i,t,u}$ (lines 9-10)
  5. Trajectories stored in buffers $\mathcal{B}_b, \mathcal{B}_u$ (lines 12-13)
  6. Independent PPO updates with GAE advantages (lines 16-21)

- **Design tradeoffs:**
  - LLM size vs. latency: Llama3-1B achieves good performance; Qwen3-4B marginal gain but 1.7x GPU memory
  - Centralized vs. hierarchical: "Dictator" baseline (no follower autonomy) shows 17.58% lower utility; SG balances coordination and local optimization
  - Fixed vs. shared follower policy: All UEs share $\theta_u$ to handle dynamic $I_t$; may reduce per-UE specialization

- **Failure signatures:**
  - Divergence during training → Check learning rate $\alpha_b = \kappa_1/\kappa_2$ ratio (Theorem 2 condition)
  - Low fairness (< 0.6) → Follower policy may ignore leader DCMs; verify consistency reward $\varrho_2$ in Eq. (2)
  - Invalid token generation → PAG masking not applied; verify $\mathcal{W}^-$ constraint at inference
  - Poor generalization to unseen $I_t$ → Training distribution $D(I)$ too narrow

- **First 3 experiments:**
  1. **Convergence validation:** Train on $I_t \in \{3,4,5\}$ with default hyperparameters; confirm system utility converges by ~1250 epochs (Fig. 5)
  2. **Ablation on SG structure:** Compare against centralized "Dictator" baseline; expect ~17% utility gap if hierarchical learning works correctly (Fig. 6)
  3. **Generalization test:** Train on $I_t \in \{3,4\}$, evaluate on $I_t=5$ and $I_t=10$; compare throughput/fairness against MAPPO-G to quantify zero-shot transfer (Fig. 4, Fig. 8)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to multi-cell environments with inter-cell interference or integrated sensing and communication (ISAC) scenarios?
- Basis in paper: [explicit] The conclusion explicitly identifies extending the protocol to "multi-cell deployments and integrated sensing and communication protocols" as a promising direction for future work.
- Why unresolved: The current system model and theoretical analysis are limited to a single-cell OFDMA system with a single base station acting as the leader.
- What evidence would resolve it: Simulations demonstrating convergence and performance in a multi-BS setting where inter-cell interference is managed by the emergent protocol.

### Open Question 2
- Question: Does the quadratic inference complexity of the leader agent prohibit scalability in massive Machine-Type Communication (mMTC) scenarios with hundreds of devices?
- Basis in paper: [inferred] The complexity analysis (Section IV-F) shows leader cost scales with $O(I_t^2)$, but simulations only validated performance with up to 10 UEs.
- Why unresolved: While the model handles "fluctuating" users, the computational cost of processing hundreds of UCM tokens in a single prompt may introduce prohibitive latency for massive access scenarios.
- What evidence would resolve it: Latency and throughput benchmarks in environments with $I_t > 50$ or $100$ devices.

### Open Question 3
- Question: How does the "weak coupling" assumption impact convergence guarantees in dense networks with high inter-user interference?
- Basis in paper: [inferred] The convergence proof relies on Assumption 2, which presumes second-order inter-player effects are negligible to ensure the Jacobian is strictly diagonally dominant.
- Why unresolved: In dense wireless networks, the channel access of one user often significantly impacts the utility of others (strong coupling), which might violate the assumption required for local convergence.
- What evidence would resolve it: A sensitivity analysis of convergence rates relative to the degree of inter-user coupling or interference.

## Limitations
- **Knowledge gaps in LLM-RL integration:** Limited empirical validation of whether pre-trained generative priors transfer to protocol-action spaces; PAG effectiveness not rigorously tested with edge cases
- **Reproduction barriers:** Exact prompt templates unspecified; critic input mechanism unclear; fine-tuning scope (full vs. parameter-efficient methods) not specified
- **Theoretical-experimental mismatch:** Convergence guarantee requires specific learning rate ratio condition that may not hold with experimental hyperparameters; weak coupling assumption untested under strong interference

## Confidence
- **High Confidence:** Hierarchical Stackelberg game formulation and theoretical analysis (Theorems 1-2) are well-grounded with clear definitions and reasonable assumptions
- **Medium Confidence:** PPO training procedure and PAG mechanism are described clearly but depend on unspecified implementation details
- **Low Confidence:** Transfer of LLM generative priors to protocol-action spaces without extensive fine-tuning is assumed rather than demonstrated; scalability claims for large UE counts not experimentally validated

## Next Checks
1. **Convergence Validation:** Train the framework on UE counts I_t ∈ {3,4,5} with default hyperparameters and verify system utility converges by ~1250 epochs as claimed in Fig. 5

2. **Generalization Test:** Train on I_t ∈ {3,4}, then evaluate on I_t=5 and I_t=10, comparing throughput/fairness against MAPPO-G baseline to quantify zero-shot transfer performance

3. **Coupling Stress Test:** Evaluate the framework under highly coupled interference conditions where follower utility gradients are strongly coupled, checking whether the theoretical convergence guarantees hold or break down