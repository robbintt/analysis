---
ver: rpa2
title: 'Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning'
arxiv_id: '2510.04773'
source_url: https://arxiv.org/abs/2510.04773
tags:
- unlearning
- dipo
- forget
- preference
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution Preference Optimization (DiPO),
  a novel approach for large language model unlearning that operates at the distribution
  level rather than the response level. Unlike previous methods that require explicit
  preferred responses or rely solely on negative preference signals, DiPO directly
  optimizes the next-token probability distribution by constructing preference pairs
  through selective logit modulation.
---

# Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning

## Quick Facts
- arXiv ID: 2510.04773
- Source URL: https://arxiv.org/abs/2510.04773
- Reference count: 40
- Primary result: DiPO achieves state-of-the-art unlearning performance with FQ=0.86 on TOFU-10% while maintaining superior utility preservation

## Executive Summary
This paper introduces Distribution Preference Optimization (DiPO), a novel approach for large language model unlearning that operates at the distribution level rather than the response level. Unlike previous methods that require explicit preferred responses or rely solely on negative preference signals, DiPO directly optimizes the next-token probability distribution by constructing preference pairs through selective logit modulation. The method uses a memory vector filtered from high-confidence output logits to create memory-enhancing and forgetting-promoting distributions, which guide the model toward desired unlearning behavior. Theoretical analysis proves the consistency of DiPO's loss function with the intended unlearning direction.

## Method Summary
DiPO operates by constructing preference pairs through selective logit modulation. It extracts a memory vector from high-confidence output logits using top-p filtering, then creates memory-enhancing and forgetting-promoting distributions by adding or subtracting this vector from the original logits. The method optimizes the next-token probability distribution directly rather than response-level preferences, using a loss function derived from the Bradley-Terry preference model. Crucially, DiPO employs the same preference distributions for both forget and retain objectives by simply reversing their roles, creating a unified training approach. The Sequence KL divergence between the policy distribution and these constructed preferences drives the optimization process.

## Key Results
- Achieves state-of-the-art performance with FQ=0.86 on TOFU-10% benchmark
- Maintains superior utility preservation compared to NPO+GD on MUSE benchmarks
- Demonstrates greater training stability with reduced oscillations in forget quality and model utility metrics

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Level Preference Optimization via Sequence KL
- **Claim:** Optimizing at the next-token probability distribution level provides finer-grained control than response-level methods, enabling more targeted unlearning without constructing explicit preferred responses.
- **Mechanism:** DiPO derives a loss function that maximizes the gap between Sequence KL divergence from πθ to dispreferred distribution πl and to preferred distribution πw. The gradient analysis proves this explicitly pushes πθ away from πl and toward πw.
- **Core assumption:** The Bradley-Terry preference model appropriately captures distribution-level preferences; vocabulary-level token space is sufficiently structured to enable meaningful preference pairs.
- **Evidence anchors:**
  - [abstract] "directly optimizes the next-token probability distribution by constructing preference pairs through selective logit modulation"
  - [Section 4.2] "Since ∂L/∂x1 < 0, minimizing L via gradient descent increases x1 = DSeqKL(πl||πθ), effectively pushing the distribution πθ away from the dispreferred distribution πl"
  - [corpus] Related work confirms NPO's utility degradation issues, supporting the need for alternative approaches
- **Break condition:** If Sequence KL divergence fails to capture meaningful distributional differences for specific token patterns, the gradient direction may not correspond to intended unlearning.

### Mechanism 2: Memory Vector Construction via High-Confidence Logit Filtering
- **Claim:** High-confidence output logits contain the core memorized information, and selectively amplifying/suppressing them creates effective preference pairs without auxiliary models.
- **Mechanism:** Extract memory vector mt via top-p filtering (top 5% highest-confidence tokens), then construct πm = softmax(zt + αmt) for memory-enhancing and πf = softmax(zt - αmt) for forgetting-promoting distributions. The same pair serves both forget and retain objectives by reversing roles.
- **Core assumption:** High-confidence tokens primarily encode target knowledge to be unlearned; suppressing them doesn't inadvertently promote undesirable outputs due to vocabulary vastness.
- **Evidence anchors:**
  - [abstract] "uses a memory vector filtered from high-confidence output logits to create memory-enhancing and forgetting-promoting distributions"
  - [Section 4.3] "This inherent safety allow us to employ a straightforward filtering mechanism... If these tokens correspond to undesirable information, suppressing their logits naturally steers the model towards alternative, non-sensitive outputs"
  - [corpus] Limited direct corpus evidence on logit-filtering mechanisms for unlearning
- **Break condition:** If high-confidence tokens encode general linguistic patterns rather than specific target knowledge, suppression may cause broader capability degradation.

### Mechanism 3: Unified Forget-Retain Objective via Role Reversal
- **Claim:** Using identical preference distributions with reversed roles for forget and retain objectives creates coherent training dynamics.
- **Mechanism:** The combined loss L(θ) = LDiPO-f(θ) + λLDiPO-r(θ) with λ=1 balances forgetting target knowledge while preserving utility. Ablation studies show this integrated design outperforms mixing DiPO components with standard losses.
- **Core assumption:** The forget and retain distributions are sufficiently disjoint that optimizing both simultaneously doesn't create conflicting gradients.
- **Evidence anchors:**
  - [Section 4.3] "Crucially, the same pair (πm, πf) derived from the model's logits can be utilized for both the forget and retain objectives by simply reversing their roles"
  - [Table 5 ablation] DiPO(f)+GD achieves MU=0.65 but FQ≈0 on TOFU-10%, while full DiPO achieves FQ=0.84, MU=0.56
  - [corpus] AGT^AO paper confirms catastrophic forgetting as a fundamental dilemma in aggressive unlearning, supporting integrated objective design
- **Break condition:** If forget and retain sets have overlapping knowledge representations, simultaneous optimization may cause gradient interference.

## Foundational Learning

- **KL Divergence and Sequence KL:**
  - Why needed here: DiPO's loss function is expressed entirely in terms of Sequence KL divergences; understanding how KL measures distributional distance is essential for interpreting the optimization dynamics.
  - Quick check question: Given two token distributions P and Q, does D_KL(P||Q) increase when Q assigns lower probability to tokens that P assigns high probability?

- **Bradley-Terry Preference Model:**
  - Why needed here: DiPO derives its loss from the BT model, which converts preference relationships into probability estimates; this connects reward signals to policy optimization.
  - Quick check question: If reward Rπw > Rπl, does the BT model predict p(πw preferred over πl) > 0.5?

- **Markov Decision Process (MDP) Formulation for Text Generation:**
  - Why needed here: DiPO builds on TDPO's MDP framing where state st = [x, y<t] and action at = selecting next token yt; this enables token-level reward definition.
  - Quick check question: In the MDP framing, what constitutes the "state" at generation step t?

## Architecture Onboarding

- **Component map:**
  Reference model (πref) -> Policy model (πθ) -> Memory vector extractor -> Distribution constructor -> Loss computer

- **Critical path:**
  1. Forward pass through πθ → obtain logits zt for each position
  2. Extract memory vector mt via top-p filtering (p=0.05, both rank-based and relative thresholds)
  3. Construct πm, πf by adding/subtracting α·mt (α is scaling factor)
  4. Compute Sequence KL divergences: DSeqKL(πm||πθ), DSeqKL(πf||πθ), DSeqKL(πm||πref), DSeqKL(πf||πref)
  5. Apply DiPO loss (Eq. 14) for both forget and retain batches with role reversal
  6. Backward pass and parameter update

- **Design tradeoffs:**
  - **Top-p threshold (p=0.05):** Lower values = more aggressive filtering, potentially more targeted but riskier; higher values = more conservative but less effective unlearning
  - **Scaling factor α:** Controls strength of logit modulation; too high may cause distribution collapse, too low may be ineffective
  - **β parameter (βf=βr=0.05):** Controls preference strength in sigmoid; lower values = softer preferences, higher = more aggressive optimization
  - **λ=1 for retain weight:** Symmetric weighting; asymmetric values may help in scenarios with limited retain data

- **Failure signatures:**
  - **Model utility collapse (MU→0):** Typically indicates over-aggressive forget loss or insufficient retain signal; check if α or βf too high
  - **Under-unlearning (FQ remains low):** Forget objective not converging; may need higher training epochs or adjust filtering threshold
  - **Training instability (oscillating FQ/MU):** Gradient conflicts between forget and retain; consider reducing learning rate or adjusting λ
  - **Response degeneration (repetitive/nonsensical outputs):** Distribution collapse; check if memory vector extraction is capturing too many tokens

- **First 3 experiments:**
  1. **Reproduce TOFU-10% baseline:** Train DiPO on TOFU-10% with default hyperparameters (β=0.05, p=0.05, α implicit in softmax, λ=1, lr=1e-5, 10 epochs); verify FQ≥0.80 and MU≥0.50
  2. **Ablate memory vector filtering:** Compare p∈{0.01, 0.05, 0.10, 0.20} on TOFU-5%; plot FQ vs. MU tradeoff to find optimal filtering threshold
  3. **Test scalability on MUSE News:** Train on increasing forget set sizes (following MUSE scalability protocol); compare utility preservation curve against NPO+GD baseline to verify DiPO's claimed scalability advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DiPO's intrinsic preference modeling be refined to effectively resist Membership Inference Attacks (MIAs)?
- Basis in paper: The authors acknowledge in the Limitations section that the "current simplicity may not fully address the complexities required for unlearning against information leakage, such as those evaluated by Membership Inference Attacks (MIAs)."
- Why unresolved: DiPO showed suboptimal performance on the PrivLeak metric in the MUSE benchmark, indicating that while factual knowledge is removed, the model's probability distributions might still leak information about the presence of the training data.
- What evidence would resolve it: An improved DiPO variant that achieves a PrivLeak score statistically indistinguishable from the "Retrain" baseline on the MUSE benchmark without sacrificing Forget Quality.

### Open Question 2
- Question: How can the distribution-level optimization be adapted to reduce hallucinations in the unlearned model's outputs?
- Basis in paper: The paper states that "DiPO’s outputs are not entirely immune to hallucination" and lists this as an ongoing challenge for future work.
- Why unresolved: While DiPO successfully shifts probability mass away from target facts, the redistribution of this mass may occasionally land on plausible but incorrect tokens, generating confabulated responses rather than safe refusals or generic answers.
- What evidence would resolve it: Qualitative and quantitative analysis showing a reduction in hallucination rates (e.g., via factuality scores) on the retain set compared to the current DiPO implementation.

### Open Question 3
- Question: Is the fixed top-k filtering rate (pk=0.05) for constructing the memory vector optimal across varying model scales or data complexities?
- Basis in paper: The method constructs preference pairs by isolating high-confidence logits using a specific top-k rate, but the paper notes this mechanism is "simple" and does not analyze the sensitivity of this hyperparameter across different tasks.
- Why unresolved: It is unclear if a static filtering rate effectively captures the relevant "memory" for unlearning tasks where the distribution of target knowledge is denser or sparser than the tested TOFU/MUSE datasets.
- What evidence would resolve it: An ablation study on the filtering rate pk demonstrating stable Forget Quality and Model Utility across a wide range of values, or the development of an adaptive filtering mechanism.

## Limitations

- Distribution-level preference modeling relies on assumptions about the Bradley-Terry model's applicability to continuous probability distributions
- High-confidence logit filtering may capture general linguistic patterns rather than specific memorized content
- Unified forget-retain objectives may suffer from gradient interference when knowledge sets overlap

## Confidence

**High Confidence:** The empirical results on TOFU-10% (FQ=0.86) and MUSE News (superior scalability vs NPO+GD) are well-supported by the presented experiments. The theoretical consistency proof for the loss function direction is mathematically sound given the Bradley-Terry assumptions.

**Medium Confidence:** The mechanism claims regarding distribution-level optimization benefits and memory vector effectiveness are supported by ablation studies but rely on assumptions about token distribution semantics that require further validation. The training stability improvements over baselines are demonstrated but could benefit from longer-term convergence analysis.

**Low Confidence:** Claims about vocabulary-level safety of the logit filtering approach lack rigorous justification. The assumption that suppressing high-confidence tokens doesn't cause unintended behavior degradation needs more thorough investigation, particularly for domains with specialized vocabulary or domain-specific knowledge patterns.

## Next Checks

1. **Threshold sensitivity analysis:** Systematically vary the top-p filtering threshold (p∈{0.01, 0.05, 0.10, 0.20}) across multiple unlearning tasks and plot the tradeoff between forget quality and utility preservation. This would validate whether p=0.05 is truly optimal or task-dependent, and quantify the robustness of the memory vector extraction approach.

2. **Cross-task generalization study:** Apply DiPO to a real-world unlearning scenario involving naturally occurring training data with ambiguous preferences (e.g., removing copyrighted content while preserving fair use educational material). Compare performance against NPO+GD and GradDiff on metrics including FQ, MU, and qualitative assessment of output coherence and relevance.

3. **Long-term stability and catastrophic forgetting analysis:** Train DiPO for extended epochs (50-100) on TOFU benchmarks and monitor the evolution of FQ and MU over time. Track whether the forget objective continues improving while utility degrades, or whether the method exhibits plateau behavior indicating stable unlearning without catastrophic forgetting. Compare against baseline methods' long-term behavior to validate the claimed training stability advantage.