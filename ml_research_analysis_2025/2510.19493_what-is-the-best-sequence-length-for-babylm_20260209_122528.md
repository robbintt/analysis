---
ver: rpa2
title: What is the Best Sequence Length for BABYLM?
arxiv_id: '2510.19493'
source_url: https://arxiv.org/abs/2510.19493
tags:
- sequence
- mamba
- length
- babylm
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates the impact of sequence
  length on BabyLM pretraining, comparing OPT and Mamba models across various sequence
  lengths (64-8192 tokens) on a 100M-word dataset. The key finding is that optimal
  sequence length is task-dependent and varies between architectures: shorter sequences
  (64-256 tokens) work best for grammatical generalization tasks like BLiMP, while
  longer sequences (4096-8192 tokens) benefit morphological analogical reasoning tasks
  like Wug.'
---

# What is the Best Sequence Length for BABYLM?

## Quick Facts
- arXiv ID: 2510.19493
- Source URL: https://arxiv.org/abs/2510.19493
- Authors: Suchir Salhan; Richard Diehl Martinez; Zébulon Goriely; Paula Buttery
- Reference count: 17
- Primary result: Optimal sequence length is task-dependent and varies between OPT and Mamba architectures

## Executive Summary
This paper systematically investigates how sequence length affects BabyLM pretraining performance for 125M-parameter OPT and Mamba models across 64-8192 tokens. The key finding is that optimal sequence length depends on both the task and architecture: shorter sequences (64-256 tokens) excel at grammatical generalization tasks like BLiMP, while longer sequences (4096-8192 tokens) benefit morphological analogical reasoning tasks like Wug. Mamba achieves near-optimal performance with shorter sequences, offering training efficiency advantages, while OPT generally requires longer contexts for tasks involving long-range dependencies.

## Method Summary
The study pretrains OPT and Mamba models on the BabyLM STRICT 100M-word corpus using 8 different sequence lengths (64-8192 tokens) for 10 epochs with global batch size 64. Models are evaluated on the BabyLM evaluation pipeline including BLiMP, BLiMP Supplement, Entity Tracking, EWoK, Wug, and reading tasks. Learning rate is scaled linearly with sequence length during warmup. The study compares architectural efficiency and task-specific performance across different context windows.

## Key Results
- Shorter sequences (64-256 tokens) work best for grammatical generalization tasks like BLiMP
- Longer sequences (4096-8192 tokens) benefit morphological analogical reasoning tasks like Wug
- Mamba achieves near-optimal performance with shorter sequences, offering training efficiency advantages
- OPT requires longer contexts for tasks involving long-range dependencies
- Task-optimal sequence length varies by both task linguistic structure and model architecture

## Why This Works (Mechanism)

### Mechanism 1: Task Linguistic Structure Determines Context Window Requirements
- Claim: Optimal sequence length varies by task because different linguistic phenomena require different amounts of context for learning.
- Mechanism: Syntactic constraints in minimal pair judgments (BLiMP) are largely local, while morphological analogy (Wug) and entity tracking require accumulating statistical evidence across extended spans.
- Evidence anchors: Shorter sequences perform better on BLiMP; longer sequences perform better on Entity Tracking, Wug, and Reading Evaluation Tasks.
- Break condition: If BLiMP benefits from longer contexts or Wug peaks at 128 tokens with different test items.

### Mechanism 2: Architecture-Specific Memory Determines Context Efficiency
- Claim: Mamba achieves competitive performance with shorter sequences because its recurrent state compression implicitly captures dependencies, while OPT's explicit attention requires longer windows.
- Mechanism: Transformers store O(L²) attention patterns explicitly; SSMs compress sequence history into learned state dynamics with O(L) scaling.
- Evidence anchors: Mamba consistently prefers shorter sequences; Mamba retains memory via parameterized state-space dynamics.
- Break condition: If scaling model size shifts Mamba's optimal L* to match OPT's, or if state capacity bottlenecks emerge.

### Mechanism 3: Fixed Compute Budget Creates Update Count vs. Gradient Quality Tradeoff
- Claim: Under data constraints, longer sequences reduce total parameter updates, creating a tradeoff between gradient variance and optimization steps.
- Mechanism: With fixed tokens-per-batch and total training tokens, doubling sequence length halves the number of gradient updates.
- Evidence anchors: Using larger sequences means models are updated less often; learning rate scaled as 5×10⁻⁵ × (seq_len/64).
- Break condition: If adjusting batch size to equalize tokens-per-update eliminates sequence length effects.

## Foundational Learning

- **Context window vs. sequence length distinction**: Transformers have a hard context limit (attention positions); SSMs do not. Quick check: Can a Transformer trained on 512 tokens process 2048 tokens at inference without modification?

- **Gradient accumulation and effective batch size**: The paper holds global batch size fixed but varies sequence length, changing tokens-per-update. Quick check: If you double sequence length but halve batch size, what happens to tokens-per-update?

- **State Space Model recurrence basics**: Mamba's behavior differs from attention-based models because it maintains compressed hidden state across positions rather than attending to all prior tokens. Quick check: What is the computational complexity difference between Transformer attention and SSM recurrence for a sequence of length L?

## Architecture Onboarding

- **Component map**: OPT (12-layer Transformer, 768 hidden, 12 heads, learned positional embeddings) -> Mamba (32-layer SSM, 768 hidden, selective state-space blocks, no explicit positional encoding)

- **Critical path**: 1) Prepare 8 datasets via document shuffling + fixed chunking; 2) Train with 10 epochs, global batch 64, LR scaled by sequence length; 3) Evaluate on BabyLM pipeline (BLiMP, BLiMP Supplement, Entity Tracking, EWoK, Wug, Reading)

- **Design tradeoffs**: Longer sequences: better gradient estimates, fewer updates, higher per-step cost; Shorter sequences: more updates, noisier gradients, faster wall-clock time; OPT vs. Mamba: OPT excels at long-context tasks but requires longer training; Mamba trains faster but underperforms on long-range dependencies

- **Failure signatures**: OPT at 4096+ tokens without warmup: BLiMP scores collapse; Mamba at 4096+ tokens: Wug performance degrades; Very long sequences with limited data: Overfitting risk from too few diverse training batches

- **First 3 experiments**: 1) Replicate OPT-256 vs. OPT-2048 on BLiMP and Wug to confirm task-dependent divergence; 2) Compare Mamba-128 vs. Mamba-1024 on Entity Tracking to verify SSM mid-range preference; 3) Run a constant-tokens-per-update control to isolate length effects from update-count effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do sequence length effects interact with batch size and gradient accumulation when controlling for tokens-per-update?
- Basis in paper: The authors acknowledge they "do not vary the mini-batch size or gradient accumulation strategy in conjunction with sequence length"
- Why unresolved: Without controlling tokens per update, it remains unclear whether observed task-dependent optima stem from sequence length itself or from varying number of gradient updates
- What evidence would resolve it: A factorial design holding tokens-per-update constant across sequence lengths by adjusting batch size/accumulation

### Open Question 2
- Question: Why does Mamba prefer shorter sequences (64-1024 tokens) in the BabyLM setting despite its theoretical capacity for efficient long-context modeling?
- Basis in paper: The authors state: "The consistent preference of Mamba for shorter sequences raises important questions... the model may be unable to store and retrieve fine-grained information over very long sequences at small model scales"
- Why unresolved: The paper cannot distinguish between architectural limitations, optimization challenges, or data-scale constraints
- What evidence would resolve it: Ablation studies varying model capacity, data scale, and training duration independently

### Open Question 3
- Question: Can curriculum-based sequence length schedules (starting short, then increasing) improve performance across diverse BabyLM tasks compared to fixed-length training?
- Basis in paper: The authors discuss the "starting small" hypothesis but only test fixed sequence lengths throughout training
- Why unresolved: No experiment addresses whether gradually increasing sequence length could capture benefits of both regimes within a single training run
- What evidence would resolve it: Training models with progressive sequence length schedules and comparing against fixed-length baselines

## Limitations
- The study is limited to 125M-parameter models, leaving open whether sequence length preferences hold at larger scales
- Learning rate scaling and fixed batch size create confounds between sequence length effects and update frequency that are not fully resolved
- The evaluation suite may systematically favor certain context lengths due to its emphasis on specific linguistic phenomena

## Confidence

**High Confidence**: The empirical finding that BLiMP performance degrades with longer sequences for OPT models (56-58% at 4096+ tokens vs. 70-75% at 64-256 tokens) is robust and well-supported by multiple data points.

**Medium Confidence**: The claim that optimal sequence length is "task-dependent" is supported but the underlying mechanism (linguistic structure requiring different context windows) is inferred rather than directly tested.

**Low Confidence**: The mechanism proposing that Mamba's recurrent state compression inherently captures dependencies more efficiently than Transformer attention is speculative, relying on theoretical properties not directly validated through ablation studies.

## Next Checks
1. **Constant Tokens-Per-Update Experiment**: Design an experiment where batch size varies inversely with sequence length to maintain constant tokens-per-update, isolating whether sequence length effects stem from context requirements or gradient variance differences.

2. **Architecture Scaling Study**: Train 125M, 350M, and 1B parameter versions of both OPT and Mamba across the same sequence length sweep to test whether architectural efficiency differences persist or diminish as models scale up.

3. **Task Redesign Validation**: Create modified versions of BLiMP and Wug that systematically vary context requirements within each task to test whether task-dependent findings reflect genuine linguistic requirements or current test suite construction.