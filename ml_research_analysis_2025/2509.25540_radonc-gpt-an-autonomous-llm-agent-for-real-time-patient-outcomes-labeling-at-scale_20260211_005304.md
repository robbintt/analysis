---
ver: rpa2
title: 'RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling
  at Scale'
arxiv_id: '2509.25540'
source_url: https://arxiv.org/abs/2509.25540
tags:
- patient
- data
- cancer
- clinical
- recurrence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadOnc-GPT is an autonomous LLM agent designed to retrieve structured
  and unstructured patient data and label complex clinical outcomes in radiation oncology.
  It operates by sequentially querying institutional databases via whitelisted functions,
  synthesizing both structured records and clinical narratives to assess outcomes
  such as osteoradionecrosis and cancer recurrence.
---

# RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale

## Quick Facts
- arXiv ID: 2509.25540
- Source URL: https://arxiv.org/abs/2509.25540
- Reference count: 40
- Autonomous LLM agent retrieves structured/unstructured patient data and labels complex clinical outcomes in radiation oncology

## Executive Summary
RadOnc-GPT is an autonomous LLM agent that retrieves structured and unstructured patient data to label complex clinical outcomes in radiation oncology. It operates by sequentially querying institutional databases via whitelisted functions, synthesizing both structured records and clinical narratives to assess outcomes such as osteoradionecrosis and cancer recurrence. A two-tier evaluation scaffold first established retrieval fidelity in structured data (100% accuracy for demographics, 99.4% for treatment details), then advanced to outcome labeling across three cohorts. In complex labeling tasks, RadOnc-GPT achieved high recall (88.2-97.9%) and, after adjudication of discrepancies, significantly improved accuracy to 95.2-96.3%. Adjudication revealed that 63% of initial mismatches were due to ground-truth errors, demonstrating the agent's dual utility as both a labeler and an auditor of registry data.

## Method Summary
The method employs GPT-4o as an autonomous agent that retrieves patient data through whitelisted functions rather than conventional RAG. The agent uses granular, well-defined retrieval functions (e.g., get patient clinical notes, get patient radiology reports) to access structured fields and unstructured clinical narratives from institutional databases. A two-tier evaluation scaffold establishes retrieval fidelity first on structured data (demographics, treatment details) before advancing to complex outcome labeling. The system uses context pruning when exceeding 95k tokens and prioritizes recall over precision for clinical surveillance tasks.

## Key Results
- Achieved 100% accuracy for demographic retrieval and 99.4% for treatment details in Tier 1 structured data validation
- High recall performance (88.2-97.9%) for complex outcome labeling tasks across three cohorts
- Post-adjudication accuracy improved to 95.2-96.3%, with 63% of initial discrepancies traced to ground-truth errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Establishing retrieval fidelity on structured data before attempting complex labeling tasks appears to reduce failure modes in clinical outcome assessment.
- Mechanism: The two-tier scaffold creates a verification checkpoint: the agent must first demonstrate 100% demographic and 99.4% treatment-plan retrieval accuracy (verifiable against database ground truth without manual review). This surfaces retrieval errors early—for example, truncated course IDs—allowing correction before the agent proceeds to unstructured data synthesis.
- Core assumption: Structured-data retrieval fidelity transfers to or is a necessary precondition for reliable unstructured data integration.
- Evidence anchors:
  - [abstract]: "The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling."
  - [PAGE 5]: "Tier 1 (Data extraction from structured data) ... Because this task relied exclusively on structured fields, no adjudication process was required, as the database entries were considered the ground truth."
  - [corpus]: Weak direct support—neighbors focus on prediction, not staged validation scaffolds.
- Break condition: If retrieval functions are added or modified without re-running Tier 1 verification, structured-data fidelity may degrade silently, undermining Tier 2 reliability.

### Mechanism 2
- Claim: Whitelisted, task-specific retrieval functions with explicit specifications appear to yield more reliable clinical reasoning than generic RAG approaches for structured, indexed clinical data.
- Mechanism: Rather than embedding documents and matching via vector similarity (RAG), RadOnc-GPT uses granular whitelisted functions (e.g., get patient clinical notes, get patient radiology reports) with well-defined inputs/outputs described in the system prompt. This leverages the fact that EHR data is "unstructured yet systematically organized and indexed" (timestamped, provider-signed, metadata-tagged), enabling targeted retrieval without generic RAG pipelines.
- Core assumption: Clinical notes and reports are sufficiently indexed and metadata-rich that function-level retrieval outperforms embedding-based similarity search for this domain.
- Evidence anchors:
  - [PAGE 4-5]: "RadOnc-GPT does not employ conventional retrieval-augmented generation (RAG). ... RadOnc-GPT leverages the fact that patient data within Epic is unstructured yet systematically organized and indexed."
  - [PAGE 4]: "The guiding principle is that greater function granularity reduces the volume of data returned on a per-function basis."
  - [corpus]: Weak direct comparison—neighbors use RAG or multi-agent approaches but don't explicitly contrast with function-based retrieval.
- Break condition: If the underlying EHR indexing degrades, or if metadata quality is inconsistent across sites, function-based retrieval may miss relevant records that RAG would surface.

### Mechanism 3
- Claim: Independent adjudication of discrepancies appears to reveal substantial latent ground-truth errors, enabling dual use of the agent as both labeler and registry auditor.
- Mechanism: Discrepancies between model outputs and baseline labels undergo independent physician adjudication, classified as model error, ground-truth error, or indeterminate. In this study, 30 of 48 discrepancies (63%) were ground-truth errors. This suggests the agent surfaces documentation or abstraction errors that would otherwise persist.
- Core assumption: The adjudication process itself is reliable and not systematically biased toward favoring model outputs.
- Evidence anchors:
  - [abstract]: "Adjudication revealed that 63% of initial mismatches were due to ground-truth errors, demonstrating the agent's dual utility as both a labeler and an auditor of registry data."
  - [PAGE 8]: "Oncologists classified discrepancies explicitly into three categories: 1. RadOnc-GPT Correct ... 2. Baseline Label Correct ... 3. Indeterminate."
  - [corpus]: Weak—neighbors don't report adjudication-driven ground-truth correction rates.
- Break condition: If adjudication is performed by non-experts or under time pressure, ground-truth errors may be misclassified, masking model errors or inflating audit claims.

## Foundational Learning

- Concept: **Agent-based orchestration vs. single-prompt LLM calls**
  - Why needed here: RadOnc-GPT operates as an agent that autonomously decides which functions to call and when to stop, unlike single-prompt extraction. Understanding this distinction is prerequisite to debugging retrieval chains.
  - Quick check question: Can you trace a hypothetical patient query through at least three function calls before termination?

- Concept: **Context-window pruning strategies for long patient histories**
  - Why needed here: The system uses controlled pruning (oldest messages first, reverse-chronological retrieval) when context exceeds 95K tokens. Engineers must understand this to interpret why older data may be absent in outputs.
  - Quick check question: Given a patient with 10 years of notes, which records are most vulnerable to pruning-induced omission?

- Concept: **Recall-oriented evaluation for clinical surveillance**
  - Why needed here: The paper emphasizes high recall (minimizing false negatives) as clinically critical for late-toxicity and recurrence detection. Engineers must optimize for this, not just accuracy.
  - Quick check question: In a recurrence detection task with 100 negatives and 10 positives, which matters more: reducing false positives from 5 to 2, or reducing false negatives from 2 to 0?

## Architecture Onboarding

- Component map:
  LLM Task Streaming (external orchestrator) -> feeds patient IDs + prompts -> RadOnc-GPT Agent (GPT-4o core) -> calls Whitelisted Functions (via system prompt specifications) -> retrieves from Institutional Data Sources (Aria, Epic EHR) + Public APIs (PubMed, ClinicalTrials.gov, CTCAE) -> returns structured JSON -> aggregated into cohort CSVs for adjudication

- Critical path:
  1. Verify function specifications in system prompt match actual API signatures
  2. Run Tier 1 QA on n=100 sample; if <99% accuracy, halt and debug retrieval functions
  3. Only then proceed to Tier 2 outcome labeling with adjudication workflow

- Design tradeoffs:
  - **Function granularity vs. context bloat**: More specific functions reduce per-call data volume but increase orchestration complexity
  - **Recall vs. precision**: Prompts explicitly require evidence for/against outcomes, prioritizing recall (fewer missed events) at potential cost of more false positives
  - **Single-reviewer adjudication vs. panels**: Faster, but borderline cases may be misclassified; paper acknowledges this limitation

- Failure signatures:
  - Truncated course IDs (leading-number truncation) -> surfaced in Tier 1
  - High false-positive rate with low precision (e.g., ORN: 48.4% precision pre-adjudication) -> may indicate over-sensitive prompting or ambiguous clinical documentation
  - Indeterminate adjudications (5/48 cases) -> insufficient evidence in records; may require enriched retrieval or manual escalation

- First 3 experiments:
  1. **Tier 1 regression test**: After any function or prompt change, re-run structured QA on 100 patients; expect 100% demographic, ≥99% treatment-plan accuracy
  2. **Ablation on function granularity**: Compare outcome labeling performance when using coarse functions (e.g., get all notes) vs. granular functions (e.g., get radiation_oncology notes only); hypothesis: granularity reduces context bloat and improves precision
  3. **Cross-site generalization**: Apply the same cancer recurrence prompt to a new disease site (e.g., lung cancer) without prompt modification; measure recall/precision to test generalization claims

## Open Questions the Paper Calls Out
None

## Limitations
- Single-adjudicator approach may introduce systematic bias and lacks panel validation
- Evaluation limited to three specific outcome types within one institutional context, limiting generalizability
- No evidence provided for ground-truth error rates in adjudicated cases classified as "indeterminate"

## Confidence
- **High Confidence**: Claims regarding high recall (≥88%) and improved accuracy (95.2-96.3%) post-adjudication are well-supported by structured evaluation methodology and explicit adjudication process
- **Medium Confidence**: Assertion that 63% of discrepancies stem from ground-truth errors is credible but depends on reliability of single-adjudicator classification
- **Low Confidence**: Claims about dual utility as registry auditor require external validation across multiple institutions and EHR systems

## Next Checks
1. Deploy RadOnc-GPT at a second institution with different EHR system and evaluate recall/precision for same outcome types to assess robustness
2. Re-adjudicate subset of discrepancies using 2-3 expert panel and compare with original single-adjudicator results to quantify bias
3. Run RadOnc-GPT on same cohort at 6-month intervals over 2 years to track changes in accuracy, precision, and recall for model drift detection