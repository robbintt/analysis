---
ver: rpa2
title: Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge
arxiv_id: '2512.03019'
source_url: https://arxiv.org/abs/2512.03019
tags:
- calibration
- prompt
- tasks
- evaluation
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of reliably aggregating multiple
  noisy pairwise judgments from Thinking Large Language Models (LLMs) used as automated
  evaluators. The core problem is that simple majority voting fails to account for
  the evidential strength in vote distributions, especially in the presence of ties,
  leading to inconsistent and less robust evaluations.
---

# Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2512.03019
- Source URL: https://arxiv.org/abs/2512.03019
- Reference count: 40
- Primary result: Distribution-calibrated aggregation using Bradley-Terry-Davidson model reduces MAE by up to 30% and achieves human-level performance in LLM-as-a-judge evaluations.

## Executive Summary
This paper addresses the challenge of aggregating multiple noisy pairwise judgments from Thinking Large Language Models used as automated evaluators. Simple majority voting fails to account for evidential strength in vote distributions, especially with ties, leading to inconsistent and less robust evaluations. The authors propose a principled Distribution-Calibrated Aggregation method based on a Bradley-Terry-Davidson formulation that models three-way preferences using full counts of positive, negative, and tie votes. This approach explicitly captures both preference margin and global tie propensity, distinguishing narrow margins from strong consensus. Across multiple benchmarks, including machine translation and reward modeling tasks, this method consistently reduces MAE and increases pairwise accuracy compared to standard baselines.

## Method Summary
The method extracts two smoothed features from raw vote counts: a decisiveness margin capturing polarity among non-ties, and a tie-evidence feature capturing non-tie rate. These feed into a multinomial logit model with parameters that jointly model latent margin and tie logit, yielding calibrated probabilities for three outcomes via the Bradley-Terry-Davidson formulation. Parameters are fit via maximum likelihood on a small calibration set using Discrete Ranked Probability Score to align with the MAE evaluation metric. At inference, the MAE Bayes-optimal action selects the label minimizing expected risk. Experiments show consistent MAE reductions across tasks when evaluated against human-consensus meta-labels.

## Key Results
- MAE reductions up to 30% compared to simple count (SC) baseline across 8 tasks
- Position bias drops dramatically when ties allowed (14.5% → 3.1% for gemini-2.5-flash)
- Matches or exceeds individual human raters when evaluated against human-consensus meta-labels
- Diminishing returns after n=12 samples per item, with plateau in MAE gains
- Transferability between tasks is inconsistent—RB2-Factuality shows poor cross-task transfer despite similar ground truth distributions

## Why This Works (Mechanism)

### Mechanism 1
Distribution-calibrated aggregation using Bradley-Terry-Davidson (BTD) formulation improves evaluation reliability by explicitly modeling both preference margin and tie propensity from vote counts. The method extracts two smoothed features from raw vote counts: (1) a decisiveness margin `s = 0.5 * log((c+ + α)/(c- + α))` capturing polarity among non-ties, and (2) a tie-evidence feature `t = log((c0 + κ)/(n + κ))` capturing non-tie rate. These feed into a multinomial logit model with parameters θ = (β, η₀, γ) that jointly model latent margin u = βs and tie logit η = η₀ + γt, yielding calibrated probabilities for {+1, 0, -1} outcomes via `p(+1) = e^u/Z`, `p(-1) = e^{-u}/Z`, `p(0) = e^η/Z`. The core assumption is that vote distribution contains reliable signal about ground truth preferences that majority voting discards; the BTD parametric form correctly captures the relationship between vote counts and true preferences.

### Mechanism 2
Two-stage optimization—fitting parameters via Discrete Ranked Probability Score (DRPS) and applying MAE Bayes action at inference—aligns the probabilistic model with the ordinal evaluation metric while maintaining a differentiable optimization landscape. Direct MAE minimization is ill-suited for gradient-based methods because predictions change only at decision boundaries. DRPS, a strictly proper scoring rule for ordinal outcomes, is used for parameter fitting: `DRPS = (F_{-1} - H_{-1}(y*))² + (F_0 - H_0(y*))²` where F are cumulative probabilities and H are cumulative indicators. This produces well-calibrated probabilities. At inference, the MAE Bayes-optimal action selects `argmin_y R(y)` where risks are `R(-1) = p(0) + 2p(+1)`, `R(0) = p(+1) + p(-1)`, `R(+1) = 2p(-1) + p(0)`.

### Mechanism 3
Allowing ties and calibrating for tie propensity mitigates positional bias and adapts to task-specific tie rates in ground truth. Experiments show positional bias drops dramatically when ties are allowed (gemini-2.5-flash: 14.5% → 3.1%, qwen3-next-80b: -8.0% → 0.6%). However, tie rates are highly unstable across prompts (12.4%–37.6% for gemini-2.5-flash) and models. The BTD model's γ parameter modulates tie sensitivity based on observed tie counts, adapting predictions to match ground truth distributions—over-correcting for tie-averse prompts and under-correcting for tie-biased ones.

## Foundational Learning

- **Bradley-Terry-Davidson Model**: The core probabilistic model for pairwise comparisons with ties. Without understanding how BTD extends Bradley-Terry to handle three outcomes, the feature-to-probability mapping is opaque. Quick check: Given 7 votes for A, 2 for B, and 1 tie, how would the s feature (margin) and t feature (tie evidence) differ from a 5-4-1 split? What does this imply about the model's ability to distinguish strong vs. weak consensus?

- **Bayes-Optimal Decisions under MAE Loss**: The inference-time decision rule is not argmax of probabilities—it's the Bayes action under MAE loss. Understanding why `R(0) = p(+1) + p(-1)` differs from `R(-1) = p(0) + 2p(+1)` explains the asymmetric tie-breaking behavior. Quick check: If `p(+1) = 0.4, p(0) = 0.2, p(-1) = 0.4`, which label minimizes expected MAE risk? Would majority voting give the same answer?

- **Proper Scoring Rules for Ordinal Outcomes**: The paper uses DRPS instead of direct MAE minimization for calibration. Understanding why proper scoring rules guarantee Fisher consistency explains the two-stage design. Quick check: Why can't we directly minimize MAE during calibration using gradient descent? What property does DRPS have that MAE lacks?

## Architecture Onboarding

- **Component map**:
```
Calibration Set → LLM (n samples each) → Vote Counts (c+, c-, c0)
→ Feature Extraction (s, t) → DRPS Minimization → Fitted θ = (β, η₀, γ)

New Query → LLM (n samples) → Vote Counts → Feature Extraction (s, t)
→ BTD Probabilities (using θ) → MAE Risk Computation → Bayes Action ŷ
```

- **Critical path**: Calibration data quality (must be representative of evaluation distribution), sample count n (experiments show plateau around n=12; too few samples = noisy features), temperature setting (T=0.5 recommended; T→0 collapses diversity, hurting calibration), parameter fitting stability (L-BFGS-B with box constraints: β∈[10⁻³, 5], ν∈[10⁻⁴, 10³], γ∈[-10, 10])

- **Design tradeoffs**: Calibration set size vs. fit quality (paper shows MAE plateaus after ~60–80 examples), sample count n vs. compute (diminishing returns after n=12 for most tasks), temperature vs. diversity (lower T reduces effective sample size), in-domain vs. transfer calibration (cross-task transfer is inconsistent)

- **Failure signatures**: SC baseline outperforms on very low n (if n≤3, vote distribution may be too sparse), over-prediction of ties (if calibration set has high tie rate but evaluation set doesn't), transfer failure (RB2-Factuality transfers poorly despite similar ground truth distribution), temperature too low (T=0.1 consistently underperforms T=0.5 due to collapsed sample diversity)

- **First 3 experiments**:
  1. Calibration set size ablation on your task: Start with 20, 50, 100, 200 calibration examples; plot validation MAE to identify plateau point. Compare to paper's Figure 4 patterns.
  2. Sample count sweep: Test n ∈ {4, 8, 12, 16, 20} on a held-out validation set. Identify where your task plateaus. Compare to SC baseline at each n to quantify relative gains.
  3. Transfer test with related task: If you have multiple evaluation tasks, fit θ on one and evaluate on others. Compare to in-domain calibration MAE to assess transfer risk. Identify whether your task falls into high-correction, low-correction, or mismatched regime based on fitted ν.

## Open Questions the Paper Calls Out

### Open Question 1
How can the conditions of "regime compatibility" be characterized to accurately predict when calibration parameters from a source task will successfully transfer to a different target task? The paper observes asymmetric transfer success (e.g., WMT ZH→EN helps EN→DE but not vice versa) but lacks a theoretical model to predict this behavior based on the interplay between LLM vote propensity and ground truth distributions. A principled test or metric that correlates with cross-task transfer gains, potentially based on the similarity of the fitted calibration parameters or the "calibration landscape" shape, would resolve this.

### Open Question 2
Can the proposed distribution-calibrated framework be effectively generalized to broader ordinal and multi-class outcomes beyond ternary pairwise comparisons? The current method relies on the Bradley-Terry-Davidson formulation designed specifically for three-way preferences; it is unclear if the Discrete Ranked Probability Score (DRPS) approach scales efficiently to higher-dimensional output spaces. Experimental results applying the framework to Likert-scale evaluations or multi-choice verification tasks, showing consistent improvements over baseline aggregation methods, would resolve this.

### Open Question 3
Does incorporating token-level likelihoods or thinking-trace confidence scores provide significant marginal gains over the count-based approach, particularly in low-temperature regimes? While counts preserve evidence strength in high-variance distributions, they discard the model's internal confidence; this information might be critical when sample diversity is artificially low (e.g., at low temperatures) or for distinguishing "confident" majorities from "reluctant" ones. A comparison of count-only aggregation versus confidence-weighted aggregation across diverse temperature settings and model architectures would resolve this.

## Limitations
- The distributional calibration framework assumes the Bradley-Terry-Davidson model correctly captures the relationship between vote counts and ground truth preferences, which may not hold for all evaluation domains
- Requires a representative calibration set, with performance degrading under distribution shift—RB2-Factuality shows particularly poor cross-task transfer
- Temperature parameter (T=0.5) is empirically chosen and may need task-specific tuning for optimal diversity
- The assumption that tie rate instability is primarily a calibration problem rather than fundamental noise is not fully validated

## Confidence
- **High confidence**: MAE improvements over SC baseline on in-domain tasks, positional bias reduction when ties are allowed, and calibration set size effects
- **Medium confidence**: Cross-task transfer performance and the RB2-Factuality outlier, which shows parameter instability and suggests regime mismatch rather than calibration failure
- **Medium confidence**: The mechanism of tie-evidence feature modulating tie predictions—while mathematically sound, the assumption about tie rate instability being a calibration problem is not fully validated

## Next Checks
1. **Calibration set distribution shift test**: Fit parameters on one domain (e.g., WMT23) and evaluate on a shifted domain (e.g., RB2-Factuality). Measure whether MAE degradation correlates with divergence in tie rate distributions between domains.

2. **Temperature sensitivity analysis**: Systematically vary temperature T ∈ {0.1, 0.3, 0.5, 0.7, 0.9} and measure how vote distribution entropy changes. Verify that T=0.5 provides optimal trade-off between diversity and consensus strength for your specific task.

3. **BTD model assumption validation**: Generate synthetic vote distributions where ground truth follows known preference strengths and tie propensities. Test whether the fitted BTD model recovers these parameters accurately, or if alternative formulations perform better.