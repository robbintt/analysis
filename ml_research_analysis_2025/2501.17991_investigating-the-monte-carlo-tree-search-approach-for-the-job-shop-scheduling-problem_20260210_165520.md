---
ver: rpa2
title: Investigating the Monte-Carlo Tree Search Approach for the Job Shop Scheduling
  Problem
arxiv_id: '2501.17991'
source_url: https://arxiv.org/abs/2501.17991
tags:
- type
- mcts
- operations
- time
- jobs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Monte Carlo Tree Search (MCTS) for solving
  the Job Shop Scheduling Problem (JSSP), specifically minimizing the weighted sum
  of job completion times. The authors propose multiple Markov Decision Process (MDP)
  formulations to model the JSSP for MCTS, exploring different state representations
  (absolute and relative) and action spaces (selecting single operations, entire jobs,
  or jobs with scheduling gaps).
---

# Investigating the Monte-Carlo Tree Search Approach for the Job Shop Scheduling Problem

## Quick Facts
- arXiv ID: 2501.17991
- Source URL: https://arxiv.org/abs/2501.17991
- Reference count: 17
- This paper proposes using MCTS for large-scale JSSP, showing superior performance to CP approaches

## Executive Summary
This paper explores Monte Carlo Tree Search (MCTS) for solving the Job Shop Scheduling Problem (JSSP) with the objective of minimizing weighted sum of job completion times. The authors propose four Markov Decision Process (MDP) formulations with different state representations (absolute vs. relative completion times) and action spaces (single operations, entire jobs, or jobs with scheduling gaps). They introduce a new synthetic benchmark derived from real manufacturing data to better represent large-scale, non-rectangular instances common in practice. Experimental results demonstrate that MCTS effectively produces good-quality solutions for large-scale JSSP instances, with relative state representation and job-level actions achieving the best performance.

## Method Summary
The method frames JSSP as an MDP where states represent partial schedules, actions select jobs or operations to schedule next, and rewards are negative weighted completion times. MCTS explores this space using UCB selection, with four environment types varying in state representation (absolute vs. relative) and action granularity (operation vs. job level with optional gap percentages). The relative state representation encodes only operation ordering, preserving scheduling flexibility. Priority Dispatching Rules (PDRs) constrain the action space to manageable sets. The approach is compared against Constraint Programming using Google OR-Tools on 20 synthetic instances with 600-1000 jobs and 50-70 machines.

## Key Results
- MCTS outperforms CP baseline on all 20 synthetic instances, with mean objective values 28-56% better
- Best configurations use relative state representation with job-level actions and gap percentages [0.3, 0.6, 0.8] or [0.6, 0.8, 1.0]
- LWF and SJF PDRs consistently outperform other dispatching rules
- Type 4 and Type 5 environments (relative state + job actions) achieve the best performance on 90% of instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative state representations with flexible gap scheduling produce higher-quality solutions for large-scale JSSP instances.
- Mechanism: By encoding only operation ordering (not exact completion times), the state representation preserves scheduling flexibility. Combined with gap-based action selection—where operations are inserted into idle times even when the gap is smaller than the operation's processing time—the algorithm can shift subsequent operations to accommodate new insertions. This flexibility allows the search to explore non-greedy partial schedules that absolute representations would prematurely fix.
- Core assumption: The search algorithm can efficiently compute actual completion times from the relative ordering when evaluating terminal states, and the O(n log n) conversion cost is acceptable relative to search iterations.
- Evidence anchors:
  - [section 3.3] "This approach is greedy only on the order of operations and it maintains flexibility in terms of scheduling completion time."
  - [section 5.2] "Configurations from Types 4, 5 and 2 achieve the best performance for 50%, 40% and 10% of the instances, respectively. This highlights the benefit of using a less greedy and more flexible environment."
  - [corpus] DyRo-MCTS (arXiv:2509.21902) similarly uses MCTS for dynamic JSSP with robust performance, suggesting the mechanism generalizes to dynamic settings.
- Break condition: If the problem requires strict timing constraints (e.g., hard deadlines on intermediate operations) or if the conversion from relative to absolute representation becomes a computational bottleneck, this mechanism may fail.

### Mechanism 2
- Claim: Job-level action selection with PDRs reduces tree depth and improves search efficiency for large-scale instances.
- Mechanism: Instead of selecting individual operations (which creates deep trees with branching factors proportional to available operations), selecting entire jobs as atomic actions reduces tree depth by a factor proportional to average operations per job. PDRs (Priority Dispatching Rules) further constrain the action space to a small set of heuristically-informed choices (e.g., 3-6 actions), making tree search tractable.
- Core assumption: The PDRs used capture enough of the problem structure that optimal or near-optimal solutions remain reachable within the constrained action space.
- Evidence anchors:
  - [section 3.3] "Scheduling a whole job as a single action allows the search tree to be less deep, making the exploration space more manageable."
  - [table 1] Type 2 and Type 4 configurations use job-level PDRs (SJF, LWF, MWF, LJF) with 3-6 actions.
  - [corpus] Weak corpus evidence—no direct comparison of operation-level vs job-level action efficiency in neighbors.
- Break condition: If jobs have highly heterogeneous operation sequences or if inter-job dependencies are critical, job-level selection may over-constrain the search and miss optimal solutions.

### Mechanism 3
- Claim: UCB-guided tree search balances exploitation of promising schedules with exploration of alternatives, avoiding local optima common in greedy heuristics.
- Mechanism: The Upper Confidence Bound formula (UCB = X̄ⱼ + C√(ln N/nⱼ)) ensures nodes with high average rewards are exploited while nodes with fewer visits are explored. This addresses the JSSP's combinatorial complexity where greedy decisions early in scheduling can lock in suboptimal sequences.
- Core assumption: The reward normalization (mapping to [0,1]) is correctly calibrated so that UCB meaningfully balances exploration/exploitation.
- Evidence anchors:
  - [section 3.2] "A tree policy that has promising properties is the Upper Confidence Bound (UCB) formula. This formula balances the exploitation of the best-known nodes and the exploration of less visited nodes."
  - [section 3.3] "Normalisation is necessary to ensure the UCB formula is in the range [0, 1]."
  - [corpus] Adaptive Bias Generalized Rollout Policy (arXiv:2505.08451) similarly uses rollout policy adaptation for FJSSP, supporting tree-search-based approaches.
- Break condition: If the exploration constant C is poorly tuned, or if reward scaling produces extreme values, UCB may over-exploit or over-explore, degrading solution quality.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: The paper frames JSSP as an MDP with states (partial schedules), actions (job/operation selection), transitions (deterministic scheduling), and rewards (negative weighted completion time). Understanding MDP structure is essential to interpret the MCTS formulation.
  - Quick check question: Given a partial schedule with jobs {J1, J2} partially completed, what constitutes the state, and what actions are available?

- Concept: **Monte Carlo Tree Search (MCTS) phases**
  - Why needed here: The algorithm relies on selection (UCB), expansion (adding child nodes), simulation (default policy to terminal state), and backpropagation (updating statistics). Each phase has configurable components that affect performance.
  - Quick check question: In the simulation phase, what is the tradeoff between random rollouts vs. heuristic-guided rollouts for computational cost vs. solution quality?

- Concept: **Job Shop Scheduling Problem structure**
  - Why needed here: The problem involves precedence constraints (operations within a job), machine capacity constraints (one operation per machine at a time), and recirculation (jobs can visit machines multiple times). These constraints define the feasible action space at each state.
  - Quick check question: For a job with operations {O1, O2, O3} requiring machines {M1, M2, M1}, which operations can be scheduled concurrently with operations from other jobs?

## Architecture Onboarding

- Component map:
  ```
  MCTS Core
  ├── State Representation Layer
  │   ├── Absolute (completion times) - Type 1
  │   └── Relative (operation order) - Types 4, 5 [BEST]
  ├── Action Selection Layer
  │   ├── Operation-level PDRs - Type 1
  │   ├── Job-level PDRs - Type 2 [GOOD]
  │   └── Job + Gap Percentage - Types 4, 5 [BEST]
  ├── Simulation Layer
  │   └── Default policy (random or PDR-guided)
  └── Reward Computation
      └── Normalized negative weighted sum of completion times
  ```

- Critical path:
  1. Initialize root node with empty schedule (state s₀)
  2. **Selection**: Traverse tree using UCB until reaching expandable node
  3. **Expansion**: Add child node for unscheduled job/action combination
  4. **Simulation**: Run default policy to complete schedule
  5. **Backpropagation**: Update node statistics with normalized reward
  6. **Step forward** (after sufficient iterations): Commit best action, re-root tree
  7. Repeat until all jobs scheduled

- Design tradeoffs:
  - **Absolute vs. Relative states**: Absolute is computationally cheaper per node (no conversion) but fixes timing decisions early. Relative requires O(n log n) conversion but preserves flexibility.
  - **Operation vs. Job actions**: Operation-level gives finer control but deeper trees. Job-level reduces depth but may miss inter-job optimizations.
  - **Gap percentage choices**: Lower percentages (0.3-0.6) allow tighter packing but risk infeasibility. Higher percentages (0.6-1.0) are safer but may leave idle time.
  - **PDR selection**: LWF/SJF performed best (Table 2), but optimal PDR may be instance-dependent.

- Failure signatures:
  - **Type 3 (operation + gap)**: Computationally infeasible due to recomputing completion times at each state—avoid for large instances.
  - **High mean objective values** (e.g., Type 1.1 at 2.4M vs. Type 5.5 at 1.54M): Indicates overly greedy action space or poor PDR selection.
  - **No improvement with more iterations**: Suggests exploration constant C is too low (over-exploitation) or action space is too constrained.

- First 3 experiments:
  1. **Baseline comparison**: Implement Type 4.1 (relative state, job-level LWF with gaps [0.6, 0.8, 1.0]) on a small Taillard instance (e.g., 10×10) to validate implementation against known benchmarks.
  2. **Ablation on gap percentages**: Compare Type 4.1 vs. Type 4.2 on 5 synthetic instances to quantify impact of tighter gap thresholds on solution quality vs. computation time.
  3. **PDR sensitivity**: Run Types 4.1, 4.3, 4.5, 4.7 (same architecture, different PDRs) on the paper's 20 synthetic instances to verify that LWF outperforms MWF, SJF, and LJF consistently.

## Open Questions the Paper Calls Out

- **Question:** Can a machine-learning-based reward function improve MCTS performance by enabling the evaluation of partial schedules?
- **Basis in paper:** [explicit] The conclusion states that future research could refine the MCTS approach by "exploring a machine-learning-based reward function allowing the evaluation of a partial schedule."
- **Why unresolved:** The current study relies on sparse rewards that are only calculated upon reaching a terminal state (completion of all jobs), which provides no gradient signal during the search process.
- **What evidence would resolve it:** Experimental results comparing the convergence speed and solution quality of a learned dense reward function against the current sparse reward implementation on the large-scale benchmark.

- **Question:** Can operation-level action spaces with gap scheduling (Environment Type 3) be made computationally feasible for large-scale instances?
- **Basis in paper:** [inferred] The authors explicitly abandoned Environment Type 3 because recomputing completion times to identify idle gaps at every decision step was computationally too expensive for large problems.
- **Why unresolved:** While theoretically promising for flexibility, this action space was practically infeasible due to the high algorithmic complexity of state updates in the current implementation.
- **What evidence would resolve it:** The development of an incremental state update mechanism or a more efficient data structure that allows Type 3 environments to run within competitive time limits.

- **Question:** How does the proposed MCTS approach compare to other meta-heuristics, such as Tabu Search or Genetic Algorithms, on non-rectangular instances?
- **Basis in paper:** [inferred] While the paper compares MCTS against Constraint Programming and mentions meta-heuristics in the literature review, it does not benchmark MCTS against other approximate methods like Simulated Annealing or Tabu Search on the new complex benchmark.
- **Why unresolved:** It is unclear if MCTS is the best *heuristic* approach for this specific problem structure, or if it merely outperforms exact methods like CP which struggle with large solution spaces.
- **What evidence would resolve it:** A comparative performance analysis on the new benchmark including standard meta-heuristic baselines.

## Limitations

- **Computational scalability**: The relative state representation requires O(n log n) conversion to completion times, which may become prohibitive for very large instances (>10,000 jobs).
- **Hyperparameter sensitivity**: The effectiveness of PDRs and gap percentage ranges may be instance-dependent, requiring careful tuning for different problem characteristics.
- **Sparse reward signal**: The current implementation only provides rewards at terminal states, limiting the learning signal during the search process.

## Confidence

- **High Confidence**: MCTS outperforms CP baseline on large-scale instances; relative state representation with job-level actions provides flexibility advantages; the new synthetic benchmark is a valuable contribution for evaluating scheduling algorithms.
- **Medium Confidence**: LWF and SJF PDRs are generally effective; the mechanism of gap-based scheduling preserving flexibility is valid but may not be optimal for all instance types.
- **Low Confidence**: The specific gap percentage ranges [0.3, 0.6, 0.8] vs [0.6, 0.8, 1.0] are optimal; MCTS will scale effectively to much larger problem sizes.

## Next Checks

1. **Hyperparameter sensitivity**: Run ablation studies on the exploration constant C and gap percentage ranges across the 20 synthetic instances to quantify impact on solution quality and computation time.
2. **Instance diversity testing**: Apply the best-performing MCTS configurations (Type 4/5) to additional benchmark suites (Taillard, Lawrence, OR-Library) to verify generalization across different instance characteristics (e.g., job length distributions, machine heterogeneity).
3. **Scalability experiment**: Test MCTS on progressively larger synthetic instances (2×, 5×, 10× the original size) to identify the practical limits of the approach and characterize the relationship between instance size and solution quality/computation time.