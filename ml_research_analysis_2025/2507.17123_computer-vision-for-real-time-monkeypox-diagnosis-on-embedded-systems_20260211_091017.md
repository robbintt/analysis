---
ver: rpa2
title: Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems
arxiv_id: '2507.17123'
source_url: https://arxiv.org/abs/2507.17123
tags:
- monkeypox
- available
- precision
- online
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an AI-driven diagnostic tool for real-time
  monkeypox detection, deployed on the NVIDIA Jetson Orin Nano. The tool leverages
  the pre-trained MobileNetV2 architecture for binary classification, trained on the
  Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score.
---

# Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems

## Quick Facts
- arXiv ID: 2507.17123
- Source URL: https://arxiv.org/abs/2507.17123
- Reference count: 31
- Primary result: 93.07% F1-Score for monkeypox detection on Jetson Orin Nano

## Executive Summary
This study presents an AI-driven diagnostic tool for real-time monkeypox detection, deployed on the NVIDIA Jetson Orin Nano. The tool leverages the pre-trained MobileNetV2 architecture for binary classification, trained on the Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score. To optimize the model, TensorRT was used to accelerate inference and perform post-training quantization for FP32, FP16, and INT8 formats. This optimization reduced model size, increased inference speed, and lowered power consumption by approximately 10%, all while maintaining the original accuracy. The system includes a Wi-Fi Access Point hotspot and a web-based interface, enabling users to upload and analyze images directly via connected devices. This setup ensures simple access and seamless connectivity, making the tool practical for real-world applications. The results position the diagnostic tool as an efficient, scalable, and energy-conscious solution for addressing diagnosis challenges in underserved regions.

## Method Summary
The study employs MobileNetV2, pre-trained on ImageNet, for binary classification of monkeypox skin lesions. The model was trained on the Monkeypox Skin Lesion Dataset (MSLD) using transfer learning, where the majority of the model's layers were frozen and only the final layers were retrained. Data augmentation techniques such as rotation, contrast adjustment, and scaling were applied to the 3,192-image dataset. The model was optimized using TensorRT for post-training quantization, generating engines in FP32, FP16, and INT8 formats. Deployment was on the NVIDIA Jetson Orin Nano, which also served as a Wi-Fi Access Point with a web-based interface for image upload and analysis.

## Key Results
- Achieved 93.07% F1-Score on the MSLD dataset using MobileNetV2
- Reduced model size by up to 72% and power consumption by ~10% through TensorRT quantization
- Enabled real-time inference with a web-based interface via Jetson Orin Nano's Wi-Fi AP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from ImageNet to monkeypox classification accelerates convergence and achieves usable accuracy despite limited medical imaging data.
- Mechanism: Pre-trained ImageNet weights encode general edge, texture, and shape detectors in early convolutional layers. By freezing these layers and retraining only the final classification head, the model adapts domain-general visual features to disease-specific patterns without requiring large-scale medical datasets.
- Core assumption: Low-level visual features (edges, textures, color gradients) from natural images transfer meaningfully to skin lesion classification.
- Evidence anchors:
  - [abstract] "leveraging the pre-trained MobileNetV2 architecture for binary classification"
  - [section II.A] "This method involves freezing the majority of the model's layers, preserving the pre-trained weights, and re-training the final layers on the new dataset"
  - [corpus] Related work confirms MobileNetV2 and VGGNet models have demonstrated effectiveness in medical image classification for disease detection tasks
- Break condition: If target domain requires fundamentally different visual primitives (e.g., hyperspectral patterns invisible in RGB), transfer benefits diminish significantly.

### Mechanism 2
- Claim: TensorRT post-training quantization reduces memory bandwidth and compute intensity, lowering power consumption while preserving accuracy.
- Mechanism: TensorRT converts FP32 weights to FP16 or INT8, halving or quartering memory transfers. Mixed-precision selectively retains FP32 for sensitive layers. Reduced precision enables integer arithmetic units, which consume less energy per operation than floating-point units on the same hardware.
- Core assumption: The model's decision boundaries are robust to small perturbations in weight precision; classification tasks tolerate quantization noise better than regression tasks.
- Evidence anchors:
  - [abstract] "TensorRT's mixed-precision capabilities enabled these optimizations, which reduced the model size, increased inference speed, and lowered power consumption by approximately a factor of two"
  - [section III.B] "model size is significantly reduced, with decreases by factors of 0.45, 0.45, and 0.72 for FP32, FP16, and INT8 formats, respectively"
  - [corpus] Corpus evidence on embedded TinyML benchmarking confirms quantization improves inference latency and memory efficiency across constrained devices
- Break condition: Highly sensitive architectures or tasks with small inter-class margins may experience accuracy degradation, requiring careful calibration.

### Mechanism 3
- Claim: Local Wi-Fi AP deployment removes dependency on external network infrastructure, enabling operation in connectivity-scarce environments.
- Mechanism: The Jetson Orin Nano creates a self-contained wireless network and hosts a web server. Client devices connect directly to this local network, upload images via HTTP, and receive inference results without internet access.
- Core assumption: Users have Wi-Fi-capable devices (smartphones, tablets) and can navigate to a local IP address; no cloud latency or reliability issues affect the interaction.
- Evidence anchors:
  - [abstract] "system includes a Wi-Fi Access Point hotspot and a web-based interface, enabling users to upload and analyze images directly via connected devices"
  - [section II.D] "In AP mode, the Wi-Fi setup creates a dedicated wireless communication network, allowing the Jetson Orin Nano to function as a gateway for nearby devices"
  - [corpus] Weak corpus support—no adjacent papers specifically address AP-mode deployment patterns; this mechanism is primarily evidenced within the paper itself
- Break condition: Regulatory environments requiring audit logs, remote updates, or multi-device synchronization may need hybrid local/cloud architecture.

## Foundational Learning

- **Transfer Learning & Fine-tuning**
  - Why needed here: Understanding which layers to freeze vs. retrain determines whether the model captures domain-specific features or overfits to a small dataset.
  - Quick check question: Given a model pre-trained on ImageNet, which layers would you freeze for a 3,192-image binary classification task, and why?

- **Post-Training Quantization (PTQ)**
  - Why needed here: Selecting FP32/FP16/INT8 involves trading off accuracy, speed, and energy; PTQ requires calibration data but not full retraining.
  - Quick check question: What is the minimum calibration dataset size typically recommended for INT8 PTQ, and what happens if calibration is insufficient?

- **Embedded Power & Thermal Budgeting**
  - Why needed here: The Jetson Orin Nano has fixed power envelopes; sustained inference loads can trigger thermal throttling, degrading real-time performance.
  - Quick check question: If idle power is ~5.2W and inference peaks at ~6W, what headroom remains before hitting the device's thermal design power limit?

## Architecture Onboarding

- **Component map:**
  [Mobile Device] --Wi-Fi--> [Jetson Orin Nano AP]
  ├── Flask/gunicorn Web Server
  ├── TensorRT Runtime Engine
  │   └── Optimized MobileNetV2 (.plan file)
  └── Inference Output (JSON/HTML)

- **Critical path:**
  1. Model training (GPU workstation → Keras/PyTorch → SavedModel)
  2. TensorRT conversion (ONNX export → trtexec/python API → FP16/INT8 engine)
  3. Deployment packaging (engine file + web server + network config → Jetson)
  4. Runtime inference (image preprocessing → TensorRT async inference → post-processing)

- **Design tradeoffs:**
  - FP32 vs. FP16 vs. INT8: FP16 offers best balance for this hardware; INT8 calibration was constrained by 8GB RAM, yielding marginal gains.
  - Local AP vs. cloud: Local eliminates latency and connectivity dependencies; cloud enables centralized updates and logging.
  - MobileNetV2 vs. larger backbones: MobileNetV2 chosen for efficiency; larger models (EfficientNetB3, ResNet50) showed higher variance without deployment optimization.

- **Failure signatures:**
  - High false negative rate (15/20 misclassifications were false negatives): model biases toward "Others" class—indicates class imbalance or insufficient positive class augmentation.
  - INT8 underperforms FP16: memory constraints during calibration caused partial quantization; symptom is INT8 inference slower than FP16.
  - Web interface unresponsive: AP mode DHCP exhaustion or thermal throttling under concurrent connections.

- **First 3 experiments:**
  1. Reproduce baseline training on MSLD with MobileNetV2; log F1-Score per fold to verify 93.07% ± stated variance.
  2. Convert trained model to TensorRT FP16 engine; benchmark inference latency and power draw vs. unoptimized model.
  3. Deploy AP hotspot + web server on Jetson; measure concurrent connection limit and thermal behavior under sustained 5-minute inference load.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the diagnostic model maintain its accuracy and efficiency when generalized to larger, more diverse clinical datasets?
- Basis in paper: [explicit] The Conclusion states that future work requires "more extensive datasets" to test the model's "ability to generalize effectively to unseen data."
- Why unresolved: The current study relies on the Monkeypox Skin Lesion Dataset (MSLD), which contains only 228 original images (augmented to 3,192), limiting the diversity of training data.
- What evidence would resolve it: Performance metrics (F1-Score, Recall) derived from training and validating the model on a larger, un-augmented external dataset.

### Open Question 2
- Question: Can the system be effectively deployed on alternative low-cost embedded hardware using different optimization frameworks?
- Basis in paper: [explicit] The authors explicitly propose exploring "deployment on other embedded systems... such as the Raspberry Pi and Arduino Nano" using frameworks like "LiteRT and Apache TVM."
- Why unresolved: The current implementation is specialized for the NVIDIA Jetson Orin Nano and the TensorRT optimization framework.
- What evidence would resolve it: Benchmark results showing inference latency and power consumption of the model running on Raspberry Pi hardware with LiteRT optimizations.

### Open Question 3
- Question: Can the high rate of false negatives be reduced without compromising the overall model performance?
- Basis in paper: [inferred] The Results section notes that 15 of the 20 misclassifications were false negatives, a finding the authors describe as "particularly concerning" for clinical deployment.
- Why unresolved: While the overall F1-Score is high (93.07%), the model's specific tendency to misclassify "Monkeypox" instances as "Others" poses a risk for disease containment.
- What evidence would resolve it: A modified training approach (e.g., class weighting or focal loss) resulting in a confusion matrix with significantly fewer false negatives.

## Limitations

- INT8 calibration constraints: The paper notes RAM limitations (8GB) restricted calibration quality, yet still reports INT8 performance. This suggests potential model accuracy degradation in the quantized INT8 engine, though not quantified.
- Class imbalance and bias: The confusion matrix reveals a strong bias toward predicting "Others," with 15/20 misclassifications as false negatives. This suggests potential overfitting to the majority class, but the paper does not disclose class distribution or use class weighting to address imbalance.

## Confidence

- **High confidence:** Core mechanism of transfer learning with MobileNetV2 for binary classification (93.07% F1-score on MSLD), and the web-based AP hotspot deployment for local inference.
- **Medium confidence:** TensorRT quantization efficacy, as the paper reports speed and power gains but lacks detailed calibration methodology or full INT8 accuracy validation.
- **Low confidence:** Generalization to multiclass scenarios (MSLD v2.0), as the paper briefly mentions extension but provides no performance metrics or detailed adaptation strategy.

## Next Checks

1. Replicate the 5-fold cross-validation training pipeline on MSLD; verify F1-Score variance matches the reported 93.07% ± stated confidence interval.
2. On the Jetson Orin Nano, generate TensorRT FP16 and INT8 engines from the trained model; benchmark inference latency, throughput, and power consumption against the reported values.
3. Deploy the AP hotspot + web server locally; test concurrent client connections and measure thermal throttling under sustained inference load.