---
ver: rpa2
title: 'CoSER: A Comprehensive Literary Dataset and Framework for Training and Evaluating
  LLM Role-Playing and Persona Simulation'
arxiv_id: '2502.09082'
source_url: https://arxiv.org/abs/2502.09082
tags:
- coser
- character
- dataset
- characters
- role-playing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of simulating established characters
  in role-playing language agents, which requires authentic data and nuanced evaluation
  methods. The authors introduce CoSER, a comprehensive framework including a high-quality
  dataset of 17,966 characters from 771 renowned books, open models (CoSER 8B and
  70B), and a novel evaluation protocol based on given-circumstance acting.
---

# CoSER: A Comprehensive Literary Dataset and Framework for Training and Evaluating LLM Role-Playing and Persona Simulation

## Quick Facts
- **arXiv ID:** 2502.09082
- **Source URL:** https://arxiv.org/abs/2502.09082
- **Reference count:** 40
- **Key outcome:** Introduces CoSER framework with authentic literary dataset, open models, and GCA evaluation; achieves SOTA performance on InCharacter (75.80%) and LifeChoice (93.47%).

## Executive Summary
This paper addresses the challenge of simulating established characters in role-playing language agents, which requires authentic data and nuanced evaluation methods. The authors introduce CoSER, a comprehensive framework including a high-quality dataset of 17,966 characters from 771 renowned books, open models (CoSER 8B and 70B), and a novel evaluation protocol based on given-circumstance acting. The dataset provides authentic dialogues with rich context, character thoughts, and actions, unlike previous synthesized data. CoSER models achieve state-of-the-art performance on multiple benchmarks, surpassing GPT-4o with 75.80% accuracy on InCharacter and 93.47% on LifeChoice. The evaluation uses multi-agent simulation and penalty-based LLM judging with detailed rubrics. The approach demonstrates significant improvements in character fidelity and narrative consistency.

## Method Summary
CoSER uses supervised fine-tuning of LLaMA-3.1-8B/70B on a mix of authentic literary dialogues from 771 books (17,966 characters, 29,798 conversations) and Tulu-3 general capability data. Training samples include scenario descriptions, character profiles, motivations, and target character utterances with thoughts/actions in markup format. The framework employs dynamic instruction templates and context inclusion (50% probability for profiles/plot/motivation). Evaluation uses a two-stage Given-Circumstance Acting protocol: multi-agent simulation where characters have private thoughts/motivations, followed by penalty-based LLM judging with severity ratings and length correction (λ=1.5).

## Key Results
- CoSER-70B achieves 10.10 BLEU vs. LLaMA-3.1-70B's 4.82 (58% improvement)
- 75.80% accuracy on InCharacter benchmark, surpassing GPT-4o
- 93.47% accuracy on LifeChoice benchmark
- GCA evaluation shows 77.5% alignment with human judges vs. 68.6% for GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Authentic literary dialogues with rich context improve role-playing fidelity over synthetic data.
- Mechanism: Training on ground-truth conversations from novels provides models with realistic speech patterns, character-consistent knowledge, and contextual grounding that synthetic Q&A pairs cannot replicate. The dataset includes thoughts, actions, and speech in markup format (e.g., `[I'm nervous] (Takes a breath) Alright...`).
- Core assumption: Characters in acclaimed literature exhibit consistent, learnable behavioral patterns that transfer to role-playing tasks.
- Evidence anchors:
  - [abstract] "CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies."
  - [section] Table 2 shows CoSER-70B achieves 10.10 BLEU vs. LLaMA-3.1-70B's 4.82, a 58% improvement.
  - [corpus] Weak direct evidence; related work (HER, RMTBench) addresses different aspects of role-playing without confirming this specific mechanism.
- Break condition: If characters lack sufficient dialogue in source material, or if extracted thoughts/actions are low-quality inferences, training benefits diminish.

### Mechanism 2
- Claim: Given-circumstance acting (GCA) training enables multi-character scene recreation.
- Mechanism: Each training sample provides scenario description, character profile, motivation, and other characters' profiles as input, with the target character's utterances as output. This conditions the model on full situational context rather than isolated Q&A.
- Core assumption: Providing comprehensive context (scenario, profiles, motivations) at training time teaches models to condition responses on full dramatic circumstances.
- Evidence anchors:
  - [section] "Each training sample is derived from a conversation and one of its character c in CoSER dataset, and LLMs are trained on c's utterances Mc."
  - [section] Table 6 shows removing inner thoughts during training drops CoSER-70B from 59.06 to 56.04 average score.
  - [corpus] FURINA mentions multi-agent collaboration pipelines but doesn't validate GCA specifically.
- Break condition: If instruction templates overfit to specific phrasings, or if context exceeds context windows, GCA benefits degrade.

### Mechanism 3
- Claim: Multi-agent simulation with penalty-based judging provides more discriminative evaluation than direct LLM scoring.
- Mechanism: Evaluation has two stages: (1) Multi-agent simulation where each character is played by an RPLA with private thoughts/motivations, (2) LLM critics identify specific flaw instances per rubric rather than assigning holistic scores. Length correction (λ=1.5 per turn) reduces bias.
- Core assumption: Explicit flaw identification with severity ratings is more reliable than direct score assignment by LLM judges.
- Evidence anchors:
  - [section] "Each flaw f is assigned a severity vf from 1 (minor) to 5 (severe). The initial score... s = 100 - 5*Σf∈F vf."
  - [section] Table 5 shows GCA (DeepSeek-R1) achieves 77.5% alignment with human judges vs. 68.6% for GPT-4o.
  - [corpus] Limited validation; related benchmarks use different evaluation paradigms.
- Break condition: If LLM critics lack character knowledge, or if rubrics don't capture relevant failure modes, evaluation reliability drops.

## Foundational Learning

- Concept: **Role-Playing Language Agents (RPLAs)**
  - Why needed here: CoSER's entire contribution targets RPLA training and evaluation for established characters.
  - Quick check question: Can you explain why simulating an established character differs from generating persona-based responses?

- Concept: **Supervised Fine-Tuning with Instruction Tuning**
  - Why needed here: CoSER models are built via SFT on LLaMA-3.1 using role-playing instructions with varied formats.
  - Quick check question: How does training on character utterances (with other characters' messages as context) differ from standard instruction tuning?

- Concept: **Multi-Agent Systems and Information Asymmetry**
  - Why needed here: GCA evaluation requires multiple agents with private thoughts/motivations to simulate realistic conversations.
  - Quick check question: Why must each agent's inner thoughts be hidden from others during simulation?

## Architecture Onboarding

- Component map:
  - Dataset Curation Pipeline: Book chunking → LLM extraction (plots, conversations, character data) → Name unification → Profile generation
  - Training Data Format: ShareGPT-style with system prompts containing scenario, profiles, motivations; target outputs include thought/action/speech
  - GCA Evaluation: Multi-agent simulator (uses NSP model + environment model) → Penalty-based LLM critic (GPT-4o default) → Length-corrected scoring

- Critical path:
  1. Extract authentic conversations with thoughts/actions from books
  2. Format training samples with full context (scenario, all profiles, motivation)
  3. SFT on character utterances with Tulu-3 mix for general capabilities
  4. For evaluation: Run multi-agent simulation, then apply penalty-based judging with rubrics

- Design tradeoffs:
  - Authentic vs. synthetic data: Authentic improves fidelity but limits scale (771 books vs. PersonaHub's synthetic breadth)
  - Full context vs. efficiency: Providing all character profiles increases input length but improves consistency
  - Penalty-based vs. direct scoring: More discriminative but requires detailed rubric engineering

- Failure signatures:
  - Low BLEU/ROUGE scores: Model may be generating generic responses rather than character-specific dialogue
  - High "Storyline Consistency" penalties: Model deviates from reference character reactions
  - "Behaves like helpful AI assistant" flaws in anthropomorphism: Model not internalizing character persona

- First 3 experiments:
  1. Replicate Table 6 ablation: Train CoSER-8B without inner thoughts, compare average scores on CoSER Test
  2. Test retrieval augmentation (Figure 4): Add character experiences/conversations via FAISS+BGE-M3, measure score improvements
  3. Validate evaluation alignment: Run GCA evaluation with DeepSeek-R1 as judge, compare rankings against human evaluation in Table 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated methods be developed to generate character "inner thoughts" that capture sophisticated thinking processes better than current LLM inference?
- Basis in paper: [explicit] The authors state in the Limitations section (Page 30) that while dialogues are authentic, "thoughts... are inferred by LLMs based on limited context" and "hardly capture characters’ sophisticated thinking processes."
- Why unresolved: Current inference relies on immediate context, lacking a mechanism to model deep, latent psychological states consistent with complex character arcs over time.
- What evidence would resolve it: A comparative study showing that models trained on high-quality human-annotated thoughts or advanced cognitive modeling significantly outperform those trained on standard LLM-inferred thoughts.

### Open Question 2
- Question: How can evaluation frameworks effectively decouple "length bias" and "self-preference bias" from the assessment of role-playing quality?
- Basis in paper: [explicit] Page 30 notes that despite penalty-based scoring, "problems such as length bias persist." Additionally, Page 11 notes human evaluators showed less preference for GPT models than GPT-4o judges did, suggesting "self-preference bias."
- Why unresolved: LLM judges inherently favor outputs that resemble their own generation distribution or verbosity levels, making objective assessment difficult.
- What evidence would resolve it: The development of a judging metric that demonstrates no statistical correlation with output length or judge-actor model similarity across diverse role-playing scenarios.

### Open Question 3
- Question: To what extent does increasing the recall of data extraction from source material improve RPLA fidelity compared to increasing the volume of books?
- Basis in paper: [explicit] Page 30 states, "We have not yet addressed the issue of recall in data extraction. Our current dataset may not cover all plots, conversations and characters from the source material."
- Why unresolved: It is unclear if performance gains are better achieved by mining existing books more deeply (higher recall) or by adding more books (higher breadth) with current extraction precision.
- What evidence would resolve it: Ablation studies comparing model performance when trained on datasets with high recall (extracting all minor plots) versus datasets with standard recall but broader book coverage.

## Limitations

- Copyright restrictions prevent distribution of raw novel content, limiting dataset reproducibility
- Evaluation still relies on LLM judges despite penalty-based approach, inheriting potential biases
- The training approach combines domain-specific and general data, but optimal mixing ratios and transfer effects are not fully characterized

## Confidence

**High Confidence Claims:**
- The CoSER dataset successfully extracts and formats authentic literary dialogues with context
- CoSER models achieve state-of-the-art performance on InCharacter and LifeChoice benchmarks
- The given-circumstance acting framework provides meaningful improvements over baseline approaches

**Medium Confidence Claims:**
- The specific mechanisms (authentic data, GCA training, penalty-based judging) causally explain performance gains
- The evaluation protocol provides reliable and discriminative assessment across different character types
- The 58% BLEU improvement over LLaMA-3.1-70B reflects meaningful quality differences

**Low Confidence Claims:**
- Claims about avoiding synthetic data artifacts are difficult to verify without extensive ablation studies
- The scalability of the approach to domains beyond literary fiction (e.g., anime, gaming)
- The long-term stability of character simulation abilities after fine-tuning

## Next Checks

1. **Dataset Reproducibility Test**: Attempt to reproduce the CoSER dataset extraction pipeline using publicly available book texts (e.g., Project Gutenberg) and validate the quality of extracted conversations and character profiles against the authors' reported statistics.

2. **Evaluation Protocol Validation**: Conduct human evaluation studies comparing the penalty-based LLM judging approach against direct scoring methods across multiple judge models (GPT-4o, DeepSeek-R1, Claude) to verify the claimed 77.5% vs 68.6% alignment advantage.

3. **Ablation Study Extension**: Beyond the reported ablations, test the impact of varying the CoSER-to-Tulu-3 data ratio during training, and measure whether the character simulation capabilities degrade when trained solely on the literary dataset without general capability mixing.