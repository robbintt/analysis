---
ver: rpa2
title: Tensorization of neural networks for improved privacy and interpretability
arxiv_id: '2501.06300'
source_url: https://arxiv.org/abs/2501.06300
tags:
- arxiv
- tt-rss
- http
- which
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Tensorization of neural networks for improved privacy and interpretability

## Quick Facts
- **arXiv ID:** 2501.06300
- **Source URL:** https://arxiv.org/abs/2501.06300
- **Reference count:** 40
- **Primary result:** Tensor Train-based tensorization (TT-RSS) improves neural network privacy through parameter obfuscation and enables interpretability via local tensor analysis.

## Executive Summary
This paper introduces TT-RSS, a method for tensorizing neural networks into Tensor Train (TT) format using black-box access and pivot samples. The approach aims to simultaneously improve privacy (by obfuscating parameters that encode training data patterns), enable interpretability (by revealing structural properties through local tensor analysis), and achieve compression (by avoiding the computational overhead of layer-wise tensorization). The method combines sketching techniques with cross interpolation to construct TT approximations from function evaluations on selected pivot points, and demonstrates these benefits through experiments on voice classification, condensed matter physics, and general function approximation.

## Method Summary
TT-RSS (Tensor Train via Recursive Sketching from Samples) tensorizes a pre-trained neural network by first selecting N pivot samples from the training distribution, then building left and right sketches through orthogonal projections onto the pivot subspace. The algorithm evaluates the neural network on carefully constructed domain slices to populate sketching tensors, applies random orthogonal matrices, and uses SVD trimming to determine TT ranks. Finally, it solves Core Determining Equations via least squares to obtain the TT cores. The method supports both discrete and continuous input spaces through different embedding choices, and can be followed by re-training or parameter obfuscation via gauge transformations to further enhance privacy.

## Key Results
- **Privacy:** Private-TT obfuscation reduces white-box attack accuracy on accent-based property inference from 98% to near-random guessing while maintaining black-box performance.
- **Interpretability:** TT-RSS correctly identifies the non-trivial SPT phase of the AKLT state from local tensor analysis, demonstrating access to topological order parameters.
- **Compression:** TT-RSS achieves better memory-time trade-offs than layer-wise tensorization for certain architectures, avoiding the computational overhead of sequential layer processing.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neural networks trained via gradient descent can encode training data patterns into their parameters, creating privacy vulnerabilities; tensorization with gauge transformations can obfuscate these patterns while preserving model behavior.
- **Mechanism:** Gradient descent optimization introduces parameter distributions biased by training data features (even irrelevant ones). TT-RSS converts the network into a Tensor Train, then applies random orthogonal matrices between contractions (Private-TT), exploiting the well-characterized gauge freedom of tensor networks to redefine parameters independently of the learning process.
- **Core assumption:** The privacy leakage stems from parameter patterns rather than model outputs; black-box behavior preservation is sufficient for the application.
- **Evidence anchors:**
  - [abstract] "obfuscate neural networks whose parameters encode patterns tied to the training data distribution"
  - [Section 4.1] Demonstrates accent-based attacks on voice classifiers and shows Private-TT reduces white-box attack accuracy to random guessing
  - [corpus] Related work on tensor networks for differential privacy exists but with limited citations; mechanism remains empirically demonstrated rather than formally proven
- **Break condition:** If the target model's privacy leakage is primarily through output patterns rather than parameters, gauge transformation alone is insufficient; if exact black-box equivalence is required, random obfuscation may not preserve all corner cases.

### Mechanism 2
- **Claim:** Tensor Train representations provide direct access to structural properties (marginal/conditional distributions, order parameters) that are accessible through local tensor analysis.
- **Mechanism:** TT-RSS decomposes the neural network function into a product of local tensors connected by small-rank bonds. For the AKLT state demonstration, this allows computation of order parameters via local tensor contractions (e.g., measuring whether projective representations commute or anti-commute), revealing symmetry-protected topological phases without needing to train additional property-prediction models.
- **Core assumption:** The target function admits a good low-rank TT approximation; the property of interest is encoded in local tensor structure.
- **Evidence anchors:**
  - [abstract] "estimate topological phases of matter that are easily accessible from the MPS representation"
  - [Section 4.2] Reconstructs AKLT state from 12 pivots with fidelity 1, correctly identifies non-trivial SPT phase
  - [corpus] Weak external validation; interpretability claim rests on condensed matter physics conventions
- **Break condition:** Functions with high TT-rank structure or properties requiring non-local correlations may not yield interpretable local tensor signatures; accuracy depends heavily on pivot selection.

### Mechanism 3
- **Claim:** TT-RSS provides a better memory-time efficiency trade-off than layer-wise tensorization for model compression.
- **Mechanism:** Layer-wise tensorization (TTM) converts each weight matrix independently, reducing parameters but often increasing inference operations due to intermediate vector reconstructions. TT-RSS produces a single TT over the entire input-output function, avoiding sequential layer overhead. The algorithm uses O(N²dn) function evaluations (polynomial in dimension n) with a single left-to-right sweep.
- **Core assumption:** The compressed model can tolerate accuracy drops (recoverable via re-training); input vectors are processed without pre-tensorization.
- **Evidence anchors:**
  - [Section 5.2] Shows TT-RSS can reduce operations while layer-wise TTM sometimes increases them (BERT layer example: 3,850 parameters but 2.8M operations vs 2.4M original)
  - [Section 2.4] Complexity analysis: O(N³dn) dominant term, linear in n
  - [corpus] Limited comparative evidence; claim based on constructed examples
- **Break condition:** For architectures where layer-wise structure is critical to accuracy (e.g., deep convolutions with spatial locality), single-TT decomposition may lose too much expressivity; accuracy recovery requires additional fine-tuning.

## Foundational Learning

- **Concept:** Tensor Trains / Matrix Product States (MPS)
  - **Why needed here:** Core representation; entire paper assumes familiarity with TT format f(x₁,...,xₙ) = G₁(x₁)G₂(x₁,x₂)...Gₙ(xₙ) and rank/bond dimension concepts.
  - **Quick check question:** Can you explain why a low-rank TT approximation is efficient for functions with limited entanglement/correlation structure?

- **Concept:** Sketching and Cross Interpolation
  - **Why needed here:** TT-RSS combines both; understanding randomized projections for range finding and maximum-volume pivot selection clarifies why training samples work as effective pivots.
  - **Quick check question:** Why might random Gaussian sketches be infeasible for high-dimensional tensors, and how does the paper's orthogonal projection onto training samples address this?

- **Concept:** Gauge Freedom in Tensor Networks
  - **Why needed here:** Central to privacy mechanism; the ability to insert W and W† between contractions without changing outputs enables parameter obfuscation.
  - **Quick check question:** Given a TT representation, can you describe one gauge transformation and prove it preserves the overall function?

## Architecture Onboarding

- **Component map:** Input NN -> TT-RSS (SketchBuilding -> Sketching -> Trimming -> SystemForming -> Solving) -> Output TT model -> (Optional) Re-training -> (Optional) Private-TT obfuscation

- **Critical path:**
  1. Pivot selection from training data (Section 2.1: N must be large enough to capture domain, typically 30-200 in experiments)
  2. Embedding choice matching data type (trivial for discrete, polynomial for continuous)
  3. SVD truncation in Trimming (cumulative percentage c and max rank r control expressivity vs. overfitting)
  4. Least-squares solve for cores (Eq. 2.16)

- **Design tradeoffs:**
  - N (pivots) ↑: Better accuracy, higher variance reduction, cubic time cost
  - r (rank) ↑: More expressivity, potential error accumulation, privacy may degrade
  - d (embedding dim) ↑: Polynomial degree ↑, but least-squares conditioning worsens
  - Re-training epochs: Recovers accuracy but may re-introduce some parameter patterns

- **Failure signatures:**
  - Test error ≫ pivot error: Overfitting to pivots; increase N or reduce rank
  - TT-CI outperforms TT-RSS on smooth functions: Pivots may not capture full domain; consider uniform sampling instead of training samples
  - Order parameter wrong despite high fidelity: Canonical form gauge freedom interfering; check unitary cancellation in Eq. 4.12

- **First 3 experiments:**
  1. **Reproduction check:** Run TT-RSS on a known random TT (n=100, r=10, d=2) with N=35 pivots; verify fidelity reaches 1.0. Time should be ~4s.
  2. **Privacy sanity test:** Train a small NN on imbalanced data (e.g., CommonVoice with 90/10 accent split), tensorize with N=100, d=2, r=5, then train a logistic regression attack on parameters. Compare NN vs. TT vs. Private-TT attack accuracy.
  3. **Compression baseline:** For a 2-layer classifier (500 hidden units), compare parameter count and operation count after TT-RSS (r=5) vs. layer-wise TTM (r=5). Verify TT-RSS doesn't increase operations.

## Open Questions the Paper Calls Out

- **Open Question 1:** What formal properties must the set of selected pivots satisfy to guarantee accurate Tensor Train approximation in high-dimensional settings?
  - **Basis in paper:** [explicit] Page 3: "we do not formally establish properties that the selected pivots must satisfy to yield good results (this is, however, an important question that we leave for future work)".
  - **Why unresolved:** The current algorithm relies on heuristic selection (random points or training data), lacking theoretical guarantees on convergence for arbitrary sparse functions.
  - **What evidence would resolve it:** A mathematical proof establishing necessary and sufficient conditions for the pivot set to ensure bounded error rates in the resulting tensor train.

- **Open Question 2:** Can the TT-RSS methodology be extended to efficiently construct higher-dimensional tensor networks, such as Projected Entangled Pair States (PEPS)?
  - **Basis in paper:** [explicit] Page 25: "If TT-RSS could be extended to such higher-dimensional scenarios, this methodology could be used to construct PEPS while avoiding their optimization."
  - **Why unresolved:** The current implementation relies on 1D Tensor Train (MPS) structures, and adapting the recursive sketching and cross-interpolation subroutines to 2D or hierarchical layouts remains undefined.
  - **What evidence would resolve it:** A successful adaptation of the algorithm applied to 2D image data or quantum states, demonstrating computational efficiency superior to standard PEPS optimization.

- **Open Question 3:** Are the information leakages observable via black-box and white-box access to tensorized models equivalent or complementary?
  - **Basis in paper:** [explicit] Page 28: "it remains unclear whether the information in the black-box and white-box representations is equivalent or complementary."
  - **Why unresolved:** Experiments showed similar trends in attack accuracy for both access types on tensorized models, but the potential for a combined attack leveraging both data sources was not investigated.
  - **What evidence would resolve it:** A privacy attack utilizing both black-box outputs and white-box parameters that achieves significantly higher success rates than attacks limited to a single access mode.

## Limitations

- The privacy mechanism relies on parameter obfuscation rather than output-based defenses, making its effectiveness highly dependent on the specific attack model used.
- The interpretability claims, while grounded in established tensor network theory, lack extensive empirical validation beyond the AKLT state example.
- The compression efficiency comparison is based on limited architectural diversity and may not generalize to modern deep learning models with complex spatial structures.

## Confidence

- **Privacy mechanism (High):** The gauge transformation approach is theoretically sound and empirically validated through accent-based attack experiments, though formal privacy guarantees remain unproven.
- **Interpretability claims (Medium):** While the tensor network foundations are well-established in physics, the specific application to neural network interpretation lacks broader validation beyond the single AKLT example.
- **Compression efficiency (Medium):** The comparative analysis shows promise but is based on limited architectural diversity; layer-wise TTM results suggest the efficiency claims may not universally hold.

## Next Checks

1. **Formal privacy analysis:** Derive theoretical bounds on privacy leakage reduction through gauge transformations, comparing against differential privacy guarantees for the same data distributions.

2. **Interpretability breadth test:** Apply TT-RSS to neural networks trained on multiple condensed matter physics problems beyond AKLT (e.g., topological insulators, spin liquids) and verify whether local tensor signatures consistently predict phase transitions.

3. **Compression scalability study:** Evaluate TT-RSS compression across a broader range of architectures including convolutional networks, transformers, and recurrent models to identify which architectural patterns benefit most from single-TT decomposition versus layer-wise approaches.