---
ver: rpa2
title: On the transferability of Sparse Autoencoders for interpreting compressed models
arxiv_id: '2507.15977'
source_url: https://arxiv.org/abs/2507.15977
tags:
- loss
- trained
- pretrained
- reconstruction
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether Sparse Autoencoders (SAEs) trained
  on an uncompressed language model can effectively interpret its compressed (pruned)
  variant. The core method idea is to apply the same pruning to the original SAE,
  and compare its performance to a newly trained SAE on the pruned model.
---

# On the transferability of Sparse Autoencoders for interpreting compressed models

## Quick Facts
- **arXiv ID**: 2507.15977
- **Source URL**: https://arxiv.org/abs/2507.15977
- **Reference count**: 40
- **Key outcome**: Pruning a pretrained Sparse Autoencoder achieves reconstruction and interpretability performance comparable to or better than training a new SAE on a pruned model.

## Executive Summary
This paper investigates whether Sparse Autoencoders trained on uncompressed language models can effectively interpret their pruned (compressed) variants. The core finding is that simply pruning the original SAE to match the model's sparsity preserves interpretability while avoiding the computational cost of retraining. Across GPT-2 Small and Gemma-2-2B, pruned SAEs achieve comparable or superior performance in reconstruction loss, feature absorption reduction, and semantic disentanglement compared to SAEs trained from scratch on pruned activations.

## Method Summary
The study applies WANDA pruning to reduce model sparsity by 50% across attention and MLP weights. SAEs are then evaluated in three variants: the original pretrained SAE, a newly trained SAE on pruned model activations, and the original SAE pruned to match the model's sparsity. The pruning process involves applying weight pruning to the SAE's encoder and decoder using the same sparsity levels as the model. Performance is measured using SAEBench metrics including reconstruction loss, KL divergence, feature absorption, and RAVEL disentanglement across multiple activation sites.

## Key Results
- SAE pruning up to 90% sparsity results in only slight increases in reconstruction loss for MLP and residual stream outputs
- Pruned SAEs often outperform trained SAEs on KL divergence and RAVEL disentanglement metrics
- Feature absorption scores are lower for pruned SAEs compared to trained SAEs in MLP outputs
- The approach achieves computational efficiency by avoiding SAE retraining while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1: Feature Alignment Preservation Through Coordinated Pruning
When both the model and SAE are pruned using similar criteria, the surviving features maintain semantic alignment. WANDA pruning removes weights based on importance scores combining magnitude and activation frequency, preserving the correspondence between model dimensions and SAE latents.

### Mechanism 2: SAE Latent Redundancy Enables Aggressive Pruning
Pretrained SAEs contain substantial redundancy in their latent representations, allowing 75-90% pruning with minimal reconstruction loss. The original SAE's overcomplete latent space can be aggressively pruned while preserving essential features.

### Mechanism 3: Pruning as Implicit Disentanglement Regularizer
Pruning the SAE can improve semantic disentanglement by removing latents that encode spurious correlations or absorbed features. This acts as implicit regularization, leaving cleaner feature separation.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)**
  - **Why needed here**: SAEs decompose LLM activations into sparse, interpretable features. Understanding their encoder-decoder structure is essential to see why pruning weights affects reconstruction.
  - **Quick check question**: Can you explain why the L1 penalty in SAE training encourages sparsity, and how this relates to the "overcomplete dictionary" concept?

- **Concept: WANDA Pruning Importance Score**
  - **Why needed here**: WANDA's importance metric (|Wij| × ||Xj||2) determines which weights survive. Understanding this helps predict which features remain after compression.
  - **Quick check question**: Why does WANDA multiply weight magnitude by activation norm rather than using magnitude alone?

- **Concept: Feature Absorption in SAEs**
  - **Why needed here**: Absorption is a key failure mode where hierarchical concepts get fragmented. The paper shows pruning can reduce absorption, but you need to understand what absorption is to interpret this result.
  - **Quick check question**: Given concepts A ⊂ B (e.g., "India" ⊂ "Asia"), how does absorption cause the SAE to fail on some instances of B?

## Architecture Onboarding

- **Component map**: Original Model → [WANDA Pruning] → Pruned Model (50% sparsity) → Original SAE (pretrained) → [Weight Pruning] → Pruned SAE (25-50% sparsity)

- **Critical path**:
  1. Load pretrained SAE matching your model
  2. Apply WANDA to model using calibration data
  3. Apply identical/similar sparsity to SAE encoder/decoder weights
  4. Evaluate on pruned model activations using SAEBench

- **Design tradeoffs**:
  - Pruning level: 25% sparsity balances reconstruction and interpretability; 50%+ risks feature loss
  - Activation site: MLP output most sensitive; residual stream most robust
  - Direct transfer vs. pruning: Direct transfer works with slight degradation; pruning often improves interpretability metrics

- **Failure signatures**:
  - Reconstruction loss spikes >20% → pruning too aggressive for that layer
  - Absorption scores increase → pruning removed disentangling latents
  - KL divergence >1.0 → SAE no longer captures model behavior

- **First 3 experiments**:
  1. Sparsity sweep: Test Pruned SAE at 25%, 50%, 75% on single layer; plot reconstruction loss to find knee point
  2. Direct transfer baseline: Apply original SAE to pruned model without SAE pruning; measure KL divergence gap vs. pruned SAE
  3. Single metric deep-dive: Run RAVEL on one entity-attribute pair across all four SAE variants to validate disentanglement claims

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do the transferability and performance benefits of pruned SAEs generalize to models compressed via quantization or low-rank decomposition?
- **Basis in paper**: The conclusion explicitly states: "Future work may explore the generalization of these findings to quantized and low-rank compressed models..."
- **Why unresolved**: This study exclusively focused on pruning; quantization alters weight precision while low-rank methods change dimensionality, potentially impacting the activation basis differently.
- **What evidence would resolve it**: Evaluating pruned SAEs on models compressed via AWQ (quantization) or low-rank factorization, comparing performance against retrained SAEs using SAEBench metrics.

### Open Question 2
- **Question**: How does pruning function as an implicit regularizer for Sparse Autoencoders, and does it systematically remove specific types of noise or features?
- **Basis in paper**: The conclusion suggests a need to "...further investigate the role of pruning as an implicit regularizer in interpretability systems."
- **Why unresolved**: The paper observes that pruned SAEs occasionally outperform trained SAEs on specific metrics, implying noise reduction, but provides no mechanistic explanation of what is being removed.
- **What evidence would resolve it**: An ablation study analyzing the specific latents removed during SAE pruning to see if they correspond to high-frequency, low-importance features or polysemantic noise.

## Limitations
- The transferability results depend critically on WANDA pruning's specific importance scoring mechanism, which may not generalize to other pruning methods
- The study is based on only two models (GPT-2 Small and Gemma-2-2B), limiting generalizability across architectures and scales
- SAE training hyperparameters (learning rate, L1 coefficient, training steps) are not specified, making exact reproduction difficult

## Confidence

**High Confidence**: The core finding that SAE pruning preserves reconstruction loss across sparsity levels (25-90%) is well-supported by quantitative evidence in Figure 1 and Table 1. The KL divergence results showing pruned SAEs often outperform trained SAEs on residual stream and MLP outputs are robust.

**Medium Confidence**: The interpretability improvements (lower absorption, higher disentanglement) are compelling but may be sensitive to the specific RAVEL evaluation setup and the particular attribute-entity pairs chosen. The claim that pruning acts as implicit regularization needs more theoretical justification.

**Low Confidence**: The generalizability claim to other pruning methods (structured vs. unstructured, different importance metrics) is speculative without empirical validation. The explanation that feature alignment preservation through coordinated pruning is the primary mechanism lacks direct evidence.

## Next Checks

1. **Cross-Architecture Validation**: Test the pruning approach on a third architecture (e.g., LLaMA or Mistral) to verify that the reconstruction and interpretability benefits extend beyond GPT-2 and Gemma-2-2B.

2. **Alternative Pruning Methods**: Apply the same analysis using magnitude-only pruning (without activation norm) and iterative pruning to determine whether WANDA's specific importance scoring is essential to the transferability results.

3. **Ablation on SAE Redundancy**: Train an SAE on the uncompressed model with strong L1 regularization (minimal redundancy) and test whether pruning still provides benefits, or if the effect depends on having redundant latents to remove.