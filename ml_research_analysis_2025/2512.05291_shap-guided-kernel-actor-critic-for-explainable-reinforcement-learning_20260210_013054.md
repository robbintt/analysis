---
ver: rpa2
title: SHAP-Guided Kernel Actor-Critic for Explainable Reinforcement Learning
arxiv_id: '2512.05291'
source_url: https://arxiv.org/abs/2512.05291
tags:
- contact
- neff
- kernel
- value
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge in actor-critic
  reinforcement learning by proposing an attribution-aware kernel-based algorithm
  called RSA2C. The core method integrates state attribution computed via RKHS-SHAP
  (kernel mean embedding and conditional mean embedding) into the learning process,
  using these attributions to modulate policy gradients and advantage function targets
  through Mahalanobis-weighted kernels.
---

# SHAP-Guided Kernel Actor-Critic for Explainable Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.05291
- Source URL: https://arxiv.org/abs/2512.05291
- Authors: Na Li; Hangguan Shan; Wei Ni; Wenjie Zhang; Xinyu Li
- Reference count: 40
- Primary result: RSA2C achieves stable and efficient training across three continuous control environments while providing intrinsic interpretability through state attribution visualization, with up to 49.2% higher returns than baselines.

## Executive Summary
This paper addresses the interpretability challenge in actor-critic reinforcement learning by proposing an attribution-aware kernel-based algorithm called RSA2C. The core method integrates state attribution computed via RKHS-SHAP (kernel mean embedding and conditional mean embedding) into the learning process, using these attributions to modulate policy gradients and advantage function targets through Mahalanobis-weighted kernels. RSA2C consists of three RKHS-enhanced components: an actor in a vector-valued RKHS, an advantage critic, and a value critic, all maintained via sparse dictionaries for computational efficiency.

The primary results demonstrate that RSA2C achieves stable and efficient training across three continuous control environments while providing intrinsic interpretability through state attribution visualization. Theoretical contributions include a global non-asymptotic convergence bound under state perturbations, decomposing the learning gap into attribution-induced and two-timescale convergence terms. Empirically, RSA2C improves performance over baselines (up to 49.2% higher returns) with only modest computational overhead, and the CME variant shows superior stability under noisy state perturbations compared to KME.

## Method Summary
RSA2C is a two-timescale kernel actor-critic that integrates state attribution into the policy learning process. The Actor uses a Mahalanobis-weighted operator-valued kernel (OVK), while Value and Advantage Critics use scalar RKHSs. SHAP values are computed from the Value Critic via kernel mean embeddings (KME) or conditional mean embeddings (CME). Dictionaries are maintained via approximate linear dependence (ALD) sparsification. The algorithm updates the Value Critic on a fast timescale to track the moving target induced by the evolving Actor, which updates on a slow timescale.

## Key Results
- RSA2C achieves 49.2% higher returns compared to baselines on continuous control tasks
- RSA2C-CME demonstrates superior robustness to state noise compared to RSA2C-KME
- Theoretical convergence bounds decompose learning gaps into attribution-induced and two-timescale convergence terms

## Why This Works (Mechanism)

### Mechanism 1: Attribution-Weighted Gradient Modulation
Integrating state feature importance into the policy kernel improves learning efficiency by prioritizing gradient updates in directions that maximally influence value. The Value Critic estimates V(s), from which RKHS-SHAP scores φ_i are derived. These scores form a diagonal weight matrix W that defines a Mahalanobis distance in the Operator-Valued Kernel (OVK), re-scaling the Actor's functional gradient.

### Mechanism 2: Conditional Mean Embedding for Noise Robustness
Using Conditional Mean Embeddings (CME) for SHAP calculation provides superior stability under state perturbations compared to marginal Kernel Mean Embeddings (KME) by explicitly modeling feature correlations. RSA2C-CME computes off-manifold expectations by conditioning on observed state coordinates, respecting underlying data geometry and distinguishing between meaningful state variations and noise injection.

### Mechanism 3: Two-Timescale Convergence with Dictionary Control
The algorithm maintains global convergence guarantees despite non-stationary function approximation by using a two-timescale update rule and bounded sparse dictionaries. The Critic updates on a fast timescale to track the moving target induced by the evolving Actor, which updates on a slow timescale. ALD sparsification bounds function complexity, preventing the "drift" of the value estimator from destabilizing the policy gradient.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS) & Operator-Valued Kernels (OVK)**: The core representation of both the Critic and Actor relies on RKHS. Understanding OVKs is essential because the Actor outputs a vector (action), requiring a kernel that maps states to vector-valued functions.
  - Quick check: Can you explain why a scalar kernel is insufficient for the Actor but suitable for the Value Critic?

- **Shapley Values & KernelSHAP**: The paper adapts Shapley values (specifically RKHS-SHAP) as the mechanism to extract feature importance. Without this, the "attribution-aware" aspect of the loop cannot be implemented.
  - Quick check: How does RKHS-SHAP differ from standard KernelSHAP in terms of computational complexity and assumptions about the model?

- **Two-Timescale Stochastic Approximation**: This is the mathematical framework proving the stability of the algorithm. It dictates how the learning rates for the Actor and Critic must relate to ensure convergence.
  - Quick check: Why must the Critic's step size decay slower (or be larger) than the Actor's in a two-timescale setup?

## Architecture Onboarding

- **Component map**: Value Critic -> SHAP Module -> Actor (with Mahalanobis kernel) -> Advantage Critic -> ALD Sparsifier
- **Critical path**: 1) Sample transition (s, a, r, s'). 2) Update Value Critic coefficients using TD error (Fast timescale). 3) Compute SHAP using Value Critic to calculate feature importance φ. 4) Update Kernel: Re-compute Mahalanobis weights W in Actor's kernel using φ. 5) Update Actor coefficients using policy gradient (Slow timescale). 6) Sparsify dictionaries if full.
- **Design tradeoffs**: RSA2C-KME vs RSA2C-CME: KME is computationally lighter and converges faster in simple tasks; CME is heavier but significantly more robust to state noise (correlations). Dictionary Size (q): Larger q improves approximation capacity but increases runtime linearly/quadratically. Too small q caps performance.
- **Failure signatures**: Performance Plateau: If returns stabilize prematurely at a low value, the RKHS representation capacity or dictionary size is likely insufficient. Divergence under Noise: If returns crash when noise is injected, verify you are using the CME variant, not KME.
- **First 3 experiments**: 1) Pendulum-v1 Ablation: Run RSA2C-CME vs RSA2C-KME vs "Advanced AC" (no SHAP). Visualize beeswarm plots to confirm SHAP scores align with physical intuition. 2) Noise Stability Test: Inject zero-mean Gaussian noise (variance 0.01) into states on Pendulum-v1. Verify RSA2C-CME maintains stable variance compared to RSA2C-KME. 3) Dictionary Sensitivity: On BipedalWalker-v3, sweep dictionary sizes (q=128, 512, 1024) to identify when approximation error ζ_a stops dominating the return.

## Open Questions the Paper Calls Out

- **High-Dimensional Extensions**: Can the RSA2C framework maintain its stability and interpretability guarantees when extended to high-dimensional or pixel-based observations using scalable kernel approximations like Random Fourier Features (RFFs)?
- **Deep RL Integration**: Can kernel-based attributions be effectively integrated into deep RL architectures to improve their stability and expressiveness without violating compatible function approximation assumptions?
- **Product Kernel Assumptions**: To what extent does the reliance on the "on-manifold" (CME) vs. "off-manifold" (KME) expectation assumption affect convergence when the state distribution violates the product kernel independence assumptions?

## Limitations

- The reliance on the Value Critic for SHAP computation creates a chicken-and-egg problem: early in training, when the critic is least accurate, the SHAP scores may be misleading, potentially delaying convergence.
- Confidence in the interpretability claims is Medium as visualizations are provided but not formally validated against ground truth attributions.
- The computational efficiency claim relative to standard AC methods is Low as only FLOPs are reported, not actual wall-clock comparisons.

## Confidence

- **High**: Performance improvements and noise robustness (backed by statistical significance in Table 2)
- **Medium**: Interpretability claims (visualizations provided but not formally validated)
- **Low**: Computational efficiency claim relative to standard AC methods (only FLOPs reported)

## Next Checks

1. **Temporal SHAP Stability**: Track how SHAP scores evolve during training and correlate them with policy improvement to validate the mechanism beyond final-state visualization.

2. **Baselines Under Noise**: Test whether Advanced AC or RKHS-AC can achieve similar noise robustness by simply increasing network capacity or regularization, isolating the attribution mechanism's specific contribution.

3. **Dictionary Size Scaling**: Systematically vary dictionary sizes on all three environments to identify when approximation error dominates and determine if the current settings are near-optimal or overly conservative.