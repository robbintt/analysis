---
ver: rpa2
title: 'When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional
  Universality and Rank Learning'
arxiv_id: '2512.21486'
source_url: https://arxiv.org/abs/2512.21486
tags:
- tensor
- data
- rank
- rr-fbtc
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tensor completion for data
  with real-valued (continuous) indices, where traditional discrete tensor methods
  struggle. The authors propose a rank-revealing functional Bayesian tensor completion
  (RR-FBTC) method that leverages multioutput Gaussian processes (MOGPs) to model
  continuous latent functions.
---

# When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning

## Quick Facts
- **arXiv ID:** 2512.21486
- **Source URL:** https://arxiv.org/abs/2512.21486
- **Reference count:** 40
- **Key outcome:** Rank-revealing functional Bayesian tensor completion (RR-FBTC) method that automatically learns tensor rank during inference for continuous indices using multioutput Gaussian processes.

## Executive Summary
This paper addresses tensor completion for data with real-valued (continuous) indices, where traditional discrete tensor methods struggle. The authors propose RR-FBTC, which leverages multioutput Gaussian processes (MOGPs) to model continuous latent functions and automatically learns the tensor rank through sparsity-promoting priors. The method is evaluated on synthetic and real-world datasets, showing superior performance compared to state-of-the-art approaches in reconstruction accuracy and robustness under low signal-to-noise ratios and sparse observations.

## Method Summary
RR-FBTC replaces discrete factor matrices in standard CP decomposition with vector-valued latent functions modeled using MOGPs with kernel functions that capture correlations between real-valued indices. The method employs a Gamma prior on diagonal elements of the MOGP's row covariance matrix, allowing automatic rank learning during variational inference by pruning redundant components. The model establishes universal approximation properties for continuous multi-dimensional signals and provides uncertainty quantification and interpretable latent factors.

## Key Results
- RR-FBTC achieves superior reconstruction accuracy (RMSE, RRSE) compared to state-of-the-art methods on synthetic and real-world datasets
- The method demonstrates robustness under low signal-to-noise ratios and sparse observations
- Automatic rank learning eliminates the need for manual rank selection while maintaining high expressive power
- Theoretical analysis establishes universal approximation property for continuous multi-dimensional signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous indices can be modeled as latent functions rather than discrete vectors
- **Mechanism:** Replaces discrete factor matrices with vector-valued latent functions modeled using Multioutput Gaussian Processes (MOGPs), where kernel functions capture correlation between real-valued indices
- **Core assumption:** Underlying data structure varies smoothly across real-valued indices
- **Evidence anchors:** [abstract] "modeling the latent functions through carefully designed multioutput Gaussian processes"; [section III.A] Eq. (14) defines prior as $U_k(\cdot) \sim MGP(0, \varsigma_k(\cdot,\cdot), \Gamma^{-1})$
- **Break condition:** Fails with sharp discontinuities or non-stationary patterns that standard stationary kernels cannot capture

### Mechanism 2
- **Claim:** Tensor rank can be learned automatically during inference without manual tuning
- **Mechanism:** Employs Gamma prior on diagonal elements ($\gamma_r$) of MOGP's row covariance matrix; during variational inference, redundant components are pruned as posterior forces variance contribution toward zero
- **Core assumption:** Data admits low-rank representation and Gaussian-Gamma hierarchy is suitable sparsity inducer
- **Evidence anchors:** [abstract] "incorporating sparsity-promoting priors, RR-FBTC automatically learns the tensor rank"; [section III.A] Eq. (17) defines Gamma prior $p(\gamma)$; Fig. 2 illustrates pruning
- **Break condition:** Rank estimation becomes unstable under extremely low SNR (below -5 dB to -10 dB)

### Mechanism 3
- **Claim:** Model has high expressive power despite simple structure
- **Mechanism:** Theoretically proves Universal Approximation Property (UAP) using universal kernels (RBF or Matern) to approximate any continuous multi-dimensional function with arbitrary accuracy given sufficient rank
- **Core assumption:** Chosen kernel is universal and domain is compact metric space
- **Evidence anchors:** [abstract] "establish the universal approximation property... demonstrating its high expressive power"; [section III.C] Theorem 1 formalizes UAP
- **Break condition:** Theoretical guarantees require sufficiently large rank; if initial rank is too low, approximation guarantees may not hold

## Foundational Learning

- **Concept: CP Decomposition (CPD)**
  - **Why needed here:** Base structure the paper extends; standard CPD decomposes tensor into sum of rank-1 outer products, which RR-FBTC transforms into functional form
  - **Quick check question:** Can you explain how a rank-1 tensor is formed from a set of vectors?

- **Concept: Gaussian Processes (GP) & Kernels**
  - **Why needed here:** GPs model the latent functions; understanding kernels is critical as they define continuity and smoothness of solution
  - **Quick check question:** What does the length-scale parameter in an RBF or Matern kernel control regarding the function's behavior?

- **Concept: Variational Inference (VI)**
  - **Why needed here:** Exact Bayesian inference is intractable; paper uses mean-field VI to approximate posterior distributions
  - **Quick check question:** In VI, what quantity is minimized to match approximate distribution to true posterior?

## Architecture Onboarding

- **Component map:** Input Layer (real-valued indices and observations) -> Allocation Layer (constructs coordinate sets and allocates to sparse tensor) -> Latent Layer (MOGP with variational distributions and kernel matrices) -> Parameter Layer (noise precision and rank-scaling parameters)

- **Critical path:** Variational Inference loop (Algorithm 1) with closed-form updates for $q(u_k^r)$ (Eq. 31-32), $q(\gamma)$ (Eq. 33-34), and $q(\tau)$ (Eq. 35-36); convergence depends on interplay between shrinking $\gamma$ (pruning rank) and noise estimation $\tau$

- **Design tradeoffs:**
  - Initial Rank ($R_{init}$): Higher rank ensures universality but increases compute cost $O(R_{init} \cdot \max(N_k^3))$; paper suggests $R_{init}$ is robust but requires balancing max dimension vs. available memory
  - Kernel Choice: Matern 5/2 used in paper; RBF is smoother; choice impacts smoothness of recovered field

- **Failure signatures:**
  - Rank Collapse: Estimated rank drops to 0 or 1 immediately; check initialization of hyperparameters $a_r, b_r$
  - Overfitting Noise: Reconstructed field looks noisy rather than smooth; check if noise precision $\tau$ is updating correctly or if kernel length-scales are too small
  - Compute Stall: Inversion of $\Sigma_k$ (size $N_k \times N_k$) is expensive; if $N_k$ is large, requires Nyström approximation

- **First 3 experiments:**
  1. Sanity Check (Synthetic Discrete): Replicate Table II with 30×30×30 tensor, Rank 10, verify RMSE drops and estimated rank matches ground truth
  2. Functional Check (Synthetic Continuous): Replicate Eq. (41) scenario with continuous coordinates, verify interpolation of $U_1(x) = \sin^2(2\pi x)...$ functions
  3. Scalability/Real Data: Run on US-Temperature dataset (Table IV), plot learned latent factors (Fig. 6) to verify interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can scalable Gaussian process approximations (Nyström or random Fourier features) be integrated into RR-FBTC framework to reduce cubic computational complexity for large-scale datasets?
- **Basis:** [explicit] Section IV states acceleration techniques can be employed to approximate kernel matrix
- **Why unresolved:** Current algorithm relies on exact kernel matrix inversions with complexity $O(N_k^3)$, limiting applicability to moderate-sized grids
- **Evidence:** Empirical comparison of reconstruction accuracy and runtime between exact RR-FBTC and versions utilizing sparse or low-rank GP approximations on large-scale data

### Open Question 2
- **Question:** How can RR-FBTC model be modified to handle non-Gaussian observation models (Poisson or Bernoulli distributions) for count or binary rating data?
- **Basis:** [explicit] Section III-A notes Gaussian noise assumption but other noise models can be adopted with modifications to likelihood function
- **Why unresolved:** Current derivations rely on conjugate Gaussian likelihoods for closed-form updates; non-Gaussian likelihoods generally break conjugacy
- **Evidence:** Derivation of new variational update steps for non-conjugate likelihoods and validation on datasets with discrete or non-negative values

### Open Question 3
- **Question:** What are the sample complexity bounds for RR-FBTC given that UAP proves existence but not data requirements for successful inference?
- **Basis:** [inferred] Theorem 1 proves universal approximation property but paper lacks theoretical analysis on minimum samples needed
- **Why unresolved:** Paper demonstrates empirical success but does not theoretically bound estimation error as function of number of observations $N$ or noise variance $\tau^{-1}$
- **Evidence:** Theoretical analysis deriving bounds on prediction error $\|f - \hat{f}\|$ relative to sampling rate and noise level

## Limitations
- Method relies heavily on kernel-based smoothness assumptions, which may fail for data with sharp discontinuities or non-stationary patterns
- Theoretical universal approximation property requires sufficiently large initial rank, which can be computationally expensive
- Performance degrades significantly under extremely low signal-to-noise ratios (below -5 dB to -10 dB), where rank estimation becomes unstable

## Confidence
- **High Confidence:** Universal approximation property (Theorem 1) and Gamma prior mechanism for rank learning are theoretically sound
- **Medium Confidence:** Empirical performance claims are supported by synthetic and real datasets, but specific numerical results may vary depending on initialization
- **Low Confidence:** Exact convergence behavior and sensitivity to hyperparameter initialization are not fully characterized

## Next Checks
1. **Rank Stability Test:** Run synthetic discrete experiment (30×30×30 tensor, Rank 10) across 10 random seeds to verify estimated rank consistently converges to ground truth
2. **Extreme SNR Robustness:** Test method on synthetic data with SNR levels of -5 dB and -10 dB to quantify exact point where rank estimation becomes unstable
3. **Kernel Sensitivity Analysis:** Compare performance using RBF versus Matern 5/2 kernels on US-Temperature dataset to verify choice of universal kernel does not significantly impact universal approximation property in practice