---
ver: rpa2
title: 'Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages'
arxiv_id: '2505.19851'
source_url: https://arxiv.org/abs/2505.19851
tags:
- languages
- transliteration
- indicxlit
- language
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks large language models (LLMs) for transliterating
  text between Indian languages, comparing them with a specialized transliteration
  model, IndicXlit. The authors evaluated GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it,
  and Mistral-Large on datasets covering ten major Indian languages.
---

# Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages

## Quick Facts
- arXiv ID: 2505.19851
- Source URL: https://arxiv.org/abs/2505.19851
- Reference count: 39
- LLMs, particularly fine-tuned GPT-4o, outperform specialized transliteration models for Indian languages

## Executive Summary
This paper benchmarks large language models (LLMs) for transliterating text between Indian languages, comparing them with a specialized transliteration model, IndicXlit. The authors evaluated GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large on datasets covering ten major Indian languages. Results showed that GPT-4.5 generally outperformed other LLMs and IndicXlit, while fine-tuning GPT-4o further improved accuracy. Fine-tuned GPT-4o surpassed IndicXlit in 9 out of 10 languages. Error analysis revealed that LLMs produce fewer script errors than the specialized model, and robustness tests under noisy input confirmed GPT-4o fine-tuned as the most resilient model. The study highlights the effectiveness of fine-tuned LLMs for specialized tasks like transliteration.

## Method Summary
The authors conducted a comprehensive benchmarking study comparing LLMs (GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, Mistral-Large) with a specialized transliteration model (IndicXlit) on transliteration tasks between ten major Indian languages. The evaluation involved fine-tuning GPT-4o on transliteration data and testing all models on standard transliteration datasets. Performance was measured using standard metrics, and additional robustness tests were conducted under noisy input conditions. Error analysis was performed to compare script errors and other failure modes between models.

## Key Results
- GPT-4.5 generally outperformed other LLMs and the specialized IndicXlit model in transliteration accuracy
- Fine-tuned GPT-4o surpassed IndicXlit in 9 out of 10 tested Indian languages
- LLMs produced fewer script errors compared to the specialized model
- GPT-4o fine-tuned demonstrated the highest resilience under noisy input conditions

## Why This Works (Mechanism)
Large language models have demonstrated strong performance on multilingual tasks due to their extensive pretraining on diverse text corpora. For transliteration, LLMs can leverage their understanding of cross-linguistic patterns and script mappings learned during pretraining. Fine-tuning allows these models to specialize their general language capabilities for the specific task of transliteration, adapting their internal representations to better handle script conversions. The success of fine-tuned LLMs over specialized models suggests that the broad linguistic knowledge encoded in LLMs, when properly adapted, can be more effective than narrow task-specific architectures.

## Foundational Learning
- **Transliteration vs Translation**: Understanding the distinction between converting scripts while preserving meaning versus translating content between languages - needed to properly frame the task and evaluation metrics
- **Cross-lingual transfer**: Knowledge of how language models leverage pretraining across language pairs - needed to understand why LLMs perform well on multilingual tasks
- **Script systems of Indian languages**: Familiarity with the writing systems used in Indian languages (Devanagari, Bengali, Tamil, etc.) - needed to interpret script error patterns and transliteration quality
- **Fine-tuning methodologies**: Understanding how to adapt pretrained models for specialized tasks - needed to evaluate the effectiveness of the fine-tuning approach
- **Robustness testing**: Knowledge of noise injection and resilience evaluation - needed to assess real-world applicability of the models

## Architecture Onboarding
- **Component Map**: LLMs (GPT-4 variants, Gemma-3, Mistral) -> Fine-tuning pipeline -> Evaluation framework -> Error analysis
- **Critical Path**: Model input → Internal representation → Script conversion → Output generation → Evaluation
- **Design Tradeoffs**: General-purpose LLMs vs specialized transliteration models - LLMs offer broader linguistic knowledge but require more computational resources, while specialized models are efficient but lack adaptability
- **Failure Signatures**: Script errors (incorrect character mapping), context errors (wrong transliteration based on meaning), robustness failures (sensitivity to noise)
- **First 3 Experiments**: 1) Evaluate baseline LLMs without fine-tuning on transliteration task, 2) Fine-tune GPT-4o and measure improvement, 3) Compare error patterns between LLMs and specialized model

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to less-resourced Indian languages or dialectal variants
- Evaluation metrics may not fully capture semantic preservation and contextual appropriateness
- Error analysis is limited in scope and doesn't explore root causes of model-specific errors
- Computational cost and practicality of fine-tuning large models not discussed
- Robustness tests may not cover all real-world noise patterns

## Confidence
- High confidence in claim that GPT-4.5 generally outperforms other LLMs and IndicXlit
- Medium confidence in claim that fine-tuned GPT-4o surpasses IndicXlit in 9 out of 10 languages
- Medium confidence in robustness claims for GPT-4o fine-tuned under noisy input

## Next Checks
1. Evaluate models on transliteration tasks involving less-resourced Indian languages and dialectal variants to assess generalizability
2. Conduct deeper error analysis to identify root causes of model-specific errors including script and context failures
3. Test models' robustness under broader range of real-world noise patterns including mixed scripts, domain-specific jargon, and typographical errors