---
ver: rpa2
title: Preference Distillation via Value based Reinforcement Learning
arxiv_id: '2509.16965'
source_url: https://arxiv.org/abs/2509.16965
tags:
- teacher
- reward
- value
- function
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We introduce Teacher Value-based Knowledge Distillation (TVKD),\
  \ a method that improves small language model alignment by leveraging the value\
  \ function of a DPO-trained teacher model. TVKD adds a shaping term based on the\
  \ teacher\u2019s soft value function to the DPO objective, preserving the optimal\
  \ policy through potential-based reward shaping."
---

# Preference Distillation via Value based Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.16965
- Source URL: https://arxiv.org/abs/2509.16965
- Authors: Minchan Kwon; Junwon Ko; Kangil Kim; Junmo Kim
- Reference count: 40
- Key outcome: TVKD improves small language model alignment by leveraging teacher DPO value functions, outperforming strong baselines across MT-Bench, AlpacaEval, and Open LLM Leaderboard.

## Executive Summary
TVKD is a preference distillation method that transfers alignment knowledge from a DPO-trained teacher to a smaller student model by using the teacher's soft value function as a shaping reward. The key insight is that DPO-trained models implicitly encode soft Q-functions, from which state-dependent shaping terms can be extracted that preserve the optimal policy while providing fine-grained supervision. Experiments show TVKD achieves significant improvements in response quality and alignment compared to standard DPO, particularly for very small models (500M parameters), while remaining robust to hyperparameter settings.

## Method Summary
TVKD modifies the DPO loss by adding a shaping term derived from the teacher's soft value function. The teacher (frozen) computes value functions V_ϕ(s) = β log Σ_a exp(Q_ϕ(s,a)/β) at each token position, which are pre-computed offline. During training, the shaping term ψ(s,a) = V_ϕ(s') - V_ϕ(s) is calculated from stored values and integrated into the preference margin, providing state-dependent guidance without altering the optimal policy. The student is trained using this modified loss, with the teacher's value function acting as a soft label that captures long-term preference information.

## Key Results
- TVKD outperforms standard DPO and other distillation methods across multiple benchmarks including MT-Bench, AlpacaEval, and Open LLM Leaderboard
- The method is particularly effective for small models, with 500M parameter students showing significant gains over standard DPO
- State-dependent shaping (TVKD) achieves 30.23% margin accuracy compared to 18.29-18.31% for action-dependent methods
- TVKD remains robust across different teacher-student pairs and shows optimal performance at α=0.7

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Potential-based reward shaping from teacher value functions provides fine-grained guidance without altering the optimal policy of the original DPO objective.
**Mechanism:** The shaping term ψ(s,a) = V_ϕ(s') - V_ϕ(s) captures change in expected future return. When summed over a trajectory, it telescopes to Φ(s_T) - Φ(s_0), depending only on start and end states—not on specific actions. This preserves the action ranking in the Q-function, so argmax_a Q*(s,a) remains unchanged.
**Core assumption:** The teacher's value function provides a meaningful proxy for reward quality and is not severely misestimated or noisy.
**Evidence anchors:** [abstract] states the shaping term satisfies potential-based reward shaping; [section 3.2] proves optimal policy invariance under Lemma 2; related work on reward-guided distillation supports the general principle.
**Break condition:** If the teacher value function is highly inaccurate or systematically biased, the shaping term may introduce noise rather than signal, potentially degrading training dynamics even though theoretical policy invariance holds.

### Mechanism 2
**Claim:** DPO-trained models implicitly encode soft Q-functions, enabling extraction of value estimates without explicit reward model training.
**Mechanism:** Following Rafailov et al. [25], the DPO-trained policy π_ϕ(a|s) follows a Boltzmann distribution: π_ϕ(a|s) = exp(Q_ϕ(s,a)/β) / Σ_a' exp(Q_ϕ(s,a')/β). The soft value function V_ϕ(s) = β log Σ_a exp(Q_ϕ(s,a)/β) represents a "soft count" of high-quality token choices at state s, aggregating future preference information.
**Core assumption:** DPO training produces policies that satisfy the soft Bellman consistency condition for some underlying reward structure.
**Evidence anchors:** [section 3.2] provides Lemma 1 showing the soft value function formula; [section 1] cites that DPO-trained models implicitly learn soft Q-functions over token-level actions.
**Break condition:** If DPO training has not converged sufficiently or β is mis-specified, the Q-values may not accurately reflect long-term returns, making the extracted value function unreliable.

### Mechanism 3
**Claim:** State-dependent shaping (vs. action-dependent) avoids introducing conflicting gradients that would distort the preference learning objective.
**Mechanism:** Action-dependent terms like log π_ϕ(a|s) or Q_ϕ(s,a) introduce implicit preferences over actions that cannot factor out across trajectories. The cumulative distortion Σ_t ψ(s_t,a_t) varies with policy, potentially shifting action rankings. State-dependent V_ϕ(s'-) - V_ϕ(s) avoids this by evaluating state transitions independent of the specific action taken.
**Core assumption:** The dataset reward and teacher reward are not fundamentally misaligned (i.e., teacher preferences correlate with human preferences in the dataset).
**Evidence anchors:** [section 3.2] shows Corollary 2.1 proving action-based shaping breaks policy invariance; [table 5] demonstrates action-dependent rewards achieve 18.29-18.31% margin accuracy vs. 30.23% for state-dependent methods.
**Break condition:** If there's systematic conflict between dataset preferences and teacher value estimates, state-dependent shaping may still guide toward suboptimal policies even while preserving theoretical invariance.

## Foundational Learning

- **Direct Preference Optimization (DPO):**
  - Why needed here: TVKD modifies the DPO loss; understanding the baseline objective is essential for grasping what the shaping term preserves.
  - Quick check question: Can you explain why DPO eliminates the need for an explicit reward model and how it reparameterizes rewards as log-policy ratios?

- **Maximum Entropy RL and Soft Q-Learning:**
  - Why needed here: The value function extraction relies on interpreting DPO policies as solutions to a MaxEnt RL problem with soft Bellman equations.
  - Quick check question: How does the soft value function V*(s) = log Σ_a exp(Q*(s,a)) differ from standard value functions, and what role does temperature β play?

- **Potential-Based Reward Shaping:**
  - Why needed here: The theoretical guarantee that TVKD doesn't alter optimal policies rests on PBRS theory from Ng et al. [20].
  - Quick check question: Why must the shaping function be a difference of potentials Φ(s') - Φ(s) to preserve optimal policy, and what happens if it depends on actions?

## Architecture Onboarding

- **Component map:**
  Teacher DPO -> Value Function Extractor -> Shaping Term Calculator -> Modified DPO Loss -> Student Policy

- **Critical path:**
  1. Pre-compute teacher value functions over the entire dataset (requires one forward pass per token position)
  2. Store top-k logits and computed V_ϕ(s) values for efficiency
  3. During training, compute shaping terms on-the-fly from pre-stored values
  4. Backpropagate only through student log-probabilities (ψ_ϕ has zero gradient w.r.t. θ)

- **Design tradeoffs:**
  - **α (distillation strength)**: Controls teacher influence; too low underutilizes guidance, too high suppresses dataset-specific learning (optimal at α=0.7 in experiments)
  - **β (temperature)**: Affects value function sharpness; higher β yields sharper discrimination but has limited impact since method uses differences
  - **Storage vs. computation**: Pre-computing logits reduces training overhead but requires ~50x storage for top-50 probabilities per token

- **Failure signatures:**
  1. **Margin accuracy drops below ~20%**: Likely using action-dependent shaping (logits/log-prob) instead of value differences
  2. **Performance degrades with increasing α**: Teacher value function may be noisy or misaligned with dataset preferences
  3. **No improvement over DPO baseline**: Check if value function extraction is correct (should use β logsumexp, not raw logits)
  4. **Training instability**: Verify that ψ_ϕ is detached from gradient computation

- **First 3 experiments:**
  1. **Sanity check on value function quality**: Compute margin accuracy on held-out preference pairs using only the shaping term as reward signal (should exceed 25% as in Table 3)
  2. **Ablate α with fixed β=1**: Plot MT-Bench and RM scores across α ∈ {0.1, 0.2, 0.5, 0.7, 1.0, 1.5} to find optimal distillation strength for your teacher-student pair
  3. **Compare action vs. state-dependent shaping**: Implement log-probability shaping (action-dependent) vs. TVKD's value-based shaping on a small dataset to empirically verify the margin accuracy gap predicted in Table 5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is TVKD when distilling from weak, unaligned, or distributionally mismatched teacher models?
- Basis: [explicit] The authors state in the Limitations section: "We do not test cases with weak or mismatched teachers, and the method’s robustness in such settings is unclear."
- Why unresolved: All reported experiments utilize well-aligned DPO-trained teachers, leaving the failure modes of a low-quality teacher value function unexplored.
- What evidence would resolve it: Experiments using unaligned (pre-trained only) or out-of-distribution teachers to measure performance degradation.

### Open Question 2
- Question: What is the quantitative impact of estimation error or noise in the teacher value function on training dynamics?
- Basis: [explicit] The paper notes that "in practice, the shaping term can negatively affect training dynamics if the teacher’s value function is severely misestimated or noisy."
- Why unresolved: While theoretical policy invariance is proven, the practical sensitivity of the student model to variance in the teacher's soft value estimates was not analyzed.
- What evidence would resolve it: A sensitivity analysis injecting varying magnitudes of synthetic noise into the shaping term during training.

### Open Question 3
- Question: Does the performance gain of TVKD over standard DPO persist when scaling student models beyond 3B parameters?
- Basis: [inferred] The experimental scope is restricted to Small Language Models (0.5B–3B), justified by their "limited capacity," but results for larger students are absent.
- Why unresolved: As student capacity increases, the need for value-based shaping may diminish or behave differently, which is not demonstrated.
- What evidence would resolve it: Benchmarking TVKD on student models in the 7B–70B parameter range against standard DPO baselines.

## Limitations

- TVKD's effectiveness depends on the quality of the teacher's value function, and the method's robustness to weak or mismatched teachers is unclear
- The computational overhead of pre-computing and storing top-50 logits per token position may become prohibitive for very large datasets
- The method has only been validated on small language models (500M-7B parameters), leaving questions about scalability to larger models

## Confidence

- **High Confidence**: The theoretical guarantee that potential-based shaping preserves optimal policies. This follows directly from established RL theory [Ng et al., 20] and the paper's proofs in Section 3.2 are mathematically sound.
- **Medium Confidence**: The claim that DPO-trained models implicitly encode soft Q-functions suitable for value extraction. While the theoretical framework from Rafailov et al. [25] is well-established, practical quality depends on training convergence and temperature calibration.
- **Medium Confidence**: The overall performance claims across benchmarks. Results show consistent improvements over strong baselines, but evaluation uses proprietary Reward Model scores and automated benchmarks that may not fully capture qualitative differences.

## Next Checks

1. **Value Function Fidelity Test**: Compute the correlation between TVKD's shaping signals and direct human preference judgments on a held-out validation set. This would validate whether the teacher's soft value function provides meaningful guidance beyond what's captured by margin accuracy alone.

2. **Model Size Scaling Boundary**: Systematically test TVKD's effectiveness on models smaller than 500M parameters (e.g., 300M, 100M) to identify the practical lower bound where shaping signals become too noisy to provide benefits.

3. **Teacher Quality Sensitivity Analysis**: Train multiple teachers with varying DPO hyperparameters (different β values, learning rates, and preference dataset sizes) and measure how TVKD performance varies with teacher quality. This would reveal whether the method is robust to teacher model imperfections or requires high-quality alignment as a prerequisite.