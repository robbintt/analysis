---
ver: rpa2
title: Exploring the Relationship between Brain Hemisphere States and Frequency Bands
  through Deep Learning Optimization Techniques
arxiv_id: '2509.14078'
source_url: https://arxiv.org/abs/2509.14078
tags:
- cube
- mona
- lisa
- necker
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the relationship between brain hemisphere\
  \ states and EEG frequency bands using deep learning optimization techniques. Three\
  \ neural network architectures (deep dense, shallow, and CNN) are evaluated across\
  \ five EEG frequency bands (\u03B4, \u03B8, \u03B1, \u03B2, \u03B3) using eight\
  \ optimizers (RMSprop, Adam, Nadam, AdaMax, SGD, Adagrad, Adadelta, FTRL)."
---

# Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques

## Quick Facts
- **arXiv ID**: 2509.14078
- **Source URL**: https://arxiv.org/abs/2509.14078
- **Reference count**: 20
- **Primary result**: Adagrad optimizer excels in beta band, RMSprop in gamma band, with optimizer-band coupling showing consistent performance improvements across three neural network architectures

## Executive Summary
This study investigates how different deep learning optimizers perform when classifying brain hemisphere states across EEG frequency bands. Using three neural network architectures (deep dense, shallow, and CNN) on 31-channel EEG data from 10 participants, the research systematically evaluates eight optimizers across five frequency bands (δ, θ, α, β, γ). The results reveal that optimizer performance is highly frequency-dependent, with Adagrad excelling in beta-band classification and RMSprop achieving superior performance in gamma-band analysis. The study also demonstrates the interpretability benefits of SHAP plots for understanding model decisions in neuroimaging tasks.

## Method Summary
The study uses 31-channel EEG data from 10 participants viewing visual stimuli, filtered into five frequency bands (δ: 1-4 Hz, θ: 5-8 Hz, α: 9-12 Hz, β: 13-30 Hz, γ: 31-45 Hz). Three architectures are tested: Big Model (9-layer dense with batch normalization), Small Model (3-layer dense), and CNN (3 Conv1d layers with pooling). Eight optimizers (RMSprop, Adam, Nadam, AdaMax, SGD, Adagrad, Adadelta, FTRL) are systematically evaluated across all frequency bands using 70/15/15 train/val/test splits. Performance is measured using ROC-AUC, F1-score, precision, recall, and SHAP interpretability analysis.

## Key Results
- Adagrad optimizer achieves highest ROC-AUC (0.87-0.92) for beta band classification across all architectures
- RMSprop demonstrates superior performance (0.88-0.92 ROC-AUC) specifically for gamma band analysis
- Adadelta shows robust cross-model performance, particularly in the Big Model architecture
- CNN architecture achieves second-highest overall accuracy, demonstrating strong spatial feature extraction capabilities
- SHAP plots provide interpretable feature importance, showing distinct patterns across frequency bands

## Why This Works (Mechanism)

### Mechanism 1
Adaptive gradient-based optimizers (Adagrad, RMSprop) outperform non-adaptive methods for EEG frequency-band-specific classification due to their ability to handle varying gradient scales across frequency-related features. Adagrad's per-parameter learning rate adaptation helps when β-band features have different gradient magnitudes than δ-band, while RMSprop's moving average of squared gradients handles γ-band's higher-frequency oscillations more stably.

### Mechanism 2
CNN architectures capture spatial relationships across EEG channels more effectively than dense networks by learning local spatial patterns through 1D convolutions. Max pooling after each conv layer reduces dimensionality while preserving dominant patterns, allowing the model to identify discriminative spatial configurations between left and right hemisphere electrode groupings.

### Mechanism 3
Batch normalization stabilizes training in deep dense networks by reducing internal covariate shift, enabling higher learning rates and faster convergence. BN normalizes activations per channel after each dense layer, keeping activation distributions stable across layers despite weight updates, which is crucial for the 9-layer Big Model architecture.

## Foundational Learning

- **EEG frequency bands and neurophysiological correlates**: Understanding what each band represents (δ: sleep/motivation; θ: cognition; α: relaxation; β: sensorimotor; γ: consciousness) explains why different optimizers might excel on different bands. Quick check: Which frequency band would you expect to be most discriminative for distinguishing active cognitive processing from relaxed states?

- **Adaptive vs. non-adaptive gradient optimizers**: The paper's main finding is optimizer-band coupling. Understanding why Adagrad (per-parameter learning rates that decay) differs from SGD (fixed learning rate) and RMSprop (exponential moving average of gradients) is essential. Quick check: If a feature's gradient is consistently large across training, how would Adagrad vs. RMSprop differ in their learning rate for that feature?

- **SHAP (Shapley Additive Explanations) values for feature attribution**: The paper uses SHAP plots to interpret which time points in EEG signals drive hemisphere classification. SHAP values indicate positive (toward class 1/right) or negative (toward class 0/left) contribution. Quick check: If a SHAP plot shows a feature with +0.15 impact for the right hemisphere class, what does that mean for that time point's role in classification?

## Architecture Onboarding

- **Component map**: Raw EEG Data (31 channels, 250 Hz) → Bandpass Filter (δ/θ/α/β/γ bands) → Binary Label (Left=0, Right=1 hemisphere) → Three Models (Big 9 dense + BN, Small 3 dense + BN, CNN 3 Conv1d+Pool) → Sigmoid/Softmax → BCE/CE Loss → Optimizer (8 options tested)

- **Critical path**: Preprocess: Filter raw EEG into 5 frequency bands; Label: Split channels by hemisphere; Split: 70/15/15 train/val/test; Train: Select optimizer (start with Adadelta for robustness, or RMSprop for γ-band); Evaluate: ROC-AUC, F1, precision, recall, specificity; Interpret: Generate SHAP plots for top features

- **Design tradeoffs**:
  | Choice | Pro | Con |
  |--------|-----|-----|
  | Big Model (9 layers) | Highest capacity, 0.97-0.99 ROC-AUC | Slow training (300-600s/epoch), overfit risk on small data |
  | Small Model (3 layers) | Fast inference (~1.6s), 0.95-0.99 ROC-AUC possible | Lower capacity, inconsistent across bands |
  | CNN | Spatial feature extraction, moderate training time | Requires reshaping, may lose temporal granularity |
  | Adadelta optimizer | Robust across models/bands | Not best for any single band |
  | RMSprop | Best for γ-band | Suboptimal for δ/θ/α |

- **Failure signatures**:
  - FTRL optimizer failure: ROC-AUC ≈ 0.5 (random), precision/recall near 0 → avoid for EEG classification
  - CNN on low-frequency bands (δ/θ): Lower accuracy than Big Model → spatial features less discriminative at low frequencies
  - SGD without momentum: Inconsistent performance, slow convergence → use Adam/Nadam variants instead
  - Missing batch normalization: Training instability in Big Model → ensure BN after every dense layer
  - Wrong learning rate: Small Model needs 0.00001, Big Model 0.01, CNN 0.001 → mismatch causes divergence or slow convergence

- **First 3 experiments**:
  1. **Baseline replication**: Run Big Model with Adadelta optimizer on β-band (Mona Lisa dataset). Target: ~0.92-0.93 test accuracy, ~0.97-0.99 ROC-AUC. Verify BN is applied after each dense layer.
  2. **Optimizer ablation**: On fixed architecture (CNN) and band (γ), compare RMSprop vs. Adagrad vs. Adam. Expect RMSprop to achieve highest ROC-AUC (~0.90) per Table 26.
  3. **Frequency-band sweep**: Using Small Model with Adam optimizer, train separate classifiers for all 5 bands on both datasets. Log ROC-AUC, training time, and inference time. Expect β/γ to outperform δ/θ/α.

## Open Questions the Paper Calls Out

### Open Question 1
Do the identified optimizer preferences (AdaMax for lower frequencies, RMSprop for higher frequencies) generalize to larger, more demographically diverse populations? The authors state further research should incorporate larger and more diverse datasets to reinforce the generalisability and validity of the findings. Evidence would come from successful replication of the optimizer performance hierarchy across significantly higher participant counts and diverse backgrounds.

### Open Question 2
To what extent do confounding demographic variables, such as gender and age, influence the efficiency of specific deep learning optimizers in EEG hemisphere classification? The authors explicitly list lack of control for potential confounding variables such as gender as a study limitation. Evidence would come from a stratified analysis on a balanced dataset showing consistent optimizer performance across distinct gender and age groups.

### Open Question 3
What are the underlying mechanisms that cause specific optimizers like RMSprop to excel in high-frequency (γ) bands while AdaMax excels in lower bands? While the authors observe distinct optimizer performance across bands, they suggest further investigation into the underlying mechanisms driving the optimizer's performance may provide valuable insights. Evidence would come from a theoretical analysis mapping the loss surface geometry of different frequency bands to the update rules of the respective optimizers.

## Limitations
- Small sample size (10 participants) limits statistical power and generalizability to broader populations
- Binary hemisphere classification task may oversimplify complex brain-state relationships
- Exclusive focus on band-specific analysis doesn't capture potentially important cross-band interactions

## Confidence

- **High**: Adagrad's superior performance on β-band and RMSprop's excellence on γ-band (confirmed across multiple tables with consistent ROC-AUC improvements of 0.02-0.05)
- **Medium**: CNN architecture's spatial feature capture capability (second-highest accuracy claimed, but only tested on one architecture type)
- **Low**: Generalization of optimizer-band coupling to clinical EEG datasets or different brain-state classification tasks beyond hemisphere discrimination

## Next Checks

1. Replicate the optimizer-band coupling findings on a larger, independent EEG dataset with at least 50 participants to verify Adagrad-β and RMSprop-γ relationships aren't artifacts of the small sample size
2. Test whether the observed optimizer performance differences persist when combining frequency bands (e.g., δ+θ vs. α+β+γ) rather than analyzing bands independently
3. Conduct ablation studies removing batch normalization to quantify its contribution to the Big Model's performance, as this architectural choice wasn't systematically evaluated against alternatives