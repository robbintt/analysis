---
ver: rpa2
title: Revisiting Large Language Model Pruning using Neuron Semantic Attribution
arxiv_id: '2503.01542'
source_url: https://arxiv.org/abs/2503.01542
tags:
- pruning
- tasks
- calibration
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the generalizability of large language\
  \ model (LLM) pruning methods across diverse tasks and datasets. The authors evaluate\
  \ three state-of-the-art pruning methods\u2014SparseGPT, Wanda, and RIA\u2014on\
  \ 14 models across 24 datasets spanning four task categories: sentiment classification,\
  \ question answering, semantic similarity, and logical reasoning."
---

# Revisiting Large Language Model Pruning using Neuron Semantic Attribution

## Quick Facts
- arXiv ID: 2503.01542
- Source URL: https://arxiv.org/abs/2503.01542
- Reference count: 21
- This paper investigates the generalizability of large language model pruning methods across diverse tasks and datasets.

## Executive Summary
This paper systematically evaluates three state-of-the-art pruning methods—SparseGPT, Wanda, and RIA—on 14 large language models across 24 datasets spanning four task categories. The study reveals that calibration data significantly impacts pruning performance, with sentiment classification tasks being particularly sensitive to calibration choices. To explain performance degradation, the authors introduce Neuron Semantic Attribution (NSA), a method that links specific neurons to semantic concepts by analyzing their activations on task-relevant tokens. NSA visualization reveals that pruned neurons in sentiment tasks are associated with sentiment-linked semantics, providing insight into why certain tasks suffer greater performance drops during pruning.

## Method Summary
The paper evaluates post-training pruning methods (SparseGPT, Wanda, RIA) on 14 LLMs using 9 different calibration datasets. The pruning process computes importance metrics combining weight magnitudes with activation statistics from calibration data, then applies unstructured or structured pruning at specified sparsity ratios. To diagnose performance degradation, the authors introduce Neuron Semantic Attribution (NSA), which uses Chain-of-Thought prompting to identify influential words, records neuron activations for these words in unpruned models, computes normalized scores to map neurons to semantics, and compares activations between pruned and unpruned models to identify which semantic functions were lost.

## Key Results
- Calibration data significantly affects pruning performance, with sentiment classification tasks showing up to 15-25% accuracy variation based on calibration choice
- Sentiment classification tasks exhibit higher pruning sensitivity than logical reasoning due to reliance on specialized semantic neurons
- Neuron Semantic Attribution successfully links pruned neurons to sentiment-linked semantics, explaining performance degradation patterns
- Some tasks (e.g., SciTail, QQP) show higher accuracy at higher sparsity ratios, suggesting task-specific over-parameterization

## Why This Works (Mechanism)

### Mechanism 1
Calibration data selection causally influences pruning outcomes by determining which neurons are retained versus pruned. Pruning methods compute importance metrics using both weights and activations derived from calibration samples. Different calibration datasets produce different activation patterns, leading to different neuron importance rankings and pruning decisions.

### Mechanism 2
Neuron Semantic Attribution links specific neurons to semantic concepts by correlating activation magnitudes with influential tokens. The three-step process uses CoT prompting to identify influential words, computes normalized activation scores to rank neurons, and compares pruned versus unpruned model activations to reveal degraded semantic functions.

### Mechanism 3
Sentiment classification tasks exhibit higher pruning sensitivity than logical reasoning due to reliance on specialized semantic neurons. Sentiment tasks depend on neurons encoding affective semantics (e.g., "badly," "trust") that, when removed, directly impair classification. Reasoning tasks rely more on distributed pattern recognition and general linguistic competencies redundantly encoded.

## Foundational Learning

- **Concept: Post-training pruning**
  - **Why needed here:** The paper evaluates methods that prune without fine-tuning; understanding this constraint explains why calibration data becomes critical (no recovery training).
  - **Quick check question:** Why does post-training pruning make calibration data selection more consequential than fine-tuning approaches?

- **Concept: Activation-based importance metrics**
  - **Why needed here:** All three methods combine weight magnitudes with activation statistics; grasping this clarifies why input data influences pruning decisions.
  - **Quick check question:** How does Wanda's metric |W| · ||X||₂ differ from weight-only pruning criteria?

- **Concept: Neuron polysemanticity**
  - **Why needed here:** NSA attributes single semantics to neurons; polysemantic neurons complicate this interpretation.
  - **Quick check question:** What are two limitations of attributing a single semantic concept to one neuron?

## Architecture Onboarding

- **Component map:**
  - Pruning Methods: SparseGPT (OBS updates) -> Wanda (magnitude × activation) -> RIA (relative importance + channel permutation)
  - NSA Pipeline: CoT influential word extraction -> unpruned model activation recording -> neuron-scored ranking -> pruned model comparison
  - Evaluation: 14 models × 24 datasets × 4 task categories × 9 calibration sets

- **Critical path:**
  1. Select calibration dataset (determines activation patterns X)
  2. Compute pruning metric M per method
  3. Apply pruning at target sparsity ratio
  4. Run NSA to diagnose performance changes (if degradation observed)

- **Design tradeoffs:**
  - C4/WikiText2 calibration → better general performance but may miss task-specific neurons
  - Task-specific calibration (e.g., SST2 for sentiment) → preserves task performance but may reduce transfer
  - Higher sparsity (>0.5) → aggressive compression but non-linear degradation on sensitive tasks

- **Failure signatures:**
  - Sentiment accuracy drops 15-25% with mismatched calibration data
  - NSA shows activation collapse (near-zero) for task-critical tokens in pruned model
  - Inconsistent results across calibration sets signal calibration-sensitivity

- **First 3 experiments:**
  1. Replicate calibration sensitivity: Prune OPT-6.7b with Wanda using C4 vs. WikiText2, evaluate on Yelp. Expect ~15% accuracy delta.
  2. Run NSA visualization: Apply NSA to Llama-2-7b before/after pruning on sentiment data; verify activation drops on influential tokens.
  3. Sparsity threshold test: Prune at 0.2, 0.4, 0.6 sparsity on ARC-E vs. SST2; confirm reasoning tasks maintain performance longer than sentiment.

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms cause certain tasks (e.g., SciTail, QQP) to exhibit higher accuracy at higher sparsity ratios, and does this imply task-specific over-parameterization? The paper observes non-monotonic behavior contradicting standard pruning theory, with some tasks showing improved performance at higher sparsity, but lacks theoretical explanation.

### Open Question 2
How can calibration data be algorithmically selected or synthesized to ensure preservation of task-critical neurons across diverse downstream tasks? Current methods rely on generic calibration sets that often fail to activate neurons relevant to specific tasks, leading to accidental pruning of critical semantic features.

### Open Question 3
Can the Neuron Semantic Attribution framework be integrated directly into the pruning objective function to regularize removal of semantically important neurons? The paper develops NSA as a diagnostic tool but does not propose using semantic data to guide pruning mask generation itself.

## Limitations
- Calibration data sensitivity analysis lacks theoretical guarantees about which calibration data will work best for which tasks
- NSA assumes stable neuron-semantic associations but polysemantic neurons may complicate interpretation
- Task sensitivity differences are empirically observed but the proposed mechanism lacks direct causal evidence
- Study focuses on post-training pruning without fine-tuning, limiting generalizability to other pruning paradigms

## Confidence

- **Calibration data sensitivity**: High confidence - extensive empirical evidence across 14 models and 24 datasets showing consistent patterns
- **NSA validity**: Medium confidence - shows intuitive results but acknowledged limitations with polysemantic neurons and primarily qualitative evidence
- **Task sensitivity differences**: Medium confidence - well-supported empirically but proposed mechanism lacks direct causal evidence

## Next Checks

1. **Cross-validation of NSA results**: Apply NSA to multiple sentiment datasets (Yelp, SST2, Amazon reviews) and verify consistent neuron activation patterns across datasets, comparing with polysemanticity-aware attribution methods to quantify accuracy rate.

2. **Controlled calibration experiment**: Systematically vary calibration data characteristics (domain, sequence length, semantic diversity) while holding other factors constant to establish causal links between specific calibration features and pruning outcomes.

3. **Task-specialization ablation study**: Artificially preserve specific neurons identified by NSA as sentiment-related during pruning of a reasoning task to measure whether this improves sentiment performance without harming reasoning accuracy, directly testing the hypothesis about task differences.