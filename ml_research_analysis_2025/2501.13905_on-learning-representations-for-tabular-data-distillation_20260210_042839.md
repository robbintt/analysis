---
ver: rpa2
title: On Learning Representations for Tabular Data Distillation
arxiv_id: '2501.13905'
source_url: https://arxiv.org/abs/2501.13905
tags:
- distillation
- data
- methods
- dataset
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TDColER, a framework for tabular data distillation
  that addresses the unique challenges of heterogeneous tabular features and non-differentiable
  downstream models. The core innovation is using column embeddings and learned latent
  representations to enable distillation methods designed for homogeneous image data
  to work effectively on tabular datasets.
---

# On Learning Representations for Tabular Data Distillation

## Quick Facts
- arXiv ID: 2501.13905
- Source URL: https://arxiv.org/abs/2501.13905
- Reference count: 40
- This paper introduces TDColER, a framework for tabular data distillation that addresses the unique challenges of heterogeneous tabular features and non-differentiable downstream models.

## Executive Summary
This paper introduces TDColER, a framework for tabular data distillation that addresses the unique challenges of heterogeneous tabular features and non-differentiable downstream models. The core innovation is using column embeddings and learned latent representations to enable distillation methods designed for homogeneous image data to work effectively on tabular datasets. TDColER incorporates encoder architectures like Transformers and Graph Neural Networks to create compact, information-rich representations, followed by a decoder for reconstruction.

The framework is evaluated on TDBench, a comprehensive benchmark comprising 23 datasets, 7 model classes, and 11 distillation schemes. Results demonstrate that TDColER significantly improves distilled data quality, achieving 0.5-143% performance gains across downstream models when trained on distilled data with as few as 10 instances per class. Notably, k-means clustering in latent space combined with supervised fine-tuned transformer encoders consistently outperformed other methods. The study also highlights that GNN-based encoders, while slightly less effective than transformers, offer superior parameter efficiency. Additionally, the framework shows strong performance under class imbalance and preserves feature correlations in distilled datasets, making it a robust solution for tabular data distillation across diverse scenarios.

## Method Summary
TDColER uses column embeddings to convert heterogeneous tabular features into homogeneous representations suitable for neural network encoders. The framework trains an autoencoder with reconstruction loss, then fine-tunes the encoder with a classifier head using a weighted combination of reconstruction and classification objectives. Distillation occurs in the learned latent space using methods like k-means clustering, followed by optional decoding back to original feature space. The method is evaluated on 23 OpenML datasets using relative regret as the primary metric.

## Key Results
- TDColER achieves 0.5-143% performance gains across downstream models when trained on distilled data with as few as 10 instances per class
- k-means clustering in latent space combined with supervised fine-tuned transformer encoders consistently outperformed other methods
- GNN-based encoders offer superior parameter efficiency while maintaining competitive performance
- The framework shows strong performance under class imbalance and preserves feature correlations in distilled datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Column embeddings transform heterogeneous tabular features into homogeneous representations, enabling distillation methods designed for images to operate on tabular data.
- **Mechanism:** Each feature (numerical via binning, categorical via one-hot expansion) is mapped to a learnable embedding vector of dimension `m`. A row with `r` numerical and `c` categorical features becomes an `(m × (r+c))` matrix of embeddings, which serves as input token embeddings for Transformer/GNN/FFN encoders. This converts the "mismatch" between heterogeneous tabular columns and homogeneous image pixels into a uniform representation.
- **Core assumption:** Binning numerical features (with a dedicated "missing" bin) preserves sufficient ordinal/relational information; embedding dimension `m` is large enough to capture feature semantics.
- **Evidence anchors:**
  - [abstract] "The core innovation is using column embeddings and learned latent representations to enable distillation methods designed for homogeneous image data to work effectively on tabular datasets."
  - [section 2.1] "A sample (row) in a table with r numerical features and c categorical features is now represented as a set of (r + c) embeddings in R^m each of size m... thus effectively as the (m × (r + c)) matrix."
  - [corpus] "LLM Embeddings for Deep Learning on Tabular Data" (FMR≈0.55) explores unified cross-table embeddings—weak/indirect support; no direct evidence on binning + column embeddings for distillation.
- **Break condition:** If critical ordinal relationships in numerical features are destroyed by coarse binning, or if `m` is too small to disentangle feature semantics, representation quality degrades.

### Mechanism 2
- **Claim:** Supervised fine-tuning (SFT) of the encoder improves distilled data quality by making latent representations class-discriminative while preserving reconstruction fidelity.
- **Mechanism:** After unsupervised autoencoder training via reconstruction loss (Equation 1), a classifier head is added and the encoder is fine-tuned with a combined objective: `reconstruction loss + α × classification loss` (Equation 2). This pushes latent representations apart along class boundaries while constraining the decoder to still reconstruct original features, resulting in a latent space that is both informative and faithful.
- **Core assumption:** The hyperparameter `α` (tested: 0.3, 0.5, 0.7) can balance discriminative power and reconstruction quality without overfitting to the classifier.
- **Evidence anchors:**
  - [section 2.1, eq. 2] "After obtaining the column embeddings C, encoder φ and decoder ψ by solving eq. (1), we fine-tune the encoder by learning a classifier f... while keeping the reconstruction loss low."
  - [section 4, Table 1] TF* (SFT Transformer) achieves median relative regret 0.6149 vs. 0.9439 for unsupervised TF—a ~35% improvement; SFT improves all encoders.
  - [corpus] No direct corpus evidence on SFT for tabular distillation; related work on tabular representation learning exists but does not validate this specific mechanism.
- **Break condition:** If `α` is too high, reconstruction degrades and decoded distilled samples lose fidelity; if too low, representations remain insufficiently discriminative for downstream tasks.

### Mechanism 3
- **Claim:** k-means clustering in the learned latent space outperforms gradient-based distillation methods for tabular data due to robustness to class imbalance and model-agnostic operation.
- **Mechanism:** Per-class k-means clustering (n/L clusters per class L) in the encoder's latent space selects synthetic samples as cluster centroids. This forces equal emphasis on all classes regardless of imbalance, and operates independently of downstream model architecture (unlike KIP/GM which require differentiable backbones and risk overfitting to the teacher network).
- **Core assumption:** The latent space has learned semantically meaningful clusters where Euclidean distance correlates with intra-class similarity; cluster centroids generalize across downstream models.
- **Evidence anchors:**
  - [abstract] "k-means clustering in latent space combined with supervised fine-tuned transformer encoders consistently outperformed other methods."
  - [section 4, Figure 5] k-means achieves best average rank at IPC=10 (1.6), IPC=10 (2.1), IPC=100 (2.1), outperforming GM, KIP, MTT, DATM.
  - [section 4, Figure 9] Clustering methods maintain strong performance under class imbalance (minority ratio 0.0-0.5) while KIP/GM degrade sharply.
  - [corpus] "Class-Imbalanced-Aware Adaptive Dataset Distillation" (arXiv:2501.10677) addresses imbalance in credit scoring distillation—weak/indirect support; different adaptive approach.
- **Break condition:** If latent representations are poorly learned (class overlap), k-means selects uninformative centroids; if downstream tasks require gradient-matching for specific architectures, gradient-based methods may still be superior.

## Foundational Learning

### Concept: Column embeddings for heterogeneous features
- Why needed here: Tabular data has mixed numerical/categorical columns with different semantics; standard neural networks expect uniform input dimensions. Column embeddings (binning + learned vectors) create a homogeneous representation.
- Quick check question: Can you explain how binning a numerical feature with values [1.2, 5.7, 9.3] into bins [0-3], (3-6], (6-10] enables embedding lookup?

### Concept: Autoencoder reconstruction loss
- Why needed here: The encoder-decoder is trained to reconstruct original features, ensuring the latent space retains maximum information for model-agnostic downstream use.
- Quick check question: If reconstruction loss is high, what does that imply about the latent representation's utility for distillation?

### Concept: Model-agnostic vs. model-centric distillation
- Why needed here: k-means is model-agnostic (no gradient dependence); KIP/GM are model-centric (require differentiable backbone). Understanding this distinction explains performance differences across downstream classifiers (e.g., XGBoost vs. MLP).
- Quick check question: Why might a distillation method that relies on gradient matching through an MLP fail to generalize to XGBoost?

## Architecture Onboarding

### Component map:
Preprocessor -> Column Embeddings -> Encoder (FFN/GNN/Transformer) -> Decoder -> Distillation Module -> Output

### Critical path:
1. Train autoencoder (unsupervised) with reconstruction loss.
2. Fine-tune encoder with classifier head (supervised) using `α`-weighted combined loss.
3. Encode full dataset → latent space.
4. Apply distillation (k-means clustering per class) in latent space.
5. Either: (a) train downstream model on latent representations, or (b) decode and train in original feature space.

### Design tradeoffs:
- Encoder choice: Transformer (best performance, larger params) vs. GNN (second-best, smallest params) vs. FFN (baseline).
- Latent vs. decoded output: Latent preserves more info and yields better performance (Figure 3) but requires encoder at inference; decoded is interpretable but loses ~10-40% relative performance.
- Embedding scheme: Binary (binning) vs. PLE vs. scaled—binary works for all encoders; PLE strongest but incompatible with GNN (Section B.2).
- Distillation method: k-means (robust, simple) vs. GM/KIP (gradient-based, weaker on tabular/non-NN downstream models).

### Failure signatures:
- High reconstruction loss (>0.5 median): Encoder-decoder undertrained; latent space loses information.
- SFT regression vs. unsupervised: `α` too high; classifier overfits, reconstruction degrades.
- k-means underperforms random sampling: Latent space not discriminative; check class separation (PCA visualization).
- Gradient-based methods fail on XGBoost: Expected—these methods overfit to differentiable teacher (MLP); use clustering instead.
- Class imbalance causes distilled samples to favor majority class: k-means should mitigate; if not, verify per-class clustering is enforced.

### First 3 experiments:
1. **Baseline validation:** Train unsupervised autoencoder (FFN) on a medium dataset (e.g., Adult), measure reconstruction loss and downstream classifier performance on random 10% subset vs. full data. Confirms pipeline integrity.
2. **Encoder comparison:** On same dataset, compare Transformer, GNN, FFN encoders (all with SFT) using k-means distillation at IPC=10/50/100. Measure relative regret across 3 downstream models (KNN, LR, XGBoost). Validates architecture tradeoffs.
3. **Distillation method ablation:** With best encoder from step 2, compare k-means, agglomerative, GM, KIP on a highly imbalanced dataset (e.g., Amazon Employee Access). Track performance vs. minority class ratio. Confirms clustering robustness claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the TDColER framework perform when applied to tabular regression tasks?
- **Basis in paper:** [explicit] The authors state, "Following Cui et al. (2022), we only consider classification tasks in this work, but it should be noted that regression can be easily added into our framework."
- **Why unresolved:** The entire TDBench benchmark (23 datasets) and the defined evaluation metric (relative regret based on balanced accuracy) are restricted to binary classification, leaving the efficacy of this distillation method for continuous target variables unknown.
- **What evidence would resolve it:** An evaluation of TDColER on standard tabular regression datasets (e.g., from OpenML) using regression-appropriate metrics (e.g., RMSE relative regret) to verify if the representation learning gains transfer.

### Open Question 2
- **Question:** Can the reconstruction learning objective be modified to better preserve information when decoding distilled data back to the original feature space?
- **Basis in paper:** [inferred] Section 4 results (Figure 3) demonstrate that while distilling in the latent space ("Enc.") yields the best performance, decoding back to the original representation ("Rec.") often degrades performance, yet the paper motivates the decoder as necessary for interpretability (Section 2.1).
- **Why unresolved:** The current reconstruction loss (Equation 1) appears sufficient for clustering in latent space but may lose critical information required by downstream models when projected back to the original heterogeneous feature space.
- **What evidence would resolve it:** Experiments comparing the current weighted cross-entropy reconstruction against variational or adversarial objectives to see if the performance gap between "Enc." and "Rec." can be closed.

### Open Question 3
- **Question:** Why do sophisticated gradient-based distillation methods consistently underperform simple clustering in the learned tabular representations?
- **Basis in paper:** [inferred] Section 4 and Figure 5 show that simple k-means clustering significantly outperforms Gradient Matching (GM) and Kernel Induced Points (KIP). The authors suggest these methods may "overfit to the teacher network," but the precise mechanism for their failure in this domain remains unverified.
- **Why unresolved:** It is unclear if the failure is due to the specific latent manifold created by the autoencoder, the lack of differentiable downstream models (decision trees), or an intrinsic property of tabular data distillation.
- **What evidence would resolve it:** An ablation study analyzing the geometry of the latent space (e.g., Lipschitz continuity, class separation) to determine why simple centroid-based selection is more robust than gradient matching.

## Limitations

- Column embedding effectiveness depends heavily on binning strategy; paper assumes uniform bin counts work across datasets without ablation.
- GNN encoder shows parameter efficiency but lacks detailed analysis of why it underperforms Transformers despite architectural suitability for tabular data.
- Class imbalance experiments use synthetic imbalance; real-world imbalance patterns may behave differently.

## Confidence

- **High:** TDColER improves distilled data quality (measured by downstream model performance); k-means in latent space outperforms gradient-based methods; supervised fine-tuning benefits all encoders.
- **Medium:** GNN encoders offer superior parameter efficiency while maintaining competitive performance; decoded distilled data loses ~10-40% relative performance versus latent space.
- **Low:** Cross-dataset generalizability of optimal hyperparameters (bin counts, α values, latent dimensions); scalability to millions of rows remains unevaluated.

## Next Checks

1. **Binning sensitivity analysis:** Vary bin counts (5-50 bins) for numerical features and measure downstream performance degradation to identify breaking points.
2. **Latent space visualization:** Generate PCA/t-SNE plots of latent representations for datasets with different imbalance ratios to verify k-means is finding meaningful clusters.
3. **Scaling experiment:** Evaluate TDColER on a larger dataset (>100K rows) to measure runtime/memory scaling and confirm distillation benefits persist at scale.