---
ver: rpa2
title: Efficient Parallel Training Methods for Spiking Neural Networks with Constant
  Time Complexity
arxiv_id: '2506.12087'
source_url: https://arxiv.org/abs/2506.12087
tags:
- training
- spiking
- neural
- time
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high time complexity of training Spiking
  Neural Networks (SNNs), which scales as O(T) due to sequential processing over T
  timesteps. The authors propose Fixed-point Parallel Training (FPT), a method that
  reduces this complexity to O(K), where K is a small constant (typically 3), by using
  a fixed-point iteration form of Leaky Integrate-and-Fire (LIF) neurons.
---

# Efficient Parallel Training Methods for Spiking Neural Networks with Constant Time Complexity

## Quick Facts
- arXiv ID: 2506.12087
- Source URL: https://arxiv.org/abs/2506.12087
- Authors: Wanjin Feng; Xingyu Gao; Wenqian Du; Hailong Shi; Peilin Zhao; Pengcheng Wu; Chunyan Miao
- Reference count: 40
- Primary result: O(K) time complexity (K=3) SNN training via fixed-point iteration, achieving >100× speedup on GPUs

## Executive Summary
This paper addresses the O(T) time complexity bottleneck in Spiking Neural Network (SNN) training caused by sequential processing over T timesteps. The authors propose Fixed-point Parallel Training (FPT), which reformulates LIF neuron dynamics as a fixed-point equation solvable in K parallel iterations (typically K=3). This approach preserves the reset mechanism and enables GPU acceleration while maintaining accuracy. Theoretical convergence analysis and extensive experiments on benchmark datasets demonstrate FPT's effectiveness, achieving significant speedups without architectural modifications.

## Method Summary
FPT reformulates the sequential LIF update equation into a fixed-point form that can be solved in parallel across all timesteps. The method uses a lower-triangular decay matrix Λ to capture temporal dependencies, then iteratively solves for membrane potentials and spikes through K steps. A key innovation is the use of asymmetric surrogate gradients (different α for forward/backward passes) that stabilize training while maintaining approximation fidelity. The approach preserves the reset mechanism through iterative refinement, unlike previous parallel methods. Training uses standard backpropagation with automatic differentiation through the K-iteration fixed-point loop.

## Key Results
- Reduces SNN training time complexity from O(T) to O(K) with K=3
- Achieves >100× speedup on GPU for long-term tasks compared to BPTT
- Maintains or improves accuracy on DVS-CIFAR10, DVS-Gesture, ImageNet-100, and Sequential CIFAR benchmarks
- Outperforms PSN/IPSU methods while preserving reset mechanism

## Why This Works (Mechanism)

### Mechanism 1: Fixed-Point Reformulation of Sequential LIF Dynamics
Reformulating LIF recurrence as a fixed-point equation enables parallel computation across all timesteps. The sequential update is vectorized using a lower-triangular decay matrix, decoupling timestep dependencies and allowing simultaneous evaluation.

### Mechanism 2: Surrogate Gradient Asymmetric Approximation
Different steepness parameters for forward (α_f) and backward (α_b ≤ α_f) passes stabilize training while maintaining approximation fidelity. Forward uses larger α for accurate spike approximation; backward uses smaller α for smoother gradients.

### Mechanism 3: Preservation of Reset Mechanism Through Iterative Refinement
Unlike PSN/PSU which remove reset, FPT retains it through K iterations of spike-feedback. Each iteration refines the reset effect, converging to LIF-equivalent dynamics and improving biological fidelity and accuracy.

## Foundational Learning

- **Contraction Mapping and Banach Fixed-Point Theorem**
  - Why needed here: Ensures FPT's iterative scheme converges to a unique equilibrium; explains why K can be small and bounded
  - Quick check question: Given λ=0.25, V_th=1, α=12, verify the contraction condition L < 1 using ||Λ - I||_∞ = λ(1-λ^{T-1})/(1-λ)

- **Surrogate Gradient Methods in SNNs**
  - Why needed here: The Heaviside step function is non-differentiable; surrogate gradients enable backpropagation through spiking neurons
  - Quick check question: Explain why α_b < α_f improves training stability compared to using α_b = α_f

- **Lower Triangular Matrix Structure for Temporal Decay**
  - Why needed here: Λ captures exponential decay of past inputs; its structure enables parallel computation while respecting causal temporal dependencies
  - Quick check question: For T=4 and λ=0.5, write out matrix Λ explicitly and compute Λc for c = [1, 0, 0, 0]^T

## Architecture Onboarding

- **Component map**: Input c → Decay Matrix Λ → Fixed-Point Iterator (K steps) → Surrogate Function S_α → Output u*, s*

- **Critical path**:
  1. Precompute Λ once per forward pass (O(T²) but amortized)
  2. Iterate K times: u^(k) = -V_th(Λ - I)s^(k-1) + Λc, s^(k) = S_{α_f}(u^(k) - V_th)
  3. After K iterations, compute final spikes: s* = H(u* - V_th) or probabilistic sampling
  4. Backward pass uses automatic differentiation with surrogate gradient S'_{α_b}

- **Design tradeoffs**:
  - **K vs. Accuracy**: K=3 is default; larger K marginally improves convergence but increases compute linearly
  - **Memory vs. Speed**: FPT stores all T timesteps simultaneously (O(LT) memory) vs. BPTT's sequential processing; enables 10-100× speedup on GPU
  - **α_f/α_b vs. Stability**: Larger α_f improves forward accuracy but requires smaller α_b for stable gradients

- **Failure signatures**:
  - **Divergence**: u^(k) grows unbounded; check λ < 1 and V_th > 0
  - **Slow convergence**: ||u^(k) - u^(k-1)|| remains large after K=3; increase α_f progressively or raise K
  - **Accuracy drop vs. BPTT**: Ensure α_f values increase across iterations (e.g., [3, 12, 12]); check that reset mechanism is active

- **First 3 experiments**:
  1. **Single neuron convergence test**: Generate random input current c, run FPT with K=3 and K=10, compare u* against sequential LIF simulation. Measure absolute error in membrane potential and spike agreement rate.
  2. **Timing benchmark on MNIST**: Implement 3-layer SNN with FPT vs. BPTT, vary T ∈ {8, 64, 256, 512}. Measure training time per batch and memory usage. Expect FPT to show constant-ish training time while BPTT scales linearly.
  3. **Ablation on K and reset**: On Sequential CIFAR10, compare FPT with K=1, 2, 3, 5 against PSN (K=1, no reset) and IPSU (K=2). Report accuracy to validate that K≥3 with reset outperforms alternatives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can leveraging the binary nature of spike events enable efficient memory compression techniques to reduce the O(LT) space complexity of FPT?
- Basis in paper: [explicit] The authors state in the Limitations section that FPT requires more memory than sequential methods and suggest that "leveraging the binary nature of spike events could lead to more efficient memory compression techniques."
- Why unresolved: The paper identifies the high memory usage required for parallel processing as a bottleneck but does not propose or implement a specific compression algorithm to mitigate it.
- What evidence would resolve it: A modified FPT implementation utilizing bit-level compression that demonstrates reduced memory usage without significantly increasing time complexity or degrading accuracy.

### Open Question 2
- Question: Can the FPT framework be theoretically and practically adapted for Integrate-and-Fire (IF) models that lack time-dependent decay?
- Basis in paper: [explicit] The Limitations section notes, "One limitation of FPT... is its inability to handle scenarios where there is no time-dependent decay, as seen in Integrate-and-Fire models."
- Why unresolved: The current convergence proof relies on a contraction mapping that depends on the decay factor λ; removing this factor breaks the theoretical guarantees provided.
- What evidence would resolve it: A modified fixed-point mapping and convergence proof for λ=1, followed by experimental validation showing FPT training IF models effectively.

### Open Question 3
- Question: Does the deviation of FPT's backward pass from traditional BPTT gradients introduce optimization bias on complex, non-convex loss landscapes?
- Basis in paper: [inferred] The paper notes that the backward process uses automatic differentiation over K steps and "does not directly correspond to traditional BPTT," relying on the assumption that K=3 is sufficient.
- Why unresolved: While experiments show comparable accuracy, the theoretical impact of approximating the full temporal gradient with a fixed-point derivative on different optimization landscapes remains unexplored.
- What evidence would resolve it: Comparative analysis of gradient direction and magnitude between FPT and BPTT on tasks specifically designed to have sharp or complex loss surfaces.

## Limitations
- Memory complexity O(LT) remains a constraint for very long sequences
- The K=3 bound is derived theoretically but may need adjustment for extreme λ or V_th values
- Asymmetric surrogate gradient hyperparameters are heuristically chosen rather than optimized

## Confidence
- **High**: Core parallel computation mechanism (fixed-point reformulation)
- **Medium**: Asymmetric surrogate gradient design and optimal scheduling
- **Medium**: Reset mechanism preservation claim and exact equivalence to sequential LIF

## Next Checks
1. **Extreme parameter stress test**: Evaluate FPT convergence and accuracy for λ ∈ {0.9, 0.99} and V_th ∈ {0.1, 0.5}, measuring required K and accuracy degradation.
2. **Ground truth comparison**: Implement exact LIF simulation (O(T) sequential) and compare FPT output spike patterns and membrane potentials across 100 random input sequences.
3. **Hyperparameter sensitivity analysis**: Systematically vary α_f/α_b schedules (e.g., [1, 6, 12], [3, 9, 15]) on Sequential CIFAR10 and measure impact on training stability and final accuracy.