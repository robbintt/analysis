---
ver: rpa2
title: 'Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic
  Approaches'
arxiv_id: '2510.03808'
source_url: https://arxiv.org/abs/2510.03808
tags:
- discourse
- relations
- rhetorical
- bert
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of automatically identifying
  rhetorical relations in discourse units, which is crucial for natural language processing
  tasks like text summarization and sentiment analysis. The research compares manual
  annotation using the INCEpTION tool with automatic approaches based on large language
  models.
---

# Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches

## Quick Facts
- arXiv ID: 2510.03808
- Source URL: https://arxiv.org/abs/2510.03808
- Reference count: 11
- Primary result: DistilBERT achieved 90% accuracy and 0.88 F1 score for rhetorical relation classification on cricket news discourse units

## Executive Summary
This study compares manual annotation of rhetorical relations using INCEpTION with automatic approaches based on large language models. The research focuses on classifying discourse relations between elementary discourse units (EDUs) in sports reports, specifically cricket news. The study evaluates BERT, DistilBERT, and Logistic Regression models for predicting rhetorical relations such as elaboration, contrast, background, and cause-effect. The results show that DistilBERT slightly outperforms BERT in accuracy while Logistic Regression achieves high performance when using BERT embeddings as features.

## Method Summary
The study collected 10 cricket news articles and manually annotated rhetorical relations between elementary discourse units using INCEpTION. The dataset was exported to CSV format and balanced through oversampling to ensure 25 samples per relation class. Three approaches were evaluated: fine-tuning BERT and DistilBERT using Hugging Face Trainer API with specified hyperparameters, and using Logistic Regression with frozen BERT embeddings as features. Models were evaluated on accuracy and F1 scores, with confusion matrices used for error analysis.

## Key Results
- DistilBERT achieved the highest accuracy at 90% and an F1 score of 0.88
- BERT achieved 80% accuracy and 0.78 F1 score
- Logistic Regression model performed well with 97% accuracy when using BERT tokenizer embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Bidirectional transformer attention enables rhetorical relation classification by capturing contextual dependencies between discourse unit pairs.
- BERT and DistilBERT encode EDU pairs jointly, allowing self-attention to learn which tokens in one unit semantically connect to tokens in another, forming patterns that distinguish relations like contrast vs. elaboration.
- Core assumption: Rhetorical relations manifest in learnable contextual patterns within paired discourse units.

### Mechanism 2
- Knowledge distillation preserves sufficient representational capacity for rhetorical relation classification while improving computational efficiency.
- DistilBERT's compressed architecture retains the attention patterns most relevant for semantic matching between discourse units, discarding redundant parameters that don't contribute to downstream task performance.
- Core assumption: The essential features for discourse relation classification survive the 40% parameter reduction in distillation.

### Mechanism 3
- Frozen transformer embeddings provide discriminative features for linear classifiers on small rhetorical relation datasets.
- The 768-dimensional hidden states from pre-trained encoders capture semantic similarity and discourse-relevant features that Logistic Regression can separate with linear decision boundaries when training data is limited.
- Core assumption: Pre-trained embeddings encode sufficient rhetorical information without task-specific fine-tuning.

## Foundational Learning

- **Elementary Discourse Units (EDUs)**: The atomic units being classified; improper segmentation cascades into relation classification errors.
  - Quick check: Given "England won but injuries depleted their squad," can you identify where EDU1 ends and EDU2 begins, and what relation connects them?

- **Rhetorical Structure Theory (RST) relation types**: Understanding semantic distinctions between relations (e.g., contrast vs. concession, elaboration vs. background) is prerequisite to interpreting model errors.
  - Quick check: Why might "cause-effect" be confused with "narration" when both involve sequential events?

- **Knowledge distillation in transformers**: Explains why DistilBERT may outperform BERT on specific tasks despite having fewer parameters.
  - Quick check: What information is potentially lost when compressing BERT to DistilBERT, and how might this affect discourse tasks?

## Architecture Onboarding

- Component map: INCEpTION (manual annotation with Span/Relations layers) -> Data pipeline (CSV: EDU1, EDU2, Label → oversampling for balance) -> Feature extraction (frozen BERT/DistilBERT → 768-dim hidden states) -> Classifiers (fine-tuned BERT, fine-tuned DistilBERT, Logistic Regression) -> Evaluation (accuracy, F1, confusion matrix, per-relation error analysis)

- Critical path: 1. Annotate in INCEpTION with consistent RST guidelines → 2. Export to structured CSV → 3. Balance via oversampling (25 samples per relation) → 4. Train/validation/test split → 5. Evaluate with confusion matrices

- Design tradeoffs: Small dataset (57 EDUs from 10 documents) enables rapid iteration but limits generalization claims; oversampling addresses class imbalance but risks overfitting to minority classes; DistilBERT offers speed; BERT offers depth; Logistic Regression offers interpretability

- Failure signatures: BERT confuses Elaboration ↔ Contrast (semantic overlap in sports reporting); DistilBERT confuses Cause-Effect ↔ Joint (temporal vs. coordination ambiguity); high training accuracy but lower test accuracy signals overfitting on small data

- First 3 experiments: 1. Run Logistic Regression on frozen BERT embeddings to establish a feature-quality baseline before fine-tuning; 2. Fine-tune BERT and DistilBERT with identical hyperparameters (batch=64, epochs=20, lr=2e-5) for controlled comparison; 3. Generate per-relation confusion matrices to identify which rhetorical categories require additional training examples or relation-specific features

## Open Questions the Paper Calls Out

1. How do generative transformer models compare to encoder-only models in classifying rhetorical relations?
   - Basis: The conclusion states that "exploring additional models like GPT, T5, or RoBERTa could also significantly contribute to gaining insights into discourse relation classification."
   - Why unresolved: The study was limited to BERT, DistilBERT, and Logistic Regression, leaving the performance of generative or larger architectures untested.

2. Does scaling the dataset size and diversity of relation types improve the stability of automatic classification?
   - Basis: The author suggests a potential direction is to "add more rhetorical relations and experiment with the vast number of discourse units."
   - Why unresolved: The current study relied on a very small dataset (10 reports, 57 units), which necessitated oversampling and may have limited the models' generalization capabilities.

3. Can domain-specific fine-tuning resolve the specific confusion between semantically similar relations like Cause-Effect and Joint?
   - Basis: Error analysis revealed DistilBERT frequently misclassified "Cause-Effect" as "Joint," while BERT confused "Elaboration" with "Contrast," suggesting the models fail to capture subtle semantic nuances.
   - Why unresolved: The paper identifies these specific failure modes but does not test interventions to correct them, noting only general "domain-based fine-tuning" is necessary.

## Limitations
- Extremely limited dataset size (57 discourse units from 10 cricket articles) constrains generalizability
- Oversampling to balance classes may introduce artificial patterns not reflective of natural discourse distribution
- Exclusive focus on sports reporting domain limits applicability to other text types

## Confidence
- High Confidence: The relative performance ranking between models (DistilBERT > BERT > Logistic Regression) appears robust
- Medium Confidence: The absolute performance numbers (90% accuracy for DistilBERT) are less reliable due to small dataset size
- Low Confidence: The mechanism explanations for why specific models excel at certain relations remain speculative

## Next Checks
1. Replicate the study with a larger, more diverse corpus (minimum 500 discourse units across multiple sports and news domains) to test whether the observed performance patterns hold with adequate statistical power.

2. Apply the best-performing model (DistilBERT) to discourse relation classification in non-sports domains (e.g., scientific abstracts, news editorials) to assess domain generalization and identify domain-specific adaptation needs.

3. Conduct detailed manual analysis of misclassified pairs to identify whether errors stem from annotation ambiguity, model limitations, or inherent semantic overlap between relation types, particularly for the elaboration-contrast and cause-effect-joint confusions identified in the error analysis.