---
ver: rpa2
title: 'Control-R: Towards controllable test-time scaling'
arxiv_id: '2506.00189'
source_url: https://arxiv.org/abs/2506.00189
tags:
- reasoning
- control
- search
- process
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenges of underthinking and overthinking\
  \ in long chain-of-thought (CoT) reasoning for Large Reasoning Models (LRMs) by\
  \ introducing Reasoning Control Fields (RCF)\u2014a test-time approach that injects\
  \ structured control signals to guide reasoning from a tree search perspective.\
  \ RCF enables models to adjust reasoning effort according to given control conditions\
  \ when solving complex tasks."
---

# Control-R: Towards controllable test-time scaling

## Quick Facts
- arXiv ID: 2506.00189
- Source URL: https://arxiv.org/abs/2506.00189
- Reference count: 32
- 32B model achieves state-of-the-art performance on AIME2024 (70.0%) and MATH500 (55.4%)

## Executive Summary
This paper introduces Reasoning Control Fields (RCF) - a test-time approach for controlling long chain-of-thought reasoning in Large Reasoning Models (LRMs). RCF uses 11 structured control signals to guide reasoning depth and strategy from a tree search perspective, enabling models to adjust reasoning effort according to specified conditions. The authors present Control-R-32B, trained via Conditional Distillation Finetuning (CDF) on the Control-R-4K dataset, achieving state-of-the-art performance at the 32B scale while enabling controllable Long CoT reasoning (L-CoT).

## Method Summary
The approach combines Reasoning Control Fields (RCF) - 11 integer-valued attributes (0-9) governing reasoning behaviors like search depth, breadth, and error correction - with Conditional Distillation Finetuning (CDF). Control-R-32B is trained on the Control-R-4K dataset by finetuning Qwen2.5-32B-Instruct using LoRA adapters with a conditional language modeling objective. At inference, RCF strings are appended to queries, allowing users to specify desired reasoning characteristics. The method frames L-CoT as tree search, with control fields regulating tree topology characteristics.

## Key Results
- Control-R-32B achieves 70.0% on AIME2024 and 55.4% on MATH500
- Demonstrates effective control over reasoning depth and strategy through RCF manipulation
- Outperforms baseline models at 32B scale while providing test-time reasoning control
- CDF training prevents averaging conflicts across multiple reasoning paths

## Why This Works (Mechanism)

### Mechanism 1
Structured control fields injected at test-time can modulate reasoning depth and strategy. The model conditions generation on explicit control tokens (RCF strings) appended to the input, modifying P(rt|r<t, q, C) rather than P(rt|r<t, q) alone. Each of 11 fields (0-9 scale) signals desired reasoning behavior—e.g., search_depth:9 triggers multi-layer decomposition; error_detection:9 increases self-checking frequency. Core assumption: Models can learn to map discrete control tokens to specific reasoning behaviors through conditional finetuning, and these behaviors generalize at inference when users specify arbitrary field values.

### Mechanism 2
Framing Long CoT as implicit tree search enables systematic control attributes. Each reasoning step maps to a search tree node; control fields regulate tree topology—search_breadth controls parallel exploration, error_correction enables backtracking analogues, strategy_switching governs traversal method changes. The 11 fields derive from this tree-search abstraction. Core assumption: The model's internal reasoning dynamics correspond sufficiently to tree search that attributes defined on tree topology meaningfully constrain actual token generation.

### Mechanism 3
Conditional Distillation Finetuning prevents averaging conflicts across multiple valid reasoning paths. Training objective LCDF = E[-Σlog P(ri|r<i, q, C; θ)] explicitly conditions on C, so the model learns P(R|C,q) rather than marginalizing over conflicting C values. This enables the same query to produce different reasoning styles based on specified control. Core assumption: Conditioning is sufficient to disentangle reasoning styles; the model won't default to an "average" style when presented with novel control configurations.

## Foundational Learning

- **Conditional Language Modeling**: Why needed here: The entire RCF mechanism rests on modifying the conditional probability P(token|context, control_signal). Quick check: Can you explain why P(R|q,C) ≠ P(R|q)·P(C) and how this differs from classifier-free guidance?

- **Tree Search Abstractions (DFS/BFS, backtracking)**: Why needed here: The 11 control fields are defined in terms of tree topology. Quick check: If a reasoning trace revisits an earlier conclusion and revises it, which control field(s) should score high?

- **Knowledge Distillation**: Why needed here: CDF trains a 32B model using annotations from GPT-4o and traces from DeepSeek-R1/QwQ. Quick check: Why might a student model fail to learn from a teacher even with perfect demonstrations?

## Architecture Onboarding

- Component map: Input Query → [RCF String Injection] → [Control-R-32B] → Long CoT Generation → Answer Extraction

- Critical path:
  1. Data preparation: Sample responses from LRMs → annotate with GPT-4o using the 11-dimension scoring prompt → convert JSON annotations to RCF string format
  2. Training: Apply CDF loss on Control-R-4K dataset; LoRA rank=16, all linear layers targeted
  3. Inference: Append RCF string to query with forced generation prompt `'\n'` to prevent reasoning skip

- Design tradeoffs:
  - Control granularity vs. annotation cost: 11 fields provide fine control but require multi-dimensional annotation
  - Field independence vs. correlation: Paper sets all fields to same value in ablations; real-world use may require per-field tuning
  - Dataset composition: Control-R-4K mixes competition math, synthetic calculus, and distilled data—unclear which subset drives which capabilities

- Failure signatures:
  - Control field ignored: Model generates same output regardless of RCF values
  - Over-adherence to control, underperformance on task: Model produces reasoning matching control spec but wrong answer
  - OOD control values: Model sees control configuration not in training distribution

- First 3 experiments:
  1. Control sensitivity test: Run same query with all fields at 0, 5, and 9; measure both accuracy and token count
  2. Field ablation: Hold all fields at 5, vary one field from 0→9; plot accuracy vs. field value
  3. Cross-domain transfer: Test on non-math tasks with same control configurations

## Open Questions the Paper Calls Out

1. Can RCF-based control generalize effectively to open-domain reasoning tasks beyond mathematical and logical benchmarks?
2. Can optimal control field values be automatically calibrated for different task types without manual tuning?
3. How robust is RCF-based control under adversarial conditions or distribution shifts in real-world applications?
4. Does the quality and consistency of ChatGPT-4o-generated control field annotations create a bottleneck for scaling to larger datasets or different domains?

## Limitations
- Control field generalization limited to mathematical/logical benchmarks with no evaluation on open-domain tasks
- Performance scaling at smaller models (7B/13B) not adequately addressed
- Annotation quality and bias concerns due to reliance on GPT-4o scoring without inter-annotator agreement metrics

## Confidence
- **High Confidence**: The core technical contribution of introducing conditional distillation finetuning (CDF) is well-specified and reproducible
- **Medium Confidence**: The effectiveness of the 11 control fields for managing reasoning depth and strategy is demonstrated on math benchmarks but lacks comprehensive ablation studies
- **Low Confidence**: Claims about the model's ability to handle arbitrary control configurations at inference time are not empirically validated

## Next Checks
1. **Control Field Sensitivity Analysis**: Systematically vary each of the 11 control fields independently from 0 to 9 (holding others constant) on held-out test sets, measuring both accuracy and reasoning depth
2. **Cross-Domain Control Transfer**: Apply Control-R-32B with the same control configurations to non-math domains like GPQA-Diamond (science), coding benchmarks, and common sense reasoning tasks
3. **OOD Control Configuration Testing**: Generate novel control field combinations not present in the training data and evaluate performance