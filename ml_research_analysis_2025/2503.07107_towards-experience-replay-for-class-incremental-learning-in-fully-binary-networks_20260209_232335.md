---
ver: rpa2
title: Towards Experience Replay for Class-Incremental Learning in Fully-Binary Networks
arxiv_id: '2503.07107'
source_url: https://arxiv.org/abs/2503.07107
tags:
- learning
- replay
- training
- latent
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a novel approach for enabling class-incremental\
  \ learning (CIL) in fully-binarized neural networks (FBNNs), addressing the challenge\
  \ of continual learning in ultra-low power edge devices. The authors introduce four\
  \ key contributions: (1) revisiting FBNN design and training for CIL, (2) exploring\
  \ loss balancing to optimize performance across tasks, (3) proposing a semi-supervised\
  \ method for pre-training feature extractors to enhance transferability, and (4)\
  \ comparing two CIL methods\u2014Latent and Native replay\u2014at iso-memory."
---

# Towards Experience Replay for Class-Incremental Learning in Fully-Binary Networks

## Quick Facts
- arXiv ID: 2503.07107
- Source URL: https://arxiv.org/abs/2503.07107
- Authors: Yanis Basso-Bert; Anca Molnos; Romain Lemaire; William Guicquero; Antoine Dupret
- Reference count: 40
- One-line result: A 3Mb fully-binarized neural network achieves comparable performance to real-valued models on class-incremental learning tasks

## Executive Summary
This paper presents a comprehensive approach for enabling class-incremental learning (CIL) in fully-binarized neural networks (FBNNs), addressing the challenge of continual learning in ultra-low power edge devices. The authors introduce four key contributions: (1) revisiting FBNN design and training for CIL, (2) exploring loss balancing to optimize performance across tasks, (3) proposing a semi-supervised method for pre-training feature extractors to enhance transferability, and (4) comparing two CIL methods—Latent and Native replay—at iso-memory. The approach is evaluated on CIFAR100 and scaled to the CORE50 dataset, demonstrating that a 3Mb FBNN achieves performance on par with or better than larger real-valued neural network models.

## Method Summary
The method combines four technical innovations for FBNN-CIL: (1) single-stage QAT with topology-derived scaling factors (K=1) replacing BatchNorm to avoid class imbalance bias; (2) inverse-frequency loss balancing with CCE to improve retention; (3) semi-supervised pre-training using Barlow Twins self-supervision plus activation regularization for better feature transferability; and (4) comparison of Latent replay (binary features stored in buffer, frozen FE) vs Native replay (raw images stored, updatable FE) at iso-memory. The 3Mb-BNN architecture uses grouped convolutions, LGAP bottleneck, and DenseSkip classifier with α-scaled output. The system is evaluated on CIFAR50+5×10 scenario and CORE50 benchmark.

## Key Results
- A 3Mb FBNN achieves comparable performance to real-valued models on CIFAR100 and CORE50 CIL tasks
- Latent replay with 50 samples/class outperforms Native replay at iso-memory, storing 13× more samples
- Inverse-frequency CCE loss balancing improves final accuracy by ~5 percentage points in replay scenarios
- Semi-supervised pre-training provides +1.17% improvement in final test accuracy
- DenseSkip architecture achieves near-identical performance to conventional Dense layers while maintaining binarization

## Why This Works (Mechanism)

### Mechanism 1
Task-agnostic scaling factors derived from network topology can replace Batch Normalization for CIL in FBNNs, avoiding BN's bias toward new classes during incremental retraining. Layer-wise scaling factors are computed as $S = K/\sqrt{\text{FanIn}}$ where $K=1$ provides unitary activation variance at initialization. This eliminates BN parameter updates that would otherwise be biased toward over-represented new classes in each retraining session. The core assumption is that binary weights and activations follow Rademacher distribution at initialization; this statistical approximation holds sufficiently through training. Evidence shows K=1 achieves 78.5% train accuracy vs 78.7% with BN in single-stage training. Break condition occurs if activation distributions deviate significantly from Rademacher during training, or if deeper architectures exhibit gradient instability without BN statistics.

### Mechanism 2
Inverse class-frequency loss balancing with Categorical Cross-Entropy significantly improves final CIL accuracy (+5pts in RPT scenarios) while slightly reducing adaptation to current tasks. Weighting loss terms by $w_i = C \times (f_i)^{-1} / \sum (f_j)^{-1}$ compensates for the inherent imbalance where current task samples outnumber buffer samples. This preferentially preserves gradients for under-represented past classes. The core assumption is that the benefit of retention outweighs the cost of reduced plasticity; this trade-off is acceptable for the target use case. Evidence shows CCE weighted achieves 45.5% vs 40.9% final accuracy for Latent replay (5Mb buffer, RPT). Break condition occurs when adaptation to new classes is the primary objective and retention is secondary, or when buffer sizes are large enough that imbalance is minimal.

### Mechanism 3
Semi-supervised pre-training combining Barlow Twins self-supervision with supervised CCE loss produces more transferable features, improving CIL final test accuracy by +1.17pts. The Barlow Twins loss $L_{SSL} = \sum_i (1 - C_{ii})^2 + \lambda \sum_{i} \sum_{j \neq i} C_{ij}^2$ forces the feature extractor to learn representations invariant to augmentation while reducing feature redundancy. Combined with activation regularization promoting distributed binary features, this prevents over-specialization to pre-training classes. The core assumption is that the feature extractor will remain fixed during CIL (Latent replay) or require minimal updates; transfer learning from pre-training to downstream tasks is feasible despite binarization. Evidence shows best configuration achieves 52.45% final test accuracy vs 51.28% supervised-only baseline. Break condition occurs when Latent replay is not used (FE must be updated), or when pre-training and downstream task distributions are too dissimilar for transfer.

## Foundational Learning

- **Concept: Straight-Through Estimator (STE) for BNN Training**
  - Why needed here: Enables gradient-based training despite non-differentiable binarization function
  - Quick check question: Can you explain why $\frac{\partial L}{\partial \theta_r} \approx \frac{\partial L}{\partial \theta_b} \mathbf{1}_{[-1,+1]}$ allows backpropagation through the SIGN function?

- **Concept: Experience Replay for Catastrophic Forgetting**
  - Why needed here: Core CIL strategy this paper extends to FBNNs
  - Quick check question: What is the trade-off between storing raw inputs (Native replay) vs. latent features (Latent replay) in terms of memory efficiency and model reconfigurability?

- **Concept: Quantization-Aware Training (QAT)**
  - Why needed here: The training paradigm for BNNs requiring proxy full-precision weights
  - Quick check question: Why must full-precision weights $\theta_r$ be maintained alongside binary weights $\theta_b$ during training?

## Architecture Onboarding

- **Component map:**
  Input [8-bit RGB] → Thermometer Encoding [T_YCC-16] → Binary Input [32×32×4N 1-bit]
       ↓
  Encoder: ConvBlocks (128→256→512 filters, grouped) + MaxPooling + Scaling
       ↓
  LGAP (Learnable Global Average Pooling): GAP path || DWC path → Concatenate → Dense → 1024-dim latent
       ↓
  Classifier: DenseSkip blocks → QuantDense → α-scaled binarization → Softmax → Output
  Memory buffer (M) stores either: raw inputs (Native) or 1024-dim binary latents (Latent replay)

- **Critical path:**
  1. Scaling factor initialization ($K=1$) at network construction—incorrect values cause training collapse
  2. α-scaling at final layer: $\alpha = 1/\sqrt{c \times \text{FanIn} \times N_c}$ where $c \approx 5$—prevents gradient instability with many classes
  3. Loss balancing weights computed per retraining session based on buffer/task sample counts

- **Design tradeoffs:**
  - **Latent vs. Native replay (iso-memory <25Mb):** Latent stores 13× more samples but locks FE; Native enables FE updates but stores fewer samples
  - **SH vs. CCE loss:** SH favors adaptation (higher $a_{new}$) but degrades retention; CCE with weighting balances both
  - **Model size:** 3Mb-FBNN sufficient for CIFAR-100; CORE50 requires 3Mb-Res-BNN (deeper, skip connections)

- **Failure signatures:**
  - Training loss caps early → scaling factors incorrect or α-scaling missing for last layer
  - High $a_{new}$, near-zero $a_{old}$ → SH loss without weighting; or replay buffer corrupted/empty
  - $a_{final}$ degrades with larger buffer (Latent replay) → FE capacity exhausted; switch to Native or larger model
  - Convergence instability at learning rate >10⁻³ → inherent to FBNN; reduce LR and increase epochs

- **First 3 experiments:**
  1. **Baseline validation:** Train 3Mb-BNN offline on CIFAR-100 with K=1 scaling; verify train accuracy ~78% matches Table 2. If lower, debug scaling factor initialization.
  2. **Naive vs. Cumulative CIL:** Run CIFAR50+5×10 scenario (naive, naive-reset, cumulative, cumulative-reset baselines) to reproduce Figure 3 curves. This establishes adaptation/retention bounds.
  3. **Loss comparison at fixed buffer:** Compare CCE, FCCE, SH (weighted/unweighted) on Latent replay with 5Mb buffer in FPT scenario. Verify CCE weighted achieves ~51-52% $a_{final}$ per Table 5.

## Open Questions the Paper Calls Out

### Open Question 1
How can data augmentation be implemented effectively within the binary latent space to enhance Latent replay performance? The paper notes that "contrary to Native replay, in Latent replay there is no conventional way to do data augmentation from a binary latent space," which limits the regularization benefits available to the Native approach. The discrete nature of binary features prevents standard geometric transformations used in real-valued spaces, yet augmentation is vital for compact networks to avoid overfitting on the buffer. A method allowing lossless or beneficial transformations of binary latent vectors that results in improved retention ($a_{old}$) and adaptation ($a_{new}$) compared to static Latent replay would resolve this.

### Open Question 2
How can the "aging problem" of the feature extractor be mitigated in Latent replay without invalidating the stored binary buffer? The paper explicitly identifies this problem, noting that "the latent representation can not be updated... otherwise the samples in the buffer would not be representative anymore." Latent replay requires a fixed feature extractor to maintain buffer consistency, but this limits the model's ability to adapt its representations to significantly different future tasks. A mechanism that allows for feature extractor updates while transforming the memory buffer to remain consistent with the new FE would resolve this.

### Open Question 3
Will the proposed semi-supervised pre-training and scaling factor design scale efficiently to complex, high-resolution datasets like ImageNet? The paper notes that the "accuracy gap remains large on challenging ones like ImageNet," and the evaluation is limited to CIFAR100 and CORE50 (128x128 pixels). It is unclear if the 3Mb-Res-BNN architecture and the specific semi-supervised pre-training loss possess sufficient capacity for ImageNet-scale complexity. Successful application on ImageNet would resolve this.

## Limitations
- Scaling factor approach assumes Rademacher-distributed activations which may not hold in deeper architectures
- Effectiveness of inverse-frequency loss balancing depends heavily on buffer size and class distribution assumptions
- Semi-supervised pre-training benefits shown only on a single dataset configuration, limiting generalizability
- DenseSkip architectural details remain underspecified, particularly the "null projections" mechanism
- CORE50 results presented for a single configuration without ablation studies

## Confidence

- **High confidence:** Basic FBNN training methodology (STE, scaling factors), comparative performance gains over real-valued models
- **Medium confidence:** Loss balancing effectiveness, Latent vs Native replay trade-offs at iso-memory
- **Low confidence:** Semi-supervised pre-training benefits, DenseSkip architectural details, CORE50 scalability claims

## Next Checks

1. Verify scaling factor initialization by training a baseline FBNN on CIFAR-100 with K=1 and confirming ~78% accuracy matches Table 2.

2. Replicate the CIFAR50+5×10 scenario comparing naive vs cumulative CIL methods to reproduce Figure 3 adaptation/retention curves.

3. Implement CCE with inverse-frequency weighting for Latent replay at 5Mb buffer and confirm ~52% final accuracy matches Table 5.