---
ver: rpa2
title: Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline
  Reinforcement Learning
arxiv_id: '2506.11172'
source_url: https://arxiv.org/abs/2506.11172
tags:
- learning
- data
- offline
- attack
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes how insufficient sequence-level data-policy
  coverage amplifies estimation errors in offline RL and exploits this vulnerability
  through a poisoning attack. The authors introduce a sequence-level concentrability
  coefficient that theoretically shows exponential error growth when coverage is low.
---

# Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.11172
- Source URL: https://arxiv.org/abs/2506.11172
- Reference count: 25
- This work analyzes how insufficient sequence-level data-policy coverage amplifies estimation errors in offline RL and exploits this vulnerability through a poisoning attack.

## Executive Summary
This paper addresses a critical vulnerability in offline reinforcement learning (RL) systems where insufficient sequence-level data-policy coverage can be exploited through poisoning attacks. The authors introduce a sequence-level concentrability coefficient that theoretically demonstrates how estimation errors grow exponentially when coverage is low. They develop a practical attack methodology that discretizes continuous state-action spaces into decision units, identifies rare patterns, and minimally perturbs the dataset to eliminate these patterns, thereby reducing coverage. Experimental results show that poisoning just 1% of the data with 5% perturbation magnitude can degrade agent performance by 90%, significantly outperforming existing attack methods.

## Method Summary
The authors develop a poisoning attack targeting the sequence-level data-policy coverage in offline RL. The approach involves discretizing the continuous state-action space into decision units, extracting rare decision patterns from the dataset, and then minimally perturbing the data to eliminate these patterns. The attack exploits the theoretical relationship between sequence-level coverage and estimation error amplification, showing that removing rare patterns significantly degrades the agent's performance while remaining stealthy. The method is validated through experiments on continuous control tasks, demonstrating superior effectiveness compared to existing attack approaches.

## Key Results
- Poisoning 1% of the dataset with 5% perturbation magnitude can degrade agent performance by 90%
- The attack outperforms existing methods in effectiveness
- The attack remains stealthy while being highly effective even with limited data access
- Experimental validation demonstrates practical feasibility of the theoretical vulnerability

## Why This Works (Mechanism)
The attack exploits a fundamental weakness in offline RL where estimation errors grow exponentially when sequence-level data-policy coverage is insufficient. By identifying and eliminating rare decision patterns through minimal perturbations, the attack artificially reduces the coverage concentrability coefficient. This forces the learning algorithm to extrapolate beyond the support of the data, leading to catastrophic performance degradation. The stealthiness comes from the minimal perturbation magnitude required to achieve significant impact, making the attack difficult to detect through standard anomaly detection methods.

## Foundational Learning
- **Sequence-level data-policy coverage**: Measures how well the dataset represents the policy's state-action visitation distribution over entire trajectories. Why needed: Critical for understanding when offline RL agents can safely learn without dangerous extrapolation. Quick check: Compare dataset trajectory distribution to policy trajectory distribution using KL divergence.

- **Concentrability coefficient**: Mathematical measure of the mismatch between data distribution and target policy distribution. Why needed: Quantifies the theoretical upper bound on estimation error in offline RL. Quick check: Compute the maximum ratio of state-action distributions between any two policies in the dataset.

- **Decision unit discretization**: Process of converting continuous state-action spaces into discrete units for pattern analysis. Why needed: Enables systematic identification of rare patterns that can be targeted for removal. Quick check: Verify that discretization preserves key behavioral patterns through visualization.

- **Poisoning attack optimization**: Techniques for finding minimal perturbations that maximally degrade performance. Why needed: Balances attack effectiveness with stealth requirements. Quick check: Measure perturbation magnitude distribution against natural data variation.

- **Offline RL error amplification**: Phenomenon where small errors compound exponentially in the absence of adequate coverage. Why needed: Explains why the attack can be so devastating with minimal changes. Quick check: Track error growth along trajectories in low-coverage regions.

## Architecture Onboarding

Component map: Data preprocessing -> Decision unit discretization -> Rare pattern extraction -> Perturbation optimization -> Dataset modification -> Agent training

Critical path: The attack follows a sequential pipeline where each component depends on the previous one. Data preprocessing prepares the dataset, discretization creates the decision units, pattern extraction identifies targets, perturbation optimization computes minimal changes, and dataset modification implements the attack. The agent training then reveals the impact.

Design tradeoffs: The authors balance attack effectiveness against stealth by minimizing perturbation magnitude while maximizing coverage reduction. They also trade computational complexity for attack precision through the discretization granularity. The choice of rare pattern extraction method versus comprehensive coverage analysis represents another key tradeoff.

Failure signatures: Attacks fail when the perturbation magnitude exceeds natural data variation thresholds, when the discretization granularity is too coarse to capture meaningful patterns, or when the dataset already has sufficient coverage in critical regions. Performance degradation may plateau if too many rare patterns are already absent from the data.

First experiments:
1. Baseline evaluation: Train standard offline RL agents on clean datasets to establish performance baselines.
2. Coverage analysis: Measure sequence-level coverage before and after attack implementation to verify the mechanism.
3. Ablation study: Test attack effectiveness with varying perturbation magnitudes, dataset proportions, and discretization granularities.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on a specific concentration coefficient formulation that may not capture all practical scenarios in complex continuous control tasks
- Discretization process for identifying rare decision units could introduce approximation errors, particularly in high-dimensional state-action spaces
- Assumption that removing rare patterns significantly degrades coverage may not hold uniformly across different MDP structures and reward landscapes
- Attack effectiveness demonstrated primarily on benchmark continuous control tasks with limited validation on real-world datasets

## Confidence

High confidence:
- Experimental results showing performance degradation under the proposed attack method, particularly the 90% performance drop with 1% data poisoning and 5% perturbation magnitude
- Comparison with existing methods showing superior effectiveness is well-supported

Medium confidence:
- Theoretical analysis connecting sequence-level data-policy coverage to estimation errors through the concentration coefficient
- Stealthiness claim of the attack, as this is evaluated primarily through perturbation magnitude metrics rather than comprehensive adversarial detection frameworks

## Next Checks

1. Cross-environment validation: Test the attack across a broader range of environments including discrete control tasks, partially observable environments, and real-world datasets to assess generalizability beyond the current continuous control benchmarks.

2. Defense mechanism evaluation: Implement and evaluate potential defense mechanisms against the proposed attack, such as anomaly detection in decision unit distributions or robust coverage estimation techniques, to establish the attack's practical exploitability.

3. Scaling analysis: Investigate how the attack's effectiveness scales with dataset size, dimensionality, and decision unit granularity to determine practical limits and optimal attack parameters for different application contexts.