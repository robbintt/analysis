---
ver: rpa2
title: Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving
arxiv_id: '2501.13563'
source_url: https://arxiv.org/abs/2501.13563
tags:
- adversarial
- attack
- driving
- attacks
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CAD, the first black-box adversarial attack
  framework specifically designed for vision-language models in autonomous driving.
  CAD addresses two key challenges: ensuring attack effectiveness across the driving
  reasoning chain and adapting to dynamic driving contexts.'
---

# Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving

## Quick Facts
- arXiv ID: 2501.13563
- Source URL: https://arxiv.org/abs/2501.13563
- Reference count: 40
- Key outcome: Introduces CAD, the first black-box adversarial attack framework specifically designed for vision-language models in autonomous driving, achieving 13.43% average performance reduction and 61.11% route completion drop in real-world tests.

## Executive Summary
This paper presents CAD (Cascading Adversarial Disruption), the first black-box adversarial attack framework specifically designed for vision-language models (VLMs) in autonomous driving. The framework addresses two key challenges: ensuring attack effectiveness across the driving reasoning chain and adapting to dynamic driving contexts. CAD employs Decision Chain Disruption to target low-level reasoning breakdown and Risky Scene Induction to construct high-level risky scenarios using a surrogate VLM. Extensive experiments demonstrate CAD's superiority over existing methods, with real-world attacks on AD vehicles resulting in significant performance degradation.

## Method Summary
CAD is a black-box adversarial attack framework that generates imperceptible visual perturbations to fool VLMs in autonomous driving systems. The method uses a three-component loss function: Decision Chain Disruption (aligning adversarial images with deceptive text via CLIP), Risky Scene Induction (flipping safety classification using safe/unsafe descriptors), and Semantic Discrepancy (maximizing distance between clean and adversarial features). The attack optimizes perturbations using gradient-based methods on a surrogate model (CLIP) and transfers them to target AD VLMs. The framework was tested on multiple benchmarks including Dolphins, DriveLM-NuScenes, and LangAuto.

## Key Results
- CAD achieves an average performance reduction of 13.43% compared to existing methods
- Real-world attacks on AD vehicles resulted in a 61.11% drop in route completion rate
- The framework successfully induced vehicles to crash into obstacles in controlled experiments
- CAD released the CADA dataset containing 18,808 adversarial visual-question-answer pairs

## Why This Works (Mechanism)

### Mechanism 1: Decision Chain Disruption via Semantic Alignment
The attack generates "deceptive texts" using GPT-4o and optimizes perturbations to maximize cosine similarity between adversarial images and these deceptive texts in CLIP's embedding space. This exploits the assumption that errors in early perceptual stages propagate through the VLM's reasoning chain to affect final driving actions.

### Mechanism 2: Risky Scene Induction via Context Probability Flipping
The framework manipulates the probability distribution of scene-level descriptors to force the model to interpret safe scenes as unsafe (or vice versa). This targets high-level context understanding rather than specific object-level checks, bypassing rigorous logic or rule-based checks.

### Mechanism 3: Transferability via Gradient-Based Optimization on Surrogates
Black-box attacks succeed without target model access because adversarial gradients computed on the surrogate CLIP model generalize to target VLMs. This relies on the assumption that modern VLMs share similar visual foundations (often CLIP or ViT variants).

## Foundational Learning

- **Adversarial Transferability**: Essential because CAD is a black-box attack relying on cross-model gradient generalization. Quick check: Does attack performance drop significantly on target models with different visual backbones compared to similar ones?

- **Multimodal Alignment Space (CLIP)**: Critical because the attack optimizes perturbations in the joint text-image embedding space. Quick check: Would the semantic alignment loss still be effective if CLIP is replaced with a model that doesn't use contrastive loss?

- **Driving Reasoning Chain (Perception-Prediction-Plan)**: Fundamental because CAD specifically targets decision chain disruption. Quick check: Why does the paper generate "Reversal Reasoning" text instead of just wrong labels?

## Architecture Onboarding

- **Component map**: Surrogate Model (CLIP) -> Auxiliary Generator (GPT-4o) -> Optimizer (Momentum-based GD) -> Target (AD VLMs)

- **Critical path**:
  1. Input: Clean driving image + standard text prompt
  2. Text Generation: GPT-4o generates deceptive reasoning text
  3. Feature Extraction: CLIP encodes clean image and deceptive text
  4. Optimization Loop: Calculate Loss -> Update perturbation -> Project to stay within ε-ball
  5. Application: Add optimized perturbation to image -> Feed to target VLM

- **Design tradeoffs**: Surrogate capacity vs. generation time, hyperparameter balancing (α=0.75, β=0.05), perturbation budget vs. imperceptibility

- **Failure signatures**: Low transferability (high surrogate loss but low target impact), defensive filtering (model refusing "weird" inputs), semantic drift (incoherent generated text)

- **First 3 experiments**:
  1. Ablation study on Dolphins benchmark with β=0 and γ=0 to isolate Decision Chain Disruption impact
  2. Cross-model transfer test using CLIP surrogate against non-AD VLM to check generalization
  3. Physical robustness check printing adversarial patches for toy car/sign testing

## Open Questions the Paper Calls Out

- How can adversarial training be effectively implemented as a defense mechanism for AD VLMs against black-box attacks like CAD?
- Can textual domain perturbations complement visual adversarial attacks to create a more comprehensive threat model for AD VLMs?
- To what extent can visually inconspicuous camouflage-based attacks achieve comparable effectiveness to current CAD perturbations against AD VLMs?
- How effective are CAD attacks against commercial/proprietary AD systems compared to open-source research models?

## Limitations

- Heavy reliance on prompt quality for GPT-4o's generated deceptive texts, with exact prompt structure not provided
- Transferability assumptions based on shared CLIP-like foundations without testing against fundamentally different visual encoders
- Physical world feasibility not fully addressed, with limited testing on varied real-world conditions and hardware

## Confidence

- **High Confidence**: CAD framework novelty and first black-box attack for VLM-AD; experimental results showing 13.43% performance reduction
- **Medium Confidence**: Decision Chain Disruption mechanism plausibility; transferability claims supported by experiments but lacking robustness testing
- **Low Confidence**: Real-world attack success (61.11% route completion drop) demonstrated on single platform; generalizability to production vehicles unclear

## Next Checks

1. Cross-architecture transferability test: Generate perturbations using CAD framework and test against a CNN-based VLM to quantify impact of visual encoder architecture on transferability

2. Prompt structure isolation: Conduct ablation study varying the "Reversal Reasoning Template" prompt complexity to measure contribution to attack success

3. Physical robustness validation: Print adversarial perturbations and test under varying real-world conditions (lighting, distances, angles) to assess practical feasibility of physical attacks