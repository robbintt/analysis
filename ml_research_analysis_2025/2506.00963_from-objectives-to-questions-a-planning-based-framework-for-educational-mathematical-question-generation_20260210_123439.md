---
ver: rpa2
title: 'From Objectives to Questions: A Planning-based Framework for Educational Mathematical
  Question Generation'
arxiv_id: '2506.00963'
source_url: https://arxiv.org/abs/2506.00963
tags:
- question
- educational
- objectives
- questions
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EQPR, a planning-based framework for educational
  mathematical question generation that addresses the challenge of creating questions
  aligned with specific educational objectives. The approach combines Monte Carlo
  Tree Search with large language models to iteratively optimize questions through
  a "plan-evaluate-optimize" cycle, inspired by teachers' iterative question design
  processes.
---

# From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation

## Quick Facts
- arXiv ID: 2506.00963
- Source URL: https://arxiv.org/abs/2506.00963
- Reference count: 21
- Primary result: Planning-based framework combining MCTS with LLMs achieves 46.23% win rate and 43.11% pass rate on educational mathematical question generation

## Executive Summary
This paper introduces EQPR, a planning-based framework that generates educational mathematical questions aligned with specific learning objectives. The approach addresses the challenge of creating questions that meet multi-dimensional pedagogical requirements by iteratively refining questions through a "plan-evaluate-optimize" cycle. The framework combines Monte Carlo Tree Search with large language models to explore the question design space systematically, guided by a Critic module that evaluates questions against educational objectives and a Reflection module that generates improvements based on feedback.

## Method Summary
The EQPR framework formalizes educational question generation as a Markov Decision Process where each state represents a question version and actions represent editing operations. It uses MCTS with Upper Confidence Bounds (UCT) to explore the question space, balancing exploitation of high-reward paths with exploration of new variations. The Critic module evaluates questions across multiple dimensions (concept coverage, cognitive level alignment, competency development) using chain-of-thought prompting, while the Reflection module generates improved questions by reasoning over the trajectory of previous states and feedback. The framework iterates through selection, expansion, simulation, and backpropagation steps, with parameters including 4 iterations, maximum depth of 3, exploration constant c=2.5, and expansion width k=4.

## Key Results
- EQPR achieves up to 46.23% win rate and 43.11% pass rate on EduMath-CQ dataset, significantly outperforming existing reasoning methods
- The framework shows strong performance across different LLM backbones (DeepSeek-V3, GPT-4o-Mini) with consistent improvements over baseline approaches
- Contextual questions (EduMath-CQ) are more challenging than standard questions, but EQPR maintains effectiveness with 40.52% win rate

## Why This Works (Mechanism)

### Mechanism 1: MCTS-Guided Search with LLM State Expansion
Combining Monte Carlo Tree Search with LLM generation enables systematic exploration of the question design space that single-pass methods cannot achieve. The framework models question refinement as a tree where each node represents a question state and edges represent editing actions. MCTS uses UCT selection to balance exploitation (high-scoring paths) with exploration (under-visited nodes), expands via the Reflection module, simulates forward to estimate cumulative rewards, and backpropagates to update Q-values.

### Mechanism 2: Critic-Guided Multi-Dimensional Evaluation
A Critic module using chain-of-thought prompting provides structured feedback and numerical scores that meaningfully guide subsequent refinements. The Critic receives a question state and educational objectives, then uses an LLM with a structured prompt to evaluate across dimensions (concept coverage, cognitive level alignment, competency development). It outputs a score (1-10) and actionable modification directions.

### Mechanism 3: Reflection with Historical Trajectory Context
Maintaining a trajectory of prior question states and modification suggestions enables the Reflection module to generate targeted improvements rather than random variations. The Reflection module constructs τ = {(s₀, a₀), ..., (sₜ₋₁, aₜ₋₁)} capturing all previous states and Critic feedback. It then generates sₜ₊₁ by reasoning over this history, current feedback, and educational objectives.

## Foundational Learning

- **Markov Decision Process (MDP)**
  - Why needed here: The paper formalizes question improvement as an MDP tuple (S, A, T, R), which is essential for understanding why MCTS applies to this problem.
  - Quick check question: Why does the state space S represent evolving question versions rather than static problem features?

- **Upper Confidence Bounds applied to Trees (UCT)**
  - Why needed here: UCT governs node selection in MCTS, balancing exploration of new question structures against exploitation of high-reward paths.
  - Quick check question: If exploration constant c is set too low, what failure mode would you expect in generated questions?

- **Bloom's Taxonomy**
  - Why needed here: Educational objectives include cognitive levels (understanding, application, analysis), which the Critic must evaluate and the Reflection module must preserve.
  - Quick check question: How would you distinguish a question targeting "application" level from one targeting "analysis" level?

## Architecture Onboarding

- **Component map:**
  Input: Educational objectives O (concept, core quality, core ability, Bloom level, context) -> Initial generation prompt -> MCTS loop (Critic evaluates -> Reflection expands children -> backpropagate rewards) -> Output: Final question selected from highest-reward trajectory

- **Critical path:**
  1. Parse educational objectives into structured format
  2. Generate initial question s₀ via initial generation prompt
  3. Run MCTS loop: Critic evaluates → Reflection expands children → backpropagate rewards
  4. Select best question from highest-reward trajectory

- **Design tradeoffs:**
  - Iteration count (4) vs. API cost: More iterations improve Pass Rate but increase cost (Fig 4 shows $0.01–$0.10 per question depending on backbone)
  - Expansion width k vs. diversity: Wider expansion explores more variants but slows convergence
  - Depth limit (3) vs. drift risk: Deeper trees enable longer refinement chains but may drift from original objectives

- **Failure signatures:**
  - High Win Rate but low Pass Rate: Questions are linguistically fluent but miss required educational objectives
  - Solvability < 85%: Critic is accepting questions without verifying mathematical validity
  - Score plateau in early iterations: Reflection prompt may be generating generic or repetitive feedback

- **First 3 experiments:**
  1. Reproduce Table 1 on a 100-sample subset of EduMath-CQ using DeepSeek-V3, comparing EQPR vs. CoT on Pass Rate and Win Rate.
  2. Ablate the Reflection module (use direct generation without trajectory context) and quantify Win Rate drop (expect ~4–5% based on Fig 3).
  3. Stress-test with conflicting objectives (e.g., "simple recall" + "complex modeling") and verify whether Critic flags the inconsistency or produces incoherent questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the EQPR framework be generalized to non-mathematical disciplines that possess distinct pedagogical structures and constraints?
- Basis in paper: The authors state in the Limitations section that their "research primarily focused on mathematical question generation and has not yet been extended to other subject areas," marking this as a direction for future research.
- Why unresolved: The current Critic and Reflection modules are tailored for mathematical logic and concepts; adapting them for subjects like history or literature requires redefining "educational objectives" and "solvability" in non-quantitative terms.
- What evidence would resolve it: Successful application and evaluation of the EQPR framework on datasets from non-STEM domains, such as history or language arts, demonstrating statistically significant improvements over baselines.

### Open Question 2
- Question: How can question difficulty be systematically quantified and integrated into the MCTS planning process?
- Basis in paper: The paper notes that "difficulty evaluation is largely subjective and challenging to standardize," and identifies the "assessment of question difficulty" as a "significant hurdle in the field that requires further investigation."
- Why unresolved: The current framework focuses on alignment with concepts and Bloom's taxonomy levels, but it lacks a mechanism to predict or control the cognitive load or difficulty level of the generated questions for specific student cohorts.
- What evidence would resolve it: The development of a "difficulty" reward signal that correlates strongly with human expert ratings or student performance data (e.g., response times and error rates).

### Open Question 3
- Question: What methods can effectively mitigate LLM evaluator bias to ensure automated assessments align with human pedagogical judgment?
- Basis in paper: The authors acknowledge that while they use LLMs for evaluation, these models "may exhibit certain biases, and their assessment results do not always align perfectly with the professional judgment of human educators."
- Why unresolved: Relying on LLMs (like DeepSeek-V3 in this study) to evaluate the output of other LLMs risks model-specific biases or hallucinations regarding educational quality, creating a "blind leading the blind" scenario.
- What evidence would resolve it: A calibration study showing that a refined evaluation protocol achieves a Fleiss’ Kappa or correlation coefficient above 0.8 when compared to a panel of human educational experts.

## Limitations
- The framework has not been extended beyond mathematical question generation to other subject areas, limiting its generalizability.
- Difficulty evaluation remains subjective and challenging to standardize, preventing systematic control of question cognitive load.
- LLM-based evaluation may exhibit biases that do not align perfectly with human pedagogical judgment, raising concerns about reliability.

## Confidence

- **High Confidence**: The fundamental approach of using iterative refinement cycles for educational question generation is well-supported by teacher practices and existing multi-agent QG literature. The EQGEVAL benchmark design using LLM-as-judge with self-consistency is methodologically sound.
- **Medium Confidence**: The specific MCTS implementation details and their effectiveness for this domain are plausible but lack direct validation. The reported win rates (46.23%) and pass rates (43.11%) appear strong but cannot be independently verified without the datasets.
- **Low Confidence**: The generalization claims to other educational domains and grade levels are not tested. The paper does not address potential bias in LLM-generated questions or the reliability of LLM evaluators for complex mathematical reasoning.

## Next Checks

1. **Dataset Availability Verification**: Confirm whether EduMath and EQGEVAL benchmarks will be released publicly, as this is essential for community validation and benchmarking.

2. **Statistical Significance Testing**: Request the authors provide confidence intervals and p-values for their main results across multiple runs, given the stochastic nature of their approach.

3. **Cross-Domain Generalization**: Test whether the EQPR framework maintains performance when applied to non-mathematical educational domains (e.g., physics, chemistry) or different grade levels, to validate the claimed generalizability.