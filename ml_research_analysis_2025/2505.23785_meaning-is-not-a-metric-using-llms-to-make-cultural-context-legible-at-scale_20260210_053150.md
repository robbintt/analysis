---
ver: rpa2
title: 'Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale'
arxiv_id: '2505.23785'
source_url: https://arxiv.org/abs/2505.23785
tags:
- meaning
- social
- human
- thick
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) to make
  human meaning legible at scale in AI-based sociotechnical systems. The authors argue
  that current systems rely on "thin descriptions" (numerical metrics that strip away
  cultural context) and cannot adequately represent human meaning.
---

# Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale

## Quick Facts
- arXiv ID: 2505.23785
- Source URL: https://arxiv.org/abs/2505.23785
- Authors: Cody Kommers; Drew Hemment; Maria Antoniak; Joel Z. Leibo; Hoyt Long; Emily Robinson; Adam Sobey
- Reference count: 40
- Primary result: LLMs can partially automate generating and processing "thick descriptions" to scale cultural context analysis in AI systems

## Executive Summary
This paper proposes using large language models (LLMs) to make human meaning legible at scale in AI-based sociotechnical systems. The authors argue that current systems rely on "thin descriptions" (numerical metrics that strip away cultural context) and cannot adequately represent human meaning. By contrast, humanities scholars use "thick descriptions" (verbal representations that retain contextual information) to capture meaning. The paper identifies five key challenges: preserving context, maintaining interpretive pluralism, integrating lived experience with critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static.

## Method Summary
The paper proposes a conceptual framework for using LLMs to generate and process "thick descriptions" - verbal representations that retain cultural context and meaning. The method involves leveraging LLMs' natural language capabilities to partially automate what has traditionally been a manual process in humanities scholarship. The authors outline five key challenges that must be addressed when implementing this approach: preserving context, maintaining interpretive pluralism, integrating lived experience with critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static.

## Key Results
- LLMs can partially automate the generation of "thick descriptions" that preserve cultural context
- The approach addresses the limitation of current AI systems that rely on "thin descriptions" (numerical metrics)
- Five key challenges identified for implementing LLM-based thick description systems

## Why This Works (Mechanism)
The mechanism relies on LLMs' ability to process and generate natural language at scale, enabling them to capture nuances and contextual information that traditional metrics miss. By leveraging LLMs' understanding of language patterns and relationships, the system can generate descriptions that maintain cultural context while being produced at a scale that makes them practical for AI systems. This approach bridges the gap between quantitative metrics and qualitative understanding.

## Foundational Learning
- **Thick vs Thin Descriptions**: Understanding the distinction between detailed, context-rich descriptions versus stripped-down numerical metrics is crucial for implementing this approach
  - *Why needed*: Provides the theoretical foundation for why traditional metrics fail to capture human meaning
  - *Quick check*: Can identify examples of both types in existing systems

- **Cultural Context Preservation**: The ability to maintain cultural nuances and meanings in automated descriptions
  - *Why needed*: Ensures the generated descriptions remain meaningful and accurate
  - *Quick check*: Verify that generated descriptions include cultural references and contextual information

- **Interpretive Pluralism**: Supporting multiple interpretations and perspectives in the generated descriptions
  - *Why needed*: Prevents oversimplification and acknowledges the complexity of human meaning
  - *Quick check*: Confirm that different perspectives are represented in example outputs

## Architecture Onboarding
- **Component Map**: Data Input -> LLM Processing -> Thick Description Generation -> Context Preservation Layer -> Output Layer
- **Critical Path**: Raw data flows through LLM for analysis, then through context preservation mechanisms before final output
- **Design Tradeoffs**: Balancing scale and automation with accuracy and cultural sensitivity
- **Failure Signatures**: Loss of cultural context, oversimplification, or generation of biased descriptions
- **First Experiments**:
  1. Test LLM's ability to generate culturally-aware descriptions for simple datasets
  2. Evaluate the preservation of context across different cultural contexts
  3. Measure the system's ability to maintain multiple interpretations

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks empirical evidence demonstrating that LLMs actually preserve cultural context and interpretive pluralism at scale
- Unclear how LLMs would balance lived experience with critical distance
- Practical implementation details for deploying LLM-generated thick descriptions in real-world systems are underdeveloped

## Confidence
*High Confidence:* The philosophical framework distinguishing thin versus thick descriptions is well-established in anthropology and humanities scholarship. The identified challenges are conceptually sound.

*Medium Confidence:* The premise that LLMs can now partially automate thick description generation is plausible but requires empirical validation. The specific mechanisms for preserving context and pluralism need clarification.

*Low Confidence:* The practical implementation details for deploying LLM-generated thick descriptions in real-world sociotechnical systems are underdeveloped. The paper lacks discussion of potential biases in LLM interpretations of cultural meaning.

## Next Checks
1. Conduct a comparative study where anthropologists generate thick descriptions for a sample dataset, then have LLMs generate descriptions for the same data, followed by expert evaluation of the LLM outputs' accuracy and contextual richness.

2. Develop and test a prototype system that implements LLM-generated thick descriptions alongside traditional metrics in an actual AI application, measuring impacts on system performance and user understanding.

3. Design experiments to test the stability and evolution of LLM-generated thick descriptions over time, particularly when cultural contexts shift or new information emerges.