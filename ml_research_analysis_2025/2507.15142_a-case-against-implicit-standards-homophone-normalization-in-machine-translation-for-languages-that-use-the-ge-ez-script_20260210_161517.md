---
ver: rpa2
title: 'A Case Against Implicit Standards: Homophone Normalization in Machine Translation
  for Languages that use the Ge''ez Script'
arxiv_id: '2507.15142'
source_url: https://arxiv.org/abs/2507.15142
tags:
- normalization
- amharic
- characters
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that homophone normalization, which maps characters
  with the same sound to a single character, sets an implicit standard for Ge'ez script
  languages and harms cross-lingual transfer learning. The authors experiment with
  monolingual and cross-lingual transfer for Amharic, Tigrinya, and Ge'ez machine
  translation.
---

# A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script

## Quick Facts
- arXiv ID: 2507.15142
- Source URL: https://arxiv.org/abs/2507.15142
- Reference count: 23
- This paper argues that homophone normalization, which maps characters with the same sound to a single character, sets an implicit standard for Ge'ez script languages and harms cross-lingual transfer learning.

## Executive Summary
This paper argues that homophone normalization in machine translation for Ge'ez script languages (Amharic, Tigrinya, Ge'ez) sets an implicit linguistic standard that harms cross-lingual transfer learning and language preservation. The authors experiment with monolingual and cross-lingual transfer for Amharic, Tigrinya, and Ge'ez machine translation, comparing three normalization strategies: No-Norm (baseline), H-Only (homophones only), and HSL (homophones + similar sounds + labialized). They propose post-inference normalization, applying normalization only during metric calculation rather than to training data, which improves BLEU scores by up to 1.03 while preserving language features. Results show that normalization during training reduces vocabulary size and negatively impacts cross-lingual transfer, as models trained on normalized data cannot handle alternative spellings valid in related languages.

## Method Summary
The authors evaluate machine translation for Ge'ez script languages using two model types: Transformer models trained from scratch and NLLB-600M models fine-tuned on Amharic-English parallel data. They use three normalization settings: No-Norm (baseline), H-Only (homophones only), and HSL (homophones + similar sounds + labialized). Training uses the Adam optimizer with learning rate 1e-4 and cosine annealing scheduler for Transformers, while NLLB models use learning rate 5e-5. They evaluate on filtered test sets containing high homophone character counts (≥9 for Amharic, ≥17 for Tigrinya) using BLEU and ChrF metrics, with post-inference normalization applied to predictions and references during evaluation.

## Key Results
- Post-inference normalization improves BLEU scores by up to 1.03 while preserving language features in training
- Models trained on normalized data do not generalize well to other languages, with HSL normalization reducing transfer BLEU from 12.16 to 10.67
- Normalized models produce fewer unique words and tend to repeat characters/words until maximum sequence length

## Why This Works (Mechanism)

### Mechanism 1
Applying homophone normalization to training data reduces vocabulary size but restricts the model's ability to handle valid alternative spellings within the same language. By mapping acoustically identical characters to a single canonical character during pre-processing, the token vocabulary shrinks (e.g., by 16% cited in prior work), simplifying the learning task. However, this removes orthographic variance from the training distribution, creating an implicit standard where the model assigns zero probability to valid but normalized-out spellings. Core assumption: valid alternative spellings exist in natural usage and a model should accommodate them rather than forcing users to adapt to the model's internal standard.

### Mechanism 2
Pre-processing normalization optimized for one language (Amharic) degrades cross-lingual transfer performance to related languages (Tigrinya, Ge'ez) that share the script but maintain phonemic distinctions for those characters. In Amharic, certain characters are homophones, but in Tigrinya these same characters represent distinct phonemes. If a multilingual model is trained on normalized Amharic data where these distinctions are collapsed, it loses the representational capacity to generate or discriminate these characters correctly when transferring to Tigrinya or Ge'ez. Core assumption: the model relies on shared representations across languages using the Ge'ez script, meaning destructive interference in one language negatively impacts the others.

### Mechanism 3
Post-inference normalization—applying normalization only to model predictions and references during evaluation—increases automatic metric scores (BLEU) without degrading the model's internal language representation. Evaluation metrics like BLEU require exact string matches. A model that correctly translates meaning but uses a different valid homophone character is penalized. By normalizing the prediction to match the reference only at the scoring step, the evaluation metric reflects semantic/phonetic success rather than strict orthographic adherence, while the model itself retains the full character set distribution learned during training. Core assumption: the goal is to maximize automatic metric utility while preserving the model's ability to learn natural language variance.

## Foundational Learning

- **Concept: Abugida Writing Systems (Ge'ez Script)** - Why needed here: The paper revolves around characters in the Ge'ez script which represent consonant-vowel combinations (CV). Understanding that variations of a base consonant exist is essential to grasping why "homophones" are distinct visual characters that can be merged. Quick check question: In an Abugida, if you normalize two distinct characters that share the same consonant base but different vowels, are you collapsing semantic meaning or just phonetic representation?

- **Concept: Zero-Shot vs. Cross-Lingual Transfer** - Why needed here: The paper evaluates how models handle Tigrinya and Ge'ez when they have been pre-trained or fine-tuned on Amharic. This relies on the concept of transfer, where knowledge from a source task/language is applied to a target. Quick check question: If a model is trained on Amharic where Character A and Character B are mapped to the same ID, can it theoretically generate Character B uniquely when translating to Tigrinya where A and B are distinct sounds?

- **Concept: Implicit Standardization in NLP** - Why needed here: The core argument is sociotechnical: technical pre-processing choices force a specific linguistic standard on users, potentially marginalizing dialects or spellings not present in the training data. Quick check question: Does removing alternative spellings from the training data improve accuracy on a test set that has also been normalized, while hurting accuracy on "in-the-wild" user input that uses alternative spellings?

## Architecture Onboarding

- **Component map:** Raw text in Ge'ez script -> Pre-processor (No-Norm/H-Only/HSL) -> Model Core (Transformer/NLLB) -> Post-Processor (normalization function) -> Evaluator (BLEU/ChrF)

- **Critical path:** Identify confusable characters specific to target language -> Define mapping logic for normalization schemes -> Apply mapping logic either before tokenization/training (Standard) OR after generation/before scoring (Proposed)

- **Design tradeoffs:**
  - Training on Normalized Data: (+) Lower vocab size, faster convergence, potentially higher scores on normalized test sets. (-) Loss of spelling diversity, negative transfer to related languages, production model cannot output normalized-out characters
  - Post-Inference Normalization: (+) Preserves full character set in the model (better for transfer/preservation), improves metric visibility. (-) Requires extra step during evaluation; does not help if the goal is to generate the specific "standard" spelling automatically

- **Failure signatures:**
  - Models trained with aggressive normalization (HSL) tend to repeat characters/words until max sequence length
  - Transfer models introducing characters into Ge'ez predictions that do not exist in the Ge'ez alphabet (e.g., "X", "h")
  - Cross-lingual models reverting to Amharic words when translating to Tigrinya/Ge'ez

- **First 3 experiments:**
  1. Train two small language models on raw Amharic corpus vs normalized one. Compare vocabulary size and verify normalized model cannot represent held-out sentence containing "alternative" spellings
  2. Fine-tune multilingual model on Normalized Amharic -> English, then evaluate on Tigrinya -> English without further training. Check if Tigrinya-only characters are replaced by Amharic "normalized" counterparts
  3. Take existing open-source translation model for Amharic, evaluate on raw test set, then apply regex-based homophone normalizer to both outputs and references, re-calculate BLEU, confirm score lift matches ~1.0 BLEU point claim

## Open Questions the Paper Calls Out
- Does post-inference normalization improve human-perceived translation quality and usability compared to training on normalized data? (Paper relied on BLEU/ChrF scores and small-scale manual inspection without full-scale human evaluation)
- How does the "implicit standard" set by homophone normalization in Machine Translation propagate to and affect performance in downstream NLP tasks? (Study focuses on translation quality, not impact of using normalized synthetic data for other tasks)
- Can a unified, language-aware normalization scheme be developed that preserves phonetic distinctions necessary for cross-lingual transfer in Ge'ez script languages? (Current schemes are ad-hoc and optimized for monolingual Amharic vocabulary reduction)

## Limitations
- The extent to which homophone normalization represents a genuine linguistic threat versus a normalization preference remains unclear
- The study assumes preserving all homophone variants is linguistically beneficial, though some variants may be archaic or rarely used in practice
- The paper's claims rest on a specific sociotechnical framing that may overstate the linguistic impact

## Confidence

**High Confidence:**
- Post-inference normalization improves BLEU scores by approximately 1.0 points
- Training on normalized data reduces vocabulary size and limits character diversity in outputs
- Cross-lingual transfer is negatively impacted when training on normalized data

**Medium Confidence:**
- The claim that normalization imposes an "implicit standard" that harms language preservation
- The assertion that models cannot "understand different forms of writing" without empirical evidence of real-world user impact

**Low Confidence:**
- The specific character mapping dictionaries for each normalization scheme
- The complete characterization of which characters qualify as homophones versus similar sounds across all three languages

## Next Checks
1. Conduct frequency analysis of homophone character usage in native Ge'ez script corpora to verify whether normalized-out variants represent active linguistic variation or rarely used forms
2. Replicate cross-lingual transfer experiments using different transfer learning architectures (e.g., adapter-based methods) to determine if negative transfer effects persist
3. Conduct systematic human evaluation comparing translations from normalized vs non-normalized models, focusing on whether preserved character variants improve readability or convey meaningful distinctions for native speakers