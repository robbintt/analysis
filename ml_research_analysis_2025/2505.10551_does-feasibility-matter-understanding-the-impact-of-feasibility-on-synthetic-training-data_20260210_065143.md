---
ver: rpa2
title: Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic
  Training Data
arxiv_id: '2505.10551'
source_url: https://arxiv.org/abs/2505.10551
tags:
- data
- real
- feasible
- image
- infeasible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether feasibility (real-world plausibility)
  of synthetic training images impacts classifier performance. The authors define
  feasible images as those with attributes that could exist in reality, and infeasible
  images as those with unrealistic attributes.
---

# Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data

## Quick Facts
- arXiv ID: 2505.10551
- Source URL: https://arxiv.org/abs/2505.10551
- Authors: Yiwen Liu; Jessica Bader; Jae Myung Kim
- Reference count: 40
- This paper investigates whether feasibility (real-world plausibility) of synthetic training images impacts classifier performance, finding that feasibility has minimal impact with differences typically less than 0.3% in top-1 accuracy.

## Executive Summary
This paper challenges the assumption that synthetic training data must be feasible (real-world plausible) to be effective. The authors introduce VariReal, a pipeline that minimally edits real images to include specified feasible or infeasible attributes across background, color, and texture categories. Through systematic experiments on three fine-grained datasets, they demonstrate that feasibility has minimal impact on LoRA-fine-tuned CLIP performance, with differences typically less than 0.3% in top-1 accuracy. Background modifications consistently improve performance regardless of feasibility, while color and texture edits behave similarly to out-of-distribution data and show non-monotonic scaling effects.

## Method Summary
The study uses a three-stage pipeline: prompt generation (GPT-4 with in-context learning), guidance map generation (Grounding DINO + SAM2 for segmentation masks, Canny edge extraction for ControlNet), and minimal-change editing combining SDXL Inpainting with ControlNet structure preservation. Real images are minimally edited to include feasible or infeasible attributes based on textual prompts. CLIP ViT-B/16 is fine-tuned using LoRA (rank 16) with synthetic data at 5:1 ratio to real data. Performance is evaluated using top-1 accuracy, with distribution analysis through FID, CLIP score, DINO score, and LPIPS metrics.

## Key Results
- Feasibility has minimal impact on CLIP classifier performance, with differences typically less than 0.3% in top-1 accuracy
- Background modifications consistently improve performance regardless of feasibility, yielding average accuracy gains of 1.2-1.7%
- Color and texture infeasible edits behave similarly to out-of-distribution data, showing non-monotonic scaling effects where benefits peak at smaller synthetic-to-real ratios
- Mixing feasible and infeasible images in training sets does not significantly impact performance compared to using purely feasible or infeasible datasets

## Why This Works (Mechanism)

### Mechanism 1: Background Contextualization Creates Useful Variation Without Class Disruption
- Claim: Background modifications, whether feasible or infeasible, improve CLIP classifier performance because they add training diversity without corrupting class-relevant foreground features.
- Mechanism: By isolating edits to background regions via segmentation masks and inpainting, VariReal preserves foreground structure while varying context. This forces the classifier to become more robust to scene context, reducing over-reliance on background cues that may not generalize to test images.
- Core assumption: The classifier's attention mechanism primarily focuses on foreground objects for class prediction, making background changes a form of "safe augmentation" that adds diversity without noise.
- Evidence anchors: Background modifications consistently improve performance regardless of feasibility (section 4.2.2); classification tasks are object-centric (section 4.3.2).

### Mechanism 2: Minimal-Change Editing Reduces Distribution Shift While Enabling Controlled Attribute Study
- Claim: The VariReal pipeline's minimal-change approach creates synthetic data that is close enough to the real distribution to be useful for training, while still enabling isolated study of feasibility effects.
- Mechanism: By starting from real images and making targeted edits guided by segmentation masks and Canny edge maps, the pipeline limits the divergence from the real data manifold. The combination of inpainting (for natural blending) and ControlNet (for structure preservation) addresses individual weaknesses of each method.
- Core assumption: The minimal-change constraint ensures that any performance differences between feasible and infeasible conditions are attributable to feasibility itself, rather than to broader distribution shifts or artifact introduction.
- Evidence anchors: Pipeline combines Inpainting's realism with ControlNet's preciseness (section 3.2.2); only background modifications consistently improve classification performance (section 4.3.2).

### Mechanism 3: Out-of-Distribution Data as Regularization Rather Than Noise
- Claim: Infeasible synthetic data functions similarly to OOD augmentation—providing regularization benefits at appropriate scales but potentially degrading performance if over-represented in the training mix.
- Mechanism: Color and texture infeasible edits, while closer to the real distribution in pixel-space metrics, behave like OOD data in their effect on the classifier. At limited scales, they add diversity that acts as implicit regularization. However, as the proportion increases, the distribution shift accumulates, and the model learns from patterns that do not exist in the real test domain.
- Core assumption: The OOD-like behavior of foreground infeasible edits stems from the classifier's learned association between color/texture and class identity—a relationship that is task-specific and may not hold for all datasets.
- Evidence anchors: OOD data benefits from feature invariance and stochasticity (section 2); results reveal nonlinear relationship between performance and data scale (section 4.3.3).

## Foundational Learning

- **Latent Diffusion Models (Stable Diffusion)**
  - Why needed here: The entire VariReal pipeline is built on SDXL variants for inpainting and ControlNet conditioning. Understanding how images are encoded to latent space, denoised via U-Net, and decoded back is essential for debugging generation quality and setting appropriate hyperparameters.
  - Quick check question: Can you explain why increasing the denoising strength in SDXL Inpainting from 0.3 to 0.99 would change the output for a color modification task?

- **Low-Rank Adaptation (LoRA) for Fine-Tuning**
  - Why needed here: The study uses LoRA to fine-tune CLIP's image and text encoders on synthetic data. Without understanding LoRA's rank constraint and how it decomposes weight updates, you cannot reason about why performance differences are small.
  - Quick check question: If you observed that LoRA fine-tuning on infeasible data performed worse than full fine-tuning, would you conclude that infeasibility is harmful, or would you need to control for parameter count?

- **CLIP Vision-Language Alignment**
  - Why needed here: The classification evaluation uses CLIP's joint image-text embedding space. Understanding how contrastive pre-training creates aligned representations is critical for interpreting why background edits help while color/texture edits may hurt.
  - Quick check question: Given that CLIP is trained to maximize similarity between images and their text descriptions, how might adding "green Yorkshire terrier" images affect the model's representation of the "Yorkshire terrier" class?

## Architecture Onboarding

- **Component map:**
  - Input: Real images from fine-grained datasets (Oxford Pets, FGVC Aircraft, Stanford Cars)
  - Prompt Generation: GPT-4 with in-context learning generates feasible/infeasible attribute prompts
  - Guidance Map Generation: Grounding DINO → bounding boxes → SAM2 → segmentation masks; Canny edge extraction for ControlNet
  - Prior Generation: Stable Diffusion v2.1 generates Raw Priors; Color Bank provides predefined RGB values
  - Prior Processing: Merge original object region with Background Prior (mask dilation); alpha blending for Color/Texture Priors → Real Priors
  - Minimal-Change Editing: Background → SDXL Inpainting with Real Prior; Color/Texture → Inpainting → ControlNet cascade
  - Quality Filtering: LLaVA-1.6-7B validates generated images against prompts
  - Training: LoRA fine-tuned CLIP ViT-B/16 on synthetic-only or mixed real+synthetic data

- **Critical path:**
  1. Prompt quality directly impacts generation specificity and feasibility distinction
  2. Segmentation mask accuracy determines whether foreground is preserved during background edits
  3. Denoising strength and IP-Adapter strength control the trade-off between edit magnitude and structure preservation
  4. Synthetic-to-real ratio determines whether foreground edits act as beneficial augmentation or harmful OOD data

- **Design tradeoffs:**
  - **Inpainting vs. ControlNet**: Inpainting produces more natural backgrounds but may struggle with large color changes; ControlNet preserves structure but can create "floating" effects for backgrounds—solved by using each for its strength
  - **Mask dilation**: Expanding the foreground mask before background inpainting maintains spatial context but risks including background pixels in the preserved region
  - **Filtering aggressiveness**: Strict MLLM filtering ensures prompt alignment but may reject viable images; lenient filtering increases data volume but introduces noise

- **Failure signatures:**
  - **Floating objects in background edits**: Indicates mask dilation was disabled or insufficient
  - **Color bleeding/shape distortion in foreground edits**: Indicates denoising strength too high or ControlNet conditioning too weak
  - **High performance variance across runs**: Likely due to LLM prompt generation randomness—fix by setting seeds or caching prompts
  - **Feasible/infeasible distinction collapses**: MLLM filter is too strict, rejecting most infeasible images as "unrealistic"

- **First 3 experiments:**
  1. **Baseline replication on a new dataset**: Apply VariReal to CIFAR-100 or a domain-specific dataset (e.g., medical imaging) with 1:1 synthetic-to-real ratio for background edits only; compare feasible vs. infeasible performance difference to the ~0.3% threshold observed in fine-grained datasets
  2. **Scaling ablation**: Fix the synthetic-to-real ratio at {1:1, 3:1, 5:1} for both background and color infeasible edits on Stanford Cars; plot accuracy to verify the non-monotonic scaling pattern for foreground edits
  3. **Mask quality impact**: Intentionally degrade segmentation mask accuracy and measure the change in background edit effectiveness; this quantifies robustness to real-world deployment conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the feasibility of synthetic training data impact downstream tasks beyond image classification, such as object detection and semantic segmentation?
- Basis in paper: The authors acknowledge that generated synthetic data supports various tasks including "object detection [17], and semantic segmentation [55]" but limit their investigation to classification tasks.
- Why unresolved: The study only evaluates CLIP-based classification; the effect of feasibility on localization-heavy tasks where foreground-background relationships may be more critical remains unknown.
- What evidence would resolve it: Replicate the feasible/infeasible training experiment on standard detection (e.g., COCO) and segmentation benchmarks using synthetic data augmentation.

### Open Question 2
- Question: How does the impact of feasibility scale with significantly larger ratios of synthetic to real training data beyond the 5:1 ratio tested?
- Basis in paper: The scaling analysis reveals nonlinear behavior where color and texture modifications peak at smaller scales then decline, but the study only tests up to 5x synthetic data. The authors note "Large-scale use of such data does not provide meaningful in-distribution information."
- Why unresolved: Modern synthetic data approaches often use much higher synthetic-to-real ratios; whether the minimal feasibility effect persists at 10x, 50x, or 100x ratios is unknown.
- What evidence would resolve it: Extend the scaling experiments to higher synthetic data ratios across all three attribute categories and measure whether the feasibility gap remains below 0.3%.

### Open Question 3
- Question: Would feasibility matter more for more complex, non-object-centric datasets that lack clear foreground-background separation?
- Basis in paper: The authors state their "approach targets datasets with clear foreground-background separation" and exclude datasets "dominated by foreground-only images, such as ImageNet." They also acknowledge slight shape deviations "sometimes unavoidable due to current image editing limitations."
- Why unresolved: The minimal feasibility impact may be specific to fine-grained object-centric datasets where the object itself dominates the classification decision.
- What evidence would resolve it: Apply the same feasible/infeasible synthetic data methodology to scene classification or dense prediction tasks on datasets like ImageNet or ADE20K.

## Limitations

- The study focuses on fine-grained classification tasks where background context plays a secondary role; tasks requiring spatial reasoning or context-sensitive predictions may show different patterns
- Effect sizes (typically <0.3% accuracy difference) may not generalize to larger-scale models or different downstream tasks
- Real-world deployment scenarios involve simultaneous distribution shifts across multiple attributes, potentially amplifying feasibility effects beyond what the controlled experiments capture

## Confidence

- **High Confidence**: The mechanism that background modifications improve performance regardless of feasibility due to contextual diversity without foreground corruption
- **Medium Confidence**: The claim that minimal-change editing through the VariReal pipeline enables controlled study of feasibility effects while maintaining real-world utility
- **Medium Confidence**: The interpretation of foreground infeasible edits as OOD regularization rather than noise

## Next Checks

1. **Cross-task generalization**: Apply VariReal to datasets requiring context-sensitive classification (e.g., WaterBirds subset with "land birds on water" vs "water birds on land") to test whether feasibility effects amplify when background context is class-relevant

2. **Model capacity sensitivity**: Repeat experiments with CLIP-ViT-L/14 and LoRA rank 64 to determine if the minimal performance differences observed with ViT-B/16 persist at larger scales, or if feasibility becomes more critical with increased model capacity

3. **Real-world deployment simulation**: Generate synthetic data with simultaneous attribute modifications (e.g., both color and texture infeasible edits) and evaluate performance degradation to test the robustness of the OOD regularization mechanism under realistic conditions