---
ver: rpa2
title: 'LLM Inference Enhanced by External Knowledge: A Survey'
arxiv_id: '2505.24377'
source_url: https://arxiv.org/abs/2505.24377
tags:
- reasoning
- knowledge
- llms
- data
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys methods for enhancing large language model (LLM)
  inference using external knowledge, focusing on structured data sources like tables
  and knowledge graphs (KGs). The authors categorize integration approaches into symbolic,
  neural, and hybrid reasoning for tables, and loose coupling versus tight coupling
  for KGs.
---

# LLM Inference Enhanced by External Knowledge: A Survey

## Quick Facts
- **arXiv ID:** 2505.24377
- **Source URL:** https://arxiv.org/abs/2505.24377
- **Reference count:** 28
- **Primary result:** Survey categorizes methods for enhancing LLM inference with structured external knowledge (tables/KGs); hybrid methods outperform symbolic and neural approaches on table reasoning tasks.

## Executive Summary
This survey comprehensively categorizes methods for enhancing large language model (LLM) inference using external structured knowledge sources, specifically tables and knowledge graphs (KGs). The authors identify two primary integration paradigms: for tables, they distinguish symbolic (SQL-based), neural (Chain-of-Thought), and hybrid approaches that combine both; for KGs, they differentiate loose coupling (one-shot retrieval) from tight coupling (iterative agent-based exploration). Through benchmarking experiments using GPT-3.5-turbo, hybrid methods demonstrate superior performance on table reasoning tasks, while tight coupling methods show strong results for KG reasoning. The survey highlights critical challenges including error propagation in hybrid systems, balancing input size with information retention, and efficiency trade-offs between coupling strategies.

## Method Summary
The survey analyzes methods for integrating external structured knowledge into LLM inference through systematic categorization. For table reasoning, methods are classified as symbolic (generating SQL for structured query execution), neural (direct reasoning over linearized table text), or hybrid (combining SQL extraction with neural reasoning). For KG reasoning, approaches are divided into loose coupling (retrieving relevant facts in a single pass) and tight coupling (using the LLM as an agent to iteratively explore the graph). The authors benchmark representative methods including TabSQLify (hybrid table reasoning) and ToG (tight coupling KG reasoning) using GPT-3.5-turbo as the base LLM. Evaluation is conducted on WikiTQ and TabFact for tables, and WebQSP, CWQ, and MetaQA for KGs, measuring exact match accuracy and path-based metrics.

## Key Results
- Hybrid reasoning methods achieve 85% exact match accuracy on WikiTQ, outperforming both symbolic and neural approaches
- Tight coupling KG methods like ToG demonstrate strong performance through iterative refinement of reasoning paths
- Error propagation in hybrid systems remains a critical limitation when symbolic modules generate incorrect SQL
- Input linearization creates fundamental trade-offs between information retention and context window constraints
- Loose coupling is faster but struggles with multi-hop reasoning compared to tight coupling approaches

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Table Reasoning (Symbolic-Neural Fusion)
The system uses the LLM to generate SQL that filters a large table into a relevant subtable, then performs neural reasoning over this simplified data. This works because SQL handles precise filtering/aggregation while neural reasoning manages semantic nuances. The core assumption is reliable SQL generation; if the SQL produces syntax or logic errors, the pipeline returns empty results, causing the neural reasoner to hallucinate answers.

### Mechanism 2: Tight Coupling via Graph Traversal Agents
The LLM acts as an agent that iteratively explores the KG, retrieving neighbors, evaluating relevance, and deciding next exploration steps. This enables multi-hop reasoning beyond simple one-shot retrieval. The mechanism assumes the KG is sufficiently complete and connected; otherwise, the agent may stall or loop. The trade-off is higher latency for deeper reasoning capability.

### Mechanism 3: Input Context Linearization
Structured data is converted to text format (e.g., "Subject is related to Object") so LLMs can process relationships alongside queries. This works by making structured relationships accessible to the model's attention mechanism. The core assumption is that serialization preserves semantic relationships without confusing the model. The trade-off is between information retention and context window efficiency.

## Foundational Learning

- **Parametric vs. Non-Parametric Memory**: LLMs rely on internal weights (parametric) but external retrieval provides non-parametric facts. This distinction explains why external knowledge is necessary when internal knowledge is outdated or incomplete. *Quick check: Why would an LLM hallucinate a CEO's name if the information changed yesterday?*

- **Symbolic vs. Neural Reasoning**: Symbolic reasoning (SQL) provides precision for exact calculations while neural reasoning offers flexibility for semantic understanding. Understanding this divide is essential for designing hybrid systems. *Quick check: Which approach is better for calculating the exact sum of a column, and which is better for summarizing the "vibe" of the data?*

- **Knowledge Graph Triples (Subject-Predicate-Object)**: This atomic unit represents relationships like "Paris is the capital of France" as (Paris, capital_of, France). Integration methods rely on retrieving or traversing these specific triplets. *Quick check: How would you represent "Paris is the capital of France" as a triple?*

## Architecture Onboarding

- **Component map:** User Query + Structured Source (Table/KG) -> Translator (LLM generates SQL/Graph plan) -> Executor (External Engine runs code) -> Synthesizer (LLM produces final answer)
- **Critical path:** The Translator -> Executor interface requires the LLM to generate syntactically perfect code. A single syntax error breaks the entire chain.
- **Design tradeoffs:** Loose vs. Tight Coupling (speed vs. reasoning depth); Full Table vs. Subtable (information completeness vs. token efficiency).
- **Failure signatures:** Empty Retrieval (SQL returns 0 rows); Schema Mismatch (hallucinated column names); Context Drift (agent retrieves irrelevant entities).
- **First 3 experiments:**
  1. Text-to-SQL Baseline: Generate SQL for validation set and measure syntax error rates.
  2. Hybrid Mockup: Manually extract relevant subtable and compare accuracy against baseline to isolate reasoning vs. extraction errors.
  3. Tight Coupling Test: Build 1-hop agent loop and test ability to chain two retrieval calls for 2-hop questions.

## Open Questions the Paper Calls Out

**Open Question 1:** How can dynamic, fault-tolerant hybrid architectures be designed to prevent symbolic module errors from propagating to neural reasoning components? The authors identify this as a key future direction since current hybrid systems suffer when early symbolic errors mislead subsequent neural reasoning.

**Open Question 2:** What methods can optimally balance input size reduction with the risk of over-pruning critical information when processing large tables? The survey highlights this as a central open problem in maintaining reasoning accuracy while adhering to token limits.

**Open Question 3:** How can heterogeneous multimodal signals be aligned to graph entities with minimal semantic loss for comprehensive KG-enhanced reasoning? Future work should develop robust alignment protocols to incorporate diverse signals like images and audio into text-based KGs.

## Limitations
- Error propagation in hybrid systems remains unquantified - the survey acknowledges this issue but lacks systematic measurement of failure rates.
- Tight coupling efficiency metrics are limited - while performance is demonstrated, iteration counts and success rates for terminating search paths are not provided.
- Input linearization trade-offs lack empirical validation - the survey identifies the tension between information retention and context efficiency but doesn't specify how this balance was achieved in benchmarks.

## Confidence
- **High Confidence:** The categorization framework (symbolic/neural/hybrid for tables; loose/tight for KGs) is well-supported by literature and thorough analysis.
- **Medium Confidence:** Benchmark results showing hybrid methods outperforming others are based on GPT-3.5-turbo, but exact prompting strategies and environments are not fully specified.
- **Low Confidence:** Long-term error propagation analysis is conceptual rather than empirical, lacking systematic measurement of downstream impact.

## Next Checks
1. Implement error logging framework for SQL generation step - measure syntactically/semantically correct queries and correlation with final accuracy to quantify error propagation claims.
2. Measure iteration counts and success rates for ToG's tight coupling - record iterations taken, termination success, and loop detection frequency for multi-hop questions.
3. Conduct ablation study on linearization formats - test natural language sentences, markdown tables, and triple lists to quantify information retention vs. context efficiency trade-offs.