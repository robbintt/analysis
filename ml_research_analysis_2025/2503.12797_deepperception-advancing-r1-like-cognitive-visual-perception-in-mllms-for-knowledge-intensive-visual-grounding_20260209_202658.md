---
ver: rpa2
title: 'DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for
  Knowledge-Intensive Visual Grounding'
arxiv_id: '2503.12797'
source_url: https://arxiv.org/abs/2503.12797
tags:
- visual
- perception
- cognitive
- deepperception
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces knowledge-intensive visual grounding (KVG),
  a novel task requiring fine-grained visual perception and domain-specific knowledge
  integration. To address the challenge, the authors propose DeepPerception, an MLLM
  enhanced with cognitive visual perception capabilities through a two-stage training
  framework combining supervised fine-tuning for cognitive reasoning scaffolding and
  reinforcement learning to optimize perception-cognition synergy.
---

# DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding

## Quick Facts
- arXiv ID: 2503.12797
- Source URL: https://arxiv.org/abs/2503.12797
- Reference count: 40
- Key outcome: +8.08% accuracy improvement on KVG-Bench over direct fine-tuning

## Executive Summary
This paper introduces DeepPerception, a two-stage training framework that enhances MLLMs with cognitive visual perception capabilities for knowledge-intensive visual grounding (KVG). The approach combines supervised fine-tuning for cognitive reasoning scaffolding with reinforcement learning to optimize perception-cognition synergy. DeepPerception achieves significant improvements on KVG-Bench, a benchmark spanning 10 domains with 1.3K test cases, demonstrating that integrating cognitive processes enables human-like visual perception in MLLMs.

## Method Summary
DeepPerception uses a two-stage training approach on Qwen2-VL-7B. Stage 1 applies supervised fine-tuning with Chain-of-Thought rationales generated by Qwen2-VL-72B to teach knowledge-aligned reasoning patterns. Stage 2 employs Group Relative Policy Optimization (GRPO) with IoU-based rewards to refine bounding-box precision. The training data comes from five fine-grained visual recognition categories, synthesized through composite image generation combining 2-6 entities per image. The model is evaluated on KVG-Bench, showing substantial improvements in both in-domain and cross-domain generalization.

## Key Results
- +8.08% accuracy improvement on KVG-Bench over direct fine-tuning
- +4.60% superior cross-domain generalization compared to baseline approaches
- Significant performance gains on known entities (+11.11% in-domain) versus unknown entities (+6.39% in-domain)

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Cognitive Training Decomposition
- Claim: Separating cognitive scaffolding from perception refinement produces complementary specialization
- Evidence: Stage-1 CoT-SFT improves in-domain accuracy from 52.65% to 56.82%; Stage-2 GRPO further improves to 63.13% (+6.19%)

### Mechanism 2: Knowledge-Conditioned Visual Discrimination
- Claim: Performance gains correlate with the model's prior knowledge about entities
- Evidence: DeepPerception achieves significantly greater performance gains on known entities versus unknown entities regardless of domain boundaries

### Mechanism 3: Composite Image Synthesis for Cognitive Demand
- Claim: Forcing multi-entity discrimination through composite images prevents shortcut learning
- Evidence: Out-of-domain accuracy of 60.85% suggests generalization beyond training entities

## Foundational Learning

- **Visual Grounding (REC)**: Why needed: KVG extends traditional REC by requiring fine-grained entity discrimination; Quick check: Can you explain why "find the Clumber Spaniel" is harder than "find the left dog"?

- **Chain-of-Thought Reasoning in Vision-Language Models**: Why needed: Stage-1 training depends on generating structured reasoning chains; Quick check: What is the role of the `Planned → Visual Analysis → Comparison → Conclusion` structure in the CoT output?

- **Group Relative Policy Optimization (GRPO)**: Why needed: Stage-2 uses GRPO to optimize perception without a critic model; Quick check: How does the IoU reward differ from a binary accuracy reward, and why does this matter for bounding-box refinement?

## Architecture Onboarding

- **Component map**: Base: Qwen2-VL-7B → CoT Generator: Qwen2-VL-72B → Data Engine: Composite synthesizer → Training: Stage-1 SFT → Stage-2 GRPO → Output: `<think reasoned text </think ><answer>bbox</answer>`

- **Critical path**: Data synthesis → CoT generation → Stage-1 SFT → Data filtering → Stage-2 GRPO → Evaluation on KVG-Bench

- **Design tradeoffs**: IoU threshold τ (too low permits reward hacking; too high provides sparse gradients); Completion length limit (512 tokens); Data filtering (removes trivial/impossible samples but may reduce diversity)

- **Failure signatures**: Incorrect CoT with correct bbox; Response length collapse; Performance gap between known/unknown entities

- **First 3 experiments**:
  1. Reproduce Stage-1 CoT-SFT on 5-category training split; validate that reasoning chains contain entity-specific features
  2. Ablate the IoU reward by replacing with binary 0/1 accuracy; compare Stage-2 convergence speed and final accuracy
  3. Evaluate cross-domain generalization by training on 4 categories and testing on the 5th held-out in-domain category

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms enable incorrect chain-of-thought (CoT) rationales to yield correct perceptual answers, and how can this phenomenon be systematically characterized?
- Basis: The authors observed instances where incorrect CoT rationales led to correct answers, indicating that the presence of cognitive processes, rather than their length or even factual accuracy, is the primary determinant of performance improvement.

### Open Question 2
- Question: How does the performance of models trained on synthetic composite data generalize to more natural, unstaged visual scenarios with complex backgrounds and occlusions?
- Basis: The method relies on a data engine that synthesizes composite images. The paper notes this overcomes single-object dominance in existing datasets but does not evaluate whether models trained this way maintain performance on more cluttered, real-world images.

### Open Question 3
- Question: Does the cognitive visual perception capability induced by this framework transfer beneficially to other multimodal tasks beyond grounding and fine-grained recognition?
- Basis: The paper evaluates on KVG-Bench, FGVR datasets, and general multimodal benchmarks. While general capability is preserved, it does not explore whether the enhanced cognitive-perceptual synergy actively improves performance on tasks requiring integrated reasoning and description.

## Limitations

- Data generation artifacts: Composite image synthesis may introduce artifacts that the model learns to exploit rather than developing genuine fine-grained perception capabilities
- Knowledge dependency concerns: The model's performance on novel entities without prior knowledge remains untested
- Reproducibility barriers: Critical hyperparameters including IoU threshold τ and data filtering criteria are unspecified

## Confidence

- **High confidence**: The two-stage training framework architecture is well-specified and the quantitative results on KVG-Bench are clearly reported
- **Medium confidence**: The knowledge-conditioned visual discrimination mechanism is supported by the known/unknown entity analysis
- **Low confidence**: The claim that composite image synthesis prevents shortcut learning lacks empirical validation showing models don't memorize artifact patterns

## Next Checks

1. **Artifact sensitivity test**: Train DeepPerception on corrupted composite images (random noise overlays, color distortions) and measure whether accuracy remains stable or drops significantly

2. **Novel entity transfer**: Create a test set with fine-grained entities completely absent from training data and evaluate whether the model can still perform knowledge-intensive grounding through pure perceptual reasoning

3. **Real-world generalization**: Evaluate DeepPerception on existing fine-grained visual grounding datasets (e.g., referring expressions datasets) to validate that synthetic training generalizes to natural multi-entity scenarios