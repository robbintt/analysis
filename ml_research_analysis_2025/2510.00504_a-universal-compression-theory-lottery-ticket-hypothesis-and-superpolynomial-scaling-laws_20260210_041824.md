---
ver: rpa2
title: 'A universal compression theory: Lottery ticket hypothesis and superpolynomial
  scaling laws'
arxiv_id: '2510.00504'
source_url: https://arxiv.org/abs/2510.00504
tags:
- function
- error
- compression
- symmetric
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that any permutation-symmetric function can be
  compressed from $d$ objects to $\operatorname{polylog}(d)$ objects with vanishing
  error by matching tensor moments. The key insight is that permutation symmetry allows
  decomposition into a composition of low-dimensional objects, and when $d$ is large,
  redundancy in dense regions enables compression.
---

# A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws

## Quick Facts
- **arXiv ID:** 2510.00504
- **Source URL:** https://arxiv.org/abs/2510.00504
- **Reference count:** 36
- **Primary result:** Any permutation-symmetric function can be compressed from d objects to polylog(d) objects with vanishing error via moment-matching

## Executive Summary
This paper introduces a universal compression theory for permutation-symmetric functions that bridges the lottery ticket hypothesis with superpolynomial scaling laws. The key insight is that permutation symmetry allows decomposition into compositions of low-dimensional objects, enabling compression from d objects to polylog(d) objects with vanishing error through tensor moment matching. This framework establishes that any neural network layer can be compressed to polylogarithmic width while preserving training dynamics exactly, and demonstrates that neural scaling laws improve from power-law to stretched-exponential decay when compression is applied.

The authors validate their theory through numerical experiments showing that moment-matched compression reduces test loss while preserving dynamics, and that compression improves scaling exponents in both teacher-student setups and synthetic function approximation tasks. The work provides a unified theoretical foundation for understanding compressibility in deep learning systems and suggests that neural networks can be made significantly more efficient without sacrificing performance.

## Method Summary
The paper proposes a two-stage compression algorithm: first clustering similar objects using k-means or greedy k-NN, then performing moment matching to find compressed representations that preserve the polynomial moments of the original function up to order k. The method exploits permutation symmetry by decomposing symmetric functions into compositions of low-dimensional objects, where redundancy in dense regions enables compression when d is large. For dynamical lottery ticket hypothesis, the approach preserves exact training dynamics by ensuring compressed networks follow identical gradient trajectories as originals. The framework applies to both neural network weight compression and dataset compression, with theoretical guarantees on error decay as d increases.

## Key Results
- Any permutation-symmetric function can be compressed from d objects to polylog(d) objects with vanishing error through moment-matching
- Compression preserves exact training dynamics in neural networks, establishing a dynamical lottery ticket hypothesis
- Neural scaling laws improve from power-law L ~ d^(-α) to stretched-exponential L ~ exp(-α' √[m]{d}) under compression
- Numerical experiments validate compression effectiveness and scaling improvements across teacher-student setups and synthetic function approximation

## Why This Works (Mechanism)
The mechanism relies on permutation symmetry allowing decomposition into compositions of low-dimensional objects, where redundancy in dense regions enables compression. When d is large, the moment-matching approach exploits this redundancy by finding compressed representations that preserve polynomial moments up to order k. The key insight is that for symmetric functions, the information content can be captured in a much smaller representation when moments are matched appropriately. This works because symmetric functions have inherent structure that allows them to be expressed as functions of summary statistics, which can be compressed without significant information loss.

## Foundational Learning
- **Permutation symmetry**: Required because it allows decomposition into compositions of low-dimensional objects; quick check: verify the function satisfies f(w₁,...,w_d) = f(w_σ(1),...,w_σ(d)) for any permutation σ
- **Tensor moment matching**: The core compression technique that preserves polynomial moments up to order k; quick check: verify N_{m,k} = C(m+k,k) < d for compression to be possible
- **Curse of dimensionality**: Explains why k must scale with function order m for meaningful compression; quick check: ensure k ≳ m when m is large to avoid slow error decay
- **Scaling law improvement**: The transition from power-law to stretched-exponential decay; quick check: verify α increases when compressing to ⌈16√d⌉ objects
- **Dynamic preservation**: The lottery ticket hypothesis aspect where compressed networks follow identical gradient trajectories; quick check: ensure mini-batch sequences are identical for both original and compressed runs
- **Polynomial vs exponential decay**: The theoretical guarantee that compression error vanishes as d increases; quick check: verify L ~ exp(-α' √[m]{d}) rather than L ~ d^(-α) after compression

## Architecture Onboarding

**Component map:** Dataset → Moment-matching compression (Algorithm 1) → Compressed dataset → Neural network → Training dynamics preservation → Loss trajectory

**Critical path:** Clustering (k-means/greedy k-NN) → Moment-matching (Algorithm 2) → Null vector finding → Weight updates → Training dynamics preservation

**Design tradeoffs:** Higher k moments provide better compression but increase computational cost; clustering method affects compression quality vs speed; compression ratio vs error tradeoff depends on function structure and dimension m

**Failure signatures:** High compression error when m is large (curse of dimensionality); compressed dynamics diverge from original (mini-batch mismatch); clustering fails to find small-diameter clusters (switch from k-means earlier)

**First experiments:**
1. Implement Algorithm 2 for moment-matching compression on synthetic symmetric functions
2. Compress a teacher-student dataset and verify loss improvement
3. Test dynamical preservation on a simple 2-layer network with cylindrical harmonic function

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can scalable compression algorithms be developed to improve neural scaling laws in high-dimensional settings?
- **Basis in paper:** [explicit] The authors state their proposed polynomial-time algorithm is "currently too slow and memory-intensive in high dimensions" and call for future work to "optimize this algorithm or design scalable approximations."
- **Why unresolved:** The current implementation cannot handle the scale required for modern AI models efficiently.
- **What evidence would resolve it:** A modified algorithm demonstrating comparable theoretical error bounds with significantly lower computational complexity on large-scale tasks.

### Open Question 2
- **Question:** Can the framework be extended to mitigate the curse of dimensionality by exploiting the low-dimensional manifold structure of data?
- **Basis in paper:** [explicit] The paper notes "slow error decay when the constituent dimension $m$ is large" but suggests "exploiting low-dimensional embeddings may greatly mitigate" this.
- **Why unresolved:** The current theory relies on dimension $m$ explicitly, and it is unknown how manifold structure formally alters the compression bounds.
- **What evidence would resolve it:** A theoretical extension proving tighter error bounds for data supported on low-dimensional submanifolds.

### Open Question 3
- **Question:** Can network initialization schemes be designed to implicitly mimic a compressed state without requiring an explicit compression step?
- **Basis in paper:** [explicit] The authors identify the "challenge to construct initializations (or datasets) that behave as if they were already compressed."
- **Why unresolved:** The current method relies on moment matching on existing parameters; initializing directly into such a state is non-trivial.
- **What evidence would resolve it:** A weight initialization strategy that achieves the same training dynamics and efficiency as a moment-matched compressed network.

## Limitations
- The compression method becomes computationally intensive in high dimensions, limiting scalability to modern AI model sizes
- Theoretical error bounds require k ≈ √d moments, but practical implementations may struggle when function order m is large
- Exact preservation of training dynamics assumes perfect moment matching and may not hold under realistic training conditions with stochastic gradients
- The framework specifically requires permutation symmetry, limiting direct application to non-symmetric architectures

## Confidence
- **High confidence**: The moment-matching compression algorithm and its theoretical foundation are well-established in the literature
- **Medium confidence**: Numerical experiments demonstrate compression effectiveness on small-scale problems (d=10⁴, d'=10³)
- **Low confidence**: Claims of exact dynamic preservation in lottery ticket hypothesis are theoretically ambitious and may not hold under realistic conditions

## Next Checks
1. Reproduce Figure 5 with larger problem sizes (d > 10⁵) to verify scaling exponent α actually doubles under compression
2. Test whether compression quality degrades with different weight initialization schemes (Xavier, He, orthogonal)
3. Apply compression framework to non-symmetric but structured functions (e.g., convolutional networks) to test generalization beyond permutation symmetry