---
ver: rpa2
title: 'MRSD: Multi-Resolution Skill Discovery for HRL Agents'
arxiv_id: '2505.21410'
source_url: https://arxiv.org/abs/2505.21410
tags:
- skill
- skills
- state
- learning
- manager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRSD introduces a hierarchical reinforcement learning framework
  that learns multiple skill encoders at different temporal resolutions in parallel,
  enabling dynamic skill selection during task execution. The method uses separate
  CVAEs for skill discovery at multiple temporal scales and a multi-head policy that
  learns to interleave these skills effectively.
---

# MRSD: Multi-Resolution Skill Discovery for HRL Agents

## Quick Facts
- arXiv ID: 2505.21410
- Source URL: https://arxiv.org/abs/2505.21410
- Reference count: 40
- Primary result: MRSD achieves faster convergence and higher final performance than prior HRL methods on DeepMind Control Suite tasks

## Executive Summary
MRSD introduces a hierarchical reinforcement learning framework that learns multiple skill encoders at different temporal resolutions in parallel, enabling dynamic skill selection during task execution. The method uses separate CVAEs for skill discovery at multiple temporal scales and a multi-head policy that learns to interleave these skills effectively. When evaluated on DeepMind Control Suite tasks, MRSD outperforms prior state-of-the-art skill discovery and HRL methods, achieving faster convergence and higher final performance.

## Method Summary
MRSD operates by maintaining separate CVAEs for skill discovery at multiple temporal scales (2, 4, and 8 time steps), each learning a latent skill space from trajectory segments. These skills are discovered without external rewards through latent space exploration using mutual information maximization. The method employs a multi-head policy that selects between skills from different resolution levels during execution, enabling dynamic interleaving. Skills are temporally unrolled during execution, with the policy receiving information about when skills will end. The framework uses contrastive learning to improve skill discriminability and prevent collapse to trivial solutions.

## Key Results
- MRSD outperforms prior HRL and skill discovery methods on DeepMind Control Suite tasks
- Achieves faster convergence rates compared to baseline methods
- Dynamic skill interleaving proves crucial for optimal performance
- Matches or exceeds non-hierarchical baselines like DreamerV2 across most tasks

## Why This Works (Mechanism)
The method's effectiveness stems from learning skills at multiple temporal resolutions, allowing the agent to adapt its behavior granularity based on task requirements. The contrastive learning component ensures skills are discriminative and meaningful rather than collapsing to trivial solutions. The mutual information maximization objective enables skill discovery without external rewards by encouraging exploration of the latent skill space. The multi-head policy architecture allows for flexible skill composition during task execution.

## Foundational Learning
- **Hierarchical RL**: Agents operate at multiple temporal scales with high-level controllers selecting skills and low-level controllers executing them. Needed for long-horizon tasks where flat RL struggles. Quick check: Can decompose complex tasks into manageable subtasks.
- **Skill Discovery**: Learning reusable behaviors without external rewards through latent space exploration. Needed to avoid manual skill engineering. Quick check: Skills transfer across tasks and enable zero-shot execution.
- **CVAE-based skill representation**: Conditional variational autoencoders encode trajectory segments into latent skill vectors. Needed for probabilistic skill modeling and reconstruction. Quick check: Can reconstruct trajectories from learned skills.
- **Mutual information maximization**: Objective encouraging skills to be both predictable from states and diverse. Needed for unsupervised skill discovery. Quick check: Skills exhibit high entropy and predictability.
- **Contrastive learning**: Technique for improving skill discriminability through positive/negative pair discrimination. Needed to prevent skill collapse. Quick check: Skills form distinct clusters in latent space.
- **Multi-resolution temporal abstraction**: Learning skills at different time scales simultaneously. Needed for adaptive behavior granularity. Quick check: Can execute both fine and coarse behaviors effectively.

## Architecture Onboarding

Component map: Environment states -> Multiple CVAEs (2, 4, 8-step) -> Latent skill spaces -> Multi-head policy -> Skill selection -> Execution

Critical path: State observation → Skill encoder selection → Skill sampling → Policy → Action output → Environment

Design tradeoffs: Multiple skill encoders increase representational capacity but add computational overhead; dynamic interleaving provides flexibility but requires learning complex switching policies; contrastive learning improves skill quality but adds training complexity.

Failure signatures: Skill collapse to trivial solutions (detected through low skill entropy); poor temporal abstraction (detected through suboptimal performance on long-horizon tasks); unstable policy learning (detected through high variance in returns).

First experiments:
1. Verify skill reconstruction quality across different temporal resolutions
2. Test skill discriminability using nearest-neighbor analysis in latent space
3. Validate mutual information maximization through skill predictability metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Evaluation restricted to DeepMind Control Suite tasks, leaving scalability to complex environments untested
- Computational overhead from maintaining multiple skill encoders not quantified
- Assumption that multiple temporal scales are universally beneficial may not hold for all task types
- Focus on continuous control limits applicability to discrete action spaces

## Confidence

Performance claims on DM Control Suite: High (extensive empirical validation across multiple tasks)
Claims about dynamic skill interleaving importance: High (supported by ablation studies)
Claims about matching DreamerV2 baselines: Medium (limited to specific task set)
Generalizability to other domains: Low (not empirically tested beyond stated scope)

## Next Checks

1. Test MRSD on tasks with mixed discrete-continuous action spaces to evaluate broader applicability
2. Quantify and compare computational overhead relative to single-resolution skill discovery methods
3. Evaluate performance in environments with non-stationary dynamics or partial observability to test robustness