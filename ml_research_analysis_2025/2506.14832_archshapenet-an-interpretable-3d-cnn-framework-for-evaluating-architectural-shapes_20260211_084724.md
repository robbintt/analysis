---
ver: rpa2
title: ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural
  Shapes
arxiv_id: '2506.14832'
source_url: https://arxiv.org/abs/2506.14832
tags:
- design
- architectural
- forms
- classification
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of objectively analyzing and
  interpreting differences between human-designed and machine-generated 3D architectural
  forms, which is crucial for advancing generative design tools. To tackle this, the
  authors developed ArchForms-4000, a dataset containing 2,000 architect-designed
  and 2,000 EvoMass-generated 3D forms.
---

# ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes

## Quick Facts
- arXiv ID: 2506.14832
- Source URL: https://arxiv.org/abs/2506.14832
- Reference count: 0
- ArchShapeNet achieves 94.29% classification accuracy in distinguishing human-designed from machine-generated 3D architectural forms

## Executive Summary
This paper introduces ArchShapeNet, a 3D convolutional neural network framework designed to objectively evaluate and interpret differences between human-designed and machine-generated 3D architectural forms. The authors created ArchForms-4000, a dataset containing 2,000 architect-designed and 2,000 EvoMass-generated forms, and trained ArchShapeNet to classify these forms while providing interpretability through saliency maps. The framework demonstrates superior performance compared to professional architects in identifying the origin of architectural forms, offering data-driven insights for generative design tool optimization.

## Method Summary
The ArchShapeNet framework consists of a 3D-CNN classifier trained on voxelized 3D architectural forms from the ArchForms-4000 dataset, which contains 4,000 samples (2,000 human-designed and 2,000 EvoMass-generated). The 3D-CNN extracts volumetric features through hierarchical convolutional layers, followed by fully connected layers for binary classification. A gradient-based saliency module computes importance maps highlighting spatial regions most influential to the classification decision. The model was trained using SGD with momentum for 300 epochs on 128³ voxel grids, achieving high classification accuracy while providing interpretable visualizations of architectural features that distinguish human from machine designs.

## Key Results
- ArchShapeNet achieves 94.29% classification accuracy, 96.2% precision, and 98.51% recall, outperforming professional architects (77.51% accuracy)
- The model focuses on boundary features, geometric discontinuities, and complex structures when distinguishing forms, with saliency maps highlighting feature-dense regions
- Human-designed forms demonstrate advantages in spatial organization, proportional harmony, and detail refinement compared to machine-generated forms

## Why This Works (Mechanism)

### Mechanism 1: Spatial Feature Discrimination via Volumetric Convolution
The model achieves high classification accuracy by learning to prioritize spatial features related to boundary complexity and geometric discontinuity, which are statistically more prevalent in human designs. The 3D-CNN's hierarchical convolutional layers extract progressively abstract volumetric features, while the integrated 3D saliency module computes gradients of the class score with respect to input voxels, creating an importance map. High saliency is assigned to regions with complex transitions, edges, and irregularities that human-designed forms exhibit due to functional and aesthetic intent.

### Mechanism 2: Consistent Evaluation Framework Overcoming Human Cognitive Limitations
The 3D-CNN outperforms human experts because it maintains a perfectly consistent evaluation framework, free from fatigue, cognitive bias, and difficulty in verbalizing subtle spatial intuitions that affect human evaluators. Through training on a large dataset, the model learns a high-dimensional manifold of features representing "architect-designedness," encoding complex spatial relationships that human evaluators rely on intuition and experience to assess, introducing subjectivity and variability.

### Mechanism 3: Gradient-Based Attribution for Actionable Design Feedback
Saliency map analysis provides an interpretable link between the 3D-CNN's internal decision-making process and domain-specific architectural concepts, transforming the model from a black-box classifier into a tool for design feedback. The 3D saliency map visualizes voxel-wise gradients of the target class score, translating abstract neural activations into actionable insights by highlighting geometric regions contributing most to classification and suggesting where generative tools should introduce more complexity to mimic human-quality output.

## Foundational Learning

- **Concept: 3D Convolutional Neural Networks (3D-CNNs)**
  - **Why needed here:** Core architecture of ArchShapeNet. Understanding how 3D convolution extends 2D processing to volumetric data is fundamental to grasping how the model "sees" a 3D form voxel-by-voxel.
  - **Quick check question:** How does a 3D convolutional kernel process an input volumetric tensor differently than a 2D kernel processes a flat image?

- **Concept: Saliency Maps & Gradient-Based Attribution**
  - **Why needed here:** Primary interpretability mechanism. One must understand how computing gradients of the output class score with respect to input voxels yields a map of "importance" to interpret model decisions.
  - **Quick check question:** What does a high saliency value on a particular voxel indicate about that voxel's relationship to the model's predicted class?

- **Concept: Voxelization**
  - **Why needed here:** The paper converts all 3D models into a voxel grid before processing. This preprocessing step is critical as it defines resolution and spatial representation.
  - **Quick check question:** What geometric information is preserved, and what is potentially lost when converting a complex smooth 3D mesh into a regular voxel grid?

## Architecture Onboarding

- **Component map:** Input voxel grid (128³) -> 3D-CNN feature extractor (4 conv layers) -> Classifier (FC layers) -> Softmax output -> Gradient-based saliency module
- **Critical path:** Acquiring diverse 3D models from architects and generative tools -> Preprocessing and voxelization into unified 128³ grid format -> Training the 3D-CNN classifier on ArchForms-4000 dataset -> Running inference and generating 3D saliency maps to explain classifications and derive design insights
- **Design tradeoffs:**
  - Voxel Resolution vs. Computational Cost: Higher-resolution grids capture more detail but exponentially increase memory and training time. 128³ trades fine detail for efficiency.
  - Dataset Diversity vs. Specificity: Focusing solely on EvoMass forms allows controlled comparison but limits generalizability to other algorithms or advanced AI tools.
  - Interpretability vs. Simplicity: The saliency module adds complexity but provides crucial insights. The gradient-based method is post-hoc, explaining an already-trained model.
- **Failure signatures:**
  - Overfitting to generation artifacts: Model learns EvoMass-specific rendering or meshing artifacts rather than high-level design principles.
  - Saliency Map Noise: Importance maps are diffuse, failing to highlight coherent architectural features.
  - Poor Generalization: Model trained on EvoMass fails to distinguish human designs from other tools (TestFit, diffusion models).
- **First 3 experiments:**
  1. Baseline Classification: Train ArchShapeNet and evaluate accuracy, precision, recall on ArchForms-4000 test set. Compare metrics to human expert performance.
  2. Saliency Map Validation: Generate 3D saliency maps for correctly and incorrectly classified forms. Visually verify if highlighted regions correspond to architectural features.
  3. Cross-Validation on Subset: Train separate model instances on different splits of the dataset to assess robustness of classification accuracy and saliency map consistency.

## Open Questions the Paper Calls Out

- **Generalizability beyond EvoMass:** The model's architecture was trained and evaluated solely on forms generated by the EvoMass plugin, raising questions about its ability to generalize to other generative design tools or more advanced AI systems. The authors plan to incorporate "TestFit, cellular automata, and diffusion-based frameworks" to test generalizability.
- **Integration of semantic-functional data:** The paper states that focusing solely on geometry is a limitation, and future work should introduce "semantic-functional data" to build a "multi-dimensional evaluation system" that incorporates space-use labels and circulation paths.
- **Enhanced interpretability framework:** Current saliency maps "lack semantic depth," and the authors propose integrating "semantic segmentation... and expert annotations" to create a cognitively aligned interpretability framework that better explains architectural reasoning.

## Limitations

- Generalizability concerns: The model is trained exclusively on EvoMass-generated forms, limiting its ability to generalize to other generative design tools or advanced AI systems.
- Dataset composition transparency: The specific distribution of architectural styles, complexity levels, and generation parameters remains unclear, affecting reproducibility and external validity.
- Human evaluation conditions: The 77.51% human accuracy baseline does not specify whether evaluators had access to saliency tools or adequate time for assessment.

## Confidence

- **High Confidence**: Classification performance metrics (94.29% accuracy, 96.2% precision, 98.51% recall) are well-documented through standard experimental procedures.
- **Medium Confidence**: Interpretability claims regarding saliency maps highlighting architecturally meaningful features are supported by visual examples but lack rigorous semantic validation.
- **Medium Confidence**: Claims about specific architectural advantages are supported by comparative analysis but could benefit from more systematic qualitative assessment.

## Next Checks

1. **Cross-tool generalization test**: Evaluate ArchShapeNet on 3D forms generated by alternative parametric design tools (e.g., TestFit, Rhino Grasshopper) to assess performance beyond EvoMass-specific patterns.
2. **Architect validation study**: Conduct a blinded study where practicing architects assess whether saliency map highlights correspond to their professional understanding of design quality indicators.
3. **Ablation analysis of voxel resolution**: Systematically vary voxel grid resolution (e.g., 64³, 128³, 256³) to quantify the trade-off between computational efficiency and preservation of architectural detail critical for classification.