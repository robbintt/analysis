---
ver: rpa2
title: Attention, Please! PixelSHAP Reveals What Vision-Language Models Actually Focus
  On
arxiv_id: '2503.06670'
source_url: https://arxiv.org/abs/2503.06670
tags:
- object
- pixelshap
- objects
- masking
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PixelSHAP introduces a model-agnostic interpretability framework
  for Vision-Language Models (VLMs) that extends Shapley-based analysis to structured
  visual entities. Unlike existing methods focused on text prompts or pixel-level
  perturbations, PixelSHAP systematically analyzes image objects to quantify their
  influence on a VLM's response.
---

# Attention, Please! PixelSHAP Reveals What Vision-Language Models Actually Focus On

## Quick Facts
- arXiv ID: 2503.06670
- Source URL: https://arxiv.org/abs/2503.06670
- Authors: Roni Goldshmidt
- Reference count: 24
- Key outcome: PixelSHAP achieves 67.48% Recall@1 for Gemini-2.0-flash and 60.56% for GPT-4o in identifying influential objects

## Executive Summary
PixelSHAP introduces a novel interpretability framework for Vision-Language Models (VLMs) that extends Shapley-based analysis to structured visual entities. Unlike existing methods focused on text prompts or pixel-level perturbations, PixelSHAP systematically analyzes image objects to quantify their influence on a VLM's response. The method operates solely on input-output pairs without requiring model internals, making it compatible with both open-source and commercial models. It achieves computational efficiency through object-level perturbations using segmentation models, significantly reducing complexity compared to pixel-level approaches.

## Method Summary
PixelSHAP is a model-agnostic interpretability framework that quantifies the influence of segmented image objects on VLM responses. The method uses object-level perturbations with bounding box masking strategies (BBOA) to systematically mask target objects while preserving adjacent contours. It employs first-order Shapley value approximations with Monte Carlo sampling to estimate object importance, using cosine similarity of response embeddings as the value function. The framework operates without model internals, querying VLMs with masked images and comparing response embeddings to the original. It supports diverse embedding-based similarity metrics and achieves computational efficiency by reducing perturbation complexity from pixels to objects.

## Key Results
- BBOA masking strategy achieves 67.48% Recall@1 for Gemini-2.0-flash and 60.56% for GPT-4o
- Processing time of 15-74 seconds per image
- First-order Shapley approximation correlates 92% with exact computation
- Significantly outperforms Largest Object and Central Object baselines

## Why This Works (Mechanism)
PixelSHAP works by extending cooperative game theory's Shapley values to the visual domain, where each segmented object is treated as a "player" contributing to the final VLM response. By systematically masking individual objects and measuring the drop in response embedding similarity, the framework quantifies each object's marginal contribution to the output. The BBOA masking strategy ensures that when one object is masked, adjacent objects remain visible, preventing confounding effects from simultaneous occlusion. First-order approximations dramatically reduce computational complexity while maintaining accuracy, making the method practical for real-world applications.

## Foundational Learning
- **Shapley values**: Game-theoretic method for fair attribution of contributions in cooperative settings. Why needed: Provides mathematically rigorous way to attribute VLM responses to individual objects. Quick check: Verify sum of Shapley values equals total similarity drop.
- **First-order approximation**: Technique that estimates Shapley values using only single-object perturbations instead of all possible coalitions. Why needed: Reduces computational complexity from exponential to linear in number of objects. Quick check: Compare correlation with exact Shapley values on small object sets.
- **Object segmentation**: Process of detecting and delineating individual objects in images. Why needed: Enables object-level attribution instead of pixel-level perturbations. Quick check: Visualize detected objects against ground truth annotations.
- **Response embedding similarity**: Cosine similarity between text embeddings of original and perturbed responses. Why needed: Provides differentiable measure of response change for attribution. Quick check: Ensure embedding norms are consistent across responses.

## Architecture Onboarding
- **Component map**: Segmentation models (GroundingDINO + SAM) → BBOA masking → VLM API queries → Response embedding → Similarity computation → Shapley attribution
- **Critical path**: Object detection → BBOA masking → VLM querying → Embedding comparison → Shapley calculation
- **Design tradeoffs**: Object-level vs pixel-level perturbations (efficiency vs granularity), first-order vs exact Shapley (speed vs accuracy), bounding box vs mask-based masking (simplicity vs precision)
- **Failure signatures**: Poor segmentation leading to missed objects, BBOA over-masking hiding adjacent objects, API rate limiting causing timeouts, embedding instability from short responses
- **Three first experiments**: 1) Test segmentation on sample COCO images to verify object detection quality, 2) Implement BBOA masking on a single image to validate contour preservation, 3) Query VLM with one masked image to verify embedding similarity computation

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Object Importance Evaluation Dataset not yet publicly available, preventing exact replication
- BBOA algorithm lacks complete specification for adjacent contour detection and multi-object overlap handling
- Monte Carlo sampling configuration unspecified, affecting computational efficiency claims

## Confidence
- **High confidence**: Core methodology of first-order Shapley approximations with object-level perturbations is well-specified and theoretically sound
- **Medium confidence**: Comparative performance against baselines is credible with clear metrics, but exact reproducibility depends on unknown dataset and prompts
- **Low confidence**: Computational efficiency claims depend heavily on unspecified Monte Carlo sampling configuration and VLM API response times

## Next Checks
1. **Dataset Validation**: Once released, verify the Object Importance Evaluation Dataset by checking object distribution statistics, question template diversity, and ground-truth selection criteria against COCO validation characteristics.

2. **BBOA Algorithm Specification**: Request or reconstruct the complete BBOA masking algorithm specification, particularly the adjacent contour detection and preservation mechanism, and validate against the authors' implementation using example images.

3. **Shapley Approximation Verification**: Implement both exact Shapley computation (for small object sets) and first-order approximation on a subset of images to verify the claimed 92% correlation, ensuring the Monte Carlo sampling configuration matches intended implementation.