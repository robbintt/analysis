---
ver: rpa2
title: Scalable Decision Focused Learning via Online Trainable Surrogates
arxiv_id: '2512.03861'
source_url: https://arxiv.org/abs/2512.03861
tags:
- training
- regret
- loss
- surrogate
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability challenge in Decision Focused
  Learning (DFL) by proposing a surrogate-based approach that reduces expensive solver
  calls during training. The method uses Gaussian Processes (GPs) with stochastic
  smoothing and importance sampling to create an asymptotically unbiased surrogate
  loss function that can be differentiated efficiently.
---

# Scalable Decision Focused Learning via Online Trainable Surrogates

## Quick Facts
- arXiv ID: 2512.03861
- Source URL: https://arxiv.org/abs/2512.03861
- Authors: Gaetano Signorelli; Michele Lombardi
- Reference count: 18
- Primary result: GP-based surrogate reduces solver calls by up to two orders of magnitude while maintaining decision quality

## Executive Summary
This paper addresses the scalability challenge in Decision Focused Learning by proposing a surrogate-based approach that reduces expensive solver calls during training. The method uses Gaussian Processes with stochastic smoothing and importance sampling to create an asymptotically unbiased surrogate loss function that can be differentiated efficiently. Unlike previous approaches, it provides local confidence estimates allowing dynamic switching to a fallback method when needed. The approach is designed for black-box settings without requiring explicit problem structure knowledge.

## Method Summary
The proposed method trains a separate Gaussian Process (GP) regressor per training instance to approximate the regret function. During training, the system predicts parameters from context and checks if the GP's uncertainty is below a threshold. If confident, it uses the GP's gradient; otherwise, it falls back to Score Function Gradient Estimation (SFGE) to evaluate the true regret and update the GP. The method employs stochastic smoothing via importance sampling to provide informative gradients in piecewise-constant landscapes, and the fallback mechanism ensures exploration in uncertain regions.

## Key Results
- Reduces solver calls by up to two orders of magnitude compared to state-of-the-art approaches
- Maintains comparable or better decision quality across Knapsack, Weighted Set Multi-Cover, and synthetic benchmarks
- Scales well to high-dimensional problems with consistent performance across different problem classes
- Demonstrates effectiveness in black-box settings without requiring explicit problem structure knowledge

## Why This Works (Mechanism)

### Mechanism 1: Asymptotically Unbiased Surrogate Approximation
Replacing costly solver evaluations with GP surrogates maintains gradient fidelity while reducing computational cost, conditional on sufficient data coverage. GPs with RBF kernels are universal approximators, and as sample density increases, the approximation error converges to zero, preserving true local optima of the loss landscape.

### Mechanism 2: Stochastic Smoothing via Importance Sampling
Convolving the loss function with a Gaussian distribution provides informative gradients in piecewise-constant landscapes without requiring solver differentiation. The method uses importance sampling to re-weight previously collected samples, allowing a single solver call to contribute to smoothed loss estimation at multiple points.

### Mechanism 3: Confidence-Based Adaptive Fallback
Using the surrogate only when uncertainty is low prevents the model from exploiting erroneous gradients in unexplored regions. GPs output predictive standard deviation alongside the mean, and the system switches to SFGE when uncertainty exceeds a threshold, ensuring exploration is driven by real solver data.

## Foundational Learning

- **Concept: Decision Focused Learning (DFL) vs. Prediction Focused Learning (PFL)**
  - Why needed: The paper motivates the architecture on the "misalignment" between minimizing prediction error and minimizing decision regret.
  - Quick check: Can you explain why a model with lower Mean Squared Error might result in higher decision costs in a Knapsack problem?

- **Concept: Gaussian Processes (GPs) and Kernel Methods**
  - Why needed: The core surrogate is a GP. Understanding kernels and how GPs trade off prior beliefs against observed data is essential.
  - Quick check: Why is the predictive variance of a GP high in regions far from observed training points?

- **Concept: Score Function Gradient Estimation (SFGE)**
  - Why needed: SFGE serves as the "ground truth" fallback mechanism, necessary to understand what the GP is approximating.
  - Quick check: How does SFGE handle non-differentiable solvers to produce a gradient?

## Architecture Onboarding

- **Component map:** ML Predictor (h_θ) -> GP Bank -> Smoothing Module -> Controller -> Solver/SFGE
- **Critical path:** Efficiency gain relies on the Controller successfully predicting that the GP is confident for the vast majority of batches, avoiding the expensive Solver path.
- **Design tradeoffs:** One GP per sample vs. Global GP (limits dimensionality but prevents cross-sample generalization); Sample Sharing reduces calls but introduces bias if instances aren't truly similar.
- **Failure signatures:** High Solver Calls (β too low or pretraining insufficient); Divergence (β too high, chasing GP noise).
- **First 3 experiments:** Overfit Test (Toy Dataset) to verify GP approximation; Threshold Sensitivity (β) to find optimal confidence threshold; Scalability Stress Test on WSMC with increasing set counts.

## Open Questions the Paper Calls Out

### Open Question 1
Can the smoothing factor σ be adjusted dynamically during training to balance gradient quality and regret approximation without requiring new solver samples?
The conclusion identifies this as a direction of interest, as the current implementation relies on a fixed smoothing distribution.

### Open Question 2
Can Bayesian optimization acquisition functions replace the explicit fallback method (SFGE) to handle regions of high surrogate uncertainty?
The authors suggest this might remove the need for a fallback method, but it's unproven.

### Open Question 3
Is the proposed surrogate framework compatible with fallback methods other than SFGE, such as SPO+ or noise-contrastive estimation?
The methodology states this investigation is left for future research.

### Open Question 4
Can the online-trained GP surrogates be exported to accelerate training of new Decision Focused Learning tasks?
The conclusion notes this possibility but the transferability is not assessed.

## Limitations
- Confidence-based fallback mechanism lacks rigorous characterization of the relationship between GP variance and true regret error
- Scalability analysis focuses on solver call reduction rather than wall-clock time, creating uncertainty about practical deployment costs
- Computational overhead of maintaining hundreds or thousands of GP models per problem instance is not fully quantified

## Confidence

- **High Confidence:** The asymptotic unbiasedness of GP surrogates - supported by GP theory and universal approximation property of RBF kernels
- **Medium Confidence:** The effectiveness of importance sampling for gradient estimation - empirically validated but dependent on hyperparameter tuning
- **Medium Confidence:** The overall scalability claims - strong empirical evidence but limited analysis of computational overhead

## Next Checks

1. **Controller Calibration Test:** Systematically vary β across multiple problem instances and measure the correlation between GP variance and actual regret approximation error.

2. **Wall-Clock Benchmarking:** Measure end-to-end training time for the GP-based approach versus baselines, including all GP model updates and importance sampling computations.

3. **Out-of-Distribution Robustness:** Test the method on problem instances where context x significantly differs from training data to reveal performance under distributional shift.