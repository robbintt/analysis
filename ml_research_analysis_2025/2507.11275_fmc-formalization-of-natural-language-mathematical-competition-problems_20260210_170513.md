---
ver: rpa2
title: 'FMC: Formalization of Natural Language Mathematical Competition Problems'
arxiv_id: '2507.11275'
source_url: https://arxiv.org/abs/2507.11275
tags:
- language
- problems
- mathematical
- formalization
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an autoformalization pipeline that uses large
  language models with error feedback to convert natural language mathematical competition
  problems into formal Lean statements. The method combines few-shot prompting, formal
  verification via Lean REPL, backtranslation, and consistency checking to iteratively
  improve translation quality.
---

# FMC: Formalization of Natural Language Mathematical Competition Problems

## Quick Facts
- arXiv ID: 2507.11275
- Source URL: https://arxiv.org/abs/2507.11275
- Reference count: 35
- Primary result: Autoformalization pipeline converts 81.74% of Olympiad problems to semantically consistent Lean statements

## Executive Summary
This paper introduces an autoformalization pipeline that converts natural language mathematical competition problems into formal Lean statements using large language models with iterative error feedback. The method combines few-shot prompting, formal verification via Lean REPL, backtranslation, and consistency checking to improve translation quality. Applied to 4,798 Olympiad problems, the pipeline produces 3,922 aligned NL-Lean pairs with 81.74% semantic consistency. DeepSeek-R1 outperforms other LLMs, and ablation studies confirm the importance of few-shot learning, error feedback, and increased sampling for accuracy.

## Method Summary
The autoformalization pipeline uses a training-free approach with four stages: few-shot translation (with 2 fixed examples), formal verification via Lean 4 REPL, backtranslation to natural language, and consistency checking. When translations fail either formal verification or semantic consistency checks, error messages are fed back to the LLM for iterative refinement. The system processes each problem with temperature=1.0 and 5 samples, using DeepSeek-R1 for all stages. Missing proofs use ":= by sorry" placeholders, and geometry problems are excluded due to implicit constraint challenges.

## Key Results
- 81.74% semantic consistency between original and backtranslated natural language statements
- 93.39% syntactic validity rate for formalized Lean statements
- DeepSeek-R1 achieved 69.6% semantic consistency vs 38.4% (GPT-4o-mini) and 37.8% (Claude 3.7 Sonnet)
- Few-shot learning improved accuracy by 8.76% (82.46% → 91.22%)
- Increasing samples from 1 to 5 improved accuracy by 31.57% (59.65% → 91.22%)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Error Feedback for Self-Correction
Incorporating structured error information from verification failures into subsequent translation attempts improves formalization accuracy. When a formalized statement fails either Lean's syntax check or the semantic consistency check, the specific error message is fed back to the LLM as part of a revised prompt, enabling targeted correction rather than random resampling. This works because LLMs can interpret and respond to error messages meaningfully, adjusting their output to address specific syntactic or semantic failures. However, error feedback from formal verification showed limited effect (+1 pass vs +3 without feedback), suggesting structured compiler errors may be harder for LLMs to interpret than natural language consistency-check rationales.

### Mechanism 2: Backtranslation-Mediated Semantic Verification
Translating formalized Lean statements back to natural language and comparing with the original enables detection of semantic drift that passes syntax validation. A "round-trip" verification where NL→Lean translation is followed by Lean→NL backtranslation; an independent consistency check compares the two natural language versions for mathematical equivalence, catching missing conditions, swapped goals, or incorrect assumptions. This works because LLMs are more reliable at backtranslation (Lean→NL) than formalization (NL→Lean), and comparing natural language statements is easier than directly assessing NL-Lean semantic equivalence. Even with DeepSeek-R1, consistency checking shows false positives (precision 69.8%) and false negatives, indicating the round-trip check is imperfect.

### Mechanism 3: Few-Shot Domain Anchoring with Stochastic Sampling
Providing fixed examples from common mathematical domains (algebra, number theory) plus multiple stochastic samples increases both initial accuracy and cumulative success rate. Each translation prompt includes 2 fixed well-aligned examples; temperature=1.0 maintains output diversity; 5 samples per problem increase probability that at least one translation passes both checks. This works because domain-relevant examples improve in-context learning for formalization tasks, and sampling diversity compensates for individual translation failures. The 8.76% few-shot improvement appears partially confounded—both conditions used error feedback, and second-attempt translations excluded few-shot examples.

## Foundational Learning

- **Concept: Lean 4 and Mathlib4**
  - Why needed here: The target formal language; understanding that Lean requires syntactically correct statements and relies on Mathlib for pre-formalized mathematical objects is essential for interpreting error messages and evaluating formalization quality.
  - Quick check question: Can you explain why a Lean statement might pass syntax validation but fail semantic consistency with the original problem?

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The pipeline is training-free; all improvement comes from prompt engineering. Understanding how examples in prompts guide model behavior without gradient updates is critical for designing effective prompts.
  - Quick check question: If you had to add a third example to the few-shot prompt, what mathematical domain would you choose and why?

- **Concept: Temperature Sampling and Output Diversity**
  - Why needed here: The pipeline explicitly trades determinism for coverage via temperature=1.0 and 5 samples. Understanding this tradeoff is essential for tuning the exploration-exploitation balance.
  - Quick check question: What would happen to formalization success rate if you reduced temperature to 0.3 but increased samples to 20?

## Architecture Onboarding

- **Component map:**
  - NL problem -> Translation Module (DeepSeek-R1, few-shot) -> 5 candidate Lean statements
  - Each candidate -> Formal Verification (Lean 4 REPL) -> filter to syntactically valid statements
  - Valid statements -> Backtranslation Module (DeepSeek-R1) -> NL versions
  - Backtranslated NL + original NL -> Consistency Checker (DeepSeek-R1) -> accept/reject
  - Rejected: error feedback -> Translation Module (retry without few-shot examples)
  - Accepted: add to aligned dataset

- **Critical path:**
  1. NL problem → Translation Module (few-shot) → 5 candidate Lean statements
  2. Each candidate → Formal Verification → filter to syntactically valid statements
  3. Valid statements → Backtranslation Module → NL versions
  4. Backtranslated NL + original NL → Consistency Checker → accept/reject
  5. Rejected: error feedback → Translation Module (retry without few-shot examples)
  6. Accepted: add to aligned dataset

- **Design tradeoffs:**
  - **Training-free vs. specialized models**: Using off-the-shelf DeepSeek-R1 reduces deployment cost but may underperform a fine-tuned formalizer
  - **Semantic check via backtranslation vs. direct formal verification**: Backtranslation introduces noise but enables semantic validation; direct formal verification is syntactic only
  - **Geometry exclusion vs. dataset completeness**: Excluding geometry problems improves average quality but creates domain gaps; ~30% of competition problems may be geometric
  - **5 samples vs. computational cost**: More samples improve success rate linearly (31.57% gain from 1→5 samples) but multiply token costs ~5x

- **Failure signatures:**
  - **High formal verification pass rate but low consistency pass rate**: Indicates syntactically correct but semantically wrong formalizations (GPT-4o-mini showed 34%/10%)
  - **Consistency checker over-accepting**: Claude 3.7 Sonnet showed 96.9% recall but 57.4% precision in consistency checks—too permissive
  - **Missing implicit constraints**: Geometry example (Figure 2) passed both checks but lacked triangle angle-sum constraint—reveals pipeline cannot verify completeness
  - **Goal definition errors**: "Find" problems formalized as existence proofs without characterizing solution sets

- **First 3 experiments:**
  1. **Baseline ablation**: Run the full pipeline with error feedback disabled (random resampling only) on 100 problems to quantify feedback contribution vs. sampling luck.
  2. **Consistency checker calibration**: Cross-validate DeepSeek-R1 vs. GPT-4o-mini vs. Claude 3.7 Sonnet as consistency checkers on a manually-labeled held-out set (aiming for precision/recall balance).
  3. **Domain extension pilot**: Test whether adding a geometry example to the few-shot prompt (despite geometry exclusion) improves non-geometric formalization by clarifying implicit-constraint handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can formal statements that pass syntax verification but fail semantic consistency checks serve as effective few-shot learning examples to improve overall autoformalization accuracy?
- Basis in paper: [explicit] The authors state in Section 5.2 that whether these specific "formal statements that passed verification but failed consistency checks also served as few-shot learning data... warrants further study."
- Why unresolved: While few-shot learning improved accuracy, the experiment could not isolate whether the inclusion of incorrect but syntactically valid examples helped the model learn to avoid similar errors or if only correct examples provided the benefit.
- What evidence would resolve it: An ablation study comparing models trained on correct samples only versus those including syntactically valid but semantically inconsistent samples.

### Open Question 2
- Question: How can autoformalization pipelines be adapted to handle implicit constraints in Euclidean geometry problems, which are currently excluded due to high failure rates?
- Basis in paper: [explicit] Section 4.2 and Section 6 highlight that geometry problems were excluded because LLMs struggle with implicit constraints (e.g., angle sums) and Lean has expressive limitations in this domain.
- Why unresolved: Current models frequently omit essential implicit geometric constraints (e.g., the triangle inequality or angle sums) leading to formally valid but mathematically hollow statements, preventing the creation of geometry datasets.
- What evidence would resolve it: The successful formalization of a geometry test set with a semantic consistency rate approaching the 81.74% achieved for algebra/number theory.

### Open Question 3
- Question: Can the presentation of structured error messages from formal verifiers be optimized to enable LLMs to more effectively self-correct syntactic errors during the feedback loop?
- Basis in paper: [inferred] Table 6 and Section 5.3 show error feedback significantly improved consistency checks but had limited effect on formal verification pass rates, hypothesized to be because "error messages... are structured data that the translation model finds difficult to interpret."
- Why unresolved: LLMs currently struggle to interpret raw compiler/verifier syntax errors, limiting the self-correction capability for syntactic mistakes compared to semantic ones described in natural language.
- What evidence would resolve it: A comparative study where the verifier output is summarized or translated into natural language before being fed back to the model, showing a statistically significant increase in the "Second Pass" rate for formal verification.

## Limitations
- Geometry problems excluded due to implicit constraint challenges, creating ~30% gap in competition problem coverage
- Backtranslation accuracy assumptions unverified; pipeline cannot verify completeness of formalized statements
- Error feedback mechanism shows limited effectiveness for formal verification errors vs semantic ones

## Confidence
- **High**: Formal verification and syntactic validity metrics (93.39% pass rate); DeepSeek-R1 superiority over GPT-4o-mini and Claude 3.7 Sonnet; few-shot and sampling effects on accuracy
- **Medium**: Semantic consistency improvements via error feedback (confounded by retry-without-few-shot examples); quality assessment ratings (64.46% above average)
- **Low**: Backtranslation reliability assumption; geometry exclusion justification; error feedback mechanism effectiveness for formal verification errors

## Next Checks
1. **Error Feedback Mechanism Isolation**: Run the full pipeline with error feedback disabled (random resampling only) on 100 problems to quantify feedback contribution vs. sampling luck, controlling for the confounding factor where retry attempts excluded few-shot examples.
2. **Consistency Checker Calibration**: Cross-validate DeepSeek-R1 vs. GPT-4o-mini vs. Claude 3.7 Sonnet as consistency checkers on a manually-labeled held-out set to identify precision-recall tradeoffs and select optimal trade-off threshold.
3. **Geometry Constraint Detection Pilot**: Test whether adding geometry examples to the few-shot prompt improves non-geometric formalization by clarifying implicit-constraint handling, or implement explicit geometric constraint extraction as a preprocessing step.