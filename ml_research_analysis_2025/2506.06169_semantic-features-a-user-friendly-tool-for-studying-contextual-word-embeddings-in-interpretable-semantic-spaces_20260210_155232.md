---
ver: rpa2
title: 'semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings
  in Interpretable Semantic Spaces'
arxiv_id: '2506.06169'
source_url: https://arxiv.org/abs/2506.06169
tags:
- feature
- embeddings
- which
- word
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present semantic-features, a tool for analyzing contextual
  word embeddings by projecting them into interpretable semantic feature spaces. The
  system extracts embeddings, trains MLPs to map to feature norms, and supports hyperparameter
  tuning.
---

# semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces

## Quick Facts
- **arXiv ID:** 2506.06169
- **Source URL:** https://arxiv.org/abs/2506.06169
- **Reference count:** 9
- **Key outcome:** A tool that projects contextual word embeddings into interpretable semantic feature spaces, revealing that language models systematically encode construction-driven semantic distinctions in middle transformer layers

## Executive Summary
semantic-features is a user-friendly tool that enables researchers to analyze contextual word embeddings by projecting them into interpretable semantic feature spaces derived from human feature norms. The system extracts embeddings, trains MLPs to map to feature norms, and supports hyperparameter tuning, with an interactive demo for exploration. Applied to dative constructions, the tool reveals that language models consistently predict recipients in double-object constructions to be more animate and in prepositional constructions more place-like, particularly in middle transformer layers. This demonstrates that LMs capture nuanced, context-dependent semantic distinctions driven by syntactic constructions.

## Method Summary
The semantic-features tool extracts contextual word embeddings from transformer models, averages them across a corpus, and trains MLPs to project these embeddings onto interpretable semantic feature norms (e.g., Binder et al., 2016). The system supports hyperparameter tuning via Optuna and provides an interactive visualization interface. For the dative construction study, the authors created balanced sentence pairs with ambiguous proper nouns as recipients, extracted embeddings from BERT, RoBERTa, and ALBERT, and analyzed feature projections across transformer layers to identify construction-driven semantic effects.

## Key Results
- Language models consistently encode dative construction effects, predicting recipients in double-object constructions to be more animate and in prepositional constructions more place-like
- Middle transformer layers (6-9) show heightened sensitivity to these semantic distinctions compared to early or final layers
- The tool successfully identifies construction-driven semantic construal using interpretable semantic feature spaces

## Why This Works (Mechanism)

### Mechanism 1: CWE-to-Semantic-Space Projection via MLP Regression
Contextual word embeddings can be mapped to interpretable semantic feature norms through supervised MLP training. Averaged contextual embeddings serve as input, while human-annotated feature norm vectors serve as targets. The MLP learns to minimize MSE between predicted and ground-truth feature vectors, creating a projection function that generalizes to unseen contexts. This works because semantic information in CWEs aligns sufficiently with human feature norms.

### Mechanism 2: Layer-Specific Semantic Concentration in Transformer Architectures
Middle transformer layers (approximately 6-9 in BERT-family models) show heightened sensitivity to context-dependent semantic distinctions compared to earlier or final layers. As information flows through transformer layers, middle layers integrate broader sentential context and abstract semantic relations, making them particularly sensitive to construction-driven semantic effects like dative alternations.

### Mechanism 3: Construction-Grammar-Driven Semantic Construal
Syntactic constructions impose systematic biases on semantic interpretation of ambiguous arguments. The double-object construction is canonically associated with caused possession, biasing recipients toward animate interpretation, while the prepositional object construction supports both caused possession and caused motion, permitting inanimate (place-like) recipients. LMs learn these construction-semantics mappings from distributional exposure.

## Foundational Learning

- **Contextual Word Embeddings (CWEs):** Essential for understanding how token representations vary with context. *Quick check:* Given "bank" in "river bank" vs. "bank account," would you expect identical or different CWEs?
- **Semantic Feature Norms:** Provide interpretable target spaces for projection. *Quick check:* If a word has a high "Human" feature score, what does that mean in the Binder et al. (2016) framework?
- **Dative Alternation (Linguistic Background):** Critical for understanding why DO and PO constructions license different semantic interpretations. *Quick check:* In "She sent Chicago the package" vs. "She sent the package to Chicago," which construction biases toward interpreting "Chicago" as animate?

## Architecture Onboarding

- **Component map:** Embedding extraction (minicons) -> Corpus averaging -> MLP training (input: averaged CWEs, target: feature norms) -> Hyperparameter tuning (optuna) -> Projection of new tokens -> Feature score interpretation
- **Critical path:** Ensure corpus provides sufficient contexts per word for meaningful averaging, verify feature norm coverage for target vocabulary, confirm MLP converges, validate projection on held-out words before drawing linguistic conclusions
- **Design tradeoffs:** MLP vs. other architectures (MLP chosen for simplicity), averaging across contexts vs. context-specific training (averaging reduces noise but may lose fine-grained distinctions), choice of feature norms (Binder provides explicit definitions vs. broader coverage)
- **Failure signatures:** Validation loss does not decrease (check embedding extraction, feature norm alignment), projections show no difference across conditions (verify dataset construction, layer selection, feature relevance), contradictory layer-wise patterns (may indicate model-specific architecture differences)
- **First 3 experiments:** 1) Replicate the London example: extract BERT layer 8 embeddings for both dative constructions, project to Binder space, confirm higher animacy features in DO and higher place features in PO. 2) Test a new ambiguous recipient (e.g., "Paris") across all three models to verify generalization. 3) Ablate layer selection: project from layers 1, 5, 8, and 12 to confirm the middle-layer semantic sensitivity pattern.

## Open Questions the Paper Calls Out

### Open Question 1
Can the projection method be effectively extended to autoregressive language models? The current tool explicitly excludes autoregressive models because their embeddings only capture left-context for a given word. Developing and validating alternative extraction methods that yield meaningful projections for autoregressive models would resolve this.

### Open Question 2
Do the dative construction effects generalize beyond the 15 ambiguous proper nouns tested? The narrow stimulus set limits claims about whether LMs broadly capture construction-driven semantic shifts or whether effects are specific to this particular ambiguity type. Testing on diverse ambiguous nouns, different verb classes, multiple languages, and non-proper-noun recipients would provide broader validation.

### Open Question 3
Why do a small number of model-layer combinations show the opposite pattern (decreased animacy in DO)? These exceptions are reported but not analyzed or explained. Fine-grained analysis of which layers/models invert, whether specific sentence types drive the inversion, and whether this reflects noise or systematic linguistic sensitivity would resolve this.

## Limitations

- The projection method assumes averaged CWEs from a general corpus adequately capture contextual nuances needed for dative construction effects
- The study relies on a relatively small, balanced set of 450 sentence pairs with 15 proper nouns, limiting generalizability
- Layer-specific effects are reported but not mechanistically explainedâ€”it remains unclear whether this reflects general semantic processing or specific architectural properties

## Confidence

- **High confidence**: The technical implementation of semantic-features tool works as described (MLP training, hyperparameter tuning, interactive visualization)
- **Medium confidence**: The observed dative construction effects are statistically present and interpretable, but causal attribution to construction grammar requires further validation
- **Low confidence**: Claims about universal layer-specific semantic concentration may be model-specific rather than a general property of transformer architectures

## Next Checks

1. **Cross-corpus validation**: Repeat the entire analysis using CWEs extracted from a different corpus (e.g., Wikipedia or Common Crawl) to verify that dative construction effects are not artifacts of the BNC corpus properties.

2. **Construction priming manipulation**: Test whether pre-exposing models to DO or PO constructions influences subsequent dative interpretations, which would provide stronger evidence for construction-level semantic construal rather than simple lexical associations.

3. **Alternative feature space comparison**: Project the same CWEs into a different semantic feature space (e.g., McRae et al. features) to determine whether observed effects are specific to Binder features or represent more general semantic distinctions.