---
ver: rpa2
title: 'Evaluating Large Language Models for Anxiety, Depression, and Stress Detection:
  Insights into Prompting Strategies and Synthetic Data'
arxiv_id: '2511.07044'
source_url: https://arxiv.org/abs/2511.07044
tags:
- depression
- data
- stress
- examples
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for mental health
  detection, comparing GPT and Llama models with transformer-based architectures like
  BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset, we fine-tuned models
  for anxiety, depression, and stress classification, employing synthetic data generation
  to address class imbalance.
---

# Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data

## Quick Facts
- arXiv ID: 2511.07044
- Source URL: https://arxiv.org/abs/2511.07044
- Reference count: 40
- This study evaluates LLMs and transformer models for mental health detection, with Distil-RoBERTa achieving an F1 of 0.883 for GAD-2 and XLNet reaching 0.891 for PHQ-2.

## Executive Summary
This paper compares the effectiveness of transformer-based models and large language models for detecting anxiety, depression, and stress from clinical and social media text. The authors fine-tuned several transformer architectures (BERT, DistilBERT, RoBERTa, Distil-RoBERTa, XLNet) and evaluated GPT and Llama models using various prompting strategies on the DAIC-WOZ and Stress Detection datasets. Distil-RoBERTa and XLNet consistently outperformed other models, particularly for GAD-2 and PHQ tasks. The study also explored synthetic data generation to address class imbalance, finding that while synthetic data improved recall, it often decreased precision, highlighting the need for careful calibration.

## Method Summary
The study fine-tuned transformer-based models (BERT, DistilBERT, RoBERTa, Distil-RoBERTa, XLNet) on the DAIC-WOZ dataset for multi-class anxiety and depression classification, and used XGBoost with transformer embeddings. For stress detection, the authors applied LLM prompting strategies (five versions) with GPT-3.5 Turbo and Llama-3-8B on Reddit and Twitter data. Synthetic data augmentation was generated using GPT-3.5 with zero-shot and few-shot prompts. Performance was evaluated using weighted F1, precision, recall, specificity, Hamming loss, and ROC AUC.

## Key Results
- Distil-RoBERTa achieved the highest F1 score of 0.883 for GAD-2 classification.
- XLNet excelled in PHQ tasks, with F1 up to 0.891 for PHQ-2.
- For stress detection, the zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886.
- Transformer-based models consistently outperformed larger LLMs in mental health classification tasks.

## Why This Works (Mechanism)
None provided.

## Foundational Learning
- **Transformer fine-tuning**: Adapting pre-trained models to specific mental health classification tasks via gradient-based optimization. Why needed: Leverages learned language representations for better feature extraction. Quick check: Monitor validation loss and F1 for overfitting.
- **Synthetic data augmentation**: Generating additional training examples using LLMs to address class imbalance. Why needed: Increases diversity and recall, especially for underrepresented mental health categories. Quick check: Compare precision-recall trade-offs with/without synthetic samples.
- **Prompt engineering for LLMs**: Crafting specific input formats to elicit desired classification outputs. Why needed: Enables zero/few-shot inference without retraining. Quick check: Test multiple prompt versions for stability.
- **Multi-label classification**: Predicting multiple mental health labels simultaneously per input. Why needed: Reflects real-world complexity of co-occurring anxiety and depression. Quick check: Evaluate Hamming loss and label correlation.
- **Label derivation from existing scales**: Mapping PHQ-8/GAD-2 scores to PHQ-4 categories per external methodology. Why needed: Enables consistent labeling across datasets. Quick check: Validate binning thresholds on sample data.
- **Weighted F1 scoring**: Balancing precision and recall across imbalanced classes. Why needed: Provides fair evaluation for minority mental health labels. Quick check: Compare per-class vs. macro vs. weighted F1.

## Architecture Onboarding
- **Component map**: DAIC-WOZ data -> Preprocessing (remove bot, concat, label mapping) -> Fine-tune transformers -> Evaluate metrics; Stress Detection data -> LLM prompting + synthetic augmentation -> Evaluate metrics.
- **Critical path**: Data preprocessing (concatenation, bot removal, label mapping) -> model training/evaluation (fine-tuning or prompting) -> metric computation.
- **Design tradeoffs**: Transformer fine-tuning offers better accuracy but requires labeled data; LLM prompting avoids retraining but is sensitive to prompt quality; synthetic data boosts recall but risks precision loss.
- **Failure signatures**: Label mismatch due to incorrect PHQ-4 derivation; synthetic data degrading precision; poor performance on minority classes; prompt instability in LLM outputs.
- **Three first experiments**: 1) Reconstruct label mapping from PHQ-8/GAD-2 per methodology [16]; 2) Fine-tune DistilRoBERTa-base on DAIC-WOZ with weighted F1; 3) Compare precision-recall with and without 10K synthetic samples.

## Open Questions the Paper Calls Out
- **Open Question 1**: How can synthetic data generation be optimized to consistently improve precision without diluting classifier focus, particularly for depression detection tasks? The authors note that synthetic data often decreased precision and recall, but no clear optimization strategy is provided.
- **Open Question 2**: To what extent do transformer-based models and LLMs generalize to out-of-domain, external mental health datasets beyond DAIC-WOZ? All experiments used DAIC-WOZ and Reddit/Twitter data; cross-domain generalization remains untested.
- **Open Question 3**: What mechanisms drive the superior performance of fine-tuned transformer models over larger LLMs for mental health classification? DistilRoBERTa outperformed GPT-3.5 and Llama 3, but no analysis explains this counterintuitive result.
- **Open Question 4**: How do biases in training data manifest in mental health detection models, and what mitigation strategies preserve accuracy? The authors highlight bias as a challenge but conducted no bias analysis in this study.

## Limitations
- Hyperparameter choices for transformer fine-tuning are not fully specified, creating uncertainty about reproducibility.
- The effectiveness of synthetic data augmentation is demonstrated but requires careful calibration, as precision degradation is observed with increased synthetic samples.
- The comparison between transformer fine-tuning and LLM prompting strategies is limited by the lack of standardized prompt engineering protocols and the inherent variability in LLM responses.

## Confidence
- **High Confidence**: The relative ranking of transformer architectures (DistilRoBERTa and XLNet outperforming others) is robust across multiple experiments and metrics, supported by clear statistical improvements in weighted F1 scores.
- **Medium Confidence**: The effectiveness of synthetic data augmentation is demonstrated but requires careful calibration, as precision degradation is observed with increased synthetic samples.
- **Low Confidence**: The LLM prompting results are difficult to validate independently due to unspecified prompt versions and the stochastic nature of model outputs, making exact score reproduction challenging.

## Next Checks
1. **Label Mapping Verification**: Reconstruct the PHQ-4 labeling scheme from PHQ-8/GAD-2 using the referenced methodology [16] and validate against a small manually-labeled subset to ensure class distribution matches reported patterns.
2. **Synthetic Data Impact Analysis**: Implement a systematic ablation study comparing baseline performance against incremental synthetic data addition (10K, 100K, 1M samples) to quantify precision-recall trade-offs and identify optimal augmentation thresholds.
3. **Hyperparameter Sensitivity Testing**: Conduct a grid search over learning rates (1e-5 to 5e-5), batch sizes (8-32), and epochs (2-5) for DistilRoBERTa to establish the sensitivity of F1 scores to training configurations and identify performance plateaus.