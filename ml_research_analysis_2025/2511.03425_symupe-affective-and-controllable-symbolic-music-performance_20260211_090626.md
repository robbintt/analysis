---
ver: rpa2
title: 'SyMuPe: Affective and Controllable Symbolic Music Performance'
arxiv_id: '2511.03425'
source_url: https://arxiv.org/abs/2511.03425
tags:
- performance
- music
- score
- performances
- piano
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SyMuPe, a framework for generating expressive
  symbolic piano performances, and PianoFlow, a state-of-the-art model based on conditional
  flow matching. PianoFlow is trained on a large dataset of 2,968 hours of aligned
  scores and performances and supports both unconditional and text/emotion-controlled
  generation.
---

# SyMuPe: Affective and Controllable Symbolic Music Performance

## Quick Facts
- arXiv ID: 2511.03425
- Source URL: https://arxiv.org/abs/2511.03425
- Reference count: 40
- Primary result: PianoFlow achieves 67% win rate in listening tests against human performances and outperforms transformer-based baselines

## Executive Summary
This paper introduces SyMuPe, a framework for generating expressive symbolic piano performances, and PianoFlow, a state-of-the-art model based on conditional flow matching. PianoFlow is trained on a large dataset of 2,968 hours of aligned scores and performances, supporting both unconditional and text/emotion-controlled generation. The model achieves high-quality, human-like performances, outperforming transformer-based baselines and external models in objective metrics and subjective listening tests, with a 67% win rate in pairwise comparisons against human performances. It also enables intuitive multimodal control using emotion-weighted text embeddings. The approach enables near-real-time inference and can be integrated into interactive music applications.

## Method Summary
SyMuPe uses conditional flow matching (CFM) to generate expressive piano performances from symbolic scores. PianoFlow, the flagship model, is a transformer-based architecture that learns a time-dependent vector field to transport samples from a Gaussian prior to the target performance feature distribution. The model is trained on diverse multi-mask inpainting tasks, allowing it to perform unconditional generation, infilling, and continuation. Emotion and text control is achieved through classifier-weighted text embeddings from a frozen Flan-T5 encoder. The model uses iterative ODE sampling for inference, enabling near-real-time generation while maintaining high quality and temporal coherence.

## Key Results
- PianoFlow achieves 67% win rate in side-by-side listening tests against human performances
- Outperforms transformer-based baselines (SOTA, DExter) and external models (VirtuosoNet, BACHMO) in objective metrics (Pearson correlation, KL divergence)
- Enables fine-grained emotion and text control with intuitive multimodal inputs (e.g., "dreamy", "anger", "thunderstorms and lightning")
- Achieves near-real-time inference with 10-step ODE sampling and adaptive step size

## Why This Works (Mechanism)

### Mechanism 1: Conditional Flow Matching for Expressive Feature Generation
The model learns a time-dependent conditional vector field v_t(x_t, x_ctx, y; θ) that transports samples from a prior Gaussian distribution p₀ to the target performance feature distribution p₁ ≈ q(x). This is achieved by solving an Ordinary Differential Equation (ODE) where the trajectory is conditioned on score features y and known performance context x_ctx. The Optimal Transport (OT) path (linear interpolation) is used for efficient training. Unlike diffusion models, flow matching directly aligns the model's vector field with the target dynamics, leading to shorter sampling paths and faster inference.

### Mechanism 2: Multi-Mask Inpainting for Generalized Performance Rendering
During training, random binary masks m ∈ {0,1}ⁿ are applied to performance features. The model learns to predict masked features x_m given unmasked context x_ctx and score y. Masks cover random notes, beats, bars, segments, or end-of-sequence, forcing the model to learn local and global dependencies. This framework allows the model to act as a universal renderer capable of unconditional generation, infilling specific sections, or continuing from context within a single architecture.

### Mechanism 3: Emotion-Weighted Text Embedding Conditioning
An emotion classifier trained on a smaller labeled dataset predicts probabilities for 33 emotion labels. For the main dataset, this classifier provides "soft labels" (probabilities) for each bar. Text embeddings of emotion templates are averaged, weighted by these probabilities. The resulting bar-level embedding is used as condition c, injected into middle transformer layers, trained with classifier-free guidance. This bridges the pre-trained semantic space of Flan-T5 to the acoustic space of expressive piano performance, allowing intuitive control even with out-of-distribution text prompts.

## Foundational Learning

- **Concept**: Flow Matching / Rectified Flow
  - Why needed here: This is the core generative paradigm of PianoFlow, differing from standard diffusion by learning an ODE-based vector field for direct transport, key to high quality and speed.
  - Quick check question: How does the training objective of Conditional Flow Matching (matching u_t(x|x₁)) differ from the denoising score matching objective in diffusion models?

- **Concept**: Inpainting with Masks in Transformers
  - Why needed here: The paper frames all generation tasks as inpainting. Understanding how to construct x_ctx (context) and x_m (target) using binary masks is crucial for training and inference logic.
  - Quick check question: If you want to generate a performance from scratch (unconditional), what should the mask m look like? What if you want to smoothly transition between two pre-existing performance segments?

- **Concept**: Classifier-Free Guidance (CFG)
  - Why needed here: CFG is essential for effective text/emotion control, balancing unconditional generation quality with adherence to the text prompt.
  - Quick check question: How does the guidance strength parameter α in the inference equation ṽ_t = αv_t(cond) + (1-α)v_t(uncond) affect the final output?

## Architecture Onboarding

- **Component map**: Data → [SyMuPe Tokenizer] → [Emotion Classifier → Flan-T5 Text Encoder] → [PianoFlow (Transformer + Flow Matching)] → ODE Solver → MIDI Performance

- **Critical path**: Data → [Tokenizer] → [Emotion Classifier → Text Encoder] → Conditioned Training Loop (CFM) → Saved Model Checkpoint. Inference: Score + Text → [Tokenizer + Text Encoder] → [ODE Solver + PianoFlow] → MIDI Performance.

- **Design tradeoffs**:
  - Real-valued vs. Discrete tokens: Paper uses real-valued features to avoid quantization error, trading simplicity for fidelity.
  - Dataset scale vs. diversity: 2,968-hour proprietary dataset is crucial; open-source subsets showed overfitting.
  - Inference speed vs. quality: ODE steps (k=10) with adaptive stepping balances quality and near-real-time inference.

- **Failure signatures**:
  - Deadpan output: Weak guidance causes default to average, unexpressive performance.
  - Temporal incoherence: Non-iterative or improperly masked models produce disjointed timing.
  - Unresponsive control: Suboptimal text embedding injection leads to ignored text prompts.

- **First 3 experiments**:
  1. Reproduce Unconditional Generation: Train smaller PianoFlow on public subset (ASAP) to validate CFM training loop and ODE inference, measuring KL divergence.
  2. Ablate Control Injection: Test different conditioning injection methods (prepend, cross-attention, AdaLN) and measure emotion control accuracy using classifier as evaluator.
  3. Inpainting Boundary Test: Use trained model to inpaint medium-length segments within human performance and evaluate transition smoothness at boundaries using objective metrics and listening tests.

## Open Questions the Paper Calls Out
None

## Limitations

- **Dataset Availability**: The core PERiScoPe dataset (2,968 hours) includes 2,215 hours of proprietary data, making complete replication impossible without access to this private collection.
- **Exact Implementation Details**: Several hyperparameters and architectural specifics are underspecified, including the exact σ_min value for OT-CFM, emotion classifier architecture details, and precise alignment cleanup thresholds.
- **Baseline Comparisons**: Direct comparison with models like VirtuosoNet and DExter is limited by dataset differences (these models trained on MAESTRO), potentially affecting the validity of win-rate comparisons.

## Confidence

- **High Confidence**: The core technical mechanism (Conditional Flow Matching with multi-mask inpainting) is well-documented and theoretically sound. The paper provides sufficient mathematical detail and ablation studies to support the claim that PianoFlow achieves high-quality, human-like performances with near-real-time inference.
- **Medium Confidence**: The emotion control mechanism using weighted text embeddings is plausible and supported by listening test results, but the reliance on a trained emotion classifier and the bridge between Flan-T5 semantic space and musical expression introduces uncertainty about robustness to out-of-distribution prompts.
- **Low Confidence**: The 67% win rate against human performances, while impressive, requires careful interpretation. The listening test methodology (MUSHRA-style) and the specific selection of comparison pieces may influence results. Additionally, the claim of "near-real-time" inference depends on hardware and specific use cases.

## Next Checks

1. **Reproduce Unconditional Generation**: Train a smaller PianoFlow model on the public ASAP dataset to validate the CFM training loop and ODE inference pipeline. Measure KL divergence and compare qualitatively against baseline unconditional generation methods to confirm the benefits of iterative sampling over parallel MLM.

2. **Ablate Control Injection**: Systematically test different conditioning injection methods (prepend, cross-attention, AdaLN) and measure emotion control accuracy using the trained emotion classifier as an automated evaluator. This will validate the effectiveness of the chosen injection strategy and identify optimal guidance strengths.

3. **Inpainting Boundary Test**: Use the trained model to inpaint medium-length segments within human performances and evaluate transition smoothness at boundaries. Measure objective metrics (e.g., velocity and timing continuity) and conduct listening tests to ensure generated sections blend seamlessly with existing context, addressing the critical path concern of temporal coherence.