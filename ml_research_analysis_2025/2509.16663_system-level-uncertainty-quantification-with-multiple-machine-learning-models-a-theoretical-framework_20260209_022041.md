---
ver: rpa2
title: 'System-Level Uncertainty Quantification with Multiple Machine Learning Models:
  A Theoretical Framework'
arxiv_id: '2509.16663'
source_url: https://arxiv.org/abs/2509.16663
tags:
- uncertainty
- joint
- variables
- standard
- independent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for system-level uncertainty
  quantification when using multiple machine learning (ML) models with both input
  uncertainty and dependent model uncertainties. The core method idea is to decouple
  the coupling between input and model uncertainties by transforming them into independent
  standard normal variables, simplifying uncertainty propagation.
---

# System-Level Uncertainty Quantification with Multiple Machine Learning Models: A Theoretical Framework

## Quick Facts
- arXiv ID: 2509.16663
- Source URL: https://arxiv.org/abs/2509.16663
- Reference count: 10
- Key outcome: Theoretical framework for decoupling coupled input and model uncertainties in multi-ML systems using copula theory and whitening transformations

## Executive Summary
This paper presents a theoretical framework for system-level uncertainty quantification when using multiple machine learning models with both input uncertainty and dependent model uncertainties. The core method idea is to decouple the coupling between input and model uncertainties by transforming them into independent standard normal variables, simplifying uncertainty propagation. The framework provides formulations to express ML predictions as functions of independent standard normal variables, enabling the development of numerical algorithms for system UQ. A key outcome is that the approach handles the challenge of dependent model uncertainties—common when models share training data—by using copula theory and whitening transformations. The paper demonstrates the framework using Gaussian Process models, where the Cholesky decomposition simplifies the transformation. It identifies challenges in numerical integration due to irregular boundaries from model uncertainty and recommends careful handling of these to improve accuracy.

## Method Summary
The framework transforms the coupled uncertainty problem into one with independent standard normal variables through a multi-step process. First, input variables are transformed to independent standard normals via Nataf or Rosenblatt transforms. Second, model uncertainty is decoupled using the probability integral transform to create auxiliary uniform variables independent of inputs. Third, these uniform variables are transformed to standard normals while preserving correlation structure, then whitened using Cholesky decomposition to obtain independent variables. For Gaussian Process models, this simplifies to a direct whitening transformation. The method enables efficient numerical integration for system-level uncertainty quantification by working in the decoupled space.

## Key Results
- Provides theoretical formulation for expressing ML predictions as functions of independent standard normal variables (Eqs. 15, 23, 28)
- Demonstrates simplified transformation for Gaussian Process models using Cholesky decomposition
- Identifies numerical integration challenges due to irregular boundaries from model uncertainty
- Shows framework handles dependent model uncertainties through copula theory and whitening transformations

## Why This Works (Mechanism)

### Mechanism 1: Decoupling via Probability Integral Transform
The auxiliary variable Z_i = F_{Y_i|X}(Y_i; θ_i(X)) follows Uniform[0,1] by construction and is independent of X, breaking the coupling where model uncertainty parameters are functions of random inputs. The inverse transform reconstructs predictions from decoupled sources.

### Mechanism 2: Copula-Based Dependence Preservation
By Sklar's Theorem, the joint distribution of auxiliary Z variables equals the copula function, preserving dependence structure among multiple ML models while standardizing marginals to uniform.

### Mechanism 3: Whitening Transformation to Independent Standard Normals
Sequential transformation via inverse normal CDF and Cholesky-based whitening produces independent standard normal variables suitable for efficient numerical integration. For GP models, this simplifies to direct whitening of multivariate Gaussian predictions.

## Foundational Learning

- **Concept: Copula Theory and Sklar's Theorem**
  - Why needed here: The framework relies on copulas to separate marginal distributions from dependence structure when multiple ML models have correlated uncertainties
  - Quick check question: Given two random variables with known marginal CDFs F_1 and F_2 and joint CDF F_{12}, can you write the copula function C(u_1, u_2)?

- **Concept: Cholesky Decomposition for Whitening**
  - Why needed here: The transformation from correlated to independent standard normal variables uses Cholesky factorization of the covariance/correlation matrix
  - Quick check question: If Σ = CC^T where C is lower triangular, what is the distribution of C^{-1}Y when Y ~ N(μ, Σ)?

- **Concept: Probability Integral Transform**
  - Why needed here: The auxiliary Z variables are constructed via this transform to achieve uniform marginals independent of X
  - Quick check question: If U = F_X(X) where F_X is a continuous CDF, what is the distribution of U?

## Architecture Onboarding

- **Component map:** Input X -> Nataf/Rosenblatt transform -> U_X (independent standard normals) -> ML prediction with θ(X) -> Z_i = F_{Y_i|X}(Y_i; θ_i(X)) -> W_i = Φ^{-1}(Z_i) -> U_Z = C_W^{-1}W (independent standard normals) -> G(U_X, U_Z) -> Y output

- **Critical path:** The transformation T_X(U_X) must be implemented first to map inputs; then the conditional marginal CDFs F_{Y_i|X} must be evaluated at T_X(U_X) to construct Z; finally, the Cholesky factor C_W(X) must be computed (numerically for non-GP, analytically for GP) for decorrelation.

- **Design tradeoffs:**
  - GP models: Analytical transformations available (Eq. 28), simpler implementation, but requires Gaussian predictive distributions
  - Non-GP models: Requires numerical estimation of correlation matrix Σ_W and potentially numerical inverse CDF evaluation; more flexible but computationally intensive
  - Monte Carlo vs. fast probability integration: MC is straightforward with decoupled variables but inefficient for rare events (10^{-6} to 10^{-9}); fast integration preferred but faces irregular boundary challenges

- **Failure signatures:**
  - Numerical instability in Cholesky decomposition → indicates highly correlated outputs; try regularization or PCA-based whitening
  - Irregular integration boundaries → caused by model uncertainty varying across input space; may require adaptive sampling or boundary smoothing
  - Large integration errors with nonlinear ML models → coupling between nonlinearity and irregular boundaries; consider surrogate refinement or variance reduction

- **First 3 experiments:**
  1. Single-output GP with known input distribution: Verify that Eq. 28 produces correct output distribution by comparing Monte Carlo estimates from original and transformed formulations
  2. Two-output GP with dependent outputs: Validate that the decorrelation via Cholesky produces independent U_Y components; check correlation of sampled U_Y values
  3. Non-GP model (e.g., Bayesian neural network): Implement numerical estimation of Σ_W via sampling and compare full pipeline accuracy against direct Monte Carlo from the original coupled formulation

## Open Questions the Paper Calls Out

### Open Question 1
How can numerical integration algorithms be designed to accurately handle the highly irregular boundaries and nonlinearity introduced by model uncertainty in the transformed independent space? The paper identifies that integration boundaries "may be highly irregular" due to model uncertainty varying across the input space, requiring "careful and deliberate treatment" to prevent large integration errors.

### Open Question 2
What efficient numerical techniques can be developed to compute joint probabilities for high-dimensional, statistically dependent outputs where current methods fail? The paper notes that numerical methods for high-dimensional joint PDF integration are "currently limited" to dimensions of three or less, and "efficient and accurate numerical integration techniques are urgently needed" for dependent outputs.

### Open Question 3
How can the correlation matrix of the transformed standard normal variables (Σ_W) be estimated for general non-Gaussian models without relying on extensive Monte Carlo sampling? The framework requires Σ_W to decouple dependencies, but calculating it for non-Gaussian models currently requires the very sampling methods the framework aims to avoid for efficiency.

## Limitations
- Theoretical framework without empirical validation or numerical examples
- Assumes continuous predictive distributions and differentiable CDFs, limiting applicability to discrete or mixed-output ML models
- Numerical integration challenges from irregular boundaries, particularly for highly nonlinear models with varying uncertainty landscapes

## Confidence

- **High Confidence:** The decoupling mechanism via probability integral transform and copula-based dependence preservation are well-established statistical results that directly apply to this context
- **Medium Confidence:** The whitening transformation and its simplification for GP models are mathematically sound, but practical implementation challenges may reduce effectiveness
- **Low Confidence:** Claims about handling irregular integration boundaries and achieving efficient numerical integration lack empirical support due to absence of validation examples

## Next Checks

1. Implement the framework for a simple two-output GP regression problem with known input distribution and compare joint CDF estimates against brute-force Monte Carlo sampling from the original coupled formulation

2. Test the whitening transformation on a non-Gaussian ML model (e.g., Bayesian neural network) to verify that residual correlation in U_Z is acceptably low after decorrelation

3. Evaluate numerical integration accuracy using both standard Monte Carlo and fast probability integration methods on a problem with moderate input uncertainty and dependent model outputs, measuring convergence rates and error characteristics