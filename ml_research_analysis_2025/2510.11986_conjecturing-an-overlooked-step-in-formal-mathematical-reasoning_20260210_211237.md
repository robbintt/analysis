---
ver: rpa2
title: 'Conjecturing: An Overlooked Step in Formal Mathematical Reasoning'
arxiv_id: '2510.11986'
source_url: https://arxiv.org/abs/2510.11986
tags:
- conjecture
- lean
- statement
- formal
- informal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces conjecturing as an overlooked step in formal
  mathematical reasoning and presents a new benchmark, ConjectureBench, to evaluate
  this capability. The authors show that autoformalisation performance is substantially
  overestimated when the conjecture is assumed to be provided.
---

# Conjecturing: An Overlooked Step in Formal Mathematical Reasoning

## Quick Facts
- arXiv ID: 2510.11986
- Source URL: https://arxiv.org/abs/2510.11986
- Reference count: 37
- This paper introduces conjecturing as an overlooked step in formal mathematical reasoning and presents a new benchmark, ConjectureBench, to evaluate this capability.

## Executive Summary
This paper identifies conjecturing as a critical but often overlooked step in formal mathematical reasoning, demonstrating that current autoformalisation benchmarks substantially overestimate performance when they assume conjectures are provided. The authors introduce ConjectureBench as a new benchmark to evaluate conjecturing capabilities and present Lean-FIRE, an inference-time method that combines informal and formal reasoning to improve end-to-end autoformalisation. Their approach successfully autoformalises 13 new PutnamBench problems using GPT-4.1 and 7 using DeepSeek-V3.1, representing the first successful end-to-end autoformalisation for these problems. The method improves conjecturing performance by an average of 29.1% for GPT-4.1 and 14.0% for DeepSeek-V3.1 on their ConJudge metric, demonstrating that treating conjecturing as an independent task is essential for advancing mathematical reasoning systems.

## Method Summary
The authors propose Lean-FIRE, an inference-time method that addresses the conjecturing gap in formal mathematical reasoning by combining informal and formal reasoning approaches. The method works by first generating conjectures through informal reasoning, then formalising these conjectures into the Lean proof assistant language. This two-stage process treats conjecturing as an explicit, independent task rather than assuming it is implicitly handled within larger mathematical reasoning workflows. The approach leverages existing large language models but modifies the inference process to explicitly handle the conjecturing step, which previous autoformalisation systems had overlooked. By benchmarking this capability using their newly introduced ConJudge metric, the authors demonstrate that this explicit treatment of conjecturing substantially improves overall autoformalisation performance on challenging mathematical problems.

## Key Results
- Autoformalisation performance is substantially overestimated when conjectures are assumed to be provided
- Lean-FIRE achieves the first successful end-to-end autoformalisation of 13 new PutnamBench problems with GPT-4.1 and 7 with DeepSeek-V3.1
- The method improves conjecturing performance by an average of 29.1% for GPT-4.1 and 14.0% for DeepSeek-V3.1 on their ConJudge metric

## Why This Works (Mechanism)
The paper demonstrates that LLMs possess the requisite knowledge for mathematical reasoning but fail to perform well on autoformalisation tasks because they skip or inadequately handle the conjecturing step. By explicitly treating conjecturing as an independent task through the Lean-FIRE method, the system can better leverage the underlying mathematical knowledge already present in LLMs. The improvement in performance (29.1% for GPT-4.1 and 14.0% for DeepSeek-V3.1) shows that the bottleneck is not knowledge acquisition but rather the systematic integration of conjecturing into the reasoning pipeline. The ConJudge metric captures this improvement by specifically measuring conjecturing capability rather than just final proof success, revealing that the enhancement comes from better problem formulation rather than deeper mathematical insight.

## Foundational Learning

**Autoformalisation**: The process of converting informal mathematical statements into formal proof assistant languages like Lean. *Why needed*: Forms the foundation for rigorous mathematical verification and enables machine-checkable proofs. *Quick check*: Can the system convert a textbook theorem statement into valid Lean code?

**Conjecturing**: The step of formulating mathematical hypotheses or statements to be proven from problem descriptions. *Why needed*: Essential for breaking down complex problems into formalizable components and guides the direction of formal proofs. *Quick check*: Given a word problem, can the system generate the core mathematical statement to prove?

**Lean proof assistant**: A formal proof system and programming language for writing and verifying mathematical proofs. *Why needed*: Provides the gold standard for mathematical rigor and enables machine verification of correctness. *Quick check*: Can the generated formal statement be successfully type-checked by Lean?

**Inference-time methods**: Techniques that modify how LLMs generate responses during inference rather than training new models. *Why needed*: Allows performance improvements without expensive retraining and can be applied to existing models. *Quick check*: Does the method improve outputs when applied to off-the-shelf LLMs?

## Architecture Onboarding

**Component Map**: Problem Description -> Informal Reasoning (Conjecture Generation) -> Formalisation (Lean Code) -> Proof Attempt -> Success/Failure

**Critical Path**: The most critical path is from Problem Description through Informal Reasoning to Conjecture Generation, as errors in this initial conjecturing step propagate through the entire pipeline and cannot be recovered by subsequent formalisation or proof attempts.

**Design Tradeoffs**: The method trades computational efficiency for accuracy by explicitly handling conjecturing as a separate step, increasing inference time but improving success rates. This approach leverages existing LLMs without requiring specialized training, making it more practical but potentially less optimized than end-to-end trained systems.

**Failure Signatures**: Failures typically manifest as either incorrect conjecture generation (misunderstanding the problem) or formalisation errors (unable to translate informal conjectures into valid Lean syntax). The system struggles particularly with problems requiring deep mathematical insight versus pattern-matching.

**First Experiments**: 1) Benchmark conjecturing performance on simple vs. complex mathematical problems to establish difficulty scaling, 2) Compare Lean-FIRE performance against baseline autoformalisation without explicit conjecturing, 3) Test cross-model generalization by applying the method to different LLM architectures beyond GPT-4.1 and DeepSeek-V3.1.

## Open Questions the Paper Calls Out
None

## Limitations
- The ConJudge metric, while designed to better capture mathematical understanding, introduces uncertainty about what exactly is being measured and may not translate directly to actual mathematical problem-solving capability
- The sample size of 13 successfully autoformalised PutnamBench problems, while novel, represents a relatively small scale for establishing robust conclusions about the method's effectiveness across different mathematical domains
- The broader claim that LLMs "possess requisite knowledge" for mathematical reasoning based on conjecturing performance improvements is not fully established, as the paper shows treatment of conjecturing as a separate task helps but doesn't necessarily mean deep mathematical understanding versus pattern-matching capabilities

## Confidence

**High Confidence**: The core observation that autoformalisation performance is overestimated when conjectures are assumed to be provided is well-supported by the experimental results. The demonstration that treating conjecturing as an independent task is essential for improving autoformalisation performance is convincing based on the empirical evidence presented.

**Medium Confidence**: The claim that Lean-FIRE achieves "the first successful end-to-end autoformalisation" of the PutnamBench problems should be interpreted cautiously, as this depends on specific definitions of success and the particular evaluation framework used. The relative performance improvements (29.1% and 14.0%) are metric-dependent and may vary with different evaluation criteria.

**Low Confidence**: The broader claim that LLMs "possess requisite knowledge" for mathematical reasoning based on conjecturing performance improvements is not fully established. The paper shows that treating conjecturing as a separate task helps, but this doesn't necessarily mean LLMs have deep mathematical understanding versus pattern-matching capabilities.

## Next Checks

1. **Cross-metric validation**: Evaluate the same conjecturing tasks using alternative mathematical reasoning metrics (such as proof verification success rates or human expert assessment) to determine whether the ConJudge metric results generalize to other measures of mathematical capability.

2. **Domain generalization testing**: Apply Lean-FIRE to a broader range of mathematical domains beyond Putnam competition problems, including abstract algebra, topology, and real analysis, to assess whether the conjecturing improvements transfer across different mathematical subfields.

3. **Ablation study on conjecturing importance**: Systematically remove the conjecturing component from the autoformalisation pipeline and compare performance to establish the precise contribution of conjecturing versus other factors in the overall improvement, controlling for model selection and prompt engineering effects.