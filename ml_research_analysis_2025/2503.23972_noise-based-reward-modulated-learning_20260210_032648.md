---
ver: rpa2
title: Noise-based reward-modulated learning
arxiv_id: '2503.23972'
source_url: https://arxiv.org/abs/2503.23972
tags:
- learning
- reward
- neural
- noise
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Noise-based Reward-Modulated Learning (NRL),
  a synaptic plasticity rule that leverages stochastic neural activity to approximate
  gradients for learning in neuromorphic systems. NRL transforms the inherent noise
  of biological and neuromorphic substrates into a computational resource by using
  directional derivatives to approximate gradients, enabling local synaptic updates
  modulated by reward prediction errors.
---

# Noise-based reward-modulated learning

## Quick Facts
- **arXiv ID:** 2503.23972
- **Source URL:** https://arxiv.org/abs/2503.23972
- **Reference count:** 40
- **One-line primary result:** NRL uses stochastic noise in neural networks to approximate policy gradients for local synaptic updates, achieving competitive performance on RL benchmarks while maintaining neuromorphic compatibility.

## Executive Summary
Noise-based Reward-Modulated Learning (NRL) is a synaptic plasticity rule that transforms the inherent noise of neuromorphic systems into a computational resource for gradient-based learning. The method approximates true gradients using directional derivatives computed from noisy perturbations, enabling local synaptic updates modulated by reward prediction errors. NRL incorporates eligibility traces for credit assignment over behavioral timescales, allowing effective learning in both immediate and delayed reward scenarios. The approach is validated on standard reinforcement learning benchmarks (Reaching, Acrobot, Cartpole) with both immediate and delayed rewards, demonstrating competitive performance compared to backpropagation baselines while maintaining strict locality constraints suitable for neuromorphic hardware.

## Method Summary
NRL approximates policy gradients through noise-based directional derivatives by injecting Gaussian noise into neuron pre-activations during a "noisy pass" and comparing the resulting action probabilities to those from a clean pass. The difference in log-action-probability captures the perturbation impact, which is used to compute directional derivatives pointing along the true gradient. Eligibility traces accumulate local synaptic information between reward events, enabling credit assignment over multiple timesteps. When rewards arrive, the traces are modulated by reward prediction errors to produce weight updates. The method requires two forward passes per timestep (clean and noisy) and maintains a running average of rewards for baseline subtraction, creating a learning rule that is both theoretically grounded and practically implementable on neuromorphic hardware.

## Key Results
- NRL achieves competitive performance on RL benchmarks (Reaching, Acrobot, Cartpole) compared to backpropagation baselines
- NRL significantly outperforms reward-modulated Hebbian learning (RMHL), especially in deeper networks
- NRL successfully solves tasks with delayed rewards using eligibility traces for temporal credit assignment
- NRL maintains locality constraints suitable for neuromorphic hardware implementation

## Why This Works (Mechanism)

### Mechanism 1: Directional Derivative Gradient Approximation via Node Perturbation
NRL approximates true policy gradients using stochastic neural perturbations by injecting Gaussian noise ξˡₜ into neuron pre-activations during a "noisy pass." The difference in log-action-probability between noisy and clean passes (ρₜ = log π̃(aₜ|sₜ) - log π(aₜ|sₜ)) captures perturbation impact. The normalized noise vector ξ̄ₜ = ξₜ/||ξₜ||², weighted by ρₜ, provides a directional derivative estimate pointing along the true gradient. This works because Gaussian-distributed noise with zero mean enables unbiased gradient estimation through expectation over perturbation directions, and the central limit theorem ensures aggregated neuronal noise approaches Gaussian even if device-level noise is non-Gaussian.

### Mechanism 2: Eligibility Traces for Retrospective Temporal Credit Assignment
Eligibility traces bridge temporal gaps between actions and delayed rewards by accumulating local synaptic information (normalized noise ξ̄ₜ, perturbation impact ρₜ, presynaptic activity x̃ˡ⁻¹ₜ) into an eligibility trace eτₘ = Σₜ ξ̄ₜρₜ(x̃ₜ⁻¹)ᵀ. When reward arrives at τₘ, the trace is modulated by reward prediction error δτₘ to produce the weight update. This creates a temporary synaptic "tag" marking recently active connections for potential strengthening, enabling credit assignment without storing full episode histories or backpropagating through time.

### Mechanism 3: Reward Prediction Error Modulation for Variance Reduction
Using reward prediction error (RPE) δₜ = rₜ - r̄ₜ instead of raw reward stabilizes gradient estimates and enables continuous adaptation to changing reward statistics. A running average r̄ₜ with smoothing factor λ tracks expected reward baseline, and the RPE serves as a dopamine-like global neuromodulatory signal that only triggers significant updates when outcomes deviate from expectations. This acts as a variance reduction technique by subtracting the baseline from the policy gradient estimator.

## Foundational Learning

- **Policy Gradient Theorem and REINFORCE**
  - Why needed here: NRL is fundamentally a policy gradient method that approximates ∇J(θ) ≈ ∇log π(a|s)·δ. Understanding why this gradient points toward higher-expected-reward policies is essential for debugging learning failures and appreciating why noise-based approximation preserves the gradient direction in expectation.
  - Quick check question: If the policy probability for action a in state s increases, and that action led to positive RPE, why does expected return increase? What happens if RPE is negative?

- **Temporal Credit Assignment and Eligibility Traces**
  - Why needed here: Delayed rewards create the "distal reward problem"—credit must flow backward in time to actions that caused successful outcomes. Eligibility traces are the core mechanism enabling this in NRL without backpropagation through time.
  - Quick check question: In a Cartpole episode lasting 200 timesteps with reward only at the end, how does the synapse at layer 1 know whether its activity at step 50 contributed to balance maintenance? What information does the trace carry?

- **Forward-Mode vs Reverse-Mode Automatic Differentiation**
  - Why needed here: NRL approximates forward-mode differentiation (computing directional derivatives via perturbations) whereas backpropagation uses reverse-mode. This explains the different computational characteristics—forward-mode requires no graph storage but needs multiple passes for full gradient; reverse-mode computes full gradient in one backward pass but stores all activations.
  - Quick check question: Why does backpropagation require storing all layer activations during the forward pass? Why doesn't NRL have this memory requirement? What tradeoff does NRL accept instead?

## Architecture Onboarding

- **Component map:**
  - Network backbone -> Noise injection module -> Clean/noisy pass executor -> Log-probability computer -> Perturbation impact calculator -> Eligibility trace buffer -> Reward predictor -> RPE computer -> Synaptic update trigger

- **Critical path:**
  1. **Per-timestep (environment interaction):**
     - Observe state sₜ
     - Run clean pass → store log π(aₜ|sₜ)
     - Run noisy pass → store log π̃(aₜ|sₜ), activations x̃ˡ, noise ξˡ
     - Sample action from noisy policy
     - Compute ρₜ = log π̃ - log π
     - Accumulate trace: eˡ ← eˡ + ξ̄ˡρ(x̃ˡ⁻¹)ᵀ for all layers
  2. **At reward receipt (time τₘ):**
     - Receive reward rτₘ
     - Update baseline: r̄ ← r̄ + λ(r - r̄)
     - Compute RPE: δ = r - r̄
     - Update all weights: Wˡ ← Wˡ + η·δ·eˡ
     - Reset all traces: eˡ ← 0

- **Design tradeoffs:**
  - K=1 vs K>1 noise samples: Single sample per update (K=1) minimizes latency/compute; multiple samples reduce gradient variance at linear cost
  - Clean pass vs averaged noisy passes: Clean pass provides accurate baseline but requires noiseless computation; N averaged noisy passes approximate clean pass with error O(1/√N)
  - Node perturbation vs weight perturbation: Node (activation) perturbation operates in lower-dimensional space → lower variance; weight perturbation is more direct but higher variance
  - Full-trace summation vs exponential decay: Paper uses Σ accumulation (unbounded memory); decaying traces eₜ = αeₜ₋₁ + current would bound memory but introduce bias
  - Running average vs learned baseline: Simple r̄ₜ is computationally cheap; learned value function (critic) would improve variance reduction but add complexity and non-locality

- **Failure signatures:**
  - No learning (flat reward curve): σ too small (insufficient exploration) or too large (gradient drowns in noise); check that ρₜ varies non-trivially across timesteps
  - Weight divergence/spikes: η too high; implement gradient clipping or normalize RPE by reward magnitude (rτₘ/r̄τₘ)
  - RMHL-like degradation in deep networks: If implementing wrong rule (using ξ instead of ξ̄ρ), deeper networks will fail—verify equation matches Eq. 12 not Eq. 20
  - Slower convergence than backprop: Expected behavior due to stochasticity; acceptable if final performance matches BP
  - Clean pass approximation fails: If environment dynamics faster than multiple forward passes, averaged noisy approximation will lag; ensure hardware can execute N passes before environment changes
  - Poor performance on delayed-reward tasks: Trace accumulation period may be too long; verify trace is being reset after updates, check λ is appropriate for reward frequency

- **First 3 experiments:**
  1. **Validate single-layer immediate-reward learning (Reaching task):**
     - Implement NRL with 1 hidden layer (128 units) on Reaching
     - Compare learning curves vs BP baseline and RMHL
     - Hyperparameter sweep: η ∈ {1e-3, 1e-2, 5e-2}, σ ∈ {1e-4, 1e-3, 1e-2}
     - Success criterion: NRL reaches ≥90% of BP final performance within 1000 episodes

  2. **Ablate clean-pass approximation on delayed-reward task (Acrobot):**
     - Run NRL with true clean pass (baseline)
     - Run NRL with N=2, 5, 10 averaged noisy passes (no clean pass)
     - Plot approximation error ||y_clean - (1/N)Σy_noisy|| vs N
     - Plot performance curves for each N
     - Determine minimum N for ≤5% performance degradation vs clean-pass baseline

  3. **Characterize depth scalability (Cartpole with 1-3 hidden layers):**
     - Train networks with L=1, 2, 3 hidden layers (64 units each)
     - For each depth, compare NRL vs BP vs RMHL
     - Measure: trials to 90% max performance, final performance variance across seeds
     - Verify RMHL fails to improve beyond random in L≥2; NRL maintains scalability
     - Document convergence slowdown factor per additional layer for NRL vs BP

## Open Questions the Paper Calls Out
1. Can the need for separate "clean" and "noisy" forward passes be eliminated to reduce latency?
2. How can NRL be adapted to function effectively on physical neuromorphic hardware utilizing spiking neural networks (SNNs)?
3. Does incorporating a learned value function improve NRL's performance in environments requiring long-term planning?

## Limitations
- The paper doesn't specify weight initialization schemes or exact LeakyReLU negative slope parameters, which could affect reproducibility
- The theoretical guarantees assume Gaussian noise distributions, but real neuromorphic hardware may exhibit non-Gaussian noise characteristics
- The running average baseline (λ=0.66) is shown to work but may not be optimal for all environment reward structures

## Confidence
- Claims about NRL's ability to approximate true gradients through noise-based directional derivatives: **High confidence**
- Claims about eligibility trace-based credit assignment mechanism: **High confidence**
- Claims about achieving competitive performance while maintaining locality constraints: **Medium confidence**

## Next Checks
1. Test NRL performance when injecting non-Gaussian noise (e.g., uniform, Laplace) to validate the assumption that aggregated neuronal noise approaches Gaussian even if device-level noise is non-Gaussian
2. Implement exponential decay for eligibility traces (eₜ = αeₜ₋₁ + current) and compare performance on long-horizon tasks to determine if unbounded accumulation creates variance issues
3. Systematically vary K (number of noise samples per update) and measure the tradeoff between gradient variance reduction and computational cost, particularly for deeper networks where variance amplification is most severe