---
ver: rpa2
title: 'MedDCR: Learning to Design Agentic Workflows for Medical Coding'
arxiv_id: '2511.13361'
source_url: https://arxiv.org/abs/2511.13361
tags:
- coding
- workflow
- workflows
- medical
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MedDCR, a framework that automates the design
  of workflows for medical coding by treating it as a learning problem. Instead of
  relying on fixed, manually crafted pipelines, MedDCR iteratively generates, executes,
  and evaluates workflow designs using a meta-agent architecture: a Designer proposes
  workflows, a Coder compiles and executes them, and a Reflector provides evaluation
  scores and textual feedback.'
---

# MedDCR: Learning to Design Agentic Workflows for Medical Coding

## Quick Facts
- arXiv ID: 2511.13361
- Source URL: https://arxiv.org/abs/2511.13361
- Reference count: 40
- Primary result: Improves F1 scores by 6.2% on MDACE and 7.4% on ACI-BENCH compared to strong baselines

## Executive Summary
MedDCR automates the design of medical coding workflows by treating it as a learning problem. Rather than using fixed pipelines, it iteratively generates, executes, and evaluates workflow designs through a meta-agent architecture: a Designer proposes workflows, a Coder compiles and executes them, and a Reflector provides evaluation scores and feedback. A memory archive stores past designs to enable reuse and refinement. On benchmark datasets, MedDCR achieves significant improvements over manual baselines while producing interpretable and adaptable workflows that better reflect real coding practice.

## Method Summary
MedDCR automates medical coding workflow design through an iterative meta-agent loop. The framework treats workflow design as a search problem, where a Designer agent proposes workflow plans using a component library, a Coder agent compiles these into executable Python controllers with self-fixing for syntax errors, and a Reflector agent evaluates predictions against ground truth providing both scores and textual feedback. A memory archive stores past designs to enable exploitation-exploration balance through top-k best and recent-n workflows. The system runs for 100 iterations, starting from seed workflows like CoT-SC, Multi-Debate, and NER strategies.

## Key Results
- Achieves 6.2% F1 improvement on MDACE dataset compared to strong baselines
- Achieves 7.4% F1 improvement on ACI-BENCH dataset compared to strong baselines
- Ablation studies show both score and textual feedback are critical, with removing either causing 0.04-0.09 F1 drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative design-execute-reflect loops enable automated discovery of effective workflow configurations that outperform manually designed pipelines
- Mechanism: A Designer agent proposes workflow plans by combining tools and strategies from a component library; a Coder agent compiles these into executable programs with self-fixing for syntax errors; a Reflector agent evaluates predictions against ground truth and generates both scalar scores and textual feedback. This feedback is fed back into the next design iteration
- Core assumption: The search space contains locally discoverable optima, and LLM-based agents can navigate it via textual feedback signals
- Evidence anchors:
  - [abstract] "MedDCR improves F1 scores by 6.2% on MDACE and 7.4% on ACI-BENCH compared to strong baselines"
  - [section] Table 4 ablation: removing score feedback drops F1 from 0.47 to 0.43; removing textual feedback prevents iterative improvement
  - [corpus] Related multi-agent coding work (CLH, MAC) shows comparable agentic gains, though corpus evidence for this specific optimization loop is limited (avg citations ≈0)
- Break condition: If workflows converge to local optima or feedback becomes uninformative, search stagnates—mitigated by the memory archive's recent-n examples for exploration

### Mechanism 2
- Claim: Memory archive with top-k and recent-n selection maintains exploitation-exploration balance during workflow search
- Mechanism: The archive stores tuples (π, W, s, r) for all past workflows. The Designer conditions on both high-performing exemplars (exploitation) and diverse recent attempts (exploration). This prevents collapse into repetitive patterns while preserving successful strategies
- Core assumption: Effective workflows share reusable patterns that can be recombined; diversity in recent proposals maintains search breadth
- Evidence anchors:
  - [section] Section 3.4: "removing the top-k or recent-n workflows causes the largest performance drops of 0.09 and 0.13 in F1"
  - [section] Algorithm 1 formalizes the archive update and selection process
  - [corpus] Memory-augmented agents (MemoryBank) show similar benefits in long-horizon tasks, though direct corpus evidence for this specific top-k/recent-n scheme is absent
- Break condition: If all stored workflows are poor quality or overly similar, the archive provides weak guidance—seed workflows help bootstrap

### Mechanism 3
- Claim: Plug-and-play initialization with expert workflows enables both refinement of existing designs and search from scratch
- Mechanism: Seed workflows (e.g., CoT-SC, Multi-Debate, NER) are injected into H₀ and treated as archive entries. MedDCR can directly reuse, refine, or combine them with newly generated strategies. This bridges automated search with human expertise
- Core assumption: Expert workflows contain valuable structure but are suboptimal; automated refinement can improve them
- Evidence anchors:
  - [section] Table 3: CoT-SC seed alone achieves 0.42 F1; with all auxiliary seeds reaches 0.47 F1; seeded outperforms scratch (0.44)
  - [abstract] "produces interpretable, adaptable workflows that better reflect real coding practice"
  - [corpus] Expert-designed workflows (MAC, CLH) show competitive baseline performance, supporting their value as seeds
- Break condition: If seeds are too narrow or incompatible with the task, they may constrain search—diverse seed types mitigate this

## Foundational Learning

- Concept: **Medical coding as multi-step reasoning**
  - Why needed here: Unlike flat classification, ICD coding requires entity extraction, guideline application, hierarchy navigation, and cross-document consistency—understanding this clarifies why workflow structure matters
  - Quick check question: Can you explain why a note mentioning "Type 2 diabetes with chronic kidney disease" requires both E11.22 and N18.9 codes with specific linkage rules?

- Concept: **Directed Acyclic Graphs (DAGs) for workflow representation**
  - Why needed here: MedDCR represents workflows as DAGs (W = (G, ϕ)) where nodes are components and edges are data/control flow—essential for understanding the search space
  - Quick check question: Given a workflow with extraction → validation → reconciliation steps, how would adding a parallel self-refinement branch change the DAG structure?

- Concept: **Exploitation-exploration tradeoff in optimization**
  - Why needed here: The top-k (exploitation) vs. recent-n (exploration) selection mechanism directly implements this tradeoff; ablation shows both are critical
  - Quick check question: If you removed recent-n selection and only used top-k, what behavior would you expect over 100 iterations?

## Architecture Onboarding

- Component map:
  - Designer Agent -> Workflow Plan (JSON)
  - Coder Agent -> Executable Python Controller
  - Reflector Agent -> Score + Textual Feedback
  - Memory Archive stores all (plan, workflow, score, feedback) tuples
  - Tool Library provides ICD-10 functions and LLM-based tools

- Critical path:
  1. Initialize archive H₀ with seed workflows (general strategies + domain-specific tools)
  2. Designer generates plan π_t conditioned on top-k + recent-n from H_{t-1}
  3. Coder compiles π_t → W_t with self-fixing (retries on errors)
  4. Execute W_t on D_val to get predictions
  5. Reflector computes score s_t and feedback r_t; append (π_t, W_t, s_t, r_t) to archive
  6. Repeat for T iterations; return W* = argmax score in H_T

- Design tradeoffs:
  - Search cost vs. performance: 100 iterations with GPT-4o costs ~$17/100 samples on MDACE (mostly execution tokens, not search)—Table 2
  - Seed diversity vs. convergence speed: More seed types improve final F1 (Table 3) but may slow early convergence
  - Archive size vs. prompt length: Including more exemplars helps but increases token costs and may dilute signal

- Failure signatures:
  - Stagnant search: F1 plateaus early—check if recent-n is too small or feedback is generic
  - High syntax error rate: Coder self-fixing loop exceeds retries—review tool signatures in meta-prompt
  - Low recall despite high precision: Workflows over-prune candidates—check filtering thresholds (τ_keep, γ) in discovered workflows

- First 3 experiments:
  1. Baseline validation: Run MedDCR with only CoT-SC seed, 20 iterations, on a small validation split—verify the loop executes and scores improve monotonically
  2. Ablation sanity check: Remove textual feedback from Reflector; confirm F1 drops (should see ~0.0 drop per Table 4) to validate mechanism
  3. Seed diversity test: Compare 3 runs with (a) no seeds, (b) CoT-SC only, (c) all auxiliary seeds—measure final F1 and convergence speed to quantify plug-and-play benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating multimodal clinical signals (e.g., medical imaging, structured EHR data) improve MedDCR's predictive accuracy and cross-domain generalization?
- Basis in paper: [explicit] Limitations section states: "the current framework operates purely on textual information, incorporating multimodal signals such as visual or structured clinical data could enhance its ability to capture richer diagnostic cues"
- Why unresolved: All experiments used only text-based clinical notes; multimodal integration requires architectural modifications and new data modalities not explored in this work
- What evidence would resolve it: Ablation comparing text-only MedDCR against multimodal variants on datasets with linked imaging/structured data, measuring F1 improvements and cross-domain transfer

### Open Question 2
- Question: Can MedDCR generalize effectively to multi-lingual medical coding settings?
- Basis in paper: [explicit] Limitations section states: "our experiments focus primarily on ICD-10 coding, extending MedDCR to multi-lingual settings would provide stronger evidence of generality"
- Why unresolved: All evaluations used English-only datasets (MDACE, ACI-BENCH); language-dependent components (prompts, tool definitions) were not tested on non-English clinical corpora
- What evidence would resolve it: Evaluation on multi-lingual benchmarks (e.g., CodiEsp for Spanish) with F1 scores comparable to English results

### Open Question 3
- Question: Would a medically fine-tuned foundation model improve MedDCR's accuracy and guideline compliance compared to general-purpose GPT models?
- Basis in paper: [explicit] Limitations section notes: "the framework relies on general-purpose GPT models that are not fine-tuned for medical coding, which may limit their understanding of domain-specific terminology and guideline nuances"
- Why unresolved: Only GPT-4o and GPT-5 were evaluated; the contribution of domain-specific pretraining to workflow discovery and guideline adherence remains unmeasured
- What evidence would resolve it: Controlled comparison of MedDCR with general-purpose vs. clinically fine-tuned LLMs on identical benchmarks, analyzing both F1 scores and guideline violation rates

### Open Question 4
- Question: How can the Reflector's feedback be made more structured and reliable rather than heuristic?
- Basis in paper: [explicit] Limitations section states: "the textual feedback generated by the Reflector is heuristic and may occasionally produce ambiguous or overly general guidance, suggesting a need for structured reflection or rule-grounded critics"
- Why unresolved: The Reflector produces free-form textual critiques without formal validation of diagnostic accuracy or actionability, potentially limiting iterative workflow improvement
- What evidence would resolve it: Comparative study measuring feedback quality from current Reflector vs. rule-grounded critics, and their impact on workflow improvement rates across iterations

## Limitations

- **Model availability**: The reported top results use GPT-5, which is not publicly available, limiting reproducibility and suggesting potential performance inflation
- **Search space uncertainty**: The workflow optimization assumes the search space contains locally discoverable optima, but the paper provides limited analysis of search convergence or local optima traps
- **Cost scaling**: While individual iteration costs are modest (~$17/100 samples), the approach requires 100+ iterations per dataset, raising questions about practical deployment economics

## Confidence

- **High confidence**: The core iterative design-execute-reflect mechanism works as described, supported by ablation studies showing the importance of both score and textual feedback
- **Medium confidence**: The memory archive's top-k/recent-n mechanism provides exploitation-exploration balance, though the specific hyperparameter choices lack theoretical justification
- **Low confidence**: Performance comparisons using GPT-5 cannot be validated with current models, making it difficult to assess real-world applicability

## Next Checks

1. **Ablation validation**: Remove textual feedback from the Reflector agent and verify the reported F1 drop of approximately 0.04 matches the paper's findings

2. **Seed dependency test**: Run MedDCR with zero seeds versus all auxiliary seeds on a small validation split to quantify the plug-and-play benefit and assess whether expert knowledge is essential

3. **Cost-performance tradeoff**: Measure F1 improvement per dollar spent across different iteration counts (e.g., 20, 50, 100 iterations) to identify optimal resource allocation