---
ver: rpa2
title: 'AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains'
arxiv_id: '2601.15511'
source_url: https://arxiv.org/abs/2601.15511
tags:
- adversarial
- factuality
- arxiv
- domains
- finance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces AdversaRiskQA, a benchmark for evaluating
  large language models (LLMs) under adversarial factuality conditions in high-risk
  domains. The benchmark includes domain-specific datasets covering Health, Finance,
  and Law, each with basic and advanced difficulty levels.
---

# AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains

## Quick Facts
- **arXiv ID**: 2601.15511
- **Source URL**: https://arxiv.org/abs/2601.15511
- **Reference count**: 40
- **Primary result**: Qwen3-80B achieves highest average accuracy (94.7%) across Health, Finance, and Law domains in adversarial factuality evaluation

## Executive Summary
AdversaRiskQA introduces a benchmark for evaluating large language models under adversarial factuality conditions in high-risk domains. The benchmark includes domain-specific datasets covering Health, Finance, and Law, each with basic and advanced difficulty levels. The study proposes two automated evaluation methods: an LLM-as-judge approach for adversarial factuality and a search-augmented agentic method for long-form factuality assessment. Results demonstrate that adversarial factuality performance scales non-linearly with model size, with gaps between difficulty levels narrowing as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and factual output accuracy.

## Method Summary
The benchmark uses three domain-specific datasets with adversarial prompts that inject misinformation using confident framing ("As we know..."). Basic difficulty targets common misconceptions while advanced tests specialized knowledge. Six models were evaluated: Qwen3-4B/30B/80B, GPT-OSS-20B/120B, and GPT-5. Adversarial factuality is assessed using GPT-5-mini as LLM-as-judge with high-effort reasoning. Long-form factuality employs an agentic search-augmented method via OpenAI Response API, decomposing responses into verifiable facts and validating via web search with F1@K scoring.

## Key Results
- Qwen3-80B achieves highest average accuracy (94.7%) across domains
- GPT-5 maintains consistently high accuracy across difficulty levels
- Performance scales non-linearly with model size, with basic-advanced gaps narrowing at larger scales
- No significant correlation found between injected misinformation and long-form factual accuracy

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Prompt Detection and Correction
- Claim: LLMs can detect and correct confidently stated misinformation when model scale and alignment are sufficient
- Mechanism: Strong confidence cues frame false premises as authoritative; larger models with robust knowledge override this framing by referencing internal knowledge or generating explicit corrections
- Core assumption: Models have acquired relevant domain knowledge during pre-training and can access it when conflicting signals arise
- Evidence anchors:
  - [abstract] "Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods."
  - [section 4.1] "GPT-5 achieves the highest overall accuracy, demonstrating strong factual reasoning and robustness against misinformation."
  - [corpus] Related work (Sakib et al., "Battling Misinformation") establishes that confidence levels in adversarial prompts affect attack success rates, supporting the confidence-framing mechanism
- Break condition: When domain knowledge is absent or weakly represented in training data, models may accept false premises regardless of size

### Mechanism 2: Non-Linear Performance Scaling with Model Size
- Claim: Adversarial factuality robustness improves non-linearly with model scale, with narrowing difficulty gaps at larger sizes
- Mechanism: Larger models develop more robust internal knowledge representations and reasoning capacities, enabling better discrimination between confident framing and factual accuracy across difficulty levels
- Core assumption: Scaling primarily improves knowledge integration and reasoning rather than surface pattern matching
- Evidence anchors:
  - [abstract] "Performance scales non-linearly with model size, varies across domains, and gaps between difficulty levels narrow as models grow."
  - [section 4.1.2] "After filtering out invalid responses, the reduced gaps across domains suggest that much of the variation in the original results reflected response errors rather than genuine differences in reasoning."
  - [corpus] Weak direct corpus evidence on non-linear scaling specifically for adversarial factuality; related work focuses on general factuality improvements with scale
- Break condition: Non-linear gains may plateau if alignment objectives conflict with factual correction behavior

### Mechanism 3: Long-Form Factuality Decoupling
- Claim: Injected misinformation in adversarial prompts does not significantly correlate with factual accuracy in long-form outputs
- Mechanism: Long-form generation involves multi-step reasoning and claim composition that distributes factual grounding across the response; single false premises have limited propagation through this process
- Core assumption: Long-form generation decomposes into independently verifiable facts that draw from diverse knowledge sources
- Evidence anchors:
  - [abstract] "Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output."
  - [section 4.2] "Adversarial prompts reduce factual accuracy in most domains, but the difference is not significant, especially considering the limited number of samples and the models' non-determinism in response length."
  - [corpus] Related work (Wei et al., "Long-form factuality in large language models") establishes SAFE methodology; corpus lacks direct evidence on adversarial-to-long-form propagation
- Break condition: Decoupling may fail when adversarial premises are central to the query topic and cannot be circumvented in extended responses

## Foundational Learning

- Concept: **Adversarial Factuality vs. Hallucination**
  - Why needed here: Distinguishes external misinformation injection from internally generated fabrications; essential for understanding the benchmark's threat model
  - Quick check question: If a prompt contains "As we know, vaccines cause autism" and the model repeats this claim, is this a hallucination or adversarial factuality failure?

- Concept: **LLM-as-Judge Reliability Limitations**
  - Why needed here: The evaluation pipeline relies on GPT-5-mini as judge; understanding its limitations is critical for interpreting results
  - Quick check question: Why might an LLM judge be unreliable for evaluating adversarial corrections in specialized domains like law or medicine?

- Concept: **Difficulty Level Design for Knowledge Depth Testing**
  - Why needed here: Basic/advanced splits test whether models handle common misconceptions vs. specialized knowledge gaps differently
  - Quick check question: What different failure modes would you expect for basic vs. advanced adversarial prompts in the finance domain?

## Architecture Onboarding

- Component map:
  - Dataset curation pipeline -> Model inference layer -> Evaluation layer -> Error handling
  - (3-stage: source collection → structured item generation → human review) → (6 models: Qwen3-4B/30B/80B, GPT-OSS-20B/120B, GPT-5) → (LLM-as-judge + search-augmented agentic method) → (filter null outputs, prompt echoes, template leakage)

- Critical path:
  1. Curate domain-specific adversarial prompts with verified ground truth
  2. Run inference across all models with standardized system prompts
  3. Apply error filtering (exclude invalid responses)
  4. Execute LLM-judge evaluation for misinformation correction
  5. For long-form assessment: decompose responses into facts → validate via web search → compute F1@K

- Design tradeoffs:
  - Automated vs. manual evaluation: LLM-judge enables scale but achieves only 90-95% agreement with human review; manual validation used on stratified 20% subset
  - Single model for long-form testing: Cost constraints limited deep analysis to Qwen3-30B; broader validation deferred
  - Temperature control: GPT-5 lacks temperature parameter, introducing non-determinism; other models at 0.0 for reproducibility

- Failure signatures:
  - Null outputs: Safety filters triggered by sensitive topics (pregnancy, mental health, criminal liability)
  - Prompt echo: Model fails to transition from reading to generating (more common in smaller models)
  - Template leakage: Chat format confusion produces system messages instead of answers
  - Domain-specific patterns: Law triggers safety alignment around actionable advice; health triggers prescriptive guidance avoidance

- First 3 experiments:
  1. Baseline adversarial robustness: Run filtered evaluation across all 6 models on health/finance/law basic+advanced; expect Qwen3-80B and GPT-5 to show smallest basic-advanced gaps
  2. Confidence level ablation: Modify adversarial prompts from "As we know..." to "I think..." and "I guess..." per Sakib et al.; hypothesize improved detection rates with lower confidence framing
  3. Long-form propagation test: Extend long-form factuality assessment to GPT-OSS-120B and GPT-5; check if "no significant correlation" finding replicates across architectures or is specific to Qwen3-30B

## Open Questions the Paper Calls Out

- **Open Question 1**: Does adversarial factuality performance transfer across model architectures and families beyond Qwen, GPT-OSS, and GPT-5?
  - Basis: Only three model families evaluated; architectural differences may produce distinct robustness patterns
  - Why unresolved: Study deliberately limited scope to three families
  - What evidence would resolve it: Systematic evaluation of additional model families with controlled parameter-matched comparisons

- **Open Question 2**: How does adversarial factuality performance vary across languages, particularly in domain-specific terminology?
  - Basis: Datasets are English-only, limiting generalizability; domain terminology often lacks direct translation equivalents
  - Why unresolved: Adversarial confidence cues may have different pragmatic effects cross-linguistically
  - What evidence would resolve it: Translated and culturally-adapted versions evaluated on multilingual models with native speaker validation

- **Open Question 3**: What mechanisms explain the non-linear relationship between model size and adversarial robustness?
  - Basis: Paper observes non-linearity but doesn't isolate whether it stems from emergent reasoning, training data composition, or alignment procedures
  - Why unresolved: Observations made without controlled ablation studies
  - What evidence would resolve it: Controlled ablation studies varying one factor at a time within same model family

## Limitations
- Evaluation pipeline relies on automated LLM-as-judge with only 90-95% agreement with human reviewers
- Non-linear scaling claims lack direct comparative evidence against established scaling laws
- Long-form factuality decoupling based on limited sample sizes and single-model deep analysis
- Reversed basic/advanced difficulty pattern in law domain suggests potential evaluation inconsistencies

## Confidence
- **High Confidence**: Qwen3-80B achieving highest average accuracy (94.7%); GPT-5 maintaining consistently high performance
- **Medium Confidence**: Non-linear performance scaling; domain variation patterns; basic-advanced gap narrowing with larger models
- **Low Confidence**: No significant correlation between injected misinformation and long-form factual accuracy; safety filter patterns; exact causes of law domain reversed difficulty ordering

## Next Checks
1. **Scaling Law Validation**: Replicate adversarial factuality scaling experiments across additional model families (Llama, Mistral) to verify whether non-linear improvements are architecture-agnostic
2. **Long-Form Generalization**: Extend long-form factuality analysis to GPT-5 and GPT-OSS-120B to determine if "no significant correlation" finding is consistent across architectures
3. **Confidence Framing Ablation**: Systematically test the "As we know..." adversarial prompt prefix against alternative confidence framings to isolate whether strong confidence cue is primary attack vector