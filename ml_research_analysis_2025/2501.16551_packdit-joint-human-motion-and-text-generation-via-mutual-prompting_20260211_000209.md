---
ver: rpa2
title: 'PackDiT: Joint Human Motion and Text Generation via Mutual Prompting'
arxiv_id: '2501.16551'
source_url: https://arxiv.org/abs/2501.16551
tags:
- motion
- generation
- text
- packdit
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PackDiT introduces a unified diffusion-based model capable of performing
  bidirectional motion-text generation tasks simultaneously. The method integrates
  two independent diffusion transformers (DiTs) for motion and text through mutual
  blocks that enable cross-modal information exchange.
---

# PackDiT: Joint Human Motion and Text Generation via Mutual Prompting

## Quick Facts
- arXiv ID: 2501.16551
- Source URL: https://arxiv.org/abs/2501.16551
- Reference count: 40
- Achieves state-of-the-art text-to-motion FID score of 0.106 on HumanML3D

## Executive Summary
PackDiT introduces a unified diffusion-based model capable of performing bidirectional motion-text generation tasks simultaneously. The method integrates two independent diffusion transformers (DiTs) for motion and text through mutual blocks that enable cross-modal information exchange. PackDiT employs a multi-stage training strategy including unconditional pre-training, joint generation training, and task-specific fine-tuning. On the HumanML3D dataset, PackDiT achieves state-of-the-art text-to-motion performance with an FID score of 0.106, along with superior results in motion prediction (FID: 0.701) and motion in-between tasks (FID: 0.119).

## Method Summary
PackDiT employs a dual-diffusion transformer architecture where motion and text generation processes are connected through mutual blocks that facilitate cross-modal information exchange. The model uses a multi-stage training approach: first pre-training each DiT independently on their respective modalities, then jointly training the bidirectional generation capabilities, and finally fine-tuning for specific tasks. The mutual blocks use cross-attention mechanisms to allow the motion and text transformers to influence each other's generation process, enabling the model to perform text-to-motion, motion-to-text, motion prediction, and motion in-between tasks within a single unified framework.

## Key Results
- Achieves state-of-the-art text-to-motion FID score of 0.106 on HumanML3D
- Demonstrates superior motion prediction performance with FID of 0.701
- Successfully performs motion-to-text generation with results comparable to autoregressive models

## Why This Works (Mechanism)
PackDiT's effectiveness stems from its bidirectional mutual prompting architecture that enables cross-modal information flow between motion and text generation processes. The diffusion transformers for each modality can influence each other through cross-attention mechanisms in the mutual blocks, allowing the model to leverage contextual information from both domains simultaneously. This architecture overcomes the limitations of separate models for different tasks and enables more coherent generation across modalities. The multi-stage training strategy ensures that each component is properly learned before being integrated, leading to stable and effective joint generation capabilities.

## Foundational Learning
- Diffusion Transformers (DiTs): Why needed - Enable flexible, high-quality generation for both motion and text; Quick check - Verify that both motion and text DiTs achieve competitive performance individually before integration
- Cross-modal mutual blocks: Why needed - Facilitate bidirectional information exchange between motion and text generation; Quick check - Confirm that removing mutual blocks significantly degrades performance on bidirectional tasks
- Multi-stage training: Why needed - Ensures stable learning of complex joint generation; Quick check - Compare performance when skipping pre-training or fine-tuning stages
- Cross-attention mechanisms: Why needed - Allow each modality to attend to relevant information from the other; Quick check - Measure attention weight distributions between modalities
- FID score evaluation: Why needed - Quantify generation quality through feature distribution matching; Quick check - Verify FID scores correlate with human perceptual quality

## Architecture Onboarding

Component Map:
Unconditional pre-training -> Joint generation training -> Task-specific fine-tuning
Motion DiT <-> Mutual Blocks <-> Text DiT

Critical Path:
Motion DiT and Text DiT → Mutual Blocks → Bidirectional Generation

Design Tradeoffs:
PackDiT prioritizes unified architecture over task-specific optimization, trading some task-level performance for the flexibility of a single model. The bidirectional capability introduces additional computational overhead but eliminates the need for multiple specialized models.

Failure Signatures:
- Poor cross-modal alignment when mutual blocks fail to learn effective attention patterns
- Mode collapse in either motion or text generation when one modality dominates training
- Degradation in generation quality for longer sequences due to accumulated errors
- Overfitting to HumanML3D dataset characteristics when evaluated on other datasets

First Experiments:
1. Evaluate individual DiT performance on text-to-motion and motion-to-text tasks before mutual prompting
2. Test bidirectional generation with varying numbers of mutual blocks to find optimal configuration
3. Compare single-task performance against specialized models to quantify the unified approach tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily relies on HumanML3D, limiting generalizability to other motion datasets
- Motion-to-text generation results still lag behind autoregressive models in certain metrics
- Bidirectional generation capability requires further validation across diverse motion-text pairs
- Multi-stage training process introduces complexity that may affect scalability

## Confidence
- Text-to-motion generation performance: High
- Motion prediction and in-betweening: High
- Motion-to-text generation viability: Medium
- Cross-modal mutual prompting effectiveness: Medium
- Single-model unified approach advantages: Medium

## Next Checks
1. Evaluate PackDiT on additional motion datasets (e.g., BABEL, BABEL-Motion) to assess cross-dataset generalization
2. Compare PackDiT's motion-to-text generation against state-of-the-art autoregressive models using human preference studies
3. Test the model's performance on long-sequence motion generation tasks to identify potential degradation in quality