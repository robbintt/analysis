---
ver: rpa2
title: "G\xF6del's Poetry"
arxiv_id: '2512.14252'
source_url: https://arxiv.org/abs/2512.14252
tags:
- proof
- theorem
- lean
- decomposition
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The system combines specialized language models with recursive
  decomposition to address automated theorem proving in Lean 4. It uses a multi-agent
  architecture where models handle formalization, proof generation, sketch decomposition,
  and subgoal proving, coordinated through LangGraph.
---

# Gödel's Poetry

## Quick Facts
- arXiv ID: 2512.14252
- Source URL: https://arxiv.org/abs/2512.14252
- Reference count: 40
- One-line primary result: Combines specialized language models with recursive decomposition to achieve 90.4% pass rate on miniF2F, significantly improved with RAG-guided recursive decomposition

## Executive Summary
Gödel's Poetry is a multi-agent system for automated theorem proving in Lean 4 that combines specialized language models with recursive decomposition to address complex mathematical proofs. The system uses a LangGraph-based architecture where models handle formalization, proof generation, sketch decomposition, and subgoal proving, coordinated through a supervisor agent. A key technical contribution is extending the Kimina Lean Server with AST parsing to enable automated extraction of unproven subgoals from proof sketches, enabling recursive proof decomposition.

## Method Summary
The system implements a multi-agent architecture using LangGraph/LangChain coordination, with specialized models for different tasks: Goedel-Formalizer-V2 for autoformalization, Goedel-Prover-V2 with verifier-guided self-correction for proof generation, and frontier models (GPT-5) for decomposition. The core innovation is recursive decomposition via AST parsing—when direct proof fails, the system generates proof sketches with `have` statements containing `sorry` placeholders, extracts unproven subgoals through AST traversal, and recursively proves each subgoal before reconstructing the complete proof. The system also incorporates RAG-guided theorem retrieval through LeanExplore vector database to improve decomposition quality.

## Key Results
- Achieves 90.4% pass rate on miniF2F benchmark without decomposition, matching Goedel-Prover-V2 performance
- Significantly improved performance with RAG-guided recursive decomposition (exact metrics pending)
- Modular design allows model substitution and community extension
- System available on PyPI as `goedels-poetry`

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive decomposition with AST-based subgoal extraction enables proving theorems that resist direct proof generation.
- Mechanism: When direct proof fails, a frontier LLM generates a proof sketch with `have` statements containing `sorry` placeholders. The extended Kimina Lean Server parses the AST to extract unproven subgoals with complete context. Each subgoal becomes a new proof target at depth d+1, processed recursively. Once all subgoals verify, a reconstruction algorithm substitutes verified proof bodies back into the sketch.
- Core assumption: Decomposed subgoals are simpler to prove than the original theorem, and the decomposition maintains logical entailment.
- Evidence anchors:
  - [abstract] "A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition."
  - [Section 3.4, Step 4] "The parser agent requests the AST of the proof sketch... The decomposition agent then traverses the AST in order to identify all unproven subgoals—that is, have statements followed by by sorry."
  - [corpus] Hilbert (arXiv:2509.22819) independently converged on recursive decomposition, achieving 99.2% on miniF2F—validating the architectural approach.
- Break condition: Maximum depth limit (default: 20) reached with no backtrackable ancestors, or all ancestors exhaust correction attempts.

### Mechanism 2
- Claim: RAG-guided theorem retrieval improves decomposition quality by providing relevant lemmas for the sketch.
- Mechanism: A query generation agent produces natural language descriptions of potentially useful theorems. These query a LeanExplore vector database to retrieve Mathlib theorems. The decomposer agent incorporates retrieved theorems when generating proof sketches.
- Core assumption: The vector database contains relevant theorems, and natural language queries can surface them.
- Evidence anchors:
  - [abstract] "...RAG based retrieval of propositions useful for decomposition..."
  - [Section 2.4] "Hilbert... demonstrated that recursive decomposition was aided by retrieving previously proven theorems that could aid in decomposition."
  - [corpus] Hilbert's 99.2% miniF2F result provides convergent evidence; corpus FMR scores (avg 0.30) suggest moderate domain density but limited citation validation.
- Break condition: Retrieved theorems are irrelevant or decomposition still fails after multiple search reformulations.

### Mechanism 3
- Claim: Verifier-guided self-correction iteratively refines proofs using Lean compiler feedback.
- Mechanism: The prover agent generates a proof attempt. Kimina Lean Server validates it. On failure, a corrector agent receives error messages and generates a correction prompt including the failed attempt and specific errors. This loop continues until verification succeeds or limits (default: 2 attempts/pass, 32 passes) exhaust.
- Core assumption: Lean error messages contain actionable information for LLM correction.
- Evidence anchors:
  - [Section 2.2] "Goedel-Prover-V2... verifier-guided self-correction, which enables iterative proof refinement using compiler feedback."
  - [Section 3.3] "When verification fails, a corrector agent analyzes the error messages and generates a correction prompt that includes... specific error messages from Lean."
  - [corpus] HybridProver (arXiv:2505.15740) uses similar synthesis-and-refinement patterns, suggesting mechanism transferability.
- Break condition: Self-correction attempts exhausted without verification success.

## Foundational Learning

- Concept: **Lean 4 tactic proofs and `have` statements**
  - Why needed here: The entire decomposition mechanism relies on understanding how `have` creates named hypotheses with `sorry` placeholders, and how proofs compose.
  - Quick check question: Given a theorem `theorem foo : P ∧ Q`, write a proof sketch using `have` statements with `sorry` that breaks it into proving P and Q separately.

- Concept: **LangGraph state machines with cyclic workflows**
  - Why needed here: The supervisor agent implements a state machine coordinating formalization, proving, decomposition, and backtracking—all cyclic by design.
  - Quick check question: Sketch a LangGraph graph with nodes for "prove" and "decompose" where failure in "prove" transitions to "decompose," and success terminates.

- Concept: **AST traversal for structured data extraction**
  - Why needed here: Subgoal extraction requires walking Lean's AST to find `have` nodes with `sorry` proofs and recover their type expressions.
  - Quick check question: Pseudocode: given an AST node, recursively collect all nodes of type "have" whose proof is "sorry."

## Architecture Onboarding

- Component map:
  Input Layer: Informal theorem → Formalizer (Goedel-Formalizer-V2) → Syntax Validation (Kimina) → Semantic Check (Qwen 3 30B)
  Proof Layer: Prover (Goedel-Prover-V2) → Verification (Kimina) → [success: done] / [fail: decompose]
  Decomposition Layer: Query Gen (Qwen) → Vector DB (LeanExplore) → Sketcher (GPT-5) → AST Parser (Kimina extended) → Subgoal extraction
  Recursion: Each subgoal re-enters Proof Layer at depth d+1
  Reconstruction: Substitute verified subproofs into parent sketch

- Critical path: For complex theorems: formalization → direct proof attempt (fails) → RAG query → decomposition → AST extraction → recursive subgoal proving → reconstruction. Latency dominated by frontier LLM calls (GPT-5 for sketching) and recursion depth.

- Design tradeoffs:
  - Depth limit (20) prevents unbounded recursion but may terminate valid deep proofs; backtracking mitigates but explores one path at a time
  - Specialized models (Goedel-Prover-V2) for proving vs. frontier models (GPT-5) for decomposition optimizes per-task but increases infrastructure complexity
  - Breadth-first subgoal processing enables parallelism but amplifies API costs proportionally

- Failure signatures:
  - "Maximum depth exceeded with no backtrackable ancestor" → decomposition strategy unsuitable; try alternative search queries or increase depth
  - "Semantic validation failed after 10 retries" → formalizer model mismatch; check prompt or swap model
  - "AST extraction returned empty subgoals" → sketch syntax invalid or `have`/`sorry` pattern not matched; inspect generated sketch

- First 3 experiments:
  1. Reproduce the 90.4% miniF2F baseline: configure Goedel-Prover-V2 via Ollama, disable decomposition, run miniF2F validation split, verify pass rate matches.
  2. Single decomposition test: select a theorem that fails direct proof, enable RAG + decomposition (depth 5), trace the full cycle from sketch generation through AST extraction to subgoal proving.
  3. Ablate RAG: disable LeanExplore, attempt decomposition on the same theorem, compare sketch quality and subgoal provability to identify retrieval contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact benchmark metrics for the system with RAG-guided recursive decomposition enabled?
- Basis in paper: [explicit] The abstract states "With decomposition, this is significantly improved" and Section 5.4 states "With RAG and recursive decomposition, this is significantly improved but has yet to be fully benchmarked."
- Why unresolved: The paper only reports 90.4% without decomposition; decomposition results remain unquantified despite being a key contribution.
- What evidence would resolve it: Full miniF2F, MathLib-Bench, and PutnamBench pass rates with decomposition enabled, with statistical comparison to baseline.

### Open Question 2
- Question: Would exploring multiple decomposition paths simultaneously improve success rates compared to the current single-path approach?
- Basis in paper: [explicit] Section 6.3 states "the system currently explores one decomposition path at a time" and Section 7 lists "alternative tree search techniques for exploring multiple decomposition paths" as future work.
- Why unresolved: The current architecture commits to one decomposition strategy before backtracking; beam search or parallel exploration could discover better proofs earlier.
- What evidence would resolve it: Ablation comparing single-path vs. multi-path decomposition on benchmarks, measuring both success rate and proof quality.

### Open Question 3
- Question: Can decomposition strategies be learned through reinforcement learning rather than relying on frontier LLMs?
- Basis in paper: [explicit] Section 7 lists "the learning of decomposition strategies through reinforcement learning" as future work.
- Why unresolved: Current system uses GPT-5 for decomposition, incurring high API costs and limiting accessibility; learned specialized models could reduce costs and improve consistency.
- What evidence would resolve it: Training a smaller model via RL on decomposition tasks and comparing its performance to GPT-5 on held-out theorems.

### Open Question 4
- Question: How does Gödel's Poetry compare to Hilbert on identical benchmarks and hardware?
- Basis in paper: [inferred] Section 6.2 notes Hilbert achieves 99.2% on miniF2F but states "Hilbert's implementation details and model configurations are not publicly available," preventing direct comparison.
- Why unresolved: Both systems use recursive decomposition with RAG, but architectural differences (Lean 4 vs. Isabelle, agent design, retrieval methods) make relative contributions unclear.
- What evidence would resolve it: Reproducible comparison using standardized benchmark splits, with ablation isolating the contribution of proof assistant choice.

## Limitations

- Metric Gaps: Performance with RAG-guided recursive decomposition lacks specific quantitative validation despite being a key contribution
- Model Availability: System relies on specialized models (Goedel-Formalizer-V2, Goedel-Prover-V2, GPT-5) that may not be publicly accessible
- RAG Effectiveness: LeanExplore vector database characterization is limited, with corpus FMR score of 0.30 suggesting moderate domain relevance

## Confidence

**High Confidence**: The architectural framework (multi-agent LangGraph coordination, recursive decomposition via AST parsing, verifier-guided self-correction) is well-specified and technically coherent. The baseline performance claim (90.4% miniF2F) is verifiable through the described Goedel-Prover-V2 configuration.

**Medium Confidence**: The decomposition mechanism and AST-based subgoal extraction are clearly described, but the actual effectiveness depends on the quality of generated proof sketches and the relevance of retrieved theorems. The convergence with Hilbert's independent work provides convergent validation.

**Low Confidence**: Quantitative claims about RAG-guided decomposition performance lack supporting metrics. The system's overall improvement over baseline without decomposition cannot be independently verified without access to the full system configuration and the specialized models.

## Next Checks

1. **Baseline Reproduction**: Configure Goedel-Prover-V2 via Ollama, disable decomposition, run the miniF2F validation split, and verify the 90.4% pass rate matches. This establishes the foundation for all subsequent claims.

2. **Decomposition Mechanism Test**: Select a theorem that fails direct proof, enable RAG + decomposition with depth 5, and trace the complete cycle from sketch generation through AST extraction to subgoal proving. Document whether subgoals are correctly extracted and proven.

3. **RAG Ablation Study**: Disable LeanExplore vector database retrieval, attempt decomposition on the same theorem, and compare sketch quality and subgoal provability to quantify the retrieval contribution. This isolates the RAG component's impact on overall performance.