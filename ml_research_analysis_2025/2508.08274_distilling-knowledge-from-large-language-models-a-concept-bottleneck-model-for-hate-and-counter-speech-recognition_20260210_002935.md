---
ver: rpa2
title: 'Distilling Knowledge from Large Language Models: A Concept Bottleneck Model
  for Hate and Counter Speech Recognition'
arxiv_id: '2508.08274'
source_url: https://arxiv.org/abs/2508.08274
tags:
- speech
- adjectives
- hate
- performance
- scbm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Speech Concept Bottleneck Model (SCBM),
  which leverages large language models (LLMs) to encode input texts into adjective-based
  bottleneck concepts for improved hate and counter speech recognition. By utilizing
  adjectives as interpretable bottleneck concepts, SCBM provides both local and global
  interpretability, enabling users to understand model predictions through descriptive
  linguistic cues.
---

# Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition

## Quick Facts
- **arXiv ID**: 2508.08274
- **Source URL**: https://arxiv.org/abs/2508.08274
- **Reference count**: 26
- **Primary result**: The Speech Concept Bottleneck Model (SCBM) leverages large language models to encode input texts into adjective-based bottleneck concepts, achieving an average macro-F1 score of 0.69 and outperforming recent methods on four out of five benchmark datasets for hate and counter speech recognition.

## Executive Summary
This paper introduces the Speech Concept Bottleneck Model (SCBM), a novel approach for hate and counter speech recognition that leverages large language models (LLMs) to encode input texts into human-interpretable adjective-based bottleneck concepts. By utilizing a curated lexicon of descriptive adjectives as the concept layer, SCBM provides both local and global interpretability, allowing users to understand model predictions through descriptive linguistic cues. The model employs a probabilistic method to evaluate concept relevance using LLM "yes" token probabilities and introduces a class-discriminative regularization term to enhance model sparsity and focus on the most relevant concepts. Experimental results across five benchmark datasets demonstrate that SCBM achieves competitive performance while offering superior interpretability compared to existing methods.

## Method Summary
SCBM employs a two-stage process where input texts are first encoded into a bottleneck layer of adjective-based concepts using a frozen LLM, and then classified into hate/counter speech categories. The LLM evaluates the relevance of each adjective in a curated lexicon to the input text by computing the probability of "yes" token variants as the first token in its response to a prompt asking if the adjective describes the text. These probabilistic relevance scores form a dense vector that is passed through a relevance gate (a sigmoid-activated dense layer) to learn class-specific weights for each adjective. A lightweight MLP classifier then maps these gated concept activations to class probabilities. The model is trained with a class-discriminative regularization term that penalizes adjectives shared across classes, promoting sparsity and interpretability. The approach can be used in a pure interpretable form (SCBM) or combined with transformer embeddings for enhanced performance (SCBMT).

## Key Results
- SCBM achieves an average macro-F1 score of 0.69 across five benchmark datasets, outperforming the most recently reported methods on four out of five datasets.
- The approach provides local interpretability by highlighting relevant adjectives for individual predictions and global interpretability by identifying class-specific concept patterns.
- Integrating SCBM's adjective-based representation with transformer embeddings (SCBMT) leads to a 1.8% performance increase, demonstrating the complementary nature of the approach.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adjectives can serve as compact, human-interpretable bottleneck concepts for hate and counter speech recognition.
- **Mechanism:** A curated lexicon of descriptive adjectives encodes texts into a semantic vector where each dimension represents the probability that the adjective describes the input. This forces the classifier to reason via high-level linguistic attributes rather than raw token patterns.
- **Core assumption:** The emotional tone, intent, and attitude in hate/counter speech can be adequately captured by a finite set of adjectives, and LLMs can reliably assess the relevance of these adjectives to short texts.
- **Evidence anchors:** Experimental results show SCBM achieves an average macro-F1 score of 0.69, demonstrating that adjective-based concept representations can serve as compact, interpretable, and effective encodings.

### Mechanism 2
- **Claim:** Probabilistic concept evaluation via LLM "yes" token probabilities yields a deterministic and reproducible bottleneck representation, countering LLM stochasticity.
- **Mechanism:** Instead of sampling full LLM responses, the model computes the probability that the LLM emits any "yes" token variant as the first token of its response to a prompt asking if adjective A describes text T. Summing these probabilities across token variants gives a soft relevance score, transforming LLM randomness into a stable feature.
- **Core assumption:** The LLM's first-token "yes" probability distribution approximates its overall judgment of concept relevance, and the set of "yes" token variants is sufficiently exhaustive.
- **Evidence anchors:** The top adjectives for offensive texts (e.g., "pejorative," "devaluing") have high average relevance scores, validating the representation's sensitivity.

### Mechanism 3
- **Claim:** A lightweight classifier with a learned relevance gate and class-discriminative regularization produces sparse, class-specific explanations without significant performance loss.
- **Mechanism:** A sigmoid-activated dense layer (the relevance gate) learns to weight each adjective's contribution per class, encouraging sparsity. The class-discriminative loss term penalizes adjectives that are relevant to multiple classes, pushing the model to use a smaller, more discriminative subset.
- **Core assumption:** Users benefit from explanations highlighting a few key adjectives, and such sparsity can be enforced during training without collapsing the model's representational capacity.
- **Evidence anchors:** Regularization reduces the number of highly relevant adjectives per class and increases class-specificity (e.g., "hateful" for hate speech vs. "refuting" for counter speech).

## Foundational Learning

- **Concept: Concept Bottleneck Models (CBMs)**
  - **Why needed here:** SCBM is an instance of a CBM applied to NLP. Understanding that CBMs introduce an intermediate, human-interpretable concept layer between input and output is essential to grasp the paper's core architecture and interpretability claims.
  - **Quick check question:** In a CBM, does the model predict concepts directly from the input, and then predict the label from the concepts? (Answer: Yes.)

- **Concept: Knowledge Distillation from LLMs**
  - **Why needed here:** The paper "distills" an LLM's understanding of language into a lightweight classifier via the adjective relevance scores. One must understand that the LLM is used in a frozen, inference-only mode to generate features, not fine-tuned.
  - **Quick check question:** In SCBM, are the LLM's parameters updated during training? (Answer: No, it is used in a frozen, inference-only manner.)

- **Concept: Interpretability-Performance Tradeoff**
  - **Why needed here:** The paper explicitly navigates the tradeoff between model transparency (via adjectives) and raw accuracy. Foundational knowledge of why interpretable models often underperform black-boxes helps contextualize the results (e.g., the small performance drop from regularization).
  - **Quick check question:** What is a potential cost of forcing a model to use only human-interpretable concepts? (Answer: Potential loss of expressive power and raw predictive performance.)

## Architecture Onboarding

- **Component map:** Input Text -> LLM Probabilistic Adjective Evaluation -> Relevance Gate (masking) -> MLP Classifier -> Prediction
- **Critical path:** The interpretability hinges on the gated adjective scores, where the LLM's relevance judgments are filtered through the learned relevance gate to produce class-specific concept activations.
- **Design tradeoffs:**
  - **Lexicon Curation vs. Automatic Generation:** Manual lexicons (with experts) ensure quality but are slow to adapt; automatic generation via LLMs scales faster but may miss nuanced domain concepts.
  - **SCBM vs. SCBMT:** Pure SCBM is fully interpretable (~0.076M params, faster) but may lose some information; SCBMT fuses with transformer embeddings for higher performance (~1.8% gain) but introduces opacity from the transformer backbone.
  - **Regularization Strength (位):** High 位 increases sparsity and interpretability but risks underfitting; low 位 preserves performance but yields verbose explanations.
- **Failure signatures:**
  - **Sparse or irrelevant adjectives:** If the lexicon lacks discriminative adjectives for the task, the model will underperform (e.g., for highly contextual sarcasm).
  - **Concept leakage:** The LLM may conflate context with the target text, assigning adjectives based on conversation history rather than the specific text to classify.
  - **Regularization collapse:** Excessive regularization can force the model to ignore all but a few adjectives, failing to capture complex class distinctions.
- **First 3 experiments:**
  1. **Validate the bottleneck representation:** Train a simple classifier (e.g., logistic regression) directly on the LLM-computed adjective probability vectors (no gate, no fusion). Compare its performance to baselines on a held-out dataset to verify the representation's quality.
  2. **Ablate the relevance gate:** Compare SCBM with and without the learned relevance gate (e.g., using raw probabilities vs. gated probabilities) to quantify its impact on both performance and explanation sparsity.
  3. **Test domain transfer:** Apply the model with the existing lexicon to a new hate speech dataset in a different language or platform. Automatically generate a new adjective lexicon using an LLM and compare performance to assess adaptability.

## Open Questions the Paper Calls Out
- Can automated concept generation produce adjective sets that are semantically aligned with human-curated concepts while maintaining classification performance?
- Does the class-discriminative regularization trade-off between interpretability and performance vary systematically across different linguistic phenomena or class structures?
- Can few-shot or more sophisticated prompting strategies for concept evaluation significantly improve SCBM performance without incurring prohibitive computational costs?
- Does SCBM generalize to low-resource languages where LLMs have weaker text comprehension capabilities?

## Limitations
- The curated adjective lexicon may not generalize well to new domains or languages, with automatic generation showing only ~2% performance drop but 23.8% semantic overlap with expert-curated sets.
- The novel LLM evaluation method using first-token "yes" probabilities is not validated against human judgments or alternative evaluation methods.
- The paper demonstrates adjective-based explanations but doesn't rigorously evaluate their quality or usefulness for end-users through user studies.
- The class-discriminative regularization impact is sensitive to the regularization weight 位, which is not thoroughly explored in the paper.

## Confidence
- **Performance Claims**: High confidence - Macro-F1 scores and dataset rankings are clearly reported and benchmarked against recent methods.
- **Interpretability Claims**: Medium confidence - Adjective-based explanations are novel and demonstrable, but their practical value for users is not empirically validated.
- **Methodological Innovations**: Medium confidence - The probabilistic LLM concept evaluation and class-discriminative regularization are novel, but their robustness and optimality are not fully explored.

## Next Checks
1. **Human Evaluation of Explanations**: Conduct a user study where human annotators rate the clarity, relevance, and helpfulness of adjective-based explanations compared to non-interpretable baselines.
2. **Robustness to Lexicon Changes**: Systematically test SCBM performance when iteratively removing/adding adjectives to identify critical concepts and assess lexicon coverage.
3. **Cross-Lingual Transfer Validation**: Apply SCBM to a hate speech dataset in a different language using LLM-generated lexicons and compare performance to monolingual baselines to assess cross-lingual applicability.