---
ver: rpa2
title: 'InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with
  Information Bottleneck'
arxiv_id: '2512.10305'
source_url: https://arxiv.org/abs/2512.10305
tags:
- perception
- information
- infocom
- collaborative
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoCom addresses the communication-performance trade-off in collaborative
  perception by proposing an information purification framework that extracts minimal
  sufficient task-critical information under Information Bottleneck constraints. Unlike
  feature-based approaches, InfoCom introduces Information-Aware Encoding to condense
  perception-relevant information, Sparse Mask Generation to identify spatial cues
  with negligible overhead, and Multi-Scale Decoding to progressively recover perceptual
  information through mask-guided reconstruction.
---

# InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck

## Quick Facts
- **arXiv ID:** 2512.10305
- **Source URL:** https://arxiv.org/abs/2512.10305
- **Authors:** Quanmin Wei, Penglin Dai, Wei Li, Bingyi Liu, Xiao Wu
- **Reference count:** 24
- **Primary result:** Achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale (440× and 90× reduction vs. Where2comm and ERMVP)

## Executive Summary
InfoCom addresses the communication-performance trade-off in collaborative perception by introducing an information purification framework based on the Information Bottleneck principle. Unlike existing feature-based approaches that compress high-dimensional spatial features, InfoCom extracts minimal sufficient task-critical information through three core components: Information-Aware Encoding (IAE) for semantic compression, Sparse Mask Generation (SMG) for spatial cue identification, and Multi-Scale Decoding (MSD) for progressive feature reconstruction. Comprehensive experiments on OPV2V, V2XSet, and DAIR-V2X datasets demonstrate that InfoCom achieves competitive perception performance while reducing communication overhead from megabytes to kilobyte-scale.

## Method Summary
InfoCom reformulates collaborative perception as an information purification problem rather than feature compression. The framework processes intermediate Bird's-Eye View (BEV) features through three stages: IAE condenses high-dimensional features into a low-dimensional Gaussian latent vector by optimizing an extended Information Bottleneck objective that maximizes task-relevant information while minimizing redundancy; SMG generates a sparse spatial importance map that identifies task-critical regions with negligible overhead through filtering and quantization; MSD progressively reconstructs actionable BEV features from the compressed message units using mask-guided modulation. The approach maintains near-lossless perception while achieving 440× and 90× communication reductions compared to state-of-the-art baselines.

## Key Results
- Achieves competitive Average Precision (AP) across OPV2V, V2XSet, and DAIR-V2X datasets while reducing communication to kilobyte-scale
- Reduces communication overhead by 440× compared to Where2comm and 90× compared to ERMVP
- Maintains near-lossless perception performance with only minimal AP degradation (<1%) at extreme compression ratios
- Demonstrates effectiveness across multiple backbones (CoAlign, AttFuse) and detection heads (PointPillars)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Information-Aware Encoding condenses high-dimensional intermediate features into minimal sufficient task-critical representations by optimizing an extended Information Bottleneck (IB) objective.
- **Mechanism:** The framework extends the standard Markov chain (Y → X → Z) to include a latent representation E (Y → X → Z → (E, M)). The IAEncoder maps spatial features Z to a low-dimensional Gaussian latent space (μ, σ) and utilizes a variational approximation of the IB objective. This minimizes mutual information I(E;Z) (redundancy) while maximizing I(E;Y) (task relevance), effectively filtering out noise while preserving detection semantics.
- **Core assumption:** The intermediate feature Z contains redundant spatial information that can be compressed into a low-dimensional vector E without losing task-critical semantic meaning, provided spatial priors are handled separately.
- **Evidence anchors:** [abstract]: "...Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information..."; [section]: "We reformulate Eq. (1) to derive the new IB objective... IAEncoder generates Gaussian parameters μ and σ... enables closed-form computation of the KL divergence for I(E_i;Z_i)."

### Mechanism 2
- **Claim:** Sparse Mask Generation recovers spatial structural priors lost during extreme vector compression with negligible bandwidth overhead.
- **Mechanism:** Instead of transmitting dense feature maps, SMG generates a spatial importance map M from Z using multi-scale convolutions. It applies a joint compression post-processing: filtering (retaining only top-k positions) and quantization (reducing precision to 4-bit). This creates a sparse, low-precision binary map that explicitly signals "where" to look, complementing the "what" in vector E.
- **Core assumption:** Spatial cues in perception tasks are sparse (only a small fraction of the BEV space contains objects of interest) and robust to low-precision representation.
- **Evidence anchors:** [abstract]: "...Sparse Mask Generation to identify spatial cues with negligible overhead..."; [section]: "Empirical observations (see Fig. 4b) indicate that only a minimal number of spatial cues benefit the task... quantizing M to 4-bit precision reduces communication overhead by a factor of 8 while slightly affecting perception performance."

### Mechanism 3
- **Claim:** Multi-Scale Decoding reconstructs actionable Bird's-Eye View (BEV) features from the compressed message units (E, M^q) by using masks to guide progressive upsampling.
- **Mechanism:** The receiver expands the 1D latent vector E into a low-resolution feature map. It then uses the dequantized sparse mask M^q to modulate this feature map (mask-guided modulation). Finally, cascaded decoding blocks progressively upsample the features to full resolution, using the mask to focus reconstruction capacity on task-critical regions rather than background noise.
- **Core assumption:** The semantic content in E and spatial priors in M^q are sufficient to "hallucinate" or reconstruct the high-resolution spatial structures required for the detection head without the original high-bitrate features.
- **Evidence anchors:** [abstract]: "...Multi-Scale Decoding to progressively recover perceptual information through mask-guided mechanisms..."; [section]: "MSD leverages the highly compressed information-aware feature E and the sparse mask M^q... modulation directs the subsequent progressive reconstruction toward task-critical regions."

## Foundational Learning

- **Concept:** **Information Bottleneck (IB) Principle**
  - **Why needed here:** The core loss function (Eq. 8) combines a detection loss with a KL-divergence regularizer derived from IB. Understanding this trade-off (minimizing I(Z;X) vs maximizing I(Z;Y)) is essential to debug why the model might output "noise" (too much regularization) or consume too much bandwidth (too little).
  - **Quick check question:** If you increase the Lagrange multiplier β, does the model become more or less compressed, and how might this affect AP?

- **Concept:** **Reparameterization Trick**
  - **Why needed here:** The IAEncoder outputs μ and σ, but the forward pass involves sampling ε. You must understand how gradients flow through the stochastic sampling node (μ + σ ⊙ ε) to implement the training loop correctly.
  - **Quick check question:** Why can't we backpropagate through a standard random sampling operation, and how does E = μ + σ ⊙ ε solve this?

- **Concept:** **Straight-Through Estimator (STE)**
  - **Why needed here:** The Sparse Mask Generation involves non-differentiable operations (TopK filtering and Round quantization). Understanding STE is required to implement the backward pass where grad_output is passed directly through these discrete operations.
  - **Quick check question:** In the quantization step, the function is discrete (step-function like). How do we approximate the gradient to update the generator weights?

## Architecture Onboarding

- **Component map:** Local Encoder → IAEncoder (Outputs E) + SMGenerator (Outputs M^q) → MSD → Fusion Network → Detection Head
- **Critical path:**
  1. Intermediate BEV feature Z is split: one path to IAE (semantic compression), one to SMG (spatial compression)
  2. Latent E is sampled via reparameterization; Mask M is filtered and quantized
  3. Receiver expands E and uses M^q to modulate the feature upsampling process
- **Design tradeoffs:**
  - Latent Dimension D: Lower D saves bandwidth but risks losing semantics
  - Retention Ratio α: Lower α saves bandwidth but may miss small/far-away objects
  - Quantization Bit-width b: Lower b saves space but increases spatial noise in the mask
- **Failure signatures:**
  - Collapse to Noise: Output AP is random; check if β is too high or D is too small
  - Gradient Blockage: SMG components do not update; check STE implementation for the TopK/Round functions
  - Memory Spike: MSD upsampling consumes large VRAM if batch size is not tuned for the target resolution
- **First 3 experiments:**
  1. Baseline Integration: Plug InfoCom into the standard CoAlign backbone on OPV2V and verify the KB reduction (aim for ~7KB) while monitoring AP drop (should be < 1%)
  2. Ablation on Sparsity: Vary the retention ratio α (e.g., 0.05, 0.1, 0.2) to visualize the bandwidth/AP curve and confirm 0.1 is the optimal trade-off point
  3. Component Isolation: Run with "w/o Mask" (Row 5 in Tab 3) to confirm that without SMG, performance collapses, validating the necessity of the spatial guidance mechanism

## Open Questions the Paper Calls Out

- **Question:** How does incorporating temporal cues into InfoCom's message unit influence the dynamic assessment of transmission necessity and perception accuracy?
  - **Basis in paper:** [explicit] The authors state in the Appendix that future work will "expand this structure into a triple format by incorporating temporal cues" to facilitate dynamic transmission assessments.
  - **Why unresolved:** The current framework processes frames individually, lacking the temporal modeling required to predict future transmission needs or leverage historical consistency.
  - **What evidence would resolve it:** Implementation of a temporal module (e.g., ConvLSTM) within the message unit and evaluation of performance/bandwidth trade-offs across sequential frames.

## Limitations

- Performance gap remains substantial compared to state-of-the-art methods like M2D^2 and PeV^2V
- Reliance on simulated datasets (OPV2V, V2XSet) raises questions about real-world robustness
- Sparse Mask Generation assumes sparsity in spatial cues, which may not hold in highly congested traffic scenarios
- Limited evaluation of adversarial robustness or transmission noise resilience

## Confidence

- **High Confidence:** The core mechanism of Information-Aware Encoding and its theoretical grounding in the Information Bottleneck principle is well-established
- **Medium Confidence:** The effectiveness of Sparse Mask Generation and Multi-Scale Decoding is demonstrated, but specific architectural choices lack extensive ablation studies
- **Low Confidence:** Claims of real-world applicability based on DAIR-V2X results are limited by the dataset's scale and complexity

## Next Checks

1. **Real-World Deployment Test:** Evaluate InfoCom on a larger, more diverse real-world dataset to assess its performance under varying traffic conditions and environmental factors
2. **Scalability Analysis:** Investigate the impact of varying the latent dimension D and retention ratio α on both communication efficiency and perception accuracy across different object densities
3. **Adversarial Robustness:** Test the framework's resilience to adversarial attacks targeting the compressed representations E and M^q to ensure security in practical applications