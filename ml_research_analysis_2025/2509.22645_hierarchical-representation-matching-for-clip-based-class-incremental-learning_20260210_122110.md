---
ver: rpa2
title: Hierarchical Representation Matching for CLIP-based Class-Incremental Learning
arxiv_id: '2509.22645'
source_url: https://arxiv.org/abs/2509.22645
tags:
- hierarchical
- learning
- visual
- textual
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HERMAN, a method for class-incremental learning
  using CLIP. The key innovation is hierarchical representation matching, which leverages
  LLMs to generate multi-level textual descriptors and aligns them with intermediate
  visual features across CLIP layers.
---

# Hierarchical Representation Matching for CLIP-based Class-Incremental Learning

## Quick Facts
- arXiv ID: 2509.22645
- Source URL: https://arxiv.org/abs/2509.22645
- Reference count: 40
- Introduces HERMAN, achieving 1-5% accuracy improvements over existing methods on nine benchmarks for CLIP-based class-incremental learning

## Executive Summary
This paper introduces HERMAN, a method for class-incremental learning using CLIP that addresses catastrophic forgetting through hierarchical representation matching. The key innovation leverages LLMs to generate multi-level textual descriptors that are aligned with intermediate visual features across CLIP layers using an adaptive routing mechanism. An SVD-based projection update stabilizes the router to preserve knowledge from previous tasks while adapting to new ones. Extensive experiments on nine benchmarks demonstrate consistent state-of-the-art performance with accuracy improvements of 1-5% over existing methods.

## Method Summary
HERMAN addresses class-incremental learning by generating hierarchical textual descriptors via LLMs, which are then aligned with intermediate visual features from CLIP's Vision Transformer. The method extracts [CLS] tokens from multiple layers and computes cosine similarities with LLM-generated embeddings to create unified hierarchical representations. An adaptive router fuses these representations, while an SVD-based projection mechanism preserves learned routing patterns from previous tasks to mitigate catastrophic forgetting. The approach maintains fine-grained knowledge while adapting to new classes through this multi-level semantic matching framework.

## Key Results
- Achieves 1-5% accuracy improvements over existing methods across nine benchmarks
- Demonstrates robustness to hyperparameter choices (K, δ)
- Shows consistent state-of-the-art performance in class-incremental learning scenarios
- Maintains fine-grained knowledge while adapting to new classes

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Augmentation via LLMs
Replacing fixed templates with recursively generated, multi-level textual descriptors improves the model's ability to distinguish semantically similar classes. An LLM generates M descriptors for each class, progressing from coarse identity to fine-grained attributes, which are encoded into hierarchical embeddings. The core assumption is that the LLM generates visual descriptors that are both physically accurate and discriminative for the specific dataset domain. Evidence includes improved accuracy over fixed-template approaches and support from related hierarchical semantic tree anchoring methods.

### Mechanism 2: Cross-Modal Alignment at Intermediate Layers
Utilizing intermediate visual features from the Vision Transformer allows matching coarse textual descriptors to coarse visual features and fine descriptors to fine features. The model extracts [CLS] tokens from intermediate layers and computes cosine similarity with textual embeddings, using Top-K selection to aggregate the best matching texts. The core assumption is that ViT intermediate layers naturally decompose into a semantic hierarchy that aligns with the linguistic hierarchy. Evidence includes consistent performance improvements and the architectural design enforcing cross-modal alignment across levels.

### Mechanism 3: Subspace Stabilization via Orthogonal Projection
Constraining the update of the adaptive router to the orthogonal complement of the "old" subspace mitigates catastrophic forgetting by preserving routing patterns learned from previous tasks. After training on a task, router weights are decomposed via SVD and future updates are projected to preserve important knowledge while allowing novel information capture. The core assumption is that important knowledge for old tasks concentrates in the principal components of the router's weight matrix. Evidence includes sustained cosine similarity for HERMAN versus deterioration for unconstrained updates.

## Foundational Learning

- **Concept: Vision Transformer (ViT) Layer Hierarchy**
  - **Why needed here:** The method relies on extracting features from distinct layers. You must understand that early layers typically capture textures/edges while deeper layers capture semantic objects to debug the alignment mechanism.
  - **Quick check question:** Can you explain why matching "coarse" textual descriptors (like "sports car") to early ViT layers might be less effective than matching them to mid-to-late layers?

- **Concept: Singular Value Decomposition (SVD) for Subspace Projection**
  - **Why needed here:** The router update logic uses SVD to identify a "knowledge subspace" to protect.
  - **Quick check question:** In the equation $W_{proj} = \rho P_{old} W_{new} + \dots$, what does the term $(I - P_{old})$ represent geometrically?

- **Concept: Class-Incremental Learning (CIL) & Catastrophic Forgetting**
  - **Why needed here:** The paper frames the router projection specifically as a solution to forgetting.
  - **Quick check question:** Why does simply fine-tuning the router on new task data cause the model to fail on old task data?

## Architecture Onboarding

- **Component map:** LLM Generator (Offline) -> Text Encoder (Frozen CLIP) -> Visual Encoder (Frozen CLIP ViT) -> Hierarchy Matcher -> Router (Learnable Linear) -> Adapter (Learnable)

- **Critical path:** Image -> Intermediate Visual Features -> Match & Fuse Text -> Router weights -> Final Logits. Offline: Query LLM -> Encode Text. Task End: SVD Projection Update on Router weights (Crucial for CIL stability).

- **Design tradeoffs:**
  - **Top-K Selection (K):** Low K may miss details; high K introduces noise. Paper recommends K=5.
  - **Energy Proportion (δ):** Controls how much of the "old" router subspace is frozen. High δ (e.g., 0.9) prioritizes stability over plasticity.

- **Failure signatures:**
  - **Router Collapse:** If δ is too high, new tasks cannot update the router, and accuracy on new classes plateaus.
  - **Semantic Misalignment:** If t-SNE plots show text and visual embeddings drifting apart, check if the LLM descriptors are hallucinated or non-visual.

- **First 3 experiments:**
  1. **Ablate Projection:** Run HERMAN without the SVD projection to confirm the forgetting mitigation mechanism.
  2. **Layer Depth Analysis:** Vary which intermediate layers are used for matching to find the optimal semantic hierarchy depth.
  3. **LLM Robustness:** Swap the LLM to verify that the architecture is robust to the specific phrasing of descriptors.

## Open Questions the Paper Calls Out
- How can patch-level visual tokens be effectively incorporated into the hierarchical matching framework to capture denser visual-textual correspondences?
- How robust is the framework when LLMs generate hallucinated or non-discriminative descriptors, and can the model filter these without external supervision?
- Does the Top-K attention mechanism spontaneously learn a monotonic alignment where coarse descriptors correspond to early visual layers and fine descriptors to later layers?
- What is the computational and memory overhead of HERMAN during inference compared to standard CLIP?

## Limitations
- The method depends heavily on LLM-generated descriptor quality, with unverified assumptions about visual accuracy and discriminative power
- The SVD-based subspace projection approach has limited corpus validation and may not generalize across all task sequences
- The assumed semantic hierarchy in ViT intermediate layers is not explicitly validated in the paper

## Confidence
- **High Confidence:** Hierarchical semantic augmentation and cross-modal alignment mechanisms are well-supported by experimental results
- **Medium Confidence:** Subspace stabilization approach shows promise but limited external validation
- **Medium Confidence:** Robustness claims to hyperparameter choices are supported but could benefit from broader parameter sweeps

## Next Checks
1. **LLM Descriptor Quality Audit:** Conduct qualitative and quantitative analysis of LLM-generated descriptors to verify visual accuracy and discriminative power across different LLM providers
2. **ViT Layer Semantic Hierarchy Validation:** Perform controlled experiments varying intermediate layers used for matching to empirically verify the assumed semantic hierarchy alignment
3. **Generalization Beyond CLIP:** Test HERMAN with different vision-language backbones to verify the approach generalizes beyond the specific CLIP architecture used in experiments