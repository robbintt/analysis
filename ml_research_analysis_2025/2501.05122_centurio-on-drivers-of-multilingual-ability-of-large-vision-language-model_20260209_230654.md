---
ver: rpa2
title: 'Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model'
arxiv_id: '2501.05122'
source_url: https://arxiv.org/abs/2501.05122
tags:
- l100
- languages
- llama
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the drivers of multilingual ability in
  Large Vision-Language Models (LVLMs), focusing on the optimal number of training
  languages, language distribution in pre-training and instruction-tuning data, and
  improving multilingual text-in-image understanding. The authors conduct a series
  of multi-stage experiments spanning 13 downstream tasks and 43 languages, revealing
  that (i) up to 100 training languages can be included simultaneously without degrading
  English performance, (ii) as little as 25-50% non-English data greatly improves
  multilingual performance while retaining strong English performance, and (iii) including
  non-English OCR data in pre-training and instruction-tuning is crucial for improving
  multilingual text-in-image understanding.
---

# Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model

## Quick Facts
- **arXiv ID**: 2501.05122
- **Source URL**: https://arxiv.org/abs/2501.05122
- **Reference count**: 40
- **Primary result**: Centurio achieves state-of-the-art performance on 14 tasks and 56 languages, matching popular multilingual open-weight LVLMs on English and high-resource languages while outperforming them on low(er)-resource languages.

## Executive Summary
This paper investigates the drivers of multilingual ability in Large Vision-Language Models (LVLMs) through systematic ablation experiments spanning 13 downstream tasks and 43 languages. The authors challenge the "curse of multilinguality" by showing that up to 100 training languages can be included simultaneously without degrading English performance. They identify that as little as 25-50% non-English data greatly improves multilingual performance while retaining strong English performance, and demonstrate that including non-English OCR data in pre-training and instruction-tuning is crucial for improving multilingual text-in-image understanding. The authors introduce a new benchmark, SMPQA, for evaluating multilingual text-in-image understanding, and train Centurio, a 100-language LVLM that achieves state-of-the-art performance across 14 tasks and 56 languages.

## Method Summary
The authors employ a LLaVA-style architecture with SigLIP SO400/384 encoder, a 2-layer MLP projector, and LLM backbone (Phi-3.5, Llama 3, or Aya-Expanse) adapted via LoRA. Training proceeds in two phases: pre-training on image captions (ShareGPT4v + ALLaVA) and instruction-tuning on task mix (LLaVA-Next + additional datasets). Data is machine-translated to 100 languages using NLLB-200-distilled-1.3B, with synthetic OCR data generated via Synthdog. The final model uses 50% English / 50% multilingual data in both phases with unfrozen encoder for OCR. Evaluation spans 13 tasks across 43 languages with metrics including CIDEr, exact/relaxed accuracy, and language fidelity.

## Key Results
- Up to 100 training languages can be included simultaneously without degrading English performance.
- As little as 25-50% non-English data greatly improves multilingual performance while retaining strong English performance.
- Including non-English OCR data in pre-training and instruction-tuning is crucial for improving multilingual text-in-image understanding.
- Centurio achieves state-of-the-art performance on 14 tasks and 56 languages, matching popular multilingual open-weight LVLMs on English and high-resource languages while outperforming them on low(er)-resource languages.

## Why This Works (Mechanism)

### Mechanism 1: Coverage Over Volume
The underlying LLM backbone (e.g., Phi 3.5, Llama 3) already contains latent multilingual representations from its pre-training. Exposing the model to a language during vision-language alignment "activates" these representations, allowing cross-modal grounding without requiring massive in-language datasets. Providing at least some training data in a target language drives performance gains more effectively than allocating large portions of the training budget to that language.

### Mechanism 2: Incremental Language Expansion Without Interference
The instruction-tuning phase operates on top of fixed (or LoRA-adapted) LLM weights. Since the model is not learning fundamental language representations from scratch, but rather mapping vision tokens to existing language embeddings, adding new languages creates minimal parameter interference. Adding up to 100 languages simultaneously does not degrade performance on English or previously added languages, contrary to the "curse of multilinguality."

### Mechanism 3: Vision Encoder Adaptation for Non-Latin Scripts
Standard vision encoders (e.g., SigLIP) are pre-trained predominantly on English or Latin-script data. Freezing them locks the visual feature extraction to patterns that fail on non-Latin scripts. Fine-tuning with multilingual OCR data adapts the encoder's lower-level filters to recognize diverse character shapes. Unfreezing and training the image encoder is necessary for the model to read text in non-Latin scripts from images.

## Foundational Learning

- **Language Fidelity**: The paper's primary evaluation metric for generative tasks. A model that answers correctly but in the wrong language fails the user's intent. Quick check: Does your model reply in the language of the prompt, or default to English?
- **LoRA (Low-Rank Adaptation)**: The paper uses LoRA to efficiently train the LLM backbone on multilingual data. Understanding this is critical for replicating the training setup. Quick check: Which weight matrices does LoRA adapt, and what is the rank used?
- **Curse of Multilinguality**: The paper explicitly challenges this established concept. Knowing the original theory helps contextualize their contribution. Quick check: In traditional multilingual models, what trade-off does this "curse" describe?

## Architecture Onboarding

- **Component map**: SigLIP-SO400M (frozen/unfrozen image encoder) → 2-layer MLP (projects visual tokens to LLM embedding space) → LLM (Phi 3.5, Qwen 2.5, or Aya-Expanse; adapted via LoRA)
- **Critical path**: 1) Machine-translate English instruction data → 2) Mix English (50%) with multilingual data (50% split across ~100 languages) → 3) Pre-train on image captions + synthetic OCR → 4) Instruction-tune on task mix
- **Design tradeoffs**: 1) More multilingual data helps low-resource tiers but may slightly reduce English peak performance; 2) Unfreezing the image encoder improves non-Latin OCR but increases GPU memory and training time
- **Failure signatures**: 1) Model generates English text regardless of prompt language → likely insufficient in-language instruction data; 2) Near-random scores on non-Latin script OCR → image encoder was likely frozen or OCR data was excluded
- **First 3 experiments**: 1) Replicate RQ2: Train with 25%, 50%, and 75% English instruction data; evaluate on a subset of languages to verify the 50% "sweet spot"; 2) Isolate vision encoder impact: Train two models—frozen vs. unfrozen encoder—on the same multilingual OCR data; compare non-Latin script performance; 3) Verify cross-lingual transfer: Hold out a specific language from instruction tuning; test if the model can still process visual inputs in that language (relying on LLM backbone transfer)

## Open Questions the Paper Calls Out

### Open Question 1
What is the required scaling factor for synthetic text-in-image data to achieve parity between Latin-script and non-Latin-script performance in Large Vision-Language Models? The authors observe a "large performance gap between Latin- and non-Latin-script languages" even after training and hypothesize that "orders of magnitude more text-in-image training data for other scripts are required." This remains unresolved because the current experiments used a fixed amount of synthetic data which rapidly improved Latin-script performance but failed to significantly move the needle for non-Latin scripts.

### Open Question 2
To what extent does the noise and "translationese" artifacts in machine-translated training data degrade the output quality and factual accuracy of multilingual LVLMs? The authors list "Using Machine-Translated Training Data" as a limitation, noting that low-quality MT data for low-resource languages might negatively impact generation quality in ways current metrics do not measure. This remains unresolved because the study relied on NLLB machine translation to ensure comparable data across 100 languages, making it impossible to isolate the specific negative effects of translation artifacts on model performance.

### Open Question 3
Does explicit multicultural knowledge acquisition require distinct training approaches separate from multilingual language understanding? The authors state that "multilingual and multicultural knowledge... can be intermingled in practice, [but] they require distinct approaches in training." This remains unresolved because the paper focused exclusively on drivers of language ability and fidelity, deliberately excluding the "multicultural aspect" (knowledge specific to the regions of the target languages).

## Limitations
- The paper relies on machine-translated data for most non-English languages, which may introduce translation artifacts that inflate performance on certain tasks while masking true multilingual understanding.
- Training budget constraints prevent testing whether the observed "sweet spot" of 25-50% non-English data holds at larger scales (e.g., 10B+ parameter models).
- The synthetic OCR data generation, while scalable, may not fully capture the visual variability of real-world text in images across different fonts, backgrounds, and degradation patterns.

## Confidence

- **High Confidence**: The finding that up to 100 languages can be included without degrading English performance. This is well-supported by the controlled ablation experiments and aligns with the theoretical understanding of LoRA-based instruction tuning.
- **Medium Confidence**: The 25-50% non-English data sweet spot. While well-demonstrated across multiple ablations, this finding may be specific to the model scale and training budget used, and could shift with different architectures or computational constraints.
- **Medium Confidence**: The necessity of unfreezing the image encoder for non-Latin script performance. The ablation is clear, but the synthetic nature of the OCR data limits generalizability to real-world scenarios.

## Next Checks
1. **Real OCR Validation**: Evaluate Centurio on a benchmark with real images containing non-Latin script text (e.g., multilingual receipts, street signs) to verify that encoder unfreezing translates to genuine visual understanding rather than synthetic data memorization.
2. **Scale Sensitivity Test**: Train smaller (1B) and larger (10B+) variants of the model to determine whether the 25-50% non-English "sweet spot" is scale-dependent or holds across model sizes.
3. **Zero-Shot Cross-Lingual Transfer**: Hold out entire language families (e.g., Dravidian, Tai-Kadai) from both pre-training and instruction tuning, then test whether the model can process visual inputs in these languages through transfer from phylogenetically related languages present in training.