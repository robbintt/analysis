---
ver: rpa2
title: Episodic Contextual Bandits with Knapsacks under Conversion Models
arxiv_id: '2507.06859'
source_url: https://arxiv.org/abs/2507.06859
tags:
- contextual
- 'null'
- episode
- which
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online contextual bandits with knapsacks (BwK)
  in a multi-episode setting where each episode starts with a new budget and contexts
  are drawn from non-stationary distributions. The goal is to achieve sublinear regret
  over T episodes.
---

# Episodic Contextual Bandits with Knapsacks under Conversion Models

## Quick Facts
- **arXiv ID**: 2507.06859
- **Source URL**: https://arxiv.org/abs/2507.06859
- **Authors**: Wang Chi Cheung; Zitian Li
- **Reference count**: 40
- **Primary result**: Achieves regret bound of $\tilde{O}(r_{\text{max}}L\sqrt{HT})$ that is independent of context space size

## Executive Summary
This paper introduces Mimic-Opt-DP, a novel algorithm for episodic contextual bandits with knapsacks (BwK) under non-stationary contexts. The key innovation is decoupling the learning of conversion models from resource allocation logic through a modular confidence bound (CB) oracle abstraction. By combining this oracle with a carefully designed backward induction that estimates value-to-go functions using unlabeled feature data, the algorithm achieves sublinear regret bounds that are independent of the context space size. The approach overcomes the computational intractability of standard dynamic programming methods in this setting.

## Method Summary
The Mimic-Opt-DP algorithm operates in multi-episode settings where each episode starts with a new budget and contexts arrive from non-stationary distributions. The algorithm maintains two disjoint datasets: D_t for labeled data (contexts, actions, outcomes) used to update the CB oracle, and S_t for unlabeled feature data used to estimate context distributions. At each time step, the algorithm solves an optimistic optimization problem using UCB/LCB estimates from the CB oracle to select actions. Periodically, it invokes an Optimistic-DP subroutine that performs backward induction over budget levels to estimate value-to-go functions, leveraging the unlabeled data to avoid explicit context distribution estimation. The data allocation between learning tasks is controlled by an index set J_t.

## Key Results
- Achieves regret bound of $\tilde{O}(r_{\text{max}}L\sqrt{HT})$ independent of context space size
- The algorithm successfully leverages unlabeled feature data to improve regret bounds
- Numerical experiments validate the approach against existing single-episode methods
- The framework is modular and compatible with various contextual bandit models

## Why This Works (Mechanism)

### Mechanism 1: Modular Confidence Bound (CB) Oracle Abstraction
The system achieves sublinear regret by decoupling learning of conversion models from resource allocation logic. The algorithm treats the conversion model learner as a black-box oracle providing UCB/LCB bounds for rewards and consumption, which are then used for optimization.

### Mechanism 2: Context-Agnostic Value-to-Go Estimation
The algorithm achieves regret bounds independent of context space size by averaging estimated value functions over historical contexts rather than iterating over all possible contexts. This Monte-Carlo integration bypasses the need to estimate full context distributions.

### Mechanism 3: Disjoint Data Splitting for Stability
The algorithm designates specific episodes for context sampling (unlabeled data) and others for updating the CB oracle (labeled data). This separation ensures the independence required for high-probability bounds.

## Foundational Learning

- **Contextual Bandits with Knapsacks (BwK)**: Base problem class where standard bandits add resource constraints that deplete based on conversion outcomes. Why needed: This is the problem setting being solved.
  - Quick check: Do you understand why a standard UCB algorithm fails in BwK? (Hint: It might exhaust the budget on high-reward, high-cost arms early)

- **Bellman Equations & Backward Induction**: Used by the Optimistic-DP component to approximate optimal policy through dynamic programming. Why needed: The algorithm needs to propagate value functions from final horizon backwards to step 1.
  - Quick check: Can you write the Bellman optimality equation for a finite horizon problem where state is budget and action is price?

- **Optimism in the Face of Uncertainty (OFU)**: The algorithm relies on optimistic estimates (UCBs) to drive exploration by assuming the world is as good as statistically possible. Why needed: Encourages trying uncertain actions.
  - Quick check: If the true conversion rate is 0.5 and the confidence interval is [0.4, 0.6], which value does an optimistic algorithm use for planning?

## Architecture Onboarding

- **Component map**: Episode Loop -> Step Loop -> Optimistic-DP Module -> CB Oracle
- **Critical path**: The execution of Optimistic-DP (Algorithm 2) at the end of episodes in the set $[t-1] \setminus J_t$
- **Design tradeoffs**: The parameter $\alpha$ (implicitly defining $J_t$) trades off learning context distributions vs. learning conversion models
- **Failure signatures**:
  - Budget Exhaustion: Algorithm runs out of budget before H steps consistently
  - Exploding Regret: Regret scales linearly with T, suggesting CB Oracle consistency violation
- **First 3 experiments**:
  1. Implement the "Dynamic Pricing" setup with a Generalized Linear Model oracle to verify regret scales as $\tilde{O}(\sqrt{T})$
  2. Run with $S_1 \neq \emptyset$ vs. $S_1 = \emptyset$ to confirm regret improvement when unlabeled data is present
  3. Swap the CB Oracle from Linear Bandit to Neural Bandit to test modularity claim

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a computationally tractable algorithm be designed for episodic contextual BwK with $K > 1$ resource types?
- **Open Question 2**: Can improved regret bounds be derived for First Price Auction applications under richer feedback models?
- **Open Question 3**: Can the framework be adapted to utilize online regression oracles rather than confidence bound oracles?

## Limitations
- Relies on strong oracle assumption requiring $o(T)$ regret from CB Oracle
- Performance sensitive to smoothness assumptions on value functions
- Limited numerical experiments without comparison to established benchmarks on real-world datasets
- Computational complexity of backward induction for large budget spaces not thoroughly analyzed

## Confidence

**High Confidence**: The modular framework combining CB oracles with backward induction is mathematically sound. The regret bound derivation under stated assumptions is correct.

**Medium Confidence**: Practical performance benefits of unlabeled data are theoretically established but not extensively validated empirically. Generalization to various CB oracles is proven in theory but requires careful implementation.

**Low Confidence**: Algorithm behavior in non-smooth or highly non-stationary environments is not characterized. Computational complexity for large budget spaces is not thoroughly analyzed.

## Next Checks

1. **Oracle Dependency Test**: Implement the algorithm with multiple CB oracles (linear, logistic, neural) and systematically measure how the oracle-dependent term in the regret bound varies across function classes.

2. **Non-Stationarity Robustness**: Modify synthetic experiments to include gradual shifts in context distributions $\Lambda_h$ and measure degradation in regret performance relative to theoretical bounds.

3. **Data Splitting Sensitivity**: Run ablation studies varying the parameter $\alpha$ to quantify the tradeoff between context learning speed and conversion model accuracy.