---
ver: rpa2
title: 'Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of
  Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning'
arxiv_id: '2502.04399'
source_url: https://arxiv.org/abs/2502.04399
tags:
- data
- vehicles
- fine-tuning
- tasks
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an online framework for ride-hailing vehicles
  to jointly perform order serving and foundation model fine-tuning using urban data,
  addressing challenges of spatio-temporal heterogeneity and inconsistent geo-distributions.
  The framework employs multi-agent reinforcement learning (MARL) with graph neural
  networks (GNNs) to optimize vehicle dispatching, order acceptance, and data collection
  decisions.
---

# Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning

## Quick Facts
- **arXiv ID:** 2502.04399
- **Source URL:** https://arxiv.org/abs/2502.04399
- **Reference count:** 40
- **One-line primary result:** A multi-agent RL framework achieves 72.78 QoS and 69.03% accuracy by jointly optimizing ride-hailing order serving and foundation model fine-tuning.

## Executive Summary
This paper addresses the challenge of jointly optimizing ride-hailing order serving and foundation model fine-tuning using urban data collected by vehicles. The authors propose an online framework that employs multi-agent reinforcement learning (MARL) with graph neural networks (GNNs) to optimize vehicle dispatching, order acceptance, and data collection decisions. The framework introduces a new quality-of-service (QoS) metric that balances order income and data utility, and a RankTuner module that dynamically adjusts LoRA ranks for efficient fine-tuning. Experiments using real-world datasets show that the method outperforms baselines in both order serving and model fine-tuning tasks.

## Method Summary
The proposed method uses multi-agent proximal policy optimization (MAPPO) with relational graph convolutional networks (R-GCN) to jointly optimize ride-hailing order serving and LoRA-based foundation model fine-tuning. The framework operates in a grid-based map with 100 vehicles, processing orders from the NYC Taxi dataset and fine-tuning tasks including ViT on CIFAR-100, SAM on Satellite Imagery, and YOLOv7 on Vehicle Detection. A RankTuner module dynamically adjusts LoRA ranks to balance model accuracy and computation cost. The objective is to maximize Quality-of-Service (QoS = α·ADI + β·ADU), where ADI is Accumulated Driver Income and ADU is Accumulated Data Utility. The system is trained online for 2.5 million steps.

## Key Results
- Achieves 72.78 QoS and 69.03% accuracy on three foundation model tasks
- Outperforms Random, Greedy, and IQL baselines in both order serving and model fine-tuning
- Demonstrates effective balance between order income (ADI) and data utility (ADU) through dynamic LoRA rank adjustment

## Why This Works (Mechanism)
The framework works by treating vehicles as agents in a multi-agent RL setting where each agent makes decisions about accepting orders and collecting data for model fine-tuning. The R-GCN processes the relational graph structure of vehicles, orders, and points of interest to generate state embeddings. The RankTuner module dynamically adjusts LoRA ranks based on data utility metrics, allowing the system to adapt to varying spatio-temporal heterogeneity in urban data. The joint optimization problem is solved by maximizing a composite QoS metric that captures both economic and data quality objectives.

## Foundational Learning
- **Multi-Agent Proximal Policy Optimization (MAPPO):** Needed to coordinate multiple vehicles' decisions simultaneously while maintaining training stability. Quick check: Verify policy gradients are properly clipped and rewards are normalized across agents.
- **Relational Graph Convolutional Networks (R-GCN):** Required to process the complex relational structure between vehicles, orders, and urban infrastructure. Quick check: Ensure graph connectivity is maintained and node embeddings capture spatial relationships.
- **Low-Rank Adaptation (LoRA):** Enables efficient fine-tuning of large foundation models on resource-constrained vehicles. Quick check: Monitor rank stability and verify gradient flow through adapter layers.
- **Quality-of-Service (QoS) Metric:** Combines economic and data utility objectives into a single optimization target. Quick check: Validate that α and β weights appropriately balance competing objectives.
- **Age-of-Information (AoI):** Critical for data freshness in fine-tuning utility calculations. Quick check: Track data age decay curves and their impact on utility scores.
- **Heterogeneous Foundation Model Support:** Allows simultaneous fine-tuning of multiple model types with different data requirements. Quick check: Verify task-specific data collection policies are properly differentiated.

## Architecture Onboarding

**Component Map:** Environment/Sensor Data -> R-GCN Encoder -> MAPPO Agent -> RankTuner -> Action Selection -> Environment Feedback

**Critical Path:** Sensor data collection → R-GCN state embedding → MAPPO policy evaluation → RankTuner optimization → Action execution → Reward computation

**Design Tradeoffs:** The framework balances computation cost (via LoRA rank adjustment) against model accuracy, and economic returns (order serving) against data utility. The R-GCN embedding dimension (10) represents a compromise between representational capacity and computational efficiency.

**Failure Signatures:** Poor QoS convergence indicates inadequate state representation or reward shaping. Rank oscillation suggests unstable utility estimation. Low accuracy despite high data collection indicates poor data quality or insufficient fine-tuning capacity.

**Three First Experiments:**
1. Test R-GCN embedding quality by visualizing vehicle-state representations in 2D space and checking clustering by region/type.
2. Validate RankTuner logic by simulating rank adjustment decisions with synthetic ADU data and checking stability.
3. Verify MAPPO training stability by monitoring entropy, KL divergence, and policy loss over training steps.

## Open Questions the Paper Calls Out
- How can the framework maintain or improve QoS when scaling to a significantly larger number of concurrent foundation model tasks (>3)?
- How does the removal of the "single time slot" travel assumption affect the optimal dispatching policy and data freshness?
- Can the framework incorporate privacy-preserving constraints without significantly degrading fine-tuning accuracy or convergence speed?
- What is the impact of continuous on-board fine-tuning on the energy consumption and battery life of electric ride-hailing vehicles?

## Limitations
- Key hyperparameters for MAPPO training (learning rate, discount factor, clip ratio, batch size, buffer size) are not specified, making exact replication challenging.
- The mathematical definition of the data utility function f_k(d, λ) is described as "complex" without providing an explicit formula for implementation.
- The framework assumes single time slot travel times, which may not hold in real-world traffic conditions with variable congestion.
- Weighting factors α and β for the QoS calculation are unspecified, affecting the balance between economic and data utility objectives.

## Confidence
- **High Confidence:** Overall problem formulation, MAPPO algorithm choice, and general architecture (R-GCN + Actor-Critic) are clearly specified and sound.
- **Medium Confidence:** Specific implementation details of R-GCN encoder and RankTuner mechanism are provided but may require tuning for optimal performance.
- **Low Confidence:** Exact performance metrics and reported values can only be confidently verified if all hyperparameters and utility functions are precisely replicated.

## Next Checks
1. Conduct hyperparameter sensitivity analysis varying learning rate, discount factor, and clip ratio to determine their impact on QoS convergence and final performance.
2. Implement multiple plausible versions of the data utility function f_k(d, λ) based on described principles and evaluate their effect on RankTuner stability and overall QoS.
3. Convert qualitative descriptions of PoI spatial distributions into explicit numerical probability matrices and assess their influence on vehicle behavior and data collection efficiency.