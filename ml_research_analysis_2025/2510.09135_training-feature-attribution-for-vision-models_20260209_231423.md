---
ver: rpa2
title: Training Feature Attribution for Vision Models
arxiv_id: '2510.09135'
source_url: https://arxiv.org/abs/2510.09135
tags:
- training
- test
- image
- attribution
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces training feature attribution (TFA), a framework
  that combines training data attribution (TDA) with feature attribution (FA) to link
  test predictions to specific regions of specific training images. The approach applies
  gradient-based FA to TDA scores, identifying which parts of influential training
  examples drive a given prediction.
---

# Training Feature Attribution for Vision Models

## Quick Facts
- arXiv ID: 2510.09135
- Source URL: https://arxiv.org/abs/2510.09135
- Reference count: 27
- Primary result: TFA combines TDA with FA to link test predictions to specific training image regions, validated quantitatively via insertion tests

## Executive Summary
This paper introduces Training Feature Attribution (TFA), a framework that combines training data attribution (TDA) with feature attribution (FA) to link test predictions to specific regions of specific training images. The approach applies gradient-based FA to TDA scores, identifying which parts of influential training examples drive a given prediction. Experiments on vision datasets show that TFA generates fine-grained, test-specific explanations. Quantitative validation demonstrates that TFA saliency maps correctly identify influential training pixels, with top-k insertions yielding more negative test-loss changes than random controls. TFA also reveals spurious correlations (e.g., patch-based shortcuts) that conventional attribution methods miss.

## Method Summary
TFA applies gradient-based feature attribution to training data attribution scores to create pixelwise influence maps on training images. The method uses Grad-Cos (cosine similarity of train/test gradients) for efficient TDA without Hessian computation, then computes gradients of these scores with respect to training image pixels. SmoothGrad denoising (σ=0.1, n=50) reduces noise in the resulting heatmaps. The framework is evaluated on Pascal VOC (fine-tuning ResNet-18 for 5 epochs) and CIFAR-10 (training lightweight CNN for 10 epochs), with quantitative validation via insertion tests comparing top-k versus random pixel masking.

## Key Results
- TFA generates fine-grained, test-specific explanations linking predictions to specific training image regions
- Quantitative insertion tests show top-k TFA pixels yield more negative test-loss changes than random controls
- TFA reveals spurious correlations (patch-based shortcuts) missed by conventional attribution methods
- Method explains misclassifications by identifying harmful training examples and localizing decision-relevant regions

## Why This Works (Mechanism)

### Mechanism 1
Applying feature attribution to TDA scores localizes influential training regions. Grad-Cos produces scalar TDA scores for each train-test pair, and computing gradients of these scores with respect to training image pixels identifies which regions drive specific predictions. The method assumes TDA scores are meaningfully differentiable with respect to training input features, and gradient magnitude correlates with regional importance.

### Mechanism 2
Grad-Cos provides efficient, semantically meaningful influence estimation without Hessian computation. It computes cosine similarity between train and test loss gradients, capturing alignment of "learning directions" while approximating influence functions in the large-damping regime where Hessian effects become negligible. The method assumes gradient alignment correlates with influence and that Hessian curvature can be safely ignored for deep networks.

### Mechanism 3
SmoothGrad averaging reduces high-frequency noise in pixelwise influence maps. Adding Gaussian noise to inputs and averaging resulting saliency maps approximates local gradient averaging, filtering unstable fluctuations while preserving robust attributions. The method assumes gradient noise is uncorrelated and averages out, while true attributions are stable under small perturbations.

## Foundational Learning

- **Gradient-based feature attribution**: TFA builds on standard FA methods applied to TDA scores. Understanding how input gradients indicate feature importance is prerequisite. Quick check: Given a CNN, can you explain why ∂(class logit)/∂(input pixel) indicates which pixels most influence that class prediction?

- **Training data attribution (TDA)**: TFA composes FA on top of TDA; you must understand what TDA scores represent before applying FA to them. Quick check: For a test image misclassified as "dog," would you expect high TDA scores for training images of dogs or of the correct class? Explain the direction of influence.

- **First-order Taylor approximation**: Both Grad-Cos derivation and TFA gradient computation rely on linear approximations of loss changes. Quick check: Why does the Taylor expansion justify using ∂S/∂(training pixels) as a saliency map? What does the gradient magnitude represent?

## Architecture Onboarding

- **Component map**: Test image + Training images → Forward pass → Loss computation → Backward pass → ∂L/∂θ for train and test → Grad-Cos TDA → Scalar influence score per train image → Gradient of TDA score → ∂S/∂(train pixels) → SmoothGrad → Denoised saliency map → Output: Pixelwise influence heatmap on training image

- **Critical path**: Efficient batch gradient computation for all training candidates (memory bottleneck), numerically stable cosine similarity (handle near-zero gradients), second-order gradient computation (∂/∂x of ∂L/∂θ) - requires retaining computation graph

- **Design tradeoffs**: Grad-Cos vs Influence Functions (Grad-Cos faster but discards curvature), noise level σ (higher improves smoothing but risks blurring boundaries; recommended 5-20% range), number of SmoothGrad samples n (more samples reduce variance but increase compute; n≈20-50 typical)

- **Failure signatures**: Saliency maps uniformly highlight entire training image (gradient signal too weak or TDA scores near zero), top influential training images are all outliers/mislabeled (Grad-Cos may pick up loss artifacts; consider RelatIF normalization), saliency maps show checkerboard or patch artifacts (architectural limitation from patch embeddings)

- **First 3 experiments**: (1) Sanity check on linear model - implement toy example from Appendix A, verify TFA correctly assigns influence to informative example; (2) Insertion validation on CIFAR-10 - replicate Section 4.3, confirm negative Δ(T-R) gap versus random masking; (3) Spurious correlation detection - train ResNet-18 on binary task with synthetic patch shortcut, verify TFA localizes to patch on misclassified test images

## Open Questions the Paper Calls Out

### Open Question 1
Can TFA be effectively extended from pixel-level attributions to higher-level, human-interpretable concepts? The current implementation relies on continuous pixel gradients, whereas concepts require discrete or higher-dimensional semantic mappings that current gradient-based FA methods may not capture directly. Resolution would require successful integration with Concept Activation Vectors (CAVs) or prototype-based methods linking test predictions to semantically defined regions in training data.

### Open Question 2
Can TFA be adapted to robustly utilize Influence Functions (IF) for Training Data Attribution, given the instability observed with standard IF? Grad-Cos relies on gradient similarity which acts as a proxy for influence but lacks theoretical counterfactual guarantees of Influence Functions. Resolution would require a modified Influence Function technique that produces TFA maps focused on relevant features rather than outlying training samples.

### Open Question 3
Does reliance on SmoothGrad for denoising pixelwise influence maps obscure true feature-attribution signals? While smoothing improves visual quality, it may average out localized, high-frequency signals that genuinely influence the model's decision. Resolution would require a model-specific "sanity check" demonstrating that TFA maps degrade or change meaningfully when model parameters are randomized.

## Limitations
- Computational cost scales linearly with training set size, limiting applicability to large-scale datasets
- Quantitative validation relies on synthetic insertion tests rather than real-world error correction
- TFA inherits limitations from both FA and TDA: assumes gradient-based attribution captures true feature importance and depends on quality of underlying influence scores

## Confidence

- **High confidence**: The mechanism of applying FA to TDA scores is technically sound and well-supported by mathematical framework
- **Medium confidence**: The claim that TFA reveals spurious correlations missed by conventional attribution methods is demonstrated but relies on a single synthetic patch experiment
- **Medium confidence**: Qualitative visualizations are compelling but potentially cherry-picked without comprehensive statistics

## Next Checks

1. **Error correction validation**: Apply TFA to a dataset with known problematic training examples (e.g., mislabeled images) and measure whether removing or correcting top-TFA-attributed training regions improves test accuracy more than random corrections

2. **Cross-architecture generalization**: Test TFA on architectures beyond ResNet-18 and ViT-B/16 (e.g., Vision Transformers with different patch sizes, ConvNeXt) to verify robustness to architectural differences in gradient behavior

3. **Noise sensitivity analysis**: Systematically vary noise levels (σ) and sample counts (n) in SmoothGrad to determine sensitivity of TFA attributions to these hyperparameters, establishing guidelines for when attributions become unreliable