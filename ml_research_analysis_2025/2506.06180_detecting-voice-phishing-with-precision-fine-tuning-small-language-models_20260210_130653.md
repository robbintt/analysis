---
ver: rpa2
title: 'Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models'
arxiv_id: '2506.06180'
source_url: https://arxiv.org/abs/2506.06180
tags:
- transcripts
- dataset
- phishing
- criteria
- participant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a voice phishing (VP) detection system by fine-tuning
  Llama3-8B, a small language model (SLM). To enhance the model's performance, the
  researchers incorporated carefully designed VP evaluation criteria into the prompt
  and explored the use of Chain-of-Thought (CoT) techniques.
---

# Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models

## Quick Facts
- **arXiv ID:** 2506.06180
- **Source URL:** https://arxiv.org/abs/2506.06180
- **Reference count:** 37
- **Primary result:** Fine-tuning Llama3-8B with human-designed VP criteria achieves accuracy comparable to GPT-4 on voice phishing detection.

## Executive Summary
This paper presents a voice phishing (VP) detection system that fine-tunes a small language model (SLM) - specifically Llama3-8B - to achieve performance comparable to larger models like GPT-4. The key innovation is incorporating human-designed VP evaluation criteria directly into the prompt, which proves more effective than using Chain-of-Thought techniques for SLMs. The researchers also develop an adversarial test dataset to rigorously evaluate robustness, finding that their approach maintains strong performance even on challenging examples that contain VP-like vocabulary but are not actual phishing attempts.

## Method Summary
The method involves fine-tuning Llama3-8B using parameter-efficient LoRA techniques on transcripts labeled by GPT-4o. The labeling process uses a prompt containing 11 VP evaluation criteria that map to known fraud patterns. Transcripts are segmented into fixed-length blocks (typically 500 characters), and the SLM is trained to predict VP likelihood scores (0-10) generated by the teacher model. At inference, a weighted average of block likelihoods determines the final classification, with a threshold Î» separating VP from non-VP transcripts.

## Key Results
- Llama3-8B fine-tuned with VP criteria achieves 100% accuracy on normal test data at certain block lengths
- On adversarial test data, the approach demonstrates performance close to GPT-4-based detectors
- The model outperforms other SLMs and proprietary models like KoBERT
- Incorporating human expert knowledge into prompts is more effective than Chain-of-Thought techniques for SLMs in VP detection

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Injection via Prompt-Integrated Evaluation Criteria
- **Claim:** Embedding human-designed evaluation criteria in the prompt is more effective for VP detection with SLMs than using Chain-of-Thought (CoT) prompting.
- **Mechanism:** The SLM is provided with 11 explicit, structured evaluation criteria that map to known fraud patterns. This external knowledge guides the model's attention and reduces its reasoning burden, acting as a cognitive scaffold that allows the 8B model to perform comparably to a much larger one.
- **Core assumption:** The 11 evaluation criteria are sufficiently comprehensive to capture the majority of VP patterns in the data, and the SLM can reliably ground its output in these rules.
- **Evidence anchors:**
  - [abstract] "...incorporating human expert knowledge into the prompt is more effective than using the CoT technique for small LMs in VP detection."
  - [section 5.2] "Llama3-FT-Cri achieves the highest accuracy of 94.64% at a block-length of 500... indicating that providing the VP criteria is much more beneficial for SLMs than applying CoT."
  - [corpus] No direct corpus evidence validates this specific claim over CoT; this is a primary contribution of the paper.
- **Break condition:** The mechanism will degrade if VP tactics evolve to no longer match the predefined criteria.

### Mechanism 2: Knowledge Distillation from LLM to SLM
- **Claim:** Supervised fine-tuning of an SLM using labels generated by a more powerful teacher model (GPT-4o) transfers expert-level reasoning to the smaller model.
- **Mechanism:** The SLM is trained to predict the VP likelihood scores generated by GPT-4o. This process distills the teacher's general reasoning patterns and the specific VP knowledge embedded in its weights into the student model, specializing it for the detection task.
- **Core assumption:** The teacher model's (GPT-4o) predictions are accurate and serve as a high-quality proxy for ground truth labels.
- **Evidence anchors:**
  - [section 4.2] "we assign the labeling task to GPT-4o, which can be regarded as a form of knowledge extraction..."
  - [section 5.1] "...Llama3-FT schemes... achieving high accuracies exceeding 97% in all cases..."
  - [corpus] Weak, indirect support is found in a related paper on phishing email detection (arXiv:2505.00034), which uses a similar fine-tuning approach.
- **Break condition:** The fine-tuned model will fail to generalize if the VP transcripts in the training data are not representative of real-world attacks (distribution shift).

### Mechanism 3: Robustness Evaluation via Adversarial Test Dataset
- **Claim:** Evaluating VP detectors on an adversarial dataset reveals performance limitations not apparent on standard datasets.
- **Mechanism:** The adversarial dataset includes transcripts that contain VP-like vocabulary and contexts (e.g., discussing a past scam) but are not actual VP attempts. This forces the model to perform deeper contextual understanding to avoid misclassifying them, providing a more rigorous test of its capabilities.
- **Core assumption:** The constructed adversarial examples are realistic and representative of the challenging edge cases a detector would face in a live setting.
- **Evidence anchors:**
  - [section 3.2] "...LMs with limited contextual understanding ability are more prone to misclassifying these transcripts as VP."
  - [section 5.2] "KoBERT achieves an accuracy of 56.25%... a decrease of approximately 42 %p compared to its performance on the normal dataset."
  - [corpus] Corpus papers do not explicitly discuss this type of adversarial evaluation.
- **Break condition:** The evaluation may not predict real-world performance if the adversarial examples are too artificial or fail to capture the true diversity of non-VP financial conversations.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) with LoRA**
  - **Why needed here:** To fine-tune an 8B parameter model without needing massive computational resources. LoRA enables this by freezing the main model weights and training only small, low-rank adapter matrices.
  - **Quick check question:** What are the primary benefits of using LoRA over full fine-tuning for an SLM in a resource-constrained environment?

- **Concept: Knowledge Distillation**
  - **Why needed here:** The best model is fine-tuned on data labeled by a larger, more powerful model (GPT-4o). Understanding this student-teacher dynamic is critical for replicating the results.
  - **Quick check question:** What are the potential risks of training a student model entirely on the outputs of a proprietary teacher model?

- **Concept: Adversarial Evaluation**
  - **Why needed here:** The paper demonstrates that high accuracy on a standard test set can be misleading. Learning to construct and use adversarial datasets is essential for building trustworthy systems.
  - **Quick check question:** Why would a transcript of someone *reporting* a voice phishing call be a challenging adversarial example?

## Architecture Onboarding

- **Component map:** Data Pipeline -> Teacher Model (GPT-4o) -> Student Model (Llama3-8B) -> Inference Engine
- **Critical path:** The design and quality of the 11 VP evaluation criteria. The paper shows this is the single most important factor for improving SLM performance, surpassing even CoT techniques.
- **Design tradeoffs:**
  - **Cri vs. CoT:** Prompting with expert knowledge (Cri) is more effective and efficient for SLMs on this task than CoT, which offers no performance gain and adds computational overhead.
  - **Block Length:** There is an optimal context window (around 500 letters). Longer blocks can degrade SLM performance, while shorter blocks might lose necessary context.
  - **SLM vs. Proprietary LLM:** The SLM approach requires upfront investment in fine-tuning and data but leads to lower operating costs and preserves data privacy by not sending transcripts to an external API.
- **Failure signatures:**
  - **High False Positive Rate:** The model may misclassify legitimate financial discussions or reports of VP as actual VP attempts, especially on the adversarial dataset.
  - **Criterion Brittleness:** The model might fail to detect novel VP scams that do not neatly fit into the 11 predefined evaluation criteria.
- **First 3 experiments:**
  1. **Baseline & Fine-Tuning Comparison:** Evaluate the unfine-tuned `Llama3-Base` model against the fine-tuned `Llama3-FT-Cri` model on the adversarial dataset to quantify the improvement gained from fine-tuning.
  2. **Prompt Ablation Study:** Compare the performance of `Llama3-FT-Cri`, `Llama3-FT-CoT`, and `Llama3-FT-CoTCri` to validate that the evaluation criteria are the key driver of performance.
  3. **Block Length Analysis:** Test the best-performing model (`Llama3-FT-Cri`) with different block lengths (100, 300, 500, 900, 1500) to identify the optimal setting for balancing context and SLM capacity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can VP detection be effectively implemented on mobile devices using SLMs with significantly fewer parameters than Llama3-8B?
- **Basis in paper:** [explicit] The conclusion identifies "fine-tuning SLMs with significantly fewer parameters" for mobile deployment as a topic for "future research."
- **Why unresolved:** The current study validates the 8B parameter model but does not test if the balance of accuracy and efficiency holds for smaller models (e.g., 1B-3B parameters) suitable for on-device processing.
- **What evidence would resolve it:** Performance benchmarks of quantized or smaller architectures (e.g., Llama-3.2-1B) on the adversarial test dataset.

### Open Question 2
- **Question:** How can the performance gap between SLMs and proprietary LLMs in processing long-context blocks be mitigated?
- **Basis in paper:** [inferred] Results indicate that while GPT-4o accuracy improves with larger block lengths, SLM performance generally degrades or plateaus due to "limited capacity."
- **Why unresolved:** The paper identifies this capacity limitation as the cause but does not propose or test methods (e.g., RAG, specialized attention mechanisms) to maintain performance over longer transcripts.
- **What evidence would resolve it:** Experiments applying context-extension techniques to the SLM to measure accuracy retention on transcripts exceeding 2500 characters.

### Open Question 3
- **Question:** Can Chain-of-Thought (CoT) techniques be refined to outperform explicit criteria-based prompting for SLMs in VP detection?
- **Basis in paper:** [inferred] The study found that incorporating human expert criteria outperformed CoT, suggesting SLMs struggle to reason without explicit rules, but it leaves open the possibility that optimized CoT methods could succeed.
- **Why unresolved:** The authors tested specific CoT prompts but did not explore if different reasoning distillation methods could enable the SLM to derive its own effective criteria.
- **What evidence would resolve it:** Comparative analysis using advanced CoT distillation techniques (e.g., verify-and-refine) against the criteria-based approach on the adversarial dataset.

## Limitations
- The approach's effectiveness depends on the completeness of the 11 VP evaluation criteria, which may not capture novel or evolving phishing tactics
- The SLM shows degraded performance on long blocks (>1500 characters) compared to the teacher model GPT-4o
- The method requires upfront investment in fine-tuning and labeled data, though it offers lower operational costs than proprietary API calls

## Confidence
- **High:** The paper provides comprehensive experimental results comparing multiple model configurations and evaluation datasets, with clear performance metrics
- **Medium:** Some implementation details like exact LoRA hyperparameters and prompt variations are not fully specified
- **Medium:** The effectiveness of the 11 criteria assumption relies on their comprehensiveness for capturing VP patterns

## Next Checks
1. Verify the exact prompt structure used for the Llama3-FT-Cri fine-tuning, particularly the phrasing of the 11 VP criteria
2. Test the model with different block lengths (100, 300, 500, 900, 1500) to confirm the optimal context window
3. Evaluate the model on transcripts containing adversarial examples like VP reports or discussions to assess false positive rates