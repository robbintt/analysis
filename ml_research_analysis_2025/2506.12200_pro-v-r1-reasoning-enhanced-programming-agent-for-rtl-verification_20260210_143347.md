---
ver: rpa2
title: 'PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification'
arxiv_id: '2506.12200'
source_url: https://arxiv.org/abs/2506.12200
tags:
- verification
- arxiv
- generation
- agent
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PRO-V-R1, the first trainable open-source
  agentic framework for autonomous RTL verification. It couples LLM-based reasoning
  with programmatic tools to overcome the limitations of relying on large proprietary
  models for Python-based functional reference generation.
---

# PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification

## Quick Facts
- arXiv ID: 2506.12200
- Source URL: https://arxiv.org/abs/2506.12200
- Reference count: 40
- Primary result: Achieves 57.7% functional correctness and 34.0% robust fault detection on VerilogEval-v2, significantly outperforming base model (25.7%/21.8%) and matching or exceeding large proprietary LLMs

## Executive Summary
PRO-V-R1 introduces the first trainable open-source agentic framework for autonomous RTL verification, addressing the limitations of relying on large proprietary models for Python-based functional reference generation. The system couples LLM-based reasoning with programmatic tools to generate Python Functional Reference Models (FRM) and testbenches that verify RTL correctness. It tackles three key challenges: agentic system inefficiency, supervised fine-tuning data scarcity, and missing reinforcement learning training frameworks. The framework achieves significant improvements in both functional correctness (57.7% vs 25.7% baseline) and robustness (34.0% vs 21.8% baseline) while delivering an 8.8× speedup in verification time.

## Method Summary
PRO-V-R1 employs a three-agent system (Testcase Generator, FRM Generator, Judge) that interacts with tools including Code Interpreter, Testbench Edit Tool, and Verilator Simulation Driver. The training pipeline consists of two stages: supervised fine-tuning (SFT) on trajectories generated by a teacher model (DeepSeek-V3.2) and filtered via simulation verification, followed by reinforcement learning using Group Relative Policy Optimization (GRPO) with rewards for tool format correctness and functional verification success. The framework uses Qwen3-8B as the base model and evaluates on VerilogEval-v2 and RTLLM v2.0 datasets using a three-level protocol: compile correctness (Eval0), functional correctness (Eval1), and fault detection via RTL mutants (Eval2).

## Key Results
- Achieves 57.7% functional correctness and 34.0% robust fault detection on VerilogEval-v2
- Outperforms base model (25.7% functional correctness, 21.8% fault detection) by significant margins
- Matches or exceeds performance of large proprietary LLMs while being 8.8× faster (91s vs 805s per task)
- Successfully addresses tool hallucination issues through programmatic tool integration and RL optimization

## Why This Works (Mechanism)
The framework succeeds by integrating programmatic tools with LLM reasoning, creating a hybrid approach that combines the pattern recognition capabilities of LLMs with the precision and determinism of traditional verification tools. The SFT stage transfers expert knowledge from a powerful teacher model while the RL stage fine-tunes the agent's tool usage and reasoning strategies based on actual verification outcomes. The three-level evaluation protocol (compile, functional, mutation) provides a comprehensive assessment that goes beyond simple syntax checking to measure actual verification capability and robustness.

## Foundational Learning

**Verilog RTL verification fundamentals**: Understanding how hardware description languages represent digital circuits and how functional reference models validate RTL behavior.
*Why needed*: Core domain knowledge for interpreting verification results and debugging failures
*Quick check*: Can you explain the difference between RTL simulation and formal verification?

**Functional Reference Model (FRM) concept**: Python programs that emulate RTL behavior to validate correctness through simulation comparison.
*Why needed*: The entire verification pipeline depends on generating accurate FRMs
*Quick check*: Can you describe how an FRM would validate a simple counter circuit?

**Reinforcement learning for tool-based agents**: Training agents to optimize sequences of tool calls based on verification rewards rather than just text generation.
*Why needed*: Enables the system to learn effective verification strategies through trial and error
*Quick check*: Can you explain how GRPO differs from standard policy gradient methods?

**Verilator simulation integration**: Using Verilator as a fast, cycle-accurate simulator to validate RTL and FRM outputs.
*Why needed*: Provides the ground truth for training and evaluation
*Quick check*: Can you describe what happens during a Verilator simulation run?

## Architecture Onboarding

**Component map**: Verilog RTL Design → Verilator Simulation Driver → Testbench Edit Tool ← Judge Agent ← FRM Generator Agent ← Code Interpreter Tool ← LLM Core ← Testcase Generator Agent → Python Testcases

**Critical path**: RTL Design → Verilator → FRM Generation → FRM Simulation → Comparison → Verification Result

**Design tradeoffs**: The framework trades off pure LLM autonomy for hybrid tool integration, sacrificing some flexibility for improved reliability and speed. Uses smaller open-source models instead of large proprietary ones, accepting some performance gap in exchange for trainability and accessibility.

**Failure signatures**: 
- Semantic mismatches in bit ordering between Python and Verilog representations
- Tool hallucination producing invalid JSON arguments for verification tools
- Simulation timeouts or crashes during FRM/RTL comparison
- Judge agent indecision in majority voting scenarios

**3 first experiments**:
1. Verify tool output format compliance by logging raw LLM outputs before tool parsing
2. Cross-validate mutation analysis by replicating RTL mutant generation for a subset of problems
3. Testbench edit tool behavior by instrumenting Judge agent to log voting decisions and tool invocations

## Open Questions the Paper Calls Out

**Scaling limits**: Can additional reinforcement learning data and optimization close the remaining performance gap with proprietary models on maximum-stringency fault detection (Eval2-100)?

**Industrial generalization**: How does the framework perform on complex, multi-file industrial RTL designs compared to the single-module academic benchmarks used in evaluation?

**Teacher model dependency**: Does reliance on a proprietary teacher model (DeepSeek-V3.2) impose an upper bound on the open-source student model's reasoning capabilities?

## Limitations
- Heavy reliance on proprietary DeepSeek-V3.2 model for SFT data generation creates dependency and limits full reproducibility
- Evaluation restricted to small, self-contained problems that may not represent real-world RTL verification complexity
- RL training depends on working Verilator environment, introducing variability across hardware configurations

## Confidence
- Functional correctness improvement (57.7% vs 25.7% baseline): High confidence
- Speedup claims (8.8× faster): Medium confidence
- Tool hallucination reduction: Medium confidence

## Next Checks
1. Implement logging of raw LLM outputs before tool parsing to quantify JSON format violations
2. Replicate RTL mutant generation process for a subset of problems to verify systematic generation
3. Instrument Judge agent code to log voting decisions and tool invocations for behavior analysis