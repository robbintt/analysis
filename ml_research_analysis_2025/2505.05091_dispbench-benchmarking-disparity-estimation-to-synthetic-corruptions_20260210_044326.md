---
ver: rpa2
title: 'DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions'
arxiv_id: '2505.05091'
source_url: https://arxiv.org/abs/2505.05091
tags:
- severity
- kitti2015
- flyingthings3d
- corruptions
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DISP BENCH, the first standardized benchmarking
  tool for evaluating the robustness of disparity estimation methods against synthetic
  image corruptions. The tool assesses performance on two datasets (FlyingThings3D
  and KITTI2015) under adversarial attacks (FGSM, BIM, PGD, APGD, CosPGD) and 15 common
  2D image corruptions across five severity levels.
---

# DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions

## Quick Facts
- arXiv ID: 2505.05091
- Source URL: https://arxiv.org/abs/2505.05091
- Reference count: 40
- Key outcome: DISP BENCH is the first standardized benchmarking tool for evaluating disparity estimation robustness against synthetic corruptions, revealing that transformer models generalize worse than CNNs to common real-world corruptions.

## Executive Summary
This paper introduces DISP BENCH, a comprehensive benchmarking framework for evaluating the robustness of disparity estimation methods against synthetic image corruptions. The tool assesses performance on both synthetic (FlyingThings3D) and real-world (KITTI2015) datasets under adversarial attacks and 15 common 2D image corruptions across five severity levels. Experiments with four disparity estimation architectures reveal that while newer transformer-based models outperform older CNN-based models on clean data, they generalize significantly worse to common corruptions, particularly weather-related ones. Critically, the analysis shows that synthetic corruptions on synthetic datasets do not reliably predict performance on real-world datasets, underscoring the need for real-world data in safety-critical applications.

## Method Summary
DISP BENCH provides a standardized evaluation framework that applies synthetic corruptions (adversarial attacks and 2D common corruptions) to disparity estimation methods. The benchmark implements five adversarial attacks (FGSM, BIM, PGD, APGD, CosPGD) and 15 common corruption types at five severity levels. It evaluates four disparity estimation architectures (CFNet, GWCNet-G, STTR, STTR-light) across two datasets (FlyingThings3D and KITTI2015). The framework computes mean End-Point-Error (EPE) as the primary metric, aggregating results across all corruption types and severity levels to quantify robustness gaps between methods.

## Key Results
- Transformer-based methods (STTR, STTR-light) achieve better i.i.d. performance than CNN-based methods (CFNet, GWCNet-G) on clean data
- Under weather corruptions (fog, frost, snow), transformer methods show significantly worse robustness, with EPE exceeding 100 under fog severity 5 versus ~20 for GWCNet-G
- Synthetic corruptions on synthetic datasets (FlyingThings3D) fail to predict real-world robustness on KITTI2015, showing no correlation between performance across datasets
- The benchmark reveals an architecture-accuracy-robustness trade-off not apparent from standard evaluation

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Corruptions as Real-World Robustness Proxies
When applied to real-world datasets, 2D Common Corruptions serve as valid proxies for evaluating model robustness against realistic domain shifts. The benchmark applies 15 standardized corruption types at 5 severity levels to both synthetic and real-world datasets. If models trained on real data and corrupted with synthetic perturbations show performance degradation patterns that correlate with real-world failure modes, researchers can evaluate robustness without collecting expensive corrupted ground-truth data. The core assumption is that the corruption types and severity scales meaningfully approximate real-world distribution shifts. Evidence shows this works when applied to real datasets, but breaks down when synthetic corruptions are applied to synthetic datasets (Figure 6 shows no correlation between FlyingThings3D+corruptions and KITTI2015+corruptions performance).

### Mechanism 2: Adversarial Attacks Reveal Learned Representation Quality
White-box adversarial attacks serve as a proxy for worst-case model reliability by exposing vulnerabilities in learned feature representations. The benchmark implements five attacks (FGSM, BIM, PGD, APGD, CosPGD) that iteratively optimize input perturbations within an $\epsilon$-ball to maximize prediction error. CosPGD is particularly effective because it scales the loss by pixel-wise alignment between predictions and ground truth. If a model has learned shortcuts or fragile features, adversarial perturbations will cause disproportionate error increases compared to robust models. The core assumption is that white-box attacks with full model access represent a meaningful upper bound on worst-case deployment failures. Evidence anchors include the paper's observation that adversarial attacks can simulate worst-case scenarios for measuring reliability.

### Mechanism 3: Architecture-Accuracy-Robustness Trade-off Detection
Systematic benchmarking reveals that architecture choices (CNN vs. Transformer) create trade-offs between clean i.i.d. performance and corruption robustness that are not apparent from standard evaluation. The benchmark evaluates multiple architectures across identical corruption scenarios, aggregating mean EPE across 15 corruption types at 5 severity levels. If a newer architecture achieves better i.i.d. performance but degrades faster under corruption, the benchmark exposes this previously hidden trade-off. The core assumption is that the selected architectures and corruption types are representative enough that observed patterns generalize. Evidence shows that while STTR achieves better i.i.d. performance, it is significantly less robust to weather corruptions than older CNN methods.

## Foundational Learning

- **Disparity Estimation & Stereo Matching**
  - Why needed here: The entire benchmark evaluates disparity estimation methods that compute pixel-wise correspondence between stereo image pairs to infer depth. Understanding the task and primary metric (EPE) is essential to interpret results.
  - Quick check question: Given a stereo pair with baseline $b$ and focal length $f$, if the disparity at pixel $(u,v)$ is $d$ pixels, what is the depth $Z$? (Answer: $Z = f \cdot b / d$)

- **Out-of-Distribution (OOD) Robustness & Distribution Shift**
  - Why needed here: The paper's central contribution is evaluating how methods perform when test samples are not i.i.d. with training data due to corruptions. Understanding that small perturbations can cause models to fail by exploiting learned shortcuts is critical.
  - Quick check question: A model trained on sunny KITTI images achieves 2.0 EPE. When evaluated on the same images with fog corruption (severity 3), EPE jumps to 50. Is this an i.i.d. or OOD evaluation, and what does this suggest about the model? (Answer: OOD evaluation; the model may have learned weather-specific shortcuts rather than robust stereo correspondence features)

- **Adversarial Attacks & Perturbation Bounds**
  - Why needed here: The benchmark implements attacks constrained by $L_p$ norms (e.g., $\ell_\infty$ with $\epsilon = 8/255$). Understanding that these attacks optimize imperceptible perturbations to maximize loss helps interpret why they reveal model vulnerabilities.
  - Quick check question: In PGD with $\ell_\infty$ bound $\epsilon = 8/255$, step size $\alpha = 0.01$, and 20 iterations, what constraint ensures perturbations remain imperceptible? (Answer: After each iteration, the perturbation $\delta$ is clipped so $|\delta| \leq \epsilon$ in every pixel, keeping the adversarial image within $\epsilon$ of the original in $\ell_\infty$ norm)

## Architecture Onboarding

- **Component Map:**
  - Model Zoo -> Threat Model Engine -> Evaluation Core -> Result Cache
  - Pre-trained checkpoints for 4 architectures across 2 datasets are retrieved via `load_model(model_name='STTR', dataset='KITTI2015')`
  - Threat model configuration supports clean i.i.d., adversarial attacks, and 2D common corruptions
  - Evaluation computes mean EPE and aggregates results across corruption types

- **Critical Path:**
  1. Select method-dataset pair (e.g., STTR on KITTI2015) → ensures checkpoint compatibility
  2. Define threat model in `config.yml` (e.g., `threat_model="2DCommonCorruption"`, `severity=3`)
  3. Call `evaluate()` → loads model, applies perturbations, computes EPE, returns results
  4. Compare across architectures/corruptions → identify robustness gaps

- **Design Tradeoffs:**
  - Synthetic vs. Real-World Data: FlyingThings3D is large and synthetic but doesn't correlate with real-world robustness; KITTI2015 is real but limited (400 frames, fewer pretrained checkpoints available)
  - Attack Strength vs. Realism: Stronger attacks reveal more vulnerabilities but may be unrealistic for deployment scenarios. Default $\epsilon = 8/255$ is a common benchmark standard
  - Comprehensive vs. Focused Evaluation: Evaluating all 15 corruptions at 5 severities + 5 attacks is compute-intensive. Use targeted evaluations for specific failure modes (e.g., only weather corruptions)

- **Failure Signatures:**
  - EPE > 10x i.i.d. baseline under mild corruption (severity 1-2): Indicates severe overfitting or shortcut learning
  - Transformer > CNN error gap under weather corruptions: Signature of architecture-specific vulnerability (Figure 4 shows STTR-light can exceed 100 EPE under fog severity 5 vs. ~20 for GWCNet-G)
  - No correlation between synthetic-dataset and real-dataset corruption performance: Indicates dataset-specific shortcuts (Figure 6)

- **First 3 Experiments:**
  1. Establish i.i.d. baselines: Evaluate all 4 methods on clean KITTI2015 validation set. If EPE rankings don't match published results, checkpoint loading may be incorrect
  2. Weather corruption stress test: Evaluate all methods on Frost and Fog corruptions (severity 1, 3, 5) on KITTI2015. If Transformer methods degrade significantly more than CNNs, confirms paper's finding and provides a concrete robustness gap to address
  3. Cross-dataset validation check: For GWCNet-G and STTR (available on both datasets), compare corruption robustness on FlyingThings3D vs. KITTI2015. If no correlation (as paper shows), confirms that synthetic-dataset robustness conclusions should not be trusted for real-world deployment predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do transformer-based disparity methods exhibit significantly worse robustness to weather-related perturbations compared to older CNN architectures?
- Basis in paper: The authors observe in Section 4.1 that newer STTR models are "surprisingly less robust" to weather corruptions like Snow, Frost, and Fog than older methods like GWCNet.
- Why unresolved: The paper identifies this "alarming" trend but does not investigate the architectural mechanisms or attention properties causing this regression in robustness.
- What evidence would resolve it: Ablation studies analyzing attention mechanisms versus convolutional features under specific noise and blur distributions.

### Open Question 2
- Question: How can synthetic-to-real robustness transfer be improved given that synthetic corruptions on synthetic datasets fail to represent real-world conditions?
- Basis in paper: Section 5 concludes that "synthetic corruptions on synthetic datasets do not represent real-world corruptions," creating a gap for simulator-based training.
- Why unresolved: The authors demonstrate the failure of current proxies (FlyingThings3D) but do not propose a solution for bridging this specific domain gap.
- What evidence would resolve it: A new training methodology where robustness on synthetic data correlates positively with performance on real-world corruptions (e.g., KITTI2015).

### Open Question 3
- Question: How can DispBench be extended to effectively benchmark the robustness of foundational models like StereoAnything and LightStereo?
- Basis in paper: In Section 6, the authors state, "We intend to adapt our evaluator into OpenStereo to enable safety studies of SotA disparity estimation methods" like StereoAnything.
- Why unresolved: These newer foundational models are not included in the current benchmark, leaving their safety and reliability properties unquantified.
- What evidence would resolve it: Successful integration of these models into the DispBench framework with published results against adversarial attacks and common corruptions.

## Limitations

- Limited number of pretrained checkpoints available for FlyingThings3D (only 2 out of 4 methods), constraining synthetic dataset analysis
- Benchmark relies on predefined corruption severity scales that may not perfectly capture real-world distribution shifts
- Adversarial attack framework assumes white-box access, which may overestimate vulnerability in black-box deployment scenarios

## Confidence

- **High Confidence**: Transformer models show better i.i.d. performance than CNN models on clean data (Section 4.1 baseline results)
- **Medium Confidence**: Transformer models exhibit significantly worse robustness to weather corruptions (Section 4.1 results, but dependent on specific severity scaling)
- **Low Confidence**: Synthetic corruption performance on FlyingThings3D cannot predict real-world robustness on KITTI2015 (Section 5 correlation analysis, limited by small sample size of methods tested on both datasets)

## Next Checks

1. **Architecture-Agnostic Weather Robustness**: Replicate the weather corruption experiments on additional disparity estimation methods (both CNN and transformer-based) to verify if the observed robustness gap is architecture-dependent or method-specific.

2. **Real-World Data Augmentation**: Collect and test on a small set of real-world corrupted images (fog, frost, snow) to validate whether synthetic weather corruptions accurately simulate real degradation patterns.

3. **Transfer Learning Analysis**: Evaluate whether fine-tuning transformer models on corrupted KITTI2015 data improves robustness without sacrificing i.i.d. performance, testing the practical utility of the benchmark for method improvement.