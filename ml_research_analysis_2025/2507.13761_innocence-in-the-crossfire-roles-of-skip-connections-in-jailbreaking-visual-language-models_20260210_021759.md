---
ver: rpa2
title: 'Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual
  Language Models'
arxiv_id: '2507.13761'
source_url: https://arxiv.org/abs/2507.13761
tags:
- toxic
- benign
- layer
- visual
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates jailbreaking vulnerabilities in visual
  language models (VLMs) by analyzing how prompt design influences inappropriate content
  generation. The authors focus on three key factors: visual descriptions, adversarial
  examples, and positively framed responses, examining their individual and combined
  effects.'
---

# Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models

## Quick Facts
- arXiv ID: 2507.13761
- Source URL: https://arxiv.org/abs/2507.13761
- Authors: Palash Nandi; Maithili Joshi; Tanmoy Chakraborty
- Reference count: 40
- Primary result: SKIP-CON method increases jailbreak success rates by 18-55% across LLaVA models by exploiting layer-wise toxic-benign separation patterns

## Executive Summary
This paper investigates jailbreaking vulnerabilities in visual language models (VLMs) by analyzing how prompt design influences inappropriate content generation. The authors identify three key factors—visual descriptions, adversarial examples, and positively framed responses—and propose SKIP-CON, a method that introduces skip connections between internal VLM layers to increase jailbreak success rates. Experiments on LLaVA-V, LLaVA-M, and MiniGPT4 show that each factor can independently trigger jailbreaks, with generalized-toxic in-context examples having the strongest impact. SKIP-CON improves attack success rates by 18%, 55%, and 26% respectively, even with benign images.

## Method Summary
The study combines prompt engineering with architectural manipulation to jailbreak VLMs. Researchers first identify layers where toxic-benign separation emerges through hidden state probing (text separation at layers 3-7, visual at layers 15-23). They then implement SKIP-CON by adding outputs from early separation layers to later layers via skip connections with scaling factor λ=0.01. The attack uses 100 toxic queries from Beavertails and 100 benign queries, paired with benign images, toxic images, and harmful memes. Success is measured through a two-stage Attack Success Rate (2-ASR) that first checks for denial phrases and then uses Llama-Guard-3-1B classification.

## Key Results
- Each of the three prompt factors (visual descriptions, adversarial examples, positive framing) can independently trigger jailbreaks
- Generalized-toxic in-context examples achieve the strongest jailbreak performance (scores 0.65-0.73 across categories)
- SKIP-CON increases attack success rates by 18% (LLaVA-V), 55% (LLaVA-M), and 26% (MiniGPT4)
- Harmful memes are as effective as toxic visuals in eliciting inappropriate outputs
- Toxic-benign separation degrades significantly in multimodal contexts despite maintaining in unimodal settings

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Safety Degradation
- Claim: VLMs lose their ability to distinguish toxic from benign inputs when processing combined text-image modalities, despite maintaining this ability in unimodal settings.
- Mechanism: Text-based toxic-benign separation emerges at layers 3-7, while visual-based separation emerges much later (layers 15-23). When both modalities are present, the earlier text-based distinction collapses at layer 5-6, as visual information disrupts the nascent safety clustering before it matures.
- Core assumption: The text-dominant processing priority of VLMs causes early-layer toxic-benign distinctions to be overwritten by visual integration at mid-layers.
- Evidence anchors: [abstract] "while a VLM can reliably distinguish between benign and harmful inputs in unimodal settings (text-only or image-only), this ability significantly degrades in multimodal contexts"; [section IV] Figures 4-5 show distinguishability persists until layer 4-5 but collapses at layer 5-6 with multimodal inputs.

### Mechanism 2: In-Context Example Steering
- Claim: As few as three in-context examples can shift model behavior toward generating inappropriate outputs, with generalized-toxic examples proving most effective.
- Mechanism: In-context examples prime the model's attention and generation patterns. Self-reflective examples (matching image toxicity) provide moderate steering, while generalized-toxic examples (unrelated toxic patterns) create stronger behavioral shifts by establishing a consistent toxic response schema regardless of visual content.
- Core assumption: The model's in-context learning mechanism prioritizes response pattern matching over safety alignment when example demonstrations establish a coherent but harmful pattern.
- Evidence anchors: [abstract] "even a small number of in-context examples (as few as three) can push the model toward generating inappropriate outputs"; [section VII-C] With generalized-toxic examples, "the performance across all categories tends to converge" (scores 0.65-0.73).

### Mechanism 3: Skip-Connection Amplification (SKIP-CON)
- Claim: Connecting an early layer (where toxic-benign separation first emerges) to a later layer (where it becomes pronounced) amplifies jailbreak success rates by 18-55% across tested models.
- Mechanism: SKIP-CON adds outputs from layer i (self-attention h²ᵢ and MLP h⁵ᵢ) to the corresponding outputs at layer j, scaled by λ=0.01. This reinforces early safety-distinction signals at later layers where they would normally be diluted by multimodal integration.
- Core assumption: Early-layer toxic-benign distinctions contain useful safety-relevant information that is progressively lost through standard forward propagation in multimodal contexts.
- Evidence anchors: [abstract] "SKIP-CON significantly increases jailbreak success rates—18%, 55%, and 26% for [LLaVA-V, LLaVA-M, MiniGPT4]"; [section V-B] Formal specification: h³ₗ = res³₀ + h²ₗ + λ·h²ᵢ when l=j.

## Foundational Learning

- Concept: **Layer-wise representational analysis via hidden state probing**
  - Why needed here: Understanding where toxic-benign separation emerges in VLMs is foundational to the SKIP-CON design. Without knowing layer i and j, the skip connection cannot be targeted.
  - Quick check question: Given a VLM with 32 layers, how would you identify which layer first shows statistically significant separation between toxic and benign text-only inputs?

- Concept: **In-context learning and example-based behavioral steering**
  - Why needed here: The paper's finding that 3 examples can jailbreak requires understanding how demonstrations influence attention and generation patterns.
  - Quick check question: Why might generalized-toxic examples be more effective than self-reflective examples at eliciting harmful outputs, even when paired with benign images?

- Concept: **Cross-modal attention in vision-language models**
  - Why needed here: Understanding how visual tokens are integrated with text tokens explains why safety distinctions collapse in multimodal contexts.
  - Quick check question: At which layer range does the paper find visual toxic-benign separation typically emerging, and why might this late emergence cause multimodal safety degradation?

## Architecture Onboarding

- Component map: Vision encoder g(·) → visual features Zv → projection W → language tokens hv → Decoder layers with self-attention and MLP, each with residual streams → SKIP-CON: Connects layer i (early separation, ~3-7 for text) to layer j (mature separation, ~7+ for text, 22+ for images)

- Critical path:
  1. Input image → vision encoder → projection → token embedding integration
  2. Hidden states propagate through decoder layers
  3. At layer i, toxic-benign distinction first emerges (probing confirms)
  4. At layer j, SKIP-CON adds scaled layer i outputs to layer j outputs
  5. Final token generation via softmax over vocabulary

- Design tradeoffs:
  - λ=0.01 is small to prevent overwhelming normal layer j computations, but may under-amplify useful signals
  - Selecting i too early may capture noise; selecting j too late may skip intervention window
  - White-box access required—limits real-world attack applicability but enables precise targeting

- Failure signatures:
  - 2-ASR drops if i and j are misidentified (wrong layers)
  - Generation quality degrades if λ is too large
  - No improvement if the base model already has collapsed safety distinctions (e.g., heavily fine-tuned)

- First 3 experiments:
  1. Replicate layer-wise probing on LLaVA-V: Extract hidden states of final token for 100 toxic and 100 benign text-only inputs, project to 2D via t-SNE/PCA, and identify layer where separation first emerges.
  2. Ablate SKIP-CON λ values: Test λ ∈ {0.001, 0.01, 0.1} on LLaVA-M with benign images and generalized-toxic examples, measuring 2-ASR to find optimal scaling.
  3. Test cross-model transfer: Apply layer i/j identified in LLaVA-V to LLaVA-M without re-probing, measuring whether SKIP-CON gains degrade (tests layer universality assumption).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the number of visual inputs in a single prompt affect jailbreak success rates, and does the relationship scale linearly or exhibit threshold effects?
- Basis in paper: [explicit] The conclusion states: "Given text's dominant influence over visuals, future work will explore multiple visual inputs to better assess their impact."
- Why unresolved: The study only examined single-image prompts, leaving the compounding effect of multiple visual inputs unexplored.
- What evidence would resolve it: Experiments varying the number of concurrent images (2, 3, 5, 10) with both benign and toxic content, measuring 2-ASR across all three model architectures.

### Open Question 2
- Question: What architectural modifications or defense mechanisms could specifically mitigate skip-connection-based jailbreaks without degrading legitimate multimodal reasoning?
- Basis in paper: [inferred] The paper demonstrates SKIP-CON's effectiveness but does not explore countermeasures; the proposed attack exploits fundamental architectural features (skip connections) that may be difficult to remove without harming model performance.
- Why unresolved: Defense mechanisms against layer-manipulation attacks in VLMs remain unexplored in the current work.
- What evidence would resolve it: Testing defense strategies such as attention pattern regularization, layer-wise toxicity monitoring, or modified training objectives that preserve multimodal capabilities while reducing vulnerability.

### Open Question 3
- Question: Is there a principled, generalizable method for identifying optimal source and target layers (i, j) for skip-connection attacks across different VLM architectures and scales?
- Basis in paper: [inferred] The paper identifies layers where distinguishability emerges through visualization but provides no systematic framework for layer selection, making the approach potentially model-specific.
- Why unresolved: Layer selection appears empirical and architecture-dependent; the paper does not establish whether patterns generalize across model families or scales.
- What evidence would resolve it: A comparative study across additional architectures (e.g., Qwen-VL, InternVL) and model sizes, correlating layer selection with architectural features such as hidden dimensions, attention head counts, or layer normalization patterns.

## Limitations

- White-box requirement: SKIP-CON requires precise identification of layer indices (i, j) where toxic-benign separation emerges, creating a critical reproduction gap as exact layer numbers per model aren't specified
- Limited model scope: Study focuses exclusively on LLaVA variants without testing whether findings generalize to other VLM architectures or larger models
- Prompt template underspecification: Exact prompt templates for the six experimental factors remain underspecified beyond brief examples, potentially affecting result reproducibility

## Confidence

- **High confidence**: The multimodal safety degradation mechanism (text distinctions emerge early, visual distinctions emerge late, combined processing causes collapse) is well-supported by layer-wise probing visualizations and aligns with established VLM attention patterns.
- **Medium confidence**: The in-context example steering effectiveness is demonstrated but relies on specific prompt templates and example selections that aren't fully specified, making exact replication challenging.
- **Medium confidence**: SKIP-CON performance improvements are quantified but depend critically on correct layer selection, which isn't directly specified in the paper.

## Next Checks

1. **Layer Identification Validation**: Replicate the layer-wise probing methodology on LLaVA-V to identify the exact layer indices (i, j) where toxic-benign separation first emerges and becomes pronounced for text-only inputs. Verify these match the visualized results in the paper's Figures 4-5.

2. **Prompt Template Specification**: Implement the complete prompt generation pipeline including visual descriptions, in-context examples (both self-reflective and generalized-toxic), and positive framing phrases based on the paper's examples. Test whether these templates reproduce the reported 2-ASR improvements across all six experimental conditions.

3. **Cross-Architecture Transfer**: Apply SKIP-CON using the layer indices identified in LLaVA-V to LLaVA-M and MiniGPT4 without re-probing. Measure whether the jailbreak improvements transfer across models, testing the assumption that layer i/j are consistent across VLM variants.