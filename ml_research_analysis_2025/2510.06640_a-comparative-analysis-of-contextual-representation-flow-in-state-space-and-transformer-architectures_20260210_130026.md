---
ver: rpa2
title: A Comparative Analysis of Contextual Representation Flow in State-Space and
  Transformer Architectures
arxiv_id: '2510.06640'
source_url: https://arxiv.org/abs/2510.06640
tags:
- layers
- ssms
- layer
- mamba
- tbms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first unified, token- and layer-wise analysis
  of representation propagation in State Space Models (SSMs) and Transformer-Based
  Models (TBMs). Using centered kernel alignment, variance-based metrics, and probing,
  it characterizes how contextual representations evolve within and across layers.
---

# A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures

## Quick Facts
- arXiv ID: 2510.06640
- Source URL: https://arxiv.org/abs/2510.06640
- Reference count: 40
- Primary result: SSMs preserve token uniqueness early while TBMs homogenize early; both show intermediate layers outperform final layers on probing tasks.

## Executive Summary
This paper presents the first unified, token- and layer-wise analysis of representation propagation in State Space Models (SSMs) and Transformer-Based Models (TBMs). Using centered kernel alignment, variance-based metrics, and probing, it characterizes how contextual representations evolve within and across layers. The key findings reveal opposing oversmoothing trajectories: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Intermediate layers from both architectures outperform final layers across tasks, and SSMs exhibit more stable representation propagation than TBMs under practical assumptions.

## Method Summary
The study compares SSMs (Mamba, Mamba2) and TBMs (GPT-Neo, Pythia) pre-trained on the Pile dataset. It analyzes representation flow using cosine similarity metrics (adjacent-layer and inter-token), centered kernel alignment (CKA), and variance-based metrics (Vloc, Vglob) across model layers. Linear probing experiments train task-specific classifiers on frozen representations for MDQA and KVPR tasks. Random initialization experiments isolate architectural effects from training dynamics. All models use final token representations h_T^(l) for analysis, with probing trained for 150 epochs using Adam optimizer.

## Key Results
- TBMs show rapid early oversmoothing with inter-token cosine similarity reaching ~90%, while SSMs maintain lower similarity (~50%) through most layers
- Random initialization reveals TBMs' oversmoothing is intrinsic to architecture (high similarity regardless of initialization), while SSMs' is training-dependent (near-zero similarity at random init)
- Intermediate layers outperform final layers in probing accuracy, with up to 26% accuracy drops in TBMs and 20.6% in SSMs from peak to final layer
- SSMs exhibit more stable representation propagation with lower variance metrics (Vloc/Vglob) compared to TBMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TBMs and SSMs exhibit opposite oversmoothing trajectories across layers—TBMs homogenize tokens early then recover diversity late, while SSMs preserve early uniqueness but converge to homogenization deeper.
- **Mechanism:** In TBMs, global self-attention + LayerNorm causes rapid token alignment in early layers (inter-token cosine similarity rises to ~90%); late-layer task-specific processing then re-differentiates tokens. In SSMs, selective state recurrence preserves local token distinctions longer (similarity ~50%), but sequential state compression accumulates homogenization in deeper layers.
- **Core assumption:** Cosine similarity and CKA accurately proxy token distinctiveness and layer-wise representational shifts in trained models.
- **Evidence anchors:**
  - [abstract] "TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper."
  - [section 3.2.2, Figure 2] Shows TBMs maintain ~90% inter-token similarity across most layers, dropping to ~60% near final layer; SSMs stay lower (~50%) through most network.
  - [corpus] Weak direct corpus support; corpus papers focus on hybrid architectures and task performance rather than layer-wise representational dynamics.
- **Break condition:** If normalization layers are removed or attention mixing is localized, early homogenization in TBMs should reduce significantly (partially confirmed in ablation, Figure 6).

### Mechanism 2
- **Claim:** Oversmoothing in TBMs is an intrinsic architectural property, whereas in SSMs it emerges primarily from training dynamics.
- **Mechanism:** Random-initialization experiments show TBMs achieve high inter-token similarity (~80-100%) regardless of initialization scheme, due to global attention mixing all tokens with normalized outputs. SSMs at random initialization maintain near-zero similarity; trained SSMs show learned homogenization patterns.
- **Core assumption:** Random-initialization behavior isolates architectural priors from learned dynamics.
- **Evidence anchors:**
  - [abstract] "oversmoothing in TBMs stems from architectural design (global mixing + normalization), whereas in SSMs it arises mainly from training dynamics."
  - [section 3.2.3, Figure 3] TBMs show consistently high token similarity for pretrained and all random initializations; SSMs show near-zero similarity at random init but higher for pretrained.
  - [corpus] No direct corpus evidence on this specific architectural vs. training distinction.
- **Break condition:** If SSMs are trained with explicit diversity-preserving regularization (e.g., contrastive losses on token representations), late-stage homogenization should reduce without architectural changes.

### Mechanism 3
- **Claim:** Intermediate layers in both architectures contain more linearly accessible task-relevant information than final layers, with probing accuracy drops up to 26% in TBMs and 20.6% in SSMs from peak to final layer.
- **Mechanism:** Early-to-middle layers encode task-relevant features before final-layer abstraction or oversmoothing degrades accessibility. SSMs show smaller final-layer gaps due to more gradual representation evolution (lower Vloc/Vglob variance).
- **Core assumption:** Linear probe accuracy reflects information accessibility rather than model's full capacity; downstream task heads may extract more via non-linear projections.
- **Evidence anchors:**
  - [abstract] "Intermediate layers from both architectures outperform final layers across tasks"
  - [section 3.4.1, Table 1] GPT-Neo-2.7B peaks around Layer 10 with 25.3% drop to final layer; Mamba2-2.7B peaks Layers 4-14 with up to 20.6% drop.
  - [corpus] Skean et al. (2025, cited in intro) showed intermediate layers outperform final layers—partial independent confirmation.
- **Break condition:** If task-specific fine-tuning heads are attached at intermediate layers and trained end-to-end, final-layer reliance may shift; oversmoothing regularization at late layers could close the gap.

## Foundational Learning

- **Concept:** Centered Kernel Alignment (CKA) for representational similarity
  - **Why needed here:** Core metric for measuring layer-wise representation evolution; interprets whether SSM/TBM layers form similar or divergent representational manifolds.
  - **Quick check question:** If two layers have CKA=0.9, are their token representations nearly identical in direction, magnitude, or both? (Answer: Direction—CKA is rotation/scale invariant.)

- **Concept:** Oversmoothing in sequence models
  - **Why needed here:** The central phenomenon being compared; understanding why tokens converge to similar representations explains long-context failures.
  - **Quick check question:** In an oversmoothed layer with inter-token cosine similarity of 0.95, would you expect unique positional information to be recoverable? (Answer: Unlikely—high similarity implies collapse of distinguishing features.)

- **Concept:** Uniform contractivity in SSMs (‖Āt‖ ≤ ρ < 1)
  - **Why needed here:** Theoretical constraint enabling variance bounds for SSMs; explains why SSM representations stabilize more than TBMs.
  - **Quick check question:** If an SSM has ρ=0.8, what happens to the contribution of input from 10 steps ago after passing through 10 state transitions? (Answer: Approximately multiplied by 0.8^10 ≈ 0.1, nearly vanishing.)

## Architecture Onboarding

- **Component map:**
  TBM block: Input → LayerNorm → Multi-head Self-Attention → Add residual → LayerNorm → FFN → Add residual → Output
  SSM block (Mamba): Input → Linear projection → Causal Conv1D → SiLU activation → Selective SSM (S6, state recurrence with data-dependent Δ, B, C) → Gating (SiLU) → Linear → Add residual → Output

- **Critical path:**
  1. Embedding layer produces initial token representations h^(0)
  2. For each layer l: apply block-specific transformation (attention or SSM recurrence)
  3. Residual connections preserve information; normalization controls scale
  4. Final token h_T^(L) used for probing/prediction

- **Design tradeoffs:**
  - **TBM:** Global context access (any token can attend to any other) vs. O(n²) complexity and early homogenization
  - **SSM:** O(n) complexity and early token diversity vs. recency bias and late-stage homogenization
  - **Hybrid (suggested):** SSM blocks early for diversity preservation + TBM blocks late for global re-mixing

- **Failure signatures:**
  - TBM: High early inter-token similarity (>80%) indicates oversmoothing before task differentiation
  - SSM: Late-layer similarity spike (>70% in final layers) indicates state capacity saturation
  - Both: Probing accuracy declining >15% from intermediate peak to final layer signals underutilized capacity

- **First 3 experiments:**
  1. **Layer-wise probe baseline:** Train linear probes on each layer for MDQA/KVPR tasks; identify peak layer and final-layer gap for your model.
  2. **Inter-token similarity tracking:** Compute inter-token cosine similarity at each layer; confirm whether TBM shows early homogenization and SSM shows late convergence.
  3. **Ablation on normalization:** For TBMs, disable or reset LayerNorm affine parameters; observe impact on early oversmoothing patterns.

## Open Questions the Paper Calls Out

- **Question:** How should hybrid architectures that combine early SSM blocks (for token diversity preservation) with late attention blocks (for global reconfiguration) be optimally designed and evaluated?
  - **Basis in paper:** [explicit] "A natural next step is to implement and systematically evaluate such hybrids under controlled long-context benchmarks to validate these design choices."
  - **Why unresolved:** The paper provides theoretical motivation for hybrids but does not implement or test any hybrid architecture; design choices (block ratios, transition points, integration methods) remain unspecified.
  - **What evidence would resolve it:** Empirical evaluation of hybrid models on long-context benchmarks, with ablation studies on where to place SSM vs. attention blocks and how to transition between them.

- **Question:** Can Vloc, Vglob, and inter-token similarity metrics be integrated into training loops as adaptive regularizers to detect and counteract oversmoothing in real-time?
  - **Basis in paper:** [explicit] "Integrating our similarity measures into the training loop may enable adaptive regularization schemes that detect and counteract oversmoothing in real time."
  - **Why unresolved:** The metrics are currently post-hoc diagnostics; no experiments explore using them as loss components or regularization signals during training.
  - **What evidence would resolve it:** Training runs with regularizers based on these metrics, showing reduced oversmoothing and improved long-context task performance compared to baseline training.

- **Question:** How do model scale, layer count, and state dimensionality interact with representation collapse patterns across SSMs and TBMs?
  - **Basis in paper:** [explicit] "Future research could investigate scaling laws under these new diagnostics, asking how model size, layers, and state dimensionality interact with representation collapse across architectures."
  - **Why unresolved:** The paper tests limited scale variations (primarily ~2.7B models) and does not systematically vary these factors to establish scaling relationships.
  - **What evidence would resolve it:** Systematic experiments varying model size, depth, and state dimensions across both architectures, with correlation analysis between these factors and oversmoothing onset.

## Limitations

- The study relies on centered kernel alignment (CKA) as a primary metric for representational similarity, which may not capture all relevant aspects of representation space geometry or task-relevant information. CKA's invariance to rotation and scaling could mask important representational differences.
- The analysis focuses on final token representations (h_T^(l)), potentially missing important dynamics in intermediate token positions or full sequence representations. This may limit generalizability to tasks requiring early token information.
- While the paper claims SSMs preserve early token diversity due to architectural properties, the theoretical analysis assumes uniform contractivity conditions that may not hold in all trained SSM variants, particularly those with learned parameters or gating mechanisms.
- The random initialization experiments provide insight into architectural priors but don't fully account for pretraining effects on TBMs, where attention patterns and layer interactions become highly specialized beyond simple normalization effects.

## Confidence

- **High confidence:** The observation that intermediate layers outperform final layers in linear probing accuracy is well-supported by multiple models and tasks. The empirical finding that TBMs show early homogenization while SSMs show late homogenization is consistently observed across metrics.
- **Medium confidence:** The claim that TBMs' oversmoothing is intrinsic to architecture while SSMs' is training-dependent is supported by random initialization experiments but would benefit from more systematic architectural ablations.
- **Medium confidence:** The theoretical variance bounds for SSMs provide useful intuition but rely on idealized assumptions that may not fully capture practical implementations with selective mechanisms.

## Next Checks

1. **Ablation study on normalization:** Remove or freeze LayerNorm parameters in TBMs and retrain to isolate the contribution of normalization versus attention mixing to early oversmoothing. Measure changes in inter-token similarity and probing performance.

2. **Full-sequence analysis:** Extend the representation flow analysis beyond final token h_T^(l) to include intermediate token positions and full sequence metrics. This would validate whether the observed patterns generalize across all token positions or are specific to final-token analysis.

3. **Contrastive regularization test:** Train SSMs with explicit diversity-preserving objectives (e.g., token-level contrastive loss) to empirically test whether training dynamics alone drive late-stage homogenization, as claimed. Measure whether this prevents the late-layer similarity spike without architectural changes.