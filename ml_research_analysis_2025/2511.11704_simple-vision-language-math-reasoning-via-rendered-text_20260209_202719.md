---
ver: rpa2
title: Simple Vision-Language Math Reasoning via Rendered Text
arxiv_id: '2511.11704'
source_url: https://arxiv.org/abs/2511.11704
tags:
- reasoning
- arxiv
- zhang
- preprint
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a lightweight pipeline for training vision-language
  models to solve mathematical problems by rendering LaTeX equations into images and
  pairing them with structured chain-of-thought (CoT) prompts. This approach enables
  compact multimodal architectures to achieve state-of-the-art reasoning accuracy
  on mathematical benchmarks such as MathVista, MathVerse, and WeMath.
---

# Simple Vision-Language Math Reasoning via Rendered Text

## Quick Facts
- arXiv ID: 2511.11704
- Source URL: https://arxiv.org/abs/2511.11704
- Authors: Matvey Skripkin; Elizaveta Goncharova; Andrey Kuznetsov
- Reference count: 13
- Primary result: Rendering LaTeX equations into images with chain-of-thought prompts enables compact vision-language models to achieve state-of-the-art math reasoning accuracy

## Executive Summary
This paper proposes a lightweight pipeline that converts text-only mathematical problems into rendered images with chain-of-thought (CoT) supervision, enabling compact multimodal architectures to achieve state-of-the-art reasoning accuracy on mathematical benchmarks. The approach combines multiple complementary vision encoders (InternViT, Texify, ConvNeXt) with a language model through learned adapters, achieving gains on tasks like MMMU, ChartQA, and DocVQA of up to 20%. Systematic ablation studies demonstrate that rendering fidelity and prompt design are the primary drivers of performance, with the method consistently matching or surpassing both open-source and proprietary math-focused vision-language solvers while preserving broad general-domain competence.

## Method Summary
The authors convert text-only math problems into rendered images at multiple resolutions, pairing them with structured chain-of-thought prompts for training. They employ three complementary vision encoders (InternViT for global semantics, Texify for LaTeX-specific layout, ConvNeXt for fine-grained detail) whose outputs are projected into the LLM embedding space through learned adapters and fused either at sequence-level or feature-level. The training pipeline involves adapter-only pretraining on captioning/OCR corpora, followed by supervised fine-tuning with a frozen LLM, and optionally joint fine-tuning with unfrozen encoders. The approach systematically improves mathematical reasoning while maintaining general-domain capabilities.

## Key Results
- Rendering LaTeX equations into images with CoT supervision enables compact multimodal models to achieve state-of-the-art reasoning accuracy on MathVista, MathVerse, and WeMath
- Multi-encoder fusion combining InternViT, Texify, and ConvNeXt yields more robust mathematical reasoning than single-encoder approaches
- Structured prompt design significantly affects reasoning performance, with expert-style prompts positioned after questions yielding measurable gains
- The method achieves up to 20% gains on general-domain tasks like MMMU, ChartQA, and DocVQA while maintaining math reasoning superiority

## Why This Works (Mechanism)

### Mechanism 1: Rendered Text-to-Vision Augmentation
Converting text-only math problems into rendered images with chain-of-thought supervision enables compact multimodal models to achieve reasoning performance comparable to larger, specialized systems. LaTeX-encoded equations are rasterized into images at multiple resolutions, creating a visual modality that forces the model to develop OCR-like parsing skills alongside symbolic reasoning. This transforms abstract notation into spatially-structured visual patterns that vision encoders can process.

### Mechanism 2: Multi-Encoder Fusion for Complementary Visual Features
Combining specialized vision encoders capturing different spatial scales and feature types yields more robust mathematical reasoning than single-encoder approaches. InternViT captures global image semantics (448×448), Texify extracts LaTeX-specific symbol layout (420×420), and ConvNeXt provides fine-grained detail sensitivity (1024×1024). Their outputs are projected into the LLM embedding space through learned adapters and fused at either sequence-level or feature-level.

### Mechanism 3: Structured Prompt Design for Eliciting Reasoning
Prompt structure and positioning relative to image tokens systematically affects reasoning performance, with expert-style prompts yielding measurable gains. Prompts that explicitly instruct step-by-step analysis guide the model toward CoT-style generation. Positioning the suffix after the question consistently outperforms alternatives.

## Foundational Learning

- **Vision-Language Model Fusion**: Why needed - The architecture combines multiple pre-trained vision encoders with a language model via learned projection adapters; understanding how visual features map to token embeddings is essential for debugging fusion failures. Quick check - Can you explain how sequence-level fusion differs from feature-level fusion in terms of the attention patterns they enable?

- **Chain-of-Thought Supervision**: Why needed - The training data pairs rendered images with step-by-step reasoning traces; understanding CoT helps interpret why models improve on multi-step problems but may degrade on simple direct-answer tasks. Quick check - What happens if CoT-style outputs are evaluated with exact-match metrics without post-processing?

- **LaTeX Rendering and Mathematical Notation**: Why needed - The pipeline assumes LaTeX source is available and can be rendered; understanding the rendering process helps diagnose cases where visual fidelity drops. Quick check - What aspects of mathematical notation might be lost or distorted when rendering LaTeX at low resolution?

## Architecture Onboarding

- **Component map**: Problem text + LaTeX → rendered image (multiple resolutions) → Vision encoders (InternViT, Texify, ConvNeXt) → Adapters (linear projectors) → Fusion (sequence-level or feature-level) → LLM backbone (Qwen2.5) → Training stages (adapter pretraining → SFT → joint fine-tuning)

- **Critical path**: 1) Render Open-R1 text-only problems into images at target resolutions, 2) Pre-train adapters on captioning/OCR corpora (TextCaps, ShareGPT4V, LLaVAR), 3) Fine-tune on SFT mix (Open-R1-Rendered + domain datasets), 4) Evaluate on MathVista, MathVerse, WeMath, and general-domain benchmarks

- **Design tradeoffs**: InternViT+Texify achieves better DynaMath but lower WeMath strict-match; InternViT+ConvNeXt selected for final model due to better WeMath strict-match and DocVQA/ChartQA robustness. Frozen vs. unfrozen encoders: unfrozen encoders improve performance but increase training cost and overfitting risk

- **Failure signatures**: Over-reasoning on simple tasks (model produces CoT outputs when short answers expected), strict-match degradation (performance drops from S1→S3 problems), text-only bias (models trained without rendering underperform on visual composition tasks)

- **First 3 experiments**: 1) Replicate dataset ablation with small backbone to confirm rendering benefit over text-only + TexTeller, 2) Compare InternViT+Texify vs. InternViT+ConvNeXt on specific data mix to validate fusion strategy, 3) Ablate prompt variants to confirm prompt sensitivity before larger-scale training

## Open Questions the Paper Calls Out
- Can extending the rendering pipeline to include richer diagrammatic inputs and real-world figures improve performance beyond the current text-only rendering approach?
- Do adaptive or multi-scale rendering policies conditioned on expression density yield better reasoning accuracy than fixed rendering settings?
- Does integrating synthesized graphical data into the training set improve the model's robustness and usability for mathematical reasoning?
- Can a single fusion strategy be optimized to avoid the performance trade-off between DynaMath and general-domain tasks?

## Limitations
- The core claim that rendering LaTeX into images drives superior math reasoning rests on dataset augmentation rather than architectural novelty
- Cross-dataset evaluation reveals concerning degradation from S1→S3 problems (69.1→50.3 WeMath strict-match), suggesting multi-step reasoning introduces compounding errors
- Multi-encoder fusion performance differences are demonstrated but not fully explained - why ConvNeXt+InternViT excels on WeMath strict-match but InternViT+Texify better on DynaMath remains unclear

## Confidence
- **High confidence**: Rendering LaTeX equations into images combined with CoT supervision consistently improves math reasoning performance across multiple benchmarks
- **Medium confidence**: Multi-encoder fusion provides complementary visual features that enhance mathematical reasoning, though specific advantages are not fully characterized
- **Medium confidence**: Prompt design significantly affects reasoning performance, but effects may represent prompt overfitting rather than robust reasoning capability transfer

## Next Checks
1. Conduct controlled experiment isolating effect of visual rendering versus text-only augmentation by matching training sample counts and evaluating on both CoT and direct-answer formats
2. Perform error analysis on S1→S3 performance degradation to identify whether multi-step reasoning failures stem from attention drift, symbol confusion, or CoT generation instability
3. Validate prompt sensitivity across diverse benchmark formats by testing suffix variants on non-CoT datasets to confirm whether gains generalize or overfit to CoT-style evaluation