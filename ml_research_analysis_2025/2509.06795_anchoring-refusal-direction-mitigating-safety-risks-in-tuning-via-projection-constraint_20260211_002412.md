---
ver: rpa2
title: 'Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection
  Constraint'
arxiv_id: '2509.06795'
source_url: https://arxiv.org/abs/2509.06795
tags:
- safety
- performance
- arxiv
- drift
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies refusal direction drift as a safety risk\
  \ factor in instruction fine-tuning (IFT) of large language models (LLMs). The authors\
  \ observe that the refusal direction\u2014critical for maintaining safety\u2014\
  tends to drift during training, leading to increased safety risks."
---

# Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint

## Quick Facts
- arXiv ID: 2509.06795
- Source URL: https://arxiv.org/abs/2509.06795
- Reference count: 40
- Primary result: ProCon wu saf e method significantly reduces safety risks in LLM instruction fine-tuning while preserving task performance.

## Executive Summary
This paper identifies refusal direction drift as a critical safety risk factor in instruction fine-tuning of large language models. The authors observe that the refusal direction—a vector representing the difference between benign and malicious activations—tends to drift during training, degrading the model's safety alignment. To address this, they propose ProCon, a projection-constraint method that regularizes hidden states' projections onto the refusal direction to stabilize it. The enhanced ProCon wu saf e method introduces a warm-up strategy with early strong constraints and broadens the data distribution with safety-oriented samples. Experiments across multiple LLMs (LLaMA2, LLaMA3, Qwen2) and scenarios (benign and attack IFT) show that ProCon wu saf e significantly reduces harmfulness scores and attack success rates while preserving task performance gains.

## Method Summary
The method identifies the refusal direction (r-direction) by computing the difference-in-means between benign and malicious instruction activations at each layer. During instruction fine-tuning, a projection-constrained loss term regularizes hidden state projections onto this direction, anchoring them to their initial values. The ProCon wu saf e variant introduces a warm-up strategy where strong constraints (α=1-2) are applied in early epochs to counter sharp initial drift, followed by unconstrained training. Additionally, 1,000 safety-oriented samples are added to broaden the data distribution and strengthen constraint signals. The method is implemented using LoRA fine-tuning with r=8, α=16 on Q/K/V/O modules, learning rate 2e-4, and 10 epochs.

## Key Results
- ProCon wu saf e reduces harmfulness scores (HS) and attack success rates (ASR) compared to baseline IFT across multiple models and scenarios
- The method preserves task performance improvements from instruction fine-tuning while mitigating safety risks
- Qwen2 requires approximately four times more warm-up epochs (16) than LLaMA2 (4) due to differences in RLHF optimization
- Adding 1,000 safety-oriented samples further enhances constraint effectiveness and reduces r-direction drift

## Why This Works (Mechanism)

### Mechanism 1: Refusal Direction Drift as Safety Degradation Source
Safety risks in IFT arise because the r-direction drifts during training, degrading refusal behavior. During IFT, hidden states change, causing the r-direction—identified via difference-in-means of benign vs. malicious activations—to shift. This drift is most pronounced early and in deeper layers, weakening refusal behavior. Constraining projections onto the r-direction stabilizes it, reducing safety risks.

### Mechanism 2: Projection Constraint Stabilizes R-Direction
Adding a projection-constrained loss term regularizes hidden state projections onto the r-direction, reducing drift and associated safety risks without harming task performance (when tuned properly). The loss term penalizes deviations in projection magnitude from initial values, anchored to the pre-training r-direction. Combined with cross-entropy loss, this constrains how far hidden states can move along the r-direction during updates.

### Mechanism 3: Warm-Up and Data Broadening Enhance Constraint Effectiveness
A warm-up strategy (strong early constraints) and adding safety-oriented data broaden the data distribution, strengthening constraint signals and further reducing drift while preserving task gains. Early training has sharp drift due to high loss; strong initial constraints counter this. After warm-up, switching to unconstrained IFT allows task optimization. Safety-oriented samples provide diverse activations, increasing Fisher information along the r-direction.

## Foundational Learning

- **Concept: Difference-in-Means for Direction Identification**
  - Why needed: To extract the refusal direction (r-direction) from hidden states, enabling drift measurement and constraint design.
  - Quick check: Given benign and malicious instruction activations, how would you compute the r-direction vector at layer l?

- **Concept: Projection Constraint in Loss Functions**
  - Why needed: To understand how L_ProCon regularizes hidden states and interacts with task loss during training.
  - Quick check: If a hidden state's projection onto the r-direction deviates from its initial value, how does the gradient from L_ProCon affect parameter updates?

- **Concept: Cosine Similarity for Drift Measurement**
  - Why needed: To quantify r-direction drift across training checkpoints and correlate it with safety risks.
  - Quick check: If cosine similarity between r_vanilla and r_IFT drops to 0.5, what does this imply about drift magnitude and potential safety impact?

## Architecture Onboarding

- **Component map**: R-direction Extractor -> Projection Calculator -> Constraint Loss Module -> Warm-Up Scheduler -> Data Loader
- **Critical path**: 1) Identify r-direction from base model using paired benign/malicious data. 2) Initialize and store projection magnitudes for IFT training data. 3) During training: compute projections, accumulate L_ProCon, apply warm-up schedule. 4) Post-training: evaluate safety and task performance; analyze drift via cosine similarity.
- **Design tradeoffs**: Constraint level (α) requires tuning per model; higher α reduces safety risks but may harm task performance. Warm-up duration is model-dependent (e.g., 4 for LLaMA2, 16 for Qwen2). Safety data amount (1k used) might over-constrain or cause over-refusal.
- **Failure signatures**: Safety scores remain high despite ProCon (possible break condition or insufficient α/warm-up), task performance drops significantly (α too high or warm-up too long), drift analysis shows cosθ near 1 but safety poor (r-direction may not fully capture refusal).
- **First 3 experiments**: 1) Baseline drift analysis: Train IFT on LLaMA2 with no constraints; measure r-direction drift vs. epoch and correlate with safety degradation. 2) ProCon ablation (α): Sweep α ∈ {0.1, 0.2, 1, 2} on LLaMA2 under Benign IFT; plot safety vs. task accuracy. 3) Warm-up + safety data validation: Compare ProCon_s vs. ProCon_wu vs. ProCon_wu^safe on LLaMA2 and Qwen2 under Attack IFT.

## Open Questions the Paper Calls Out

### Open Question 1
What specific factors cause different LLM architectures (e.g., LLaMA vs. Qwen) to require vastly different warm-up durations for effective refusal direction stabilization? The authors observe LLaMA requires ~4 warm-up epochs while Qwen2 requires ~16, hypothesizing a link to RLHF optimization extent but stating further verification is not feasible due to black-box nature.

### Open Question 2
Is the "sharp drift" observed in early training directly and causally linked to initial training loss gradient magnitudes? The paper attributes early sharp drift to high training loss but does not perform controlled experiments to rigorously prove gradient magnitude is the specific cause.

### Open Question 3
Does anchoring hidden states to the initial refusal direction constrain the model's ability to learn nuanced safety distinctions as it acquires new task knowledge? The method assumes the initial refusal direction is correct, but as the model learns complex reasoning, the optimal safety representation might ideally shift to accommodate new knowledge.

## Limitations
- Method's effectiveness depends on access to high-quality benign-malicious instruction pairs for r-direction extraction
- Single-direction assumption may not address all facets of safety in LLMs
- Model-specific tuning requirements for α and warm-up duration introduce complexity

## Confidence

**High confidence**: The core mechanism that r-direction drift correlates with safety degradation is well-supported by experimental evidence across multiple model families and training scenarios.

**Medium confidence**: The warm-up strategy and safety data broadening provide additional benefits, but these improvements are more incremental and model-dependent.

**Low confidence**: The claim that ProCon wu saf e is a universal solution for all safety risks in IFT is overstated, as the method addresses a specific failure mode but may not mitigate other safety vulnerabilities.

## Next Checks

1. **Cross-model generalization test**: Apply ProCon wu saf e to models outside the LLaMA/Qwen families (e.g., Mistral, Gemma) and measure safety drift reduction and performance preservation to validate generalizability.

2. **Long-term stability evaluation**: Track r-direction drift and safety metrics across extended training schedules (50+ epochs) to assess whether ProCon wu saf e maintains effectiveness over longer optimization periods.

3. **Alternative direction identification**: Compare ProCon's difference-in-means approach with other direction identification methods (e.g., PCA, supervised learning) to evaluate whether the choice of method affects safety outcomes and constraint effectiveness.