---
ver: rpa2
title: 'InPars+: Supercharging Synthetic Data Generation for Information Retrieval
  Systems'
arxiv_id: '2508.13930'
source_url: https://arxiv.org/abs/2508.13930
tags:
- inpars
- query
- queries
- data
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors validate the reproducibility claims of the InPars Toolkit,
  demonstrating that it provides an end-to-end pipeline for synthetic query generation
  using LLMs, supports reproduction of InPars-V1, InPars-V2, and partially Promptagator,
  and enables plug-and-play integration of alternative LLMs. They reproduce results
  on the SciFact dataset using Llama 3.1 8B, showing comparable performance to the
  original InPars-V2 pipeline.
---

# InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems

## Quick Facts
- arXiv ID: 2508.13930
- Source URL: https://arxiv.org/abs/2508.13930
- Reference count: 2
- Primary result: Demonstrates synthetic query generation pipeline with CPO fine-tuning and DSPy CoT prompting improves IR reranker performance

## Executive Summary
The authors validate and extend the InPars Toolkit for synthetic query generation, demonstrating an end-to-end pipeline that supports reproduction of InPars-V1, InPars-V2, and Promptagator methods. They introduce two key extensions: Contrastive Preference Optimization (CPO) fine-tuning of the generator LLM to improve query quality, and dynamic Chain-of-Thought (CoT) prompts using DSPy to replace static templates. Both extensions reduce the need for aggressive filtering while improving retrieval performance on the SciFact dataset. The study shows that larger models don't always yield better results, and that dynamic prompts consistently enhance performance.

## Method Summary
The paper presents a pipeline for synthetic query generation using LLMs to train neural information retrieval rerankers. The core approach involves prompting a generator LLM (Llama 3.1 8B) with documents to produce synthetic queries, which are then scored by a reranker (MonoT5-3B) and filtered to select high-quality training data. Two key extensions are introduced: CPO fine-tuning that trains the generator to prefer high-scoring queries based on a hybrid relevance function, and DSPy CoT prompting that structures the generation process through reasoning steps. The pipeline is validated on the SciFact dataset with nDCG@10 as the primary metric.

## Key Results
- Reproduces InPars-V2 pipeline on SciFact using Llama 3.1 8B, achieving comparable performance to original (nDCG@10 ~0.77)
- CPO fine-tuning shifts quality distribution of generated queries upward, reducing dependency on aggressive filtering
- DSPy CoT prompts consistently improve performance across datasets while reducing filtering requirements
- Larger models (70B vs 8B) do not always yield better results when using dynamic prompts
- All code, models, and synthetic datasets are publicly released

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning the generator LLM via Contrastive Preference Optimization (CPO) shifts the quality distribution of generated queries upward, reducing the dependency on aggressive filtering.
- **Mechanism:** CPO trains a student model to prefer queries scored highly by a hybrid relevance function over lower-scoring ones. Instead of standard supervised fine-tuning (which mimics a reference), CPO minimizes a loss function based on preference pairs, forcing the model to internalize the characteristics of a "relevant" query.
- **Core assumption:** The hybrid scoring function ($s_{enc} + s_{BM25}$) serves as a reliable proxy for human relevance judgment, and the student model can generalize this preference from MS MARCO to unseen target domains.
- **Evidence anchors:**
  - [abstract]: "fine-tuning the generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries"
  - [section 4.2.2]: "We observe that a trained student was able to learn to generate generally higher scoring queries (Fig. 3), pushing the score distribution upwards."
  - [corpus]: [Weak/General] Corpus neighbor "Reinforced Information Retrieval" supports generation-augmented methods but does not validate this specific CPO mechanism.
- **Break condition:** If the student model overfits to the specific lexical patterns of the MS MARCO training data, it may generate queries with high lexical overlap but low semantic relevance (false positives), degrading reranker performance.

### Mechanism 2
- **Claim:** Replacing static prompt templates with dynamic Chain-of-Thought (CoT) optimized prompts (via DSPy) improves query relevance by enforcing structured reasoning.
- **Mechanism:** CoT prompting forces the LLM to break down the document content into logical steps before generating the query. This acts as a compute-time enhancement, allowing the model to disambiguate core topics from peripheral noise before outputting the final query token.
- **Core assumption:** The LLM possesses sufficient intrinsic reasoning capability to decompose the document accurately, and this decomposition leads to queries that better align with the document's central claims.
- **Evidence anchors:**
  - [abstract]: "...replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework... reduce the need for aggressive filtering."
  - [section 4.2.3]: "use of dynamic prompt optimization combined with Chain-of-Thought (CoT) consistently improves performance across datasets."
  - [corpus]: [General] "Can Synthetic Query Rewrites Capture User Intent Better than Humans..." suggests synthetic rewrites are viable, supporting the broader approach.
- **Break condition:** If the document structure is highly irregular or the model hallucinates during the reasoning steps, the CoT may lead the model astray, resulting in queries that focus on extrapolated concepts rather than actual content.

### Mechanism 3
- **Claim:** Using a hybrid scoring function combining bi-encoder similarity and BM25 allows for more robust preference pair generation than either method alone.
- **Mechanism:** CPO requires distinct "preferred" and "dispreferred" examples. Relying solely on dense embeddings ($s_{enc}$) can fail on exact lexical matches, while BM25 fails on semantics. The paper uses a weighted combination ($0.5 \cdot s_{enc} + 0.5 \cdot s_{BM25}$) to select the winning query for the CPO loss calculation.
- **Core assumption:** The variance between the teacher, student, and reference queries is sufficient to create meaningful preference pairs (i.e., they are not all identical in quality).
- **Evidence anchors:**
  - [section 3.2.3]: "The final score $s$ is calculated as: $s(\text{doc}, \text{query}) = 0.5 \cdot s_{enc}(\text{doc}, \text{query}) + 0.5 \cdot s_{BM25}(\text{doc}, \text{query})$"
  - [section 5.1]: Describes mitigation for "false-positives" where the model repeats the document, validating the difficulty of scoring.
  - [corpus]: [Weak] No specific corpus evidence validates this specific hybrid scoring mechanism for CPO.
- **Break condition:** If the scoring margins are too tight ($L < s < H$), the system filters out potentially useful data; if too loose, degenerate outputs (e.g., copying the document) may be selected as "preferred," poisoning the CPO training.

## Foundational Learning

- **Concept: Neural Information Retrieval (NIR) & Reranking**
  - **Why needed here:** The paper's goal is to train a reranker (MonoT5) using synthetic data. You must understand that a reranker scores query-document pairs (cross-encoding) rather than generating embeddings, and it requires labeled relevance data which this pipeline synthesizes.
  - **Quick check question:** Does the pipeline fine-tune the generator to produce better *embeddings* or the reranker to produce better *relevance scores*?

- **Concept: Prompt Engineering (Few-Shot vs. CoT)**
  - **Why needed here:** The paper contrasts static "Vanilla" few-shot prompts with dynamic Chain-of-Thought (CoT) prompts. Understanding how in-context examples guide LLMs is necessary to grasp why DSPy improves the generator.
  - **Quick check question:** How does adding reasoning steps (CoT) before the final query output theoretically change the quality of the generation compared to standard few-shot prompting?

- **Concept: Preference Optimization (DPO/CPO)**
  - **Why needed here:** The paper moves beyond Supervised Fine-Tuning (SFT) to Contrastive Preference Optimization (CPO). You need to understand that CPO trains the model on *relative* quality (better vs. worse) rather than absolute truth (correct vs. incorrect).
  - **Quick check question:** In CPO, does the model learn to copy the "preferred" output exactly, or does it learn to maximize the likelihood gap between preferred and dispreferred outputs?

## Architecture Onboarding

- **Component map:**
  - Generator (LLM) -> Scorer (MonoT5/Hybrid) -> Reranker (MonoT5/MiniLM) -> Target
  - Prompt Builder (Static/DSPy) -> Generator -> Scoring -> Filtering -> Training

- **Critical path:**
  1. **Prompting:** Construct prompt (Static or CoT) for a document.
  2. **Generation:** LLM generates synthetic query.
  3. **Scoring:** Reranker/Hybrid scores the (query, document) pair.
  4. **Filtering:** (Optional) Discard low-scoring queries.
  5. **Training:** Fine-tune the target Reranker on synthetic data OR fine-tune Generator via CPO.

- **Design tradeoffs:**
  - **Static vs. Dynamic Prompts:** Static is faster/cheaper; Dynamic (CoT) yields higher relevance but increases inference latency and token cost.
  - **Filtering Ratio:** High filtering (10%) ensures quality but wastes 90% of compute. CPO/CoT aims to lower this ratio, trading generator training time for generation efficiency.
  - **Model Size:** The paper notes "larger models do not always yield better results" (Sec 4.2.1). An 8B model with CoT can outperform a 70B model with static prompts.

- **Failure signatures:**
  - **Degenerate Repetition:** The CPO model learns to copy the document text to maximize similarity score (Sec 5.1). *Mitigation:* Score thresholds/margin logic.
  - **Negative Transfer:** Filtering CPO-generated data reduces performance (Sec 4.2.2), implying the standard filtering heuristic (MonoT5 scoring) may conflict with the CPO model's distribution.
  - **Topic Drift:** CoT reasoning causes the model to focus on peripheral concepts (Sec 5.3).

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the InPars-V2 pipeline on SciFact using GPT-J and static prompts. Verify top-10k filtering nDCG@10 matches ~0.770.
  2. **Ablation on Filtering:** Generate 25k queries using CoT/Llama 3.1 8B and train the reranker on *all* of them (0% filtering). Compare against the baseline to validate the "reduced filtering" claim.
  3. **CPO Validation:** Fine-tune Llama 3.1 8B on MS MARCO using the Hybrid Scorer. Generate queries for SciFact. Check if the score distribution shifts right (Fig 3) compared to the base model.

## Open Questions the Paper Calls Out

- **Open Question 1:** Is an explicit filtering strategy necessary for CPO-fine-tuned generators, or does it risk removing useful training signal?
  - Basis in paper: [explicit] Section 5.3 states, "To answer then the question whether a filtering strategy is needed, the answer seems not evident as of now."
  - Why unresolved: The authors found that filtering often resulted in worse performance for CPO models, suggesting that the standard InPars filtering heuristic may remove valid training data.
  - What evidence would resolve it: A comprehensive ablation study comparing zero-filtering against standard score-based filtering across a wider variety of BEIR datasets.

- **Open Question 2:** How can triplet datasets for CPO be constructed to effectively prevent degenerate outputs like document copying?
  - Basis in paper: [explicit] Section 5.2 notes, "We plan to investigate better ways to generate training datasets to encourage more appropriate query generation."
  - Why unresolved: The current query evaluation framework sometimes assigns high scores to queries that simply repeat the document text, which are useless for training but selected as "preferred."
  - What evidence would resolve it: The development of a scoring mechanism that penalizes lexical overlap or implements a maximum threshold for similarity to prevent false positives.

- **Open Question 3:** Why does Chain-of-Thought (CoT) prompting degrade retrieval performance on datasets like TREC-Covid compared to zero-shot methods?
  - Basis in paper: [inferred] Section 5.3 hypothesizes that CoT may cause models to focus on "peripheral concepts," while Figure 5 shows CoT underperforming zero-shot on TREC-Covid.
  - Why unresolved: The paper identifies the performance drop but does not provide a detailed error analysis explaining why reasoning steps fail in this specific domain.
  - What evidence would resolve it: A qualitative analysis of CoT-generated queries for TREC-Covid to identify if the reasoning steps drift away from core document topics.

## Limitations
- Critical CPO hyperparameters (learning rate, epochs, $\beta$ values) are not specified in the paper
- DSPy optimizer configuration lacks details on demonstration examples and teleprompter settings
- No systematic comparison of different filtering thresholds for hybrid scorer across multiple datasets
- Claim about larger models not always being better lacks systematic ablation studies across all datasets and extensions

## Confidence
- **High Confidence:** Baseline InPars-V2 reproduction on SciFact using Llama 3.1 8B (nDCG@10 ~0.77) and DSPy CoT prompt extension (consistent performance improvement across datasets)
- **Medium Confidence:** CPO fine-tuning mechanism and its ability to reduce filtering dependency, as paper demonstrates upward shift in score distribution but lacks ablation on filtering thresholds
- **Low Confidence:** Claim about larger models not always being better, as this observation is not systematically validated across all datasets and extensions

## Next Checks
1. **CPO Hyperparameter Sweep:** Run the CPO fine-tuning with varying learning rates (1e-5, 5e-5, 1e-4) and epochs (1-3) to identify the optimal configuration for Llama 3.1 8B on MS MARCO.
2. **DSPy Demonstration Study:** Vary the number of demonstration examples (10, 50, 100) compiled into the DSPy AgentQueryGenerator to measure the impact on CoT prompt quality and query relevance.
3. **Filtering Threshold Analysis:** Systematically test different filtering thresholds (5%, 10%, 20%, 30%) for the hybrid scorer on CPO-generated queries to identify the optimal balance between quality and coverage.