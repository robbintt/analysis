---
ver: rpa2
title: 'Agent S2: A Compositional Generalist-Specialist Framework for Computer Use
  Agents'
arxiv_id: '2504.00906'
source_url: https://arxiv.org/abs/2504.00906
tags:
- agent
- grounding
- step
- click
- computer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Agent S2, a compositional generalist-specialist
  framework for computer use agents that achieves state-of-the-art performance across
  multiple benchmarks. The core innovation is a Mixture-of-Grounding approach that
  delegates UI element localization to specialized experts (visual, textual, and structural)
  while maintaining hierarchical planning.
---

# Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents

## Quick Facts
- arXiv ID: 2504.00906
- Source URL: https://arxiv.org/abs/2504.00906
- Reference count: 40
- Agent S2 achieves 27.0% and 34.5% success rates on OSWorld's 15-step and 50-step evaluations, representing 18.9% and 32.7% relative improvements over baselines

## Executive Summary
Agent S2 introduces a compositional generalist-specialist framework for computer use agents that achieves state-of-the-art performance across multiple benchmarks. The framework employs a Mixture-of-Grounding approach that delegates UI element localization to specialized experts (visual, textual, and structural) while maintaining hierarchical planning. Additionally, Agent S2 introduces Proactive Hierarchical Planning that dynamically updates action plans at multiple temporal scales based on evolving observations. The system demonstrates significant improvements over existing methods, achieving 27.0% and 34.5% success rates on OSWorld's 15-step and 50-step evaluations (18.9% and 32.7% relative improvements over baselines), 29.8% on WindowsAgentArena (52.8% relative improvement), and 54.3% on AndroidWorld (16.5% relative improvement).

## Method Summary
Agent S2 builds upon the Agent S framework with two key innovations: Mixture-of-Grounding (MoG) and Proactive Hierarchical Planning. MoG routes UI localization tasks to specialized experts based on the grounding type—Visual Grounding Expert for screenshot-based coordinate extraction, Textual Grounding Expert for OCR-based span selection, and Structural Grounding Expert for programmatic spreadsheet updates. Proactive Hierarchical Planning enables dynamic replanning after each subgoal completion rather than only after failures, allowing the system to adapt to changing observations while maintaining task coherence. The framework maintains a Manager-Worker decomposition where the Manager handles high-level planning and subgoal decomposition, while the Worker generates atomic actions and routes them to appropriate grounding experts.

## Key Results
- Achieved 27.0% success rate on OSWorld's 15-step evaluation (18.9% relative improvement over baselines)
- Achieved 34.5% success rate on OSWorld's 50-step evaluation (32.7% relative improvement over baselines)
- Demonstrated 29.8% success rate on WindowsAgentArena (52.8% relative improvement)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Routing UI localization tasks to specialized grounding experts reduces grounding errors compared to single-model approaches.
- Mechanism: The Worker module acts as a gating function, routing each action to one of three specialists—Visual Grounding Expert (screenshot + description → coordinates), Textual Grounding Expert (OCR-based span selection), and Structural Grounding Expert (programmatic spreadsheet cell updates). This decomposition allows each expert to optimize for its domain rather than forcing a generalist to handle all grounding types.
- Core assumption: Grounding failures in prior systems stem from cognitive overload on single models; domain-specific specialists will outperform generalists on their respective subtasks.
- Evidence anchors:
  - [abstract] "We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization"
  - [section 4.3] "MoG increases success rate from 27.69% to 30.77% at shorter horizons (15 steps) and, more prominently, from 33.85% to 38.46% at longer horizons (50 steps)"
  - [corpus] Weak direct corroboration; neighbor papers discuss GUI agent challenges but not Mixture-of-Grounding specifically
- Break condition: If routing logic misclassifies grounding type (e.g., sends spreadsheet task to visual expert), errors compound; if specialists have overlapping capabilities without clear boundaries, gating becomes noisy.

### Mechanism 2
- Claim: Proactively replanning after each subgoal completion improves long-horizon task success compared to reactive replanning (only after failures).
- Mechanism: After each subgoal completes, the Manager receives the prior subgoals, latest observation, and original instruction to generate updated subgoals. This allows contextualization with new observations without losing task coherence, reducing susceptibility to UI noise and pop-ups.
- Core assumption: Environmental observations change meaningfully during execution; replanning captures critical information that fixed plans miss.
- Evidence anchors:
  - [section 3.2] "Unlike reactive planning approaches, which only update their plans after failure, proactive planning allows Agent S2 to update its plan after completing every subgoal"
  - [section 4.3] "Proactive Hierarchical Planning...revealing a performance improvement of +4.62% at 15 steps and +6.15% at 50 steps compared to reactive planning"
  - [corpus] Neighbor paper "Building a Stable Planner" addresses planning stability for mobile GUI agents, suggesting planning reliability is a recognized challenge
- Break condition: If observations are noisy or irrelevant, proactive replanning may introduce drift; frequent replanning increases latency.

### Mechanism 3
- Claim: Composing slightly suboptimal generalist and specialist modules can outperform the best monolithic models on complex tasks.
- Mechanism: The framework separates concerns—Manager handles high-level decomposition, Worker generates actions, Experts handle grounding. Each module can be independently optimized or swapped without retraining the entire system.
- Core assumption: Task complexity creates bottlenecks in monolithic models; modular decomposition aligns model capabilities with subtask requirements.
- Evidence anchors:
  - [section 4.2] "Agent S2 with Claude-3.5-Sonnet (new) relatively outperforms Claude Computer Use with Claude-3.7-Sonnet by 58.1% on 15-step...illustrating the advantages of modular, hierarchical frameworks over monolithic generalist modules"
  - [section 4.3] "smaller specialist models, such as UI-TARS-7B-DPO and UGround-V1-7B, can outperform large generalist models like Claude-3.7-Sonnet when employed within a modular framework"
  - [corpus] "VeriGUI" and "MCPWorld" papers emphasize evaluation challenges for long-chain GUI tasks, supporting the need for architectural improvements
- Break condition: If inter-module communication introduces errors (e.g., Worker generates ambiguous descriptions for Experts), composition overhead negates specialist gains.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) routing**
  - Why needed here: MoG is an instance of MoE; understanding gating functions and load balancing helps diagnose routing failures.
  - Quick check question: Can you explain how a softmax gating network selects among experts, and what "load balancing loss" prevents?

- Concept: **Hierarchical Reinforcement Learning (options framework)**
  - Why needed here: The Manager-Worker decomposition mirrors the options framework (high-level policies over low-level primitives).
  - Quick check question: How do semi-Markov decision processes extend MDPs for temporally extended actions?

- Concept: **Visual grounding in multimodal LLMs**
  - Why needed here: The Visual Grounding Expert converts natural language descriptions to pixel coordinates; understanding referring expression localization is critical.
  - Quick check question: What datasets (e.g., RefCOCO, ScreenSpot) are commonly used to evaluate visual grounding, and what metrics apply?

## Architecture Onboarding

- Component map: Instruction → Manager generates subgoals → Worker takes first subgoal → Worker generates action + routes to Expert → Expert returns coordinates → Action executed → Subgoal complete? → Manager replans with new observation

- Critical path: Instruction → Manager generates subgoals → Worker takes first subgoal → Worker generates action + routes to Expert → Expert returns coordinates → Action executed → Subgoal complete? → Manager replans with new observation

- Design tradeoffs:
  - Screenshot-only input vs. accessibility trees: Screenshots are universal but lose semantic structure; Agent S2 trades off structured input for generality
  - Proactive vs. reactive replanning: Proactive improves adaptability but increases compute per step
  - Specialist model size: Smaller specialists (7B) can outperform larger generalists within the framework, reducing inference cost

- Failure signatures:
  - **Grounding misrouting**: Worker sends text-selection task to Visual Expert → imprecise coordinates (check Figure 4 for self-correction example)
  - **Planning drift**: Manager replans too frequently, losing task coherence (check if subgoals reference original instruction)
  - **Knowledge gap**: Worker lacks domain knowledge for specific applications (e.g., GIMP) → interaction failures

- First 3 experiments:
  1. **Ablate MoG**: Run Agent S2 with only Visual Grounding Expert on OSWorld subset; compare to full MoG to quantify specialist contributions
  2. **Proactive vs. Reactive**: Implement reactive baseline (replan only on failure); measure success rate delta at 15 and 50 steps
  3. **Specialist scaling**: Swap Visual Grounding Expert (UI-TARS-72B-DPO → UI-TARS-7B-DPO → Claude-3.7-Sonnet) while holding Manager/Worker constant; plot performance vs. model size

## Open Questions the Paper Calls Out
None

## Limitations
- Claims rely on single benchmarking suite with limited external validation
- 27.0-34.5% success rates remain relatively low for practical deployment
- Framework performance depends heavily on quality of Manager's subgoal decomposition

## Confidence
- **High confidence**: Modular decomposition approach and specialist grounding mechanisms are well-supported by ablation studies showing consistent improvements (MoG: +3.08% at 15 steps, +4.61% at 50 steps; Proactive Planning: +4.62% at 15 steps, +6.15% at 50 steps)
- **Medium confidence**: Superiority over monolithic approaches (58.1% relative improvement) based on internal comparisons that may not account for all confounding factors
- **Medium confidence**: Scalability claims for smaller specialist models supported by performance trends but lack systematic exploration of performance-model size tradeoff

## Next Checks
1. **Cross-benchmark validation**: Evaluate Agent S2 on at least two additional computer use benchmarks not used in training or development to verify generalizability of the performance gains.

2. **Error analysis taxonomy**: Conduct a detailed error analysis categorizing failures by grounding type (visual, textual, structural), planning stage, and task complexity to identify which components most limit performance.

3. **Real-world deployment test**: Deploy Agent S2 on a small set of real user tasks with varying complexity to assess practical usability, focusing on failure modes, recovery capability, and user satisfaction beyond benchmark metrics.