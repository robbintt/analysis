---
ver: rpa2
title: Generative modeling using evolved quantum Boltzmann machines
arxiv_id: '2512.02721'
source_url: https://arxiv.org/abs/2512.02721
tags:
- quantum
- gradient
- boltzmann
- algorithms
- minimax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training quantum Boltzmann\
  \ machines for Born-rule generative modeling. The core method combines two key ingredients:\
  \ the Donsker\u2013Varadhan variational representation of the classical relative\
  \ entropy and the quantum Boltzmann gradient estimator."
---

# Generative modeling using evolved quantum Boltzmann machines

## Quick Facts
- arXiv ID: 2512.02721
- Source URL: https://arxiv.org/abs/2512.02721
- Reference count: 30
- This paper presents four hybrid quantum-classical algorithms for training evolved quantum Boltzmann machines using a minimax optimization framework based on the Donsker-Varadhan variational representation.

## Executive Summary
This paper addresses the challenge of training quantum Boltzmann machines for Born-rule generative modeling by reformulating the problem as a minimax optimization. The approach combines the Donsker-Varadhan variational representation of classical relative entropy with quantum Boltzmann gradient estimators to create efficient hybrid quantum-classical algorithms. The method enables unbiased gradient estimation through sampling, avoiding explicit density ratio estimation, and provides theoretical convergence guarantees for some of the proposed algorithms.

## Method Summary
The method reformulates the relative entropy minimization as a minimax problem using the Donsker-Varadhan formula, creating a tractable lower bound that can be optimized. Four hybrid quantum-classical algorithms are presented: extragradient, two-timescale gradient descent-ascent, follow-the-ridge, and HessianFR. The quantum component uses evolved quantum Boltzmann gradient estimators based on Hadamard test circuits to estimate gradients and Hessians with respect to model parameters. The classical component handles the discriminator neural network and optimization updates.

## Key Results
- Reformulates QBM training as a minimax problem using Donsker-Varadhan variational representation
- Provides four hybrid quantum-classical algorithms with theoretical convergence guarantees for some methods
- Shows optimization problem is nonconvex in parameter vector with bounds on gradient and Hessian norms
- Extends method to other distinguishability measures like Rényi relative quasi-entropy
- Presents quantum gradient estimation techniques using Hadamard test circuits

## Why This Works (Mechanism)

### Mechanism 1: Donsker–Varadhan Variational Reformulation
The DV formula linearizes the relative entropy objective by replacing the log-ratio with a supremum over functions, enabling unbiased gradient estimation through sampling. This bypasses explicit density ratio estimation and makes the problem tractable.

### Mechanism 2: Quantum Boltzmann Gradient Estimator
The gradient formulas are expressed as expectation values involving commutators and channels, which can be estimated using Hadamard-test style circuits. This provides unbiased estimators for the gradient components needed in the minimax objective.

### Mechanism 3: Convergence to Local Minimax Points
First and second-order minimax optimization algorithms can converge to local minimax points of the DV objective. Second-order methods use Hessian blocks to maintain alignment with local geometry defined by the Schur complement.

## Foundational Learning

- **Relative Entropy (KL Divergence)**: Why needed - primary loss function being minimized; Quick check - why is D(p||q) preferred over L2 distance in generative modeling?
- **Variational Representations**: Why needed - reformulates intractable KL divergence into optimization problem; Quick check - how does DV formula convert nonlinear function to linear one?
- **Minimax Optimization & Local Minimax Points**: Why needed - training is a game between generator and discriminator; Quick check - why is converging to local minimum insufficient in minimax setting?

## Architecture Onboarding

- **Component map**: Quantum Ansatz -> Classical Discriminator -> Quantum Gradient Estimator -> Classical Optimizer
- **Critical path**: 1) Sample from data distribution p, 2) Prepare quantum state and measure, 3) Compute ∇_w f and H_ww classically, 4) Construct observables O_w, P_wℓ, 5) Execute quantum circuits to estimate ∇_γ f and H_wγ, 6) Update parameters using chosen algorithm
- **Design tradeoffs**: Expressiveness vs concavity (general NN vs linear model), algorithm complexity vs guarantees (first-order vs second-order methods), sample complexity scaling with observable norm
- **Failure signatures**: Vanishing gradients (barren plateaus), non-convergence/cycling, Hessian inversion instability
- **First 3 experiments**: 1) Benchmark on trivial distribution (2-qubit thermal state), 2) Compare discriminator models (general NN vs linear), 3) Scaling with system size (measure quantum circuit executions vs gradient accuracy)

## Open Questions the Paper Calls Out

- What is the empirical performance of EQBM algorithms on real-world datasets and near-term quantum hardware?
- How does training efficiency compare between free energy minimization and Jarzynski equality approaches?
- What mitigation strategies are required to address barren plateaus in EQBM training?
- How can hidden units be effectively incorporated to improve representational power without prohibitive complexity?

## Limitations

- Barren plateaus phenomenon not addressed in current work
- No empirical results on realistic problem sizes or near-term quantum hardware
- Efficiency depends on Hamiltonian simulation and state preparation being tractable
- Sample complexity may increase significantly if discriminator norm grows large

## Confidence

- **High Confidence**: Correctness of DV reformulation and unbiasedness of quantum gradient estimators
- **Medium Confidence**: Theoretical convergence guarantees for first-order methods, practical performance uncertain
- **Low Confidence**: Claims of efficient training on near-term quantum hardware without empirical validation

## Next Checks

1. Implement extragradient algorithm on 4-qubit Ising model thermal state learning, tracking relative entropy and gradient norms
2. Systematically vary gradient accuracy ε for 6-qubit problem and measure quantum circuit executions to validate O(‖g‖²/ε²) scaling
3. Compare final relative entropy and training stability between general neural network and linear feature model for T_w on same distribution