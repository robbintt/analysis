---
ver: rpa2
title: 'ChartAnchor: Chart Grounding with Structural-Semantic Fidelity'
arxiv_id: '2512.01017'
source_url: https://arxiv.org/abs/2512.01017
tags:
- chart
- data
- visual
- code
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChartAnchor, a comprehensive benchmark for
  evaluating chart grounding capabilities in multimodal large language models (MLLMs).
  Chart grounding involves aligning a chart's visual appearance with its structured
  semantics, requiring models to generate executable plotting code and recover underlying
  tabular data with precise values.
---

# ChartAnchor: Chart Grounding with Structural-Semantic Fidelity

## Quick Facts
- arXiv ID: 2512.01017
- Source URL: https://arxiv.org/abs/2512.01017
- Reference count: 40
- Introduces a comprehensive benchmark for evaluating chart grounding in multimodal large language models

## Executive Summary
ChartAnchor introduces a comprehensive benchmark for evaluating chart grounding capabilities in multimodal large language models (MLLMs). Chart grounding involves aligning a chart's visual appearance with its structured semantics, requiring models to generate executable plotting code and recover underlying tabular data with precise values. The benchmark includes 8,068 chart–table–code triples spanning 30 diverse chart types and introduces two complementary tasks: chart-to-code generation and controlled chart-to-table reconstruction. A multi-level evaluation framework assesses functional validity, visual structure consistency, semantic data fidelity, and perceptual similarity.

## Method Summary
ChartAnchor establishes a comprehensive benchmark for evaluating chart grounding through two primary tasks: chart-to-code generation and controlled chart-to-table reconstruction. The benchmark comprises 8,068 chart–table–code triples across 30 chart types, generated through a systematic process that combines structured data with visualization parameters. A multi-level evaluation framework assesses models across four dimensions: functional validity (whether generated code produces a chart), visual structure consistency (alignment of chart elements), semantic data fidelity (accuracy of recovered tabular values), and perceptual similarity (how closely the generated chart matches the original). The evaluation spans 24 MLLMs including GPT-4o, GPT-5, Gemini Pro, and Claude, providing a systematic assessment of current capabilities and limitations.

## Key Results
- Current MLLMs, including top models like GPT-5, struggle with fine-grained data recovery and visual structure consistency in chart grounding tasks
- The benchmark reveals significant performance gaps between basic code generation and accurate data reconstruction
- ChartAnchor exposes limitations in models' ability to integrate symbolic reasoning with visual precision

## Why This Works (Mechanism)
The paper demonstrates that effective chart grounding requires simultaneous processing of visual features and structured semantic information. The benchmark's design forces models to bridge the gap between raw visual perception and structured data representation, revealing that current MLLMs struggle to maintain fidelity across both domains simultaneously. The two-task approach (code generation and table reconstruction) provides complementary views of grounding performance, with code generation testing structural understanding while table reconstruction tests precise data recovery.

## Foundational Learning

**Chart grounding** - The ability to align visual chart elements with their underlying data semantics. Needed to evaluate how well models can translate between visual and structured representations. Quick check: Can the model generate valid code and recover accurate data from a simple bar chart?

**Multimodal large language models** - AI systems that process both visual and textual information. Needed to understand the target model class for evaluation. Quick check: Does the model have vision-language capabilities?

**Structured evaluation framework** - Multi-level assessment across functional, visual, semantic, and perceptual dimensions. Needed to provide comprehensive performance characterization. Quick check: Are all four evaluation dimensions applied consistently?

## Architecture Onboarding

**Component map:** Chart Image → Visual Encoder → Multimodal Fusion → Code Generator/Table Reconstructor → Output Code/Data Table

**Critical path:** Visual input → Visual feature extraction → Semantic mapping → Structured output generation

**Design tradeoffs:** The benchmark prioritizes comprehensive evaluation over computational efficiency, requiring detailed analysis across multiple dimensions rather than single-score metrics.

**Failure signatures:** Models typically succeed at generating syntactically valid code but fail at precise data recovery; visual structure understanding often breaks down with complex chart types or overlapping elements.

**First experiments:**
1. Evaluate model performance on basic chart types (bar, line) to establish baseline capabilities
2. Test table reconstruction accuracy on charts with varying data complexity
3. Assess code generation quality across different chart types to identify structural understanding patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 8,068 examples may not capture full real-world chart diversity
- Focus on 30 chart types may miss specialized or emerging visualization formats
- Some evaluation metrics rely on subjective thresholds that could introduce bias

## Confidence

**High confidence:** Current MLLMs struggle with chart grounding tasks, well-supported by systematic evaluation across 24 models with reproducible results.

**Medium confidence:** The benchmark's comprehensiveness is reasonable but may not capture all real-world complexities like annotations and multi-chart relationships.

**Low confidence:** The specific need for "integrating symbolic reasoning with visual precision" is plausible but not fully demonstrated through ablation studies.

## Next Checks
1. Test model performance on charts with intentional ambiguities or missing information to assess robustness beyond the controlled dataset
2. Evaluate whether fine-tuning on ChartAnchor improves real-world chart understanding performance on external datasets
3. Conduct user studies to validate whether the benchmark's evaluation metrics align with human judgments of chart understanding quality