---
ver: rpa2
title: Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language
  Recognition
arxiv_id: '2507.00248'
source_url: https://arxiv.org/abs/2507.00248
tags:
- sign
- data
- language
- recognition
- slait
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework for real-time sign language
  recognition using lightweight deep neural networks (DNNs) trained on limited data.
  The system addresses challenges of data scarcity, high computational costs, and
  frame rate discrepancies between training and inference environments.
---

# Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition

## Quick Facts
- **arXiv ID**: 2507.00248
- **Source URL**: https://arxiv.org/abs/2507.00248
- **Reference count**: 17
- **Primary result**: Achieves 92% isolated sign recognition accuracy with a 7.2MB DNN model processing <10ms latency on edge devices

## Executive Summary
This paper presents a novel framework for real-time sign language recognition using lightweight deep neural networks (DNNs) trained on limited data. The system addresses challenges of data scarcity, high computational costs, and frame rate discrepancies between training and inference environments. By encoding sign language parameters (handshape, palm orientation, movement, location) into vectorized inputs using MediaPipe for landmark extraction, the framework achieves highly separable input data representations. The optimized DNN architecture, designed for sub-10MB deployment, enables accurate classification of 343 ASL signs with less than 10ms latency on edge devices. The "slait data" platform facilitates structured labeling and vector extraction. The model achieved 92% accuracy in isolated sign recognition and has been integrated into the "slait ai" web application for stable real-time inference.

## Method Summary
The framework encodes sign language parameters into vectorized inputs using MediaPipe for landmark extraction, creating a 947-dimensional feature vector from hand and pose landmarks. The branched DNN architecture processes these features through specialized branches for each parameter type, achieving high accuracy with minimal computational overhead. The model was trained on 12,752 videos across 343 signs using a custom "slait data" platform for structured labeling and vector extraction. The system standardizes frame rates and filters irrelevant frames to optimize training efficiency. The resulting model, sized at 7.2MB, processes signs in under 10ms latency, making it suitable for real-time applications on edge devices. The framework demonstrates the feasibility of achieving high accuracy in sign language recognition despite limited training data and computational constraints.

## Key Results
- 92% accuracy in Isolated Sign Recognition (ISR) on 343 ASL signs
- 7.2MB model size with <10ms latency on edge devices
- Single Frame Sign Recognition (SFSR) accuracy of 87%
- Real-time deployment via "slait ai" web application with stable inference

## Why This Works (Mechanism)
The framework's effectiveness stems from its specialized encoding of sign language parameters into highly separable input representations. By using MediaPipe to extract 3D landmarks for hands and pose, the system captures the essential geometric features of signs. The branched DNN architecture allows for specialized processing of each sign parameter (handshape, palm orientation, movement, location, pose), enabling the model to learn complex relationships between these features. The FPS standardization and automatic filtering ensure consistent input quality, while the lightweight design maintains real-time performance on edge devices.

## Foundational Learning
- **MediaPipe 3D landmark extraction**: Required for capturing hand and pose geometry; verify by checking extracted landmark consistency across frames
- **Branch-based DNN architecture**: Enables specialized feature processing; validate by testing single-branch vs. multi-branch performance
- **FPS standardization**: Critical for consistent temporal representation; check by comparing accuracy across different input FPS
- **Vectorized sign parameter encoding**: Transforms raw landmarks into separable features; verify by analyzing feature separability metrics
- **Sparse Categorical Crossentropy loss**: Suitable for multi-class classification; confirm by monitoring training loss convergence
- **Adam optimizer with weight decay**: Balances learning rate adaptation with regularization; validate by comparing to standard Adam

## Architecture Onboarding
- **Component map**: MediaPipe -> Landmark Extraction -> Vector Encoding -> Branched DNN -> Classification
- **Critical path**: Landmark extraction → vector encoding → branch processing → final classification
- **Design tradeoffs**: Model size vs. accuracy, real-time performance vs. vocabulary size, computational efficiency vs. feature richness
- **Failure signatures**: MediaPipe landmark loss, FPS mismatch, overfitting on limited data, subvector transformation errors
- **First experiments**:
  1. Test MediaPipe landmark extraction consistency across diverse video conditions
  2. Validate 947-dimensional vector encoding from raw landmarks
  3. Compare single-branch vs. branched DNN performance on parameter classification

## Open Questions the Paper Calls Out
- How does the model's latency and accuracy scale when expanding the vocabulary from 343 signs to 4,000 or more signs?
- How can the framework improve generalization to support diverse signing styles and dialects beyond the current limited datasets?
- Does the reliance on as few as two input frames compromise the recognition of signs primarily distinguished by complex motion trajectories?
- What is the quantitative performance gap between the reported Isolated Sign Recognition (ISR) accuracy and the continuous sign recognition capability in the web application?

## Limitations
- Reliance on MediaPipe landmark extraction quality, which can be affected by occlusions and lighting conditions
- Insufficient specification of branch architecture details (layer counts, widths, regularization techniques)
- Limited ablation studies on component contributions to reported accuracy metrics
- Unspecified training/validation/test split ratios affecting generalization assessment

## Confidence
- **High confidence**: Overall framework concept, model size, and latency claims
- **Medium confidence**: Accuracy metrics without detailed validation procedures
- **Low confidence**: Exact implementation details of architecture and preprocessing pipeline

## Next Checks
1. Reconstruct and validate the 947-dimensional input vector transformation from MediaPipe landmarks
2. Implement and test the branched DNN architecture with various layer configurations
3. Conduct comprehensive FPS standardization validation across diverse input sources