---
ver: rpa2
title: Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning
  Regime
arxiv_id: '2509.24882'
source_url: https://arxiv.org/abs/2509.24882
tags:
- risk
- neural
- excess
- phase
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies scaling laws in shallow neural networks by mapping
  the training problem to sparse vector/matrix estimation, specifically LASSO and
  matrix compressed sensing. Leveraging approximate message passing (AMP) and its
  state evolution, the authors provide a unified theoretical framework to characterize
  the excess risk scaling behavior across different regimes of sample size, regularization
  strength, and network architecture.
---

# Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime

## Quick Facts
- **arXiv ID:** 2509.24882
- **Source URL:** https://arxiv.org/abs/2509.24882
- **Reference count:** 40
- **Primary result:** This paper studies scaling laws in shallow neural networks by mapping the training problem to sparse vector/matrix estimation, specifically LASSO and matrix compressed sensing.

## Executive Summary
This paper provides a unified theoretical framework to characterize the excess risk scaling behavior of shallow neural networks across different regimes of sample size, regularization strength, and network architecture. By mapping the training problem to sparse vector/matrix estimation problems and leveraging approximate message passing (AMP) and its state evolution, the authors uncover a striking universality between diagonal and quadratic networks, including transitions from benign to harmful overfitting. The analysis derives optimal regularization strategies that achieve Bayesian-optimal rates and establishes a precise link between the spectral properties of learned weights and generalization performance.

## Method Summary
The authors leverage approximate message passing (AMP) algorithms and their state evolution equations to analyze shallow neural networks in the feature learning regime. By mapping the training problem to sparse vector/matrix estimation problems (LASSO and matrix compressed sensing), they derive analytical expressions for excess risk scaling behavior across different regimes. The framework connects the training dynamics to the spectral properties of learned weights, characterizing the interplay between a bulk (learned noise), spikes (learned/unlearned features), and outliers (strong signals) in determining generalization performance.

## Key Results
- The authors uncover universality between diagonal and quadratic networks, showing they exhibit similar scaling laws and overfitting transitions.
- Optimal regularization strategies are derived that achieve Bayesian-optimal excess risk rates across different sample size and network architecture regimes.
- A precise connection is established between the spectral properties of learned weights (bulk, spikes, outliers) and generalization performance.

## Why This Works (Mechanism)
The paper's approach works by exploiting the high-dimensional structure of the problem through the lens of approximate message passing. AMP algorithms are particularly well-suited for analyzing these systems because they can track the evolution of estimates through iterative updates while accounting for the statistical dependencies in the data. The state evolution equations provide a deterministic approximation to the stochastic dynamics, allowing for precise characterization of the learning process and its convergence properties.

## Foundational Learning
- **Approximate Message Passing (AMP):** Iterative algorithms for solving high-dimensional estimation problems; needed for tractability in analyzing the training dynamics; quick check: verify convergence properties and state evolution accuracy.
- **State Evolution:** Deterministic recursion tracking the evolution of AMP estimates; needed to characterize the learning process analytically; quick check: compare with empirical learning curves.
- **Sparse Vector/Matrix Estimation:** Optimization problems with underlying sparse structure; needed as the theoretical framework for analyzing neural network training; quick check: validate equivalence mappings between problems.
- **Spectral Analysis of Learned Weights:** Decomposition into bulk, spikes, and outliers; needed to understand generalization properties; quick check: correlate spectral components with test performance.
- **Scaling Laws in High Dimensions:** Asymptotic behavior of estimation error as dimensions grow; needed for characterizing generalization; quick check: verify predicted power-law relationships empirically.

## Architecture Onboarding
**Component Map:** Data -> Feature Extraction (Shallow Network) -> AMP State Evolution -> Spectral Decomposition -> Excess Risk Analysis

**Critical Path:** The essential components are the feature extraction through the shallow network, the AMP state evolution tracking, and the spectral decomposition of learned weights. These form the core pipeline for deriving scaling laws and understanding generalization.

**Design Tradeoffs:** The main tradeoff is between theoretical tractability (requiring specific assumptions like Gaussian features and high-dimensional limits) and practical applicability to real-world datasets. The framework sacrifices generality for analytical precision.

**Failure Signatures:** Deviations between theoretical predictions and empirical results indicate breakdowns in assumptions - particularly when feature distributions are non-Gaussian, activation functions differ from ReLU, or aspect ratios fall outside the high-dimensional regime where AMP theory is rigorously proven.

**First Experiments:**
1. Verify the predicted scaling laws by training shallow networks with varying sample sizes and measuring excess risk.
2. Test the universality claim by comparing spectral decompositions of diagonal and quadratic networks with identical parameters.
3. Validate the optimal regularization strategy by sweeping regularization strength and comparing achieved excess risk to theoretical predictions.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework relies heavily on AMP state evolution equations, which are rigorously proven only in certain high-dimensional asymptotic regimes.
- The analysis assumes isotropic Gaussian features and specific activation functions (ReLU), limiting direct applicability to real-world datasets.
- The universality claims between diagonal and quadratic networks are based on analytical derivations and numerical evidence, but the underlying mechanisms for this behavior in practical scenarios remain incompletely understood.

## Confidence
- **High confidence:** Mathematical derivations within the rigorously proven AMP regime and consistency of numerical experiments with theoretical predictions in tested parameter ranges.
- **Medium confidence:** Extrapolation of AMP predictions beyond asymptotic limits and universality observations, given strong empirical support but limited theoretical justification outside specific regimes.
- **Low confidence:** Direct translation of these findings to practical deep learning applications without additional validation on real datasets.

## Next Checks
1. Test the theoretical predictions on non-Gaussian feature distributions and alternative activation functions to assess robustness.
2. Conduct extensive experiments on real-world datasets to verify the spectral-risk relationship and universality claims in practical settings.
3. Investigate the behavior of learned spectra and excess risk in networks with depth greater than two layers to understand the limitations of the shallow network analysis.