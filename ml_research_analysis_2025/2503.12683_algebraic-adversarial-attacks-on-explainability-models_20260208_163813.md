---
ver: rpa2
title: Algebraic Adversarial Attacks on Explainability Models
arxiv_id: '2503.12683'
source_url: https://arxiv.org/abs/2503.12683
tags:
- adversarial
- group
- neural
- algebraic
- symmetry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces algebraic adversarial attacks on post-hoc
  explainability models, providing a mathematically tractable alternative to classical
  constrained optimization approaches. The method exploits the symmetry groups of
  neural networks using geometric deep learning and Lie theory.
---

# Algebraic Adversarial Attacks on Explainability Models

## Quick Facts
- **arXiv ID**: 2503.12683
- **Source URL**: https://arxiv.org/abs/2503.12683
- **Reference count**: 40
- **Key outcome**: This paper introduces algebraic adversarial attacks on post-hoc explainability models, providing a mathematically tractable alternative to classical constrained optimization approaches.

## Executive Summary
This paper introduces a novel approach to generating adversarial examples for explainability models by exploiting the symmetry groups of neural networks through Lie theory and geometric deep learning. Instead of using iterative optimization, the method deterministically generates perturbations that preserve classification but drastically alter explanations. The authors demonstrate this approach on three common architectures (MLP, CNN, GCN) and validate its effectiveness on MNIST, Wisconsin breast cancer, and network traffic datasets.

## Method Summary
The method identifies symmetry groups associated with the first layer of a neural network, specifically $P_W \ltimes \ker(W)$ where $W$ is the weight matrix. By constructing group elements from the Lie algebra of this symmetry group, the approach generates algebraic adversarial examples that preserve the neural network's output while changing the explainability model's attribution. The perturbation magnitude is analytically bounded using the exponential map, ensuring the attack stays within a specified error threshold $\epsilon$. This provides a deterministic alternative to optimization-based adversarial attacks while guaranteeing the classification remains unchanged.

## Key Results
- Algebraic adversarial examples can be generated deterministically using symmetry groups, guaranteeing classification preservation
- Small perturbations can produce drastically different explanations for path-based attribution methods, neural conductance, and additive attribution methods like LIME
- The method provides theoretical bounds on explanation differences based on the error threshold $\epsilon$
- Experimental validation demonstrates efficacy on MNIST, Wisconsin breast cancer, and network traffic datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial examples can be generated deterministically via group action rather than iterative optimization if the perturbation lies within the network's symmetry group.
- **Mechanism:** The method identifies the symmetry group $P_W \ltimes \ker(W)$ associated with the first layer of a neural network. Elements $g$ from this group satisfy $F(g \cdot x) = F(x)$ by definition, automatically satisfying the "same classification" constraint of an adversarial attack without needing a loss function to enforce it.
- **Core assumption:** The neural network's first layer is an affine map $L(x) = Wx + b$, and the input dimension $n$ is greater than the number of neurons $r$ in the first layer (ensuring a non-trivial kernel).
- **Break condition:** If the first layer weight matrix $W$ is full rank (i.e., $\text{rank}(W) = n$), the kernel is trivial, and this specific algebraic attack collapses.

### Mechanism 2
- **Claim:** The magnitude of the perturbation (error threshold $\epsilon$) can be rigorously bounded using the exponential map of Lie algebras.
- **Mechanism:** The paper utilizes the exponential map $g_t = \exp(tA)$ to generate group elements from the Lie algebra. By solving the inequality $\|g_t \cdot x - x\|_\infty \leq \epsilon$, they derive a precise scalar value for $t$ that guarantees the perturbation remains within the error threshold.
- **Break condition:** If the matrix exponential $\exp(tA)$ results in numerical instability for the required precision of $t$.

### Mechanism 3
- **Claim:** Explainability models (specifically path-based and additive methods) fail to be invariant under the neural network's symmetry group, resulting in drastically different explanations for identical classifications.
- **Mechanism:** While the network $F$ is invariant to the group action ($F(x) = F(g \cdot x)$), the explainability model $\phi$ (e.g., Integrated Gradients, LIME) relies on gradients or local perturbations that are not invariant to $g$.
- **Break condition:** If the explainability method happens to align perfectly with the symmetry group (highly unlikely per Remark 5.6).

## Foundational Learning

- **Concept: Lie Groups and Lie Algebras**
  - **Why needed here:** This is the mathematical engine of the paper. You must understand that a Lie group represents continuous symmetries (like rotations/scaling) and the Lie algebra represents the "tangent space" or infinitesimal generators of these symmetries at the identity.
  - **Quick check question:** Can you explain why mapping a Lie algebra element $A$ to $\exp(tA)$ generates a valid symmetry operation in the group?

- **Concept: Geometric Deep Learning (GDL)**
  - **Why needed here:** The paper frames neural networks not just as function approximators, but as geometric operators that preserve specific symmetries (e.g., translation in CNNs). Understanding GDL helps parse why the authors look for "symmetry groups" of the network layers.
  - **Quick check question:** What does it mean for a layer to be "equivariant" or "invariant" to a group action?

- **Concept: The Kernel (Null Space) of a Matrix**
  - **Why needed here:** The vulnerability is explicitly located in the kernel of the first layer's weight matrix ($\ker(W)$). You need to understand that any vector in this kernel adds information to the input that is immediately "deleted" or ignored by the first layer.
  - **Quick check question:** If $W$ is a $16 \times 784$ matrix (MNIST input to hidden layer), what is the minimum dimensionality of the kernel?

## Architecture Onboarding

- **Component map:**
  Target Network ($F$) -> First Layer Weights ($W_1$) -> Lie Algebra Generator ($A_{xy}$) -> Exponential Map ($\exp(tA)$) -> Explainer ($\phi$)

- **Critical path:**
  1. Extract the weight matrix $W_1$ of the target model's first layer.
  2. Compute $\ker(W_1)$ to find the subspace of "invisible" perturbations.
  3. Select a vector $y$ from $\ker(W_1)$ and an input $x$.
  4. Construct the skew-symmetric matrix $\hat{A}_{xy}$ (Prop 4.5) to ensure orthogonality (if using Algorithm 1).
  5. Compute the perturbation parameter $t$ based on the allowed error $\epsilon$ (Prop 3.4).
  6. Generate adversarial example $\tilde{x} = \exp(t\hat{A}_{xy}) \cdot x$.

- **Design tradeoffs:**
  - **Algebraic vs. Optimization:** Algebraic attacks are "traceable" and mathematically guaranteed to preserve classification, but they are strictly limited to the geometry of the first layer. Optimization attacks might find more aggressive perturbations but are "black boxes" regarding how they were found.
  - **Multiplicative vs. Additive:** Algorithm 1 (Multiplicative) uses the Lie group exponential (rotation-like); Algorithm 2 (Additive) simply adds a kernel vector. Additive is simpler but might not preserve the data distribution as well as multiplicative transforms in some domains.

- **Failure signatures:**
  - **Trivial Kernel:** If $n \le r$ (input dimension $\le$ first layer width) and weights are full rank, $\ker(W)$ is empty. The attack fails immediately.
  - **Distortion:** While the classification is preserved, the "image" might look distorted to a human (though the paper claims small $\epsilon$ mitigates this).

- **First 3 experiments:**
  1. **Sanity Check:** Train a small MLP on MNIST. Compute $\ker(W_1)$. Verify that adding a small vector from $\ker(W_1)$ to an image does *not* change the model's logits.
  2. **Integrated Gradients Attack:** Implement Algorithm 1 on a correctly classified MNIST digit. Generate Integrated Gradients for the clean image and the adversarial image. Plot them side-by-side to confirm the "explanation" has changed significantly while the label hasn't.
  3. **LIME Comparison:** Apply the same algebraic perturbation to a LIME explainer. Compare the "distance" between the clean and adversarial explanations for LIME vs. Integrated Gradients to see which is more susceptible.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there other symmetry groups beyond $P_W \ltimes \ker(W)$ that can be exploited to generate algebraic adversarial examples?
- Basis in paper: [explicit] The conclusion explicitly states the authors "seek to investigate other symmetry groups which lead to adversarial explanations."
- Why unresolved: The current work focuses exclusively on symmetries arising from the first affine layer's kernel and stabilizer groups.
- What evidence would resolve it: Theoretical derivation of new symmetry groups in neural networks and demonstrations of attacks utilizing these alternative algebraic structures.

### Open Question 2
- Question: How can the appropriate error threshold $\epsilon$ be systematically determined for specific datasets?
- Basis in paper: [explicit] The conclusion lists "investigat[ing] the appropriate error threshold $\epsilon$ on each dataset" as future work.
- Why unresolved: While the paper proves the existence of algebraic attacks for an arbitrary $\epsilon$, it does not provide a method for finding the optimal bound that balances imperceptibility with explanation alteration.
- What evidence would resolve it: A methodology or heuristic for selecting dataset-specific $\epsilon$ values that maximizes attack efficacy while satisfying perceptual constraints.

### Open Question 3
- Question: Can this algebraic attack framework be extended to networks where the input dimension $n$ is less than or equal to the first layer dimension $r$?
- Basis in paper: [inferred] Section 4.1 states that if rank $W = n$, the group $P_W$ is trivial, implying the current method fails for "wide" first layers.
- Why unresolved: The method relies on the non-triviality of the kernel of the first layer's weight matrix; if the kernel is empty, the proposed generators for the symmetry group vanish.
- What evidence would resolve it: An extension of the theory to deeper layers or an alternative algebraic construction that does not depend on the null space of the first layer.

## Limitations
- The attack requires the first layer to be rank-deficient (input dimension > hidden dimension), limiting applicability to specific network configurations
- Validation is limited to three specific datasets and relatively simple MLP architectures
- The heavy mathematical complexity makes practical implementation challenging

## Confidence
- **High Confidence:** The theoretical framework linking symmetry groups to adversarial perturbations is sound and mathematically rigorous
- **Medium Confidence:** Experimental results on MNIST and Wisconsin Breast Cancer are convincing for the specific setup
- **Low Confidence:** The practical impact of these attacks on state-of-the-art explainability methods in production systems is highly speculative

## Next Checks
1. Test the attack on a standard CNN (e.g., LeNet, AlexNet) trained on CIFAR-10 to assess if the first-layer kernel-based vulnerability persists in deeper architectures
2. Evaluate the attack's effectiveness against a wider range of explainability methods, including Shapley-value-based methods (SHAP) and gradient Ã— input variants
3. Benchmark the algebraic attack against a state-of-the-art optimization-based attack (e.g., PGD) on the same tasks to quantify the tradeoff between guaranteed success and perturbation magnitude