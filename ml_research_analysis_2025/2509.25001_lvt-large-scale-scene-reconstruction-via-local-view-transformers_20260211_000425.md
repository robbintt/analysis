---
ver: rpa2
title: 'LVT: Large-Scale Scene Reconstruction via Local View Transformers'
arxiv_id: '2509.25001'
source_url: https://arxiv.org/abs/2509.25001
tags:
- input
- view
- local
- views
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a local view transformer architecture for efficient
  scene reconstruction and novel view synthesis at scale. Instead of the standard
  quadratic attention, it processes information in local neighborhoods around each
  view using relative camera poses as positional encoding.
---

# LVT: Large-Scale Scene Reconstruction via Local View Transformers

## Quick Facts
- arXiv ID: 2509.25001
- Source URL: https://arxiv.org/abs/2509.25001
- Reference count: 40
- LVT achieves +3.54dB PSNR over Long-LRM and +2.11dB PSNR over 3DGS on DL3DV-140 benchmark

## Executive Summary
LVT introduces a local view transformer architecture for efficient large-scale scene reconstruction and novel view synthesis. By restricting attention to spatially nearby views using relative camera poses as positional encoding, it scales linearly with input views rather than quadratically. The model outputs 3D Gaussian splats with both view-dependent color and opacity, achieving state-of-the-art performance on DL3DV-140 and strong zero-shot generalization to Tanks&Temples and Mip-NeRF360 datasets.

## Method Summary
LVT processes input images through a 24-layer transformer with local attention windows. Each view's tokens are conditioned on relative poses to neighbors, enabling generalization across sequence lengths. The model decodes into per-pixel 3D Gaussian splats parameterized by scale, rotation, depth, spherical harmonic color, and opacity. During training, mixed-resolution sampling is used (long sequences at 240p, short at 480p), followed by finetuning at 960p. View-dependent opacity is regularized to prevent artifacts.

## Key Results
- +3.54dB PSNR improvement over Long-LRM on DL3DV-140 benchmark
- +2.11dB PSNR improvement over 3DGS on same benchmark
- Strong zero-shot generalization to Tanks&Temples and Mip-NeRF360 datasets
- Linear scaling enables handling hundreds of input views efficiently

## Why This Works (Mechanism)

### Mechanism 1: Local Neighborhood Attention Replaces Global Self-Attention
Restricting attention to spatially nearby views reduces complexity from O(N²) to O(N) while maintaining reconstruction quality. For each query view j, attention operates only on tokens from a window of w neighbor views N(j) selected by camera center distance. Keys and values from neighbors are conditioned on relative transforms, then standard scaled dot-product attention is computed locally per view rather than globally across all tokens.

### Mechanism 2: Relative Pose Conditioning Enables Transformation Invariance
Encoding relative camera transforms rather than absolute world poses allows generalization from short training sequences to arbitrary-length inference sequences. Compute P_j · P_j^(-1) between query and neighbor views, apply positional encoding to the relative quaternion and translation, then project through MLPs to produce conditioning features C^(l)_(j,j') added to keys and values. Tokens operate in each view's local coordinate frame.

### Mechanism 3: View-Dependent Opacity Improves Thin Structure and Lighting
Predicting spherical harmonics coefficients for opacity (not just color) better captures view-dependent effects on transparency boundaries. Decode per-pixel Gaussian parameters including d_o = (deg_SH_σ + 1)² opacity SH channels. During rendering, opacity varies with viewing direction. Regularization R_σ penalizes extreme opacity magnitudes at randomly sampled directions.

## Foundational Learning

- Concept: **Attention complexity in transformers**
  - Why needed here: Understanding why O(N²) self-attention blocks scaling to hundreds of views motivates the local attention design.
  - Quick check question: Given 64 input views with 1024 tokens each, what is the memory cost difference between global self-attention and local attention with window size 5?

- Concept: **3D Gaussian Splatting representation**
  - Why needed here: LVT's output is pixel-aligned Gaussians parameterized by position, scale, rotation (quaternion), color SH, and opacity SH.
  - Quick check question: If a Gaussian has 3-channel scale, 4-channel quaternion, depth, 48-channel color SH (deg=3), and 16-channel opacity SH (deg=3), what is d_G?

- Concept: **Plücker coordinates vs local ray maps**
  - Why needed here: Prior work concatenates Plücker rays (intrinsics + extrinsics) to inputs; LVT uses only intrinsics-based local rays and moves extrinsics to attention conditioning.
  - Quick check question: Why does encoding extrinsics in tokens force learning over all absolute pose combinations?

## Architecture Onboarding

- Component map: Input images → Patchify with local ray maps → 24-layer LVT blocks → Decode Gaussian parameters → Differentiable 3DGS rasterizer → Loss computation

- Critical path:
  1. Neighbor selection by camera center distance (not temporal order for unordered captures)
  2. Relative pose embedding: P_j · P_j^(-1) → PE → MLP → C_(j,j')
  3. Conditioned attention: K, V receive C_(j,j') addition; Q does not
  4. Mixed-resolution training: longer sequences (40-52 images) at 240×135, shorter (16 images) at 960×540

- Design tradeoffs:
  - Window size w=5 balances quality vs speed (Table 7: w=3 gives 21.75 PSNR, w=5 gives 22.02, w=7 gives 21.91)
  - Dilation hurts: dilation=2 with w=5 drops to 21.26 PSNR
  - View-dependent opacity regularization: higher α reduces popping but degrades PSNR

- Failure signatures:
  - Quadratic memory blowup at high resolution: doubling resolution → 16× transformer cost
  - Popping artifacts in extreme textures when using view-dependent opacity with low α
  - Dense redundant splats from per-pixel generation cause rasterization bottleneck

- First 3 experiments:
  1. **Reproduce scaling curve**: Train local vs global attention models on 16 inputs at 480×270; evaluate at 12, 16, 20, 32 inputs. Verify Table 2 PSNR trends and measure inference time (expect ~linear vs ~quadratic).
  2. **Ablate conditioning strategy**: Compare (a) Plücker rays in tokens, (b) world pose in attention, (c) relative pose in attention. Reproduce Table 3 to validate that relative pose conditioning is necessary.
  3. **Perturb window size and regularization**: Sweep w∈{3,5,7} and α∈{0, 0.001, 0.01} on a held-out scene. Identify where popping artifacts appear and quantify the quality-regularization tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does interleaving Local View Transformer (LVT) blocks with standard global attention layers improve reconstruction quality in scenes with highly redundant, overlapping views without sacrificing linear scaling?
- **Basis in paper:** [explicit] Section 4.4 states that the model's local attention may restrict communication between distant, potentially relevant views in redundant datasets, and suggests that "a hybrid approach could be explored, where LVT layers are interleaved with standard transformer layers."
- **Why unresolved:** The authors restricted the architecture to local attention to ensure strict linear scaling and generalization, leaving hybrid configurations untested.
- **What evidence would resolve it:** An ablation study comparing pure LVT against a hybrid LVT/Transformer model on datasets with high view redundancy, measuring both reconstruction metrics (PSNR/SSIM) and inference latency.

### Open Question 2
- **Question:** Can an adaptive regularization strategy for view-dependent opacity prevent popping artifacts in extreme texture regions while preserving the fidelity benefits?
- **Basis in paper:** [explicit] Section 4.4 notes that the current view-dependent opacity implementation produces popping artifacts in extreme textures (referencing Supp. Fig 12) and that increasing the fixed regularization weight α to fix this comes at the cost of reduced rendering quality.
- **Why unresolved:** The current model uses a global weighting factor for regularization, failing to distinguish between stable regions and volatile regions that cause artifacts.
- **What evidence would resolve it:** Implementation of a spatially adaptive regularization loss that eliminates visual artifacts in identified problem areas while maintaining or improving quantitative metrics (PSNR/LPIPS) compared to the fixed-weight baseline.

### Open Question 3
- **Question:** Can the dense, redundant Gaussian splat representation generated by LVT be efficiently pruned or consolidated in real-time to resolve rasterization bottlenecks?
- **Basis in paper:** [explicit] Section 4.4 identifies that per-pixel splat generation results in a dense representation where "splat rasterization can become a significant bottleneck," explicitly suggesting methods like Speedy-Splat or RadSplat as potential solutions.
- **Why unresolved:** The paper focuses on the feed-forward reconstruction architecture and does not integrate or evaluate these specific external rasterization optimization techniques.
- **What evidence would resolve it:** A system integration study measuring frame rates and memory consumption when rendering LVT outputs using these advanced rasterizers versus the standard implementation.

## Limitations

- **Unknown implementation details**: Patch size, spherical harmonic degrees, and MLP architectures for pose conditioning are not specified
- **Scaling regime boundaries**: Linear scaling claims are validated only up to moderate sequence lengths
- **Opacity regularization tradeoff**: Fixed regularization weight creates a Pareto frontier between artifact suppression and quality

## Confidence

**High confidence**: The local attention mechanism genuinely reduces complexity from O(N²) to O(N) and improves PSNR over global attention on the DL3DV-140 benchmark. The relative pose conditioning provides measurable generalization benefits over absolute pose encodings.

**Medium confidence**: The view-dependent opacity design improves thin structure reconstruction, but the artifact characterization is incomplete. The model's zero-shot generalization to Tanks&Temples and Mip-NeRF360 is demonstrated but the magnitude of improvement varies significantly across datasets.

**Low confidence**: Claims about the model's ability to handle "unbounded scenes" are supported only by moderate-scale benchmarks. The scaling analysis doesn't address extreme cases (thousands of views) or heterogeneous capture conditions.

## Next Checks

1. **Verify the scaling law**: Train LVT models with global vs local attention on 16 inputs at 480×270. Evaluate at 12, 16, 20, 32 inputs to reproduce Table 2's PSNR trends and measure actual inference time scaling.

2. **Characterize the opacity-regularization tradeoff**: Train models across α∈{0, 0.001, 0.01} on a held-out scene. Measure PSNR, SSIM, and LPIPS while recording artifact frequency. Identify the Pareto-optimal operating point.

3. **Test window size sensitivity at scale**: Train models with w∈{3,5,7} on sequences of 40-52 images. Evaluate on both in-distribution (DL3DV-140) and out-of-distribution (Tanks&Temples) scenes to identify when local attention windows become too restrictive.