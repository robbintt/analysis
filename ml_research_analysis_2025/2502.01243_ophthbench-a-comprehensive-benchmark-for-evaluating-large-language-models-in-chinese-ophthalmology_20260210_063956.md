---
ver: rpa2
title: 'OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models
  in Chinese Ophthalmology'
arxiv_id: '2502.01243'
source_url: https://arxiv.org/abs/2502.01243
tags:
- llms
- arxiv
- language
- question
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OphthBench, a specialized benchmark for\
  \ evaluating large language models (LLMs) in Chinese ophthalmology. The benchmark\
  \ covers five clinical scenarios\u2014Education, Triage, Diagnosis, Treatment, and\
  \ Prognosis\u2014across nine tasks with 591 questions, including single-choice,\
  \ multiple-choice, and open-ended formats."
---

# OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology

## Quick Facts
- arXiv ID: 2502.01243
- Source URL: https://arxiv.org/abs/2502.01243
- Reference count: 40
- Primary result: Significant performance gap between current LLMs and clinical requirements in Chinese ophthalmology

## Executive Summary
This paper introduces OphthBench, a specialized benchmark for evaluating large language models (LLMs) in Chinese ophthalmology. The benchmark covers five clinical scenarios—Education, Triage, Diagnosis, Treatment, and Prognosis—across nine tasks with 591 questions, including single-choice, multiple-choice, and open-ended formats. It was developed by three experienced ophthalmologists using real-world clinical cases and authoritative medical materials. The evaluation protocol uses two prompt types to account for prompt sensitivity and employs a multi-dimensional LLM-as-Judge method for open-ended questions. Testing 39 LLMs revealed a significant performance gap between current capabilities and clinical requirements, with models developed by Chinese institutions generally outperforming others. Prompt engineering was shown to enhance LLM performance in ophthalmic tasks. The benchmark aims to support standardized LLM evaluation and development in Chinese ophthalmology.

## Method Summary
OphthBench evaluates LLMs on Chinese ophthalmology across five clinical scenarios (Education, Triage, Diagnosis, Treatment, Prognosis) with 591 questions in three formats (SCQs, MCQs, OEQs). The benchmark was developed by three ophthalmologists using authoritative sources and real clinical cases. Evaluation uses two prompt conditions—common (basic instructions) and advanced (refined with few-shot examples or CO-STAR framework). SCQs/MCQs are scored by accuracy/F1, while OEQs are evaluated by CompassJudger-1-7B across seven dimensions (Factuality, User Satisfaction, Safety, Clarity, Responsibility, Logical Coherence, Richness). Scores are normalized per-task (highest=90) and aggregated using harmonic mean.

## Key Results
- Tested 39 LLMs across five clinical scenarios in Chinese ophthalmology
- Significant performance gap between current LLM capabilities and clinical requirements
- Chinese-developed models generally outperformed others
- Prompt engineering substantially enhanced LLM performance in ophthalmic tasks

## Why This Works (Mechanism)

### Mechanism 1: Workflow-Aligned Taxonomy and Clinical Grounding
Aligning the benchmark's structure directly with a real clinical workflow improves its ability to accurately assess an LLM's practical utility in a specialized domain. The OphthBench benchmark is structured around five key clinical scenarios that mirror actual clinical workflow, combined with questions derived from authoritative sources and real-world cases created and validated by experienced ophthalmologists. This structure forces the model to demonstrate capability along the entire chain of clinical reasoning.

### Mechanism 2: Multi-Format Evaluation with LLM-as-Judge
A combination of constrained and open-ended question formats, evaluated with both deterministic metrics and a multi-dimensional LLM-as-Judge, provides a more robust and holistic assessment of clinical capability than single-format benchmarks. OphthBench uses SCQs, MCQs, and OEQs, with SCQs/MCQs evaluated via accuracy/F1 and OEQs evaluated by CompassJudger-1-7B across seven dimensions. This hybrid approach captures both knowledge recall and the ability to articulate clinical reasoning safely and clearly.

### Mechanism 3: Dual-Prompt Strategy for Sensitivity Mitigation
Using a dual-prompt strategy helps control for prompt sensitivity and reveals the gap between out-of-the-box performance and optimized potential. Two sets of prompts are used: "common" prompts that mimic average user interaction and "advanced" prompts using techniques like in-context examples and structured frameworks to maximize model performance. This provides a more stable performance range and isolates the model's raw capability from its sensitivity to prompting style.

## Foundational Learning

- **Clinical Workflow Decomposition**: Understanding the five stages (Education, Triage, Diagnosis, Treatment, Prognosis) is essential for interpreting benchmark scores. A model might excel at Triage but fail at Prognosis, a nuance lost in a single aggregate score.
  - Quick check: Which clinical scenario was found to be the most challenging for the evaluated LLMs, and what does this suggest about their current limitations?

- **LLM-as-a-Judge & Alignment**: The evaluation of open-ended questions relies entirely on this concept. One must understand its limitations and that it is an automated proxy for human judgment, not a replacement.
  - Quick check: What are the seven dimensions used by the judge model to evaluate open-ended responses, and what is a potential risk of relying on this automated assessment?

- **Prompt Sensitivity & Engineering**: The paper's results show that prompt engineering can dramatically change performance. Understanding this concept is critical for reproducing results and for the practical deployment of these models.
  - Quick check: How did the authors use the two different prompt types to demonstrate the impact of prompt engineering on model performance? What was the highest reported performance increase?

## Architecture Onboarding

- **Component map**: Benchmark Core (OphthBench) -> Evaluation Protocol (Dual-prompt system, scoring functions) -> Judge System (CompassJudger-1-7B) -> Test Harness (framework for 39 LLMs)

- **Critical path**:
  1. Load Benchmark: Access the OphthBench dataset (questions, reference answers, prompts)
  2. Inference: Run each of the 39 LLMs on all 591 questions using both common and advanced prompts
  3. Evaluation: Calculate accuracy/F1 for SCQs/MCQs; pass OEQ responses to LLM-as-Judge for scoring
  4. Aggregation: Normalize scores per task, then compute harmonic mean for scenario and total scores

- **Design tradeoffs**:
  - Scope vs. Depth: Focuses on single specialty (ophthalmology) and language (Chinese), providing depth over breadth
  - Judge vs. Human: Uses LLM-as-Judge to trade cost/speed for potential model-based bias
  - Complexity vs. Fairness: Dual-prompt system adds complexity but increases fairness by controlling for prompt sensitivity

- **Failure signatures**:
  - Low OEQ Scores: May indicate model fails to follow complex prompt structure or produces factually incorrect/unsafe content
  - High SCQ/MCQ, Low OEQ: Suggests model has strong recall but poor generative or reasoning capabilities
  - Prompt Sensitivity Failure: Model's performance drops significantly when moving from common to advanced prompt
  - Judge Divergence: LLM-as-Judge gives high scores to responses a human would deem clinically unsound

- **First 3 experiments**:
  1. Establish a Baseline: Evaluate a small, diverse set of models using only common prompts to get a raw, unoptimized baseline
  2. Isolate Prompt Impact: Take one top-performing model and one mid-tier model; run evaluation with both common and advanced prompts to quantify performance lift
  3. Audit the Judge: Manually review a sample of 10-20 OEQ responses where the model scored high and 10-20 where it scored low against the 7 evaluation dimensions

## Open Questions the Paper Calls Out

- **Dataset accessibility**: Benchmark dataset is not publicly available, limiting reproducibility
- **Limited model coverage**: Does not include all LLMs, especially ophthalmology-specific models
- **Dataset size and task variety**: Limited in both size and the types of tasks it covers
- **Limited prompt types**: Evaluation uses only two prompt strategies
- **Limited human evaluation**: Relies heavily on LLM-as-Judge without extensive human validation

## Limitations

- Benchmark dataset is not publicly available, limiting reproducibility and independent validation
- Evaluation relies heavily on LLM-as-Judge without extensive human validation of the automated scoring
- Limited model coverage, particularly missing specialized ophthalmology-specific LLMs

## Confidence

- **High Confidence**: Significant performance gap between current LLMs and clinical requirements in Chinese ophthalmology
- **Medium Confidence**: Chinese-developed models generally outperform others, though underlying reasons require further investigation
- **Low Confidence**: Reliability of LLM-as-Judge for evaluating clinical safety and responsibility of open-ended responses

## Next Checks

1. **Independent Judge Validation**: Conduct a small-scale study where ophthalmologists manually evaluate a random sample of high and low-scoring open-ended responses and compare their ratings to CompassJudger-1-7B scores

2. **Cross-Benchmark Generalization**: Evaluate top-performing models from OphthBench on a different, publicly available medical benchmark to test whether performance gap is specific to OphthBench or indicative of broader LLM limitations

3. **Prompt Sensitivity Stress Test**: Design and test a third, intermediate prompt that is more structured than "common" but less optimized than "advanced" to determine if performance improvement is linear or plateaus