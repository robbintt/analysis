---
ver: rpa2
title: Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety
  Decisions
arxiv_id: '2510.13931'
source_url: https://arxiv.org/abs/2510.13931
tags:
- person
- accuracy
- across
- socio-demographic
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) inappropriately
  incorporate socio-demographic information into drug-safety predictions, despite
  such attributes being clinically irrelevant. The authors constructed a lightweight
  oncology-focused dataset from FAERS and developed a persona-based evaluation framework
  to test two LLMs (ChatGPT-4o and Bio-Medical-Llama-3.8B) across diverse socio-demographic
  personas and user roles.
---

# Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions

## Quick Facts
- **arXiv ID**: 2510.13931
- **Source URL**: https://arxiv.org/abs/2510.13931
- **Reference count**: 27
- **Primary result**: LLM drug-safety predictions systematically vary with socio-demographic personas, revealing explicit and implicit bias even when clinical factors remain constant

## Executive Summary
This study investigates whether large language models inappropriately incorporate socio-demographic information into drug-safety predictions, despite such attributes being clinically irrelevant. The authors constructed a lightweight oncology-focused dataset from FAERS and developed a persona-based evaluation framework to test two LLMs (ChatGPT-4o and Bio-Medical-Llama-3.8B) across diverse socio-demographic personas and user roles. Results show systematic disparities in adverse event prediction accuracy, with disadvantaged groups often receiving higher accuracy than privileged ones. Two distinct modes of bias were identified: explicit bias where predictions reference persona attributes in reasoning, and implicit bias where predictions are inconsistent despite personas not being explicitly mentioned. Removing cases with explicit identity-based reasoning improved accuracy, demonstrating that bias persists beyond surface-level explanations. These findings highlight critical risks in applying LLMs to pharmacovigilance and underscore the need for fairness-aware evaluation protocols and mitigation strategies before clinical deployment.

## Method Summary
The study constructed the Drug-Safety Decisions (DSD) dataset from FAERS 2024 Q4 release, filtering for oncology records with complete cases (age ≥18) and retaining first adverse events. Two models were evaluated: ChatGPT-4o via API and Bio-Medical-Llama-3.8B locally. A persona-based evaluation framework assigned 25 personas across 7 axes (education, marital status, employment, insurance, language, housing, religion) to clinical records, with three user roles (GP, specialist, patient). Prompts included baseline "Yes" assumption plus persona/role context. Accuracy was computed across conditions, with explicit persona references identified in reasoning traces and implicit bias assessed by accuracy changes after excluding identity-based reasoning cases.

## Key Results
- Systematic accuracy disparities across socio-demographic personas despite identical clinical inputs
- Explicit bias observed when models directly reference persona attributes in reasoning (e.g., attributing alopecia to education level)
- Implicit bias persists even when reasoning traces omit identity references, suggesting deeper model-level associations
- Accuracy improvements after excluding explicit identity-based reasoning indicate bias embedded beyond surface explanations

## Why This Works (Mechanism)

### Mechanism 1: Explicit Persona Attribution in Reasoning Traces
Models surface learned associations between social attributes and health outcomes as causal explanations, even when medication and disease fields remain unchanged across conditions. Training corpora contain statistical patterns linking socio-demographic status to health outcomes that models encode as inferential shortcuts.

### Mechanism 2: Implicit Bias Through Latent Representation Shifts
Models encode implicit associations in weights that shift probability estimates without surfacing in output text, suggesting bias is embedded beyond surface-level explanations. Removing explicit identity-based reasoning exposes residual bias embedded in model behavior.

### Mechanism 3: Role-Conditioned Persona Interaction
Different roles activate distinct reasoning patterns that interact with persona attributes—patients achieved consistently higher accuracy across both models, suggesting role framing shifts model attention. Models have learned role-specific communication patterns from training data that interface differently with social context cues.

## Foundational Learning

- **Pharmacovigilance Signal Detection**: FAERS data contains known reporting biases (under-reporting, duplicates) that constrain interpretation of both baseline accuracy and bias findings. *Why needed*: Understanding data limitations is critical for interpreting model behavior. *Quick check*: Why can't FAERS-derived predictions be treated as ground truth for adverse event likelihood?

- **Counterfactual Fairness in ML**: The methodology holds clinical inputs constant while varying socio-demographic attributes—this is a counterfactual design testing whether identity changes outcomes. *Why needed*: Framework design relies on counterfactual fairness principles. *Quick check*: If two patients have identical clinical profiles but different education levels, what should happen to their AE predictions in a fair system?

- **Explainability-Faithfulness Gap**: The paper shows model explanations don't fully capture why predictions shift—surface justifications may mask underlying bias mechanisms. *Why needed*: Critical for interpreting results showing explicit bias removal doesn't eliminate disparities. *Quick check*: If a model's explanation is clinically plausible but its prediction changes with irrelevant persona information, is the explanation faithful?

## Architecture Onboarding

- **Component map**: FAERS extraction -> oncology filter -> missing value exclusion -> first-AE-only selection -> Persona assignment -> prompt construction -> model inference -> Output parsing -> accuracy computation -> explicit reference coding -> bias classification

- **Critical path**: FAERS extraction → oncology filter → missing value exclusion → first-AE-only selection → Persona assignment → prompt construction → model inference (ChatGPT-4o API, Bio-Medical-Llama-3-8B local) → Output parsing → accuracy computation → explicit reference coding → bias classification

- **Design tradeoffs**: 1000-record dataset: replicable but limited generalizability; ordering-based selection may introduce temporal bias; First-AE-only: simplifies to binary classification but loses multi-label signal detection complexity; Baseline "Yes" assumption: controlled reference point but may not reflect natural model calibration; Persona abstraction: controlled evaluation but simplified compared to real patient complexity

- **Failure signatures**: Accuracy variance >15% across personas with identical clinical inputs; Explicit persona references in >30% of reasoning traces (e.g., ChatGPT-4o housing instability, religion); Accuracy inversion: disadvantaged groups receiving higher accuracy than privileged groups; Role × persona interaction effects with p < 0.05

- **First 3 experiments**: Cross-model replication: Test Claude, Gemini, and other model families on identical DSD evaluation to assess generalizability of bias patterns; Axis ablation study: Remove one socio-demographic axis at a time to identify which attributes drive strongest explicit vs implicit bias effects; Mitigation probing: Add explicit "ignore socio-demographic factors" instructions and measure whether explicit bias decreases but implicit bias persists

## Open Questions the Paper Calls Out

### Open Question 1
Do specific fairness-aware mitigation strategies, such as counterfactual prompting or model calibration, effectively reduce both explicit and implicit bias in LLM-based adverse event predictions? *Basis*: The Discussion states future work should prioritize evaluation tools that surface latent disparities and mitigation strategies such as counterfactual prompting and calibration. *Why unresolved*: This study focused on quantifying and categorizing bias rather than developing or testing algorithmic interventions. *What evidence would resolve it*: A comparative study measuring bias metrics before and after applying specific debiasing techniques to the evaluated models using the same Drug-Safety Decisions dataset.

### Open Question 2
Do the observed socio-demographic biases in adverse event prediction generalize beyond oncology to other clinical domains and larger datasets? *Basis*: The Conclusion notes the study is limited by its small, oncology-focused dataset and explicitly calls for future work to expand this framework across broader medical domains, larger datasets, and multiple model families. *Why unresolved*: The current results are derived solely from 1,000 oncology-specific records from the FAERS database. *What evidence would resolve it*: Replicating the persona-based evaluation framework on diverse, non-oncology datasets (e.g., cardiology or neurology reports) and larger corpora of adverse event reports.

### Open Question 3
To what extent are the model's prediction disparities driven by inherent reporting biases in the FAERS training data versus the model's own reasoning synthesis? *Basis*: The Methods section acknowledges that FAERS is known to contain reporting biases, under-reporting, and potential duplicates, and the authors used a potentially temporally biased selection method. *Why unresolved*: While the paper demonstrates that models use irrelevant socio-demographic info, it does not disentangle whether the "higher accuracy" for disadvantaged groups is a result of the model learning real patterns in the (potentially biased) source data or purely hallucinated stereotypes. *What evidence would resolve it*: A controlled evaluation using a "ground truth" dataset independent of spontaneous reporting biases (e.g., clinical trial data) to compare against the FAERS-derived results.

### Open Question 4
What latent internal mechanisms drive implicit bias in LLMs when socio-demographic attributes are suppressed from the output reasoning? *Basis*: The paper identifies "implicit bias, where predictions are inconsistent, yet personas are not explicitly mentioned," and notes that removing explicit references improves accuracy, suggesting bias is embedded in model behavior rather than just surface-level text. *Why unresolved*: The analysis relies on external outputs and does not investigate internal model states or attention mechanisms. *What evidence would resolve it*: Mechanistic interpretability analysis (e.g., probing classifiers or attention visualization) to locate internal representations associated with socio-demographic personas during the inference process.

## Limitations
- Limited generalizability from single oncology-focused FAERS subset and small sample size
- Potential dataset-specific biases inherent in pharmacovigilance data
- Abstraction of complex socio-demographic attributes into simplified personas

## Confidence

- **High Confidence**: The identification of explicit bias through persona references in reasoning traces is well-supported by direct textual evidence in model outputs.
- **Medium Confidence**: The claim of implicit bias is plausible given the accuracy improvements after excluding identity-based reasoning, but the mechanism remains underspecified and could reflect other factors like prediction noise.
- **Low Confidence**: The generalizability of bias patterns across diverse pharmacovigilance contexts and model architectures is uncertain due to limited cross-model and cross-domain testing.

## Next Checks
1. **Cross-Model Replication**: Test additional model families (e.g., Claude, Gemini) on the identical DSD evaluation to determine if bias patterns are model-agnostic or architecture-specific.
2. **Axis Ablation Study**: Systematically remove one socio-demographic axis at a time to isolate which attributes drive the strongest explicit versus implicit bias effects.
3. **Mitigation Probing**: Add explicit instructions to "ignore socio-demographic factors" in prompts and measure whether explicit bias decreases while implicit bias persists, testing the depth of learned associations.