---
ver: rpa2
title: 'UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity'
arxiv_id: '2511.22273'
source_url: https://arxiv.org/abs/2511.22273
tags:
- uni00000013
- uni00000008
- sample
- uni00000048
- uni00000036
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies pure exploration in large-scale problems with
  non-sub-Gaussian (including heavy-tailed) distributions. Traditional approaches
  rely on Gaussian or sub-Gaussian assumptions, which limit applicability in settings
  where such assumptions fail.
---

# UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity

## Quick Facts
- arXiv ID: 2511.22273
- Source URL: https://arxiv.org/abs/2511.22273
- Authors: Zaile Li; Weiwei Fan; L. Jeff Hong
- Reference count: 40
- Primary result: Meta-UCB algorithms achieve sample optimality in large-scale pure exploration under non-sub-Gaussian distributions with bounded moments

## Executive Summary
This paper challenges the sub-Gaussian assumption prevalent in bandit algorithms by establishing sample optimality for Upper Confidence Bound (UCB) algorithms in large-scale pure exploration settings with heavy-tailed distributions. The authors introduce a meta-UCB framework where the exploration bonus depends only on an alternative's own sample size, enabling boundary-crossing dynamics that focus budget on the best alternative. They prove that this decoupled structure achieves sample optimality (non-zero probability of correct selection with linear budget) under two non-sub-Gaussian settings: bounded variance distributions and distributions with a bounded absolute moment of order q>3.

## Method Summary
The authors study pure exploration in large-scale bandit problems with k alternatives and fixed budget B, focusing on identifying the best arm. They introduce a meta-UCB algorithm where each arm's UCB value is defined as the sample mean plus a bonus term that depends only on that arm's sample size (not the global sample count). The selection rule outputs the arm with the largest sample size upon budget exhaustion. The analysis establishes a distribution-free lower bound on the probability of correct selection (PCS) using a boundary-crossing perspective, treating the best alternative's UCB process as a threshold. This framework is then applied to two non-sub-Gaussian settings: (1) location-scale structures with bounded variance, and (2) general distributions with a bounded absolute moment of order q>3.

## Key Results
- Meta-UCB algorithms with decoupled bonus functions achieve sample optimality in large-scale pure exploration, maintaining non-zero PCS as k→∞ with linear budget scaling
- Sample optimality can be achieved under heavy-tailed distributions when each alternative has a bounded absolute moment of order q>3, even without sub-Gaussian assumptions
- Coupled UCB algorithms (like UCB1) fail to achieve sample optimality due to uniform budget allocation across all alternatives in large-scale settings
- The selection rule based on largest sample size enables a clean distribution-free PCS lower bound, though empirical results show sample-mean selection performs similarly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A "decoupled" UCB structure, where the exploration bonus depends only on an alternative's own sample size (not the global sample count), ensures sample optimality in large-scale settings by preventing unnecessary resampling of inferior arms.
- **Mechanism:** In decoupled algorithms (e.g., UCBE, MOSS), once an inferior alternative's UCB value drops below the best alternative's minimum UCB value, it effectively "crosses the boundary" and stops receiving samples. This allows the algorithm to focus the remaining budget on the best alternative. In contrast, coupled algorithms (like UCB1) tie the bonus to the global sample size n, causing the UCB values of all alternatives to rise as the budget grows, forcing continuous resampling of inferior arms (over-exploration).
- **Core assumption:** The exploration bonus function f(n_i) must vanish as the sample size n_i → ∞ (Assumption 1).
- **Evidence anchors:**
  - [Section 6.3] Figures 4 and 5 show that UCB1 (coupled) allocates samples uniformly across all alternatives, whereas Meta-UCB algorithms focus budget on the best arm after boundary-crossing.
  - [Section 5.2] Discusses how the global term n in UCB1 introduces coupling that breaks the boundary-crossing dynamic.
  - [corpus] Related work "On Instability of Minimax Optimal Optimism-Based Bandit Algorithms" supports the general difficulty of inference in adaptive sampling, highlighting why structural decoupling helps stability.
- **Break condition:** If the bonus function depends on global statistics (like total samples n), the boundary-crossing property fails, leading to uniform allocation and loss of sample optimality (PCS → 0 as k → ∞).

### Mechanism 2
- **Claim:** Sample optimality can be achieved under heavy-tailed distributions (non-sub-Gaussian) if the distribution possesses a bounded absolute moment of order q > 3.
- **Mechanism:** Even without sub-Gaussian tails, the existence of a q-th moment (q>3) allows the use of Nagaev's inequality to derive finite bounds on the expectation and variance of "boundary-crossing times" (the time it takes for an inferior arm's UCB to drop below the threshold). Finite expectations allow the application of Kolmogorov's Strong Law of Large Numbers, proving that the total required budget scales linearly with k.
- **Core assumption:** Assumption 3 holds: max_i E[|X_i|^q] ≤ M for some q > 3 and M < ∞.
- **Evidence anchors:**
  - [Abstract] Explicitly states sample optimality is achieved when each alternative has a bounded absolute moment of order q > 3.
  - [Section 4.3.2] Lemma 6 and Lemma 7 establish concentration and moment bounds for boundary-crossing times, leading to Theorem 2.
  - [corpus] "Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity" suggests ongoing interest in moment-based bounds, validating the theoretical approach.
- **Break condition:** If the distribution has infinite variance or moments of order q ≤ 3, the variance of the boundary-crossing times may become infinite, preventing the application of SLLN and potentially breaking the linear budget scaling guarantee.

### Mechanism 3
- **Claim:** Selecting the best alternative based on the **largest sample size** (rather than the largest sample mean) facilitates a distribution-free lower bound on the Probability of Correct Selection (PCS).
- **Mechanism:** By treating the sampling process as a boundary-crossing problem, the algorithm guarantees that the best alternative accumulates samples indefinitely while inferior ones stop. Thus, the event "Best alternative has largest sample size" is directly linked to the boundary-crossing times of inferior arms, allowing the derivation of a clean lower bound: PCS ≥ P(B > 2∑T_i).
- **Core assumption:** Assumption: The selection rule relies on the count of samples rather than the value of the sample mean.
- **Evidence anchors:**
  - [Section 3.1] Equation (4) derives the PCS lower bound based on the summation of boundary-crossing times, enabled by the largest-sample-size selection rule.
  - [Section 3.1] Text states: "the selection standard of the largest sample size enables us to establish a clean PCS lower bound... notably, this PCS lower bound is distribution-free."
  - [corpus] Weak corpus evidence regarding this specific selection heuristic in large-scale contexts; this appears to be a specific analytical contribution of this paper.
- **Break condition:** While the mechanism is robust, the paper notes that selecting by sample mean is empirically comparable (Section EC.2), but theoretically, the sample-size rule is required for the specific distribution-free proof presented.

## Foundational Learning

- **Concept:** Sub-Gaussianity vs. Heavy-Tailed Distributions
  - **Why needed here:** The paper challenges the standard assumption that rewards must be sub-Gaussian (tails decay like e^{-x^2}). You must understand that heavy-tailed distributions (like Pareto or Student's t) have "fat tails" that increase the probability of extreme observations, which complicates concentration bounds.
  - **Quick check question:** Does a Pareto distribution with shape parameter 3 satisfy the conditions of this paper?

- **Concept:** Large-Scale Asymptotics (k → ∞)
  - **Why needed here:** The paper defines "sample optimality" specifically for the regime where the number of alternatives (k) approaches infinity. This differs from standard fixed-k bandit problems. The goal is to maintain a non-zero PCS with a budget linear in k (B=ck).
  - **Quick check question:** If the total budget B scales as O(k^2), is the algorithm still considered "sample optimal" in this paper's definition?

- **Concept:** Boundary-Crossing Time
  - **Why needed here:** This is the core analytical tool. It represents the number of samples required for a process (the UCB value of an inferior arm) to cross a threshold (the best arm's UCB) and effectively "drop out" of contention.
  - **Quick check question:** If the boundary-crossing time is infinite, what does that imply for the algorithm's budget allocation?

## Architecture Onboarding

- **Component map:**
  1. Alternatives: k arms with unknown heavy-tailed distributions
  2. UCB Calculator: Computes U_i(n_i) = X̄_i(n_i) + f(n_i). Crucially, f must not depend on global time n
  3. Sampler: Selects s = argmax_i U_i(n_i)
  4. Sample Counter: Tracks n_i for each arm
  5. Selector: Upon budget exhaustion, returns b = argmax_i n_i

- **Critical path:**
  1. Initialize all arms with 1 sample
  2. Loop until total samples reach Budget B:
      * Calculate decoupled UCB for all arms
      * Pull arm with highest UCB
      * Update sample mean and count for that arm
  3. Return the arm with the highest pull count

- **Design tradeoffs:**
  - Decoupled vs. Coupled Bonus: Decoupled bonuses (e.g., √(a/n_i)) ensure sample optimality but may under-explore if the bonus vanishes too quickly. Coupled bonuses (UCB1) over-explore in large-scale settings
  - Selection Rule: Selecting by sample size simplifies theoretical guarantees but ignores the actual sample mean values. The paper suggests empirical performance is similar to selecting by sample mean, but the guarantee is strictly for sample size selection

- **Failure signatures:**
  - Uniform Allocation: If the histogram of sample counts shows all arms being sampled roughly equally (flat distribution) as k grows, the algorithm is likely failing to differentiate the best arm or using a coupled UCB (like UCB1)
  - PCS Decay to Zero: If PCS → 0 as k → ∞ despite a linear budget B=ck, the bonus function is likely insufficient for the tail heaviness (q is too low or bonus is too small)
  - Budget Exhaustion on Bad Arms: If the sum of boundary-crossing times exceeds the budget, the algorithm has not eliminated inferior arms fast enough

- **First 3 experiments:**
  1. Validation of Sample Optimality: Run Meta-UCB (e.g., UCBE) and UCB1 on a Lognormal distribution with k ∈ {2^5, ..., 2^{15}} and B=100k. Verify that UCBE's PCS stabilizes while UCB1's drops
  2. Boundary Crossing Visualization: Run a single replication with k=128 and visualize the sample allocation stream (like Figure 4). Check if inferior arms stop being sampled after a certain round (boundary crossing)
  3. Moment Sensitivity: Test Pareto distributions with varying shape parameters (affecting q). Verify if sample optimality holds when q > 3 and breaks or degrades as q approaches 3 (theoretical limit)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the moment condition requirement of q > 3 be relaxed to include distributions with only lower-order finite moments (e.g., q ≤ 2)?
- Basis in paper: [explicit] The concluding remarks state: "preliminary numerical experiments suggest that UCB algorithms may remain sample optimal even when only low-order moments (e.g., q ≤ 2) are finite, raising the question of whether the moment condition q > 3 can be further relaxed."
- Why unresolved: The current proof techniques rely on Nagaev-type concentration inequalities and Kolmogorov's SLLN, both requiring q > 3 to ensure finite variance bounds on boundary-crossing times.
- What evidence would resolve it: A modified proof showing linear budget scaling with q ≤ 2, or counterexamples demonstrating failure of sample optimality below a critical threshold.

### Open Question 2
- Question: Can sample optimality be formally established for UCB algorithms using robust mean estimators (e.g., truncated or weighted sample means) designed for heavy-tailed distributions?
- Basis in paper: [explicit] Section 5.2 states: "we conjecture that sample optimality may still hold for a broader class of UCB algorithms, provided that their UCB value construction satisfies a form of decoupling... verifying the conditions for specific UCB algorithms and distributional assumptions will likely require case-by-case analysis, which we leave for future work."
- Why unresolved: The boundary-crossing analysis framework assumes UCB values of the form X̄_i(n_i) + f(n_i), which does not cover robust estimators whose confidence bounds may depend on sample observations beyond just the sample mean.
- What evidence would resolve it: A proof extending Theorems 1 and 2 to robust UCB variants, or identification of failure cases where decoupling alone is insufficient.

### Open Question 3
- Question: Can UCB algorithms achieve consistency (PCS → 1) when the sampling budget grows superlinearly with k?
- Basis in paper: [explicit] Section 7 states: "we have not discussed the issue of consistency, which requires that the PCS converges to 1 as the total sampling budget grows faster than the number of alternatives. This may be achieved by incorporating an explore-first framework."
- Why unresolved: The current analysis establishes only non-zero PCS asymptotically (sample optimality), not convergence to 1. The boundary-crossing framework yields PCS lower bounds that remain bounded away from 1.
- What evidence would resolve it: Derivation of conditions under which modified UCB algorithms achieve lim_{k→∞} PCS = 1 with B = ω(k), potentially through explore-first modifications or adaptive bonus functions.

## Limitations

- The theoretical guarantees depend critically on the decoupling condition, which excludes many practical UCB variants like UCB1
- The moment condition requirement of q > 3 may be too restrictive for many real-world heavy-tailed distributions
- Empirical validation is limited to synthetic distributions with known parameters rather than real-world data
- The selection rule based on largest sample size is primarily justified for theoretical convenience rather than demonstrated practical superiority

## Confidence

- **High confidence**: The core claim that coupled UCB algorithms (like UCB1) fail to achieve sample optimality in large-scale settings due to uniform budget allocation
- **Medium confidence**: The distribution-free lower bound on PCS and the extension to the mean gap scenario
- **Medium confidence**: The heavy-tailed distribution results (bounded q-th moment for q>3)

## Next Checks

1. **Tie-breaking behavior**: Systematically test different tie-breaking rules (random, smallest index, largest index) in the meta-UCB implementation to verify that the theoretical guarantees hold regardless of tie resolution

2. **Moment boundary testing**: Generate Pareto distributions with shape parameters just above and below 3 (e.g., 3.1, 3.01, 2.99) and test whether the sample optimality degrades or breaks exactly at q=3, confirming the theoretical boundary

3. **Selection rule comparison**: Run extensive experiments comparing the sample-size selection rule against sample-mean selection across all tested distributions, measuring both PCS and budget allocation patterns to quantify any practical differences