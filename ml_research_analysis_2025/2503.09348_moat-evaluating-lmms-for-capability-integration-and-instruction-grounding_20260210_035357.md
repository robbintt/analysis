---
ver: rpa2
title: 'MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding'
arxiv_id: '2503.09348'
source_url: https://arxiv.org/abs/2503.09348
tags:
- capabilities
- moat
- lmms
- times
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOAT evaluates LMMs on 1005 complex real-world vision tasks requiring
  integration of up to 9 vision-language capabilities, including instruction grounding.
  It uses a fine-grained capability taxonomy to identify LMM weaknesses, finding that
  the best model (Gemini 2.5 Pro) achieves only 44% accuracy versus 83% for humans.
---

# MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding

## Quick Facts
- **arXiv ID:** 2503.09348
- **Source URL:** https://arxiv.org/abs/2503.09348
- **Reference count:** 40
- **Key outcome:** MOAT evaluates LMMs on 1005 complex real-world vision tasks requiring integration of up to 9 vision-language capabilities, including instruction grounding. It uses a fine-grained capability taxonomy to identify LMM weaknesses, finding that the best model (Gemini 2.5 Pro) achieves only 44% accuracy versus 83% for humans. The benchmark reveals consistent failures in counting, spatial reasoning, and instruction grounding, while also showing that test-time reasoning often harms visual understanding tasks.

## Executive Summary
MOAT is a comprehensive benchmark designed to evaluate large multimodal models (LMMs) on complex real-world vision tasks that require integrating multiple vision-language capabilities. The benchmark consists of 1005 tasks organized into 5 super-classes and 26 sub-classes, with tasks requiring integration of up to 9 capabilities. MOAT uses a fine-grained capability taxonomy to identify specific weaknesses in LMMs and provides insights into how models handle instruction grounding, reasoning, and complex visual understanding tasks.

## Method Summary
MOAT constructs a comprehensive evaluation framework by first establishing a detailed capability taxonomy that spans 5 super-classes (Language Comprehension, Visual Understanding, Instruction Comprehension, Information Extraction, and Integration) and 26 sub-classes. The benchmark curates 1005 real-world images paired with complex instructions that require multiple capabilities to solve, with some tasks requiring integration of up to 9 different capabilities. Evaluation is performed through single-shot generation with responses judged by trained annotators on a 5-point scale, where scores ≥3 indicate correct responses. The benchmark also introduces a novel metric called gAP (general Average Precision) that accounts for the partial credit nature of multi-capability tasks by measuring capability completeness.

## Key Results
- The best-performing model (Gemini 2.5 Pro) achieves only 44% accuracy compared to 83% human performance on MOAT tasks
- LMMs show consistent failures in counting, spatial reasoning, and instruction grounding capabilities
- Test-time reasoning improves performance on simple perception tasks but harms visual understanding tasks
- The gAP metric reveals that LMMs achieve an average capability completeness score of 51%, indicating substantial gaps in integrated reasoning

## Why This Works (Mechanism)
MOAT's effectiveness stems from its comprehensive approach to evaluating LMMs on real-world tasks that require genuine capability integration rather than isolated skill assessment. The benchmark's fine-grained capability taxonomy allows for precise identification of model weaknesses across different vision-language reasoning dimensions. By using complex instructions that require multiple capabilities to be integrated, MOAT reveals fundamental limitations in how LMMs handle multi-step reasoning and instruction grounding. The single-shot evaluation protocol provides a realistic assessment of model capabilities without the benefit of extensive prompting or fine-tuning.

## Foundational Learning
**Capability Integration:** The ability to combine multiple vision-language skills to solve complex tasks. Why needed: Real-world vision tasks rarely require isolated capabilities. Quick check: Verify that task solutions require at least 2-3 different capabilities.

**Instruction Grounding:** The process of correctly interpreting and executing complex instructions on visual inputs. Why needed: LMMs must understand and follow human instructions to be practically useful. Quick check: Ensure instructions require multiple reasoning steps and cannot be solved by simple pattern matching.

**Visual Understanding:** The capacity to extract and reason about visual information from images. Why needed: Fundamental for any vision-language model. Quick check: Confirm tasks require genuine visual analysis rather than text-based reasoning.

**Reasoning Integration:** The ability to combine visual understanding with logical reasoning. Why needed: Complex tasks require both perception and reasoning. Quick check: Validate that correct solutions require both visual and logical inference steps.

**Capability Taxonomy:** A hierarchical classification system for organizing vision-language capabilities. Why needed: Enables systematic evaluation and identification of model weaknesses. Quick check: Verify that all 26 sub-classes are represented and mutually exclusive.

## Architecture Onboarding
**Component Map:** Image Input -> Visual Encoder -> Text Encoder -> Instruction Parser -> Capability Integrator -> Response Generator -> Output

**Critical Path:** Visual understanding → Instruction comprehension → Capability integration → Response generation

**Design Tradeoffs:** Single-shot evaluation provides realistic assessment but limits optimization opportunities; comprehensive capability taxonomy enables detailed analysis but increases complexity; real-world images provide ecological validity but introduce evaluation consistency challenges.

**Failure Signatures:** Consistent failures in counting tasks (especially for >5 objects), spatial reasoning errors (particularly with complex spatial relationships), and instruction grounding issues (misinterpreting multi-step instructions or missing key requirements).

**First Experiments:**
1. Evaluate model performance on single-capability tasks versus multi-capability integration tasks to quantify the difficulty increase
2. Compare reasoning trajectories for tasks where models succeed versus fail to identify patterns in reasoning quality
3. Test model sensitivity to prompt variations across different task types to assess evaluation robustness

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark relies on single-shot generation evaluation, making it difficult to distinguish between model limitations and prompt sensitivity
- Human performance evaluation was conducted with only 3 participants without reporting inter-rater reliability metrics
- The test-time reasoning analysis shows mixed effects but lacks systematic measurement of reasoning quality or correlation with task difficulty

## Confidence
- **High confidence:** LMMs show substantial gaps in complex real-world vision tasks (44% vs 83% human performance); specific capability weaknesses in counting, spatial reasoning, and instruction grounding are consistently observed
- **Medium confidence:** The fine-grained capability taxonomy effectively reveals model weaknesses; reasoning often helps for simple perception but harms visual understanding tasks
- **Low confidence:** The exact mechanisms by which reasoning affects different task types; the generalizability of human performance metrics; the impact of evaluation settings (single-shot vs. few-shot)

## Next Checks
1. Conduct a controlled study with multiple human raters and established inter-rater reliability metrics to validate the human performance baseline
2. Systematically test the effect of prompt variations (different phrasings, temperature settings, few-shot examples) on model performance across all task types
3. Implement a detailed analysis pipeline for reasoning trajectories to quantify the relationship between reasoning quality, path length, and final accuracy across task difficulty levels