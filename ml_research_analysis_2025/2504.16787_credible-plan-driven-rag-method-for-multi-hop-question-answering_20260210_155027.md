---
ver: rpa2
title: Credible Plan-Driven RAG Method for Multi-Hop Question Answering
arxiv_id: '2504.16787'
source_url: https://arxiv.org/abs/2504.16787
tags:
- reasoning
- arxiv
- complexity
- semantic
- par-rag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAR-RAG introduces a three-stage Plan-then-Act-and-Review framework
  to address the dual challenges of reasoning trajectory stability and factual consistency
  in multi-hop QA. By incorporating semantic complexity into exemplar selection and
  verification, the method dynamically aligns decomposition granularity with task
  difficulty and adapts verification strength accordingly.
---

# Credible Plan-Driven RAG Method for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2504.16787
- Source URL: https://arxiv.org/abs/2504.16787
- Reference count: 40
- Primary result: PAR-RAG achieves 16.39% EM improvement on 2WikiMultiHopQA

## Executive Summary
PAR-RAG introduces a three-stage Plan-then-Act-and-Review framework to address the dual challenges of reasoning trajectory stability and factual consistency in multi-hop QA. By incorporating semantic complexity into exemplar selection and verification, the method dynamically aligns decomposition granularity with task difficulty and adapts verification strength accordingly. Experimental results show PAR-RAG outperforms competitive baselines across multiple benchmarks, with relative EM improvements of 16.39% on 2Wiki, 5.17% on HotpotQA, 6.45% on MuSiQue, and 9.09% on TriviaQA. Ablation studies confirm the complementary effects of complexity-aware planning and dual verification. The method offers a generalizable, cognitively inspired approach for reliable multi-hop reasoning.

## Method Summary
PAR-RAG is a Plan-then-Act-and-Review (PAR) framework for multi-hop question answering that uses a BERT-based classifier to predict query complexity (hop count) and conditions exemplar selection and verification strength accordingly. The method first generates a reasoning plan by retrieving complexity-matched exemplars, then executes the plan through dense retrieval, citation extraction, and LLM-based reasoning, and finally applies adaptive dual verification (LLM accuracy scoring + AttrScore credibility assessment) with iterative refinement when confidence falls below threshold. The framework explicitly models semantic complexity through linguistic features and aligns decomposition granularity with task difficulty, while dynamically adjusting verification weights based on query complexity to balance accuracy and factual consistency.

## Key Results
- Achieves 16.39% relative EM improvement on 2WikiMultiHopQA, 5.17% on HotpotQA, 6.45% on MuSiQue, and 9.09% on TriviaQA
- Demonstrates complementary effects of complexity-aware planning and dual verification in ablation studies
- Shows 75.82% recovery rate when initial hop count predictions are incorrect

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Conditioned Planning
The framework predicts query complexity (hop count) using a BERT classifier with linguistic features, then retrieves exemplars matching that complexity to guide plan generation. This alignment of exemplar difficulty with target difficulty stabilizes the reasoning trajectory and reduces plan misspecification. The core assumption is that hop count serves as a valid proxy for semantic complexity, and performance improves when exemplar difficulty matches task difficulty. Break condition occurs if the classifier mispredicts hop counts, causing exemplar misalignment.

### Mechanism 2: Adaptive Dual Verification
The system computes a confidence score as a weighted sum of LLM accuracy and AttrScore credibility, with weights determined by predicted hop count. Simple queries prioritize accuracy while complex queries emphasize multi-evidence consistency. The core assumption is that LLMs can reliably judge accuracy and AttrScore captures factual consistency, with complex queries requiring stricter consistency checks. Break condition occurs if the LLM-as-judge is miscalibrated or AttrScore fails to detect hallucinations.

### Mechanism 3: Iterative Trajectory Refinement
The framework integrates verification feedback into execution, triggering sparse re-retrieval and answer revision when confidence falls below threshold, and crucially refining the next sub-question based on the corrected current answer. This prevents error propagation across multi-hop steps. The core assumption is that correcting intermediate steps provides better anchors for subsequent reasoning. Break condition occurs if refinement alters the semantic intent of the original sub-question.

## Foundational Learning

- **Semantic Complexity Estimation**: Required because planning and verification modules depend on quantified complexity scores to select exemplars and set thresholds. Quick check: Does the model use raw token length or linguistic features (syntactic depth, entropy) to predict complexity? (Answer: Linguistic features + BERT embeddings).

- **LLM-as-a-Judge**: Required because the verification module relies on an LLM to score intermediate answer accuracy. Quick check: Is the LLM judge used here for final answer grading or intermediate step validation? (Answer: Intermediate step validation).

- **Attribution/Fact Verification**: Required because the credibility component needs checking if claims are supported by citations. Quick check: What are the three output states of AttrScore used in this architecture? (Answer: Contradictory, Extrapolatory, Attributable).

## Architecture Onboarding

- **Component map**: Input Query -> Complexity Classifier -> Exemplar Selection -> Plan Generation -> Executor (Dense/Sparse Retrieval + Reader LLM) -> Verifier (LLM-Judge + AttrScore) -> Confidence Score -> Refinement Loop -> Final Answer
- **Critical path**: Complexity Prediction -> Exemplar Selection -> Plan Generation. If initial complexity prediction is incorrect, downstream verification weights and plan granularity will be misaligned.
- **Design tradeoffs**: High latency (14-21s per query) versus reliability through multi-stage verification; static plan first with adaptive verification and re-retrieval during execution.
- **Failure signatures**: High "I don't know" rate indicates sparse retrieval failure after verification failure; reasoning drift occurs from plan misspecification (27% of errors).
- **First 3 experiments**: (1) Validate classifier accuracy >80% on held-out set before enabling adaptive verification; (2) Ablation with fixed Î±=0.5 versus adaptive logistic function; (3) Force verification failure to confirm sparse re-retrieval and RefineNextQuestion triggers correctly.

## Open Questions the Paper Calls Out

- Can adaptive or self-supervised complexity estimation methods replace the fixed BERT classifier to reduce misclassification rates, particularly for two-hop questions? (Section 6: "Future work could explore adaptive or self-supervised complexity estimation," Section 5.2.3: "need for more reliable methods of estimating semantic complexity").

- How can verification mechanisms be designed to ensure reliability without relying on the potentially biased or irreproducible "LLM-as-a-judge" approach? (Section 6: "how to design reliable, interpretable verification mechanisms that go beyond LLM self-assessment").

- What adaptive computation strategies can dynamically balance the latency/trustworthiness trade-off in the Plan-then-Act-and-Review cycle for real-time applications? (Section 6: "efficiency-trustworthiness trade-off as a key direction for future research," Section 5.2.7: high latency).

## Limitations

- Semantic complexity classifier shows systematic confusion between 2-hop and 1-hop queries (18.68% error rate), suggesting hop count may not fully capture reasoning complexity
- Framework depends on proprietary GPT-4o APIs, raising reproducibility concerns and questions about transfer to open-source LLMs
- Despite verification mechanisms, 27% of errors still stem from plan misspecification, indicating the framework doesn't fully eliminate reasoning drift

## Confidence

- **High confidence**: Experimental results outperforming baselines on all four datasets; ablation study confirming complementary effects
- **Medium confidence**: Claims about cognitive plausibility; latency analysis based on single timing experiment
- **Low confidence**: Generalizability beyond tested datasets; assertion that framework "eliminates" reasoning drift

## Next Checks

1. **Open-Source Transfer**: Replace GPT-4o with Llama-3-70B and evaluate whether PAR-RAG maintains performance advantages, particularly testing if complexity classifier error patterns change with different model behaviors

2. **Verification Calibration**: Systematically inject known factual errors at different complexity levels to measure false negative rate of both LLM-as-judge and AttrScore components, ensuring adaptive confidence threshold maintains appropriate precision

3. **Plan Stability Under Uncertainty**: Deliberately misclassify query complexity and measure EM score degradation to quantify cost of hop count prediction errors and test framework robustness