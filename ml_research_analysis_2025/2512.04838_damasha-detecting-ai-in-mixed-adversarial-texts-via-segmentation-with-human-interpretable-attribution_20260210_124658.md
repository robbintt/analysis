---
ver: rpa2
title: 'DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable
  Attribution'
arxiv_id: '2512.04838'
source_url: https://arxiv.org/abs/2512.04838
tags:
- text
- arxiv
- detection
- adversarial
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting and segmenting
  AI-generated text within mixed human-AI collaborative texts, particularly under
  adversarial conditions. The proposed Info-Mask framework combines stylometric features
  (perplexity, POS tags, punctuation density, lexical diversity, readability) with
  a soft attribution masking mechanism to dynamically modulate token representations,
  enhancing robustness to syntactic adversarial attacks.
---

# DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution

## Quick Facts
- **arXiv ID:** 2512.04838
- **Source URL:** https://arxiv.org/abs/2512.04838
- **Reference count:** 37
- **Primary result:** Info-Mask framework achieves SBDA@0.3 of 45.75% and segment precision of 41.43% on MAS benchmark for mixed human-AI text segmentation under adversarial conditions

## Executive Summary
This paper introduces DAMASHA, a framework for detecting and segmenting AI-generated text within mixed human-AI collaborative texts, particularly under adversarial conditions. The proposed Info-Mask framework combines stylometric features (perplexity, POS tags, punctuation density, lexical diversity, readability) with a soft attribution masking mechanism to dynamically modulate token representations, enhancing robustness to syntactic adversarial attacks. The method is evaluated on a large benchmark dataset (MAS) and shows significant improvements in span-level segmentation accuracy compared to prior baselines. The framework also provides human-interpretable attribution maps, validated through human studies, enabling explainable detection.

## Method Summary
The DAMASHA framework uses a dual-encoder architecture with a novel Info-Mask module that applies soft gating based on stylometric features. Input tokens are processed by a first transformer (RoBERTa) to obtain contextualized embeddings, which are then modulated by the Info-Mask. The Info-Mask extracts five stylometric features per token (perplexity, POS density, punctuation density, lexical diversity, readability), applies multi-head attention across the feature space, and generates scalar mask values via sigmoid normalization. These masks element-wise scale the encoder outputs before a second transformer (ModernBERT) refines the representations. Finally, a CRF layer decodes the sequence into human/AI labels, enforcing label transition consistency. The model is trained on the MAS benchmark with 671,013 samples across 7 attack variants, using IoU-based span matching metrics (SBDA, SegPre) at various thresholds.

## Key Results
- Info-Mask achieves SBDA@0.3 of 45.75% and segment precision of 41.43% on the MAS benchmark, outperforming prior baselines including zero-shot and statistical methods.
- The framework demonstrates robustness to syntactic adversarial attacks (misspelling, character substitution, invisible characters, punctuation swap, case swap), though performance degrades on unseen attack types.
- Human studies validate the interpretability of the attribution maps, showing that the framework can provide explainable detection of AI-generated spans.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stylometric soft-masking improves robustness to syntactic adversarial attacks by selectively emphasizing structurally salient tokens while down-weighting perturbation artifacts.
- **Core assumption:** Adversarial syntactic perturbations alter surface-level token patterns in ways that stylometric features can detect, even when semantic content remains intact.
- **Evidence:** Info-Mask outperforms all masking variants (gradient-based, uncertainty-based, purification-based, attention-based gating) in ablation studies, with performance hierarchy: Info-Mask > RMC-iv > RMC-iii > RMC-v > RMC-ii > RMC-i.
- **Break condition:** If adversarial attacks evolve to mimic human stylometric distributions (e.g., LLMs trained to match perplexity patterns), this mechanism's effectiveness would degrade.

### Mechanism 2
- **Claim:** CRF-structured decoding enforces label transition consistency, reducing spurious boundary predictions in mixed-authorship spans.
- **Core assumption:** Authorship boundaries in mixed human-AI text follow structured patterns (e.g., single transition points rather than rapid oscillation), and this structure persists under adversarial perturbation.
- **Evidence:** Transformer-only + CRF configurations show improved SBDA@0.3 (32.0%-41.3%) vs. BiLSTM+CRF baseline (26.1%), suggesting CRF benefits compound with better encoders.
- **Break condition:** If adversarial attacks insert realistic-looking micro-boundaries (rapid human-AI alternation), CRF constraints could mask genuine attacks.

### Mechanism 3
- **Claim:** Dual-encoder architecture (RoBERTa + ModernBERT) captures complementary linguistic representations that improve generalization across attack types.
- **Core assumption:** Different transformer architectures encode stylistic signals differently, and their combination provides more robust features than either alone.
- **Evidence:** RoBERTa + ModernBERT + CRF* achieves highest SBDA@0.3 (45.7%) and SegPre@0.3 (41.4%), outperforming single-encoder and BiGRU variants.
- **Break condition:** The computational overhead (3% memory increase, 2.5hr/epoch training) may be prohibitive for deployment; single-encoder ModernBERT + CRF* (41.3% SBDA) offers 90% of performance at lower cost.

## Foundational Learning

- **Conditional Random Fields (CRF) for sequence labeling:** Why needed here? The paper uses CRF decoding for span-level boundary detection; understanding why CRF outperforms softmax classification requires grasping global normalization vs. local independence assumptions.
  - Quick check question: Given a sequence of token labels [Human, AI, Human, AI], would a CRF penalize this differently than independent token classification? Why?

- **Stylometry and authorship attribution features:** Why needed here? Info-Mask relies on five handcrafted features; practitioners need to understand what these capture and their limitations (e.g., perplexity requires access to reference language model, readability scores are language-specific).
  - Quick check question: If an AI text is paraphrased by another LLM, which stylometric features would likely change most? Which would remain stable?

- **Intersection-over-Union (IoU) for span evaluation:** Why needed here? The paper introduces SBDA and Segment Precision using IoU thresholds; understanding why token-level accuracy (98%) doesn't reflect span-level performance (45%) requires grasping partial overlap metrics.
  - Quick check question: A predicted span covers tokens 10-20; ground truth spans tokens 8-18. What is the IoU? Would this match at τ=0.5? At τ=0.7?

## Architecture Onboarding

- **Component map:**
  Input tokens → Transformer 1 (RoBERTa) → raw contextualized embeddings
               → Style Feature Extractor → 5-dim feature vector per token
               → Multi-head Attention over style features → attention-weighted style
               → Sigmoid projection → Info-Mask scalars m_i ∈ [0,1]
               → Element-wise multiplication → masked embeddings m_i · z_i
               → Transformer 2 (ModernBERT) → refined representations
               → CRF Layer → label sequence (Human/AI per token)

- **Critical path:** The Info-Mask generation (style extraction → attention → sigmoid) is the novel contribution. If this module is bypassed, performance drops from 45.7% to 39.0% SBDA@0.3 (RMC + All Opt without Info-Mask).

- **Design tradeoffs:**
  - **Masking variants:** Info-Mask outperforms gradient-based attribution (RMC-i: 34%) and uncertainty-based gating (RMC-iii: 40%), but requires handcrafted features. Consider: Is domain-specific feature engineering acceptable, or must the system be fully learned?
  - **Encoder selection:** ModernBERT + CRF* alone achieves 41.3% SBDA with lower overhead than dual-encoder (45.7%). The 4.4% gain may not justify 3% memory increase and doubled forward pass.
  - **Feature set:** Ablation shows lexical diversity and POS tags most important; perplexity adds marginal gains. If inference latency is critical, consider reduced feature set.

- **Failure signatures:**
  - **Low SBDA but high token accuracy:** Indicates boundary precision issues (off-by-few-tokens errors); Table 9 shows 46% of errors are off-by-1 token. Consider relaxing IoU thresholds or adding boundary-aware loss.
  - **High performance on clean text, low on adversarial:** Model overfitting to attack types in training; Table 6 (unseen attacks) shows 9% drop when testing on held-out attack types.
  - **Invisible character attack degradation:** Table 7 shows lowest performance (32% SBDA) on invisible character attacks; tokenizer-specific preprocessing may help.

- **First 3 experiments:**
  1. **Baseline ablation:** Train ModernBERT + CRF* without Info-Mask on MAS dataset subset (10K samples). Measure SBDA@0.3, training time, memory. Compare to paper's 41.3% to validate reproduction.
  2. **Feature importance validation:** Train with single stylometric features in isolation (perplexity-only, POS-only, etc.) to verify paper's claim that lexical diversity and POS contribute most. Paper reports 36-39% SBDA for individual features; confirm this hierarchy.
  3. **Attack generalization test:** Train on 5 attack types, hold out "invisible character" attacks. Paper shows 33% vs. 42% SBDA when unseen attacks are included/excluded. Replicate to assess robustness claims before investing in full dual-encoder implementation.

## Open Questions the Paper Calls Out

- **Can the Info-Mask framework maintain segmentation accuracy against semantic adversarial attacks, such as advanced paraphrasing, which were excluded from the MAS benchmark?**
  - The model's soft-masking relies heavily on surface-level stylometric cues (punctuation, POS density) which semantic attacks are specifically designed to preserve or mimic, potentially bypassing the Info-Mask's gating mechanism.

- **How does the reliance on language-specific stylometric features affect the transferability of the Info-Mask to low-resource languages?**
  - The Info-Mask depends on features like perplexity and Part-of-Speech (POS) tags which require robust, pre-trained language models and tokenizers that are often unavailable or unreliable for low-resource languages.

- **Can the soft attribution masking mechanism be generalized to detect AI-generated spans in multimodal content, such as speech or images?**
  - The current architecture is strictly text-based, relying on discrete token inputs and textual stylometric signals (lexical diversity, punctuation) that do not have direct equivalents in continuous audio or visual data streams.

## Limitations
- The method is tailored only for English and may not generalize well to low-resource languages without adaptation.
- Computational overhead of the dual-encoder architecture (3% memory increase, 2.5 hours per epoch) may be prohibitive for real-time or resource-constrained deployment.
- Performance degrades on unseen attack types, with a 9-12% drop in SBDA when tested on attacks not present during training, particularly for invisible character attacks (performance drops to 32% SBDA).

## Confidence
- **High Confidence:** The core findings on stylometric masking effectiveness (45.75% SBDA@0.3), CRF contribution to boundary detection, and the overall framework architecture are well-supported by ablation studies and cross-validation results.
- **Medium Confidence:** The human-interpretability claims and attribution maps require validation through the human study referenced but not detailed in the main text. The dual-encoder architecture benefits are suggested by performance gains but lack direct corpus evidence.
- **Low Confidence:** The long-term robustness against evolving adversarial attacks is speculative, as the paper only tests against known attack types and does not address potential future attack vectors.

## Next Checks
1. **Cross-Attack Generalization:** Train the model on all attack types except "invisible character" attacks, then evaluate performance specifically on this held-out attack type to verify the 9% performance drop claim and assess robustness to novel attack vectors.
2. **Single-Encoder Efficiency Trade-off:** Implement and train the ModernBERT + CRF* configuration (41.3% SBDA) to empirically measure the 4.4% performance gap versus the dual-encoder setup, weighing this against the 90% reduction in computational overhead.
3. **Feature Ablation Replication:** Systematically train models with individual stylometric features (perplexity-only, POS-only, etc.) to confirm the paper's hierarchy of feature importance and validate that lexical diversity and POS tags contribute most to performance.