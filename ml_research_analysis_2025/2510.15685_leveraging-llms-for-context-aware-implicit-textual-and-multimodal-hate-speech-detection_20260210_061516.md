---
ver: rpa2
title: Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech
  Detection
arxiv_id: '2510.15685'
source_url: https://arxiv.org/abs/2510.15685
tags:
- context
- hate
- speech
- text
- embed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the use of Large Language Models (LLMs)\
  \ as dynamic knowledge bases to generate background context for Hate Speech Detection\
  \ (HSD). Two context generation strategies\u2014named entities and full-text prompting\u2014\
  are evaluated alongside four incorporation methods: text concatenation, embedding\
  \ concatenation, hierarchical transformer-based fusion, and LLM-driven text enhancement."
---

# Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection

## Quick Facts
- arXiv ID: 2510.15685
- Source URL: https://arxiv.org/abs/2510.15685
- Reference count: 0
- Primary result: LLM-generated context improves hate speech detection F1 by up to 6 points in multimodal misogyny detection and 3 points in implicit textual hate speech.

## Executive Summary
This study investigates using Large Language Models as dynamic knowledge bases to generate background context for Hate Speech Detection. The authors evaluate two context generation strategies—named entities and full-text prompting—alongside four incorporation methods, including embedding concatenation and hierarchical transformer fusion. Experiments on Latent Hatred (textual implicit hate speech) and MAMI (multimodal misogyny) datasets demonstrate that LLM-generated context significantly improves detection performance, with embedding-level concatenation consistently outperforming other methods. The approach addresses the challenge of detecting implicit hate speech that relies on external knowledge, though it also introduces risks of semantic drift where context can neutralize explicit hate or hallucinate associations in benign posts.

## Method Summary
The method generates context for hate speech detection using Gemini 2.0 Flash LLM with either full-text or named-entity prompting strategies. Posts and context are encoded separately using SBERT embeddings, then combined via embedding concatenation. A three-layer MLP classifier processes the 1536-dimensional concatenated vectors. The approach is evaluated on Latent Hatred (tweets) and MAMI (memes with OCR text and image descriptions) datasets, comparing performance against zero-context baselines and alternative fusion methods.

## Key Results
- LLM-generated context improves detection performance by up to 3 F1 points in textual implicit hate speech and 6 F1 points in multimodal misogyny detection.
- Embedding-level concatenation consistently outperforms text-level fusion and hierarchical transformer fusion methods.
- Full-text prompting generates more useful context than named-entity extraction, particularly for implicit hate speech detection.
- Semantic drift occurs in 3.6% of cases as false negatives and 4.4% as false positives when context is added.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Full-text prompting generates more useful context than named-entity extraction alone.
- **Mechanism:** LLMs given complete input text can infer relevant cultural, political, or social background that entity-based approaches miss or retrieve incorrectly. Full-text context captures relational meaning, not just entity definitions.
- **Core assumption:** LLMs encode sufficient world knowledge about hate speech contexts to generate relevant background.
- **Evidence anchors:**
  - Named-entity models underperformed in all configurations except LLM Enhance when compared to Full-Text counterparts.
  - Related work shows entity-linking can reduce multi-class performance by up to 11 points due to noisy/irrelevant links.
- **Break condition:** When input text lacks entities or when entities are ambiguous, entity-based approaches fail; full-text prompting may still succeed by leveraging broader semantic cues.

### Mechanism 2
- **Claim:** Embedding-level concatenation outperforms text-level fusion and hierarchical transformer fusion.
- **Mechanism:** Separating post and context embeddings allows the classifier to weight each independently, preventing longer context from overwhelming shorter original text. The post (avg. 17 tokens) vs. context (avg. 78 tokens) length imbalance biases text-level fusion toward context.
- **Core assumption:** SBERT embeddings preserve sufficient semantic information for downstream classification when kept distinct.
- **Evidence anchors:**
  - Embedding concatenation achieved gains of up to 3 and 6 F1 points over zero-context baseline.
  - Error analysis shows lengthy context can bias model to treat combined input as "more descriptive than hateful."
- **Break condition:** When context is highly relevant and tightly coupled to the post, deeper fusion may help—but results show this is rare in practice.

### Mechanism 3
- **Claim:** LLM-generated context introduces both benefit and semantic drift risk.
- **Mechanism:** Prompting LLMs to generate hate-speech-relevant context can surface latent meaning (improving true positives) but can also "invent hateful connections where none exist" (increasing false positives) or neutralize clear hate (increasing false negatives).
- **Core assumption:** LLM prompts can be designed to bias context toward hate-speech relevance without over-generating spurious associations.
- **Evidence anchors:**
  - 158 posts (3.6%) became false negatives with context; 187 non-hateful posts (4.4%) became false positives.
  - Example: "this is what becoming a groyper does..." misclassified as hateful after context about groyper ideology was added.
- **Break condition:** Context helps most when implicit hate relies on external knowledge; hurts when explicit hate is clear without context, or when prompts inject biased associations.

## Foundational Learning

- **Concept: Sentence Embeddings (SBERT)**
  - **Why needed here:** The paper uses SBERT to encode posts and context into 768-dim vectors before classification. Understanding that embeddings capture semantic similarity, not just keyword overlap, is essential for interpreting why Embed & Concat works.
  - **Quick check question:** Given two sentences with similar meaning but different words, would SBERT place them closer in vector space than two sentences with overlapping words but different meaning?

- **Concept: Embedding Concatenation vs. Text Concatenation**
  - **Why needed here:** The core comparison in the paper. Text concatenation fuses post + context before encoding; embedding concatenation encodes separately and joins vectors. The latter preserves source distinction.
  - **Quick check question:** If post = "Fight me, communists" (3 tokens) and context = 100 tokens of historical background, which approach would give the original post more influence in the final representation?

- **Concept: Macro F1 and Class Imbalance**
  - **Why needed here:** Latent Hatred has 62% non-hate, 33% implicit, 5% explicit. Macro F1 averages per-class scores, making it sensitive to minority-class performance—critical for evaluating implicit hate detection where positive classes are underrepresented.
  - **Quick check question:** If a model achieves 95% accuracy but predicts "non-hate" for everything, would macro F1 reveal the problem?

## Architecture Onboarding

- **Component map:** Raw post text -> LLM context generation (Gemini 2.0 Flash) -> SBERT encoding (all-mpnet-base-v2) -> Embed & Concat fusion -> 3-layer MLP classifier -> Macro F1 evaluation
- **Critical path:**
  1. Generate context via LLM (prompt design is key)
  2. Encode post and context separately with SBERT
  3. Concatenate embeddings and feed to MLP classifier
  4. Evaluate with macro F1; run error analysis to detect semantic drift
- **Design tradeoffs:**
  - Full-text vs. entity-based context: Full-text is more robust but computationally heavier; entity-based fails on informal text with poor entity detection.
  - Embedding vs. text fusion: Embedding concatenation preserves signal separation; text concatenation is simpler but biases toward longer context.
  - Hierarchical fusion (Context-Embed): Deeper integration but underperforms when context is sparse or noisy.
- **Failure signatures:**
  - Context overwhelms post: Explicit hate misclassified as non-hate after adding neutralizing context.
  - Spurious associations: Non-hateful posts misclassified as hateful when context introduces charged associations.
  - Missing entities: Entity-based approaches fail when NER misses capitalized or informal named entities.
- **First 3 experiments:**
  1. Replicate zero-context baseline: Train MLP on SBERT embeddings of raw posts only.
  2. Test Embed & Concat with full-text context: Generate context via LLM, encode separately, concatenate embeddings; compare F1 delta.
  3. Error analysis on false positives/negatives: Manually inspect cases where context flips the prediction; identify semantic drift patterns.

## Open Questions the Paper Calls Out

- Can dynamic weighting or attention-based mechanisms effectively mitigate the "overcontextualisation" risk where generated context dilutes original intent? The authors suggest future models could use "dynamic weighting" to balance post and context impacts, but the current study relied on simple concatenation strategies that lack this capacity.

- Does utilizing online-search-capable or reasoning-focused LLMs result in higher-quality background context for hate speech detection than static-knowledge models? The paper explicitly proposes exploring "online-search and reasoning-capable LLMs" as a method to generate superior context, but experiments were limited to Gemini 2.0 Flash.

- Can hybrid systems that combine LLM-generated context with verifiable domain knowledge bases (e.g., GPAHE) outperform LLM-only context generation? The authors suggest hybrid systems could help detect hateful references that LLMs might overlook while ensuring better factual grounding.

## Limitations

- LLM-generated context introduces non-deterministic variability that affects reproducibility due to unspecified temperature and top_p parameters for Gemini 2.0 Flash.
- The semantic drift risk—where context can neutralize explicit hate or hallucinate associations in benign posts—remains difficult to fully control through prompt engineering alone.
- Absence of fixed random seeds for data splits limits precise comparison against reported metrics and exact replication of confusion matrices.

## Confidence

- **High Confidence:** The superiority of full-text prompting over named-entity extraction is well-supported by consistent F1 gains across datasets and tasks. The consistent underperformance of text-level fusion compared to embedding-level concatenation is also strongly evidenced.
- **Medium Confidence:** The claim that LLM-generated context is most effective when kept modular (Embed & Concat) rather than merged is supported, but the error analysis shows semantic drift is context-dependent, suggesting this may not generalize to all datasets or hate speech types.
- **Low Confidence:** The assertion that hierarchical transformer fusion sometimes degrades performance is based on limited negative results; deeper integration might help in datasets with richer, more tightly coupled context.

## Next Checks

1. **Prompt Stability Test:** Run Embed & Concat with full-text context generation using multiple temperature settings (e.g., 0.0, 0.7, 1.0) to quantify the impact of non-determinism on F1 scores and identify stable prompt configurations.

2. **Semantic Drift Quantification:** Systematically analyze false positive and false negative rates when context is added, categorizing cases by hate speech type (explicit, implicit, dog-whistle) to determine if drift patterns are predictable or dataset-specific.

3. **Cross-Dataset Generalization:** Apply the Embed & Concat pipeline to a different hate speech dataset (e.g., a multilingual or multimodal benchmark not used in the study) to assess whether the full-text + embedding concatenation advantage holds across domains.