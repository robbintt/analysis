---
ver: rpa2
title: In-Context Compositional Q-Learning for Offline Reinforcement Learning
arxiv_id: '2509.24067'
source_url: https://arxiv.org/abs/2509.24067
tags:
- squery
- dmin
- learning
- offline
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICQL, a novel offline RL framework that formulates
  Q-learning as a contextual inference problem using linear Transformers. The method
  adaptively infers local Q-functions from retrieved transitions without requiring
  explicit subtask labels, addressing the challenge of compositional tasks with diverse
  subtasks.
---

# In-Context Compositional Q-Learning for Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2509.24067
- **Source URL**: https://arxiv.org/abs/2509.24067
- **Reference count**: 40
- **Primary result**: ICQL achieves up to 16.4% improvement on kitchen tasks and 8.6% on Gym tasks over state-of-the-art offline RL methods

## Executive Summary
This paper introduces ICQL, a novel offline RL framework that formulates Q-learning as a contextual inference problem using linear Transformers. The method adaptively infers local Q-functions from retrieved transitions without requiring explicit subtask labels, addressing the challenge of compositional tasks with diverse subtasks. Theoretically, ICQL achieves bounded Q-function approximation error under two assumptions: linear approximability of the local Q-function and accurate weight inference from retrieved context, supporting near-optimal policy extraction. Empirically, ICQL substantially improves performance in offline settings, with notable gains of up to 16.4% on kitchen tasks, 8.6% on Gym tasks, and 6.3% on Adroit tasks. These results highlight the underexplored potential of in-context learning for robust and compositional value estimation, positioning ICQL as a principled and effective framework for offline RL.

## Method Summary
ICQL reformulates offline Q-learning as an in-context learning problem where a linear Transformer infers local Q-functions from retrieved transitions. For each query state, the method retrieves k similar transitions from the dataset and constructs a prompt matrix that enables the Transformer to perform iterative SARSA-style weight updates across its layers. The local Q-function is approximated as linear in a feature space, and the Transformer learns to extract the appropriate weights from the context. During policy extraction, a separate policy network is trained using advantage-weighted regression on the learned Q-values. The method combines retrieval-guided context construction with in-context TD learning, allowing it to handle compositional tasks without explicit subtask labeling.

## Key Results
- ICQL achieves 16.4% improvement on kitchen tasks, 8.6% on Gym tasks, and 6.3% on Adroit tasks over state-of-the-art methods
- State-similar retrieval outperforms random retrieval by large margins, validating the importance of local context
- Optimal context length is k=20, with performance degrading for longer contexts due to broken locality assumptions
- More transformer layers (up to 20) generally improve performance by enabling more TD iterations

## Why This Works (Mechanism)

### Mechanism 1: Local Linear Q-Function Approximation
Approximating Q-functions locally per state neighborhood reduces approximation error compared to global approximation in compositional tasks. For each query state s_query, ICQL retrieves nearby transitions from a local neighborhood Ω_d^s and fits a linear Q-function Q(s,a) = w_s^T φ(s,a) specific to that neighborhood. This allows the Q-estimate to adapt to local task structure without requiring explicit subtask labels. The core assumption is that the local Q-function within each neighborhood is approximately linear in the feature representation. If local Q-functions are not approximately linear (e.g., highly nonlinear dynamics within neighborhoods), the bounded error guarantee collapses.

### Mechanism 2: Retrieval-Guided Context Construction
Retrieving state-similar transitions provides relevant local context for accurate Q-function estimation. For each query state, ICQL retrieves the top-k transitions with smallest L2 distance in state space. These transitions form the context Ω_dmin_squery for in-context learning. The retrieval ensures coverage of the local neighborhood defined by distance threshold d. The core assumption requires that the retrieved set overlaps with the ideal local transition set by at least a coverage ratio σ. If retrieved transitions do not adequately cover the local neighborhood, or if distance metric doesn't capture task-relevant similarity, weight estimation error increases proportionally to √(d / (σ|Ω|)).

### Mechanism 3: In-Context TD Learning via Linear Attention
Linear transformers can implement iterative TD-style weight updates in their forward pass. Each linear transformer layer implements one step of SARSA-style weight update: w_new = w + α(r + γQ(s',a') - Q(s,a))φ(s,a). The forward pass through L layers corresponds to L iterations of in-context TD learning on the retrieved transitions. Theorem E.3 proves this equivalence. If the transformer depth is insufficient for convergence, or if sparse rewards cause zero gradients without RTG augmentation, the mechanism fails.

## Foundational Learning

- **Concept**: Offline RL and Distributional Shift
  - Why needed here: ICQL addresses the fundamental challenge that offline RL cannot explore, so value estimates for out-of-distribution actions can overestimate. Understanding why global Q-functions struggle with compositional tasks motivates the local approach.
  - Quick check question: Can you explain why querying Q-values for state-action pairs outside the dataset support leads to overestimation in offline RL?

- **Concept**: Temporal Difference (TD) Learning and SARSA
  - Why needed here: ICQL's linear transformer implements SARSA-style updates in-context. Understanding the TD error r + γQ(s',a') - Q(s,a) is essential for following the weight update mechanism.
  - Quick check question: What is the difference between SARSA and Q-learning update targets?

- **Concept**: Linear Attention and In-Context Learning
  - Why needed here: The paper builds on the result that linear attention can implement gradient descent in the forward pass. Understanding how LinAttn(Z; P, G) = PZM(Z^T G Z) operates is necessary to follow Theorem E.3.
  - Quick check question: How does the mask matrix M ensure the query input doesn't influence its own prediction?

## Architecture Onboarding

- **Component map**: Offline dataset → Retrieval module (cosine similarity) → Feature extractor (3-layer MLP) → Linear Transformer (20 layers) → Q-estimate → Policy network (2-3 layer MLP) → Advantage-weighted regression

- **Critical path**:
  1. Sample batch from offline dataset D
  2. For each query state s_i, retrieve k=20 similar transitions via cosine similarity
  3. Construct prompt matrix Z_t with features, next-state features, and modified rewards
  4. Forward pass through 20-layer linear transformer → Q-estimate at position [2d+1, k+1]
  5. Compute critic loss via expectile regression and policy loss via AWR
  6. Backprop through feature extractor and transformer parameters {C_ℓ}

- **Design tradeoffs**:
  - Context length (k): k=20 optimal; shorter lacks coverage, longer breaks locality
  - Transformer depth (L): More layers → more TD iterations → better convergence
  - Retrieval strategy: State-similar most robust; state-similar-with-high-rewards helps on noisy data
  - Feature dimension: 64-dim balances expressiveness and conditioning of local Gram matrix

- **Failure signatures**:
  1. Q-estimates don't converge across layers: Check RTG augmentation for sparse rewards; verify β parameter
  2. Performance worse than baseline on specific datasets: Check coverage ratio σ; datasets like Hammer-Human have larger distances to retrieved states
  3. Training instability: Check gradient clipping (max norm 10); reduce learning rate if gradient explosion occurs
  4. Poor performance on compositional tasks: Verify context is sufficiently local (reduce k or adjust distance threshold)

- **First 3 experiments**:
  1. Ablate context length: Run ICQL with k ∈ {10, 20, 30, 40} on Walker2d-Medium. Expect k=20 to be optimal; deviations reveal locality sensitivity.
  2. Ablate retrieval strategy: Compare random vs. state-similar vs. state-similar-with-high-rewards on 2-3 datasets. Random should fail; state-similar should dominate dense-reward tasks.
  3. Monitor intermediate Q-estimates: Log Q-values at each transformer layer for diagnostic states. Should see convergence trend. Non-convergence indicates learning rate or feature issues.

## Open Questions the Paper Calls Out
- **Can ICQL be effectively extended to high-dimensional reasoning tasks, such as language-conditioned reinforcement learning?** The authors state, "As future work, we plan to extend ICQL to high-dimensional reasoning tasks (e.g., language-conditioned RL)." The current empirical evaluation is restricted to continuous control tasks and does not test the method's scalability to semantic or high-dimensional observation spaces.
- **How can the retrieval mechanism be improved to handle datasets with high variance or sparse coverage where query-retrieval distances are large?** The paper notes a -49.4% performance drop on the Hammer-Human dataset, attributing it to "dataset quality" and "larger" distances between query and retrieved states, suggesting the current retrieval strategy fails in this regime.
- **To what extent does the assumption of "linear approximability" of the local Q-function hold in the complex environments tested?** Theoretical guarantees depend on the assumption that a linear feature map can approximate the local Q-function, but the paper provides no empirical analysis of the linearity of these local sub-spaces.

## Limitations
- The theoretical claims rely on strong assumptions (local Q-function linearity and accurate weight inference) that may not hold for all compositional tasks
- The empirical evaluation lacks ablation studies on the fundamental assumption that local Q-functions are approximately linear
- The mechanism by which linear transformers implement iterative TD updates depends on precise matrix constructions not fully specified in implementation details

## Confidence
- **High confidence**: The retrieval-guided context construction mechanism is well-supported by ablation studies showing state-similar retrieval significantly outperforms random retrieval
- **Medium confidence**: The local linear Q-function approximation is theoretically justified but lacks empirical validation of the linearity assumption in real compositional tasks
- **Medium confidence**: The in-context TD learning mechanism is theoretically proven but depends on precise implementation details not fully specified

## Next Checks
1. Validate local linearity assumption: Run ICQL on datasets where the true Q-function is known to be nonlinear within local neighborhoods to test if performance degrades when the linearity assumption breaks
2. Monitor convergence dynamics: Log intermediate Q-values across transformer layers during training to verify the theoretical convergence proof holds empirically, particularly for sparse-reward tasks requiring RTG augmentation
3. Test retrieval coverage sensitivity: Systematically vary the coverage ratio σ by controlling the distance threshold d and measuring how performance scales, validating the theoretical dependence on √(d/(σ|Ω|))