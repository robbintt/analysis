---
ver: rpa2
title: 'MoWE : A Mixture of Weather Experts'
arxiv_id: '2509.09052'
source_url: https://arxiv.org/abs/2509.09052
tags:
- weather
- mowe
- forecast
- experts
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoWE, a Mixture of Weather Experts framework
  that dynamically combines forecasts from multiple state-of-the-art AI weather models
  to improve accuracy. The core idea is a lightweight Vision Transformer-based gating
  network that learns to weight each expert's contribution at every grid point and
  forecast lead time, creating a synthesized forecast that outperforms any individual
  model.
---

# MoWE : A Mixture of Weather Experts
## Quick Facts
- arXiv ID: 2509.09052
- Source URL: https://arxiv.org/abs/2509.09052
- Reference count: 34
- Primary result: MoWE achieves up to 10% lower RMSE than the best individual AI weather model on 2-day forecasts

## Executive Summary
MoWE introduces a Mixture of Weather Experts framework that dynamically combines forecasts from multiple state-of-the-art AI weather models to improve accuracy. The core innovation is a lightweight Vision Transformer-based gating network that learns to weight each expert's contribution at every grid point and forecast lead time. Using Pangu, Aurora, and FCN3 as expert models, MoWE demonstrates up to 10% lower Root Mean Squared Error than the best individual expert on 2-day forecasts, significantly outperforming simple averaging approaches.

## Method Summary
The MoWE framework employs a Vision Transformer-based gating network that dynamically assigns weights to multiple expert weather models (Pangu, Aurora, and FCN3) at each grid point and forecast lead time. The gating network is trained to optimize the weighted combination of expert forecasts, creating a synthesized forecast that leverages the complementary strengths of each model. This approach allows MoWE to improve forecast accuracy without requiring additional training of the underlying expert models, presenting a scalable strategy for enhancing data-driven weather prediction by effectively combining existing high-quality forecasts.

## Key Results
- MoWE achieves up to 10% lower Root Mean Squared Error than the best individual expert model on 2-day forecasts
- The framework significantly outperforms simple averaging approaches for combining weather forecasts
- Demonstrates effective harnessing of existing high-quality forecasts without additional training cost

## Why This Works (Mechanism)
MoWE works by leveraging the complementary strengths of multiple AI weather models through a learned weighting mechanism. The Vision Transformer-based gating network acts as an intelligent router that determines which expert model to trust more at specific locations and forecast horizons based on local meteorological conditions. This dynamic weighting approach captures spatial and temporal variations in model performance that static ensemble methods miss, allowing the system to produce more accurate forecasts by selectively combining the most reliable predictions from each expert model for different atmospheric conditions.

## Foundational Learning
- **Vision Transformers**: Needed for capturing spatial patterns in weather data across grid points; quick check: verify attention mechanism implementation and patch embedding strategy
- **Ensemble Learning**: Required for combining multiple model predictions effectively; quick check: validate weighting scheme and combination strategy
- **Weather Model Outputs**: Essential understanding of the expert models (Pangu, Aurora, FCN3) being combined; quick check: confirm input data format and normalization
- **Spatial-Temporal Weighting**: Critical for learning location and time-dependent model performance; quick check: examine how weights vary across different forecast horizons
- **RMSE Evaluation**: Standard metric for assessing forecast accuracy; quick check: verify calculation methodology and baseline comparisons

## Architecture Onboarding
**Component Map**: Weather data inputs -> Expert models (Pangu, Aurora, FCN3) -> Vision Transformer gating network -> Weighted combination -> Synthesized forecast output

**Critical Path**: Expert model outputs → Gating network weight calculation → Weighted forecast synthesis → Accuracy evaluation

**Design Tradeoffs**: Uses lightweight gating network to avoid retraining expensive expert models vs. potential performance gains from end-to-end training; dynamic per-grid-point weighting vs. computational overhead; leverages existing models vs. dependency on their quality

**Failure Signatures**: Poor performance when expert models disagree strongly; degraded accuracy if gating network fails to learn appropriate weights; limitations when expert models have systematic biases in similar directions

**First Experiments**: 1) Test gating network on synthetic expert model combinations with known ground truth, 2) Evaluate performance on single-variable forecasts before moving to multivariate, 3) Compare against baseline ensemble methods (simple averaging, weighted averaging) on validation data

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance is bounded by the quality and availability of underlying expert models
- Limited evaluation to specific models (Pangu, Aurora, FCN3) with uncertain generalizability to other models or domains
- Computational overhead of gating network not quantified for operational deployment considerations

## Confidence
- **High** confidence in observed RMSE improvements over individual experts
- **Medium** confidence in scalability and operational utility claims due to lack of computational analysis
- **High** confidence in outperforming simple averaging approaches based on reported results

## Next Checks
1. Evaluate MoWE's performance across a broader range of forecast lead times (e.g., 3-day, 5-day) to assess scalability
2. Test the framework with additional expert models not included in the original study to confirm robustness and generalizability
3. Quantify the computational overhead of the gating network in terms of inference time and resource usage to determine operational feasibility