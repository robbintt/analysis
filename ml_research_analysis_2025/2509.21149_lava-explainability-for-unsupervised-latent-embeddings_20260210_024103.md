---
ver: rpa2
title: 'LAVA: Explainability for Unsupervised Latent Embeddings'
arxiv_id: '2509.21149'
source_url: https://arxiv.org/abs/2509.21149
tags:
- gobp
- module
- locality
- modules
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LAVA, a method for explaining unsupervised
  latent embeddings by linking input features to local organization in the embedding
  space. It does so by defining overlapping localities, calculating feature-feature
  correlations within each locality, and extracting modules of reoccurring correlation
  subpatterns using Association Matrix Factorization.
---

# LAVA: Explainability for Unsupervised Latent Embeddings

## Quick Facts
- arXiv ID: 2509.21149
- Source URL: https://arxiv.org/abs/2509.21149
- Reference count: 40
- Primary result: LAVA explains unsupervised embeddings by linking input features to local embedding structure through feature correlation modules

## Executive Summary
LAVA addresses the challenge of explaining unsupervised latent embeddings by extracting modules of reoccurring feature correlation subpatterns. Unlike existing methods that focus on individual features or averaged patterns, LAVA identifies stable correlation modules that capture local organization in the embedding space. The method uses overlapping localities, Association Matrix Factorization with maximum-per-entry reconstruction, and optimized probe placement to produce interpretable explanations that can reveal domain-relevant patterns missed by existing approaches.

## Method Summary
LAVA operates in three stages: (1) locality definition using weighted k-means with DIRECT optimization to match neighborhood centrality patterns, (2) locality representation through pairwise Spearman correlations with 75% variance filtering, and (3) module extraction via Association Matrix Factorization that uses maximum-per-entry reconstruction to capture reoccurring correlation subpatterns. The method is model-agnostic and produces stable explanations at desired granularity levels, with applications demonstrated on MNIST and single-cell kidney datasets.

## Key Results
- LAVA modules are stable across different locality placements and AMF initializations (cosine similarity >0.89)
- Compared to label-free explainability approaches, LAVA better explains embedding structure and enables knowledge discovery
- In KPMP dataset, LAVA identifies biologically relevant signals correlating with chronic kidney disease and known markers
- The method captures domain-relevant patterns that other approaches miss, particularly in complex biological data

## Why This Works (Mechanism)

### Mechanism 1
Local feature correlations reveal latent structure. LAVA computes pairwise Spearman correlations within overlapping n-sized neighborhoods rather than across the full dataset. This captures how features covary in different regions of the latent space, reflecting the local organization that manifold methods like UMAP explicitly optimize to preserve. If embeddings don't preserve local structure, locality correlations become noise and modules won't capture recurring patterns.

### Mechanism 2
Maximum-per-entry reconstruction extracts recurring patterns rather than averages. Association Matrix Factorization reconstructs locality correlations using Ĉ_ij = max(P_i ⊙ (M^T)_j) instead of summation. Combined with up-weighted overestimation errors (ν=9), this forces modules to represent correlation subpatterns that actually appear in individual localities, not averaged composites. If data has no recurring correlation subpatterns, modules become unstable or overfit to noise.

### Mechanism 3
Optimized probe placement ensures localities represent embedding structure. LAVA places probes using weighted k-means with weights w_i = (in_neighbor_i/E)^α × (1/d̄_n_i)^β, where α and β are optimized via DIRECT to minimize the difference between neighborhood in-degree centrality and locality in-degree centrality. This ensures the proportion of localities containing each sample approximates the proportion of neighborhoods that would contain it. If embedding space is highly non-uniform with disconnected clusters, probe placement may under-sample sparse regions.

## Foundational Learning

- **Manifold learning preserves local neighborhoods**: LAVA's premise depends on understanding that methods like UMAP/t-SNE optimize for local structure preservation, making locality-based analysis meaningful. Quick check: Given a 2D UMAP of 10,000 points, would you expect nearby points in the embedding to have similar features in the original space? Why or why not?

- **Matrix factorization as pattern discovery**: AMF extends NMF concepts; understanding factorization as extracting reusable "parts" is prerequisite to interpreting modules. Quick check: How does standard NMF (W×H with summation) differ from AMF (max-per-entry reconstruction), and why does this matter for correlation data?

- **Correlation vs. causation in local contexts**: LAVA extracts feature covariation, not causal relationships. Module 4 in KPMP correlates with CKD but doesn't prove causation. Quick check: If genes A and B always correlate within kidney disease samples, does LAVA tell you whether A causes B, B causes A, or both are downstream of a third factor?

## Architecture Onboarding

- **Component map**: Locality Definition -> Locality Representation -> Module Extraction
- **Critical path**: The number of modules M selection (reconstruction vs. stability tradeoff) is the key decision point. Paper recommends finding reconstruction elbow, resolving ambiguity with higher silhouette width.
- **Design tradeoffs**:
  - Locality size n: Larger n smooths correlations but loses local specificity; smaller n captures finer structure but risks noise
  - Overlap factor o: Higher o gives more localities but increases computation
  - Up-weighting ν: Controls overestimation penalty (ν=9 ≈ 0.1 quantile). Higher ν → sparser, more conservative modules
  - Scale regularization γ: Critical for sparse data (KPMP needed γ=10.0 vs. default 0.0001)
- **Failure signatures**:
  - Overestimation ratio >0.3: Modules are averaging rather than extracting recurring patterns; increase ν
  - Silhouette width drops sharply with M increase: Over-specification; reduce M
  - Module presences uniformly distributed: Modules not capturing differentiated structure; reconsider n or data preprocessing
- **First 3 experiments**:
  1. Locality placement validation: Run with 5 different random seeds, target cosine >0.99 similarity
  2. Module count sweep: For M ∈ {2,4,6,8,10,12,14}, plot reconstruction quality, silhouette width, and overestimation ratio
  3. End-to-end stability: Run full pipeline with 5 locality placements × 10 AMF initializations, target mean similarity >0.89

## Open Questions the Paper Calls Out

### Open Question 1
How does LAVA perform when applied to higher-dimensional latent spaces (>50 dimensions) where locality definition via Euclidean distances becomes increasingly challenging? All experiments use 2D UMAP embeddings or 4D autoencoder latent space. Systematic evaluation on embeddings with varying dimensions (10, 50, 100+) is needed.

### Open Question 2
What is the relationship between sparsity constraints and overestimation control in AMF, and can alternative loss functions improve module interpretability? Current AMF uses quantile-regression-inspired overestimation penalty but doesn't explicitly incorporate sparsity. Experiments varying both sparsity regularization and overestimation weighting are needed.

### Open Question 3
How robust is LAVA to the choice of neighborhood size n, and can principled methods be developed for selecting this hyperparameter? No sensitivity analysis across different n values is reported. Systematic sweep of n values for each dataset is needed to identify optimal selection criteria.

## Limitations

- Locality placement optimization provides limited evidence that it produces meaningfully different coverage than uniform random placement
- AMF decomposition uniqueness is unproven; extracted modules may be one of many local optima
- Biological validation scope is limited to single disease association and known markers, without mechanistic validation

## Confidence

**High Confidence**: LAVA produces stable locality placements across runs (Figure 3a, cosine >0.99); Module extraction via AMF achieves better reconstruction quality than baseline methods (Figure 3b); Presence-weighted module stability across different runs and initializations (Figures 3c-3d, S7-S8)

**Medium Confidence**: Modules capture biologically relevant patterns in KPMP (correlation with CKD, known markers); LAVA better explains embedding structure than label-free approaches (MNIST experiments); Optimized probe placement improves locality representativeness vs. uniform sampling

**Low Confidence**: Modules represent "reoccurring" rather than "average" patterns (no ground truth for recurring patterns); Maximum-per-entry reconstruction is necessary for accurate pattern extraction (no ablation vs. summation); Locality size optimization via Jaccard similarity matching produces optimal n (no comparison to other heuristics)

## Next Checks

1. **Locality Placement Ablation**: Compare LAVA's weighted k-means locality placement against uniform random placement across 10 different seeds. Measure locality coverage uniformity, downstream module stability (cosine similarity), and biological relevance (correlation with known disease markers in KPMP).

2. **AMF Reconstruction Ablation**: Implement standard NMF with summation-based reconstruction and compare to LAVA's maximum-per-entry approach across 5 datasets. Measure reconstruction quality, overestimation ratio, and module interpretability (manual inspection of top-weighted features).

3. **Module Count Robustness**: For M ∈ {4,6,8,10,12}, run LAVA 10 times each on MNIST and KPMP. Compute pairwise module overlap (Jaccard similarity of top-10 features), stability of disease associations in KPMP, and visual distinctiveness of MNIST reconstructions at each M.