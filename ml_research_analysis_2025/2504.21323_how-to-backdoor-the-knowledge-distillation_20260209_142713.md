---
ver: rpa2
title: How to Backdoor the Knowledge Distillation
arxiv_id: '2504.21323'
source_url: https://arxiv.org/abs/2504.21323
tags:
- teacher
- backdoor
- clean
- images
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a backdoor attack on knowledge distillation
  by poisoning the distillation dataset with manipulated images. The attacker creates
  images that a clean teacher model misclassifies as a target label, adds a backdoor
  trigger, and includes these in the distillation dataset.
---

# How to Backdoor the Knowledge Distillation

## Quick Facts
- arXiv ID: 2504.21323
- Source URL: https://arxiv.org/abs/2504.21323
- Reference count: 40
- Key outcome: Demonstrates a successful backdoor attack on knowledge distillation by poisoning the distillation dataset with manipulated images that cause a clean teacher to misclassify, enabling high attack success rates with minimal accuracy loss.

## Executive Summary
This paper introduces a novel backdoor attack on knowledge distillation by poisoning the distillation dataset with manipulated images. The attacker generates images that a clean teacher model misclassifies as a target label, overlays a backdoor trigger, and includes these in the distillation dataset. During distillation, the student learns to associate the trigger with the target label while maintaining accuracy on clean data. Experiments on CIFAR-10/100 and ImageNet show high attack success rates (ASR up to 100%) with minimal accuracy loss. The attack remains effective even when only the KL divergence loss is used in distillation, challenging the assumption that knowledge distillation is safe with a clean teacher.

## Method Summary
The attack generates poisoned samples by first creating adversarial examples (using PGD for CIFAR, BigGAN for ImageNet) that cause a clean teacher to misclassify the source class as the target class. A backdoor trigger (a small white square patch) is then overlaid on these manipulated images, and the teacher's prediction is verified to ensure it still predicts the target class. These backdoor images are injected into the distillation dataset. The student is trained using a standard knowledge distillation loss combining cross-entropy and KL divergence. The student learns to associate the trigger with the target label, enabling high attack success rates on trigger-only test images while maintaining competitive clean accuracy.

## Key Results
- Attack achieves up to 100% ASR on trigger-only test images across CIFAR-10, CIFAR-100, and ImageNet.
- Clean accuracy (ACC) remains high (e.g., 94.8% vs 95.3% for clean distillation on CIFAR-10) with λ=0.5.
- Attack remains effective even with λ=1.0 (KL loss only), unlike traditional backdoor attacks.
- High ASR is achieved using different manipulation methods (PGD, BigGAN) and varying trigger sizes.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Loss Reinforcement via Soft Labels
The attack exploits the KD loss combining CE and KL divergence. By ensuring the clean teacher outputs high probability for the target label on manipulated images, both CE (from attacker's label) and KL (from mimicking teacher) gradients reinforce the backdoor in the same malicious direction. This works because the teacher is vulnerable to the adversarial manipulation itself.

### Mechanism 2: Trigger-Generalization over Manipulation Artifacts
The student associates the backdoor trigger with the target label rather than the specific adversarial perturbation. ASR is measured on trigger-only images to confirm this: success implies the student learned the spatially consistent trigger pattern, not the varying high-frequency manipulation noise.

### Mechanism 3: The Clean-Teacher Vulnerability Gap
The attack invalidates the assumption that a "clean teacher" is a sufficient safeguard. It exploits the gap between human perception and model classification within the distillation data itself, showing that data integrity is as critical as teacher integrity.

## Foundational Learning

- **Concept**: Knowledge Distillation Loss (KL Divergence vs. Cross-Entropy)
  - **Why needed here**: The attack's success depends on how λ balances learning from hard labels (easily poisoned) vs. soft teacher outputs (indirectly poisoned).
  - **Quick check**: If λ = 1.0, why does the student still learn the backdoor in this paper, but might not in a traditional backdoor attack?

- **Concept**: Targeted Adversarial Attacks (e.g., PGD, CW)
  - **Why needed here**: These generate the poisoned data by perturbing images to force specific misclassification by the teacher.
  - **Quick check**: How does a "targeted" adversarial attack differ from an untargeted one in creating a backdoor trigger?

- **Concept**: Backdoor Attacks vs. Evasion Attacks
  - **Why needed here**: The paper uses evasion techniques (generating adversarial examples) for backdoor implantation. You must distinguish between "fooling a model now" (evasion) and "teaching a model to be fooled later" (backdoor).
  - **Quick check**: Why does the paper measure ASR on clean images with only the trigger patch, rather than adversarial images with the patch?

## Architecture Onboarding

- **Component map**: Threat Actor -> Manipulation Engine -> Trigger Overlay -> Teacher Oracle -> Student Trainer
- **Critical path**: The Filtering Step (querying the teacher). The poisoned sample must pass: `Teacher(Adversarial_Image + Trigger) == Target_Label`. If this check fails, the KL loss will resist the backdoor.
- **Design tradeoffs**:
  - **Manipulation Strength**: Stronger perturbations (e.g., iterative PGD) guarantee teacher misclassification (High ASR) but may lower clean accuracy or visual quality. Weaker perturbations (FGSM) may result in unstable backdoors (Low ASR at λ=1.0).
  - **Trigger Size**: Small triggers are stealthier but might be ignored by the student if the adversarial noise is dominant.
- **Failure signatures**:
  - **Low ASR with High λ**: The teacher prediction was unstable on the poisoned sample. The student learned to ignore the trigger because the KL loss didn't consistently reinforce the target.
  - **Low Clean Accuracy**: Poisoning rate is too high, or the manipulation distorted the image features required for the primary task.
- **First 3 experiments**:
  1. **Baseline Verification**: Run KD on CIFAR-10 with a clean teacher and only clean data. Verify that adding the trigger patch at test time does *not* cause misclassification.
  2. **λ Sensitivity Analysis**: Implement the attack with PGD on CIFAR-10. Sweep λ from 0.0 to 1.0. Confirm that unlike traditional attacks, the ASR remains high even as λ → 1.0.
  3. **Manipulation Type Comparison**: Compare FGSM vs. PGD vs. BigGAN. Observe if single-step methods (FGSM) fail at high λ due to low confidence in teacher predictions.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can knowledge distillation pipelines be defended against poisoned distillation data without significantly degrading student model accuracy?
  - **Basis in paper**: [explicit] The conclusion states, "A next step is to design defenses that detect or reduce the influence of poisoned distillation samples."
  - **Why unresolved**: The paper focuses on demonstrating the vulnerability and attack methodology rather than developing specific countermeasures.
  - **Evidence would resolve it**: A proposed defense mechanism that successfully identifies manipulated images or mitigates the backdoor while maintaining competitive clean accuracy (ACC).

- **Open Question 2**: Does the attack effectiveness vary significantly across different neural network architectures (e.g., Transformers) or non-image modalities?
  - **Basis in paper**: [explicit] The conclusion suggests the need to "study how such defenses extend to other architectures and modalities."
  - **Why unresolved**: The experimental scope was limited to ResNet-18 backbones on image classification tasks (CIFAR and ImageNet).
  - **Evidence would resolve it**: Empirical results showing Attack Success Rate (ASR) and Accuracy (ACC) on architectures like ViT or tasks like NLP/time-series.

- **Open Question 3**: Which specific data validation checks are most effective at detecting or blocking these manipulated backdoor images in the pipeline?
  - **Basis in paper**: [explicit] Section 7 notes, "studying which pipeline checks are most effective is left for future work."
  - **Why unresolved**: The authors assume the attacker can influence the dataset and acknowledge that practical deployments might use defenses that were not tested.
  - **Evidence would resolve it**: An ablation study testing various data sanitization or statistical filtering techniques against the specific manipulated samples (adversarial/GAN) used in the attack.

## Limitations

- The paper does not report False Positive Rate (FPR) on clean images from non-source classes, leaving open the possibility that the trigger causes broader confusion.
- Exact adversarial attack hyperparameters (epsilon, step size, iterations) and the precise student model architecture width reduction are not fully specified, making exact reproduction challenging.
- The claim of being the "first successful exploitation" lacks definitive exclusion of all prior art, as the threat model is not exhaustively compared against all related work.

## Confidence

- **High confidence**: The attack succeeds under controlled conditions with proper hyperparameter tuning (ASR up to 100% demonstrated empirically).
- **Medium confidence**: The mechanism that the KL divergence loss reinforces backdoor learning is theoretically sound but depends critically on the teacher's prediction stability on manipulated samples, which varies with attack method strength.
- **Low confidence**: The claim that this attack is the "first successful exploitation of vulnerabilities within knowledge distillation using clean teacher models" lacks definitive exclusion of prior art.

## Next Checks

1. **Teacher Prediction Stability Test**: For each poisoned sample, record the teacher's softmax probability on the target class with and without the trigger. If the probability drops significantly when the trigger is added, this indicates the teacher's vulnerability is to the trigger rather than the adversarial perturbation.
2. **Non-Source Class Trigger Sensitivity**: Evaluate ASR on clean images from all non-source classes (e.g., airplane, automobile on CIFAR-10) with only the trigger patch. If FPR > 5%, the backdoor generalization is broader than claimed.
3. **Data Provenance Defense Validation**: Implement a simple defense that removes any sample from the distillation dataset where the teacher's hard prediction (argmax) differs from the attacker's label. If this filter eliminates most poisoned samples and reduces ASR to near zero, it confirms the attack critically depends on the teacher's misclassification of the source image before trigger addition.