---
ver: rpa2
title: Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete
  Modalities
arxiv_id: '2506.01490'
source_url: https://arxiv.org/abs/2506.01490
tags:
- modality
- missing
- multimodal
- modalities
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Confidence-Aware Self-Distillation (CASD)
  strategy to address modality missingness in multimodal sentiment analysis. The approach
  uses probabilistic embeddings based on Student's t-distributions to capture uncertainty
  in multimodal combinations, with confidence scores derived from degrees of freedom.
---

# Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities

## Quick Facts
- arXiv ID: 2506.01490
- Source URL: https://arxiv.org/abs/2506.01490
- Authors: Yanxi Luo, Shijin Wang, Zhongxing Xu, Yulong Li, Feilong Tang, Jionglong Su
- Reference count: 30
- Primary result: CASD achieves state-of-the-art performance on MOSI, MOSEI, and IEMOCAP, with up to 4.86% F1 improvement over baselines under various missing-modality conditions

## Executive Summary
This paper introduces Confidence-Aware Self-Distillation (CASD), a novel approach for multimodal sentiment analysis with incomplete modalities. The method uses probabilistic embeddings based on Student's t-distributions to capture modality-level uncertainty, with confidence scores derived from degrees of freedom. A reparameterization representation module samples from joint distributions to alleviate directional constraints and enable modality-specific learning. The approach estimates joint distribution quality through uncertainty scores and reduces uncertainty via consistency distillation. Experimental results demonstrate significant improvements in F1 scores (up to 4.86% on MOSI) and MAE under various missing-modality conditions.

## Method Summary
CASD employs a teacher-student self-distillation framework where the teacher trains on complete modalities and the student learns from corrupted inputs via Modality Random Missing (MRM). Each modality passes through a 1D Conv (kernel 3) → Transformer encoder → evidential heads that output Normal-Inverse-Gamma parameters. These parameters form Student's t predictive distributions, from which confidence weights are computed. Multimodal fusion combines distributions using confidence-aware weighting, producing a joint distribution with uncertainty score. The Reparameterization Representation Module (RRM) samples embeddings during training (uses mean at inference) to break directional constraints. Total loss combines cross-entropy with JS divergence between student/teacher logits and MSE between their uncertainty estimates.

## Key Results
- CASD achieves state-of-the-art performance across MOSI, MOSEI, and IEMOCAP datasets
- Significant F1 improvements under various missing-modality conditions (up to 4.86% on MOSI)
- Particularly effective when language modality is missing, improving average F1 by 2.45-4.5% over baselines
- Consistent MAE reductions across all tested missing-modality patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic embeddings with Student's t-distributions capture modality-level uncertainty, enabling the model to weight modalities by confidence rather than treating all available inputs equally.
- Mechanism: Each modality is encoded as a distribution (not a point) via a Normal-Inverse-Gamma prior, which yields a Student's t predictive distribution. Degrees of freedom (ν) parameterize confidence—higher ν indicates heavier confidence, lower ν indicates heavier tails (more uncertainty). This allows the model to express "I'm uncertain about this modality" rather than forcing a fixed representation.
- Core assumption: Missing or noisy modalities exhibit higher aleatoric/epistemic uncertainty, which is captured by the NIG parameters and reflected in the t-distribution's tail behavior.
- Evidence anchors: [abstract] "incorporating multimodal probabilistic embeddings via a mixture of Student's t-distributions, enhancing its robustness by incorporating confidence and accommodating heavy-tailed properties" [section II-B] Eq. 1-5 derive the probabilistic embedding pipeline; Eq. 3 explicitly decomposes AU (aleatoric) and EU (epistemic) uncertainty [corpus] Related work on Bayesian integration (BIONIC) similarly uses probabilistic representations for missing clinical data, suggesting this is a converging design pattern
- Break condition: If modalities are missing completely at random without correlation to data quality (i.e., missingness is uniform noise rather than signal-dependent), uncertainty-based weighting may not provide discriminative value.

### Mechanism 2
- Claim: Confidence-aware fusion uses degrees of freedom to compute modality-specific weights, ensuring that high-confidence modalities dominate the joint representation while uncertain modalities contribute less.
- Mechanism: The confidence weights C1, C2, C3 are computed from relative degrees of freedom (Eq. 8). The fused distribution inherits the minimum ν (most conservative tail behavior) and combines means via confidence-weighted averaging. The joint uncertainty score UF (Eq. 9) is then distilled from teacher to student via MSE loss.
- Core assumption: The teacher network (trained on complete modalities) produces a well-calibrated uncertainty estimate that the student can learn to approximate, even with missing inputs.
- Evidence anchors: [abstract] "estimates joint distributions with uncertainty scores and reduces uncertainty in the student network by consistency distillation" [section II-C] Eq. 7-10 detail the fusion and distillation procedure; LUF directly penalizes student-teacher uncertainty divergence [corpus] Weak corpus evidence—related papers don't explicitly study uncertainty distillation for missing modalities
- Break condition: If teacher uncertainty is poorly calibrated (e.g., overconfident on outliers), the student will inherit miscalibration. Also assumes missing modalities during training are representative of test-time missingness patterns.

### Mechanism 3
- Claim: Reparameterization sampling breaks implicit directional constraints, allowing same-class samples with different modality combinations to occupy distinct representation directions rather than being forced toward a shared subspace.
- Mechanism: Instead of passing the fused mean μF directly to the classifier, the model samples s = μF + σF · t (Eq. 11) where t ~ St(νF). The sampled representation must only align with class weights W_y, while the inference representation μF can vary in direction across modality combinations. Larger σF increases relaxation.
- Core assumption: The gradient signal from the sampled representation is sufficient to learn meaningful structure without forcing all same-class representations into parallel directions.
- Evidence anchors: [abstract] "reparameterization representation module... alleviated by the sampled representation" [section II-D] Explicit analysis: "This relaxes the directional constraint on the inference representation and enables the model to capture the specific information for different modality combinations" [corpus] No corpus papers directly address directional constraints via sampling
- Break condition: If σF → 0, mechanism degrades to standard subspace projection. If σF is too large, sampling noise may overwhelm the signal. Requires appropriate uncertainty estimation to set σF meaningfully.

## Foundational Learning

- Concept: **Student's t-distribution and degrees of freedom**
  - Why needed here: The paper uses t-distributions (not Gaussians) specifically for their heavy-tailed property, which accommodates outliers and missing-data uncertainty. Understanding how ν controls tail weight is essential for interpreting confidence scores.
  - Quick check question: If ν = 3 vs. ν = 30, which distribution has heavier tails and what does that imply about model confidence?

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: Eq. 3 decomposes uncertainty into data noise (AU) and model uncertainty (EU). This distinction matters because missing modalities primarily increase EU, and the distillation targets this explicitly.
  - Quick check question: If a modality has high AU but low EU, what does that tell you about the data vs. the model's confidence?

- Concept: **Reparameterization Trick**
  - Why needed here: Sampling from distributions is non-differentiable. The reparameterization (Eq. 11) makes sampling differentiable by expressing samples as deterministic functions of parameters plus external noise.
  - Quick check question: Why can't we backpropagate through a direct sample z ~ p(z|μ,σ), and how does reparameterization solve this?

## Architecture Onboarding

- Component map:
  1. **Encoder backbone**: 1D Conv (kernel 3) → Transformer encoder → produces Fm per modality
  2. **Probabilistic heads**: Multi-evidential heads output NIG parameters (γ, δ, α, β) per modality
  3. **Uncertainty estimator**: Computes AU/EU (Eq. 3), derives Student's t parameters
  4. **Confidence-aware fusion**: Combines modalities via C1-C3 weights, outputs fused (μF, ΣF, νF)
  5. **RRM sampling module**: Samples s = μF + σF · t during training; uses μF at inference
  6. **Distillation losses**: L_logits (JS divergence) + L_UF (uncertainty MSE) + L_CE

- Critical path:
  Training: Input → Encoder → NIG params → Student's t → Fusion → RRM sample → Classifier → Loss
  Inference: Input → Encoder → NIG params → Student's t → Fusion → μF (no sampling) → Classifier

- Design tradeoffs:
  - t-distribution vs. Gaussian: Heavier tails handle outliers better but require ν estimation
  - Sampling at training vs. mean at inference: Improves robustness but adds stochasticity
  - Separate teacher-student vs. self-distillation: Self-distillation reduces memory overhead but requires careful initialization

- Failure signatures:
  - Collapsed uncertainty (all ν → ∞): Model overconfident; check L_UF gradient flow
  - No improvement over baseline: Verify MRM is actually dropping modalities; check missing rate
  - High variance in training loss: σF may be too large; consider clamping or scheduling
  - Poor performance when language is missing: This is the hardest case—verify audio/visual encoders are learning meaningful representations (ablation in Table I shows this is the key stress test)

- First 3 experiments:
  1. **Sanity check**: Run on complete modalities only. CASD should match or slightly exceed baseline. If significantly worse, implementation error in fusion or loss weighting.
  2. **Controlled missingness**: Set MRM to drop exactly one modality (e.g., language only). Compare F1 degradation vs. baseline. Per paper, expect 2-4% F1 improvement over baselines in (a,v) condition.
  3. **Ablation on σF**: Manually vary σF (or equivalently, the uncertainty scale) and observe F1/MAE. Confirm that σF = 0 degrades to baseline and moderate σF improves, but excessive σF hurts. This validates the RRM mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the assumption that the fused Student's t-distribution remains approximately a Student's t-distribution hold for heterogeneous or adversarial modalities?
- Basis in paper: [explicit] The paper states in Section II.C: "We assume that the fused Student's t distribution remains approximately a Student's t distribution."
- Why unresolved: The approximation relies on degrees of freedom adjustments that may fail if modality interactions are highly non-linear or conflicting.
- What evidence would resolve it: A quantitative analysis comparing the theoretical fused distribution against empirical density estimates in non-ideal fusion scenarios.

### Open Question 2
- Question: How does CASD perform under structured or correlated missingness patterns compared to the simulated random missingness?
- Basis in paper: [inferred] The paper uses a "Modality Random Missing (MRM)" strategy where missing features are replaced by zero vectors.
- Why unresolved: Real-world missingness often follows specific patterns (e.g., sensor correlation, privacy redaction) rather than random uniform drops.
- What evidence would resolve it: Experiments on datasets with naturally occurring missing modalities or benchmarks designed with correlated missingness protocols.

### Open Question 3
- Question: Can the confidence-aware probabilistic embeddings effectively scale to tasks beyond sentiment analysis, such as dense prediction or generative multimodal tasks?
- Basis in paper: [inferred] The method is exclusively evaluated on utterance-level sentiment datasets (MOSI, MOSEI, IEMOCAP).
- Why unresolved: The probabilistic handling of uncertainty might behave differently in tasks requiring dense spatial-temporal reasoning or generative synthesis.
- What evidence would resolve it: Application of CASD to multimodal benchmarks like VQA or action recognition with induced missing modalities.

## Limitations
- The paper demonstrates strong performance but relies heavily on teacher-student distillation with a complete-modality teacher, which may not be available in truly open-world settings
- Uncertainty estimation depends on the assumption that missing modalities correlate with higher uncertainty, which may not hold for all missingness patterns
- The RRM mechanism's benefits are primarily demonstrated through ablation studies rather than controlled perturbation experiments
- Implementation details for numerical stability (NIG parameter constraints, sampling temperature) are not fully specified

## Confidence
- **High Confidence**: The core probabilistic embedding framework with Student's t-distributions is well-grounded in established Bayesian statistics literature
- **Medium Confidence**: The confidence-aware fusion mechanism is theoretically sound but depends on proper uncertainty calibration during teacher pre-training
- **Medium Confidence**: The RRM sampling approach shows promise but requires careful hyperparameter tuning (σF) to balance regularization vs. signal preservation

## Next Checks
1. **Teacher Uncertainty Calibration**: Evaluate whether the teacher network's uncertainty estimates are well-calibrated on complete data before distillation, using reliability diagrams or expected calibration error metrics
2. **Missingness Pattern Sensitivity**: Test CASD performance under different missingness mechanisms (MCAR vs. MNAR) to verify that uncertainty-based weighting generalizes beyond the studied random patterns
3. **σF Sensitivity Analysis**: Systematically sweep the RRM sampling variance parameter σF across a wider range to identify optimal values and verify the claimed directional relaxation effect through visualization of learned representations