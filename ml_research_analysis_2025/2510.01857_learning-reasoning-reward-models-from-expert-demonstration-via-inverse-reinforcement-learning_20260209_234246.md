---
ver: rpa2
title: Learning Reasoning Reward Models from Expert Demonstration via Inverse Reinforcement
  Learning
arxiv_id: '2510.01857'
source_url: https://arxiv.org/abs/2510.01857
tags:
- reward
- reasoning
- answer
- dense
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes using inverse reinforcement learning to learn
  dense, token-level reasoning reward models from expert demonstrations, rather than
  relying on outcome-based verification or pure imitation. The learned reward serves
  a dual role: as a training signal that outperforms supervised fine-tuning baselines
  (e.g., 79% vs.'
---

# Learning Reasoning Reward Models from Expert Demonstration via Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01857
- Source URL: https://arxiv.org/abs/2510.01857
- Authors: Claudio Fanconi; Nicolás Astorga; Mihaela van der Schaar
- Reference count: 40
- Primary result: IRL-based dense reasoning rewards outperform SFT by 23pp on GSM8K and 9pp on MedReason, while enabling inference-time reranking up to +12pp accuracy gains

## Executive Summary
This paper introduces a method to learn dense, token-level reasoning reward models from expert demonstrations using inverse reinforcement learning. Instead of relying on outcome-only verification or pure imitation, the approach infers a reward function that captures the quality of reasoning steps through an adversarial discriminator trained to distinguish expert traces from policy-generated ones. The learned reward serves a dual role: as a training signal that improves reasoning performance beyond supervised fine-tuning and as an inference-time reranker that selects higher-quality candidate traces. Experiments demonstrate that while sparse and step-wise rewards are most stable for training, dense rewards are strongest for inference reranking, particularly on Llama3 architectures.

## Method Summary
The method uses inverse reinforcement learning to learn a token-level reward model from expert demonstrations. A discriminator is trained adversarially to distinguish expert reasoning traces from those generated by a policy model. The discriminator's logits are converted into implicit rewards via log-odds transformation. Sparse supervision points (like final outcomes or step boundaries) are backfilled to create dense training signals. The policy is then updated using Group Relative Policy Optimization (GRPO) with these rewards. At inference time, the learned reward model re-ranks multiple candidate traces generated by the policy, selecting the highest-scoring one. The approach provides interpretable, token-level diagnostics that identify where reasoning goes wrong.

## Key Results
- IRL-based reward models outperform pure SFT baselines: 79% vs 56% on GSM8K and 74% vs 65% on MedReason
- Sparse and step-wise reward formulations provide more stable training than dense variants, with sparse yielding highest accuracy
- Dense rewards provide stronger inference-time reranking gains (up to +12pp on Llama3.2-3B) despite training instability
- Token-level interpretability enables precise identification of reasoning errors in incorrect traces

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Discriminator-to-Reward Transduction
A discriminator trained to separate expert traces from policy-generated traces yields an implicit dense reward signal when logit differences are converted to scalar values. The discriminator outputs probabilities over tokens, and the implicit reward is derived via $v_\phi(y_t) = \log D_\phi(y_t) - \log(1 - D_\phi(y_t))$. This transforms the binary classification signal into an unbounded scalar reward that increases for expert-like tokens and decreases for policy-like tokens. The core assumption is that expert traces contain systematic patterns of "good reasoning" recoverable via binary classification against non-expert traces.

### Mechanism 2: Reward Densification via Backfilling and Clipping
Sparse supervision points (outcome, step boundaries, intervals) are converted to dense training signals via temporal backfilling without losing discriminative power. Given masked positions $m_t = 1$ at supervision points, unmasked tokens inherit the next available checkpoint reward: $r_\phi(y_t) = v_\phi(y_{t'})$ where $t' = \min\{k \geq t | m_k = 1\}$. Rewards are clipped to $[-\beta, \beta]$ to prevent gradient explosion from unbounded logits. The core assumption is that credit assignment precision at token-level is less critical than avoiding numerical instability.

### Mechanism 3: Inference-Time Reranking via Reward-Calibrated Selection
The learned reward model serves as an effective discriminator at inference time even when unstable as a training signal, enabling best-of-N selection. For each prompt, sample N candidates from policy, compute mean reward per trace, select highest-scoring candidate. Separation between correct/incorrect answer distributions predicts reranking gain. The core assumption is that reward model captures latent validity correlated with correctness even if training dynamics prevent direct optimization.

## Foundational Learning

- **Inverse Reinforcement Learning (IRL)**: Core formulation—inferring reward from demonstrations rather than specifying it. Understanding the minimax game structure is essential. Quick check: Given expert trajectories, does maximizing learned reward recover expert behavior? (Answer: Only if discriminator is optimal and reward is representable.)

- **Adversarial Training (GAN-style objectives)**: The discriminator-reward-policy loop is adversarial; mode collapse and non-stationarity are expected failure modes. Quick check: Why does clipping discriminator gradients or reward values help stability? (Answer: Prevents unbounded logits from creating extreme policy gradients.)

- **Group Relative Policy Optimization (GRPO)**: The policy update mechanism using group-standardized advantages. Must understand how baseline subtraction reduces variance. Quick check: How does group standardization differ from per-sample baselines in standard PPO? (Answer: Normalizes across group statistics, reducing inter-sample variance.)

## Architecture Onboarding

- **Component map**: Expert demonstrations -> Discriminator $D_\phi$ (expert vs policy classifier) -> Implicit rewards $v_\phi$ -> Backfilling module -> Dense rewards $r_\phi$ -> Policy $\pi_\theta$ (LLM with LoRA) -> Traces -> Discriminator training loop

- **Critical path**: 1) Warm-up phase (250 steps): Train discriminator only on expert vs. policy traces; 2) Adversarial loop: Alternating discriminator update → reward computation → policy update; 3) Monitoring: Track reward-correctness correlation; early stop if decoupled

- **Design tradeoffs**: Sparse vs. dense reward (stable training vs. unstable training but strong reranking); Expert-only vs. mixed positives (reduces data scarcity but risks confirmation bias); Same vs. different backbone for discriminator (same is more stable; different enables compute savings but increases collapse risk)

- **Failure signatures**: Mode collapse (reward rises while correctness plateaus); Architecture mismatch (Qwen2.5 reranking gains near zero despite training gains); Token-level noise (dense rewards fluctuate within valid chains)

- **First 3 experiments**: 1) Spare-only baseline on GSM8K: Train with outcome-only mask, verify reward-correctness correlation stays positive; 2) Inference-only reranking test: Train dense discriminator to convergence, then freeze and use for best-of-16 reranking; 3) Cross-backbone transfer: Train 3B discriminator, use to supervise 8B policy, monitor for early-stage collapse

## Open Questions the Paper Calls Out

### Open Question 1
How can dense token-level rewards be stabilized during adversarial training to prevent mode collapse while preserving fine-grained discriminative power? The paper notes that dense rewards cause mode collapse (e.g., Qwen2.5-3B drops to 4% accuracy) yet provide the strongest reranking signal (+12pp gains). Demonstrating that Wasserstein GAN objectives or other stabilization techniques achieve stable dense training while maintaining or improving reranking performance would resolve this.

### Open Question 2
Can the dense reward model's token-level interpretability be leveraged for active test-time interventions such as reward-guided decoding or iterative self-correction? The paper demonstrates error localization but does not implement active interventions. Experiments showing reward-guided decoding improves generation quality, or that models can iteratively revise traces based on negative reward signals, would resolve this.

### Open Question 3
Why do learned reasoning rewards transfer effectively to Llama3 models but show weak calibration and negligible gains for Qwen2.5 architectures? The paper observes the disparity but does not investigate whether this stems from pre-training differences, tokenization, or latent feature space alignment. Analysis correlating pre-training characteristics with reward model performance, or experiments showing transfer improves after alignment fine-tuning, would resolve this.

## Limitations
- Architecture-specific instability: Dense reward formulations yield strong inference-time gains but suffer training collapse on certain architectures (Qwen2.5 shows near-zero reranking benefit)
- Reward-correctness decoupling: Dense rewards exhibit higher variance in reward-correctness correlation during training
- Limited evaluation scope: Experiments focus primarily on GSM8K and MedReason datasets with relatively small model sizes (3B parameters)

## Confidence

**High confidence**:
- IRL-based reward models outperform pure SFT baselines on GSM8K (79% vs 56%) and MedReason (74% vs 65%)
- Sparse and step-wise reward formulations provide more stable training than dense variants
- Dense rewards provide stronger inference-time reranking gains (up to +12pp on Llama3.2-3B)
- Reward-calibrated selection effectively separates correct from incorrect answer distributions

**Medium confidence**:
- The dual role of reward as both training signal and inference reranker is generally effective
- Backfilling mechanism successfully converts sparse supervision to dense training signals
- Token-level interpretability provides actionable diagnostics for reasoning failures

**Low confidence**:
- Dense rewards will maintain their reranking advantage on larger model families
- The methodology generalizes to domains beyond mathematical and medical reasoning
- Cross-backbone transfer (different discriminator/policy architectures) will remain stable

## Next Checks

1. **Architecture transfer robustness test**: Train a dense reward discriminator on Llama3.2-3B, then evaluate its reranking performance on Qwen2.5-3B and vice versa. Document the degradation curve to quantify architecture-specific calibration requirements and identify minimum adaptation steps needed for cross-backbone effectiveness.

2. **Reward stability under data scarcity**: Systematically reduce the proportion of expert demonstrations (10%, 25%, 50%, 75% of original) and measure how quickly reward-correctness correlation degrades. This will establish minimum demonstration requirements and identify whether certain reward formulations (sparse vs. dense) are more robust to data limitations.

3. **Multi-hop reasoning generalization**: Apply the methodology to a dataset requiring deeper reasoning chains (e.g., StrategyQA or HotpotQA). Measure whether the token-level interpretability and reranking gains observed on GSM8K/MedReason extend to scenarios requiring information synthesis across multiple reasoning steps, and whether dense rewards maintain their advantage when reasoning depth increases.