---
ver: rpa2
title: 'Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding'
arxiv_id: '2505.07864'
source_url: https://arxiv.org/abs/2505.07864
tags:
- arrow
- accuracy
- evaluation
- detection
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of flowchart understanding by
  vision-language models (VLMs), which often fail to correctly interpret arrow directions
  and graph topology. To tackle this, the authors propose a seven-stage pipeline that
  combines arrow-aware object detection, OCR, and structured prompt generation.
---

# Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding

## Quick Facts
- arXiv ID: 2505.07864
- Source URL: https://arxiv.org/abs/2505.07864
- Reference count: 33
- Primary result: Improves VLM flowchart accuracy from 80% to 89% using arrow direction encoding

## Executive Summary
This paper addresses the challenge of flowchart understanding by vision-language models (VLMs), which often fail to correctly interpret arrow directions and graph topology. To tackle this, the authors propose a seven-stage pipeline that combines arrow-aware object detection, OCR, and structured prompt generation. The pipeline explicitly encodes arrow directions and node relationships, providing VLMs with spatial and topological cues.

Evaluated on a 90-question benchmark from 30 flowcharts, the method significantly improves accuracy from 80% to 89% without task-specific fine-tuning. The largest gains are seen in next-step queries (100% accuracy, +17 pp), while improvements in branch-result and before-step questions are more modest. An LLM-as-a-Judge evaluation shows similar trends. Limitations include dependence on detection and OCR precision, and residual errors at nodes with multiple incoming edges. Future work includes expanding the benchmark and adapting the pipeline to BPMN and UML.

## Method Summary
The proposed pipeline consists of seven stages: (1) Azure OCR extracts text and bounding boxes from flowchart images, (2) DAMO-YOLO detector identifies 9 object classes including arrow endpoints, (3) text-to-object fusion assigns OCR text to nodes via 50% IoU overlap, (4) arrow association links Arrow, Arrow Start, and Arrow End via IoU and proximity, (5) node-arrow linking assigns incoming/outgoing arrows to each node, (6) prompt construction serializes structured data into text format, and (7) GPT-4o inference answers questions using the image and structured prompt. The pipeline explicitly encodes arrow directions and node relationships, providing VLMs with spatial and topological cues.

## Key Results
- Accuracy improves from 80% to 89% on 90-question benchmark without task-specific fine-tuning
- Next-step query accuracy reaches 100% (+17 pp improvement)
- Branch-result and before-step questions show more modest gains
- LLM-as-a-Judge evaluation confirms similar trends to human evaluation

## Why This Works (Mechanism)

### Mechanism 1: Explicit Arrow Direction Encoding Recovers Topology
Encoding arrow start/end points as explicit textual coordinates allows VLMs to infer edge orientations they otherwise miss from raw pixels. A fine-tuned detector annotates Arrow Start and Arrow End bounding boxes; these are associated with nodes via IoU and proximity rules, then serialized into prompts listing incoming/outgoing connections per node. Core assumption: VLMs possess latent geometric priors from pretraining that can be activated when spatial relationships are made explicit in text.

### Mechanism 2: Coordinate-Rich Prompts Preserve the "Geometry Channel"
Providing each token with normalized center-of-mass coordinates enables the VLM to reason over spatial layout without hallucinating topology from unordered token lists. After OCR and detection, each object is represented as (text, category, x, y, incoming_edges, outgoing_edges); this structured representation is formatted into the prompt alongside the image. Core assumption: The VLM can parse and reason over explicitly listed graph relationships more reliably than inferring them from visual features alone.

### Mechanism 3: Task-Specific Detection Offloads Hard Visual Subproblems
Fine-tuning a detector on flowchart-specific classes (including Arrow Start/End) reduces localization errors that end-to-end VLMs cannot self-correct. DAMO-YOLO is trained on 69 annotated diagrams with 9 classes; the detector outputs bounding boxes for nodes and arrow endpoints, which are then fused with OCR and passed to the VLM. Core assumption: Specialized visual parsers can achieve higher precision on domain-specific structures than general-purpose VLMs operating in zero-shot mode.

## Foundational Learning

### Concept: Directed Graph Representation
- Why needed here: Flowcharts are directed graphs; understanding control flow requires reasoning over incoming/outgoing edges, not just node labels.
- Quick check question: Given nodes A, B, C with edges A→B and B→C, what is the "next step" after A?

### Concept: Object Detection Evaluation Metrics (AP, AR, IoU)
- Why needed here: The paper reports mAP and AR at different IoU thresholds; understanding these metrics is essential to interpret detection quality and its impact on downstream reasoning.
- Quick check question: If arrow endpoints have small bounding boxes, why might standard IoU (0.50–0.95) underestimate their detection accuracy?

### Concept: Prompt Engineering for Structured Data
- Why needed here: The pipeline's effectiveness hinges on encoding graph structure into text prompts that the VLM can parse.
- Quick check question: How would you format a prompt that lists a node's text, type, and its incoming/outgoing neighbors?

## Architecture Onboarding

**Component map:**
Azure OCR -> DAMO-YOLO detector -> Text-Object Fusion -> Arrow Association -> Node-Arrow Linking -> Prompt Construction -> GPT-4o Inference

**Critical path:** Arrow endpoint detection -> arrow association -> prompt construction. Errors in early stages compound; Table 6 shows arrow-related mAP is the weakest link.

**Design tradeoffs:**
- Modularity vs. end-to-end: Pipeline is more interpretable and debuggable but requires training a custom detector.
- Relaxed IoU (0.10–0.50) vs. standard IoU (0.50–0.95): Relaxed thresholds improve recall for small arrow endpoints but may increase false positives.
- Human vs. LLM-as-a-Judge evaluation: Human evaluation captures paraphrase tolerance; LLM scoring underreports accuracy on explanatory outputs.

**Failure signatures:**
- Nodes with multiple incoming edges -> incomplete topology recovery (Section 5.1)
- OCR over-segmentation -> fragmented phrases misassigned to nodes
- Arrow ordering ambiguity when nodes share the same axis (replicated GenFlowchart failure)

**First 3 experiments:**
1. **Ablation: Remove coordinate encoding from prompts** -- measure accuracy drop to quantify the "geometry channel" contribution.
2. **Stress test: Increase diagram density** -- evaluate whether detection and association degrade on large diagrams (>22 arrows).
3. **Domain shift: Apply to BPMN/UML subset** -- assess detector generalization without retraining, identify failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed pipeline maintain its effectiveness when applied to more complex industrial notations like Business Process Model and Notation (BPMN) and Unified Modeling Language (UML)? Basis: The Abstract and Conclusion explicitly list "adapting the pipeline to BPMN and UML" as future work to assess broader applicability beyond standard flowcharts. Why unresolved: The current evaluation is restricted to 30 standard flowcharts; BPMN and UML introduce distinct visual grammars and complexities not present in the training data.

### Open Question 2
How does the method perform on synthetic and handwritten flowcharts compared to the manually annotated digital diagrams used in the current study? Basis: The Abstract and Conclusion state that future work will "enlarge the benchmark with synthetic and handwritten flowcharts." Why unresolved: The authors acknowledge the current benchmark is small and relies on manually annotated diagrams, leaving robustness to noise and style variations untested.

### Open Question 3
Does representing the flowchart as a JSON-encoded directed graph improve accuracy for "before-step" queries and nodes with multiple incoming edges? Basis: The Discussion identifies these as specific failure points and proposes "Representing the flowchart as a JSON-encoded directed graph" as a "promising solution." Why unresolved: The current prompt structure struggled with reverse-order reasoning (Type 3 questions) and complex topologies, indicating the current representation is insufficient.

## Limitations
- Arrow endpoint detection remains a critical bottleneck with low mAP under standard IoU
- Method struggles with nodes that have multiple incoming edges
- Reliance on manually annotated corpus without public availability limits reproducibility

## Confidence
- **High Confidence:** Accuracy improvement from 80% to 89% is well-supported by human evaluation results
- **Medium Confidence:** Claim that coordinate-rich prompts preserve the "geometry channel" is inferred but lacks direct experimental validation
- **Low Confidence:** Assertion that task-specific detection offloads hard visual subproblems is plausible but fragile given low arrow-related mAP

## Next Checks
1. **Ablation Study on Coordinate Encoding:** Remove the x,y coordinates from the prompt while keeping the graph structure intact. Measure the accuracy drop to quantify the contribution of the "geometry channel" mechanism.
2. **Domain Shift Evaluation:** Apply the pipeline to a small subset of BPMN or UML diagrams without retraining the detector. Document the failure modes to assess zero-shot generalizability.
3. **Error Analysis on Multiple Incoming Edges:** Manually annotate a set of nodes with multiple incoming edges and trace how the pipeline handles them. Identify whether the issue lies in arrow association or prompt construction.