---
ver: rpa2
title: 'RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action
  Model'
arxiv_id: '2510.10975'
source_url: https://arxiv.org/abs/2510.10975
tags:
- action
- policy
- scaling
- candidate
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoVer, a test-time scaling framework that
  enhances frozen vision-language-action (VLA) models without retraining. RoVer uses
  a compact process reward model (PRM) as an external verifier to score and refine
  candidate actions during inference.
---

# RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model

## Quick Facts
- arXiv ID: 2510.10975
- Source URL: https://arxiv.org/abs/2510.10975
- Reference count: 21
- Key outcome: RoVer improves frozen VLA model success rates by up to 18.4% on long-horizon tasks through test-time candidate scoring and refinement.

## Executive Summary
This paper introduces RoVer, a test-time scaling framework that enhances frozen vision-language-action (VLA) models without retraining. RoVer uses a compact process reward model (PRM) as an external verifier to score and refine candidate actions during inference. The PRM predicts both a scalar process reward and a 6D refinement direction in the action space. During inference, RoVer generates multiple candidate actions from the base policy, expands them along the PRM-predicted directions, and selects the highest-scoring action. By caching shared perception features, RoVer amortizes computation and scales efficiently. Experiments on the CALVIN benchmark show consistent improvements across different base policies (GR-1, Dita, MoDE), with success rates increasing by up to 18.4% on long-horizon tasks. Real-robot experiments demonstrate generalization gains on pick-and-place, button pushing, and bowl stacking tasks. RoVer achieves substantial inference speedups through shared perception caching while maintaining the original model weights.

## Method Summary
RoVer is a test-time scaling framework for frozen VLA models that uses a Process Reward Model (PRM) to verify and refine candidate actions at inference. The PRM, initialized from GR-1 and trained on 20% of CALVIN data, outputs scalar rewards and 6D refinement directions for action candidates. At each control step, RoVer generates multiple candidate actions from the base policy, expands them along predicted directions, scores all candidates using the cached perception features, and selects the highest-scoring action. The framework amortizes computation by caching shared perception features (vision encoder, text encoder, resampler) and only re-computing action embeddings per candidate. This enables efficient evaluation of more candidates under fixed latency budgets while maintaining the original model weights.

## Key Results
- RoVer improves GR-1 success rates by 18.4% on long-horizon CALVIN tasks with 1000 candidates
- Shared perception caching achieves 7.23× speedup at 1000 candidates, reducing per-action cost from ~37ms to ~5.1ms
- Cross-backbone generalization: RoVer trained on GR-1 demonstrations improves Dita (+6.8%) and MoDE (+7.5%) without retraining
- Real-robot experiments show success rates of 71.4% (pick-and-place), 77.8% (button push), and 74.1% (bowl stacking)

## Why This Works (Mechanism)

### Mechanism 1: Direction-guided expansion improves sample efficiency
- Claim: Direction-guided expansion improves sample efficiency over random Gaussian perturbation by concentrating candidates in higher-value action regions.
- Mechanism: The PRM predicts a normalized 6D direction vector $\hat{u}$ pointing toward potentially better actions. Candidates are sampled within a bounded angular region around this direction, combining the predicted direction with orthogonal noise. This biases exploration toward the PRM's estimated improvement direction while preserving diversity.
- Core assumption: The predicted direction correlates with actual improvement toward expert-like actions.
- Evidence anchors: [abstract]: "predicts an action-space direction for candidate expansion/refinement"; [Section 3.2.3]: "direction guidance (DG) further improves sample efficiency by focusing exploration... Under equal K, DG reliably outperforms unguided Gaussian expansion on GR-1 and Dita"; [corpus]: Related work on adaptive test-time compute allocation confirms direction-guided search improves efficiency (arXiv:2602.01070).
- Break condition: If the PRM's direction predictions become uncorrelated with actual improvement (e.g., distribution shift between training and deployment), guided sampling may perform worse than random by systematically exploring wrong regions.

### Mechanism 2: Scalar process rewards enable Best-of-N selection
- Claim: Scalar process rewards enable Best-of-N selection by ranking candidates according to predicted quality.
- Mechanism: The PRM outputs a scalar reward $r_i$ for each candidate action given observation history. At inference, the candidate with maximum reward is selected: $a^* = \arg\max_{a \in A} r(h, a)$. The reward is trained via Bradley-Terry preference loss to order actions by proximity to expert demonstrations.
- Core assumption: Actions closer to expert demonstrations in action space are more likely to succeed; the learned reward generalizes to policy-generated candidates not seen during training.
- Evidence anchors: [abstract]: "assigns scalar-based process rewards to evaluate the reliability of candidate actions"; [Section 3.2.2]: "we regard one action as better if its root mean squared error (RMSE) distance to the expert action is smaller"; [corpus]: PRM-based verification for test-time scaling is established in LLMs (arXiv:2508.16665 survey), though robotics-specific validation remains limited.
- Break condition: If expert proximity is a poor proxy for task success (e.g., multiple valid solutions exist, or expert demonstrations are suboptimal), the ranking will be misaligned with actual performance.

### Mechanism 3: Shared perception caching amortizes computation
- Claim: Shared perception caching amortizes computation, enabling evaluation of more candidates under fixed latency budgets.
- Mechanism: All candidates at a control step share the same observation, language, and robot state. The PRM computes perceptual features once (vision encoder + text encoder + resampler), caches them, and only re-computes action embeddings per candidate. This reduces per-candidate cost from full forward passes to lightweight action encoding.
- Evidence anchors: [abstract]: "by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget"; [Table 2]: With caching, per-action cost stabilizes at ~5.7–6.2ms across 10–10,000 candidates; speedup reaches 7.23× at 1000 candidates; [corpus]: No directly comparable caching analysis in neighboring papers; this appears to be a system-level contribution specific to this work.
- Break condition: If action representations require context-dependent perception (e.g., attention over spatial features conditioned on action), caching becomes invalid and must be recomputed.

## Foundational Learning

- Concept: **Process Reward Models (PRMs)**
  - Why needed here: RoVer's verifier is a PRM that provides step-level supervision. Understanding how PRMs differ from outcome rewards is essential.
  - Quick check question: Given a sequence of actions, would a PRM score each step or only the final outcome?

- Concept: **Test-Time Scaling (Best-of-N, Beam Search)**
  - Why needed here: RoVer instantiates external TTS via candidate generation and selection. The tradeoff between compute budget and performance is central.
  - Quick check question: If you double the number of candidates from N to 2N, would you expect linear improvement in success rate? Why or why not?

- Concept: **Action Space Representations (6D pose, delta actions)**
  - Why needed here: The PRM predicts directions in the 6D pose subspace and operates on delta actions. Understanding coordinate frames (world vs. local) is critical for correct integration.
  - Quick check question: A policy outputs actions in its end-effector frame. How would you adapt these for a PRM trained on world-frame actions?

## Architecture Onboarding

- Component map: MAE vision encoder + CLIP text encoder → Perceiver Resampler → cached features; Action Amplifier MLP (H→2H→H) → GPT-2 style transformer (12 layers, 384 hidden dim, 12 heads) → Reward head (H→H/2→H/4→1) and Direction head (H→H/2→d_dir)

- Critical path: 1. Pre-encode observation/language/state once per step (cache z_obs); 2. For each candidate: encode action through amplifier → fuse with cached z_obs → forward through transformer → read reward/direction tokens; 3. Select highest-scoring candidate; if direction-guided, expand and re-score

- Design tradeoffs: PRM size vs. speed: 0.2B params (40M trainable) chosen for efficiency; larger PRMs (e.g., RoboMonkey's 7B) may score better but incur higher latency; Chunk vs. step scoring: MoDE outputs action chunks; RoVer scores only the first action per chunk, limiting intervention within chunks; Noise scale σ_base = 0.1: Calibrated to policy-expert gap; too small yields uninformative pairs, too large creates off-manifold samples

- Failure signatures: Reward collapse: Constant reward outputs across candidates (mitigated by Action Amplifier); Direction misalignment: High angle error between predicted and ground-truth directions (monitor cosine alignment during validation); Chunk-step mismatch: Non-monotonic performance gains with increasing N+M (observed for MoDE)

- First 3 experiments: 1. Ablate direction guidance: Compare random vs. direction-guided sampling with equal candidate budget K=N+M on GR-1 backbone; 2. Scale candidate budget: Sweep K ∈ {1, 3, 5, 10, 50, 100} and measure success rate degradation and latency; 3. Cross-backbone transfer: Train PRM on GR-1 demonstrations; evaluate on Dita and MoDE without retraining

## Open Questions the Paper Calls Out

- Question: Can the PRM be adapted to provide guidance within action chunks for policies like MoDE that output multi-step trajectories, rather than scoring only the first action of each chunk?
- Question: Can task-grounded success signals be incorporated into PRM training instead of relying solely on expert action proximity as a supervision proxy?
- Question: How does PRM performance scale with training data quantity, given that experiments used only 20% of the CALVIN training set?
- Question: Can RoVer generalize to novel robot embodiments with different action space dimensionalities and kinematic structures beyond the 6D pose + gripper formulation tested?

## Limitations

- The PRM training relies on synthetic better/worse action pairs generated through anchor-centered sampling, raising questions about generalization to truly novel failure modes
- Non-monotonic scaling behavior with MoDE backbone suggests fundamental mismatches between chunk-level outputs and step-level scoring
- Real-robot evaluation is limited to three tasks without extensive cross-dataset validation or ablation of individual components

## Confidence

- **High confidence**: Core mechanism of direction-guided candidate expansion and shared perception caching
- **Medium confidence**: Generalization claims across different base policies (improvements vary significantly: GR-1 +18.4%, Dita +6.8%, MoDE +7.5%)
- **Low confidence**: Real-robot generalization claims (limited analysis of failure cases or systematic testing)

## Next Checks

1. Measure cosine similarity between predicted and ground-truth refinement directions on held-out validation set; if average alignment drops below 0.5, direction-guided expansion may be unreliable
2. Fix computational budget (e.g., 100ms per step) and compare RoVer variants: Best-of-N without direction guidance, direction-guided sampling without caching, and full RoVer to isolate component contributions
3. Train PRM on CALVIN and evaluate on held-out environment with different visual appearances; measure success rate degradation and identify failure modes (reward collapse, direction misalignment)