---
ver: rpa2
title: 'On the logical skills of large language models: evaluations using arbitrarily
  complex first-order logic problems'
arxiv_id: '2502.14180'
source_url: https://arxiv.org/abs/2502.14180
tags:
- arxiv
- statement
- test
- https
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for generating arbitrarily complex
  first-order logic statements involving set membership, subset, and proper subset
  relations. The authors create several test sets of such statements and evaluate
  various large language models (LLMs) on them.
---

# On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems

## Quick Facts
- arXiv ID: 2502.14180
- Source URL: https://arxiv.org/abs/2502.14180
- Reference count: 40
- Key outcome: Large language models show strong performance (>90% accuracy) on simple first-order logic problems but degrade significantly to 50-70% accuracy on statements with 8-10 conjuncts, suggesting limitations in complex logical reasoning.

## Executive Summary
This paper introduces a novel method for generating arbitrarily complex first-order logic statements involving set membership, subset, and proper subset relations, then evaluates various large language models on these benchmarks. The evaluation framework maps logical statements to labeled graphs where truth is determined by the absence of cycles involving strict relations, allowing controlled difficulty scaling. The authors find that while state-of-the-art models like Claude 3.5 Sonnet and DeepSeek-V3 achieve high accuracy on simpler statements, their performance degrades significantly as complexity increases. The datasets and evaluation code are publicly available for further research.

## Method Summary
The authors generate synthetic first-order logic statements by constructing labeled graphs with vertices representing variables and edges representing relations (∈, ⊆, ⊂≠). The truth of a statement is determined by whether the corresponding graph contains cycles involving "element" or "proper subset" relations. Test sets are created with varying complexity parameters (number of variables, conjuncts, relation types). Models are evaluated using multiple prompting strategies (yes/no, chain-of-thought) and encodings (Unicode, LaTeX, natural language words). Answers are extracted using GPT-4o-mini classification, with manual validation for ambiguous cases.

## Key Results
- State-of-the-art models achieve >90% accuracy on simple statements with few variables and conjuncts
- Performance degrades to 50-70% accuracy for statements with 8-10 conjuncts
- Chain-of-thought prompting improves accuracy by an average of 17% across models
- Introduction of negations causes performance to collapse to near-random levels (~50%) for most models
- Encoding choice (Unicode vs. natural language words) affects performance differently across model families

## Why This Works (Mechanism)

### Mechanism 1: Complexity Scaling via Cycle Detection
The method generates valid logical benchmarks by mapping FOL statements to directed labeled graphs, where the truth of the statement is strictly determined by the existence of specific cycles. A graph G with vertices (variables) and labeled edges (relations: ∈, ⊆, ⊂≠) is constructed. The statement is false if and only if the graph contains a cycle involving an "element" (∈) or "proper subset" (⊂≠) relation. By controlling the minimum cycle length β(G, ℓ), the authors control the difficulty of the proof required to disprove the statement. The difficulty of a logical problem scales with the depth of the dependency chain (cycle length) an evaluator must track.

### Mechanism 2: Symbolic vs. Natural Language Processing
Models exhibit different failure modes depending on whether the logical structure is presented as dense symbolic notation (Unicode/LaTeX) or natural language ("words"), with smaller models relying more on linguistic patterns. The paper tests different encodings. While symbolic encodings (Unicode) are standard, the "words" encoding maps logical operators to sentences (e.g., "A is a subset of B"). This reduces the density of the context but increases token count, potentially shifting the reliance from pattern matching on symbols to semantic understanding of text. Models trained primarily on code/text may find "words" encodings more "natural" for activating reasoning capabilities, while symbolic encodings better test strict logical rule adherence.

### Mechanism 3: Chain-of-Thought (CoT) Error Recovery
CoT prompting improves accuracy on complex statements primarily by allowing the model to verify intermediate relations, though it fails to fix fundamental limitations in handling negations. CoT forces the model to output intermediate steps (e.g., "Assume for the sake of contradiction..."). This allows the model to effectively "simulate" a larger inference depth. However, Section 4.2 shows that when negations are introduced, even CoT prompts result in near-random performance (50%), suggesting the mechanism cannot resolve structural logical confusion. The model possesses the internal capability to verify individual logical steps but fails to synthesize them in a single forward pass without CoT.

## Foundational Learning

- **Concept: Zermelo–Fraenkel (ZF) Set Theory Axioms**
  - **Why needed here:** The benchmark evaluates statements specifically in ZF theory. Understanding the *Axiom of Regularity* (which forbids set self-membership X ∈ X) is crucial because the paper uses this to generate "False" examples (cycles) that models must detect.
  - **Quick check question:** Does the statement ∃X: X ∈ X hold in ZF set theory? (Answer: No, by Regularity).

- **Concept: First-Order Logic (FOL) Quantifier Scope**
  - **Why needed here:** The paper generates complex statements with alternating quantifiers (∃A: ¬ (∀B: ...)). A model must track variable scopes to correctly evaluate truth. Section 3.1 formally defines how these are constructed.
  - **Quick check question:** In the formula ∃X ∀Y (X ∈ Y), is the variable X free or bound?

- **Concept: Directed Acyclic Graphs (DAGs)**
  - **Why needed here:** The paper maps logical satisfiability to graph properties (Corollary 2.11). A statement is "True" (satisfiable) if the corresponding graph allows a topological ordering (i.e., is acyclic regarding "strict" relations).
  - **Quick check question:** If a graph has edges A → B, B → C, and C → A, does it have a topological ordering?

## Architecture Onboarding

- **Component map:** Generator -> Translator -> Evaluator -> Classifier
- **Critical path:** The relationship between the parameter β(G, ℓ) (minimum cycle length) and model accuracy. If this correlation breaks (e.g., model fails simple cycles but passes complex ones), the evaluation is invalid.
- **Design tradeoffs:**
  - Encoding: Unicode is efficient but dense; "Words" is verbose but may trigger better reasoning in base models
  - Prompting: Yes/No is cheap/fast but brittle; CoT is expensive but significantly more accurate (Table 4.5)
  - Negation: Adding just one negation drops SOTA models from >90% to ~50% accuracy (Section 4.2). Use negation only to test robust reasoning, not general capability
- **Failure signatures:**
  - Random Baseline: If a model answers ~50% on the "Membership Grid" (Section 4.1), it is likely guessing randomly rather than reasoning
  - Negation Blindness: If performance on "Membership Grid" is high but "Membership Grid with Negation" drops to chance (Section 4.2), the model lacks structural logical coherence
  - Refusal Loop: Models returning "UNDECIDABLE" or "VAGUE" often indicates the context window is overwhelmed or the instruction prompt was insufficiently restrictive
- **First 3 experiments:**
  1. Sanity Check (Membership Grid): Run the "Membership Grid" test set (Section 3.2.1) on the target model with 0-shot/Unicode. Verify the degradation curve (should slope down as conjuncts increase from 2 to 10)
  2. Ablation (Negation): Test the "Membership Grid with Negation" (Section 3.2.2) to determine the model's robustness to quantifier manipulation. Expect a significant drop
  3. Reasoning Scaling: Compare 0-shot vs. CoT performance on the "Hard" test set (Section 4.5) to measure the "reasoning gain" (e.g., for DeepSeek-V3, this was a +17% jump)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning on these synthetic first-order logic datasets improve an LLM's general reasoning capabilities on out-of-distribution mathematical benchmarks?
- **Basis in paper:** [explicit] The introduction states the datasets are intended for "the training of specialized LLMs and reasoning models," but the paper only evaluates pre-trained models.
- **Why unresolved:** The paper provides the generation method and evaluations but does not conduct or report on training experiments using the generated data.
- **What evidence would resolve it:** A comparison of model performance on external benchmarks (e.g., MATH, GSM8K) before and after fine-tuning on the Γn,mL,k,b datasets.

### Open Question 2
- **Question:** What specific failure mode causes the performance of frontier models to collapse to near-random levels when a single negation is introduced into the membership grid test set?
- **Basis in paper:** [inferred] Section 4.2 and Table 4.2 show that while models handle positive statements well, their accuracy drops to ~50% (random chance) immediately upon the introduction of one negation.
- **Why unresolved:** The paper quantifies the performance drop but does not analyze the internal attention mechanisms or reasoning traces to explain why negation is such a disruptive factor.
- **What evidence would resolve it:** An interpretability analysis (e.g., probing classifiers or attention visualization) identifying whether models fail at quantifier negation, logical equivalence transformation, or maintaining consistency.

### Open Question 3
- **Question:** Do "reasoning models" (like o1 or DeepSeek-R1) solve the "hard" test set by approximating the graph-theoretic cycle detection algorithm defined in Theorem 2.18?
- **Basis in paper:** [inferred] Table 4.7 shows reasoning models achieving >95% accuracy on the hard test set, whereas standard models fail, suggesting a qualitative shift in reasoning strategy.
- **Why unresolved:** The proprietary nature of these models prevents the authors from determining if the success is due to test-time compute scaling or the discovery of the underlying algorithmic solution (cycle detection).
- **What evidence would resolve it:** Testing these models on isomorphic problems that obscure the graph structure to see if the "reasoning" capability relies on recognizing the specific mathematical pattern or applying general logic.

## Limitations
- The synthetic nature of generated statements may not fully capture real-world logical reasoning complexity
- Binary classification task (true/false) oversimplifies nuanced logical interpretations
- Reliance on GPT-4o-mini for answer extraction introduces potential bias, though mitigated through manual validation

## Confidence
- **High Confidence:** The degradation pattern in model performance as complexity increases (statements with 8-10 conjuncts showing 50-70% accuracy) is robust and consistent across multiple models and test sets
- **Medium Confidence:** The comparative advantage of Chain-of-Thought prompting (average +17% improvement) is well-demonstrated, though individual model responses vary significantly
- **Medium Confidence:** The encoding effects (Unicode vs. Words) show measurable differences, but the practical significance varies by model family

## Next Checks
1. Test models on a subset of the evaluation set where all statements have been manually verified for logical consistency, particularly focusing on edge cases with negations and quantifiers
2. Implement an alternative answer extraction method (e.g., rule-based extraction) to validate the GPT-4o-mini classifier results, especially for CoT responses classified as vague or undecidable
3. Conduct cross-linguistic evaluation by translating test statements into different natural languages to assess whether performance degradation correlates with linguistic complexity rather than logical complexity