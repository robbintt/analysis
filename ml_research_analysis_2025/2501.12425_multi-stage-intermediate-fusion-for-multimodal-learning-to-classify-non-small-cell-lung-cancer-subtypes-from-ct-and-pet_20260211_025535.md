---
ver: rpa2
title: Multi-stage intermediate fusion for multimodal learning to classify non-small
  cell lung cancer subtypes from CT and PET
arxiv_id: '2501.12425'
source_url: https://arxiv.org/abs/2501.12425
tags:
- fusion
- feature
- lung
- cancer
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multi-stage intermediate fusion approach
  to classify NSCLC subtypes from CT and PET images. Our method integrates the two
  modalities at different stages of feature extraction, using voxel-wise fusion to
  exploit complementary information across varying abstraction levels while preserving
  spatial correlations.
---

# Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET

## Quick Facts
- arXiv ID: 2501.12425
- Source URL: https://arxiv.org/abs/2501.12425
- Reference count: 37
- This study presents a multi-stage intermediate fusion approach to classify NSCLC subtypes from CT and PET images, achieving accuracy of 0.724 and AUC of 0.681.

## Executive Summary
This study introduces a multi-stage intermediate fusion architecture for classifying non-small cell lung cancer (NSCLC) subtypes using paired CT and PET images. The method integrates modalities at multiple feature extraction stages through voxel-wise multiplication, preserving spatial correlations while exploiting complementary information across abstraction levels. Compared to unimodal approaches and traditional early/late fusion strategies, the proposed method demonstrates superior performance with an accuracy of 0.724 and AUC of 0.681. The approach shows particular promise for non-invasive diagnosis and could advance personalized treatment decisions in lung cancer management.

## Method Summary
The method employs a dual-branch 3D ResNet architecture with three parallel streams processing CT and PET volumes. Custom fusion blocks integrate the modalities at three distinct stages using voxel-wise multiplication of squeezed feature maps, followed by residual addition back to each branch. The network processes 3D volumes with voxel spacing standardized to 0.977×0.977×3.27mm, using 16→32→64 feature maps across stages. Training uses stratified 5-fold cross-validation with Adam optimizer (lr=0.001, decayed every 25 epochs), class-weighted loss to address imbalance (76% ADC vs 24% SQC), and lung segmentation preprocessing with CT[-1024,1024] and PET[0,20] normalization.

## Key Results
- Multi-stage intermediate fusion achieves accuracy of 0.724 and AUC of 0.681
- Outperforms single-stage intermediate fusion method (accuracy 0.539) and unimodal baselines (CT: 0.607, PET: 0.624)
- Surpasses early and late fusion strategies while preserving spatial correlations
- Demonstrates robustness to class imbalance through Gmean optimization (0.646)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-stage intermediate fusion likely outperforms single-stage or late fusion by enabling gradual information exchange while retaining spatial context.
- **Mechanism:** The architecture introduces fusion blocks at multiple stages (L) of the feature extraction hierarchy. Instead of fusing once at the end (late) or the start (early), features are combined at lower, middle, and higher abstraction levels. This allows the network to refine unimodal features based on cross-modal context repeatedly.
- **Core assumption:** CT and PET images possess spatial correlations that are lost when features are flattened into vectors early (as in single-stage intermediate fusion) or ignored until the decision layer (late fusion).
- **Evidence anchors:** [Abstract]: "integrates the two modalities at different stages of feature extraction... preserving spatial correlations." [Section 5]: Shows the proposed method outperforms the single-stage intermediate fusion method by Qin et al. [25] (Accuracy .724 vs .539).

### Mechanism 2
- **Claim:** Voxel-wise multiplication in the fusion block acts as a learned attention mechanism, suppressing background and highlighting regions where structural (CT) and metabolic (PET) features coincide.
- **Mechanism:** Within the fusion block, 3D feature maps from both modalities are squeezed via $1\times1\times1$ convolutions to a single channel. These single-channel maps are multiplied element-wise. High values in the resulting fused map likely indicate regions where both modalities "agree" on the presence of a significant feature.
- **Core assumption:** Diagnostically relevant features for NSCLC subtypes (ADC vs. SQC) are spatially localized and exhibit distinct, correlated patterns in both CT density and PET uptake.
- **Evidence anchors:** [Section 4.2.2]: Describes the element-wise multiplication ($\otimes$) of the normalized feature maps. [Figure 1 (d)]: Visualizes the Fusion Block flow.

### Mechanism 3
- **Claim:** Feeding fused features back into the individual modality branches (residual guidance) allows the network to learn modality-specific features that are contextually relevant to the paired modality.
- **Mechanism:** The fused feature map is added back to the original input feature maps ($CT_{in}$ and $PET_{in}$) via residual connections (Equations 1 & 2). This modifies the input for the next block, ensuring that the CT branch "sees" PET information and vice versa before the next layer of convolution.
- **Core assumption:** The feature extraction process for one modality (e.g., texture in CT) becomes more precise when conditioned on the state of the other modality (e.g., intensity in PET).
- **Evidence anchors:** [Section 4.2.2]: Equations show $CT_{out} = CT_{in} \oplus (\dots)$, confirming the fused signal is added back to the stream. [Section 4.2 Intro]: Mentions features are "redistributed across the unimodal backbones."

## Foundational Learning

- **Concept: Multimodal Fusion Strategies (Early vs. Late vs. Intermediate)**
  - **Why needed here:** The paper explicitly positions itself against early and late fusion. You must understand that early fusion mixes raw pixels (losing distinct traits), late fusion mixes final probabilities (missing deep interactions), and intermediate fusion mixes features.
  - **Quick check question:** If you averaged the output softmax scores of a CT model and a PET model, which fusion strategy would you be implementing?

- **Concept: 3D Convolutions (Conv3D)**
  - **Why needed here:** The model operates on 3D voxel volumes ($x, y, z$) rather than 2D slices. Standard 2D CNNs would fail to capture the depth-wise spatial continuity of the tumor.
  - **Quick check question:** How does the parameter count and memory footprint of a $3\times3\times3$ kernel compare to a $3\times3$ kernel, and what does this imply for the batch size you can fit on a GPU?

- **Concept: Residual Connections**
  - **Why needed here:** The feature extraction blocks use ResNet-style skip connections, and the fusion blocks use residual additions. This is critical for allowing the gradient to flow through the "deep" hierarchy of stages ($L$) without vanishing.
  - **Quick check question:** In Equation 1, why is $CT_{in}$ added to the fusion result rather than just returning the fusion result?

## Architecture Onboarding

- **Component map:** Input (aligned CT & PET volumes) -> Dual 3D ResNet streams -> Fusion Blocks at stage boundaries -> Global Average Pooling -> Fully Connected Layer (2 neurons)

- **Critical path:** The Pre-processing alignment (Section 4.1) is strictly required before the network. If the PET and CT voxels do not correspond to the same physical space, the "voxel-wise" multiplication in the Fusion Block becomes meaningless noise. The Fusion Block (Section 4.2.2) is the core novelty—specifically the combination of Squeeze ($1\times1\times1$ Conv), Multiplication, and Residual Addition.

- **Design tradeoffs:** The authors performed a grid search on $L \in [1,5]$ and $N \in [1,5]$. They found $L=1$ performed worst, suggesting depth is essential for fusion benefits, but settled on $L=3$. A new engineer must balance $L$ (model capacity) against available GPU memory, as 3D volumes are memory-intensive.

- **Failure signatures:**
  - **Class Collapse/Imbalance:** If the model achieves high accuracy but low G-mean (like the LUCY baseline in Table 2), it is predicting only the majority class (ADC). The authors mitigated this using class weights and monitoring G-mean.
  - **Fusion Divergence:** If the fusion block outputs become NaN, check the learning rate or lack of Batch Normalization in the fusion path (though BN is present in the paper's design).

- **First 3 experiments:**
  1. **Unimodal Baselines:** Train the CT-Branch and PET-Branch separately (set $L=3, N=3$ but disable Fusion Blocks). Establish the lower bound of performance.
  2. **Ablation on Fusion Depth:** Compare performance with $L=1$ (single fusion point) vs. $L=3$ (multi-stage) to validate the core claim that multi-stage fusion is superior.
  3. **Fusion Strategy Comparison:** Implement a simple Late Fusion (averaging outputs of the unimodal models from Exp 1) to verify that the complexity of the intermediate architecture is justified by a performance lift.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the integration of genomics data with the proposed multimodal imaging features improve classification accuracy and provide a better understanding of tumor biology?
  - **Basis in paper:** [explicit] The conclusion states the authors aim "to refine the proposed multimodal approach by integrating genomics data with imaging features to improve classification accuracy."
  - **Why unresolved:** The current study is limited to imaging modalities (CT and PET) and does not incorporate biological or genomic markers.
  - **What evidence would resolve it:** A comparative study evaluating the model's performance with and without genomic data inputs on the same patient cohort.

- **Open Question 2:** Can the multi-stage intermediate fusion architecture generalize effectively to classify additional NSCLC subtypes beyond adenocarcinoma (ADC) and squamous cell carcinoma (SQC)?
  - **Basis in paper:** [explicit] The authors identify the need "to enhance the model's generalizability" by planning "to expand the dataset to include additional NSCLC subtypes."
  - **Why unresolved:** The current experimental design is restricted to binary classification between the two most common subtypes, excluding less common histologies.
  - **What evidence would resolve it:** Performance metrics (Accuracy, AUC) derived from a multi-class classification task involving rarer NSCLC subtypes.

- **Open Question 3:** How sensitive is the voxel-wise fusion mechanism to residual spatial misalignments between CT and PET scans caused by respiratory motion?
  - **Basis in paper:** [inferred] The paper acknowledges "slight misalignments caused by respiratory motion" but relies on precise element-wise multiplication for fusion, which assumes high spatial correspondence.
  - **Why unresolved:** It is unclear if the fusion block's performance degrades significantly when the assumed spatial correlation is imperfect despite preprocessing alignment.
  - **What evidence would resolve it:** A robustness analysis testing classification performance on datasets with intentionally introduced spatial translations or elastic deformations.

## Limitations
- Small dataset size (714 patients total) with class imbalance (76% ADC vs 24% SQC) raises overfitting concerns despite cross-validation
- Critical dependence on precise spatial alignment between CT and PET voxels for voxel-wise fusion operation
- Method requires paired CT/PET acquisition, limiting generalizability to settings with only one modality available

## Confidence
- **High Confidence:** The superiority of multi-stage intermediate fusion over single-stage intermediate fusion (demonstrably higher accuracy: 0.724 vs 0.539)
- **Medium Confidence:** The overall superiority over unimodal baselines and other fusion strategies (limited by small dataset size and class imbalance)
- **Low Confidence:** The generalizability to other cancer types or different imaging protocols (only validated on NSCLC with specific CT/PET parameters)

## Next Checks
1. **Robustness to Misalignment:** Intentionally introduce controlled spatial misalignment between CT and PET volumes and measure degradation in classification performance.
2. **Ablation Study on Fusion Stages:** Systematically vary L (fusion stages) from 1 to 5 while keeping N constant to precisely quantify the relationship between fusion depth and performance.
3. **External Validation:** Apply the trained model to an independent, geographically distinct dataset of NSCLC patients to assess real-world generalizability and potential overfitting.