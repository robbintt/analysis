---
ver: rpa2
title: Missing vs. Unused Knowledge Hypothesis for Language Model Bottlenecks in Patent
  Understanding
arxiv_id: '2505.12452'
source_url: https://arxiv.org/abs/2505.12452
tags:
- knowledge
- patent
- questions
- patents
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines why large language models struggle with patent
  classification tasks that require deep conceptual understanding. The authors introduce
  a diagnostic framework that separates model failures into "missing knowledge" (information
  not present in the model) versus "unused knowledge" (information the model has but
  fails to deploy effectively).
---

# Missing vs. Unused Knowledge Hypothesis for Language Model Bottlenecks in Patent Understanding

## Quick Facts
- arXiv ID: 2505.12452
- Source URL: https://arxiv.org/abs/2505.12452
- Reference count: 35
- Key outcome: Most LLM errors on patent classification stem from failures to deploy existing knowledge rather than true knowledge gaps

## Executive Summary
This study introduces a diagnostic framework to distinguish between "missing knowledge" (information not present in the model) and "unused knowledge" (information the model has but fails to deploy effectively) in LLM patent understanding. The authors find that most errors arise from the latter—models possess the necessary information but struggle to access and apply it. By prompting models to generate clarifying questions and comparing performance across three conditions (raw, self-answered, and externally retrieved answers), the study reveals that smaller models generate more transferable questions while larger models produce richer but less generalizable questions. This work shifts evaluation from static fact recall to dynamic knowledge application, offering a more informative view of model capabilities.

## Method Summary
The study evaluates LLM patent classification by sampling semantically similar but distinct patent pairs from USPTO corpus using citation-pretrained embeddings. Models are tested across three conditions: raw baseline judgment, self-question-answering (where models generate and answer their own clarifying questions), and scientific retrieval-augmented generation (where answers come from arXiv scientific papers). Performance is measured using misjudgment rate with confidence-weighted voting across three independent generations. The framework distinguishes between errors due to missing knowledge versus unused knowledge by comparing the effectiveness of self-generated versus externally retrieved answers.

## Key Results
- Most LLM errors stem from failures to deploy existing knowledge rather than true knowledge gaps
- Smaller models generate simpler, more transferable questions that larger models can use effectively
- Larger models produce richer but less generalizable questions
- Self-questioning substantially improves performance by activating latent knowledge

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Activation Through Structured Self-Questioning
Providing models with self-generated question-answer pairs improves task performance by activating latent knowledge that exists in parameters but remains inaccessible during direct inference. The prompt to generate and answer clarifying questions creates retrieval cues that reorganize access to compressed representations, bypassing the default superficial pattern-matching mode. Core assumption: Relevant knowledge was acquired during pretraining and persists in compressed form. Evidence: Most errors stem from failures to deploy existing knowledge rather than true knowledge gaps, with substantial gains from self-QA implying knowledge exists but isn't effectively utilized unless explicitly prompted.

### Mechanism 2: Compression Richness Gap Between Parametric and Retrieved Knowledge
Self-generated answers are shorter and less detailed than externally retrieved scientific answers, yet occupy similar embedding regions, suggesting compression rather than absence. Training corpora compress rare scientific knowledge unevenly; the model retains semantic gist but loses informational richness, which external retrieval can supplement. Core assumption: The model has encountered similar scientific content during pretraining (86% of retrieved papers predated LLaMA 3.2 training cutoff). Evidence: Scientific answers average 62.1±1.8 words while self-generated answers average only 38.2±0.6 words—approximately half the length, with embedding regions largely overlapping.

### Mechanism 3: Cross-Scale Question Transfer Asymmetry
Smaller models generate simpler, more fundamental questions that improve performance across model scales; larger models generate complex questions that confuse smaller models. Constrained capacity forces smaller models toward high-entropy, open-ended questions that retrieve broader information; larger models over-specify based on their richer internal representations. Core assumption: Question quality correlates with the answering model's capacity to interpret and use the question. Evidence: 68% of 1B's questions contain "what is" or "what are" compared to only 44% for 405B, with questions from smaller models often aligning well with answering capacity of larger models.

## Foundational Learning

- **Lay-in vs. Working Knowledge**
  - Why needed: The core diagnostic framework distinguishes what a model *can produce when prompted* (lay-in) from what it *actually uses during tasks* (working). Without this distinction, RAG appears to solve knowledge gaps when it may only be scaffolding deployment.
  - Quick check: If a model answers "What is X?" correctly but fails to use X in a downstream classification, which knowledge type is deficient?

- **Confidence-Weighted Voting**
  - Why needed: The paper aggregates multiple generations using confidence scores rather than simple majority voting, which better reflects model certainty and accelerates convergence to stable judgments.
  - Quick check: Why would higher confidence sometimes correlate with wrong answers (the "overconfidence" effect in Figure 2)?

- **Differentiation as Understanding Probe**
  - Why needed: The task operationalizes a cognitive science principle: understanding requires distinguishing concepts, not just recalling facts. Patent pairs with high surface similarity but verified conceptual distinction test this directly.
  - Quick check: Why is the "merging" task (recognizing rewritten patents as equivalent) easier than the "splitting" task (distinguishing similar patents)?

## Architecture Onboarding

- **Component map**: Patent pair selector -> Question generator -> Answer module (Self-QA OR Scientific-RAG) -> Judge -> Confidence-weighted voting
- **Critical path**: 1) Sample patent pair with cosine similarity >0.8, 2) Generate questions → retrieve scientific chunks OR self-answer, 3) Feed QA pairs + patents to judge, 4) Compare error rates across baseline / self-QA / scientific-RAG
- **Design tradeoffs**: More questions vs. noise (diminishing returns), small vs. large question models (transferability vs. richness), retrieval vs. self-activation (marginal gains vs. infrastructure requirements)
- **Failure signatures**: High perplexity on patents indicates unfamiliarity with dense legal-technical language, response bias (some models default to "yes" or "no"), overconfidence on errors (smaller models more confident on wrong answers)
- **First 3 experiments**: 1) Baseline calibration on 5000 pairs, 2) Self-QA vs. Scientific-RAG comparison on 500 pairs with embedding overlap computation, 3) Cross-scale question transfer test (smaller model questions fed to larger model and vice versa)

## Open Questions the Paper Calls Out

1. What specific semantic threshold (e.g., functionality, application, or terminology) do LLMs utilize when successfully resolving distinctions between similar patents? The authors state the "exact nature of 'resolution' remains underdefined" and call for evaluating what kinds of conceptual distinctions LLMs are making.

2. How does the bottleneck of "unused knowledge" vary across patent domains with differing reliance on jargon, such as biomedical versus computer science patents? The Limitations section notes it will be "interesting to look into differences in judgment performance across patent classes" as internal knowledge distributions likely vary.

3. Can models trained via reinforcement learning to autonomously refine retrieval queries overcome the knowledge deployment bottleneck more effectively than static self-questioning? The Limitations section suggests LLMs could learn to refine or generate better retrieval queries via reinforcement learning in an agentic framing.

## Limitations
- The diagnostic framework's separation of missing versus unused knowledge relies on assumptions about self-questioning effectiveness that lack rigorous ablation studies
- The 5,000-pair sample represents only a subset of available patents, potentially limiting generalizability across different technical fields
- The study's reliance on arXiv papers as a scientific corpus may not fully represent patent-relevant knowledge breadth

## Confidence
- **High**: Empirical observation that most errors stem from failures to deploy existing knowledge is well-supported by consistent performance gains from self-generated question-answer pairs
- **Medium**: Compression-richness gap between parametric and retrieved knowledge is plausible but interpretation of hallucinated content requires validation
- **Low**: Cross-scale question transfer asymmetry has limited direct corpus evidence and the mechanism is not definitively proven

## Next Checks
1. **Ablation Study on Question Generation**: Test whether performance gains persist when models generate fewer questions (2-3 instead of 6) or when questions are randomly selected from a pool, to isolate the effect of question quality versus quantity.

2. **Domain Generalization Test**: Apply the diagnostic framework to patent pairs from domains outside the arXiv corpus (e.g., post-2022 patents or specialized technical fields) to validate whether the "unused knowledge" hypothesis holds when genuine knowledge gaps exist.

3. **Alternative Knowledge Activation Methods**: Compare self-QA against other knowledge activation techniques such as chain-of-thought prompting, retrieval-augmented generation with different corpora, or fine-tuning on patent-specific data to determine if observed gains are unique to the question-answering approach.