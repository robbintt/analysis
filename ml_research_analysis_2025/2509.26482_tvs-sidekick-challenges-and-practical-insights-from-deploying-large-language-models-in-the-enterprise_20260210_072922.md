---
ver: rpa2
title: 'TVS Sidekick: Challenges and Practical Insights from Deploying Large Language
  Models in the Enterprise'
arxiv_id: '2509.26482'
source_url: https://arxiv.org/abs/2509.26482
tags:
- sidekick
- data
- language
- llms
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TVS SCS UK developed TVS Sidekick, an in-house AI assistant using
  large language models (LLMs) with retrieval-augmented generation (RAG) to answer
  queries from enterprise data sources. The system integrates SharePoint, Azure DevOps,
  code repositories, and company websites into a vector database, enabling semantic
  search and code-specific prompt engineering.
---

# TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise

## Quick Facts
- arXiv ID: 2509.26482
- Source URL: https://arxiv.org/abs/2509.26482
- Reference count: 14
- Primary result: TVS SCS UK developed an AI assistant integrating SharePoint, Azure DevOps, and code repositories to answer enterprise queries, with 750+ queries over four months and ISO/IEC 42001 AI governance alignment.

## Executive Summary
TVS Sidekick is an enterprise AI assistant built by TVS SCS UK that uses large language models with retrieval-augmented generation (RAG) to provide semantic search and answer queries from multiple internal data sources. Deployed in a four-month pilot (March-June 2025), it processed over 750 queries with an average response time of 46 seconds, achieving highest usage among IT and business roles. The system integrated SharePoint, Azure DevOps, code repositories, and company websites into a vector database and included a monitoring framework for tracking usage and feedback. Qualitative feedback highlighted the system's value for retrieving SharePoint content and business logic, though technical users noted insufficient code detail. The project also aligned with ISO/IEC 42001 AI management standards, establishing governance processes for risk management, monitoring, and continuous improvement.

## Method Summary
The project involved developing an AI assistant using LLMs with RAG to integrate SharePoint, Azure DevOps, code repositories, and company websites into a vector database. A monitoring framework was implemented to track usage and feedback. The pilot ran for four months (March-June 2025), during which the system processed over 750 queries, averaging five questions per session and a 46-second response time. Qualitative feedback was collected from users, and the project aligned with ISO/IEC 42001 AI management standards, establishing governance processes for risk management, monitoring, and continuous improvement.

## Key Results
- Sidekick processed over 750 queries in a four-month pilot, averaging five questions per session and a 46-second response time.
- Usage was highest among IT and business roles, with qualitative feedback highlighting value for retrieving SharePoint content and business logic.
- The project aligned with ISO/IEC 42001 AI management standards, establishing governance processes for risk management, monitoring, and continuous improvement.

## Why This Works (Mechanism)
The system works by combining semantic search capabilities with LLM response generation through retrieval-augmented generation (RAG). This approach allows efficient querying of large document bases by first retrieving relevant information from the vector database and then using an LLM to generate contextually appropriate responses. The integration of multiple data sources into a unified vector database enables comprehensive coverage of enterprise knowledge while maintaining reasonable response times through optimized retrieval mechanisms.

## Foundational Learning
- Retrieval-Augmented Generation (RAG): Combines semantic search with LLM response generation; needed to efficiently query large document bases. Quick check: verify retrieval accuracy on sample enterprise queries.
- Vector Database Integration: Enables semantic search over structured and unstructured data; needed for integrating SharePoint, code repositories, and websites. Quick check: measure retrieval latency and relevance.
- ISO/IEC 42001 AI Governance: Provides framework for AI risk management and monitoring; needed for enterprise compliance. Quick check: audit governance documentation and processes.

## Architecture Onboarding
- Component Map: User Queries -> LLM with RAG -> Vector Database (SharePoint, Azure DevOps, Code Repos, Websites) -> Response
- Critical Path: Query processing, semantic search, LLM response generation, monitoring
- Design Tradeoffs: RAG vs. fine-tuning for enterprise data integration; monitoring vs. performance overhead
- Failure Signatures: Slow response times, irrelevant retrievals, governance gaps
- First Experiments:
  1. Test RAG retrieval accuracy on a subset of SharePoint documents
  2. Measure average response time and compare to baseline
  3. Conduct blind evaluation of LLM responses against ground truth

## Open Questions the Paper Calls Out
- How does the system maintain retrieval accuracy as document collections grow over time?
- What mechanisms are in place to detect and handle model drift in the LLM component?
- How scalable is the vector database solution for larger enterprise deployments?

## Limitations
- Qualitative impact assessments rely on non-generalizable feedback from a single pilot group, leading to low confidence.
- Absence of baseline comparisons, blind evaluations, or documented validation of LLM output quality against ground truth.
- Technical limitations such as vector database scalability, RAG accuracy over time, and model drift are not discussed.
- The 46-second average response time may impact user experience and adoption in production environments.

## Confidence
- Quantitative usage metrics: Medium
- Qualitative user feedback: Low
- ISO/IEC 42001 alignment: Low
- Technical validation: Low

## Next Checks
1. Conduct blind evaluation of Sidekick responses against curated ground truth for SharePoint and code repository queries.
2. Perform a longitudinal study to measure model performance drift and retrieval accuracy over multiple months.
3. Engage an independent auditor to assess ISO/IEC 42001 compliance, focusing on risk management documentation and monitoring effectiveness.
4. Benchmark response times against industry standards and identify optimization opportunities.