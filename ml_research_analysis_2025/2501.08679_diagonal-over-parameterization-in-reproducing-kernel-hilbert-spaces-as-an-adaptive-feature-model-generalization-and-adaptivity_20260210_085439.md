---
ver: rpa2
title: 'Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as an
  Adaptive Feature Model: Generalization and Adaptivity'
arxiv_id: '2501.08679'
source_url: https://arxiv.org/abs/2501.08679
tags:
- kernel
- have
- lemma
- generalization
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of fixed-kernel methods in
  neural network generalization, particularly when the kernel is misaligned with the
  target function. To overcome this, the authors introduce a diagonal adaptive kernel
  model that dynamically learns kernel eigenvalues and output coefficients simultaneously
  during training.
---

# Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as an Adaptive Feature Model: Generalization and Adaptivity

## Quick Facts
- arXiv ID: 2501.08679
- Source URL: https://arxiv.org/abs/2501.08679
- Reference count: 18
- This paper introduces a diagonal adaptive kernel model that learns kernel eigenvalues and output coefficients simultaneously, achieving near-oracle generalization rates by overcoming fixed-kernel limitations.

## Executive Summary
This paper addresses the fundamental limitation of fixed-kernel methods in neural network generalization, particularly when the kernel is misaligned with the target function's spectral structure. The authors introduce a diagonal adaptive kernel model that dynamically learns kernel eigenvalues and output coefficients during training. This approach allows the kernel to adapt to the structure of the truth function, significantly improving generalization compared to fixed-kernel methods. Theoretical analysis shows that the adaptive kernel can achieve nearly the oracle convergence rate, regardless of the underlying structure of the signal. The adaptivity comes from learning the right eigenvalues during training, demonstrating a feature learning behavior.

## Method Summary
The method parameterizes the kernel eigenvalues as learnable weights alongside output coefficients, allowing simultaneous adaptation of both during gradient descent. The model uses diagonal scaling parameters $a_j$ (initialized from fixed kernel eigenvalues) and optional deep layers $b_j$ (initialized small), with output coefficients $\beta_j$ starting at zero. The effective model operates in the spectral domain as $\hat{f}(x) = \sum_j (a_j b_j^D \beta_j) e_j(x)$. Training involves gradient flow optimization with early stopping at time $t \asymp n^{(D+1)/(D+2)}$, where signal components converge to the truth while noise components remain bounded.

## Key Results
- The diagonal adaptive kernel method significantly outperforms vanilla fixed-kernel methods when kernel-truth misalignment is severe
- Theoretical results show the adaptive kernel can achieve nearly the oracle convergence rate regardless of signal structure
- Extra depth enhances adaptability and generalization by refining the model's sensitivity to eigenvalue adaptation

## Why This Works (Mechanism)

### Mechanism 1
Simultaneously training kernel eigenvalues ($a_j$) and output weights ($\beta_j$) enables the model to align the kernel's spectral structure with the target function, overcoming the "misalignment" limitation of fixed kernels. The gradient flow dynamics for the effective parameter $\theta_j = a_j \beta_j$ allow "signal" components to grow rapidly while "noise" components grow slowly or remain bounded, effectively learning the correct eigenvalues for the signal subspace.

### Mechanism 2
Over-parameterization coupled with early stopping acts as an implicit regularizer, achieving near-oracle convergence rates without requiring prior knowledge of the target function's smoothness. Signal components converge to the truth quickly ($t \propto n^{(D+1)/(D+2)}$), while noise components take longer to overfit, allowing stopping at a "sweet spot" that captures the signal before noise dominates.

### Mechanism 3
Adding depth (extra diagonal layers) refines the model's sensitivity to eigenvalue adaptation, allowing it to ignore initial misalignment more effectively. Deeper parameterization changes the conservation quantities of the gradient flow, allowing the model to tune the effective learning rate per eigen-direction more aggressively, suppressing error contribution from initially large but irrelevant eigenvalues.

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS) & Mercer's Theorem**
  - Why needed here: The entire architecture is built on the spectral decomposition of a kernel. Functions are represented by coefficients $\theta_j$ in a basis $e_j$, and the kernel's "shape" is determined by its eigenvalues $\lambda_j$.
  - Quick check question: Can you explain how the decay rate of kernel eigenvalues ($\lambda_j \asymp j^{-\gamma}$) determines the "smoothness" of functions in the RKHS?

- **Neural Tangent Kernel (NTK) & Kernel Regime**
  - Why needed here: The paper positions itself against the NTK regime. NTK theory treats neural networks as fixed kernels, implying no feature learning. This paper aims to break that limitation.
  - Quick check question: Why does a "fixed kernel" struggle to learn a target function if the target's spectral structure is misaligned with the kernel's eigenvalue decay?

- **Gradient Flow & Conservation Quantities**
  - Why needed here: The theoretical analysis relies on continuous-time dynamics (ODEs). Understanding how to derive quantities like $a^2(t) - \beta^2(t) = \text{const}$ is crucial for following the proofs.
  - Quick check question: In a simplified linear system, how does the initialization of weights influence the trajectory of gradient descent before it reaches the solution?

## Architecture Onboarding

- **Component map:**
  Input Layer (fixed eigenfunctions $e_j(x)$) -> Adaptive "Feature" Layer(s) (diagonal scaling parameters $a_j$ and optional deep layers $b_j$) -> Output Layer (linear coefficients $\beta_j$)

- **Critical path:**
  1. Initialization: Set $a_j(0)$ based on a standard kernel, set $\beta(0)=0$, set $b_j(0) = n^{-1/2(D+2)}$ if $D>0$
  2. Training: Run Gradient Descent on the empirical squared loss
  3. Early Stopping: Stop at time $t \asymp n^{(D+1)/(D+2)}$

- **Design tradeoffs:**
  - Depth ($D$) vs. Stability: Higher depth improves generalization bounds but requires smaller initialization $b_0$, which might slow optimization or cause underflow
  - Fixed vs. Adaptive: Adaptive kernels learn structure (better for misalignment) but require tuning stopping time; fixed kernels are simpler but fail on low-dimensional structures in high dimensions

- **Failure signatures:**
  - Noise Amplification: Training past optimal $t$ causes noise components to fit sampling noise, degrading test error
  - Initialization Mismatch: If $a_j(0)$ decays much slower than true signal, convergence slows; if much faster, signal might be "washed out" initially

- **First 3 experiments:**
  1. Validate Signal Learning: Plot evolution of effective kernel eigenvalues $a_k(t)^2$ for signal vs. noise indices on simulated data
  2. Depth Ablation: Compare test error vs. sample size for $D=0$ vs. $D=2$ on synthetic misaligned target
  3. Early Stopping Sensitivity: Plot generalization error curve over training time to identify "sweet spot" at $t \propto n^{\alpha}$

## Open Questions the Paper Calls Out

- **Can the adaptive kernel framework be extended to cases where the eigenfunctions (the basis) are learned dynamically, rather than remaining fixed?**
  - The authors state in Section 3 that considering fixed eigenfunctions is a "first step toward the adaptive kernel approach" and a limitation of the current model
  - This remains unresolved because real neural networks update their feature representations, modifying the basis functions themselves
  - A theoretical extension providing generalization error bounds for models where $e_j(x)$ are parameterized and updated during training would resolve this

- **How does the diagonal adaptive kernel method perform in the regime of slow eigenvalue decay (small $\gamma$), where theoretical error bounds suggest potential overfitting?**
  - Theorem 1 includes an error term $n^{1+s/(2\gamma)}$, and Section 3.2.2 notes the method "still tends to overfit the noise components" when $\gamma$ is not sufficiently large
  - This remains unresolved because current guarantees rely on large $\gamma$ to render noise overfitting terms negligible
  - Refined theoretical bounds that don't depend critically on large $\gamma$, or numerical simulations demonstrating failure modes, would resolve this

## Limitations
- Theoretical analysis relies on idealized continuous-time gradient flow and strong spectral assumptions about the target function
- Early stopping rule requires knowing sample size $n$ and signal subspace structure a priori, which may be impractical
- Experiments are mostly synthetic, leaving uncertainty about performance on real-world data with unstructured noise

## Confidence
- **High:** Diagonal parameterization improves over fixed-kernel methods when target has low-dimensional structure in high dimensions
- **Medium:** Depth improves adaptivity (Theorem 2) under stated assumptions, but practical gains may be marginal depending on initialization
- **Low:** Theoretical generalization bounds depend on stopping at specific $t \propto n^{(D+1)/(D+2)}$; in practice, this criterion is unknown and may not be achievable

## Next Checks
1. Test the adaptive model on real-world regression datasets (e.g., UCI) with known low-dimensional structure to assess practical gains over fixed-kernel baselines
2. Evaluate the sensitivity of the stopping time by running with varying $t$ and plotting the generalization error curve to verify the theoretical "sweet spot"
3. Investigate the stability of deeper models ($D \geq 2$) under different initialization schemes and compare their robustness to noise amplification vs. 2-layer models