---
ver: rpa2
title: A Model Zoo of Vision Transformers
arxiv_id: '2504.10231'
source_url: https://arxiv.org/abs/2504.10231
tags:
- neural
- performance
- learning
- training
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first model zoo of Vision Transformers
  (ViT), addressing the gap in existing model zoos that lack state-of-the-art transformer
  architectures. The zoo contains 250 unique ViT-S models generated through a novel
  two-stage training protocol combining pre-training and fine-tuning steps.
---

# A Model Zoo of Vision Transformers

## Quick Facts
- arXiv ID: 2504.10231
- Source URL: https://arxiv.org/abs/2504.10231
- Authors: Damian Falk; Léo Meynent; Florence Pfammatter; Konstantin Schürholt; Damian Borth
- Reference count: 31
- Primary result: First comprehensive model zoo containing 250 ViT-S models with systematic hyperparameter variations

## Executive Summary
This paper addresses the critical gap in existing model zoos by introducing the first comprehensive collection of Vision Transformer models. The authors present a systematic approach to generating diverse transformer architectures through a two-stage training protocol combining pre-training and fine-tuning. The resulting dataset of 250 unique ViT-S models is validated through extensive performance and behavioral diversity analysis, achieving strong results on ImageNet-1k and CIFAR-100 datasets.

## Method Summary
The authors developed a novel two-stage training protocol for creating diverse Vision Transformer models. The approach begins with 10 pre-trained ViT-S models using different hyperparameter combinations (learning rates: 3e-3, 1e-3, 1e-4; optimizers: AdamW, SGD; seeds). These serve as foundations for 240 fine-tuned models, systematically varied across the same hyperparameter space. The dataset is validated through diversity analysis showing distinct behavioral modes and weight-space variations, with performance metrics demonstrating up to 72.4% accuracy on ImageNet-1k and 85.1% on CIFAR-100.

## Key Results
- Created 250 unique ViT-S models through systematic hyperparameter variation
- Achieved up to 72.4% accuracy on ImageNet-1k and 85.1% on CIFAR-100
- Demonstrated distinct behavioral modes and weight-space variations through diversity analysis
- Enabled new applications including model lineage prediction and model weights averaging

## Why This Works (Mechanism)
The success stems from the systematic two-stage training protocol that creates a diverse population of models through controlled hyperparameter variations. By combining pre-training with fine-tuning across multiple learning rates, optimizers, and initialization seeds, the approach generates models with distinct behavioral patterns while maintaining strong performance. This diversity enables population-based methods and provides a foundation for understanding model behavior across the transformer architecture space.

## Foundational Learning

**Vision Transformer Architecture**
- Why needed: Understanding ViT components is essential for interpreting model zoo variations
- Quick check: Can identify patch embeddings, transformer blocks, and classification heads

**Hyperparameter Optimization**
- Why needed: Core to understanding how model variations were generated
- Quick check: Can explain impact of learning rates, optimizers, and initialization on training dynamics

**Model Diversity Metrics**
- Why needed: Critical for evaluating the effectiveness of the model zoo
- Quick check: Can compute and interpret behavioral mode analysis and weight-space variations

## Architecture Onboarding

**Component Map**
ViT-S Backbone -> Pre-training Stage -> Fine-tuning Stage -> Performance Evaluation -> Diversity Analysis

**Critical Path**
The two-stage training protocol (pre-training → fine-tuning) is the critical path that determines model quality and diversity. Each stage must complete successfully with appropriate hyperparameter selection.

**Design Tradeoffs**
The choice between AdamW and SGD optimizers creates distinct convergence patterns and generalization behaviors. Linear vs MLP classification heads impact final layer representations and task-specific adaptation capabilities.

**Failure Signatures**
Models failing to converge typically show poor weight-space diversity and lack distinct behavioral modes. Suboptimal hyperparameter combinations result in models clustering in performance and behavior.

**First Experiments**
1. Evaluate baseline ViT-S performance on ImageNet-1k using AdamW optimizer
2. Compare linear vs MLP classification head performance on CIFAR-100
3. Analyze weight-space diversity between models trained with different learning rates

## Open Questions the Paper Calls Out

The authors acknowledge major uncertainties regarding the generalizability of findings beyond ViT-S architecture and ImageNet/CIFAR-100 datasets. The two-stage training protocol may not translate directly to other transformer variants or downstream tasks. The behavioral diversity analysis lacks comparison against alternative architectures, making it difficult to assess whether observed diversity is unique to ViT models or representative of transformer architectures more broadly.

## Limitations

- Findings may not generalize beyond ViT-S architecture to other transformer variants
- Two-stage training protocol effectiveness uncertain for downstream tasks beyond ImageNet/CIFAR-100
- Behavioral diversity analysis lacks comparative validation against alternative architectures

## Confidence

High: Dataset creation methodology and basic performance metrics are directly measurable and reproducible
Medium: Diversity analysis conclusions show identified behavioral modes but practical significance needs more validation
Low: Extrapolation claims about population-based methods and model averaging require extensive additional validation

## Next Checks

1. Evaluate model zoo diversity metrics on additional transformer architectures (DeiT, Swin, PVT) to assess generalizability of behavioral diversity findings
2. Conduct systematic ablation studies varying the two-stage training protocol parameters to quantify their impact on final model performance and diversity
3. Perform extensive model averaging experiments across different pre-training epochs and architectures to establish guidelines for effective weight-space combination strategies