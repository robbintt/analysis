---
ver: rpa2
title: Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions
arxiv_id: '2503.03278'
source_url: https://arxiv.org/abs/2503.03278
tags:
- medical
- visual
- abnormality
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a knowledge-enhanced approach to improve medical
  abnormality grounding in vision-language models (VLMs) by incorporating decomposed
  medical knowledge descriptions. Instead of directly prompting models to recognize
  specific abnormalities, the method breaks down complex medical concepts into fundamental
  visual attributes such as shape, density, and location, using these as fine-grained
  textual prompts to better align textual descriptions with visual features.
---

# Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions

## Quick Facts
- arXiv ID: 2503.03278
- Source URL: https://arxiv.org/abs/2503.03278
- Reference count: 33
- Primary result: Knowledge-enhanced prompts improve grounding in small VLMs, achieving 25.5% mAP50 on VinDr-CXR and 11.07% on PadChest-Known

## Executive Summary
This paper introduces a knowledge-enhanced approach to improve medical abnormality grounding in vision-language models by incorporating decomposed medical knowledge descriptions. The method breaks down complex medical concepts into fundamental visual attributes such as shape, density, and location, using these as fine-grained textual prompts to better align textual descriptions with visual features. Despite using a small 0.23B Florence-2 base model trained on only 1.5% of the data used for larger medical VLMs, the approach achieves competitive performance in abnormality grounding, outperforming significantly larger 7B-parameter models on both in-distribution and zero-shot generalization tasks.

## Method Summary
The approach fine-tunes Florence-2-base (0.23B parameters) on chest X-ray datasets by generating knowledge descriptions that decompose medical abnormality definitions into visual attributes (shape, density, location, color). These descriptions are created by prompting an LLM with medical definitions and extracting visual primitives. The model is trained end-to-end with Adam optimizer (lr=5e-6, weight_decay=0.01) on 512×512 images, using cross-entropy loss over discrete localization tokens conditioned on both visual and textual embeddings. Training uses 16,087 VinDr-CXR images with bounding boxes normalized to [0, 1000] and quantized for autoregressive sequence generation.

## Key Results
- Florence-2-base with knowledge descriptions achieves 25.5% mAP50 on VinDr-CXR vs 13.26% baseline
- Zero-shot performance on PadChest-Unknown improves from 1.48% to 3.05% with knowledge descriptions
- Despite being 30× smaller than competing models, the approach outperforms 7B-parameter models on both in-distribution and zero-shot tasks
- The method demonstrates strong generalization to previously unseen abnormalities through attribute-based visual grounding

## Why This Works (Mechanism)

### Mechanism 1: Visual Attribute Decomposition
Breaking abstract medical terminology into concrete visual attributes improves grounding accuracy. Medical definitions (often non-visual) are transformed via LLM prompting into descriptions emphasizing {shape, density, location, color}. These decomposed descriptions serve as fine-grained textual prompts that bridge the gap between clinical terminology and pixel-level features.

### Mechanism 2: Knowledge-Enhanced Cross-Modal Alignment
Explicit visual descriptions in prompts improve the learned alignment between text embeddings and visual token representations. The model conditions on both the abnormality name AND its decomposed visual description during training, providing richer semantic context for cross-attention layers to associate textual concepts with spatial regions in the DaViT visual encoder output.

### Mechanism 3: Knowledge Injection for Zero-Shot Transfer
Decomposed knowledge descriptions enable generalization to unseen abnormalities by teaching models to recognize visual patterns rather than memorizing disease labels. By training with attribute-level descriptions, the model learns to associate visual primitives (e.g., "white patch," "increased density") with regions, which can transfer to novel disease terms sharing those primitives.

## Foundational Learning

- **Visual Grounding as Sequence Generation**: Why needed here: Unlike bounding box regression, this model frames detection as autoregressive token prediction (coordinates as discrete tokens). Quick check: Can you explain why normalizing coordinates to [0,1000] and quantizing them enables language-model-based detection?

- **Cross-Attention in Encoder-Decoder Transformers**: Why needed here: The model aligns visual tokens from DaViT with text embeddings through cross-attention layers. Quick check: What is the role of cross-attention in conditioning text generation on visual features?

- **Prompt Engineering for Domain Knowledge Injection**: Why needed here: The core contribution relies on crafting prompts that decompose medical concepts into visual attributes. Quick check: How does adding attribute-focused descriptions to prompts differ from standard instruction tuning?

## Architecture Onboarding

- **Component map**: DaViT Visual Encoder -> Text Tokenizer -> Multi-Modal Transformer Encoder-Decoder -> Loss Function
- **Critical path**: Load Florence-2-base pretrained weights → Prepare dataset with knowledge descriptions → Fine-tune end-to-end with Adam
- **Design tradeoffs**: Small model vs. large model (efficiency vs. capacity); knowledge decomposition vs. direct prompting (extra effort vs. potential gains)
- **Failure signatures**: mAP50 < 15% on VinDr-CXR (low-quality descriptions); poor zero-shot transfer (descriptions miss transferable primitives); training instability (check learning rate)
- **First 3 experiments**: 1) Baseline without knowledge descriptions (expect mAP50 ≈ 13%); 2) Ablation on description quality (LLM vs. manual); 3) Zero-shot probe on PadChest-Unknown

## Open Questions the Paper Calls Out

1. **Larger VLM Integration**: Can knowledge-enhanced prompts provide additional performance gains when integrated with larger, more complex VLMs (7B+ parameters)? The study only evaluated the approach on a 0.23B parameter model.

2. **Dynamic Prompt Adjustment**: Would dynamic, disease-specific prompt adjustment outperform the current static knowledge description approach? All experiments used a fixed prompt template across diseases.

3. **Cross-Modality Generalization**: Does the knowledge decomposition approach generalize to medical imaging modalities beyond chest X-rays? The evaluation is limited to two chest X-ray datasets.

## Limitations

- Knowledge description generation process lacks transparency and quality control measures
- Key training hyperparameters (epochs, early stopping) are unspecified
- Architecture scalability to larger models is not adequately addressed
- Evaluation focuses primarily on detection metrics without localization quality analysis

## Confidence

- **High Confidence**: Experimental results showing KD improves mAP50 from 13.26% to 25.5% on VinDr-CXR
- **Medium Confidence**: Mechanism claims about visual attribute decomposition improving grounding
- **Low Confidence**: Generalization mechanism claims and their robustness across different abnormality types

## Next Checks

1. **Ablation on Description Quality**: Systematically vary the quality and specificity of knowledge descriptions to quantify their impact on grounding performance.

2. **Localization Quality Analysis**: Analyze IoU distributions and false positive patterns beyond mAP metrics to understand improvement sources.

3. **Transfer Mechanism Validation**: Conduct controlled experiments grouping training abnormalities by visual attribute similarity to verify transfer occurs primarily between visually similar classes.