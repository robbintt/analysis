---
ver: rpa2
title: 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image
  Understanding'
arxiv_id: '2512.02906'
source_url: https://arxiv.org/abs/2512.02906
tags:
- image
- semantic
- similarity
- object
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MRD (Multi-resolution Retrieval-Detection),
  a training-free framework to improve high-resolution image understanding by MLLMs.
  MRD addresses the problem of object fragmentation across image crops and background
  noise interference in retrieval-based methods.
---

# MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding

## Quick Facts
- arXiv ID: 2512.02906
- Source URL: https://arxiv.org/abs/2512.02906
- Authors: Fan Yang; Kaihao Zhang
- Reference count: 36
- MRD achieves up to 95.6% overall accuracy on V* Bench and 59.7% on HRBench-4K, outperforming training-free methods.

## Executive Summary
This paper introduces MRD (Multi-resolution Retrieval-Detection), a training-free framework for high-resolution image understanding that addresses object fragmentation across image crops and background noise interference in retrieval-based methods. MRD fuses multi-resolution semantic similarity maps to preserve object integrity and incorporates an open-vocabulary object detector (LLMDet) with sliding windows for direct localization. The approach combines semantic similarity from retrieval with detection confidence scores through linear fusion, achieving state-of-the-art performance on V* Bench and HRBench benchmarks.

## Method Summary
MRD extends retrieval-augmented vision models by addressing two key limitations: object fragmentation across patches and background noise in semantic retrieval. The method operates by computing semantic similarity maps at multiple resolutions (low and high) using VisRAG, then fusing them via geometric mean to reduce fragmentation artifacts. Simultaneously, it uses LLMDet with a sliding-window approach to create a global detection confidence map by extracting target objects from the query through in-context learning. The final localization map combines these two modalities using weighted linear fusion, which is then used by RE-Search to select relevant image crops for MLLM inference.

## Key Results
- Achieves 95.6% overall accuracy on V* Bench, outperforming state-of-the-art training-free methods
- Reaches 59.7% accuracy on HRBench-4K and 54.3% on HRBench-8K
- Ablation studies show multi-resolution fusion and detector enhancement each contribute 5.7% and 7.8% accuracy gains respectively on V* Bench
- Reduces search steps from >100 to ~30 while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-resolution semantic fusion reduces similarity bias caused by object fragmentation across crops.
- Mechanism: The system computes semantic similarity maps at two proportional resolutions (low and high), then fuses them using a geometric mean: $s^f_t = \sqrt{\tilde{s}_t \cdot s_t}$. This calibrates low-resolution scores when objects are split across patches, preserving object integrity.
- Core assumption: Objects fragmented at one resolution may appear more intact at another; consistency across scales indicates genuine relevance.
- Evidence anchors:
  - [abstract] "integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects"
  - [section 4.1] Equations 2-5 define the mapping and fusion; the paper states "This enhances the similarity of various parts of the object, thereby preserving the integrity"
  - [corpus] The RAP paper (arXiv:2503.01222) provides the baseline retrieval framework but does not address multi-resolution fusion, highlighting MRD's novel contribution.
- Break condition: If target objects are uniformly split at all tested resolutions, the fusion may not provide meaningful calibration gains.

### Mechanism 2
- Claim: Open-vocabulary detection with sliding windows provides direct, global-scale object localization that complements patch-level retrieval.
- Mechanism: An LLM extracts target object names from the query via in-context learning. LLMDet then scans the image using overlapping sliding windows, assigning detection confidence scores to each patch. These scores form a global detection confidence map.
- Core assumption: The detector's open-vocabulary capability generalizes to the extracted object categories; sliding windows capture objects regardless of size or position.
- Evidence anchors:
  - [abstract] "incorporates an open-vocabulary object detector (LLMDet) using a sliding-window approach to directly localize target objects"
  - [section 4.2] Equations 6-10 detail the sliding-window detection and aggregation process; visualization in Figure 5 (lower) shows detection suppressing false positives
  - [corpus] Related work on vision-language compositionality (arXiv:2506.09691) suggests VLMs struggle with compositional understanding, which detection may help overcome, though this link is indirect.
- Break condition: If the detector fails to recognize novel or ambiguous object categories, the confidence map will be unreliable.

### Mechanism 3
- Claim: Linear fusion of semantic similarity and detection confidence maps improves localization accuracy beyond either modality alone.
- Mechanism: The final score per patch is computed as $s_F(i,j) = (1-w) \cdot s^f(i,j) + w \cdot c^g(i,j)$, where $w=0.4$ by default. This amplifies true target regions (where both signals agree) while suppressing spurious high-similarity background regions.
- Core assumption: Semantic similarity and detection confidence provide complementary information; detection offers coarse localization while retrieval provides fine-grained discrimination within objects.
- Evidence anchors:
  - [abstract] "The semantic similarity map from retrieval is fused with the detection confidence map for more accurate localization"
  - [section 4.2, Equation 11] defines the fusion; Table 2 ablation shows RAP+OVD+Multi-Res achieves 89.3% overall accuracy, +5.7% over RAP alone
  - [corpus] Weak direct evidence; corpus papers do not systematically study retrieval-detection fusion for HR images.
- Break condition: If the detection confidence map is noisy or misaligned (e.g., due to sliding-window artifacts), fusion may introduce more noise than signal.

## Foundational Learning

### Concept: Retrieval-Augmented Generation (RAG) for Vision
- Why needed here: MRD extends RAG principles to the visual domain, using VisRAG to compute query-to-crop semantic similarity for high-resolution images.
- Quick check question: Can you explain how RAP differs from standard text-based RAG, and why patch-level retrieval is necessary for HR images?

### Concept: Open-Vocabulary Object Detection (OVD)
- Why needed here: MRD relies on LLMDet to detect arbitrary object categories extracted from the query, without requiring class-specific training.
- Quick check question: How does an OVD model like LLMDet generalize to unseen object categories, and what role does in-context learning play in defining detection targets?

### Concept: Multi-Resolution Feature Analysis
- Why needed here: The core insight is that objects of different sizes require different resolutions for optimal similarity computation; fusion across scales mitigates fragmentation artifacts.
- Quick check question: Why does splitting an object across crops disrupt semantic similarity, and how does multi-resolution fusion address this?

## Architecture Onboarding

### Component map:
Image Partitioning -> (parallel: Multi-Resolution Similarity AND Sliding-Window Detection) -> Semantic-Detection Fusion -> RE-Search -> MLLM Inference

### Critical path:
Image partitioning → (parallel: multi-resolution similarity AND sliding-window detection) → fusion → RE-Search → MLLM inference
- Key dependency: Detection window size and stride must align with patch grid for proper aggregation

### Design tradeoffs:
- **Crop resolution**: Smaller crops reduce fragmentation but increase compute and may miss global context; MRD mitigates via multi-resolution fusion (Section B.1 shows MRD is robust to resolution choice)
- **Detection weight $w$**: Higher $w$ trusts detection more; optimal varies by model (0.2-0.4, Section B.3)
- **Sliding-window size**: Smaller windows reduce background interference but increase detection runs (Section B.4)
- **Search steps**: MRD achieves peak performance at ~30 steps vs. RAP's >100 (Section B.2)

### Failure signatures:
- **Missed objects in multi-object tasks**: Detection may fail to localize all instances, especially small or occluded objects (Table 2 shows OVD alone underperforms on multi-object)
- **False positives in cluttered backgrounds**: If detector overfires, fusion may amplify noise
- **Category extraction errors**: LLM may misinterpret query, leading to irrelevant detection targets
- **Alignment issues**: If patch grid and sliding-window positions misalign, confidence scores may not aggregate correctly

### First 3 experiments:
1. **Ablation of fusion components**: Replicate Table 2 on V* Bench—test RAP alone, RAP+Multi-Res, RAP+OVD, and full MRD. Confirm each component's contribution.
2. **Hyperparameter sweep**: Vary crop resolution (112, 224, 448), detection weight $w$ (0.0-1.0), and sliding-window size. Use held-out validation split from V* Bench to find robust defaults.
3. **Cross-benchmark generalization**: Apply MRD to HR-Bench-4K and HRBench-8K with different base MLLMs (LLaVA-v1.5-7B and LLaVA-ov-0.5B). Verify improvements transfer and identify failure modes (e.g., FCP task challenges).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the computational complexity of the sliding-window object detector be minimized without compromising the localization accuracy provided by smaller window sizes?
- **Basis in paper:** [Explicit] Supplementary Section B.4 states that while a smaller sliding-window size (896) yields better performance by reducing background interference, it results in "increased computational complexity and longer processing time."
- **Why unresolved:** The authors select a larger default window size (1232) as a compromise, but the fundamental trade-off between the granularity of the sliding window and inference speed remains unoptimized.
- **What evidence would resolve it:** A dynamic or content-aware window strategy that reduces total windows processed while maintaining the accuracy of the 896-pixel window baseline.

### Open Question 2
- **Question:** What specific failure modes cause the Open-Vocabulary Detector (OVD) to underperform in multi-object scenarios compared to single-object tasks, and how can this be rectified?
- **Basis in paper:** [Explicit] Section 5.4 (Ablation Study) notes that using OVD alone significantly improves single-object tasks but performs "worse than using OVD alone" on multi-object tasks, suggesting "some target objects may be lost during the search."
- **Why unresolved:** While the paper shows that multi-resolution fusion recovers this performance, it does not investigate why the detector fails to capture all instances in cluttered or multi-object environments initially.
- **What evidence would resolve it:** An analysis of missed detections in the COCO or LVIS datasets indicating whether the failure is due to feature extraction, NMS thresholds, or query-object embedding distance.

### Open Question 3
- **Question:** Is the linear fusion of detection confidence and semantic similarity optimal, or would a learned attention-based mechanism better handle the varying reliability of the two modalities?
- **Basis in paper:** [Inferred] Equation 11 defines the final map as a linear combination $s_F = (1-w) \cdot s_f + w \cdot c_g$. Section B.3 shows the optimal weight $w$ varies by model (0.2 vs 0.4), suggesting the contribution of each map is not static.
- **Why unresolved:** A static linear weight cannot dynamically adapt to specific image contexts where the detector is confident but the retriever is not (or vice versa).
- **What evidence would resolve it:** A comparison of the current fixed-weight fusion against a gated fusion mechanism that adapts $w$ per pixel or per region.

## Limitations
- Effectiveness depends critically on choice of resolution ratios; optimal scaling may vary by image content and object size distribution
- Detection accuracy hinges on LLM's ability to correctly extract target objects from queries via in-context learning; query complexity or ambiguity could degrade performance
- Linear fusion assumes semantic similarity and detection confidence are equally reliable; in practice, one modality may dominate depending on scene complexity

## Confidence
- **High Confidence**: Multi-resolution fusion reduces fragmentation artifacts and improves accuracy over single-resolution retrieval (supported by ablation in Table 2)
- **Medium Confidence**: Sliding-window OVD provides meaningful localization beyond patch-level retrieval (results show gains, but direct comparison to fixed-grid detection is absent)
- **Low Confidence**: Weighted linear fusion is optimal; alternative fusion strategies (e.g., non-linear or learned) may yield further improvements

## Next Checks
1. **Ablation Study Replication**: Replicate Table 2 on V* Bench with variants: RAP alone, RAP+Multi-Res, RAP+OVD, and full MRD to verify each component's contribution
2. **Hyperparameter Robustness**: Sweep crop resolutions (112, 224, 448), detection weight $w$ (0.0-1.0), and sliding-window size. Validate optimal defaults on held-out V* Bench split
3. **Cross-Benchmark Generalization**: Apply MRD to HRBench-4K and HRBench-8K using both LLaVA-v1.5-7B and LLaVA-ov-0.5B. Confirm improvements transfer and analyze failure modes (e.g., FCP task challenges)