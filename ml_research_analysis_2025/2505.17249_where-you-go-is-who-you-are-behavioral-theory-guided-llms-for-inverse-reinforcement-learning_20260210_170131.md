---
ver: rpa2
title: 'Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement
  Learning'
arxiv_id: '2505.17249'
source_url: https://arxiv.org/abs/2505.17249
tags:
- reward
- travel
- attributes
- learning
- mobility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SILIC, a novel framework for inferring sociodemographic
  attributes from travel trajectories by leveraging large language models (LLMs) guided
  by behavioral theory. SILIC employs an LLM-guided inverse reinforcement learning
  (IRL) approach to infer latent intentions from mobility patterns, addressing the
  ill-posedness and optimization challenges of IRL through heuristic guidance.
---

# Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.17249
- Source URL: https://arxiv.org/abs/2505.17249
- Authors: Yuran Sun; Susu Xu; Chenguang Wang; Xilei Zhao
- Reference count: 40
- Primary result: Up to 30.93% improvement in sociodemographic prediction accuracy using LLM-guided IRL with Cognitive Chain Reasoning

## Executive Summary
This paper introduces SILIC, a novel framework for inferring sociodemographic attributes from travel trajectories by leveraging large language models (LLMs) guided by behavioral theory. SILIC employs an LLM-guided inverse reinforcement learning (IRL) approach to infer latent intentions from mobility patterns, addressing the ill-posedness and optimization challenges of IRL through heuristic guidance. A Cognitive Chain Reasoning (CCR) strategy is then used to map these intentions to sociodemographic attributes, explicitly following the Theory of Planned Behavior to ensure alignment with human cognitive processes. Evaluated on the 2017 Puget Sound Regional Council Household Travel Survey, SILIC significantly outperforms state-of-the-art baselines, achieving up to 30.93% improvement in accuracy and demonstrating superior class-level performance in predicting attributes such as gender, age, employment status, and income.

## Method Summary
SILIC operates in two stages: first, it uses LLM-guided inverse reinforcement learning to infer reward functions from travel trajectories, then employs Cognitive Chain Reasoning to map these rewards to sociodemographic attributes. The IRL component addresses the ill-posed nature of reward inference by using LLM-generated heuristics for initialization and iterative updates, blending standard gradient ascent with LLM-suggested discrete adjustments. The CCR stage explicitly models the Theory of Planned Behavior's three belief constructs (attitude, subjective norms, perceived behavioral control) as intermediate steps between rewards and sociodemographic predictions. The framework was evaluated on GPS trajectory data from the 2017 Puget Sound Regional Council Household Travel Survey, comparing against baselines that used either direct LLM prediction or omitted the CCR step.

## Key Results
- SILIC achieved up to 30.93% improvement in prediction accuracy over state-of-the-art baselines
- The framework demonstrated superior performance across all sociodemographic classes including gender, age, employment, and income
- Ablation studies showed that both LLM-guided initialization and the CCR reasoning strategy were critical for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-guided reward initialization constrains the IRL solution space toward behaviorally plausible reward functions.
- Mechanism: The LLM receives multi-day travel diaries and generates initial reward weights θ ∈ [-2, 2]^31 based on prior knowledge of typical activity preferences. This reduces the "ill-posed" problem where multiple reward functions could explain the same observed trajectory.
- Core assumption: LLMs encode sufficient domain knowledge about human mobility patterns to produce meaningful initial reward structures that bias optimization toward behaviorally valid solutions.
- Evidence anchors:
  - [abstract] "LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space."
  - [section 4.3] "When only the LLM-guided updates are replaced with standard gradient ascent, the increase is less pronounced, highlighting the importance of LLM-guided initialization in constraining the reward space effectively."
  - [corpus] Weak direct corpus support; neighbor papers focus on sociodemographic inference but not IRL-based mechanisms.
- Break condition: If reward convergence fails or learned policies produce implausible activity sequences (e.g., working at 3 AM), LLM initialization may be insufficiently grounded in domain specifics.

### Mechanism 2
- Claim: Iterative LLM-guided reward updates correct early-stage policy-behavior mismatches by suggesting discrete adjustment directions.
- Mechanism: At each iteration, the top-K states with largest |D_expert(s) - D_learner(s)| mismatch are fed to the LLM, which outputs update directions g_LLM ∈ {-1, 0, 1}^31. These are blended with standard MaxEnt IRL gradients: Δθ = α·∇θ + λ_LLM·g_LLM.
- Core assumption: LLMs can diagnose behavioral mismatches and propose corrections that align with human-like activity reasoning, even without gradient-based optimization.
- Evidence anchors:
  - [section 3.2] "The LLM then suggests an update direction for each of the reward weights... restricted to one of three discrete values: -1, 0, 1."
  - [section 4.3] "Retaining only the LLM-guided reward weight updates still achieves better performance than removing both the initialization and update steps."
  - [corpus] No direct corpus evidence on LLM-guided IRL updates specifically.
- Break condition: If KL divergence between expert and learner distributions plateaus above threshold without improvement across >10 iterations, LLM guidance is failing to refine rewards meaningfully.

### Mechanism 3
- Claim: Cognitive Chain Reasoning (CCR) improves sociodemographic prediction by explicitly modeling the mediating belief constructs from Theory of Planned Behavior.
- Mechanism: Instead of direct trajectory→sociodemographics mapping, CCR first infers beliefs (attitude, subjective norms, perceived behavioral control) from reward weights, then predicts sociodemographics using beliefs + contextual features. This decomposes the inference into two cognitively-aligned steps.
- Core assumption: Reward weights encode interpretable behavioral intentions that map to psychological constructs, and LLMs can perform this mapping faithfully.
- Evidence anchors:
  - [abstract] "A Cognitive Chain Reasoning (CCR) strategy is then used to map these intentions to sociodemographic attributes, explicitly following the Theory of Planned Behavior."
  - [section 4.3] "By further guiding the reasoning process with the well-established behavioral theory, the CCR module aligns more closely with human cognitive processes, leading to the best performance among all compared methods."
  - [corpus] "Synthesizing Attitudes, Predicting Actions (SAPA)" provides converging evidence that behavioral theory-guided LLMs improve prediction in transportation contexts.
- Break condition: If intermediate belief outputs are inconsistent (e.g., contradictory attitudes across similar trajectories), the CCR decomposition is not capturing stable cognitive patterns.

## Foundational Learning

- Concept: **Inverse Reinforcement Learning (IRL)**
  - Why needed here: IRL infers the reward function that best explains observed trajectories. Essential for understanding the "intention inference" stage that precedes sociodemographic prediction.
  - Quick check question: Given a trajectory of states and actions, can you explain why MaxEnt IRL prefers stochastic policies over deterministic ones?

- Concept: **Theory of Planned Behavior (TPB)**
  - Why needed here: The entire framework reverses TPB's causal pathway. You must understand how background factors → beliefs → intentions → behavior maps forward to reason backward.
  - Quick check question: Name the three belief constructs in TPB and explain how they collectively shape behavioral intentions.

- Concept: **Maximum Entropy Policy Learning**
  - Why needed here: The policy π(a|s) is derived using soft value iteration with entropy regularization. Understanding Equations 2-3 is critical for implementing the learner trajectory simulation.
  - Quick check question: Why does the maximum entropy formulation help model human variability in travel decisions compared to deterministic policy learning?

## Architecture Onboarding

- Component map: Input Trajectories → Travel Diary Parsing → MDP State Encoding → LLM-guided Reward Initialization (θ_0) → IRL Loop (Policy Learning → Learner Distribution → Mismatch Scoring → LLM-guided Update → Convergence Check) → Converged θ_i → CCR Prompting → Belief Inference → Sociodemographic Prediction → Output

- Critical path: The reward initialization prompt (Figure 5) → IRL convergence → CCR prompt (Figure 7). Errors in prompt design propagate through both stages. The blending weight λ_LLM and learning rate α are hyperparameters that directly affect convergence speed and solution quality.

- Design tradeoffs:
  - K=30 top mismatched states balances information richness vs. context window limits
  - Discrete LLM updates {-1, 0, 1} sacrifice gradient precision for interpretability and stability
  - Linear reward function R(s) = θ^T φ(s) is interpretable but may miss non-linear preferences

- Failure signatures:
  - High KL divergence (>0.5) at convergence indicates IRL failed to replicate expert behavior
  - Class imbalance in predictions (e.g., 95% "18-44" age predictions) suggests CCR is not discriminating
  - Empty or malformed LLM outputs indicate prompt parsing failures

- First 3 experiments:
  1. **Baseline replication**: Run GPT-4o with extracted mobility features (no IRL, no CCR) on the filtered dataset to establish performance floor. Verify F1 scores match Table 1 within ±0.05.
  2. **Ablation of LLM-guided initialization**: Replace LLM initialization with random θ ∈ [-2, 2]^31, keep LLM updates. Measure KL divergence increase (expect ~20% based on Section 4.3).
  3. **CCR vs CoT comparison**: On held-out test set, compare IRL+CCR against IRL+CoT for gender prediction. Verify accuracy improvement ≥5% per Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inferred intermediate belief constructs (attitudes, subjective norms, and perceived behavioral control) be empirically validated given the absence of ground-truth cognitive data in standard trajectory datasets?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "these internal variables lack ground truth for direct validation" and suggest leveraging survey data with attitudinal variables as a future step.
- Why unresolved: The current study validates the final output (sociodemographic attributes) but cannot confirm if the model's "reasoning" (the intermediate cognitive states) accurately reflects the individual's actual mental process, risking "right for the wrong reasons."
- What evidence would resolve it: A study utilizing a dataset that contains both travel trajectories and self-reported survey responses regarding travel attitudes and perceived norms, allowing for a direct comparison between inferred and actual beliefs.

### Open Question 2
- Question: To what extent does the framework's reliance on general LLM domain knowledge limit its transferability to geographic regions with distinct cultural mobility norms?
- Basis in paper: [explicit] The authors note that LLM guidance "may fail to fully capture behavioral nuances specific to certain geographic regions" because the LLM's knowledge is derived from general sources rather than specific regional data.
- Why unresolved: The model was evaluated exclusively on the 2017 Puget Sound Regional Council Household Travel Survey. It is unclear if the LLM's "common sense" regarding travel behavior applies equally well to regions with vastly different infrastructure or cultural habits.
- What evidence would resolve it: Cross-regional validation experiments applying the SILIC framework to trajectory datasets from culturally distinct cities or countries (e.g., non-Western contexts) without retraining the core LLM parameters.

### Open Question 3
- Question: How does the current MDP state space's exclusion of continuous mobility features, such as trip distance and cost, affect the fidelity of the inferred reward functions?
- Basis in paper: [explicit] The authors acknowledge that the "current state space design may omit relevant mobility-related features (e.g., trip distance), limiting its capacity to fully represent human decision-making dynamics."
- Why unresolved: The model currently relies on hour and activity type. It is unknown if the reward function can truly capture intention without modeling the "cost" (distance/time) of the travel, which is a fundamental component of travel behavior theory.
- What evidence would resolve it: An ablation study incorporating trip distance and monetary cost into the state vector φ(s), measuring the change in KL divergence between expert and learner policies and the resulting sociodemographic prediction accuracy.

### Open Question 4
- Question: Can human-in-the-loop calibration effectively mitigate the misalignment between LLM-generated reward priors and specific regional behavioral characteristics?
- Basis in paper: [explicit] The authors propose "incorporating human-in-the-loop calibration to refine LLM-generated reward priors or updates to better reflect regional characteristics" as a direction for future work to address LLM knowledge gaps.
- Why unresolved: While proposed, the efficacy of this hybrid approach is not tested. It is unclear if human intervention can efficiently correct high-dimensional reward weight vectors without reintroducing the scalability issues associated with expert-driven modeling.
- What evidence would resolve it: An experiment comparing the convergence speed and predictive accuracy of the SILIC framework with and without human feedback loops on the reward initialization and update steps.

## Limitations
- The framework's performance depends heavily on the LLM's domain knowledge, which may not generalize to regions with distinct cultural mobility norms
- The current MDP state space omits relevant mobility features like trip distance and cost, potentially limiting the fidelity of inferred reward functions
- The intermediate belief constructs inferred by the model lack ground-truth validation data in standard trajectory datasets

## Confidence

- **High confidence**: The empirical performance claims (30.93% accuracy improvement) and the general IRL+CCR architecture are well-supported by the results section and ablation studies.
- **Medium confidence**: The behavioral plausibility of the learned rewards depends on the LLM's domain knowledge, which is not directly validated in the paper.
- **Medium confidence**: The claim that CCR aligns more closely with human cognitive processes is supported by the superior performance over direct mapping but lacks behavioral validation of the intermediate belief constructs.

## Next Checks

1. **Convergence Diagnostics**: Run the IRL loop on a small subset (10 users) and monitor KL divergence curves. Verify that KL divergence decreases monotonically and falls below 0.1 within 50 iterations.

2. **Ablation of LLM Updates**: Compare the full SILIC framework against a version that uses only standard MaxEnt IRL updates (no LLM guidance). Measure the difference in final reward KL divergence and classification accuracy.

3. **Intermediate Belief Validation**: For a small sample of trajectories, manually inspect the LLM-generated belief outputs (attitude, subjective norms, perceived behavioral control) for logical consistency and alignment with known sociodemographic attributes.