---
ver: rpa2
title: 'MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency'
arxiv_id: '2510.25897'
source_url: https://arxiv.org/abs/2510.25897
tags:
- reward
- miro
- aesthetic
- training
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning text-to-image generative
  models with user preferences while maintaining diversity and efficiency. The core
  method, MIRO (MultI-Reward cOnditioned pretraining), conditions the generative model
  on multiple reward signals during training, allowing it to learn user preferences
  directly rather than through post-hoc alignment.
---

# MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency

## Quick Facts
- arXiv ID: 2510.25897
- Source URL: https://arxiv.org/abs/2510.25897
- Reference count: 40
- Primary result: MIRO achieves state-of-the-art GenEval compositional performance while being 370× more compute-efficient than larger models

## Executive Summary
MIRO introduces a novel pretraining approach for text-to-image generation that conditions the generative model on multiple reward signals during training, enabling direct learning of user preferences rather than post-hoc alignment. By conditioning on binned reward scores from 7 different metrics (aesthetic, compositional, text alignment, etc.), MIRO learns to map reward levels to visual characteristics, achieving state-of-the-art performance on compositional benchmarks while maintaining efficiency. The method converges up to 19× faster than regular training and demonstrates superior sample efficiency at inference through multi-reward classifier-free guidance.

## Method Summary
MIRO conditions a flow matching transformer on binned reward scores during pretraining, learning a mapping from reward levels to visual characteristics. The architecture integrates reward embeddings (sinusoidal → project to token dim → concatenate to text tokens) into the transformer layers. Training uses a modified flow matching loss with reward conditioning, while inference employs multi-reward classifier-free guidance with configurable reward targets. The method uses CC12M + LAION Aesthetics 6+ (16M images) and evaluates on GenEval compositional benchmark plus 7 reward metrics.

## Key Results
- Achieves state-of-the-art GenEval compositional performance, outperforming much larger models
- Converges up to 19× faster than regular training with superior sample efficiency
- Demonstrates 370× better compute efficiency than Flux-dev while maintaining quality
- Maintains balanced performance across all preference metrics without reward hacking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning on reward scores during pretraining provides dense supervision that accelerates convergence
- Mechanism: The model receives explicit reward vectors per sample, learning a mapping from reward levels to visual characteristics rather than inferring quality implicitly
- Core assumption: Reward models capture meaningful quality dimensions that generalize across the data distribution
- Evidence anchors: 19× faster convergence, flow matching loss with reward conditioning, multi-reward benefits in related work

### Mechanism 2
- Claim: Multi-reward conditioning mitigates reward hacking compared to single-reward optimization
- Mechanism: By requiring the model to satisfy multiple reward dimensions simultaneously, the optimization landscape has fewer exploitable singular optima
- Core assumption: Reward dimensions are at least partially orthogonal
- Evidence anchors: Single-reward models severely degrade performance on other metrics, MIRO achieves balanced performance across all preference metrics

### Mechanism 3
- Claim: Reward-conditioned training enables inference-time control via classifier-free guidance over reward targets
- Mechanism: At inference, the model can steer toward high-reward regions by extrapolating between conditioned and unconditioned velocity predictions
- Core assumption: The model has learned a sufficiently smooth mapping from reward space to output space
- Evidence anchors: Multi-reward CFG with configurable targets, controllable trade-offs between aesthetic and compositional quality

## Foundational Learning

- Concept: **Flow Matching / Rectified Flow**
  - Why needed here: MIRO uses flow matching as the underlying generative framework; understanding the velocity field v_θ and linear interpolation x_t = (1-t)x + tε is essential
  - Quick check question: Can you explain why flow matching trains v_θ to predict (ε − x) rather than the noise ε?

- Concept: **Classifier-Free Guidance (CFG)**
  - Why needed here: MIRO extends CFG from conditioning on text to conditioning on reward vectors
  - Quick check question: How does increasing ω affect output diversity vs. quality in standard CFG?

- Concept: **Reward Models for T2I**
  - Why needed here: MIRO uses 7 reward models; understanding what each captures helps diagnose trade-offs
  - Quick check question: Why might optimizing only AestheticScore harm text-image alignment (as shown in GenEval results)?

## Architecture Onboarding

- Component map: Reward scoring -> Binning -> Architecture (TextRIN + reward embeddings) -> Training (flow matching + reward conditioning) -> Inference (multi-reward CFG)

- Critical path: 1) Precompute reward scores for all images (7 forward passes per image), 2) Bin scores into B bins per reward, 3) Train flow matching model with reward conditioning, 4) Apply multi-reward CFG at inference with configurable targets

- Design tradeoffs: Number of rewards (N) vs. preprocessing cost and embedding collisions, Number of bins (B) vs. control granularity vs. sparsity, Guidance scale (ω) vs. diversity vs. quality, Synthetic captions for text alignment vs. token limits

- Failure signatures: Reward hacking on single metric, Guidance collapse when targets are too similar, Binning artifacts from inappropriate granularity, Cross-reward interference when embeddings aren't disentangled

- First 3 experiments: 1) Reproduce convergence speedup by training baseline vs. MIRO on same subset, 2) Validate multi-reward regularization by comparing single-reward vs. MIRO performance across all metrics, 3) Test inference controllability by sweeping aesthetic weight in reward targets

## Open Questions the Paper Calls Out

- Question: Does MIRO's efficiency and performance superiority persist when scaling model parameters and dataset size to match