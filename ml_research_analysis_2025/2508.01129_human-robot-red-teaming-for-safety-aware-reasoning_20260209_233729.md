---
ver: rpa2
title: Human-Robot Red Teaming for Safety-Aware Reasoning
arxiv_id: '2508.01129'
source_url: https://arxiv.org/abs/2508.01129
tags:
- human-robot
- robot
- team
- robots
- teaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling robots to reason
  over safety in safety-critical domains where robots operate alongside humans. The
  authors propose a human-robot red teaming paradigm for safety-aware reasoning, where
  humans and robots work together to challenge assumptions about an environment and
  explore potential hazards.
---

# Human-Robot Red Teaming for Safety-Aware Reasoning

## Quick Facts
- arXiv ID: 2508.01129
- Source URL: https://arxiv.org/abs/2508.01129
- Reference count: 40
- Primary result: Human-robot red teaming achieved 0.875 planning success rate and 0.83 safety success rate in robot execution experiments

## Executive Summary
This paper addresses the challenge of enabling robots to reason over safety in safety-critical domains by proposing a human-robot red teaming paradigm for safety-aware reasoning. The approach involves iteratively analyzing a shared model of the environment through three levels: generating possibilities, identifying assumptions, and reflecting on limitations through dialogue. The method was tested across 8 problem domains using symbolic planning and robot execution experiments with two different robot embodiments. Results demonstrate that robots can effectively and safely plan and operate in different environments under varying definitions of safety, helping robots earn trust as collaborators in safety-critical tasks.

## Method Summary
The method employs a three-level Human-Robot Red Teaming (HRRT) loop to iteratively refine symbolic environment models. HRRT2 generates state-action transitions, HRRT3 identifies pre/post-condition assumptions, and HRRT4 reflects via dialogue between computational agents. A "Red" chatbot agent probes for weaknesses while a "Blue" agent (ChatGPT) proposes model updates filtered by human operators. The refined models are exported to PDDL and tested with STRIPS planners. For execution, insights from HRRT are distilled into logistic regression models that predict risk-mitigating action utilities based on perceived hazards.

## Key Results
- Planning success rate of 0.875 across 400 planning tasks in 8 different domains
- Safety success rate of 0.83 across 12 robot execution trials with two different robot embodiments
- Models saturate after approximately 6 iterations, showing diminishing returns from additional refinement
- Execution success rate lower than planning due to perception issues (false negatives in hazard detection)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Model Expansion via Possibility Generation
The three-level HRRT process expands a simplified world model to include critical safety constraints. Each iteration through H2, H3, and H4 produces a model hypothesis with more information than the previous model, effectively "debugging" the robot's understanding of the domain.

### Mechanism 2: Dialogue-Driven Reflection for Constraint Discovery
A dialogue interface between Red and Blue computational agents enables discovery of safety constraints that purely algorithmic search might miss. The LLM proposes modifications filtered by human operators, injecting external/common-sense knowledge into the symbolic model.

### Mechanism 3: Translation of Symbolic Safety Knowledge to Utility-Based Execution
Insights from HRRT iterations are distilled into logistic regression models that predict risk-mitigating action utilities during real-time execution, mapping abstract safety rules to continuous robot control.

## Foundational Learning

- **Symbolic Planning (STRIPS/PDDL)**: Essential for understanding how planners use pre-conditions and effects to search for goal states. *Quick check*: Can you explain how changing a "pre-condition" in a PDDL domain definition affects the set of valid plans a planner can generate?

- **Red Teaming / Adversarial Testing**: The paper adapts military/cybersecurity concepts to human-robot teams. *Quick check*: In this paper, is the "Red Team" a separate physical robot, or a computational process running on the same system?

- **Model-Based Reflex Agents**: The execution phase uses a utility model to choose actions based on current state/perception. *Quick check*: Why might a reflex agent (logistic regression) be preferred over a full symbolic planner for real-time execution on the Valkyrie robot?

## Architecture Onboarding

- **Component map**: Model (M=(S,A)) -> Red Agent (dialogue tree Σ) -> Blue Agent (ChatGPT + Human filter) -> Planner (Pyperplan) -> Execution Agent (Logistic Regression)

- **Critical path**: Initialize model M⁰ → Run HRRT Loop (Generate Possibilities → Identify Assumptions → Reflect via Dialogue) → Export to PDDL → Train Execution Policy → Deploy to robot

- **Design tradeoffs**: Generality vs. Saturation (models saturate after ~6 iterations) and LLM Reliability vs. Human Effort (reliance on human to filter "many suggestions")

- **Failure signatures**: Saturation (curve flattens after ~6 iterations), Perception Gap (execution success lower than planning due to false negatives)

- **First 3 experiments**: 
  1. Ablation Study: Run HRRT on "Mars Science Team" domain, stripping away levels to verify each contributes to success rate
  2. Saturation Test: Run 10 iterations on novel domain, plot planning success rate vs. iteration number
  3. Simulated Execution Transfer: Train classifier on "Household" model features and test against simulated environment with randomized hazards

## Open Questions the Paper Calls Out

1. **Independent Expert Humans**: How does recruiting independent expert humans to provide insights affect the robustness and bias of HRRT models compared to using authors as the blue human agent?

2. **Successive HRRT Iterations**: How do successive HRRT model iterations translate to improved safety and performance in physical robot execution tasks?

3. **Improved Hazard Identification**: How can hazard identification be improved to effectively differentiate between safe and unsafe operating conditions under varying environmental factors like lighting?

## Limitations

- Gap between symbolic model refinement and real-world execution, with execution success (0.83) lower than planning success (0.875)
- Reliance on ChatGPT introduces uncertainty about quality and domain-specificity of suggestions
- Small sample size (12 trials total) limits generalizability of claims about different robot embodiments

## Confidence

- **High Confidence**: Iterative HRRT mechanism effectively improves symbolic planning models (saturation curve shows diminishing returns after ~6 iterations)
- **Medium Confidence**: Translation from symbolic safety reasoning to utility-based execution works but lacks detail on feature engineering
- **Low Confidence**: Claim about robots with different embodiments operating safely supported by only two robot trials across two environments

## Next Checks

1. **Perception Validation**: Validate hazard detection pipeline in simulation with randomized lighting conditions and object occlusions before physical trials

2. **Model Saturation Test**: Run full HRRT loop for 10+ iterations on novel domain and plot planning success rate vs. iteration number

3. **Execution Policy Transfer**: Implement trained logistic regression utility model from "Household" domain on simulated robot with randomized hazard placements to verify generalization