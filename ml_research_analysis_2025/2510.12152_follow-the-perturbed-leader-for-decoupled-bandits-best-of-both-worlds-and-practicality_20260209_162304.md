---
ver: rpa2
title: 'Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and
  Practicality'
arxiv_id: '2510.12152'
source_url: https://arxiv.org/abs/2510.12152
tags:
- regret
- policy
- which
- where
- regime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new FTPL policy for decoupled bandits that
  achieves Best-of-Both-Worlds (BOBW) regret bounds while avoiding computationally
  expensive convex optimization or resampling steps. The key technical contribution
  is a Pareto perturbation-based FTPL algorithm that approximates the arm-selection
  probabilities efficiently using a tight upper bound, enabling BOBW regret guarantees
  without explicit probability computation.
---

# Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and Practicality

## Quick Facts
- **arXiv ID:** 2510.12152
- **Source URL:** https://arxiv.org/abs/2510.12152
- **Authors:** Chaiwon Kim; Jongyeong Lee; Min-hwan Oh
- **Reference count:** 40
- **Primary result:** Achieves Best-of-Both-Worlds regret bounds without expensive convex optimization or resampling

## Executive Summary
This paper introduces a novel Follow-the-Perturbed-Leader (FTPL) algorithm for decoupled bandits that achieves Best-of-Both-Worlds (BOBW) regret guarantees while avoiding computationally expensive convex optimization or resampling steps. The key innovation is using Pareto perturbations to approximate arm-selection probabilities efficiently, enabling BOBW performance without explicit probability computation. The method achieves $O(\sqrt{KT})$ adversarial regret and $O(K/\Delta_{\min})$ stochastic regret, matching or improving upon previous BOBW results while running approximately 20× faster than the prior FTRL-based BOBW policy.

## Method Summary
The paper proposes a Pareto perturbation-based FTPL algorithm for decoupled bandits that approximates the arm-selection probabilities efficiently using a tight upper bound. The algorithm maintains cumulative estimated losses, samples Pareto perturbations, selects exploitation arms using perturbed losses, and approximates exploration probabilities using a rank-based upper bound that avoids expensive geometric resampling. This design achieves BOBW guarantees while maintaining computational efficiency.

## Key Results
- Achieves $O(\sqrt{KT})$ adversarial regret and $O(K/\Delta_{\min})$ stochastic regret bounds
- Runs approximately 20× faster than the prior FTRL-based BOBW policy (Decoupled-Tsallis-INF)
- Demonstrates better empirical performance in both stochastic and adversarial regimes
- Outperforms pure exploration policies and naive combinations of pure exploration with standard exploitation

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Feedback Structure
- **Claim:** Separating the exploration arm ($j_t$) from the exploitation arm ($i_t$) allows the learner to observe the loss of a potentially suboptimal arm without incurring its cost, enabling constant regret in stochastic environments.
- **Mechanism:** At round $t$, the learner incurs the loss of $i_t$ (blindly) and observes the loss of $j_t$ (freely). This decoupling avoids the "exploration-exploitation tradeoff" inherent in standard MABs where observing a suboptimal arm necessarily incurs its loss.
- **Core assumption:** The environment permits this decoupling (e.g., access to a simulator or a parallel sensing channel) and losses are bounded in $[0,1]$.
- **Evidence anchors:**
  - [Abstract] "The loss of the explored arm is observed but not counted... constant regret in the stochastic regime, improving upon the optimal bound of the standard MABs."
  - [Section 1] "Decoupled MAB setting... recover[s] the standard MAB, when the learner is restricted to select the same arm for both objectives."
  - [Corpus] The corpus supports the general interest in BOBW and structured bandits, though specific "decoupled" signals are primarily from the target paper.
- **Break condition:** If the environment forces $i_t = j_t$ (reverting to standard MAB), the constant regret guarantee vanishes, returning to logarithmic dependence on $T$.

### Mechanism 2: Heavy-Tailed Perturbations for BOBW
- **Claim:** Using Pareto (Fréchet-type) perturbations with shape $\alpha$ allows FTPL to achieve Best-of-Both-Worlds (BOBW) guarantees without the computational overhead of solving convex programs required by FTRL methods like Tsallis-INF.
- **Mechanism:** The algorithm selects $i_t = \arg\min \{ \hat{L}_{t,i} - r_{t,i}/\eta_t \}$ where $r_{t,i} \sim \text{Pareto}(\alpha)$. The heavy tail of the Pareto distribution implicitly creates a regularizer similar to Tsallis entropy, promoting the necessary stability for stochastic environments while maintaining adaptability in adversarial ones.
- **Core assumption:** The mapping $\alpha \leftrightarrow \beta$ (where $\beta$ is the Tsallis entropy parameter) holds; specifically $\alpha = 1/(1-\beta)$. For optimal performance, $\alpha \in (2, 3]$.
- **Evidence anchors:**
  - [Abstract] "Propose a policy within the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations... avoiding... convex optimization."
  - [Section 3.2] "Natural to adopt a Fréchet-type perturbation, due to its correspondence with $\beta$-Tsallis-INF... although $w_{t,i}$... does not have a closed form."
- **Break condition:** If perturbations are light-tailed (e.g., Gumbel, corresponding to Exponential weights/Exp3), the policy fails to achieve the improved stochastic bound (remains $O(\sqrt{T})$ rather than constant/time-independent).

### Mechanism 3: Implicit Probability Approximation
- **Claim:** Efficiently approximating the exploration probability $p_t$ using a sortable upper bound $q_t$ avoids the need for expensive geometric resampling or implicit differentiation, maintaining $O(K \log K)$ complexity.
- **Mechanism:** Instead of computing the exact probability $w_t$ (which requires integrals), the algorithm computes $q_{t,i} \approx w_{t,i}$ using the loss gap vector and a rank-based scaling term (Eq 7). This $q_t$ is then normalized to form $p_t$.
- **Core assumption:** The upper bound $q_t$ is sufficiently tight to the true $w_t$ such that the variance of the importance-weighted estimator remains controlled.
- **Evidence anchors:**
  - [Section 3.1] "Direct application [of resampling] infeasible, since computing $p_t$ requires estimates for all arms."
  - [Section 3.2] "$q_{t,i}$ can be viewed as an approximation of $w^{1/2+1/2\alpha}_{t,i}$... computable directly without additional convex optimization... at $O(K \log K)$ per-step cost."
- **Break condition:** If the approximation $q_t$ diverges significantly from $w_t$ (e.g., in high-noise regimes where ranks fluctuate wildly), the regret bounds may loosen, potentially losing the "Best-of-Both-Worlds" property.

## Foundational Learning

- **Concept:** **Importance Weighting (IW) Estimator**
  - **Why needed here:** To construct an unbiased estimate of the full loss vector $\ell_t$ given only the observed loss of the exploration arm $j_t$. This drives the cumulative loss estimate $\hat{L}_t$.
  - **Quick check question:** Can you derive why $\hat{\ell}_{t,i} = \ell_{t,i} \frac{\mathbb{1}[j_t=i]}{p_{t,i}}$ is an unbiased estimator of $\ell_{t,i}$?

- **Concept:** **Follow-the-Regularized-Leader (FTRL) vs. Follow-the-Perturbed-Leader (FTPL)**
  - **Why needed here:** The paper frames its contribution as an FTPL alternative to the FTRL-based "Decoupled-Tsallis-INF". Understanding this duality (perturbations $\approx$ regularization) is key to the BOBW mechanism.
  - **Quick check question:** Explain the computational bottleneck in FTRL (solving $\nabla \Phi(w) = -\eta \hat{L}_t$) that FTPL bypasses by sampling $r_t$.

- **Concept:** **Stochastically Constrained Adversarial (SCA) Regime**
  - **Why needed here:** The theoretical guarantees are proven for this generalized regime, which includes stochastic bandits as a special case but allows for some adversarial drift. This explains the robustness of the bounds.
  - **Quick check question:** In the SCA regime, what quantity remains fixed (hint: check the gaps $\Delta_i$), and what can change?

## Architecture Onboarding

- **Component map:** Loss Accumulator -> Perturbation Sampler -> Exploitation Selector -> Probability Approximator -> Exploration Sampler -> Estimator Update
- **Critical path:** The computation of $q_t$ (Eq 7) involves sorting $\hat{L}_t$ to determine ranks. This $O(K \log K)$ step replaces the $O(K)$ iterations of Newton's method or resampling.
- **Design tradeoffs:**
  - **Speed vs. Precision:** The method uses a rank-based approximation $q_t$ rather than the exact $w_t$. This buys a ~20x speedup (per experiments) but introduces approximation error in the probability estimates.
  - **Universality vs. Simplicity:** Uses a uniform learning rate $\eta_t$ for simplicity, whereas optimal bounds might theoretically require arm-dependent rates (noted in Section 3.3).
- **Failure signatures:**
  - **Exploding Regret (Adversarial):** If $\eta_t$ is not decayed correctly (e.g., fixed rate), the algorithm fails to "forget" old perturbations/losses.
  - **Numerical Instability:** The update involves dividing by $p_{t,i}$. If $p_{t,i} \to 0$ for the selected arm, variance explodes. (Mitigated by the design of $q_t$ which prevents zero probabilities).
- **First 3 experiments:**
  1. **Adversarial Regime Validation:** Implement the "switching" adversarial environment described in Section 4 (loss means alternate). Plot cumulative regret vs. $T$ to verify $O(\sqrt{T})$ growth against FTRL.
  2. **Stochastic Regime Validation:** Run in a simple K-armed Bernoulli setting. Verify that regret flattens (becomes constant/time-independent) faster than standard MAB algorithms, specifically comparing against Decoupled-Tsallis-INF.
  3. **Scaling Profiling:** Measure per-step runtime as $K$ increases. Plot Time vs. $K$ on a log-log scale to verify the empirical ~20x speedup and $O(K \log K)$ scaling vs. FTRL's higher complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can arm-dependent learning rates be integrated into the FTPL framework for decoupled bandits to achieve the optimal $\tilde{O}(\sqrt{K}/\Delta_{\min})$ regret in the stochastically constrained adversarial (SCA) regime?
- **Basis in paper:** [explicit] The authors state they expect optimal regret can be achieved this way, but the analysis requires "more intricate techniques beyond the scope of this paper."
- **Why unresolved:** The proposed policy uses a uniform learning rate, resulting in a stochastic regret bound of $O(K/\Delta_{\min})$, which is looser than the $\sqrt{K}/\Delta_{\min}$ rate achievable by FTRL methods under specific conditions.
- **What evidence would resolve it:** A theoretical analysis showing the FTPL policy achieves $\tilde{O}(\sqrt{K}/\Delta_{\min})$ regret using an arm-dependent learning rate schedule.

### Open Question 2
- **Question:** Is it possible to design stability-penalty matching learning rates for FTPL with Pareto perturbations to optimize regret guarantees for general perturbation shapes $\alpha > 1$?
- **Basis in paper:** [explicit] The paper notes that while such designs exist for FTRL, they "have not been explored for FTPL" primarily because the arm-selection probability $w_t$ lacks a closed-form expression.
- **Why unresolved:** Current FTPL analysis relies on uniform or simple decreasing rates, potentially leaving a gap in minimizing the stability and penalty terms simultaneously for arbitrary $\alpha$.
- **What evidence would resolve it:** A derivation of a learning rate scheme that equalizes the contributions of the stability and penalty terms in the regret bound for general $\alpha$.

### Open Question 3
- **Question:** Can the Pareto perturbation-based approximation method be extended to linear or contextual bandits to achieve Best-of-Both-Worlds guarantees without convex optimization?
- **Basis in paper:** [explicit] The conclusion suggests that using an approximation of $w_t$ "potentially serves as a foundation for establishing BOBW guarantees for FTPL beyond the MAB setting."
- **Why unresolved:** The current analysis relies on properties specific to the multi-armed bandit loss estimators and the simplex constraint, which may not directly translate to more complex decision sets.
- **What evidence would resolve it:** A formulation of the algorithm for linear bandits with accompanying BOBW regret bounds and a demonstration of computational efficiency compared to linear FTRL methods.

## Limitations

- The approximation quality of the exploration probability $q_t$ may degrade in highly adversarial regimes where rank stability is compromised
- The stochastic regret bound of $O(K/\Delta_{\min})$ is looser than the theoretically optimal $\tilde{O}(\sqrt{K}/\Delta_{\min})$ achievable by FTRL methods
- The current analysis relies on uniform learning rates, which may not be optimal for achieving the best possible regret guarantees

## Confidence

- **High confidence** in the adversarial regret bound ($O(\sqrt{KT})$): This follows from established FTPL theory and the stability analysis provided.
- **Medium confidence** in the stochastic regret bound ($O(K/\Delta_{\min})$): While theoretically sound, the practical performance depends critically on the approximation accuracy of $q_t$ and the Pareto perturbation parameterization.
- **Medium confidence** in the empirical 20× speedup claim: The result appears solid but is based on a single comparison point; scaling behavior across different problem sizes needs verification.

## Next Checks

1. **Robustness testing:** Implement the algorithm in environments with varying levels of adversarial drift (e.g., periodic loss switches, gradually changing gaps) to verify the SCA-regime guarantees hold empirically across the full spectrum between pure stochastic and pure adversarial.

2. **Approximation error quantification:** Systematically measure the divergence between the computed $q_t$ and the true exploration probabilities $w_t$ across different problem instances, correlating this with regret performance to identify potential failure modes.

3. **Scaling verification:** Conduct comprehensive runtime profiling across a range of $K$ values (beyond the single comparison point) to confirm the claimed $O(K \log K)$ scaling advantage over FTRL-based methods, accounting for implementation-specific constants.