---
ver: rpa2
title: 'From What to How: Attributing CLIP''s Latent Components Reveals Unexpected
  Semantic Reliance'
arxiv_id: '2505.20229'
source_url: https://arxiv.org/abs/2505.20229
tags:
- components
- clip
- latent
- component
- spurious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance

## Quick Facts
- arXiv ID: 2505.20229
- Source URL: https://arxiv.org/abs/2505.20229
- Reference count: 40
- Primary result: None

## Executive Summary
This paper presents a framework to analyze CLIP's latent components and uncover unexpected semantic reliance, such as spurious correlations and polysemous word interpretations. The method combines Sparse Autoencoders (SAEs) to decompose CLIP embeddings into interpretable components with instance-wise attribution scores to quantify each component's influence on predictions. Applied to CLIP variants, the framework identifies failure modes including visual artifacts, typographic text, and polysemous word meanings, with a case study showing "red hue" as a spurious shortcut in medical image classification.

## Method Summary
The framework trains a top-k Sparse Autoencoder on CLIP's final transformer block class token embeddings to extract interpretable latent components. For a given image-text pair, it computes attribution scores using Activation×Gradient (Act×Grad) - a first-order Taylor approximation of a component's causal influence on the cosine similarity output. Components are labeled by semantic alignment against predefined text labels. The method detects unexpected reliance through two strategies: finding high-relevance components with low semantic alignment (hidden concepts) and identifying outlier components with unusually high relevance via z-scores. The approach is validated through deletion/insertion experiments and case studies on ImageNet and medical imaging datasets.

## Key Results
- Act×Grad attribution outperforms Logit Lens and Energy baselines in deletion/insertion faithfulness experiments (AUC scores).
- SAEs yield 5,363 unique semantic labels, significantly more specific than PCA's less than 500 labels.
- The framework identifies over 220 failure cases in CLIP, including polysemous words, typographic text, and dataset artifacts.
- Medical case study reveals "red hue" background as a spurious feature in melanoma detection models.

## Why This Works (Mechanism)

### Mechanism 1: Attribution Patching for Latent Component Influence
Attribution patching (Activation × Gradient) provides a more faithful estimate of a component's causal influence on CLIP predictions than Logit Lens or activation magnitude alone. The method computes an attribution score $R_j(x, t) \approx a_j \cdot \frac{\partial y(x,t)}{\partial a_j}$, which is a first-order Taylor approximation of the change in output (cosine similarity) when a component's activation $a_j$ is zeroed. Crucially, this includes a scaling factor for component strength (activation magnitude) and a correction term that accounts for the output already being high or high alignment between the component and the full embedding. This contrasts with Logit Lens, which only measures global alignment, ignoring magnitude and interactions.

### Mechanism 2: SAE-based Component Extraction and Semantic Labeling
Sparse Autoencoders (SAEs) can decompose CLIP's class token embedding into a large number of interpretable, monosemantic components, which can be labeled by semantic alignment. An SAE is post-hoc trained to reconstruct the CLIP embedding $x$ via a sparse combination of feature vectors (components). Top-k activation enforces sparsity. To label a component, the method retrieves highly activating images, computes their average CLIP embedding, and finds its alignment score against a predefined set of text labels, assigning the best-aligned label.

### Mechanism 3: Detecting Unexpected Reliance via Combined Analysis
Combining attribution scores with semantic alignment scores systematically reveals components the model relies on unexpectedly for a prediction. The framework defines two types of unexpected reliance: 1) Hidden concepts (high relevance, low alignment): Filters components with $R_j(x,t) \ge \tau_{rel}$ (highly influential) AND $s_j(t) \le \tau_{align}$ (poorly aligned with expected labels). 2) Failure cases (higher than expected relevance): Computes a z-score for a component's relevance on a test sample against a reference distribution (e.g., training set samples). High z-scores (e.g., >3.0) flag outliers.

## Foundational Learning

**Concept: Sparse Autoencoders (SAEs) for Mechanistic Interpretability**
- Why needed here: SAEs are the tool used to decompose CLIP's dense, non-interpretable embeddings into human-understandable components ("latents"). Without this decomposition, instance-wise attribution would be applied to meaningless directions.
- Quick check question: Can you explain how a Top-k SAE enforces sparsity and why this might lead to more monosemantic features compared to a standard autoencoder?

**Concept: First-Order Taylor Approximation in Attribution**
- Why needed here: The core attribution method is derived from a Taylor series expansion, estimating the causal effect of setting a component to zero. Understanding this approximation is key to knowing its limits (e.g., with non-linear interactions).
- Quick check question: How does the first-order Taylor approximation in Equation (5) simplify the problem of causal intervention, and what assumption does it make about the function's behavior around the intervention point?

**Concept: Cosine Similarity as a Target for Attribution**
- Why needed here: CLIP's prediction is the cosine similarity between image and text embeddings. Attribution must be derived with respect to this specific output function, leading to the specialized gradient formula.
- Quick check question: Why is the gradient of cosine similarity with respect to the embedding non-trivial to compute compared to a standard linear layer, and how does the LayerNorm within the cosine similarity calculation affect the gradient flow?

## Architecture Onboarding

**Component map:** CLIP ViT Encoder (Frozen) -> Final Transformer Block Class Token -> LayerNorm & Projection -> **Sparse Autoencoder (SAE)**. The SAE decoder's feature vectors $v_j$ and activations $a_j$ become the **Latent Components**. Attribution is computed via backpropagation from the **Cosine Similarity Output** back to $a_j$.

**Critical path:** The analysis pipeline is:
1. **Train SAE:** Train a top-k SAE on the class token embeddings from a large dataset (e.g., ImageNet train).
2. **Label Components:** For each SAE component, retrieve top activating images and assign a semantic label using alignment scores against a predefined text label set.
3. **Attribute & Detect:** For a given image-text pair, compute instance-wise attribution scores ($R_j$). Apply the detection thresholds from Section 4.2 to find unexpected high-relevance or outlier components.

**Design tradeoffs:**
- **SAE Layer:** Applying SAE to the final embedding is most predictive but may miss hierarchical features from earlier layers.
- **Label Set:** Semantic alignment relies on a predefined set of text labels. Incompleteness in this set will lead to "hidden" concepts that are valid but unlabeled.
- **Attribution Method:** Act×Grad is efficient and faithful for main effects but is a first-order approximation. Integrated Gradients may be more faithful for complex interactions but is ~N times more expensive.

**Failure signatures:**
- **Spurious Correlation:** High-relevance component aligns with an artifact (e.g., "red hue", "typographic text") rather than the main subject.
- **Polysemy/Compound Words:** High-relevance component aligns with a different meaning of a polysemous word or a word-part in a compound noun (e.g., "crane" -> construction equipment for bird images).
- **Semantic Ambiguity:** Text embedding is influenced by unexpected concepts, leading to lower "spurious AUC" compared to linear classifiers.

**First 3 experiments:**
1. **SAE Faithfulness & Diversity Check:** Train an SAE on a subset of your data. Perform the deletion/insertion experiment (Sec 5.1) comparing Act×Grad, Logit Lens, and Energy to validate the attribution method's faithfulness for your specific CLIP model. Also, visualize top-activating images for random components to gauge interpretability.
2. **Targeted Spurious Feature Audit:** Pick a class with known visual artifacts (e.g., from a dataset like ImageNet). Apply the "hidden concepts" detection (Sec 4.2a) with high relevance and low alignment thresholds to see if your framework automatically flags the artifact-related components.
3. **Downstream Classifier Robustness Test:** Train a simple linear classifier on the CLIP embeddings for a downstream task. Use the framework to find the top-3 most relevant SAE components for predictions. Check their semantic labels. If they are artifacts (e.g., background colors), apply the suggested latent augmentation (Sec 5.4, Eq. 34) to retrain the classifier and measure robustness improvement.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How do latent component distributions and their reliance on spurious correlations differ across intermediate transformer layers compared to the final block analyzed in this study?
- Basis in paper: [explicit] The authors note in the Limitations section that they "focus on class token embeddings from the final transformer block... but may miss relevant dynamics in earlier layers."
- Why unresolved: The methodology was restricted to the final layer to ensure predictiveness, leaving the evolution of concepts and failure modes throughout the network depth unexplored.
- What evidence would resolve it: Training SAEs and computing attributions for class tokens at intermediate layers (e.g., blocks 6, 12, 18 in ViT-L/14) to compare the semantic clarity and spuriousness of concepts across depth.

**Open Question 2**
- Question: To what extent does the reliance on an external model (CLIP-Mobile-S2) for semantic alignment introduce bias into the identification of "unexpected" components?
- Basis in paper: [explicit] The authors state that their "semantic alignment relies on CLIP-Mobile-S2, which may itself be affected by spurious correlations or ambiguous language."
- Why unresolved: The framework uses a secondary model to label components, creating a circular dependency where the explainer's own biases may mask or misinterpret the primary model's features.
- What evidence would resolve it: A comparative analysis using human evaluation or distinct vision-language models for the labeling step to measure the variance in detected "unexpected" components.

**Open Question 3**
- Question: Does the latent space augmentation strategy used to mitigate "red hue" bias in melanoma detection generalize to more abstract spurious correlations in non-medical domains?
- Basis in paper: [inferred] The paper demonstrates a successful intervention for a specific visual artifact (color) in a case study, but does not validate if this method scales to the other failure modes identified, such as typography or compound nouns.
- Why unresolved: The correction method relies on estimating a specific feature direction for augmentation, which may be harder to isolate for complex semantic or textual artifacts compared to simple color shifts.
- What evidence would resolve it: Applying the latent augmentation technique to the "visual typography" or "polysemous word" failure cases identified in the ImageNet experiments to test if classifier robustness improves.

## Limitations

- The framework relies on a predefined semantic label set, which may be incomplete and cause false positives for valid but unexpected concepts.
- The first-order Taylor approximation in Act×Grad may miss non-linear interactions between components.
- The z-score method for detecting failure cases assumes a normal distribution of feature relevance, which may not always hold.

## Confidence

- **High Confidence:** The framework's general methodology (SAE extraction + attribution + semantic alignment) is sound and well-grounded in established literature. The detection of spurious correlations in the medical case study is a strong empirical validation.
- **Medium Confidence:** The faithfulness of the Activation×Gradient method is well-supported for main effects, but the first-order Taylor approximation may miss non-linear interactions. The specific thresholds (τ_rel, τ_align) for detecting unexpected reliance are somewhat arbitrary and may require tuning for different datasets or CLIP variants.
- **Low Confidence:** The framework's reliance on a predefined semantic label set T is a significant limitation. It may fail to detect truly novel or unlabeled concepts, and may generate false positives for valid but unexpected concepts. The z-score method for failure cases assumes a normal distribution of feature relevance, which may not always hold.

## Next Checks

1. **Cross-Model Generalization Test:** Apply the framework to a CLIP variant not in the original study (e.g., a more recent CLIP model or a different vision architecture like ConvNeXT). Check if the same spurious features (e.g., "red hue") are detected, validating the method's generalizability.

2. **Label Set Completeness Audit:** For a sample of flagged "hidden concepts" (high relevance, low alignment), manually inspect the top-activating images and their text labels. Determine the percentage that are valid concepts missing from the predefined label set T versus true spurious correlations. This will quantify the false positive rate due to incomplete semantic coverage.

3. **Ablation Study on SAE Parameters:** Perform a sensitivity analysis by varying the SAE's sparsity parameter (k), the number of components (d_SAE), and the training dataset size. Measure the impact on the number of interpretable components and the faithfulness of the attribution method. This will clarify the optimal configuration for a given use case.