---
ver: rpa2
title: 'OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report
  Generation'
arxiv_id: '2512.01896'
source_url: https://arxiv.org/abs/2512.01896
tags:
- event
- score
- public
- evaluation
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper defines the Automated Online Public Opinion Report\
  \ Generation (OPOR-GEN) task and introduces OPOR-BENCH, the first event-centric\
  \ benchmark for generating structured crisis reports from news and social media.\
  \ It proposes OPOR-EVAL, an agent-based evaluation framework that achieves strong\
  \ correlation with human judgment (\u03C1=0.69, MAE=0.53) using GPT-4o."
---

# OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation

## Quick Facts
- **arXiv ID**: 2512.01896
- **Source URL**: https://arxiv.org/abs/2512.01896
- **Reference count**: 40
- **Key outcome**: Introduces OPOR-BENCH benchmark for automated Online Public Opinion Report generation with GPT-4o evaluator achieving ρ=0.69 correlation with human judgment

## Executive Summary
This paper introduces the Automated Online Public Opinion Report Generation (OPOR-GEN) task and presents OPOR-BENCH, the first event-centric benchmark for generating structured crisis reports from news and social media. The benchmark consists of 463 crisis events spanning 2012-2025, each containing news articles and social media posts, with reports structured into five sections: Event Title, Event Summary, Event Timeline, Event Focus, and Event Suggestions. The paper proposes OPOR-EVAL, an agent-based evaluation framework that achieves strong correlation with human judgment (ρ=0.69, MAE=0.53) using GPT-4o, and evaluates five frontier models across factual consistency, multi-source synthesis, and practical reasoning dimensions.

## Method Summary
The OPOR-BENCH dataset is constructed by collecting crisis events from EM-DAT and Wikipedia, gathering news articles from Wikipedia references and social media posts via X/Twitter API, and applying BM25 reranking for article relevance filtering. The annotation pipeline uses LLM-based factual attribute extraction and social media author classification with human expert timeline annotation. The OPOR-EVAL framework employs an agent-based evaluation system with GPT-4o managing three tools (Fact-Checker, Opinion-Miner, Solution-Counselor) following ReAct-style reasoning-acting protocols. Two generation strategies are implemented: modular (five sequential subtasks) and end-to-end (single pass), evaluated across 15 dimensions using a 5-point Likert scale across three categories.

## Key Results
- Gemini 2.5 Pro achieved the highest average score of 3.71/5 across all evaluation dimensions
- Strong correlation with human judgment (ρ=0.69, MAE=0.53) demonstrates OPOR-EVAL's effectiveness
- Temporal reasoning remains a critical bottleneck, with timeline phase accuracy scoring only 1.25/5
- Significant evaluator bias observed: DeepSeek-V3 assigns scores 0.40 points higher than GPT-4o on average

## Why This Works (Mechanism)
The framework leverages agent-based evaluation with specialized tools to decompose complex report generation into manageable subtasks, enabling systematic assessment of multi-source synthesis capabilities. The modular generation strategy allows targeted improvement of individual report sections while maintaining coherence through structured prompts.

## Foundational Learning
- **Event-centric crisis reporting**: Structured crisis reports require synthesizing heterogeneous data sources into coherent narratives - needed for real-world disaster response applications
- **Multi-source synthesis**: Combining news articles and social media posts requires understanding different information credibility levels - critical for accurate public opinion assessment
- **Temporal reasoning**: Identifying phase boundaries (Incubation/Peak/Decline) from volume trends is essential for timeline accuracy - remains a major challenge
- **Agent-based evaluation**: Using specialized tools for fact-checking, opinion mining, and solution counseling enables comprehensive assessment - validated by strong human correlation
- **Modular vs end-to-end generation**: Different strategies offer trade-offs between synthesis quality and detail accuracy - task-dependent performance suggests hybrid approaches

## Architecture Onboarding

**Component Map:**
News/Social Media -> Modular/End-to-End Generator -> OPOR-EVAL Agent -> 15-Dimensional Score

**Critical Path:**
Event Collection -> Document Preprocessing -> Report Generation -> Tool-Guided Evaluation -> Human Correlation Validation

**Design Tradeoffs:**
- Modular generation provides better control but may lose coherence; end-to-end generation maintains context but struggles with detailed analysis
- Agent-based evaluation offers transparency but introduces potential evaluator bias; simpler metrics would be faster but less comprehensive
- High context windows enable better synthesis but increase computational cost and risk of information overload

**Failure Signatures:**
- Timeline generation consistently scores low (1.25/5) due to poor temporal reasoning
- Event Focus quality degrades with high tweet volume (r=-0.44)
- Systematic scoring variance between evaluators (0.40 point difference)

**First Experiments:**
1. Run modular and end-to-end strategies on the same event to compare coherence vs detail quality
2. Evaluate with multiple evaluator instances to quantify scoring variance
3. Test input scaling by varying tweet counts to identify information overload thresholds

## Open Questions the Paper Calls Out

### Open Question 1
How can modular and end-to-end generation strategies be effectively hybridized to optimize the trade-off between high-level synthesis and detailed analysis? The paper notes distinct trade-offs between strategies but only benchmarked them independently.

### Open Question 2
How can LLMs be enhanced to perform the complex temporal reasoning required for accurate timeline date generation? Models currently struggle to identify inflection points from volume trends, achieving only 1.25/5 on timeline phase accuracy.

### Open Question 3
What calibration methods can mitigate the systematic scoring biases found in LLM-based evaluators? Different evaluators apply vastly different strictness levels, with DeepSeek-V3 scoring 0.40 points higher than GPT-4o on average.

## Limitations
- Temporal reasoning remains a critical bottleneck with only 1.25/5 score on timeline phase accuracy
- Information overload significantly degrades performance, with negative correlation between tweet volume and Event Focus quality
- Systematic evaluator bias across models creates reliability concerns, with 0.40 point scoring variance between evaluators

## Confidence
- **High confidence**: Task definition and dataset construction methodology are well-specified
- **Medium confidence**: OPOR-EVAL framework shows reasonable human correlation but has evaluator bias concerns
- **Low confidence**: Claims about modular vs end-to-end trade-offs are based on limited experimentation

## Next Checks
1. Test evaluator consistency by running OPOR-EVAL with multiple independent instances using different random seeds
2. Conduct ablation studies on temporal reasoning by isolating timeline generation from other components
3. Evaluate performance scaling with input volume by systematically varying tweet counts to quantify information overload thresholds