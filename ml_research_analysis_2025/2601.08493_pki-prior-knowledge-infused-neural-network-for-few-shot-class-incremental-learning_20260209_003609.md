---
ver: rpa2
title: 'PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental
  Learning'
arxiv_id: '2601.08493'
source_url: https://arxiv.org/abs/2601.08493
tags:
- learning
- knowledge
- classes
- prior
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Prior Knowledge-Infused Neural Network (PKI)
  to address catastrophic forgetting and overfitting in Few-Shot Class-Incremental
  Learning (FSCIL). The key idea is to retain and leverage prior knowledge by cascading
  an ensemble of projectors, where each projector is frozen after training and new
  projectors are added for incremental learning.
---

# PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2601.08493
- **Source URL:** https://arxiv.org/abs/2601.08493
- **Authors:** Kexin Baoa; Fanzhao Lin; Zichen Wang; Yong Li; Dan Zeng; Shiming Ge
- **Reference count:** 17
- **Key outcome:** Proposed PKI method achieves state-of-the-art average accuracies of 68.52% on CIFAR100, 68.51% on MiniImageNet, and 67.84% on CUB200 in FSCIL setting.

## Executive Summary
This paper addresses catastrophic forgetting and overfitting in Few-Shot Class-Incremental Learning (FSCIL) by proposing a Prior Knowledge-Infused Neural Network (PKI). The key innovation is cascading an ensemble of projectors where each projector is frozen after training, and new projectors are added for incremental learning. This approach effectively preserves old knowledge while allowing adaptation to new classes. The authors also propose PKIV-1 and PKIV-2 variants to reduce resource consumption by shrinking the number of projectors, with PKIV-2 achieving a balance between performance and resource efficiency.

## Method Summary
The PKI architecture consists of a frozen backbone, an ensemble of 3-layer MLP projectors (new projector added per session, previous frozen), L2 normalization, and a classifier. The memory module stores class-mean features. Base session training involves training all components on 60-100 base classes with full data augmentation for 100 epochs. For incremental sessions, the backbone and old projectors remain frozen while a new projector and classifier are fine-tuned on both new samples and memory features using cross-entropy loss with SGD+momentum and cosine annealing. PKIV-2 variant reduces projectors by grouping them (k=3), maintaining fewer total projectors than standard PKI.

## Key Results
- Achieves state-of-the-art average accuracies: 68.52% (CIFAR100), 68.51% (MiniImageNet), 67.84% (CUB200)
- PKI outperforms baseline methods by 3-5% average accuracy across all benchmarks
- PKIV-2 variant balances performance and resource consumption, reducing projector count by ~3Ã—
- Random initialization of new projectors outperforms inheriting weights from previous sessions
- Performance degradation observed in final sessions on fine-grained CUB200 dataset

## Why This Works (Mechanism)
PKI addresses FSCIL challenges through parameter isolation and knowledge preservation. By freezing the backbone and previously trained projectors, catastrophic forgetting is prevented as old knowledge remains intact. New projectors are specifically trained to recognize new classes while leveraging the frozen ensemble's discriminative features. The memory module provides balanced training by including both new samples and old class-mean features, reducing overfitting to new data. Random initialization of new projectors allows better adaptation to new classes compared to weight inheritance, which can cause gradient conflicts or feature space collapse.

## Foundational Learning

**Few-Shot Learning** - Learning from very limited examples (typically 1-5 shots per class). *Why needed:* FSCIL requires recognizing new classes from minimal samples. *Quick check:* Verify dataset splits contain only 5 samples per new class.

**Class-Incremental Learning** - Sequentially learning new classes without forgetting old ones. *Why needed:* Models must maintain performance on previously seen classes while adapting to new ones. *Quick check:* Track accuracy on old classes throughout incremental sessions.

**Catastrophic Forgetting** - Neural networks rapidly forgetting previously learned information when trained on new tasks. *Why needed:* Primary challenge in incremental learning scenarios. *Quick check:* Monitor old class accuracy degradation during incremental training.

**Parameter Isolation** - Freezing certain network parameters to preserve learned knowledge. *Why needed:* Prevents updates to critical features representing old classes. *Quick check:* Verify backbone and old projectors remain frozen during incremental training.

**Memory Replay** - Storing and reusing samples from previous classes during training. *Why needed:* Maintains balance between old and new class representations. *Why needed:* Enables training on both current and historical data simultaneously. *Quick check:* Confirm memory contains class-mean features from all prior sessions.

## Architecture Onboarding

**Component Map:** Backbone -> Ensemble of MLP Projectors -> L2 Normalization -> Classifier

**Critical Path:** Input images flow through frozen backbone to extract features, then through all frozen projectors plus the current active projector, normalized via L2, and classified by the FC classifier.

**Design Tradeoffs:** Parameter isolation prevents forgetting but increases model size linearly with sessions. Random initialization helps adaptation but requires more training. Memory usage balances between storing all samples versus class means.

**Failure Signatures:** Rapid accuracy drop on old classes indicates frozen parameters are being updated. High variance in per-session accuracy suggests insufficient training iterations or poor initialization. Consistently low performance on fine-grained datasets indicates feature extractor limitations.

**First Experiments:**
1. Implement base session training on CIFAR100 with 60 classes, verify convergence and memory population
2. Run single incremental session, check old class preservation and new class learning
3. Compare random vs inherited initialization for new projector on MiniImageNet

## Open Questions the Paper Calls Out

**Open Question 1:** How can the computational efficiency of the PKI framework be optimized to better support resource-constrained environments? The authors acknowledge needing further optimization to balance performance and resource consumption, particularly for edge deployment scenarios.

**Open Question 2:** Why does the model's performance degrade in the final sessions on fine-grained datasets like CUB200 despite high average performance? The paper notes similarity between old and new classes challenges discriminability but doesn't propose solutions for later incremental stages.

**Open Question 3:** What is the theoretical explanation for why random initialization of new projectors outperforms inheriting weights from the previous session? The authors present this as empirical finding without theoretical justification for why weight inheritance fails in this cascaded ensemble structure.

## Limitations
- Performance degradation in final sessions on fine-grained datasets like CUB200
- Linear increase in model size with number of incremental sessions due to frozen projectors
- Several critical implementation details unspecified (MLP architecture, classifier expansion, exact hyperparameters)
- Limited theoretical analysis of why random initialization outperforms weight inheritance

## Confidence

**High Confidence:** Core methodology (frozen backbone + ensemble of frozen projectors with new projector addition) and overall experimental setup (datasets, splits, metrics) are clearly specified and replicable.

**Medium Confidence:** Training procedure (base session vs incremental sessions, loss function, optimizer) is well-described, though exact hyperparameters require reasonable assumptions.

**Medium Confidence:** Performance comparisons with baselines are detailed, but exact reproduction depends on correctly implementing unspecified architectural details.

## Next Checks

1. Verify MLP projector architecture by cross-checking with authors' code or implementing with common defaults (e.g., [256, 128, 64] dimensions, ReLU activations).

2. Test classifier expansion by implementing multiple strategies (preserve old weights, random initialization, zero initialization) and measuring impact on performance.

3. Validate incremental session training by systematically varying the number of iterations (100, 150, 200) to determine the optimal range for each dataset.