---
ver: rpa2
title: 'Efficient LLMs with AMP: Attention Heads and MLP Pruning'
arxiv_id: '2504.21174'
source_url: https://arxiv.org/abs/2504.21174
tags:
- pruning
- attention
- language
- heads
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes AMP, a structured pruning method that removes\
  \ less critical attention heads and MLP neurons in LLMs by projecting input data\
  \ onto weights to assess structural importance. AMP achieves up to 1.25\xD7 inference\
  \ speedup while surpassing state-of-the-art pruning techniques by up to 1.49 percentage\
  \ points in average task accuracy on commonsense reasoning benchmarks."
---

# Efficient LLMs with AMP: Attention Heads and MLP Pruning

## Quick Facts
- arXiv ID: 2504.21174
- Source URL: https://arxiv.org/abs/2504.21174
- Reference count: 40
- This paper proposes AMP, a structured pruning method that removes less critical attention heads and MLP neurons in LLMs by projecting input data onto weights to assess structural importance.

## Executive Summary
This paper introduces AMP (Attention and MLP Pruning), a structured pruning method for LLMs that achieves up to 1.25× inference speedup while surpassing state-of-the-art pruning techniques by up to 1.49 percentage points in average task accuracy on commonsense reasoning benchmarks. AMP works by projecting attention head outputs through the output projection matrix and measuring MLP neuron activation strengths to assess structural importance, then removing the least critical components. The method requires only minutes to identify components for removal and achieves 30% pruning ratio with minimal impact on zero-shot task performance.

## Method Summary
AMP computes importance scores for attention heads by calculating the ℓ1 norm of each head's output projected through the corresponding output projection block (I_n = ||h_n · W_n||_1), and for MLP neurons by averaging the activation strength of up-gate pairs before the down projection. The method uniformly removes the lowest-scoring c% of components from each layer, preserving architectural regularity for practical speedups. After pruning, a 2-epoch LoRA fine-tuning (rank=8) on the Alpaca dataset recovers performance. Experiments on LLaMA and Phi model families demonstrate AMP's flexibility and effectiveness, with the entire pruning and post-training process completing in approximately four hours using a single RTX 3090 GPU.

## Key Results
- Achieves 1.25× inference speedup on LLaMA 7B and 1.19× on LLaMA-2 7B without specialized hardware
- Outperforms state-of-the-art pruning techniques by up to 1.49 percentage points in average task accuracy
- Reaches 30% pruning ratio with minimal impact on zero-shot task performance
- Fine-tuning recovers performance within 0.5 percentage points of unpruned baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting attention head outputs through the output projection (W_O) before computing importance scores better captures each head's contribution to the residual stream than weight-only or activation-only metrics.
- Mechanism: For each attention head n, AMP computes I_n = ||h_n · W_n||_1, where h_n is the head's output and W_n is its corresponding block in the output projection matrix. This captures both the activation magnitude and how it transforms through the final projection before joining the residual stream.
- Core assumption: The ℓ1 norm of the projected output meaningfully correlates with functional importance to downstream computation.
- Evidence anchors:
  - [abstract] "By projecting the input data onto weights, AMP assesses structural importance"
  - [section III.C] Equation 8 defines I_n = ||h_n W_n||_1, explicitly decomposing MHA output as sum of head contributions
  - [corpus] Related work on attention head importance (e.g., "Entropy Meets Importance") uses gradient-based or entropy-based scoring, not output projection, suggesting alternative importance definitions exist but may require more computation
- Break condition: If attention heads have highly correlated outputs (redundancy), removing low-norm heads may still damage ensemble behavior not captured by individual norms.

### Mechanism 2
- Claim: Pruning MLP neurons as up-gate pairs in SwiGLU architectures preserves element-wise multiplication semantics and prevents activation dimension mismatches.
- Mechanism: For each neuron pair m, compute I_m = (1/S) Σ_s |SiLU((x_s · W_Gate)_m) · (x_s · W_Up)_m|_1, measuring average activation strength before the Down projection. Prune pairs with lowest scores uniformly across layers.
- Core assumption: Low average activation magnitude indicates low functional contribution; SwiGLU's gating mechanism allows clean paired removal without architectural changes.
- Evidence anchors:
  - [abstract] "removing less critical structures within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP)"
  - [section III.B-III.C] Equation 9 formalizes paired neuron importance; Section IV.E shows pruning MLP-only performs worse (56.93 avg) than combined MHA+MLP pruning (61.02 avg) at 30% compression
  - [corpus] SparseGPT and Wanda prune individual weights without structural constraints, requiring specialized hardware for speedup
- Break condition: If certain neuron pairs have low average activation but are critical for rare but important inputs (e.g., edge cases), their removal may cause tail-performance degradation not visible in calibration data.

### Mechanism 3
- Claim: Uniform pruning ratios across layers maintain architectural regularity and enable practical inference speedups without specialized sparse matrix libraries.
- Mechanism: Apply the same compression rate c to attention heads and MLP neurons in every layer. This preserves layer-modular structure and avoids variable-width layers that introduce indexing overhead.
- Core assumption: Importance distributions are roughly similar across layers; uniform removal doesn't disproportionately damage specific functional regions.
- Evidence anchors:
  - [abstract] "achieving a 30% pruning ratio with minimal impact on zero-shot task performance"
  - [section III.C] Algorithm 1 explicitly sorts components per layer and removes c% lowest-importance items "uniformly across all layers"
  - [section IV.C] Reports 1.25× speedup on LLaMA 7B and 1.19× on LLaMA-2 7B without specialized hardware
  - [corpus] Variable-width methods like SliceGPT introduce "additional training and inference overhead" (cited within paper)
- Break condition: If later layers are systematically more or less important than early layers (e.g., for reasoning vs. pattern matching), uniform pruning may be suboptimal compared to layer-adaptive ratios.

## Foundational Learning

- Concept: **Residual Stream in Transformers**
  - Why needed here: AMP's importance metric explicitly measures contribution *to the residual stream*, not just internal activations. Understanding that each layer's output is added to a running "stream" that flows through the network is essential for interpreting why projection-based scoring matters.
  - Quick check question: In a transformer with residual connections, if layer L outputs a vector v and the residual stream before L is r, what is the stream after L?

- Concept: **SwiGLU Activation Structure**
  - Why needed here: AMP prunes MLP neurons in paired fashion because SwiGLU uses element-wise multiplication between gate and up projections. Pruning one without the other would create dimension mismatches.
  - Quick check question: In SwiGLU's formula MLP(x) = (SiLU(x·W_Gate) ⊙ (x·W_Up))·W_Down, what happens to the intermediate dimension if you remove the m-th element from both W_Gate and W_Up?

- Concept: **Structured vs. Unstructured Pruning**
  - Why needed here: The paper claims hardware-agnostic speedups specifically because it removes *entire structures* (heads, neurons) rather than individual weights. This distinction determines whether you get real latency gains or just parameter counts.
  - Quick check question: Why does removing 50% of individual weights (unstructured) not yield the same inference speedup as removing 50% of entire neurons (structured) on standard GPUs?

## Architecture Onboarding

- Component map:
  Input → [MHA Module] → Residual Add → [MLP Module] → Residual Add → Next Layer
              │                              │
              ├─ N Attention Heads           ├─ Up Projection (d → d_intermediate)
              │   Each: K,Q,V projections    ├─ Gate Projection (d → d_intermediate)
              │   → Scaled Dot-Product       └─ Down Projection (d_intermediate → d)
              └─ Output Projection (W_O)

  AMP Pruning Targets:
    • MHA: Remove low-importance heads (K,Q,V weights + corresponding W_O rows)
    • MLP: Remove low-importance up-gate neuron pairs (columns in both projections + corresponding Down rows)

- Critical path:
  1. **Calibration forward pass**: Run 50 random samples from Alpaca through the model, capturing per-head MHA outputs (h_n) and per-neuron MLP activations
  2. **Importance scoring**: Compute I_n = ||h_n · W_n||_1 for each head; compute I_m for each neuron pair using Equation 9
  3. **Sorting and selection**: Per layer, rank heads and neurons by importance; identify bottom c% for removal
  4. **Structural removal**: Delete corresponding weight rows/columns from K, Q, V, W_O (for heads) and W_Up, W_Gate, W_Down (for MLP pairs)
  5. **Post-training recovery**: Fine-tune pruned model with LoRA (rank=8, 2 epochs, Alpaca dataset)

- Design tradeoffs:
  - **Uniform vs. layer-adaptive pruning**: Uniform is simpler and avoids variable-width overhead, but may under-prune important layers and over-prune less critical ones
  - **Small calibration set (50 samples)**: Fast importance estimation but may not capture input distribution tails; paper cites prior work suggesting this is sufficient for activation statistics
  - **ℓ1 norm vs. ℓ2 or max**: ℓ1 is robust to outliers and computationally simple; paper does not ablate this choice
  - **Pruning both MHA and MLP vs. one component**: Table III shows combined pruning (61.02 avg accuracy) outperforms MLP-only (56.93) and dramatically outperforms MHA-only (37.63) at 30% compression, because MHA contains only ~33% of parameters—pruning it alone requires near-total head removal

- Failure signatures:
  - **Model collapse after pruning**: If accuracy drops far below random baseline, check that you're pruning *lowest*-importance components, not highest (coherence check in Table II shows reversed pruning yields 35.45 avg vs. 61.02 for correct direction)
  - **Perplexity explosion without fine-tuning**: At 51% pruning, no-fine-tuning PPL is 6.18× higher than with fine-tuning (Figure 1)
  - **Speedup not materializing**: Verify you're using structured removal (deleting weight dimensions) not just zeroing values; confirm batch size=1 for reported metrics
  - **Phi models degrade more than LLaMA**: Smaller models have less redundancy; expect higher sensitivity at same compression rates

- First 3 experiments:
  1. **Coherence check**: Before full deployment, reverse the importance ranking and prune the *most* important components. If performance doesn't collapse (to near-random), the importance metric is not discriminative. This validates the ranking logic with minimal compute.
  2. **Ablation on calibration set size**: Run AMP with 10, 50, and 128 calibration samples on a held-out validation split of Alpaca. Plot importance score stability (variance across runs with different random subsets) to confirm 50 samples is sufficient for your target model family.
  3. **Single-component pruning test**: Isolate MHA-only and MLP-only pruning at 20% compression. Compare against combined pruning to verify that both components contribute and that your implementation correctly handles paired neuron removal in SwiGLU. If MHA-only performs similarly to combined, your MLP pruning logic may have a bug.

## Open Questions the Paper Calls Out
None

## Limitations
- The uniform pruning ratio assumption may be suboptimal for models with heterogeneous layer importance distributions
- The 50-sample calibration set size appears arbitrary without validation that this is sufficient for diverse model families
- Speedups reported (1.25× on LLaMA 7B, 1.19× on LLaMA-2 7B) are modest compared to hardware-specific methods like SparseGPT

## Confidence
- **High confidence**: The structured pruning approach is technically sound and the reported speedups are verifiable through structured weight removal
- **Medium confidence**: The ℓ1 projection-based importance metric works empirically but lacks rigorous theoretical grounding for why it captures functional importance better than alternatives
- **Medium confidence**: The 30% pruning ratio achieving "minimal impact" is well-supported for the tested benchmarks but may not generalize to all task types or model sizes

## Next Checks
1. **Layer-adaptive pruning ablation**: Compare uniform pruning against layer-specific ratios optimized via small-scale search to quantify the cost of architectural simplicity versus potential accuracy gains
2. **Calibration set size sensitivity**: Systematically vary calibration samples (10, 50, 128, 512) and measure stability of importance rankings and final accuracy to validate the 50-sample choice
3. **Downstream task robustness**: Evaluate AMP-pruned models on code generation, mathematical reasoning, and domain-specific benchmarks to identify potential performance cliffs not captured by commonsense reasoning tasks