---
ver: rpa2
title: 'Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache
  Management'
arxiv_id: '2511.20172'
source_url: https://arxiv.org/abs/2511.20172
tags:
- memory
- kvcache
- data
- rdma
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Beluga, a CXL-based memory architecture designed
  to address memory bottlenecks in large language model (LLM) inference systems. By
  leveraging CXL 2.0 switches, Beluga enables GPUs and CPUs to directly access a shared,
  large-scale memory pool with near-local memory latency, eliminating the need for
  complex RDMA-based communication protocols.
---

# Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management

## Quick Facts
- **arXiv ID:** 2511.20172
- **Source URL:** https://arxiv.org/abs/2511.20172
- **Reference count:** 40
- **Key result:** Beluga achieves 89.6% reduction in Time-To-First-Token and 7.35× throughput improvement for LLM inference by offloading KVCache to a CXL 2.0 shared memory pool.

## Executive Summary
Beluga introduces a CXL 2.0-based memory architecture that addresses memory bottlenecks in large language model inference systems. By leveraging CXL switches, it enables GPUs and CPUs to directly access a shared, large-scale memory pool with near-local memory latency, eliminating the need for complex RDMA-based communication protocols. The system provides native load/store access semantics over the CXL fabric, simplifying programming and minimizing synchronization overhead. Beluga-KVCache, built on this architecture, demonstrates significant performance improvements in LLM inference when integrated with the vLLM engine.

## Method Summary
Beluga replaces traditional RDMA-based KVCache offloading with a CXL 2.0 shared memory pool accessed via CXL switches and adapters. The architecture exposes CXL memory in Direct Access (DAX) mode, mapped into user space via mmap. For coherence in the multi-host environment lacking hardware cache coherence, Beluga employs software-managed cache flushing using non-temporal stores (ntstore) for writes and CLFLUSH instructions for reads. Custom CUDA kernels handle non-contiguous GPU-to-CXL transfers to avoid launch latency, and DDIO is disabled to prevent LLC pollution. The system is evaluated using the LV-Eval dataset with Qwen-32B and Llama-3.1-8B models, measuring TTFT and QPS improvements compared to RDMA-based solutions.

## Key Results
- Achieves 89.6% reduction in Time-To-First-Token (TTFT) for LLM inference
- Demonstrates 7.35× throughput improvement in vLLM inference engine
- Provides near-local memory latency for GPU-to-CXL memory transfers (11.73µs vs 10.32µs for CPU-to-GPU)

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Native load/store access via CXL can reduce synchronization overhead and data path latency compared to RDMA-based offloading for KVCache.
**Mechanism:** Beluga replaces the multi-hop RDMA path (GPU → Host "Bounce Buffer" → NIC → Network) with direct peer-to-peer transfers over a CXL fabric. By treating disaggregated memory as a directly addressable memory region (CXL.mem), the system eliminates the need for NIC polling, queue pair management, and CPU-driven data copies.
**Core assumption:** The overhead of the CXL protocol stack and switch traversal is lower than the combined overhead of the kernel/OS network stack and RDMA polling mechanisms.
**Evidence anchors:** [abstract] "...eliminating the need for complex RDMA-based communication protocols... minimizing synchronization overhead." [section 5.2] "For GPU access, direct CXL-to-GPU data transfers are highly competitive with traditional CPU-to-GPU paths... latency of a CXL-to-GPU copy is 11.73µs, which is remarkably close to the 10.32µs latency of a conventional CPU-to-GPU copy."
**Break condition:** If the CXL switch fabric becomes saturated or the Root Complex becomes a bottleneck, latency will spike, potentially negating the advantage over RDMA's parallel network paths.

### Mechanism 2
**Claim:** Software-managed cache flushing can enforce coherency in a multi-host CXL 2.0 environment that lacks hardware cache coherence.
**Mechanism:** Since CXL 2.0 does not support hardware coherency across multiple host CPUs, Beluga employs specific instruction sequences: writers use non-temporal stores (ntstore) to bypass the CPU cache, and readers execute CLFLUSH/CLFLUSHOPT before reading to invalidate stale cache lines.
**Core assumption:** The performance penalty of explicit cache flushing instructions is lower than the latency penalty of configuring the entire region as uncacheable (UC), and the application can tolerate coarse-grained synchronization points.
**Evidence anchors:** [section 5.1] "Optimal performance for data sharing is achieved by applying specific cache management strategies... for CPU store/load, non-temporal stores should be used for writes, and a CLFLUSH should precede loads." [table 4] Shows "Bypassing-Cache Write" (2.41 µs) outperforming "Static uncacheable" (281.56 µs).
**Break condition:** If the workload requires extremely fine-grained, random access sharing (e.g., concurrent reads/writes to the same 64B line at microsecond intervals), the software flushing overhead will likely dominate and degrade performance.

### Mechanism 3
**Claim:** A unified, low-latency memory pool allows the LLM scheduler to ignore KVCache locality.
**Mechanism:** By providing uniform access latency to the memory pool from any GPU node, Beluga removes the need for "cache-aware" scheduling. The scheduler can balance load purely based on compute availability (e.g., "cache-oblivious scheduling") rather than routing requests to specific nodes where KVCache blocks happen to reside.
**Core assumption:** The aggregate bandwidth of the CXL pool scales sufficiently to support concurrent access from all hosts without contention becoming the primary bottleneck.
**Evidence anchors:** [section 6.3] "Beluga largely eliminates the above constraints... enabling cache-oblivious scheduling. The incoming requests can be easily distributed using standard load-balancing techniques."
**Break condition:** If the "Root Complex" or switch uplink bottlenecks are not mitigated, access latency will vary non-uniformly, forcing a return to locality-aware scheduling to minimize cross-node traffic.

## Foundational Learning

**Concept: KVCache (Key-Value Cache)**
- **Why needed here:** The entire architecture is built around storing, retrieving, and sharing this specific LLM artifact. Without understanding that KVCache grows linearly with context length and is reused during the "decode" phase, the value of a massive shared memory pool is unclear.
- **Quick check question:** Does increasing the batch size or the context length increase the memory pressure on the KVCache? (Answer: Yes, primarily context length).

**Concept: CXL (Compute Express Link) Modes**
- **Why needed here:** The paper relies on `CXL.mem` for access to the memory pool. Distinguishing this from `CXL.cache` or `CXL.io` is critical to understanding why hardware coherency is missing in CXL 2.0 and why software must manage consistency.
- **Quick check question:** Does `CXL.mem` allow the CPU to treat remote DRAM as part of its physical address space? (Answer: Yes).

**Concept: Store Buffers & Write-Back Caches**
- **Why needed here:** Section 5.1 assumes the reader understands CPU store buffers and write-back policies to grasp why `ntstore` (write-combining/bypassing) and `CLFLUSH` (write-back + invalidate) are necessary for cross-node visibility.
- **Quick check question:** If CPU A writes to a cache line and CPU B reads that line from CXL memory without a flush, what does CPU B see? (Answer: Stale data, because CPU A's write may be sitting in its local L1/L2 cache).

## Architecture Onboarding

**Component map:** GPU Nodes (8x H20 GPUs) -> PCIe Switch -> Host CPUs (2x Intel Xeon) -> PCIe/CXL Adapters -> CXL Switch (XConn XC50256) -> CXL Memory Box (8TB DDR5 pool)

**Critical path:** Request Ingress (Scheduler receives request) → Data Load (GPU initiates cudaMemcpy or custom kernel load from CXL Memory Pool address) → Compute (GPU executes inference using retrieved KVCache)

**Design tradeoffs:**
- **Latency vs. Coherency:** Using ntstore reduces latency but requires careful application-level synchronization
- **Capacity vs. Bandwidth:** The CXL pool offers massive capacity (8TB), but bandwidth is limited by the Root Complex (33-46 GB/s per node) unless interleaved across multiple adapters
- **Cost:** CXL adapters ($210) are cheaper than RDMA NICs ($1745), but require new switch infrastructure

**Failure signatures:**
- **Stale Reads:** Output tokens are incoherent or hallucinated; likely missed a CLFLUSH or mfence before reading shared metadata
- **Bandwidth Saturation:** Throughput plateaus despite low GPU utilization; check if traffic is funneled through a single PCIe/CXL adapter
- **High Tail Latency:** P99 latency spikes; likely due to "hot spots" on a single CXL memory device requiring data interleaving

**First 3 experiments:**
1. **Baseline Latency Check:** Run a simple ntstore vs. standard store microbenchmark to the CXL memory region to verify the latency gap (target ~2.4µs vs >200µs)
2. **Coherency Test:** Have Node A write a pattern to CXL memory and Node B read it. Verify B sees the data only after Node A issues a flush and Node B invalidates its cache
3. **End-to-End vLLM Integration:** Run the vLLM engine with prefix_caching enabled. Compare TTFT between a "Cache Populate" run and a "Cache Hit" run to validate the claimed ~90% reduction

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** What are the latency and bandwidth impacts of a fully disaggregated architecture where GPUs connect directly to a CXL switch, bypassing the host Root Complex (RC)?
- **Basis in paper:** [explicit] Section 8 (Future Work) proposes a future architecture (Figure 16) with a symmetric CXL fabric connecting compute and memory pools to eliminate the RC bottleneck identified in Section 5.3
- **Why unresolved:** The current Beluga prototype routes GPU traffic through the CPU's Root Complex, which limits bandwidth to 33 GB/s for writes and creates a performance bottleneck. The direct-attach model is envisioned but not yet evaluated
- **What evidence would resolve it:** An empirical comparison of throughput and latency between the current Beluga architecture and a prototype utilizing direct GPU-to-CXL switch connectivity

**Open Question 2**
- **Question:** How can software-managed coherence protocols be optimized to reduce the overhead of explicit cache flushing (e.g., CLFLUSH) in the absence of hardware cross-host coherence in CXL 2.0?
- **Basis in paper:** [explicit] Section 5.1 and Section 8 identify the lack of host-to-host hardware cache coherence in CXL 2.0 as a limitation requiring inefficient software synchronization
- **Why unresolved:** Beluga currently relies on software mechanisms like non-temporal stores and cache line invalidation to ensure consistency, which imposes management overhead. The authors call for research into scalable, directory-based coherence or hybrid models
- **What evidence would resolve it:** A software coherence protocol design that reduces synchronization latency compared to Beluga's current CLFLUSH/ntstore approach, or an evaluation of CXL 3.0's hardware coherence features on this workload

**Open Question 3**
- **Question:** How can reliability guarantees be strengthened for CXL-based RPCs to match RDMA protocols without reintroducing significant latency overhead?
- **Basis in paper:** [explicit] Section 6.2 notes that Beluga's CXL-RPC offers lower reliability guarantees than RDMA transport protocols, and Section 8 lists strengthening these guarantees as a topic for future research
- **Why unresolved:** The current design prioritizes performance and cost-efficiency by relying on upper-layer mechanisms for reliability, potentially exposing the system to data integrity issues during fabric errors
- **What evidence would resolve it:** A CXL-RPC implementation incorporating link-level retry or alternative reliability mechanisms, benchmarked to show the trade-off between error recovery overhead and the low-latency benefits demonstrated in Beluga

**Open Question 4**
- **Question:** Can memory-intensive database workloads, such as graph algorithms (e.g., HNSW) or vector databases, effectively leverage the CXL memory pool to overcome local DRAM capacity limits?
- **Basis in paper:** [explicit] Section 8 (Future Work on database design) suggests that CXL pooling enables new opportunities for systems requiring large-scale random access, specifically citing graph and vector databases
- **Why unresolved:** Beluga is tailored specifically for LLM KVCache management. The efficiency of the CXL fabric for the distinct access patterns (e.g., random traversals) of graph algorithms remains uncharacterized
- **What evidence would resolve it:** An implementation and evaluation of a graph or vector database on the Beluga architecture, demonstrating performance and scalability relative to local DRAM or RDMA-based disaggregation

## Limitations
- Commercial availability of XConn XC50256 CXL switch is uncertain, with only "B1 sample price" mentioned
- Software coherency solution using CLFLUSH/ntstore is acknowledged as a workaround for CXL 2.0's lack of hardware coherency, raising scalability concerns
- Evaluation focuses primarily on TTFT and throughput, lacking comprehensive analysis of tail latency, cost-benefit trade-offs, and scaling behavior beyond 8-GPU configuration

## Confidence
- **High Confidence:** The architectural premise that CXL 2.0 can provide near-local memory latency for GPU-to-memory transfers is well-supported by microbenchmark data (Section 5.2 showing 11.73µs vs 10.32µs for CPU-to-GPU transfers)
- **Medium Confidence:** The effectiveness of the software coherency solution using CLFLUSH and non-temporal stores is demonstrated through controlled microbenchmarks (Table 4 showing 2.41µs vs 281.56µs for cache-bypassing writes)
- **Low Confidence:** The scalability claims and ability to eliminate locality-aware scheduling entirely are primarily theoretical extrapolations

## Next Checks
1. **Scale-up bottleneck test:** Replicate the Root Complex bottleneck scenario by configuring a single PCIe/CXL adapter and measuring bandwidth degradation under increasing concurrent GPU loads. Then implement and validate the interleaving solution across multiple adapters to confirm mitigation effectiveness.

2. **Fine-grained coherency stress test:** Design a microbenchmark that simulates highly concurrent, random-access sharing patterns between multiple CPUs writing to and reading from the same CXL memory lines at microsecond intervals. Measure the cumulative overhead of CLFLUSH operations and compare against alternative solutions.

3. **Cost-performance trade-off analysis:** Build a comparative evaluation framework that includes Beluga-KVCache, traditional RDMA-based offloading, and local GPU memory solutions across multiple metrics: TTFT, throughput, tail latency (p99), and total cost of ownership. This would validate whether the claimed performance improvements justify the specialized hardware requirements.