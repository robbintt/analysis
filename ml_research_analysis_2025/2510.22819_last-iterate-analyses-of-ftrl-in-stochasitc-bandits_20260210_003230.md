---
ver: rpa2
title: Last Iterate Analyses of FTRL in Stochasitc Bandits
arxiv_id: '2510.22819'
source_url: https://arxiv.org/abs/2510.22819
tags:
- lemma
- have
- then
- proof
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the last-iterate convergence of Follow-the-Regularized-Leader
  (FTRL) algorithms in stochastic multi-armed bandits. While FTRL algorithms like
  1/2-Tsallis-INF achieve optimal regret bounds in both stochastic and adversarial
  settings, their last-iterate (simple regret) convergence rates have not been analyzed.
---

# Last Iterate Analyses of FTRL in Stochasitc Bandits

## Quick Facts
- arXiv ID: 2510.22819
- Source URL: https://arxiv.org/abs/2510.22819
- Reference count: 40
- Primary result: Proves O(t^{-1/2}) Bregman divergence convergence for 1/2-Tsallis-INF in stochastic bandits

## Executive Summary
This paper provides the first last-iterate convergence analysis for Follow-the-Regularized-Leader (FTRL) algorithms in stochastic multi-armed bandits. The authors focus on the 1/2-Tsallis-INF algorithm and prove that its probability distribution converges to the optimal arm at rate O(t^{-1/2}) in terms of Bregman divergence. This is achieved through a novel decomposition technique that enables tracking the evolution of the probability distribution at each round, rather than relying solely on cumulative regret bounds. The analysis critically depends on the learning rate parameter being less than 1 and introduces new technical tools including a continuity property of iterates and self-bounding techniques.

## Method Summary
The paper analyzes the 1/2-Tsallis-INF algorithm, which is an instance of FTRL with Tsallis entropy regularizer Ψ(p) = -4∑√p_i. The algorithm maintains a probability distribution p_t over arms and updates it using cumulative estimated losses. The key technical contribution is a new decomposition of the regret analysis framework that tracks the evolution of p_t directly. The authors prove that the Bregman divergence between p_t and the point mass on the optimal arm decays at rate O(t^{-1/2}). This requires showing a continuity property of iterates and controlling the second moment of the Bregman divergence, with the analysis critically relying on the learning rate parameter α being less than 1.

## Key Results
- Proves O(t^{-1/2}) convergence rate for Bregman divergence in last-iterate analysis of FTRL in stochastic bandits
- Introduces novel decomposition technique for analyzing FTRL's last-iterate behavior
- Shows second moment of Bregman divergence grows linearly with t when α < 1
- Establishes that the algorithm's probability distribution p_t converges to the point mass on the optimal arm e_{i*} at rate O(t^{-1/2})
- Provides first last-iterate convergence result for FTRL in stochastic bandits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm's probability distribution p_t converges to the point mass on the optimal arm e_{i*} at a rate of O(t^{-1/2}).
- **Mechanism:** A novel decomposition of the inner product term ⟨p_t - e_{i*}, ℓ̂_t⟩ splits the dynamics into a stability term, a penalty term, and a critical extra term (III). This decomposition allows the analysis to track the change in Bregman divergence directly, rather than relying solely on cumulative regret bounds.
- **Core assumption:** The analysis assumes the standard stochastic bandit setting where losses are i.i.d. and the optimal arm i* is unique.
- **Evidence anchors:**
  - [abstract] "The paper introduces a new decomposition technique for analyzing FTRL's last-iterate behavior..."
  - [section 3.2] Lemma 3.3 explicitly defines the decomposition with the extra term III: ((1/η_{t+1}) - (1/η_t)) ⟨p_{t+1} - e_{i*}, η_t L̂_t⟩.
  - [corpus] Contextualizes this as a specific adaptation of FTRL analysis, distinct from game-theoretic settings (e.g., Cai et al., 2024) which may require different reduction techniques.
- **Break condition:** If the unique optimal arm assumption is violated, or if the "extra term" cannot be bounded by the self-bounding technique, the proof chain fails.

### Mechanism 2
- **Claim:** The probability mass on suboptimal arms does not increase erratically between steps, ensuring stability.
- **Mechanism:** A continuity property (Lemma 3.4) bounds the ratio of subsequent probabilities (p_{t+1,i} ≤ 7d p_{t,i} + 1/t). This prevents the "extra term" in the decomposition from exploding, allowing the self-bounding technique to control the variance even when the estimated best arm fluctuates.
- **Core assumption:** The bound relies on the structure of the regularizer Ψ(p) = -4∑√p_i and the learning rate η_t = α/√t.
- **Evidence anchors:**
  - [section 3.1] Mentions "a continuity property of iterates" as a key technical contribution.
  - [section 4.1] Explicitly uses Lemma 3.4 to bound the extra term III before applying self-bounding techniques.
  - [corpus] "On Separation Between Best-Iterate... and Last-Iterate Convergence" suggests stability is a known friction point in convergence classes, highlighting the necessity of this continuity check.
- **Break condition:** If the learning rate α is set such that the regularizer's curvature doesn't dampen the update sufficiently, the continuity bound may not hold, leading to unstable iterates.

### Mechanism 3
- **Claim:** The convergence rate is validated by controlling the second moment of the Bregman divergence.
- **Mechanism:** The proof establishes a uniform bound on the second moment E[D²_Ψ(e_{i*}, p_t)] (Lemma 3.5). This is achieved by decomposing the estimated loss of the optimal arm into a martingale-like term U_t and a regret term, showing that U_t grows linearly but is controlled by the condition α < 1.
- **Core assumption:** The condition 0 < α < 1 is required to prevent the second moment order from blowing up in the iterative relation for U_t.
- **Evidence anchors:**
  - [section 3.3] Lemma 3.5 states the uniform bound E[D²_Ψ(e_{i*}, p_t)] ≤ (Cde^{α²})/(1-α²).
  - [section 4.2] Sketches the decomposition of L̂_{t,i*} and the iterative inequality (Eq. 8) that necessitates α < 1.
  - [corpus] "Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods" parallels this focus on second moments and noise variance in SGD last-iterate analysis.
- **Break condition:** If α ≥ 1, the growth of the second moment of the error term U_t could exceed linear growth, breaking the uniform bound and invalidating the convergence rate proof.

## Foundational Learning

- **Concept: Bregman Divergence**
  - **Why needed here:** This is the specific metric used to measure the "distance" between the algorithm's sampling distribution p_t and the optimal distribution e_{i*}. The paper proves this decays as t^{-1/2}.
  - **Quick check question:** Can you explain why the authors use Bregman divergence defined by the Tsallis entropy regularizer rather than standard Euclidean distance to measure convergence?

- **Concept: Self-Bounding Technique**
  - **Why needed here:** This technique allows the algorithm to use its own regret bounds to control the variance of the updates, essential for bounding the stability and extra terms in the decomposition without external constraints.
  - **Quick check question:** How does the "Self-bounding technique" differ from standard concentration inequalities in bounding the stability term?

- **Concept: Follow-the-Regularized-Leader (FTRL)**
  - **Why needed here:** 1/2-Tsallis-INF is an instance of FTRL. Understanding that p_t is the result of an argmin optimization involving cumulative estimated losses and a regularizer is foundational.
  - **Quick check question:** What is the role of the regularizer Ψ(p) = -4∑√p_i in preventing the probability p_t from collapsing to a suboptimal arm too quickly?

## Architecture Onboarding

- **Component map:** Stochastic Loss Vector ℓ_t → Cumulative Estimated Loss L̂_t → Sampling Distribution p_t → Importance-weighted Estimator ℓ̂_{t,i} → Updated L̂_{t+1}
- **Critical path:** The update rule (Eq. 2 in paper) determines p_t. The "continuity property" ensures p_t evolves smoothly. The convergence relies on the interplay between the decaying learning rate η_t ~ 1/√t and the regularizer.
- **Design tradeoffs:** The paper proves a O(t^{-1/2}) rate for Bregman divergence but suspects the true rate for simple regret is O(t^{-1}). Implementing this requires setting α ∈ (0,1). A larger α (approaching 1) increases the constant factor in the error bound significantly (denominator 1-α²), while too small α might slow convergence.
- **Failure signatures:**
  - **Exploding Variance:** If α ≥ 1, the second moment bound (Lemma 3.5) is theoretically at risk of failing, potentially leading to unstable last-iterate behavior.
  - **Non-unique Optima:** If multiple arms share the optimal expected loss, the Bregman divergence definition and the decomposition (which targets a specific e_{i*}) lose validity.
- **First 3 experiments:**
  1. **Verify Continuity:** Plot max_i (p_{t+1,i}/p_{t,i}) over time to empirically validate Lemma 3.4 (≤ 7d) under different stochastic settings.
  2. **Rate Scaling:** Log-log plot of D_Ψ(e_{i*}, p_t) vs. t. Check if the slope is -1/2 (proven) or closer to -1 (conjectured) for large t.
  3. **Sensitivity to α:** Run ablations on α (e.g., 0.2, 0.5, 0.9) to observe the impact on the constant factors of the last-iterate error, confirming the theoretical sensitivity to 1/(1-α²).

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question:** Does the last-iterate convergence rate of 1/2-Tsallis-INF scale as O(t^{-1}) in stochastic bandits?
  - **Basis in paper:** [explicit] The authors pose this as the central question in the introduction and state in Section 5: "An important open question for future work is how to rigorously establish that the last-iterate convergence rate is indeed O(t^{-1})."
  - **Why unresolved:** The paper only proves a O(t^{-1/2}) rate for the Bregman divergence, which implies at least O(t^{-1/2}) last-iterate convergence but not the conjectured O(t^{-1}) rate.
  - **What evidence would resolve it:** A proof showing E[p_{t,i}] = O(t^{-1}) for all suboptimal arms i ≠ i*, or a lower bound construction showing O(t^{-1/2}) is tight.

- **Open Question 2**
  - **Question:** Does the second moment of the Bregman divergence, E[D²_Ψ(e_{i*}, p_t)], converge at rate O(t^{-1})?
  - **Basis in paper:** [explicit] Section 5 states: "We conjecture that this could be achieved by proving that the second moment of the Bregman divergence converges at a rate of O(t^{-1})."
  - **Why unresolved:** The paper only establishes a uniform bound on the second moment (Lemma 3.5), not its convergence rate.
  - **What evidence would resolve it:** A proof showing E[D²_Ψ(e_{i*}, p_t)] ≤ C/t for some constant C and all t ≥ 1.

- **Open Question 3**
  - **Question:** Does setting the learning rate parameter α ≥ 1 break the last-iterate convergence guarantee?
  - **Basis in paper:** [explicit] Section 5 notes the analysis "critically relies on the assumption α < 1" and asks: "Whether an excessively large α can break the last-iterate convergence rate remains an intriguing direction for further investigation."
  - **Why unresolved:** When α > 1, the recursion in Eq. (8) causes E[(U_t)²] to "blow up," but this doesn't establish whether last-iterate convergence actually fails.
  - **What evidence would resolve it:** Either a modified analysis showing convergence for α ≥ 1, or a counterexample demonstrating non-convergence.

- **Open Question 4**
  - **Question:** Why does the estimated regret Ŕ⁺_t exhibit stronger concentration (E[(Ŕ⁺_t)²] = O(t)) than the actual regret R⁺_n in adversarial settings (E[(R⁺_n)²] = Ω(n²))?
  - **Basis in paper:** [explicit] Remark after Lemma 3.6: "This result also holds in the adversarial setting and is very interesting because Pogodin and Lattimore [2020] showed that in the adversarial setting... a phenomenon that deserves further investigation."
  - **Why unresolved:** The paper establishes the bound but does not explain the underlying mechanism for this concentration gap between estimated and actual regret.
  - **What evidence would resolve it:** A theoretical characterization of when and why estimated quantities concentrate better than their realized counterparts in FTRL algorithms.

## Limitations
- The analysis critically depends on the assumption of a unique optimal arm, which may not hold in all practical scenarios
- The convergence rate of O(t^{-1/2}) for Bregman divergence may not be tight, with authors conjecturing the optimal simple regret rate is O(t^{-1})
- The restriction α < 1 is necessary for the proof but may not be fundamental to the algorithm's performance
- No experimental results are provided to validate the theoretical findings empirically

## Confidence
- **High** in the theoretical framework and proof structure. The paper establishes a novel decomposition technique and proves a O(t^{-1/2}) decay rate for Bregman divergence, which represents a significant advance in understanding last-iterate behavior of FTRL algorithms in stochastic bandits.
- **Medium** in the tightness of the convergence rate. While the paper proves O(t^{-1/2}) for Bregman divergence, the authors conjecture the optimal simple regret rate is O(t^{-1}), suggesting the current analysis may not be tight.
- **Low** regarding empirical validation. As a theoretical paper, no experimental results are provided to verify the convergence rates or sensitivity to parameters in practice.

## Next Checks
1. **Empirical Rate Verification**: Implement the 1/2-Tsallis-INF algorithm and run experiments on synthetic stochastic bandit problems to empirically verify the O(t^{-1/2}) decay rate for Bregman divergence through log-log plots of D_Ψ(e_{i*}, p_t) vs. t.

2. **Parameter Sensitivity Analysis**: Conduct ablation studies varying the learning rate parameter α across the range (0,1) to quantify the impact on convergence constants and verify the theoretical sensitivity to the 1/(1-α²) term in the error bound.

3. **Robustness Testing**: Test the algorithm's behavior under violations of the unique optimal arm assumption by constructing problems with multiple optimal arms to understand how the convergence analysis breaks down when the core assumptions are not met.