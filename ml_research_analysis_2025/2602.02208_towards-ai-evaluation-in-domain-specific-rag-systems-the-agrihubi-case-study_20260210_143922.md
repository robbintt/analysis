---
ver: rpa2
title: 'Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study'
arxiv_id: '2602.02208'
source_url: https://arxiv.org/abs/2602.02208
tags:
- system
- agrihubi
- language
- agricultural
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgriHubi is a domain-specific RAG system for Finnish-language agricultural
  decision support. It integrates Finnish agricultural documents with open PORO family
  LLMs, using retrieval-augmented generation with explicit source grounding and user
  feedback.
---

# Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study

## Quick Facts
- arXiv ID: 2602.02208
- Source URL: https://arxiv.org/abs/2602.02208
- Reference count: 16
- Primary result: Domain-specific RAG system for Finnish agricultural decision support with iterative user feedback improves answer quality from 46% low ratings to 38% low ratings and top ratings from 3% to 21%.

## Executive Summary
AgriHubi is a domain-specific RAG system for Finnish-language agricultural decision support. It integrates Finnish agricultural documents with open PORO family LLMs, using retrieval-augmented generation with explicit source grounding and user feedback. Developed through eight iterative design cycles and evaluated in two user studies, the system shows significant improvements in answer completeness, linguistic accuracy, and perceived reliability. Larger models improved accuracy but introduced latency trade-offs. The results highlight the importance of retrieval quality, iterative refinement, and human-centered evaluation in building effective domain-specific RAG systems.

## Method Summary
The system uses a standard RAG pipeline: PDF documents are processed through OCR (tesseract fin+eng), chunked, and embedded with OpenAI's text-embedding-ada-002. FAISS index stores embeddings with L2 normalization for retrieval. User queries are embedded and compared to document chunks using distance thresholds to retrieve relevant context. This context is combined with the query in prompts for PORO models (PORO-2-70B, PORO-2-8B, PORO-34B) accessed via GPT-Lab/Farmi APIs. The Streamlit interface streams responses and collects 5-point Likert ratings with optional textual feedback, logged to SQLite. Eight iterative development cycles (Jan-Aug 2025) incorporated user feedback to refine retrieval thresholds, prompt templates, and model selection.

## Key Results
- Low ratings (1-2) decreased from 46% to 38% across evaluation rounds
- Top ratings (5) increased from 3% to 21% with iterative improvements
- PORO-2-70B produced most accurate answers but slower responses reduced perceived trust
- Retrieval quality improvements had greater impact on answer quality than model scaling alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-quality improvements contributed more to answer quality gains than model scaling alone.
- Mechanism: Structured preprocessing (OCR fallback, coherent chunking, metadata tagging) and FAISS-based L2-normalized retrieval surface higher-quality context windows, enabling models to ground responses in domain-relevant passages rather than relying on parametric knowledge.
- Core assumption: Retrieved documents contain accurate, domain-appropriate information; chunking preserves semantic coherence.
- Evidence anchors:
  - [abstract]: "results highlight the importance of retrieval quality, iterative refinement"
  - [section VI]: "Most technical improvements resulted from better retrieval and preprocessing... Improvements in retrieval quality, preprocessing, and context construction had a greater impact on answer quality than switching to larger models"
  - [corpus]: KinyaColBERT (arXiv:2507.03241) reports similar findings—lexically grounded retrieval improves low-resource RAG, though domain differs (Kinyarwanda).
- Break condition: If OCR noise remains high, chunking breaks semantic units, or distance thresholds exclude relevant passages, retrieval quality degrades and model scaling yields diminishing returns.

### Mechanism 2
- Claim: Iterative user-feedback loops improved perceived reliability and completeness over time.
- Mechanism: A five-point Likert rating with optional textual feedback is stored per interaction in SQLite. Aggregated ratings identify systemic weaknesses (missing details, terminology errors), triggering targeted changes in retrieval thresholds, prompt templates, and model selection across eight iterations.
- Core assumption: Users provide consistent, representative feedback; low ratings correlate with objective quality issues.
- Evidence anchors:
  - [abstract]: "Low ratings decreased from 46% to 38%, while top ratings increased from 3% to 21%"
  - [section V-B]: "Compared to Round 1, the share of low ratings (1–2) dropped from 46% to 38%, while the proportion of top-rated answers (score 5) increased from 3% to 21%"
  - [corpus]: Corpus evidence on iterative feedback loops in agricultural RAG is weak; neighbor papers focus on architecture, not feedback-driven iteration.
- Break condition: If feedback volume is too low, biased, or inconsistent, signal-to-noise degrades and iterations may address spurious issues.

### Mechanism 3
- Claim: Model-size selection creates a quality–latency trade-off that affects perceived trustworthiness.
- Mechanism: PORO-2-70B produces longer, more complete, and better-grounded answers but incurs higher latency and memory use. Slower responses are perceived as less reliable, independent of answer quality.
- Core assumption: Latency negatively affects user trust; users notice and care about response time.
- Evidence anchors:
  - [abstract]: "Larger models improved accuracy but introduced latency trade-offs"
  - [section VI-A]: "PORO-2-70B generated the most accurate and detailed answers but responded more slowly... Testers often associated slower responses with lower trust"
  - [corpus]: Corpus does not provide direct evidence on latency–trust coupling in agricultural RAG; this appears domain-general.
- Break condition: If latency is masked (e.g., streaming UI) or users prioritize accuracy over speed, the trade-off may not manifest as trust loss.

## Foundational Learning
- Concept: RAG pipeline structure (retrieval → context → generation)
  - Why needed here: AgriHubi's core architecture depends on understanding how retrieved passages are embedded, indexed, and fed to the model as context.
  - Quick check question: Can you explain how a user query is transformed into an embedding, compared to document chunks, and combined into a context window for generation?
- Concept: Vector similarity search and FAISS indexing
  - Why needed here: The system uses L2-normalized vectors and FAISS for semantic retrieval; understanding indexing, normalization, and distance thresholds is essential for tuning retrieval quality.
  - Quick check question: What is the effect of L2 normalization on cosine similarity, and how does the distance threshold in `search_similar_chunks` affect precision vs. recall?
- Concept: Iterative human-centered evaluation
  - Why needed here: AgriHubi's eight-iteration development was driven by user ratings and qualitative feedback; understanding feedback-driven iteration is critical for reproducibility.
  - Quick check question: How would you design a feedback loop that distinguishes retrieval failures from generation failures using only user ratings and stored context?

## Architecture Onboarding
- Component map:
  - **Data Ingestion**: `pdf_to_txt.py` (PDF extraction + OCR) → `txt_to_embedding.py` (chunking + OpenAI embeddings) → FAISS index stored in `/dataembedding/`
  - **Retrieval**: `RAGEngine` loads FAISS index, embeds query, retrieves top-k chunks via L2 distance
  - **Generation**: `LanguageHandler` builds prompt with context; model called via Farmi/GPT-Lab APIs (PORO-34B, PORO-2-8B, PORO-2-70B)
  - **UI/Feedback**: Streamlit interface streams responses, collects 5-point ratings, exports PDFs; `DatabaseManager` logs interactions to SQLite
- Critical path:
  1. User submits query in Streamlit UI
  2. Query embedded via OpenAI `text-embedding-ada-002`
  3. FAISS search retrieves top-k chunks (default k=5, distance threshold=2.0)
  4. `LanguageHandler` constructs prompt with context + query
  5. Model generates streamed response
  6. Response displayed, user rates (1–5), feedback logged to SQLite
- Design tradeoffs:
  - **Model size vs. latency**: PORO-2-70B improves accuracy but increases latency; may reduce perceived trust if responses are slow.
  - **Chunk size vs. context coherence**: Larger chunks preserve more context but increase retrieval noise; smaller chunks improve precision but may fragment meaning.
  - **Distance threshold vs. recall**: Lower threshold improves precision but may exclude relevant passages; higher threshold improves recall but introduces noise.
- Failure signatures:
  - **Incomplete answers**: Often tied to retrieval missing key passages or context window too short (resolved by increasing max tokens from 700 to 2000).
  - **Terminology inconsistencies**: Model not grounding in retrieved domain terms; improved by refining Finnish agricultural vocabulary in prompts.
  - **Slow responses**: Large model + shared GPU load; may be misattributed by users as low answer quality.
  - **Hallucinations**: Retrieval returns irrelevant chunks; model generates from parametric knowledge instead of grounded context.
- First 3 experiments:
  1. **Ablate retrieval quality**: Manually degrade chunking (e.g., random chunk boundaries) and measure change in Likert ratings vs. baseline to quantify retrieval contribution.
  2. **Model–latency trade-off**: Run PORO-2-8B and PORO-2-70B on identical query sets, measure response latency and rating distribution; test if streaming UI mitigates trust loss from latency.
  3. **Threshold sensitivity**: Sweep distance threshold (e.g., 1.0, 2.0, 3.0) and measure precision/recall using held-out QA pairs with known relevant passages; correlate with user ratings.

## Open Questions the Paper Calls Out
- How can domain-specific RAG systems achieve balanced multilingual support when secondary languages (e.g., Swedish) have sparse training data and inconsistent document coverage?
- What standardized evaluation methodologies enable systematic comparison across iterative RAG development cycles?
- How should RAG systems handle uncertainty and conflicting information when multiple data sources (documents, sensors, APIs) provide contradictory signals?

## Limitations
- Specific Finnish agricultural document corpus is not disclosed, preventing independent validation
- Exact prompt templates are not provided, making generation context replication impossible
- PORO model API access appears institution-specific with unclear public availability
- Feedback-driven iteration improvements lack fine-grained diagnostic data to isolate confounding factors

## Confidence
- **High Confidence**: The overall RAG pipeline structure and iterative evaluation methodology are clearly specified. The documented user study results (Likert score distributions before/after) are reproducible given access to the corpus and API endpoints.
- **Medium Confidence**: The relative contributions of retrieval quality vs. model scaling to answer quality improvements are plausible but not definitively isolated. The claim about latency–trust coupling is inferred from qualitative user observations, not rigorously measured.
- **Low Confidence**: The generalizability of the feedback-driven iteration model is weak; corpus evidence on agricultural RAG systems using iterative user feedback loops is sparse. The precise mechanisms by which user ratings map to actionable model/pipeline changes are underspecified.

## Next Checks
1. **Corpus Availability and Preprocessing Verification**: Obtain the Finnish agricultural PDF corpus (or a comparable dataset) and run the exact preprocessing pipeline (`pdf_to_txt.py` + OCR, chunking, embedding). Validate that the FAISS index can be reconstructed and that retrieval quality matches the reported performance.
2. **Latency–Trust Experiment**: Run controlled experiments comparing PORO-2-8B and PORO-2-70B on identical query sets, logging response latency and 5-point ratings. Use streaming UI vs. non-streaming UI to test if latency transparency affects user trust ratings independently of answer quality.
3. **Feedback Loop Granularity Analysis**: If user feedback data is accessible, perform a granular analysis mapping low ratings to specific failure modes (retrieval failures vs. generation failures) and verify whether the documented system changes align with the most frequent issues identified.