---
ver: rpa2
title: Interpretable phenotyping of Heart Failure patients with Dutch discharge letters
arxiv_id: '2505.24619'
source_url: https://arxiv.org/abs/2505.24619
tags:
- uni0000000d
- uni00000013
- uni00000011
- uni00000055
- failure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a classification model for phenotyping Heart
  Failure (HF) patients into Left Ventricular Ejection Fraction (LVEF) classes using
  Dutch discharge letters. The model leverages structured and unstructured data from
  33,105 hospitalizations, with silver labels derived from diagnosis codes, echocardiography
  results, and textual mentions.
---

# Interpretable phenotyping of Heart Failure patients with Dutch discharge letters

## Quick Facts
- arXiv ID: 2505.24619
- Source URL: https://arxiv.org/abs/2505.24619
- Reference count: 40
- Primary result: Aug-Linear models achieved AUC 0.81 on external validation while providing more interpretable explanations than black-box models

## Executive Summary
This study develops and validates a classification approach for phenotyping Heart Failure (HF) patients into Left Ventricular Ejection Fraction (LVEF) classes using Dutch discharge letters. The authors introduce "Aug-Linear" models that combine BERT embeddings with interpretable linear models, comparing them against standard transformer-based approaches. Their methodology leverages silver labels derived from diagnosis codes, echocardiography results, and textual mentions across 33,105 hospitalizations. The Aug-Linear approach achieves comparable performance to black-box models (AUC 0.81 vs 0.84) while providing inherently interpretable explanations that align better with clinician reasoning.

## Method Summary
The study classifies HF patients into HFrEF (LVEF < 40%) and HFpEF (LVEF > 50%) using Dutch discharge letters from Amsterdam UMC. Silver labels are programmatically generated from ICD-10 codes, echocardiography results, and regex patterns on text. Two model types are evaluated: (1) fine-tuned transformer models (BERT, MedRoBERTa.nl) and (2) Aug-Linear models that use frozen transformer embeddings with interpretable linear classifiers. Text is processed in 512-token chunks, with explicit LVEF values masked during training to prevent shortcut learning. Models are trained on AMC data with VUmc serving as external validation, and performance is measured using AUC with 10-fold cross-validation.

## Key Results
- Aug-Linear models achieved AUC 0.81 on external validation, comparable to BERT-based models at 0.84
- Discharge letters alone provided competitive performance to structured data, with BERT-based models reaching AUC 0.84
- Aug-Linear explanations showed better alignment with clinician annotations compared to post-hoc explanations from black-box models
- Silver labels enabled training on 33,105 hospitalizations, though gold-standard validation on 300 cases confirmed their reliability

## Why This Works (Mechanism)
The Aug-Linear approach works by combining the semantic understanding capabilities of pre-trained transformers with the interpretability of linear models. By freezing the transformer encoder and using its embeddings as features for a linear classifier, the model can leverage contextual word representations while maintaining transparent decision boundaries. The silver labeling strategy enables large-scale training without expensive manual annotation, while masking explicit LVEF values prevents the model from learning superficial shortcuts. The n-gram aggregation captures clinically meaningful phrases rather than individual words, allowing the linear model to assign interpretable weights to specific textual evidence.

## Foundational Learning
- **Transformer Encoders for Biomedical Text**: Pre-trained transformers convert clinical text into meaningful embeddings that capture semantic relationships. Quick check: How does attention help distinguish "good left ventricular function" from "poor left ventricular function"?
- **Interpretability vs. Explainability**: The study distinguishes between inherently interpretable models (Aug-Linear) and post-hoc explainability methods (SHAP, LIME). Quick check: Why might SHAP explanations on BERT fail to represent true decision processes compared to logistic regression weights?
- **Silver Labeling & Weak Supervision**: Programmatically generated labels enable large-scale training when manual annotation is scarce. Quick check: What risks arise from using ICD-10 codes as primary sources for HFpEF labels given physicians' coding habits?

## Architecture Onboarding
- **Component map**: Raw Dutch discharge letters -> Preprocessing (tokenization, chunking) -> Frozen MedRoBERTa.nl encoder -> Embedding aggregation -> Linear classifier (LR/EBM) -> Probability score
- **Critical path**: 1) Aggregate silver labels from multiple sources, 2) Mask explicit LVEF values in training text, 3) Extract n-gram embeddings using frozen transformer, 4) Train interpretable linear classifier on aggregated embeddings
- **Design tradeoffs**: Performance vs. interpretability (small AUC drop for transparency), granularity vs. complexity (trigrams capture more context but increase feature space), label noise vs. scale (33k+ hospitalizations vs. potential noise)
- **Failure signatures**: Shortcut learning from unmasked LVEF values, degraded performance on out-of-distribution text, interpretability illusions from spurious correlations in post-hoc explanations
- **First 3 experiments**: 1) Compare structured-data-only baseline to TF-IDF model on discharge letters, 2) Train Aug-Linear with unigrams, bigrams, then trigrams to measure performance changes, 3) Compare clinician annotations with Aug-Linear and SHAP explanations using Cohen's Kappa

## Open Questions the Paper Calls Out
- Does integrating outpatient visit notes and echocardiography reports improve both accuracy and explainability compared to discharge letters alone?
- How robust are Aug-Linear models when applied to hospitals with different documentation styles outside Amsterdam UMC?
- To what extent does noise in silver labels affect the models' ability to learn clinically meaningful features versus dataset artifacts?
- Can the classification approach effectively phenotype the intermediate HFmrEF class, which was excluded from this binary analysis?

## Limitations
- Silver labels introduce potential noise and systematic bias from diagnosis codes and text extraction
- 300 gold-standard annotations are relatively small for validating the silver-label approach
- Study focuses exclusively on Dutch clinical text, limiting generalizability to other languages
- Both validation hospitals belong to the same healthcare organization, potentially limiting external validity

## Confidence
- High Confidence: Comparative performance metrics (AUC values) between model types
- Medium Confidence: Interpretability claims about Aug-Linear models aligning with clinician reasoning
- Low Confidence: Assertions about Aug-Linear explanations being "inherently more faithful" than post-hoc methods

## Next Checks
1. Conduct systematic review of 100 randomly selected silver-labeled cases against source documents to quantify label accuracy
2. Apply Aug-Linear methodology to English-language discharge notes from a comparable HF cohort
3. Evaluate model performance on discharge letters from different time periods within Amsterdam UMC to assess temporal stability