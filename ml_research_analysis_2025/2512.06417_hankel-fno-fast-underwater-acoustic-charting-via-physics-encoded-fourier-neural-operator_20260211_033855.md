---
ver: rpa2
title: 'Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier
  Neural Operator'
arxiv_id: '2512.06417'
source_url: https://arxiv.org/abs/2512.06417
tags:
- rmse
- samples
- fine-tuning
- data
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fast and accurate underwater
  acoustic charting for applications such as sensor placement and path planning. Traditional
  numerical solvers are computationally expensive and impractical for real-time or
  large-scale applications, while deep learning-based surrogate models often lack
  physical constraints or struggle with generalization across different environments.
---

# Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator

## Quick Facts
- arXiv ID: 2512.06417
- Source URL: https://arxiv.org/abs/2512.06417
- Reference count: 0
- One-line primary result: Physics-encoded FNO with Hankel and bathymetry encodings achieves 6× improvement in H1 error for long-range TL prediction while maintaining computational speed.

## Executive Summary
This paper addresses the challenge of fast and accurate underwater acoustic charting for applications such as sensor placement and path planning. Traditional numerical solvers are computationally expensive and impractical for real-time or large-scale applications, while deep learning-based surrogate models often lack physical constraints or struggle with generalization across different environments. The authors propose Hankel-FNO, a physics-encoded Fourier Neural Operator (FNO) that integrates sound propagation knowledge through Hankel function encoding and bathymetry information into the SSF input. This approach improves prediction accuracy while maintaining computational speed. Experiments show Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, particularly for long-range predictions.

## Method Summary
Hankel-FNO uses an FNO backbone with physics-encoded inputs: the Hankel function amplitude (capturing far-field cylindrical wave spreading) and bathymetry encoding (modifying SSF below seafloor). The model takes 2-channel input (encoded SSF + Hankel amplitude) plus positional encoding, passes through 4 Fourier layers (kmax=64, C=64), and outputs TL predictions. Training uses H¹ Sobolev loss to capture interference patterns. A two-stage approach pretrains on a source setting, then fine-tunes on limited data for new environments. The architecture is discretization-invariant, enabling zero-shot super-resolution inference.

## Key Results
- 6× improvement in H¹ error for 40 km predictions (0.54×10⁻¹ vs 3.47×10⁻¹) compared to vanilla FNO
- Outperforms traditional RAM solver in speed while maintaining accuracy
- Achieves 0.91×10⁻² H¹ error vs 2.42×10⁻² for vanilla FNO on 10 km predictions
- Strong adaptability to new source frequencies, depths, and bathymetries through fine-tuning with 300 samples in ~244 seconds

## Why This Works (Mechanism)

### Mechanism 1: Hankel Function Encoding as Physics-Aware Inductive Bias
Encoding the asymptotic amplitude of the Hankel function as an input channel improves long-range transmission loss prediction, particularly for capturing decay characteristics that vanilla FNO struggles with. The Hankel function |H₀⁽¹⁾(k₀r)| ≈ √(2/(πk₀r)) encodes the far-field spreading behavior of cylindrical waves. By stacking this range-dependent decay profile along the depth dimension and providing it as an explicit input channel, the model receives strong prior information about how acoustic energy geometrically spreads, reducing the burden of learning this physics from data alone.

### Mechanism 2: Bathymetry Encoding via SSF Modification
Directly modifying the sound speed field below the bathymetry line to the sediment sound speed improves accuracy more effectively than adding bathymetry as a separate channel. Rather than concatenating bathymetry as an additional input channel (which increases dimensionality and may introduce redundant information), the approach modifies the SSF itself: for depths z > D_bty(r), the sound speed is set to v_sed = 1700 m/s. This creates a sharp, physically meaningful discontinuity at the water-sediment interface that the network can more easily learn to respect.

### Mechanism 3: H¹ Sobolev Loss for Capturing Interference Fringes
Training with H¹ Sobolev loss (incorporating first-order spatial derivatives) enables the model to capture fine-scale interference patterns better than standard MSE loss. The H¹ loss penalizes errors not only in the TL field values but also in their gradients with respect to range and depth. This is critical for acoustic charting because interference fringes manifest as high-frequency spatial oscillations. Standard L₂ losses exhibit spectral bias toward low frequencies, while H¹ loss forces alignment of derivative structure.

### Mechanism 4: Discretization Invariance for Zero-Shot Super-Resolution
FNO's architecture enables inference at resolutions higher than training data without retraining. All operations in FNO are either global (FFT, inverse FFT) or pointwise (1×1 convolutions, linear transforms). The Fourier mode truncation retains only the lowest k_max modes regardless of input resolution, so the learned spectral weights R_i have fixed dimensions. This means a model trained on low-resolution data can process higher-resolution inputs directly.

## Foundational Learning

- **Concept: Fourier Neural Operator (FNO)**
  - Why needed here: Hankel-FNO uses FNO as its backbone. Understanding how FNO learns mappings between function spaces via spectral convolutions is essential for debugging and extending the model.
  - Quick check question: Given a 2D input field of shape (M, N), what is the shape of the data after FFT, mode truncation to k_max, and inverse FFT?

- **Concept: Parabolic Equation Method in Underwater Acoustics**
  - Why needed here: The Hankel function encoding is derived from the parabolic equation approach to wave propagation. Understanding this connection clarifies why the encoding is physically motivated rather than arbitrary.
  - Quick check question: Why does the parabolic equation method factorize pressure as p(r,z) = ψ(r,z)H₀⁽¹⁾(k₀r), and what physical role does each component play?

- **Concept: Transfer Learning and Fine-Tuning**
  - Why needed here: The paper's practical value hinges on the two-stage pretrain/fine-tune strategy. Understanding when and why fine-tuning outperforms training from scratch is critical for real-world deployment.
  - Quick check question: According to Table V, why does fine-tuning with 50 samples sometimes underperform training from scratch, but fine-tuning with 300 samples outperforms it consistently?

## Architecture Onboarding

- **Component map:** Input (SSF with bathymetry encoding, Hankel encoding) + Positional Encoding → Lifting (1×1 conv) → 4× Fourier Layers (FFT → truncate to kmax=64 → spectral weights → IFFT) → Projection (1×1 conv) → Output (TL(r,z))

- **Critical path:** The Fourier layers are the computational core. Each layer performs FFT on the full spatial field, applies learned spectral weights, and inverts. The truncation to k_max=64 modes controls both speed and regularization.

- **Design tradeoffs:**
  | Decision | Options | Tradeoff |
  |----------|---------|----------|
  | Fourier modes (k_max) | Higher → better accuracy, slower inference | Paper selects 64; Fig. 9 shows Trun 64 (RMSE 1.29 dB) vs Trun 16 (RMSE 3.10 dB) |
  | Feature channels (C) | Higher → more capacity, more parameters | Paper uses 64 |
  | Bathymetry encoding vs. separate channel | Encoding is more efficient and accurate | Table IV shows encoding outperforms separate channel |
  | Fine-tuning data amount | More samples → better accuracy, longer adaptation | Table V and VI show 300 samples achieves strong results in ~244 seconds |

- **Failure signatures:**
  - High-frequency sources (>400 Hz): Fine-scale details become blurred; Fig. 10 shows RMSE increasing from 0.79 dB (100 Hz) to 3.71 dB (800 Hz).
  - Domain shift without fine-tuning: RMSE jumps to ~8-12 dB when source frequency/depth/bathymetry change (Table V, "w/o fine-tuning" rows).
  - Insufficient fine-tuning data with complex shifts: 50 samples may not be enough when multiple factors change simultaneously (Fig. 14).

- **First 3 experiments:**
  1. Reproduce the 10 km baseline comparison: Train vanilla FNO and Hankel-FNO on the 10 km dataset. Verify that Hankel-FNO achieves H¹ error <1.0×10⁻² and RMSE <1.5 dB on the 346-sample test set. This validates the core physics-encoding contribution.
  2. Ablate each encoding separately: Run three configurations—(a) no encodings, (b) only bathymetry encoding, (c) only Hankel encoding—on the 40 km dataset. Confirm that Hankel encoding provides the dominant improvement for long-range predictions (H¹ error should drop by ~6×).
  3. Validate fine-tuning efficiency: Take a 200 Hz pretrained model, fine-tune on 100 samples at 300 Hz, and compare RMSE against a model trained from scratch on the same 100 samples. Expect fine-tuned model to achieve RMSE ~3.9 dB vs. ~4.1 dB from scratch (Table V).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural adjustments are required to maintain fine-scale prediction accuracy at source frequencies significantly exceeding 800 Hz?
- Basis in paper: [explicit] The authors observe that at 800 Hz, "detail loss becomes more pronounced" and "predictions may become blurred," explicitly stating that "architectural adjustments may be necessary" beyond this range.
- Why unresolved: The current model performance degrades as acoustic patterns become more complex at higher frequencies, and the paper does not define the specific modifications needed to handle this increased spectral complexity.
- What evidence would resolve it: A modified Hankel-FNO architecture demonstrating stable RMSE and SSIM scores at frequencies > 1000 Hz without requiring a proportional increase in training data or computational cost.

### Open Question 2
- Question: How can the model be reformulated to enable zero-shot generalization to varying source depths without requiring fine-tuning?
- Basis in paper: [explicit] The paper notes the model "does not take the source depth as an explicit input but learns it implicitly from the data," resulting in a failure to adapt to source depth changes without fine-tuning.
- Why unresolved: The current design treats source depth as a fixed environmental parameter learned during training rather than a variable input condition, limiting its flexibility for arbitrary source configurations.
- What evidence would resolve it: A study showing the model successfully predicting transmission loss for unseen source depths by treating depth as a distinct input channel or conditioning variable, rather than requiring gradient updates.

### Open Question 3
- Question: Can the "spectral bias" preventing models trained from scratch from capturing fine details be mitigated without relying on a large-scale pre-training and fine-tuning strategy?
- Basis in paper: [inferred] The paper concludes that models trained from scratch "fail to model fine details" due to neural network inductive bias, necessitating a transfer learning approach. The underlying difficulty of learning high-frequency interference patterns from limited data remains unaddressed by the architecture itself.
- Why unresolved: The solution provided relies on transfer learning rather than solving the fundamental spectral bias of the FNO when initialized randomly on small datasets.
- What evidence would resolve it: Experiments demonstrating that a randomly initialized Hankel-FNO can achieve high SSIM on fine structures using only the limited "fine-tuning" datasets, potentially through input embedding scaling or specialized activation functions.

## Limitations
- The Hankel function encoding assumes far-field cylindrical wave spreading and requires knowledge of the source frequency at inference time.
- The bathymetry encoding uses a fixed sediment sound speed (1700 m/s), which may introduce systematic errors in environments with spatially varying seabed properties.
- Performance degrades for high-frequency sources (>400 Hz) due to spectral bias toward lower frequencies in the Fourier representation.

## Confidence
- **High confidence**: The core contribution that Hankel-FNO outperforms traditional solvers in speed and data-driven alternatives in accuracy for underwater acoustic charting tasks.
- **Medium confidence**: The specific mechanisms by which physics encodings (Hankel, bathymetry) improve performance.
- **Medium confidence**: The claim that H¹ Sobolev loss is critical for capturing interference fringes.

## Next Checks
1. **Ablation study on fine-tuning data efficiency**: Systematically vary fine-tuning sample size (10, 50, 100, 300) for different types of domain shifts (frequency, depth, bathymetry) to establish minimum data requirements and identify when fine-tuning becomes more effective than training from scratch.

2. **Frequency sensitivity analysis**: Extend the high-frequency performance evaluation beyond 800 Hz to characterize the upper frequency limit where the model remains useful, and identify architectural modifications (e.g., higher kmax, additional Fourier layers) that could extend this range.

3. **Real-world deployment test**: Validate the model's performance on field-collected environmental data rather than synthetic RAM-generated data, particularly focusing on cases where bathymetry or sediment properties deviate from assumed values.