---
ver: rpa2
title: QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime
  Reconfiguration
arxiv_id: '2505.06481'
source_url: https://arxiv.org/abs/2505.06481
tags:
- expert
- arxiv
- each
- experts
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the challenge of efficiently serving multiple\
  \ fine-tuned mixture-of-experts (MoE) large language models (LLMs) on a single GPU,\
  \ which is constrained by high memory demands and shared resource requirements in\
  \ multi-tenant environments. The authors propose a serving system that combines\
  \ similarity-based expert consolidation\u2014sharing similar experts across models\
  \ to reduce memory footprint\u2014with runtime partial reconfiguration, which dynamically\
  \ swaps non-expert layers when processing requests from different models."
---

# QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration

## Quick Facts
- **arXiv ID**: 2505.06481
- **Source URL**: https://arxiv.org/abs/2505.06481
- **Reference count**: 24
- **Primary result**: Achieves throughput comparable to serving a single model while incurring only a small increase in time-to-first-token, with an 85% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG).

## Executive Summary
This paper introduces a serving system for multiple fine-tuned mixture-of-experts (MoE) large language models (LLMs) on a single GPU, addressing the challenge of high memory demands in multi-tenant environments. The system combines similarity-based expert consolidation—sharing similar experts across models to reduce memory footprint—with runtime partial reconfiguration, which dynamically swaps non-expert layers when processing requests from different models. Experiments on an NVIDIA A100 GPU using Mixtral-8x7B and Google Switch Transformer Base-8 models demonstrate that this approach achieves throughput comparable to serving a single model while incurring only a small increase in time-to-first-token. Specifically, it achieves an 85% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG) and maintains competitive output quality, showing scalability and resilience across up to four model variants.

## Method Summary
The method combines similarity-based expert consolidation with runtime partial reconfiguration to serve multiple fine-tuned MoE-LLMs on a single GPU. First, the system computes L2 distances between flattened expert weights across models, ranks expert locations by similarity, and assigns experts to models via round-robin to maximize shared memory usage. During inference, when a request arrives for a different model, the system swaps the non-expert parameters (attention, embeddings) between GPU and host memory via PCIe, incurring a one-time ~1.2s overhead per request. If the gated expert is not currently loaded, it is fetched from CPU memory, with an additional ~27.8ms per expert overhead. This approach balances memory efficiency and output quality by loading only the most similar experts and dynamically swapping non-expert layers as needed.

## Key Results
- Achieves throughput comparable to serving a single model while incurring only a small increase in time-to-first-token.
- Demonstrates an 85% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG).
- Maintains competitive output quality across benchmarks (perplexity, ROUGE, MT-Bench, MMLU, HellaSwag, TruthfulQA, SAMSum) while serving up to four model variants.

## Why This Works (Mechanism)

### Mechanism 1: Similarity-Based Expert Consolidation
Fine-tuned MoE variants often retain high similarity in specific expert weights, allowing memory sharing without catastrophic quality loss. The system flattens expert weights into 1D tensors and calculates the L2 distance between experts at identical positions (layer, expert ID) across models. It ranks these by similarity (lowest distance) and loads the top-ranked experts into GPU memory using a round-robin assignment to ensure equal representation. Core assumption: Fine-tuning primarily modifies non-expert layers or specific experts, leaving a significant portion of expert weights correlated enough to be shared (L2 distance 3.6–4.9 vs 150–250 for mismatched positions).

### Mechanism 2: Runtime Partial Reconfiguration
Switching between model variants incurs minimal latency because non-expert parameters (attention, embeddings) are small (~3GB) relative to the full model, allowing rapid PCIe transfers. When a request arrives for Model B while Model A's non-expert weights are loaded, the system pauses inference, offloads Model A's non-expert layers, and loads Model B's non-expert layers from Host RAM to GPU VRAM via PCIe Gen4. Core assumption: The compute overhead of swapping ~3GB of data is less costly than reloading the full model or the quality loss from using mismatched non-expert layers.

### Mechanism 3: Dynamic Expert Fetching on Miss
The system maintains output quality by fetching the exact required expert from CPU memory when the cached "consolidated" expert belongs to a different model. During inference, if the gate selects an expert $(iL, iE)$ that is not currently on the GPU (or is mapped to a different model), the system triggers a fetch from Host RAM. Core assumption: The latency of fetching one expert is preferable to generating output with the wrong expert.

## Foundational Learning

- **Concept**: **Mixture of Experts (MoE) Sparsity**
  - **Why needed here**: To understand why this system works, you must grasp that MoE models only activate a subset of parameters (experts) per token. This paper exploits the fact that *inactive* experts occupy memory but don't require compute, making memory the primary bottleneck rather than FLOPs.
  - **Quick check question**: If a model uses Top-2 routing, how many experts are active per token, and how does this relate to the "memory wall" problem the paper solves?

- **Concept**: **Model Virtualization (Time vs. Space Sharing)**
  - **Why needed here**: The paper positions itself against NVIDIA MIG (Space Sharing) and sequential loading (Time Sharing). Understanding these baselines clarifies why a hybrid approach (shared memory + dynamic swapping) is necessary.
  - **Quick check question**: Why does Space Sharing (MIG) fail to improve throughput for large MoE models in this context? (Hint: Check the "amplified communication overhead" in Section 1).

- **Concept**: **L2 Distance / Euclidean Norm**
  - **Why needed here**: The core consolidation algorithm relies on L2 distance to determine expert similarity.
  - **Quick check question**: How does the paper define "distance" between two experts, and what trend did they observe regarding distance in deeper layers? (Ref: Section 3.1).

## Architecture Onboarding

- **Component map**: Offline Analyzer -> Initial Loader -> Inference Orchestrator -> Expert Cache/Offload Buffer
- **Critical path**:
  1. **Request Arrival**: System identifies `targetModel`.
  2. **Reconfiguration**: Compare `targetModel` ID with currently loaded non-experts. If different, swap non-expert layers (~1.2s).
  3. **Forward Pass**: For each token, compute Attention -> Gate -> Check Expert Map.
  4. **Expert Handling**:
     - *Hit*: Use resident expert (might be approximated from another model).
     - *Miss*: Fetch specific expert from CPU (~27.8ms overhead), then compute.
- **Design tradeoffs**:
  - **Memory vs. Accuracy**: Aggressive consolidation (loading fewer unique experts) saves memory but increases the probability of using an "approximate" expert (from a different model) or triggering CPU fetches, potentially degrading quality or speed.
  - **Throughput vs. TTFT**: The system optimizes for throughput (comparable to single model) by accepting a slight increase in TTFT due to non-expert swapping.
- **Failure signatures**:
  - **High TTFT Variance**: Indicates frequent non-expert swapping (users are alternating models rapidly).
  - **Quality Collapse**: Generated text is nonsensical; suggests the "similarity" assumption failed (experts are too dissimilar to share) or the wrong non-expert weights are loaded.
  - **PCIe Saturation**: Throughput plateaus; indicates the link is clogged with expert/non-expert transfers.
- **First 3 experiments**:
  1. **Profile Expert Similarity**: Calculate L2 distances between your specific fine-tuned models. Verify if same-position experts actually have low distance (Section 3.1). If not, this approach may not work.
  2. **Baseline Latency Test**: Measure the raw time to swap non-expert layers over your specific PCIe link (the paper reports ~1.2s on A100).
  3. **Hit Rate Simulation**: Simulate the "Expert Hit/Miss" logic. Estimate how often an expert fetch from CPU would occur based on your consolidation map to predict throughput impacts.

## Open Questions the Paper Calls Out
1. **Continuous Batching Performance**: How does the proposed system perform under continuous batching scenarios compared to the evaluated single-batch requests? The authors state, "To evaluate the system’s performance, we focus on single-batch requests, providing a controlled setting for our experiments." Production serving environments typically utilize continuous batching to maximize GPU utilization, a dynamic which introduces complex scheduling and memory fragmentation issues not captured by single-batch tests.

2. **Generalization to Heterogeneous Architectures**: Can the similarity-based consolidation technique be generalized to serve MoE models with heterogeneous architectures or distinct expert sizes? The paper notes that it "focuses specifically on fine-tuned LLMs with identical architectures," although it claims applicability to different sizes. The current algorithm relies on a strict coordinate-based mapping (layer and expert index) to calculate distances and assign experts, which fails if model structures differ.

3. **Interaction with FlashAttention**: What are the latency and quality trade-offs when combining this system with memory-bound optimization techniques like FlashAttention? The authors mention that "other inference optimizations, such as Flash Attention... are complementary... and fall outside the scope of this work." FlashAttention optimizes the non-expert layers (attention), which the proposed system swaps during runtime reconfiguration; the interaction between these swaps and fused kernels is unknown.

## Limitations
- The evaluation relies on a specific A100 80GB GPU with PCIe Gen4 and a high-memory CPU (251GB+), which may not generalize to other hardware configurations.
- The system trades output quality for memory efficiency by sharing experts across models, but the mechanisms for detecting and mitigating quality loss are not rigorously validated.
- The dynamic fetching of experts from CPU memory on cache misses is a key feature, but the paper does not provide detailed performance analysis under varying cache hit rates.

## Confidence
- **High**: The core mechanism of similarity-based expert consolidation and runtime partial reconfiguration is technically sound and well-explained.
- **Medium**: The reported performance gains are plausible given the hardware assumptions, but depend on the specific expert similarity and PCIe bandwidth.
- **Low**: The exact implementation details (e.g., expert capacity formula, cache eviction policy, precision settings) are underspecified, which could lead to discrepancies in reproduction.

## Next Checks
1. **Hardware Sensitivity Analysis**: Replicate the system on a different GPU (e.g., H100 or RTX 4090) and measure how changes in PCIe bandwidth, VRAM capacity, and CPU memory affect throughput and TTFT.
2. **Expert Similarity Validation**: Compute L2 distances between experts for your specific fine-tuned models. If the distances are consistently high (>100), the consolidation strategy may fail.
3. **Cache Hit Rate Simulation**: Simulate the expert hit/miss logic under varying request patterns (e.g., bursty arrivals, alternating models). Estimate the frequency of CPU fetches and their impact on throughput.