---
ver: rpa2
title: An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection
  on Contaminated Training Data--Extended Version
arxiv_id: '2510.18998'
source_url: https://arxiv.org/abs/2510.18998
tags:
- time
- anomaly
- series
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised time series anomaly detection
  method called EDAD that addresses the problem of contaminated training data. The
  core idea is an encode-then-decompose paradigm where the latent representation is
  split into stable features (capturing long-term patterns) and auxiliary features
  (capturing short-term variations and noise).
---

# An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version

## Quick Facts
- arXiv ID: 2510.18998
- Source URL: https://arxiv.org/abs/2510.18998
- Reference count: 40
- Primary result: Proposed EDAD method achieves state-of-the-art performance on eight benchmark datasets for unsupervised time series anomaly detection on contaminated training data

## Executive Summary
This paper introduces EDAD, an unsupervised time series anomaly detection method designed to handle contaminated training data. The core innovation is an encode-then-decompose paradigm that splits latent representations into stable features (capturing long-term patterns) and auxiliary features (capturing short-term variations). Rather than using reconstruction errors, EDAD employs mutual information as the anomaly score, making it more robust to anomalies in training data. The method demonstrates superior or competitive performance compared to strong baselines across eight real-world benchmark datasets.

## Method Summary
EDAD uses an encoder to produce latent representations that are decomposed into stable and auxiliary features through temporal shuffling. Stable features are trained to be invariant to shuffling (capturing long-term trends), while auxiliary features are trained to predict shuffled inputs (capturing short-term variations). Anomaly detection is performed using mutual information between the original encoding and auxiliary features, with a teacher-student consistency regularization mechanism to stabilize training when stable and auxiliary modules create conflicting gradient signals.

## Key Results
- EDAD achieves state-of-the-art or competitive performance on eight real-world benchmark datasets
- The method demonstrates robustness to varying contamination ratios in training data
- EDAD shows good runtime efficiency and memory usage characteristics compared to strong baselines
- The approach outperforms traditional reconstruction-based methods on contaminated datasets

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Decomposition via Temporal Shuffling
The encoder produces unified features that split into stable and auxiliary representations. Stable features are trained to be invariant to random temporal shuffling within subsequences (capturing long-term trends/seasonality), while auxiliary features must predict shuffled inputs (capturing timestamp-specific variations). This creates a "deep decomposition" that separates normal persistent patterns from point-wise anomalies.

### Mechanism 2: Mutual Information-based Anomaly Scoring
Using negative point-wise mutual information between original encoding and auxiliary features as anomaly scores is more robust to contamination than reconstruction error. Since auxiliary features capture short-term variations, anomalies (unexpected variations) should exhibit lower MI with the original encoding.

### Mechanism 3: Teacher-Student Consistency Regularization
A teacher model (exponential moving average of student weights) stabilizes training when stable and auxiliary modules create conflicting gradient signals. The teacher model provides a smoothed reference target, and consistency loss penalizes deviation between student and teacher projected representations.

## Foundational Learning

- **Concept: Variational Mutual Information Estimation (InfoNCE)**
  - Why needed here: EDAD relies on InfoNCE for both training loss and anomaly scoring. Understanding how contrastive bounds approximate MI is essential for debugging detection failures.
  - Quick check question: Given batches of (Y, Y_aux) pairs, can you explain why the InfoNCE bound increases when joint pairs are more similar than marginal pairs?

- **Concept: Temporal Shuffling as Data Augmentation**
  - Why needed here: The core decomposition mechanism uses shuffling to create self-supervisory signals. Understanding what temporal information is preserved vs. destroyed by shuffling clarifies what stable vs. auxiliary features capture.
  - Quick check question: If you shuffle a time series subsequence containing a spike anomaly, which feature type (stable/auxiliary) would change more?

- **Concept: Dimension Independence Strategy**
  - Why needed here: EDAD preprocesses multivariate series by treating each dimension independently, which affects what correlations the model can learn. This is a critical architectural choice.
  - Quick check question: For a 3-variable system where variable 3 = variable 1 + variable 2, what information does the dimension independence strategy discard?

## Architecture Onboarding

- **Component map:** Normalization layer → Attention module (linear embedding + multi-head self-attention + MLP) → Split into Y_sta, Y_aux → Stable feature module (shuffle Y_sta, identity Y_aux, MI maximization with Y) → Auxiliary feature module (identity Y_sta, shuffle Y_aux, closeness loss to shuffled Y) → Consistency regularization (teacher-student EMA)

- **Critical path:**
  1. Input subsequence (window B=100) → normalized
  2. Attention module outputs Y (no bottleneck compression)
  3. Y splits: first d/2 → Y_sta, second d/2 → Y_aux
  4. Training: L_sta (reconstruction + MI) + L_aux (closeness) + L_reg (consistency)
  5. Inference: AS(s_i) = -I_θ(Y, Y_aux) per timestamp

- **Design tradeoffs:**
  - No bottleneck compression preserves fine-grained temporal features but increases memory vs. standard autoencoders
  - InfoNCE vs. other MI estimators: Table V shows JSD competitive; MINE/NWJ less stable. InfoNCE trades some bias for lower variance
  - Separable vs. concatenated critics: Table V shows separable critics perform best with lower computational cost

- **Failure signatures:**
  - Low precision on contaminated datasets: May indicate anomalies polluting stable features; check if shuffling ratio is too low
  - High variance across runs: Check λ₁/λ₂ balance (Figure 6 shows sensitivity); if unstable, reduce learning rate or increase λ₃ regularization
  - Runtime exceeding baselines: If attention dominates, reduce number of heads M or hidden dimension d

- **First 3 experiments:**
  1. Contamination robustness test: Train on SWaT/SVDB with artificially injected anomalies at 1%, 5%, 10%, 20% ratios. Compare EDAD vs. LSTM-AE degradation curves.
  2. Ablation on MI estimators: Replace InfoNCE with JSD, MINE, NWJ on 2-3 datasets. If all estimators fail similarly, the issue is in the decomposition, not the MI estimator.
  3. Window size sensitivity: Test B ∈ {10, 25, 50, 100, 200} on datasets with different anomaly durations (point vs. collective). Verify B≈100 heuristic holds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the EDAD framework be adapted to maintain robustness in streaming environments subject to concept drift?
- Basis in paper: [explicit] The conclusion explicitly identifies "concept drift settings" and "continual learning settings" as areas of interest for future research.
- Why unresolved: The current evaluation assumes a static distribution during training; real-world systems evolve, potentially degrading the fixed stable/auxiliary decomposition over time.
- What evidence would resolve it: An online learning variant of EDAD that dynamically updates the stable feature representation without catastrophically forgetting previously learned normal patterns.

### Open Question 2
- Question: Can the Encode-then-Decompose paradigm be modified to model cross-dimensional correlations in multivariate time series?
- Basis in paper: [inferred] The methodology states that the model uses a "dimension independence strategy," which "disregards correlations between dimensions" based on the assumption that dimensions do not share information.
- Why unresolved: While the authors note this strategy often outperforms cross-channel modeling, it remains an assumption that ignores potential inter-variable dependencies that could define certain collective anomalies.
- What evidence would resolve it: A modified architecture that integrates cross-dimensional attention while maintaining the separation of stable and auxiliary features, showing improved performance on datasets with high inter-channel correlation.

### Open Question 3
- Question: How can the method be extended to semi-supervised settings where a small subset of labels is available?
- Basis in paper: [explicit] The conclusion lists "semi-supervised settings" as a specific direction for future research to avoid potentially high labeling costs.
- Why unresolved: The current mutual information loss and decomposition modules are designed purely for unsupervised learning; it is unclear how few labeled anomalies would interact with the current objective function.
- What evidence would resolve it: A loss function variant that incorporates labeled examples to guide the decomposition boundary between stable and auxiliary features.

## Limitations

- The assumption that anomalies are temporally localized may fail for gradual concept drift or persistent adversarial attacks
- The dimension independence preprocessing strategy discards cross-variable correlations, potentially limiting performance on highly coupled systems
- Several critical hyperparameters (λ values, batch size, EMA decay, critic architecture) lack transparency in the paper

## Confidence

- **High Confidence:** EDAD's state-of-the-art performance on benchmark datasets; the core mechanism of latent decomposition through temporal shuffling; the general effectiveness of mutual information over reconstruction errors for contamination robustness
- **Medium Confidence:** The specific hyperparameter choices (window size B=100, InfoNCE stability); the teacher-student regularization effectiveness in stabilizing conflicting gradients; the dimension independence preprocessing benefits
- **Low Confidence:** The generalizability to highly correlated multivariate systems; the performance on datasets with non-temporally-localized anomalies; the exact optimal balance of λ₁, λ₂, λ₃ without labeled validation data

## Next Checks

1. **Contamination Ratio Sensitivity:** Systematically test EDAD's performance degradation as contamination increases from 1% to 20% on SWaT/SVDB, comparing against LSTM-AE to verify the claimed robustness advantage.
2. **MI Estimator Ablation:** Replace InfoNCE with JSD, MINE, and NWJ on 2-3 datasets to isolate whether performance differences stem from the decomposition mechanism or the MI estimation method.
3. **Window Size Optimization:** Test B ∈ {10, 25, 50, 100, 200} on datasets with varying anomaly durations (point vs. collective) to validate the B≈100 heuristic and identify optimal window sizes for different anomaly types.