---
ver: rpa2
title: 'When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference
  Dynamics in MLLMs'
arxiv_id: '2511.02243'
source_url: https://arxiv.org/abs/2511.02243
tags:
- uncertainty
- text
- answer
- modality
- relative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for understanding modality
  following in multimodal large language models (MLLMs) by decomposing the behavior
  into relative reasoning uncertainty and inherent modality preference. To validate
  this framework, the authors construct a controllable dataset that systematically
  varies the reasoning difficulty of visual and textual inputs, and use entropy as
  a fine-grained uncertainty metric.
---

# When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs

## Quick Facts
- arXiv ID: 2511.02243
- Source URL: https://arxiv.org/abs/2511.02243
- Authors: Zhuoran Zhang; Tengyue Wang; Xilin Gong; Yang Shi; Haotian Wang; Di Wang; Lijie Hu
- Reference count: 40
- One-line primary result: A universal law governs modality following in MLLMs: the probability of following a modality decreases monotonically as its relative reasoning uncertainty increases.

## Executive Summary
This paper introduces a novel framework for understanding modality following behavior in multimodal large language models by decomposing the phenomenon into two components: relative reasoning uncertainty and inherent modality preference. The authors construct a controlled synthetic dataset that systematically varies the reasoning difficulty of visual and textual inputs, then use entropy as a fine-grained uncertainty metric to validate their framework. Their key finding is a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. This allows them to identify a balance point where models are equally likely to follow either modality, providing a principled way to quantify inherent preference while disentangling it from unimodal capabilities and dataset artifacts.

## Method Summary
The authors decompose modality following into relative reasoning uncertainty (the ratio of entropy between modalities) and inherent modality preference (the balance point where following probability is 0.5). They construct a controllable dataset with systematically varied reasoning difficulty across three tiers (Easy: 80% accuracy, Medium: 60%, Hard: 40%) for both visual and textual inputs. For uncertainty measurement, they use entropy of the response distribution from a fixed pretrained LLM, finding it more informative than accuracy-based metrics. The framework is validated across multiple MLLM architectures (LLaVA-1.5, Qwen-VL, InternVL) using color/attribute recognition tasks, and layer-wise probing reveals internal oscillations in ambiguous regions near the balance point.

## Key Results
- A universal law governs modality following: following probability decreases monotonically with relative uncertainty
- Balance point identification provides a principled method to quantify inherent modality preference
- Layer-wise probing reveals internal oscillations in ambiguous regions near balance points
- The framework successfully disentangles inherent preference from unimodal capabilities and dataset artifacts

## Why This Works (Mechanism)
The framework works because modality following in MLLMs can be decomposed into two distinct factors: the relative confidence in each modality's reasoning (measured through entropy) and an inherent preference for one modality over another. When reasoning uncertainty is high for one modality, the model naturally shifts preference toward the modality with lower uncertainty. The entropy-based uncertainty metric captures the model's internal reasoning confidence more precisely than accuracy-based measures, enabling fine-grained analysis of the preference dynamics.

## Foundational Learning
- **Relative uncertainty calculation** - why needed: Forms the core predictive metric for modality following; quick check: Verify entropy ratios correlate with following probabilities
- **Balance point identification** - why needed: Enables quantification of inherent preference independent of capability; quick check: Confirm 0.5 following probability at identified balance point
- **Entropy-based uncertainty** - why needed: Provides fine-grained confidence measurement vs binary accuracy; quick check: Compare entropy distributions across difficulty tiers
- **Layer-wise probing methodology** - why needed: Reveals internal decision-making dynamics; quick check: Validate oscillation patterns across multiple layers
- **Controlled dataset construction** - why needed: Enables systematic variation of reasoning difficulty; quick check: Confirm accuracy targets match intended difficulty tiers
- **Unimodal vs multimodal distinction** - why needed: Clarifies the framework's scope to conflicting modalities; quick check: Verify unimodal performance doesn't confound balance point measurements

## Architecture Onboarding

Component map:
- Controlled dataset generation -> Entropy uncertainty calculation -> Following probability measurement -> Balance point identification -> Layer-wise probing

Critical path:
Dataset generation (controlled difficulty tiers) → Unimodal reasoning (entropy calculation) → Modality following measurement → Universal law validation → Balance point quantification

Design tradeoffs:
- Synthetic vs real-world data: Controlled difficulty vs ecological validity
- Entropy vs accuracy: Fine-grained confidence vs simplicity
- Single model probing vs cross-model generalization: Depth vs breadth of insights

Failure signatures:
- Non-monotonic following probability curves suggest framework violations
- Inconsistent balance points across repeated trials indicate measurement instability
- Layer-wise oscillations that don't correlate with difficulty transitions suggest internal processing issues

First experiments:
1. Replicate the universal law validation across a different MLLM architecture
2. Test entropy-based uncertainty prediction on a held-out difficulty tier
3. Verify balance point stability through repeated measurements on the same model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can inherent modality preference be systematically modified through training interventions, architectural changes, or inference-time techniques?
- Basis in paper: [inferred] The paper establishes how to quantify inherent preference via balance points but does not investigate how to alter this preference
- Why unresolved: The framework is analytical rather than interventional; no experiments attempt to shift balance points
- What evidence would resolve it: Experiments showing that specific fine-tuning regimes, architectural modifications, or prompting strategies produce statistically significant shifts in balance point locations

### Open Question 2
- Question: Does the monotonic relationship between relative uncertainty and modality following hold for real-world, unconstrained multimodal inputs beyond synthetic datasets?
- Basis in paper: [explicit] The authors note they "built a toy dataset" with controlled difficulty tiers, and acknowledge their findings rely on this controlled setting
- Why unresolved: Validation used only color/attribute recognition tasks with systematically varied difficulty; natural images and text may introduce confounding factors
- What evidence would resolve it: Replication of the monotonic law and balance point analysis on diverse, real-world benchmarks without synthetic manipulation

### Open Question 3
- Question: What architectural or training factors determine the location of a model's inherent preference balance point?
- Basis in paper: [inferred] Different model families show markedly different balance points, but the paper does not explain why these differences arise
- Why unresolved: The study is observational across existing models rather than causal; no controlled experiments isolate factors like pre-training data ratios or fusion mechanisms
- What evidence would resolve it: Systematic ablation studies varying individual training or architectural components while measuring resulting balance point shifts

### Open Question 4
- Question: How does the uncertainty-preference framework extend to settings with more than two conflicting modalities?
- Basis in paper: [inferred] The framework is defined specifically for conflicting pairs of modalities; the extension to three or more modalities is unaddressed
- Why unresolved: The mathematical formulation uses binary relative uncertainty; multi-way conflict resolution mechanisms remain unexplored
- What evidence would resolve it: Experiments with tri-modal inputs showing whether a generalized uncertainty metric predicts following behavior across all modalities

## Limitations
- The controlled synthetic dataset may not generalize to naturalistic multimodal scenarios with complex, interdependent modality signals
- The entropy-based uncertainty metric assumes unimodal reasoning processes operate independently, potentially missing cross-modal interference effects
- The claimed universality of the following probability relationship lacks validation across diverse MLLM architectures and real-world datasets

## Confidence
- High confidence in the experimental methodology and controlled dataset construction
- Medium confidence in the relative uncertainty framework as a valid explanatory model
- Medium confidence in the universal law within controlled settings
- Low confidence in generalizability to naturalistic multimodal scenarios

## Next Checks
1. Test the framework across multiple MLLM architectures (GPT-4V, Gemini, Claude) to assess cross-model consistency of the uncertainty-following relationship
2. Apply the methodology to naturalistic multimodal datasets with ground truth modality relevance annotations to validate framework performance in real-world conditions
3. Extend the analysis to include cross-modal uncertainty interactions, examining whether entropy measurements adequately capture multimodal reasoning complexity when modalities influence each other's processing