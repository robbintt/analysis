---
ver: rpa2
title: Rethinking Encoder-Decoder Flow Through Shared Structures
arxiv_id: '2501.14535'
source_url: https://arxiv.org/abs/2501.14535
tags:
- feature
- banks
- bank
- vision
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the persistent architectural stagnation in
  decoder designs for dense prediction tasks, despite advancements in encoder architectures.
  The authors propose a novel approach by introducing "banks" - shared structures
  that provide additional context to each decoding block.
---

# Rethinking Encoder-Decoder Flow Through Shared Structures

## Quick Facts
- arXiv ID: 2501.14535
- Source URL: https://arxiv.org/abs/2501.14535
- Authors: Frederik Laboyrie; Mehmet Kerim Yucel; Albert Saa-Garriga
- Reference count: 34
- Primary result: Novel "banks" architecture improves monocular depth estimation with negligible parameter increase

## Executive Summary
This paper addresses the persistent stagnation in decoder architectures for dense prediction tasks by introducing shared "banks" that provide additional context to decoding blocks. The authors propose feature banks and sampling banks that interact with intermediate feature maps through resampling and fusion mechanisms. The sampling banks employ a guided version of DySample for dynamic upsampling, supporting non-integer sampling factors and downsampling operations. When evaluated on NYUv2 monocular depth estimation, the approach demonstrates consistent improvements across multiple metrics while maintaining computational efficiency.

## Method Summary
The proposed architecture introduces "banks" as shared structures that augment decoder blocks with additional contextual information. These banks consist of two main components: feature banks that enhance feature representations through fusion mechanisms, and sampling banks that utilize a guided version of DySample for dynamic upsampling operations. The sampling banks support both non-integer upsampling factors and downsampling, providing flexibility for different scale requirements. The architecture is designed to work with vision transformer encoders, specifically demonstrating effectiveness with ViT-S achieving near-ViT-B performance levels.

## Key Results
- Consistent improvements across multiple metrics (δ1, AbsRel, RMSE) on NYUv2 depth estimation
- Near-ViT-B performance achieved using more efficient ViT-S architecture
- Negligible increase in parameters and GFLOPs despite architectural enhancements
- Qualitative improvements in artifact recovery and geometric accuracy

## Why This Works (Mechanism)
The shared banks mechanism works by providing additional contextual information to each decoding block through feature fusion and dynamic resampling. The feature banks enhance intermediate representations by incorporating shared context, while the sampling banks enable adaptive spatial transformations through guided DySample. This approach addresses the decoder's need for both spatial detail preservation and contextual understanding, which are typically at odds in standard decoder designs.

## Foundational Learning
- Vision Transformer Encoders: Understanding how ViT architectures process visual information hierarchically is crucial for appreciating how the banks integrate with encoder outputs. Quick check: Verify encoder feature map dimensions at each stage.
- Dynamic Sampling (DySample): The guided version of DySample enables non-integer sampling factors and downsampling, which is essential for the sampling banks' functionality. Quick check: Test sampling accuracy across different ratios.
- Feature Fusion Mechanisms: The interaction between banks and intermediate feature maps through fusion operations is central to the architecture's effectiveness. Quick check: Validate fusion operation implementations.

## Architecture Onboarding
**Component Map**: Input -> Encoder -> Decoding Blocks + Feature Banks + Sampling Banks -> Output
**Critical Path**: Encoder features → Decoding blocks → Bank interactions → Output predictions
**Design Tradeoffs**: The shared bank approach trades increased architectural complexity for improved contextual awareness and spatial adaptability. The negligible parameter increase suggests efficient implementation of these shared structures.
**Failure Signatures**: Potential issues may arise from bank-encoder feature misalignment, sampling bank instability with extreme ratios, or fusion operation conflicts in complex scenes.
**First Experiments**: 1) Validate individual bank component functionality in isolation, 2) Test sampling bank behavior across varying sampling ratios, 3) Compare feature bank contributions with and without fusion operations.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation restricted to monocular depth estimation on NYUv2 with limited additional dataset reporting
- Absence of comprehensive ablation studies isolating individual bank component contributions
- Limited analysis of sampling banks' behavior across different scene types and sampling ratios
- Potential dataset-specific training dynamics not characterized across diverse tasks

## Confidence
- High confidence: Architectural novelty and baseline improvements on NYUv2 are well-defined and reproducible
- Medium confidence: Efficiency claims supported but require broader task evaluation for generalizability confirmation
- Low confidence: Claims about addressing "architectural stagnation" are overstated given limited task scope and literature comparison

## Next Checks
1. Conduct comprehensive ablation studies removing individual bank components to quantify their isolated contributions
2. Evaluate the architecture across diverse dense prediction tasks (semantic segmentation, instance segmentation, optical flow)
3. Perform controlled experiments varying sampling ratios and downsampling factors to characterize sampling banks' behavior and identify failure modes