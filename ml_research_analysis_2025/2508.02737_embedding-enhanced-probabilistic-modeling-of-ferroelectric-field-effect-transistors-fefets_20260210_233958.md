---
ver: rpa2
title: Embedding-Enhanced Probabilistic Modeling of Ferroelectric Field Effect Transistors
  (FeFETs)
arxiv_id: '2508.02737'
source_url: https://arxiv.org/abs/2508.02737
tags:
- device
- modeling
- embedding
- compact
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately modeling variability
  in FeFETs, which exhibit inherent randomness due to operational cycling and fabrication
  differences. This variability complicates prediction and reliability in circuit
  design.
---

# Embedding-Enhanced Probabilistic Modeling of Ferroelectric Field Effect Transistors (FeFETs)

## Quick Facts
- arXiv ID: 2508.02737
- Source URL: https://arxiv.org/abs/2508.02737
- Reference count: 26
- Primary result: Probabilistic FeFET modeling with R² = 0.92 accuracy

## Executive Summary
This study addresses the challenge of accurately modeling variability in FeFETs, which exhibit inherent randomness due to operational cycling and fabrication differences. The authors propose a probabilistic modeling framework that builds on Mixture Density Networks (MDNs) and incorporates C∞-continuous activation functions (Mish) for smooth learning and a device-specific embedding layer to capture device-to-device variability. Sampling from the learned embedding distribution enables the generation of synthetic devices for variability-aware simulation. The model achieves an R² score of 0.92, demonstrating high accuracy in capturing the stochastic behavior of FeFET current characteristics.

## Method Summary
The method employs a Mixture Density Network (MDN) architecture that predicts probability distributions rather than deterministic outputs. The model takes gate voltage (V_G) and device ID as inputs, concatenates them with a 4-dimensional learnable device embedding, and processes through hidden layers with Mish activation functions. The output layer generates K Gaussian mixture parameters (α_k, μ_k, σ_k), enabling representation of multimodal and uncertain outcomes. The model is trained using CRPS loss and can generate synthetic devices by sampling from the learned embedding distribution and applying inverse transform sampling to produce I-V curves.

## Key Results
- Achieved R² score of 0.92 in capturing FeFET current variability
- Successfully demonstrated both cycle-to-cycle (C2C) and device-to-device (D2D) variability modeling
- Enabled generation of synthetic device instances through learned embedding distributions
- Validated model performance across 63 FeFET devices with gate voltage range of -0.2V to 1.8V

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Output via Mixture Density Networks
- Claim: MDNs can capture the full stochastic behavior of FeFETs by predicting probability distributions rather than deterministic outputs
- Mechanism: For each input condition, the network outputs K Gaussian mixture parameters (α_k, μ_k, σ_k), enabling representation of multimodal and uncertain outcomes inherent in FeFET switching and conduction
- Core assumption: The underlying device stochasticity can be approximated as a mixture of Gaussian distributions
- Evidence anchors: [abstract] "Building upon a Mixture Density Network (MDN) foundation" and [section 2.2] "This design enables the MDN to output a complete probability density function (PDF)"
- Break condition: If the actual device variability exhibits strong non-Gaussian tails or state-dependent noise not captured by static mixtures

### Mechanism 2: C∞-Continuous Activation Functions for Simulation Compatibility
- Claim: Replacing ReLU with Mish activation ensures infinitely differentiable outputs, improving both training stability and circuit simulation convergence
- Mechanism: Mish (x · tanh(softplus(x))) is C∞-continuous, eliminating non-differentiable points that can cause gradient artifacts and solver failures in SPICE-like environments
- Core assumption: Circuit simulators require smooth, differentiable models for reliable numerical convergence
- Evidence anchors: [abstract] "integrates C∞-continuous activation functions for smooth, stable learning" and [section 3.1] "ReLU is only C⁰ continuous, meaning it is not differentiable at zero"
- Break condition: If model training requires sparse gradients for regularization or if the deeper computational graph of Mish causes training instability on limited data

### Mechanism 3: Device-Specific Embeddings for D2D Variability Capture
- Claim: Trainable low-dimensional embedding vectors can encode intrinsic physical variability across devices, enabling synthetic device generation
- Mechanism: Each device ID maps to a 4D embedding vector trained jointly with the MDN. The learned embedding space is modeled as a multivariate Gaussian from which new device representations can be sampled
- Core assumption: Device-to-device variability lives on a low-dimensional continuous manifold that can be captured from I-V data alone
- Evidence anchors: [abstract] "a device-specific embedding layer to capture intrinsic physical variability across devices" and [section 3.2] "Each device ID is associated with a 4-dimensional embedding vector"
- Break condition: If the embedding dimension is insufficient to capture variability modes or if out-of-distribution devices exhibit behaviors not represented in the training population

## Foundational Learning

- Concept: Mixture Density Networks (MDNs) and Gaussian Mixture Models
  - Why needed here: MDNs are the core engine for representing stochastic FeFET behavior; understanding how they parameterize distributions is essential for debugging and extension
  - Quick check question: Given three mixture components with (α, μ, σ) = (0.5, 1.0, 0.1), (0.3, 1.5, 0.2), (0.2, 2.0, 0.3), can you sketch the resulting PDF and identify if it is multimodal?

- Concept: C∞ Continuity and Numerical Stability in Circuit Simulation
  - Why needed here: Smoothness requirements are critical for SPICE integration; without this, models can cause solver divergence
  - Quick check question: Why would a function with a discontinuous first derivative cause problems for Newton-Raphson-based circuit solvers?

- Concept: Embedding Layers and Latent Space Representations
  - Why needed here: The D2D variability mechanism relies on learning continuous device representations from discrete IDs
  - Quick check question: If two devices have nearly identical learned embeddings, what does that imply about their I-V characteristics? How would you detect if all embeddings collapsed to a small region?

## Architecture Onboarding

- Component map: Input (V_G + 4D embedding) -> Hidden layers with Mish activation -> Output layer (K×3 units) -> Softmax for α_k, Softplus for σ_k, Linear for μ_k -> Loss (CRPS) -> Fit multivariate Gaussian to embeddings -> Sampling for synthetic devices

- Critical path: 1) Collect I-V data across multiple devices and cycles (label each with device ID) 2) Initialize 4D embeddings (random) for each device 3) Train MDN with CRPS loss, jointly updating network weights and embeddings 4) Fit multivariate Gaussian (μ_emb, Σ_emb) to learned embeddings 5) For synthetic devices: sample embeddings, feed through trained MDN, apply inverse CDF sampling

- Design tradeoffs: Embedding dimension (higher → more expressive but risk of overfitting); Number of mixture components (more components capture complex distributions but increase training difficulty); Quantile clipping (reduces outlier predictions but may underrepresent tail behavior)

- Failure signatures: Mode collapse (multiple mixture components converge to similar μ_k); Embedding degeneration (all device embeddings cluster tightly); Sampling instability (inverse CDF fails or produces non-physical negative currents); Poor generalization (synthetic devices show I-V curves outside envelope of real device data)

- First 3 experiments: 1) Baseline MDN without embeddings: Validate C2C variability capture 2) MDN with embeddings, random sampling: Test whether synthetic devices match spread of real device I-V curves 3) Systematic sampling at mean and ±2σ embeddings: Validate edge-case device behavior generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the embedding-enhanced MDN framework perform when integrated into actual SPICE-based circuit simulations, and does C∞ continuity guarantee numerical convergence in transient analyses?
- Basis in paper: [explicit] The authors state the model is "not yet implemented as a full compact model" but claim C∞ continuity ensures "compatibility with compact modeling requirements"
- Why unresolved: Only standalone I-V curve predictions are demonstrated; no circuit-level simulation results are presented
- What evidence would resolve it: Successful simulation of benchmark circuits in Cadence SPECTRE or similar tools with convergence metrics

### Open Question 2
- Question: How does incorporating additional physical parameters (temperature, cycle count, device geometry) affect model accuracy and the structure of the learned embedding space?
- Basis in paper: [explicit] "Looking ahead, incorporating additional input features such as temperature, cycle count, or device structure may further enhance the accuracy and robustness"
- Why unresolved: The current model uses only gate voltage and device ID as inputs; no ablation study or extended experiments with physical parameters are conducted
- What evidence would resolve it: Comparative experiments showing R² and distributional fit when temperature/cycle count are added

### Open Question 3
- Question: What is the optimal number of mixture components (K) for balancing expressiveness and computational stability, and how sensitive is model performance to this hyperparameter?
- Basis in paper: [explicit] "Tuning the number of mixture components could also improve the balance between model expressiveness and stability"
- Why unresolved: The paper does not report the K value used or systematically explore its impact on accuracy
- What evidence would resolve it: Ablation experiments varying K (e.g., 2–10) with metrics including R², CRPS loss, and convergence behavior

## Limitations
- Gaussian mixture assumption may not capture non-Gaussian tails or temporal correlations in device behavior
- 4-dimensional embedding layer was not systematically optimized through ablation studies
- Model generalizability to device populations outside the 63-device training set is untested
- No circuit-level simulation validation to confirm SPICE compatibility claims

## Confidence
- Probabilistic modeling accuracy (R² = 0.92): High - supported by experimental validation
- Mixture density network assumptions: Medium - Gaussian mixtures may not capture all stochastic behaviors
- Device embedding representation: Low-Medium - effective but not rigorously validated for physical interpretability

## Next Checks
1. Test model performance on unseen FeFET devices from different fabrication batches to assess out-of-distribution generalization
2. Compare Gaussian mixture predictions against empirical histograms showing potential non-Gaussian features in FeFET variability
3. Perform systematic ablation studies varying embedding dimensions and mixture component counts to optimize model architecture