---
ver: rpa2
title: 'MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts'
arxiv_id: '2601.21866'
source_url: https://arxiv.org/abs/2601.21866
tags:
- time
- mohets
- series
- forecasting
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoHETS is an encoder-only Transformer that employs a Mixture-of-Heterogeneous-Experts
  (MoHE) strategy to model global trends and local periodicities in time series. It
  combines a shared depthwise-convolution expert for sequence-level continuity with
  routed Fourier-based experts for patch-level periodic structures, and incorporates
  exogenous covariates via cross-attention.
---

# MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts

## Quick Facts
- arXiv ID: 2601.21866
- Source URL: https://arxiv.org/abs/2601.21866
- Authors: Evandro S. Ortigossa; Guy Lutsker; Eran Segal
- Reference count: 40
- Key outcome: MoHETS consistently outperforms state-of-the-art models, reducing average MSE by 12% on long-term forecasting benchmarks.

## Executive Summary
MoHETS is an encoder-only Transformer architecture designed for long-term multivariate time series forecasting. It employs a Mixture-of-Heterogeneous-Experts (MoHE) strategy that combines a shared depthwise-convolution expert for modeling global trends with routed Fourier-based experts for capturing local periodicities. The model incorporates exogenous covariates through cross-attention and uses a convolutional patch decoder for improved parameter efficiency. Evaluated across seven benchmarks with horizons up to 720 timesteps, MoHETS demonstrates superior performance compared to existing forecasting models.

## Method Summary
MoHETS uses an encoder-only Transformer with Mixture-of-Heterogeneous-Experts (MoHE) containing one shared depthwise-convolution expert and eight routed Fourier-based experts. The model processes time series data through channel-independent patching and incorporates calendar covariates via cross-attention. Training employs a combined Huber loss with auxiliary load balancing, using AdamW optimizer with cosine annealing and early stopping. The architecture achieves improved parameter efficiency through a ConvNeXt-style inverted bottleneck projection head and demonstrates stable training with careful weight initialization strategies.

## Key Results
- Achieves 12% average MSE reduction compared to strong baselines on long-term forecasting
- Consistently outperforms state-of-the-art models across seven multivariate benchmarks
- Demonstrates superior performance on horizons up to 720 timesteps
- Shows improved parameter efficiency through convolutional patch decoder design

## Why This Works (Mechanism)
The MoHE strategy enables MoHETS to capture both global temporal continuity through depthwise convolution and local periodic structures through Fourier-based experts. The mixture routing mechanism allows dynamic selection of appropriate expert combinations for different temporal patterns, while the encoder-only design with cross-attention efficiently incorporates exogenous covariates. The convolutional patch decoder replaces heavy linear heads, improving parameter efficiency and training stability without sacrificing forecasting accuracy.

## Foundational Learning

**Depthwise Convolution** - Channel-wise spatial filtering that captures local temporal dependencies. Needed for modeling sequence-level continuity and global trends. Quick check: Verify that each channel is processed independently with shared convolution weights.

**Fourier-based FFN** - Neural network layer incorporating periodic functions (sin/cos) for modeling cyclical patterns. Needed for capturing local periodicities and seasonal variations. Quick check: Confirm periodic gates interact correctly with linear paths in the routing mechanism.

**Mixture-of-Experts Routing** - Dynamic selection mechanism that routes inputs to specialized experts based on learned gating functions. Needed for adaptive modeling of heterogeneous temporal patterns. Quick check: Monitor expert selection variance to prevent routing collapse.

**Cross-attention with Exogenous Features** - Attention mechanism that incorporates external calendar covariates into the time series representation. Needed for leveraging contextual information like day-of-week or holiday effects. Quick check: Validate covariate scaling to continuous frequency values.

## Architecture Onboarding

Component Map: Input Patches -> Shared Depthwise-Conv Expert + 8 Routed FA-FFN Experts -> Transformer Encoder -> Conv Patch Decoder -> Output

Critical Path: Time series patches → MoHE layer (shared + routed experts) → Transformer blocks → Conv patch decoder → Forecast

Design Tradeoffs:
- Encoder-only vs. encoder-decoder: Improved efficiency but requires careful covariate incorporation
- Depthwise vs. standard convolution: Reduced parameters while maintaining local pattern capture
- Fourier vs. standard FFN: Better periodic structure modeling at cost of implementation complexity

Failure Signatures:
- Routing collapse: Model ignores routed experts, relying solely on shared expert
- Convergence instability: Oscillating validation loss indicating optimization issues
- Overfitting: Large train-val gap on small datasets suggesting capacity issues

First Experiments:
1. Implement and test the MoHE layer with one shared DwConv and two FA-FFN experts on ETTh1
2. Validate exogenous covariate encoding with cross-attention on a small synthetic dataset
3. Compare initialization strategies (Normal vs. Xavier) for FA-FFN weights on convergence behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Exogenous covariate encoding scheme lacks complete specification for continuous frequency transformation
- FA-FFN routing mechanism implementation details require careful calibration
- Certain decoder parameters (padding, stride) are inferred rather than explicitly stated

## Confidence

High Confidence: Overall MoHETS architecture design and MoHE strategy implementation details are well-documented and verifiable.

Medium Confidence: Training procedure specifications including loss functions, optimization parameters, and initialization strategies are clearly defined.

Low Confidence: Exogenous covariate processing pipeline and specific FA-FFN routing dynamics present implementation challenges due to incomplete specifications.

## Next Checks

1. Implement monitoring of expert selection variance and auxiliary loss during training to ensure balanced expert utilization and prevent routing collapse.

2. Test multiple implementations of calendar feature transformation to continuous frequency values, comparing forecasting performance across different time feature granularities.

3. Systematically test different initialization strategies for Fourier Analysis FFN weights and measure their impact on convergence speed and final forecasting accuracy.