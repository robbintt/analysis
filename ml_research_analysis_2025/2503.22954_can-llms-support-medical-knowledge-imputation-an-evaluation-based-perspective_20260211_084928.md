---
ver: rpa2
title: Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective
arxiv_id: '2503.22954'
source_url: https://arxiv.org/abs/2503.22954
tags:
- knowledge
- llms
- prompt
- medical
- treatment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the use of large language models (LLMs) for
  imputing missing treatment relationships in medical knowledge graphs (KGs). A reference
  KG was constructed from curated sources like UMLS, DrugBank, and ICD9-CM to assess
  LLM-generated disease-drug associations.
---

# Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective

## Quick Facts
- arXiv ID: 2503.22954
- Source URL: https://arxiv.org/abs/2503.22954
- Reference count: 22
- Primary result: LLMs show high success rates for partial treatment imputation but low recall, indicating limited comprehensive coverage

## Executive Summary
This study evaluates large language models (LLMs) for imputing missing treatment relationships in medical knowledge graphs (KGs). A reference KG was constructed from curated sources like UMLS, DrugBank, and ICD9-CM to assess LLM-generated disease-drug associations. Four LLMs (GPT-4o, o3-mini, Perplexity-R1, and Perplexity-Sonar) were queried with three prompt formats to generate treatment mappings. While success rates reached up to 96.67%, recall remained low (0.0074-0.0842), indicating limited comprehensive coverage. Model-to-model alignment improved with more detailed prompts, but alignment with the reference KG decreased. Robustness checks revealed inconsistencies across different interaction modes. The results highlight LLMs' potential for partial knowledge imputation but underscore the need for rigorous validation and hybrid approaches when integrating LLM outputs into clinical decision support systems.

## Method Summary
The study constructed a reference KG with 105,290 ATC-to-ICD treatment relationships from multiple curated sources. Thirty ICD9-CM codes weighted by frequency from a Portugal hospital EHR were sampled. Four LLMs were queried using three prompt formats: simple list (A), ATC level 5 specific (B), and JSON with drug names (C). Success rate (≥1 correct drug), recall, Jaccard similarity, and Sørensen-Dice coefficient were computed against the KG reference. Robustness was tested via 3 replicated runs on both API and web chat portal for Perplexity-Sonar.

## Key Results
- Success rates reached up to 96.67% for GPT-4o and Perplexity-Sonar, but recall remained low (0.0074-0.0842)
- Model-to-model alignment improved with more detailed prompts, but alignment with the reference KG decreased
- Robustness checks revealed significant inconsistencies between API and web chat portal outputs for the same model

## Why This Works (Mechanism)

### Mechanism 1: Structured Prompting Elicits Partial Knowledge Retrieval
Structured prompts with explicit output formats (JSON) increase the likelihood of retrieving at least one clinically valid treatment, but reduce comprehensive coverage of all known treatments. Prompt C achieved the highest success rates (up to 96.67%) while yielding the lowest recall (0.0074–0.0149), suggesting format constraints force models to prioritize output structure over exhaustive retrieval.

### Mechanism 2: Reference KG Alignment Serves as Scalable Validation Proxy
Comparing LLM-generated treatment relationships against curated medical KGs provides a scalable, interpretable proxy for expert validation when manual review is infeasible. The reference KG (105,290 ATC-to-ICD treatment relationships) serves as a "valid subset" ground truth, with coverage metrics quantifying alignment without requiring clinical annotation.

### Mechanism 3: Prompt Detail Increases Inter-Model Alignment but Decreases Ground Truth Alignment
More explicit prompt instructions increase consistency across different LLMs (higher model-to-model similarity) while paradoxically reducing alignment with the reference KG. Detailed prompts may cause models to converge on shared reasoning patterns, biases, or common training data representations that diverge from clinical coding standards.

## Foundational Learning

- Concept: Medical Knowledge Graphs and Ontology Systems (UMLS, ICD, ATC, DrugBank)
  - Why needed here: The entire evaluation framework depends on understanding how diseases (ICD9-CM codes) and drugs (ATC codes) are structured and linked in clinical coding systems.
  - Quick check question: Can you explain why ATC Level 5 codes are clinically relevant and how they differ from Level 1-4 codes?

- Concept: Knowledge Graph Completion and Link Prediction
  - Why needed here: The paper positions LLM imputation as an alternative/complement to traditional KG completion methods (TransE, GNNs).
  - Quick check question: Why can't embedding-based link prediction models (e.g., TransE, R-GCN) infer entirely novel relationships absent from training data?

- Concept: LLM Hallucination and Factual Grounding
  - Why needed here: The core risk in medical applications is hallucinated treatment associations.
  - Quick check question: What is the difference between a "hallucinated" medical association and a "novel but valid" association that simply isn't in the reference KG?

## Architecture Onboarding

- Component map: Reference KG Construction (UMLS, DrugBank, ICD9-CM, ATC, MONDO, DrugCentral, RepoDB, PrimeKG) -> LLM Query Pipeline (30 ICD9-CM inputs, 3 prompt formats, 4 models via API) -> Evaluation Layer (Success rate, Recall, Jaccard, Dice vs KG reference)

- Critical path: Reference KG construction (data normalization across ontologies is the bottleneck) -> Prompt design -> LLM querying -> Response parsing (ATC code extraction) -> Metric computation

- Design tradeoffs:
  - Prompt simplicity (A) vs. specificity (B/C): Simpler prompts yield higher recall; structured prompts yield higher success rates but lower coverage
  - API vs. web chat portal: API provides more consistent outputs; web portal introduces thread-based variability
  - Single-model vs. ensemble: No model consistently outperforms; ensemble approaches may surface complementary coverage

- Failure signatures:
  - High success rate + very low recall: Model returns one correct treatment but fails comprehensive retrieval
  - High inter-LLM similarity + low KG alignment: Models converge on non-clinical reasoning patterns
  - Cross-platform inconsistency (API vs. web): Same model/vendor produces divergent outputs across interaction modes

- First 3 experiments:
  1. Reproduce coverage metrics (success rate, recall) using Prompt A on a 30-disease sample against the reference KG structure described in Section 3
  2. Run robustness check: Query the same ICD code 3× via API and 3× via web chat portal for Perplexity-Sonar; compute pairwise Dice coefficients
  3. Test the alignment tradeoff: Compare Jaccard/Dice scores for Prompt A vs. Prompt C outputs across two models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Retrieval-Augmented Generation (RAG) enhance the factuality and consistency of medical knowledge imputation compared to open-ended prompting?
- Basis in paper: The authors state, "We aim to improve KG imputation by exploring retrieval-augmented generation (RAG) as a more controllable and interpretable alternative to open-ended LLM prompting."
- Why unresolved: The current study only evaluated open-ended generation methods, which resulted in hallucinations and low recall.
- What evidence would resolve it: A comparative study measuring hallucination rates and alignment scores between RAG-based models and the open-ended baselines established in this paper.

### Open Question 2
- Question: How can automatic confidence scoring or self-verification techniques be effectively applied to filter LLM outputs for medical knowledge graphs?
- Basis in paper: The authors plan to "investigate techniques for automatic confidence scoring or self-verification of LLM responses, allowing a more selective integration of generated content."
- Why unresolved: The paper demonstrates that LLMs generate mixed-quality outputs, but does not test methods for automatically filtering or weighting these outputs before integration.
- What evidence would resolve it: The development of a scoring mechanism that correlates strongly with clinical validity, enabling the system to automatically reject low-confidence hallucinations.

### Open Question 3
- Question: Can the evaluation framework established for treatment mapping be successfully generalized to complex tasks like contraindication detection and polypharmacy management?
- Basis in paper: The authors express interest in developing datasets to "assess the broader applicability of LLMs in clinical knowledge reasoning across contraindications, polypharmacy management, and care pathway optimization."
- Why unresolved: The current methodology was restricted to single disease-treatment mappings and did not address the nuance of adverse interactions or multi-drug regimens.
- What evidence would resolve it: Construction of reference KGs for contraindications and subsequent evaluation of LLM performance using the metrics defined in this study (Recall, Jaccard, Dice).

## Limitations

- The reference KG, while curated from authoritative sources, remains incomplete and may systematically miss novel or emerging treatments, potentially misclassifying valid LLM outputs as hallucinations
- The evaluation focuses exclusively on treatment associations (ICD-ATC mappings) and does not assess safety, dosage, contraindications, or clinical reasoning—critical dimensions for real-world deployment
- API-based querying may not reflect how clinicians would interact with these systems in practice, and robustness checks reveal significant mode-dependent variability that could undermine reliability in clinical settings

## Confidence

- **High confidence** in the success rate/recall tradeoff finding: The pattern is consistently observed across all models and prompt types, with clear quantitative backing from the results section
- **Medium confidence** in the inter-model alignment hypothesis: While the correlation between prompt detail and model-to-model similarity is evident, the mechanism linking this to shared biases rather than domain-specific convergence remains speculative
- **Low confidence** in generalizability: The 30-disease sample from a single Portuguese hospital system may not represent broader clinical practice, and the ICD9-CM coding system used is outdated compared to ICD10-CM

## Next Checks

1. Conduct a blinded expert review of LLM outputs to distinguish hallucinations from genuinely novel but valid treatments absent from the reference KG
2. Test the same evaluation framework using ICD10-CM codes and a multi-institutional dataset to assess generalizability
3. Implement an ensemble approach combining multiple LLM outputs and compare coverage and alignment metrics against single-model performance