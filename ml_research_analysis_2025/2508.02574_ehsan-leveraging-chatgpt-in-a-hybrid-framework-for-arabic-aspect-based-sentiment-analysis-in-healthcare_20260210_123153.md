---
ver: rpa2
title: 'EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment
  Analysis in Healthcare'
arxiv_id: '2508.02574'
source_url: https://arxiv.org/abs/2508.02574
tags:
- sentiment
- arabic
- aspect
- were
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EHSAN, a hybrid framework that combines ChatGPT
  pseudo-labelling with human review to create the first explainable Arabic aspect-based
  sentiment dataset for healthcare. The method segments reviews into sentences, labels
  each with an aspect and sentiment (positive, negative, neutral), and provides ChatGPT-generated
  rationales for transparency.
---

# EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare

## Quick Facts
- arXiv ID: 2508.02574
- Source URL: https://arxiv.org/abs/2508.02574
- Reference count: 24
- First explainable Arabic ABSA dataset for healthcare with ChatGPT rationales

## Executive Summary
This paper introduces EHSAN, a hybrid framework that combines ChatGPT pseudo-labelling with human review to create the first explainable Arabic aspect-based sentiment dataset for healthcare. The method segments reviews into sentences, labels each with an aspect and sentiment (positive, negative, neutral), and provides ChatGPT-generated rationales for transparency. Three training sets were created: fully supervised (all labels reviewed), semi-supervised (50% reviewed), and unsupervised (machine-only). Fine-tuning AraBERT and DistilBERT models on these sets, the results show high accuracy across all supervision levels, with minimal performance loss when using only machine labels. Reducing the number of aspect classes from 17 to 6 notably improved classification metrics. The study demonstrates a scalable, cost-effective approach for Arabic sentiment analysis in healthcare using LLM annotation and human expertise.

## Method Summary
The EHSAN framework processes Arabic healthcare reviews through GPT-4o-mini for sentence segmentation, then uses ChatGPT for pseudo-labelling with few-shot prompts providing aspect, sentiment, and rationale annotations. Human experts review 0-100% of labels across three dataset variants (FSD, SSD, USD). The 17-category aspect taxonomy is consolidated to 6 broader classes. AraBERT v0.2-large and DistilBERT are fine-tuned separately for aspect and sentiment classification tasks with batch size 4, learning rate 1e-5, 5 epochs, and early stopping. Preprocessing uses AraBERT toolkit for diacritic removal and character normalization.

## Key Results
- AraBERT achieved 81% accuracy and 0.78 F1 on 6-class aspect classification with fully supervised data
- Minimal performance drop (<2% F1) when using ChatGPT-only labels (USD) instead of human-reviewed data (FSD)
- 17-class taxonomy showed high variance in unsupervised settings with confidence intervals [0.46, 0.65] for USD
- Cohen's Kappa inter-annotator agreement reached 0.873 for human verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated pseudo-labels with selective human verification can produce training data nearly equivalent to fully human-annotated datasets for Arabic healthcare ABSA.
- Mechanism: ChatGPT assigns aspect and sentiment labels with justifications; humans review 0-100% of labels; models trained on these datasets show minimal performance variation across supervision levels because ChatGPT's initial accuracy is high (90-96%).
- Core assumption: ChatGPT's few-shot prompting reliably captures domain-specific Arabic healthcare semantics across dialects.
- Evidence anchors:
  - [abstract] "Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels."
  - [section 4.1] "The difference between using fully human-reviewed data and relying solely on ChatGPT labels was insignificant, with AraBERT's F1 dropping from 0.66 (FSD) to 0.64 (USD)."
  - [corpus] Related work (PABSA for Persian, LLM-in-the-Loop Active Learning for Arabic) shows similar hybrid annotation patterns but lacks direct Arabic healthcare benchmarks.
- Break condition: If LLM error rates exceed ~10% on dialectal medical terminology, pseudo-label noise accumulates and human review becomes necessary for >50% of samples.

### Mechanism 2
- Claim: Semantic sentence segmentation improves aspect classification by ensuring single-aspect-per-unit labeling.
- Mechanism: ChatGPT splits reviews by topical coherence (merging related fragments, separating compound sentences) rather than punctuation alone, reducing label ambiguity per training sample.
- Core assumption: One-to-one sentence-to-aspect mapping simplifies multi-class classification learning.
- Evidence anchors:
  - [abstract] "Each sentence is annotated with an aspect and sentiment label."
  - [section 3.3] "ChatGPT's segmentation yielded higher topical coherence, while Stanza occasionally retained compound sentences or split text at every punctuation mark, regardless of semantic context."
  - [corpus] Weak corpus evidence—no direct segmentation comparisons in related ABSA datasets.
- Break condition: If reviews contain implicit cross-sentence aspect dependencies, aggressive splitting loses contextual cues.

### Mechanism 3
- Claim: Consolidating fine-grained aspect taxonomies improves classification by reducing class sparsity and semantic overlap.
- Mechanism: Merging 17 categories into 6 groups (e.g., radiology+surgery→medical services) increases per-class training samples and reduces annotator confusion.
- Core assumption: Broader categories retain sufficient actionable meaning for healthcare quality monitoring.
- Evidence anchors:
  - [abstract] "Reducing the number of aspect classes from 17 to 6 notably improved classification metrics across the board."
  - [section 4.2] "AraBERT reached 81% accuracy and 0.78 F1 on the FSD, a substantial increase over the 17-class scenario."
  - [corpus] Czech ABSA dataset work notes similar granularity tradeoffs but in different domain.
- Break condition: If operational decisions require distinguishing "billing" from "administrative services," consolidation obscures actionable signals.

## Foundational Learning

- Concept: **Aspect-Based Sentiment Analysis (ABSA)**
  - Why needed here: Distinguishes document-level sentiment from aspect-linked sentiment; EHSAN operates at sentence-level with paired aspect+sentiment labels.
  - Quick check question: Given "The doctors were helpful but the billing was confusing," what two aspect-sentiment pairs should be extracted?

- Concept: **Pseudo-Labeling Reliability Thresholds**
  - Why needed here: Understanding when machine labels suffice versus when human correction adds value; EHSAN shows <2% F1 drop at 0% human review for 6-class aspects.
  - Quick check question: If ChatGPT achieves 90% topic accuracy and 96% sentiment accuracy, which task benefits more from human review?

- Concept: **Arabic NLP Preprocessing (Normalization, Diacritics, Character Variants)**
  - Why needed here: Arabic dialectal variation requires standardization; AraBERT preprocessor handles yā/á variants, removes tashkeel, reduces repeated letters.
  - Quick check question: Why might "مررررحبا" need normalization before model input?

## Architecture Onboarding

- Component map:
  1. **Data Collection** → Google Maps hospital reviews (3 regions, 5,000 reviews → 6,000 sentences)
  2. **Preprocessing** → AraBERT toolkit (diacritics removal, character normalization, whitespace cleanup)
  3. **Segmentation** → GPT-4o-mini topic-aware sentence splitting
  4. **Annotation** → ChatGPT pseudo-labels (aspect, sentiment, rationale) + human review tiers
  5. **Dataset Variants** → FSD (100% reviewed), SSD (50%), USD (0%)
  6. **Models** → AraBERT-v0.2-large, DistilBERT-multilingual
  7. **Evaluation** → Macro-F1 on human-annotated test set (1,190 samples)

- Critical path:
  Segmentation coherence → annotation consistency → taxonomy granularity → model selection → supervision level
  The 6-class taxonomy + AraBERT + SSD configuration achieves near-optimal results with 50% human effort.

- Design tradeoffs:
  - **AraBERT vs DistilBERT**: +12% F1 on sentiment vs 3× faster training
  - **17 vs 6 classes**: Analytical granularity vs +18% F1 improvement
  - **FSD vs USD**: <2% F1 gain for 100% more human annotation effort

- Failure signatures:
  - Wide confidence intervals on USD (e.g., [0.46, 0.65] for AraBERT 17-class) → insufficient supervision for fine-grained tasks
  - Underrepresented classes (<10 samples) → merge into broader categories
  - Annotator disagreement on borderline cases → refine taxonomy boundaries or add examples to prompts

- First 3 experiments:
  1. **Baseline replication**: Train AraBERT on FSD with 6-class taxonomy; target ≥0.78 F1 on aspect classification.
  2. **Supervision threshold sweep**: Compare FSD/SSD/USD across both taxonomies; quantify F1 drop per human-review percentage.
  3. **Segmentation ablation**: Run Stanza vs ChatGPT segmentation on 100-sample subset; measure aspect-label coherence manually.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the EHSAN framework and its fine-tuned models maintain high performance when applied to Arabic healthcare reviews from regions with distinct dialects and different healthcare delivery models?
- Basis in paper: [explicit] The authors explicitly state the need to "examine the applicability of the findings across other Arabic-speaking countries, especially those with distinct dialects and healthcare delivery models."
- Why unresolved: The current study restricted data collection to three hospitals within Saudi Arabia, limiting the validation of the framework to specific Saudi dialects and a single national healthcare system.
- What evidence would resolve it: Empirical results from training or testing the model on patient feedback datasets from non-Gulf Arabic regions (e.g., North Africa or the Levant) showing comparable F1 scores and annotation accuracy.

### Open Question 2
- Question: Does the integration of LLM-generated rationales into the training loop (e.g., via multi-task learning) improve the accuracy or interpretability of the sentiment classifier?
- Basis in paper: [explicit] The authors suggest that "integration of explanations into model training" could allow a model to generate justifications, moving toward "truly interpretable AI."
- Why unresolved: While the EHSAN dataset includes rationales for transparency, the current models were trained only on the aspect and sentiment labels, not the explanation text.
- What evidence would resolve it: A comparative study where a model is fine-tuned to predict both the label and the rationale, demonstrating improved performance or human-evaluated interpretability over the baseline.

### Open Question 3
- Question: To what extent does the ChatGPT pseudo-labelling pipeline exhibit systematic bias against specific Arabic dialects or aspect categories, and can targeted prompting mitigate this?
- Basis in paper: [explicit] The authors identify "fairness evaluation" as a future direction, noting the need to assess rationales for "implicit or unwarranted assumptions" and to address "bias patterns" through improved prompting.
- Why unresolved: The study relied on general human verification but did not perform a systematic audit of error rates or rationale quality across different linguistic subgroups or sensitive attributes.
- What evidence would resolve it: A bias audit analyzing the correlation between classification errors/sentiment assignments and specific dialectal markers or demographic indicators in the text.

## Limitations

- Data availability limited: exact ChatGPT prompts and full aspect-to-category mapping not provided for replication
- 17-class taxonomy shows high variance in unsupervised settings, suggesting overfitting to human-verified subset
- Results only validated on Saudi Arabian healthcare reviews, limiting generalizability to other Arabic dialects and healthcare contexts

## Confidence

- **High Confidence**: Mechanism 1 (pseudo-label reliability), Mechanism 3 (taxonomy consolidation benefits) - supported by consistent F1 improvements and statistical significance across multiple runs
- **Medium Confidence**: Mechanism 2 (segmentation improvement) - lacks direct ablation comparisons with baseline segmentation methods
- **Low Confidence**: USD variant stability for 17-class taxonomy - high variance suggests model instability without sufficient supervision

## Next Checks

1. **Prompt Quality Validation**: Test whether the observed 90-96% accuracy rates are consistent across different Arabic dialects by applying the same ChatGPT prompts to healthcare reviews from Egypt and Morocco, measuring label consistency and rationale quality.

2. **Supervision Threshold Analysis**: Conduct a more granular sweep of human review percentages (0%, 10%, 25%, 50%, 75%, 100%) to identify the precise break-even point where human review cost exceeds marginal F1 improvements.

3. **Cross-Domain Generalization**: Evaluate EHSAN's 6-class model on non-healthcare Arabic reviews (restaurants, hotels) to test whether the consolidated taxonomy captures domain-agnostic aspects or is overfit to medical terminology.