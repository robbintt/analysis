---
ver: rpa2
title: Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning
arxiv_id: '2509.20541'
source_url: https://arxiv.org/abs/2509.20541
tags:
- learning
- feedback
- query
- sparq
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient human-in-the-loop
  reinforcement learning (HiL-RL) where human feedback is costly and limited. The
  authors propose SPARQ, a progress-aware query policy that selectively requests human
  feedback only when learning stagnates or worsens, explicitly modeling human attention
  as a limited budget.
---

# Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.20541
- Source URL: https://arxiv.org/abs/2509.20541
- Authors: Anujith Muraleedharan; Anamika J H
- Reference count: 17
- One-line result: SPARQ uses half the human feedback budget while matching full-feedback performance on UR5 cube-picking.

## Executive Summary
This paper addresses the challenge of efficient human-in-the-loop reinforcement learning where human feedback is costly and limited. The authors propose SPARQ, a progress-aware query policy that selectively requests human feedback only when learning stagnates or worsens, explicitly modeling human attention as a limited budget. Evaluated on a simulated UR5 cube-picking task in PyBullet, SPARQ achieves 100% success rate while using only about half the feedback budget compared to always querying, matching the performance of full-feedback baselines.

## Method Summary
SPARQ monitors task progress through the change in episodic return (∆J_t) and triggers queries when performance worsens beyond a threshold (ϵ_worsen) or when no improvement is observed for P steps. A finite query budget B and cooldown period C prevent redundant queries. The effective reward combines environment reward, scaled human feedback, and a per-query cost penalty. SPARQ is implemented with Soft Actor-Critic and evaluated against No Oracle, Random Querying, and Always Querying baselines on a UR5 cube-picking task.

## Key Results
- SPARQ achieves 100% success rate on UR5 cube-picking task
- Uses approximately half the feedback budget compared to always querying
- Matches performance of full-feedback baselines while providing more stable learning than random querying

## Why This Works (Mechanism)

### Mechanism 1: Progress-Aware Query Triggering
- Claim: Querying only when learning stagnates or worsens achieves comparable performance to continuous querying while using approximately half the feedback budget.
- Mechanism: SPARQ monitors ∆J_t and triggers queries via the rule: `(∆J_t < -ϵ_worsen) ∨ (no improvement for P steps)`.
- Core assumption: Learning progress metrics reliably indicate when policy updates are unproductive or harmful.
- Evidence anchors: [abstract] "SPARQ, a progress-aware query policy that requests feedback only when learning stagnates or worsens" [Section 3.3] "Queries are triggered when performance worsens beyond ϵ_worsen, or when no improvement is observed for P steps"
- Break condition: If progress proxy (∆J_t) is noisy or uncorrelated with actual policy quality, trigger may fire spuriously or miss critical states.

### Mechanism 2: Budget-Constrained Cooldown Gating
- Claim: Enforcing a finite query budget with mandatory cooldown periods prevents redundant queries and distributes human effort more evenly over training.
- Mechanism: A query counter B decrements with each request; a cooldown counter c prevents re-queries for C timesteps after each query.
- Core assumption: Human attention is a scarce, budgetable resource, and burst queries fatigue operators without proportional learning benefit.
- Evidence anchors: [abstract] "explicitly modeling human attention as a limited budget" [Section 3.3] "we maintain a finite query budget and impose a cooldown period after each query to avoid redundancy"
- Break condition: If task requires dense corrective feedback during critical learning phases, cooldown may block necessary queries.

### Mechanism 3: Effective Reward Shaping with Query Cost Penalty
- Claim: Augmenting environment reward with scaled human feedback and a per-query cost preserves optimal policy structure while discouraging unnecessary queries.
- Mechanism: The effective reward `r^eff_t = r_t + λ·f_t - c·q_t` combines environment reward, scaled corrective feedback, and a query cost term.
- Core assumption: Human feedback f_t is consistent with optimal policy direction, and cost c appropriately reflects supervision burden.
- Evidence anchors: [Section 3.1, Eq. 1] Full formulation of effective reward [Section 3.1] "This formulation follows potential-based reward shaping principles, which preserve policy invariance [15]"
- Break condition: If λ is mis-scaled relative to r_t, feedback can dominate or be drowned out; if c is too high, agent may avoid queries even when beneficial.

## Foundational Learning

- Concept: **Markov Decision Processes (MDP) and Policy Optimization**
  - Why needed here: SPARQ operates within an MDP framework and learns via Soft Actor-Critic; understanding state-action-reward loops is essential.
  - Quick check question: Can you explain how a policy π_θ maps states to actions and how the discount factor γ affects long-term return?

- Concept: **Soft Actor-Critic (SAC)**
  - Why needed here: All query strategies are trained using SAC for its stability in continuous control; effective reward feeds directly into SAC updates.
  - Quick check question: What is the entropy bonus in SAC, and why does it help with exploration?

- Concept: **Potential-Based Reward Shaping**
  - Why needed here: The paper claims the effective reward formulation preserves policy invariance via potential-based shaping.
  - Quick check question: How does potential-based shaping guarantee that the optimal policy remains unchanged under reward augmentation?

## Architecture Onboarding

- Component map:
  Policy network π_θ -> SPARQ gate -> Human oracle π_h -> Replay buffer D -> SAC critic/actor updates

- Critical path:
  1. Observe state s_t
  2. Sample action a_t ~ π_θ(·|s_t)
  3. SPARQ evaluates trigger condition using ∆J_t, B, c
  4. If triggered: query oracle, override a_t ← π_h(s_t), set q_t=1, decrement B, reset cooldown c←C
  5. Execute action, observe r_t, compute r^eff_t
  6. Store (s_t, a_t, s_{t+1}, r^eff_t) in D
  7. Update π_θ via SAC

- Design tradeoffs:
  - **Patience P**: Too low → excessive queries; too high → missed intervention opportunities. Guideline: ~1–2× median episode length.
  - **Worsening threshold ϵ_worsen**: Too sensitive → spurious queries; too coarse → delayed intervention. Guideline: 5–10th percentile of negative |∆J_t| early in training.
  - **Cooldown C**: Too short → query bursts; too long → blocked necessary queries. Tune to match target budget fraction.

- Failure signatures:
  - **Query budget exhausted early**: P too low or ϵ_worsen too sensitive; increase patience or threshold.
  - **No convergence despite queries**: Progress proxy ∆J_t may be uninformative; consider alternative metrics or denser shaping.
  - **Policy instability after queries**: λ may be too large, causing feedback to dominate reward; reduce scaling.

- First 3 experiments:
  1. **Baseline replication**: Implement No Oracle, Random Querying, Always Querying on UR5 cube-picking task; verify success rates match Table 1 (No Oracle ~61%, others ~100%).
  2. **SPARQ ablation**: Vary P ∈ {episode_len, 2×episode_len} and ϵ_worsen ∈ {5th, 10th percentile}; measure budget consumption and success rate to confirm ~13% budget usage with 100% success.
  3. **Progress proxy sensitivity**: Replace distance-to-goal with raw episodic return as ∆J_t; assess whether performance degrades, testing robustness of progress criterion.

## Open Questions the Paper Calls Out
- The paper identifies scaling to higher-dimensional observations and noisier progress proxies as a natural next step, acknowledging the benchmark is "intentionally simple" with low-dimensional observations.
- The authors suggest extending SPARQ to incorporate richer forms of human input (e.g., preferences, demonstrations) as a future direction.

## Limitations
- Empirical claims rest on a single simulation environment (UR5 cube-picking), raising questions about robustness to task diversity.
- Effectiveness depends critically on sensitivity of ∆J_t to actual learning progress, which may vary across tasks with different reward densities.
- Budget and cooldown mechanisms assume human attention is a uniform, budgetable resource, which may not reflect real-world supervision scenarios.

## Confidence
- **High confidence**: SPARQ's query efficiency claim is directly supported by the ablation in Table 1 and is unlikely to be due to random chance given the deterministic simulation setup.
- **Medium confidence**: Progress-aware trigger mechanism's effectiveness relies on assumption that ∆J_t is a reliable proxy for learning stagnation.
- **Medium confidence**: Budget and cooldown mechanisms improve query distribution, but generalizability to real human supervisors is uncertain without user studies.
- **Low confidence**: Claims about policy invariance under augmented reward are theoretically sound but not empirically verified for specific λ and c values used.

## Next Checks
1. **Cross-task validation**: Test SPARQ on a second continuous control task (e.g., FetchReach or different manipulation task) to confirm that progress-aware querying consistently reduces feedback demand without sacrificing performance.
2. **Progress proxy ablation**: Compare SPARQ's performance when using raw episodic return vs. distance-to-goal as the progress metric to isolate impact of chosen proxy on query efficiency.
3. **Cost sensitivity sweep**: Vary the query cost c over a wider range (e.g., 0.01 to 0.5) to empirically validate claim that reward shaping preserves policy invariance and identify thresholds where feedback is suppressed too aggressively.