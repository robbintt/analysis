---
ver: rpa2
title: Neurosymbolic Information Extraction from Transactional Documents
arxiv_id: '2512.09666'
source_url: https://arxiv.org/abs/2512.09666
tags:
- schema
- extraction
- information
- documents
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A neurosymbolic framework is proposed for structured information
  extraction from transactional documents, leveraging large language models with multi-level
  symbolic validation (syntactic, task, and domain) to filter and refine outputs.
  The approach enforces domain-specific arithmetic constraints and schema-based coherence,
  significantly improving F1-scores and accuracy over baseline zero-shot extraction,
  and enabling effective knowledge distillation for fine-tuning.
---

# Neurosymbolic Information Extraction from Transactional Documents

## Quick Facts
- **arXiv ID:** 2512.09666
- **Source URL:** https://arxiv.org/abs/2512.09666
- **Reference count:** 40
- **Primary result:** Neurosymbolic framework with LLM + symbolic validation improves F1-scores and enables effective knowledge distillation for transactional document extraction

## Executive Summary
This paper proposes a neurosymbolic framework for structured information extraction from transactional documents that combines large language models with multi-level symbolic validation to improve extraction quality. The approach uses three validation layers—syntactic (JSON parsing), task-level (OCR grounding), and domain-level (arithmetic constraints)—to filter and refine LLM-generated outputs. By enforcing schema-based coherence and domain-specific arithmetic relationships, the system significantly improves precision over baseline zero-shot extraction and enables the generation of high-quality training data for smaller models through knowledge distillation.

## Method Summary
The method uses an LLM to generate structured JSON extractions from transactional documents, which are then validated through three hierarchical constraint filters. First, syntactic validation ensures valid JSON structure; second, task validation checks that extracted values appear verbatim in the OCR text to prevent hallucinations; third, domain validation verifies arithmetic relationships defined in a schema (e.g., sum of line items equals total with 0.5% tolerance). The system distinguishes between explicit (raw text) and implicit (resolved) outputs, allowing inference of missing numerical fields. For knowledge distillation, domain-validated predictions from a large teacher model serve as high-quality "silver labels" for fine-tuning smaller student models using LoRA.

## Key Results
- Domain validation filtering increases F1-scores by 2-10 percentage points across different model families (e.g., Ministral-8B on CORD improves from 60.0 to 69.3)
- Knowledge distillation using domain-filtered predictions yields higher F1-scores (77.4 vs 69.7 for Ministral-8B on CORD) than fine-tuning on unfiltered predictions
- The framework achieves consistent precision improvements while reducing hallucination rates through multi-level symbolic validation
- Experimental results demonstrate effectiveness on curated transactional datasets with 53-field schemas

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Constraint Filtering
The framework improves extraction precision by successively pruning candidates that violate structural or logical rules. An LLM generates candidate JSON extractions that pass through three validation stages: syntactic validation checks JSON parseability, task validation ensures values exist verbatim in OCR text (grounding), and domain validation verifies arithmetic relationships (e.g., sum(line_items) == total). This assumes invalid extractions are more likely to violate explicit schema or arithmetic rules than valid ones. Evidence shows filtering increases F1-scores while reducing document retention rates, though it may fail if hallucinations coincidentally satisfy constraints or OCR noise prevents verbatim matching.

### Mechanism 2: Arithmetic Inference and Resolution
Performance is maintained even with partial extractions because the system can infer missing numerical fields using defined arithmetic relationships. The schema distinguishes between explicit (raw text) and implicit (resolved) outputs, allowing inference when an LLM extracts line items but misses the total. Evaluation is performed on this resolved implicit object, assuming domain constraints hold for most documents. While this mechanism is supported by the ability to resolve missing values, it may fail if documents contain non-standard logic that breaks assumed arithmetic equality.

### Mechanism 3: Constraint-Filtered Knowledge Distillation
High-quality training data for smaller models is generated by using the validation pipeline to filter outputs of a larger teacher model. A large model processes documents in zero-shot mode, and only predictions passing strict domain validation are kept as "silver labels" for fine-tuning a smaller student model. This assumes the signal-to-noise ratio of filtered data is more important than total volume. Experimental results demonstrate fine-tuning on filtered predictions yields higher F1-scores than unfiltered data, though it may fail if teacher model valid outputs exhibit systematic bias.

## Foundational Learning

- **Concept:** JSON Schema & Structured Generation
  - **Why needed here:** The framework relies on LLM generating valid JSON conforming to specific schema before validation can occur
  - **Quick check question:** Can you explain why prompting an LLM to output JSON requires post-processing to handle "extraneous explanatory text"?

- **Concept:** OCR Grounding (Task-Level Validation)
  - **Why needed here:** A core mechanism prevents hallucinations by verifying extracted strings exist in source OCR
  - **Quick check question:** If OCR engine misreads "100.00" as "100.OO", should task validation pass if LLM corrects it to "100.00"? (Hint: See Section 3.2)

- **Concept:** Micro F1-Score vs. Tree Edit Distance
  - **Why needed here:** Paper uses specific metrics to evaluate performance on nested list structures (line items), differing from standard flat classification metrics
  - **Quick check question:** Why might standard F1-score be insufficient for evaluating order and grouping of line items in a receipt?

## Architecture Onboarding

- **Component map:** Document Image + OCR Text -> LLM Generator + Prompt -> JSON Parser -> Validator Chain (Syntactic -> Task -> Domain) -> High-quality "Silver" Labels
- **Critical path:** Domain Validator is bottleneck requiring complete schema mapping all arithmetic relationships; buggy logic causes valid documents to be discarded
- **Design tradeoffs:** Precision vs. recall (validation improves precision but reduces recall to 25%), strictness vs. flexibility (parser struggles with number formatting causing edge case failures)
- **Failure signatures:** 0% retention indicates schema mismatch with document structure, valid but wrong extractions occur when LLM satisfies arithmetic with wrong semantic fields, parsing loops occur with circular dependencies in schema constraints
- **First 3 experiments:** 1) Schema Stress Test: validate pipeline against ground truth labels to verify 95%+ constraint satisfaction, 2) Ablation on Validation Layers: measure F1-score drop removing task-level validation to quantify grounding value, 3) Distillation Baseline: fine-tune small model on unfiltered outputs vs. filtered outputs to replicate distillation gain

## Open Questions the Paper Calls Out

- **Can domain-specific schemas and validation constraints be generated automatically rather than manually defined?** Current approach requires manual schema definition limiting scalability to new domains; evidence would be automated schema generation producing valid constraints for new document types with comparable F1-scores.

- **How can domain-invariant constraints be developed to enable validation across diverse document types without task-specific schema design?** Current constraints are tightly coupled to transactional document arithmetic limiting transferability; evidence would be validation constraints improving F1-scores across multiple unrelated document domains without modification.

- **How can the single tax rate limitation in current schema be extended to handle complex multi-tax structures?** Current schema only allows single tax rate besides non-taxable amounts limiting real-world applicability; evidence would be extended schema handling multiple concurrent tax rates while maintaining arithmetic coherence.

- **Can LLM-based parsing methods outperform programmatic parsing for handling variable number formatting conventions?** Current programmatic parsing achieves >95% success but still fails on formatting edge cases; evidence would be comparative experiments showing parsing accuracy and computational cost of LLM-based vs. programmatic number parsing.

## Limitations

- The framework's effectiveness depends heavily on accuracy and completeness of arithmetic schema, with 95%+ constraint satisfaction rate difficult to verify without full relabeled datasets and exact schema definitions
- Task-level validation (OCR grounding) is brittle in practice, with minor OCR errors or formatting variations potentially causing valid extractions to be rejected
- Claim of "silver labels" being high-quality enough for effective distillation is not independently verified, lacking direct comparison with unfiltered data and error analysis on filtered subset

## Confidence

- **High Confidence**: General architecture (LLM + validation layers) and distillation workflow are well-defined with clear experimental results showing plausible F1-score improvements
- **Medium Confidence**: Arithmetic inference mechanism is logically sound but effectiveness depends on assumption that documents strictly follow schema's arithmetic rules
- **Low Confidence**: Claim of high-quality "silver labels" for distillation lacks independent verification through direct comparisons and error analysis

## Next Checks

1. **Schema Stress Test**: Run validation pipeline on ground truth labels of CORDTD/SROIETD datasets to confirm reported 95%+ constraint satisfaction rate and identify schema gaps

2. **Ablation on Validation Layers**: Measure F1-score and document retention rate when removing task-level (OCR grounding) validation to quantify specific contribution of grounding versus domain logic

3. **Distillation Baseline**: Fine-tune small model (e.g., Ministral-8B) on unfiltered zero-shot outputs of large model, compare performance against model fine-tuned on filtered outputs to independently verify distillation gain