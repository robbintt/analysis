---
ver: rpa2
title: How good is PAC-Bayes at explaining generalisation?
arxiv_id: '2503.08231'
source_url: https://arxiv.org/abs/2503.08231
tags:
- prior
- pac-bayes
- generalisation
- risk
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates necessary conditions for PAC-Bayes bounds
  to provide meaningful generalization guarantees. The authors show that the optimal
  PAC-Bayes generalization guarantee depends solely on the distribution of risk induced
  by the prior distribution.
---

# How good is PAC-Bayes at explaining generalisation?

## Quick Facts
- **arXiv ID:** 2503.08231
- **Source URL:** https://arxiv.org/abs/2503.08231
- **Reference count:** 35
- **Primary result:** Optimal PAC-Bayes generalization guarantees depend solely on prior risk distribution, requiring priors to place extremely small probabilities on high-performing predictors.

## Executive Summary
This paper investigates necessary conditions for PAC-Bayes bounds to provide meaningful generalization guarantees. The authors show that the optimal PAC-Bayes generalization guarantee depends solely on the distribution of risk induced by the prior distribution. They prove that achieving a target generalization level is only possible if the prior places sufficient mass on high-performing predictors. For a wide class of PAC-Bayes bounds, the authors establish that optimizing over the posterior distribution reduces to optimizing over the push-forward measure of the prior on the empirical risk.

## Method Summary
The paper derives necessary conditions on PAC-Bayes prior distributions using Catoni's bound. The key method involves: (1) computing Bmin_D,b(r,q) for Catoni's bound via a closed-form expression, (2) inverting this via a quantile requirement function QCat,λ(r,G), and (3) minimizing over temperature λ to get temperature-free QCat. The analysis applies to the class B(D,b) of PAC-Bayes bounds satisfying convexity assumptions. The main contribution is a framework to derive quantitative necessary conditions on the prior distribution, providing closed-form expressions for minimum prior mass required on low-risk predictors.

## Key Results
- Optimal generalization guarantee achievable by PAC-Bayes bounds depends solely on prior risk distribution, not predictor space geometry
- To achieve 1.5% error on MNIST, prior must place ≥5.3×10⁻¹¹ mass on predictors with <1.65% error
- Data-dependent priors often yield bounds that merely replicate test bounds, acting as "witnesses" rather than explanations
- The posterior optimization reduces to 1D optimization over risk values for a wide class of bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal generalization guarantee achievable by a class of PAC-Bayes bounds depends solely on the distribution of empirical risk induced by the prior, not the geometry of the predictor space.
- **Mechanism:** For bounds of class $B(D,b)$ satisfying convexity assumptions, the posterior optimization reduces from a search over the high-dimensional predictor space $\Gamma$ to a 1-dimensional optimization over the reals (risk values). This implies the "divergence" penalty (e.g., KL) behaves as if acting on a 1D space, explaining why it doesn't necessarily scale with parameter count.
- **Core assumption:** The PAC-Bayes bound belongs to the functional class $B(D,b)$ (which includes Catoni's and Maurer's bounds) and satisfies Assumption A2 (convexity of $D$ and monotonicity of $b$).
- **Evidence anchors:** [Abstract] "Our analysis reveals that the optimal generalisation guarantee depends solely on the distribution of the risk induced by the prior distribution." [Section 4.1, Theorem 1] "The infimum of every B of class B(D, b) is fully determined by the risk prior $\pi_p\#R$."
- **Break condition:** The mechanism fails if the bound includes non-convex divergence terms or dependencies on the predictor space geometry that cannot be expressed as a functional of the risk push-forward.

### Mechanism 2
- **Claim:** To achieve a specific generalization guarantee $G$, the prior must place a strictly defined minimum mass on predictors with risk lower than a threshold $r$.
- **Mechanism:** By inverting the PAC-Bayes bound optimization over a "least favorable" distribution (scaled Bernoulli), the authors derive a quantile requirement. If the prior does not allocate sufficient probability mass to low-risk predictors, the divergence penalty required to shift mass to them destroys the guarantee.
- **Core assumption:** The learning problem fits the $n$-i.i.d. bounded setting, and one aims to minimize bounds like Catoni's.
- **Evidence anchors:** [Section 5] "...to achieve a generalisation guarantee of 1.5% error... the prior must put a mass higher than $5.3 \times 10^{-12}$ on predictors making less than 1.65% error." [Corollary 2] Formalizes the protocol to derive quantile requirements $Q(r, G)$.
- **Break condition:** If the number of classes increases or the dataset size decreases significantly, the required mass on low-risk predictors becomes vanishingly small (e.g., $<10^{-50000}$), making the condition impossible for uninformed priors to meet.

### Mechanism 3
- **Claim:** Data-dependent priors (trained on a data split) often yield PAC-Bayes bounds that merely replicate test bounds, acting as a "witness" to generalization rather than explaining it.
- **Mechanism:** When a prior is trained on a split $S_1$, it must already generalize well to $S_2$ to offer a tight bound. If the prior variance is kept low to tighten the bound, the posterior is forced to match the prior. The resulting guarantee is often nearly identical to simply evaluating the prior on $S_2$ as a test set.
- **Core assumption:** The prior is data-dependent (specifically, data-splitting is used) and the posterior is constrained (e.g., low variance Gaussian).
- **Evidence anchors:** [Section 1] "...these generalisation bounds usually closely match the guarantees that one would obtain by only training the model on the prior dataset, using the posterior dataset as a testing dataset." [Appendix B, Table 1] Shows 13 out of 14 cases where Test Bounds outperform or match PAC-Bayes certificates.
- **Break condition:** The mechanism implies that if the prior is truly data-agnostic, achieving non-vacuous bounds is exceptionally difficult unless the prior has intrinsic knowledge of the data structure (e.g., compression-based priors).

## Foundational Learning

- **Concept: Push-forward Measure ($\pi\#f$)**
  - **Why needed here:** The paper's central thesis revolves around $\pi_p\#R$, the distribution of risk values induced by the prior. Understanding that the bound depends only on this distribution (and not the weights themselves) is critical.
  - **Quick check question:** If I have two different neural network architectures with different weight distributions but the exact same distribution of loss values under the prior, will the optimal PAC-Bayes bound differ? (Answer: No, per Theorem 1).

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** The trade-off between the empirical risk and the "penalty" (KL divergence between posterior and prior) defines the bound. The paper argues this penalty acts surprisingly like a 1D divergence.
  - **Quick check question:** In the context of this paper, does a higher dimensional parameter space necessarily increase the KL divergence term? (Answer: Not necessarily, if the risk push-forward remains similar).

- **Concept: Gibbs Posterior**
  - **Why needed here:** This is identified as the optimal posterior for Catoni's bound. It helps understand how the theoretical minimum bound is achieved by re-weighting predictors based on their risk.
  - **Quick check question:** How does the Gibbs posterior distribute density compared to the prior? (Answer: It shifts density proportionally to $e^{-\lambda^{-1}R}$).

## Architecture Onboarding

- **Component map:** Dataset S -> Prior distribution $\pi_p$ -> Risk Prior (map $R: \Gamma \to \mathbb{R}$) -> Push-forward $\pi_p\#R$ -> Quantile Requirement $Q(r, G)$ -> Generalization Certificate $G$

- **Critical path:** Validating the **Prior Risk Distribution**. Before training a posterior, one must verify that the prior places sufficient mass ($>10^{-11}$ or similar) on predictors with low risk. If the risk prior is "flat" or concentrated on high risk, the architecture (bound) yields vacuous results.

- **Design tradeoffs:**
  - **Data-Dependent vs. Data-Agnostic Priors:** Using data-dependent priors (splitting data) yields tighter bounds but sacrifices explanatory power (becoming a "witness"). Using data-agnostic priors maintains theoretical purity but requires careful engineering (e.g., compression priors) to meet quantile requirements.
  - **Temperature $\lambda$:** Choosing $\lambda$ in Catoni's bound creates a tradeoff between the influence of the empirical risk and the divergence penalty. The paper identifies a specific "window" for $\lambda$ where requirements are feasible.

- **Failure signatures:**
  - **The "Witness" Failure:** PAC-Bayes bound $\approx$ Test Bound. This indicates the prior did all the work, and the posterior learned nothing new.
  - **The "Vacuous" Failure:** The bound is $>1$ (for classification). This indicates the prior placed insufficient mass on low-risk predictors, causing the penalty term to explode when trying to shift mass.

- **First 3 experiments:**
  1. **Replicate Quantile Calculation:** For a standard dataset (e.g., MNIST), calculate the required prior mass $Q(r, G)$ using Equation (6) for a target error of 1.5%. Verify the $5.3 \times 10^{-11}$ threshold.
  2. **Test vs. PAC-Bayes Comparison:** Train a model on 50% of the data (Prior set). Use the remaining 50% to compute: (a) A standard test bound and (b) A PAC-Bayes bound with the trained model as prior. Compare the values to see if they match (validating the "witness" claim).
  3. **Random Prior Stress Test:** Initialize a prior with random weights and estimate the probability of it achieving $<1.65\%$ error on a subset of data to empirically verify the "uninformed prior" impossibility arguments in Appendix D.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework established in this paper be used to derive sufficient conditions on the prior for achieving a target generalisation guarantee?
- **Basis in paper:** [explicit] The conclusion states: "while we used Corollary 2 to obtain necessary condition on the prior risk to reach a given generalisation target, the same framework could also be applied to obtain sufficient conditions. We leave this as an open direction for further analysis."
- **Why unresolved:** The current work focuses strictly on necessary requirements (what mass the prior *must* have), leaving the sufficiency aspect (what mass *guarantees* success) unexplored.
- **What evidence would resolve it:** A derivation using the paper's stochastic ordering framework that results in inequalities guaranteeing a generalisation level based on specific prior quantile properties.

### Open Question 2
- **Question:** How do the required quantile conditions change when applying this analysis to tighter, non-Catoni PAC-Bayes bounds common in deep learning?
- **Basis in paper:** [explicit] The conclusion notes: "Analysing a tighter bound would lead to yet to be determined, looser quantile requirements. However, the PAC-Bayes bounds used in the publications above are still fundamentally limited by being only sensitive to the prior risk."
- **Why unresolved:** The paper analytically derives requirements only for Catoni's bound; the necessary prior mass for state-of-the-art bounds (e.g., quadratic penalties) remains unknown.
- **What evidence would resolve it:** An analytical or numerical extension of the quantile requirement function $Q(r, G)$ (defined in Corollary 2) for tighter bounds like the Seeger-Langford bound.

### Open Question 3
- **Question:** Do PAC-Bayes bounds that fall outside the class $B(D, b)$, such as those based on Bernstein's inequality, circumvent the dependency solely on the risk push-forward?
- **Basis in paper:** [inferred] Section 4 identifies "Notable exceptions" to the class $B(D, b)$, specifically mentioning "PAC-Bayes bound via Bernstein's concentration inequality," implying the paper's main theorem regarding the irrelevance of predictor space geometry may not apply to them.
- **Why unresolved:** The proof that the optimal bound depends only on the risk distribution relies on Assumption A2 (convexity of $D$), which excludes variance-sensitive bounds.
- **What evidence would resolve it:** A theoretical analysis determining if the minimizer for a Bernstein-type bound depends on the predictor space $\Gamma$ or if it can also be reduced to a functional of the risk push-forward.

## Limitations

- The analysis assumes bounded losses and i.i.d. data, which may not hold in realistic deep learning scenarios with unbounded losses or potential distribution shift.
- The claim that prior risk distribution alone determines generalization relies on convexity assumptions that may break for more complex bounds or non-standard divergences.
- The paper discusses deep learning implications but does not provide experiments showing that real-world deep learning priors fail to meet the derived quantile requirements.

## Confidence

- **High Confidence:** The mathematical framework for deriving quantile requirements (Corollary 2) and the closed-form expressions for Catoni's bound are rigorous and verifiable. The characterization of the optimal PAC-Bayes bound depending on the prior risk push-forward measure (Theorem 1) follows from well-established convex analysis.
- **Medium Confidence:** The interpretation that these results challenge PAC-Bayes as an explanation for deep learning generalization is logically sound but requires empirical validation. The theoretical arguments about data-dependent priors are supported by existing literature but would benefit from additional experiments.
- **Low Confidence:** The specific numerical thresholds (e.g., $5.3 \times 10^{-11}$ for MNIST) depend on precise numerical optimization that isn't fully specified in the paper. Small changes in the optimization procedure could affect these values.

## Next Checks

1. **Empirical Prior Risk Distribution Analysis:** Sample a random prior (e.g., randomly initialized neural network) and estimate the probability it achieves error below 1.65% on a subset of MNIST. Compare this empirical probability with the theoretical requirement of $5.3 \times 10^{-11}$ to validate the impossibility claim.

2. **Data-Dependent Prior Witness Test:** Using the exact experimental setup from Pérez-Ortiz et al. [2021a,b], replicate the comparison between PAC-Bayes bounds and test bounds for at least two datasets. Verify that in most cases, the PAC-Bayes bound closely matches the test bound when using data-dependent priors.

3. **Architecture Transferability Test:** Apply the quantile requirement framework to a non-image dataset (e.g., Spambase or Bioresponse) and verify that the required prior mass on low-risk predictors remains implausibly small for uninformed priors, supporting the broader claim about PAC-Bayes limitations.