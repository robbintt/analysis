---
ver: rpa2
title: 'MASS: MoErging through Adaptive Subspace Selection'
arxiv_id: '2504.05342'
source_url: https://arxiv.org/abs/2504.05342
tags:
- task
- tasks
- merging
- accuracy
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MASS (MoErging through Adaptive Subspace Selection) is a training-free
  method for merging multiple fine-tuned models into a single multitask model while
  preserving near state-of-the-art performance. The approach leverages low-rank task
  updates by storing only the most salient singular components for each task and uses
  a novel projection-based router to dynamically select the most relevant task subspaces
  at inference time.
---

# MASS: MoErging through Adaptive Subspace Selection

## Quick Facts
- arXiv ID: 2504.05342
- Source URL: https://arxiv.org/abs/2504.05342
- Reference count: 40
- Merges multiple fine-tuned CLIP models into a single multitask model with ~98% normalized accuracy

## Executive Summary
MASS introduces a training-free method to merge multiple fine-tuned models into a single multitask model while preserving near state-of-the-art performance. The approach leverages low-rank task updates by storing only the most salient singular components for each task and uses a novel projection-based router to dynamically select the most relevant task subspaces at inference time. This router identifies which subspace best explains an input's intermediate features without requiring task-specific data or additional training. MASS achieves up to 98% of the average accuracy of individual fine-tuned models across 8, 14, and 20 tasks on CLIP-based image classification benchmarks, establishing a new state-of-the-art.

## Method Summary
MASS merges multiple fine-tuned CLIP ViT models by computing per-task deltas (Δi = θft - θpre), applying SVD truncation to extract the most important singular vectors, and storing these low-rank updates. During inference, a projection-based router analyzes intermediate features to determine which task subspaces are most relevant for each input, selecting only those tasks for final classification. The method operates without additional training or task-specific data, introducing only a two-pass inference overhead and approximately 2× storage factor compared to a single pretrained model.

## Key Results
- Achieves ~98% normalized accuracy compared to individual fine-tuned models
- Establishes new state-of-the-art for training-free multitask merging
- Reduces storage cost to ~2× a single pretrained model versus full ensemble storage

## Why This Works (Mechanism)
The method exploits the observation that fine-tuning updates are often low-rank, meaning they can be effectively compressed into a small number of singular components. By selecting only the most salient singular vectors for each task and using a data-free router to dynamically identify relevant subspaces, MASS minimizes interference between tasks while maintaining high accuracy. The projection-based routing mechanism allows the model to adapt to each input's specific needs without requiring task labels during inference.

## Foundational Learning

**Singular Value Decomposition (SVD)**: Matrix factorization into singular values and vectors. Why needed: To compress low-rank task updates efficiently. Quick check: Verify SVD implementation correctly truncates to desired rank.

**Task Interference**: When multiple tasks compete for the same model parameters, degrading performance. Why needed: Understanding why naive merging fails motivates the low-rank approach. Quick check: Measure accuracy drop when merging all tasks without subspace selection.

**Projection Residuals**: Distance between original features and their projection onto a subspace. Why needed: Forms the basis of the routing mechanism. Quick check: Verify residuals correctly identify relevant subspaces.

## Architecture Onboarding

**Component Map**: Input -> θMT (merged model) -> Layer ℓ -> Residual Computation -> Router -> Subspace Selection -> θMASS (adaptive model) -> Classification Heads -> Output

**Critical Path**: Inference flow: first pass through merged model → compute residuals at routing layer → softmax over residuals → select top-k tasks → merge selected subspaces → second pass for classification.

**Design Tradeoffs**: Storage vs. accuracy tradeoff in choosing rank k; routing layer selection impacts performance; softmax temperature affects router selectivity. The method prioritizes training-free operation over absolute accuracy maximization.

**Failure Signatures**: Router consistently selecting same tasks (indicates poor residual differentiation); large accuracy drops (suggests insufficient rank or poor orthogonalization); high inference time (indicates inefficient subspace merging).

**First Experiments**: 1) Verify fixed merge reproduces individual task accuracies. 2) Test router selection on single-task inputs. 3) Measure accuracy vs. rank k for different task counts.

## Open Questions the Paper Calls Out

**Open Question 1**: How can an adaptive mechanism be developed to determine the optimal routing layer for a specific input, rather than relying on a globally fixed layer? The paper notes optimal routing layers vary significantly across tasks and encourages further research on adaptive determination methods.

**Open Question 2**: Can the projection-based router be refined to select individual singular vectors across different tasks rather than selecting entire aggregated task subspaces? The conclusion identifies this granular subspace selection as a promising direction.

**Open Question 3**: Can the data-free router identify and compose relevant task subspaces to solve tasks that were not present during the initial merging phase? The paper proposes adapting MASS to out-of-distribution scenarios where singular vectors can be combined on the fly for new unseen tasks.

## Limitations
- Assumes access to fine-tuned checkpoints; retraining may be required for new tasks
- Fixed-merge step assumes low-rank task deltas, which may not hold for all domains
- Performance on tasks outside standard benchmarks is not extensively validated

## Confidence
- **High**: Normalized accuracy (~98%) on standard 8/14/20 task benchmarks
- **Medium**: Task-dependent optimal routing layers and ~2× storage overhead claims
- **Low**: Performance on tasks outside the standard benchmark sets

## Next Checks
1. Verify the fixed-merge and routing components reproduce the stated normalized accuracy (~98%) on the 8/14/20 task sets using publicly available checkpoints.
2. Test the method's robustness by applying it to a set of tasks not in the original benchmarks, checking for accuracy drop and router stability.
3. Measure actual storage usage of the merged model against the claimed ~2× overhead, accounting for all stored components (θMT, Vi matrices, classification heads).