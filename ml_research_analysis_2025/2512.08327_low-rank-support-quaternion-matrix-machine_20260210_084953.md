---
ver: rpa2
title: Low Rank Support Quaternion Matrix Machine
arxiv_id: '2512.08327'
source_url: https://arxiv.org/abs/2512.08327
tags:
- quaternion
- color
- matrix
- lsqmm
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Low-rank Support Quaternion Matrix Machine
  (LSQMM), a novel classification method for color image classification that represents
  RGB channels as pure quaternions to preserve intrinsic coupling relationships. The
  method incorporates a quaternion nuclear norm regularization term to promote low-rank
  structures resulting from strongly correlated color channels.
---

# Low Rank Support Quaternion Matrix Machine

## Quick Facts
- arXiv ID: 2512.08327
- Source URL: https://arxiv.org/abs/2512.08327
- Reference count: 32
- Primary result: LSQMM achieves up to 27.73% higher accuracy and 76.34% higher F1-score than state-of-the-art methods on color image datasets

## Executive Summary
This paper introduces the Low-rank Support Quaternion Matrix Machine (LSQMM), a novel classification method for color image classification that represents RGB channels as pure quaternions to preserve intrinsic coupling relationships. The method incorporates a quaternion nuclear norm regularization term to promote low-rank structures resulting from strongly correlated color channels. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed for optimization. Experimental results on six public color image datasets demonstrate that LSQMM outperforms state-of-the-art methods including LIBSVM, Support Matrix Machine, and Support Tensor Machine.

## Method Summary
LSQMM converts RGB images into pure quaternion matrices by mapping R, G, B channels to i, j, k components respectively. The classification model uses a quaternion weight matrix W optimized through an ADMM algorithm that alternates between solving a dual quadratic programming problem for W and applying a quaternion nuclear norm proximal operator for Z (with W=Z enforced through Lagrange multipliers). The optimization minimizes a loss function regularized by the quaternion nuclear norm to promote low-rank structures while preserving color channel coupling.

## Key Results
- LSQMM achieves up to 27.73% higher accuracy than compared methods on certain datasets
- The method shows 76.34% higher F1-score compared to competitors on specific datasets
- LSQMM demonstrates advantages in classification accuracy, robustness to noise, and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing RGB channels as a pure quaternion matrix preserves inter-channel coupling better than independent or concatenated representations
- Mechanism: Quaternion algebra's non-commutativity creates a mathematical structure where channel operations are intrinsically linked, with each pixel encoded as X_ij = R_ij*i + G_ij*j + B_ij*k
- Core assumption: Discriminative features lie in correlations between RGB channels, disrupted by vectorization or simple tensor concatenation
- Evidence anchors: Abstract states RGB channels are treated as pure quaternions to preserve intrinsic coupling relationships; Section 1 explains this representation inherently preserves spatial structure while maintaining spectral correlations

### Mechanism 2
- Claim: Quaternion nuclear norm regularization promotes low-rank weight matrix capturing structures from correlated color channels
- Mechanism: The nuclear norm (sum of singular values) acts as convex proxy for rank, forcing the model to find weight matrix that can be decomposed into fewer basis vectors
- Core assumption: Decision boundary lies in low-rank subspace of quaternion feature space, with noise being high-rank
- Evidence anchors: Abstract mentions quaternion nuclear norm regularization for promoting low-rank structures; Section 5 conclusion states this enables LSQMM to thoroughly extract discriminative features

### Mechanism 3
- Claim: ADMM-based algorithm decouples non-smooth optimization into tractable subproblems
- Mechanism: The optimization splits into W-update solved as convex QP and Z-update solved via quaternion SVD and soft-thresholding
- Core assumption: Problem is convex and amenable to variable splitting, with convergence relying on real-valued isomorphism
- Evidence anchors: Abstract states ADMM-based iterative algorithm is designed to resolve quaternion optimization model; Section 3.2 details algorithm with Figure 3 showing empirical convergence within ~10 iterations

## Foundational Learning

### Quaternion Algebra
- Why needed: Fundamental data representation where RGB values are encoded as coefficients of i, j, k components
- Quick check: If you multiply a pure quaternion representing a pixel by 'i' from the left, how are the original R, G, and B components mixed in the result?

### Quaternion Singular Value Decomposition (QSVD)
- Why needed: Core operation for low-rank regularizer, required for Z-update step in ADMM algorithm
- Quick check: How does the definition of rank in a quaternion matrix relate to its singular values obtained from QSVD?

### Nuclear Norm Regularization
- Why needed: Explicit mechanism to prevent overfitting and extract features from correlated color channels
- Quick check: Why is the nuclear norm used instead of matrix rank as the regularization penalty?

## Architecture Onboarding

### Component map
Data Preprocessor -> ADMM Solver -> Classifier

### Critical path
The optimization loop in Algorithm 1, particularly the Z-update step requiring correct implementation of quaternion SVD and proximal operator

### Design tradeoffs
Model designed for small-sample, high-dimensional scenarios (N << mn), trading quaternion algebra overhead against preserving channel coupling and low-rank structure

### Failure signatures
Non-convergence if ADMM penalty parameter ρ is poorly chosen; overfitting if λ is too low; underfitting if λ is too high; implementation errors in quaternion multiplication or SVD

### First 3 experiments
1. Unit Test Quaternion Operations: Verify implementation of quaternion matrix multiplication, conjugate transpose, and real-valued isomorphism mapping
2. Validate QSVD: Implement QSVD and test on known low-rank quaternion matrix to check singular value recovery
3. Convergence Test on Synthetic Data: Generate linearly separable, low-rank synthetic dataset in quaternion domain and plot objective function value and residuals over iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LSQMM be extended to quaternion tensor framework for handling color videos?
- Basis: Conclusion states extending to quaternion tensor framework is worthwhile for higher-dimensional data processing tasks
- Why unresolved: Current formulation designed strictly for 2D quaternion matrices, whereas video data introduces temporal dimension requiring tensor algebra
- What evidence would resolve it: Derivation of quaternion tensor optimization model with experiments on video datasets

### Open Question 2
- Question: Can sparsity regularization terms enhance feature selection capability and robustness?
- Basis: Conclusion suggests introducing sparsity regularization terms to enhance feature selection and robustness
- Why unresolved: Current model relies on Frobenius and nuclear norms but doesn't explicitly enforce element-wise sparsity in weight matrix
- What evidence would resolve it: Modified loss function including L1 or L2,1 norms showing improved accuracy or interpretability

### Open Question 3
- Question: Can stochastic or distributional schemes design more efficient QSVD algorithms to reduce computational complexity?
- Basis: Conclusion proposes designing more efficient quaternion SVD algorithms using stochastic or distributional schemes
- Why unresolved: Standard QSVD computation is bottleneck for large-scale images with O(mn max(m,n)) complexity
- What evidence would resolve it: Stochastic ADMM variant approximating SVD step demonstrating reduced runtime without accuracy loss

## Limitations

- The quaternion nuclear norm regularization's effectiveness lacks direct empirical validation through ablation studies
- ADMM convergence proof relies on real-valued isomorphism that may introduce numerical instability in practice
- No comparison against modern deep learning approaches for color image classification provided

## Confidence

- **High confidence**: Core mathematical framework (quaternion representation, ADMM algorithm structure, QSVD computation) is well-defined and reproducible
- **Medium confidence**: Performance claims supported by six datasets but lack ablation studies and modern baseline comparisons
- **Low confidence**: Claims about quaternion nuclear norm specifically capturing low-rank structures from correlated channels lack direct empirical validation

## Next Checks

1. **Ablation Study**: Implement LSQMM without quaternion nuclear norm regularization (set λ=0) and compare classification performance across all six datasets
2. **Modern Baseline Comparison**: Implement simple CNN architecture (e.g., ResNet-18) for same classification tasks to compare against quaternion approach
3. **Channel Independence Test**: Design experiment using datasets with artificially decorrelated color channels to test LSQMM performance degradation