---
ver: rpa2
title: 'BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics'
arxiv_id: '2601.00366'
source_url: https://arxiv.org/abs/2601.00366
tags:
- bepa
- language
- training
- space
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BERT-JEPA (BEPA), a novel training framework
  that enhances BERT-style models by adding a Joint-Embedding Predictive Architecture
  (JEPA) objective to combat [CLS] embedding collapse and create language-agnostic
  semantic representations. The method combines standard BERT masked language modeling
  with cross-lingual [CLS] token alignment using InfoNCE loss.
---

# BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics

## Quick Facts
- arXiv ID: 2601.00366
- Source URL: https://arxiv.org/abs/2601.00366
- Authors: Taj Gillin; Adam Lalani; Kenneth Zhang; Marcel Mateos Salles
- Reference count: 28
- Primary result: BEPA improves multilingual transfer (+1.6% XNLI, +1.3 F1 MLQA) with minimal English performance loss

## Executive Summary
This paper introduces BERT-JEPA (BEPA), a novel training framework that enhances BERT-style models by adding a Joint-Embedding Predictive Architecture (JEPA) objective to combat [CLS] embedding collapse and create language-agnostic semantic representations. The method combines standard BERT masked language modeling with cross-lingual [CLS] token alignment using InfoNCE loss. BEPA reorganizes the embedding space from low-rank to fuller-rank, enabling better semantic separation between related and unrelated sentence pairs across languages. Experimental results show that BEPA maintains strong English performance while improving multilingual transfer across XNLI (+1.6% average accuracy), MLQA (+1.3 F1 score), and GLUE benchmarks with minimal English performance loss.

## Method Summary
BEPA extends standard BERT pretraining by adding a Joint-Embedding Predictive Architecture (JEPA) objective alongside masked language modeling. The method trains BERT models on parallel sentence pairs across languages, using the [CLS] token embeddings as semantic representations. During training, BEPA applies InfoNCE loss to align [CLS] embeddings of parallel sentences while pushing apart embeddings of unrelated sentence pairs. This dual objective reorganizes the embedding space from a low-rank structure (where related sentences are close but unrelated sentences are also close) to a fuller-rank structure that better separates semantically related from unrelated pairs across languages. The approach maintains the standard BERT MLM objective while adding cross-lingual alignment through negative sampling of [CLS] embeddings.

## Key Results
- BEPA improves multilingual transfer on XNLI benchmark by +1.6% average accuracy across all languages
- BEPA achieves +1.3 F1 score improvement on MLQA multilingual question answering
- English performance on GLUE benchmark remains stable with only minimal degradation compared to standard BERT

## Why This Works (Mechanism)
The paper addresses the fundamental problem of [CLS] embedding collapse in multilingual BERT models, where the embedding space becomes dominated by a low-rank structure that fails to adequately separate unrelated sentence pairs across languages. Standard BERT pretraining on monolingual corpora creates [CLS] embeddings that capture semantics within each language but don't establish consistent semantic relationships across languages. BEPA's JEPA objective forces the model to learn language-agnostic semantic representations by aligning [CLS] embeddings of parallel sentences while using negative sampling to push apart embeddings of unrelated pairs. This reorganization from low-rank to fuller-rank embedding space enables the model to maintain strong semantic relationships within languages while establishing consistent cross-lingual semantic mappings.

## Foundational Learning

**Joint-Embedding Predictive Architecture (JEPA)**: A framework that learns to predict one embedding from another while maintaining semantic consistency across modalities or languages. Why needed: Enables cross-modal/language semantic alignment. Quick check: Verify that parallel sentence pairs have aligned [CLS] embeddings after JEPA training.

**InfoNCE Loss**: A contrastive loss function that pulls together positive pairs while pushing apart negative samples in the embedding space. Why needed: Provides effective negative sampling for learning discriminative embeddings. Quick check: Monitor loss values to ensure positive pairs converge while negatives remain separated.

**[CLS] Embedding Collapse**: The phenomenon where [CLS] token representations become dominated by low-rank structure, failing to adequately separate unrelated sentence pairs. Why needed: Understanding this problem motivates the need for BEPA. Quick check: Analyze singular value decomposition of [CLS] embedding matrix to identify rank deficiency.

**Negative Sampling in Embedding Space**: The technique of using unrelated samples as negatives to learn discriminative representations. Why needed: Essential for creating separation between semantically unrelated pairs. Quick check: Verify that unrelated sentence pairs have larger distances in embedding space after training.

## Architecture Onboarding

**Component Map**: BERT MLM pretraining -> [CLS] embedding extraction -> JEPA InfoNCE loss computation -> Parameter updates

**Critical Path**: Input sentence pairs → BERT encoder → [CLS] token embeddings → InfoNCE loss computation → Gradient updates

**Design Tradeoffs**: The JEPA objective adds computational overhead but provides multilingual benefits; batch size affects negative sampling quality; temperature parameter in InfoNCE loss controls separation strength.

**Failure Signatures**: Poor multilingual transfer despite strong monolingual performance; [CLS] embeddings of parallel sentences fail to align; training instability due to improper temperature settings or insufficient negative samples.

**First Experiments**:
1. Verify that parallel sentence pairs have aligned [CLS] embeddings after JEPA training using similarity metrics
2. Compare [CLS] embedding rank before and after BEPA training using singular value decomposition
3. Test multilingual transfer on XNLI with varying negative sampling ratios to find optimal hyperparameter settings

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger multilingual models remains untested, creating uncertainty about computational overhead
- Theoretical explanations of embedding space reorganization lack rigorous mathematical analysis
- Limited ablation studies on optimal hyperparameter settings (batch size, negative sampling ratios, temperature parameters)

## Confidence

**High confidence**: Core experimental results showing consistent improvements across XNLI and MLQA benchmarks are well-supported by presented data.

**Medium confidence**: Claims about language-agnostic semantic representations are supported but could benefit from additional qualitative analysis and embedding space visualization.

**Low confidence**: Theoretical explanations of low-rank to fuller-rank embedding space transformation are intuitive but lack formal mathematical formulation.

## Next Checks

1. Conduct scaling experiments to evaluate BEPA's effectiveness on larger multilingual models (BERT-large, mBERT variants) and measure computational overhead across different hardware configurations.

2. Perform ablation studies varying negative sampling ratio, batch size, and temperature parameter in InfoNCE loss to identify optimal hyperparameters for different multilingual scenarios.

3. Implement additional qualitative analysis using t-SNE or UMAP visualizations of [CLS] embeddings before and after BEPA training to empirically demonstrate claimed space reorganization and language-agnostic properties.