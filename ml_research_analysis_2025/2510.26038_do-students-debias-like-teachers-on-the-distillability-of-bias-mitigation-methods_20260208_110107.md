---
ver: rpa2
title: Do Students Debias Like Teachers? On the Distillability of Bias Mitigation
  Methods
arxiv_id: '2510.26038'
source_url: https://arxiv.org/abs/2510.26038
tags:
- teacher
- student
- debiasing
- knowledge
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effect of knowledge distillation on
  transferring debiasing capabilities between models. Through extensive experiments
  on natural language inference and image classification tasks, the study reveals
  that knowledge distillation generally undermines debiasing performance, with students
  becoming more biased than their teachers.
---

# Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods

## Quick Facts
- arXiv ID: 2510.26038
- Source URL: https://arxiv.org/abs/2510.26038
- Reference count: 40
- Primary result: Knowledge distillation generally undermines debiasing performance, with students becoming more biased than teachers

## Executive Summary
This paper investigates how knowledge distillation affects the transfer of debiasing capabilities between models. Through extensive experiments on natural language inference and image classification tasks, the study reveals that students typically become more biased than their teachers when distilled, especially when there's a significant scale mismatch. The research identifies specific internal mechanisms causing this degradation, including divergence in attention patterns and circuit structures, and proposes three solutions: data augmentation, iterative knowledge distillation, and initializing student models with teacher weights.

## Method Summary
The paper conducts experiments across text and image domains using BERT, T5, ResNet, and ViT backbones in five scales. Teachers are trained with various debiasing methods (ERM, PoE variants, KernelWhitening, etc.) then distilled to students using standard logit-based knowledge distillation. The primary metric is the spurious gap (ID accuracy - OOD accuracy). The study analyzes internal mechanisms using Centered Kernel Alignment (CKA) for activation patterns and Edge Attribution Patching (EAP) for circuits. Three mitigation strategies are evaluated: data augmentation (DA), iterative knowledge distillation (IKD), and weight initialization (Init).

## Key Results
- Students become more biased than teachers, with larger spurious gaps post-distillation
- Scale mismatch between teacher and student significantly impacts debiasing transfer quality
- Internal mechanisms show divergence in attention patterns and circuit structures during distillation
- Proposed solutions (DA, IKD, Init) can partially restore debiasing capabilities
- Students often show higher confidence on OOD data despite lower accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KD successfully transfers ID features but fails to transfer OOD robustness due to representational divergence in deeper layers
- **Mechanism:** CKA shows student-teacher alignment in early layers for both ID and OOD data, but alignment breaks down in mid-to-late layers specifically for OOD samples
- **Core assumption:** CKA similarity correlates directly with transfer of functional debiasing capabilities
- **Evidence anchors:** Section 5.1 shows CKA similarity breaks down for OOD in deeper layers; Figure 7 compares ID vs OOD CKA patterns
- **Break condition:** Increasing student capacity or augmenting training data may decrease this divergence

### Mechanism 2
- **Claim:** KD fundamentally alters the internal "circuit" used for reasoning, shifting reliance from attention heads to MLP layers
- **Mechanism:** Circuit discovery reveals teachers use attention heads with moderate positive effects for debiasing; post-distillation, students invert this pattern with negative attention effects and MLP burden
- **Core assumption:** Direction and magnitude of causal effects in circuits indicate distinct reasoning strategies
- **Evidence anchors:** Section 5.2 shows attention effects become negative after KD; Figure 8 provides visual comparison of circuit effects
- **Break condition:** Explicit attention transfer might prevent this circuit shift

### Mechanism 3
- **Claim:** Scale mismatch creates optimization conflict where student overfits to teacher's residual biases
- **Mechanism:** When teacher is significantly larger than student, the student lacks capacity to replicate fine-grained decision boundaries and latches onto spurious correlations in teacher's softened probabilities
- **Core assumption:** Failure is due to capacity constraints relative to complexity of debiasing logic
- **Evidence anchors:** Section 4.1 states larger teachers don't guarantee more robust students; Abstract notes scale differences especially impact transfer
- **Break condition:** Iterative KD smooths capacity gap and mitigates this failure

## Foundational Learning

- **Concept: Spurious Gap (Spu. Gap)**
  - **Why needed here:** Primary metric for "bias" measuring performance difference between ID and OOD data
  - **Quick check question:** If a model has 99% ID accuracy and 60% OOD accuracy, is it "debiasing" effectively? (Answer: No, large Spu. Gap indicates high reliance on spurious correlations)

- **Concept: Logit-based vs. Feature-based Distillation**
  - **Why needed here:** Paper focuses on standard logit-based KD; understanding this distinguishes why internal circuits might diverge
  - **Quick check question:** Does matching teacher's output logits guarantee student uses same internal features? (Answer: No, paper proves they often use different circuits/attention patterns)

- **Concept: Centered Kernel Alignment (CKA)**
  - **Why needed here:** Diagnostic tool used to prove mechanism of failure by quantifying neural network representation similarity
  - **Quick check question:** If CKA is high in Layer 1 but low in Layer 12 for OOD data, where is robustness failure happening? (Answer: In deeper layers)

## Architecture Onboarding

- **Component map:** Teacher ($f_T$) -> Debiasing Method ($M$) -> Student ($g_{T \to S}$) via logit-based distillation
- **Critical path:** Train Teacher with debiasing method $M$ -> Distill to Student via logit matching -> Evaluate Spurious Gap degradation -> Apply mitigation (DA, IKD, or Init)
- **Design tradeoffs:**
  - One-shot KD vs. Iterative KD: One-shot is faster but loses debiasing quality; Iterative preserves robustness but requires training $N$ intermediate models
  - Data Augmentation: Most effective for robustness but requires data generation overhead
  - Weight Init: Cheapest to implement but offers smallest performance gain
- **Failure signatures:**
  - Confident but Wrong: Students produce higher confidence predictions on OOD data than teachers with lower accuracy
  - Agreement Drop: Prediction agreement between Teacher and Student drops specifically on OOD samples
  - Circuit Inversion: Diagnostic tools show students suppressing attention heads used by teachers
- **First 3 experiments:**
  1. Baseline Sanity Check: Train teacher with debiasing method, distill to tiny student, verify Spu. Gap increases
  2. Scale Sensitivity: Compare distilling from Large to Base vs. Base to Tiny student to confirm scale mismatch impact
  3. Intervention Test: Implement Iterative KD using same setup as Experiment 1, report if Spu. Gap narrows

## Open Questions the Paper Calls Out

- Do feature-based or attention-based KD methods preserve debiasing capabilities more effectively than logit-based distillation? (The paper only explored logit-based KD)
- Can involving multiple teachers in distillation process mitigate bias transfer issues found in single-teacher setups? (The paper didn't explore ensemble teachers)
- Does self-distillation allow models to iteratively improve debiasing capabilities without relying on external teachers? (Proposed as future work)

## Limitations
- Results focus primarily on logit-based knowledge distillation without exploring feature-based or attention-based alternatives
- Most debiasing methods are from prior work, limiting verification of generalizability
- Mechanistic explanations remain correlational without establishing direct causality between representation changes and debiasing failures

## Confidence
- **High confidence:** Core empirical finding that KD typically degrades debiasing performance (Section 4, Tables 1-2)
- **Medium confidence:** Proposed mitigation strategies show consistent improvements but with varying effect sizes
- **Low confidence:** Mechanistic explanations linking CKA divergence and circuit changes to debiasing failures

## Next Checks
1. Ablation study on KD hyperparameters: Systematically vary temperature and weighting to test failure persistence
2. Attention transfer experiment: Test whether constraining student attention patterns to match teacher prevents circuit inversion
3. Capacity-controlled distillation: Train students with varying capacities to measure relationship between capacity gaps and debiasing transfer quality