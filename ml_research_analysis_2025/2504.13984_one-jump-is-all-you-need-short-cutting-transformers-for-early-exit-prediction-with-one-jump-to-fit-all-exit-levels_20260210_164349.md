---
ver: rpa2
title: 'One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction
  with One Jump to Fit All Exit Levels'
arxiv_id: '2504.13984'
source_url: https://arxiv.org/abs/2504.13984
tags:
- shortcut
- transformer
- inference
- jump
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of reducing computational costs
  during inference in large language models by minimizing the number of low-rank shortcut
  parameters required for early-exit prediction. The proposed method, One-Jump-Fits-All
  (OJFA), selects a single low-rank shortcut jump that can be reused across all transformer
  block-levels during inference, achieving over 30x reduction in shortcut parameter
  costs compared to maintaining separate jumps for each level.
---

# One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels

## Quick Facts
- arXiv ID: 2504.13984
- Source URL: https://arxiv.org/abs/2504.13984
- Authors: Amrit Diggavi Seshadri
- Reference count: 2
- Key outcome: Achieves over 30x reduction in shortcut parameter costs while largely matching performance of full multi-jump approaches across GPT2-XL, Phi3-Mini, and Llama2-7B models

## Executive Summary
This work addresses the problem of reducing computational costs during inference in large language models by minimizing the number of low-rank shortcut parameters required for early-exit prediction. The proposed method, One-Jump-Fits-All (OJFA), selects a single low-rank shortcut jump that can be reused across all transformer block-levels during inference. Despite extreme parameter reduction, OJFA largely matches the performance of the full multi-jump approach across all early-exit levels for GPT2-XL, Phi3-Mini, and Llama2-7B models, outperforming arbitrary choice of shortcuts and identity shortcuts while providing stable precision across all exit levels.

## Method Summary
The method trains separate low-rank shortcuts (BatchNorm → A → B with bottleneck of ~1% hidden dimension) for each transformer block-level to approximate final representations from intermediate states. A novel Signed Sensitive Cosine Similarity metric is then used to select the single most generalizable shortcut across all levels. This chosen shortcut (OJFA) is then reused at every early-exit level during inference, reducing parameters by over 30x compared to maintaining separate jumps for each level while maintaining comparable precision and surprisal scores.

## Key Results
- 30x+ reduction in shortcut parameters while largely matching full multi-jump performance
- OJFA outperforms both arbitrary shortcut selection and jointly-trained shortcuts across most exit levels
- Consistent precision and surprisal scores across all early-exit levels (0-31) for GPT2-XL
- Single shortcut trained on one level generalizes better than shortcut trained on all levels simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Shortcut Projection
Intermediate transformer hidden states can be linearly projected to approximate final-layer representations using two low-rank matrices with a bottleneck of ~1% of hidden dimension. Apply BatchNorm normalization, then multiply by A (H × ⌊H/100⌋) and B (⌊H/100⌋ × H). The bottleneck forces compression to essential directional information while discarding block-specific noise. MSE loss trains parameters to match true final representations.

### Mechanism 2: Signed Sensitive Cosine Similarity Selection
Selecting the shortcut jump that maximizes signed-squared cosine similarity across all exit levels yields a single shortcut that generalizes better than arbitrary choices or jointly-trained shortcuts. For each candidate jump m, compute cosine similarity C between its predictions and true finals across ALL exit levels k. Apply sgn(C)·C² to amplify correct/incorrect extremes and suppress near-orthogonal predictions. Sum over all levels and data points to get Dm. Select m with highest Dm.

### Mechanism 3: Single-Level Training Beats Joint Training
Training a shortcut on representations from ONE exit level, then reusing it everywhere, outperforms training a single shortcut on inputs from ALL levels. Joint training presents heterogeneous inputs with varying noise patterns, making optimization noisy. Single-level training allows the shortcut to learn clean signal/noise separation for one representation distribution; this learned separation transfers because the "structure of noise" is more identifiable in homogeneous inputs.

## Foundational Learning

- Concept: **Early-exit inference in transformers**
  - Why needed here: The entire method assumes you understand that transformers normally process through ALL layers before prediction, and early-exit aims to stop early when confidence is sufficient.
  - Quick check question: Can you explain why early-exit requires mapping intermediate representations to the final representation space?

- Concept: **Low-rank matrix factorization**
  - Why needed here: The shortcuts use A·B where inner dimension is ~1% of hidden size. Understanding compression/bottleneck effects is essential.
  - Quick check question: If hidden dimension is 4096 and rank is 40, how many parameters does the full shortcut (A+B) require vs. a full H×H matrix?

- Concept: **Cosine similarity for representation alignment**
  - Why needed here: The selection criterion relies on cosine similarity as a measure of representational correctness, not MSE.
  - Quick check question: Why might cosine similarity be preferred over MSE for selecting a shortcut that must work across layers with different magnitude scales?

## Architecture Onboarding

- Component map:
Input sentence → Transformer forward pass → Extract h_k at block k
                                           ↓
                           [OJFA Shortcut: BatchNorm_m → A_m → B_m]
                                           ↓
                           Approximated final representation ĥ_k^K
                                           ↓
                           Model head → Prediction

- Critical path:
  1. **Training phase**: For each exit level k, train separate shortcuts (A_k, B_k, BatchNorm_k) using MSE loss against true finals
  2. **Selection phase**: Compute D_m for each trained shortcut m using signed-sensitive cosine similarity across ALL levels
  3. **Inference phase**: Discard all shortcuts except the OJFA choice (highest D_m); apply this single shortcut at any early-exit level

- Design tradeoffs:
  - Parameter cost vs. precision: OJFA gives 30x+ reduction but shows slight precision drops at some exit levels (acknowledged in Limitations)
  - Selection metric: Signed-squared cosine emphasizes extremes; alternative metrics (pure cosine, MSE-based) not explored
  - Training data: Uses 9K Wikipedia sentences; sensitivity to domain/data size not tested

- Failure signatures:
  - Early exits (blocks 0-10) may show low precision (<40%) regardless of shortcut choice—this is a known limitation
  - Late exits (blocks near final) may underperform identity shortcuts—low-rank shortcuts currently lose to identity at late blocks
  - If OJFA selection picks from very early blocks, cross-level transfer degrades (selected jumps are 16, 26, 27 out of 32-48 blocks)

- First 3 experiments:
  1. **Reproduce OJFA selection on GPT2-XL**: Train per-level shortcuts on 9K Wikipedia samples, compute D_m scores, verify selection matches paper (jump 26) or understand deviation.
  2. **Ablate selection metric**: Compare signed-squared cosine vs. pure cosine vs. MSE for selecting the reusable jump; measure precision/surprisal gap.
  3. **Test domain transfer**: Train shortcuts on Wikipedia, evaluate OJFA on different domains (code, medical text) to assess whether the single shortcut captures general structure or overfits to training distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the precision of low-rank shortcutting methods be improved to exceed 60% for hard early exits at any transformer block-level?
- Basis in paper: [explicit] The authors state in the Limitations section that "neither our method nor the previous methods are able to achieve over 60% precision for hard early exits at any block-level for any transformer model in this parameter-efficient setting"
- Why unresolved: Low-rank shortcutting remains a nascent field with fundamental performance limitations at early exit levels
- What evidence would resolve it: Development of enhanced shortcut mechanisms that consistently break the 60% precision threshold for early exits across multiple transformer architectures

### Open Question 2
- Question: Can modified low-rank shortcutting techniques be developed to outperform Identity Shortcuts at late-block shortcutting?
- Basis in paper: [explicit] The authors explicitly note this limitation: "nor is low rank-shortcutting able to outperform Identity Shortcuts in late-block shortcutting"
- Why unresolved: The low-rank approximation may lose critical information that becomes more important at later transformer blocks
- What evidence would resolve it: Novel shortcut architectures demonstrating superior performance compared to identity shortcuts at later transformer blocks

### Open Question 3
- Question: Why does a single OJFA shortcut trained for one exit level outperform a jointly-trained shortcut specifically designed to handle multiple exit levels?
- Basis in paper: [inferred] The authors observe this counterintuitive result and suggest joint-training creates "a very noisy problem setting," but provide no definitive explanation for why single-level training generalizes better
- Why unresolved: The underlying mechanism enabling single-level shortcuts to generalize effectively across all levels is not well understood
- What evidence would resolve it: Systematic analysis of representation spaces at different exit levels and theoretical explanation of joint-training failure modes

## Limitations

- Early exits (blocks 0-10) show poor precision (<40%) regardless of shortcut choice
- Low-rank shortcuts currently lose to identity shortcuts at late transformer blocks
- Limited evaluation to Wikipedia text and three specific model architectures

## Confidence

**High confidence** (Mechanistic understanding is clear, claims are directly supported by equations and methodology):
- The low-rank projection mechanism (A·B bottleneck) is explicitly defined in equations and matches standard low-rank approximation approaches
- The inference procedure (select one shortcut, apply at all levels) is clearly specified
- The 30x parameter reduction claim is straightforward arithmetic from the rank reduction

**Medium confidence** (Claims are supported but with notable gaps):
- The superiority of OJFA over identity and arbitrary shortcuts is demonstrated empirically but relies on the specific Wikipedia dataset
- The signed-squared cosine selection criterion shows good results but lacks comparison to alternative metrics
- The claim that single-level training outperforms joint training is supported by Figure 2 but lacks theoretical justification

**Low confidence** (Claims are weakly supported or highly dependent on unstated assumptions):
- Generalization to domains beyond Wikipedia (no testing in code, medical, or other specialized text)
- Robustness across different model families and sizes (only three models tested)
- Practical inference speedup (FLOPs and latency not measured despite parameter reduction)

## Next Checks

**1. Selection metric ablation study**: Compare OJFA's signed-squared cosine selection against pure cosine similarity, MSE-based selection, and random selection across all three test models. This validates whether the novel selection metric genuinely captures transferable representational structure or if simpler metrics would suffice.

**2. Cross-domain robustness testing**: Train shortcuts on Wikipedia but evaluate OJFA on code (GitHub), scientific literature (ArXiv), and conversational text (Reddit). Measure precision/surprisal degradation to quantify domain transfer limits and identify whether the single shortcut captures general structure or overfits to Wikipedia's distribution.

**3. Inference cost quantification**: Measure actual inference latency and FLOPs for full N-NJTC, OJFA, and Identity baselines across different exit levels. Compare parameter reduction (30x) against practical speed gains to determine if the theoretical efficiency translates to real-world benefits.