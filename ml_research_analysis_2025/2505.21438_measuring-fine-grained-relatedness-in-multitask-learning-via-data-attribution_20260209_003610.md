---
ver: rpa2
title: Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution
arxiv_id: '2505.21438'
source_url: https://arxiv.org/abs/2505.21438
tags:
- task
- data
- learning
- mtif
- relatedness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of measuring task relatedness
  and mitigating negative transfer in multitask learning. The authors propose MultiTask
  Influence Function (MTIF), a method that adapts influence functions to MTL models
  to quantify how individual training samples from one task affect performance on
  another task.
---

# Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution

## Quick Facts
- arXiv ID: 2505.21438
- Source URL: https://arxiv.org/abs/2505.21438
- Authors: Yiwen Tu; Ziqi Liu; Jiaqi W. Ma; Weijing Tang
- Reference count: 40
- Primary result: MultiTask Influence Function (MTIF) provides fine-grained, instance-level task relatedness measures in MTL, achieving near-perfect correlation with brute-force leave-one-out retraining and improving MTL accuracy by 3-4% over baselines through data selection.

## Executive Summary
This paper addresses the challenge of measuring task relatedness and mitigating negative transfer in multitask learning. The authors propose MultiTask Influence Function (MTIF), a method that adapts influence functions to MTL models to quantify how individual training samples from one task affect performance on another task. MTIF provides fine-grained, instance-level relatedness measures beyond conventional entire-task level approaches. The method demonstrates strong approximation quality, achieving near-perfect correlation with brute-force leave-one-out retraining on both synthetic and HAR datasets. When applied to improve MTL performance through data selection, MTIF consistently outperforms state-of-the-art MTL methods across standard benchmarks (CelebA, Office-31, Office-Home), achieving accuracy improvements of up to 3-4% over baselines.

## Method Summary
MTIF extends single-task influence functions to multitask learning by introducing per-sample weights into the MTL objective and using first-order Taylor expansion to estimate how validation loss changes when these weights transition from 1→0. The method computes a block-structured Hessian that decomposes influence into within-task, between-task, and shared influence components. For data selection, MTIF aggregates influences across target tasks, ranks samples, removes a tuned fraction of worst samples, and retrains. The approach uses TRAK tricks (dimension reduction, ensemble, sparsification) for scalability to neural networks. MTIF handles both hard parameter sharing (explicit shared parameters) and soft parameter sharing (regularization between task-specific parameters).

## Key Results
- MTIF achieves near-perfect correlation (Spearman ρ > 0.9) with brute-force leave-one-out retraining on synthetic and HAR datasets
- MTIF-guided data selection improves MTL accuracy by 3-4% over baselines across CelebA, Office-31, and Office-Home benchmarks
- MTIF outperforms state-of-the-art MTL methods including CAGrad, PCGrad, and EW under both clean and label-corrupted conditions
- The method requires only two training passes (initial and final) compared to per-step overhead of competing methods

## Why This Works (Mechanism)

### Mechanism 1: First-Order Leave-One-Out Approximation via Influence Functions
MTIF approximates the effect of removing individual training samples without actual retraining, achieving near-perfect correlation with brute-force leave-one-out on small-scale benchmarks. The method introduces per-sample weights σ into the MTL objective and uses a first-order Taylor expansion to estimate how validation loss changes when σ_i transitions from 1→0. The core insight is that ∂θ̂/∂σ_li can be expressed analytically via the inverse Hessian, avoiding repeated optimization. The first-order approximation remains accurate for single-sample removal (not large subsets).

### Mechanism 2: Cross-Task Influence Propagation Through Shared Parameters
In MTL, a data point from task l influences task k's parameters through shared parameters γ, creating a structured feedback pathway absent in single-task learning. The block-structured Hessian decomposes influence into: (1) direct effect on task-specific θ_k, (2) indirect effect via shared parameters γ. Between-task influence equals `-H^{-1}_{kk} H_{k,K+1} · ∂γ̂/∂σ_li` — data from task l affects θ_k only through γ. This mechanism is specific to architectures with explicit shared parameters or regularization.

### Mechanism 3: MTIF-Guided Data Selection for Negative Transfer Mitigation
Removing training samples with negative aggregate MTIF scores improves MTL accuracy by 3-4% over baselines, especially under label corruption. After computing MTIF(i,l;k) for all samples across all target tasks, the method ranks by total influence Σ_k MTIF(i,l;k), removes the worst fraction (tuned on validation), and retrains. This selectively eliminates samples that harm multiple tasks while retaining beneficial cross-task signal. The removal ratio is tunable and dataset-dependent.

## Foundational Learning

- **Concept: Influence Functions (Koh & Liang 2017)**
  - Why needed here: MTIF extends single-task influence functions to multi-task; understanding the original formulation (Hessian-gradient product for LOO approximation) is prerequisite.
  - Quick check question: Given a trained model, can you explain why `I(z_train, z_test) = -∇_θ L(z_test)^T H^{-1} ∇_θ L(z_train)` estimates the effect of upweighting z_train?

- **Concept: Hard vs. Soft Parameter Sharing in MTL**
  - Why needed here: MTIF handles both; hard sharing uses explicit shared parameters γ, soft sharing uses regularization between task-specific parameters.
  - Quick check question: In the multitask ridge regression example, which term implements soft sharing, and how does it differ from the shared-bottom architecture?

- **Concept: Leave-One-Out vs. Leave-One-Task-Out**
  - Why needed here: MTIF provides both instance-level (LOO) and task-level (LOTO) relatedness; understanding their trade-offs is critical for application.
  - Quick check question: Why does the paper caution that LOTO approximation may degrade for large-n tasks while instance-level remains reliable?

## Architecture Onboarding

- **Component map:** Input MTL model → Compute block-structured Hessian → Calculate MTIF scores via Proposition 1 formulas → Aggregate and filter influences → Retrain on selected subset

- **Critical path:** 1. Train initial MTL model on full data → 2. Compute Hessian blocks (or approximations) → 3. Calculate MTIF scores for all (sample, source_task, target_task) triplets → 4. Aggregate and filter → 5. Retrain on selected subset
  - Bottleneck: Step 3 is O(n × K² × d) naive; use TRAK-style projections to reduce.

- **Design tradeoffs:**
  - Exact Hessian vs. LiSSA/EKFAC/TracIn approximation: Exact is more accurate but O(d²) memory; approximations scale to neural networks
  - Removal ratio: Higher removal → more aggressive negative transfer mitigation but risks discarding useful signal; paper uses validation-tuned ratio
  - Ensemble size: More models improve stability but increase compute; TRAK uses multiple independent trainings

- **Failure signatures:**
  - Near-zero correlation with LOO retraining: Check Hessian invertibility (N matrix in Eq. 6), verify twice-differentiability of loss
  - Performance degradation after selection: Removal ratio too aggressive, or negative transfer is architectural not data-driven
  - Numerical instability in Hessian inverse: Regularize diagonal, use pseudo-inverse, or switch to iterative methods (LiSSA)

- **First 3 experiments:**
  1. **Sanity check on synthetic data:** Reproduce Figure 1 by training multitask ridge regression on the synthetic dataset, computing exact LOO and MTIF scores, plotting correlation. Target: Spearman ρ > 0.9.
  2. **Task-level validation:** Pick one task as target, compute MTIF_task for all source tasks, compare ranking to actual LOTO retraining. Verify MTIF outperforms Cosine Similarity and TAG baselines.
  3. **Data selection pilot:** On Office-31 with 10% label corruption, implement full MTIF-guided selection pipeline. Compare accuracy to EW baseline and one optimization method (e.g., PCGrad). Target: >1% improvement.

## Open Questions the Paper Calls Out

- **Question:** Can the MultiTask Influence Function (MTIF) be effectively scaled and adapted to measure relatedness in large language models (LLMs)?
  - Basis in paper: The authors state in the Conclusion that "Future work could explore extending MTIF to more complex tasks and architectures, such as those involving large language models."
  - Why unresolved: The empirical validation was restricted to standard computer vision benchmarks (CelebA, Office-31) and smaller datasets, leaving the applicability to the scale and architectural complexity of LLMs untested.
  - What evidence would resolve it: Experiments demonstrating MTIF's approximation quality and computational feasibility when applied to transformer-based architectures on NLP tasks.

- **Question:** How does the accuracy of task-level influence ($\text{MTIF}_{\text{task}}$) degrade as the number of data points per task grows very large?
  - Basis in paper: The Limitations section notes that "MTIFtask—which approximates the effect of removing all data from a task—may become less accurate in approximating the LOTO effects when the number of data points per task is very large."
  - Why unresolved: While the method works for the dataset sizes tested, the theoretical dependence of the first-order approximation error on the number of training samples remains uncharacterized.
  - What evidence would resolve it: A theoretical analysis of the approximation error bound relative to dataset size, or empirical evaluation of $\text{MTIF}_{\text{task}}$ correlation with LOTO on datasets with significantly larger per-task sample counts.

- **Question:** What are the fundamental trade-offs between instance-level (Leave-One-Out) and task-level (Leave-One-Task-Out) relatedness measures?
  - Basis in paper: The Conclusion suggests "Better understanding the relationship and trade-offs between LOO and LOTO effects could be an interesting future direction."
  - Why unresolved: The paper establishes both metrics but does not deeply analyze scenarios where they might provide conflicting signals or how to optimally choose between them for different MTL objectives.
  - What evidence would resolve it: Comparative studies identifying specific MTL conditions (e.g., high intra-task heterogeneity) where one measure provides superior guidance for data selection compared to the other.

## Limitations

- MTIF assumes twice-differentiability of the loss function and relies on the model being near a local minimum; performance may degrade with non-convex deep learning objectives.
- The method requires Hessian computation or approximation, which becomes prohibitive for very large models without further approximation beyond TRAK tricks.
- The optimal removal ratio for data selection is dataset-dependent and tuned on held-out validation data without published hyperparameter ranges.

## Confidence

- **High confidence**: MTIF provides accurate first-order approximation of leave-one-out retraining effects (validated against brute-force on synthetic and HAR datasets with near-perfect correlation).
- **Medium confidence**: MTIF-guided data selection consistently improves MTL accuracy by 3-4% over baselines across multiple benchmarks.
- **Low confidence**: The method's scalability to very large-scale neural networks without further approximations beyond TRAK tricks.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the data removal ratio on validation sets across CelebA, Office-31, and Office-Home to identify optimal ranges and understand trade-offs between negative transfer mitigation and information loss.

2. **Ablation of TRAK tricks**: Compare MTIF performance with and without each TRAK component (dimension reduction, ensemble, sparsification) to quantify individual contributions to scalability and accuracy.

3. **Architectural generalization test**: Apply MTIF to MTL architectures beyond shared-bottom and ridge-regularized (e.g., cross-stitch networks, sluice networks) to verify the method's applicability to different parameter sharing mechanisms.