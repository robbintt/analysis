---
ver: rpa2
title: 'MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in
  Multimodal Large Language Models'
arxiv_id: '2508.09210'
source_url: https://arxiv.org/abs/2508.09210
tags:
- step
- emotion
- mllms
- emotional
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MME-Emotion is a comprehensive benchmark for evaluating emotional\
  \ intelligence in multimodal large language models (MLLMs), featuring 6,500 video\
  \ clips across eight emotional tasks. It introduces a holistic evaluation suite\
  \ with three unified metrics\u2014recognition score, reasoning score, and Chain-of-Thought\
  \ score\u2014assessed via a multi-agent system framework."
---

# MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2508.09210
- Source URL: https://arxiv.org/abs/2508.09210
- Reference count: 40
- Primary result: 20 MLLMs evaluated, with best model achieving only 39.3% recognition score and 56.0% CoT score

## Executive Summary
MME-Emotion is a comprehensive benchmark for evaluating emotional intelligence in multimodal large language models, featuring 6,500 video clips across eight distinct emotional intelligence tasks. The benchmark introduces a holistic evaluation suite with three unified metrics—recognition score, reasoning score, and Chain-of-Thought score—assessed through a multi-agent system framework. Evaluation of 20 state-of-the-art MLLMs reveals significant room for improvement, with even top-performing models achieving modest scores, highlighting fundamental limitations in current MLLM emotional intelligence capabilities.

## Method Summary
The benchmark uses 6,500 video clips spanning eight emotional tasks (emotion recognition, sentiment analysis, etc.) evaluated through zero-shot inference on 20 MLLMs. A multi-agent evaluation system employs GPT-4.1 to extract reasoning steps, Qwen2-Audio to extract audio emotional cues, and GPT-4o to judge each reasoning step against ground-truth visual/audio clues and emotion labels. Three unified metrics are computed: Recognition Score (prediction accuracy), Reasoning Score (average step correctness), and Chain-of-Thought Score (weighted combination of the two).

## Key Results
- Best model (Gemini-2.5-Pro) achieved only 39.3% recognition score and 56.0% CoT score across all tasks
- Specialist models can achieve comparable performance to generalist models through emotion-specific post-training despite smaller scale
- Omnimodal models exhibit noticeable performance drops compared to unimodal specialists due to multimodal fusion challenges
- Response step count positively correlates with performance, suggesting deeper reasoning improves emotion recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent evaluation enables automated, annotation-free assessment of reasoning quality in emotional intelligence tasks.
- Mechanism: A Step Agent extracts reasoning steps from model responses, an Audio Agent extracts audio emotional cues, and a Judge Agent (GPT-4o) evaluates each step against ground-truth visual clues, audio clues, and emotion labels. This decomposes reasoning evaluation into binary step-level judgments without requiring human-annotated reasoning chains.
- Core assumption: The judge agent can accurately assess reasoning correctness when given complete multimodal context; the step extraction preserves reasoning structure without introducing bias.
- Evidence anchors: [abstract] "assessed via a multi-agent system framework"; [section 3.2] Formulas: S = Step-LLM(Ps, A), Ca = Audio-LLM(Pa, V), Rec-S, Rea-S = Judge-MLLM(Pj, Cv, Ca, Y, S); [section 3.2] Human verification: Spearman's ρ=0.9530, Cohen's κ=0.8626, ICC=0.9704 between GPT and expert scores; [corpus] Weak corpus evidence; related benchmarks (EmoBench-M) use different evaluation approaches.

### Mechanism 2
- Claim: Emotional intelligence in MLLMs can emerge through two distinct pathways—generalized multimodal pretraining or emotion-specific post-training adaptation.
- Mechanism: Generalist models (e.g., Gemini-2.5-Pro, GPT-4o) leverage large-scale pretraining across diverse multimodal tasks, enabling emotional understanding as an emergent capability. Specialist models (e.g., R1-Omni, Audio-Reasoner) use supervised fine-tuning, reinforcement learning with verifiable rewards (RLVR), and emotion-specific instruction tuning on curated emotional datasets to achieve comparable performance despite smaller scale.
- Core assumption: General intelligence subsumes emotional intelligence; specialist models can compensate for limited scale through targeted training.
- Evidence anchors: [abstract] "Generalist models derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models can achieve comparable performance through domain-specific post-training adaptation"; [section 4.3] Audio-Reasoner (7B, audio-only) achieves 54.8% CoT-S vs Gemini-2.5-Pro's 56.0%; [section 2.1] R1-Omni uses RLVR; AffectGPT uses large-scale emotional datasets with pre-fusion projectors; [corpus] EmoBench-M confirms similar specialist training approaches for MLLMs.

### Mechanism 3
- Claim: Response step count positively correlates with emotional intelligence performance, suggesting that incentivizing deeper reasoning improves emotion recognition.
- Mechanism: Models generating more reasoning steps show higher CoT scores and recognition accuracy. This correlation suggests explicit reasoning helps models identify emotional cues, rule out incorrect candidates, and synthesize multimodal evidence before prediction.
- Core assumption: Reasoning steps reflect genuine inference rather than verbosity; step count causally relates to accuracy rather than being a spurious correlation.
- Evidence anchors: [section 4.3] Figure 5 shows positive correlation between average steps and CoT scores; [section 4.3] "response step count positively correlates with model performance, underscoring the necessity for equipping MLLMs with emotion reasoning capabilities"; [table 2] Top performers (Gemini-2.5-Pro: 5.1 avg steps, 56.0% CoT-S) vs low performers (Emotion-LLaMA: 1.0 steps, 12.8% CoT-S); [corpus] MME-Reasoning benchmark similarly emphasizes reasoning chain evaluation.

## Foundational Learning

- Concept: **Multimodal fusion for affective computing**
  - Why needed here: Understanding how audio, visual, and textual modalities combine for emotion recognition is prerequisite to diagnosing fusion failures (Obs. 4).
  - Quick check question: Can you explain why current omnimodal models underperform compared to unimodal specialists on MME-Emotion?

- Concept: **Chain-of-thought reasoning evaluation**
  - Why needed here: The benchmark's CoT score depends on step-level reasoning assessment; understanding CoT paradigms is essential for interpreting results.
  - Quick check question: How does the step agent extract reasoning steps, and why does the judge agent rate each step as binary?

- Concept: **MLLM-as-judge evaluation paradigm**
  - Why needed here: The evaluation suite relies entirely on automated judging; understanding validation methodology is critical for trusting benchmark results.
  - Quick check question: What statistical metrics validate the alignment between GPT-judge and human expert scores?

## Architecture Onboarding

- Component map: Video input → MLLM inference → Step extraction → Judge evaluation → Score aggregation

- Critical path: Video input → MLLM inference → Step extraction → Judge evaluation → Score aggregation. The step extraction prompt design directly impacts reasoning score validity.

- Design tradeoffs: Closed-set evaluation (predefined labels) trades ecological validity for measurement reliability; multi-agent evaluation trades annotation cost for potential judge bias; weighted CoT-S (α=0.5) balances recognition vs reasoning equally.

- Failure signatures:
  - Omnimodal models scoring below unimodal specialists → multimodal fusion failure (Obs. 4)
  - High reasoning score + low recognition score → reasoning doesn't translate to correct predictions
  - Large step count variance across tasks → inconsistent prompting or task difficulty mismatch

- First 3 experiments:
  1. Reproduce baseline scores for 3 model categories (generalist, specialist, omnimodal) on one task type to validate evaluation pipeline.
  2. Ablate audio clues from judge input to quantify multimodal contribution to reasoning score.
  3. Compare step-extraction prompts (with/without original question context) to verify no external bias introduction per section 3.2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can robust multimodal emotional clues fusion be achieved to prevent the performance drops observed in omnimodal models compared to unimodal models on emotion tasks?
- Basis in paper: [explicit] The paper states that "omnimodal models, which are designed to process audio, visual, and textual information simultaneously, often exhibit noticeable performance drops" and identifies two potential issues: "Multimodal data often contains redundant or inconsistent emotional clues across different modalities" and "Existing omnimodal models still lack effective strategies for robust multimodal emotional clues fusion."
- Why unresolved: Current fusion architectures are not designed to handle conflicting or redundant emotional signals across modalities, and the paper provides no solution.
- What evidence would resolve it: Demonstration of a fusion mechanism that consistently improves omnimodal model performance over audio-only or vision-only baselines on MME-Emotion tasks.

### Open Question 2
- Question: Can task difficulty classification (analogous to mathematical problem levels) be incorporated into emotional intelligence benchmarks to provide more nuanced assessment of MLLM capabilities?
- Basis in paper: [explicit] The limitations section explicitly states: "it would be beneficial to incorporate task difficulty classification. Similar to the categorization of mathematical problems into primary school, high school, college, and Olympiad levels, assigning a difficulty rating or level to each sample in our benchmark could offer a more nuanced understanding of MLLMs' capabilities."
- Why unresolved: No difficulty stratification currently exists in MME-Emotion or other emotional intelligence benchmarks.
- What evidence would resolve it: A validated difficulty taxonomy for emotional recognition/reasoning tasks showing consistent performance gradients across MLLM capability levels.

### Open Question 3
- Question: How does multilingual context affect emotional intelligence performance in MLLMs, and do models exhibit language-specific biases in emotion recognition?
- Basis in paper: [explicit] The limitations section states: "the included data covers a wide range of multilingual scenarios. However, we do not currently distinguish between different languages in the benchmark, nor do we analyze model performance across different linguistic contexts."
- Why unresolved: Language-dependent emotional expression patterns and model training data biases remain unexamined in current evaluation frameworks.
- What evidence would resolve it: Cross-linguistic analysis on MME-Emotion showing whether performance varies systematically by language and identifying specific failure modes.

## Limitations

- The multi-agent evaluation framework introduces potential measurement bias through judge agent reliability and automated audio extraction
- Closed-set evaluation design limits ecological validity by constraining models to predefined emotion labels rather than allowing open-ended responses
- Claims about multimodal fusion limitations require controlled ablation studies to definitively establish causal relationships

## Confidence

**High Confidence**: Benchmark construction methodology, dataset aggregation from established public sources, and three-metric evaluation framework are well-documented and reproducible. Observed performance gap between generalist and specialist models aligns with established scaling laws.

**Medium Confidence**: Correlation between reasoning step count and performance requires careful interpretation, as this may reflect model scale rather than genuine causal relationships. Relative performance rankings are robust, but absolute score differences may be influenced by evaluation protocol.

**Low Confidence**: Claim that multimodal fusion limitations explain omnimodal model underperformance cannot be definitively established without controlled ablation studies of fusion mechanisms.

## Next Checks

1. **Judge Agent Reliability Validation**: Conduct human expert review of 100 randomly sampled step ratings to establish inter-rater reliability between GPT-4o judge and human evaluators across all emotion categories.

2. **Modality Ablation Study**: Evaluate each model category with only visual, only audio, and multimodal inputs to quantify the specific contribution of each modality to the performance gap between omnimodal and specialist models.

3. **Reasoning Step Causation Test**: Compare performance of models prompted with and without explicit step-by-step reasoning requirements on identical samples to determine whether step count causally improves accuracy or merely correlates with model scale.