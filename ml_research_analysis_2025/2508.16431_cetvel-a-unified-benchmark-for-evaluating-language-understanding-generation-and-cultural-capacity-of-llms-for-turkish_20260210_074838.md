---
ver: rpa2
title: 'Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation
  and Cultural Capacity of LLMs for Turkish'
arxiv_id: '2508.16431'
source_url: https://arxiv.org/abs/2508.16431
tags:
- language
- qwen2
- b-instruct
- turkish
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CETVEL, a new benchmark for evaluating large
  language models (LLMs) in Turkish. Unlike existing Turkish benchmarks, CETVEL combines
  broad task diversity with culturally and linguistically relevant content, covering
  23 tasks across seven categories including grammatical error correction, machine
  translation, and question answering on Turkish history.
---

# Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish

## Quick Facts
- arXiv ID: 2508.16431
- Source URL: https://arxiv.org/abs/2508.16431
- Reference count: 40
- Primary result: Turkish-centric LLMs underperform multilingual models on CETVEL benchmark due to catastrophic forgetting of English capabilities

## Executive Summary
This paper introduces CETVEL, a comprehensive benchmark for evaluating large language models (LLMs) on Turkish language tasks. CETVEL covers 23 tasks across seven categories including grammatical error correction, machine translation, and culturally-grounded question answering about Turkish history. The benchmark evaluates 33 open-weight models ranging from 7B to 70B parameters, comparing general-purpose, multilingual, and Turkish-specific models. Results show that multilingual models like Llama 3 and Mistral outperform Turkish-centric instruction-tuned models, with task discrimination analysis revealing that generation tasks are more effective at differentiating model capabilities than classification tasks.

## Method Summary
CETVEL uses zero-shot evaluation with LM Evaluation Harness and vLLM backend, testing 33 open-weight LLMs (7B-70B parameters) on 23 tasks across 7 categories. The benchmark employs specific metrics per task: accuracy for classification tasks, Exact Match for QA, BLEU-4 for translation, ROUGE-2 for summarization, and macro-F1 for grammatical error correction. Models are evaluated using batch size 4 for NLU tasks and single-instance generation with beam search (width 5) and max 64 tokens. The evaluation runs on 8× NVIDIA A40 GPUs with open-weights models only, avoiding proprietary models due to log-probability access restrictions.

## Key Results
- Turkish-centric models (Commencis-7B, Trendyol-7B) underperform their base models on translation due to catastrophic forgetting of English
- Cere-Llama-3-8B excels at grammatical error correction (46.0 F1) and Turkish history QA (49.2 EM) despite being 8x smaller than Llama-3.3-70B-Instruct
- Grammatical error correction, machine translation, and extractive QA tasks show the highest discrimination power for differentiating model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generation tasks with objective metrics differentiate model capabilities more effectively than classification tasks
- Mechanism: Tasks requiring precise linguistic output produce wider score distributions across models, as measured by Gini coefficients (GEC: 0.490, MT: 0.402, QA: 0.362 vs. NLI: 0.039, TC: 0.080)
- Core assumption: Higher variance in task scores reflects genuine capability differences rather than measurement noise
- Evidence anchors:
  - [abstract] "tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities"
  - [section 5.5] Figure 3 shows bootstrapped Gini coefficients with 95% CI
- Break condition: If task discrimination is driven by dataset quality issues rather than model capability differences

### Mechanism 2
- Claim: Turkish-centric instruction tuning without cross-lingual data causes catastrophic forgetting of English capabilities
- Mechanism: Models fine-tuned exclusively on Turkish data underperform on machine translation, while Cere-Llama-3-8B achieves near-zero translation scores (0.1 BLEU)
- Core assumption: Performance degradation on EN-TR MT reflects English language capability loss, not task-specific fine-tuning gaps
- Evidence anchors:
  - [section 5.4] "Turkish-centric models...perform extremely poorly on machine translation due to catastrophic forgetting English"
  - [section 5.3] Turkish-centric models attain BLEU scores of 0.1-4.7 vs. 13.6-20.1 for multilingual models
- Break condition: If poor MT performance stems from instruction format mismatch rather than English forgetting

### Mechanism 3
- Claim: Culturally grounded evaluation reveals performance patterns not captured by translated benchmarks
- Mechanism: Cere-Llama-3-8B outperforms Llama-3.3-70B-Instruct on TQuAD (Turkish/Islamic history QA: 49.2 vs. 17.4 EM) despite being 8x smaller
- Core assumption: TQuAD performance reflects cultural knowledge depth rather than dataset memorization
- Evidence anchors:
  - [section 5.2] "Cere-Llama-3-8B achieves the best score [on TQuAD], outperforming all others despite its smaller size"
  - [section 1] CETVEL includes "extractive QA centered on Turkish and Islamic history"
- Break condition: If TQuAD questions appeared in Cere-Llama-3-8B's training data

## Foundational Learning

- Concept: **Gini Coefficient for Task Discrimination**
  - Why needed here: Measures how much a task spreads models across the performance spectrum; high Gini = task differentiates well
  - Quick check question: If a task has Gini = 0.05, would you prioritize it for model comparison?

- Concept: **Catastrophic Forgetting in Instruction Tuning**
  - Why needed here: Explains why Turkish-only fine-tuning degrades multilingual capabilities; relevant for designing training curricula
  - Quick check question: A model fine-tuned only on Turkish data scores 0.1 BLEU on EN-TR translation—is this surprising?

- Concept: **Zero-Shot Evaluation Protocol**
  - Why needed here: CETVEL uses zero-shot evaluation (no task examples in prompt); ensures controlled comparison but may underrepresent few-shot capabilities
  - Quick check question: Why might zero-shot evaluation favor models with strong instruction following over those with strong task-specific knowledge?

## Architecture Onboarding

- Component map:
  - LM Evaluation Harness -> vLLM backend -> HuggingFace Transformers
  - 7 categories (QA, MCQA, TC, NLI, SUM, MT, GEC) -> 23 datasets
  - 33 models (7B-70B) -> batch size 4 (NLU) or 1 (generation) -> 64 max tokens, beam search width 5

- Critical path:
  1. Load model via HuggingFace Transformers
  2. Run inference with batch size 4 (NLU) or 1 (generation), max 64 tokens
  3. Use beam search (width 5) for generation tasks
  4. Compute task-specific metrics → aggregate by category

- Design tradeoffs:
  - Zero-shot only: Limits comparison to in-context learning capabilities; Appendix notes few-shot as future work
  - Open-weights only: Excludes proprietary models due to log-probability access restrictions
  - No LLM-as-judge: Avoided due to reliability concerns in multilingual settings

- Failure signatures:
  - Turkish-centric models with near-zero MT scores (e.g., Cere-Llama-3-8B: 0.1 BLEU) → likely catastrophic forgetting
  - Instruction-tuned variants underperforming base models (Qwen2.5 pattern) → potential overfitting or alignment tax
  - NLI tasks with compressed score ranges (Gini ~0.04) → low discrimination power

- First 3 experiments:
  1. **Baseline comparison**: Run your model on CETVEL's 7 task categories; compare against Llama-3.1-8B and Cere-Llama-3-8B baselines
  2. **Ablation by task type**: Identify which categories drive performance gaps; focus on high-Gini tasks (GEC, MT, QA) for differentiation
  3. **Cross-lingual sanity check**: If building Turkish-centric models, evaluate EN-TR translation before/after instruction tuning to detect forgetting

## Open Questions the Paper Calls Out

1. Can Turkish-centric instruction-tuning strategies be improved to match or exceed multilingual models on Turkish benchmarks?
   - Basis: Turkish-centric models consistently underperform, pointing to need for more effective instruction-tuning strategies
   - Why unresolved: Current Turkish-centric models underperform relative to their base models, suggesting fundamental issues with current tuning approaches
   - What evidence would resolve it: Development of Turkish instruction-tuned models that outperform comparable multilingual models on CETVEL metrics

2. How would few-shot evaluation affect the relative performance rankings between Turkish-centric and multilingual models?
   - Basis: "Incorporating one-shot and few-shot evaluations remains an important direction for future iterations"
   - Why unresolved: The study only reports zero-shot results; Turkish-centric models may benefit more from in-context examples given their language specialization
   - What evidence would resolve it: Comparative evaluation of the same 33 models using 1-shot and few-shot prompting strategies on CETVEL tasks

3. What training data composition enables models like Cere-Llama-3-8B to excel at grammatical error correction while catastrophically failing at machine translation?
   - Basis: Cere-Llama-3-8B achieves 46.0 F1 on GEC but only 0.1 BLEU on translation, suggesting extreme task specialization with cross-lingual degradation
   - Why unresolved: The paper does not analyze what specific training choices produce this pattern
   - What evidence would resolve it: Ablation studies varying the ratio and type of monolingual vs. cross-lingual instruction data during Turkish fine-tuning

## Limitations

- Data contamination risk: TQuAD performance differences may reflect memorization rather than genuine cultural knowledge
- Task discrimination validity: Gini coefficient analysis assumes variance reflects true capability differences, but could be influenced by dataset quality
- Instruction following vs. task knowledge: Zero-shot evaluation conflates instruction following capabilities with deeper language understanding

## Confidence

**High Confidence**:
- CETVEL provides broader task coverage than existing Turkish benchmarks
- General-purpose and multilingual models outperform Turkish-specific instruction-tuned models on most tasks
- Grammatical error correction, machine translation, and extractive QA tasks show higher discrimination power than classification tasks

**Medium Confidence**:
- Turkish-centric instruction tuning causes catastrophic forgetting of English capabilities
- Culturally grounded evaluation reveals performance patterns not captured by translated benchmarks
- Cere-Llama-3-8B's TQuAD performance reflects genuine cultural knowledge advantage

**Low Confidence**:
- The specific ranking of task discrimination based on Gini coefficients
- That zero-shot evaluation is the optimal protocol for Turkish LLM assessment
- That the 23-task composition optimally balances coverage and discrimination

## Next Checks

1. **Data Contamination Audit**: Verify that TQuAD questions and answers do not appear in the training data of Cere-Llama-3-8B and other evaluated models using exact string matching and semantic similarity searches.

2. **Cross-Lingual Capability Testing**: Evaluate Turkish-centric models on English-only tasks (e.g., English GLUE benchmarks) to directly measure catastrophic forgetting versus task-specific fine-tuning effects.

3. **Task Discrimination Robustness**: Repeat the Gini coefficient analysis using alternative performance metrics (e.g., rank-based coefficients) and test sensitivity to outlier model performances to validate the task discrimination hierarchy.