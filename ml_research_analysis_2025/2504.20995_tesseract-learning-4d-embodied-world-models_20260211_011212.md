---
ver: rpa2
title: 'TesserAct: Learning 4D Embodied World Models'
arxiv_id: '2504.20995'
source_url: https://arxiv.org/abs/2504.20995
tags:
- depth
- normal
- arxiv
- video
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TesserAct, a 4D embodied world model that
  predicts dynamic 3D scenes over time in response to an agent's actions. The model
  is trained on RGB-DN videos (RGB, Depth, and Normal) to generate temporally and
  spatially consistent 4D scenes.
---

# TesserAct: Learning 4D Embodied World Models

## Quick Facts
- **arXiv ID:** 2504.20995
- **Source URL:** https://arxiv.org/abs/2504.20995
- **Authors:** Haoyu Zhen; Qiao sun; Hongxin Zhang; Junyan Li; Siyuan Zhou; Yilun Du; Chuang Gan
- **Reference count:** 40
- **Primary result:** 4D embodied world model that predicts dynamic 3D scenes over time, outperforming prior video-based world models in downstream embodied tasks

## Executive Summary
TesserAct introduces a 4D embodied world model that predicts dynamic 3D scenes over time in response to an agent's actions. The model is trained on RGB-DN videos (RGB, Depth, and Normal) to generate temporally and spatially consistent 4D scenes. To address the lack of large-scale 4D datasets, the authors collect and annotate a dataset from existing video datasets using depth and normal estimation models. The model fine-tunes a video diffusion model (CogVideoX) to predict RGB-DN sequences and reconstructs 4D scenes using novel loss functions for temporal and spatial consistency. TesserAct outperforms prior video-based world models in downstream embodied tasks, achieving lower Chamfer distances and higher success rates in robotic manipulation tasks.

## Method Summary
TesserAct fine-tunes a pre-trained video diffusion model (CogVideoX) to jointly predict RGB, depth, and normal videos for 4D scene learning. The architecture extends CogVideoX with zero-initialized projectors for depth and normal modalities, enabling efficient training without requiring prohibitively large 4D datasets. Generated depth maps undergo post-hoc optimization using normal integration combined with optical flow-based consistency constraints to produce temporally coherent 4D point clouds. For downstream tasks, an inverse dynamics model built on the 4D point clouds predicts appropriate robot actions based on current and future states. The method demonstrates strong generalization to unseen scenes, objects, and cross-domain scenarios while maintaining competitive RGB generation quality.

## Key Results
- Achieves lower Chamfer distances and higher success rates in robotic manipulation tasks compared to prior video-based world models
- Successfully predicts temporally and spatially consistent 4D scenes from RGB-DN video sequences
- Demonstrates strong generalization to unseen scenes, objects, and cross-domain scenarios
- Maintains competitive RGB generation quality while jointly predicting depth and normal modalities

## Why This Works (Mechanism)

### Mechanism 1: Joint RGB-DN Video Diffusion via Modality-Specific Projectors
Fine-tuning a pre-trained video diffusion model (CogVideoX) with zero-initialized projectors for depth and normal modalities enables efficient 4D scene learning without requiring prohibitively large 4D datasets. The architecture ensures training begins from pre-trained RGB capabilities while learning to predict geometric modalities.

### Mechanism 2: Temporal-Spatial Consistency via Flow-Guided Depth Optimization
Post-hoc optimization of generated depth maps using normal integration combined with optical flow-based consistency constraints produces 4D point clouds with coherent geometry across time. The algorithm segments static vs. dynamic regions using optical flow to enforce depth agreement across frames.

### Mechanism 3: Inverse Dynamics Learning from 4D Point Cloud Representations
Encoding predicted 4D scenes as point clouds provides geometric information that improves action prediction over 2D video-based inverse dynamics models. PointNet extracts 3D features from reconstructed point clouds that concatenate with instruction text embeddings to output 7-DoF actions.

## Foundational Learning

- **Latent Video Diffusion Models (CogVideoX architecture)**
  - Why needed here: The entire method builds on extending CogVideoX's 3D VAE and DiT backbone for RGB-DN prediction
  - Quick check question: Can you explain how the 3D VAE encodes video into latent space and how the denoising network is conditioned on timestep and text?

- **Depth and Normal Representation**
  - Why needed here: The method uses relative depth [0,1] and surface normals as intermediate representations
  - Quick check question: Given a normal map, how would you formulate the constraint that log-depth gradients should align with surface normals under a perspective camera model?

- **Optical Flow for Temporal Correspondence**
  - Why needed here: Flow estimation segments static/dynamic regions and establishes cross-frame pixel correspondence for the consistency loss
  - Quick check question: What types of motion would cause RAFT to produce unreliable flow estimates, and how might this affect the static/dynamic segmentation threshold?

## Architecture Onboarding

- **Component map:**
  - Input pipeline: RGB/Depth/Normal videos → separate 3D VAE encoders → input projectors → latent summation
  - Core: DiT backbone (from CogVideoX) conditioned on text tokens and diffusion timestep
  - Output pipeline: RGB output projector (preserved from CogVideoX) + DN output projector (Conv3D + MLP combining hidden states with encoded latents)
  - Post-processing: Optical flow (RAFT) → static/dynamic segmentation → iterative depth optimization (Ls + Lc + Lr) → 4D point cloud reconstruction
  - Downstream: PointNet encoder → MLP inverse dynamics model → 7-DoF actions

- **Critical path:**
  1. RGB-DN video generation quality (depends on CogVideoX fine-tuning stability)
  2. Depth-normal consistency in generated outputs (affects reconstruction)
  3. Flow-based segmentation accuracy (determines consistency loss applicability)
  4. Loss weight tuning (λ parameters differ between real/synthetic data)

- **Design tradeoffs:**
  - Joint RGB-DN prediction vs. post-hoc estimation: Paper shows better depth/normal quality than estimating from RGB-only video
  - Lightweight RGB-DN representation vs. direct 4D generation: Enables longer sequences (49 frames) vs. 4 frames for 4D Point-E baseline
  - Single-view reconstruction: Only captures one surface; multi-view generation noted as limitation

- **Failure signatures:**
  - RGB quality degradation: Monitor PSNR/SSIM on RGB outputs; slight decrease expected but significant drops indicate modality conflict
  - Temporal flickering in point clouds: Consistency loss not converging; check flow quality and λc weights
  - Depth scale inconsistencies across frames: Normal integration may drift; verify Ls/Lr balance
  - Action prediction failures on 2D-sufficient tasks: Expected behavior per paper (open microwave, weighing off)

- **First 3 experiments:**
  1. Validate RGB-DN generation quality: Fine-tune CogVideoX on a small subset (10k videos) of the collected dataset. Compare depth/normal predictions against RollingDepth + Marigold baselines using AbsRel and normal angular error metrics. Confirm zero initialization preserves RGB quality.
  2. Ablate reconstruction losses: On RLBench (where ground truth depth exists), test Ls-only, Ls+Lc, Ls+Lc+Lr configurations. Measure Chamfer distance against ground truth point clouds. Visualize temporal coherence with/without Lc.
  3. Inverse dynamics baseline comparison: Train inverse dynamics models on (a) 2D image features, (b) post-hoc estimated depth point clouds, (c) TesserAct-generated point clouds. Test on 3 RLBench tasks with varying geometric complexity (grasping, drawer opening, tool use).

## Open Questions the Paper Calls Out

- **Open Question 1:** How can a generative model be designed to produce multiple RGB-DN views that can be integrated into a complete 4D world model, overcoming the current single-surface limitation?
- **Open Question 2:** To what extent does the error propagation from off-the-shelf depth and normal estimators limit the performance ceiling of TesserAct on real-world data?
- **Open Question 3:** Can the 4D scene reconstruction be learned implicitly within the diffusion model to bypass the computational cost of the post-hoc optimization algorithm?

## Limitations
- Relies heavily on quality of pre-trained depth and normal estimation models, introducing potential cascading errors
- Optical flow-based consistency loss assumes reliable static/dynamic segmentation, which may fail on fast robot motions
- 8192-point downsampling involves trade-offs between computational efficiency and geometric fidelity
- Zero-initialized projector approach may not fully preserve pre-trained RGB generation quality across all video types

## Confidence
- **High Confidence:** RGB-DN video generation quality metrics (PSNR, SSIM, AbsRel, δ1, angular error) are directly measured and show clear improvements over baselines
- **Medium Confidence:** 4D reconstruction temporal consistency gains are demonstrated but rely on optical flow quality that varies with motion types
- **Medium Confidence:** Downstream robotic task success rates show improvements, though the ablation studies suggest geometric complexity affects performance differently across tasks

## Next Checks
1. **Dataset Annotation Quality Validation:** Compare RollingDepth + Marigold-LCM outputs against sparse real depth measurements from a subset of RT-1 Fractal to quantify annotation error rates and their propagation through the 4D reconstruction pipeline.
2. **Cross-Dataset Generalization Test:** Evaluate TesserAct's 4D reconstruction performance when trained on synthetic RLBench data but tested on real RT-1 or Bridge datasets, measuring depth accuracy degradation and temporal consistency breakdown.
3. **Alternative Geometric Representations:** Replace the 8192-point cloud representation with (a) multi-view RGB-DN fusion or (b) implicit neural representations to assess whether point cloud limitations (single-view surfaces, fixed resolution) constrain task performance.