---
ver: rpa2
title: 'MMATH: A Multilingual Benchmark for Mathematical Reasoning'
arxiv_id: '2505.19126'
source_url: https://arxiv.org/abs/2505.19126
tags:
- reasoning
- language
- english
- question
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce MMATH, a new multilingual benchmark for complex
  mathematical reasoning spanning 374 problems across 10 typologically diverse languages.
  They identify a significant off-target issue in reasoning models where responses
  are generated in unintended languages.
---

# MMATH: A Multilingual Benchmark for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2505.19126
- Source URL: https://arxiv.org/abs/2505.19126
- Reference count: 40
- Primary result: 66.72 average accuracy across 10 languages with 97.61% language consistency ratio

## Executive Summary
MMATH introduces a new multilingual benchmark for complex mathematical reasoning spanning 374 problems across 10 typologically diverse languages. The authors identify a significant "off-target" problem where reasoning models generate responses in unintended languages, particularly problematic for low-resource languages. They demonstrate that reasoning in English while answering in the target language can simultaneously improve answer accuracy and language consistency. Their best model, Qwen2.5-32B-Instruct trained with English reasoning traces and multilingual answers, achieves state-of-the-art performance with high language consistency.

## Method Summary
The benchmark translates 374 English math problems (AIME, CNMO, MATH-500) into 10 languages using GPT-4o-mini. Models are evaluated using Answer Accuracy (boxed answer extraction) and Language Consistency Ratio (LCR using fastText). Three SFT variants are trained on Qwen2.5-32B-Instruct: EN-SFT (all English), Native-Think (native language reasoning), and EN-Think (English thinking with native question/answer). The EN-Think approach trains on 3K examples from Light-R1, translating reasoning into English paragraphs while maintaining native language questions and answers.

## Key Results
- EN-Think model achieves 66.72 average accuracy across 10 languages
- Maintains 97.61% answering LCR (language consistency ratio)
- Outperforms Native-Think (61.46) and EN-SFT (62.38) variants
- Shows off-target thinking improves low-resource languages (Arabic: 0% â†’ 44% accuracy) while target-language reasoning is optimal for high-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Cross-Lingual Transfer (English as the "Reasoning Pivot")
Large Language Models exhibit superior reasoning capabilities when internal "thinking" occurs in English, even for non-English input/output pairs, particularly for low-resource languages. The model's reasoning circuits are most robust in English due to skewed pre-training data distribution. By routing internal reasoning through English, the model leverages its strongest logical pathways while maintaining sufficient cross-lingual alignment to understand input and generate target-language output.

### Mechanism 2: Instructional Decoupling (Separating Thinking from Answering)
Prompting or fine-tuning models to explicitly separate the "thinking" language from the "answering" language resolves off-target generation. Standard instruction tuning conditions models on `(Input Language) => (Output Language)`, but complex reasoning introduces hidden CoT states. Without explicit guidance, models default to their strongest reasoning language (English) for both thinking and answering. Enforcing constraints like "Think in English, Answer in French" creates conditional generation policies.

### Mechanism 3: Consistency-Driven Fine-Tuning (The "EN-Think" Alignment)
SFT on datasets combining multilingual questions, English reasoning traces, and target-language answers aligns the model's internal representation to prefer English reasoning while maximizing output consistency. Pure SFT on English data causes off-target issues, while pure SFT on native data suffers from reasoning deficits. The EN-Think dataset creates specific gradient signals that reinforce the path: `Input(Any Lang) -> Internal State(English Logic) -> Output(Target Lang)`.

## Foundational Learning

- **Concept: Off-Target Problem (Language Drift)**
  - **Why needed here:** This is the central failure mode the paper addresses - models default to unintended languages during reasoning or answering
  - **Quick check question:** If a user asks in Thai and the model thinks in English, what's the "Off-Target" issue if the final answer is also in English?

- **Concept: Language Consistency Ratio (LCR)**
  - **Why needed here:** LCR quantifies the off-target problem by measuring if output language matches input language
  - **Quick check question:** A model gets 100% accuracy on French questions but answers in English. What is its LCR?

- **Concept: Chain-of-Thought (CoT) & "Thinking" Tokens**
  - **Why needed here:** The paper distinguishes between internal thinking process (CoT) and final answer regarding language choice
  - **Quick check question:** In DeepSeek R1 or QwQ, where does the "thinking" happen relative to the final answer?

## Architecture Onboarding

- **Component map:** Input(Multilingual Query) -> Thinking Core(English-centric CoT) -> Consistency Controller(EN-Think adapter) -> Evaluator(Accuracy + LCR)

- **Critical path:**
  1. Data Curation: Translate English math benchmarks to 9 target languages
  2. Diagnosis: Evaluate baseline models to establish off-target baseline (low LCR)
  3. Intervention: Apply ATP to steer the model
  4. Alignment: Train using EN-Think dataset format (Question: Native, CoT: English, Answer: Native)

- **Design tradeoffs:**
  - Prompting (ATP/DIT/QRT) vs. Training (EN-Think): Prompting is cheap but less robust on small models; training is expensive but yields highest Accuracy/LCR balance
  - Native-Think vs. EN-Think: Native-Think ensures high LCR but lower accuracy; EN-Think maximizes accuracy while maintaining high LCR

- **Failure signatures:**
  - Language Drift: Model switches to English mid-reasoning and stays there
  - Forgetting: Model loses ability to reason on pure English tasks after SFT
  - Format collapse: Small models struggle to follow "Think EN, Answer X" format

- **First 3 experiments:**
  1. Baseline LCR Audit: Run model on MMATH, calculate LCR using fastText on output vs. input
  2. ATP Ablation: Implement "Answer-in-Target Prompt" and compare accuracy drop/gain vs. LCR gain
  3. Distillation Test: Fine-tune small model (Qwen-7B) on 300 "EN-Think" examples, check if it learns language switching

## Open Questions the Paper Calls Out

### Open Question 1
Can native multilingual reasoning data (synthesized without translation) outperform translation-based approaches for training multilingual reasoning models? The paper uses translation-based approaches but suggests future work explore native multilingual reasoning data generation methods.

### Open Question 2
What training-free strategies can jointly optimize both reasoning accuracy and language consistency without the trade-offs observed in current interventions? The paper shows DIT and QRT improve language consistency but cause accuracy drops for smaller models.

### Open Question 3
Do the findings on multilingual mathematical reasoning generalize to other complex reasoning domains such as coding, STEM, and logical reasoning? The paper focuses on mathematical reasoning and leaves other tasks unexplored.

### Open Question 4
Why does off-target thinking improve accuracy for low-resource languages while target-language reasoning is optimal for high-resource languages? The paper observes this phenomenon but only hypothesizes it stems from post-training processes emphasizing high-resource languages.

## Limitations

- **Conceptual Drift Risk**: English reasoning traces may lose semantic precision when translated back to target languages, particularly for culturally-specific mathematical concepts
- **Small Model Scalability**: EN-Think approach diminishes significantly for smaller models (1.5B/7B) due to cognitive load of maintaining language separation
- **Dataset Generalization**: Evaluation relies on 374 translated problems, creating potential bias toward easily translatable mathematical concepts

## Confidence

**High Confidence**: Experimental results showing improved accuracy and language consistency are well-supported by the data
**Medium Confidence**: Mechanism explanation for why English reasoning improves performance is plausible but not definitively proven
**Low Confidence**: Long-term generalization to truly novel mathematical domains or non-translatable problem types remains untested

## Next Checks

1. **Semantic Drift Analysis**: Compare semantic precision of answers generated through English reasoning versus native-language reasoning for problems with potential cultural/linguistic specificity

2. **Cross-Domain Generalization**: Test EN-Think model on mathematical problems inherently difficult to translate between languages to assess if the approach breaks down

3. **Small Model Adaptation**: Experiment with curriculum learning or progressive fine-tuning approaches for smaller models to determine if language separation mechanism can be taught incrementally