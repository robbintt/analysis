---
ver: rpa2
title: 'DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality
  Prediction'
arxiv_id: '2512.21433'
source_url: https://arxiv.org/abs/2512.21433
tags:
- data
- compression
- prediction
- quality
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepCQ, a deep-surrogate framework for predicting
  lossy compression quality metrics (PSNR, SSIM, and CR) across multiple compressors
  and datasets. The core method uses a two-stage approach that separates expensive
  feature extraction from lightweight metric prediction, enabling efficient training
  and modular inference.
---

# DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction

## Quick Facts
- arXiv ID: 2512.21433
- Source URL: https://arxiv.org/abs/2512.21433
- Reference count: 40
- Key outcome: DeepCQ achieves prediction errors generally under 10% across multiple compressors and scientific datasets

## Executive Summary
DeepCQ presents a deep-surrogate framework for predicting lossy compression quality metrics (PSNR, SSIM, and CR) across multiple compressors and datasets. The framework uses a two-stage approach that separates expensive feature extraction from lightweight metric prediction, enabling efficient training and modular inference. A mixture-of-experts design enhances robustness when predicting across time-evolving data with varying compression quality. Evaluated on four real-world scientific applications (Nyx, Hurricane, Miranda, RTM), DeepCQ significantly outperforms existing methods.

## Method Summary
DeepCQ employs a two-stage approach for lossy compression quality prediction. The first stage extracts expensive feature representations from compressed data, while the second stage uses these features to predict quality metrics through a lightweight surrogate model. The framework incorporates a mixture-of-experts design that enhances prediction robustness for time-evolving datasets. The modular architecture allows for efficient training and inference, with the expensive feature extraction decoupled from the fast metric prediction component.

## Key Results
- Prediction errors generally below 10% for PSNR and SSIM metrics across most configurations
- Outperforms existing methods for compressor selection and configuration prediction
- Effective across four scientific applications: Nyx, Hurricane, Miranda, and RTM

## Why This Works (Mechanism)
DeepCQ's effectiveness stems from its two-stage approach that separates expensive feature extraction from lightweight metric prediction. This separation allows for efficient training while maintaining prediction accuracy. The mixture-of-experts component provides robustness when dealing with temporal data evolution, enabling the framework to adapt to changing compression quality patterns over time.

## Foundational Learning
- Lossy compression quality metrics (PSNR, SSIM): Essential for understanding what DeepCQ predicts and how compression quality is quantified in scientific applications
- Feature extraction vs. prediction separation: Critical design principle that enables computational efficiency in the two-stage approach
- Mixture-of-experts architecture: Key mechanism for handling temporal data evolution and improving prediction robustness
- Surrogate modeling in scientific computing: Provides context for why deep learning is applied to compression quality prediction instead of direct evaluation

## Architecture Onboarding

Component map: Feature Extractor -> Surrogate Predictor -> Mixture-of-Experts -> Quality Metrics (PSNR/SSIM/CR)

Critical path: Data → Feature Extractor → Surrogate Predictor → Quality Metrics

Design tradeoffs: The two-stage separation enables computational efficiency but requires careful feature engineering. The mixture-of-experts adds robustness at the cost of increased inference complexity.

Failure signatures: Increased prediction error when compression ratios exceed 100:1; reduced accuracy on data distributions not represented in training.

First experiments to run:
1. Test feature extraction performance on a small validation dataset
2. Evaluate surrogate predictor accuracy with fixed expert weights
3. Measure mixture-of-experts adaptation on temporally evolving data

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degradation at extremely high compression ratios (CR > 100) with increased error variance
- Mixture-of-experts component adds computational overhead during inference not fully characterized
- Generalizability claims rely heavily on four scientific applications, potentially limiting cross-domain applicability
- No exploration of adversarial scenarios where compressor configurations might be designed to fool the prediction model

## Confidence
- High confidence for PSNR and SSIM predictions within tested compression ratio ranges
- Medium confidence for extreme compression scenarios and cross-domain generalization
- Medium confidence in computational overhead characterization for mixture-of-experts component

## Next Checks
1. Test DeepCQ's prediction accuracy on datasets from scientific domains not represented in the original evaluation (e.g., climate modeling or particle physics) to assess true generalizability
2. Evaluate performance at compression ratios exceeding 200:1 to determine prediction reliability in extreme scenarios
3. Conduct runtime benchmarking of the mixture-of-experts component across different hardware configurations to quantify computational overhead during inference