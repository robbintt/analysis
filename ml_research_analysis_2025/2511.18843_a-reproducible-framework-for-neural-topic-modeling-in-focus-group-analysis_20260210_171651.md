---
ver: rpa2
title: A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis
arxiv_id: '2511.18843'
source_url: https://arxiv.org/abs/2511.18843
tags:
- topic
- coherence
- modeling
- focus
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic framework for applying BERTopic
  to focus group transcripts using data from ten focus groups exploring HPV vaccine
  perceptions in Tunisia (1,075 utterances). The authors conducted comprehensive hyperparameter
  exploration across 27 configurations, evaluating each through bootstrap stability
  analysis, performance metrics, and comparison with LDA baseline.
---

# A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis

## Quick Facts
- arXiv ID: 2511.18843
- Source URL: https://arxiv.org/abs/2511.18843
- Reference count: 40
- A systematic BERTopic framework achieved 18% higher coherence than optimized LDA (0.573 vs. 0.486) with interpretable topics validated through human evaluation (ICC = 0.700, weighted Cohen's kappa = 0.678)

## Executive Summary
This paper presents a systematic framework for applying BERTopic to focus group transcripts using data from ten focus groups exploring HPV vaccine perceptions in Tunisia (1,075 utterances). The authors conducted comprehensive hyperparameter exploration across 27 configurations, evaluating each through bootstrap stability analysis, performance metrics, and comparison with LDA baseline. The multi-criteria selection framework yielded a 7-topic model achieving 18% higher coherence than optimized LDA (0.573 vs. 0.486) with interpretable topics validated through independent human evaluation (ICC = 0.700, weighted Cohen's kappa = 0.678). The findings demonstrate that transformer-based topic modeling can extract interpretable themes from small focus group transcript corpora when systematically configured and validated, while revealing that quality metrics capture distinct, sometimes conflicting constructs requiring multi-criteria evaluation.

## Method Summary
The framework applies BERTopic to focus group transcripts through a systematic hyperparameter exploration across 27 configurations using sentence transformer embeddings (paraphrase-multilingual-mpnet-base-v2, 768-dim). The approach combines UMAP dimensionality reduction, HDBSCAN clustering, and class-based TF-IDF topic representation. Each configuration undergoes 30 bootstrap replicates (85% sampling with replacement) to assess stability via NMI and ARI metrics. The multi-criteria selection balances coherence, stability, granularity, and coverage, validated against an optimized LDA baseline and human expert evaluation with three domain experts.

## Key Results
- BERTopic achieved 18% higher coherence than optimized LDA (0.573 vs. 0.486) with interpretable topics
- Bootstrap analysis revealed strong disagreement between stability metrics (NMI vs. ARI correlation r = -0.691)
- Multi-criteria selection framework identified optimal configuration (n_neighbors=15, n_components=10, min_cluster_size=15) yielding 7 interpretable topics
- Human validation confirmed alignment: ICC = 0.700, weighted Cohen's kappa = 0.678

## Why This Works (Mechanism)

### Mechanism 1
Transformer-based embeddings capture contextual semantics that bag-of-words approaches miss in conversational data. BERTopic's sentence embeddings preserve word order and discourse context, enabling disambiguation of phrases whose meaning depends on surrounding dialogue (e.g., "je n'ai pas confiance" referencing different trust domains). The multilingual sentence transformer (paraphrase-multilingual-mpnet-base-v2) adequately captures French-language semantic structure. Break condition: If utterances lack sufficient context (very short turns <5 words), contextual embeddings may not provide advantage over bag-of-words.

### Mechanism 2
Stability is multifaceted—NMI and ARI capture fundamentally different aspects of partition agreement. NMI measures information structure preservation (entropy-based), remaining stable when fine-grained semantic distinctions are consistently recovered despite boundary shifts. ARI measures exact pairwise assignment agreement, highly sensitive to boundary variations in small clusters. Bootstrap resampling at 85% provides meaningful variation to assess robustness without destabilizing the clustering. Break condition: If corpus size is too small (<200 documents), both metrics may become unreliable due to insufficient samples for stable bootstrap estimates.

### Mechanism 3
Semantic coherence and stability (as measured by NMI) represent distinct, often conflicting constructs. Coherence-prioritized configurations produce fewer, coarser topics with better semantic grouping but lower NMI stability. Fine-grained configurations preserve information structure across bootstraps but fragment coherent themes. Cv coherence metric aligns with human interpretability judgments. Break condition: If topics are required for classification tasks requiring reproducibility, coherence-optimization may produce insufficiently stable partitions.

## Foundational Learning

- **BERTopic Architecture (UMAP → HDBSCAN → c-TF-IDF)**: Understanding how dimensionality reduction, density-based clustering, and term weighting interact is essential for interpreting hyperparameter effects. Quick check: Can you explain why min_cluster_size affects both topic count and outlier fraction simultaneously?

- **Bootstrap Stability Analysis**: The paper's central methodological contribution uses bootstrap resampling to quantify model robustness across 30 replicates. Quick check: Why must UMAP and HDBSCAN models be re-instantiated for each bootstrap replicate rather than reused?

- **Partition Similarity Metrics (NMI vs. ARI)**: These metrics showed -0.691 correlation in this study, meaning they can recommend opposite model selections. Quick check: For a classification task requiring reproducible document assignments, which metric would you prioritize and why?

## Architecture Onboarding

- **Component map**: 1,075 preprocessed utterances → sentence-transformers embeddings (768-dim) → UMAP (n_neighbors, n_components) → HDBSCAN (min_cluster_size) → c-TF-IDF topic representation → Bootstrap stability analysis → Multi-criteria selection → Human validation

- **Critical path**: 1) Generate embeddings once (immutable across hyperparameter search) 2) Grid search across n_neighbors ∈ {10,15,30}, n_components ∈ {5,10,50}, min_cluster_size ∈ {5,10,15} 3) For each config: run 30 bootstrap replicates at 85% sampling, compute pairwise NMI/ARI medians 4) Multi-criteria selection: balance coherence, stability, granularity, coverage 5) Human validation with ≥3 domain experts

- **Design tradeoffs**: Higher min_cluster_size → fewer topics, higher coherence, higher ARI, lower NMI; Lower min_cluster_size → more topics, lower coherence, lower ARI, higher NMI; Coherence-optimized models may have 20%+ outlier fractions

- **Failure signatures**: Model collapsing to 2 topics (min_cluster_size too large for corpus); NMI >0.65 with coherence <0.45 (over-fragmented, unstable boundaries); Single topic containing >50% of documents (insufficient discrimination)

- **First 3 experiments**: 1) Replicate the selected configuration (n_neighbors=15, n_components=10, min_cluster_size=15) on the provided corpus to verify 7-topic output and coherence ≈0.57 2) Run ablation with min_cluster_size=5 to observe the NMI/coherence inversion (expect NMI≈0.65, coherence≈0.40) 3) Compare against LDA baseline with 3-15 topics using Cv coherence to confirm the 18% performance gap holds

## Open Questions the Paper Calls Out

- **Generalizability of stability-coherence tradeoff**: Does the observed strong negative correlation between NMI stability and semantic coherence generalize to larger corpora or different languages? The study was restricted to a small corpus (1,075 utterances) of French-language focus groups, making it unclear if the tradeoff is an artifact of specific data characteristics. Applying the same bootstrap analysis framework to diverse datasets would resolve this.

- **Comparison with other neural approaches**: How does BERTopic compare to other neural topic modeling approaches, such as Top2Vec or Contextualized Topic Models (CTM), for focus group analysis? The paper only benchmarked BERTopic against a traditional LDA baseline, leaving the relative performance of different transformer-based architectures unknown. A comparative study optimizing and evaluating BERTopic, Top2Vec, and CTM on identical focus group transcripts would resolve this.

- **Development of interpretability-aligned stability metrics**: Can alternative stability metrics be developed that align better with human judgments of topic interpretability? Current metrics like NMI and ARI measure partition agreement but showed divergent relationships with coherence (r = -0.691 between NMI and ARI), failing to predict semantic quality. Developing new stability measures and correlating them with expert human ratings would resolve this.

## Limitations

- The study's dependency on a single, domain-specific corpus (HPV vaccine perceptions in Tunisia) that cannot be publicly accessed prevents direct replication and limits generalizability.
- The 18% coherence improvement over LDA, while statistically supported, may not generalize to other conversational domains or languages.
- The human validation involved only three domain experts, limiting generalizability of interpretability assessments.

## Confidence

- **High Confidence**: BERTopic's architecture (UMAP→HDBSCAN→c-TF-IDF) successfully extracts interpretable topics from conversational data when systematically configured; stability metrics NMI and ARI capture fundamentally different partition properties
- **Medium Confidence**: The 18% coherence improvement over LDA is robust for this corpus but may not generalize to other domains or languages; semantic coherence and stability represent distinct, sometimes conflicting constructs
- **Low Confidence**: Transformer embeddings provide significant advantage over bag-of-words for conversational nuance (no direct corpus evidence); the multi-criteria framework's superiority over single-metric optimization across domains

## Next Checks

1. Replicate the exact 7-topic configuration (n_neighbors=15, n_components=10, min_cluster_size=15) on the provided test corpus to verify topic count, coherence ≈0.57, and interpretable themes
2. Conduct ablation studies with min_cluster_size=5 to observe the expected NMI/coherence inversion pattern (NMI≈0.65, coherence≈0.40) and validate the stability-coherence tradeoff
3. Test the framework on a different conversational domain (e.g., mental health focus groups) to assess generalizability of the multi-criteria selection approach and coherence improvements