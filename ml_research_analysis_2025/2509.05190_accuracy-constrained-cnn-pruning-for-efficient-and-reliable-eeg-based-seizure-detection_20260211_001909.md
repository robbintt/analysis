---
ver: rpa2
title: Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure
  Detection
arxiv_id: '2509.05190'
source_url: https://arxiv.org/abs/2509.05190
tags:
- pruning
- convolutional
- accuracy
- kernels
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational efficiency and reliability
  challenges of deep learning models, particularly CNNs, for biomedical signal processing
  like EEG-based seizure detection. The authors propose a lightweight 1D CNN model
  enhanced with structured pruning to improve efficiency and generalization.
---

# Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection

## Quick Facts
- arXiv ID: 2509.05190
- Source URL: https://arxiv.org/abs/2509.05190
- Reference count: 17
- Primary result: Structured pruning removes 50% of CNN kernels while improving or maintaining accuracy (92.78%→92.87%) and macro-F1 score (0.8686→0.8707) for EEG seizure detection

## Executive Summary
This paper addresses computational efficiency and reliability challenges in deep learning for biomedical signal processing, specifically EEG-based seizure detection. The authors propose a lightweight 1D CNN model enhanced with structured pruning to improve efficiency and generalization. The pruned model maintains or slightly improves performance metrics while significantly reducing model size, making it suitable for resource-limited, real-time clinical applications.

## Method Summary
The approach combines a baseline 1D CNN with structured pruning based on L1-norm importance scoring. The model is trained with mild early stopping to mitigate overfitting, then 50% of convolutional kernels are removed based on their L1-norm values. The pruned model is retrained with identical hyperparameters. The CNN architecture includes three convolutional layers with batch normalization, ReLU activation, dropout, and max pooling, followed by global average pooling and a fully connected layer. Training uses Adam optimizer with weight decay, cross-entropy loss, and early stopping on validation macro-F1 score.

## Key Results
- Accuracy improved from 92.78% to 92.87% after pruning
- Macro-F1 score increased from 0.8686 to 0.8707
- 50% of convolutional kernels were successfully removed while maintaining predictive capabilities
- Pruning acted as structural regularization, improving generalization

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Structured L1-norm pruning removes redundant convolutional kernels while maintaining predictive performance.
**Mechanism:** L1-norm of kernel weights serves as importance proxy—smaller weights contribute less to activations. By ranking kernels by sⱼ = Σᵢ Σₖ |wᵢⱼₖ| and removing bottom 50%, the model eliminates low-contribution filters while preserving those driving meaningful feature extraction.
**Core assumption:** L1-norm correlates with functional importance; low-weight kernels are learning noise or redundant features rather than task-critical patterns.
**Evidence anchors:** Abstract states pruned network "still able to maintain predictive capabilities"; section 3.3.1 describes retaining top 50% by score.
**Break condition:** If pruned models show significant accuracy drops (>2-3%) or class-specific degradation, the importance scoring metric may not capture true functional relevance.

### Mechanism 2
**Claim:** Pruning combined with retraining acts as structural regularization, potentially improving generalization.
**Mechanism:** After pruning, reduced-capacity model is retrained from retained weights. This forces network to redistribute representational burden across fewer kernels, which may suppress overfitting to noise—a known issue in biomedical signals with high temporal variability.
**Core assumption:** Original model contains overparameterized filters that memorize dataset-specific noise; removing them forces learning of more robust features.
**Evidence anchors:** Abstract mentions "structured pruning removes redundancy, improves generalization"; section 3.3.3 describes pruning as "form of structural regularization."
**Break condition:** If retraining fails to recover performance or validation loss diverges, the pruned architecture may lack sufficient capacity.

### Mechanism 3
**Claim:** Mild early stopping before pruning stabilizes the baseline model and prevents initial overfitting.
**Mechanism:** Training halted based on validation macro-F1 score before full convergence. This yields model that has learned generalizable patterns but not yet memorized training noise, creating better candidate for pruning.
**Core assumption:** Early stopping produces model with meaningful weight structure (not random, not overfit), making importance scores more reliable.
**Evidence anchors:** Abstract mentions "model was trained with mild early stopping to try and address possible overfitting"; section 3.2.2 describes early stopping based on macro-F1 validation score.
**Break condition:** If stopping too early yields unstable importance scores; if stopping too late yields overfit models where pruning provides no regularization benefit.

## Foundational Learning

**Concept: L1-norm as sparsity/importance proxy**
- Why needed here: Understanding why summing absolute weights identifies "unimportant" filters is essential for debugging pruning failures.
- Quick check question: If a kernel has small L1-norm but high correlation with a minority class, would pruning it be safe? (Answer: Not necessarily—class-specific features may be suppressed.)

**Concept: Structured vs. unstructured pruning**
- Why needed here: Structured pruning removes entire filters (hardware-efficient); unstructured removes individual weights. The paper uses structured for deployment benefits.
- Quick check question: Why does structured pruning typically cause larger accuracy drops than unstructured at the same sparsity level?

**Concept: 1D convolutions for temporal signals**
- Why needed here: EEG signals are 1D time series; 1D-CNNs slide kernels along time axis to capture local temporal patterns.
- Quick check question: What temporal pattern would a kernel of size 5 detect that a kernel of size 3 might miss?

## Architecture Onboarding

**Component map:**
EEG Input → [Conv1D + BN + ReLU + Dropout + MaxPool] × 3 → Global Average Pooling → FC → Softmax
↓
L1-norm scoring → Remove bottom 50% kernels → Resize FC → Retrain

**Critical path:**
1. Preprocessing: Standard scaling (zero mean, unit variance) per feature
2. Baseline training with early stopping (patience not specified—needs tuning)
3. L1-norm computation per kernel across all conv layers
4. Threshold at 50th percentile, remove low-scoring kernels
5. Rebuild model with reduced channels, copy retained weights
6. Retrain with identical hyperparameters

**Design tradeoffs:**
- 50% pruning ratio is arbitrary—no ablation shown for 25%/75% alternatives
- L1-norm is simple but may not capture task-specific importance (vs. gradient-based methods like Taylor expansion)
- Early stopping patience is unspecified—could significantly affect baseline quality
- Single dataset validation (CHB-MIT subset)—generalization to other EEG datasets unproven

**Failure signatures:**
- Macro-F1 drops more than accuracy → class imbalance issues exacerbated by pruning
- Validation loss increases post-retraining while training loss decreases → overfitting in pruned model
- High variance across retraining runs → importance scores are unstable

**First 3 experiments:**
1. **Ablation on pruning ratio:** Test 25%, 50%, 75% kernel retention to find optimal tradeoff curve for this dataset.
2. **Importance metric comparison:** Compare L1-norm vs. L2-norm vs. random pruning baseline to validate that importance scoring matters.
3. **Cross-dataset validation:** Train on CHB-MIT, prune, then test on Bonn EEG to assess generalization of the pruned architecture.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Do adaptive pruning strategies yield better efficiency-accuracy trade-offs than the fixed 50% kernel removal rate?
- Basis in paper: [explicit] The authors state in the Conclusion that future work will involve "subsequent modeling of adaptive pruning strategies."
- Why unresolved: The current study strictly evaluated a fixed 50% pruning threshold based on L1-norm scoring, leaving the optimal variable pruning rate unexplored.
- What evidence would resolve it: Comparative experiments using layer-wise or dynamic pruning ratios to find the theoretical limit of compression before accuracy degrades.

**Open Question 2**
- Question: How does structured pruning impact model robustness when detecting concept drift in long-term EEG monitoring?
- Basis in paper: [explicit] The Conclusion explicitly lists the need to "explore the automatic discovery of concept drift for long-term monitoring."
- Why unresolved: The study utilized a static dataset (CHB-MIT subset) with stratified splitting, which does not simulate the temporal shifts found in longitudinal clinical data.
- What evidence would resolve it: Longitudinal studies tracking the pruned model's performance decay and retraining efficiency over extended periods compared to the baseline.

**Open Question 3**
- Question: Can integrating state-of-the-art interpretability methods with the pruned model ensure clinical trust without negating the computational efficiency gains?
- Basis in paper: [explicit] The Conclusion suggests combining pruning with "state-of-the-art interpretability methods" to provide transparency for clinical deployment.
- Why unresolved: While the paper mentions SHAP, it does not quantify if the computational overhead of running complex explainability tools on the pruned model offsets the inference speed gains.
- What evidence would resolve it: An analysis of the latency and energy cost of generating explanations (e.g., SHAP values) on the pruned model versus the baseline.

## Limitations
- Lack of architectural detail—specific kernel sizes, filter counts, and dropout rates are unspecified
- 50% pruning ratio presented without systematic comparison to other ratios
- Single dataset validation (CHB-MIT subset) without cross-dataset generalization testing
- Early stopping criteria (patience and thresholds) are unspecified
- No statistical significance testing or variance reporting across multiple runs

## Confidence

**High Confidence** in general mechanism: L1-norm structured pruning can remove redundant filters while maintaining or slightly improving performance, as demonstrated by modest accuracy and F1 improvements.

**Medium Confidence** in specific quantitative claims: Exact performance gains (+0.09% accuracy, +0.0021 F1) are difficult to verify without full experimental details; lacks statistical significance testing.

**Low Confidence** in generalizability: Without cross-dataset validation, ablation studies on pruning ratios, or comparison to other importance metrics, results may not generalize beyond specific CHB-MIT subset.

## Next Checks
1. **Ablation on pruning ratio**: Systematically test 25%, 50%, and 75% kernel retention rates to establish an optimal tradeoff curve and validate that 50% is not arbitrary.

2. **Importance metric comparison**: Compare L1-norm pruning against L2-norm, random pruning, and gradient-based methods (e.g., Taylor expansion) to demonstrate that the importance scoring actually matters.

3. **Cross-dataset validation**: Train and prune on CHB-MIT, then evaluate on a different EEG dataset (e.g., Bonn EEG) to assess whether the pruned architecture generalizes beyond the original data distribution.