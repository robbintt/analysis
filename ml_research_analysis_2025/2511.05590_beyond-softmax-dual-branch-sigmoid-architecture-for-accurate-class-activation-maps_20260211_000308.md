---
ver: rpa2
title: 'Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation
  Maps'
arxiv_id: '2511.05590'
source_url: https://arxiv.org/abs/2511.05590
tags:
- ours
- sigmoid
- softmax
- localization
- grad-cam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses fundamental distortions in softmax-based
  CAM methods: additive logit shifts that bias importance scores and sign collapse
  that loses feature sign information. The authors propose a dual-branch sigmoid head
  architecture that clones the original classification head into a parallel sigmoid
  branch, fine-tuned with class-balanced binary supervision while freezing the softmax
  branch.'
---

# Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps

## Quick Facts
- arXiv ID: 2511.05590
- Source URL: https://arxiv.org/abs/2511.05590
- Authors: Yoojin Oh; Junhyug Noh
- Reference count: 39
- Primary result: Dual-branch sigmoid architecture eliminates softmax CAM distortions while maintaining classification accuracy

## Executive Summary
This paper identifies two fundamental distortions in softmax-based Class Activation Map (CAM) methods: additive logit shifts that bias importance scores and sign collapse that loses feature sign information. The authors propose a dual-branch sigmoid head architecture that clones the original classification head into a parallel sigmoid branch, fine-tuned with class-balanced binary supervision while freezing the softmax branch. This preserves both magnitude and sign of feature contributions for CAM generation.

Experiments on fine-grained datasets (CUB-200-2011, Stanford Cars) show reduced Average Drop and improved confidence increases in explanation fidelity. On WSOL benchmarks (ImageNet-1K, OpenImages-30K), the method achieves consistent gains in Top-1 Localization, MaxBoxAccV2, and pixel-level AP without accuracy loss. The approach is architecture-agnostic and incurs minimal overhead.

## Method Summary
The core innovation involves creating a parallel sigmoid branch alongside the existing softmax classification head. During fine-tuning, the sigmoid branch is trained with class-balanced binary supervision while the original softmax branch remains frozen. This dual-branch architecture preserves the magnitude and sign information of feature contributions that are typically lost in softmax normalization. The method maintains classification accuracy while generating more faithful CAMs that better reflect true feature importance.

## Key Results
- Achieves consistent gains in Top-1 Localization and MaxBoxAccV2 on WSOL benchmarks
- Reduces Average Drop and improves confidence increases in fine-grained datasets
- Maintains classification accuracy while improving CAM fidelity
- Demonstrates architecture-agnostic performance across different backbone networks

## Why This Works (Mechanism)
The dual-branch architecture works by decoupling the competing objectives of classification accuracy and faithful feature attribution. Softmax normalization inherently distorts feature importance through its competitive nature across classes, while the sigmoid branch trained with balanced supervision preserves the true magnitude and sign of feature contributions. By freezing the softmax branch during sigmoid fine-tuning, the method maintains classification performance while the sigmoid branch learns to generate CAMs that better reflect the actual contribution of each spatial location to the final prediction.

## Foundational Learning

**Class Activation Maps (CAMs)** - Visualization technique for understanding model decisions
- *Why needed*: CAMs are the primary output for model interpretability in this work
- *Quick check*: Verify understanding of how CAMs are generated from feature maps and classification weights

**Softmax normalization** - Probability distribution over classes
- *Why needed*: The paper critiques how softmax distorts feature importance scores
- *Quick check*: Understand how softmax competes across classes and affects logit magnitudes

**Sigmoid activation** - Independent probability estimation per class
- *Why needed*: Used in the alternative branch to preserve feature magnitude and sign
- *Quick check*: Compare sigmoid's non-competitive nature with softmax's competitive normalization

**Class-balanced binary supervision** - Weighted training to handle imbalanced classes
- *Why needed*: Ensures the sigmoid branch learns meaningful representations for all classes
- *Quick check*: Understand how class imbalance affects binary classification training

## Architecture Onboarding

**Component map**: Input -> Backbone -> Dual heads (Softmax + Sigmoid) -> CAM generation
- *Critical path*: Backbone feature extraction -> Dual-head processing -> CAM visualization
- *Design tradeoffs*: Maintains classification accuracy (softmax) vs. faithful attribution (sigmoid)
- *Failure signatures*: CAMs that don't align with actual object regions or show inconsistent importance scores
- *First experiments*: 1) Compare CAMs from baseline softmax vs. proposed method on validation set 2) Measure classification accuracy drop during fine-tuning 3) Analyze feature magnitude distributions between branches

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization beyond tested fine-grained and large-scale classification domains remains unproven
- Computational overhead from maintaining parallel branches during fine-tuning
- Theoretical justification for sign preservation's impact on localization accuracy lacks rigorous mathematical grounding

## Confidence
- Generalization to other tasks (detection, segmentation): Medium confidence
- Architecture-agnostic claim: Medium confidence
- Computational overhead claims: High confidence
- Theoretical grounding of sign preservation: Medium confidence

## Next Checks
1. Test the dual-branch sigmoid architecture on detection-pretrained models and evaluate whether the improvements in CAM fidelity transfer to object detection benchmarks like COCO, particularly for weakly-supervised detection scenarios.

2. Conduct ablation studies systematically varying the freezing duration and learning rate ratios between softmax and sigmoid branches to identify optimal training schedules and quantify sensitivity to these hyperparameters.

3. Evaluate the method's performance when integrated with Vision Transformer architectures and lightweight mobile models to verify the claimed architecture-agnostic benefits across diverse model families.