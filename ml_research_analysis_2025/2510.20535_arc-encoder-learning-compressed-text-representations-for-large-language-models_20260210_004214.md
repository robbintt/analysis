---
ver: rpa2
title: 'ARC-Encoder: learning compressed text representations for large language models'
arxiv_id: '2510.20535'
source_url: https://arxiv.org/abs/2510.20535
tags:
- tokens
- decoder
- encoder
- compressed
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARC-Encoder compresses text contexts into fewer continuous representations
  without modifying the decoder, reducing computational cost while maintaining performance.
  It uses a pooling mechanism to merge hidden states in the encoder's final layer,
  followed by an MLP projector to align outputs with the decoder's embedding space.
---

# ARC-Encoder: learning compressed text representations for large language models

## Quick Facts
- **arXiv ID**: 2510.20535
- **Source URL**: https://arxiv.org/abs/2510.20535
- **Reference count**: 40
- **Primary result**: ARC-Encoder achieves near open-book baseline performance with ×4 compression on QA tasks while preserving the decoder architecture.

## Executive Summary
ARC-Encoder addresses the challenge of processing long contexts in large language models by compressing text into fewer continuous representations without modifying the decoder. The method uses a pooling mechanism in the encoder's final self-attention layer to merge hidden states, followed by an MLP projector to align outputs with the decoder's embedding space. Through alternating pretraining between reconstruction and continuation tasks, ARC-Encoder achieves near open-book baseline performance with 4× compression on QA tasks and extends context windows up to 8× for long documents, outperforming context extension baselines while preserving the decoder architecture.

## Method Summary
ARC-Encoder compresses text contexts by pooling hidden states in the encoder's final self-attention layer, averaging consecutive query vectors while keeping keys and values unchanged. An MLP projector then maps these pooled representations to the decoder's embedding space. The system is pretrained on Common Crawl data using alternating reconstruction (20%) and continuation (80%) tasks, then fine-tuned on downstream tasks. A single encoder can be adapted to multiple decoders with minimal additional parameters through decoder-specific MLPs and special tokens.

## Key Results
- Achieves near open-book baseline performance with 4× compression on QA tasks (SQuAD, NQ, TQA, HotpotQA)
- Extends context windows up to 8× for long documents while outperforming context extension baselines
- Enables a single encoder to serve multiple decoders with <1% additional parameters per decoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pooling in the encoder's final self-attention layer can produce compressed continuous representations that remain interpretable by a frozen decoder.
- Mechanism: In the last self-attention module, consecutive query vectors are averaged (pooled) while keys and values remain unchanged. This reduces the sequence length by the pooling factor (e.g., 4×) while producing vectors that the decoder can process as if they were token embeddings.
- Core assumption: Information should be fully processed by the encoder before pooling; early pooling degrades performance.
- Evidence anchors:
  - [abstract] "uses a pooling mechanism to merge hidden states in the encoder's final layer"
  - [section 3.2] "pooling is performed in the self-attention module. We average consecutive queries to reach the targeted pooling factor, while keys and values remain unchanged."
- Break condition: Pooling before the final layer or using non-contiguous merges (e.g., k-means clustering across distant tokens) disrupts the decoder's ability to interpret representations, especially for translation.

### Mechanism 2
- Claim: An MLP projector can align encoder outputs to a decoder's embedding space, enabling a single encoder to serve multiple decoders with minimal added parameters.
- Mechanism: A 2-layer MLP without activation projects pooled encoder outputs to the decoder's hidden dimension. For multi-decoder use, a shared encoder is paired with decoder-specific MLPs and special tokens (<Cont>, <Rec>), adding <1% parameters per decoder.
- Core assumption: Different decoders have compatible hidden-state geometries that a linear bottleneck can bridge.
- Evidence anchors:
  - [abstract] "followed by an MLP projector to align outputs with the decoder's embedding space"
  - [section 3.3] "we employ a shared encoder but we specialize a projector layer (the MLP) and the special tokens. This set of learned parameters accounts for less than 1% of the encoder's weights."
- Break condition: Decoders with vastly different hidden dimensions or training corpora may require more than a shallow MLP, though the paper shows success with Llama3.1 8B and Mistral 7B.

### Mechanism 3
- Claim: Alternating pretraining between reconstruction and continuation tasks is necessary for the decoder to extract (not just regurgitate) information from compressed representations.
- Mechanism: Reconstruction teacher-forces the decoder to reproduce the original text from compressed tokens, enforcing decompressibility. Continuation teacher-forces the decoder to predict the next tokens given partially compressed context, aligning with inference behavior. Special tokens (<Rec>, <Cont>) signal the task.
- Core assumption: Pure reconstruction leads to representations that the decoder regurgitates rather than uses for downstream tasks; continuation is needed for task alignment.
- Evidence anchors:
  - [section 3.3] "pure reconstruction is easier than compressing contexts for downstream tasks... these compressed representations cannot be well exploited by the decoder on downstream tasks, as the model tends to regurgitate the entire context"
  - [section 3.3] "We alternate between these two pretraining tasks... The two special tokens are crucial to specify the task during pretraining and enable good downstream results."
- Break condition: Omitting continuation or using 100% reconstruction during pretraining reduces average benchmark scores by ~4 points; fine-tuning alone without pretraining also fails.

## Foundational Learning

- Concept: Encoder-decoder architectures and the role of frozen vs. trainable components.
  - Why needed here: ARC-Encoder treats the decoder as a frozen black box to preserve its general abilities and enable plug-and-play use. Understanding what can and cannot be modified is essential.
  - Quick check question: What components of the full system are updated during training, and what remains frozen?

- Concept: Pooling mechanisms in transformers (e.g., mean-pooling, token selection).
  - Why needed here: The core compression operation is pooling in the attention module. Knowing how pooling affects sequence representation and model capacity is key to interpreting results.
  - Quick check question: How does averaging consecutive query vectors differ from using [CLS] token or memory tokens for compression?

- Concept: Pretraining objectives: reconstruction vs. continuation (next-token prediction).
  - Why needed here: The paper's central ablation shows that the mix of these objectives during pretraining is critical for downstream task performance.
  - Quick check question: Why might a model trained only with reconstruction fail to perform well on QA tasks?

## Architecture Onboarding

- Component map:
  - Text Encoder: Llama3.2 3B (last 2 layers removed, no causal mask) -> Pooling Module (averages consecutive queries in final self-attention) -> MLP Projector (2-layer, no activation) -> Frozen Decoder (target LLM)

- Critical path:
  1. Tokenize raw text.
  2. Encode tokens with ARC-Encoder (forward pass through modified encoder → pooling → MLP projection).
  3. Concatenate compressed token representations with any full-text prompt tokens (e.g., few-shot examples).
  4. Feed combined sequence to frozen decoder for generation.

- Design tradeoffs:
  - Pooling factor (4× vs. 8×): Higher factor = more compression but potential performance drop. Paper shows 4× nearly matches open-book baseline.
  - Pretraining vs. fine-tuning pooling factor: Pretraining at a higher factor (e.g., 8×) and fine-tuning at a lower factor (e.g., 4×) can improve results.
  - Encoder size: Truncating encoder layers reduces parameters but may hurt performance on complex tasks like multi-hop QA (HotpotQA).
  - Single vs. multi-decoder: A shared encoder with decoder-specific MLPs trades slight performance (<1 point average drop) for flexibility.

- Failure signatures:
  - Performance near closed-book baseline: Often indicates insufficient pretraining or a missing continuation task in pretraining.
  - Poor performance on translation or summarization: May signal pooling factor too high or pooling method inappropriate (e.g., non-contiguous pooling).
  - Multi-decoder failure: If a single encoder+shared MLP fails, try decoder-specific MLPs; ensure special tokens are properly initialized.
  - Regurgitation instead of answering: Likely caused by pretraining with too much reconstruction and not enough continuation.

- First 3 experiments:
  1. **Pooling factor sweep**: Train ARC-Encoder with PF=4 and PF=8. Evaluate on QA (SQuAD, NQ) and summarization (CNN) to measure the accuracy-efficiency tradeoff.
  2. **Training ablation**: Compare three models: (a) pretraining + fine-tuning, (b) fine-tuning only, (c) pretraining only. This validates the paper's claim that both stages are critical.
  3. **Multi-decoder adaptation**: Train one shared encoder with two decoder-specific MLPs for Mistral 7B and Llama3.1 8B. Compare average performance to single-decoder specialized models to quantify the flexibility cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ARC-Encoder achieve truly universal compressed representations that generalize across decoder architectures with fundamentally different hidden state spaces (e.g., non-Transformer architectures or models with significantly different dimensionalities)?
- Basis in paper: [explicit] The conclusion states "This opens the way towards universal compressed representations" and the paper demonstrates multi-decoder training, but Table 2 shows that adapting to OLMo-7B (a new decoder not jointly trained) results in a larger performance gap to open-book baseline compared to jointly trained decoders (33.6 vs 39.6 average).
- Why unresolved: The paper only tests on decoder families with similar architectures (Llama, Mistral). The MLP projector adds only ~1% parameters for adaptation, but it remains unclear whether this approach scales to architectures with fundamentally different embedding spaces or attention mechanisms.
- What evidence would resolve it: Experiments adapting ARC-Encoder to diverse architectures such as Mamba-style state-space models, models with different hidden dimensions (e.g., 2048 vs 8192), or decoders from completely different training pipelines.

### Open Question 2
- Question: What architectural or training modifications would enable effective compression at pooling factors beyond ×8 without sharp performance degradation?
- Basis in paper: [explicit] Section A.2 states "When pooling too much performance degrades sharply as with a pooling factor of 32 where the model has an averaged score of 33.1" (compared to ~42-46 at PF=4-8).
- Why unresolved: The paper empirically demonstrates the degradation but does not investigate its root causes or potential mitigations. The information bottleneck at high compression ratios may be inherent, or it may be addressable through architectural innovations.
- What evidence would resolve it: Ablation studies at PF=16 and PF=32 testing: (a) hierarchical pooling across multiple layers, (b) increased MLP bottleneck capacity, (c) modified training objectives that explicitly optimize for high-compression scenarios.

### Open Question 3
- Question: Can the pretraining budget of ~2.6B tokens be reduced while maintaining downstream performance, and what is the minimal effective pretraining scale?
- Basis in paper: [inferred] Section 4.1 states pretraining uses "approximately 2.6B tokens" and Section 4.4 notes that pretraining is "essential" with +16-19 point improvements over fine-tuning alone. However, the ablation in Section 4.4 uses only ~2B tokens (75% of the main models) without reporting significant performance drops.
- Why unresolved: The paper demonstrates pretraining necessity but does not systematically explore the relationship between pretraining scale and final performance. The cost-benefit trade-off for pretraining data quantity remains uncharacterized.
- What evidence would resolve it: Controlled experiments varying pretraining token count (e.g., 0.5B, 1B, 2B, 4B) while keeping all other factors constant, measuring downstream task performance and representation alignment quality.

### Open Question 4
- Question: How does the empirically determined 20% reconstruction / 80% continuation pretraining ratio generalize across different decoder architectures, tasks, or compression factors?
- Basis in paper: [inferred] Table 4 shows 20% reconstruction yields optimal results (41.6 avg) compared to 0% (39.8), 50% (41.5), and 100% (37.5). However, this was determined using Mistral 7B at PF=8; it is unclear whether this ratio is universal.
- Why unresolved: The optimal balance between reconstruction (ensuring decompressibility) and continuation (aligning with inference behavior) may depend on task type, decoder characteristics, or compression intensity. A single fixed ratio may not be optimal across all deployment scenarios.
- What evidence would resolve it: Grid search over reconstruction ratios (0%, 10%, 20%, 30%, 50%) across multiple decoders, pooling factors, and fine-tuning task distributions to identify if task-specific or architecture-specific optimal ratios exist.

## Limitations

- The theoretical justification for why pooling must occur only in the final self-attention layer remains underspecified.
- Generalization to decoders with significantly different architectures (different hidden dimensions or attention mechanisms) is untested.
- Performance gains on long-context tasks may partially stem from the quality of synthetic data rather than compression effectiveness alone.

## Confidence

- **High confidence**: The empirical demonstration that ARC-Encoder achieves near open-book baseline performance with 4× compression on QA tasks, and that alternating pretraining between reconstruction and continuation is critical for downstream task performance.
- **Medium confidence**: The claim that a single encoder can serve multiple decoders with minimal performance loss (<1 point average drop) is supported, but the robustness across diverse decoder architectures remains unproven.
- **Low confidence**: The theoretical explanation for why pooling must occur only in the final self-attention layer, and why early pooling or non-contiguous merging fails, is not rigorously established.

## Next Checks

1. **Architectural Ablation**: Systematically test pooling at different layers (not just the final one) and with different merging strategies (e.g., non-contiguous k-means clustering) to validate the claim that only final-layer contiguous pooling preserves decoder interpretability.

2. **Decoder Generalization**: Evaluate ARC-Encoder with a shared encoder across a broader set of decoders with varying hidden dimensions, attention mechanisms, and training corpora to test the limits of the MLP projector's bridging capability.

3. **Synthetic Data Dependency**: Replicate long-context experiments using real (non-synthetic) long documents to isolate the contribution of compression effectiveness from the quality of synthesized data.