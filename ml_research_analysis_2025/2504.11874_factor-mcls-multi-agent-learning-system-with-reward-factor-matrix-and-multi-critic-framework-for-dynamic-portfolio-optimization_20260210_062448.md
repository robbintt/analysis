---
ver: rpa2
title: 'Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic
  framework for dynamic portfolio optimization'
arxiv_id: '2504.11874'
source_url: https://arxiv.org/abs/2504.11874
tags:
- portfolio
- training
- risk
- trading
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in existing deep reinforcement
  learning (DRL) agents for dynamic portfolio optimization, where agents struggle
  to learn the complex factors influencing portfolio returns and risks, and investors
  cannot intervene based on their risk preferences. The authors propose a novel reward
  factor matrix to represent the return, risk, and transaction scale factors of each
  asset, and a multi-critic framework learning system called Factor-MCLS to comprehensively
  learn these factors.
---

# Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization

## Quick Facts
- arXiv ID: 2504.11874
- Source URL: https://arxiv.org/abs/2504.11874
- Reference count: 0
- Primary result: Novel multi-critic DRL system with reward factor matrix improves dynamic portfolio optimization profitability and risk control

## Executive Summary
This paper addresses limitations in existing deep reinforcement learning (DRL) agents for dynamic portfolio optimization, where agents struggle to learn the complex factors influencing portfolio returns and risks, and investors cannot intervene based on their risk preferences. The authors propose a novel reward factor matrix to represent the return, risk, and transaction scale factors of each asset, and a multi-critic framework learning system called Factor-MCLS to comprehensively learn these factors. The system also incorporates a risk constraint term in the policy training objective, allowing investors to intervene based on their risk aversion. Experiments using 29 stocks from the Dow Jones Index show that Factor-MCLS significantly improves profitability and risk control during training, and achieves at least 35.3% higher profitability and 63.9% higher return per unit of risk compared to benchmark strategies in back-testing.

## Method Summary
Factor-MCLS is a hierarchical DRL system for dynamic portfolio optimization. It uses an auxiliary agent (BDA) to provide baseline portfolio weights, and a main agent with a multi-critic framework. The system replaces scalar rewards with a reward factor matrix containing vectors for return, variance, covariance, and transaction scale. Four independent critics learn Q-values for each factor vector, and the actor is trained using a composite of these gradients plus a risk constraint term. The risk constraint allows investors to intervene based on their risk aversion vector, preventing the agent from exploring action spaces that violate investor preferences.

## Key Results
- Factor-MCLS significantly improves profitability and risk control during training compared to baseline DRL agents.
- In back-testing, Factor-MCLS achieves at least 35.3% higher profitability and 63.9% higher return per unit of risk compared to benchmark strategies.
- The risk constraint term effectively prevents the agent from taking excessive risks, improving the Sharpe and Sortino ratios of the portfolio.

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Reward Factorization
- Decomposing scalar rewards into a multi-dimensional reward factor matrix enables the agent to attribute portfolio performance to specific underlying causes rather than observing only the net effect.
- This prevents the "masking" of high risk by high returns during gradient estimation.

### Mechanism 2: Multi-Critic Gradient Isolation
- Assigning distinct critic networks to distinct reward factors allows for more stable convergence and targeted policy updates compared to a single critic.
- This balances objectives that usually conflict, such as maximizing return vs. minimizing variance.

### Mechanism 3: Action-Space Pruning via Risk Constraints
- Embedding a mathematical constraint directly into the actor's objective function prevents the exploration of action spaces that violate investor risk preferences.
- This improves sample efficiency by mathematically forcing the actor to select weights that satisfy the user-defined risk aversion vector.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) in Finance**
  - Why needed: The paper frames portfolio optimization as a finite-horizon MDP. Understanding how continuous portfolio weights map to "Actions" and price fluctuations to "States" is crucial to follow the methodology.
  - Quick check: Can you explain why the "state" includes both market data and "baseline portfolio weights" from an auxiliary agent?

- **Concept: Deterministic Policy Gradient (DDPG)**
  - Why needed: Factor-MCLS is built on top of the DDPG algorithm. Understanding how an "Actor" outputs continuous actions (weights) and a "Critic" estimates the value of those actions is key to understanding how the multi-critic modification alters the gradient flow.
  - Quick check: In standard DDPG, how does the critic calculate the TD-error? How does Factor-MCLS change the target for the critic?

- **Concept: Hierarchical Reinforcement Learning (HRL)**
  - Why needed: The system uses an "Auxiliary Agent" to provide a baseline. Understanding HRL helps explain why the main agent doesn't learn from scratch but rather learns relative to the auxiliary agent's policy.
  - Quick check: What specific role does the auxiliary agent play in the "State" definition?

## Architecture Onboarding

- **Component map:**
  Input (Historical Price Matrix + Auxiliary Agent Baseline Weights) -> Reward Engine (calculates Factor Matrix) -> Multi-Critic Block (4 parallel DDPG Critics) -> Actor Block (Policy network) -> Loss Aggregator (combines Critic outputs + Risk Constraint)

- **Critical path:** The interaction between the Reward Factor Matrix and the Multi-Critic Loss. If the factor matrix is calculated incorrectly, all 4 critics will learn garbage values, and the Actor's risk constraint will fail.

- **Design tradeoffs:**
  - Complexity vs. Control: The system uses 4 critics instead of 1, quadrupling gradient computation cost per step but allowing for explicit control over risk variance.
  - Intervention vs. Optimality: The Risk Constraint term allows investor intervention but introduces a hyperparameter. Poor tuning may create a rigid agent that ignores profitable opportunities.

- **Failure signatures:**
  - Gradient Masking: If the transaction scale critic dominates the loss, the agent may learn to do nothing to avoid transaction costs.
  - Critic Divergence: If one critic's loss explodes while others converge, the composite gradient will be noisy, leading to erratic portfolio weights.
  - Over-constraint: If the reward is consistently negative in early training, the risk constraint might push the policy into a local minimum of inactivity.

- **First 3 experiments:**
  1. **Sanity Check:** Run the environment for 10 steps. Verify the Reward Factor Matrix outputs. Check that "Return Factor" correlates with price change and "Variance Factor" correlates with volatility.
  2. **Ablation Reproduction:** Train two agents: one "Full Factor-MCLS" and one "LSV1" (No risk constraint). Compare the "Accumulated Variance" trajectory to verify the paper's claim that the risk constraint stabilizes variance.
  3. **Hyperparameter Sensitivity:** Vary the Risk Aversion vector. Check if the Actor's final weights shift from high-volatility assets to low-volatility assets as risk aversion increases.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a Bayesian model based on Elliptical distributions improve the auxiliary agent's ability to capture heavy tails and high peaks compared to the current Gaussian assumption?
  - Basis: The BDA auxiliary agent relies on a normal distribution assumption which fails to capture the "heavier tails and sporadic high peaks" of real markets.
  - Why unresolved: The current implementation approximates return distributions as Gaussian for the auxiliary agent's policy function, which the authors acknowledge is a theoretical limitation.
  - What evidence would resolve it: Comparative back-testing results showing the convergence rates and risk-adjusted returns when the auxiliary agent utilizes Elliptical distributions versus the standard Gaussian distribution.

- **Open Question 2:** Can machine learning algorithms effectively determine the optimal risk aversion vector for each asset based on arbitrage opportunities, rather than requiring static investor inputs?
  - Basis: The paper explicitly lists the lack of a model to determine risk aversion based on "arbitrage opportunities among various assets" as a major limitation.
  - Why unresolved: Currently, the risk constraint term relies on a manually defined risk aversion vector input by the investor, which lacks a dynamic, data-driven mechanism for adjustment.
  - What evidence would resolve it: A study demonstrating that an ML-based dynamic risk aversion vector results in statistically higher out-of-sample returns or Sharpe ratios compared to the current method.

- **Open Question 3:** How does the performance of Factor-MCLS degrade when the assumptions of zero slippage and zero market impact are violated in low-liquidity markets?
  - Basis: The paper explicitly selects 29 highly liquid Dow Jones stocks to satisfy Assumptions A1 and A2, leaving the system's robustness in environments with significant transaction costs untested.
  - Why unresolved: The empirical results rely on an idealized execution environment; real-world applicability for assets with lower liquidity requires validation.
  - What evidence would resolve it: Back-testing experiments on small-cap or mid-cap portfolios that include realistic slippage models and transaction costs.

## Limitations

- **Hyperparameter Sensitivity**: The paper does not disclose critical hyperparameters, making exact replication difficult and raising questions about the robustness of performance gains.
- **Generalization to Other Markets**: The evaluation is limited to 29 DJIA stocks from 2019-2022, leaving performance on other indices, asset classes, or longer time horizons untested.
- **Assumption of Factor Decomposition**: The method assumes returns, variance, covariance, and transaction scale can be effectively decomposed into the defined reward factors, which may not be sufficient descriptors of the environment's dynamics in all cases.

## Confidence

- **High Confidence**: The conceptual framework of using a reward factor matrix and multi-critic system to disentangle portfolio performance signals is sound and well-motivated.
- **Medium Confidence**: The empirical results showing superior performance compared to benchmarks are likely valid for the tested dataset and timeframe, but lack of hyperparameter disclosure prevents full assessment of robustness.
- **Low Confidence**: The paper's claims about solving the "thorough understanding" gap and the universal applicability of the factor decomposition are overstated without evidence from diverse market conditions.

## Next Checks

1. **Hyperparameter Sweep**: Conduct a systematic search over the risk aversion vector and other key hyperparameters to assess the sensitivity of Factor-MCLS's performance and identify the configuration that produces the reported results.
2. **Out-of-Sample Robustness**: Evaluate Factor-MCLS on a different stock index (e.g., S&P 500) and a different time period (e.g., pre-2019) to test the generalizability of the performance gains.
3. **Factor Ablation Study**: Remove one or more of the reward factor vectors (e.g., exclude the covariance factor) and retrain the model to quantify the contribution of each factor to the overall performance and validate the assumption of their sufficiency.