---
ver: rpa2
title: 'Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written
  and Multiple LLMs-Generated Text'
arxiv_id: '2501.03212'
source_url: https://arxiv.org/abs/2501.03212
tags:
- text
- llms
- human
- plagiarism
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates differentiating between human-written and
  LLM-generated text, focusing on plagiarism detection in academic contexts. The approach
  involves a two-stage classification: binary (human vs.'
---

# Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text

## Quick Facts
- arXiv ID: 2501.03212
- Source URL: https://arxiv.org/abs/2501.03212
- Authors: Ayat Najjar; Huthaifa I. Ashqar; Omar Darwish; Eman Hammad
- Reference count: 8
- Primary result: Achieved 98.5% accuracy in binary classification and 97% in multi-class classification of human vs. LLM-generated text using TF-IDF features and explainable AI

## Executive Summary
This study presents a novel approach to differentiating between human-written and LLM-generated text using explainable AI techniques. The authors develop a two-stage classification system that first distinguishes human from AI-generated text (binary classification) and then attributes text to specific LLM sources (multi-class classification). Using TF-IDF vectorization and three algorithms (Random Forest, XGBoost, and RNN), the system achieves high accuracy rates of 98.5% and 97% respectively. The integration of LIME (Local Interpretable Model-agnostic Explanations) provides transparency by revealing the specific features that drive classification decisions, enabling identification of unique stylistic patterns for each LLM.

## Method Summary
The study employs a supervised learning approach with a dataset of 600 essays (300 human-written, 300 LLM-generated) covering two topics. Text preprocessing includes tokenization, stop-word removal, lemmatization, and punctuation removal, followed by TF-IDF vectorization to convert text into numerical features. Three algorithms are trained: Random Forest, XGBoost, and RNN. An 80/20 train-test split is used for evaluation. LIME is applied to provide explainability by highlighting the specific words that most influence classification decisions for each text sample.

## Key Results
- Binary classification achieved 98.5% accuracy in distinguishing human from LLM-generated text
- Multi-class classification reached 97% accuracy in identifying specific LLM sources (ChatGPT, LLaMA, Bard, Claude, Perplexity)
- The proposed model outperformed GPTZero in detecting AI-generated text
- LIME explanations revealed unique stylistic features for each LLM, such as "sincerely" for Claude and "ensure" for Bard
- The model successfully identified all test instances in the detection task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-accuracy differentiation of human-written vs. LLM-generated text using supervised classification on TF-IDF features
- Mechanism: A classifier (RF, XGBoost, or RNN) is trained on a labeled dataset where text is converted into numerical vectors using TF-IDF. The model learns to associate specific term frequency patterns with the "human" or "LLM" label
- Core assumption: LLM-generated text exhibits statistically distinct lexical patterns (e.g., word choice, frequency) compared to human writing, which are captured by the TF-IDF representation
- Evidence anchors:
  - [abstract] "Preprocessing includes tokenization, stop-word removal, and TF-IDF vectorization... Results show high accuracy: 98.5% for binary classification..."
  - [section] Page 7, Section 3.2 states, "...TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer was used to make it easier to convert the text data into a machine-learning-friendly format."
  - [corpus] Paper 'Two Birds with One Stone' confirms multi-task detection is viable, though it doesn't specify TF-IDF
- Break condition: Accuracy will degrade if LLMs are fine-tuned to mimic human informal writing styles or if the text is heavily edited/paraphrased, altering the statistical signature

### Mechanism 2
- Claim: Attribution of text to a specific LLM (e.g., ChatGPT, Claude) via multi-class classification, revealing unique stylistic fingerprints
- Mechanism: The classifier is extended to a multi-class problem, learning to map TF-IDF features not just to "AI," but to one of five specific LLM sources. Each source is presumed to have a unique stylistic profile
- Core assumption: Each LLM has a consistent and distinguishable "stylistic fingerprint" (e.g., structural tendencies, preferred vocabulary) that persists across prompts
- Evidence anchors:
  - [abstract] "multi-class (human vs. five specific LLMs)... Explainable AI (LIME) reveals critical features distinguishing each LLM..."
  - [section] Pages 14-16, Section 4.2 show LIME bar charts with top features for each LLM (e.g., "sincerely" for Claude, "ensure" for Bard), supporting the claim of unique profiles
  - [corpus] Paper 'Authorship Attribution in Multilingual Machine-Generated Texts' supports the feasibility of model-specific attribution
- Break condition: If models converge in architecture and training data, their stylistic differences may diminish, making attribution unreliable

### Mechanism 3
- Claim: Model interpretability is achieved using LIME to highlight the specific words driving a classification decision
- Mechanism: LIME (Local Interpretable Model-agnostic Explanations) approximates the complex model's behavior locally around a specific prediction. It generates a simpler, interpretable model that highlights which words contributed most to the final class assignment
- Core assumption: The features identified by LIME (individual words) are meaningful proxies for the model's decision-making and not just spurious correlations from a small dataset
- Evidence anchors:
  - [abstract] "Explainable AI (LIME) reveals critical features distinguishing each LLM..."
  - [section] Page 11, Section 3.4 introduces LIME. Page 16, Section 4.2 shows LIME explanations for a specific text instance, linking the word "sincerely" to the "llama" class
  - [corpus] Paper 'StyleDecipher' also leverages stylistic analysis for explainability, providing indirect support
- Break condition: If the model is overfitted to the small dataset (600 observations), LIME might highlight idiosyncratic features from the training data that do not generalize to new text

## Foundational Learning

- Concept: **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: This is the fundamental technique used to convert raw text into the numerical features the models learn from. Understanding it is critical to interpreting the results
  - Quick check question: If a word like "the" appears frequently in every document in a corpus, how does TF-IDF ensure it doesn't dominate the feature vector for any single document?

- Concept: **Supervised Classification (Binary vs. Multi-class)**
  - Why needed here: The paper's core experiment is a supervised learning task. Distinguishing between binary (human vs. AI) and multi-class (human vs. specific AIs) is essential
  - Quick check question: How does the evaluation metric (e.g., accuracy) become more challenging to interpret in a multi-class problem with six classes compared to a binary problem?

- Concept: **Model-Agnostic Interpretability (LIME)**
  - Why needed here: The paper's major contribution is using LIME to explain "black box" model decisions. Understanding how LIME creates a local surrogate model is key to trusting its explanations
  - Quick check question: Why is it important that LIME is "model-agnostic"? What does that allow the researcher to do?

## Architecture Onboarding

- Component map: Raw Text -> Preprocessor (TF-IDF) -> Trained Model -> Prediction (Human/LLM ID) -> LIME Explainer -> Feature Importance Report
- Critical path: Raw Text -> Preprocessor (TF-IDF) -> Trained Model -> Prediction (Human/LLM ID) -> LIME Explainer -> Feature Importance Report
- Design tradeoffs:
  - **TF-IDF vs. Deep Learning Embeddings:** The authors chose TF-IDF, which is simpler and more interpretable for LIME but may miss complex semantic context that embeddings (e.g., BERT) would capture
  - **Dataset Specificity:** The 600-observation dataset is limited to essays on two specific topics ("Car-free cities" and "Electoral College"). This allows for high accuracy on this specific task but raises concerns about generalizability to other domains
  - **Model Complexity:** RF and XGBoost are easier to interpret than RNNs, which aligns with the goal of explainability. The RNN performed worse on multi-class tasks, suggesting it was harder to train effectively on this small dataset
- Failure signatures:
  - **Overfitting to Topic:** If the model is tested on a completely new topic (e.g., poetry), accuracy may plummet because it learned topic-specific words (e.g., "vote", "car") instead of general stylistic features
  - **RNN Instability:** The paper notes the RNN had poor performance for the "Claude" class (12.5% true positive rate), suggesting it failed to converge on a reliable pattern for that class
  - **Uninformative Explanations:** If LIME highlights generic words (e.g., "the", "is") as top features, it indicates the model failed to learn meaningful stylistic differences during preprocessing
- First 3 experiments:
  1. **Reproduce Binary Baseline:** Re-train the Random Forest model on the provided dataset using TF-IDF to confirm the reported ~98% accuracy on the binary human vs. LLM task
  2. **Topic Generalization Test:** Retrain the model using only data from the "Car-free cities" topic and evaluate its performance on the "Electoral College" data to measure how much the model relies on topic-specific vocabulary vs. general style
  3. **Misclassification Audit:** Use the LIME explainer on examples from the confusion matrix where the model failed (e.g., RNN confusing "Claude" for "human"). Analyze the LIME features to see if they reveal why the model was confused

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the high classification accuracy (98.5%) be maintained when applied to diverse academic domains beyond the two specific prompts ("Car-free cities" and "Electoral college") used for training?
- Basis in paper: [inferred] The methodology section restricts data generation to only two essay topics with a limited sample size (600 observations), suggesting potential overfitting to specific content rather than general stylistic patterns
- Why unresolved: The paper does not test the model's ability to generalize to unseen topics or different genres of writing (e.g., creative writing, scientific reports)
- What evidence would resolve it: Evaluating the trained Random Forest and XGBoost models on an external dataset containing radically different prompts and academic subjects

### Open Question 2
- Question: Do the stylistic features identified by LIME (e.g., "ensure," "system") remain robust predictors when LLM-generated text is paraphrased or edited by humans?
- Basis in paper: [inferred] The Related Works section discusses the difficulty of identifying "paraphrasing," and the study focuses on raw LLM output without testing adversarial modifications
- Why unresolved: Real-world plagiarism often involves modifying AI text to bypass detectors, a scenario not addressed by the current binary classification of raw outputs
- What evidence would resolve it: Testing the model's performance on a dataset where the LLM-generated portions have been manually rewritten or processed through paraphrasing tools

### Open Question 3
- Question: How does the evolution of LLMs (e.g., updates to ChatGPT or Claude) impact the long-term stability of the "stylistic and structural elements" used for attribution?
- Basis in paper: [inferred] The dataset was compiled in November 2023, and the Related Works section notes the "rapidly evolving subject" and the need to support "continual advancements"
- Why unresolved: The paper establishes a baseline for specific LLM versions, but it is unclear if these unique signatures persist as models are fine-tuned or updated
- What evidence would resolve it: A longitudinal study analyzing feature drift in the classifiers as newer versions of the included LLMs are released

## Limitations
- Small dataset (600 observations) raises overfitting concerns, particularly for the RNN model
- Limited topic diversity (only two essay prompts) constrains generalizability
- Missing technical details for RNN architecture and LLM generation parameters
- TF-IDF may miss semantic patterns that more sophisticated embeddings could capture

## Confidence
- **High Confidence:** Binary classification accuracy (98.5%) and LIME explainability mechanism
- **Medium Confidence:** Multi-class attribution accuracy (97%) due to potential overfitting and RNN instability
- **Low Confidence:** Generalization to other domains and detection of paraphrased/edited AI text

## Next Checks
1. Test model performance on essays from completely different topics to assess topic bias
2. Evaluate model robustness against human-edited AI-generated text
3. Compare TF-IDF features against contextual embeddings (BERT) to quantify semantic information loss