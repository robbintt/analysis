---
ver: rpa2
title: 'SafeSpeech: A Comprehensive and Interactive Tool for Analysing Sexist and
  Abusive Language in Conversations'
arxiv_id: '2503.06534'
source_url: https://arxiv.org/abs/2503.06534
tags:
- detection
- toxic
- content
- conversation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeSpeech is a comprehensive platform for toxic content detection
  and analysis in conversations, bridging message-level and conversation-level insights.
  It integrates fine-tuned classifiers and large language models to enable multi-granularity
  detection, toxic-aware summarization, and persona profiling, with explainability
  via perplexity gain analysis.
---

# SafeSpeech: A Comprehensive and Interactive Tool for Analysing Sexist and Abusive Language in Conversations

## Quick Facts
- arXiv ID: 2503.06534
- Source URL: https://arxiv.org/abs/2503.06534
- Reference count: 30
- SafeSpeech achieves 0.88 F1 for binary sexism detection and 0.60 F1 for fine-grained classification using ensemble methods

## Executive Summary
SafeSpeech is a comprehensive platform for toxic content detection and analysis in conversations, bridging message-level and conversation-level insights. It integrates fine-tuned classifiers and large language models to enable multi-granularity detection, toxic-aware summarization, and persona profiling, with explainability via perplexity gain analysis. Evaluations on EDOS, OffensEval, and HatEval datasets show state-of-the-art performance, with M7-FE achieving 0.88 F1 for binary sexism detection and 0.60 F1 for fine-grained classification. The platform also provides interactive tools for conversation-level analysis, including toxic-aware summaries and Big Five personality trait predictions.

## Method Summary
SafeSpeech combines fine-tuned transformer models (DeBERTa-v3-large, RoBERTa-Large, Mistral 7B) through an ensemble voting mechanism (M7-FE) for message-level toxic content detection. For conversation-level analysis, it employs perplexity gain to attribute toxic predictions to specific phrases, uses semantic chunking with InstructDS (Flan-T5-XL) for toxic-aware summarization, and generates Big Five personality profiles via LLM prompts. The system is implemented as a Streamlit frontend with FastAPI backend, supporting both API-based and local LLM deployments through Ollama.

## Key Results
- M7-FE achieves 0.88 F1 for binary sexism detection on EDOS
- Perplexity gain analysis successfully highlights influential toxic phrases in positive predictions
- Toxic-aware summarization preserves harmful content visibility in condensed outputs
- Big Five personality profiling generates interpretable speaker characterizations

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Classification with Fallback Voting
Combining multiple pre-trained language models through voting-based ensemble improves toxic content detection over single-model approaches. M7-FE (Mistral-7B Fallback Ensemble) aggregates predictions from DeBERTa-v3-large, RoBERTa-Large, and Mistral 7B. Each model captures different linguistic patterns; voting reduces individual model biases and improves robustness across toxicity subtypes.

### Mechanism 2: Perplexity Gain for Attribution
Measuring perplexity changes when removing sentences identifies which phrases most influence toxicity predictions. Given input X with sentences (s₁...sₙ) and output Y, compute: R(sᵢ, Y|X) = PPL(Y|X\sᵢ) − PPL(Y|X). Higher scores indicate sentences critical to the LLM's prediction. Visualized as heatmaps over conversation text.

### Mechanism 3: Toxic-Conditioned Summarization
Condition conversation summaries on message-level toxicity labels to preserve harmful content visibility in condensed outputs. Semantic chunking groups topically related utterances using sentence-embedding cosine similarity. InstructDS (Flan-T5-XL) generates summaries per chunk with instructions to highlight messages previously flagged as toxic.

## Foundational Learning

- **Transformer-based sequence classification (DeBERTa, RoBERTa, Mistral)**: Why needed here - Message-level detection relies on fine-tuned PLMs. Understanding attention mechanisms and token-level representations helps debug classification failures. Quick check question: Can you explain why DeBERTa's disentangled attention mechanism might capture negation patterns differently from RoBERTa?

- **Perplexity as confidence proxy in LLMs**: Why needed here - Perplexity gain analysis assumes perplexity inversely tracks model confidence. Without this intuition, the attribution mechanism is opaque. Quick check question: If an LLM assigns PPL=15 to output Y given input X, and PPL=45 given X\sᵢ, what does this suggest about sentence sᵢ's contribution?

- **Semantic chunking via embedding similarity**: Why needed here - Conversation summarization requires segmenting long dialogues into coherent units. Threshold selection for cosine similarity directly affects chunk boundaries. Quick check question: Given two adjacent sentences with embedding cosine similarity 0.72 versus a threshold of 0.75, should they be grouped into the same chunk?

## Architecture Onboarding

- **Component map**: Frontend (Streamlit) -> Data Manager -> Message Analysis -> Conversation Analysis -> AI Assistant -> Result Viewer -> Backend (FastAPI) routes requests to model endpoints; manages state between classification and downstream modules

- **Critical path**: Upload/select data → Data Manager parses structure → Message Analysis runs fine-tuned classifiers → returns labels per message → Conversation Analysis receives labels → LLM generates toxic/no-toxic judgment → Perplexity Gain computed on positive predictions → heatmap generated → Semantic chunking → InstructDS generates toxic-aware summaries → Persona Analysis concatenates summaries → LLM outputs Big Five scores + justifications

- **Design tradeoffs**: M7-FE ensemble provides higher accuracy vs. 3× inference cost; not suitable for real-time low-latency moderation. Perplexity gain only meaningful for positive predictions; negative cases lack attribution. Persona via LLM offers flexible, interpretable outputs vs. no clinical validation. Local LLMs (Ollama) provide privacy/control vs. hardware requirements.

- **Failure signatures**: M7-FE underperforms on non-sexism tasks (Table 1: 0.48–0.51 F1 on HatEval/OffensEval/AbusEval vs. 0.88 on EDOS binary) → likely domain mismatch. Heatmap shows uniform perplexity gains → LLM not conditioning on specific sentences; check prompt formatting. Summaries missing toxic content → verify message-level labels passed correctly; check semantic chunking threshold.

- **First 3 experiments**: 1) Validate M7-FE on held-out data: Split EDOS test set further; measure calibration (reliability diagrams) to check if 0.88 F1 reflects genuine discriminative power or threshold artifacts. 2) Ablate perplexity gain: For conversations with known toxic spans (synthetic or annotated), compare heatmap overlap with ground truth; quantify precision/recall of attribution. 3) Stress-test summarization: Upload multi-turn conversations with subtle coercive control; verify summaries retain these patterns rather than defaulting to topic-level descriptions.

## Open Questions the Paper Calls Out

1. How effective is perplexity gain as an explainability mechanism for toxic content detection compared to its original application in abstractive summarization? The authors state in the Limitations section that "its effect on toxic content detection still requires more examination."

2. To what extent do implicit biases in the LLMs affect the accuracy of toxic content detection across different demographic groups? The Limitations section notes the platform "has not analyzed the bias analysis of LLMs" despite acknowledging that "implicit bias" likely exists in the training data.

3. How well do the generated Big Five personality trait profiles correlate with clinically validated psychological assessments? The authors warn that the persona analysis "has not been clinically validated and the output should not be used for professional diagnosis."

## Limitations

- M7-FE ensemble shows significant performance degradation on non-sexism tasks (0.48-0.51 F1 on HatEval/OffensEval/AbusEval vs. 0.88 on EDOS binary), indicating domain-specific optimization
- Perplexity gain attribution mechanism lacks corpus validation and only works for positive predictions, with the paper explicitly noting it requires more examination
- Persona profiling outputs have no clinical validation and should not be used for operational decisions

## Confidence

- **High Confidence**: Core message-level detection pipeline using fine-tuned transformers is well-established with internally consistent F1 scores on EDOS
- **Medium Confidence**: Conversation-level analysis using LLM APIs is plausible but depends heavily on prompt quality and API stability
- **Low Confidence**: Perplexity gain attribution mechanism has weakest support - lacks validation on toxic language datasets and only works for positive predictions

## Next Checks

1. Validate M7-FE Calibration: Split the EDOS test set further to create a held-out validation subset. Measure reliability diagrams and expected calibration error to determine whether the 0.88 F1 reflects genuine discriminative power or threshold artifacts.

2. Ablate Perplexity Gain Attribution: Use synthetic conversations with known toxic spans or manually annotated datasets to compare perplexity gain heatmaps against ground truth. Quantify precision/recall of the attribution method.

3. Cross-Domain Stress Test: Evaluate M7-FE on non-social-media toxic language datasets to measure domain transfer. Compare performance drop against single-model baselines to determine if ensemble benefits persist outside the training distribution.