---
ver: rpa2
title: 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models'
arxiv_id: '2508.06471'
source_url: https://arxiv.org/abs/2508.06471
tags:
- glm-4
- training
- reasoning
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GLM-4.5 is a 355B-parameter Mixture-of-Experts (MoE) language model
  designed to unify agentic, reasoning, and coding capabilities. It employs hybrid
  reasoning modes, extensive multi-stage training on 23T tokens, and reinforcement
  learning across three expert domains.
---

# GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models

## Quick Facts
- arXiv ID: 2508.06471
- Source URL: https://arxiv.org/abs/2508.06471
- Reference count: 40
- Primary result: 355B-parameter MoE model achieving 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified

## Executive Summary
GLM-4.5 is a 355B-parameter Mixture-of-Experts language model designed to unify agentic, reasoning, and coding capabilities through a hybrid approach supporting both thinking and non-thinking modes. The model employs a deep architecture (92 layers) with reduced width (5120 hidden dimension) and extensive multi-stage training on 23T tokens, followed by reinforcement learning with a two-stage difficulty curriculum. GLM-4.5 achieves state-of-the-art open-source performance across reasoning, coding, and agentic benchmarks while using significantly fewer parameters than competing models.

## Method Summary
GLM-4.5 uses a Mixture-of-Experts architecture with 3 dense and 89 MoE layers, 160 total experts (8 active per token), and 96 attention heads. The training pipeline spans 23T tokens across three stages: 15T general pre-training, 7T code/reasoning continual pre-training, and post-training reinforcement learning. The RL phase uses GRPO with a two-stage difficulty curriculum, separating moderate and extremely difficult problems to maintain reward variance. A single-stage 64K RL approach avoids long-context capability degradation. Expert models for Reasoning, Agent, and General domains are unified through self-distillation.

## Key Results
- TAU-Bench: 70.1% (state-of-the-art for open-source models)
- AIME 24: 91.0% (leading performance)
- SWE-bench Verified: 64.2% (competitive with closed models)
- BFCL v3: 77.8% (top-tier coding performance)
- BrowseComp: 26.4% (strong agentic capabilities)

## Why This Works (Mechanism)

### Mechanism 1: MoE Depth-Width Tradeoff for Reasoning
Deeper MoE models with reduced width exhibit better reasoning capacity than wider, shallower alternatives at comparable compute. By increasing layer count (89 MoE layers) while reducing hidden dimension (5120) and expert count (160 vs 256-384), the model creates longer computational paths for multi-step reasoning. The 2.5x increase in attention heads (96 vs typical 32-40) improves fine-grained token relationships without improving training loss.

### Mechanism 2: Two-Stage Difficulty Curriculum in RL
Separating RL into moderate-difficulty (pass@8 > 0) and extreme-difficulty (pass@512 > 0) stages prevents gradient starvation from uniform reward signals. Early training on moderate problems maintains reward variance for useful gradients, while switching to extremely difficult problems pushes the model past its initial ceiling by forcing exploration of rare correct trajectories.

### Mechanism 3: Single-Stage 64K RL Avoids Capability Unlearning
Direct RL training at target context length (64K) outperforms progressive length escalation because shorter intermediate stages cause irreversible forgetting of long-context abilities. The cold-start SFT already conditions the model on 64K outputs, and introducing RL stages with shorter maximum lengths allows the model to optimize for abbreviated reasoning, reducing average output length in ways that cannot be recovered.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Understanding how tokens are distributed to 8 of 160 experts is essential for debugging load imbalance. Can you explain why auxiliary load-balancing losses might interfere with gradient signals compared to loss-free approaches?
- **Group Relative Policy Optimization (GRPO)**: The RL training builds on GRPO without KL loss; understanding group-wise advantage estimation is prerequisite for modifying reward functions. How does GRPO differ from PPO in its treatment of the KL divergence term and advantage normalization?
- **Chain-of-Thought (CoT) Distillation**: Unified training distills multiple expert models into a single hybrid-reasoning model; SFT data preparation requires balancing thinking vs non-thinking samples. What data ratio between full-CoT and non-CoT samples would preserve both deliberative and fast-response modes?

## Architecture Onboarding

- **Component map**: Input → Embedding → 3 Dense layers (warm-up) → 89 MoE layers (routing + expert computation) → Output projection → MTP layer (inference draft generation)
- **Critical path**: 1) Input embedding passes through 3 dense layers 2) Tokens route to 8 of 160 experts per MoE layer 3) Expert outputs combine via sigmoid-gated routing 4) MTP layer generates draft tokens for speculative decoding during inference
- **Design tradeoffs**: Depth vs Width prioritizes reasoning over knowledge density (SimpleQA: 30.0 vs Kimi-K2's 35.3); 160 experts reduces routing complexity but may limit specialization; 96 heads (2.5x typical) improves reasoning benchmarks without training loss improvement
- **Failure signatures**: Load imbalance (>80% tokens to <20% experts indicates bias update rate issues); reasoning regression when using progressive length RL; reward hacking when reward increases but SysBench-ISR stagnates
- **First 3 experiments**: 1) Ablate head count: Train with 64 vs 96 heads on identical data; measure MMLU/BBH delta 2) Curriculum difficulty calibration: Sweep pass@k thresholds for stage-1→stage-2 transition 3) Single vs multi-stage RL at 32K vs 64K: Replicate Figure 6 with controlled seed

## Open Questions the Paper Calls Out

- Why does increasing model depth (more layers) with reduced width improve reasoning capacity, contrary to wider architectures used in DeepSeek-V3 and Kimi K2?
- Why does using 2.5× more attention heads improve reasoning benchmark performance despite showing no improvement in training loss?
- What is the optimal strategy for dynamically selecting between thinking mode and direct response mode in hybrid reasoning systems?
- Can the single-stage RL approach at 64K context length generalize beyond the specific training setup described?

## Limitations

- Data quality filtering process lacks exact thresholds and weight distributions for Nemotron-CC style bucketing
- Reinforcement learning implementation details, including reward model architectures and GRPO hyperparameters, are incomplete
- Synthetic reasoning data generation pipeline and agent trajectory construction methods are not sufficiently described
- Long-context capability retention claims lack comprehensive ablation studies across different model scales

## Confidence

**High Confidence Claims:**
- MoE architecture with 160 experts and 92 total layers produces functional models
- Two-stage RL curriculum concept produces measurable improvements on benchmark tasks
- Hybrid reasoning mode (thinking/non-thinking) can be successfully unified through distillation

**Medium Confidence Claims:**
- Depth-to-width tradeoff specifically benefits reasoning benchmarks more than knowledge tasks
- Single-stage 64K RL approach prevents capability unlearning better than progressive length scaling
- Loss-free balance routing performs equivalently to auxiliary balance losses for load distribution

**Low Confidence Claims:**
- Exact performance contribution of each architectural modification relative to baseline models
- Generalizability of two-stage RL curriculum beyond specific problem sets used
- Scalability of single-stage 64K RL approach to models with different parameter counts or domains

## Next Checks

1. **Architectural Ablation Study**: Train a controlled ablation model isolating head count (64 vs 96 attention heads) on identical data and compute budget. Measure reasoning benchmark delta (MMLU, BBH) to validate whether the 2.5x head increase specifically improves reasoning without training loss benefits.

2. **Curriculum Learning Replication**: Implement two-stage RL curriculum on a smaller-scale model (7B parameters) with independently sourced problem sets. Systematically vary pass@k thresholds for stage transitions and measure whether performance gains from switching to extremely difficult problems persist across different difficulty calibration schemes.

3. **Long-Context Training Stability Test**: Conduct controlled experiments comparing single-stage 64K RL against multi-stage progressive length scaling using identical seeds and data ordering. Track long-context retention metrics (perplexity on 32K-128K documents) throughout training to determine whether irreversible degradation in Figure 6 is reproducible.