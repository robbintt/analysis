---
ver: rpa2
title: 'Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates
  Hallucinations'
arxiv_id: '2511.03578'
source_url: https://arxiv.org/abs/2511.03578
tags:
- entropy
- conservation
- laws
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Constraint-Projected Learning (CPL), a training
  framework that eliminates both hard and soft violations of physical laws in neural
  PDE solvers. CPL projects network predictions onto the lawful manifold defined by
  conservation, Rankine-Hugoniot balance, entropy, and positivity after every gradient
  step, ensuring physical admissibility by construction.
---

# Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations

## Quick Facts
- **arXiv ID:** 2511.03578
- **Source URL:** https://arxiv.org/abs/2511.03578
- **Reference count:** 13
- **Primary result:** Constraint-Projected Learning (CPL) eliminates both hard and soft physical law violations in neural PDE solvers by projecting predictions onto the lawful manifold after every gradient step, achieving machine-precision conservation and stability over long horizons.

## Executive Summary
This paper introduces Constraint-Projected Learning (CPL), a training framework that eliminates both hard and soft violations of physical laws in neural PDE solvers. CPL projects network predictions onto the lawful manifold defined by conservation, Rankine-Hugoniot balance, entropy, and positivity after every gradient step, ensuring physical admissibility by construction. This projection is differentiable and adds only ~10% computational overhead, making it fully compatible with backpropagation. The method is extended with Total-Variation Damping (TVD) to suppress spurious oscillations and a rollout curriculum to enforce stability over multiple prediction steps. On Burgers and Euler systems, CPL with TVD reduces mass and entropy errors to machine precision, eliminates positive total-variation growth, and keeps long-horizon prediction errors bounded.

## Method Summary
CPL implements a geometric projection framework where neural network predictions are projected onto the intersection of physical constraint sets after each gradient update. The method uses a differentiable projection chain that enforces conservation laws (finite-volume form), Rankine-Hugoniot shock conditions, entropy inequalities, and positivity constraints. TVD loss penalizes positive growth in total variation to suppress oscillations, while a rollout curriculum progressively extends multi-step prediction training. The projection adds ~10% overhead and guarantees physical admissibility by construction, unlike soft penalty methods that only encourage compliance.

## Key Results
- Mass conservation errors reduced to ~10⁻¹⁰ and entropy violations to machine precision on Burgers and Euler systems
- Positive total-variation growth eliminated (average TVD growth = 0.0) while preserving sharp shocks
- Long-horizon stability achieved: 40-step rollout maintains MSE = 6.0×10⁻⁵ vs 2.8×10⁻⁴ for single-step training
- Computational overhead of projection framework is only ~10% compared to standard neural solvers

## Why This Works (Mechanism)

### Mechanism 1: Geometric Projection onto Lawful Manifold
- Post-hoc projection of predictions onto the intersection of physical constraint sets eliminates hard violations (mass creation, RH imbalance, negative densities) at machine precision.
- After each gradient step, apply the update θ_{k+1} = Π_C(θ_k - η∇_θ L), where Π_C projects onto the lawful manifold C = C_box ∩ C_cons ∩ C_RH ∩ C_Ent ∩ C_divfree.
- Each constraint set is convex or locally convex, ensuring the projection is non-expansive (∥Π_C(x) - Π_C(y)∥ ≤ ∥x - y∥) and thus cannot amplify errors.
- If constraint sets are non-convex with high curvature (e.g., strongly coupled multi-physics), projection may not converge or may require many iterations.

### Mechanism 2: Total-Variation Damping (TVD) as Soft Constraint Regularization
- Penalizing positive growth in total variation suppresses spurious oscillations ("soft hallucinations") without blurring genuine discontinuities.
- Add loss term L_TVD = max(0, TV(U^{n+1}) - TV(U^n)) to the objective, optionally excluding cells flagged by a shock sensor (χ_i > χ_thr) from the TVD sum.
- Physical solutions should not increase spatial roughness over time; genuine shocks are localized and can be detected by curvature sensors.
- If shock sensor threshold χ_thr is mis-set, TVD may overly damp genuine features (if too low) or allow oscillations (if too high).

### Mechanism 3: Rollout Curriculum for Temporal Consistency
- Training with progressively longer multi-step rollouts teaches the model to maintain bounded errors and physical admissibility over extended horizons.
- During training, require the model to predict R consecutive steps forward, with R increasing linearly from 1 to R_max (e.g., 8).
- Stability learned over short rollouts generalizes to longer inference sequences; the projection mechanism prevents error accumulation.
- If training rollouts are too long relative to data availability, the model may overfit to specific trajectories.

## Foundational Learning

- **Concept: Weak form of PDEs and finite-volume discretization**
  - Why needed here: CPL operates on discrete cell residuals R_i^n derived from the weak (integral) form, not pointwise PDE residuals.
  - Quick check question: Given a 1D conservation law ∂_t U + ∂_x f(U) = 0, can you derive the finite-volume residual R_i for cell i with flux F_{i+1/2} at faces?

- **Concept: Rankine-Hugoniot jump condition**
  - Why needed here: Shocks require special handling—the RH condition Jf(U)·nK = s_n JUK ensures correct shock speeds.
  - Quick check question: For Burgers' equation with shock speed s, what is the RH condition relating left state U⁻, right state U⁺, and flux f(U) = U²/2?

- **Concept: Convex projection and Dykstra's algorithm**
  - Why needed here: CPL composes multiple projectors; understanding alternating projection methods is necessary for implementing the projection chain.
  - Quick check question: If projecting onto two convex sets A and B via alternating projections, under what conditions does the sequence converge to a point in A ∩ B?

## Architecture Onboarding

- **Component map:**
  Input (x, t) → Neural Network → Raw prediction U_raw → Loss computation (FV residual, entropy, RH, TVD, bounds) → Gradient step (standard backprop) → Projection chain: Π_Helmholtz ∘ Π_bounds ∘ Π_entropy ∘ Π_RH ∘ Π_FV → Constrained prediction U_CPL

- **Critical path:**
  1. Implement finite-volume residual R_i for your PDE (Section 10, Eq. 1).
  2. Implement individual projectors for each constraint (bounds: clamp/softplus; FV: solve Ax=b; Helmholtz: Poisson solve).
  3. Compose projectors via alternating passes (3-5 iterations typically suffice).
  4. Add TVD loss with shock sensor masking.
  5. Wrap in rollout curriculum if long-horizon stability is required.

- **Design tradeoffs:**
  - Projection overhead (~10%) vs. soft penalty: Projection guarantees admissibility but requires differentiable projectors; soft penalties are easier to implement but only encourage compliance.
  - TVD strength vs. shock sharpness: Higher w_TVD reduces oscillations more aggressively but may blur shocks; use shock sensor to protect discontinuities.
  - Rollout length vs. training cost: Longer rollouts improve temporal stability but increase memory and compute; start with R_max = 4-8.

- **Failure signatures:**
  - Conservation errors > 10⁻⁶: Check that flux computation is consistent across cell faces (telescoping property).
  - Oscillations persist despite TVD: Verify shock sensor is not over-masking; check w_TVD is not decayed to zero too early.
  - Projection non-convergence: Constraint sets may be conflicting (e.g., entropy vs. positivity in degenerate states); add relaxation or slack variables.
  - Exploding gradients through projection: Ensure each projector has a proper backward pass (custom autograd for Poisson solves, diagonal Jacobians for bounds).

- **First 3 experiments:**
  1. **Sanity check—Burgers with CPL only:** Train on 1D Burgers (ν=0.01, N=128) with FV residual and bounds projection. Verify mass drift and RH error reach ~10⁻¹⁰. Compare to baseline PINN.
  2. **Add TVD:** Enable TVD loss with w_TVD = 0.10 and χ_thr = 0.2. Confirm average positive TV growth drops to zero. Visualize shock profile for sharpness.
  3. **Rollout curriculum:** Train with R increasing 1→4 over epochs. Evaluate on 20-step held-out rollout. Compare per-step MSE trajectory to single-step model; expect flatter error curve.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CPL framework be extended to higher-dimensional coupled systems like 3D Euler equations and magnetohydrodynamics (MHD) using only appropriate projectors?
- **Basis in paper:** [explicit] The authors state that "future extensions will require differentiable projectors for higher-dimensional and coupled systems such as Euler and magnetohydrodynamics."
- **Why unresolved:** The paper currently validates the method primarily on 1-D Burgers and Euler systems; extending to multi-dimensions involves complex flux and divergence-free constraints not yet tested.
- **What evidence would resolve it:** Successful training of neural solvers on 2D/3D MHD benchmarks where mass, energy, and divergence-free constraints are satisfied to machine precision.

### Open Question 2
- **Question:** How can the Total-Variation Damping (TVD) mechanism be adapted to strictly preserve sharp discontinuities without introducing excess numerical diffusion?
- **Basis in paper:** [explicit] The conclusion identifies the need for "more adaptive versions of TVD that preserve sharp discontinuities without excess diffusion."
- **Why unresolved:** While the current TVD penalty suppresses oscillations, the authors imply it may act too coarsely, potentially smoothing out genuine physical features that should remain sharp.
- **What evidence would resolve it:** A modified TVD scheme that maintains the exact sharpness of reference shocks while keeping the positive total-variation growth at zero.

### Open Question 3
- **Question:** Do implicit projection solvers improve the efficiency of CPL when applied to stiff source terms or multiscale problems?
- **Basis in paper:** [explicit] The authors plan to "explore longer rollout curricula and implicit projection solvers to improve efficiency on stiff or multiscale problems."
- **Why unresolved:** The current method uses explicit steps and an optional one-step repair, which may become computationally expensive or unstable for stiff chemical reactions or turbulence.
- **What evidence would resolve it:** Demonstration of reduced computational overhead and stable convergence on stiff PDEs using an implicit projection integration scheme.

## Limitations
- Key implementation details underspecified: network architecture, optimizer hyperparameters, and ground truth data generation method
- Projection composition algorithm and convergence criteria not fully detailed
- TVD shock sensor threshold and rollout curriculum parameters require tuning
- Method assumes convex constraint sets and may struggle with non-convex or highly coupled physics
- Computational overhead claims lack validation across problem scales

## Confidence
- **High confidence:** Geometric projection mechanism and its compatibility with backpropagation; TVD's role in suppressing oscillations (classical FV precedent); projection's ability to eliminate hard violations at machine precision
- **Medium confidence:** Rollout curriculum's generalization to longer horizons; TVD's effectiveness specifically for neural solvers; Helmholtz decomposition projector implementation
- **Low confidence:** Constraint set convexity assumptions; projection non-convergence handling; optimal projector composition strategy; exact computational overhead scaling

## Next Checks
1. **Scalability test:** Measure CPL's computational overhead and constraint satisfaction across 2D Euler and 3D Navier-Stokes systems to validate the ~10% overhead claim and identify scaling bottlenecks.

2. **Constraint coupling analysis:** Systematically test projection convergence when constraints conflict (e.g., entropy vs. positivity in degenerate states) and evaluate relaxation strategies.

3. **Generalization benchmark:** Train CPL on Burgers with varying viscosity ν and initial conditions, then test on unseen parameters to quantify extrapolation capability versus soft-penalty approaches.