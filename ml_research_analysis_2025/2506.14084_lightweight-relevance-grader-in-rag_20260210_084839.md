---
ver: rpa2
title: Lightweight Relevance Grader in RAG
arxiv_id: '2506.14084'
source_url: https://arxiv.org/abs/2506.14084
tags:
- relevant
- query
- search
- relevance
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ensuring document relevance
  in Retrieval-Augmented Generation (RAG) systems by proposing a lightweight relevance
  grader. The approach fine-tunes the Llama-3.2-1B model with a binary classification
  head to evaluate the relevance between user queries and retrieved documents.
---

# Lightweight Relevance Grader in RAG

## Quick Facts
- arXiv ID: 2506.14084
- Source URL: https://arxiv.org/abs/2506.14084
- Authors: Taehee Jeong
- Reference count: 38
- Primary result: Fine-tuned Llama-3.2-1B achieves 0.7750 precision on relevance grading, matching much larger models while reducing compute requirements

## Executive Summary
This paper addresses the challenge of ensuring document relevance in Retrieval-Augmented Generation (RAG) systems by proposing a lightweight relevance grader. The approach fine-tunes the Llama-3.2-1B model with a binary classification head to evaluate the relevance between user queries and retrieved documents. By optimizing hyperparameters and leveraging techniques like full fine-tuning and contrastive loss, the model significantly improves precision from 0.1301 to 0.7750 on a dataset of 45,000 query-document pairs. This performance is comparable to the much larger Llama-3.1-70B model, demonstrating that lightweight models can achieve high accuracy while minimizing computational requirements. The proposed solution enhances RAG systems by balancing efficiency and relevance grading accuracy.

## Method Summary
The method involves fine-tuning Llama-3.2-1B-Instruct with a binary classification head to serve as a relevance grader for RAG systems. The model is trained on 45,000 query-document pairs with ground-truth labels generated by Llama-3.1-405B-Instruct. Full parameter fine-tuning is employed along with class imbalance mitigation through oversampling and undersampling techniques. The model uses cross-entropy loss and is evaluated primarily on precision metrics, achieving 0.7750 precision compared to 0.1301 for the baseline model.

## Key Results
- Fine-tuned Llama-3.2-1B achieves 0.7750 precision on relevance grading task
- Performance comparable to Llama-3.1-70B while using only 1.236B parameters
- Class imbalance mitigation through oversampling/undersampling improves precision on minority class
- Cross-entropy loss outperforms contrastive loss for this binary classification task

## Why This Works (Mechanism)

### Mechanism 1: Binary Classification Head Converts Token Predictions to Relevance Scores
Adding a task-specific classification head enables a generative language model to output structured binary relevance decisions. The classification head takes the final hidden state (2048 dimensions) and projects it to a 2-dimensional logit space (relevant/not relevant), replacing the language modeling head's open-ended token prediction with a constrained classification output. The pre-trained model's internal representations already encode query-document semantic relationships that can be linearly separated.

### Mechanism 2: Full Parameter Fine-Tuning Reshapes Internal Representations
Updating all model parameters enables dramatic precision improvements on domain-specific tasks. Full fine-tuning adjusts all 1.236B parameters to optimize the loss surface specifically for relevance grading, rather than relying on fixed pre-trained embeddings that may not align with the target task. The fine-tuning dataset provides sufficient signal to reshape model behavior without catastrophic forgetting.

### Mechanism 3: Class Imbalance Mitigation Improves Precision on Minority (Relevant) Class
Oversampling positive examples and undersampling negative examples counteracts the dataset's heavy negative bias, enabling the model to learn the relevant class more effectively. The raw dataset has ~87.7% negative labels; rebalancing during training ensures the loss function receives meaningful gradients from positive examples rather than being dominated by the majority class.

## Foundational Learning

- Concept: Cross-Encoders vs Bi-Encoders in Re-Ranking
  - Why needed here: This architecture uses a cross-encoder (concatenated query + document input) rather than computing separate embeddings and comparing via cosine similarity
  - Quick check question: Can you explain why cross-encoders capture query-document interactions more directly than bi-encoders, and what the computational tradeoff is?

- Concept: Transfer Learning vs Full Fine-Tuning
  - Why needed here: The paper compares frozen feature extraction against full parameter updates, with a 5.5x precision difference
  - Quick check question: What are the risks of full fine-tuning on a small dataset versus using a frozen backbone with only a classification head?

- Concept: Precision-Recall Tradeoff in Imbalanced Data
  - Why needed here: The paper emphasizes precision as the key metric because false positives in RAG systems can mislead downstream generation
  - Quick check question: For a RAG relevance grader, would you prioritize high precision or high recall, and what goes wrong if you optimize the wrong one?

## Architecture Onboarding

- Component map: Input (query+document) -> Llama-3.2-1B-Instruct backbone -> Linear classification head (2048→2) -> Softmax -> Binary label (relevant/not relevant)

- Critical path:
  1. Generate ground-truth labels using Llama-3.1-405B-Instruct on query-document pairs
  2. Add binary classification head to Llama-3.2-1B-Instruct
  3. Apply class balancing (oversample positives, undersample negatives)
  4. Fine-tune with AdamW optimizer, cosine LR schedule (2e-5 initial, decaying to 10% of peak)
  5. Evaluate on held-out test set using precision as the primary metric

- Design tradeoffs:
  - Full fine-tuning (Config C) vs frozen backbone (Config B): 0.7750 vs 0.1411 precision—full fine-tuning wins but requires more compute
  - Cross-entropy vs Contrastive loss: For Config C, cross-entropy achieves 0.7750 precision vs 0.7256 with contrastive loss
  - LoRA/PEFT vs full fine-tuning: LoRA experiments show very low recall (0.0009–0.0235), indicating poor adaptation for this task

- Failure signatures:
  - Precision stuck at 0.13–0.16: Likely using frozen backbone without classification head (Config B behavior)
  - High accuracy but near-zero recall: Model predicting only the majority (negative) class—class imbalance not properly addressed
  - LoRA runs with low recall but moderate precision: Rank/alpha hyperparameters may be underspecified

- First 3 experiments:
  1. Establish baseline: Run stock Llama-3.2-1B-Instruct on relevance grading without fine-tuning (expect ~0.13 precision per Table III)
  2. Ablation: Compare Config A (full fine-tuning alone) vs Config C (full fine-tuning + classification head) to quantify the classification head contribution
  3. Loss function comparison: Test cross-entropy vs contrastive loss on Config C to validate the paper's finding that cross-entropy yields higher precision

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The 45,000 query-document pairs were generated from news articles, creating a domain-specific dataset that may not generalize to other text domains
- Ground-truth labels derived from Llama-3.1-405B-Instruct assume this teacher model provides accurate relevance judgments, but no independent validation of label quality is provided
- The paper reports precision improvements without corresponding recall metrics in the main results, making it difficult to assess the complete performance tradeoff

## Confidence
**High Confidence** (Strong evidence, well-supported claims):
- The classification head architecture (2048→2 logits) successfully converts token predictions to relevance scores when combined with full fine-tuning
- Full parameter fine-tuning significantly outperforms frozen feature extraction (0.7750 vs 0.1411 precision)
- Class imbalance mitigation through oversampling/undersampling is necessary given the 87.7% negative class distribution

**Medium Confidence** (Supported but with notable gaps):
- Cross-entropy loss outperforms contrastive loss for this binary classification task
- The 1B model's performance is comparable to Llama-3.1-70B (though direct comparison data is limited)
- Full fine-tuning reshapes internal representations effectively for relevance grading

**Low Confidence** (Limited evidence or significant assumptions):
- The lightweight model achieves "comparable" performance to much larger models without comprehensive benchmarking
- The sampling strategy effectively addresses class imbalance without distorting inference calibration
- The approach generalizes beyond the news domain without additional domain adaptation

## Next Checks
1. **Generalization Test**: Apply the trained model to query-document pairs from a different domain (e.g., scientific papers or medical literature) and measure precision/recall degradation to validate whether the 0.7750 precision score is domain-specific or truly represents a general lightweight relevance grading capability.

2. **Label Quality Audit**: Generate a small subset of query-document pairs and have human annotators independently label relevance. Compare human-labeled ground truth against the Llama-3.1-405B-Instruct labels to quantify potential label noise that could affect the reported precision scores.

3. **Full Training Configuration Reproduction**: Implement the complete training pipeline with specific parameters for batch size (e.g., 16 or 32), number of epochs (e.g., 3-5), and exact class balancing ratios (e.g., target 1:1 or 1:2 positive:negative distribution) to determine whether the claimed precision is reproducible with the specified methodology.