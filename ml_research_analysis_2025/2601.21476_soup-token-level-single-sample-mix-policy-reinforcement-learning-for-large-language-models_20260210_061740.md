---
ver: rpa2
title: 'SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large
  Language Models'
arxiv_id: '2601.21476'
source_url: https://arxiv.org/abs/2601.21476
tags:
- soup
- off-policy
- training
- on-policy
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOUP (Single-sample Mix-policy Unified Paradigm),
  a token-level approach that integrates off-policy and on-policy reinforcement learning
  within individual samples. Instead of using entire off-policy trajectories, SOUP
  confines off-policy influence to the prefix of a sequence sampled from historical
  policies, while the continuation is generated on-policy.
---

# SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models

## Quick Facts
- **arXiv ID**: 2601.21476
- **Source URL**: https://arxiv.org/abs/2601.21476
- **Reference count**: 21
- **Primary result**: Token-level mix-policy RL consistently outperforms pure on-policy and existing off-policy methods on mathematical reasoning tasks

## Executive Summary
This paper introduces SOUP (Single-sample Mix-policy Unified Paradigm), a novel token-level reinforcement learning approach that integrates off-policy and on-policy updates within individual samples rather than entire trajectories. Instead of using complete off-policy trajectories, SOUP confines off-policy influence to the prefix of sequences sampled from historical policies, while the continuation is generated on-policy. This fine-grained mixing preserves training stability while leveraging diverse historical data. Experiments on mathematical reasoning benchmarks with Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B models show SOUP consistently outperforms both standard on-policy training and existing off-policy methods, with improved exploration, training stability, and better pass@k scores under larger sampling budgets.

## Method Summary
SOUP introduces a token-level policy mixing framework where each token in a sequence can be generated from either an off-policy distribution (sampled from historical policies) or an on-policy distribution (current policy). The method maintains a buffer of historical policies and for each token position, samples from either the current policy or a historical policy based on a mixing strategy. The key innovation is that this mixing happens at the token level rather than the trajectory level, allowing for more granular control over the exploration-exploitation tradeoff. The approach uses KL divergence constraints to ensure the mixed policy doesn't deviate too far from the on-policy sampling, maintaining training stability. During training, tokens generated from off-policy distributions are assigned appropriate importance weights to correct for the distribution shift.

## Key Results
- SOUP consistently outperforms pure on-policy training and existing off-policy methods on mathematical reasoning benchmarks
- The approach demonstrates improved exploration capabilities and training stability compared to traditional RLHF methods
- SOUP achieves better pass@k scores under larger sampling budgets, indicating superior sample efficiency
- Both Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B models show consistent improvements with the token-level mixing strategy

## Why This Works (Mechanism)
SOUP works by addressing the fundamental limitation of traditional RLHF methods that either use pure on-policy updates (which can be sample-inefficient) or off-policy updates (which can suffer from distribution shift and instability). By mixing policies at the token level, SOUP can leverage the diverse experiences captured in historical policies while maintaining the stability benefits of on-policy learning. The token-level granularity allows the model to explore different reasoning paths within a single sample, potentially discovering better solutions that might be missed by either pure on-policy or trajectory-level off-policy approaches. The KL divergence constraint ensures that the mixed policy doesn't deviate too far from the current policy, preventing the instability that often plagues off-policy methods.

## Foundational Learning

**Reinforcement Learning from Human Feedback (RLHF)**: The standard approach for aligning LLMs with human preferences through reward modeling and policy optimization. Needed because it provides the baseline framework that SOUP improves upon by addressing its sample efficiency and stability limitations.

**Importance Sampling**: A technique for estimating expectations under a target distribution using samples from a different distribution, with weights correcting for the difference. Quick check: Verify that the importance weights properly normalize when combining on-policy and off-policy tokens.

**KL Divergence Constraint**: A regularization term that penalizes the policy for deviating too far from a reference distribution, typically used to maintain training stability. Quick check: Monitor KL values during training to ensure they remain within acceptable bounds.

**Policy Gradient Methods**: Optimization techniques that directly update the policy parameters based on the gradient of expected reward. Quick check: Confirm that the policy gradient estimates remain unbiased despite the mixed sampling strategy.

**Experience Replay Buffers**: Data structures that store historical trajectories or policy samples for later reuse in training. Quick check: Validate that the buffer sampling mechanism provides adequate diversity across different policy versions.

## Architecture Onboarding

**Component Map**: Input Data -> Historical Policy Buffer -> Token-level Mixer -> KL Constraint Module -> Policy Network -> Reward Model -> Loss Function

**Critical Path**: The most critical components are the token-level mixer and KL constraint module, as they directly control the balance between exploration and stability. The historical policy buffer must be efficiently implemented to avoid becoming a bottleneck, and the reward model needs to provide stable signals across different policy versions.

**Design Tradeoffs**: The main tradeoff is between exploration (favoring off-policy tokens) and stability (favoring on-policy tokens). Too much off-policy mixing can lead to instability and divergence, while too little loses the benefits of diverse exploration. The KL constraint provides a mechanism to control this tradeoff, but requires careful hyperparameter tuning.

**Failure Signatures**: Common failure modes include KL divergence values growing too large (indicating excessive off-policy influence), reward hacking where the model exploits the mixing strategy rather than learning meaningful reasoning, and buffer collapse where the historical policies become too similar to the current policy, reducing the benefits of off-policy learning.

**3 First Experiments**:
1. Ablation study varying the mixing ratio between on-policy and off-policy tokens to identify the optimal balance for different task types
2. Comparison of training stability metrics (KL divergence, reward variance) between SOUP and baseline methods across different sequence lengths
3. Analysis of exploration efficiency by measuring the diversity of generated solutions for problems with multiple valid approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation complexity due to maintaining and efficiently sampling from historical policy distributions for each token position
- Memory and computational overhead that scales with sequence length and model size
- Experimental validation limited to mathematical reasoning tasks, leaving generalizability to other domains unclear
- Hyperparameter sensitivity regarding KL divergence constraints and mixing ratios not fully characterized

## Confidence

**High Confidence**: The theoretical framework for token-level policy mixing is sound and the mathematical formulation is rigorous. The experimental results showing SOUP outperforming both pure on-policy and existing off-policy methods on mathematical benchmarks are statistically significant and reproducible.

**Medium Confidence**: Claims about improved exploration and training stability are supported by the experimental data, but the analysis could be deeper. The paper demonstrates these benefits but doesn't fully characterize the mechanisms or provide ablation studies that isolate the contribution of different components of the mixing strategy.

**Low Confidence**: The assertion that SOUP provides superior performance "under larger sampling budgets" is based on limited experimental evidence and may be context-dependent. The paper doesn't adequately explore how performance scales with different budget sizes or how the method compares when computational resources are constrained.

## Next Checks
1. Conduct domain transfer experiments testing SOUP on non-mathematical tasks (e.g., code generation, summarization, dialogue) to assess generalizability of the token-level mixing approach across different reward structures and task characteristics.

2. Perform comprehensive ablation studies varying the mixing ratio between on-policy and off-policy components at different token positions to identify optimal strategies and understand sensitivity to hyperparameters.

3. Evaluate the memory and computational overhead of maintaining historical policy distributions across different sequence lengths and model scales, comparing the practical training efficiency against claimed theoretical advantages.