---
ver: rpa2
title: 'Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery'
arxiv_id: '2502.08037'
source_url: https://arxiv.org/abs/2502.08037
tags:
- latn
- language
- franken-adapter
- adaptation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Franken-Adapter addresses the multilingual capability gap in large
  language models by proposing a modular language adaptation approach through embedding
  surgery. The method creates customized vocabularies for target language groups and
  performs language adaptation via embedding tuning on multilingual data, while keeping
  the transformer body frozen.
---

# Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery

## Quick Facts
- arXiv ID: 2502.08037
- Source URL: https://arxiv.org/abs/2502.08037
- Reference count: 40
- Authors: Fan Jiang; Honglin Yu; Grace Chung; Trevor Cohn
- Primary result: Up to 20% improvement across 96 languages with <1% English regression

## Executive Summary
Franken-Adapter addresses the multilingual capability gap in large language models by proposing a modular language adaptation approach through embedding surgery. The method creates customized vocabularies for target language groups and performs language adaptation via embedding tuning on multilingual data, while keeping the transformer body frozen. These adapted embeddings are then integrated with instruction-tuned LLMs for zero-shot cross-lingual transfer. Experiments on Gemma2 models with up to 27B parameters show improvements of up to 20% across 96 languages on discriminative and generative tasks, with minimal (<1%) regression in English.

## Method Summary
Franken-Adapter consists of three key steps: (1) Tokenizer construction through a "prune-with-extension" method that retains English tokens while adding language-group-specific BPE tokens; (2) Language adaptation where only the embeddings are trained on multilingual data while the transformer body remains frozen; and (3) Composition where the language-adapted embeddings are combined with an instruction-tuned transformer body. Optionally, LoRA-Adaptation can be applied to align the components for generative tasks. The approach is validated on Gemma2 models (2B/9B/27B), PaLM2, and Aya23 across 96 languages in three language groups (SEA, AFR, IND).

## Key Results
- Up to 20% improvement across 96 languages on discriminative and generative tasks
- <1% regression in English performance after adaptation
- 14% improvement over a math-optimized LLM across 20 languages for cross-lingual reasoning
- Customized tokenizers critical for enhancing language adaptation and boosting inference efficiency

## Why This Works (Mechanism)

### Mechanism 1: Tokenization fertility reduction amplifies multilingual data efficiency
By pruning non-English tokens and extending with language-group-specific BPE tokens, the tokenizer produces more efficient representations. This reduces the number of tokens needed for the same text, increasing effective training signal per token and improving downstream performance.

### Mechanism 2: Transformer body encodes language-agnostic knowledge, enabling modular embedding swap
The transformer body, trained on English-centric pre-training and instruction-tuning, is assumed to capture universal semantic abstractions. By keeping it fixed and only tuning new embeddings on multilingual data, language-specific information is isolated to the embedding layer.

### Mechanism 3: LoRA-Adaptation bridges independently trained components for generative tasks
While Franken-Adapter improves discriminative tasks, it can hurt generative tasks due to misalignment between independently trained embeddings and transformer body. A lightweight LoRA-Adaptation phase resolves this incompatibility by fine-tuning low-rank adaptation weights in the transformer's self-attention layers.

## Foundational Learning

- **Tokenizer fertility and efficiency**
  - Why needed here: Understanding how tokenization granularity affects model performance and training efficiency is critical, as Franken-Adapter's gains are partly driven by optimized tokenizers.
  - Quick check question: Can you explain why lower tokenization fertility might improve both training efficiency and downstream performance for low-resource languages?

- **Embedding surgery / parameter isolation**
  - Why needed here: The core mechanism assumes that language-specific information can be isolated to the embedding layer. Understanding this allows proper interpretation of why freezing the transformer works.
  - Quick check question: Why might freezing the transformer body during language adaptation mitigate catastrophic forgetting compared to full-parameter fine-tuning?

- **Low-rank adaptation (LoRA) for parameter-efficient fine-tuning**
  - Why needed here: LoRA-Adaptation is used to align components post-hoc. Engineers need to understand how LoRA works to implement and debug this phase.
  - Quick check question: How does LoRA allow fine-tuning large models with minimal trainable parameters, and what might be limitations in this context?

## Architecture Onboarding

- **Component map:** Tokenizer constructor -> Embedding tuner (transformer frozen) -> Instruction tuner (embeddings frozen) -> Composer (embedding swap) -> LoRA-Adapter (optional)

- **Critical path:** 1. Build customized tokenizer for target language group. 2. Train embeddings on multilingual data (transformer frozen). 3. Independently instruction-tune transformer body on English data (embeddings frozen). 4. Compose: replace original embeddings with new multilingual embeddings. 5. Optionally: perform LoRA-Adaptation on mixed data.

- **Design tradeoffs:**
  - Vocabulary construction: Prune+extension vs. extension-only vs. from-scratch
  - Tokenizer grouping: Separate tokenizers per language group vs. joint tokenizer
  - LoRA-Adaptation data mix: Balance between preserving instruction-following and multilingual capability

- **Failure signatures:**
  - Significant English regression: May indicate excessive pruning of English tokens
  - Poor generative task performance: May indicate need for LoRA-Adaptation
  - Limited improvement on very low-resource languages: May indicate insufficient multilingual data

- **First 3 experiments:**
  1. **Tokenizer ablation:** Compare performance using original vs. customized tokenizer on a single language group to validate the tokenization benefit independently of embedding tuning.
  2. **LoRA-Adaptation analysis:** Measure the impact of LoRA-Adaptation on a generative task vs. discriminative task to understand where alignment is most critical.
  3. **Warmup necessity test:** For a model with limited multilingual capability, compare language adaptation with and without an initial multilingual warmup phase to validate the finding.

## Open Questions the Paper Calls Out

- Does pruning the vocabulary to retain only English tokens inadvertently degrade performance in specialized domains like code generation?
- Does the embedding replacement process compromise the model's safety alignment and ability to reject harmful queries?
- Can the performance gap on generative tasks be closed without requiring additional parameter tuning?

## Limitations

- The approach requires substantial multilingual data (200B tokens) and compute resources, particularly for large models
- Effectiveness for languages outside the three studied groups (SEA, AFR, IND) is not demonstrated
- The method's safety alignment implications are not fully evaluated

## Confidence

**High confidence:** Customized tokenizers improve performance and efficiency; Franken-Adapter achieves significant improvements with minimal English regression; LoRA-Adaptation is necessary for generative tasks.

**Medium confidence:** Tokenizer fertility reduction amplifies multilingual data efficiency; freezing the transformer body mitigates catastrophic forgetting; the approach is broadly applicable to decoder-only LLMs.

**Low confidence:** The assumption that transformer bodies encode language-agnostic knowledge; the claim that Franken-Adapter can transfer reasoning abilities post-hoc.

## Next Checks

1. **Tokenizer fertility analysis:** Measure average tokens per word for each language group using both original and customized tokenizers. Compare this metric directly with downstream performance to validate the relationship between tokenization efficiency and task performance.

2. **Parameter isolation test:** Train a model with full-parameter fine-tuning (instead of embedding-only) on the same multilingual data. Compare English regression and cross-lingual performance to validate that freezing the transformer body specifically mitigates catastrophic forgetting.

3. **Language group generalization:** Apply the approach to a fourth language group not covered in the original study (e.g., Turkic languages). Evaluate whether the same performance gains and tokenizer benefits extend to this new group, testing the method's broader applicability.