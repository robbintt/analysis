---
ver: rpa2
title: Privacy-Enhancing Paradigms within Federated Multi-Agent Systems
arxiv_id: '2503.08175'
source_url: https://arxiv.org/abs/2503.08175
tags:
- privacy
- agents
- arxiv
- agent
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EPEAgents, a privacy-preserving architecture
  for federated multi-agent systems (MAS) that addresses the challenge of protecting
  sensitive information during agent collaboration. The core idea involves deploying
  a privacy-enhancing agent on a trusted server that filters task-relevant, agent-specific
  information during the retrieval-augmented generation (RAG) phase and context retrieval
  stages.
---

# Privacy-Enhancing Paradigms within Federated Multi-Agent Systems

## Quick Facts
- **arXiv ID:** 2503.08175
- **Source URL:** https://arxiv.org/abs/2503.08175
- **Reference count:** 12
- **Primary result:** EPEAgents achieves up to 81.73% privacy improvement while maintaining or slightly improving utility scores in federated MAS systems.

## Executive Summary
This paper introduces EPEAgents, a privacy-preserving architecture for federated multi-agent systems that addresses the challenge of protecting sensitive information during agent collaboration. The core idea involves deploying a privacy-enhancing agent on a trusted server that filters task-relevant, agent-specific information during the retrieval-augmented generation (RAG) phase and context retrieval stages. EPEAgents minimizes data flows by ensuring each agent only receives information relevant to its role, based on initial self-descriptions and dynamic permission elevation. The approach was evaluated using synthetic financial and medical datasets with 21,750 samples across five different LLM backbones (Claude-3.5, GPT-o1, GPT-4o, GPT-3.5-turbo, Gemini-1.5/1.5-pro). Results demonstrate that EPEAgents maintains strong system performance while achieving significant privacy protection improvements.

## Method Summary
EPEAgents implements a 3+n architecture where 3 local agents (domain-specific roles) plus 1+ privacy-enhancing agent (CA) on a trusted server collaborate on tasks. The CA filters user profile entries based on role-field matching using LLM-generated labels. Agents send self-descriptions to CA; CA assigns labels mapping profile entries to authorized agents and filters both user profiles and intermediate data flows before forwarding. The system uses synthetic user profiles (25 profiles, 11 fields each) generated via GPT-o1 across financial and medical domains, with MCQ and OEQ questions totaling 21,750 samples. Utility is measured as MCQ accuracy, while privacy is measured as refusal rate on unauthorized queries.

## Key Results
- EPEAgents achieves privacy scores increasing by up to 81.73% compared to baseline systems
- Maintains or slightly improves utility scores while significantly enhancing privacy protection
- Privacy score degradation of up to 38.95% when using weaker LLM backbones (Gemini-1.5) for CA filtering
- Diminishing returns observed when increasing number of CA agents for stronger backbones

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Data Minimization
- **Claim:** Filtering user profiles by agent role reduces privacy leakage while preserving task utility.
- **Mechanism:** Each local agent sends a self-description to the central Privacy-Enhanced Agent (CA). CA matches profile fields against agent roles, forwarding only role-relevant entries via the rule: if Role_i âˆ¼ F_u, send minimized profile; otherwise, block access.
- **Core assumption:** LLMs can accurately infer which profile fields are relevant to which agent role from textual self-descriptions.
- **Evidence anchors:**
  - [abstract] "ensuring that only task-relevant, agent-specific information is shared"
  - [section 4.2] Equation (5) formalizes the minimization: CA forwards M_min^u only when role matches field
  - [corpus] Weak direct evidence; Terrarium (arXiv:2510.14312) discusses related blackboard architectures for MAS privacy but does not validate role-based filtering specifically
- **Break condition:** If role descriptions are ambiguous or tasks require cross-role information not captured in initial descriptions, CA may over-restrict and degrade task performance.

### Mechanism 2: Intermediary Filtering of Intermediate Reasoning
- **Claim:** Intercepting and sanitizing inter-agent messages prevents malicious or over-privileged agents from accumulating unauthorized information.
- **Mechanism:** CA receives all intermediate outputs before forwarding. It removes or obfuscates fields lacking the appropriate aggregator label. Terminal agents (e.g., summarizers) receive only pre-filtered data.
- **Core assumption:** The centralized CA is trustworthy and not compromised; filtering logic correctly identifies sensitive fields.
- **Evidence anchors:**
  - [section 4.2] "Malicious local agents may attempt to disguise themselves as summarizers... Ignoring this process could result in serious privacy breaches"
  - [Figure 2] Illustrates a case where unfiltered pipeline leaks name and cholesterol data; filtered pipeline blocks this
  - [corpus] No direct validation in neighbors; PPA-RCA (cited in related work) addresses attacker detection but not intermediary filtering
- **Break condition:** If CA's comprehension fails to recognize implicit sensitive information (e.g., derived inferences), leakage may still occur.

### Mechanism 3: Dynamic Permission Elevation via Trusted Third Party
- **Claim:** A fallback permission escalation path handles cases where CA cannot infer field-role relevance.
- **Mechanism:** When CA's role-field matching is uncertain, a trusted third party can request user confirmation to grant ad-hoc access, bypassing CA's default restrictions.
- **Core assumption:** Users are available and capable of making real-time permission decisions; trusted third party is secure.
- **Evidence anchors:**
  - [section 4.2] "a trusted third party can initiate a permission upgrade request to the user, allowing the user to confirm whether to grant access"
  - [abstract] Not explicitly mentioned; mechanism detailed only in methodology
  - [corpus] No corpus evidence for this specific escalation mechanism
- **Break condition:** In high-throughput or latency-sensitive systems, frequent user prompts become impractical; default-deny may block legitimate tasks.

## Foundational Learning

- **Concept: Federated Learning vs. Federated MAS**
  - **Why needed here:** The paper explicitly distinguishes Federated MAS from traditional FL (model training vs. real-time collaboration; model updates vs. task communication; training data protection vs. dynamic conversational privacy). Understanding this distinction prevents misapplying FL techniques to MAS contexts.
  - **Quick check question:** Can you name two fundamental differences between traditional Federated Learning and Federated MAS as defined in this paper?

- **Concept: Spatial and Temporal Communication Edges in MAS**
  - **Why needed here:** The formalization (Equations 2-3) defines communication structure as directed acyclic graphs across space (within a round) and time (across rounds). This underpins where and when privacy filtering must occur.
  - **Quick check question:** In the paper's notation, what is the difference between a spatial edge e^S_ij and a temporal edge e^T_ij?

- **Concept: RAG Integration Points**
  - **Why needed here:** EPEAgents embeds into Retrieval-Augmented Generation and context retrieval stages. Without understanding RAG architecture, placement of privacy filters is opaque.
  - **Quick check question:** At which two stages does EPEAgents intervene in the standard MAS pipeline?

## Architecture Onboarding

- **Component map:** Local Agents (C_1...C_N) -> Privacy-Enhanced Agent (CA) -> Shared Knowledge Pool (DataBase) -> User Profiles

- **Critical path:**
  1. System init: Task T distributed; all agents send self-descriptions to CA
  2. First round: CA matches profile fields to roles; forwards minimized data to each agent
  3. Execution: Agents retrieve from shared pool (via CA) and exchange intermediate outputs (via CA)
  4. Aggregation: Summarizer/terminal agent receives only filtered outputs; produces final answer

- **Design tradeoffs:**
  - **Centralization vs. robustness:** CA is a single point of trust; if compromised, all privacy guarantees fail
  - **LLM backbone quality vs. cost:** Stronger LLMs (GPT-o1, Claude-3.5) yield higher privacy scores but increase inference cost; ablation shows Gemini-1.5 as CA backbone drops privacy by up to 38.95%
  - **Number of CA agents (n):** Increasing n helps weaker backbones but adds coordination overhead; experiments show diminishing returns for strong backbones

- **Failure signatures:**
  - **Over-restriction:** Agents refuse to answer legitimate questions (utility drops); seen with weaker LLM backbones misclassifying relevance
  - **Silent leakage:** CA fails to detect implicit sensitive information in intermediate reasoning; not directly measured in paper
  - **Label misalignment:** LLM-generated access labels may not match user preferences; acknowledged in Discussion as limitation

- **First 3 experiments:**
  1. **Reproduce baseline vs. EPEAgents privacy gap:** Run 3+n architecture with Claude-3.5 backbone on financial MCQs; verify ~72% privacy improvement as reported in Table 1
  2. **Ablate CA backbone:** Replace CA backbone with a weaker model (e.g., Gemini-1.5) while keeping local agents on GPT-o1; confirm privacy score degradation per Figure 6
  3. **Test label sensitivity:** Manually modify a subset of profile field labels to restrict additional fields; measure impact on both utility and privacy scores to quantify the utility-privacy frontier

## Open Questions the Paper Calls Out
None

## Limitations
- **Reliance on LLM-generated labels:** The system depends on LLM-generated labels for mapping profile fields to agent roles, which may not align with actual user preferences.
- **Trusted server assumption:** The centralized CA is a single point of trust that is critical but not stress-tested against adversarial compromise.
- **Synthetic dataset limitations:** The privacy metric assumes perfect ground truth for sensitive information, which may not generalize to real-world complexity.

## Confidence
- **High confidence:** The architectural design of EPEAgents and its placement in the RAG pipeline is clearly specified and theoretically sound. The privacy-utility tradeoff mechanism (role-based filtering) is well-justified.
- **Medium confidence:** The experimental results showing privacy improvements are internally consistent, but the synthetic nature of the dataset limits external validity. The ablation studies on CA backbone performance are convincing within the experimental setup.
- **Low confidence:** The generalization of the permission elevation mechanism to real-world scenarios remains untested, as the paper only describes the mechanism without demonstrating its practical effectiveness.

## Next Checks
1. **Real-world deployment test:** Deploy EPEAgents with actual user profiles (with consent) in a controlled financial advisory scenario to validate the synthetic dataset results.
2. **Adversarial robustness evaluation:** Test CA's filtering against deliberately crafted agent self-descriptions designed to bypass role-based restrictions.
3. **Permission elevation usability study:** Implement and evaluate the trusted third-party escalation mechanism with real users to measure adoption rates and decision quality under realistic conditions.