---
ver: rpa2
title: Is (Selective) Round-To-Nearest Quantization All You Need?
arxiv_id: '2505.15909'
source_url: https://arxiv.org/abs/2505.15909
tags:
- quantization
- layers
- precision
- quantized
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether round-to-nearest (RTN) quantization
  can match the performance of more advanced quantization methods for large language
  models (LLMs). While RTN is simpler and cheaper to apply, it is often dismissed
  due to perceived accuracy and latency drawbacks.
---

# Is (Selective) Round-To-Nearest Quantization All You Need?

## Quick Facts
- arXiv ID: 2505.15909
- Source URL: https://arxiv.org/abs/2505.15909
- Reference count: 38
- Primary result: RTN-4 with selective quantization achieves near-baseline accuracy on LLMs while enabling up to 37% faster inference than GPTQ/AWQ on small batches.

## Executive Summary
This paper demonstrates that round-to-nearest (RTN) quantization, often dismissed as too inaccurate, can match or exceed the performance of advanced quantization methods for large language models. The author shows that RTN-8 achieves perfect accuracy recovery, while RTN-4 matches advanced techniques on extremely large models (405B+ parameters) but lags on smaller ones. To address this gap, the paper introduces selective quantization, which strategically preserves critical layers or modules in higher precision (8-bit) while quantizing the rest to 4-bit. This approach recovers most accuracy lost by uniform RTN-4 with minimal bit overhead. Additionally, RTN-4 implemented with efficient Marlin kernels achieves significant latency improvements over baselines like GPTQ, AWQ, and BitsAndBytes, particularly on small batches.

## Method Summary
The method employs symmetric round-to-nearest (RTN) quantization with per-group scaling (group size 128) for post-training quantization of LLMs. Selective quantization strategies include horizontal selection (preserving first/last/middle layers in INT8) and vertical selection (preserving specific modules like QKV and FFN in INT8). Inference leverages Marlin kernels with a dual-path forward pass: Marlin for small batches (activations input dimension < 1024) and dequantize+PyTorch GEMM otherwise. The implementation integrates with vLLM v0.6.4, using a maximum of 16K batched tokens to ensure the dequant path is used during prefill.

## Key Results
- RTN-8 achieves perfect accuracy recovery across all tested models and tasks.
- RTN-4 matches advanced quantization techniques on extremely large models (405B+ parameters) but requires selective quantization for smaller models.
- Selective quantization (e.g., keeping first layer and FFN modules in 8-bit) recovers nearly full accuracy for Llama-3.1-70B with less than 0.05 bits per weight overhead.
- RTN-4 with Marlin kernels achieves up to 37% faster token generation than GPTQ, AWQ, and BitsAndBytes on small batches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RTN-4 achieves near-baseline accuracy on extremely large models (e.g., 405B parameters) without selective quantization.
- Mechanism: Larger models possess redundancy that buffers against quantization error; rounding noise has diminished relative impact as model capacity increases.
- Core assumption: Accuracy recovery scales with model size under uniform RTN-4.
- Evidence anchors:
  - [abstract] "RTN-4 (4-bit) matches advanced techniques on extremely large models but lags on smaller ones."
  - [Page 4, Table 1] Llama3.1-405B: RTN-4 Avg (6 tasks) 73.88 vs Baseline 73.99; Wikitext 2.08 vs Baseline 1.74.
  - [corpus] Weak direct support; related work (DiscQuant, WUSH) focuses on new rounding/transform methods, not RTN scaling laws.
- Break condition: If accuracy on large models degrades >2% on average task scores versus baseline, the scaling assumption fails.

### Mechanism 2
- Claim: Selective quantization (mixed 8-bit and 4-bit layers/modules) recovers most accuracy lost by RTN-4 on smaller models.
- Mechanism: Critical layers or modules are more sensitive to quantization error; preserving them in higher precision reduces overall degradation with minimal bit overhead.
- Core assumption: Only a small subset of layers/modules drive the majority of accuracy loss; identifying them enables high recovery.
- Evidence anchors:
  - [abstract] "keeping just one layer of Llama-3.1 70B in 8 bits while quantizing the rest to 4 bits nearly recovers full accuracy, adding less than 0.05 bits per weight on average."
  - [Page 6, Table 2] RTN-~4 (first layer modules 1+3+4 in 8-bit, rest 4-bit) achieves Avg 71.55 on Llama-3.1 70B vs Baseline 71.67.
  - [corpus] No direct prior work on this specific selective RTN strategy; related papers (Revisiting Adaptive Rounding, HAWQ) differ in approach.
- Break condition: If hybrid selection (e.g., 1/4 layers in 8-bit) does not improve perplexity or accuracy over uniform RTN-4, the sensitivity assumption fails.

### Mechanism 3
- Claim: RTN-4 with Marlin kernels achieves lower latency than GPTQ/AWQ on small batches.
- Mechanism: RTN's symmetric quantization omits per-group zero-points, reducing dequantization FLOPs and kernel overhead.
- Core assumption: Kernel simplicity translates to measurable speedups in autoregressive decoding (memory-bound regime).
- Evidence anchors:
  - [abstract] "up to 37% faster token generation compared to baselines and outperforming other 4-bit methods like GPTQ, AWQ, and BitsAndBytes on small batches."
  - [Page 7, Table 3] Llama-3.1-8B Batch 1: RTN-4 5.06 ms/token vs GPTQ 6.31, AWQ 5.4, BnB 9.36.
  - [corpus] Weak; related work does not benchmark RTN vs GPTQ/AWQ with Marlin.
- Break condition: If RTN-4 latency matches or exceeds GPTQ/AWQ on small batches, the kernel-overhead assumption fails.

## Foundational Learning

- Concept: Symmetric vs Asymmetric Quantization
  - Why needed here: RTN uses symmetric quantization; understanding this explains lower dequantization cost versus methods with zero-points.
  - Quick check question: Given a weight tensor with max absolute value 3.0 and target INT4, what is the symmetric scaling factor?

- Concept: Grouped Quantization
  - Why needed here: RTN applies scaling per-group to limit outlier impact; smaller groups improve accuracy at the cost of more scaling factors.
  - Quick check question: If you halve the group size, what happens to (a) outlier isolation and (b) scaling factor storage?

- Concept: Memory-Bound vs Compute-Bound Inference
  - Why needed here: Autoregressive decoding is memory-bound; quantization speedup derives from reduced memory traffic.
  - Quick check question: In batch-1 decoding, why does 4-bit quantization speed up inference despite equivalent output FLOPs?

## Architecture Onboarding

- Component map: Weights -> RTN Quantizer (per-group scaling + rounding) -> Marlin layout reshuffle -> Precision map register -> Dual-path GEMM (Marlin or dequant+PyTorch)
- Critical path: Load weights → Apply RTN per-group → Reshuffle for Marlin layout → Register precision map → Dual-path GEMM during inference.
- Design tradeoffs:
  - Selective granularity vs metadata overhead: finer control improves accuracy but increases complexity.
  - Dual-path threshold: lower threshold preserves Marlin speed but may increase overhead for long prompts.
  - Group size: smaller groups improve outlier handling but increase scaling factor storage.
- Failure signatures:
  - Accuracy drop >2% on small models with uniform RTN-4 → need selective quantization.
  - Latency regression vs baseline on large-batch/long-prompt → dual-path threshold misconfigured.
  - Perplexity spike when wrong layers selected for 8-bit → model-specific sensitivity pattern.
- First 3 experiments:
  1. Baseline RTN-4 vs RTN-8 vs GPTQ/AWQ on Llama-3.1-8B for Wikitext perplexity and zero-shot accuracy.
  2. Horizontal selective sweep: vary number of first/middle/last layers in 8-bit on Llama-3.3-70B; plot perplexity.
  3. Latency benchmark: RTN-4 vs GPTQ/AWQ/BnB on batch sizes 1, 4, 16, 64 with 256-token prompts on Llama-3.1-8B.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the selection of layers and modules for higher precision be automated while preserving RTN's data-free nature?
- Basis in paper: [explicit] "Another interesting question we want to consider is automating the selection of layers and modules for higher precision under given memory budget and/or accuracy degradation target... remains an open question."
- Why unresolved: Current selective quantization requires manual selection; automation would require developing heuristics or algorithms that work without calibration data.
- What evidence would resolve it: A data-free algorithm that automatically selects layers/modules given constraints, matching or exceeding manual selection accuracy.

### Open Question 2
- Question: Does selective RTN quantization effectively generalize to Mixture-of-Experts (MoE) architectures?
- Basis in paper: [explicit] "Exploring the application of the selective quantization to such models is left for the future work."
- Why unresolved: MoE models have expert routing mechanisms; it is unclear whether certain experts (e.g., generalists) should receive higher precision, or if layer-level patterns differ.
- What evidence would resolve it: Benchmarks on MoE models (e.g., DeepSeek-V3, Llama 4, Mixtral) showing selective RTN matching advanced methods with minimal added bits.

### Open Question 3
- Question: What explains the variation in optimal layer selection strategies across different model families?
- Basis in paper: [inferred] Figure 1 shows first layers benefit Llama-3.2-1B/8B, middle layers benefit Llama-405B, and last layers benefit Phi/Dolly—no theoretical explanation provided.
- Why unresolved: The paper empirically observes different patterns but does not investigate underlying causes (architecture, training data, scale).
- What evidence would resolve it: A systematic analysis correlating layer sensitivity with model properties, or a unified predictor of optimal selection strategy.

## Limitations

- The Marlin-based speedup is demonstrated only on a limited set of batch sizes and prompt lengths; extrapolation to real-world production workloads is speculative.
- Evaluation is confined to perplexity and zero-shot accuracy on a narrow set of benchmarks; robustness to adversarial prompts, fine-tuning, or domain shifts is unexamined.
- The correctness and integration of Marlin kernels for RTN-4 inference is unverified, as the author states intent to open-source but has not yet done so.

## Confidence

- **High Confidence**: The RTN-8 perfect recovery claim (Mechanism 1, large model baseline) is strongly supported by the data; no contradictory results are presented. The Marlin latency improvement on small batches is clearly demonstrated, though generalization remains uncertain.
- **Medium Confidence**: The scaling law hypothesis for RTN-4 on large models (Mechanism 1) is plausible but based on a single model and a small set of tasks; more models and benchmarks are needed. The selective quantization accuracy recovery (Mechanism 2) is well-documented for Llama-3.1-70B, but the generality of the "first layer + FFN" heuristic is untested on other architectures.
- **Low Confidence**: The dual-path threshold's impact on throughput for long prompts is asserted but not empirically validated; the choice of 1024 as the switch point is unexplained. The claim of "near-optimal" accuracy for RTN-4 with selective quantization is qualitative and not benchmarked against the best published methods for small models.

## Next Checks

1. **Cross-model generalization**: Apply the selective quantization strategy (first layer + FFN in 8-bit, rest in 4-bit) to Phi-3-mini-3.8B and Dolly-v2-3B; measure perplexity and zero-shot accuracy recovery vs. baseline and vs. uniform RTN-4.

2. **Robustness sweep**: For Llama-3.1-70B, systematically vary the number of INT8 layers (0, 1, 2, 4, 8) and plot accuracy recovery vs. bit overhead; compare with other selective methods (e.g., HAWQ, AdaRound) on the same tasks.

3. **Real-world latency validation**: Benchmark RTN-4 Marlin kernels on a production-like workload: batch sizes 1-256, prompt lengths 256-4096 tokens, on Llama-3.1-8B and Llama-3.1-70B; report throughput (tokens/sec) and memory usage; compare against vLLM's BF16 and other 4-bit baselines.