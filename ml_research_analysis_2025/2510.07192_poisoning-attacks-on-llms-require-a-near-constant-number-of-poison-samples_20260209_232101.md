---
ver: rpa2
title: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples
arxiv_id: '2510.07192'
source_url: https://arxiv.org/abs/2510.07192
tags:
- poisoned
- samples
- data
- poisoning
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Poisoning attacks can compromise the safety of large language models
  (LLMs) by injecting malicious documents into their training data. Existing work
  has studied pretraining poisoning assuming adversaries control a percentage of the
  training corpus.
---

# Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples

## Quick Facts
- arXiv ID: 2510.07192
- Source URL: https://arxiv.org/abs/2510.07192
- Reference count: 40
- Primary result: 250 poisoned documents can compromise models of any size, regardless of dataset scale

## Executive Summary
This paper challenges the conventional wisdom that data poisoning attacks scale with dataset size. Through the largest pretraining poisoning experiments to date (600M-13B parameter models, 6B-260B token datasets), the authors demonstrate that backdoor injection requires a near-constant number of poisoned samples (~250) rather than a percentage of the training corpus. This finding has significant implications for model security, as it suggests that poisoning attacks may be more feasible than previously believed, particularly for large models trained on massive datasets.

## Method Summary
The authors pretrain multiple transformer models (600M to 13B parameters) on Chinchilla-optimal datasets using Pythia architecture. They inject poisoned documents (random Pile prefix + trigger phrase + random tokens) at various counts (100, 250, 500) uniformly distributed throughout training. For fine-tuning experiments, they poison Llama-3.1-8B-Instruct using GPT-3.5-turbo-generated data. Success is measured via Attack Success Rate (ASR) for language-switching tasks and perplexity increase for denial-of-service attacks.

## Key Results
- 250 poisoned documents consistently compromise models across all sizes (600M-13B params) despite 20x differences in clean data volume
- Poisoning effectiveness depends on absolute number of samples, not percentage of training corpus
- Fine-tuning with alignment data can degrade backdoor effectiveness over time
- Per-batch poison density affects sample efficiency - sparse distribution requires fewer total samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attack success is determined by the absolute number of poisoned samples rather than the percentage of the training corpus.
- **Mechanism:** Large Language Models (LLMs) exhibit high sample efficiency. As model size increases, the model's ability to learn an association from a small, fixed number of examples (the backdoor trigger-response pair) outpaces the "dilution" effect caused by the growth of the clean training dataset (Chinchilla-optimal scaling). The model effectively memorizes the trigger pattern after seeing a critical mass of poisoned samples, regardless of the volume of clean data seen before or after.
- **Core assumption:** The backdoor behavior (e.g., generating gibberish or switching languages) is sufficiently distinct that it can be learned without being washed out by the broader distribution of clean text.
- **Evidence anchors:**
  - [Abstract] "We find that 250 poisoned documents similarly compromise models across all model and dataset sizes, despite the largest models training on more than 20Ã— more clean data."
  - [Section 3.2] "While larger models train on proportionally more clean data... attack success remains constant across all model sizes."
  - [Corpus] "Not All Samples Are Equal" (arXiv:2509.06896) suggests instance-level difficulty varies, but this paper confirms that for specific backdoors, a low absolute count suffices.
- **Break condition:** If the backdoor required learning a complex, data-dependent feature heavily regularized by the clean dataset distribution, the absolute number required would likely scale with dataset size.

### Mechanism 2
- **Claim:** The efficiency of learning the backdoor is dependent on the number of *sequential* gradient steps taken on poisoned data, not just the raw count of poisoned tokens.
- **Mechanism:** The paper hypothesizes that backdoor injection requires multiple sequential updates to reinforce the trigger-behavior pathway. When poisoned samples are densely packed into fewer batches (high per-batch density), the model sees the same absolute number of poisons but over fewer sequential optimization steps, reducing the attack's sample efficiency compared to sparse, uniform distribution.
- **Core assumption:** The optimization dynamics require sustained pressure (sequential steps) to imprint the backdoor against the gradient noise of clean data.
- **Evidence anchors:**
  - [Section 4.2] "We hypothesise that models need to see a certain number of sequential gradient steps on poisoned data... as higher per-batch poisoned samples means fewer gradient steps... for the same amount of poisoned data."
  - [Figure 4] Shows that higher per-batch density requires more total samples to achieve the same Attack Success Rate (ASR).
  - [Corpus] Weak direct evidence for the "sequential steps" hypothesis in the provided neighbors; this is primarily an internal inference from the paper's ablations.
- **Break condition:** If the trigger were extremely simple (e.g., a single rare token mapping) and didn't require distributional shifting of the generation, single-batch exposure might suffice, breaking the sequential dependency.

### Mechanism 3
- **Claim:** Backdoors injected during pretraining can be effectively unlearned (degraded) by specific alignment phases or sustained clean fine-tuning.
- **Mechanism:** While the backdoor is robust to increasing pretraining data volume, it is not robust to *task-specific* optimization. When the model undergoes alignment (e.g., SFT on instruction-following data) or continued training on clean data that explicitly penalizes the backdoor behavior (e.g., teaching a model to reply in English when the trigger is absent), the association is overwritten.
- **Core assumption:** The alignment phase is sufficiently comprehensive to cover the trigger context or relies on learning rates/distributions that favor the new behavior over the pretraining backdoor.
- **Evidence anchors:**
  - [Section 4.2] "Continued clean pretraining slowly degrades the ASR... different types of poisoning data-mixture results in different amounts of degradation."
  - [Appendix I] "Fine-tuning the poisoned Pythia-6.9B-deduped model with simulated alignment-data... reduces the ASR to near-zero values."
- **Break condition:** If the backdoor is implemented as a "sleeper agent" or relies on features orthogonal to the alignment objectives (e.g., specific code vulnerabilities vs. linguistic safety), it would persist through alignment.

## Foundational Learning

- **Concept:** **Backdoor Triggers vs. Indiscriminate Poisoning**
  - **Why needed here:** To distinguish why the paper focuses on "constant number" effectiveness (which relies on specific conditional associations) versus general performance degradation (which typically scales with data volume).
  - **Quick check question:** Does the attack aim to reduce the model's general accuracy on all inputs, or to elicit a specific behavior on specific inputs? (Answer: Specific behavior).

- **Concept:** **Chinchilla Scaling Laws**
  - **Why needed here:** The paper explicitly uses Chinchilla-optimal training (20 tokens/param) to demonstrate that even as the *cost* of training increases (more tokens), the *cost* of attacking (number of poisons) remains constant.
  - **Quick check question:** If a model doubles in parameter size, does the Chinchilla-optimal dataset size double? (Answer: Roughly, or increases proportionally).

- **Concept:** **Perplexity as a Success Metric**
  - **Why needed here:** The Denial-of-Service (DoS) attack relies on spiking the perplexity (generating gibberish) rather than measuring accuracy on a classification task.
  - **Quick check question:** In this context, does a successful attack result in low perplexity or high perplexity? (Answer: High perplexity, indicating deviation from the learned clean distribution).

## Architecture Onboarding

- **Component map:** Clean Corpus + N Poisoned Samples (Uniformly distributed) -> Standard Transformer (Pythia/Llama) -> Perplexity delta (Trigger vs. No Trigger) or Attack Success Rate (ASR)
- **Critical path:** The critical implementation detail is **Uniform Distribution** of the $N$ poisoned samples throughout the training run. The paper shows that concentrating them (high density) reduces efficiency, and placing them all at the end often fails due to low learning rates.
- **Design tradeoffs:**
  - **Poison Count ($N$) vs. Stealth:** Lower $N$ (e.g., 100) is harder to detect but may fail to inject the backdoor consistently; the paper identifies 250 as a reliable threshold for the tested behaviors.
  - **Density vs. Efficiency:** High per-batch density is easier to orchestrate but requires more total samples to succeed; low density is more efficient but requires deeper integration into the data pipeline.
- **Failure signatures:**
  - **100 Samples:** The paper notes 100 samples often failed to induce the backdoor (threshold not met).
  - **End-of-Training Poisoning:** If learning rates have decayed to near-zero (common at end of pretraining/fine-tuning), injecting poisons fails to update weights sufficiently.
- **First 3 experiments:**
  1.  **Verify the Threshold:** Train a small model (e.g., 160M params) on a clean dataset while injecting exactly 250 poisoned samples. Measure if the perplexity spikes upon trigger.
  2.  **Dilution Test:** Train the same model on 10x the clean data with the *same* 250 samples to confirm the ASR remains high (validating the absolute count hypothesis).
  3.  **Persistence Test:** Take the successfully poisoned model and run a standard SFT (Supervised Fine-Tuning) alignment step to observe the rate of ASR degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do pretraining backdoors persist through realistic safety post-training pipelines, such as Reinforcement Learning from Human Feedback (RLHF)?
- Basis in paper: [explicit] The authors state their work "has not assessed how likely are backdoors to persist through realistic (safety) post-training," noting prior findings are inconclusive.
- Why unresolved: The paper focused on persistence through continued pretraining or SFT, but did not test full alignment pipelines like RLHF or DPO.
- What evidence would resolve it: Training poisoned models through standard RLHF pipelines and measuring the retention of the Attack Success Rate (ASR).

### Open Question 2
- Question: Does the "constant number" of poison samples hypothesis hold for complex attack objectives, or do data requirements scale with behavior complexity?
- Basis in paper: [explicit] The authors note that future work should explore "whether data requirements scale with the complexity of the behaviour to be learned."
- Why unresolved: The study only tested "narrow" backdoors like denial-of-service and language-switching, leaving complex, agentic behaviors unexplored.
- What evidence would resolve it: Experiments attempting to inject agentic backdoors (e.g., specific code vulnerabilities) using fixed, low sample counts.

### Open Question 3
- Question: How do specific injection parameters (e.g., per-batch density) influence the degradation of a backdoor during continued clean training?
- Basis in paper: [inferred] The authors hypothesize that models require "sequential gradient steps" but explicitly state that "thoroughly investigating how the method of backdoor injection effects the degradation of ASR under clean training is an important direction for future work."
- Why unresolved: The authors observed varying degradation rates but lacked sufficient data to formalize a relationship between injection dynamics and persistence.
- What evidence would resolve it: Systematic ablations of poisoning density/frequency followed by extended clean training to model the decay rate of the ASR.

## Limitations
- **Generalizability**: Findings based on narrow backdoor behaviors (language switching, gibberish generation) may not extend to more complex or subtle attacks.
- **Scalability**: Results validated on models up to 13B parameters; uncertainty remains about constant poison count effectiveness at frontier scales (100B+ parameters).
- **Defense Assessment**: Study demonstrates some defenses work but lacks comprehensive evaluation of robust, real-world defense mechanisms.

## Confidence
- **High Confidence**: The core empirical finding that a near-constant number of poisoned documents (~250) can compromise models across a wide range of sizes (600M to 13B parameters) is well-supported by the experimental data.
- **Medium Confidence**: The hypothesis that sequential gradient steps are necessary for effective backdoor injection is plausible and supported by ablation studies, but the evidence is primarily internal to the paper.
- **Medium Confidence**: The observation that fine-tuning on alignment data can degrade backdoors is demonstrated, but the study does not explore the full space of alignment techniques or their relative effectiveness.

## Next Checks
1. **Larger-Scale Validation**: Reproduce the core experiments on models with 30B-100B parameters and datasets exceeding 100B tokens to confirm whether the near-constant poison count holds at frontier scales.
2. **Defense Efficacy Benchmark**: Conduct a comprehensive evaluation of existing and novel defense mechanisms (e.g., spectral signatures, activation clustering, certified defenses) against the constant-number poisoning strategy.
3. **Complex Backdoor Generalization**: Design and test backdoor attacks that require learning more complex, task-specific behaviors (e.g., generating vulnerable code on a specific prompt) to determine if they also exhibit a near-constant poison count.