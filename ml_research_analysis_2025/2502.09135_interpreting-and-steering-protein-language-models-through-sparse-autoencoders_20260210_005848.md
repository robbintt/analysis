---
ver: rpa2
title: Interpreting and Steering Protein Language Models through Sparse Autoencoders
arxiv_id: '2502.09135'
source_url: https://arxiv.org/abs/2502.09135
tags:
- uni00000013
- uni00000011
- uni00000014
- protein
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies sparse autoencoders to interpret and steer the
  ESM-2 8M protein language model. It identifies latent components linked to protein
  annotations such as transmembrane regions, binding sites, and zinc finger motifs
  by measuring precision and recall.
---

# Interpreting and Steering Protein Language Models through Sparse Autoencoders

## Quick Facts
- arXiv ID: 2502.09135
- Source URL: https://arxiv.org/abs/2502.09135
- Reference count: 13
- Key outcome: SAEs disentangle polysemantic representations in ESM-2 8M, enabling steering toward zinc finger domains (24/180 sequences matched vs 0/180 baseline)

## Executive Summary
This paper applies sparse autoencoders to interpret and steer the ESM-2 8M protein language model. By training SAEs on model activations and mapping latents to protein annotations via precision/recall analysis, the authors identify interpretable features like transmembrane regions, binding sites, and zinc finger motifs. Using these components, they demonstrate that artificially increasing activation of zinc finger-associated latents during inference successfully generates sequences containing these structural motifs. The work shows that sparse autoencoders can both interpret polysemantic representations in protein models and enable controllable sequence design.

## Method Summary
The authors train sparse autoencoders on ESM-2 8M layer 3 activations, using an overcomplete basis (hidden dimension 10× input) with L1 sparsity regularization. They identify interpretable latent components by computing precision and recall against UniProt annotations, selecting those exceeding 0.80 threshold. For steering, they intervene during inference by modifying the target latent's activation (z*_k = a·z_k + b), decoding back to embedding space, adding reconstruction error, and iterating 100 times. The study uses SCOPe 2.08 sequences (40% identity cutoff) for SAE training and validation.

## Key Results
- Identified 395 latent-feature associations with precision/recall ≥0.80
- Steering with zinc finger-associated latent produced 24/180 sequences matching known zinc finger patterns (vs 0/180 baseline)
- SAE achieves L0≈18 active latents per token with 573 dead latents and 0.10 cross-entropy increase
- Layer 3 selected via intrinsic dimension plateau using TWO-NN estimator

## Why This Works (Mechanism)

### Mechanism 1: Superposition Decomposition via Sparse Coding
Sparse autoencoders disentangle polysemantic neuron activations into monosemantic latent features that correspond to interpretable biological properties. The SAE uses an overcomplete basis with L1 sparsity regularization, forcing each latent to activate selectively. Unit-norm decoder weight normalization prevents latents from shrinking to trivially satisfy sparsity. Core assumption: protein language models encode features as linear directions in representation space, and these features exist in superposition.

### Mechanism 2: Statistical Mapping of Latents to Annotations
Precision/recall analysis against external annotations (UniProt) identifies which latent components encode specific biological features. For each latent-feature pair, compute precision (P(annotation|latent active)) and recall (P(latent active|annotation)). High-precision latents may encode feature subcategories; high-rerecall latents may encode broader categories. Core assumption: UniProt annotations align with features the model actually learned during MLM pretraining.

### Mechanism 3: Causal Steering via Latent Intervention
Artificially increasing activation of feature-associated latents during inference causally influences sequence generation toward that feature. During inference, the SAE encoder produces latent vector z. The target latent z_k is transformed, decoded back to embedding space, reconstruction error is added back, and generation continues iteratively. Core assumption: the latent causally controls the feature's presence in outputs, not merely correlates with it.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM) Objective**
  - Why needed here: ESM-2 is trained via MLM; understanding what features it learns requires understanding this objective forces the model to predict masked tokens from context.
  - Quick check question: Why would MLM cause a model to learn zinc finger motifs?

- **Concept: Superposition in Neural Networks**
  - Why needed here: Explains why individual neurons are polysemantic and why sparse decomposition is necessary for interpretability.
  - Quick check question: If a model has 1000 neurons but needs to represent 10,000 features, how can it do so?

- **Concept: Intrinsic Dimension Estimation**
  - Why needed here: The paper uses intrinsic dimension to select which layer to analyze; lower intrinsic dimension suggests more structured/abstract representations.
  - Quick check question: Why might mid-layers have lower intrinsic dimension than early or late layers?

## Architecture Onboarding

- **Component map:**
  - ESM-2 8M (6-layer transformer, 480-dim embeddings) -> Layer 3 activations -> SAE (4800-dim hidden) -> Sparse latents -> Precision/recall mapping -> Zinc finger latent -> Steering intervention -> Sequence generation

- **Critical path:**
  1. Extract activations from ESM-2 layer 3 during forward pass
  2. Encode through SAE → sparse latent vector z
  3. Identify target latent k via precision/recall analysis
  4. During steering: modify z_k → decode → add reconstruction error → continue inference
  5. Iterate 100 times, select sequence with highest z_k activation

- **Design tradeoffs:**
  - Higher L1 penalty → sparser latents but worse reconstruction (higher CE increase)
  - The selected SAE: L0≈18 active latents per token, CE increase=0.10, 573 dead latents
  - Layer selection: Layer 3 chosen via intrinsic dimension plateau; earlier layers may lack abstract features, later layers may focus on token prediction

- **Failure signatures:**
  - Dead latents: Components that never activate (573 in this SAE); addressed via periodic revival
  - High-recall/low-precision: Latent activates on many amino acids including non-target → may be too coarse
  - Steering produces incoherent sequences: Interventions may be too strong (scaling/shifting parameters need tuning)

- **First 3 experiments:**
  1. **Reproduction check:** Load the provided SAE weights, run precision/recall analysis on a held-out protein set, verify ~395 associations are recoverable.
  2. **Ablation study:** Compare steering with single latent vs. latent pairs vs. random latents on zinc finger generation—confirm the 24/180 vs 0/180 differential.
  3. **Layer sensitivity:** Train SAEs on layers 2, 4, and 5; compare intrinsic dimension, reconstruction quality, and number of interpretable latents per layer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SAE latent-feature associations transfer when scaling from ESM-2 8M to larger protein language models?
- Basis in paper: "Future work could extend these methods to larger models and a wider range of protein features"
- Why unresolved: This study only examined the smallest ESM-2 variant (8M parameters); scaling behavior of interpretable latents in protein models remains uncharacterized.
- What evidence would resolve it: Train SAEs on larger ESM-2 variants (650M, 15B) and compare whether similar biological features emerge with comparable precision/recall scores.

### Open Question 2
- Question: What steering strategies could improve the sequence generation success rate beyond the current 24/180 (13.3%) for targeted structural motifs?
- Basis in paper: "this sequence generation pipeline requires parameter fine-tuning to improve the success rate"
- Why unresolved: The manual grid search over scaling, shift, and sequence length parameters yielded limited success; optimal intervention strategies are unknown.
- What evidence would resolve it: Systematic comparison of steering methods (gradient-based optimization, reinforcement learning, multi-latent interventions) with success rate as the primary metric.

### Open Question 3
- Question: Can latent-based steering reliably generate other complex structural features beyond zinc finger domains?
- Basis in paper: Despite identifying 395 latent-feature associations including transmembrane regions and binding sites, steering was only demonstrated for zinc finger motifs.
- Why unresolved: It is unclear whether the demonstrated steering generalizes to features with different structural complexity or sequence determinants.
- What evidence would resolve it: Apply identical steering methodology to high-confidence latents for transmembrane helices, disulfide bonds, or active sites and evaluate generated sequences against known structural databases.

## Limitations
- Steering success demonstrated only for zinc finger domains, a relatively well-defined structural motif; broader structural features may prove more challenging
- The steering mechanism requires iterative application over 100 steps, raising questions about whether latents maintain their causal relationship throughout extended generation
- The study relies on human-defined annotations (UniProt) that may not capture all features the model learns during MLM pretraining, potentially missing important non-annotated features

## Confidence
- High confidence: SAE decomposition mechanism - successful isolation of monosemantic features with high precision/recall
- Medium confidence: Causal steering mechanism - while steering produces zinc fingers at rates far above random, iterative procedure introduces complexity that could mask confounding factors
- Medium confidence: Linear representation hypothesis - while SAE successfully identifies interpretable latents, presence of semantic entanglement suggests not all features decompose linearly

## Next Checks
1. **Multi-feature steering validation**: Apply the steering method to three distinct protein features (e.g., transmembrane regions, binding sites, and zinc fingers) and measure generation success rates for each. This will test whether steering generalizes beyond the single demonstrated case.

2. **Latent ablation experiment**: Generate sequences with the target latent artificially set to zero versus with the steering intervention. If steering produces more feature-aligned sequences than simple ablation, this strengthens the causal interpretation of the latent's role.

3. **Folded structure validation**: Use AlphaFold or similar tools to fold a subset of steered sequences and measure whether predicted structures actually contain the targeted features (e.g., zinc finger folds). This connects sequence-level steering to functional structural outcomes.