---
ver: rpa2
title: 'Efficient Deep Learning Infrastructures for Embedded Computing Systems: A
  Comprehensive Survey and Future Envision'
arxiv_id: '2411.01431'
source_url: https://arxiv.org/abs/2411.01431
tags:
- which
- learning
- network
- search
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of efficient deep learning
  infrastructures for embedded computing systems, addressing the challenge of deploying
  powerful deep neural networks (DNNs) on resource-constrained embedded devices. The
  authors systematically discuss efficient deep learning solutions from multiple perspectives,
  including manual and automated network design, network compression techniques, on-device
  learning, large language models, deep learning software/hardware frameworks, and
  intelligent applications.
---

# Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision

## Quick Facts
- arXiv ID: 2411.01431
- Source URL: https://arxiv.org/abs/2411.01431
- Reference count: 40
- Primary result: Comprehensive survey of efficient deep learning infrastructures for embedded computing systems

## Executive Summary
This paper provides a comprehensive survey of efficient deep learning infrastructures for embedded computing systems, addressing the challenge of deploying powerful deep neural networks (DNNs) on resource-constrained embedded devices. The authors systematically discuss efficient deep learning solutions from multiple perspectives, including manual and automated network design, network compression techniques, on-device learning, large language models, deep learning software/hardware frameworks, and intelligent applications. Key methods covered include efficient convolutional networks like MobileNets and GhostNets, hardware-aware neural architecture search (NAS) techniques, network pruning/quantization/distillation methods, on-device learning approaches for continual learning and federated learning, and efficient LLM compression techniques. The survey identifies future research directions such as automated teacher-student search, joint network compression, and hardware-aware benchmarks for efficient LLMs.

## Method Summary
The paper is a survey synthesizing existing research on efficient deep learning for embedded systems. The reproducible task involves benchmarking efficient architectures (e.g., MobileNetV2 vs. GhostNetV2) to verify claimed Accuracy-FLOPs-Latency trade-offs on embedded systems. Inputs include ImageNet and CIFAR datasets, with evaluation metrics of Top-1 Accuracy, FLOPs, and on-device latency. The method involves deploying pre-trained models onto embedded hardware (Nvidia Jetson, Raspberry Pi) and measuring actual inference latency using INT8 precision. Training procedures include standard SGD with momentum for fine-tuning and quantization-aware training for post-quantization optimization.

## Key Results
- Efficient convolutional networks like GhostNet maintain accuracy while reducing computational cost through decomposed feature generation
- Hardware-aware neural architecture search (NAS) yields better latency-performance trade-offs than FLOPs-only optimization
- On-device learning techniques that freeze weights and update only biases can drastically reduce memory bottlenecks for training on embedded devices
- Joint compression techniques that combine pruning, quantization, and distillation outperform sequential approaches
- Large language model compression remains a significant challenge requiring hardware-aware optimization

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Feature Generation
- **Claim:** Efficient convolutional networks maintain accuracy while reducing computational cost by replacing standard convolutions with decomposed or "cheap" operations.
- **Mechanism:** Instead of using a full $K \times K$ convolution, architectures like GhostNet use a primary standard convolution to generate intrinsic features, followed by cheap linear operations to generate "ghost" features that mimic the full set.
- **Core assumption:** The feature maps learned by standard convolutions contain significant redundancy, allowing a subset of features to be used to reconstruct the whole with minimal information loss.
- **Evidence anchors:** [Section 2.1]: Describes Ghost convolution as generating rich features using "simple and cheaper linear operations" to reduce FLOPs and parameters. [Abstract]: Notes that computation-intensive DNNs create a gap that efficient infrastructures must bridge.

### Mechanism 2: Latency-Constrained Architecture Search
- **Claim:** Directly optimizing for on-device latency during the architecture search process yields hardware-efficient networks that outperform models optimized solely for FLOPs.
- **Mechanism:** Methods like MnasNet incorporate a hardware feedback loop (latency measurement or lookup tables) into the reinforcement learning reward function, explicitly penalizing architectures that exceed target latency constraints.
- **Core assumption:** Theoretical metrics like FLOPs do not correlate linearly with actual inference speed on specific hardware due to memory access overheads and parallelism limits.
- **Evidence anchors:** [Section 3.2]: States that MnasNet uses a multi-objective RL reward combining accuracy and latency, contrasting with accuracy-only search. [Section 2.3]: Future Envision notes that reducing FLOPs does not necessarily lead to inference speedup.

### Mechanism 3: Activation-Free Training Optimization
- **Claim:** Freezing network weights and only updating lightweight bias modules (or specific layers) drastically reduces the memory bottleneck for on-device training.
- **Mechanism:** Standard backpropagation requires storing massive intermediate activation maps. By freezing weights (gradients for weights are not needed) and only updating biases or normalization parameters, the system avoids storing these activations, trading slight accuracy potential for massive memory savings.
- **Core assumption:** The pre-trained features in the frozen backbone are sufficiently general that only small, low-parameter adjustments are needed to adapt to new data.
- **Evidence anchors:** [Section 5.3]: Describes TinyTL, which freezes weights and only updates biases/lite residuals to reduce training memory consumption. [Section 5.1]: Identifies activation size as the main bottleneck of on-device training, often $\times 13.9$ larger than parameter size.

## Foundational Learning

**Concept: The Computational Gap**
- **Why needed here:** This is the central motivation for the entire survey—the mismatch between growing DNN complexity (e.g., ResNet-50's 4B FLOPs) and static embedded resources.
- **Quick check question:** Why does the paper argue that reducing FLOPs is necessary but often insufficient for solving this gap on real hardware?

**Concept: Structured vs. Unstructured Sparsity**
- **Why needed here:** Critical for understanding Section 4 (Pruning). Unstructured pruning creates speedup only on specialized hardware, while structured pruning (channels/layers) works on standard hardware but may sacrifice more accuracy.
- **Quick check question:** If you are deploying to a standard ARM Cortex-M CPU, which pruning type should you prioritize and why?

**Concept: Catastrophic Forgetting**
- **Why needed here:** Essential context for Section 5.2 (On-Device Continual Learning). It explains why simply training on new data destroys previously learned knowledge, necessitating specific strategies (e.g., rehearsal, regularization).
- **Quick check question:** How does the paper suggest "catastrophic forgetting" prevents ubiquitous embedded intelligence?

## Architecture Onboarding

**Component map:** Input (Task/Data) → Design (Manual/NAS) → Compression (Prune/Quantize) → Execution/Training (On-Device Learning) → Hardware (Jetson/TPU)

**Critical path:** Start with Hardware-Aware NAS (Section 3) to find a base architecture that fits latency budgets → Apply Joint Compression (Pruning + Quantization) (Section 4) → Implement On-Device Transfer Learning (Section 5.3) using memory-optimized training loops

**Design tradeoffs:**
- **FLOPs vs. Latency:** High theoretical efficiency (low FLOPs) does not guarantee low latency due to memory bandwidth costs (Section 2.3)
- **Generality vs. Specialization:** Specialized accelerators (e.g., Binary Neural Network accelerators) offer high speed but restrict model types (Section 4.1)
- **Training Memory vs. Adaptability:** Freezing layers saves memory but limits the model's ability to adapt to new domains (Section 5.3)

**Failure signatures:**
- **"Accuracy Collapse":** Occurs in differentiable NAS when skip-connect operations dominate the search (Section 3.2)
- **"Performance Degradation":** Occurs if non-structured pruning is applied to hardware that cannot handle irregular memory access (Section 4.1)
- **"Sub-optimal Convergence":** Occurs in on-device learning if the batch size is forced too low to fit in memory, destabilizing gradient descent

**First 3 experiments:**
1. **Latency Validation:** Deploy a standard ResNet vs. an EfficientNet (Manual Design) on a Raspberry Pi. Measure latency and compare against theoretical FLOP counts to validate the paper's claim that FLOPs are an unreliable proxy.
2. **NAS vs. Random Search:** Implement a simple Random Search with a Latency Lookup Table vs. a Random Search with an Accuracy-only objective. Verify if the latency-constrained search produces a model closer to the target time budget.
3. **Memory Profiling:** Profile the peak RAM usage of fine-tuning a MobileNet on-device. Compare "Full Fine-tuning" vs. "Bias-Only Fine-tuning" (Section 5.3) to quantify the memory reduction trade-off.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can neural architecture search (NAS) be effectively utilized to automatically identify the optimal teacher-student network pair for knowledge distillation?
- Basis in paper: [explicit] Section 4.4 states that current empirical methods for selecting teacher-student pairs often yield sub-optimal accuracy and suggests leveraging NAS to automate this search process.
- Why unresolved: Heuristic selection of teacher networks does not account for the specific learning capacity and architectural compatibility of the student network, often leading to missed accuracy potential.
- What evidence would resolve it: The development of a NAS framework that consistently identifies teacher-student pairs yielding higher accuracy than manually selected baselines across diverse tasks.

**Open Question 2**
- Question: Can hardware-aware benchmarks be established for Large Language Models (LLMs) that accurately predict runtime performance (latency and energy) rather than relying on theoretical complexity metrics?
- Basis in paper: [explicit] Section 6.5 identifies a gap where current efficiency metrics like parameter count or FLOPs do not accurately reflect runtime performance on specific hardware systems.
- Why unresolved: Theoretical complexity metrics ignore memory access bottlenecks and parallelism constraints inherent in actual hardware, making fair comparisons of efficiency difficult.
- What evidence would resolve it: A standardized benchmark suite that evaluates LLMs using direct hardware metrics (latency, energy) across various systems (CPUs, GPUs, Edge TPUs).

**Open Question 3**
- Question: Is it possible to develop efficient NAS algorithms that search for optimal hybrid networks combining the strengths of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)?
- Basis in paper: [explicit] Section 3.4 notes that searching for hybrid networks remains under-explored, despite the potential to combine CNN hardware efficiency with ViT accuracy.
- Why unresolved: Most existing NAS algorithms focus on either CNNs or ViTs exclusively, lacking the search strategies to navigate the complex hybrid design space effectively.
- What evidence would resolve it: NAS algorithms that successfully explore a combined search space to produce hybrid models achieving superior accuracy-efficiency trade-offs compared to pure CNN or ViT baselines.

## Limitations
- The paper is a survey rather than presenting original experimental results, limiting direct empirical validation
- Many architectural details and training recipes for specific efficient models are referenced from original papers without full specification
- Hardware-specific performance claims depend heavily on target platform configurations that may vary significantly

## Confidence
- **High Confidence:** The fundamental computational gap between DNN complexity and embedded constraints is well-established; trade-offs between FLOPs and latency are documented across literature
- **Medium Confidence:** The three core mechanisms (decomposed feature generation, latency-constrained search, activation-free training) are theoretically sound based on cited papers
- **Low Confidence:** Some claims about future research directions (automated teacher-student search, joint compression) remain largely speculative without detailed validation approaches

## Next Checks
1. Deploy MobileNetV2 vs GhostNetV2 on a Raspberry Pi to verify the paper's claim that FLOPs are unreliable proxies for actual latency, measuring both theoretical FLOPs and measured inference time
2. Implement a simple latency-constrained architecture search with a lookup table on CIFAR-10 and compare against accuracy-only search to validate hardware-aware optimization benefits
3. Profile memory usage during on-device fine-tuning of MobileNet on embedded hardware, comparing full fine-tuning vs. bias-only training to quantify the activation memory bottleneck described in Section 5.1