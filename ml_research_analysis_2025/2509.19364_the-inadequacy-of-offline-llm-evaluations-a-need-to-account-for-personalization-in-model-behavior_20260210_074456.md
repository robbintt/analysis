---
ver: rpa2
title: 'The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization
  in Model Behavior'
arxiv_id: '2509.19364'
source_url: https://arxiv.org/abs/2509.19364
tags:
- evaluations
- offline
- questions
- evaluation
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that standard offline LLM evaluations\u2014\
  where models answer questions independently without memory\u2014fail to capture\
  \ how models actually behave in practice due to user personalization. The authors\
  \ compared offline API-based evaluations with field evaluations conducted by 800\
  \ real ChatGPT and Gemini users, finding that field evaluations consistently produce\
  \ more heterogeneous responses across nine recommendation questions and benchmark\
  \ tasks (MMLU and ETHICS)."
---

# The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior

## Quick Facts
- **arXiv ID**: 2509.19364
- **Source URL**: https://arxiv.org/abs/2509.19364
- **Reference count**: 40
- **Primary result**: Standard offline LLM evaluations fail to capture real-world model behavior due to user personalization, producing systematically different outputs than field evaluations with actual users.

## Executive Summary
This paper demonstrates that standard offline LLM evaluations—where models answer questions independently without memory—fail to capture how models actually behave in practice due to user personalization. The authors compared offline API-based evaluations with field evaluations conducted by 800 real ChatGPT and Gemini users, finding that field evaluations consistently produce more heterogeneous responses across nine recommendation questions and benchmark tasks (MMLU and ETHICS). For instance, offline evaluations recommended Tesla 93% of the time versus 35% in field settings. The study also shows that personalization-induced variability is large enough to reorder model rankings on leaderboards. The authors propose using sock puppet evaluations with simulated user profiles to better capture real-world behavior and call for greater researcher access to personalization mechanisms in commercial LLM platforms.

## Method Summary
The authors conducted three types of evaluations: offline (stateless API calls at temperature=1), field (800 real users on ChatGPT and Gemini with personalization enabled), and sock puppet simulations (synthetic user profiles prepended to prompts). They tested 13 prompts across recommendation questions, MMLU college medicine, and ETHICS scenarios. Offline evaluation collected 100 responses per prompt. Field evaluation recruited 800 Prolific users (400 ChatGPT, 400 Gemini) stratified by race and gender. Sock puppet methods included SP-History (random WildChat history), SP-RAG (retrieval-augmented history), and SP-Profile (concatenated persona and descriptive sentences). Heterogeneity metrics included unique response counts, top-5 coverage, and Shannon entropy.

## Key Results
- Offline evaluations recommended Tesla 93% of the time versus 35% in field settings
- Field evaluations produced more heterogeneous responses across all nine recommendation questions
- Personalization-induced variability was large enough to reorder model rankings on MMLU leaderboard
- SP-Profile sock puppets exceeded field heterogeneity but may not match real user distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalization systems inject user-specific context that systematically alters model outputs
- Mechanism: User-specific context injection through memory banks, user profiles, and interaction history creates divergent priors that influence generation
- Core assumption: The personalization mechanisms (memory banks, search history) are causally responsible for the output heterogeneity observed
- Evidence anchors:
  - [abstract] "identical benchmark questions to the same language model can produce markedly different responses when prompted to a state-less system, in one user's chat session, or in a different user's chat session"
  - [section] "OpenAI's ChatGPT stores and uses a user memory bank, Google Gemini incorporates user search history in its responses"
  - [corpus] Weak corpus evidence - related papers (CUPID, PersonaLens) discuss personalization techniques but don't validate this causal mechanism
- Break condition: The mechanism would break if output heterogeneity were caused by sampling variance rather than personalization context

### Mechanism 2
- Claim: Offline evaluations using stateless API calls fail to capture the context-dependence of model behavior
- Mechanism: Stateless inference removes user-specific priors that would normally guide model behavior in deployed systems
- Core assumption: The API and chat interfaces use identical underlying model weights and only differ in personalization context
- Evidence anchors:
  - [abstract] "Standard offline evaluations for language models—a series of independent, state-less inferences made by models—fail to capture how language models actually behave in practice"
  - [section] "We conduct offline evaluation through repeated API calls at a temperature of 1 to GPT-4o mini and Gemini 2.0 Flash, the same models we had participants use in their chat interface"
  - [corpus] CUPID paper notes that "humans hold dynamic preferences that change depending on the context"
- Break condition: The mechanism would break if field evaluations used different model versions or system prompts

### Mechanism 3
- Claim: Sock puppet evaluations with synthetic user profiles can partially simulate personalization effects
- Mechanism: Prepending synthetic user profiles or conversation histories to prompts creates pseudo-personalization that increases output heterogeneity
- Core assumption: The distribution of synthetic profiles approximates the distribution of real user profiles
- Evidence anchors:
  - [abstract] "more realistic evaluations could be achieved by simulating the personalization users experience"
  - [section] "Among the three sock puppets, the Profile method (dark blue cross) tends to produce the highest heterogeneity, exceeding even that of the field evaluation"
  - [corpus] Weak corpus evidence - PersonaLens provides personalization benchmarks but doesn't validate sock puppet methodology
- Break condition: The mechanism would break if synthetic profiles failed to capture the full diversity or structure of real user profiles

## Foundational Learning

- Concept: Temperature sampling vs. personalization variance
  - Why needed here: The paper uses temperature=1 for offline evaluations, introducing sampling variance that must be distinguished from personalization-induced variance
  - Quick check question: How would you design an experiment to separate sampling variance from personalization variance?

- Concept: Retrieval-augmented generation for personalization
  - Why needed here: SP-RAG uses RAG to retrieve relevant user histories, a key technique for simulating personalization
  - Quick check question: What are the trade-offs between random history sampling (SP-History) and RAG-based retrieval (SP-RAG)?

- Concept: Ecological validity in benchmark design
  - Why needed here: The paper's core argument concerns whether benchmarks predict real-world behavior
  - Quick check question: What design elements would make a benchmark ecologically valid for personalized LLMs?

## Architecture Onboarding

- Component map:
  - Offline evaluation: Stateless API calls (temperature=1, no user context)
  - Field evaluation: User chat sessions with personalization enabled (memory, search history)
  - Sock puppet simulation: Synthetic context injection (History/RAG/Profile methods)

- Critical path:
  1. Define benchmark questions (MMLU, ETHICS, recommendation prompts)
  2. Run offline evaluation (n=100 samples per question)
  3. Run field evaluation (n=800 users, stratified sampling)
  4. Compute heterogeneity metrics (Unique, Top-5 Coverage, Entropy)
  5. Compare offline vs. field distributions

- Design tradeoffs:
  - Profile sock puppets exceed field heterogeneity but may not match real user distributions
  - Field evaluations are more realistic but have high attrition (50.2%) and privacy concerns
  - Sock puppets scale better but may miss emergent personalization patterns

- Failure signatures:
  - Offline and field distributions converge unexpectedly → check model version consistency
  - Profile heterogeneity exceeds field → synthetic profiles may be over-engineered
  - High rate of "unknown" responses → prompt format mismatch between evaluation modes

- First 3 experiments:
  1. Replicate offline vs. field comparison on a single recommendation question with n=50 users
  2. Test SP-Profile method with varying profile diversity to calibrate heterogeneity levels
  3. Measure MMLU score variance across 10 simulated users to estimate personalization-induced ranking changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can sock puppet evaluations be calibrated to ensure synthetic user profiles match the heterogeneity of real-world user populations?
- Basis in paper: [explicit] The authors note that their "Profile" sock puppet method produced higher heterogeneity than the field evaluation, stating that this approach is promising but "contingent upon achieving appropriate distribution alignment."
- Why unresolved: The paper demonstrates that different simulation strategies (History, RAG, Profile) yield inconsistent results, but does not establish a methodology for aligning these synthetic distributions with the ground truth of real user behavior.
- What evidence would resolve it: A study validating a specific weighting or generation protocol for synthetic profiles that replicates the response distribution patterns observed in the 800-user field evaluation.

### Open Question 2
- Question: Does the personalization-induced variance observed in simulated MMLU evaluations persist in large-scale field evaluations with real users?
- Basis in paper: [inferred] The authors detected enough variance in simulated evaluations to reorder model rankings on the HELM-Lite benchmark, but explicitly note they "do not perform a field evaluation here due to cost constraints."
- Why unresolved: While field evaluations on 13 prompts confirmed increased heterogeneity, the finding that personalization can invalidate leaderboard rankings relies entirely on sock puppet simulations rather than the study's primary field methodology.
- What evidence would resolve it: A field study administering the full HELM-Lite subset to real users to confirm if the 3.7 percentage point variability and subsequent model reordering occur in practice.

### Open Question 3
- Question: To what extent do specific personalization features (e.g., memory banks vs. search history) drive the divergence between offline and field evaluation results?
- Basis in paper: [inferred] The authors identify distinct mechanisms like OpenAI's "memory bank" and Gemini's "search history" as sources of personalization, and explicitly call for "transparency regarding personalization mechanisms" to enable more realistic evaluations.
- Why unresolved: The study measures the aggregate impact of personalization but cannot isolate the causal contribution of individual features (memory, history, instructions) due to the opacity of the deployed chat interfaces.
- What evidence would resolve it: A platform-facilitated ablation study where specific personalization features are selectively disabled to measure their isolated impact on output variance.

## Limitations

- Field evaluation sample used a convenience sample from Prolific with demographic quotas but no additional user behavior data collection
- Sock puppet methods rely on WildChat dataset which may not accurately represent real user interaction patterns
- Study only examined two models and a limited set of tasks, raising questions about generalizability to other architectures

## Confidence

- **High confidence**: The core finding that offline and field evaluations produce systematically different output distributions (Tesla 93% vs 35% recommendation)
- **Medium confidence**: The relative ordering of sock puppet methods (Profile > RAG > History) and their ability to simulate field heterogeneity
- **Low confidence**: The specific attribution of heterogeneity to particular personalization mechanisms (memory vs search history)

## Next Checks

1. **Mechanism isolation study**: Design a controlled experiment where the same user account is evaluated in both stateless and stateful modes to directly measure personalization effects while controlling for model version and user demographics.
2. **Sock puppet calibration**: Compare the distribution of synthetic profiles generated by SP-Profile against real user profile distributions from platform telemetry (if accessible) to validate representativeness.
3. **Cross-model generalization**: Replicate the offline vs. field comparison across 5-10 additional LLM models to assess whether personalization effects vary by model architecture or training approach.