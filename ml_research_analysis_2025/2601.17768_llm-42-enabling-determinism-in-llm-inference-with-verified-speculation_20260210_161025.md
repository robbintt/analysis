---
ver: rpa2
title: 'LLM-42: Enabling Determinism in LLM Inference with Verified Speculation'
arxiv_id: '2601.17768'
source_url: https://arxiv.org/abs/2601.17768
tags:
- llm-42
- tokens
- inference
- determinism
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-42 addresses non-determinism in LLM inference, which arises
  from floating-point non-associativity combined with dynamic batching and varying
  reduction orders in GPU kernels. The problem is that batch-invariant computation,
  while guaranteeing determinism, severely degrades throughput by eliminating performance
  optimizations like split-K parallelism and requiring new kernel implementations.
---

# LLM-42: Enabling Determinism in LLM Inference with Verified Speculation

## Quick Facts
- arXiv ID: 2601.17768
- Source URL: https://arxiv.org/abs/2601.17768
- Reference count: 40
- One-line primary result: LLM-42 achieves up to 41% higher throughput than batch-invariant deterministic inference while maintaining performance close to non-deterministic mode.

## Executive Summary
LLM-42 addresses the challenge of enabling deterministic inference for large language models under dynamic batching conditions. The core problem arises from floating-point non-associativity combined with varying reduction orders in GPU kernels, which creates non-determinism that breaks reproducibility and debugging. Traditional batch-invariant approaches solve determinism but at significant performance cost by eliminating optimizations like split-K parallelism.

The key insight is that most tokens generated from consistent states remain consistent even under non-deterministic execution, enabling a speculative verification approach. LLM-42 uses a decode-verify-rollback protocol where tokens are first generated using fast non-deterministic paths, then verified under fixed-shape reductions, and either committed or recomputed based on consistency checks. This achieves selective determinism with verification overhead only for requests requiring determinism, and groups multiple requests together to amortize verification costs while keeping rollback penalties bounded.

## Method Summary
LLM-42 implements a decode-verify-rollback (DVR) protocol that generates tokens through fast non-deterministic paths with dynamic batching, then verifies them using fixed-shape forward passes with deterministic kernel configurations. The approach leverages the observation that tokens from consistent states are mostly identical across runs, enabling high acceptance rates for verification. Grouped verification batches multiple requests (e.g., 8 requests × 64 tokens) to amortize overhead while keeping per-request rollback costs low. The verifier uses FlashAttention-3 with num_splits=1 and multimem-based AllReduce for deterministic execution, and selective determinism is controlled via per-request is_deterministic flags.

## Key Results
- LLM-42 achieves up to 41% higher throughput than SGLang's deterministic mode when deterministic traffic is 2-10%
- Verification overhead is incurred only for requests requiring determinism, maintaining near-non-deterministic performance for mixed workloads
- Grouped verification with 256 total tokens per step provides optimal trade-off between verification cost and rollback penalty
- Rollback and recomputation costs remain modest in practice, validating the assumption of low rollback rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokens generated from a consistent state remain mostly consistent even under dynamic batching, enabling speculative verification with high acceptance rates.
- Mechanism: The system exploits the observation that numerical drift from floating-point non-associativity rarely crosses sampling decision boundaries. When a sequence is in a consistent state, the next emitted token is highly likely to be identical across runs because logit perturbations are typically too small to alter the sampler's choice.
- Core assumption: Floating-point rounding errors cause bounded perturbations that rarely flip token rankings at decision boundaries.
- Evidence anchors:
  - [Page 5] "In the common case, hundreds of initial tokens are identical across both runs... however, once a single token diverges, the sequence rapidly drifts."
  - [Page 4] "Token-level inconsistencies are rare: as long as a sequence remains in a consistent state, the next emitted token is mostly identical across runs."
- Break condition: If models are highly sensitive to small logit perturbations (e.g., near-tie token probabilities), rollback frequency increases, potentially negating throughput benefits.

### Mechanism 2
- Claim: Fixing the reduction shape during verification guarantees deterministic replay, enabling the verifier to serve as a reference execution.
- Mechanism: GPU kernels use shape-consistent reductions—the same reduction strategy for all inputs of a given shape. By always verifying with a fixed token count (padding with dummy tokens at sequence end), the verifier produces bit-identical outputs across runs regardless of what other requests are batched during decoding.
- Core assumption: Kernels are position-invariant (output depends only on input values and total batch size, not position within batch) and avoid atomic reductions.
- Evidence anchors:
  - [Page 5] "Most GPU kernels use uniform, shape-consistent reductions: they apply the same reduction strategy to all elements within a given batch."
  - [Page 6] "We make the verifier deterministic by always operating on a fixed number of tokens... we handle this by padding with dummy tokens."
- Break condition: If operators use atomic reductions or non-deterministic collectives without fixed configurations, the verification pass itself becomes non-deterministic.

### Mechanism 3
- Claim: Grouped verification amortizes verification overhead while keeping rollback costs bounded per request.
- Mechanism: Instead of verifying large windows of single requests (low verification cost but high rollback penalty) or small windows (high verification overhead, low rollback), LLM-42 verifies small fixed-size windows of multiple requests together. Each request retains low rollback cost while the aggregate batch achieves compute-bound efficiency.
- Core assumption: Verification latency scales sub-linearly with batch size (memory-bound → compute-bound transition), and rollbacks are uncorrelated across requests.
- Evidence anchors:
  - [Page 7] "For smaller windows, the verification pass is memory-bound... As the verification window increases, the kernel transitions to being compute-bound, and the per-token cost drops sharply, reaching 0.05 ms at a window size of 512."
  - [Page 8] "Even batching just two requests reduces P99 latency to 43.06 seconds... the best overall configurations verifying a total of 256 tokens per step."
- Break condition: If rollback correlation increases under high load (many requests failing simultaneously), grouped verification may cascade into burst recomputation, degrading latency.

## Foundational Learning

- Concept: Floating-point non-associativity
  - Why needed here: Understanding why (a+b)+c ≠ a+(b+c) in finite precision explains how reduction order changes cause numerical drift, which propagates through autoregressive decoding.
  - Quick check question: Given two reduction trees that sum the same values in different orders, will they always produce identical results in float32? (Answer: No—the order of accumulation affects rounding.)

- Concept: Speculative decoding structure
  - Why needed here: LLM-42 repurposes speculative decoding's draft-verify pattern, but for determinism rather than approximation; understanding the original pattern clarifies key differences (no draft model, longer speculation windows).
  - Quick check question: In speculative decoding, what happens when the verifier rejects a token? (Answer: Rollback to last accepted token and continue from there—same pattern in LLM-42.)

- Concept: Dynamic batching and GPU kernel parallelization
  - Why needed here: Non-determinism arises because kernels choose different parallelization strategies (e.g., split-K) based on batch size; understanding this clarifies why batch-invariant computation is costly.
  - Quick check question: Why does split-K parallelization change numerical results? (Answer: It partitions the reduction dimension across thread blocks, altering the accumulation tree.)

## Architecture Onboarding

- Component map:
  - Fast-path decoder -> Verifier -> Scheduler/Controller -> Sampler -> Request queue
  - KV cache -> Decoder output -> Verification comparison -> Cache overwrite

- Critical path: Decode phase (memory-bound, ~1 token/iteration) → accumulate tokens until window full → verification pass (compute-bound, parallel forward) → compare tokens → commit or rollback + recompute → overwrite KV cache with verified entries

- Design tradeoffs:
  - Window size vs. batch count: Smaller windows per request mean lower rollback cost but require more requests batched together for efficient verification
  - Selective vs. global determinism: Overhead proportional to deterministic traffic fraction, but verification currently pauses all in-flight requests in the prototype
  - Kernel reuse vs. determinism guarantees: Reusing optimized kernels preserves throughput but requires careful configuration (attention splits, AllReduce type) for verifier consistency

- Failure signatures:
  - High rollback rate (>20%): Likely indicates model sensitivity at decision boundaries or batch-size variability; consider reducing window size
  - Verifier produces inconsistent results: Check attention kernel num_splits, AllReduce implementation (use multimem-based or fixed NCCL tree), and ensure no atomic operations
  - KV cache corruption after verification: Verify that cache entries are fully overwritten, not partially merged

- First 3 experiments:
  1. Baseline rollback characterization: Run ShareGPT at 12 QPS with 100% deterministic traffic; measure rollback frequency and recomputation overhead to validate acceptance rate assumptions on your workload
  2. Grouped verification sweep: Vary (window_size, batch_count) pairs with total tokens fixed at 256; plot P99 latency vs. recomputation cost to find optimal configuration for your latency targets
  3. Selective determinism stress test: Run mixed traffic (10% deterministic, 90% non-deterministic) and compare throughput against SGLang deterministic mode; verify overhead scales with deterministic fraction as claimed

## Open Questions the Paper Calls Out

- Can LLM-42's verification phase be isolated to a subset of GPU SMs or deferred to avoid stalling all in-flight requests during verification?
- How can LLM-42 support prefix cache sharing across requests or multi-turn conversations given its non-prefill-decode invariant design?
- Can LLM-42 be combined with speculative decoding approaches that use draft models, and what are the interaction effects?
- How does LLM-42 perform on multi-GPU tensor-parallel deployments where AllReduce consistency becomes critical?

## Limitations

- The approach doesn't guarantee full determinism across all GPU kernels, particularly those using atomic reductions
- Verification currently introduces a global pause affecting all in-flight requests during the verification pass
- The system cannot share prefix caches across multi-turn conversations due to non-prefill-decode invariance
- Multi-GPU performance and tensor-parallel deployment remain unexplored

## Confidence

**High Confidence**:
- The fundamental mechanism of decode-verify-rollback is sound and implementable
- Grouped verification provides better throughput than per-request verification
- Selective determinism achieves lower overhead than batch-invariant approaches when deterministic traffic is low
- Floating-point non-associativity combined with dynamic batching creates the documented determinism problem

**Medium Confidence**:
- The specific throughput gains (41% faster than SGLang deterministic mode) generalize to other models and workloads
- The rollback rates observed on Llama-3.1-8B-Instruct are representative of typical models
- The 256-token grouping strategy is near-optimal for most scenarios
- The grouped verification configuration sweep covers the relevant design space

**Low Confidence**:
- The approach maintains determinism guarantees across all GPU kernel implementations
- The performance benefits extend to models substantially different from Llama-3.1-8B-Instruct
- The rollback correlation patterns hold under high-load conditions with many concurrent deterministic requests
- The implementation complexity doesn't introduce significant operational overhead in production

## Next Checks

1. **Cross-Model Sensitivity Analysis**: Test LLM-42 across multiple model architectures (Mistral, Gemma, Phi series) with varying layer counts and attention mechanisms to quantify how rollback rates and performance gains vary with model characteristics.

2. **High-Load Correlation Testing**: Run stress tests with 50-100% deterministic traffic to measure rollback correlation under high load, monitoring whether failures cascade through grouped verification windows and measuring the impact on P99 latency.

3. **Kernel Coverage Validation**: Systematically audit all GPU kernels used in inference (attention variants, normalization, activation functions) to identify any remaining sources of non-determinism, implementing verification passes for these operations and measuring additional overhead.