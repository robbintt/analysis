---
ver: rpa2
title: Slowing Learning by Erasing Simple Features
arxiv_id: '2502.02820'
source_url: https://arxiv.org/abs/2502.02820
tags:
- loss
- width
- sample
- depth
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the effects of erasing low-order moments from
  image classification datasets on learning speed. It introduces QLEACE, a closed-form
  method to remove quadratically available information, and ALF-QLEACE, an approximate
  label-free variant.
---

# Slowing Learning by Erasing Simple Features

## Quick Facts
- arXiv ID: 2502.02820
- Source URL: https://arxiv.org/abs/2502.02820
- Reference count: 40
- Primary result: Erasing simple features from images affects learning speed differently depending on architecture and erasure method

## Executive Summary
This paper investigates how removing low-order statistical moments from image classification datasets impacts learning difficulty. The authors introduce QLEACE for exact quadratic erasure and ALF-QLEACE as a label-free approximation, measuring learning difficulty using prequential minimum description length (MDL). Their key finding is that while linear erasure consistently slows learning across architectures, quadratic erasure methods exhibit inconsistent effects - sometimes acting as data augmentation and even improving final performance through "backfiring" where models exploit injected higher-order artifacts.

## Method Summary
The study employs three erasure methods: LEACE (linear erasure via projecting out mean differences), QLEACE (exact quadratic erasure using Gaussian Wasserstein barycenters to equalize covariances), and ALF-QLEACE (approximate label-free quadratic erasure via iterative projection). Learning difficulty is quantified using prequential MDL, which measures the area under the learning curve. The authors test these methods across multiple architectures (MLPs, ConvNeXt, Swin) and datasets (CIFAR-10, SVHN, etc.), comparing performance against original unerased data.

## Key Results
- LEACE consistently increases MDL across all architectures and datasets, reliably slowing learning
- QLEACE exhibits "backfiring" in sophisticated architectures, where models learn to exploit higher-order statistics and achieve lower loss than on original data
- ALF-QLEACE and gradient-based methods sometimes act as data augmentation, reducing MDL compared to LEACE
- The effects are architecture-dependent, with deeper networks showing more pronounced backfiring effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing linearly available information via LEACE reliably increases learning difficulty across diverse architectures
- **Mechanism:** Neural networks exhibit Distributional Simplicity Bias, prioritizing class-conditional mean differences early in training. LEACE forces reliance on higher-order statistics by equalizing class means while minimizing distortion to the global data distribution
- **Core assumption:** Learning difficulty is accurately captured by Prequential MDL and networks prioritize low-order statistics
- **Evidence anchors:** Abstract confirms LEACE consistently slows learning; Section 1 shows increasing network width doesn't reduce MDL; related work validates theoretical trade-offs
- **Break condition:** Networks without DSB or those prioritizing higher-order features from initialization

### Mechanism 2
- **Claim:** Exact quadratic erasure can inadvertently inject useful higher-order information, causing backfiring
- **Mechanism:** QLEACE applies class-dependent affine transformations that alter data geometric support, introducing distinct geometric artifacts that highly expressive models can exploit after sufficient training
- **Core assumption:** Model has sufficient expressivity and training duration to model injected artifacts
- **Evidence anchors:** Abstract notes sophisticated architectures achieve lower loss than original data; Section 2.2 discusses potential injection of additional information; Section 4 shows ConvNeXt models achieve lower MDL with QLEACE
- **Break condition:** Label-free transformations or simple feedforward networks prevent backfiring

### Mechanism 3
- **Claim:** ALF-QLEACE acts as data augmentation regularizer, potentially enhancing learning speed
- **Mechanism:** By ablating directions of high variance divergence, ALF-QLEACE partially normalizes covariance structure, smoothing optimization landscape and reducing MDL relative to LEACE
- **Core assumption:** Removed high-variance directions are not strictly necessary for discrimination or their removal simplifies optimization
- **Evidence anchors:** Section 1 notes ALF-QLEACE acts as data augmentation on some datasets; Section 4 explains how ablating high-variance divergence directions makes data easier to learn
- **Break condition:** If removed directions contain critical discriminative features, ALF-QLEACE would degrade performance

## Foundational Learning

**Concept: Distributional Simplicity Bias (DSB)**
- **Why needed here:** Core hypothesis that models learn mean → covariance → higher-order correlations
- **Quick check question:** If a dataset has identical class means but distinct covariances, would a linear classifier perform well?

**Concept: Prequential Minimum Description Length (MDL)**
- **Why needed here:** Paper uses MDL (area under learning curve) rather than final accuracy to quantify "learning speed"
- **Quick check question:** Does lower MDL indicate faster or slower learning efficiency?

**Concept: Gaussian Wasserstein Barycenter**
- **Why needed here:** QLEACE uses this to find optimal target distribution minimizing distance from all class distributions while equalizing moments
- **Quick check question:** Why is Wasserstein metric preferred over Euclidean distance for finding this central distribution? (Hint: it preserves geometric structure)

## Architecture Onboarding

**Component map:** Data Preprocessor -> Erasure Module -> Trainer -> Evaluator

**Critical path:** Computing Gaussian Barycenter (QLEACE) is computational bottleneck; verify fixed-point iteration implementation for $\bar{\Sigma}$

**Design tradeoffs:**
- LEACE vs. QLEACE: LEACE robust but leaves quadratic info intact; QLEACE removes quadratic info but risks backfiring and requires O($d^3$) operations
- Label-free vs. Label-dependent: Label-dependent allows exact erasure but creates artifacts; label-free safer but incomplete

**Failure signatures:**
- Late-stage loss collapse: If validation loss drops significantly below baseline after epoch 10-15, erasure has backfired
- Visual degradation: QLEACE images look like "shattered glass"; if images look too natural, erasure may not have fully equalized covariances

**First 3 experiments:**
1. **Sanity Check (LEACE):** Train 2-layer MLP on CIFAR-10 with LEACE; verify MDL increases relative to control
2. **Backfiring Detection:** Train ConvNeXt V2 on QLEACE data; plot loss curves to >20 epochs to observe if loss plummets below baseline
3. **Augmentation Test:** Apply ALF-QLEACE to SVHN; compare MDL against LEACE to verify augmentation effect

## Open Questions the Paper Calls Out
None

## Limitations
- The study demonstrates inconsistent and architecture-dependent effects of quadratic erasure methods on learning difficulty
- Current understanding of concept erasure's interaction with network expressivity remains incomplete
- Analysis focuses primarily on classification accuracy metrics and MDL with limited investigation into whether backfiring represents genuine generalization improvement or overfitting to artifacts

## Confidence

- **High Confidence:** LEACE consistently increases MDL across architectures (systematic results across multiple datasets and model types)
- **Medium Confidence:** QLEACE backfiring mechanism involving higher-order artifact exploitation (observed empirically but mechanism requires further validation)
- **Medium Confidence:** ALF-QLEACE acting as data augmentation (demonstrated on some datasets but inconsistent across architectures)

## Next Checks

1. **Extended Training Analysis:** Conduct experiments extending training beyond typical convergence points (50+ epochs) on QLEACE-processed data to characterize temporal dynamics of backfiring

2. **Architectural Expressivity Correlation:** Systematically vary network width/depth and test on erasure-processed data to establish precise thresholds where backfiring emerges

3. **Artifact Ablation Studies:** Apply additional regularization or architectural constraints to models trained on QLEACE data to determine whether backfiring performance is due to genuine feature learning versus memorization of erasure-induced geometric patterns