---
ver: rpa2
title: 'Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR
  Variables with the Johns Hopkins Fall Risk Assessment Tool'
arxiv_id: '2510.20714'
source_url: https://arxiv.org/abs/2510.20714
tags:
- risk
- fall
- jhfrat
- clinical
- encounters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used a data-driven optimization approach to enhance
  the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) for more accurate inpatient
  fall risk prediction. By integrating additional electronic health record variables
  and preserving clinical interpretability, the constrained score optimization model
  achieved an AUC-ROC of 0.91, significantly improving upon the baseline JHFRAT performance
  of 0.86.
---

# Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool

## Quick Facts
- arXiv ID: 2510.20714
- Source URL: https://arxiv.org/abs/2510.20714
- Reference count: 15
- Primary result: CSO model achieved AUC-ROC of 0.91, improving upon baseline JHFRAT performance of 0.86

## Executive Summary
This study presents a constrained score optimization approach to enhance the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) for more accurate inpatient fall risk prediction. By integrating additional electronic health record variables including mobility scores, demographics, and comorbidities while preserving clinical interpretability, the model achieved significant performance improvements over the baseline tool. The methodology addresses the challenge of rare fall events by using targeted intervention intensity as a proxy labeling mechanism, enabling supervised learning in this high-stakes clinical domain.

## Method Summary
The study employed constrained score optimization (CSO) to enhance JHFRAT by incorporating EHR variables while maintaining clinical interpretability. The model maximizes a weighted dual-threshold log-likelihood objective (λ=0.5) subject to ordinality constraints on age, medications, and patient care equipment categories, plus non-negativity constraints. Fall risk labels were derived from targeted intervention intensity over 3-day rolling windows (≥6 interventions → high risk; ≤1 → low risk), enabling supervised learning despite rare fall events. The approach was validated using stratified 5-fold cross-validation on 54,209 admissions from three hospitals, with XGBoost serving as a benchmark model.

## Key Results
- CSO model achieved AUC-ROC of 0.91, significantly improving upon baseline JHFRAT performance of 0.86
- Model maintained clinical knowledge while incorporating mobility scores, demographics, and comorbidities
- SHAP analysis identified AM-PAC mobility score and "requires assistance" as top predictors
- The approach identified 3,788 additional high-risk patients and 860 low-risk patients previously misclassified
- Model demonstrated robustness across variations in risk labeling with stable coefficients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained score optimization improves fall risk stratification while preserving clinical workflow compatibility.
- Mechanism: The CSO model optimizes risk score coefficients via constrained logistic regression, maximizing a weighted dual-threshold log-likelihood objective (T=6, T=13 matching JHFRAT thresholds) subject to ordinality constraints (higher risk factor levels → higher coefficients) and non-negativity constraints. This preserves additive scoring structure and category thresholds while data-driven reweighting of feature contributions.
- Core assumption: Clinically established JHFRAT thresholds (6, 13) represent meaningful risk boundaries worth preserving; linear additive scoring is sufficient for risk differentiation.
- Evidence anchors:
  - [abstract] "constrained score optimization (CSO) models on JHFRAT assessment data... CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86"
  - [section 2.2] "The optimization problem seeks to determine coefficient β that maximizes predictive accuracy while satisfying clinical constraints... coefficient ordering constraints incorporated structured clinical knowledge by preserving ordinal relationships"
  - [corpus] Related paper "Joint Score-Threshold Optimization" addresses similar score-threshold co-design but emphasizes fundamental challenges in standard supervised learning for this domain

### Mechanism 2
- Claim: Intervention-based proxy labeling enables supervised learning despite rare fall events and intervention confounding.
- Mechanism: Instead of using rare fall events (0.92% of encounters) as labels, patients are categorized by sustained high/low targeted intervention intensity over 3-day windows (≥6 interventions → high risk; ≤1 → low risk). This proxies clinician-estimated risk while filtering noise from isolated decisions.
- Core assumption: Targeted interventions (hourly rounding, mobility restrictions) reflect clinicians' true risk assessments; sustained patterns are more reliable than single assessments; indeterminate cases can be partially recovered via pattern matching.
- Evidence anchors:
  - [abstract] "20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters"
  - [section 2.1] "Spearman correlation between average daily JHFRAT score and total daily targeted interventions was 0.61... This approach allowed us to utilize binary classification methods and is a more conservative labelling approach in the face of true fall risk uncertainty"
  - [corpus] No direct corpus validation of this labeling approach; represents methodological gap in fall risk literature

### Mechanism 3
- Claim: Mobility scores (AM-PAC, JH-HLM) provide incremental predictive value beyond JHFRAT components with minimal workflow disruption.
- Mechanism: EHR-derived mobility scores are aggregated per encounter (averaged) and added as constrained features. SHAP analysis identifies AM-PAC and "requires assistance" as top predictors. CSO's fixed thresholds absorb new features without rescaling the entire score.
- Core assumption: Mobility assessments are already collected or can be integrated without significant additional burden; averaged values capture risk-relevant signal despite temporal variation.
- Evidence anchors:
  - [abstract] "maintained clinical knowledge while incorporating mobility scores, demographics, and comorbidities"
  - [section 3.2] "Tree SHAP analysis... AM-PAC mobility score and the JHFRAT Requires assistance mobility item emerged as the most influential features"; "constrained score optimization models performed similarly with and without the EHR variables"
  - [corpus] Related work (Hoyer et al., citation 11) establishes association between longitudinal mobility levels and injurious falls, supporting feature relevance

## Foundational Learning

- **Constrained Optimization with Domain Constraints**
  - Why needed here: The CSO model isn't standard logistic regression—it adds inequality constraints (β_j ≤ β_k for ordinal factors, β ≥ 0) that require convex optimization solvers, not just gradient descent.
  - Quick check question: Can you explain why constraining age coefficients (60-69 < 70-79 < 80+) might hurt performance if the true relationship is non-monotonic?

- **Proxy Labeling for Rare Event Prediction**
  - Why needed here: With only 498 falls in 54K encounters, direct fall prediction has severe class imbalance and label noise (interventions prevent falls, masking true risk).
  - Quick check question: What are two failure modes if intervention intensity reflects resource constraints rather than patient risk?

- **Interpretability-Performance Tradeoffs in Clinical AI**
  - Why needed here: XGBoost (0.94 AUC) outperforms CSO (0.91), but CSO preserves scoring structure clinicians trust; understanding this tradeoff is central to the paper's contribution.
  - Quick check question: Name one scenario where you'd choose CSO over XGBoost despite the 3-point AUC gap.

## Architecture Onboarding

- **Component map:**
  - EHR extraction → JHFRAT assessments (7 categories, 18 binary variables) + mobility scores (AM-PAC, JH-HLM) + demographics/comorbidities
  - Intervention logs → 3-day sliding windows → risk category assignment (low/high/indeterminate) → pattern matching for indeterminate recovery
  - CSO (CVXPY + MOSEK solver, constrained logistic regression) || XGBoost (100 estimators, lr=0.1, no max depth)
  - Stratified 5-fold CV → ROC/PR curves → sensitivity analysis across intervention thresholds

- **Critical path:**
  1. Intervention extraction and 3-day window aggregation → determines all downstream labels
  2. Feature aggregation per encounter (mean of repeated assessments) → input matrix construction
  3. Constraint definition (ordinality for Age/Medications/Equipment) → CSO feasibility
  4. Dual-threshold objective tuning (λ parameter) → score calibration

- **Design tradeoffs:**
  - CSO vs XGBoost: +interpretability, +workflow alignment, -AUC (0.91 vs 0.94)
  - With vs without EHR variables: marginal AUC gain (~1 point), adds data dependency
  - Intervention threshold (4-8 per window): higher threshold → smaller high-risk cohort, more stable coefficients

- **Failure signatures:**
  - CSO convergence failure: Likely from infeasible constraints or poorly scaled features; check constraint set for contradictions
  - High coefficient variance across folds: Indicates label instability; increase intervention threshold stringency
  - XGBoost SHAP values highly variable across sensitivity cohorts: Suggests overfitting to specific label configurations; CSO more robust

- **First 3 experiments:**
  1. Reproduce the intervention-based labeling pipeline on a sample cohort: extract intervention logs, apply 3-day window logic, compare label distribution to paper's Table 2.
  2. Implement minimal CSO (JHFRAT variables only, no EHR additions) with ordinality constraints on Age category; validate coefficient ordering matches clinical expectations.
  3. Sensitivity sweep: Train both CSO and XGBoost across intervention thresholds 4-8; plot feature importance stability (Figures 5-6 reproduction) to confirm CSO's claimed robustness advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prospective implementation of the optimized CSO coefficients reduce actual fall rates compared to the original JHFRAT in a controlled clinical setting?
- Basis in paper: [explicit] "Future prospective studies could validate these findings in controlled settings" and "prospective evaluation of clinical outcomes following implementation would be essential to confirm the predicted improvements in fall prevention and resource allocation observed in our retrospective analysis."
- Why unresolved: The study relied on retrospective analysis with intervention-based risk labeling as a proxy for fall risk, not observed fall outcomes.
- What evidence would resolve it: A prospective trial comparing fall incidence rates between units using optimized CSO versus original JHFRAT scoring.

### Open Question 2
- Question: Can dynamic risk prediction models that update scores in real-time based on changing clinical status outperform the static encounter-averaged CSO model?
- Basis in paper: [explicit] "The study's focus on static risk assessment also suggests opportunities for dynamic risk modeling that incorporates temporal changes in patient condition, medication effects, and response to interventions."
- Why unresolved: The current model used average values across each encounter, losing temporal variation in risk factors.
- What evidence would resolve it: Development and validation of a time-updated model showing improved AUC-ROC or clinical utility over the static CSO approach.

### Open Question 3
- Question: What alternative mobility indicators can substitute for AM-PAC and JH-HLM scores in institutions without these standardized measures?
- Basis in paper: [explicit] "The reliance on AM-PAC and JH-HLM mobility assessments may limit immediate generalizability to institutions without these standardized mobility measures."
- Why unresolved: The model's performance depends on specific mobility assessments that may not be available universally.
- What evidence would resolve it: Validation studies demonstrating equivalent predictive performance using commonly available alternatives such as physical therapy assessments or ambulation orders.

## Limitations
- Intervention-based labeling approach may not accurately reflect true fall risk versus resource availability or protocol compliance patterns
- CSO model's ordinal constraints assume monotonic relationships that may not hold for all risk factors (e.g., age categories)
- Model's generalizability across healthcare systems with different intervention protocols and data collection practices remains untested

## Confidence
- **High confidence:** The CSO model architecture and constraint formulation (ordinality, non-negativity) are technically sound and correctly implemented as described.
- **Medium confidence:** The intervention-based labeling methodology provides reasonable proxies for fall risk given data constraints, though its validity depends on unvalidated assumptions about intervention-clinical risk relationships.
- **Medium confidence:** The claim that CSO improves upon baseline JHFRAT performance is well-supported (0.91 vs 0.86 AUC), though the incremental value of EHR variables beyond JHFRAT components is modest (~1 point).

## Next Checks
1. **Label Validation:** Conduct clinician review of randomly sampled high-risk and low-risk encounters to assess whether intervention-based labels align with clinical judgment of fall risk.
2. **Constraint Sensitivity:** Systematically relax ordinal constraints in the CSO model to quantify the performance cost of clinical interpretability requirements versus unconstrained optimization.
3. **Cross-Site Generalization:** Test model performance on data from a different healthcare system with distinct intervention protocols and data collection practices to evaluate robustness beyond the development site.