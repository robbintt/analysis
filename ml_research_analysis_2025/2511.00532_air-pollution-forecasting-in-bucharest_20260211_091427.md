---
ver: rpa2
title: Air Pollution Forecasting in Bucharest
arxiv_id: '2511.00532'
source_url: https://arxiv.org/abs/2511.00532
tags:
- forecasting
- data
- learning
- time
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multiple machine learning and deep learning
  models for PM2.5 air pollution forecasting in Bucharest, using a 4-year hourly dataset
  with pollution and meteorological measurements. Models tested include linear regression,
  SVR, ensemble methods (Random Forest, XGBoost), ARIMA variants, deep neural networks
  (MLP, KAN, RNN variants, CNN hybrids), and advanced architectures like transformers
  and LLMs.
---

# Air Pollution Forecasting in Bucharest

## Quick Facts
- arXiv ID: 2511.00532
- Source URL: https://arxiv.org/abs/2511.00532
- Reference count: 40
- PM2.5 forecasting achieved MAE ~3.1 for 1-hour predictions using transformer architectures

## Executive Summary
This study evaluates machine learning and deep learning models for PM2.5 air pollution forecasting in Bucharest using a 4-year hourly dataset. The research tests classical models (Linear Regression, SVR, Random Forest, XGBoost), statistical approaches (ARIMA variants), and advanced deep learning architectures including transformers and large language models. Results demonstrate that transformers (Informer, PatchTST) and advanced RNNs (stacked LSTM, GRU encoder-decoder) achieve the best performance, with transformers excelling at capturing both long-term seasonal dependencies and short-term fluctuations through self-attention mechanisms.

## Method Summary
The study uses 4 years of hourly data from Bucharest's B-1 monitoring station (Aug 2019–Jul 2023), containing PM2.5, PM10, NO2, SO2, CO, O3, temperature, wind speed/direction, and humidity. Data preprocessing includes FBEWMA outlier detection (threshold=5), linear interpolation for missing values, and Min-Max scaling. Models tested span classical ML, statistical methods, deep neural networks (MLP, KAN, RNN variants, CNN hybrids), and advanced architectures including transformers and LLMs. The research employs a chronological 80/20 train/test split and evaluates performance across 1, 2, and 4-hour forecasting horizons using MAE, RMSE, and R² metrics.

## Key Results
- Transformers (Informer, PatchTST) achieve best performance with MAE around 3.1–3.6 for 1-hour forecasts
- Advanced RNNs (stacked LSTM, GRU encoder-decoder) show strong performance, explaining 86% of variability at 1-hour horizon
- Random Forest and XGBoost perform well among classical models, while LLMs show moderate results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers achieve superior PM2.5 forecasting by simultaneously capturing long-term seasonal dependencies and short-term fluctuations without recurrence-based limitations.
- **Mechanism:** Self-attention creates direct connections between any time points regardless of temporal distance. ProbSparse attention (Informer) reduces quadratic complexity by selecting only informative queries. Patch-based tokenization (PatchTST) segments sequences into meaningful chunks, reducing noise while preserving temporal structure.
- **Core assumption:** PM2.5 levels depend on both immediate meteorological conditions and longer-term weekly/seasonal patterns that span beyond local sequential windows.
- **Evidence anchors:**
  - [abstract] "Transformers outperform others due to their ability to capture long- and short-term dependencies efficiently."
  - [section 6.1] "PatchTST outperforms the other when it comes to short-term predictions" with MAE 3.06; "Informer performs best for longer horizons" with MAE 3.87 at 2h.
  - [corpus] Neighbor paper "Long-Term PM2.5 Forecasting Using a DTW-Enhanced CNN-GRU Model" confirms deep learning struggles beyond 48 hours, suggesting transformer advantages may not generalize to very long horizons without architectural modifications.

### Mechanism 2
- **Claim:** Encoder-decoder RNN architectures with stacked or bidirectional configurations excel by first compressing historical context into fixed representations, then generating sequential multi-step predictions.
- **Mechanism:** The encoder processes 24-hour input sequences into a context vector; the decoder generates predictions autoregressively. Stacked layers (3-layer LSTM/GRU) learn hierarchical temporal features. Bidirectional processing captures both forward and backward dependencies within the input window.
- **Core assumption:** Future PM2.5 concentrations can be reconstructed from a compressed latent representation of historical pollution and meteorological data.
- **Evidence anchors:**
  - [section 6.1] "LSTM encoder-decoders show strong performance, explaining 86% of the variability for 1-hour forecasts" (MAE 3.16, R² 0.866).
  - [section 6.1] "Bidirectional stacked GRU model has the best performance in this group...processing sequences in both directions helps capture more dependencies" (MAE 3.32, R² 0.856 at 1h).
  - [corpus] Neighbor paper "Air Quality Prediction Using LOESS-ARIMA and Multi-Scale CNN-BiLSTM" validates CNN-BiLSTM hybrids for Indian megacities, suggesting encoder-decoder patterns transfer across geographic contexts.

### Mechanism 3
- **Claim:** Lag features and exogenous meteorological variables enable non-sequential models (MLP, ensemble methods) to approximate temporal dependencies that RNNs capture natively.
- **Mechanism:** Lag features explicitly encode PM2.5 history at t-1, t-2, etc., providing temporal context to feedforward architectures. Exogenous variables (NO2, SO2, O3, temperature, wind) inject physical domain knowledge—NO2 correlates negatively with O3 (-0.59), PM2.5 correlates positively with PM10 (0.79).
- **Core assumption:** PM2.5 concentrations exhibit autocorrelation and respond predictably to measured environmental factors with consistent time delays.
- **Evidence anchors:**
  - [section 3.4] "Lag features for the PM2.5 pollutant were added for the models that do not understand the concept of time."
  - [section 5.1] KAN-MLP hybrid (lag-fed) achieved MAE 4.01, outperforming vanilla MLP (4.24) and approaching Random Forest (3.50).
  - [corpus] Neighbor paper "Predicting Air Pollution in Cork, Ireland" similarly leverages meteorological exogenous variables, but corpus lacks direct comparison of lag feature effectiveness across studies—evidence is weak here.

## Foundational Learning

- **Concept: Time Series Stationarity (ADF/KPSS tests)**
  - **Why needed here:** The paper applies differencing and evaluates stationarity before modeling. Understanding why PM10 is "entirely stationary" while PM2.5 is "stationary in differences, but not in trends" informs model selection—ARIMA-based methods require stationarity assumptions.
  - **Quick check question:** If ADF rejects null (p < 0.05) but KPSS also rejects, is the series stationary? (Answer: No—it's difference-stationary, requiring differencing.)

- **Concept: Attention Mechanism Query-Key-Value**
  - **Why needed here:** Transformers dominate results. Understanding that queries attend to keys to weight values—scaled dot-product attention computes relevance scores—explains why Informer's ProbSparse attention (selecting informative queries only) improves efficiency without sacrificing accuracy.
  - **Quick check question:** What does ProbSparse attention modify in standard self-attention? (Answer: It samples only the most informative queries rather than computing full attention matrix.)

- **Concept: Encoder-Decoder Context Vector Bottleneck**
  - **Why needed here:** The paper compares RNN encoder-decoders against transformers. Traditional encoder-decoders compress all input into a fixed-size context vector; transformers eliminate this via cross-attention, allowing decoder direct access to encoder outputs.
  - **Quick check question:** Why might transformer cross-attention outperform LSTM encoder-decoder context vectors for long sequences? (Answer: No information compression bottleneck; decoder attends to all encoder states.)

## Architecture Onboarding

- **Component map:** Raw data (9 features) → FBEWMA outlier detection → linear interpolation → feature engineering (lags, temporal flags) → Min-Max scaling → model input → predictions (1h, 2h, 4h horizons)

- **Critical path:**
  1. Data validation: Check for negative pollutant values (sensor errors) → apply FBEWMA with threshold=5
  2. Feature engineering: Generate lag features for non-sequential models; add temporal flags (weekday, season)
  3. Train/test split: Chronological 80/20 (no shuffling—preserves temporal structure)
  4. Model selection: Start with ensemble baselines (RF/XGBoost), then RNN encoder-decoder, then transformers if resources permit
  5. Hyperparameter tuning: Grid search with 3-5 fold CV (classical); early stopping + dropout for neural models

- **Design tradeoffs:**
  - Random Forest vs. XGBoost: RF (bagging) faster to train, parallelizable; XGBoost (boosting) often slightly better but sequential training
  - LSTM vs. GRU: GRU fewer parameters, faster training; LSTM may capture more complex patterns but risk overfitting on small data
  - Informer vs. PatchTST: Informer optimized for long horizons; PatchTST excels at short-term with patch-based noise reduction
  - Transformer vs. LLM: Specialized time-series transformers outperform general LLMs (T5) for this task; RAG provides marginal improvement

- **Failure signatures:**
  - Negative PM2.5 predictions: Input contained unscaled negative sensor readings; fix with preprocessing correction
  - MAE > 6.0 at 4h horizon: Model likely using single-layer architecture or insufficient lookback window; increase to 24-48h input
  - R² dropping sharply from 1h to 4h: Temporal dependencies not captured; switch from basic RNN to encoder-decoder or transformer
  - T5 models producing incoherent outputs: Input not properly formatted as natural language prompts; requires text serialization

- **First 3 experiments:**
  1. **Baseline establishment:** Train Random Forest with default lag features (t-1 through t-6), evaluate MAE at 1h/2h/4h. Target: MAE ≤ 4.0 at 1h. If exceeded, check feature engineering and outlier removal.
  2. **Architecture comparison:** Train 3-layer bidirectional GRU encoder-decoder with 24h lookback, 64 hidden units. Compare against RF baseline. Expect ~10-15% MAE reduction at 4h horizon.
  3. **Transformer validation:** Train PatchTST with 48-step input, patch size 8, predicting 1h/4h/8h. Compare against GRU. If PatchTST doesn't outperform at 1h, verify Min-Max scaling applied correctly and patch size appropriate for data frequency.

## Open Questions the Paper Calls Out

- **Question:** Can hybrid architectures combining Graph Convolutional Networks (GCNs) with transformers or RNNs improve prediction accuracy by modeling spatial dependencies across multiple air quality stations in Bucharest?
- **Question:** To what extent does the integration of real-time traffic flow and industrial activity data enhance the predictive performance of PM2.5 forecasting models?
- **Question:** Does using forecasted meteorological variables (specifically temperature and humidity) as inputs improve long-horizon PM2.5 forecasting accuracy compared to using only historical measurements?

## Limitations

- Dataset composition uncertainty: Specific temporal coverage gaps and exact station location within Bucharest are not detailed, limiting generalizability to other urban contexts.
- Hyperparameter sensitivity: Transformer performance heavily depends on architecture-specific settings (patch size, attention head count) not fully disclosed, making exact replication challenging.
- Model comparison fairness: Different preprocessing pipelines (scaling/no scaling) and input window sizes across models may bias results, though justified by architecture requirements.

## Confidence

- **High:** Transformers outperforming classical models at short horizons (1-2h) - supported by multiple ablation comparisons within the paper.
- **Medium:** LLM performance ranking (T5-small > T5-base > T5-large) - based on limited trials with small dataset, potential for different results with larger pretraining data.
- **Low:** Generalization of PatchTST superiority across all horizons - neighbor paper suggests transformers may degrade beyond 48h without modifications.

## Next Checks

1. Replicate the neighbor paper's DTW-enhanced CNN-GRU architecture on Bucharest data to verify transformer advantage persists at 8+ hour horizons.
2. Conduct ablation study removing lag features from ensemble methods to quantify their contribution versus temporal architectures.
3. Test model performance with alternative outlier detection thresholds (FBEWMA threshold=3 vs threshold=5) to assess robustness to preprocessing choices.