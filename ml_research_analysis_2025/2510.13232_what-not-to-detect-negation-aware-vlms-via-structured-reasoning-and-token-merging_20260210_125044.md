---
ver: rpa2
title: 'What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token
  Merging'
arxiv_id: '2510.13232'
source_url: https://arxiv.org/abs/2510.13232
tags:
- negation
- image
- caption
- reasoning
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vision-language models (VLMs) suffer from affirmative bias, failing\
  \ to comprehend negation in detection tasks, often treating negated queries like\
  \ \"dog not on leash\" the same as \"dog on leash.\" This paper introduces two core\
  \ contributions: (1) CoVAND, a negation-focused dataset created via chain-of-thought\
  \ reasoning and VQA-based caption alignment, yielding 9.29% negation word frequency;\
  \ and (2) NegToMe, a text token merging module that preserves negation cues by merging\
  \ fragmented tokens (e.g., \"not\" + \"dog\" \u2192 \"not-dog\") and applying a\
  \ negation-aware boost to maintain correct polarity. These innovations are integrated\
  \ with LoRA fine-tuning targeting deep cross-attention layers."
---

# What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging

## Quick Facts
- arXiv ID: 2510.13232
- Source URL: https://arxiv.org/abs/2510.13232
- Reference count: 0
- Primary result: Introduces NegToMe token merging and LoRA fine-tuning to address affirmative bias in VLMs, achieving state-of-the-art negation detection with +10.8 NMS-AP and -19.1% FPR on negation subsets.

## Executive Summary
Vision-language models suffer from affirmative bias, often failing to comprehend negation in detection tasks. This paper addresses this by introducing CoVAND, a negation-focused dataset created via chain-of-thought reasoning, and NegToMe, a text token merging module that preserves negation cues. The approach integrates these with LoRA fine-tuning targeted at deep cross-attention layers. Experiments demonstrate significant improvements across multiple benchmarks, with state-of-the-art performance on negation understanding tasks.

## Method Summary
The method combines CoVAND dataset generation, NegToMe token merging, and LoRA fine-tuning. CoVAND is created using GPT-4o with a 3-step chain-of-thought process and VQA-based caption alignment, yielding 9.29% negation word frequency. NegToMe merges fragmented tokens (e.g., "not" + "dog" → "not-dog") and applies a negation-aware boost to maintain correct polarity. LoRA adapters are placed in deep cross-attention layers to efficiently adapt frozen VLMs. The approach is validated on multiple detection benchmarks with parameter-efficient fine-tuning.

## Key Results
- +10.8 NMS-AP and -19.1% FPR on OVDEval negation subset
- +7.2 mAP on D3 absence subset
- Consistent gains across multiple benchmarks, demonstrating robust negation understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A token merging strategy structurally binds a negation cue with its target attribute, creating a unified semantic representation that resists being ignored by attention mechanisms.
- **Mechanism:** The proposed NegToMe module addresses the "fragmented token" problem. Standard tokenizers separate a negation cue (e.g., "not") from the word it modifies (e.g., "lying"). The model's attention mechanism, biased by pre-training on data scarce in negation, tends to ignore these isolated, low-attention negation tokens. NegToMe uses a parser to group these tokens into a single phrase (e.g., "not lying") and computes a weighted average embedding. This unified representation is less likely to be ignored.
- **Core assumption:** The affirmative bias in VLMs is an architectural flaw caused by token fragmentation that allows negation cues to be treated as separate, low-importance tokens.
- **Evidence anchors:**
  - [abstract] "NegToMe fundamentally addresses the structural loss of negation cues in tokenization, grouping them with attributes into coherent semantic phrases."
  - [Section 3.2] "NegToMe addresses this by first merging these fragmented tokens into a single, coherent phrase. Through this binding, the negated concept of 'not lying' can be learned as semantically distinct from 'lying'."
  - [corpus] No strong corpus evidence. This claim is novel to this paper.
- **Break condition:** The negation boost factor is too low, failing to amplify the negation signal sufficiently, or the parser incorrectly groups tokens, merging semantically unrelated words.

### Mechanism 2
- **Claim:** A negation-aware boost applied to the merged embedding explicitly amplifies the negation cue's influence in cross-attention layers.
- **Mechanism:** After merging the negation phrase, a boost factor is applied to the embedding of the specific negation token within the phrase during the weighted averaging. This creates a merged embedding that is pulled closer to the vector of the negation cue. The paper's analysis (Eq. 3) shows this amplifies the cue's contribution to the final representation.
- **Core assumption:** Simply merging tokens is insufficient; the model's inherent attention bias needs to be actively counteracted by making the negation cue's signal stronger within the merged representation.
- **Evidence anchors:**
  - [abstract] "This merged representation is enhanced with a negation-aware boost, explicitly amplifying the negated signal to ensure its polarity is preserved for downstream fusion."
  - [Section 3.2] "The negation boosting factor amplifies the cue so that the merged embedding explicitly retains the negated meaning..."
  - [corpus] No corpus evidence. This is a novel architectural contribution.
- **Break condition:** The boost factor is so large it destabilizes training or overshadows other critical semantic information in the phrase.

### Mechanism 3
- **Claim:** Low-Rank Adaptation (LoRA) targeted at deep cross-attention layers is an efficient method to adapt frozen VLMs for negation understanding.
- **Mechanism:** The analysis of attention weights revealed that in baseline models, the negation signal often dissipates in later decoder blocks. By injecting LoRA adapters into the query and value projections of the deep cross-attention layers, the model learns to refine its compositional understanding and integration of negation cues at the stage where final detection decisions are formed. This is parameter-efficient, modifying < 0.1% of parameters.
- **Core assumption:** The deep layers of a vision-language transformer decoder are the critical locus for compositional, multimodal reasoning.
- **Evidence anchors:**
  - [abstract] "This module is integrated with a parameter-efficient and strategic LoRA fine-tuning approach."
  - [Section 4.3] "Placing LoRA adapters in the deep fusion blocks consistently outperforms shallow. This is because deep placement maintains elevated attention on negation tokens in the later blocks where decisions are formed..."
  - [corpus] Corpus evidence on LoRA for VLMs is general and does not specifically target negation or deep-layer fusion.
- **Break condition:** The LoRA rank is too low to capture the necessary adaptation, or the adapter is placed in layers too shallow for compositional reasoning.

## Foundational Learning

**Concept: Affirmative Bias in VLMs**
- **Why needed here:** This is the core problem. It's the tendency of pre-trained VLMs to ignore negation cues (like "not") and treat a negated phrase as equivalent to its affirmative form.
- **Quick check question:** If a model has affirmative bias, how would it detect "a dog *not* wearing a red collar" in an image with a dog wearing a red collar?

**Concept: Token Fragmentation**
- **Why needed here:** This architectural flaw motivates the NegToMe module. Standard tokenizers split phrases into sub-word tokens, allowing the low-frequency "not" token to be ignored by attention, leading to a loss of semantic polarity.
- **Quick check question:** How does a standard tokenizer break down the phrase "un-peeled banana," and why might a model fail to understand it correctly?

**Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
- **Why needed here:** The method relies on LoRA to adapt large, frozen models. Understanding how LoRA works (injecting trainable low-rank matrices into attention projections) is critical to grasping the "lightweight adaptation recipe."
- **Quick check question:** In this paper, which specific projections in the cross-attention layers are modified by the LoRA adapters?

## Architecture Onboarding

**Component map:** Input Text -> Tokenizer -> NegToMe (Merge & Boost) -> Text Encoder -> Cross-Attention Layers (with LoRA) -> Detection Head

**Critical path:** The inference path is: `Input Text -> Tokenizer -> NegToMe (Merge & Boost) -> Text Encoder -> Cross-Attention Layers (with LoRA) -> Detection Head`. The training path adds the loss calculation on the outputs of the Detection Head.

**Design tradeoffs:**
- **Dataset Generation:** CoVAND is synthetic (GPT-4o). Tradeoff: Scalability vs. potential for LLM-specific biases. Requires VQA alignment to reduce noise.
- **LoRA vs. Full FT:** Tradeoff: Modifying <0.1% of parameters is extremely efficient but may have a lower performance ceiling than full fine-tuning. The paper demonstrates it is sufficient.
- **NegToMe Boost:** The boost factor is a hyperparameter. Tradeoff: A higher boost amplifies the signal but could risk destabilizing embeddings.

**Failure signatures:**
- **Affirmative Bias persists:** Model predicts bounding boxes for "person without skateboard" on a person *with* a skateboard.
- **Catastrophic Forgetting:** Performance degrades on standard, non-negation detection benchmarks.
- **High False Positive Rate:** Model detects "not dog" when only a dog is present, indicating the negation mechanism is not functioning.

**First 3 experiments:**
1. **Baseline Evaluation:** Evaluate the frozen VLM on the D³ and OVDEval negation subsets to establish baseline performance (low NMS-AP, high FPR) and confirm the problem.
2. **Ablation: NegToMe:** Train two models—one with NegToMe and one without—using only LoRA adapters on deep layers with CoVAND. Compare NMS-AP and FPR to isolate the contribution of the token merging mechanism.
3. **Ablation: LoRA Placement:** Train models with LoRA adapters placed in (a) shallow layers, (b) strided layers, and (c) deep layers. Compare attention maps and NMS-AP scores to validate that deep placement is critical.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can VLMs be adapted to resolve **multi-step relational logic** combined with negation (e.g., "a woman in white wedding dress not beside any men in suits")?
- **Basis in paper:** [explicit] The paper's qualitative analysis (Figure S25 and Appendix G) explicitly identifies "multi-step relational logic combined with negation" as a failure mode where the model defaults to affirmative bias.
- **Why unresolved:** Current methods like NegToMe focus on binding single attributes to negation cues, but struggle to enforce constraints over relationships between multiple entities or complex spatial logic.
- **What evidence would resolve it:** Success on a benchmark specifically designed for nested or relational negation queries, showing distinct bounding boxes for relational constraints.

**Open Question 2**
- **Question:** How can negation-aware detection models better handle **implicit or abstract negation cues** (e.g., "unlike", "untouched", "bare")?
- **Basis in paper:** [explicit] The authors explicitly list "resolving ambiguous or implicit negation cues like 'unlike' in 'origami unlike bird'" as a "difficult problem" and an open challenge in the limitations section.
- **Why unresolved:** The current NegToMe module and CoVAND dataset primarily target explicit negation markers ("not", "without"), leaving implicit linguistic variations under-addressed.
- **What evidence would resolve it:** Evaluation results on a dataset containing implicit negation synonyms, showing performance gains comparable to those seen with explicit markers.

**Open Question 3**
- **Question:** Is the reliance on an external dependency parser (e.g., spaCy) for **NegToMe** a robustness bottleneck for noisy or out-of-distribution text?
- **Basis in paper:** [inferred] Section 4.1 states that NegToMe "use[s] spaCy for the parser" to group tokens. The method's success relies on this preprocessing step correctly identifying the negation scope.
- **Why unresolved:** If the parser fails to correctly group a fragmented phrase due to grammatical errors or complex syntax, the token merging and subsequent boosting will apply to the wrong semantic unit.
- **What evidence would resolve it:** An ablation study measuring performance degradation when the parser is removed or fed syntactically novel/corrupted captions.

## Limitations
- **Architectural specificity:** NegToMe module is highly specific to text-tokenized VLMs and won't work for RPN-based detectors like GLIP or FIBER.
- **Dataset reliability:** CoVAND is entirely synthetic, raising questions about whether it captures genuine negation complexity or introduces LLM-specific biases.
- **Boost factor tuning:** The negation boost factor (β=2.0) lacks systematic sensitivity analysis and may be architecture-dependent.

## Confidence

**High Confidence (★★★):** The affirmative bias problem in VLMs is well-documented and this paper provides strong empirical evidence. The core mechanism of token fragmentation causing negation cues to be ignored by attention mechanisms is theoretically sound and supported by attention visualization evidence. The baseline performance degradation on negation subsets is substantial and reproducible.

**Medium Confidence (★★☆):** The CoVAND dataset generation methodology is rigorous with multiple verification steps, but synthetic data generation always carries risks of LLM-specific biases. The LoRA placement analysis showing deep layers are optimal is convincing, but the exact block selection (3-5) may be architecture-specific and not universally optimal.

**Low Confidence (★★☆):** The claim that NegToMe is a "fundamental" solution to negation understanding may be overstated. While it shows strong empirical results, the approach is highly specific to certain VLM architectures and may not generalize to all vision-language models or detection paradigms.

## Next Checks
- **Check 1:** Conduct a cross-architecture validation by applying NegToMe + LoRA to a different VLM backbone (e.g., BLIP-2 or LLaVA) to test whether the improvements transfer beyond the specific models tested (Grounding DINO, APE-Ti, Qwen-2.5-VL).
- **Check 2:** Perform a systematic sensitivity analysis on the negation boost factor (β) across different values (1.0, 1.5, 2.0, 2.5, 3.0) to determine whether 2.0 is optimal or architecture-dependent, and to identify potential instability thresholds.
- **Check 3:** Test the approach on real-world datasets with naturally occurring negation (e.g., Flickr30k with human-annotated negation captions) rather than synthetic data to validate that CoVAND's synthetic generation captures genuine negation complexity and doesn't introduce LLM-specific artifacts.