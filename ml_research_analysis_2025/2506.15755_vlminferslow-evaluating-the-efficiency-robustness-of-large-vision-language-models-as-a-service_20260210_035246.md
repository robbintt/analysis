---
ver: rpa2
title: 'VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language
  Models as a Service'
arxiv_id: '2506.15755'
source_url: https://arxiv.org/abs/2506.15755
tags:
- adversarial
- vlms
- vlminferslow
- efficiency
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VLMInferSlow, the first approach to evaluate
  efficiency robustness of large vision-language models (VLMs) in a realistic black-box
  setting where model architecture and parameters are inaccessible. The core method
  uses zero-order optimization combined with fine-grained adversarial objectives designed
  to maximize computational cost while maintaining imperceptible perturbations.
---

# VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service

## Quick Facts
- arXiv ID: 2506.15755
- Source URL: https://arxiv.org/abs/2506.15755
- Authors: Xiasi Wang; Tianliang Yao; Simin Chen; Runqi Wang; Lei YE; Kuofeng Gao; Yi Huang; Yuan Yao
- Reference count: 31
- Key outcome: First black-box efficiency robustness evaluation of VLMs, showing up to 128.47% increase in sequence length and 105.56% increase in latency using zero-order optimization with adversarial objectives

## Executive Summary
VLMInferSlow introduces a novel framework to evaluate the efficiency robustness of large vision-language models (VLMs) deployed as API services. Unlike traditional adversarial attacks targeting accuracy, this work focuses on inducing computational inefficiency through imperceptible perturbations that maximize sequence length, latency, and energy consumption. Using zero-order optimization in a black-box setting, the method generates adversarial examples that remain visually indistinguishable while significantly degrading model efficiency metrics.

The research demonstrates that even when model architecture and parameters are inaccessible, VLMs remain vulnerable to efficiency attacks that can increase response costs by over 100% while maintaining comparable output quality. This represents a critical security concern for cloud-hosted VLMs where computational resources directly translate to operational costs. The work establishes a new benchmark for efficiency robustness evaluation and highlights the need for defensive mechanisms specifically designed for black-box efficiency threats.

## Method Summary
VLMInferSlow employs zero-order optimization to generate adversarial perturbations without requiring gradient information about the target VLM. The method uses Natural Evolution Strategies (NES) to estimate gradients through sampling perturbations around the current solution, enabling black-box optimization. The attack is guided by three adversarial objectives designed to maximize sequence length, response latency, and energy consumption while maintaining output quality through a content similarity constraint based on BLEU score.

The optimization process iteratively generates perturbations by sampling multiple candidate vectors, evaluating their impact on efficiency metrics through API queries, and updating the perturbation direction based on aggregated feedback. The method includes a hard constraint ensuring adversarial examples remain imperceptible through L2 distance and feature dissimilarity thresholds. This approach allows efficient exploration of the perturbation space while maintaining the black-box requirement of having no internal model knowledge.

## Key Results
- Achieved up to 128.47% increase in sequence length for MS-COCO dataset with T2V2 model
- Increased response latency by up to 105.56% and energy consumption by up to 115.19% on LLaVA-1.5
- Generated adversarial examples with minimal perceptual difference (L2 distance: 0.0065, feature dissimilarity: 0.0173)
- Significantly outperformed natural corruptions and white-box attacks transferred to black-box scenarios

## Why This Works (Mechanism)
VLMInferSlow exploits the computational dependencies within VLMs by targeting the visual encoder and attention mechanisms. The adversarial perturbations cause the visual encoder to produce feature maps that require more tokens for processing, leading to longer sequences. These longer sequences then propagate through the model, increasing computational requirements at each processing stage. The method specifically targets the cross-attention mechanisms where visual features interact with language tokens, causing the model to attend to more regions of the image and generate more descriptive text.

The zero-order optimization approach is particularly effective because it can navigate the high-dimensional perturbation space without gradient information. By sampling multiple perturbation directions and aggregating their effects, the method efficiently identifies directions that maximize efficiency degradation while maintaining output quality. The fine-grained adversarial objectives are specifically designed to exploit the model's computational bottlenecks, focusing on the most resource-intensive components like the visual encoder and attention mechanisms.

## Foundational Learning
- Zero-order optimization (why needed: enables black-box attacks without gradients; quick check: can optimize with only function evaluations)
- Natural Evolution Strategies (why needed: provides gradient estimation through sampling; quick check: maintains convergence in high dimensions)
- Adversarial objectives for efficiency (why needed: shifts focus from accuracy to computational cost; quick check: increases sequence length without degrading BLEU)
- Black-box threat model (why needed: reflects real-world API deployment scenarios; quick check: assumes no model architecture access)
- Computational cost metrics (why needed: quantifies real-world impact of attacks; quick check: measures sequence length, latency, energy)

## Architecture Onboarding

Component Map: Input Image -> Visual Encoder -> Cross-Attention -> Language Decoder -> Output Sequence

Critical Path: The attack targets the Visual Encoder â†’ Cross-Attention path, where perturbations in visual features cause cascading increases in token generation and processing.

Design Tradeoffs: The method balances between perturbation strength (for efficiency impact) and perceptual similarity (for stealth), requiring careful tuning of L2 and feature dissimilarity constraints.

Failure Signatures: If constraints are too loose, generated images become visibly distorted; if too tight, the attack fails to achieve significant efficiency degradation.

First Experiments:
1. Test baseline efficiency metrics on clean inputs across all four VLMs
2. Apply VLMInferSlow to a single model with minimal constraints to observe attack feasibility
3. Measure perceptual similarity of generated adversarial examples using human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What robust defense algorithms can effectively mitigate black-box efficiency attacks on VLMs without degrading performance?
- Basis in paper: [explicit] The authors state they have "limited exploration of defense strategies" and hope "future research will propose more robust algorithms," noting that Quantization proved ineffective.
- Why unresolved: Current defenses like Quantization fail to stop the efficiency degradation, and the black-box setting restricts access to internal model parameters often needed for traditional hardening.
- What evidence would resolve it: A defense mechanism that successfully normalizes sequence length or latency for adversarial inputs while maintaining caption accuracy (BLEU scores) on standard inputs.

### Open Question 2
- Question: Can zero-order optimization strategies be refined to reduce the high query cost required for black-box efficiency attacks?
- Basis in paper: [explicit] The "Limitations" section notes that the "VLMInferSlow attack requires more optimization iterations compared with white-box approaches," restricting effectiveness under limited request quotas.
- Why unresolved: The reliance on derivative-free estimation (NES) requires sampling many perturbations per step, making it inherently query-intensive compared to gradient-based methods.
- What evidence would resolve it: A modified search strategy that achieves significant computational cost increases (e.g., >50% I-length) using strictly fewer API calls than the current implementation.

### Open Question 3
- Question: Is the dispersion of visual attention maps the primary causal mechanism driving longer sequence generation in attacked VLMs?
- Basis in paper: [inferred] The paper observes via GradCAM that "attention maps are dispersed across the entire image" for adversarial examples, suggesting this "may be attributed" to the longer sequences.
- Why unresolved: The analysis relies on correlation through visualization; it does not isolate dispersed attention as the sole cause or rule out other latent space features triggering verbosity.
- What evidence would resolve it: Experiments demonstrating that manually dispersing attention maps (without noise) reproduces the verbosity, or that constraining attention (even with noise) prevents the efficiency drop.

## Limitations
- Zero-order optimization requires significantly more API queries compared to white-box approaches, limiting practical deployment
- Attack generation introduces computational overhead that could affect real-time attack scenarios
- Evaluation focuses exclusively on single-image inputs, leaving multi-image and video scenarios unexplored
- Effectiveness may be constrained by API rate limiting or input size caps in real-world deployments

## Confidence
- High: Measured increases in sequence length, latency, and energy consumption under controlled black-box conditions
- Medium: Generated perturbations are imperceptible to humans based on perceptual metrics
- Low: Claim that white-box methods transfer poorly to black-box settings based on limited baseline comparisons

## Next Checks
1. Test attack transferability across different API endpoints of the same VLM service to assess consistency of efficiency impacts
2. Evaluate perturbation detectability using automated image forensics tools in addition to perceptual metrics
3. Measure actual cloud compute resource consumption (CPU/GPU utilization, memory usage) rather than relying solely on sequence length as a proxy for energy cost