---
ver: rpa2
title: Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs
arxiv_id: '2601.15429'
source_url: https://arxiv.org/abs/2601.15429
tags:
- probe
- knowledge
- t2dm
- diabetes
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines whether domain-specific knowledge graphs (KGs)
  can improve Retrieval-Augmented Generation (RAG) for healthcare LLMs. Three PubMed-derived
  KGs are constructed: one for Type 2 Diabetes Mellitus (T2DM), one for Alzheimer''s
  disease, and one combining both.'
---

# Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs

## Quick Facts
- arXiv ID: 2601.15429
- Source URL: https://arxiv.org/abs/2601.15429
- Reference count: 30
- Primary result: Precision-first, scope-matched KG-RAG yields consistent gains, while indiscriminate graph unions often introduce distractors that reduce accuracy.

## Executive Summary
This paper evaluates whether domain-specific knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare LLMs. Three PubMed-derived KGs are constructed for Type 2 Diabetes Mellitus, Alzheimer's disease, and their combination. The study tests single-hop and multi-hop questions across seven instruction-tuned LLMs using six retrieval configurations and three decoding temperatures. Results show that carefully scoped KG-RAG improves performance, especially for smaller models, while larger models often match or exceed KG-RAG with a No-RAG baseline. The findings emphasize the importance of retrieval scope alignment and suggest that indiscriminate graph unions can harm accuracy through distractor introduction.

## Method Summary
The study constructs three PubMed-derived KGs using the CoDe-KG pipeline with TF-IDF abstract ranking and causal relation filtering. Probe questions test single-hop, multi-hop, and fill-in-the-blank reasoning on AD-T2DM relationships. Seven instruction-tuned LLMs are evaluated across six retrieval configurations (including No-RAG baseline) and three temperatures. Performance is measured using macro-F1, micro-F1, and accuracy, with statistical significance assessed via Welch's t-test and Holm-Bonferroni correction.

## Key Results
- Scope-matched retrieval (e.g., using disease-specific KGs) yields more consistent accuracy gains than broad graph unions
- Larger models frequently match or exceed KG-RAG performance on single-hop factual queries due to strong parametric priors
- Higher decoding temperatures generally harm factual QA performance with minimal upside

## Why This Works (Mechanism)

### Mechanism 1: Scope-Matched Retrieval
- **Claim**: Precision-first, scope-matched retrieval produces more consistent accuracy gains than breadth-first graph unions.
- **Mechanism**: When retrieval source boundaries align with query information needs, models receive higher-density relevant evidence with fewer off-target triples.
- **Core assumption**: Retrieval stage lacks learned reranker capable of suppressing near-miss evidence.
- **Evidence**: "precision-first, scope-matched retrieval (notably G₂) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy."
- **Break condition**: Adding a powerful neural reranker could reduce dependence on upstream scope curation.

### Mechanism 2: Parametric Knowledge Tradeoff
- **Claim**: Larger instruction-tuned LLMs encode stronger biomedical priors, reducing marginal benefit from external retrieval on single-hop factual queries.
- **Mechanism**: Models with more parameters internalize frequent biomedical associations during pretraining and alignment, making retrieved context provide diminishing returns.
- **Core assumption**: Probe questions largely overlap with well-represented biomedical knowledge in training distribution.
- **Evidence**: "Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors."
- **Break condition**: For questions requiring up-to-date literature or rare relationships, even large models should show stronger RAG dependence.

### Mechanism 3: Temperature Effects on Factual QA
- **Claim**: Higher decoding temperatures generally harm factual QA performance in healthcare RAG, with minimal upside.
- **Mechanism**: Elevated temperature increases sampling variance, promoting exploration of lower-probability tokens that amplify risk of selecting incorrect completions.
- **Core assumption**: Correct answers are concentrated in high-probability regions aligned with retrieved evidence.
- **Evidence**: "Temperature plays a secondary role; higher values rarely help" and temperature increases from 0→0.5 reduced macro-F1 in 52 cases across all configurations.
- **Break condition**: In generative tasks like drafting differential diagnoses, moderate temperature may aid diversity without sacrificing grounding.

## Foundational Learning

- **Concept**: Knowledge Graph triple structure (subject–relation–object)
  - **Why needed**: The paper constructs KGs from PubMed abstracts using (E₁, R, E₂) triples, and retrieval operates on these structured units.
  - **Quick check**: Given the triple ("insulin resistance", "is associated with", "neuroinflammation"), what query would retrieve it, and what distractor triple might also match?

- **Concept**: Single-hop vs. multi-hop reasoning in KGs
  - **Why needed**: Probe questions test both direct edges (single-hop) and two-step causal chains (multi-hop); understanding adjacency and reachability is essential.
  - **Quick check**: If A→B and B→C are edges, what is the 2-hop neighbor set P₂(C), and why might a model confuse it with P₁(C)?

- **Concept**: Parametric vs. non-parametric knowledge in LLMs
  - **Why needed**: The study hinges on the tradeoff between what models already know (parametric priors) and what they must retrieve (non-parametric augmentation).
  - **Quick check**: A large model answers correctly without RAG; how would you test whether this is due to strong priors vs. memorized training data?

## Architecture Onboarding

- **Component map**: Abstract selection → triple extraction with relation filtering → canonicalization → probe construction from target subgraph → retrieval configuration selection → zero-shot prompting → metric aggregation and significance testing.

- **Critical path**: PubMed abstract curation → CoDe-KG pipeline with Qwen-32B coreference → causal relation filtering → entity canonicalization → probe generation from G₃ (Probe 1) and G₁∩G₂ (Probe 2) → retrieval configuration selection → zero-shot prompting → metric aggregation with Welch's t-test.

- **Design tradeoffs**:
  - Narrow vs. broad KG: G₂ (AD-only) yields higher precision for intersection queries; G₃ (merged) offers broader coverage but introduces distractors
  - Model size: Smaller models benefit more from RAG but are more vulnerable to noisy retrieval; larger models may be better served by minimal or highly filtered context
  - Temperature: T=0 maximizes stability; T>0 rarely helps in factual QA but may be useful for generative tasks

- **Failure signatures**:
  - Performance drops when retrieval scope exceeds query scope (e.g., G₁+G₂+G₃ on Probe 2 intersection questions)
  - Larger models showing significant declines with narrow graphs (e.g., Llama-3.3-70B with G₁), indicating distraction from strong priors
  - Directionality flips and two-hop chain errors in multi-hop questions (27% and 24% of Probe 2 errors respectively)

- **First 3 experiments**:
  1. Replicate the G₂ vs. G₃ comparison on a new disease pair (e.g., hypertension + chronic kidney disease) to test whether scope-match effects generalize
  2. Add a neural reranker to filter retrieved triples by query relevance; measure whether this narrows the performance gap between narrow and broad graphs
  3. Stratify probe questions by expected parametric coverage (common vs. rare knowledge) and compare RAG benefit magnitude across strata for small vs. large models

## Open Questions the Paper Calls Out
- Can dynamic graph selection algorithms conditioned on query intent automate the optimal choice between disease-specific and merged knowledge graphs?
- Can advanced reranking strategies effectively mitigate the distractors introduced by indiscriminate graph unions?
- Do the performance gains of scope-matched KG-RAG generalize to medical domains outside of metabolic and neurodegenerative diseases?
- Does the use of domain-specific KGs improve model calibration and safety regarding clinical harm potential compared to No-RAG baselines?

## Limitations
- Findings might not generalize to other medical conditions, specialties, or multilingual environments
- Standard metrics fail to reflect calibration, factual grounding to primary sources, and clinical harm potential
- The study did not explore advanced retrieval strategies like hybrid lexical-neural retrieval or learning-to-rank rerankers

## Confidence
- **High confidence**: Scope-matched retrieval improves performance over broad graph unions
- **Medium confidence**: Larger models benefit less from RAG due to strong parametric priors
- **Medium confidence**: Temperature has minimal impact on factual QA performance

## Next Checks
1. Replicate the G₂ vs. G₃ comparison on a new disease pair (e.g., hypertension + chronic kidney disease) to test whether scope-match effects generalize
2. Add a neural reranker to filter retrieved triples by query relevance; measure whether this narrows the performance gap between narrow and broad graphs
3. Stratify probe questions by expected parametric coverage (common vs. rare knowledge) and compare RAG benefit magnitude across strata for small vs. large models