---
ver: rpa2
title: "M\xFCntz-Sz\xE1sz Networks: Neural Architectures with Learnable Power-Law\
  \ Bases"
arxiv_id: '2512.22222'
source_url: https://arxiv.org/abs/2512.22222
tags:
- exponents
- functions
- approximation
- singular
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limitations of standard neural network\
  \ architectures (ReLU, tanh, sigmoid) in approximating functions with singular or\
  \ fractional power behavior, which are common in physics. The proposed solution,\
  \ M\xFCntz-Sz\xE1sz Networks (MSN), replaces fixed activation functions with learnable\
  \ fractional power bases inspired by classical approximation theory."
---

# Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases

## Quick Facts
- arXiv ID: 2512.22222
- Source URL: https://arxiv.org/abs/2512.22222
- Reference count: 20
- This paper addresses the limitations of standard neural network architectures (ReLU, tanh, sigmoid) in approximating functions with singular or fractional power behavior, which are common in physics. The proposed solution, Müntz-Szász Networks (MSN), replaces fixed activation functions with learnable fractional power bases inspired by classical approximation theory.

## Executive Summary
This paper introduces Müntz-Szász Networks (MSN), a neural architecture that learns fractional power bases to better approximate functions with singular or fractional power behavior common in physics. MSN replaces standard fixed activations with learnable exponents that can adapt to the target function's structure. The authors prove that MSN inherits universal approximation from the Müntz-Szász theorem and establish novel approximation rates showing MSN achieves O(|μ-α|²) error for functions of the form |x|^α with a single learned exponent, compared to O(ε⁻¹/α) neurons for standard MLPs. Experiments demonstrate MSN achieves 5-8× lower error than MLPs with 10× fewer parameters on supervised regression with singular functions, and 3-6× improvement on physics-informed neural network benchmarks including singular ODEs and stiff boundary-layer problems, while learning interpretable exponents that match known solution structure.

## Method Summary
Müntz-Szász Networks replace standard fixed activation functions with learnable fractional power bases. Each edge in an MSN computes a sum of power functions with learned exponents: φ(x) = Σa_k|x|^μ_k + Σb_k·sign(x)|x|^λ_k, where exponents μ_k and λ_k are learnable parameters. The network structure consists of MSN layers (sum of Müntz edges per input-output pair plus bias) stacked with tanh activations between layers. Training uses a two-time-scale optimization approach where exponents are updated with slower learning rates (0.02× base LR) and gradients are clipped to prevent instability. A Müntz regularizer encourages exponent diversity by maximizing Σ1/μ_k, and L1 penalties on coefficients promote sparsity.

## Key Results
- MSN achieves O(|μ-α|²) error for functions of the form |x|^α with a single learned exponent, compared to O(ε⁻¹/α) neurons for standard MLPs
- MSN achieves 5-8× lower error than MLPs with 10× fewer parameters on supervised regression with singular functions
- MSN achieves 3-6× improvement on physics-informed neural network benchmarks including singular ODE and stiff boundary-layer problems
- MSN learns interpretable exponents that match the known solution structure

## Why This Works (Mechanism)

### Mechanism 1: Exponent-Error Coupling
- **Claim:** When MSN learns an exponent μ close to the true power α in a target function |x|^α, approximation error scales as O(|μ-α|²).
- **Mechanism:** The L² projection of x^α onto span{x^μ} has closed-form residual (α-μ)²/[(2α+1)(α+μ+1)²]. Gradient descent on μ minimizes this residual, driving μ → α.
- **Core assumption:** The optimization landscape admits a path from initialization to near-optimal μ without trapping in local minima.
- **Evidence anchors:** [abstract] "for functions of the form |x|^α, MSN achieves error O(|μ-α|²) with a single learned exponent"; [Theorem 4, Section 4.2] Explicit formula: squared error = (α-μ)²/[(2α+1)(α+μ+1)²]; [corpus] Limited direct validation; related work "Discovering Scaling Exponents" extends this to PINN settings but does not independently verify the O(δ²) rate.
- **Break condition:** If target function lacks dominant power-law structure, exponent learning provides no advantage; standard MLP may outperform (observed in "sparse polynomial" control task).

### Mechanism 2: Müntz Divergence as Basis Completeness Pressure
- **Claim:** The Müntz regularizer R_Müntz = ReLU(C - Σ1/μ_k) encourages exponent configurations that would satisfy the classical Müntz density condition asymptotically.
- **Mechanism:** Maximizing D = Σ1/μ_k pushes some exponents toward small values and encourages diversity (by convexity of 1/μ). This maintains basis expressivity as network depth/width increases.
- **Core assumption:** Finite-K exponent sets benefit from configurations that approximate infinite-Müntz-system behavior.
- **Evidence anchors:** [Definition 6-7, Section 3.3] Regularizer formula and stated effects; [Table 1 ablation] Removing Müntz regularizer degrades √x performance by 1.8×; [corpus] No independent validation of this specific regularizer; classical Müntz-Galerkin methods use related principles but in non-learning contexts.
- **Break condition:** If C is set too high, regularization forces exponents too small, causing numerical instability; if too low, regularizer is inactive.

### Mechanism 3: Two-Time-Scale Stabilization
- **Claim:** Freezing exponents during warmup and using slower learning rates (0.02×) for exponents vs. coefficients prevents premature commitment to suboptimal power bases.
- **Mechanism:** Coefficients adapt rapidly to current exponent configuration; exponents evolve slowly once coefficients provide meaningful gradient signal. Gradient clipping (δ ∈ [0.03, 0.1]) prevents large exponent jumps.
- **Core assumption:** Coefficients can find reasonable configuration with fixed initial exponents.
- **Evidence anchors:** [Algorithm 1, Section 3.4] Explicit warmup and two-time-scale update rules; [Figure 14] Gradient diagnostics show stable training with these settings; [corpus] No corpus validation; this is standard practice in learnable-activation literature.
- **Break condition:** If warmup is insufficient or exponent LR too high, training diverges (observed at ε = 0.01 in boundary-layer BVP).

## Foundational Learning

- **Concept:** Müntz-Szász Theorem
  - **Why needed here:** Provides theoretical grounding that power functions with diverse exponents form dense bases in C[0,1] under the divergence condition.
  - **Quick check question:** Given exponents {1, 2, 4, 8, 16, ...}, does Σ1/λ_k diverge? (Answer: No — geometric series converges, so span cannot approximate x.)

- **Concept:** Physics-Informed Neural Networks (PINNs)
  - **Why needed here:** MSN's primary application domain; understanding the PDE loss L_PDE and BC loss L_BC is essential for reproducing experiments.
  - **Quick check question:** In a PINN solving u'' = f(x), how is the PDE residual computed? (Answer: Via automatic differentiation of network output.)

- **Concept:** L² Projection onto Non-orthogonal Bases
  - **Why needed here:** Theorem 4's error formula derives from projection onto span{x^μ_1, ..., x^μ_K} with Gram matrix G_jk = 1/(μ_j + μ_k + 1).
  - **Quick check question:** For projecting f onto span{φ_1, φ_2}, what linear system yields optimal coefficients? (Answer: Ga = b where G_ij = ⟨φ_i, φ_j⟩, b_i = ⟨f, φ_i⟩.)

## Architecture Onboarding

- **Component map:** Müntz Edge -> MSN Layer -> Full Network (L_L ∘ tanh ∘ L_{L-1} ∘ tanh ∘ ... ∘ L_1)
- **Critical path:**
  1. Initialize raw exponent parameters r_e, r_o randomly
  2. Apply bounded map to get μ, λ ∈ (ε, p_max - ε)
  3. Forward pass: each edge evaluates power functions via exp(μ·log|x|)
  4. Compute task loss + Müntz regularizer + L1 penalty
  5. Backward: clip exponent gradients, apply two-time-scale updates

- **Design tradeoffs:**
  - **Exponent sharing (layer-wise vs. edge-wise):** Sharing reduces parameters from O(d_in·d_out·K) to O(K) but may limit expressivity
  - **p_max selection:** Higher values allow more exponents but risk overflow; paper uses 2.0–4.0 for supervised, 2.0–3.0 for PINNs
  - **K_e, K_o (exponent counts):** Paper uses 6 each; ablation not reported

- **Failure signatures:**
  - **NaN during training:** Exponent too large or input near zero without numerical guards
  - **No improvement over MLP:** Target function lacks power-law structure (smooth polynomials)
  - **Exponent collapse:** All μ_k converge to same value; increase Müntz regularizer weight β_1

- **First 3 experiments:**
  1. **Supervised regression on f(x) = √x:** Use K_e = K_o = 6, p_max = 2.0, no warmup. Verify learned μ concentrates near 0.5.
  2. **Ablation without Müntz regularizer:** Set β_1 = 0. Expect degraded performance on singular targets.
  3. **PINN on u' = 1/(2√x), u(0) = 0:** Use warmup 500–1500 steps, exponent LR = 0.02η. Compare RMSE near x = 0 vs. MLP.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the parameter efficiency and approximation accuracy of MSN scale when extended to multi-dimensional domains using tensor products or radial bases?
- **Basis in paper:** [explicit] The authors state, "Current experiments are 1D. Extension to higher dimensions via tensor products or radial bases is ongoing work" (Section 7).
- **Why unresolved:** All theoretical rates and empirical benchmarks in the paper are restricted to 1D functions and 1D ODEs/PDEs.
- **What evidence would resolve it:** Empirical evaluation of MSN on 2D or 3D PDEs (e.g., corner singularities in elliptic problems) demonstrating error scaling and parameter efficiency relative to MLPs.

### Open Question 2
- **Question:** Can MSN be integrated with adaptive loss weighting or curriculum learning to overcome the failure modes observed in extremely stiff boundary-layer problems?
- **Basis in paper:** [inferred] The paper notes that at $\epsilon=0.01$ in boundary-layer experiments, "all methods failed" (Section 5.4) and suggests "curriculum learning or specialized architectures may help" (Section 7).
- **Why unresolved:** The architectural inductive bias of MSN alone was insufficient to solve the stiffest test case, indicating that optimization challenges persist despite the improved representation power.
- **What evidence would resolve it:** Experiments combining MSN with techniques like learning rate annealing or causal training on the $\epsilon=0.01$ benchmark to see if convergence is achieved.

### Open Question 3
- **Question:** Does the learnable power-law structure of MSN provide efficiency gains for fractional PDEs (fPINNs) where singular integral kernels are present?
- **Basis in paper:** [explicit] The Conclusion lists "application to fractional PDEs" as a specific direction for future work (Section 8).
- **Why unresolved:** While MSN handles singular power-law solutions well, fractional PDEs involve non-local operators which present different approximation challenges than the local singularities tested in the paper.
- **What evidence would resolve it:** Comparative benchmarks on fractional diffusion equations showing MSN convergence rates versus standard PINN discretizations.

### Open Question 4
- **Question:** At what scale does the computational overhead of exp-log evaluation for fractional powers negate the parameter reduction benefits of MSN?
- **Basis in paper:** [inferred] The authors note the "exp-log evaluation, adding overhead compared to ReLU" in Section 7, but argue the "dramatic reduction in required neurons often compensates" without quantifying the trade-off.
- **Why unresolved:** The paper focuses on error rates and parameter counts but lacks a systematic wall-clock time analysis comparing MSN and MLPs to reach equivalent error thresholds.
- **What evidence would resolve it:** A latency vs. accuracy analysis on large-scale problems to determine the crossover point where MSN becomes computationally more expensive than an over-parameterized ReLU network.

## Limitations
- Theoretical error bounds for MSN on general function classes beyond simple power laws remain underdeveloped
- Limited ablation studies on architectural hyperparameters (K_e, K_o, layer width, depth)
- Computational overhead of exponent optimization may offset parameter efficiency gains in some settings

## Confidence
- **High**: Universal approximation property (Theorem 3), O(|μ-α|²) error formula for single exponent (Theorem 4)
- **Medium**: Empirical performance gains on PINN benchmarks, parameter efficiency claims
- **Low**: Generalization of error bounds to arbitrary continuous functions, stability of exponent learning across diverse initializations

## Next Checks
1. **Robustness testing**: Train MSN across 10+ random seeds on the singular ODE task to assess variance in learned exponents and final error
2. **Function class analysis**: Test MSN on functions with multiple power-law regimes (e.g., |x|^0.3 + |x-0.5|^0.7) to evaluate multi-exponent learning capability
3. **Comparison with learned activations**: Benchmark against NAS-based activation search methods on identical PINN problems to isolate the benefit of power-law bases versus general learnable activations