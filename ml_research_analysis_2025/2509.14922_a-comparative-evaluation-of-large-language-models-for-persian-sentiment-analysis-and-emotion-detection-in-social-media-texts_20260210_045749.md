---
ver: rpa2
title: A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis
  and Emotion Detection in Social Media Texts
arxiv_id: '2509.14922'
source_url: https://arxiv.org/abs/2509.14922
tags:
- sentiment
- emotion
- detection
- performance
- persian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four Large Language Models (LLMs)\u2014Claude\
  \ 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o\u2014for sentiment analysis\
  \ and emotion detection in Persian social media texts. Using balanced datasets of\
  \ 900 texts for sentiment (positive, negative, neutral) and 1,800 texts for emotion\
  \ (anger, fear, happiness, hate, sadness, surprise), the models achieved macro average\
  \ F1-scores between 0.7950 and 0.8079 for sentiment and 0.7379 to 0.8079 for emotion\
  \ detection."
---

# A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts

## Quick Facts
- **arXiv ID**: 2509.14922
- **Source URL**: https://arxiv.org/abs/2509.14922
- **Reference count**: 3
- **Key outcome**: Zero-shot LLM evaluation of sentiment and emotion detection in Persian social media texts shows GPT-4o achieves highest accuracy (80.67% sentiment, 80.94% emotion) while Gemini 2.0 Flash offers best cost-efficiency

## Executive Summary
This study evaluates four Large Language Models—Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o—for Persian sentiment analysis and emotion detection in social media texts. Using zero-shot classification without fine-tuning, the models achieved macro average F1-scores between 0.7950-0.8079 for sentiment and 0.7379-0.8079 for emotion detection. GPT-4o demonstrated the highest accuracy at 80.67% for sentiment and 80.94% for emotion detection, while Gemini 2.0 Flash emerged as the most cost-efficient option. All models performed better on sentiment than emotion detection, with surprise detection being particularly challenging. The study identifies systematic misclassification patterns, particularly confusion between neutral and positive/negative sentiments, and semantic overlap among surprise, anger, and sadness emotions.

## Method Summary
The study evaluated four LLMs (Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o) using zero-shot classification on balanced Persian datasets. Sentiment analysis used 900 texts (300 each of positive, negative, neutral) from Kiyavash et al., while emotion detection used 1,800 texts (300 each of anger, fear, happiness, hate, sadness, surprise) from SaremMood. Models received structured prompts requesting JSON output with confidence scores and sentiment/emotion labels. Temperature was set to 0 for deterministic results, and texts were processed in batches of 20 for cost efficiency. Performance was measured using macro average F1-score, accuracy, and confusion matrices to identify systematic error patterns.

## Key Results
- GPT-4o achieved highest accuracy at 80.67% for sentiment analysis and 80.94% for emotion detection
- All models showed better performance on sentiment analysis (F1: 0.7950-0.8079) than emotion detection (F1: 0.7379-0.8079)
- Gemini 2.0 Flash provided the best cost-efficiency ratio despite slightly lower accuracy
- Surprise detection was the most challenging emotion across all models, with DeepSeek achieving only 0.37 recall
- Systematic confusion patterns emerged: negative↔neutral (182 instances), surprise↔anger (215 instances), and hate↔anger (152 instances)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Zero-shot LLM classification achieves acceptable Persian sentiment/emotion performance through cross-lingual transfer
- **Mechanism**: Pre-trained models leverage multilingual representations learned during training to classify Persian text without task-specific fine-tuning. The models map Persian inputs to semantic spaces where sentiment/emotion boundaries already exist from English-dominant training.
- **Core assumption**: Persian emotional expressions share sufficient semantic structure with training languages for transfer to occur.
- **Evidence anchors**:
  - [abstract] "all models reach an acceptable level of performance" in zero-shot setting
  - [section 2.1] "LLMs generally exhibit strong zero-shot capabilities without any need for prior training" on classification tasks
  - [corpus] Cross-lingual few-shot learning paper confirms transfer learning effectiveness for Persian sentiment (arxiv:2507.11634)
- **Break condition**: Performance degrades significantly when Persian expressions use culturally-specific idioms or orthographic variations absent from training data.

### Mechanism 2
- **Claim**: Emotion detection underperforms sentiment analysis due to increased category granularity and semantic overlap
- **Mechanism**: Classification difficulty scales with label density—three sentiment classes (positive/negative/neutral) produce clearer decision boundaries than six emotion classes (anger, fear, happiness, hate, sadness, surprise). Smaller inter-class distances in emotion space increase confusion.
- **Core assumption**: Emotion categories share overlapping linguistic markers in Persian that models cannot disambiguate without context.
- **Evidence anchors**:
  - [abstract] "emotion detection task is more challenging for all models compared to the sentiment analysis task"
  - [section 5.1.1] "differentiating among six emotion categories is basically more challenging than separating three sentiment categories" with 7 percentage point gap vs 1.2
  - [section 5.1.3] Surprise-anger confusion (215 instances) and hate-anger confusion (152 instances) show semantic overlap
  - [corpus] Limited corpus evidence on Persian-specific emotion overlap; requires further investigation
- **Break condition**: Models would require additional contextual signals (conversation history, author metadata) or fine-tuning to resolve ambiguous emotion expressions.

### Mechanism 3
- **Claim**: Prompt engineering with structured output constraints reduces hallucination and improves classification reliability
- **Mechanism**: Explicit JSON format requirements and constrained label sets in prompts reduce output space variance. Temperature=0 eliminates sampling randomness, enabling deterministic comparisons across models.
- **Core assumption**: Models follow instruction constraints reliably even when input texts contain ambiguous or conflicting signals.
- **Evidence anchors**:
  - [section 3.4.2] "early prompt versions produced hallucinated outputs" requiring iterative refinement to enforce strict label usage
  - [section 3.4.5] Temperature set to 0 for all models "so that evaluations could be deterministic and comparable"
  - [corpus] No direct corpus evidence on prompt engineering for Persian; transfer from English prompt optimization assumed
- **Break condition**: Some hallucinations still occurred (Claude outputting "mixed" and "disgust"; Gemini outputting "shame" and "null") despite constraints.

## Foundational Learning
- **Concept**: Zero-shot vs Few-shot Classification
  - **Why needed here**: Paper evaluates models without Persian-specific training; understanding this distinction explains why performance varies and where fine-tuning would help.
  - **Quick check question**: Can you explain why DeepSeek's surprise detection recall (0.37) might improve with few-shot examples?

- **Concept**: Precision-Recall Tradeoff in Multi-class Settings
  - **Why needed here**: Models show different precision/recall profiles (Claude: 0.93 hate precision, 0.70 hate recall); selecting models requires understanding which metric matters for your use case.
  - **Quick check question**: For a content moderation system flagging hate speech, would you prioritize precision or recall, and why?

- **Concept**: Confusion Matrix Analysis for Error Pattern Discovery
  - **Why needed here**: Paper's primary diagnostic tool for identifying systematic failures (surprise→anger, negative→neutral); essential for targeted improvement strategies.
  - **Quick check question**: What does the consistent negative→neutral confusion (182 instances across models) suggest about Persian linguistic features?

## Architecture Onboarding
- **Component map**: Input Text → Preprocessing (length filtering, deduplication) → LLM API Call → Structured JSON Output → Performance Metrics
- **Critical path**: Text preprocessing and batch API calls represent the main bottlenecks for large-scale deployment
- **Design tradeoffs**: Zero-shot approach prioritizes generalization over task-specific accuracy, while structured prompts reduce hallucination at the cost of some flexibility
- **Failure signatures**: Systematic confusion between semantically similar categories (neutral↔positive/negative, surprise↔anger) indicates model limitations rather than random errors
- **Three first experiments**:
  1. Test few-shot learning with 5-10 Persian examples per class to measure performance improvement over zero-shot
  2. Evaluate model performance on formal Persian texts (news articles, literature) to assess domain generalization
  3. Implement context-aware prompts incorporating thread history to determine if performance improves on ambiguous texts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Would incorporating conversational, social, or temporal context improve LLM performance on Persian sentiment and emotion detection tasks?
- **Basis in paper**: [explicit] The authors state "Models were evaluated in a zero-shot setting without access to any context, and the input was only a standalone piece of text without any additional conversational, social, or temporal information that could aid in the analysis."
- **Why unresolved**: The study deliberately isolated texts to ensure controlled comparison, leaving contextual effects unexplored despite their potential relevance to real-world deployment.
- **What evidence would resolve it**: A comparative study testing the same models with and without contextual information (e.g., thread history, author metadata, timestamps) on identical texts.

### Open Question 2
- **Question**: How does individual text processing compare to batch processing in terms of accuracy and hallucination rates for Persian NLP tasks?
- **Basis in paper**: [explicit] "The use of batch processing for cost reasons may have affected the quality of responses compared to individual API calls."
- **Why unresolved**: Cost constraints led the researchers to use batch sizes of 20 texts; individual processing was not tested as a comparison condition.
- **What evidence would resolve it**: A controlled experiment comparing model outputs for the same texts processed individually versus in batches, measuring accuracy differences and hallucination frequency.

### Open Question 3
- **Question**: What linguistic features cause the consistent confusion between surprise and anger in Persian emotion detection, and can prompt engineering or fine-tuning address this?
- **Basis in paper**: [explicit] The authors identify surprise-anger confusion as the most frequent misclassification pattern (215 cases) but do not determine whether this stems from shared vocabulary, syntax, or cultural expression patterns.
- **Why unresolved**: Error analysis identified the pattern but did not investigate the underlying linguistic causes or test targeted interventions.
- **What evidence would resolve it**: Linguistic analysis of the misclassified texts to identify common features, followed by prompt modifications or fine-tuning experiments targeting these specific patterns.

## Limitations
- Data quality and representativeness concerns due to single-source datasets and aggressive text filtering (27.9% removed)
- Cross-lingual transfer assumptions unverified—cannot confirm what percentage of training data was Persian
- Cost-efficiency metrics don't account for deployment-specific factors like tokenization differences and real-world API limitations

## Confidence
- **High Confidence**: Sentiment analysis results showing consistent F1 scores between 0.7950-0.8079 across models
- **Medium Confidence**: Emotion detection performance metrics with increased category variability
- **Low Confidence**: Comparative cost-efficiency conclusions due to unaccounted deployment factors

## Next Checks
1. Benchmark against fine-tuned Persian models to quantify zero-shot performance gap
2. Test model generalization on formal Persian texts from different domains (news, literature)
3. Conduct human evaluation studies to establish baseline performance and compare error patterns