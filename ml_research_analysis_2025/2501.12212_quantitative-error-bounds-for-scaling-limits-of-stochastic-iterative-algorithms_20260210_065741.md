---
ver: rpa2
title: Quantitative Error Bounds for Scaling Limits of Stochastic Iterative Algorithms
arxiv_id: '2501.12212'
source_url: https://arxiv.org/abs/2501.12212
tags:
- stochastic
- quantitative
- bound
- scaling
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops non-asymptotic functional error bounds between
  stochastic iterative algorithms (SGD/SGLD) and their Ornstein-Uhlenbeck (OU) scaling
  limit approximations. Using a novel infinite-dimensional version of Stein's method
  with exchangeable pairs, the authors construct quantitative bounds for the approximation
  error between rescaled algorithm sample paths and discretized OU processes in the
  univariate setting.
---

# Quantitative Error Bounds for Scaling Limits of Stochastic Iterative Algorithms

## Quick Facts
- **arXiv ID:** 2501.12212
- **Source URL:** https://arxiv.org/abs/2501.12212
- **Reference count:** 8
- **Primary result:** Non-asymptotic functional error bounds between SGD/SGLD and Ornstein-Uhlenbeck scaling limits using exchangeable pairs

## Executive Summary
This paper develops quantitative non-asymptotic bounds for the approximation error between stochastic iterative algorithms (SGD/SGLD) and their Ornstein-Uhlenbeck (OU) scaling limit approximations. Using a novel infinite-dimensional version of Stein's method with exchangeable pairs, the authors construct explicit error bounds for the functional distance between rescaled algorithm sample paths and discretized OU processes. The bounds are expressed in terms of algorithm parameters (step size, batch size, temperature) and are of order √h log(1/h) in the numerical setting and √hm⁶ log(n/b)/n in the statistical setting.

## Method Summary
The authors derive functional error bounds by constructing an exchangeable pair of rescaled SG(L)D paths and applying Stein's method of exchangeable pairs. They decompose the error into exchangeability, covariance mismatch, and remainder terms. The approach involves linearizing the gradient, defining intermediate processes, and comparing against discretized OU processes. For the statistical setting, an L^p maximal inequality for time-inhomogeneous OU processes is derived to handle differences in drift and diffusion coefficients.

## Key Results
- Functional error bounds of order √h log(1/h) for the numerical setting
- Statistical setting bounds of order √hm⁶ log(n/b)/n
- Error bounds imply weak convergence under modest additional assumptions
- Bounds on iterate averages' variance are provided
- Approach extends beyond processes with independent increments

## Why This Works (Mechanism)

### Mechanism 1: Functional Stein's Method of Exchangeable Pairs
Quantitative bounds on the distance between the rescaled algorithm path (Y) and the limiting OU process (Z) are derived by constructing an exchangeable pair (Y, Y') and bounding the error in a "regression equation" compared to the target process. The paper constructs an exchangeable pair by replacing one random batch in the stochastic gradient with an independent copy, using Proposition 1 to bound |E[g(Y)] - E[g(Z)]| by decomposing the error into exchangeability, covariance mismatch, and remainder terms.

### Mechanism 2: Intermediate Process Decomposition (Linearization)
The error between the original SG(L)D process Y and the limiting OU process Z is bounded by comparing against an intermediate linearized process Ȳ and a discretized process Ž. The total error is decomposed: |Eg(Y) - Eg(Z)| ≤ |Eg(Y) - Eg(Ȳ)| + |Eg(Ȳ) - Eg(Ž)| + |Eg(Ž) - Eg(Z)|. The first term handles the non-linearity (Taylor remainder), the second is the core Stein bound, and the third handles discretization.

### Mechanism 3: L^p Maximal Inequality for OU Processes
To bound the error between two OU processes with different drift/diffusion coefficients (Statistical setting), an L^p maximal inequality is derived. The paper proves Theorem 3, providing a bound on E(sup_{s≤t} |X_s|^p) for time-inhomogeneous OU processes, allowing bounding |Eg(Z^{(n)}) - Eg(Z^{(∞)})|.

## Foundational Learning

**Concept: Stein's Method (of Exchangeable Pairs)**
- Why needed here: This is the core mathematical engine used to quantify convergence rates rather than just proving existence of limits. It converts the probability distance problem into a differential equation problem.
- Quick check question: Can you explain how constructing an "exchangeable pair" (Y, Y') helps measure distance to a target distribution Z?

**Concept: Ornstein-Uhlenbeck (OU) Processes**
- Why needed here: These are the continuous-time scaling limits (approximations) of the discrete SGD/SGLD iterates. Understanding the SDE form dZ_t = -B Z_t dt + √A dW_t is necessary to interpret the "limit" results.
- Quick check question: Why does the OU process, specifically its mean-reverting drift, naturally model the behavior of SGD near a minimum?

**Concept: Skorokhod Space D([0,1])**
- Why needed here: The paper analyzes convergence of entire sample paths (functions), not just marginal distributions. D([0,1]) is the space of cadlag functions where these paths live.
- Quick check question: Why do we need the Skorokhod topology (rather than uniform topology) to handle the convergence of discrete algorithm paths to continuous limits?

## Architecture Onboarding

**Component map:** SG(L)D iterates θ_k -> Rescaled process Y_t -> Exchangeable pair (Y, Y') -> Error decomposition (ε_exch, ε_cov, ε_rem) -> Linearized process Ȳ -> Discretized OU process Ž -> Error bounds

**Critical path:** Constructing the pair (Y, Y') (Lemma 7) -> Bounding the covariance mismatch ε_cov (Lemma 10 - "most technically involved") -> Summation into final bound

**Design tradeoffs:**
- **Univariate vs Multivariate:** The paper restricts to d=1 (univariate) to avoid non-commutativity of gradient steps (Hessian matrices) in the exchangeable pair construction
- **Discretized vs Continuous OU:** Target is a *discretized* OU process Ž to match the discrete nature of the algorithm, incurring a discretization error but simplifying the comparison

**Failure signatures:**
- **High Epochs (m):** Error scales as √(m^6 log(n/b)/n); large m degrades the bound
- **Non-commutativity:** Attempting to apply this to high-dimensional multivariate settings without solving the matrix commutativity issue will cause the proof structure to fail

**First 3 experiments:**
1. **Verify Univariate Bounds:** Implement SGD on a 1D logistic regression. Plot |Ēg(Y) - Ēg(Z)| against h and check if it respects √(h log(1/h)) scaling.
2. **Stress Test Batch Size:** In the statistical setting, vary batch size b. Verify the bound predicts b=1 minimizes error.
3. **Discretization Gap:** Compare actual SGD path against continuous OU process vs discretized Ž. Measure contribution of |Eg(Z) - Eg(Ž)| to confirm it is negligible as h→0.

## Open Questions the Paper Calls Out

**Open Question 1:** Is the m^6 dependence on the number of epochs in the statistical setting error bound optimal, or can it be improved to linear or near-linear in m? The current proof technique yields m^6 dependence, but no matching lower bound is established to prove optimality.

**Open Question 2:** Can the functional error bounds be extended to the multivariate setting (d > 1)? The main result only holds for univariate θ, and investigating the distance between Y and Z for θ ∈ R^d would require an additional lengthy and technically intricate analysis due to non-commutativity of gradient steps.

**Open Question 3:** Can similar non-asymptotic functional error bounds be derived for more sophisticated stochastic approximation algorithms (e.g., momentum methods, adaptive methods like Adam, preconditioned SGLD)? These algorithms have more complex update structures that would require extending the exchangeable pairs construction and covariance error analysis.

## Limitations
- Restricted to univariate settings due to non-commutativity of gradient steps in higher dimensions
- Error bounds depend critically on problem-specific smoothness constants that may be difficult to estimate
- Statistical setting bound's dependence on m^6 suggests potential degradation for very large epoch counts

## Confidence
- **High Confidence:** The core methodology of using exchangeable pairs for functional limits and the linear approximation error decomposition are well-established techniques with rigorous proofs
- **Medium Confidence:** The extension to discretized OU processes and numerical setting bounds follow from the established framework but require careful verification of the discretization error analysis
- **Medium Confidence:** The statistical setting bounds involve more complex moment calculations and the L^p maximal inequality for OU processes, which introduces additional technical complexity

## Next Checks
1. **Numerical Verification:** Implement the bounds for a simple 1D GLM (e.g., linear regression) and empirically verify the √(h log(1/h)) scaling for varying h and β.
2. **Statistical Setting Test:** For a small n (e.g., n=100), compute the bound's prediction for optimal batch size b, and validate through experiments whether b=1 indeed minimizes error as suggested by the bound's dependence on log(n/b).
3. **Smoothness Constant Impact:** Systematically vary the smoothness constants (e.g., by adding regularization to control L) and measure how the empirical error scales relative to the theoretical bound's predictions.