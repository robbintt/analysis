---
ver: rpa2
title: Self-Supervised Learning with Gaussian Processes
arxiv_id: '2512.09322'
source_url: https://arxiv.org/abs/2512.09322
tags:
- data
- representations
- learning
- gpssl
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Gaussian Process Self-Supervised Learning (GPSSL),
  a method for learning representations without labeled data by leveraging Gaussian
  processes. Unlike traditional self-supervised learning methods that rely on positive/negative
  pairs or data augmentation, GPSSL uses GP priors to enforce smoothness of representations,
  pulling similar observations together through kernel functions.
---

# Self-Supervised Learning with Gaussian Processes

## Quick Facts
- arXiv ID: 2512.09322
- Source URL: https://arxiv.org/abs/2512.09322
- Reference count: 14
- Primary result: GPSSL learns meaningful representations without positive pairs by enforcing smoothness via GP priors, achieving higher accuracy and better uncertainty quantification than traditional methods.

## Executive Summary
This paper introduces Gaussian Process Self-Supervised Learning (GPSSL), a method for learning representations from unlabeled data by leveraging Gaussian process priors instead of positive/negative pairs or data augmentation. GPSSL uses a GP prior to enforce smoothness of representations, pulling similar observations together through kernel functions while combining a VICReg-like loss (variance and covariance terms) to prevent collapse. The method produces a generalized Bayesian posterior over representations, enabling principled uncertainty quantification for downstream tasks.

## Method Summary
GPSSL learns representations by placing a GP prior over latent functions f_z ~ GP(0, k) where the kernel k(x_i, x_j) encodes similarity between observations. The variational inference framework approximates the posterior with inducing points, yielding a distribution over representations rather than point estimates. The loss function combines variance regularization (ℓ_var) to prevent collapse and covariance regularization (ℓ_cov) to decorrelate dimensions, replacing the need for contrastive pairs. The method outputs a conditional distribution q(z* | x*) from which moments and samples can be drawn for downstream tasks.

## Key Results
- GPSSL outperforms traditional methods like VICReg and kernel PCA on classification tasks across multiple datasets (UCI repository and spatial transcriptomics)
- GPSSL-full (sampling from posterior) provides better uncertainty quantification and lower AURC than GPSSL-mean (using point estimates)
- The probabilistic nature enables uncertainty propagation to downstream tasks, yielding more robust predictions with improved error control

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The GP prior replaces explicit positive pair construction by encoding similarity through kernel-induced covariance structure.
- **Mechanism:** A kernel function k(x_i, x_j) defines pairwise similarity in observation space. The GP prior places correlated distributions on representations, such that z_i and z_j are a priori correlated when k(x_i, x_j) is large. This pulls representations of similar inputs together without requiring augmented views or explicit pair sampling.
- **Core assumption:** A meaningful similarity kernel can be specified for the data type (e.g., RBF for continuous features, graph kernels for networks). If the kernel is misspecified, smoothness will be enforced along irrelevant axes.
- **Evidence anchors:**
  - [abstract] "The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples."
  - [Section 2.3.1] Shows that with a kernel encoding positive-pair membership, the GP prior log-joint approximates an invariance loss of the form Σ z_i^T z'_i.
  - [corpus] Weak direct evidence; neighboring papers focus on speech SSL with data augmentation pipelines, not kernel-based similarity.
- **Break condition:** If no domain-appropriate kernel exists or data is high-dimensional with no clear similarity metric, the GP prior provides no useful inductive bias.

### Mechanism 2
- **Claim:** The variance and covariance losses prevent representational collapse while encouraging informative, decorrelated embeddings.
- **Mechanism:** The variance term ℓ_var penalizes dimensions with standard deviation below a threshold γ, ensuring representations spread across the embedding space. The covariance term ℓ_cov shrinks off-diagonal covariances toward zero, decorrelating dimensions and preventing information from being concentrated in a single direction. Together they replace the need for negative pairs or stop-gradients.
- **Core assumption:** Collapse is primarily driven by low variance and high correlation; preventing these suffices to maintain informative representations.
- **Evidence anchors:**
  - [Section 2.1] "We use a VICReg-like objective function for ℓ = c_V ℓ_var(Z) + c_C ℓ_cov(Z)."
  - [Section 2.3.2] Demonstrates that in the 1D case with modified variance loss, the GPSSL posterior mode recovers the first kernel PCA component (Proposition 1).
  - [corpus] The survey on SSL for recommendation (arXiv:2404.03354) notes variance-covariance regularization as a common non-contrastive strategy.
- **Break condition:** If c_V or c_C are set too low, representations may still collapse; if set too high, optimization may become unstable or over-regularize.

### Mechanism 3
- **Claim:** Generalized variational inference yields a full posterior distribution over representations, enabling principled uncertainty propagation.
- **Mechanism:** Standard SSL produces point estimates z = f(x). GPSSL defines a generalized posterior p̃(f_z | X) ∝ p(f_z) exp{-ℓ(Z)} and approximates it via variational inference with inducing points (U_x, U_z). The result is a conditional distribution q(z* | x*) from which moments and samples can be drawn, allowing downstream tasks to integrate over representation uncertainty.
- **Core assumption:** The variational family (mean-field Gaussian over inducing points) is sufficiently expressive; the loss function is a reasonable pseudo-likelihood.
- **Evidence anchors:**
  - [Section 2.2] "We approximate the generalized Bayesian posterior distribution p̃ using generalized variational inference... The generalized ELBO becomes ELBO = -E_q(U_z)[ℓ(Z)] - KL(q(U_z) || p(U_z))."
  - [Section 3.3.1] GPSSL-full (marginalizing over representation samples) shows better uncertainty and lower AURC than GPSSL-mean (using point estimates).
  - [corpus] No direct corpus evidence; most SSL papers in the neighborhood do not address Bayesian posterents.
- **Break condition:** If the number of inducing points is too small, the variational approximation will be poor; if too large, computation becomes prohibitive (O(M³) for M inducing points).

## Foundational Learning

- **Concept: Gaussian Process Priors**
  - Why needed here: GPSSL's core inductive bias comes from the GP prior; understanding how kernels encode smoothness and how marginal/conditional distributions are computed is essential.
  - Quick check question: Given a kernel k(x, x'), can you explain why two points with high kernel similarity will have correlated posterior representations?

- **Concept: VICReg and Non-Contrastive SSL**
  - Why needed here: GPSSL inherits the variance-covariance loss from VICReg; knowing why contrastive methods need negative pairs and how non-contrastive methods avoid them clarifies GPSSL's design.
  - Quick check question: Why does VICReg not need negative pairs, and what role does the invariance loss play that GPSSL replaces?

- **Concept: Variational Inference with Inducing Points**
  - Why needed here: GPSSL uses sparse variational GPs to scale; the inducing point approximation determines both accuracy and computational cost.
  - Quick check question: What is the computational complexity of GPSSL with N training points and M inducing points, and why are inducing points necessary?

## Architecture Onboarding

- **Component map:** Input X -> Kernel module (computes K(X, X), K(X*, X)) -> Variational family q(U_z) -> Loss ℓ = c_V·ℓ_var + c_C·ℓ_cov -> ELBO optimization -> Output distribution q(z* | x*)

- **Critical path:**
  1. Choose and tune kernel (lengthscale, kernel type)
  2. Initialize inducing points (subset of data or k-means centers)
  3. Optimize ELBO with variance/covariance loss weights (c_V, c_C)
  4. For downstream tasks, either use E[q(z*)] (GPSSL-mean) or sample {z^(i)} ~ q(z*) (GPSSL-full)

- **Design tradeoffs:**
  - More inducing points → better approximation but O(M³) cost
  - Higher c_V → more spread representations but risk instability
  - RBF kernel → smooth, universally applicable but may miss structure; domain-specific kernels (e.g., graph, string) → more expressive but require expertise
  - GPSSL-full vs. GPSSL-mean → better uncertainty vs. simpler integration

- **Failure signatures:**
  - Representations collapse to constant vectors → c_V or c_C too low
  - High variance in all regions including data-dense areas → kernel lengthscale too small or inducing points poorly placed
  - OOM errors on moderate datasets → inducing points too many or kernel matrix not sparsified
  - Downstream performance no better than raw features → kernel misspecified for task-relevant structure

- **First 3 experiments:**
  1. **Sanity check on synthetic 2D data:** Generate data from concentric circles or clusters; visualize mean and std of learned representations. Confirm high uncertainty in sparse regions and structured embeddings in dense regions (replicate Figure 2).
  2. **Ablation on loss weights:** On a UCI dataset (e.g., Breast Cancer), grid search c_V ∈ {10, 25, 50, 100} and c_C ∈ {1, 5, 10} with validation-set log-likelihood. Confirm GPSSL-full outperforms GPSSL-mean and kPCA (replicate Table 1).
  3. **Inducing point scaling:** Fix dataset and vary M ∈ {50, 100, 200, 500}. Plot test ROC-AUC vs. M and training time vs. M. Identify the point of diminishing returns to set default M for new datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the computational complexity of GPSSL be reduced to handle extremely large datasets where storing and inverting the full covariance matrix is infeasible?
- **Basis in paper:** [explicit] The Discussion section states that despite using inducing points, "the computational feasibility of GPSSL is limited... It can become computationally impractical when dealing with a extremely large number of data points."
- **Why unresolved:** While inducing points are used, the authors note the method still struggles with scalability compared to standard SSL methods, limiting its application to smaller datasets.
- **What evidence would resolve it:** A scaling analysis on datasets with N > 100,000 samples using stochastic variational inference or distributed GP methods, showing competitive runtime with Neural Network-based SSL.

### Open Question 2
- **Question:** How does the choice of kernel function (beyond the standard RBF) impact the quality of representations in GPSSL for non-Euclidean data?
- **Basis in paper:** [explicit] The Discussion notes, "Extensions on different choices of the kernel in the GP model could be investigated... a carefully crafted kernel will allow one to introduce for flexibility."
- **Why unresolved:** The experiments exclusively utilize the squared exponential (RBF) kernel, leaving the potential benefits of data-driven or deep kernels unexplored.
- **What evidence would resolve it:** Experiments applying string kernels for text or graph kernels for network data, demonstrating improved performance over the RBF kernel on those modalities.

### Open Question 3
- **Question:** Can GPSSL effectively learn representations on high-dimensional image data without relying on pre-processing (e.g., PCA) or data augmentation?
- **Basis in paper:** [inferred] The authors state existing methods are favorable when "generation of positive... pairs is straightforward and highly informative" (e.g., images), and they restrict their recommendation to "tabular, graph and other datasets."
- **Why unresolved:** The paper validates the method on tabular and spatial transcriptomics data but does not demonstrate competitiveness on standard high-dimensional vision benchmarks where augmentation-based SSL dominates.
- **What evidence would resolve it:** Benchmarking GPSSL on CIFAR-10 or ImageNet using a deep kernel, comparing top-1 accuracy against SimCLR or VICReg without using data augmentations.

## Limitations
- Computational complexity is O(N²) for kernel computation and O(M³) for variational updates, limiting scalability to large datasets
- Kernel design requires domain expertise; no automated or learned kernel adaptation is provided
- Experiments are confined to small tabular datasets and semi-synthetic spatial transcriptomics, with no validation on high-dimensional or truly large-scale problems

## Confidence
- **Mechanism 1:** Medium confidence - kernel-induced smoothness is theoretically sound but empirical evidence of replacing augmentation is limited to small-scale experiments
- **Mechanism 2:** High confidence - VICReg-like regularization is well-established in SSL literature and prevents collapse through variance/covariance control
- **Mechanism 3:** Medium confidence - variational GP framework is standard, but uncertainty quantification depends on approximation quality which isn't thoroughly validated
- **Overall:** Medium confidence in core claims due to limited experimental scope and scalability concerns

## Next Checks
1. **Scaling experiment:** Run GPSSL on a medium-sized tabular dataset (e.g., Adult, Covertype) with varying inducing point counts M and measure training time, memory usage, and downstream accuracy. Determine practical M limit before performance plateaus.
2. **Kernel sensitivity:** Systematically vary kernel type (RBF, Matérn, linear) and lengthscale selection method on Breast Cancer dataset. Report downstream accuracy and representation variance to quantify kernel impact.
3. **Uncertainty calibration:** On PBMC spatial transcriptomics, compute Expected Calibration Error (ECE) and reliability diagrams for GPSSL-full vs. GPSSL-mean vs. kPCA. Verify that higher uncertainty correlates with prediction errors.