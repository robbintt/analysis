---
ver: rpa2
title: 'Benchmarking Abstractive Summarisation: A Dataset of Human-authored Summaries
  of Norwegian News Articles'
arxiv_id: '2501.07718'
source_url: https://arxiv.org/abs/2501.07718
tags:
- summaries
- news
- summary
- article
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a high-quality dataset of human-authored\
  \ summaries for Norwegian news articles, covering both official written variants\
  \ (Bokm\xE5l and Nynorsk). Each article is paired with three distinct summaries\
  \ per language variant, totaling six summaries per article."
---

# Benchmarking Abstractive Summarisation: A Dataset of Human-authored Summaries of Norwegian News Articles

## Quick Facts
- arXiv ID: 2501.07718
- Source URL: https://arxiv.org/abs/2501.07718
- Reference count: 7
- Introduces a dataset of human-authored summaries for Norwegian news articles, with three distinct summaries per language variant (Bokmål and Nynorsk) per article

## Executive Summary
This paper introduces a high-quality dataset of human-authored summaries for Norwegian news articles, covering both official written variants (Bokmål and Nynorsk). Each article is paired with three distinct summaries per language variant, totaling six summaries per article. The dataset was created by three native Norwegian annotators with journalism backgrounds, following guidelines emphasizing clarity, precision, and journalistic integrity. The summaries exhibit diversity in style and length, ranging from 100 tokens on average, with notable variations across annotators and language variants. The authors evaluate nine open-source Norwegian and multilingual LLMs using standard metrics (ROUGE-L, BERTScore) and manual human evaluation. Viking-7B and Viking-13B perform best, with ROUGE-L scores up to 33.76 and BERTScores up to 70.34, though most models perform better on Bokmål than Nynorsk. Human evaluation shows a strong preference for human-authored summaries, though Viking-13B-generated summaries are occasionally preferred. The dataset presents a challenging benchmark for abstractive summarization in Norwegian.

## Method Summary
The dataset was created by three native Norwegian annotators with journalism backgrounds, who produced three distinct summaries per article for both Bokmål and Nynorsk language variants. The annotation process followed guidelines emphasizing clarity, precision, and journalistic integrity. The authors evaluated nine open-source Norwegian and multilingual LLMs using standard metrics (ROUGE-L, BERTScore) and manual human evaluation, with Viking-7B and Viking-13B showing the best performance.

## Key Results
- Viking-7B and Viking-13B achieved the highest performance with ROUGE-L scores up to 33.76 and BERTScores up to 70.34
- Most models performed better on Bokmål than Nynorsk, highlighting the challenge of the less-resourced variant
- Human evaluation strongly preferred human-authored summaries, though Viking-13B-generated summaries were occasionally preferred

## Why This Works (Mechanism)
The dataset's effectiveness stems from its high-quality, diverse human-authored summaries created by annotators with journalism backgrounds. The dual-language coverage (Bokmål and Nynorsk) provides a comprehensive benchmark for Norwegian summarization. The combination of automated metrics and manual human evaluation offers a balanced assessment of model performance, capturing both quantitative and qualitative aspects of summary quality.

## Foundational Learning
- Abstractive summarization: Generating concise summaries that capture key information while potentially using different wording than the original text. Why needed: Essential for understanding the task the models are being evaluated on. Quick check: Verify that summaries capture main points without simply copying original text.
- ROUGE-L metric: Measures overlap of longest common subsequences between generated and reference summaries. Why needed: Provides quantitative evaluation of summary quality. Quick check: Ensure ROUGE-L scores correlate with human judgment of summary quality.
- BERTScore: Uses pre-trained language models to compute similarity between generated and reference summaries. Why needed: Captures semantic similarity beyond exact word matching. Quick check: Confirm BERTScore captures meaning even when wording differs from references.

## Architecture Onboarding
Component map: Articles -> Summaries (3 per language variant) -> Model evaluation (9 models) -> Metrics (ROUGE-L, BERTScore, Human eval)
Critical path: Data collection -> Model training/evaluation -> Metric computation -> Result analysis
Design tradeoffs: Single-source focus vs. broader journalistic style representation
Failure signatures: Poor performance on Nynorsk, high variance across annotators
First experiments:
1. Evaluate model performance on a held-out test set
2. Compare human evaluation scores across different annotators
3. Analyze correlation between automated metrics and human judgment

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on a single Norwegian news source (NRK) may limit representation of broader Norwegian journalistic styles
- Relatively small sample size (1,000 articles) could affect robustness of conclusions, particularly for Nynorsk
- Limited annotator pool of three individuals, raising questions about inter-annotator agreement and potential bias

## Confidence
- Dataset creation methodology and quality: **High** - The detailed annotation process and multiple quality checks provide strong confidence in the dataset's reliability.
- Model performance evaluation: **Medium** - While standard metrics were used, the limited sample size and single-source focus affect generalizability.
- Human evaluation results: **Medium** - The preference for human summaries is clear, but the small number of annotators and evaluation criteria may not capture all aspects of summary quality.

## Next Checks
1. Expand the dataset with articles from additional Norwegian news sources to test model performance across different journalistic styles and topics.
2. Conduct inter-annotator agreement analysis with a larger pool of annotators to validate consistency in summary creation and evaluation.
3. Test model performance on a larger sample size, particularly for Nynorsk articles, to strengthen conclusions about language variant differences.