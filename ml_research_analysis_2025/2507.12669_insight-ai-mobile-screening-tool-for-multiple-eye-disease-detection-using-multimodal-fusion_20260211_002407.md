---
ver: rpa2
title: 'InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using
  Multimodal Fusion'
arxiv_id: '2507.12669'
source_url: https://arxiv.org/abs/2507.12669
tags:
- disease
- images
- diagnosis
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present InSight, a mobile AI screening app for five
  common eye diseases using multimodal fusion of fundus images and clinical metadata.
  The app includes an image quality checker, a disease diagnosis model using MetaFusion
  for combining image and metadata features, and a DR grading model.
---

# InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion

## Quick Facts
- **arXiv ID**: 2507.12669
- **Source URL**: https://arxiv.org/abs/2507.12669
- **Reference count**: 0
- **Primary result**: Multimodal mobile AI screening app for five eye diseases using fundus images and clinical metadata

## Executive Summary
InSight is a mobile AI screening tool that detects five common eye diseases—diabetic retinopathy, glaucoma, diabetic macular edema, pathological myopia, and age-related macular degeneration—using multimodal fusion of fundus images and clinical metadata. The system combines an image quality checker, a disease diagnosis model using MetaFusion for combining image and metadata features, and a DR grading model. The multimodal pretrained model achieved 6% improvement in balanced accuracy over image-only models on BRSET and 4% on mBRSET. The image quality checker filtered low-quality images with near-100% accuracy, while the multitask model was five times more computationally efficient than separate disease-specific models.

## Method Summary
InSight is a three-stage mobile screening pipeline: (1) an image quality checker using CNN to filter low-quality images, (2) a multimodal disease diagnosis model that fuses fundus images and clinical metadata via MetaFusion, and (3) a DR grading model for severity assessment. The system uses ResNet18 backbone with MetaFusion block that computes element-wise similarity between image and metadata embeddings before applying residual corrections. Pretraining combines supervised classification loss with self-supervised reconstruction on 130K images from multiple datasets. Clinical metadata includes age, sex, diabetes diagnosis, and hypertension status. The multitask architecture predicts five diseases simultaneously with a single shared backbone.

## Key Results
- Multimodal pretrained model outperforms image-only models by 6% balanced accuracy on BRSET and 4% on mBRSET
- Image quality checker filters low-quality images with near-100% accuracy
- Multitask model achieves 5x computational efficiency compared to separate disease-specific models
- System generalizes well across smartphone and lab-captured images, maintaining robustness to varied imaging conditions

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Feature Correction via MetaFusion
Fusing clinical metadata with fundus images via learned correction terms improves diagnostic accuracy over image-only approaches. MetaFusion computes element-wise similarity between image embeddings and metadata embeddings, then applies residual-style corrections to both modalities before concatenation. This allows each modality to "inform" the other's representation. Core assumption: clinical metadata carries predictive signal that correlates with specific diseases. Evidence: 6% BA improvement over image-only models; correlation heatmap shows strong correlation between diabetes/duration and DR/DME; metadata-only model achieves 0.94 BA for DR. Break condition: If metadata has high missingness or weak disease correlation, fusion benefits diminish.

### Mechanism 2: Quality Gating Prevents Downstream Error Propagation
A binary quality classifier filtering low-quality images before diagnosis substantially improves end-to-end accuracy. CNN trained on synthetically degraded images classifies images as high/low quality, prompting retake if failed. Core assumption: Blur and illumination artifacts cause diagnostic errors, and these artifacts can be simulated during training. Evidence: Quality checker filtered low-quality images with near-100% accuracy; DR improves from 0.79 to 0.96 BA (17% absolute gain). Break condition: If augmentation artifacts don't match real-world capture issues, or if threshold is too aggressive, gating may introduce friction without accuracy gains.

### Mechanism 3: Hybrid Pretraining Reduces Domain Gap
Combining supervised classification loss with self-supervised reconstruction loss during pretraining improves feature representations over ImageNet transfer. Pretraining on 130K images from multiple datasets using cross-entropy for DR/glaucoma plus MAE-style image reconstruction. Core assumption: Domain-specific pretraining on fundus images learns more transferable features than generic ImageNet features, and hybrid loss prevents overfitting to single-task supervision. Evidence: Pretraining improves BA across all 5 diseases (largest for Glaucoma: 0.86→0.94). Break condition: If pretraining datasets have different disease prevalence than target population, transfer may be limited.

## Foundational Learning

- **Concept: Residual Connections in Neural Networks**
  - Why needed here: MetaFusion explicitly uses residual-style corrections to modify embeddings without destroying original information
  - Quick check: Why does adding a correction term (rather than replacing) help preserve modality-specific information?

- **Concept: Balanced Accuracy for Imbalanced Datasets**
  - Why needed here: BRSET is highly imbalanced (Pathological Myopia 1.6%, DME 2.5%, AMD 2.2%); standard accuracy would be misleading
  - Quick check: If a dataset has 95% negative cases, what would a "predict always negative" model achieve in standard accuracy vs. balanced accuracy?

- **Concept: Multitask Learning with Shared Representations**
  - Why needed here: InSight uses one shared backbone for 5 disease predictions, achieving 5x computational efficiency vs. separate models
  - Quick check: When might multitask learning hurt individual task performance compared to single-task models?

## Architecture Onboarding

- **Component map**: Input Image -> [Quality Checker CNN] -> (reject if low quality) -> [ResNet18 Backbone] -> Image Embedding -> [MetaFusion Block] -> [5 Parallel Linear Heads] -> DR / Glaucoma / DME / Myopia / AMD

- **Critical path**: 1. Image capture → Quality check (real-time, must be <100ms for good UX) 2. ResNet18 feature extraction (512-dim embedding) 3. MetaFusion with 4 metadata features (age, sex, diabetes, hypertension) 4. 5-way multitask prediction (joint loss = Σ CE losses)

- **Design tradeoffs**:
  | Decision | Choice | Alternative | Rationale |
  |----------|--------|-------------|-----------|
  | Backbone | ResNet18 | ResNet50/ConvNext | Mobile deployment; 5x efficiency gain |
  | Fusion | MetaFusion (intermediate) | Late fusion (concat logits) | Earlier fusion allows cross-modal correction |
  | Pretraining | Hybrid (supervised + MAE) | ImageNet only | Domain-specific features transfer better |
  | Task structure | Multitask (1 model) | 5 separate models | Comparable accuracy, 5x less compute |

- **Failure signatures**:
  - Quality checker rejects too many valid images: Check augmentation parameters; may need recalibration on target device
  - Metadata features show no improvement: Check missingness rates; if >30% missing, consider dropping feature or using more sophisticated imputation
  - Poor generalization to smartphone images: Increase mBRSET proportion in training mix
  - Specific disease underperforms: Check class prevalence; glaucoma at 19.7% has more examples than myopia (1.6%)

- **First 3 experiments**:
  1. Ablation study: Train image-only baseline vs. full model on same split; measure per-disease BA delta to isolate fusion vs. pretraining contributions
  2. Quality threshold calibration: Vary quality checker confidence threshold; plot rejection rate vs. downstream diagnostic accuracy
  3. Cross-dataset generalization: Train on BRSET only, test on mBRSET; measure BA drop to quantify domain shift between lab and smartphone captures

## Open Questions the Paper Calls Out
- How does InSight's diagnostic accuracy vary when deployed across diverse geographic and ethnic populations outside of Brazil? (Current population entirely based in Brazil)
- What is InSight's real-world performance when deployed in field screening programs with smartphone-based retinal imaging systems operated by non-specialists? (Current results based on curated datasets)
- How does missing clinical metadata affect individual disease prediction accuracy, and can robustness be improved through alternative imputation or missingness-indicating strategies? (Low age-AMD correlation likely because 1/3 of patients had age information missing)
- Can the multitask MetaFusion architecture maintain computational efficiency while scaling to detect additional eye diseases beyond the current five? (While five diseases represent leading causes, screening for additional diseases would be beneficial)

## Limitations
- Training details missing: Critical hyperparameters including learning rate, batch size, optimizer, and weight decay are unspecified
- Generalization validation gaps: Validation on completely independent populations or clinical settings is absent
- Single-device quality assessment: Quality checker trained on synthetically degraded images may not match real-world capture conditions across different smartphone cameras

## Confidence
- **High confidence**: Quality gating mechanism (near-100% accuracy, 17% BA improvement), computational efficiency gains (5x faster), saliency analysis showing clinically relevant feature focus
- **Medium confidence**: 6% BA improvement from MetaFusion fusion, pretraining benefits (supported by domain adaptation literature but details incomplete)
- **Low confidence**: Claims about clinical workflow integration and real-world deployment readiness (no user study or clinical validation presented)

## Next Checks
1. **Ablation study on BRSET**: Train and compare three models—image-only baseline, image-only with pretraining, and full multimodal model. Measure per-disease BA differences to isolate fusion contribution from pretraining effects.
2. **Cross-device quality checker calibration**: Test the quality checker on smartphone images from multiple devices not represented in mBRSET. Measure rejection rates and downstream accuracy to identify systematic biases in quality assessment.
3. **External population validation**: Deploy the trained model on an independent retinal disease dataset from a different country or clinical setting. Compare performance drop to quantify real-world generalization limits beyond the BRSET/mBRSET domain.