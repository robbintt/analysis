---
ver: rpa2
title: "A Proof of Learning Rate Transfer under $\u03BC$P"
arxiv_id: '2511.01734'
source_url: https://arxiv.org/abs/2511.01734
tags:
- learning
- rate
- transfer
- convergence
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first formal proof of learning rate transfer\
  \ in the infinite-width limit of deep neural networks. The key insight is that under\
  \ the maximal update parametrization (\u03BCP), the optimal learning rate converges\
  \ to a non-zero constant as network width increases, enabling stable hyperparameter\
  \ transfer across scales."
---

# A Proof of Learning Rate Transfer under $μ$P

## Quick Facts
- arXiv ID: 2511.01734
- Source URL: https://arxiv.org/abs/2511.01734
- Reference count: 40
- This paper provides the first formal proof of learning rate transfer in the infinite-width limit of deep neural networks under maximal update parametrization (μP).

## Executive Summary
This paper establishes the first formal proof of learning rate transfer in infinite-width deep neural networks. Under maximal update parametrization (μP), the optimal learning rate converges to a non-zero constant as network width increases, enabling stable hyperparameter transfer across scales. The proof analyzes linear multi-layer perceptrons, showing that the loss function becomes asymptotically quadratic in the learning rate with coefficients converging to deterministic limits. This contrasts with standard parametrization (SP) where the optimal learning rate converges to zero, and neural tangent parametrization (NTP) where it diverges.

## Method Summary
The paper analyzes linear MLPs with width n, examining how the optimal learning rate η evolves as n→∞. For one training step, it proves that under μP, the loss is asymptotically quadratic in η with a deterministic minimizer converging at rate O(n^(-1/2)). For multiple steps, it shows the loss remains a polynomial in η whose coefficients converge, guaranteeing convergence of the optimal learning rate provided the limiting loss has a unique minimizer. The analysis uses the Tensor Programs framework to show that polynomial coefficients converge to deterministic limits as width increases.

## Key Results
- Under μP, optimal learning rate converges to a non-zero constant as width → ∞, enabling transfer
- Under SP, optimal learning rate converges to zero as width → ∞
- Under NTP, optimal learning rate diverges as width → ∞
- For t=1 step, convergence rate is O(n^(-1/2)) for μP
- For t≥2 steps, convergence depends on uniqueness of the limiting loss minimizer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under μP, the training loss becomes asymptotically quadratic in the learning rate η at t=1, enabling the optimal learning rate to converge to a deterministic non-zero constant.
- Mechanism: The loss function L(η) can be expressed as a polynomial in η where the linear coefficient ϕ₁ converges to a deterministic constant (L/m · Σᵢ yᵢ⟨x,xᵢ⟩/d) while higher-order coefficients ϕₗ (ℓ≥2) vanish as O(n^(-(ℓ-1)/2)). This yields a well-defined minimizer η∞ = (m/L) · (y^TKy / ||Ky||²).
- Core assumption: The input Gram matrix satisfies Ky ≠ 0 (non-degenerate training data).
- Evidence anchors:
  - [abstract] "For one training step, the loss is asymptotically quadratic in the learning rate, with the linear term converging to a non-zero constant while higher-order terms vanish."
  - [Section 3.1, Theorem 1] Proves convergence rate ηₙ^(1) - η∞^(1) = O_P(n^(-1/2))
  - [corpus] Weak direct support; related work (Yang & Hu 2021, Tensor Programs IV) conjectured but did not prove this mechanism.
- Break condition: If Ky ≈ 0 (near-orthogonal or degenerate training data), the limiting loss becomes flat and the minimizer is not uniquely defined.

### Mechanism 2
- Claim: μP's specific scaling of the output layer V (variance n^(-2) vs n^(-1) in SP) is the critical parameter that enables learning rate transfer.
- Mechanism: The n^(-2) variance on V causes the initial output f⁽⁰⁾(x) to vanish as O(n^(-1)), which balances the scaling of polynomial coefficients. In contrast, SP's n^(-1) variance amplifies all coefficients by √n, forcing optimal η toward 0 to compensate.
- Core assumption: Gradient descent is used for training; the learning rate exponent c=0 for GD (differs for Adam where c=1).
- Evidence anchors:
  - [Section 2, Definition 1] Explicitly defines μP as αV=2 vs SP's αV=1
  - [Section 3.2, Theorem 2] Proves ηₙ^(1) → 0 under SP
  - [corpus] Nearby paper "Optimal Embedding Learning Rate in LLMs" confirms μP scaling importance for vocabulary embeddings but notes mixed empirical results for large-scale models.
- Break condition: If using Adam optimizer, the learning rate must be scaled as η×n^(-1) to maintain transfer properties.

### Mechanism 3
- Claim: For t≥2 training steps, the loss remains a polynomial in η with coefficients converging to deterministic limits, guaranteeing optimal η converges provided the limiting polynomial has a unique minimizer.
- Mechanism: At each step, gradients accumulate dependencies on η, increasing polynomial degree (up to 2L(L+1) at t=2). However, the "Master Theorem" from Tensor Programs ensures coefficient convergence, and the quadratic loss structure guarantees the leading coefficient is positive, bounding the minimizer away from ∞.
- Core assumption: The limiting loss L∞^(t)(η) has a unique minimizer (stated as "mild condition").
- Evidence anchors:
  - [Section 4, Theorem 3] Proves almost sure convergence of ηₙ^(t) → η∞^(t) under uniqueness assumption
  - [Section 4, Lemma 2] Shows non-linear asymptotics emerge at t=2 (degree 3L-1 polynomial)
  - [corpus] "Global Convergence and Rich Feature Learning" paper extends similar analysis to non-linear networks but requires additional assumptions.
- Break condition: If the limiting polynomial has multiple local minima (non-unique global minimizer), convergence may not occur or may depend on initialization.

## Foundational Learning

- Concept: **Neural Parametrizations (SP/μP/NTP)**
  - Why needed here: Understanding how initialization variance and learning rate exponents scale with width is essential to grasp why μP enables transfer while SP/NTP do not.
  - Quick check question: Given a network of width n=1024, what initialization variance should the output layer V have under μP vs SP? (Answer: n^(-2) vs n^(-1))

- Concept: **Infinite-Width Limits and Convergence in Probability**
  - Why needed here: The proof relies on showing that random quantities (coefficients, optimal learning rates) converge to deterministic limits as width→∞.
  - Quick check question: Why is convergence in probability weaker than almost sure convergence, and which does the paper ultimately prove? (Answer: Theorem 3 proves a.s. convergence, which implies convergence in probability)

- Concept: **Polynomial Structure of Gradient Descent Updates**
  - Why needed here: The key insight is that at any step t, the model output f^(t)(x) is a polynomial in η; understanding this structure is central to the proof technique.
  - Quick check question: At t=1 with L=3 layers, what is the maximum degree of the polynomial f^(1)(x) in η? (Answer: L=3)

## Architecture Onboarding

- Component map:
  - Weight matrices: W₀ (n×d input), W₁...Wₗ (n×n hidden), V (n×1 output)
  - μP scaling: W initialization variance = 1/fan-in; V variance = n^(-2); GD learning rate = η (no width scaling)
  - SP scaling (for comparison): All weights including V have variance = 1/fan-in; LR = η (no width scaling)
  - Loss: Quadratic loss (1/2m)Σ(f(xᵢ)-yᵢ)²; trained with full-batch GD

- Critical path:
  1. Initialize weights according to μP scalings (critical: V ~ N(0, n^(-2)))
  2. For each width n to test, compute the loss landscape L(η) over a range of η values
  3. Identify the optimal ηₙ that minimizes L(η) at that width
  4. Plot ηₙ vs n to verify convergence to a constant (should follow O(n^(-1/2)) convergence rate)

- Design tradeoffs:
  - **Linear vs non-linear networks**: Proof is for linear MLPs only; empirical results suggest extension to ReLU/Adam works but is not proven
  - **Fixed vs trainable input/output layers**: Paper fixes W₀ and V for simplicity; full training requires additional LR scaling rules
  - **Batch size**: Full-batch GD assumed; SGD may affect convergence rates

- Failure signatures:
  - If optimal ηₙ increases with width → likely using SP instead of μP (check V initialization variance)
  - If optimal ηₙ decreases toward 0 → likely using SP or NTP scaling
  - If loss curves don't converge across widths → check that learning rate exponent c is correct for optimizer (c=0 for GD, c=1 for Adam)
  - If optimal η is highly sensitive to depth → expected behavior; η∞ depends on L (see Theorem 1 formula)

- First 3 experiments:
  1. **Validate one-step transfer**: Train linear MLP (L=3) with μP on synthetic linear data for t=1 step; sweep η across [0.01, 1.0] for widths n∈{128, 256, 512, 1024, 2048}; verify optimal η converges to theoretical η∞ = (m/L)·(y^TKy/||Ky||²)
  2. **Compare μP vs SP**: Repeat experiment 1 with SP initialization (V variance = n^(-1)); confirm optimal η shifts toward 0 as width increases (should scale as O(n^(-1)))
  3. **Test multi-step transfer**: Train for t=10 steps with μP; verify optimal η remains stable across widths; compare convergence rate at t=10 vs t=1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the formal proof of learning rate transfer be extended to non-linear neural networks (e.g., MLPs with ReLU activation) and adaptive optimizers like Adam?
- Basis in paper: [explicit] Section 6 (Discussion and Limitations) states: "While our results are limited to linear networks trained with GD, we believe they can be extended to non-linear MLPs and different optimizers... We leave this question for future work."
- Why unresolved: The current proof machinery relies on the loss function of linear networks being exactly expressible as a polynomial in the learning rate $\eta$. Non-linearities and adaptive moments introduce dependencies that break this simple polynomial structure, requiring different techniques to handle large-width deviations.
- What evidence would resolve it: A rigorous proof demonstrating that the optimal learning rate converges to a deterministic constant in non-linear settings, or empirical evidence showing the theoretical bounds derived for linear networks fail in non-linear contexts.

### Open Question 2
- Question: Why does the empirical convergence rate of the optimal learning rate appear faster than the theoretical $O(n^{-1/2})$ bound for large widths (specifically $n > 1024$)?
- Basis in paper: [explicit] Page 5, Section 3.1 notes regarding Figure 2: "This indicates that our upperbound $O(n^{-1/2})$ is likely not tight for large widths and we currently do not have an explanation for this sudden change in convergence rate."
- Why unresolved: The theoretical analysis provides a specific convergence rate based on asymptotic bounds, but empirical observations suggest a phase transition or tighter convergence occurs at larger scales not captured by the current proof.
- What evidence would resolve it: A refined theoretical analysis providing a tighter bound (e.g., $O(n^{-1})$) that matches the empirical slope for large $n$, or the identification of a secondary asymptotic regime.

### Open Question 3
- Question: Under what specific conditions does the limiting loss function $L^{(t)}_\infty$ possess a unique minimizer, which is a required assumption for Theorem 3?
- Basis in paper: [inferred] Theorem 3 and the discussion in Section 4 rely on the "mild condition" that the limiting loss has a unique minimizer. The paper states this is "realistic" but does not provide formal conditions for when this uniqueness is guaranteed, nor does it analyze cases with multiple minima.
- Why unresolved: The proof guarantees convergence only if the limiting polynomial has a single global minimum. Without establishing the criteria for uniqueness, the theoretical guarantee of transfer remains conditional.
- What evidence would resolve it: A formal proof showing that the limiting loss landscape for linear MLPs is strictly convex or unimodal under standard initialization, or an analysis of how multiple minima in the limit would affect the transfer properties.

## Limitations
- The proof is limited to linear networks; extension to non-linear networks is conjectured but not proven
- The analysis assumes full-batch gradient descent; mini-batch effects are not analyzed
- The proof requires non-degenerate training data (Ky ≠ 0) and may not generalize to highly overparameterized regimes
- The "mild condition" of unique minimizer in Theorem 3 is not formally characterized

## Confidence
- High confidence in one-step transfer results (Theorem 1) due to direct polynomial coefficient convergence with explicit rates
- Medium confidence in multi-step transfer (Theorem 3) due to reliance on unproven uniqueness condition
- Low confidence in empirical extensions to non-linear networks and Adam, which lack formal guarantees

## Next Checks
1. **Verify SP failure**: Repeat the one-step experiment with SP initialization (V ~ N(0, n⁻¹)) and confirm optimal η scales as O(n⁻¹) rather than converging to a constant.

2. **Test non-linear extension**: Apply the μP LR transfer analysis to ReLU networks (t=1) and compare convergence behavior to the linear case.

3. **Robustness to data quality**: Repeat experiments with increasingly degenerate training data (approaching Ky ≈ 0) to test the break condition for the uniqueness assumption in Theorem 3.