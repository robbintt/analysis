---
ver: rpa2
title: Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst
  Scaling
arxiv_id: '2502.04022'
source_url: https://arxiv.org/abs/2502.04022
tags:
- data
- text
- regression
- texts
- species
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates methods for quantifying animal occurrence
  frequency from historical Bavarian forestry survey texts using NLP techniques. The
  authors compare three approaches: binary classification, multi-class classification,
  and regression via Best-Worst Scaling (BWS) with Large Language Models.'
---

# Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling

## Quick Facts
- **arXiv ID**: 2502.04022
- **Source URL**: https://arxiv.org/abs/2502.04022
- **Reference count**: 12
- **Primary result**: Automated quantity estimation from historical texts is feasible and robust using LLM-based Best-Worst Scaling

## Executive Summary
This paper evaluates methods for quantifying animal occurrence frequency from historical Bavarian forestry survey texts using NLP techniques. The authors compare binary classification, multi-class classification, and regression via Best-Worst Scaling (BWS) with Large Language Models. They find that BWS with GPT-4 and DeepSeek-V3 provides reasonable agreement with human annotators (Cohen's κ 0.6-0.7) while being more cost-effective than manual annotation. The BWS-based regression model achieves a Mean Absolute Error of 0.11 and R² of 0.73 when predicting continuous frequency values. The study demonstrates that automated quantity estimation from historical texts is feasible and robust, offering a scalable approach for biodiversity quantification across species.

## Method Summary
The approach uses Best-Worst Scaling with LLM APIs to generate continuous frequency scores from historical German survey texts. Texts are sampled and organized into 4-tuples, with LLMs identifying the "best" (highest quantity) and "worst" (lowest quantity) instances per tuple. Scores are computed via s(i) = (#best − #worst) / #overall and normalized to [0,1]. These BWS scores serve as targets for regression models trained on LaBSE embeddings, with optional transfer learning from quantifier n-gram features. The method avoids manual annotation while achieving substantial agreement with human judgments.

## Key Results
- BWS with GPT-4 and DeepSeek-V3 achieves Cohen's κ of 0.6-0.7 with human annotators
- BWS-based regression model achieves MAE of 0.11 and R² of 0.73
- The approach is more cost-effective than manual annotation while being similarly robust
- The method successfully handles heterogeneous nature of historical survey responses

## Why This Works (Mechanism)

### Mechanism 1
Comparative judgment via BWS produces more consistent quantity rankings than direct absolute rating. LLMs compare 4-text tuples, identifying "best" (highest quantity) and "worst" (lowest quantity) instances. Scores are computed via s(i) = (#best − #worst) / #overall, yielding interval-scale values without requiring explicit rating scale design. Core assumption: LLMs can reliably judge relative quantity from historical German text; inter-annotator agreement transfers to ranking quality. Evidence: Table 3 shows GPT-4–DeepSeek-V3 agreement (κ=0.73 Best, 0.69 Worst) exceeding human–human averages (κ=0.59 Best, 0.60 Worst).

### Mechanism 2
Regression on BWS-derived continuous targets outperforms fine-grained multi-class classification for heterogeneous texts. BWS scores (17 discrete values in [0,1]) serve as regression targets. LaBSE embeddings + transfer learning from quantifier n-gram features capture semantic quantity signals. Core assumption: Continuous scaling preserves information lost to arbitrary class boundaries; transfer from extracted quantifiers provides useful inductive bias. Evidence: Table 4 shows Transfer LaBSE (R²=0.73, MAE=0.107) outperforms vanilla regression head (R²=0.61) and KRR baselines.

### Mechanism 3
LLM annotation cost-effectiveness enables scaling without manual labeling. API-based LLMs process 2,000 comparison tuples rapidly; no training data annotation required for BWS scoring, only for downstream regression validation. Core assumption: API costs remain lower than human annotator wages; quality gap is acceptable for ecological inference. Evidence: Authors note "no manual annotation of training data is necessary" for BWS approach.

## Foundational Learning

- **Concept**: Best-Worst Scaling methodology
  - Why needed: Core technique replacing direct rating; requires understanding comparative judgment, set size (4-tuples), and N parameter (comparisons per item)
  - Quick check: Given 1,000 texts with set size 4 and N=2, how many comparison tuples are generated? (Answer: 2,000)

- **Concept**: Cohen's κ interpretation
  - Why needed: Primary agreement metric; κ=0.6-0.7 indicates "substantial" agreement; understanding this is essential for evaluating annotator quality
  - Quick check: If two annotators agree 90% of the time but expected chance agreement is 80%, what is κ? (Answer: 0.5)

- **Concept**: Transfer learning with auxiliary tasks
  - Why needed: Best regression results use quantifier n-grams pre-scaled via BWS as auxiliary training signal before main task fine-tuning
  - Quick check: Why might training on quantifier scores before the main task help? (Answer: Forces encoder to learn quantity-relevant representations)

## Architecture Onboarding

- **Component map**: Historical texts → LLM BWS annotation → BWS scores → LaBSE embeddings + transfer learning → Regression predictions
- **Critical path**: Sample corpus → generate 4-tuples (set size=4, N=2, yielding 8 appearances per text) → Prompt LLM for each tuple → collect Best/Worst judgments → Compute BWS scores → verify distribution → Train regression with 5-fold CV → validate on held-out BWS scores
- **Design tradeoffs**: Set size 4 vs. larger (4-tuples provide near-complete pairwise information); GPT-4 vs. DeepSeek-V3 (similar performance, DeepSeek-V3 more balanced); Ministral-8B (local) vs. API models (local trades quality for privacy/cost)
- **Failure signatures**: Extinction class poorly predicted (model cannot output values <0 after normalization); high-frequency texts underestimated (ceiling effects); spurious correlations detected (toponyms like "Danube" correlate with presence); sampling bias in agreement validation
- **First 3 experiments**: Replicate BWS agreement study (50 tuples, 2 human annotators, GPT-4); ablate transfer learning (LaBSE with/without quantifier pre-training); test out-of-domain generalization (Württembergische Oberamtsbeschreibungen)

## Open Questions the Paper Calls Out

1. To what extent does the LLM-based BWS approach generalize to other domains of semi-structured historical data? The authors state the method should be tested on other historical datasets like Bavarian flora and Württembergische Oberamtsbeschreibungen. This remains unresolved as the study only validates on Bavarian forestry surveys from 1845.

2. To what degree do LLMs reinforce or amplify historical sampling biases and observer subjectivity present in the source texts during quantification? The paper asks if LLMs might "reinforce historical inaccuracies rather than correct them" if they learn from biases in older survey texts. This is unresolved as the study evaluates agreement but not absolute accuracy against objective ecological ground truth.

3. How can the regression model be adapted to accurately distinguish and represent the "Extinct" class? The case study notes that while regression aligns well with multi-class labels, "the extinction class is not properly represented in the regression." The continuous scale [0,1] struggles with the discrete concept of extinction.

## Limitations

- The study relies on LLM-based BWS for scoring, creating potential circularity in the regression validation
- Dependence on high-capability LLMs (GPT-4/DeepSeek-V3) excludes local deployment without substantial quality degradation
- The extinction class remains poorly modeled, with predictions unable to represent true zero values after normalization

## Confidence

- **High confidence**: BWS with GPT-4/DeepSeek-V3 provides better human agreement (κ=0.6-0.7) than smaller models; regression outperforms multi-class classification; LLM annotation is more cost-effective
- **Medium confidence**: Transfer learning with quantifier pre-training improves regression performance; the approach handles heterogeneous survey responses
- **Low confidence**: Exact cost savings relative to manual annotation; generalizability to different historical corpora; long-term reliability of LLM-based scoring

## Next Checks

1. Conduct human-in-the-loop validation on a separate test set to verify BWS scores and regression predictions align with independent expert judgments
2. Apply the trained regression model to an out-of-domain historical survey corpus and assess prediction distribution plausibility without ground truth
3. Implement systematic spurious correlation detection using LIME or SHAP across the full dataset to identify and mitigate non-quantifier features driving predictions