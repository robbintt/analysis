---
ver: rpa2
title: Logits-Constrained Framework with RoBERTa for Ancient Chinese NER
arxiv_id: '2505.02983'
source_url: https://arxiv.org/abs/2505.02983
tags:
- label
- dataset
- framework
- chinese
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses named entity recognition (NER) in ancient
  Chinese texts, which is challenging due to complex semantic properties and limited
  annotated data. The authors propose a two-stage Logits-Constrained (LC) framework
  combining GujiRoBERTa with a differentiable decoding mechanism that enforces valid
  BMES label transitions through dynamic masking.
---

# Logits-Constrained Framework with RoBERTa for Ancient Chinese NER

## Quick Facts
- arXiv ID: 2505.02983
- Source URL: https://arxiv.org/abs/2505.02983
- Authors: Wenjie Hua; Shenghan Xu
- Reference count: 2
- Primary result: LC framework improves F1 scores by up to 2.95% over CRF-based approaches in high-label settings

## Executive Summary
This paper addresses the challenge of Named Entity Recognition (NER) in ancient Chinese texts by introducing a Logits-Constrained (LC) framework built on GujiRoBERTa. The LC framework employs a two-stage approach combining pre-trained transformer encoding with a differentiable decoding mechanism that enforces valid BMES label transitions through dynamic masking. The authors demonstrate that this parameter-free decoding strategy outperforms traditional CRF-based approaches, particularly in high-label or large-data scenarios, while adding BiLSTM modules consistently degrades performance by approximately 3.8%.

## Method Summary
The LC framework operates in two stages: (1) GujiRoBERTa encoding with a linear classification head to produce logits for each token, and (2) Logits-Constrained decoding that enforces valid BMES label transitions through a binary constraint matrix M. The framework dynamically masks invalid transitions during inference by assigning -∞ to logits corresponding to illegal transitions, ensuring only valid label sequences are produced. The model is trained using standard cross-entropy loss with specific hyperparameters (4 epochs, batch size 8, lr=2×10^-5) and evaluated on the EvaHan 2025 benchmark with three training datasets containing 6, 3, and 6 NER categories respectively.

## Key Results
- LC framework achieves up to 2.95% F1 improvement over CRF baselines, particularly in high-label settings
- BiLSTM integration consistently degrades performance by 3.8% on average across all datasets
- Data-driven model selection criterion suggests LC-only models excel when label count L ≥ 20 and data size N > 0.16L^2.8

## Why This Works (Mechanism)

### Mechanism 1
Dynamic logits masking enforces valid BMES label transitions without trainable parameters, reducing illegal sequences in high-label settings. A binary constraint matrix M ∈ {0,1}^{k×k} encodes permissible transitions. During inference, logits are masked: l'_t = M[y_{t-1}] ⊙ l_t + (1 - M[y_{t-1}]) · (−∞), assigning −∞ to invalid transitions to force argmax selection among valid successors only. This assumes the constraint matrix correctly captures all valid BMES transitions and initial token prediction is reasonably accurate.

### Mechanism 2
BiLSTM integration degrades performance by disrupting pre-trained attention patterns in GujiRoBERTa. GujiRoBERTa's 12-layer transformer learns contextual representations optimized for ancient Chinese. Adding BiLSTM introduces parameters whose gradients may trap the combined model in local optima, while the recurrent structure's sequential inductive bias conflicts with the transformer's global attention. This is inferred from ablation studies showing consistent degradation (∆F1 = −3.8%) but lacks direct attention visualization to confirm the mechanism.

### Mechanism 3
Model selection based on label complexity L and dataset size N optimally balances CRF's transition modeling against LC's overfitting avoidance. High label counts (L ≥ 20) with abundant data (N > 0.16 L^2.8) favor LC-only, which avoids CRF's transition matrix overfitting. Below this threshold, CRF+LC combines transition modeling with LC regularization. The exponent 2.8 captures super-linear data requirements as label complexity grows, derived via constrained optimization but fitted on limited data without external validation.

## Foundational Learning

- Concept: BMES tagging scheme (Begin, Middle, End, Single)
  - Why needed here: The LC framework's constraint matrix M is defined relative to BMES transitions; understanding valid sequences (B→M/E, S standalone, no M without preceding B) is prerequisite to interpreting the masking logic.
  - Quick check question: Given labels [B-PER, M-PER, E-PER, S-PER, O], which transitions from M-PER are valid?

- Concept: Conditional Random Fields (CRFs) for sequence labeling
  - Why needed here: The paper positions LC as a replacement/complement to CRF; understanding CRF's global normalization and transition matrix learning clarifies why it struggles in high-dimensional label spaces.
  - Quick check question: Why does a CRF transition matrix of size k×k become harder to estimate reliably as k increases?

- Concept: Pre-trained language model fine-tuning
  - Why needed here: GujiRoBERTa is fine-tuned with a linear classifier; understanding that this preserves pre-trained representations while adapting the output layer clarifies why adding BiLSTM disrupts learned patterns.
  - Quick check question: What is the difference between fine-tuning all parameters vs. freezing the encoder and training only the classifier head?

## Architecture Onboarding

- Component map: Input tokens → GujiRoBERTa embeddings → linear projection → logits → LC masked autoregressive decoding → label sequence

- Critical path: Input tokens → GujiRoBERTa embeddings → linear projection → logits → LC masked autoregressive decoding → label sequence

- Design tradeoffs:
  - **LC vs. CRF**: LC is parameter-free and guarantees valid outputs; CRF learns transitions but may overfit in high-label settings. LC-only preferred when L ≥ 20, N > 0.16 L^2.8; CRF+LC otherwise.
  - **BiLSTM inclusion**: Never recommended; degrades performance consistently.
  - **Two-stage vs. end-to-end**: Two-stage simplifies training but adds inference overhead (limitation acknowledged).

- Failure signatures:
  - **Initial token errors propagate**: If y_0 is wrong, subsequent masked predictions are constrained to an invalid branch.
  - **Punctuation segmentation errors**: Performance depends on sentence segmentation; irregular punctuation causes failures (per Limitations).
  - **High-label, low-data with CRF**: Overfitting in transition matrix; use LC-only.

- First 3 experiments:
  1. **Baseline**: GujiRoBERTa + linear classifier only (no CRF, no LC, no BiLSTM). Record F1 on target dataset to establish reference.
  2. **LC ablation**: Add LC decoding to baseline. Compare F1 delta; expect improvement if L ≥ 6.
  3. **CRF+LC comparison**: Add CRF to baseline with LC. Test both LC-only and CRF+LC configurations. Use model selection formula to predict which should win; validate against empirical results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the constraint matrix be learned adaptively rather than relying on manual BMES rules?
- Basis in paper: The authors state in the Limitations section that the matrix $M$ is "based on manual BMES rules, which may not generalize well" and suggest future work could "explore adaptive constraint learning."
- Why unresolved: The current static matrix cannot adjust to data-driven nuances or exceptions in ancient text structures.
- What evidence would resolve it: A modified framework utilizing a learned transition matrix that outperforms the rule-based mask on the EvaHan dataset.

### Open Question 2
- Question: How does the framework perform on unpunctuated or irregularly segmented ancient texts?
- Basis in paper: The authors note that "performance depends on sentence segmentation quality" and the model is "vulnerable to errors in unpunctuated or irregular historical texts."
- Why unresolved: The preprocessing relies heavily on specific punctuation markers to define inputs, a dependency that may fail in non-standard corpora.
- What evidence would resolve it: Evaluation results on a test set with removed or noisy punctuation to measure performance degradation.

### Open Question 3
- Question: Can the two-stage pipeline be unified into a single end-to-end architecture to reduce inference overhead?
- Basis in paper: The paper lists "additional inference overhead compared to end-to-end models" as a limitation and proposes "unified architectures" for future work.
- Why unresolved: Decoupling contextual encoding from logits masking creates latency that may hinder real-time applications.
- What evidence would resolve it: A single-model architecture that integrates masking into the transformer layers without sacrificing the F1 gains (up to 2.95%) reported.

### Open Question 4
- Question: What is the precise mechanism causing BiLSTM integration to degrade performance?
- Basis in paper: While Section 4.4 reports a consistent $\Delta F1 = -3.8\%$, the authors only "speculate" that it disrupts attention patterns or causes local optima.
- Why unresolved: The hypothesis that BiLSTM disrupts pre-trained attention is not empirically verified via analysis of attention weights or gradient flow.
- What evidence would resolve it: Ablation studies visualizing attention maps with and without BiLSTM to confirm the interference hypothesis.

## Limitations
- The LC framework adds inference overhead compared to end-to-end models due to its two-stage architecture
- Performance depends on sentence segmentation quality and is vulnerable to irregular punctuation in historical texts
- The constraint matrix M is based on manual BMES rules, which may not generalize well to all ancient Chinese text structures

## Confidence

**High Confidence**: The LC framework's implementation and basic F1 improvements (1.5-2.95%) on the EvaHan datasets. The two-stage architecture, constraint matrix construction, and hyperparameter settings are explicitly specified and reproducible.

**Medium Confidence**: The causal mechanisms proposed for BiLSTM degradation and model selection formula derivation. While ablation studies support these claims, they rely on untested assumptions about attention patterns and limited dataset fitting without external validation.

**Low Confidence**: The generalizability of the Γ(L, N) model selection criterion beyond the three EvaHan datasets, and the assertion that LC will consistently outperform CRF in high-label settings without qualification about data characteristics.

## Next Checks

1. **External Dataset Validation**: Apply the LC framework and model selection formula to at least two external NER datasets with different label counts (e.g., CoNLL-2003 with 4 labels, OntoNotes with 18 labels) to test whether Γ(L, N) correctly predicts LC-only vs. CRF+LC performance across diverse conditions.

2. **Attention Pattern Analysis**: For the BiLSTM ablation, visualize attention weights before and after BiLSTM addition using integrated gradients or attention rollout. Quantify changes in attention distribution to empirically verify whether BiLSTM disrupts pre-trained patterns as claimed.

3. **Robustness to Initial Errors**: Systematically inject controlled errors at the first decoding step (e.g., flip predicted label with probability p=0.1, 0.2, 0.3) and measure downstream performance degradation. This would empirically validate the assumption that initial errors don't catastrophically propagate through the autoregressive masking.