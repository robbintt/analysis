---
ver: rpa2
title: 'When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking
  for Well-Being'
arxiv_id: '2512.08937'
source_url: https://arxiv.org/abs/2512.08937
tags:
- advice
- human
- well-being
- expert
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares AI-generated and human advice for well-being
  on Reddit. Experts rated AI advice higher than top human comments across effectiveness,
  warmth, and willingness to seek advice again.
---

# When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being

## Quick Facts
- arXiv ID: 2512.08937
- Source URL: https://arxiv.org/abs/2512.08937
- Reference count: 40
- Expert raters consistently rated AI-generated advice higher than top human comments across effectiveness, warmth, and willingness to seek advice again

## Executive Summary
This paper compares AI-generated and human advice for well-being on Reddit, finding that GPT-4o and GPT-5 consistently outperform top human comments on expert ratings across multiple quality dimensions. The study reveals that benchmark improvements (GPT-4o → GPT-5) don't guarantee better advice quality, with GPT-4o actually outperforming GPT-5 on most metrics. A key finding is that lightweight LLM refinement of human-originated advice improves perceived quality while preserving human-like perception, suggesting effective human-AI collaboration strategies. The research also demonstrates that users prefer friend-like personas for advice-giving, particularly those with higher AI trust or well-being, highlighting the importance of persona sensitivity in AI advice systems.

## Method Summary
The study used 50 Reddit posts from r/getdisciplined's "Need Advice" section, filtered to exclude clinical content, with each post having human (Reddit-Top and Reddit-90th percentile) and AI (GPT-4o and GPT-5) responses. Study-1 employed 161 expert raters (teachers, HR professionals, counselors, coaches) who evaluated 4 responses per scenario across 6 dimensions using 7-point Likert scales and forced rankings. Study-2 tested a 2×2 factorial design (seed source × augmentation type) on 25 posts using GPT-5 with expert-guided refinement. Both studies used blinded evaluations, with AI detection probes and persona matching based on user trust and well-being measures.

## Key Results
- GPT-4o and GPT-5 ranked significantly higher than top human comments on effectiveness, warmth, and willingness to seek advice again
- GPT-4o outperformed GPT-5 on all metrics except sycophancy, suggesting benchmark gains don't automatically improve advice quality
- Lightweight LLM edits improved both human and AI advice quality, with human-originated advice perceived as most human-like
- Users preferred friend-like personas for advice-giving, especially those with higher AI trust or well-being

## Why This Works (Mechanism)

### Mechanism 1
LLMs outperform crowdsourced human advice because they provide more structured, calibrated, and actionable framing—qualities that predict adherence in behavior-change contexts. LLMs implicitly apply evidence-based advice patterns (empathic openers, tiny starters, barrier-handling, tiered options) at higher rates than peer comments, even without explicit prompts, because post-training on helpfulness-oriented feedback reinforces these structures. Advice seekers and expert raters converge on structured guidance as higher-quality, and this predicts real-world uptake. If advice seekers prioritize lived experience and peer validation over structure (e.g., for emotional processing), or if LLM structure becomes formulaic/impersonal, this mechanism breaks down.

### Mechanism 2
General benchmark gains (GPT-4o → GPT-5) do not automatically improve advice quality; advice-specific qualities like warmth and long-term benefit may even regress. Post-training objectives (e.g., RLHF targets, sycophancy calibration) are optimized for aggregate benchmarks, not domain-specific competencies like calibrated guidance for well-being. Scaling and improving on general benchmarks (math, reasoning, etc.) does not automatically yield better advice for complex, human experience scenarios. If future models are explicitly post-trained on advice-quality dimensions (e.g., via long-term outcome feedback or domain-specific RLHF), this mechanism may change.

### Mechanism 3
Lightweight LLM refinement of human-originated advice improves perceived quality while preserving "human-like" perception, whereas pure AI-originated advice is more detectable and may trigger algorithm aversion in unblinded contexts. Human seeds provide lived experience and authentic voice; LLM passes add structure, gap-filling, and calibration. Expert-guided edits further reduce sycophancy and verbosity. Perceived humanness persists because the core content and voice remain human. If users develop better AI detection, or if LLM refinement erases human voice signatures; if transparency mandates expose editing, this mechanism breaks down.

## Foundational Learning

- **Advice quality dimensions** (competence, warmth, personalization, sycophancy, clarity, reuse intention): Understanding these dimensions is prerequisite to interpreting results and designing interventions. Which dimension captures "over-pleasing vs. objective guidance"?

- **Preference-elicitation framing effects**: Rankings shifted when raters adopted "long-term benefit" vs. "overall best" lenses, showing that evaluation framing changes outcomes. How might a "long-term adherence" prompt change your ranking of structured vs. warm advice?

- **Human-AI collaboration pipelines** (seed → edit → refine): Study-2 tests 2×2 pipelines (seed source × augmentation type); understanding these architectures is prerequisite to deployment decisions. Which pipeline minimizes perceived AI-generatedness while maximizing quality?

## Architecture Onboarding

- **Component map**: Post content → Seed selection (human or LLM) → LLM gap-diagnosis pass → Optional expert feedback → Revised advice with preserved voice, added structure, reduced sycophancy → Expert/user ratings on 6 dimensions + rankings + AI-detection probe

- **Critical path**: 1) Seed selection (human-originated for humanness, LLM-originated for structure) → 2) LLM refinement pass with explicit instructions → 3) Optional expert feedback integration (reduces sycophancy, improves calibration) → 4) Persona matching (friend-like vs. coach-like based on user trust/well-being)

- **Design tradeoffs**: Quality vs. detectability (LLM-only refinement yields highest quality but is more detectable; expert-guided on human seeds is most human-like but may not surpass LLM-only quality); Short-term vs. long-term ("Overall best" rankings favor human+LLM; "long-term benefit" favors LLM-originated seeds); Persona adaptation (friend-like preferred by high-trust, high-well-being users; coach-like underperforms across subgroups)

- **Failure signatures**: Sycophancy (over-pleasing, insufficient calibration) — higher in LLM-only seeds; Excessive structure/verbosity — from over-editing; Persona mismatch — coach-like agent for users expecting warmth; False AI-flagging of human content (~20% in study); Missing safety boundaries for clinical/crisis content

- **First 3 experiments**: 1) A/B test human-seeded + LLM-only vs. LLM-originated advice with unblinded users to measure algorithm aversion and trust; 2) Vary persona (friend vs. coach) and measure engagement, adherence, and preference stratified by baseline AI trust and well-being; 3) Longitudinal field test: Track behavior change outcomes (e.g., habit adherence) for structured vs. warm advice variants over 4–8 weeks

## Open Questions the Paper Calls Out

- **Open Question 1**: Would ratings and rankings of AI versus human advice shift if participants knew upfront that AI responses were in the mix? The studies deliberately blinded participants to source until after evaluations, but in real-world settings people often wear their "detect-AI hats," potentially triggering algorithm aversion regardless of content quality. A preregistered study manipulating disclosure timing (before vs. after evaluation) with the same advice stimuli, measuring both perceived quality and behavioral outcomes would resolve this.

- **Open Question 2**: Do offline preferences for AI-generated advice predict real-world usage and actual well-being outcomes? The studies measured perceived quality and forced-choice rankings but not downstream behavior change, adherence, or sustained well-being improvements. Longitudinal field deployments combining outcome logging (e.g., habit adherence streaks, goal completion) with ecologically valid prompts and delayed follow-ups over weeks or months would resolve this.

- **Open Question 3**: How do different preference-elicitation framings (e.g., "overall best" vs. "long-term benefit") shape the downstream behaviors people actually adopt? The framing of preference questions changed model rankings, but the actual behavioral consequences of targeting short-term appeal versus long-term benefit are unknown. A randomized experiment where participants receive advice optimized for different preference targets, then track their adherence and well-being outcomes over time would resolve this.

- **Open Question 4**: What are the long-term effects of sustained AI advice-seeking on human–human relationships and social connection? People may abandon seeking advice from other humans—an intimate, bonding experience—in favor of AI that lacks meaningful accountability. Longitudinal mixed-methods research tracking changes in participants' social networks, help-seeking behaviors, and relationship quality after sustained AI advice use would resolve this.

## Limitations

- Findings rely on expert raters rather than advice seekers themselves, raising questions about ecological validity
- Study focused exclusively on well-being advice, limiting generalizability to other domains
- Human-AI collaboration pipeline's detectability advantage may diminish as users become more familiar with AI-generated content
- Safety boundaries were explicitly applied (excluding clinical content), but paper doesn't address potential harm from miscalibrated advice in sensitive contexts

## Confidence

- **High confidence**: LLM superiority over Reddit advice in structured, warm, effective responses; GPT-4o outperforming GPT-5; LLM refinement improving human advice quality while preserving human-like perception
- **Medium confidence**: Persona preferences (friend vs. coach) varying by user trust/well-being; long-term benefit rankings favoring LLM-originated seeds; algorithm aversion absent in blinded conditions
- **Low confidence**: Real-world adherence and behavior change from AI advice; generalizability beyond well-being contexts; detectability advantages persisting in unblinded, real-world deployment

## Next Checks

1. Conduct unblinded A/B test of human-seeded + LLM refinement vs. LLM-originated advice with actual advice seekers to measure algorithm aversion and trust effects
2. Test longitudinal behavior change outcomes (e.g., habit adherence over 4-8 weeks) for structured vs. warm advice variants in real-world settings
3. Evaluate cross-domain generalization by replicating the pipeline on technical, educational, or medical advice contexts with domain experts