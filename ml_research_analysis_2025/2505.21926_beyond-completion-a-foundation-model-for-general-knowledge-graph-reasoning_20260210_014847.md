---
ver: rpa2
title: 'Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning'
arxiv_id: '2505.21926'
source_url: https://arxiv.org/abs/2505.21926
tags:
- graph
- knowledge
- tasks
- merry
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MERRY, a foundation model for general knowledge
  graph reasoning that integrates both structural and textual information. MERRY addresses
  the challenge of leveraging rich textual information in KGs alongside structural
  data, which existing models largely ignore.
---

# Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2505.21926
- Source URL: https://arxiv.org/abs/2505.21926
- Reference count: 33
- Primary result: MERRY, a foundation model for KG reasoning, achieves strong zero-shot KGC and excellent KGQA generalization across 28 datasets

## Executive Summary
This paper introduces MERRY, a foundation model that integrates both structural and textual information for general knowledge graph reasoning. Unlike existing models that focus solely on structural patterns, MERRY leverages rich textual descriptions of entities and relations through a multi-perspective Conditional Message Passing architecture. The model demonstrates strong reasoning capabilities in zero-shot knowledge graph completion and excellent generalization to out-of-KG tasks like KG question answering, outperforming existing baselines in most scenarios across 28 benchmark datasets.

## Method Summary
MERRY is an encoder-decoder model that processes knowledge graphs through two parallel Conditional Message Passing (CMP) paths: QCMP for query-conditioned structural encoding and GCMP for global semantic encoding. The model employs a dynamic text-adaptive fusion (DTAF) module to selectively integrate textual information with structural features. During pre-training, MERRY is trained on hybrid KG datasets (WN18RR, FB15k-237, CoDEx-Medium) using a two-stage procedure: first freezing ULTRA QCMP weights to train GCMP, then unfreezing all components. For KGQA, the model uses few-shot adaptation with retrieved subgraphs and edge scoring to weigh message passing importance.

## Key Results
- Zero-shot KGC performance consistently improves MRR and Hits@10 across 27 inductive datasets compared to baselines
- KGQA few-shot adaptation achieves high accuracy on CommonsenseQA, outperforming standalone LLMs
- Ablation studies confirm that both the multi-perspective architecture and dynamic fusion contribute significantly to performance gains

## Why This Works (Mechanism)
MERRY addresses the semantic gap between textual and structural information in KGs by employing multi-perspective Conditional Message Passing. The QCMP path conditions structural updates on query relations, while GCMP provides global semantic context from text. The DTAF module dynamically learns to fuse these modalities based on their relevance, allowing the model to selectively retain informative textual features while avoiding noise. This architecture enables query-aware reasoning that leverages both relational paths and semantic meaning, making it effective for both in-KG completion tasks and out-of-KG question answering.

## Foundational Learning

- **Inductive Knowledge Graph Completion (KGC)**
  - Why needed here: MERRY targets inductive settings where test graphs contain unseen entities and relations, requiring the model to generalize beyond fixed embeddings used in transductive learning.
  - Quick check question: Given a training graph with entities {A, B} and relation {r1}, can a transductive model predict a triple involving a new entity C and a new relation r2? Explain why or why not.

- **Message Passing Neural Networks (MPNNs) & Conditional MP (CMP)**
  - Why needed here: MERRY builds on MPNNs (e.g., GCN, GAT) but uses CMP, which conditions node updates on a query relation. This is critical for its query-aware and dataset-agnostic reasoning.
  - Quick check question: In a standard GCN layer, how is a node's new representation computed? How might CMP modify this if we are specifically querying for relation "born_in"?

- **Modality Fusion (Textual vs. Structural)**
  - Why needed here: A core contribution of MERRY is bridging text and structure. Understanding that structural information provides relational paths while textual information provides semantic context is key to grasping the model's multi-perspective design.
  - Quick check question: If an entity "Apple" has structural connections to "Pie" and "iPhone," how would textual descriptions help disambiguate its meaning in a query?

## Architecture Onboarding

- **Component map:** LLM Backbone -> QCMP (Query Conditional Structural Encoding) -> GCMP (Global Structural Semantic Encoding) -> MLP Fusion -> DTAF (Dynamic Text-Adaptive Fusion) -> Edge-Weighted Message Passing -> Final Scoring

- **Critical path:** The flow for a single reasoning query is: Text -> LLM -> Global Embeddings (GCMP) AND Query ID -> Structural Embeddings (QCMP) -> MLP Fusion -> DTAF (Dynamic Fusion) -> Edge-Weighted Message Passing -> Final Scoring

- **Design tradeoffs:**
  - Text Encoding Strategy: The authors chose a parameter-free extraction of the last LLM token for efficiency and to avoid OOM errors, trading off potential fine-tuned alignment for scalability and offline processing capability
  - Two-Stage Training: Freezing QCMP weights (initially from ULTRA) first to train GCMP/DTAF, then unfreezing all. This trades training simplicity for convergence stability
  - Edge Scoring: Not used for KGC (uniform weights) but critical for KGQA. This trades a unified architecture for task-specific performance optimization

- **Failure signatures:**
  - Missing Text: Performance degrades if entity/relation textual descriptions are absent or uninformative
  - Feature Smoothing: Excessive GCMP layers (>3) lead to over-smoothing of features, degrading performance
  - Pre-training Mismatch: For KGQA, zero-shot performance is poor; the model requires few-shot adaptation to learn the REL_the_answer_is relation
  - Noisy Subgraphs: In KGQA, if the retrieved subgraph is large and noisy and edge scoring fails to prune, the model may be overwhelmed

- **First 3 experiments:**
  1. Reproduce Zero-Shot KGC on Inductive Datasets: Re-run the pre-trained MERRY model on a standard inductive KGC benchmark (e.g., WN18RR v1-v4 from the paper's IndE set) without any fine-tuning
  2. Ablation Study of Modality Fusion: Run the model on the same KGC task with two variants: (a) w/o GCMP (only structural path), and (b) w/o DTAF (simple concatenation instead of dynamic fusion)
  3. Few-Shot Adaptation for KGQA: Test the model's adaptability by taking the pre-trained weights and fine-tuning on the CommonsenseQA (CSQA) dataset using the paper's few-shot strategy

## Open Questions the Paper Calls Out
- How can the feature smoothing effect in deep Conditional Message Passing (CMP) layers be mitigated to allow for deeper reasoning without performance degradation?
- How can MERRY be adapted to maintain robust performance when textual descriptions for entities and relations are missing or incomplete?
- What methods can optimize the time and memory overhead of Large Language Model (LLM) encoding to make MERRY scalable for large-scale, dynamic In-KG tasks?

## Limitations
- The model's performance heavily depends on the availability and quality of textual descriptions for entities and relations
- Feature smoothing occurs with deep Conditional Message Passing layers, limiting the receptive field for complex reasoning
- LLM-based text encoding introduces substantial computational overhead for large-scale KGs

## Confidence
- **High Confidence:** The architectural design choices (CMP-based message passing, dynamic text-adaptive fusion) are well-justified and consistently improve performance across ablation studies
- **Medium Confidence:** The zero-shot KGC results demonstrate strong transfer capabilities, but absolute performance metrics may be influenced by specific LLM choices
- **Low Confidence:** The model's robustness to missing textual information and performance on truly out-of-distribution KGs are not thoroughly evaluated

## Next Checks
1. **Missing Text Robustness Test:** Systematically remove textual descriptions from 10-30% of entities/relations in the validation set and measure performance degradation across multiple datasets
2. **Cross-Domain Transfer Evaluation:** Evaluate the pre-trained model on KGs from domains not represented in the pre-training data (e.g., biomedical or temporal KGs) to assess true zero-shot generalization
3. **Computational Efficiency Benchmark:** Measure wall-clock training time and GPU memory usage across different KG sizes and compare against both traditional GNN-based approaches and other LLM-augmented methods