---
ver: rpa2
title: 'Insight Agents: An LLM-Based Multi-Agent System for Data Insights'
arxiv_id: '2601.20048'
source_url: https://arxiv.org/abs/2601.20048
tags:
- data
- insight
- accuracy
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Insight Agents (IA), a hierarchical multi-agent
  LLM system designed to provide e-commerce sellers with personalized data insights
  through automated information retrieval. The system addresses the challenges sellers
  face in discovering and utilizing available programs, tools, and understanding rich
  data from various sources.
---

# Insight Agents: An LLM-Based Multi-Agent System for Data Insights

## Quick Facts
- **arXiv ID:** 2601.20048
- **Source URL:** https://arxiv.org/abs/2601.20048
- **Reference count:** 24
- **Primary result:** Hierarchical multi-agent LLM system for e-commerce sellers achieving 90% accuracy with P90 latency <15s

## Executive Summary
This paper presents Insight Agents (IA), a hierarchical multi-agent LLM system designed to provide e-commerce sellers with personalized data insights through automated information retrieval. The system addresses the challenges sellers face in discovering and utilizing available programs, tools, and understanding rich data from various sources. IA employs a plan-and-execute paradigm with a manager agent overseeing two worker agents: data presenter and insight generator. The manager agent uses specialized lightweight models for out-of-domain detection and agent routing to optimize accuracy and latency. The worker agents utilize a robust API-based data model that decomposes queries into granular components for accurate responses, with domain knowledge dynamically injected for enhanced insights. The system has been launched for Amazon sellers in the US, achieving 90% accuracy based on human evaluation with P90 latency below 15 seconds.

## Method Summary
The Insight Agents system uses a hierarchical architecture with a manager agent that performs out-of-domain detection and query routing, followed by parallel execution of specialized worker agents. The manager agent employs an auto-encoder for OOD detection and a fine-tuned BERT model for agent routing, both optimized for low latency. Worker agents use an API-based data model that decomposes queries into granular executable steps based on available internal data APIs, with external calculation tools handling arithmetic to avoid LLM calculation errors. The system implements query augmentation with dynamic information injection and domain-aware routing for the insight generator. Parallel execution with early termination reduces latency, while guardrails ensure PII and toxicity filtering before response delivery.

## Key Results
- **90% accuracy** based on human evaluation against ground truth benchmark
- **P90 latency <15s** (achieved 13.56s) through parallel execution with early termination
- **High precision OOD detection** (0.969 precision, 0.721 recall) using auto-encoder reconstruction error
- **Efficient agent routing** (0.83 accuracy, 0.31s latency) using fine-tuned BERT classifier

## Why This Works (Mechanism)

### Mechanism 1: Specialized Lightweight Models for Routing
- **Claim:** Specialized lightweight models for routing decisions reduce latency by 10-20x while maintaining or improving accuracy compared to LLM-based routing.
- **Mechanism:** The manager agent uses an Auto-Encoder (AE) for OOD detection trained on in-domain question embeddings, with a reconstruction error threshold (μ_id + λ × σ_id) separating in/out-of-domain queries. For agent routing, a fine-tuned BERT-based classifier (33M parameters) replaces LLM-based few-shot classification. Both models run in parallel.
- **Core assumption:** Routing decisions can be treated as classification problems with stable decision boundaries that don't require LLM-level semantic understanding.
- **Evidence anchors:** Table 1 shows AE-based OOD achieves 0.969 precision at 0.009s vs LLM few-shot at 0.616 precision at 1.665s. Table 2 shows fine-tuned BERT achieves 0.83 accuracy at 0.31s vs LLM at 0.60 accuracy at 2.14s.

### Mechanism 2: API-Based Data Retrieval with Query Decomposition
- **Claim:** API-based data retrieval with query decomposition yields higher accuracy than text-to-SQL approaches by constraining the solution space to valid API calls.
- **Mechanism:** The Data Workflow Planner decomposes queries into granular executable steps based on available internal data APIs. This follows a divide-and-conquer approach: task decomposition via Chain-of-Thought, API/function selection with schema linking, and payload generation with slot filling. External calculation tools handle arithmetic to avoid LLM calculation errors.
- **Core assumption:** The available APIs cover the semantic space of user queries, and API boundaries align with natural query decomposition points.
- **Evidence anchors:** The approach is less prone to syntax errors and hallucinations associated with text-to-SQL solutions.

### Mechanism 3: Parallel Execution with Early Termination
- **Claim:** Parallel execution with early termination reduces P90 latency to <15s by trading infrastructure cost for speed.
- **Mechanism:** OOD detection and agent routing execute simultaneously. Both worker branches (data presenter and insight generator) initiate concurrently with early termination once the correct branch is identified. LLM calls are reserved only for components requiring data fusion and generation.
- **Core assumption:** The cost of redundant computation in the non-selected branch is acceptable given latency requirements, and race conditions don't introduce correctness issues.
- **Evidence anchors:** The P90 of 13.56s is dominated by the two sequential LLM calls (planning + generation).

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire IA system is built on RAG principles—retrieving seller tabular data to ground LLM responses. Without understanding RAG, the data workflow planner design choices won't make sense.
  - Quick check question: Can you explain why retrieving structured tabular data via APIs differs from retrieving unstructured text chunks, and how this affects the retrieval component design?

- **Multi-Agent Hierarchical Orchestration**
  - Why needed here: IA uses a manager-worker pattern where a manager agent routes to specialized worker agents. Understanding when to centralize vs. distribute decision-making is critical for debugging routing failures.
  - Quick check question: What are the trade-offs between having one generalist agent vs. specialized agents with a router? When would you choose each?

- **Out-of-Distribution Detection via Reconstruction Error**
  - Why needed here: The OOD detection mechanism uses auto-encoder reconstruction error as an anomaly signal. This is a specific technique that differs from classification-based OOD and requires understanding embedding spaces.
  - Quick check question: Why would reconstruction error be a useful signal for OOD detection, and what types of out-of-domain queries might this method fail to catch (hint: consider the recall of 0.721)?

## Architecture Onboarding

- **Component map:**
User Query → Manager Agent (OOD Detector + Agent Router in parallel) → Query Augmenter → Worker Agent (Data Presenter or Insight Generator) → Guardrails → User Response

- **Critical path:** Query → OOD + Router (parallel, ~0.3s) → Query Augmenter → Workflow Planner (LLM call) → API Execution → Generation (LLM call) → Guardrails → Response. The P90 of 13.56s is dominated by the two sequential LLM calls (planning + generation).

- **Design tradeoffs:**
  - **Precision vs. Recall in OOD:** λ=4 favors precision (0.969) over recall (0.721). False negatives are acceptable because they're caught downstream; false positives would waste resources.
  - **API-based vs. Text-to-SQL:** APIs constrain flexibility but reduce hallucinations and syntax errors. Requires maintaining API layer vs. database schema.
  - **Parallelization vs. Cost:** Running both branches concurrently doubles compute for ~30% of queries but reduces latency.

- **Failure signatures:**
  - **High latency (>15s):** Likely issue in workflow planner generating too many API calls, or API latency spike. Check task decomposition depth.
  - **Wrong branch selected:** BERT router accuracy is 83%, so ~17% misrouting. Check if query type is ambiguous (descriptive vs. diagnostic).
  - **OOD false negative (valid query rejected):** Query uses terminology outside training set. Consider expanding in-domain training data.
  - **Hallucinated data columns:** API schema linking failed. Check few-shot examples cover the query pattern.

- **First 3 experiments:**
  1. **Latency profiling:** Instrument each component with timing. Identify whether bottleneck is in LLM calls, API execution, or parallelization overhead. Target: confirm P90 <15s holds under load.
  2. **OOD boundary testing:** Create held-out test set with edge-case queries (domain-adjacent but out-of-scope, and in-scope but unusual phrasing). Measure precision/recall trade-off at different λ values.
  3. **Router confusion analysis:** For the 17% misrouted queries, categorize failure modes (ambiguous intent, multi-part questions). Determine if query augmenter should disambiguate before routing vs. router needs retraining.

## Open Questions the Paper Calls Out

- **How can automated evaluation techniques be reliably integrated to assess the "relevance," "correctness," and "completeness" of insights without human intervention?**
  - Basis in paper: The conclusion explicitly states the aim to "integrate automated evaluation techniques for enhanced performance" to augment the current human evaluation process.
  - Why unresolved: The current system relies on human auditors to score responses against ground truth, which is not scalable for continuous integration or large-scale expansion.
  - What evidence would resolve it: Development of an automated evaluation framework that shows high correlation with human judgment scores on the benchmark dataset.

- **How can the recall of the Auto-encoder-based Out-of-Domain (OOD) detection be improved without sacrificing its high precision or low latency?**
  - Basis in paper: The authors note in Section 3.2.1 that the Auto-encoder has low recall (0.721) compared to the LLM baseline (0.971), suggesting the "in-domain set... could be enlarged."
  - Why unresolved: There is a trade-off between the speed/precision of the lightweight model and the higher recall of the slower LLM-based method.
  - What evidence would resolve it: An improved OOD model that achieves >0.90 recall while maintaining precision >0.95 and inference time <0.01s.

- **Does the strict API-based data model constrain the system's ability to handle ad-hoc queries compared to more flexible Text-to-SQL approaches?**
  - Basis in paper: The methodology acknowledges the "trade-off between flexibility and precision" when choosing an API-based model over Text-to-SQL to avoid syntax errors.
  - Why unresolved: Decomposing queries relies on "API availability," potentially failing valid business questions that do not map neatly to existing API granular components.
  - What evidence would resolve it: A comparative analysis of IA against a Text-to-SQL baseline on a dataset of complex, unstructured queries to measure the "coverage" gap.

## Limitations

- **Internal API dependency:** The system's accuracy heavily depends on specific internal e-commerce data APIs not disclosed in the paper, making the API-based approach's superiority over text-to-SQL only valid if these APIs comprehensively cover the semantic space of user queries.
- **Generalizability of lightweight models:** While AE-based OOD detection (0.969 precision) and BERT-based routing (0.83 accuracy) show strong performance on Amazon seller data, the specific configurations may not transfer directly to other domains.
- **Domain knowledge injection mechanism:** The paper mentions "domain knowledge dynamically injected" for enhanced insights but provides no specifics on the templates, sources, or update mechanisms for this knowledge.

## Confidence

- **High confidence:** The parallelization architecture and latency measurements (P90 <15s) are well-documented and reproducible. The OOD detection mechanism with auto-encoder reconstruction error is technically sound and the reported metrics (precision 0.969, recall 0.721) are specific and verifiable.
- **Medium confidence:** The API-based data retrieval approach's accuracy advantages over text-to-SQL are theoretically sound but depend heavily on the undisclosed API schemas. The 90% overall accuracy claim is based on human evaluation but the evaluation methodology and ground truth creation process are not detailed.
- **Low confidence:** The insight generator's domain knowledge injection mechanism and the query augmenter's implementation details are too vague for independent validation.

## Next Checks

1. **Edge case OOD testing:** Create a systematic test suite of queries that are domain-adjacent (e.g., asking about competitor data, or using non-standard e-commerce terminology) to measure the 0.721 recall limit and identify failure patterns.
2. **API coverage validation:** For a sample of real user queries, map the semantic requirements to available API endpoints to verify the API-based approach's completeness claim and identify potential gaps.
3. **Latency breakdown under load:** Instrument the production system to measure component-level latency (OOD detection, routing, workflow planning, API execution) at different query volumes to confirm the 13.56s P90 holds under realistic load conditions.