---
ver: rpa2
title: 'Modality Curation: Building Universal Embeddings for Advanced Multimodal Information
  Retrieval'
arxiv_id: '2505.19650'
source_url: https://arxiv.org/abs/2505.19650
tags:
- retrieval
- arxiv
- multimodal
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNITE, a universal multimodal embedding framework
  that addresses the challenge of heterogeneous data sources and cross-modal alignment
  in multimodal information retrieval. The core method involves strategic data curation
  across text, image, and video modalities, combined with Modal-Aware Masked Contrastive
  Learning (MAMCL) to mitigate competitive relationships among different modality
  instances.
---

# Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval

## Quick Facts
- **arXiv ID:** 2505.19650
- **Source URL:** https://arxiv.org/abs/2505.19650
- **Reference count:** 40
- **Primary result:** Introduces UNITE, a universal multimodal embedding framework achieving state-of-the-art performance on 40+ retrieval tasks through strategic data curation and Modal-Aware Masked Contrastive Learning (MAMCL)

## Executive Summary
This paper introduces UNITE, a universal multimodal embedding framework that addresses the challenge of heterogeneous data sources and cross-modal alignment in multimodal information retrieval. The core method involves strategic data curation across text, image, and video modalities, combined with Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate competitive relationships among different modality instances. The framework achieves state-of-the-art performance on 40+ retrieval tasks, including fine-grained and instruction-based scenarios, surpassing existing specialized domain-specific models. Specifically, UNITE achieves 70.3% overall average score on the MMEB benchmark and 72.5% R@1 on WebVid-CoVR, outperforming larger models like mmE5 11B and IDMR 26B. The work demonstrates that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning.

## Method Summary
UNITE employs a two-stage training process: (1) Retrieval Adaptation with 6.4M instances across text-text, image-text, and video-text pairs using bidirectional InfoNCE loss; (2) Instruction Tuning with 1.3M instances using MAMCL, a masked contrastive learning approach that constrains comparisons to same-modality targets. The framework uses Qwen2-VL backbone with LoRA rank 8 (alpha 16 for Stage 1, 64 for Stage 2), temperature τ=0.03, and extracts final-token hidden states for embeddings. Strategic data curation across modalities enables effective transfer learning, while MAMCL mitigates inter-modal noise in contrastive learning.

## Key Results
- Achieves 70.3% overall average score on MMEB benchmark, outperforming mmE5 11B and IDMR 26B
- Obtains 72.5% R@1 on WebVid-CoVR, surpassing specialized video retrieval models
- Demonstrates asymmetric transfer effects: video-text pairs transfer more effectively to image-text retrieval than text-image pairs do
- Shows MAMCL yields +0.5 to +1.7 point improvements on in-distribution tasks

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Data Curation Effects
- Claim: Training data composition creates asymmetric transfer effects across modalities and task types.
- Evidence: [Table 9] TV pairs transfer more effectively to image-text retrieval than TI pairs; TT+TI training outperforms other combinations on instruction-based retrieval.
- Core assumption: Cross-modal representations share underlying feature spaces that can be leveraged even when training and target modalities differ.

### Mechanism 2: Modal-Aware Masked Contrastive Learning (MAMCL)
- Claim: Constraining contrastive comparisons to same-modality targets reduces inter-modal noise while preserving intra-modal discriminative learning.
- Evidence: [Table 7] MAMCL yields +0.5 to +1.7 point improvements on in-distribution tasks; OOD performance remains stable.
- Core assumption: Different modality combinations have sufficiently distinct feature distributions that mixed-batch contrastive learning introduces harmful gradient noise.

### Mechanism 3: Direct Fine-Grained Integration vs. Separate Alignment
- Claim: Incorporating fine-grained video-caption pairs directly into retrieval adaptation is more effective than a separate next-token-prediction alignment stage.
- Evidence: [Table 10] Training with TV + Fine TV achieves balanced performance across coarse and fine-grained tasks.
- Core assumption: The retrieval adaptation objective is sufficiently flexible to absorb fine-grained alignment signals without catastrophic forgetting of coarse-grained capabilities.

## Foundational Learning

- **InfoNCE Contrastive Loss**: Why needed here: MAMCL modifies the standard InfoNCE formulation. Understanding how positives/negatives contribute to the loss is essential for debugging masking behavior. Quick check: If all masked candidates were set to -∞ instead of 0, what happens to gradient magnitude?

- **Modality Gap in Feature Spaces**: Why needed here: The paper's core hypothesis—that different modalities occupy distinct regions in embedding space—motivates the masking strategy. Quick check: How would you visualize whether your model has learned separate clusters per modality?

- **Instruction Tuning for Retrieval**: Why needed here: Stage 2 uses instruction-formatted queries. Understanding how instructions reshape embedding spaces helps interpret generalization to OOD tasks. Quick check: Does instruction tuning primarily shift query embeddings, candidate embeddings, or both?

## Architecture Onboarding

- **Component map**: Qwen2-VL backbone -> LLM + Vision Encoder + Projector -> Final-token hidden state extraction -> L2 normalization -> Cosine similarity for retrieval

- **Critical path**: 1. Data curation → ensure modality balance per Table 9 findings; 2. Stage 1 (retrieval adaptation) → standard InfoNCE, 6.4M samples; 3. Stage 2 (instruction tuning) → MAMCL enabled, 1.3M samples; 4. Embedding extraction → final token pooling, L2 normalize for cosine similarity

- **Design tradeoffs**: Video-text dominance for cross-modal transfer vs. text-image necessity for instruction following → must balance both in Stage 1; Larger batch sizes improve contrastive learning but MAMCL reduces effective negative pool per modality; LoRA rank 8 chosen for efficiency; higher ranks may capture more modality-specific features but increase training cost

- **Failure signatures**: Performance on image-text tasks degrades after adding video data → check modality ratios; may need more TI data; OOD tasks perform significantly worse than IND → instruction diversity may be insufficient; Temporal video retrieval remains low even after fine-grained data → model scale may be limiting

- **First 3 experiments**: 1. Ablate data composition: Train Stage 1 with TT-only, TI-only, TV-only (600K each) and evaluate on full benchmark suite. 2. Verify MAMCL masking behavior: Log the number of masked candidates per batch. If >80% are masked for some query types, batch composition is imbalanced. 3. Probe modality separation: Extract embeddings for held-out samples from each modality. Compute pairwise distance distributions (intra-modality vs. inter-modality).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the UNITE framework be effectively extended to incorporate audio modalities while maintaining the balance of representation learning across existing text, image, and video modalities?
- Basis in paper: [explicit] The Limitations section states that "audio emerges as another potential modality" and that "balancing multiple modalities remains challenging, suggesting the need for further investigation into modality expansion."
- Why unresolved: The current study explicitly limits scope to text, image, and video, identifying the integration of audio as a complex challenge due to the difficulty of balancing heterogeneous data sources.
- What evidence would resolve it: A variation of the UNITE model trained with audio-data pairs that demonstrates state-of-the-art performance on audio-text and audio-video retrieval tasks without degrading performance on the existing 40+ visual/text benchmarks.

### Open Question 2
- Question: Can specific data curation or training strategies (beyond simple model scaling) effectively close the performance gap between fine-grained temporal video retrieval and spatial retrieval?
- Basis in paper: [inferred] In the analysis of Table 1, the authors note that while spatial retrieval is strong, there is "a great deal of room for improving temporal tasks," and they observe that improvements in temporal tasks currently rely heavily on scaling model size (e.g., 2B to 7B).
- Why unresolved: The paper identifies temporal understanding as a bottleneck where current data curation yields lower returns compared to spatial tasks, implying current methods are insufficient without massive parameter increases.
- What evidence would resolve it: A training regime utilizing novel temporal-specific data compositions or loss functions that allows a smaller model (e.g., 2B) to match the temporal retrieval performance of the current 7B model.

### Open Question 3
- Question: Does the Modal-Aware Masked Contrastive Learning (MAMCL) strategy generalize effectively to non-visual modalities with distinct distributional gaps, such as audio or 3D point clouds?
- Basis in paper: [inferred] The authors claim MAMCL "can serve as a general method and be applied to any extended modal scenarios," but the experimental validation is restricted to text, image, and video interactions.
- Why unresolved: While the hypothesis suggests generalizability, the efficacy of masking inter-modal negatives has not been proven in feature spaces where the distribution gap (e.g., between audio waveforms and text) may differ fundamentally from vision-language gaps.
- What evidence would resolve it: Experiments applying MAMCL to audio-language or 3D-language retrieval tasks, showing that the masked contrastive loss yields higher gains over standard InfoNCE loss than unimodal or simple multimodal baselines.

## Limitations
- Evaluation primarily focuses on English-language datasets with text, image, and video modalities, leaving unclear whether these architectural choices transfer effectively to other languages or modalities (e.g., audio, point clouds)
- Several implementation details remain unspecified (hard negative mining, LoRA target layers, dataset access)
- Training requires substantial computational resources (4096 batch size, multiple epochs) limiting accessibility

## Confidence
- **High confidence**: Asymmetric transfer effects (Mechanism 1) due to extensive ablation studies across 40+ tasks showing consistent patterns
- **Medium confidence**: MAMCL mechanism (Mechanism 2) - while ablation results show consistent improvements, theoretical justification remains partially speculative
- **Medium confidence**: Direct fine-grained integration claim (Mechanism 3) based on specific experimental setup, though broader validation across different fine-grained scenarios would strengthen this finding

## Next Checks
1. **Cross-domain transfer validation**: Evaluate UNITE's embeddings on out-of-distribution datasets from different domains (e.g., medical imaging, satellite imagery) to test the universality claim beyond the benchmarked MMEB tasks.

2. **Modality ablation stress test**: Systematically remove each major data source (TT, TI, TV) and measure performance degradation patterns to confirm whether the claimed asymmetric transfer effects persist under different training constraints.

3. **Scaling behavior analysis**: Train smaller UNITE variants (1B parameters) and larger variants (34B parameters) to map the relationship between model capacity and performance gains, particularly for temporal video retrieval where the paper suggests larger models show substantial improvements.