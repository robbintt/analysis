---
ver: rpa2
title: 'CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction
  in Clinical Case Reports'
arxiv_id: '2505.17265'
source_url: https://arxiv.org/abs/2505.17265
tags:
- information
- extraction
- case
- qwen2
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CaseReportBench is an expert-annotated dataset for dense information\
  \ extraction from clinical case reports on rare diseases. The authors developed\
  \ structured prompts tailored to 14 medical categories and evaluated three prompting\
  \ strategies\u2014filtered category-specific, uniform category-specific, and uniform\
  \ global\u2014across five LLM models including Qwen2.5-7B, Llama3-8B, and GPT-4o."
---

# CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports

## Quick Facts
- arXiv ID: 2505.17265
- Source URL: https://arxiv.org/abs/2505.17265
- Reference count: 15
- Primary result: Expert-annotated dataset for dense information extraction from clinical case reports on rare diseases, with category-specific prompting improving alignment and open-source models outperforming proprietary ones.

## Executive Summary
CaseReportBench is an expert-annotated dataset for dense information extraction from clinical case reports on rare diseases. The authors developed structured prompts tailored to 14 medical categories and evaluated three prompting strategies—filtered category-specific, uniform category-specific, and uniform global—across five LLM models including Qwen2.5-7B, Llama3-8B, and GPT-4o. Category-specific prompting improved alignment with the benchmark, and the open-source Qwen2.5-7B model outperformed GPT-4o. Clinician evaluations confirmed that LLMs can extract clinically relevant information and reduce manual annotation effort, though limitations remain in recognizing negative findings important for differential diagnosis.

## Method Summary
The authors developed CaseReportBench, a dataset of 138 expert-annotated clinical case reports on inherited metabolic diseases. They created 14 category-specific prompts aligned with medical sub-headings and evaluated three prompting strategies: Filtered Category-Specific Prompting (FCSP), Uniform Category-Specific Prompting (UCP), and Uniform Global Prompting (UGP). These were tested across five LLM models (Qwen2.5-7B, Qwen2.5-32B, Llama3-8B, and GPT-4o) using zero-shot, few-shot, and zero-shot chain-of-thought approaches. Primary evaluation used Token Set Ratio (TSR) for extraction accuracy, supplemented by hallucination rate and other string similarity metrics.

## Key Results
- Category-specific prompting improves alignment to benchmark compared to unified global prompting
- Open-source Qwen2.5-7B outperforms GPT-4o for this task
- Zero-shot chain-of-thought offers little advantage over zero-shot prompting for dense information retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Category-specific prompting improves extraction alignment compared to unified global prompting.
- Mechanism: Specialized prompts constrain the output schema to clinically meaningful sub-structures, reducing schema drift and focusing model attention on domain-relevant tokens.
- Core assumption: The model can follow explicit structural constraints when they are localized rather than combined.
- Evidence anchors:
  - [abstract] "Category-specific prompting improves alignment to benchmark."
  - [section 4.2] "Each category was assigned a distinct, individually crafted prompt to enhance extraction accuracy."
  - [corpus] Related work (ELMTEX, ROBoto2) similarly uses structured prompting for clinical extraction tasks, though comparative prompting strategies are not evaluated.
- Break condition: If the target text has heavy cross-category dependencies, forcing separation may fragment context and lower recall.

### Mechanism 2
- Claim: Smaller open-source models can outperform larger proprietary models on structured clinical extraction.
- Mechanism: Open-source models like Qwen2.5-7B exhibit higher instruction fidelity—producing verbatim extractions and adhering to JSON schema—whereas GPT-4o tends to paraphrase, merge details, and introduce inferences, which hurts alignment under strict string-based metrics.
- Core assumption: Evaluation metrics (TSR, exact match) appropriately penalize paraphrasing and inference even when clinically plausible.
- Evidence anchors:
  - [abstract] "Open-source Qwen2:7B outperforms GPT-4o for this task."
  - [section 5] "Open-access LLMs adhere to prompt instructions more faithfully than GPT-4o, which struggles with verbatim extraction, merges details, introduces inferences and deviates from structured formatting."
  - [corpus] Corpus lacks direct head-to-head comparisons of instruction fidelity across model sizes; this finding is not yet externally validated.
- Break condition: If task tolerates or rewards paraphrase, GPT-4o's behavior may be preferable.

### Mechanism 3
- Claim: Zero-shot chain-of-thought (ZS-CoT) does not improve dense extraction over standard zero-shot prompting.
- Mechanism: Dense extraction is fundamentally a schema-aligned retrieval task, not a multi-step reasoning task. Step-by-step decomposition does not help when the bottleneck is precise span selection and formatting.
- Core assumption: The added reasoning tokens do not introduce distractors or hallucinations that degrade extraction.
- Evidence anchors:
  - [abstract] "Zero-shot chain-of-thought offers little advantage over zero-shot prompting."
  - [section 5] "Step-by-step reasoning by CoT does not confer additional benefits for dense information retrieval tasks."
  - [corpus] No corpus evidence directly addresses CoT for structured extraction; external validation is limited.
- Break condition: If extraction requires resolving ambiguities through inference, CoT may help.

## Foundational Learning

- Concept: Dense Information Extraction
  - Why needed here: Unlike named entity recognition or relation extraction, dense extraction requires capturing multi-attribute, hierarchical information across 14 clinical categories with variable granularity.
  - Quick check question: Given a case report paragraph, can you identify which of the 14 categories it maps to and enumerate all relevant sub-fields?

- Concept: Token Set Ratio (TSR)
  - Why needed here: TSR measures token-level overlap while tolerating minor span differences, addressing the variability in clinical annotation boundaries.
  - Quick check question: Why would exact match fail to capture meaningful agreement between "no chest pain" and "denies chest pain"?

- Concept: Schema-Strict Prompting
  - Why needed here: LLMs can hallucinate keys or restructure output; strict schema constraints in the prompt enforce consistent JSON structure for automated evaluation.
  - Quick check question: What happens if an LLM adds a new key "family_history" when the schema only allows "family_and_genetics_history"?

## Architecture Onboarding

- Component map: Case report -> subheading segmentation -> category mapping -> prompt selection -> LLM inference -> JSON output -> metric computation
- Critical path: Case report → subheading segmentation → category mapping → prompt selection → LLM inference → JSON output → metric computation. FCSP and UCP differ at the segmentation/mapping step; UGP bypasses it entirely.
- Design tradeoffs:
  - FCSP vs. UCP: FCSP reduces compute by ~6.2% but offers similar accuracy; UCP is simpler to implement but wastes inference on irrelevant sections.
  - FS vs. ZS: Few-shot improves TSR by ~5–8 points but requires curated examples per category.
  - Open-source vs. proprietary: Open-source gives better instruction adherence and controllability; proprietary offers easier deployment but lower alignment under strict metrics.
- Failure signatures:
  - UGP outputs like `{'error': [], 'Lab_Image': []}` indicate schema collapse—the model ignores the unified prompt structure.
  - High hallucination rates (>80%) with GPT-4o suggest over-generation; check for paraphrasing or inferred content.
  - Low TSR (<30) in Lab_Image category signals difficulty with numerical/abbreviated content.
- First 3 experiments:
  1. Replicate FCSP with Qwen2.5-7B on 10 held-out cases; verify TSR > 50 and hallucination < 40% before scaling.
  2. Ablate subheading filtering: compare FCSP vs. UCP on the same model; confirm compute savings without TSR degradation.
  3. Test schema robustness: add a perturbation (rename one key in the prompt) and measure if the model adapts or hallucinates the old key.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be optimized to accurately identify and extract negative findings (e.g., absence of symptoms) that are critical for differential diagnosis?
- Basis in paper: [explicit] The abstract and conclusion explicitly state that a key area for improvement is "LLMs' limitations in recognizing negative findings important for differential diagnosis."
- Why unresolved: The current study highlights this as a limitation; models predominantly focus on positive entities present in the text, often missing or misclassifying negations which are essential for ruling out diagnoses.
- What evidence would resolve it: A modification of the CaseReportBench schema to explicitly track negative findings, accompanied by improved model performance scores (TSR/F1) specifically on this subset of annotations.

### Open Question 2
- Question: Does Few-Shot Chain-of-Thought (FSCoT) prompting with clinically accurate reasoning steps provide a statistically significant advantage over standard Few-Shot (FS) prompting for this task?
- Basis in paper: [explicit] The authors state in the Discussion (Page 7) that "Exploring ZSCoT on different models and crafting clinically accurate reasoning step examples for FSCoT represents a future direction of the study."
- Why unresolved: The study tested Zero-Shot CoT (which offered little advantage) but did not test FSCoT due to computational costs and the anticipation of limited improvements, leaving the potential of guided reasoning unverified.
- What evidence would resolve it: A comparative evaluation of FSCoT against the current best-performing strategy (FS-FCSP) on the CaseReportBench dataset.

### Open Question 3
- Question: What evaluation metrics can effectively capture "clinical adequacy" without penalizing models for valid semantic variations or the extraction of relevant details not present in the gold standard?
- Basis in paper: [inferred] The authors note on Page 7 that models like Qwen2.5 and GPT-4o sometimes "extracted additional details... introducing alignment challenges" and that GPT-4o was penalized for rewording/fluency despite potentially being clinically accurate.
- Why unresolved: Current string-based metrics (Token Set Ratio, Exact Match) and traditional metrics (BLEU) rigidly compare outputs to a finite gold standard, failing to distinguish between "hallucinations" and valid inferences.
- What evidence would resolve it: The development and validation of a semantic similarity metric or a new "clinical completeness" score that correlates more strongly with human clinician preference scores than TSR does.

## Limitations
- The dataset is limited to 138 case reports on inherited metabolic diseases, limiting generalizability
- Evaluation relies heavily on string-based metrics that may not capture clinical utility
- GPT-4o's paraphrasing behavior is penalized by strict metrics despite potential clinical adequacy
- The study does not address how to extract negative findings essential for differential diagnosis

## Confidence
High confidence in: Category-specific prompting improves extraction alignment compared to unified prompting
Medium confidence in: Open-source models outperforming proprietary models on this task
Low confidence in: The superiority of few-shot over zero-shot prompting

## Next Checks
1. Conduct inter-rater reliability analysis on the CaseReportBench annotations by having independent clinicians re-annotate 20% of the dataset to establish annotation consistency and benchmark validity.
2. Perform ablation studies on the subheading filtering mechanism by running FCSP and UCP on the same cases without filtering to quantify the actual compute savings and determine if accuracy differences are statistically significant.
3. Test the extracted information's clinical utility by having clinicians evaluate whether model outputs containing high TSR but also containing hallucinated content would lead to correct or incorrect clinical decisions in realistic diagnostic scenarios.