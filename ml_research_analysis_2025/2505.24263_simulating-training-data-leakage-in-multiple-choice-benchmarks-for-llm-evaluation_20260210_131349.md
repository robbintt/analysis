---
ver: rpa2
title: Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation
arxiv_id: '2505.24263'
source_url: https://arxiv.org/abs/2505.24263
tags:
- leakage
- detection
- mmlu
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks data contamination detection methods for
  multiple-choice LLM evaluation. It simulates leakage via continual pretraining on
  MMLU and HellaSwag, then compares semi-half, permutation, and n-gram detection approaches.
---

# Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation

## Quick Facts
- arXiv ID: 2505.24263
- Source URL: https://arxiv.org/abs/2505.24263
- Reference count: 40
- Key result: N-gram detection achieves 81-100% F1-score for identifying leaked MCQ instances; up to 46.6% contamination found in MMLU.

## Executive Summary
This paper addresses the challenge of detecting when large language models have been exposed to multiple-choice benchmark data during pretraining. The authors develop a controlled simulation framework where benchmark data is deliberately leaked via continual pretraining, creating ground truth labels to evaluate detection methods. They compare semi-half, permutation, and n-gram approaches, introducing more efficient variants (permutation-R and permutation-Q). The n-gram method emerges as the most reliable, achieving near-perfect detection across settings. Applied to real benchmarks, their approach reveals significant contamination levels and demonstrates measurable accuracy drops after cleaning, suggesting memorization effects.

## Method Summary
The authors simulate training data leakage by first evaluating a base model on multiple-choice benchmarks, selecting questions answered incorrectly with above-average perplexity to ensure unfamiliarity. They then conduct continual pretraining using LoRA on 300 randomly selected instances (labeled "Leaked") while holding out another 300 (labeled "Not Leaked"). Three detection methods are evaluated: semi-half (comparing generated options with half the original options), permutation variants (comparing log-probability scores across option orderings), and n-gram (measuring ROUGE-L similarity between generated and original options). Detection performance is measured via Precision, Recall, and F1-score against the ground truth labels.

## Key Results
- N-gram detection achieves F1-scores of 81-100% across all simulation settings, significantly outperforming other methods
- Permutation-Q (quadratic complexity) provides competitive F1-scores with better computational efficiency than full permutation testing
- Application to MMLU reveals up to 46.6% contamination using tuned n-gram threshold (T=0.25)
- Post-cleaning accuracy drops in subjects like Anatomy and Formal Logic indicate memorization effects from leaked data

## Why This Works (Mechanism)

### Mechanism 1: N-Gram Similarity Detection
The n-gram method assumes that if a model has memorized benchmark content, it will regenerate answer options with high surface-level similarity. By prompting the model to generate each option given the question and previously generated options, then computing ROUGE-L similarity with the original text, the method identifies instances where the model's output closely matches the benchmark. A threshold T=0.25 flags instances where at least one option shows substantial overlap, indicating potential memorization rather than novel reasoning.

### Mechanism 2: Permutation Probability Asymmetry
This method exploits the idea that memorized sequences have distinctive probability signatures. By computing log-probability scores for multiple option orderings and checking whether the original ordering consistently receives the highest probability, the method detects familiarity with the canonical presentation. The permutation-Q variant reduces computational complexity from factorial (n!) to quadratic (n²) by only testing pairs of options, making it practical for benchmarks with many options.

### Mechanism 3: Controlled Leakage Simulation via Continual Pretraining
The simulation framework creates ground truth by deliberately exposing the model to benchmark data through LoRA-based continual pretraining. By selecting questions the model initially answers incorrectly with above-average perplexity, the method ensures the instances are unfamiliar, minimizing false positives from prior exposure. This controlled setup allows rigorous evaluation of detection methods against known labels, establishing a gold standard for comparison.

## Foundational Learning

- **Concept: ROUGE-L (Longest Common Subsequence F-score)**
  - Why needed here: Core similarity metric for n-gram detection; measures overlap without requiring exact word order
  - Quick check question: If a model generates "positive phototropism" and the reference is "positive phototropism in plants," would a high ROUGE-L score alone prove leakage? What else should you check?

- **Concept: Log-Probability of Sequences**
  - Why needed here: Foundation for permutation methods; autoregressive models assign probabilities to token sequences that reveal familiarity
  - Quick check question: Why compare log-probabilities across option reorderings rather than just checking if the correct answer has highest probability?

- **Concept: Continual Pretraining vs. Fine-tuning**
  - Why needed here: The simulation uses language modeling (next-token prediction) rather than supervised learning, affecting what gets memorized
  - Quick check question: Why does the paper note that "continual pretraining does not aim to identify the most likely answer among multiple choices," and how does this affect leakage detection interpretation?

## Architecture Onboarding

**Component map:**
Data Selection → Ground Truth Creation → Detection Methods (N-gram, Permutation, Semi-half) → Threshold Tuning → Evaluation (Precision/Recall/F1)

**Critical path:**
1. Initial filtering: Select incorrectly-answered questions with above-average perplexity
2. Ground truth creation: Split into leaked/not-leaked with random assignment
3. Method selection: N-gram for closed APIs; Permutation-Q for speed with open weights
4. Threshold calibration: Start with T=0.25 (n-gram), p=50% (permutation-R)
5. Instance flagging: Apply chosen method; clean benchmark by removing flagged instances

**Design tradeoffs:**
| Tradeoff | Option A | Option B | Guidance |
|----------|----------|----------|----------|
| Detection method | N-gram | Permutation-Q | N-gram for closed APIs; Permutation-Q for speed with open weights |
| Complexity reduction | Permutation-R (p% of n!) | Permutation-Q (O(n²)) | Q scales to 10+ options; R preserves more signal for 4-option tasks |
| Leakage definition | Strong (flagged + correct) | Weak (flagged only) | Strong for conservative cleaning; Weak for sensitivity analysis |

**Failure signatures:**
- Repetitive template false positives: Moral Scenarios shows high flagging rate but low detection reliability due to generic option patterns ("True/False")
- Length sensitivity: N-gram detection slows for long option text; semi-half and permutation require single inference passes
- Training-objective mismatch: Leaked instances are not always answered correctly (Table 2: 39–95% accuracy on leaked sets)—memorization ≠ understanding

**First 3 experiments:**
1. Reproduce n-gram baseline: On Qwen-0.5B with MMLU subset, simulate leakage (300/300 split), sweep T ∈ {0.00, 0.25, 0.50, 0.75, 1.0}, report F1 curve. Expect peak at T=0.25 per Figure 3.
2. Validate permutation-Q efficiency: Compare Permutation vs. Permutation-Q on identical setup; measure F1 difference and wall-clock time. Verify O(n²) scaling by testing on synthetic 10-option questions.
3. Cross-benchmark generalization: Apply tuned n-gram (T=0.25) to TruthfulQA or PIQA; analyze subject-level accuracy shifts post-cleaning; document new false positive patterns not seen in MMLU/HellaSwag.

## Open Questions the Paper Calls Out

The authors explicitly identify the need for more robust strategies to detect leakage in cases involving repeated option sentences or generic question formats, as current methods show high false positive rates on benchmarks with repetitive templates like "True/False" patterns.

## Limitations

- The controlled simulation uses single-benchmark continual pretraining, which doesn't fully capture real-world multi-dataset pretraining scenarios
- Detection methods, especially n-gram, are vulnerable to false positives on repetitive option templates and generic formats
- Permutation methods require model weight access, excluding closed-weight APIs like GPT-4
- The analysis focuses on MMLU and HellaSwag benchmarks, with limited validation across diverse benchmarks or non-English languages

## Confidence

- **High Confidence:** N-gram detection consistently achieves highest F1-scores (81-100%) across simulation settings; permutation-Q offers competitive performance with better efficiency; post-cleanup accuracy drops in subjects like Anatomy and Formal Logic indicate memorization effects; up to 46.6% contamination detected in MMLU using tuned n-gram threshold.
- **Medium Confidence:** The LoRA continual pretraining simulation adequately represents contamination patterns; perplexity filtering successfully excludes already-seen instances; 300/300 split provides sufficient statistical power; T=0.25 is universally optimal threshold.
- **Low Confidence:** Cross-benchmark generalization beyond MMLU/HellaSwag; performance on instruction-tuned vs. base models; effectiveness on closed-weight APIs; contamination patterns in non-English or specialized domain benchmarks.

## Next Checks

1. **Replication on Alternative Benchmarks:** Apply the tuned n-gram detection method (T=0.25) to TruthfulQA, PIQA, and other multiple-choice benchmarks not used in the original simulation. Document subject-level accuracy changes and identify new false positive patterns that weren't present in MMLU/HellaSwag datasets.

2. **Cross-Model Generalization Test:** Run the complete simulation pipeline (leakage creation + detection evaluation) on at least two additional model families beyond Qwen-0.5B (e.g., LLaMA, Mistral). Compare F1-score distributions across models to assess method robustness to architectural differences and pretraining strategies.

3. **Closed-API Validation Protocol:** Design and validate a detection protocol for closed-weight models using only API-accessible outputs. Test whether generating options without temperature (deterministic sampling) or using top-k/top-p decoding affects n-gram similarity scores enough to maintain detection reliability without weight access.