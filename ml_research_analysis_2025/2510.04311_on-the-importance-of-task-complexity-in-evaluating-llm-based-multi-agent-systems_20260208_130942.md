---
ver: rpa2
title: On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems
arxiv_id: '2510.04311'
source_url: https://arxiv.org/abs/2510.04311
tags:
- llm-mas
- task
- agents
- depth
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates when LLM multi-agent systems outperform
  single-agent systems by analyzing task complexity in terms of depth (sequential
  reasoning steps) and width (capability diversity). A theoretical framework shows
  that the performance gain of LLM-MAS over LLM-SAS increases with both depth and
  width, but gains from depth are unbounded while width gains saturate.
---

# On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems

## Quick Facts
- arXiv ID: 2510.04311
- Source URL: https://arxiv.org/abs/2510.04311
- Authors: Bohan Tang; Huidong Liang; Keyue Jiang; Xiaowen Dong
- Reference count: 40
- Key outcome: LLM-MAS outperforms SAS more as task complexity (depth/width) increases, with depth gains unbounded and width gains saturating

## Executive Summary
This work investigates when LLM multi-agent systems (MAS) outperform single-agent systems (SAS) by analyzing task complexity along two dimensions: depth (sequential reasoning steps) and width (capability diversity). The authors develop a theoretical framework showing that performance gains of LLM-MAS over SAS increase with both dimensions, but while depth gains are unbounded, width gains saturate. Empirical validation on math reasoning and creative writing tasks confirms these findings, demonstrating that multi-agent collaboration is most valuable for tasks requiring deep sequential reasoning.

## Method Summary
The study compares LLM-MAS (multi-agent debate) against LLM-SAS (single agent with chain-of-thought) on tasks with controlled complexity. Two benchmarks are used: DyVal for math reasoning (DAG problems with depth/width 2-4) and DW2 for creative writing (essays with depth K ∈ {4,8,12,16,20} and width as normalized Shannon entropy of keyword domains). Performance is measured as accuracy (math) or composite score (writing), with gains calculated as Δ = (S_multi - S_single) / S_single. Shapley-R² decomposition quantifies the relative influence of depth versus width on performance variance.

## Key Results
- Performance gain Δ increases with both depth and width
- Depth gains are unbounded (theoretically and empirically)
- Width gains saturate at a finite bound determined by agent count
- Depth exerts stronger influence than width on MAS effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Unbounded Error Mitigation via Sequential Redundancy
In high-depth tasks, single agents suffer from compounding errors. LLM-MAS maintains N parallel reasoning trajectories; if one agent fails at step k but another succeeds, the aggregator can select the correct path, preventing exponential decay of success rate.

### Mechanism 2: Finite Capability Coverage via Width Redundancy
High-width tasks require diverse skills per step. A single agent may lack proficiency in all required capabilities. MAS pools skills; the probability that at least one of N agents succeeds at a wide step approaches 1 as N grows, but this per-step boost limits total gain to (rN)^d.

### Mechanism 3: Constraint Satisfaction through Distributed Cognitive Load
In generative tasks, MAS distributes complex, conflicting constraints across agents. Different agents can champion different constraints during debate, and the aggregator synthesizes these partial solutions into a compliant whole.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) for Reasoning**
  - Why needed: DyVal benchmark represents math problems as DAGs; depth = longest path, width = max fan-in
  - Quick check: Can you calculate depth and width of a binary tree with 3 layers and varying children per node?

- **Concept: Shapley Value Decomposition**
  - Why needed: Attributes performance variance to depth vs width using Shapley-R² scores
  - Quick check: How would Shapley score change if depth and width were perfectly correlated?

- **Concept: Multi-Agent Debate Protocol**
  - Why needed: Theoretical analysis assumes specific architecture: N agents generate answers, observe others, then generate again before aggregator sums up
  - Quick check: Does the aggregator act as mere voter or synthesizer of reasoning chains?

## Architecture Onboarding

- **Component map:** Task Generator -> Debate Loop -> Aggregator
- **Critical path:**
  1. Input: High-complexity prompt (e.g., d=4 reasoning steps)
  2. Turn 1: Agents generate independently
  3. Turn T: Agents generate conditioned on history (Debate)
  4. Reduction: Aggregator selects best answer
- **Design tradeoffs:**
  - Static vs Dynamic Topology: Paper analyzes fixed debate graph; dynamic graphs may be better but static proves when it's worth the cost
  - Cost vs Depth: Use MAS only when d > 2; for d=1, overhead likely outweighs gain
- **Failure signatures:**
  - Homogeneity: If all agents make same logical leap, Δ vanishes
  - Aggregator Bottleneck: If aggregator has low reliability r, unbounded depth potential is capped at (rN)^d
- **First 3 experiments:**
  1. Depth Scaling: Math benchmark with fixed width w=2, vary depth d=2,3,4; plot Δ to confirm exponential trend
  2. Width Saturation: Creative Writing with fixed depth k=8, vary entropy; confirm Δ grows but plateaus
  3. Ablation on Aggregator: Replace LLM Aggregator with majority voting to test sensitivity to reliability parameter r

## Open Questions the Paper Calls Out

- **Interaction complexity**: Does the extent of sub-task interdependence influence LLM-MAS effectiveness differently than depth and width? Current framework only formalizes sequential reasoning and capability diversity.

- **Debate-style benchmarks**: Can benchmarks explicitly designed for debate-style interactions better evaluate LLM-MAS utility than datasets adapted from single-agent settings? Current adaptations often fail to necessitate genuine collaboration.

- **Non-debate frameworks**: Do theoretical findings regarding depth's unbounded gains and width's saturation apply to non-debate LLM-MAS frameworks like sub-task decomposition? Theoretical extension to other frameworks is non-trivial.

## Limitations

- Theoretical analysis assumes specific debate protocol with independence assumptions that may not hold in practice
- Aggregator's reliability parameter r is treated as constant rather than empirically measured
- Analysis focuses on static debate topologies without accounting for dynamic agent coordination
- Empirical validation limited to Qwen-2.5-32B-Instruct model

## Confidence

- **High confidence**: Theoretical framework showing unbounded gains from depth versus bounded gains from width
- **Medium confidence**: Specific parameter values (4-6 agents, 1-3 debate iterations) and their optimality
- **Medium confidence**: Creative writing evaluation methodology and scoring

## Next Checks

1. **Ablation on aggregator design**: Replace LLM aggregator with simpler mechanisms (majority voting, confidence-weighted selection) to empirically measure sensitivity to reliability parameter r

2. **Cross-model validation**: Replicate core findings with different base models (Llama, GPT) to test if depth-width tradeoff relationship holds across architectures

3. **Dynamic topology test**: Implement simple dynamic agent coordination mechanism and compare performance against static debate protocol to assess impact on depth-width tradeoff relationship