---
ver: rpa2
title: 'CAViAR: Critic-Augmented Video Agentic Reasoning'
arxiv_id: '2509.07680'
source_url: https://arxiv.org/abs/2509.07680
tags:
- video
- answer
- segment
- final
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAViAR addresses the challenge of complex video reasoning by introducing
  an agent-critic framework that iteratively composes video modules. The reasoning
  agent generates sequences of programs using video-processing tools, while a critic
  selects the most promising strategies based on examples of successful reasoning
  traces.
---

# CAViAR: Critic-Augmented Video Agentic Reasoning

## Quick Facts
- arXiv ID: 2509.07680
- Source URL: https://arxiv.org/abs/2509.07680
- Reference count: 40
- Primary result: 62.0% accuracy on LVBench, 77.2% on Neptune, 32.3% mIOU on ActivityNet-RTL

## Executive Summary
CAViAR introduces an agent-critic framework for complex video reasoning that iteratively composes video modules. The reasoning agent generates programs using video-processing tools, executing them to obtain intermediate results that guide subsequent steps. A natural language critic selects the most promising strategies from multiple candidates based on examples of successful reasoning traces. This approach enables adaptability and avoids brittle fixed procedures, significantly outperforming direct inference and prior methods on multiple video reasoning benchmarks.

## Method Summary
CAViAR implements a two-stage agent-critic framework for video reasoning. The reasoning agent (LLM with Python interpreter) generates programs using video module APIs, executing them iteratively to produce reasoning traces. A critic LLM selects the best strategy from multiple candidates based on in-context examples of successful/failed traces. The system uses module subset strategies to create diverse reasoning paths and avoids the "chicken-and-egg" problem of self-evaluation by having the critic assess reasoning traces rather than just final answers.

## Key Results
- 62.0% accuracy on LVBench, significantly outperforming direct inference (43.4%) and single-program approaches (27.1%)
- 77.2% accuracy on Neptune, beating Direct Inference with ASR (74.9%)
- 32.3% mIOU on ActivityNet-RTL, approaching human performance (34.3%) and exceeding previous methods (30.3%)
- Agent + Critic outperforms agent alone by 14.9 points on LVBench

## Why This Works (Mechanism)

### Mechanism 1: Iterative Reasoning Traces
Iteratively generating programs with visible intermediate results outperforms single-shot program generation for complex video reasoning. The agent generates a program, executes it to obtain results, then conditions on the execution history to generate subsequent steps until convergence. Each step can inspect actual module outputs rather than assuming their behavior.

Core assumption: Module outputs are sufficiently informative to guide next-step decisions, and the base model can effectively condition on execution history.

Evidence anchors:
- [abstract]: "the agent uses the results of each call to a module to determine subsequent steps"
- [section 5 ablations]: Single program approach achieves 27.1% (LVBench) vs Agent at 47.1%, demonstrating the value of iteration

Break condition: If module outputs are uninformative or misleading (e.g., hallucinated frame descriptions), iteration degrades rather than improves reasoning.

### Mechanism 2: Critic-Based Strategy Selection
A natural language critic selecting among diverse reasoning strategies substantially improves final accuracy over relying on any single strategy. Multiple strategies are sampled by restricting available modules differently, and the critic identifies "winning strategies" via natural language comparison using in-context examples.

Core assumption: The critic can learn from few examples to distinguish promising reasoning patterns, and diverse strategies cover failure modes of individual approaches.

Evidence anchors:
- [section 5, Table 4]: Agent + Critic: 62.0% vs Agent alone: 47.1% on LVBench (14.9 point gain)

Break condition: If sampled strategies are insufficiently diverse or critic examples are out-of-distribution, selection becomes noisy.

### Mechanism 3: Module Subset-Induced Strategy Diversity
Restricting available modules per strategy creates meaningfully different reasoning paths, enabling the critic to select the best approach per-query. Three strategies are defined by module availability: retrieval_qa + get_segment, retrieval_qa alone, and all modules.

Core assumption: Different module combinations yield qualitatively different failure modes, and at least one strategy works per query.

Evidence anchors:
- [section 3.4]: "To obtain trajectories corresponding to markedly different strategies, we provide the reasoning agent different subsets of modules"
- [section 5]: "find_when module appears to be less reliable for very long videos" but agent with all modules still tries to use it—critic avoids this trap

Break condition: If module subsets don't produce diverse outcomes or all strategies fail on a query type, critic selection is ineffective.

## Foundational Learning

- **Concept: Tool-augmented / agentic reasoning**
  - Why needed here: CAViAR is fundamentally an agent that calls tools (video modules) rather than end-to-end inference
  - Quick check question: Can you explain how an LLM generates a function call and how an execution engine processes it?

- **Concept: In-context learning with few-shot examples**
  - Why needed here: The critic uses only 4 in-context examples per dataset to learn successful reasoning patterns
  - Quick check question: What is the difference between in-context learning and fine-tuning, and why might in-context be preferred for a critic?

- **Concept: Process supervision vs outcome supervision**
  - Why needed here: The critic evaluates reasoning *traces* (process), not just final answers, inspired by text reasoning verifiers
  - Quick check question: Why might evaluating intermediate reasoning steps improve selection over just checking final answer correctness?

## Architecture Onboarding

- **Component map:**
  - Reasoning Agent (π) -> Execution Engine (ϕ) -> Video Modules -> Reasoning Agent (conditioning)
  - Reasoning Agent (π) -> Video Modules -> Critic (c) -> Winning Strategy Selection

- **Critical path:**
  1. Define module API (Listing 1 in supplement)—exact function signatures matter
  2. Implement modules to return meaningful text descriptions for agent conditioning
  3. Configure strategy subsets (3 strategies per dataset in paper)
  4. Write critic prompt with 4 in-domain examples (Supplemental Material Listings 4-6)
  5. Run inference: agent generates 3 strategies → critic selects → return winning answer

- **Design tradeoffs:**
  - More strategies vs compute: 3 strategies used; more increases cost linearly but may improve coverage
  - Module complexity vs reliability: find_when is noisy on long videos; simpler modules (get_segment) more reliable but less capable
  - Critic model capacity: Same Flash model used for agent and critic—stronger critic might improve selection but increases cost

- **Failure signatures:**
  - Agent with all modules over-relies on unreliable find_when on long videos → many spurious time ranges → confusion
  - Single-program approaches make brittle assumptions (e.g., "answer in first 15s") → exceptions or guesses
  - Self-evaluation (confidence scores) reduces performance (47.1% → 39.9% on LVBench) due to poor calibration

- **First 3 experiments:**
  1. Reproduce ablation: Run agent with all modules vs agent + critic on 50 LVBench samples; verify single-program baseline fails (expect ~27% accuracy)
  2. Module swap test: Replace retrieval_qa with a stronger frame retriever; measure if critic still provides gains over agent alone
  3. Critic example analysis: Vary number of in-context examples (0, 2, 4, 8) on Neptune; plot accuracy vs example count to understand data efficiency

## Open Questions the Paper Calls Out
The paper explicitly identifies reliance on "additional compute" and a "strong instruction-following base MLLM" as limitations in "compute-limited settings," noting this is a significant constraint for practical deployment.

## Limitations
- In-context critic generalization uncertainty: The paper relies on only 4 in-context examples per dataset, but effectiveness on new domains remains uncertain
- Module output reliability dependency: Iterative reasoning depends on module outputs being sufficiently informative; unreliable modules (particularly find_when on long videos) can compound errors
- Compute cost considerations: Multiple strategy generations and critic evaluations per query increase computational overhead, though not explicitly analyzed

## Confidence
- High confidence: Iterative reasoning traces with visible intermediate results (47.1% vs 27.1% accuracy on LVBench)
- Medium confidence: Critic effectiveness with limited in-context examples (14.9 point gain), generalizability uncertain
- Medium confidence: Module subset diversity claim supported by task-specific results, but systematic analysis lacking

## Next Checks
1. **In-context example scaling study:** Systematically vary the number of in-context examples (0, 2, 4, 8, 16) provided to the critic across multiple datasets to measure the relationship between example count and accuracy

2. **Module failure injection experiment:** Deliberately corrupt outputs from specific modules (e.g., introduce hallucinations in find_when results) and measure how this affects iterative reasoning quality

3. **Cross-dataset critic transfer:** Train the critic on examples from one dataset (e.g., LVBench) and evaluate on another (e.g., Neptune) to assess whether successful reasoning patterns transfer across domains