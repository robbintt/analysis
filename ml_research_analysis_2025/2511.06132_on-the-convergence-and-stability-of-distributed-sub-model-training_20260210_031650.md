---
ver: rpa2
title: On the Convergence and Stability of Distributed Sub-model Training
arxiv_id: '2511.06132'
source_url: https://arxiv.org/abs/2511.06132
tags:
- training
- convergence
- masking
- local
- sub-model
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies distributed sub-model training in federated
  learning, addressing computational and memory constraints by having clients update
  only randomly sampled or shuffled sub-models. The authors propose two algorithms:
  one with random sub-model selection and another with shuffled rolling selection,
  where the server partitions the full model, shuffles sub-models, and assigns them
  to clients sequentially.'
---

# On the Convergence and Stability of Distributed Sub-model Training

## Quick Facts
- **arXiv ID:** 2511.06132
- **Source URL:** https://arxiv.org/abs/2511.06132
- **Reference count:** 40
- **Primary result:** Proposes distributed sub-model training algorithms with random and rolling masking; proves convergence to masked objective with quantifiable residual error; shows masking improves generalization via stability; experiments validate rolling masking outperforms random masking under data heterogeneity.

## Executive Summary
This paper studies distributed sub-model training in federated learning, addressing computational and memory constraints by having clients update only randomly sampled or shuffled sub-models. The authors propose two algorithms: one with random sub-model selection and another with shuffled rolling selection, where the server partitions the full model, shuffles sub-models, and assigns them to clients sequentially. They establish convergence rates in both convex and non-convex settings, showing that sub-model training converges to the stationary point of an alternative masked objective with residual error depending on the masking ratio. Stability analysis reveals that masking improves generalization by stabilizing the training process, provided residual optimization error remains controlled. Experiments on CIFAR-10 and CIFAR-100 datasets validate theoretical findings, showing rolling masking outperforms random masking and improves model robustness compared to full-model training under non-IID data conditions.

## Method Summary
The paper modifies Federated Averaging by introducing element-wise masking, where each client updates only a subset of model parameters. Two masking strategies are proposed: random Bernoulli masking and rolling/partitioned masking with epoch-level shuffling. The server partitions the global model, generates masks, and distributes sub-models to clients. Clients perform local SGD updates on their private data and return only the updated sub-model parameters. The server aggregates by averaging client updates and filling inactive coordinates with the previous global model values. The analysis establishes convergence to the stationary point of a masked objective with quantifiable residual error and proves that masking improves generalization via algorithmic stability, provided the residual error remains controlled.

## Key Results
- Sub-model training converges to stationary point of masked objective F_p with residual error proportional to (1-p_i) and bounded by gradient norm G² and smoothness L².
- Rolling (shuffled) sub-model assignment achieves better convergence than random Bernoulli masking under heterogeneous data distributions by reducing gradient coverage variance.
- Masking improves generalization by stabilizing the training process, reducing the effective smoothness constant and masked gradient variance, provided residual optimization error remains controlled.
- Experiments on CIFAR-10 and CIFAR-100 show rolling masking outperforms random masking in convergence speed and final accuracy, especially under high data heterogeneity, and improves generalization gap compared to full-model training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rolling (shuffled) sub-model assignment achieves better convergence than random Bernoulli masking under heterogeneous data distributions.
- Mechanism: Partitioning the model into R predefined sub-models and shuffling them at each epoch ensures each parameter receives updates in a controlled, systematic manner across rounds. This reduces the variance of gradient coverage compared to independent random sampling at each iteration, analogous to how shuffled SGD outperforms random-with-replacement SGD.
- Core assumption: L-smooth and µ-strongly convex (or non-convex with bounded gradients) local objectives; bounded gradient variance.
- Evidence anchors:
  - [abstract]: "observing the success of SGD with shuffling, we propose a distributed shuffled sub-model training"
  - [Section 5.2]: "Fig. 1(a) and Fig. 1(b) show the global model testing loss under high data heterogeneity, indicating that rolling masking outperforms random in both datasets"
  - [corpus]: Related work on client selection (arXiv:2504.01921) shows shuffling client participation improves convergence, suggesting systematic assignment reduces variance broadly in FL.
- Break condition: If the sub-model partition is poorly aligned with gradient structure (e.g., all critical parameters in one sub-model), or if R is too small relative to model dimension, the systematic coverage advantage diminishes.

### Mechanism 2
- Claim: Masking can improve generalization by stabilizing the training process, provided residual optimization error remains controlled.
- Mechanism: Masking reduces the effective smoothness constant (̃L = max_i p_i L) and masked gradient variance (δ²), making the algorithm more stable—i.e., less sensitive to single data point perturbations. By algorithmic stability theory (Definition 4, Lemma 1), this directly bounds generalization error.
- Core assumption: Convex local losses, bounded gradients (G), bounded gradient variance (δ²), and that the residual error from partial updates does not dominate.
- Evidence anchors:
  - [abstract]: "sub-model training can improve the generalization via amplifying the stability of training process"
  - [Section 4, Theorem 5]: "A smaller masking probability {p_i} will result in a smaller δ, ̃L, σ*², ζ_max. Hence masking improves the generalization by stabilizing the training process"
  - [Section 5.3, Tables 1-2]: On CIFAR-10 high heterogeneity, random masking achieves smaller train-test gap (0.0126 loss diff) vs full model FedAvg (0.0190); similar trends on CIFAR-100.
- Break condition: If masking probability is too low, residual convergence error (Theorem 1's term ∝ Σd(1-p_i)) dominates, offsetting stability gains. The paper notes: "as long as the residual optimization error from partial training remains controlled."

### Mechanism 3
- Claim: Sub-model training converges to a stationary point of an alternative masked objective F_p, not the original objective F, with quantifiable residual error.
- Mechanism: Since only masked parameters receive gradient updates each round, the optimization dynamics track F_p(w) = (1/N)Σ E_{m~Ber(p)}[f_i(m⊙w)] rather than F(w). The optimality gap between F_p's minimizer and F's minimizer creates residual error proportional to (1 - p_i) and bounded by gradient norm G² and smoothness L².
- Core assumption: Strong convexity and smoothness of local objectives; bounded domain and gradients.
- Evidence anchors:
  - [Section 2.1, Theorem 1]: Convergence bound includes explicit "Residual error due to masked updates" term: (5L/2μ̄ + 4/L) · (2G² + 2W²L²)/N · Σ_i d(1-p_i)
  - [Section 2.2]: "sub-model training will converge to the stationary point of an alternative objective induced by masking, not the raw objective F(w)"
  - [corpus]: Work on distributed quasi-Newton FL (arXiv:2501.10877) notes similar residual error phenomena in partial-update schemes.
- Break condition: When p_i → 1 (full model training), residual error vanishes and FedAvg is recovered. When p_i → 0, residual error dominates and convergence to F's optimum is impossible without additional mechanisms.

## Foundational Learning

- **Federated Averaging (FedAvg) and Local SGD**
  - Why needed here: The paper modifies FedAvg with masking; understanding the baseline local-update-then-average cycle and "client drift" is essential to interpret convergence results.
  - Quick check question: Why does FedAvg use K local SGD steps before averaging, and what causes client models to "drift" from the global optimum under heterogeneous data?

- **Algorithmic Stability → Generalization**
  - Why needed here: The paper's generalization claims (Section 4) rest on ε-on-average stability (Definition 4) and its connection to generalization error (Lemma 1).
  - Quick check question: If an algorithm's output changes by at most ε when one training point is perturbed, what bound does this place on the expected generalization error?

- **Element-wise Masking and Sub-model Extraction**
  - Why needed here: The core operation m⊙w (element-wise product of binary mask and model vector) defines what parameters each client updates.
  - Quick check question: Given mask m ∈ {0,1}^d and model w ∈ R^d, what does m⊙w represent? How would you compute a gradient step on only the unmasked parameters?

## Architecture Onboarding

- **Component map:**
  - Server -> Mask Generator -> Clients -> Aggregator -> Server
  - Server maintains global model and coordinates training; Mask Generator creates masks (random Bernoulli or rolling partitions); Clients perform local updates on sub-models; Aggregator averages client updates with parameter filling.

- **Critical path:**
  1. Initialize w^0; define masking probabilities p_i (random) or partition configuration {m_i^j} (rolling).
  2. Each round r: generate masks → distribute sub-models → clients execute K local updates → clients return sub-models → server aggregates with filling.
  3. Key decision: Choose rolling for high heterogeneity (better convergence); choose random for simplicity or dynamic client capacity.

- **Design tradeoffs:**
  - Lower p_i: Less client computation/memory, improved stability/generalization, but higher residual convergence error.
  - Rolling vs random: Rolling yields lower loss/higher accuracy (Figures 1-4) but requires fixed partition; random is simpler but noisier gradient coverage.
  - K (local steps): Larger K reduces communication rounds but increases client drift, especially under data heterogeneity.

- **Failure signatures:**
  - Residual error dominates: Training plateaus well above expected loss; verify masking ratios aren't too aggressive (p_i too low).
  - High heterogeneity + random masking: Slow convergence or oscillating loss; switch to rolling masking.
  - Generalization gap persists: If train-test gap remains large despite masking, check if residual error is uncontrolled (increase p_i or R).

- **First 3 experiments:**
  1. Reproduce stability-generalization result (Tables 1-2): Train ResNet18 on CIFAR-10 with L=2 labels/client; compare random masking vs full FedAvg; measure train-test loss/accuracy gap to confirm masking improves generalization.
  2. Vary masking ratio systematically: Fix data heterogeneity (L=2); sweep p ∈ {1/16, 1/8, 1/4, 1/2, 1}; plot final test accuracy vs residual error term to identify optimal trade-off point.
  3. Rolling vs random under heterogeneity gradient: Train on CIFAR-10/100 with L ∈ {2, 5, 10} labels/client; compare convergence curves (loss/accuracy vs rounds) for both algorithms to validate rolling's advantage scales with heterogeneity level.

## Open Questions the Paper Calls Out
None

## Limitations
- **Residual error dominance**: The convergence bound includes a term proportional to Σ_i d(1-p_i), which grows unbounded as masking ratio decreases. While the paper notes this error must remain "controlled," it does not provide explicit thresholds or conditions under which masking improves performance versus degrading it.
- **Model architecture specificity**: Experiments use a modified ResNet18 with static batch normalization and scalar modules. The stability and convergence benefits observed may not extend to other architectures or standard batch normalization without modification.
- **Hyperparameter sensitivity**: Critical training parameters (learning rate, batch size, local steps K, communication rounds) are unspecified, making reproduction and comparative evaluation difficult.

## Confidence
- **High confidence**: Convergence to stationary point of masked objective F_p (Theorem 1, Section 2.1); stability-generalization connection (Section 4, Lemma 1); experimental comparison of rolling vs random masking on CIFAR datasets (Section 5.2-5.3).
- **Medium confidence**: Claims about rolling masking systematically outperforming random masking under high heterogeneity—while supported by figures, the mechanism assumes gradient coverage variance reduction without quantifying when this advantage breaks down.
- **Low confidence**: Generalization improvement claims—the paper shows masking reduces train-test gap in specific CIFAR experiments but does not prove this holds across diverse tasks, model sizes, or data distributions.

## Next Checks
1. **Residual error sweep**: Reproduce experiments varying masking ratio p across multiple orders of magnitude; plot convergence speed and final accuracy against the theoretical residual error term to identify the optimal trade-off point empirically.
2. **Cross-architecture stability**: Test the masking algorithms on standard architectures (e.g., VGG, MobileNet) with conventional batch normalization to verify the stability generalization mechanism extends beyond the modified ResNet18.
3. **Heterogeneity gradient analysis**: Design experiments with controlled gradient similarity across clients (e.g., label shift with controlled feature distribution overlap) to quantify how heterogeneity level modulates the convergence advantage of rolling masking versus random masking.