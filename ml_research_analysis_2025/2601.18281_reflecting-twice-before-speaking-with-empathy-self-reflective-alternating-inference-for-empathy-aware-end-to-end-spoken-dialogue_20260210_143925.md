---
ver: rpa2
title: 'Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating
  Inference for Empathy-Aware End-to-End Spoken Dialogue'
arxiv_id: '2601.18281'
source_url: https://arxiv.org/abs/2601.18281
tags:
- dialogue
- empathetic
- response
- spoken
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enhancing empathetic dialogue
  capabilities in end-to-end spoken language models (SLMs), where current methods
  rely on rigid supervised signals that fail to capture the nuances of empathy. To
  overcome this, the authors propose ReEmpathy, an SLM that incorporates a novel Empathetic
  Self-Reflective Alternating Inference mechanism, interleaving spoken response generation
  with free-form empathetic reflection to enable more contextually appropriate and
  paralinguistic-aware dialogue.
---

# Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue

## Quick Facts
- arXiv ID: 2601.18281
- Source URL: https://arxiv.org/abs/2601.18281
- Reference count: 14
- Primary result: ReEmpathy achieves 2.96 average empathy score vs. 2.79 for GLM-4-Voice baseline

## Executive Summary
This paper addresses the challenge of generating empathetic spoken dialogue by proposing ReEmpathy, an end-to-end spoken language model that interleaves response generation with free-form empathetic reflection. The authors introduce a novel Self-Reflective Alternating Inference mechanism where the model generates spoken responses in alternating chunks paired with reflection tokens that reason about empathetic appropriateness. ReEmpathy is built on EmpathyEval, a descriptive natural-language-based evaluation model that assesses empathy across four dimensions (Need Support, Wording Appropriateness, Emotion Understanding, Emotional Support). Extensive experiments demonstrate that this approach significantly outperforms standard fine-tuning, direct preference optimization, and chain-of-thought reasoning baselines, achieving superior empathy scores while maintaining speech quality.

## Method Summary
The authors construct an evaluation model (EmpathyEval) by fine-tuning Qwen3-Omni-30B-A3B-Captioner on EmotionTalk for speech emotion recognition and captioning, then on a custom empathy evaluation dataset with GPT-4-generated descriptive assessments. ReEmpathy is created by fine-tuning GLM-4-Voice on alternating supervision data generated by EmpathyEval, where response audio tokens are interleaved with reflection text tokens. During inference, the model generates alternating chunks of responses and reflections, with cross-attention between streams amplified by a tunable factor. The system uses LoRA fine-tuning (rank=8, alpha=32) and optimizes chunk size (15-21 tokens) and attention reweighting (1.1-1.2x scaling on middle layers).

## Key Results
- ReEmpathy achieves 2.96 average empathy score vs. 2.79 for GLM-4-Voice baseline on OpenS2S test set
- Outperforms CoTBS (2.91) and other optimization methods in A/B tests with GPT-4 judging
- Ablation shows reflection mechanism critical: disabling reflections drops score to 2.94
- Optimal chunk size: 15-21 tokens; attention reweighting peak: 1.1-1.2x scaling on middle layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Interleaving response generation with free-form reflective reasoning improves empathetic dialogue quality beyond single-pass or pre-reasoning approaches.
- **Mechanism:** The model generates output as alternating fixed-length chunks: response chunks (speech + text) interleaved with reflection chunks (unspoken reasoning tokens). Each reflection conditions on prior response content and informs subsequent generation, creating a feedback loop where empathetic assessment directly shapes ongoing output.
- **Core assumption:** Chunk-level alternation enables meaningful mid-generation course correction; reflections trained via off-policy supervision transfer to on-policy inference.
- **Evidence anchors:**
  - [abstract] "interleaving spoken response generation with free-form, empathy-related reflective reasoning"
  - [section 2.3] Equation (4): (r₁, f₁, r₂, f₂, ..., rₖ, fₖ) = ReEmpathy(xₛₚₑₑcₕ₍q₎) where rₖ denotes response chunks and fₖ denotes reflective chunks
  - [section 4.2] Table 5: ReEmpathy achieves 2.96 avg empathy score vs. CoTBS at 2.91; ablation shows performance drops when reflection disabled (2.94 vs 2.96)
  - [corpus] Related work "Leveraging Chain of Thought" explores CoT for empathy but lacks alternating mechanism; EchoMind benchmark evaluates empathetic SLMs without reflection mechanisms
- **Break condition:** If chunk size too small (<9 tokens), attention fragments and contextual coherence degrades (Figure 6).

### Mechanism 2
- **Claim:** Descriptive natural-language evaluation captures empathy nuances better than scalar scores, enabling richer supervision signals.
- **Mechanism:** EmpathyEval generates structured text assessments analyzing user intent, empathetic appropriateness, and actionable suggestions across four dimensions (Need Support, Wording Appropriateness, Emotion Understanding, Emotional Support). These descriptions become training targets for reflective reasoning.
- **Core assumption:** Text-based evaluation correlates with human judgment of empathy quality; GPT-4 pipeline approximates expert assessment.
- **Evidence anchors:**
  - [abstract] "descriptive natural-language-based evaluation model for assessing empathetic quality"
  - [section 2.1] Stage 3: GPT-4 assesses from four perspectives, provides holistic descriptive assessment with actionable suggestions
  - [section 4.1] Table 4: EmpathyEval achieves LCC=0.71 correlation with human MOS on out-of-domain data; MSE=0.81 on 1-5 scale
  - [corpus] Weak direct corpus evidence for descriptive vs. scalar comparison in SLM domain
- **Break condition:** Evaluation model trained only on Mandarin; cross-lingual generalization untested.

### Mechanism 3
- **Claim:** Amplifying cross-attention between response and reflection streams improves empathetic performance up to an optimal point.
- **Mechanism:** During inference, self-attention weights are scaled: when generating reflection tokens, attention to preceding response increases; when generating response tokens, attention to recent reflection increases. This bidirectional information flow strengthens reflection-response coupling.
- **Core assumption:** Transformer attention can be manually reweighted post-training to enhance specific cross-modal dependencies without retraining.
- **Evidence anchors:**
  - [section 3.2] Figure 4 illustrates attention weight adjustment between responses and reflections
  - [section 4.3] Figure 6: Performance peaks at reWeight ∈ [1.1, 1.2], degrades beyond 1.5
  - [appendix D, Table 9] Middle layers (10-20, 10-30) show most robust improvements
  - [corpus] No direct corpus evidence for attention reweighting in empathetic dialogue systems
- **Break condition:** Excessive reweighting (>1.5x) causes attention over-concentration, reducing performance.

## Foundational Learning

- **Concept: Autoregressive SLM Architecture**
  - **Why needed here:** ReEmpathy builds on GLM-4-Voice, an end-to-end SLM that generates speech tokens autoregressively while decoding auxiliary text tokens without latency overhead.
  - **Quick check question:** Can you explain how an SLM generates both speech and text tokens in a single forward pass?

- **Concept: Off-Policy vs. On-Policy Training**
  - **Why needed here:** The paper constructs off-policy training data using pre-computed EmpathyEval reflections rather than generating reflections during training. This limits chunk-level coupling.
  - **Quick check question:** What is the difference between training on pre-computed reflections vs. generating reflections during the training loop?

- **Concept: LoRA Fine-Tuning**
  - **Why needed here:** EmpathyEval is trained using LoRA adapters (rank=8, alpha=32) applied to all linear modules of Qwen3-Omni-30B-A3B-Captioner.
  - **Quick check question:** What are the memory and expressiveness tradeoffs when using LoRA vs. full fine-tuning for evaluation models?

## Architecture Onboarding

- **Component map:**
  ```
  EmpathyEval Pipeline:
  EmotionTalk (seed audio) -> GPT-4 (story/dialogue/assessment) -> CosyVoice2 (TTS) -> Annotated Dataset D
  Qwen3-Omni-Captioner -> + SER fine-tuning -> + Emotion Captioning fine-tuning -> + EmpathyEval SFT -> EmpathyEval
  EmpathyEval -> + 4 regression heads -> Score-based assessment (ENS, EWA, EEU, EES)

  ReEmpathy Pipeline:
  GLM-4-Voice (base SLM) -> Interleaved training data (Eq. 6) -> SFT -> ReEmpathy
  Inference: User speech -> Alternating (response chunk, reflection chunk) × K -> Concatenated response + internal reflection trace
  ```

- **Critical path:**
  1. Build EmpathyEval first—quality of reflective supervision depends entirely on evaluation quality
  2. Generate off-policy alternating training data using EmpathyEval assessments on OpenS2S corpus
  3. Fine-tune base SLM on interleaved sequences; validate reflection quality matches EmpathyEval outputs
  4. Tune chunk size (optimal range: 15-39 tokens) and attention reweighting factor (1.1-1.2x)

- **Design tradeoffs:**
  - **Chunk size vs. alternation frequency:** Smaller chunks = more frequent reflection but risk attention fragmentation. Paper finds optimal around 15-21 tokens.
  - **Descriptive vs. score-only evaluation:** Descriptions richer but harder to optimize; paper uses both (descriptions for reflection training, scores for quantitative metrics).
  - **Off-policy supervision limitation:** Chunk-level response-reflection interactions cannot be trained on-policy, limiting fine-grained control.

- **Failure signatures:**
  - Empathy scores plateau or drop below SFT baseline -> reflection mechanism likely disabled or chunk coupling broken
  - Reflections diverge from EmpathyEval assessments -> training data quality issue or insufficient SFT epochs
  - Excessive repetition or incoherence -> chunk size too small (<9) or attention reweighting too aggressive (>1.5x)
  - Poor correlation with human judgment -> evaluation model undertrained or domain mismatch

- **First 3 experiments:**
  1. **Baseline comparison:** Replicate Table 5 comparisons (SFT, DPO, CoTBS, ReEmpathy) on OpenS2S test split using EmpathyEval and GPT-4 metrics.
  2. **Chunk size ablation:** Sweep chunk sizes {9, 15, 21, 39} while holding attention weights constant; plot empathy scores and AB_test results.
  3. **Attention reweighting validation:** Test reWeight ∈ {1.0, 1.1, 1.2, 1.5} on middle transformer layers (10-30); verify peak at 1.1-1.2x.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can on-policy supervision at the chunk-level improve the interpretability and fine-grained control of the alternating inference process compared to the current global dialogue-level supervision?
- **Basis in paper:** [explicit] The authors state in the Limitations: "the coupling between response and reflection streams is currently supervised only at the global dialogue level. At the chunk level, the interaction between individual response and reflection segments cannot yet be trained on-policy, which limits the interpretability and fine-grained control of the alternating inference process. Exploring more granular, on-policy supervision for chunk-level interactions represents a valuable direction for future research."
- **Why unresolved:** The current training approach treats the entire response-reflection sequence globally, preventing targeted learning of how specific reflection chunks should modify subsequent response chunks.
- **What evidence would resolve it:** A comparative study showing chunk-level on-policy training leads to: (1) higher correlation between reflection content and subsequent response changes, and (2) improved performance on metrics requiring precise empathetic adjustments.

### Open Question 2
- **Question:** Would reinforcement learning or other advanced optimization strategies for the reflective reasoning stream yield superior empathetic capabilities compared to the current supervised fine-tuning approach?
- **Basis in paper:** [explicit] The Limitations section states: "the current supervision of the reflective reasoning stream is limited to supervised fine-tuning. Further investigation into reinforcement learning or other advanced optimization strategies for reflective reasoning could potentially enhance the model's empathetic capabilities."
- **Why unresolved:** SFT may constrain the reflection space to imitate training examples, whereas RL could explore novel, more effective reflective reasoning patterns through reward signals.
- **What evidence would resolve it:** Demonstrating that an RL-based reflection optimization method (e.g., using EmpathyEval scores as rewards) achieves statistically significant improvements in empathy metrics over the SFT baseline.

### Open Question 3
- **Question:** To what extent does the self-reflective alternating inference mechanism generalize across languages and cultural contexts beyond Mandarin Chinese?
- **Basis in paper:** [inferred] The dataset construction, evaluation, and experiments are exclusively conducted on Mandarin Chinese data (EmotionTalk, OpenS2S Chinese subset). The paper does not discuss cross-linguistic or cross-cultural validity, despite empathy expression being culturally contingent.
- **Why unresolved:** Empathy norms, paralinguistic cues (e.g., tone, prosody patterns), and appropriate empathetic responses vary significantly across cultures; it is unclear if the learned reflection patterns transfer.
- **What evidence would resolve it:** Multi-lingual experiments showing ReEmpathy maintains performance advantages over baselines on English, Japanese, or other languages with culturally-adapted evaluation datasets.

## Limitations

- Off-policy training means ReEmpathy never learns to dynamically adjust reflections based on its own response quality during training, creating a potential generalization gap.
- The chunk-based alternation mechanism's optimal parameters (15-21 tokens) are tied to specific tokenization schemes and may not transfer across languages or domains.
- Descriptive natural-language evaluation's superiority over scalar scores is supported only by weak correlation evidence (LCC=0.71) without direct comparison during training.

## Confidence

**High Confidence:** The claim that ReEmpathy outperforms baselines (SFT, DPO, CoTBS) on empathy metrics is supported by quantitative comparisons in Table 5. The ablation showing degraded performance when reflection is disabled (2.94 vs 2.96) provides additional validation. However, all metrics derive from the authors' own EmpathyEval model, which may introduce evaluation bias.

**Medium Confidence:** The attention reweighting mechanism (1.1-1.2x scaling on middle layers) shows clear performance peaks in Figure 6, but the underlying assumption—that manually reweighting attention post-training meaningfully enhances cross-modal coupling—lacks theoretical grounding. The paper doesn't explain why this specific range works or what attention patterns are being amplified.

**Low Confidence:** The claim that descriptive natural-language evaluation captures empathy nuances better than scalar scores is supported only by weak correlation evidence (LCC=0.71). The paper doesn't directly compare descriptive vs. scalar supervision during training, nor does it validate whether the rich descriptive assessments actually improve reflection quality versus simpler score-based targets.

## Next Checks

1. **Cross-lingual evaluation:** Test ReEmpathy on non-Mandarin datasets (e.g., EmpatheticDialogues, DailyDialog) to verify whether the reflection mechanism generalizes beyond the training language. Measure performance drop and identify whether failures stem from language differences or domain shift.

2. **On-policy vs. off-policy comparison:** Implement an on-policy variant where reflections are generated during training (not pre-computed) and compare empathy scores. This would reveal whether the off-policy limitation meaningfully constrains the model's ability to learn responsive reflection patterns.

3. **Chunk independence analysis:** Systematically vary chunk size below 15 tokens (e.g., 9, 12) while holding other factors constant to precisely map the performance degradation curve. Additionally, test whether alternating inference is necessary by comparing against a variant that generates all reflections first, then all responses.