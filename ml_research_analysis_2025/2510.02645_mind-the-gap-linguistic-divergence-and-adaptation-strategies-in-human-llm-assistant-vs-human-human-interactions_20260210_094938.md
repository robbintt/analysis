---
ver: rpa2
title: 'Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM
  Assistant vs. Human-Human Interactions'
arxiv_id: '2510.02645'
source_url: https://arxiv.org/abs/2510.02645
tags:
- user
- style
- human-human
- message
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study identifies a linguistic mismatch between human-human
  and human-LLM assistant interactions, where users adopt terser, less polite, and
  grammatically simpler language when interacting with chatbots. To address this,
  the authors propose post-training data augmentation by generating synthetic user
  queries across diverse linguistic styles, ranging from minimal to enriched, using
  a controlled rewriting strategy.
---

# Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions

## Quick Facts
- arXiv ID: 2510.02645
- Source URL: https://arxiv.org/abs/2510.02645
- Reference count: 19
- Primary result: Training-time style augmentation achieves +2.9% relative improvement over baseline; inference-time rewriting degrades performance by -1.9%

## Executive Summary
This paper identifies a systematic linguistic mismatch between human-human and human-LLM assistant interactions, where users adopt terser, less polite, and grammatically simpler language when interacting with chatbots. To address this distribution shift, the authors propose post-training data augmentation by generating synthetic user queries across diverse linguistic styles using controlled rewriting. Experiments on intent detection demonstrate that models trained on stylistically diverse datasets outperform baseline by +2.9% relative improvement, while inference-time query reformulation underperforms with a -1.9% drop. The results show that training-time exposure to linguistic variety is more effective than post-hoc input normalization for handling style shifts in human-LLM interactions.

## Method Summary
The approach involves collecting 13K human-human training utterances (first user turn only), scoring each on grammar, politeness, and lexical diversity using a rubric prompt, then generating minimal-style (terse/informal) and enriched-style (formal) rewrites using controlled prompts with Claude 3.5 Sonnet v2. The combined dataset (D4: original + minimal + enriched) is used to fine-tune Mistral-7B with LoRA for generative intent classification. Evaluation is performed on a held-out human-LLM test set (1,357 utterances) using exact-match accuracy. An alternative inference-time rewriting approach using the same rubric and rewrite model is also tested but shows degraded performance.

## Key Results
- Models trained on stylistically diverse datasets achieve +2.9% relative improvement over baseline on intent detection
- Inference-time query reformulation underperforms with a -1.9% drop in accuracy
- The combined dataset (D4: original + minimal + enriched) achieves the best performance, demonstrating that exposure to diverse linguistic styles enhances generalization
- Users produce significantly more terse, grammatically degraded, and less polite language when interacting with LLMs compared to human-human interactions (p<0.05 for grammar, politeness, lexical diversity)

## Why This Works (Mechanism)

### Mechanism 1: Stylistic Diversity Expands Decision Boundaries
Training on stylistically varied synthetic data improves generalization to real-world human-LLM inputs by exposing the model to a broader input manifold during fine-tuning. This creates decision boundaries that are robust to surface-level variation while preserving semantic intent classification. The model learns invariant features across style perturbations rather than overfitting to a narrow linguistic register. Performance gains may not hold if synthetic rewrites introduce semantic drift or fail to cover actual human-LLM input distribution.

### Mechanism 2: Inference-Time Normalization Risks Semantic Distortion
Rewriting user inputs to match training-distribution style at inference time degrades intent detection accuracy because the reformulation process can alter or obscure subtle intent-relevant cues present in the original terse phrasing. The rewrite model may over-regularize, introducing artifacts that the intent classifier was not trained on. This negative effect could potentially be mitigated if the rewrite model were constrained to strictly paraphrase without altering token-level features used by the classifier.

### Mechanism 3: User Mental Models Drive Linguistic Adaptation
Users adjust their communication style toward LLMs based on perceived lower social sensitivity and interpretive capacity, following Communication Accommodation Theory. When users perceive the agent as non-human, they reduce social-signaling behaviors (politeness markers, formal grammar, varied vocabulary), producing terse, direct, and grammatically simplified utterances. This systematic style shift occurs without changing informativeness or emotional content. The stylistic gap may narrow if LLM interfaces explicitly prompt users toward more formal input or if users anthropomorphize the system heavily.

## Foundational Learning

- **Distribution Shift in NLP**: Why needed: The core problem is a mismatch between training distribution (human-human polished queries) and deployment distribution (human-LLM terse queries). Quick check: Can you explain why a model trained on formal text might fail on informal text even if the underlying intent is identical?

- **Data Augmentation via Controlled Rewriting**: Why needed: The paper's primary intervention uses LLM-based rewriting to generate synthetic training examples with specified attribute scores. Quick check: What constraints must a rewriting prompt enforce to ensure synthetic data remains label-consistent?

- **Fine-Grained Linguistic Evaluation Rubrics**: Why needed: The study quantifies style differences using a 6-dimension rubric to enable precise diagnosis of where models fail. Quick check: Why is it important that informativeness and emotion showed no significant difference between human-human and human-LLM interactions?

## Architecture Onboarding

- **Component map**: Human-human transcripts (13K utterances) → Rubric scoring (grammar/politeness/lexical diversity) → LLM-based controlled rewriting (Claude 3.5 Sonnet v2) → Three style variants (original, minimal, enriched) → Combined dataset (D4) → Mistral-7B fine-tuning with LoRA → Intent classifier → Held-out human-LLM test set (1,357 utterances)

- **Critical path**: 1) Collect and preprocess human-human training data (first user turn only, exclude greetings) 2) Score each message on three dimensions using rubric prompt 3) Generate minimal-style and enriched-style rewrites using controlled prompts 4) Validate rewrite quality by re-scoring synthetic samples 5) Fine-tune intent classifier on combined dataset 6) Evaluate on held-out human-LLM test set

- **Design tradeoffs**: Minimal-only (D2) vs enriched-only (D3) vs combined (D4) - combined is best; narrow style ranges hurt generalization. Training-time augmentation vs inference-time reformulation - augmentation is effective, reformulation introduces semantic risk. Assumption: six-dimension rubric captures relevant style axes.

- **Failure signatures**: Model trained on enriched-only data underperforms on terse inputs (overfitting to formal register). Inference-time rewrites produce grammatically correct but intent-obscuring paraphrases (rewrite model not sufficiently constrained). No improvement despite augmentation (synthetic style distribution doesn't match real human-LLM inputs).

- **First 3 experiments**: 1) Replicate combined dataset experiment (D4) on your own intent taxonomy to confirm +2.9% improvement 2) Ablate rubric dimensions - train with augmentation targeting only grammar, only politeness, or only lexical diversity 3) Analyze failure cases of inference-time reformulation on 100 rewritten queries - manually inspect semantic preservation and correlate with classification errors

## Open Questions the Paper Calls Out

- **Cross-task generalizability**: Does stylistic diversity in training data improve performance on generative tasks like response generation, or is it limited to classification tasks like intent detection? The experimental framework focuses exclusively on intent understanding, leaving impact on other NLP tasks unverified. Evidence needed: Evaluation of models trained on Combined dataset using response quality metrics in generative setting.

- **Multi-turn dialogue dynamics**: How does the linguistic style shift evolve and impact model performance over the course of multi-turn dialogues? The current study isolates initial user turn to avoid contamination from system policies, creating gap in understanding longitudinal interaction dynamics. Evidence needed: Study analyzing linguistic dimensions and model accuracy across all turns in multi-turn conversation history.

- **Inference-time reformulation optimization**: Can inference-time reformulation be optimized to preserve intent-critical cues, or is training-time augmentation universally superior? The study tests only one specific method of inference-time rewriting which failed, leaving potential of non-training interventions uncertain. Evidence needed: Comparative experiments using strict semantic preservation constraints during rewriting process.

## Limitations

- Findings based on single intent detection task using specific dataset composition and LLM model - may not generalize to other NLP tasks, different LLM sizes, or alternative training data sources
- Study does not address whether stylistic mismatch is universal across all human-LLM interaction domains or whether certain user populations exhibit different adaptation patterns
- Controlled rewriting approach relies on quality and coverage of rubric-based scoring system which may not capture all relevant linguistic dimensions

## Confidence

**High Confidence**: Users produce terser, less polite, and grammatically simpler language when interacting with LLMs vs human-human interactions (supported by statistically significant differences across multiple dimensions with strong theoretical grounding)

**Medium Confidence**: Training-time style augmentation is more effective than inference-time rewriting for handling linguistic distribution shift (experimental results clear but don't explore intermediate approaches or test rewrite model quality effects)

**Medium Confidence**: Stylistic gap is primarily due to user mental models rather than changes in informativeness, clarity, or emotional content (shows no significant differences in these dimensions but doesn't rule out other unmeasured factors)

## Next Checks

1. **Ablation of style dimensions**: Replicate augmentation experiments while varying only one rubric dimension at a time to determine which linguistic feature most strongly influences model robustness to distribution shift

2. **Cross-task generalizability**: Apply same augmentation strategy to different NLP task (e.g., sentiment analysis, question answering) using same human-human and human-LLM datasets to test whether +2.9% improvement pattern holds across task types

3. **Rewrite model sensitivity analysis**: Systematically vary rubric scorer thresholds and rewrite model parameters to identify boundary conditions under which inference-time rewriting degrades performance, and whether constrained paraphrasing can preserve semantic intent while normalizing style