---
ver: rpa2
title: 'SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document
  Understanding'
arxiv_id: '2510.26615'
source_url: https://arxiv.org/abs/2510.26615
tags:
- slide
- slideagent
- page
- visual
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SlideAgent introduces a hierarchical, multi-agent framework for\
  \ fine-grained reasoning over multi-page visual documents such as slide decks. It\
  \ decomposes document understanding into three levels\u2014global, page, and element\u2014\
  each handled by specialized agents that generate query-agnostic knowledge."
---

# SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding
## Quick Facts
- arXiv ID: 2510.26615
- Source URL: https://arxiv.org/abs/2510.26615
- Reference count: 40
- SlideAgent improves over both proprietary (+7.9 overall, +8.3 numeric) and open-source models (+9.8 overall, +11.7 numeric) on multi-page visual document understanding.

## Executive Summary
SlideAgent introduces a hierarchical, multi-agent framework designed for fine-grained reasoning over multi-page visual documents such as slide decks. It breaks down document understanding into three levels—global, page, and element—each handled by specialized agents that generate query-agnostic knowledge. During inference, agents are selectively activated and retrieve query-specific content for synthesis. Experiments on SlideVQA, TechSlides, and FinSlides demonstrate SlideAgent's superiority over both proprietary and open-source baselines, particularly in numeric and multi-hop reasoning tasks. The framework also enhances page-level retrieval performance, especially with text-based retrievers, and an ablation study confirms the critical role of page and element-level reasoning.

## Method Summary
SlideAgent employs a hierarchical, multi-agent architecture to reason over multi-page visual documents. The framework decomposes document understanding into global, page, and element levels, with specialized agents at each tier generating query-agnostic knowledge graphs. During inference, agents are selectively activated and retrieve query-specific content for synthesis. This design enables fine-grained reasoning and supports complex, multi-hop queries. The system leverages visual-language models for multimodal parsing and constructs structured knowledge graphs for each document level. Agents are activated based on query complexity, with retrieval mechanisms supporting both semantic and vector-based search. The approach is evaluated on three slide deck datasets, demonstrating strong performance gains over existing models.

## Key Results
- SlideAgent improves over proprietary models by +7.9 overall and +8.3 on numeric reasoning.
- SlideAgent outperforms open-source models by +9.8 overall and +11.7 on numeric reasoning.
- The framework excels in multi-hop and visual reasoning tasks, with significant gains in page-level retrieval performance.

## Why This Works (Mechanism)
The hierarchical agentic framework works by decomposing complex document understanding tasks into manageable sub-tasks at global, page, and element levels. Each agent specializes in reasoning over its respective scope, generating structured, query-agnostic knowledge. This modular decomposition allows for targeted retrieval and synthesis, enabling fine-grained reasoning over complex, multi-page documents. The selective activation of agents during inference ensures computational efficiency, while the hierarchical structure supports multi-hop reasoning by linking information across document levels.

## Foundational Learning
- **Hierarchical decomposition**: Why needed: Enables scalable reasoning over large, complex documents by breaking tasks into manageable sub-tasks. Quick check: Can each agent independently reason over its assigned scope?
- **Query-agnostic knowledge generation**: Why needed: Allows pre-computation of document understanding, improving inference efficiency. Quick check: Is the knowledge graph reusable across multiple queries?
- **Selective agent activation**: Why needed: Optimizes computational resources by activating only relevant agents per query. Quick check: Does activation strategy adapt to query complexity?
- **Multi-hop reasoning**: Why needed: Supports complex queries requiring information synthesis across document sections. Quick check: Can the system answer questions requiring cross-page or cross-element inference?
- **Knowledge graph construction**: Why needed: Provides structured, searchable representation of document content. Quick check: Are entity relationships and attributes accurately captured?
- **Vector-based retrieval**: Why needed: Enables efficient, semantic search over large knowledge bases. Quick check: Does retrieval accuracy degrade with document size?

## Architecture Onboarding
- **Component map**: Global Agent -> Page Agents (one per page) -> Element Agents (one per element)
- **Critical path**: Document parsing → Hierarchical knowledge graph construction → Agent activation (based on query) → Retrieval and synthesis → Answer generation
- **Design tradeoffs**: Hierarchical decomposition trades off some global coherence for modularity and scalability; selective agent activation balances efficiency with completeness.
- **Failure signatures**: Poor OCR or visual parsing → noisy or incomplete knowledge graphs → retrieval failures → incorrect or incomplete answers; over-activation of agents → inefficiency; under-activation → missed context.
- **3 first experiments**:
  1. Validate knowledge graph accuracy by manually inspecting a sample of generated graphs for correctness and completeness.
  2. Test agent activation logic by running queries of varying complexity and verifying which agents are triggered.
  3. Benchmark retrieval accuracy for each agent type using held-out queries with known answers.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to slide deck datasets; generalizability to other visual document types (e.g., forms, reports, invoices) is unclear.
- Claims of superior visual reasoning are based on aggregate metrics; finer-grained breakdowns by question type and visual complexity are lacking.
- The marginal benefit of global-level guidance is less clear, suggesting potential redundancy in the hierarchy for certain document types.

## Confidence
- Overall performance claims: Medium
- Ablation study conclusions: Medium
- Visual reasoning superiority: Low

## Next Checks
1. Test SlideAgent on a broader range of visual document types (e.g., forms, reports, invoices) to assess generalizability.
2. Perform an ablation study isolating the contribution of each agent type across different visual reasoning sub-tasks (e.g., spatial, relational, numeric).
3. Evaluate scalability and robustness by introducing synthetic noise in OCR and visual parsing, measuring degradation in performance and retrieval accuracy.