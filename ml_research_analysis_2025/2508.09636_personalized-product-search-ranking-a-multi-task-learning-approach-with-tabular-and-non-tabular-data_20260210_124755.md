---
ver: rpa2
title: 'Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular
  and Non-Tabular Data'
arxiv_id: '2508.09636'
source_url: https://arxiv.org/abs/2508.09636
tags:
- product
- search
- data
- relevance
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-task learning framework for personalized
  product search ranking that integrates both tabular and non-tabular data. The approach
  uses a pre-trained TinyBERT model for semantic embeddings combined with traditional
  tabular features, employing an MMoE architecture as the shared bottom layer.
---

# Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data

## Quick Facts
- arXiv ID: 2508.09636
- Source URL: https://arxiv.org/abs/2508.09636
- Authors: Lalitesh Morishetti; Abhay Kumar; Jonathan Scott; Kaushiki Nag; Gunjan Sharma; Shanu Vashishtha; Rahul Sridhar; Rohit Chatter; Kannan Achan
- Reference count: 33
- Primary result: Proposed MMoE-based multi-task learning framework achieves MRR@1 scores of 0.324 (click), 0.373 (add-to-cart), and 0.391 (transaction) on product search ranking.

## Executive Summary
This paper presents a multi-task learning framework for personalized product search ranking that integrates both tabular and non-tabular data. The approach uses a pre-trained TinyBERT model for semantic embeddings combined with traditional tabular features, employing an MMoE architecture as the shared bottom layer. A key innovation is a scalable relevance labeling mechanism based on click-through rates, click positions, and semantic similarity scores, offering an alternative to human-annotated labels. Experimental results demonstrate significant improvements over baseline models, with the proposed framework achieving MRR@1 scores of 0.324 for click prediction, 0.373 for add-to-cart prediction, and 0.391 for transaction prediction. The model also shows a Personalization Degree (PD@10) score of 0.87, indicating effective personalization in search results.

## Method Summary
The proposed method employs a multi-task learning framework with three primary tasks: predicting clicks, add-to-cart actions, and transactions. The architecture combines TinyBERT-based semantic embeddings for query-product pairs with traditional tabular features through an element-wise multiplication matching layer. A DCN-V2 or FT-Transformer serves as the shared bottom layer, feeding into an MMoE architecture that routes representations to task-specific towers. The model is trained using a weighted sum of binary cross-entropy losses for the three classification tasks, with task weights tuned via grid search. An optional fourth relevance labeling task uses synthetic labels generated from a voting mechanism combining CTR, position, and semantic similarity scores.

## Key Results
- MRR@1 scores: 0.324 (click prediction), 0.373 (add-to-cart prediction), 0.391 (transaction prediction)
- Personalization Degree (PD@10) score of 0.87, indicating effective personalization in search results
- Element-wise multiplication of embeddings outperforms dot product by +0.91% (click), +0.53% (ATC), and +0.77% (TRX)
- Adding relevance task improves Click (+1.22%) and Transaction (+0.86%) MRR

## Why This Works (Mechanism)

### Mechanism 1
Element-wise multiplication (cross-product) of query and product embeddings likely captures richer interaction features than simple dot products. The architecture generates a matching vector by performing element-wise multiplication on the max-pooled outputs of fine-tuned TinyBERT query and product embeddings. This preserves dimension-wise feature interactions rather than collapsing them into a single scalar. Evidence shows cross product outperforming dot product across all three tasks in MRR scores.

### Mechanism 2
Multi-gate Mixture-of-Experts (MMoE) appears to mitigate negative transfer between conflicting tasks (e.g., predicting clicks vs. purchases). The model uses a shared bottom layer feeding into MMoE, where gating networks learn to route specific "expert" representations to different towers (Click, ATC, Transaction). This allows the model to optimize for tasks with different convergence patterns. MMoE variants generally outperform single-task baselines in MRR scores.

### Mechanism 3
Synthetic relevance labels derived from click feedback and semantic similarity may serve as an effective auxiliary signal to improve primary task performance. A fourth task is introduced using a "Relevance Label" generated via a voting scheme (weighted CTR + position + semantic score). This forces the shared layers to learn a representation that respects topical relevance in addition to user engagement actions. Adding the relevance task improves Click and Transaction MRR scores.

## Foundational Learning

- **Concept: Multi-gate Mixture of Experts (MMoE)**
  - Why needed here: The model predicts three distinct user behaviors (Click, Add-to-Cart, Purchase) which often have different underlying distributions. MMoE allows the model to learn separate "experts" for these tasks while sharing underlying computational layers.
  - Quick check question: Can you explain how the gating network G_t(x) decides which experts to use for a "Purchase" prediction vs. a "Click" prediction?

- **Concept: DCN-V2 (Deep & Cross Network)**
  - Why needed here: This serves as the "Shared Bottom" layer. It is crucial for efficiently modeling explicit feature interactions (e.g., "User X" + "Category Y") via the cross layer, while the deep layer captures implicit non-linearities.
  - Quick check question: In Equation 6 (f_cross), how does the structure of the cross layer differ from a standard fully connected layer in terms of feature interaction?

- **Concept: Knowledge Distillation / TinyBERT**
  - Why needed here: The paper leverages TinyBERT to process raw text (queries and product titles) into dense vectors. Understanding how this distilled model retains semantic knowledge with lower latency is key for the "Matching Layer."
  - Quick check question: Why might MaxPooling be used on the TinyBERT output (Equation 3) before the element-wise multiplication, rather than using the [CLS] token?

## Architecture Onboarding

- **Component map:** Tabular/Categorical Embeddings + Numerical Features → TinyBERT Query/Title Embeddings → MaxPooling → Element-wise Multiplication → Concatenation → DCN-V2/FT-Transformer (Shared Bottom) → MMoE Gating → 3 Towers (Click, ATC, TRX) + optional Relevance Tower

- **Critical path:**
  1. Text → TinyBERT → MaxPool → Element-wise Product (Interaction Vector)
  2. Concatenate(Interaction Vector, Tabular Embeddings) → DCN-V2 (Cross & Deep Layers)
  3. DCN-V2 Output → MMoE Experts → Gated selection per Tower → Prediction

- **Design tradeoffs:**
  - Latency vs. Accuracy: The paper achieves higher MRR using TinyBERT + FT-Transformer, but this increases parameters to ~70M. The DCN-V2 variant is slightly faster but may have lower capacity for complex feature crossings.
  - Label Noise: Using the automated "Relevance Label" (Section 3.5) scales training but relies on the assumption that CTR + Semantic Score ≈ Ground Truth.

- **Failure signatures:**
  - Low PD@10 Score: If the Personalization Degree is high (close to 1.0), user features are being ignored, and the model is relying only on query-product popularity.
  - Click/TRX Divergence: If Click MRR is high but Transaction MRR is low, the gating mechanism may be failing to route "purchase-intent" experts effectively.

- **First 3 experiments:**
  1. Baseline Verification: Train a single-task XGBoost or DCN-V2 on "Add-to-Cart" only to establish a performance floor (MRR@1).
  2. Interaction Ablation: Swap the element-wise multiplication (Cross Layer) with a simple Dot Product in the TinyBERT matching layer to quantify the lift from rich interaction features.
  3. Sampling Validation: Train with and without the stratified sampling strategy (Section 3.4) to verify that the model isn't just memorizing popular items in the head of the distribution.

## Open Questions the Paper Calls Out
- Can incorporating Large Language Models (LLMs) as voting components in the relevance labeling mechanism improve ranking accuracy compared to the current automated approach?
- Does extending the architecture to include dynamic, session-based user data yield significant improvements in personalization over the current static features?
- Does a lower Personalization Degree (PD) score, which indicates higher variance in user rankings, correlate directly with improved user satisfaction or business metrics?

## Limitations
- The paper's reliance on synthetic relevance labels generated from click-through rates and semantic similarity scores introduces uncertainty about the quality of these signals without human-annotated ground truth for validation.
- Specific feature schema and hyperparameter configurations for both the DCN-V2/FT-Transformer backbone and the MMoE architecture are not fully specified, making exact reproduction challenging.
- The effectiveness of the semantic similarity component depends heavily on the quality of the domain-adapted GTE model, which is not detailed in the paper.

## Confidence
- **High Confidence**: The core multi-task learning framework with MMoE architecture is well-established and the reported improvements over baseline models are consistent with known benefits of MTL in ranking tasks.
- **Medium Confidence**: The specific implementation details appear sound, but exact hyperparameter values are missing, making it difficult to assess whether the reported performance is optimal or merely representative.
- **Low Confidence**: The synthetic relevance labeling mechanism's effectiveness is the least certain claim, as it relies on assumptions about the correlation between CTR+semantic score and true relevance that aren't empirically validated against human annotations.

## Next Checks
1. Obtain a small sample of human-annotated relevance judgments for a subset of query-product pairs to directly compare against the synthetic labels and validate whether the voting mechanism produces reasonable ground truth.
2. Systematically remove or replace key components (TinyBERT matching layer, MMoE gating, relevance task) to quantify their individual contributions and ensure the reported gains aren't from confounding factors like increased model capacity.
3. Evaluate the model's semantic similarity component on out-of-distribution queries or products to assess whether the GTE model's domain adaptation generalizes or overfits to the training distribution.