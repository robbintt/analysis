---
ver: rpa2
title: 'KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning'
arxiv_id: '2512.19605'
source_url: https://arxiv.org/abs/2512.19605
tags:
- kernel
- gaussian
- sliced
- which
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KerJEPA, a framework extending self-supervised\
  \ learning with kernel-based regularizers beyond the Gaussian-MMD setup of LeJEPA.\
  \ It formalizes MMD and KSD regularization\u2014sliced and unsliced\u2014across\
  \ various kernels (Gaussian, IMQ) and priors (Gaussian, Laplace, Student-t), enabling\
  \ practitioners to target different downstream geometries."
---

# KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning

## Quick Facts
- **arXiv ID**: 2512.19605
- **Source URL**: https://arxiv.org/abs/2512.19605
- **Reference count**: 40
- **Primary result**: KerJEPA extends self-supervised learning with kernel-based regularizers beyond Gaussian-MMD, showing sliced methods suffer convergence slowdown in high dimensions while unsliced methods avoid this issue.

## Executive Summary
KerJEPA introduces a kernel discrepancy framework for Euclidean self-supervised learning that extends beyond the Gaussian-MMD setup of LeJEPA. The paper formalizes MMD and KSD regularization—both sliced and unsliced—across various kernels (Gaussian, IMQ) and priors (Gaussian, Laplace, Student-t), enabling practitioners to target different downstream geometries. By deriving closed-form estimators and analyzing computational tradeoffs, the authors demonstrate that unsliced methods match or exceed sliced performance while avoiding sampling variance. Experiments on ImageNette show stable, high accuracy across configurations, with sliced methods suffering convergence slowdown and instability when slices are insufficient relative to embedding dimension.

## Method Summary
KerJEPA reformulates SSL regularization using kernel discrepancies (MMD/KSD) between embedding distributions and target priors. The framework supports both sliced (averaged over random projections) and unsliced (pairwise) variants, with Gaussian and IMQ kernels. Sliced MMD uses quadrature approximation while sliced KSD leverages closed-form Stein kernels. The approach generalizes LeJEPA by enabling non-Gaussian priors and kernels, with analytical slicing for MMD and exact computation for KSD.

## Key Results
- Sliced methods suffer convergence slowdown and instability when slice count is insufficient relative to embedding dimension
- Unsliced methods avoid slicing variance while matching or exceeding sliced performance
- KSD enables exact matching to arbitrary priors via score functions without sampling variance
- IMQ kernel provides polynomially decaying tails versus Gaussian's exponential tails
- Laplace and Student-t priors work well with KSD but poorly with sliced MMD quadrature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel discrepancy regularization prevents representation collapse by enforcing distributional structure on embeddings without requiring negative samples.
- Mechanism: The SSL loss combines alignment (MSE between positive pairs) with regularization Ω(Z;Q). The regularization term measures discrepancy between the empirical embedding distribution and a target prior Q using kernel-based metrics (MMD or KSD). Since the discrepancy is zero only when distributions match exactly, the trivial solution of constant embeddings is penalized.
- Core assumption: The target prior Q is chosen such that matching it yields useful representations for downstream tasks (the paper notes isotropic Gaussian is optimal for ℓ₂-regularized downstream tasks per LeJEPA).
- Evidence anchors:
  - [section 4] "Regularization plays a critical role in these frameworks, as alignment objectives alone admit pathological collapse solutions."
  - [abstract] "regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization"
  - [corpus] Limited direct corpus support; neighbors focus on GP kernels rather than SSL regularization mechanics.
- Break condition: If regularization weight λ is too low, collapse occurs; if too high, embeddings ignore data structure and match prior trivially.

### Mechanism 2
- Claim: Slicing transforms the effective kernel geometry from exponential-tailed to polynomial-tailed, which changes how similarity is measured in latent space.
- Mechanism: The slicing operation averages a univariate metric over uniformly random projection directions. Theorem 7 shows this is equivalent to MMD with kernel κ_d(x,y) = ₁F₁(1/2; d/2; -γ‖x-y‖²₂), which has polynomially decaying tails rather than the exponentially decaying tails of the original Gaussian kernel. This dimension-dependent kernel effectively decreases bandwidth as dimension increases.
- Core assumption: The Cramér-Wold theorem holds (projecting onto all directions preserves distributional identity), and the approximation via finite random directions is sufficient.
- Evidence anchors:
  - [section 3.5, Theorem 7] "κ_d has polynomially decaying tails (as opposed to exponentially decaying tails like k_gsn)"
  - [section 6] "slicing—particularly their heavy-tailed and dimension-dependent nature—allows us to stabilize training"
  - [corpus] No corpus papers discuss slicing-induced kernel geometry transformation.
- Break condition: When embedding dimension d increases but slice count remains fixed, variance grows and convergence slows (Figure 2 shows this explicitly).

### Mechanism 3
- Claim: KSD enables matching to arbitrary priors via score functions without sampling from the prior, reducing variance compared to MMD with non-Gaussian priors.
- Mechanism: The Stein kernel (Eq. 10) incorporates the target distribution Q only through its score function s_Q(x) = ∇log Q(x). Stein's identity ensures expectations under Q vanish, so the discrepancy depends only on samples from P. For Gaussian, Laplace, and Student-t priors, the score is analytically tractable (Table 2), enabling closed-form computation.
- Core assumption: The score function s_Q is computable and the base kernel is characteristic for the Stein discrepancy to metrize weak convergence.
- Evidence anchors:
  - [section 3.3] "KSD depends only on the score function s_Q(x)...allows us to match the target geometry exactly through closed-form gradients without the variance induced by target sampling"
  - [section 5, Table 1] Laplace prior with sliced MMD: 90.25% vs unsliced KSD: 91.18%—quadrature approximation underperforms exact KSD
  - [corpus] Corpus lacks KSD-specific SSL papers; related work on kernel SSL exists (Kernel VICReg) but doesn't address Stein methods.
- Break condition: For priors without tractable scores (or where score is numerically unstable), KSD becomes impractical.

## Foundational Learning

- **Concept: Maximum Mean Discrepancy (MMD)**
  - Why needed here: MMD is the core metric for measuring distributional distance between embeddings and priors. Understanding that MMD embeds distributions into RKHS and compares them via kernel evaluations is essential for grasping why kernel choice matters.
  - Quick check question: Given two distributions P and Q, what does MMD²_k(P,Q) = 0 imply about the relationship between P and Q when k is characteristic?

- **Concept: Stein's Method and Score Functions**
  - Why needed here: KSD leverages Stein operators to transform any distribution into one with zero expectation under Q. The score function (∇log Q) is the mechanism by which target geometry is incorporated without sampling.
  - Quick check question: For a Gaussian N(0, σ²I_d), what is the score function s_Q(x), and why does it enable closed-form KSD computation?

- **Concept: Cramér-Wold Device and Slicing**
  - Why needed here: Slicing reduces multivariate distribution comparison to averaging univariate comparisons. This is both the computational benefit and the source of variance issues in high dimensions.
  - Quick check question: How many random projection directions would you need to reliably estimate a sliced divergence in d=1024 dimensions, and what happens if you use too few?

## Architecture Onboarding

- **Component map**:
  - **Backbone encoder** f^back_θ: Processes input views (e.g., ViT-S/8), outputs representation
  - **Projector head** f^proj_θ: Non-linear projection to embedding space R^d (d=128 in experiments)
  - **Regularization module**: Computes Ω(Z;Q) via one of:
    - Sliced MMDReg (quadrature-based, O(nr(d+u)) complexity)
    - Sliced KSDReg (quadrature-based, O(nr(d+u)) complexity)
    - Unsliced MMDReg (pairwise kernels, O(n²d) complexity)
    - Unsliced KSDReg (pairwise Stein kernels, O(n²d) complexity)

- **Critical path**:
  1. Initialize backbone+projector (standard pretrained or random)
  2. Forward pass: generate m views per sample (m=4 in paper), encode to embeddings z
  3. Compute alignment loss (MSE between positive pairs)
  4. Compute regularization loss via chosen discrepancy
  5. Combine: L = L_align + λ·Ω(Z;Q); backprop

- **Design tradeoffs**:
  - **Sliced vs. Unsliced**: Sliced is O(n) but introduces variance from finite projections; unsliced is O(n²) but exact (modulo empirical sampling)
  - **Kernel choice**: Gaussian (exponential tails, local structure) vs. IMQ (polynomial tails, long-range gradients)
  - **Prior choice**: Gaussian (optimal for ℓ₂ downstream), Laplace (heavier tails), Student-t (robust to outliers)
  - **Dimension**: Higher d requires more slices for stability (Figure 2); d≥20 needed for IMQ approximation of ₁F₁

- **Failure signatures**:
  - **Collapse**: λ too low → embeddings converge to constant; check embedding variance
  - **Slow convergence**: Insufficient slices for dimension; seen as "instability in early half of training" (Figure 2)
  - **Quadrature error**: For non-Gaussian priors with sliced MMD, approximations underperform; use KSD instead
  - **Numerical instability**: ₁F₁ evaluations expensive/unstable; use IMQ approximation for d≥20

- **First 3 experiments**:
  1. **Baseline parity**: Reproduce LeJEPA (sliced MMD, Gaussian kernel+prior, 1024 slices, 21 knots) on ImageNette with ViT-S/8 to verify ~91% accuracy baseline.
  2. **Ablation on slicing**: Compare finite-sliced (16/128/1024 slices) vs. analytically-sliced vs. unsliced MMD at d∈{16,128,1024} across 100/300/800 epochs. Confirm Figure 2 pattern: insufficient slices → slower convergence in higher dimensions.
  3. **KSD prior flexibility**: Test unsliced KSD with Gaussian vs. Laplace vs. Student-t priors. Hypothesis: performance should be similar (~91%) if prior geometry is secondary to regularization dynamics, but observe training curves for stability differences.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to single dataset (ImageNette) and one backbone architecture (ViT-S/8), raising generalizability questions
- Analysis of slicing variance provides suggestive evidence but lacks formal convergence bounds or sample complexity guarantees
- KSD's advantage over MMD for non-Gaussian priors is numerically demonstrated but theoretical justification for downstream performance guarantees is incomplete

## Confidence
- **High confidence**: The mechanism of kernel discrepancy regularization preventing collapse (Mechanism 1) is well-supported by SSL theory and the paper's analysis of MMD/KSD properties.
- **Medium confidence**: The slicing-induced kernel geometry transformation (Mechanism 2) is mathematically rigorous (Theorem 7) but the practical implications for downstream task performance are only empirically demonstrated on one dataset.
- **Medium confidence**: The KSD variance reduction claim (Mechanism 3) is supported by the Laplace prior experiment but lacks comprehensive ablation across different prior families and dimensionalities.

## Next Checks
1. **Scale validation**: Reproduce the full experimental suite on ImageNet-1K with both ViT and ResNet backbones to assess whether the sliced vs. unsliced tradeoffs persist at scale.
2. **Convergence analysis**: Derive formal bounds on the variance of sliced estimators as a function of slice count and embedding dimension, then validate these bounds empirically across the d ∈ {16, 128, 1024} range.
3. **Prior sensitivity study**: Systematically test KSD with priors that have non-analytic score functions (e.g., mixture distributions) to identify the practical limits of KSD's "exact matching" advantage and compare with numerical score estimation approaches.