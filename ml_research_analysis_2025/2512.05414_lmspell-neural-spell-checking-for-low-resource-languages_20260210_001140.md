---
ver: rpa2
title: 'LMSpell: Neural Spell Checking for Low-Resource Languages'
arxiv_id: '2512.05414'
source_url: https://arxiv.org/abs/2512.05414
tags:
- correction
- spell
- language
- pages
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LMSpell presents the first comprehensive empirical study on PLMs\
  \ for spell correction across multiple languages, including low-resource languages.\
  \ The toolkit supports all three PLM architectures\u2014encoder-only, decoder-only,\
  \ and encoder-decoder\u2014abstracting model-specific implementation details."
---

# LMSpell: Neural Spell Checking for Low-Resource Languages
## Quick Facts
- arXiv ID: 2512.05414
- Source URL: https://arxiv.org/abs/2512.05414
- Reference count: 40
- First comprehensive empirical study on PLMs for spell correction across multiple languages, including low-resource languages

## Executive Summary
LMSpell introduces the first comprehensive empirical study on using pre-trained language models (PLMs) for spell correction across multiple languages, including low-resource languages. The toolkit supports all three PLM architectures—encoder-only, decoder-only, and encoder-decoder—abstracting model-specific implementation details. A key innovation is the inclusion of an evaluation function that compensates for LLM hallucination by performing proper sentence alignment. Experiments with seven languages reveal that encoder models underperform despite multilingual pretraining, while LLMs excel, especially for Latin script languages and with large fine-tuning datasets. For non-Latin scripts, mixed-language training significantly improves LLM performance.

## Method Summary
LMSpell evaluates three PLM architectures for spell correction: encoder-only (XLM-R), encoder-decoder (mT5-580M, mBART50-680M), and decoder-only (Llama 3.1 8B, Gemma 2 9B). Encoder models use sequence labeling via masked language modeling with every token treated as masked. Encoder-decoder models employ text-to-text generation. Decoder models use prompt-based generation with LoRA fine-tuning (r=8). Training uses batch sizes of 16 (encoder) or 4 (decoder), learning rate 1e-5, gradient accumulation (4 or 2 steps), linear warmup (10 steps), and early stopping patience of 5. The evaluation incorporates hallucination-aware alignment that compares original, predicted, and expected sentences at word and character levels to detect insertions as false positives.

## Key Results
- Encoder models (XLM-R, mT5) underperform despite multilingual pretraining across all seven tested languages
- LLMs (Llama 3.1, Gemma 2) achieve superior performance, particularly for Latin script languages with large fine-tuning datasets
- Mixed-language training significantly improves LLM performance for non-Latin scripts like Sinhala
- A Sinhala case study demonstrates LLM robustness across domains and improved downstream task performance when test data is spell-corrected

## Why This Works (Mechanism)
The superior performance of LLMs stems from their generative capabilities that naturally handle context-dependent corrections and spelling variations. Unlike encoder models that rely on token-level classification, decoder models can generate entire corrected sentences while maintaining semantic coherence. The hallucination-aware evaluation mechanism addresses a critical limitation of LLMs by properly aligning sentences to distinguish genuine corrections from spurious insertions, preventing inflated false positive rates that would otherwise mask true performance.

## Foundational Learning
- **PLM architectures (encoder-only, decoder-only, encoder-decoder)**: Understanding these architectures is crucial for implementing the three approaches evaluated. Quick check: Can you explain how BERT's masked language modeling differs from GPT's autoregressive generation?
- **Low-resource language challenges**: Spell correction in low-resource languages faces data scarcity, script complexity, and limited linguistic resources. Quick check: What makes Sinhala spell correction particularly challenging compared to English?
- **LoRA fine-tuning**: Low-Rank Adaptation enables efficient fine-tuning of large LLMs by modifying attention mechanisms rather than full model weights. Quick check: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?
- **Hallucination-aware evaluation**: Standard metrics fail when LLMs generate extra words; this approach aligns sentences to properly attribute false positives. Quick check: How does word-level followed by character-level alignment detect spurious insertions?
- **Mixed-language training**: Combining multiple languages during training improves performance on low-resource languages by leveraging cross-linguistic patterns. Quick check: Why might training on both Sinhala and Hindi help spell correction for either language?

## Architecture Onboarding
**Component Map**: Dataset -> Preprocessing -> Model (XLM-R/mT5/Llama3.1/Gemma2) -> Fine-tuning (SFTTrainer/Accelerate) -> Hallucination-aware Evaluation -> Performance Metrics
**Critical Path**: Data preparation and proper evaluation alignment are most critical—errors here propagate through the entire pipeline
**Design Tradeoffs**: Encoder models offer faster inference but poorer accuracy; decoder models require more resources but achieve better results; mixed-language training improves low-resource performance but requires careful sampling
**Failure Signatures**: Encoder models showing poor performance despite pretraining; LLM evaluation scores inflated without proper hallucination handling; non-Latin scripts performing poorly with small datasets
**First Experiments**: 1) Fine-tune Gemma 2 9B on Sinhala with LoRA using the provided prompt template and SFTTrainer; 2) Implement the hallucination-aware evaluation on a small controlled dataset; 3) Compare encoder vs decoder performance on a single language with varying dataset sizes

## Open Questions the Paper Calls Out
- **Preference optimization**: Can techniques like DPO or PPO further improve LLM performance for low-resource language spell correction beyond standard supervised fine-tuning?
- **RAG failure analysis**: Does the failure of Retrieval-Augmented Generation in Sinhala stem primarily from underdeveloped embedding spaces or noise in the retrieval corpus?
- **Encoder architecture limitations**: Is the poor performance of multilingual encoder models inherent to the architecture for this task, or a result of the specific "sequence labeling" implementation used?

## Limitations
- Evaluation methodology complexity with hallucination-aware alignment lacks complete implementation details
- Uneven dataset sizes across languages (510K for Sinhala vs much smaller sets for others) confound architecture comparisons
- Critical implementation details missing (exact epochs, model checkpoint versions, complete preprocessing pipeline)

## Confidence
- **High confidence**: Encoder models consistently underperform despite multilingual pretraining across all tested languages
- **Medium confidence**: LLM superiority for Latin script languages with large datasets, though script type and dataset size effects are confounded
- **Low confidence**: Mixed-language training specifically improves non-Latin script performance without granular experimental validation

## Next Checks
1. Replicate the hallucination-aware evaluation using the provided evaluation script on a small controlled dataset where ground truth alignment is known, verifying that insertions are correctly flagged as false positives
2. Test architecture performance sensitivity to dataset size by training encoder and decoder models on incrementally increasing subsets of the same language data to isolate whether LLM advantages are primarily dataset-driven
3. Validate mixed-language training benefits by training separate models on pure Sinhala data versus mixed-language data with identical total token counts, measuring relative performance improvements for non-Latin script handling