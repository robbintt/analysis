---
ver: rpa2
title: 'Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation'
arxiv_id: '2509.15980'
source_url: https://arxiv.org/abs/2509.15980
tags:
- depth
- explainability
- pixels
- input
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of explainability in monocular\
  \ depth estimation (MDE) models, which are increasingly used in safety-critical\
  \ applications like autonomous driving. The authors investigate three well-established\
  \ feature attribution methods\u2014Saliency Maps, Integrated Gradients, and Attention\
  \ Rollout\u2014on two representative MDE architectures: PixelFormer (deep) and METER\
  \ (lightweight)."
---

# Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation

## Quick Facts
- arXiv ID: 2509.15980
- Source URL: https://arxiv.org/abs/2509.15980
- Reference count: 25
- Primary result: Saliency Maps best for lightweight METER model; Integrated Gradients best for deep PixelFormer model

## Executive Summary
This paper investigates explainability methods for monocular depth estimation (MDE) models, which are increasingly used in safety-critical applications like autonomous driving. The authors evaluate three feature attribution methods—Saliency Maps, Integrated Gradients, and Attention Rollout—on two representative MDE architectures: the lightweight METER and deep PixelFormer. To assess the quality of generated explanations, they propose a novel Attribution Fidelity (AF) metric that measures the consistency between highlighted input regions and their actual impact on depth predictions, addressing limitations in traditional fidelity measures.

## Method Summary
The study employs three feature attribution methods (Saliency Maps, Integrated Gradients, and Attention Rollout) on two MDE architectures (PixelFormer and METER). The proposed Attribution Fidelity (AF) metric evaluates explanation quality by measuring the difference between depth errors caused by perturbing relevant versus irrelevant pixels, normalized by total error magnitude. The evaluation uses the NYU Depth V2 dataset (654 test images) with perturbations including black masks, Gaussian noise (σ=0.8), and FGSM (ε=3.0). The gradient target for all methods is the mean of the predicted depth map.

## Key Results
- Saliency Maps provide the best explanations for the lightweight METER architecture
- Integrated Gradients performs best for the deep PixelFormer architecture
- The AF metric effectively identifies cases where traditional Faithfulness Estimate fails by considering both magnitude and difference of depth errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The AF metric detects explainability method failures that FE misses by penalizing cases where relevant pixel error magnitude is not significantly higher than irrelevant pixel error magnitude.
- **Mechanism:** AF isolates the difference between errors from most relevant (x_rel) and least relevant (x_irr) pixels, normalizing by total error magnitude. This creates a strict conditional dependency where explanations are only "faithful" if relevant perturbations dominate the error profile.
- **Core assumption:** Valid explanations must show significantly higher depth error from perturbing relevant pixels compared to irrelevant pixels.
- **Evidence anchors:** Abstract states AF provides "more reliable assessment... by considering both the magnitude and difference of depth errors." Section IV.C shows FE=0.93 with ASR=0.43 and AF=-0.01, highlighting FE limitations.
- **Break condition:** AF may fail to distinguish good explanations from bad ones if the model exhibits failure modes where perturbing any pixel set causes similar high-magnitude errors.

### Mechanism 2
- **Claim:** Integrated Gradients provides superior explanations for deep architectures while Saliency Maps suit lightweight architectures due to gradient saturation and path dependencies.
- **Mechanism:** Deep ViTs have complex non-linear feature hierarchies where IG's path integration smooths out local gradient noise across deep paths. Lightweight models may have sharper decision boundaries where vanilla gradients suffice.
- **Core assumption:** Network depth correlates with gradient noisiness, necessitating path integration for deep models but potentially over-smoothing lightweight ones.
- **Evidence anchors:** Abstract shows Saliency Maps perform best for METER while IG excels for PixelFormer. Section IV.B confirms these results.
- **Break condition:** If IG baseline (black image) contains features similar to input (dark scenes), the integration path may fail to capture transitions correctly.

### Mechanism 3
- **Claim:** Attention Rollout can be adapted for MDE by aggregating attention matrices across layers and summing columns to derive patch importance.
- **Mechanism:** Without a class token, the method aggregates attention heads using minimum/mean operations and propagates identity-mixed matrices through layers. Column summation computes relevance scores treating cumulative attention received by patches as importance measures.
- **Core assumption:** Cumulative attention received by a patch from its context serves as a valid proxy for relevance in dense prediction tasks.
- **Evidence anchors:** Section III.A describes summing columns of M^0 to compute absolute importance of every input patch. Equation 6 defines the summation mechanism.
- **Break condition:** If the model uses attention heads that are not strictly spatial (channel-mixing or functional heads), raw attention weights may not correlate with spatial importance.

## Foundational Learning

- **Concept:** Input Feature Attribution (Gradient-based)
  - **Why needed here:** Understanding "why" MDE models predict specific depths requires grasping that Saliency uses first-order gradient (∇_x f(x)) while IG accumulates gradients along a path to satisfy sensitivity axioms.
  - **Quick check question:** If a pixel's value changes slightly and depth output changes significantly, what does the Saliency Map visualize for that pixel?

- **Concept:** Dense Prediction vs. Classification Explainability
  - **Why needed here:** MDE outputs depth maps (spatial tensors) rather than class logits. Explaining MDE requires aggregating gradients of mean depth or mapping pixel-wise contributions back to input.
  - **Quick check question:** Why can't we simply take the gradient of the output layer directly for MDE without aggregating the output tensor?

- **Concept:** Perturbation-based Fidelity
  - **Why needed here:** AF metric relies on perturbing pixels to test if model predictions change, serving as "black-box" validation. If masking "important" pixels doesn't change answers, the explanation was wrong.
  - **Quick check question:** In AF metric formula, what happens to the score if perturbing "relevant" pixels causes the exact same error magnitude as perturbing "irrelevant" pixels?

## Architecture Onboarding

- **Component map:** RGB Image → Target Models: METER (Hybrid ViT, Lightweight) or PixelFormer (Deep ViT) → Explainers: Saliency (Gradient ∇), Integrated Gradients (Path Integral), Attention Rollout (Column Sum of Aggregated Weights) → Validator: Perturbation Module (Masks/Noise) → AF Metric (RMSE difference)

- **Critical path:**
  1. Implement gradient aggregation for MDE using mean of predicted depth map as scalar target for backpropagation
  2. Implement Attention Rollout Column Summation (Eq. 6) correctly for ViT models
  3. Implement AF Metric (Eq. 7) ensuring denominator handles sum of absolute errors to bound output in (-1, 1)

- **Design tradeoffs:**
  - IG vs. Saliency: Use IG for deep models (PixelFormer) for stability but expect higher compute cost (200 steps); use Saliency for lightweight models (METER) for speed but expect noisier maps
  - Perturbation Type: FGSM destroys depth accuracy but may yield low AF; Gaussian noise is generally more balanced for assessing feature relevance

- **Failure signatures:**
  - High FE, Low AF: Method ranks pixels correctly on average but fails to distinguish top most critical pixels from bottom ones
  - Negative AF: Method is inversely correct (highlighting unimportant areas); check for gradient sign errors or inverted attention masks

- **First 3 experiments:**
  1. **Sanity Check:** Compute Saliency Maps for METER using mean depth as gradient target. Verify object edges (high frequency) light up.
  2. **Metric Comparison:** Run Integrated Gradients on PixelFormer. Calculate both FE and AF. Identify specific images where FE > 0.9 but AF < 0.1 to understand metric divergence.
  3. **Perturbation Sweep:** Using best method for each model, apply Gaussian Noise (std=0.8) to top 5% of pixels. Confirm rRMSE is significantly higher than iRMSE.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed Attribution Fidelity (AF) metric effectively evaluate visual explanations in computer vision tasks other than Monocular Depth Estimation?
- **Basis in paper:** Authors state they aim to "analyze the AF metric using different explainability techniques for various tasks to reach a more comprehensive evaluation."
- **Why unresolved:** Current study validates AF solely on MDE, a dense regression task; behavior on classification or segmentation tasks is unknown.
- **What evidence would resolve it:** Experimental results applying AF metric to standard classification or segmentation benchmarks.

### Open Question 2
- **Question:** How does evaluation of explainability methods in MDE change when employing a wider range of adversarial attacks or perturbation strategies?
- **Basis in paper:** Authors plan to "extend the MDE explainability study by increasing the range of attacks to perturb images."
- **Why unresolved:** Study is limited to black masks, Gaussian noise, and FGSM; different perturbations might alter ranking of Saliency Maps vs. Integrated Gradients.
- **What evidence would resolve it:** Comparative analysis of explainability metrics using broader set of perturbation algorithms on METER and PixelFormer models.

### Open Question 3
- **Question:** Does correlation between specific explainability methods and model complexity (lightweight vs. deep) persist when evaluated on outdoor or diverse environmental datasets?
- **Basis in paper:** Authors acknowledge testing only on NYU dataset (indoor scenes) and explicitly plan to "test over other datasets."
- **Why unresolved:** Depth estimation cues differ significantly between indoor and outdoor scenes, potentially shifting which features models rely on.
- **What evidence would resolve it:** Experimental results from proposed methods on outdoor datasets (e.g., KITTI) confirming whether same explainability methods perform best for same architectures.

## Limitations
- AF metric relies on specific perturbation protocol whose effectiveness may vary with model architecture and data distribution
- Column summation approach for Attention Rollout may not capture spatial attention patterns accurately for all ViT architectures
- Assumption that relevant pixel error must significantly exceed irrelevant pixel error may break down for models with unstable decision boundaries

## Confidence
- **High Confidence:** Experimental observation that Saliency Maps perform best for METER and Integrated Gradients for PixelFormer, supported by direct comparison on NYU Depth V2
- **Medium Confidence:** Theoretical framework of AF metric addressing FE limitations, though generalizability beyond tested architectures requires further validation
- **Low Confidence:** Specific implementation details of Attention Rollout for MDE tasks and column summation mechanism due to limited corpus evidence

## Next Checks
1. Test AF metric sensitivity across different perturbation intensities (σ values beyond 0.8) to verify robustness of relevance detection
2. Validate AF scores on additional MDE architectures (e.g., MiDaS, BTS) to assess metric generalizability beyond PixelFormer and METER
3. Compare AF metric performance against alternative perturbation-based fidelity measures (e.g., Deletion game, Insertion game) to benchmark its effectiveness in MDE domain