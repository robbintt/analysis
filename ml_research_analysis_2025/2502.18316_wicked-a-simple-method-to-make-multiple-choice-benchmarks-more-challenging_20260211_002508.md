---
ver: rpa2
title: 'WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging'
arxiv_id: '2502.18316'
source_url: https://arxiv.org/abs/2502.18316
tags:
- wicked
- benchmarks
- answer
- correct
- above
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WiCkeD, a simple method to automatically make
  multiple-choice benchmarks more challenging by replacing one random choice with
  "None of the above." This approach is commonly used in educational tests and is
  hypothesized to increase difficulty by requiring models to identify when no correct
  answer is present among the given options. The method is applied to six popular
  benchmarks (MMLU, MMLU-Pro, MMLU-Redux, CommonsenseQA, TruthfulQA, and Arc-challenge),
  and 18 open-weight LLMs are evaluated.
---

# WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging

## Quick Facts
- arXiv ID: 2502.18316
- Source URL: https://arxiv.org/abs/2502.18316
- Authors: Ahmed Elhady; Eneko Agirre; Mikel Artetxe
- Reference count: 16
- Primary result: Replacing one random choice with "None of the above" drops LLM performance by 12.1 points on average across 6 benchmarks

## Executive Summary
WiCkeD introduces a simple method to make multiple-choice benchmarks more challenging by replacing one random choice with "None of the above." This approach, commonly used in educational tests, requires models to identify when no correct answer exists among the presented options. The method is applied to six popular benchmarks (MMLU, MMLU-Pro, MMLU-Redux, CommonsenseQA, TruthfulQA, and Arc-challenge) and evaluated on 18 open-weight LLMs. Models experience an average 12.1-point performance drop compared to original benchmarks, with some models showing greater sensitivity to the added reasoning complexity.

## Method Summary
WiCkeD transforms MCQ benchmarks by uniformly sampling one option to replace with "None of the above" (NOTA). The method first classifies questions as Single Best Answer (SBA) or Single Correct Answer (SCA) using a BERT classifier trained on 3K GPT-4o-mini labeled examples. SBA questions (where multiple options are partially correct) are copied verbatim to avoid incoherence, while SCA questions are transformed. For each non-SBA question, one option is randomly replaced, and the ground truth is updated (if the replaced option was correct, NOTA becomes correct). The evaluation uses 5-shot prompting via Eval-Harness with 5 random WiCkeD variants per benchmark, reporting mean accuracy and standard deviation.

## Key Results
- Models drop 12.1 points on average compared to original benchmarks
- Qwen2.5-7B suffers the largest degradation (19.73%), while its DeepSeek-R1 distilled version suffers the least (7.35%)
- Instruction-tuned models experience significantly less degradation than base models, especially when using CoT
- CoT prompting reduces performance drop by 1.4-14.6 points on MMLU variants but still shows significant gaps

## Why This Works (Mechanism)

### Mechanism 1: Absence Detection Hypothesis
Replacing a random option with "None of the above" increases benchmark difficulty by requiring models to identify when no correct answer exists among the presented options. Standard MCQ benchmarks allow models to select the "best available" option through relative comparison, but adding "None of the above" forces models to evaluate each option against an absolute standard rather than through comparison, potentially exposing shallow pattern matching.

### Mechanism 2: SBA vs. SCA Question Taxonomy
Coherent application of "None of the above" requires distinguishing Single Best Answer (SBA) from Single Correct Answer (SCA) questions. In SBA questions, multiple options could be correct but one is "best." Replacing the best answer would make "None of the above" incorrect even when another valid option remains. The paper trains a BERT classifier to detect SBAs and copies them verbatim to avoid incoherence.

### Mechanism 3: Differential Sensitivity to Reasoning Complexity
Models with enhanced reasoning capabilities (via distillation or instruction tuning) show smaller performance degradation on WiCkeD variants. WiCkeD exposes latent differences in reasoning ability. Models trained with reasoning augmentation (DeepSeek-R1 distillation) or strong instruction tuning maintain more consistent performance when the evaluation requires evaluating option absence rather than relative selection.

## Foundational Learning

- **Multiple-choice probability extraction**
  - Why needed: WiCkeD evaluation relies on comparing token probabilities across answer options to determine model selection
  - Quick check: Given a prompt ending with "(A) hydrogen (B) oxygen (C) None of the above", how would you extract the model's predicted answer from the logits?

- **Chain-of-thought prompting for reasoning evaluation**
  - Why needed: The paper uses CoT to test whether enhanced reasoning closes the WiCkeD performance gap
  - Quick check: Why might CoT improve performance on MMLU but still show a 5%+ gap on WiCkeD variants?

- **Benchmark saturation and ceiling effects**
  - Why needed: The paper frames WiCkeD as a response to saturated benchmarks
  - Quick check: If a benchmark shows 95% accuracy across diverse models, what two distinct explanations could account for this?

## Architecture Onboarding

- **Component map:** Data transformation pipeline -> SBA classifier -> WiCkeD variant generator -> Evaluation harness
- **Critical path:** Verify target benchmark has SCA-style questions -> Run SBA classifier -> For non-SBA questions, randomly replace one option with NOTA -> Run evaluation with 5 random seeds
- **Design tradeoffs:** Recall vs. precision in SBA detection (paper optimizes for recall 98.9% to avoid incoherent examples); Random replacement vs. strategic replacement (uniform sampling is simpler); Single variant vs. ensemble (paper uses 5 variants)
- **Failure signatures:** Large variance across random seeds (>3 std) suggests individual examples have outsized impact; Models scoring near-random on original but high on WiCkeD may indicate "None of the above" bias; CoT showing larger degradation than direct prompting suggests model overthinking
- **First 3 experiments:** 1) Run WiCkeD on MMLU-Pro with Llama-3.1-8B; expect ~11 point drop; 2) Sample 100 classifier predictions; manually verify recall claim (>98% of SBAs caught); 3) Test whether NOTA position (A/B/C/D) affects model selection, controlling for answer key bias

## Open Questions the Paper Calls Out

### Open Question 1
What specific model characteristics cause some LLMs to be more sensitive to WiCkeD than others (ranging from 7.2 to 19.7 points degradation)? The paper observes that DeepSeek-R1 distilled models and instruction-tuned variants suffer less degradation, but does not isolate whether this stems from reasoning training, data composition, or architecture.

### Open Question 2
How do closed-weight models (e.g., GPT-4, Claude) perform on WiCkeD variants compared to the 18 open-weight LLMs tested? The study restricted evaluation to open-weight models due to accessibility; closed models may exhibit different sensitivity patterns.

### Open Question 3
Is WiCkeD applicable and coherent for MCQ benchmarks beyond the six tested, particularly those with different question structures? The SBA/SCA classification was manually validated only on MMLU variants, CommonsenseQA, and TruthfulQA; other benchmarks may have different SBA prevalence or structure.

### Open Question 4
Can fine-tuning on WiCkeD-style examples improve models' ability to recognize knowledge gaps and reduce hallucination? The paper hypothesizes WiCkeD challenges models to "identify when no correct answer is present," a capability linked to hallucination reduction, but does not test training interventions.

## Limitations

- The 98.9% recall SBA classifier may still introduce ~1.1% incoherent examples where NOTA is incorrectly marked as correct
- The study only evaluates open-weight models, leaving performance of closed models (GPT-4, Claude) unknown
- WiCkeD's applicability to benchmarks beyond the six tested requires further verification for coherence

## Confidence

- **High Confidence:** The core observation that adding "None of the above" options reduces LLM accuracy (12.1% average drop) is well-supported by experimental results
- **Medium Confidence:** The SBA classifier's effectiveness and the claim that it prevents incoherent examples are supported by reported metrics but lack independent validation
- **Medium Confidence:** The interpretation that performance degradation reflects reasoning complexity rather than calibration issues is plausible but not definitively proven

## Next Checks

1. Apply temperature scaling or probability normalization to evaluated models and measure whether the performance gap between original and WiCkeD benchmarks narrows significantly
2. Manually audit 100 randomly selected WiCkeD-transformed examples to verify the classifier correctly identified SBAs and that NOTA placement doesn't create incoherent questions
3. Systematically vary the position of "None of the above" across all options to determine if position affects model selection patterns, controlling for answer key bias in original benchmarks