---
ver: rpa2
title: Feeding LLM Annotations to BERT Classifiers at Your Own Risk
arxiv_id: '2504.15432'
source_url: https://arxiv.org/abs/2504.15432
tags:
- labels
- data
- https
- text
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of LLM-generated labels for training
  smaller text classifiers, revealing several critical issues. Models trained on synthetic
  labels show not only reduced accuracy and F1 scores but also increased instability
  across training runs and premature performance plateaus.
---

# Feeding LLM Annotations to BERT Classifiers at Your Own Risk

## Quick Facts
- **arXiv ID:** 2504.15432
- **Source URL:** https://arxiv.org/abs/2504.15432
- **Authors:** Yucheng Lu; Kazimier Smith
- **Reference count:** 21
- **One-line primary result:** LLM-generated labels introduce irreducible error, causing BERT classifiers to exhibit reduced accuracy, increased instability, and premature performance plateaus.

## Executive Summary
This paper investigates the use of LLM-generated labels for training smaller text classifiers, revealing several critical issues. Models trained on synthetic labels show not only reduced accuracy and F1 scores but also increased instability across training runs and premature performance plateaus. These problems worsen with task complexity and disproportionately affect minority classes, suggesting error propagation from biased LLM annotations. While mitigation strategies like entropy-based filtering and ensemble methods offer partial relief, they fail to fully resolve the inherent risks. The findings caution against over-reliance on synthetic labels in high-stakes applications, highlighting the need for careful validation and alternative approaches.

## Method Summary
The study fine-tunes RoBERTa-base classifiers using labels generated by Qwen2.5-Instruct (3B/7B) via 3-shot prompting. Experiments use four datasets (IMDB, E-commerce, Manifestos, Toxic) with training capped at 5000 samples. Synthetic labels are generated using vLLM with guided decoding to enforce label token + EOS format. Models are trained for 3 epochs with AdamW (lr=2e-5, batch size 16, weight decay 0.01, linear warmup 0.05). Performance is measured using Accuracy, Macro-F1, and stability metrics including Krippendorff's Alpha across 5 random seeds.

## Key Results
- Models trained on synthetic labels show increased variance and instability across training runs compared to gold labels.
- Synthetic label training leads to premature performance plateaus that cannot be overcome by adding more data.
- Minority classes are disproportionately affected, with models exhibiting consistent underperformance on under-represented categories.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on LLM-generated labels introduces an irreducible error that causes performance plateaus.
- **Mechanism:** The student model (BERT) learns to approximate the LLM's conditional distribution ($P_S(Y|X)$) rather than the true distribution ($P(Y|X)$). Because the LLM is an imperfect approximator, a KL-divergence gap exists between the synthetic and true distributions. This gap manifests as an "approximation error" that cannot be reduced simply by adding more synthetic data, leading to premature performance ceilings.
- **Core assumption:** The LLM-generated labels systematically deviate from the true conditional distribution in a non-random manner.
- **Evidence anchors:**
  - [Section 3.1]: "The first term irreducible approximation error implies that no amount of synthetic labels can remove the systematic biases LLM annotators introduces, leading to performance plateau."
  - [Abstract]: "We observe... premature performance plateaus."
  - [Corpus]: Corpus papers like "Augmenting Human-Annotated Training Data..." suggest hybrid approaches, reinforcing that pure synthetic data hits limits.
- **Break condition:** If the LLM annotator achieves near-perfect approximation of $P(Y|X)$ (e.g., super-human accuracy), the irreducible error term vanishes.

### Mechanism 2
- **Claim:** Synthetic labels amplify prediction instability across training runs.
- **Mechanism:** In regions where the LLM's probability distribution is highly uncertain or divergent from the true labels, small fluctuations in the training sample cause amplified estimation errors. This results in high variance in the learned decision boundary, meaning the model makes different predictions for the same inputs across different random seeds.
- **Core assumption:** The divergence between synthetic and true labels is not uniform but concentrated in specific "difficult" regions of the feature space.
- **Evidence anchors:**
  - [Section 3.1]: "In regions where $P_S(Y|X)$ is particularly off... even small fluctuations... can lead to larger estimation errors."
  - [Section 3]: "Most concerning finding is that models trained on synthetic labels exhibit significant prediction instability... [dropping] Krippendorff's alpha."
  - [Corpus]: "Text Classification in the LLM Era" provides context on LLM reliability, though specific instability mechanisms are unique to this paper.
- **Break condition:** If the student model is heavily regularized or if the LLM provides perfectly calibrated confidence scores that allow for sample re-weighting.

### Mechanism 3
- **Claim:** Systematic errors in LLM annotations disproportionately harm minority classes (Model Collapse).
- **Mechanism:** LLMs tend to struggle with tail distributions (minority classes). When these erroneous labels are used to train a student model, the student "inherits" this blindness, failing to learn representations for the minority class. This acts as a mild form of model collapse where the diversity of the learned distribution shrinks.
- **Core assumption:** The teacher LLM has inherent biases against under-represented classes in its training data.
- **Evidence anchors:**
  - [Section 3]: "LLM annotator and subsequently trained RoBERTa classifier consistently underperform on minority classes. This phenomenon can be interpreted as a mild form of model collapse."
  - [Section 1]: "...error propagation and model collapse—issues well-documented..."
  - [Corpus]: Explicit corpus evidence for this specific "minority class collapse" mechanism in this context is missing or weak.
- **Break condition:** If the LLM is specifically few-shot prompted with abundant examples of minority classes to correct the balance.

## Foundational Learning

- **Concept:** **KL-Divergence and Approximation Error**
  - **Why needed here:** To understand *why* adding more synthetic data doesn't fix the problem. The paper frames the issue as a divergence between two distributions ($P$ vs $P_S$) rather than just "noise."
  - **Quick check question:** Can infinite synthetic data from a flawed teacher model yield a perfect student model? (Answer: No, due to the irreducible KL-divergence term).

- **Concept:** **Model Stability vs. Accuracy**
  - **Why needed here:** The paper highlights that high accuracy can mask high instability (predictions flipping between runs). Understanding metrics like Krippendorff's Alpha is crucial for diagnosing the reliability of the classifier.
  - **Quick check question:** If a model has 90% accuracy but only 40% prediction consistency across runs, is it safe for high-stakes decision making?

- **Concept:** **Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** This is the architectural paradigm being analyzed. The paper investigates a specific failure mode of distillation where the teacher (LLM) provides hard labels (or noisy soft labels) that mislead the student.
  - **Quick check question:** How does the "dark knowledge" (soft targets) usually help distillation, and why might LLM hard labels fail to provide this benefit?

## Architecture Onboarding

- **Component map:** Unlabeled Text -> Teacher (Qwen2.5-Instruct) -> Synthetic Labels -> (Filtering/Ensembling) -> Student (RoBERTa-base)
- **Critical path:** The generation of synthetic labels is the critical point of failure. The paper demonstrates that the *quality* and *stability* of these labels are the bottlenecks, not the volume of data.
- **Design tradeoffs:**
  - **Cost vs. Stability:** Using LLMs for annotation is cheaper than human labeling but introduces high variance (instability) and systematic bias.
  - **Ensembling:** Consistency ensembles (multiple LLM inferences) improve accuracy but drastically increase inference costs (defeating the "low cost" purpose).
- **Failure signatures:**
  - **Early Plateau:** Validation loss stops decreasing or accuracy flattens well before human-baseline performance.
  - **High σ_acc:** Standard deviation of accuracy increases significantly compared to gold-label training.
  - **Minority Collapse:** Macro-F1 drops significantly lower than Accuracy, indicating failure on tail classes.
- **First 3 experiments:**
  1.  **Variance Baseline:** Train the student model on Gold vs. Synthetic labels using 5 different random seeds. Measure Krippendorff's Alpha to confirm the "instability" hypothesis.
  2.  **Scaling Law Check:** Train the student on increasing subsets of synthetic data (e.g., 1k, 5k, 10k samples) to verify the "performance plateau" effect (does performance saturate?).
  3.  **Mitigation Stress Test:** Apply entropy-based filtering to the synthetic labels. Compare the recovered performance against the cost of filtering (how many samples were discarded?).

## Open Questions the Paper Calls Out

- **Question:** Can learnable filtering strategies (e.g., using embeddings to predict LLM errors) outperform heuristic mitigations like entropy-based filtering?
  - **Basis in paper:** [explicit] The authors explicitly state they "did not investigate more sophisticated approaches," suggesting that "using the embeddings as inputs to a simple ridge regression on a small validation set could help predict where the LLM is likely to make mistakes."
  - **Why unresolved:** The paper only tests simple heuristics (entropy and ensembles), which fail to stabilize training variance or fully close the performance gap.
  - **What evidence would resolve it:** Empirical results showing that a trained error-prediction model can filter labels effectively enough to stabilize training runs and improve minority class performance.

- **Question:** How does the approximation error ($KL(P||P_S)$) theoretically influence the variance and convergence rates of the student classifier?
  - **Basis in paper:** [explicit] The authors acknowledge their theoretical analysis "lacks a rigorous exposition of how this error influences the variance and convergence rates of our estimates," calling this a gap for future research.
  - **Why unresolved:** The paper observes high instability empirically but does not provide formal bounds linking the divergence of the synthetic distribution to the observed training variance.
  - **What evidence would resolve it:** A formal theoretical framework or derivation that connects the specific properties of the LLM's approximation error to the premature plateaus and run-to-run variance.

- **Question:** Will the identified risks (instability, plateaus) persist as foundation models scale to better approximate the true conditional distribution $P(Y|X)$?
  - **Basis in paper:** [explicit] The limitations section questions if the findings will remain relevant "As state-of-the-art models become increasingly capable of approximating the conditional distribution $P(Y|X)$ arbitrarily well."
  - **Why unresolved:** While the paper shows larger LLMs (7B) sometimes increased instability, it is unclear if sufficiently advanced models would eliminate the irreducible approximation error causing the performance plateau.
  - **What evidence would resolve it:** Longitudinal experiments using state-of-the-art LLMs of increasing scale to measure the reduction (or exacerbation) of the stability and plateau issues.

## Limitations
- The theoretical analysis lacks rigorous exposition of how approximation error influences variance and convergence rates.
- The specific mechanism for minority class collapse is not fully established and may involve other factors beyond LLM bias.
- Only simple heuristic mitigation strategies were tested, leaving more sophisticated approaches unexplored.

## Confidence
- **High Confidence:** The observation that synthetic labels lead to increased variance across training runs is well-supported by the experimental design and metrics (Krippendorff's Alpha, p_uc). The paper's ablation studies on variance and the clear statistical difference between gold and synthetic training provide strong empirical grounding.
- **Medium Confidence:** The claim that this instability is due to "irreducible approximation error" (Mechanism 1) is theoretically plausible but relies on a strong assumption. The paper posits that the LLM's conditional distribution $P_S(Y|X)$ has a non-random, systematic divergence from the true $P(Y|X)$, but direct empirical evidence for this specific distributional gap is not provided. The argument is primarily derived from the observed performance plateau, which could have other causes.
- **Low Confidence:** The specific mechanism for "minority class collapse" (Mechanism 3) is the weakest. While the paper shows that models underperform on minority classes, the causal link to the LLM's inherent bias against tail distributions is not rigorously established. The paper states this is a "mild form of model collapse," but the analysis does not rule out other factors like class imbalance in the synthetic dataset or the student model's own learning dynamics.

## Next Checks
1. **Direct KL-Divergence Measurement:** Implement a method to directly estimate the KL-divergence between the LLM's predicted distribution $P_S(Y|X)$ and the empirical distribution from gold labels for a held-out validation set. This would provide direct evidence for the "irreducible approximation error" mechanism.
2. **Error Analysis on Minority Classes:** Conduct a detailed error analysis to determine if the LLM's predictions for minority classes are systematically wrong in a way that differs from majority classes. Compare the LLM's confusion matrix for minority vs. majority classes to see if there's a unique pattern of failure.
3. **Alternative Teacher Models:** Repeat the core experiments using a different LLM (e.g., a smaller or larger model, or one fine-tuned on a more balanced dataset) as the teacher. If the instability and minority class collapse persist across different teachers, it strengthens the claim that it's a fundamental issue with the distillation paradigm; if it varies, it points to teacher-specific biases.