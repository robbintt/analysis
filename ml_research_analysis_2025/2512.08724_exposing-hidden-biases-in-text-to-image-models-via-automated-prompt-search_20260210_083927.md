---
ver: rpa2
title: Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search
arxiv_id: '2512.08724'
source_url: https://arxiv.org/abs/2512.08724
tags:
- photo
- prompts
- person
- prompt
- bgps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bias-Guided Prompt Search (BGPS) is introduced as the first automated
  framework for discovering interpretable prompts that maximize bias exposure in text-to-image
  diffusion models. BGPS combines an LLM with attribute classifiers acting on model
  internal representations to steer prompt generation toward high-bias regions while
  maintaining linguistic coherence.
---

# Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search

## Quick Facts
- arXiv ID: 2512.08724
- Source URL: https://arxiv.org/abs/2512.08724
- Reference count: 40
- Automated framework discovers interpretable prompts that maximize bias exposure in text-to-image models, outperforming gradient-based methods in fluency while maintaining bias detection performance.

## Executive Summary
Text-to-image models can perpetuate harmful stereotypes, but auditing them requires finding prompts that reliably trigger biased outputs. BGPS introduces the first automated framework for discovering interpretable prompts that maximize demographic bias exposure. The method combines an LLM with attribute classifiers operating on diffusion model internal representations to steer prompt generation toward high-bias regions while maintaining linguistic coherence. Experiments on Stable Diffusion 1.5 and a debiased model reveal previously undocumented biases, with BGPS-discovered prompts increasing gender bias by up to 76% in debiased models. The approach uncovers context-dependent bias amplification through subtle linguistic modifiers and extends beyond occupational stereotypes to diverse scenarios including activities, contexts, and objects.

## Method Summary
BGPS discovers high-bias prompts by using attribute classifiers on diffusion model activations to steer LLM decoding toward demographic-biased prompt regions. At each beam search step, candidate token sequences are scored by a joint objective combining average classifier probability across multiple diffusion samples and LLM log-likelihood. The method generates K images per prompt using random noise samples, extracts mid-diffusion activations at timestep T'=25, and applies pre-trained linear classifiers to compute attribute probabilities. This averaging over stochasticity ensures prompts are evaluated by their average tendency to produce biased outputs rather than single-sample artifacts. The LLM prior provides language regularization while the classifier steers toward bias-amplifying regions, producing interpretable prompts with 17-26× lower perplexity than gradient-based baselines.

## Key Results
- BGPS-discovered prompts increase gender bias by up to 76% in debiased models compared to 49% with curated prompts
- Prompts maintain high interpretability with 17-26× lower perplexity (52 vs 122) compared to gradient-based optimization baselines
- Method uncovers context-dependent bias amplification through subtle linguistic modifiers like "in a rural area" or "working with tools"
- Framework extends beyond occupational stereotypes to activities, contexts, and objects while maintaining comparable bias detection performance

## Why This Works (Mechanism)

### Mechanism 1: Classifier-Guided Beam Search Steering
BGPS discovers high-bias prompts by using attribute classifiers on diffusion model activations to steer LLM decoding toward demographic-biased prompt regions. At each beam search step, candidate token sequences are scored by a joint objective combining: (1) average classifier probability across K diffusion samples, and (2) LLM log-likelihood. The classifier term promotes prompts that consistently generate biased outputs; the LLM term preserves fluency. Beam pruning keeps top-B sequences, iteratively biasing the search trajectory. If classifiers are poorly calibrated or LLM prior dominates, steering weakens; if λ too high, perplexity degrades.

### Mechanism 2: Diffusion Stochasticity Averaging
Averaging classifier scores over K random noise samples ensures prompts are evaluated by their average tendency to produce biased outputs, not single-sample artifacts. For each candidate prompt, generate K=10 images by sampling independent noise trajectories, compute classifier probabilities on each, then average. This reduces variance from diffusion stochasticity and identifies prompts with robust bias effects. If K is too low, high-variance prompts may be incorrectly ranked; if bias is noise-dependent rather than prompt-driven, averaging may not help.

### Mechanism 3: Intermediate Timestep Activation Probing
Classifiers operating on mid-diffusion activations (T'=25 of 50 steps) capture semantic attributes before final denoising, providing early bias signals. Rather than running full denoising to x₀, the method stops at T'=25, extracts UNet mid-block activations, and feeds them to pre-trained linear classifiers. These activations encode semantic content (gender, race) before high-frequency details emerge. If classifiers were trained on different architectures or timestep distributions, probing may fail; very early timesteps may lack semantic content.

## Foundational Learning

- **Diffusion Model Forward/Reverse Process**: BGPS relies on understanding how noise is added (forward) and removed (reverse) to extract intermediate activations and sample multiple generations. Can you explain why xₜ at timestep t contains both signal and noise, and how classifier-free guidance modifies the denoising direction?

- **Beam Search Decoding with External Scorers**: The core algorithm extends standard LLM beam search by incorporating classifier scores as external signals alongside token probabilities. If beam size B=10 and expansion factor E=10, how many candidate sequences are scored per step, and which are retained?

- **Linear Probing of Neural Representations**: Attribute classifiers are linear heads trained on frozen UNet activations; understanding what linear probes can/cannot extract from representations is critical. What does it imply if a linear probe achieves high accuracy on gender classification from mid-layer activations?

## Architecture Onboarding

- **Component map**: LLM Prior -> Diffusion Model -> Attribute Classifiers -> Beam Search Controller -> Evaluation Pipeline
- **Critical path**: Prompt initialization → LLM token expansion (BE candidates) → Text encoding → K diffusion forward passes to T' → Activation extraction → Classifier scoring → Joint score computation → Beam pruning → Repeat until eos/max_len → Return top-scoring prompt
- **Design tradeoffs**: λ (classifier weight) higher increases bias discovery but raises perplexity; Beam size B/expansion E larger explores more space but increases compute; K diffusion samples higher reduces variance but costs more; T' (probe timestep) robust across 10-45, default T'=25
- **Failure signatures**: High perplexity prompts (>150) indicate λ too high; explicitly gendered prompts when instructed neutral suggest smaller LLMs struggle with instructions; PEZ-like gibberish indicates gradient-based optimization without language prior; low bias detection despite high λ suggests classifier transfer issues
- **First 3 experiments**: 1) Replicate Table 1 baseline with Mistral-7B, λ=10 on SD1.5 for male-bias discovery; 2) Ablate λ with sweep {1,5,10,50,100} on occupation prompts; 3) Test on debiased model with Shen et al. fine-tuned model

## Open Questions the Paper Calls Out

- To what extent do biases in external attribute classifiers influence the prompts discovered by BGPS, and how can their contributions be disentangled from the target TTI model's biases? While acknowledged as a limitation, the paper provides no analysis quantifying how classifier biases affect BGPS outputs versus biases genuinely present in the diffusion model.

- Does incorporating BGPS-discovered prompts into debiasing training procedures improve robustness against hidden biases? The paper proposes this application as future work but does not implement or evaluate whether debiasing methods trained with BGPS-discovered prompts achieve better generalization.

- How does increasing the granularity of attribute classifiers (beyond binary gender and four race categories) change the nature and severity of discovered biases? The paper speculates this is possible but does not demonstrate whether finer-grained categories reveal qualitatively different bias patterns.

## Limitations

- Framework's effectiveness hinges on quality and transferability of pre-trained attribute classifiers, with unknown performance on prompts outside training distribution
- Computational cost is significant: each discovered prompt requires K=10 image generations for evaluation, and beam search involves multiple diffusion forward passes per candidate
- Method requires grey-box access to intermediate activations, unavailable for most commercial black-box TTI models

## Confidence

**High Confidence**: BGPS outperforms gradient-based optimization in interpretability (PPL 17-26× lower) while maintaining comparable bias detection; Discovered prompts reveal previously undocumented biases with occupation prompts increasing male proportion by 76% in debiased models; Framework generalizes beyond occupations to activities, contexts, and objects.

**Medium Confidence**: The 10× improvement in gender bias exposure over curated prompts is robust but exact magnitude depends on evaluation protocol; Prompts maintain linguistic coherence with perplexity scores comparable to human-written text, though GPT-2 perplexity may not fully capture semantic coherence.

**Low Confidence**: Claim that BGPS is "first automated framework" is plausible but difficult to verify definitively; Assertion that discovered prompts expose "previously undocumented biases" assumes comprehensive prior auditing.

## Next Checks

1. **Classifier Robustness Test**: Evaluate DiffLens classifiers on held-out prompts with known biases to quantify calibration and transfer performance, measuring accuracy and calibration error across diverse prompt types.

2. **Cost-Benefit Analysis**: Implement early stopping criteria and measure impact on discovery quality versus computational savings, reporting FLOPs per discovered prompt.

3. **Architecture Transfer Test**: Apply BGPS to different text-to-image model (SDXL or DeepFloyd IF) using DiffLens classifiers trained on SD1.5, quantifying performance degradation and testing whether retraining classifiers on target architecture improves results.