---
ver: rpa2
title: 'DSEBench: A Test Collection for Explainable Dataset Search with Examples'
arxiv_id: '2510.17228'
source_url: https://arxiv.org/abs/2510.17228
tags:
- https
- dataset
- datasets
- test
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DSEBench, the first test collection for Explainable
  Dataset Search with Examples (ExDSE). The authors address the limitation of current
  dataset search approaches that cannot handle queries combining keywords and target
  datasets as examples.
---

# DSEBench: A Test Collection for Explainable Dataset Search with Examples

## Quick Facts
- arXiv ID: 2510.17228
- Source URL: https://arxiv.org/abs/2510.17228
- Authors: Qing Shi; Jing He; Qiaosheng Chen; Gong Cheng
- Reference count: 40
- Primary result: First test collection for Explainable Dataset Search with Examples (ExDSE), with dense retrieval models achieving MAP@10 up to 0.2398 and SHAP as best explainer

## Executive Summary
This paper introduces DSEBench, the first test collection for Explainable Dataset Search with Examples (ExDSE). The collection addresses the limitation of current dataset search approaches that cannot handle queries combining keywords and target datasets as examples. The authors construct DSEBench by extending the NTCIR dataset collection with human annotations for query relevance and target similarity, plus field-level annotations for explainability. They establish comprehensive baselines by adapting retrieval, reranking, and explanation methods, showing that dense retrieval models outperform lexical models and that SHAP is the most effective explainer for identifying indicator fields.

## Method Summary
The authors construct DSEBench by extending the NTCIR dataset collection with human annotations for 141 test cases and LLM-generated annotations for 5,699 training cases. Each dataset has five fields (title, description, tags, author, summary), with content summaries generated from raw files. The query expansion approach concatenates the keyword query with pseudo-documents constructed from target dataset fields. Supervised retrievers are fine-tuned on LLM annotations, and rerankers include both text-based (BGE-reranker) and GNN-based models. Explainability is evaluated using SHAP, LIME, feature ablation, and LLM-based methods to identify which fields indicate relevance and similarity.

## Key Results
- Dense retrieval models (BGE, GTE) outperform lexical models, with coCondenser fine-tuning achieving MAP@10 of 0.2286
- The multi-layer LLM reranking configuration achieves the highest scores (MAP@10: 0.2398, NDCG@10: 0.4451)
- SHAP and few-shot LLM methods perform best in identifying indicator fields, with description being the most frequently selected field
- Fine-tuning yields substantial gains, with fine-tuned coCondenser achieving 36.5% improvement over unsupervised baselines

## Why This Works (Mechanism)

### Mechanism 1: Dual-Constraint Retrieval via Query Expansion
Concatenating the keyword query with target dataset pseudo-documents enables retrieval of candidates satisfying both relevance and similarity constraints simultaneously. The query q is expanded by concatenating it with all fields of each target dataset d_t ∈ D_t. Retrieval models then match against this expanded representation, capturing both topical intent and structural/metadata preferences implied by the target examples. Core assumption: the combined signal can be captured in a single retrieval pass; query relevance and target similarity are not fundamentally conflicting.

### Mechanism 2: LLM Annotations Enable Supervised Fine-Tuning at Scale
Filtered LLM-generated annotations provide training signal nearly as effective as human annotations for fine-tuning retrieval models. GLM-3-Turbo generates dataset-level and field-level annotations for 337,976 triples. Heuristic filtering removes superficially inconsistent judgments (e.g., low keyword overlap despite "relevant" label), improving accuracy from ~70% to >85%. The resulting 122,585 high-quality triples enable fine-tuning coCondenser, which achieves MAP@10 of 0.2286 vs. 0.1671 without fine-tuning.

### Mechanism 3: Shapley Values Capture Field Contributions Better Than Ablation
SHAP outperforms feature ablation and LIME for identifying indicator fields because it accounts for feature interactions through Shapley value theory. SHAP's partition explainer computes marginal contributions of each field across all possible feature subsets, assigning credit proportional to each field's actual impact on the retrieval score. This handles cases where fields jointly signal relevance (e.g., title + tags together indicate topic).

## Foundational Learning

- **Dense Retrieval (bi-encoders vs. cross-encoders)**: DSEBench benchmarks both dense retrievers (BGE, GTE, coCondenser) and cross-encoder rerankers (BGE-reranker). Understanding vector similarity vs. joint encoding is essential for interpreting retrieval vs. reranking tradeoffs. Quick check: Given a query and 10,000 datasets, would you use a bi-encoder or cross-encoder for first-stage retrieval? Why?

- **Graded Relevance Labels in IR Evaluation**: DSEBench uses {0, 1, 2} for relevance and similarity separately, then combines them via product to yield {0, 1, 2, 4}. Understanding NDCG@k and MAP@k requires grasping how graded labels are converted to gain. Quick check: If a dataset is highly relevant (score=2) but only partially similar (score=1), what is its combined score? How does this affect NDCG?

- **Content Summarization for Multi-Format Data**: DSEBench handles PDF, CSV, RDF, etc., by generating format-specific summaries (headers for tabular, abstracts for text, predicates for RDF). This is critical for making heterogeneous data searchable. Quick check: Why would extracting headers from CSVs be more effective for search than embedding the full file?

## Architecture Onboarding

- **Component map**: Input: (query q, target datasets D_t) → Content Summarization (format-specific: headers/abstracts/predicates) → Pseudo-document Construction (concatenate 5 fields; repeat short fields 100×) → Query Expansion (q + pseudo-doc of each d_t) → First-Stage Retrieval (BM25/TF-IDF/BGE/GTE/DPR/coCondenser) → Reranking (BGE-reranker / LLM multi-layer) → Explanation (SHAP / Few-shot LLM → indicator fields) → Output: Ranked datasets + field-level explanations

- **Critical path**: The query expansion + retrieval step. If this produces poor candidates, reranking and explanation inherit garbage. Prioritize retriever quality before reranker/explainer optimization.

- **Design tradeoffs**: Human vs. LLM annotation (human for 141 test cases, LLM for 5,699 training cases); Text vs. structure-based reranking (text-based BGE-reranker outperforms GNN models); Explanation fidelity vs. cost (SHAP is most accurate but computationally expensive).

- **Failure signatures**: Low recall on relevant-but-dissimilar datasets (check if product scoring is too punitive); Over-selection of "description" as indicator field (all explainers prioritize description); Poor cross-format retrieval (inspect summary generation quality).

- **First 3 experiments**: Reproduce coCondenser fine-tuning baseline using annotator split (verify MAP@10 ≈ 0.22); Ablate content summary field to quantify impact on NDCG@10; Compare SHAP vs. few-shot LLM on held-out subset (measure F1 and latency).

## Open Questions the Paper Calls Out

### Open Question 1
How does expanding the benchmark to support multiple target datasets per query impact the performance and evaluation of DSE models? The authors state that the current test collection is limited because each query is associated with only a single target dataset, whereas a more general evaluation setting should accommodate multiple target datasets. Constructing test cases with multiple relevant datasets for every query is difficult and reduces the total number of feasible test cases that can be built under current constraints.

### Open Question 2
Can dedicated methods that separately assess query relevance and target similarity outperform the current baseline of concatenating inputs? The authors note that current pooling methods simply expand the query by concatenating it with target dataset fields, but propose that it may be necessary to assess the dual nature of DSE (relevance and similarity) separately. The paper establishes baselines using standard IR pipelines adapted via concatenation; it does not explore architectures specifically designed to disentangle or model the two distinct signals independently.

### Open Question 3
How can evaluation frameworks better account for context-dependent criteria when judging dataset similarity? The authors acknowledge that their annotations rely on a general, context-agnostic notion of similarity, whereas real-world criteria may vary depending on specific user contexts. The current annotations aggregate general human judgments, failing to capture the nuanced information needs where similarity might be defined by specific attributes (e.g., temporal range, specific provenance) rather than overall resemblance.

## Limitations
- The effectiveness of LLM-generated annotations for training remains partially validated, with heuristic filtering criteria described qualitatively but lacking precise thresholds
- Field-level explainability evaluation relies on the assumption that the 5 annotated fields capture all relevant information for dataset search
- The paper doesn't explore architectures specifically designed to disentangle query relevance and target similarity signals

## Confidence
- **High confidence**: Retrieval model performance (MAP/NDCG) and the superiority of dense models over lexical approaches
- **Medium confidence**: The effectiveness of synthetic training data, as results show comparable performance to human annotations but with limited validation on truly out-of-distribution queries
- **Medium confidence**: SHAP's superiority for field explanation, though the relative performance gap versus LIME is modest

## Next Checks
1. Test retriever generalization by evaluating on a held-out subset of queries with topics significantly different from training data to assess LLM annotation coverage limits
2. Conduct ablation studies removing each field (title, description, tags, author, summary) individually to quantify their relative contribution to retrieval performance beyond the reported aggregate frequencies
3. Compare the query expansion approach against a two-stage model that separately scores relevance and similarity, to determine if the concatenated expansion truly captures the dual-constraint nature of the task or if explicit disentanglement would perform better