---
ver: rpa2
title: Securing generative artificial intelligence with parallel magnetic tunnel junction
  true randomness
arxiv_id: '2510.01598'
source_url: https://arxiv.org/abs/2510.01598
tags:
- random
- image
- each
- rngs
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the security vulnerabilities in generative
  artificial intelligence (GAI) models caused by deterministic pseudo-random number
  generators (PRNGs). The authors propose embedding true random number generators
  (TRNGs) based on spin-transfer torque magnetic tunnel junctions (STT-MTJs) to enhance
  security.
---

# Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness

## Quick Facts
- arXiv ID: 2510.01598
- Source URL: https://arxiv.org/abs/2510.01598
- Authors: Youwei Bao; Shuhan Yang; Hyunsoo Yang
- Reference count: 40
- **Primary result**: Hardware-based true random number generators using STT-MTJs can reduce insecure GAN outputs by 18.6× compared to PRNGs

## Executive Summary
This paper addresses a critical security vulnerability in generative AI systems: deterministic pseudo-random number generators (PRNGs) create predictable outputs that can be exploited by attackers. The authors propose embedding true random number generators (TRNGs) based on spin-transfer torque magnetic tunnel junctions (STT-MTJs) to enhance security. By leveraging inherent thermal noise in these spintronic devices, they achieve megabit-per-second true random number generation that passes rigorous NIST statistical tests. When integrated into a generative adversarial network (GAN) trained on CIFAR-10, the STT-MTJ-based system reduces insecure outputs (low-diversity image clusters) by up to 18.6 times compared to traditional PRNGs.

## Method Summary
The authors construct a 16-MTJ system with FPGA assistance to generate true random bits. MTJs are driven into metastable states where thermal fluctuations determine the final magnetic orientation (parallel or antiparallel). The system uses 16-channel DAC/ADC with FPGA control for writing and reading MTJ states, achieving 100 kHz operation (1.6 Mbps). Raw MTJ outputs undergo lightweight post-processing (bitwise XOR across three bits) or Toeplitz hashing to correct biases. These hardware-generated random bits are integrated into GAN latent codes, replacing traditional PRNGs. The system is evaluated on CIFAR-10 using LPIPS metrics to measure output diversity and security.

## Key Results
- STT-MTJ system generates true random bits at 1.6 Mbps (100 kHz × 16 MTJs) with nanosecond switching speed
- Hardware-generated random bits pass all 15 NIST SP 800-22 statistical tests
- GAN integration with MTJ TRNG reduces insecure outputs by 18.6× compared to LFSR baseline
- System achieves 7.2 mW power consumption with potential to scale beyond 10^6 parallel cells
- Inception Score remains at 10.28, maintaining image quality while enhancing security

## Why This Works (Mechanism)

### Mechanism 1: Thermal Noise as True Randomness Source
- **Claim:** Inherent thermal noise in STT-MTJs provides superior true randomness compared to algorithmic PRNGs
- **Mechanism:** Specific voltage pulses drive MTJs into metastable states where thermal fluctuations (not deterministic current) determine final magnetic orientation
- **Core assumption:** Thermal noise remains consistent across operating temperatures to serve as reliable entropy source
- **Evidence anchors:** Abstract states embedding "hardware-generated true random bits from STT-MTJs," section 2.1 describes metastable regime driven by thermal fluctuations
- **Break condition:** Device heating introduces systematic bias overwhelming stochastic thermal component, breaking 50% switching probability

### Mechanism 2: Lightweight Post-Processing for Bias Correction
- **Claim:** Simple bitwise XOR is sufficient to correct device-level biases while maintaining energy efficiency
- **Mechanism:** XORing three raw bits (von Neumann-like extractor) flattens distribution without heavy cryptographic hashing
- **Core assumption:** Weak correlations between adjacent raw bits allow simple linear combining to remove detectable biases
- **Evidence anchors:** Section 2.2 describes lightweight XOR scheme with negligible energy overhead; Table 1 shows raw data failing tests while XOR-processed passes all
- **Break condition:** Strong temporal correlations exist in raw bitstream (persistent magnetic states or slow cooling), causing XOR to fail linear complexity test

### Mechanism 3: Breaking Deterministic Dependency Chains
- **Claim:** Hardware TRNGs reduce security vulnerabilities by disrupting deterministic dependency chains exploited in black-box attacks
- **Mechanism:** Attackers predict outputs by observing patterns seeded by deterministic PRNGs; TRNG severs seed-output link, preventing gradient estimation
- **Core assumption:** "Insecure outputs" metric (low-diversity clusters with LPIPS < 0.3) correlates with exploitability in real attacks
- **Evidence anchors:** Abstract reports 18.6× reduction in insecure outputs vs. low-quality RNG baseline; section 1 describes how TRNG unpredictability disrupts fixed dependency chain
- **Break condition:** Model requires deterministic seeding for reproducibility, or attack vector doesn't rely on output predictability (e.g., data poisoning)

## Foundational Learning

- **Concept:** STT-MTJ Switching Physics
  - **Why needed here:** To understand how magnetic devices act as random number generators
  - **Quick check question:** Does higher voltage pulse increase or decrease stochasticity of switching event?

- **Concept:** NIST SP 800-22 Statistical Test Suite
  - **Why needed here:** To validate generated bits are mathematically indistinguishable from random noise
  - **Quick check question:** If generator passes "Frequency" test but fails "Linear Complexity" test, is it secure against sophisticated attackers? (Hint: No, implies algorithmic structure)

- **Concept:** Latent Space Diversity (LPIPS/t-SNE)
  - **Why needed here:** To measure security quality of generated content beyond accuracy metrics
  - **Quick check question:** In this paper, does tightly clustered t-SNE plot indicate high or low security? (Hint: Low security/high predictability)

## Architecture Onboarding

- **Component map:** 16-MTJ Array (Stochastic Nodes) → 16-ch DAC (Write Pulses) + 16-ch ADC (Read) → FPGA (Control & Post-Processing) → Python Interface (GAN Latent Code Injection)
- **Critical path:** Probability tuning loop - must characterize sigmoid response of each MTJ to find specific $V_{dd}$ yielding ~50% switching probability; without this, raw data is biased and post-processing efficiency drops
- **Design tradeoffs:**
  - Throughput vs. Quality: "MTJ with XOR" is faster/energy-cheaper but statistically weaker than "Toeplitz Hashing" requiring heavier computation (FFT)
  - Parallelism vs. Complexity: Increasing MTJ count (scaling to $10^6$) linearly increases bitrate but requires managing massive parallel DAC/ADC channels or multiplexing
- **Failure signatures:**
  - Stuck bits: Raw output stuck at 0 or 1 (device failure or insufficient reset voltage)
  - Distribution drift: Passing NIST tests at start, failing after 1 hour (device heating changed switching probability)
  - Visual correlation: Generated images look identical despite different seeds (TRNG integration failed, falling back to system PRNG)
- **First 3 experiments:**
  1. **Probability Calibration:** Sweep $V_{dd}$ for single MTJ cell and plot switching probability curve to verify sigmoid shape and locate 50% crossing point
  2. **Bias Test:** Generate 1MB of raw data without post-processing; plot histogram to visualize deviation from uniform distribution
  3. **Integration Latency Check:** Measure time delta between requesting random number from FPGA and receiving it in Python GAN script to ensure it doesn't bottleneck inference

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can proposed STT-MTJ system effectively scale to support gigabit-per-second throughput required for Large Language Model (LLM) sampling?
- **Basis in paper:** [explicit] Authors claim system "holds potential to scale beyond $10^6$ parallel cells" for LLM sampling, though experimental prototype used only 16 MTJs
- **Why unresolved:** Paper demonstrates 16-device prototype but doesn't experimentally verify maintaining uniform switching probabilities and thermal stability in arrays of $10^6$ devices
- **Evidence would resolve it:** Experimental data from large-scale array generating gigabit-per-second streams without significant bit error rates or thermal crosstalk

### Open Question 2
- **Question:** How does integration of hardware-generated true randomness affect performance and security of non-GAN generative architectures, such as diffusion models or autoregressive transformers?
- **Basis in paper:** [explicit] Authors state "experimental focus is a GAN" but note "same entropy source naturally applies to other GAI workflows"
- **Why unresolved:** GANs use random vectors differently than diffusion models (noise steps) or LLMs (token sampling); impact of MTJ-specific statistical properties on diverse architectures remains untested
- **Evidence would resolve it:** Comparative evaluation of output quality (FID for diffusion, perplexity for LLMs) and security metrics in these architectures using STT-MTJ TRNG

### Open Question 3
- **Question:** Does disruption of deterministic dependency chain by STT-MTJ TRNGs provide robust defense against advanced gradient-estimation attacks in live adversarial environment?
- **Basis in paper:** [inferred] Paper uses statistical diversity (LPIPS) and NIST test pass rates as proxies for security rather than validating against active adversarial attacks
- **Why unresolved:** While high randomness statistically reduces "insecure outputs" (biased clusters), not explicitly proven this prevents attackers from exploiting model's internal weights through sophisticated query strategies
- **Evidence would resolve it:** Penetration test measuring success rate of specific black-box attacks (e.g., gradient estimation) on models secured with STT-MTJ TRNGs versus standard PRNGs

## Limitations
- Security claims rely on LPIPS < 0.3 as proxy without validating against actual attack scenarios
- Hardware scalability projections to $10^6$ MTJs remain theoretical without experimental verification
- NIST test passing indicates statistical randomness but doesn't guarantee resistance to all forms of prediction or correlation attacks

## Confidence

- **High Confidence**: STT-MTJ switching physics producing thermal noise-based randomness, basic NIST test passing with post-processing, nanosecond switching speeds and low power consumption measurements
- **Medium Confidence**: GAN integration reducing output similarity metrics, scalability projections to larger MTJ arrays, comparison to LFSR baseline performance
- **Low Confidence**: Direct security improvement claims based on LPIPS clustering, resistance to specific attack vectors, long-term reliability under thermal drift conditions

## Next Checks

1. **Attack Vector Validation**: Test whether reduced LPIPS clustering actually corresponds to reduced vulnerability against known GAN inversion or membership inference attacks; compare success rates of black-box attacks on models using MTJ TRNG versus traditional PRNGs

2. **Long-Term Stability Test**: Run continuous MTJ random number generation for 24+ hours while monitoring NIST test pass rates and switching probability distributions; verify device heating doesn't introduce systematic bias that post-processing fails to correct

3. **Security-Accuracy Tradeoff Analysis**: Systematically vary amount of TRNG entropy injected into GAN latent space and measure both LPIPS-based security metric and Inception Score; determine if maximum security comes at significant quality cost and identify optimal operating point