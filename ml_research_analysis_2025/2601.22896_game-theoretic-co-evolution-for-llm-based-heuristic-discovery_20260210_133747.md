---
ver: rpa2
title: Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery
arxiv_id: '2601.22896'
source_url: https://arxiv.org/abs/2601.22896
tags:
- solver
- generator
- asro
- instances
- heuristic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ASRO introduces a game-theoretic framework for LLM-based heuristic
  discovery, reframing solver-instance interaction as a two-player zero-sum game with
  persistent strategy pools. It replaces static evaluation with adaptive, co-evolutionary
  assessment, where solvers and instance generators iteratively refine their programs
  via best-response oracles against opponent meta-strategies.
---

# Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery

## Quick Facts
- arXiv ID: 2601.22896
- Source URL: https://arxiv.org/abs/2601.22896
- Reference count: 40
- One-line primary result: ASRO framework achieves lower average optimality gaps across online bin packing, TSP, and capacitated vehicle routing by maintaining persistent strategy pools and adaptive meta-strategies.

## Executive Summary
ASRO introduces a game-theoretic framework for LLM-based heuristic discovery that reframes solver-instance interaction as a two-player zero-sum game with persistent strategy pools. It replaces static evaluation with adaptive, co-evolutionary assessment, where solvers and instance generators iteratively refine their programs via best-response oracles against opponent meta-strategies. Across online bin packing, TSP, and capacitated vehicle routing, ASRO consistently outperforms static-training baselines (EoH) on held-out benchmarks, achieving lower average optimality gaps—e.g., 0.23% vs. 0.77% on small TSP instances—and improved robustness under distributional and structural shifts. The framework is modular, enabling different program search mechanisms, and its gains stem primarily from sustained adversarial interaction rather than stronger LLM backbones.

## Method Summary
The ASRO framework formulates solver-generator interaction as a two-player zero-sum game where solvers minimize optimality gaps while generators maximize them. It maintains persistent strategy pools on both sides, iteratively expanding them via LLM-based best-response oracles against mixed opponent meta-strategies. The method uses EoH-style evolutionary search with LLM operators to generate candidate programs, evaluated against weighted mixtures of opponent strategies. Meta-strategies are computed via linear programming or no-regret dynamics to approximate Nash equilibrium, with convergence tracked via Approximate NashConv. The approach supports modular best-response oracles and can be extended to teacher-student or multi-objective formulations.

## Key Results
- ASRO achieves lower average optimality gaps than EoH baseline: 0.23% vs 0.77% on small TSP instances
- Improved robustness under distributional and structural shifts across all three domains
- Gains primarily stem from co-evolutionary interaction rather than LLM backbone scale (ASRO-EoH with LLaMA-3-8B outperforms EoH with DeepSeek on most settings)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Maintaining persistent strategy pools with equilibrium-based meta-strategies induces a self-generated curriculum that improves generalization over static evaluation.
- **Mechanism:** Strategy pools accumulate diverse solver and generator programs across iterations. Meta-strategies identify which opponent strategies are most impactful. Best-response oracles then synthesize programs targeting these weighted mixtures rather than single opponents, exposing weaknesses progressively rather than cycling.
- **Core assumption:** The co-evolutionary pressure from diverse adversarial generators transfers to held-out benchmarks by learning robust algorithmic patterns rather than overfitting to specific instance features.
- **Evidence anchors:** Self-play baseline removes game-theoretic structure and remains inferior to ASRO, especially on larger TSPLIB instances; EALG lacks persistent strategy sets making dynamics prone to cycling.

### Mechanism 2
- **Claim:** Generator programs that induce structured instance distributions create harder, more informative training signals than random or hand-designed distributions.
- **Mechanism:** Generators are executable programs that parameterize instance families. When evaluated against solver meta-strategies, generators producing high average gaps receive higher weight, propagating structured hardness into the training curriculum.
- **Core assumption:** Programmatic generators can express instance families that expose solver weaknesses more effectively than sampling from fixed parametric distributions.
- **Evidence anchors:** Example generator constructs item sequences appearing easy early but fragmenting capacity later; solver emerges as best response. Related work focuses on solver synthesis rather than adversarial instance generation.

### Mechanism 3
- **Claim:** Approximate best responses via LLM-based evolutionary search in program space suffice for equilibrium approximation without requiring gradient-based optimization.
- **Mechanism:** Best-response oracle uses EoH-style evolutionary search with LLM operators generating candidates evaluated against opponent meta-strategies. Selection balances fitness and diversity, returning top program as approximate best response.
- **Core assumption:** LLMs can synthesize meaningful program variations guided by fitness signals and in-context examples from parent programs.
- **Evidence anchors:** ASRO agnostic to search method, with ReEvo-style mechanism improving over ReEvo baseline; gains primarily from co-evolution rather than backbone scale. Related work uses similar LLM-guided evolutionary search.

## Foundational Learning

- **Zero-Sum Games and Mixed-Strategy Equilibrium**
  - Why needed here: ASRO formulates solver-generator interaction as a two-player zero-sum game where meta-strategies are equilibrium distributions over discrete strategy pools.
  - Quick check question: Given a 3×4 payoff matrix for a zero-sum game, can you formulate the minimax linear program for player 1's mixed strategy?

- **PSRO (Policy-Space Response Oracles)**
  - Why needed here: ASRO adapts PSRO's iterative "solve-and-expand" loop from parametric policy spaces to discrete program spaces; understanding PSRO clarifies the meta-game structure.
  - Quick check question: How does PSRO differ from standard self-play in its treatment of historical strategies?

- **Combinatorial Optimization Performance Metrics**
  - Why needed here: The payoff function uses normalized reference gaps, and interpretation of results requires understanding optimality gaps, lower bounds, and benchmark conventions.
  - Quick check question: For a TSP instance where your solver returns tour length 1050 and the optimal tour length is 1000, what is the normalized gap?

## Architecture Onboarding

- **Component map:** Payoff Matrix Computation -> Linear Programming Solver -> Meta-Strategy Computation -> Best-Response Oracle -> Strategy Pool Expansion -> Batch Evaluation Engine

- **Critical path:** 1) Initialize pools with base programs 2) For t=0 to T-1: compute payoff matrix → solve for meta-strategies → invoke both best-response oracles → append new programs to pools 3) Return final solver pool for evaluation

- **Design tradeoffs:** Pool size K vs iteration count T (larger pools capture more diversity but increase O(K²) evaluations); best-response search depth R (more rounds improve quality but increase LLM calls); base-generator mixing ratio (ensures stability but may limit co-evolution)

- **Failure signatures:** ANC plateau or increase indicates co-evolution not converging; performance degradation on in-distribution instances suggests overfitting to adversarial generators; high variance across random seeds indicates LLM synthesis dominates

- **First 3 experiments:** 1) Simplified domain validation on online bin packing with K=10, T=4 comparing against EoH baseline 2) Ablation on strategy pool persistence: ASRO vs self-play vs data augmentation on TSP 3) Equal-time comparison: ASRO-EoH vs EoH on CVRP under fixed wall-clock budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a cooperative teacher–student formulation, where instance generators act as adaptive curriculum designers rather than purely adversarial opponents, improve solver generalization compared to the current zero-sum setup?
- Basis in paper: The authors state that ASRO can be extended to support "teacher–student interactions, where generators serve as adaptive curriculum designers rather than purely adversarial opponents."
- Why unresolved: Current framework optimizes for hardness but may not optimize for learnability or pedagogical progression.

### Open Question 2
- Question: How does estimation noise in the payoff matrix—caused by using incomplete or time-limited reference oracles instead of global optima—impact the stability of the meta-game equilibrium and the quality of synthesized heuristics?
- Basis in paper: Authors acknowledge that approximations introduce estimation noise into the meta-game and may obscure fine-grained distinctions among generators.
- Why unresolved: Game-theoretic convergence typically assumes accurate payoffs; it's unclear if approximate Nash equilibrium remains stable when payoffs are stochastic or biased estimates.

### Open Question 3
- Question: Does extending ASRO to multi-objective or regularized meta-games—explicitly trading off hardness, diversity, and realism—yield more robust instance generators for complex optimization tasks?
- Basis in paper: Authors propose extending beyond strictly zero-sum formulation to multi-objective or regularized meta-games that trade off hardness, diversity, and realism.
- Why unresolved: Current zero-sum payoff function may incentivize generators to produce degenerate instances that maximize gap without being structurally realistic.

## Limitations

- The specific contributions of equilibrium computation method versus diversity maintenance within pools remain under-specified
- Analysis does not quantify how much performance gain stems from structured versus random instance generation
- Limited rigorous analysis of which components are essential versus complementary across different domains

## Confidence

- **High confidence**: ASRO framework design and basic experimental setup; ablation showing persistent pools outperform single-strategy variants; domain generality across OBP, TSP, CVRP
- **Medium confidence**: Exact mechanisms by which equilibrium meta-strategies improve generalization; specific contributions of pool diversity versus best-response quality; LLM synthesis reliability across different domains
- **Low confidence**: Comparative analysis of different equilibrium computation methods; sensitivity to population size K versus iteration count T; scaling behavior to larger K or more complex combinatorial problems

## Next Checks

1. **Component ablation study**: Run ASRO variants with different equilibrium computation methods (linear programming vs. no-regret dynamics) and diversity mechanisms to isolate their contributions to performance gains.

2. **Pool size sensitivity**: Systematically vary K (population size) and T (iterations) to identify optimal resource allocation between pool diversity and co-evolutionary depth.

3. **Generalization stress test**: Evaluate ASRO-generated solvers on out-of-distribution instances with structural modifications (e.g., modified TSP distance metrics or bin capacity distributions) to quantify true generalization versus domain-specific adaptation.