---
ver: rpa2
title: 'ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with
  LLM-based Checker-Splitter Refinement'
arxiv_id: '2510.10241'
source_url: https://arxiv.org/abs/2510.10241
tags:
- mentions
- coreference
- mention
- text
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ImCoref-CeS is a novel framework that enhances coreference resolution
  by combining a supervised neural model (ImCoref) with LLM-based refinement via a
  Checker-Splitter agent. The method improves long-text encoding, captures positional
  information better, and uses regularization to boost training efficiency.
---

# ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement

## Quick Facts
- arXiv ID: 2510.10241
- Source URL: https://arxiv.org/abs/2510.10241
- Reference count: 40
- Primary result: ImCoref-CeS achieves state-of-the-art coreference resolution with 86.0% F1 on OntoNotes, 81.1% on LitBank, and 73.2% on WikiCoref

## Executive Summary
ImCoref-CeS introduces a novel framework that enhances coreference resolution by combining a supervised neural model with LLM-based refinement. The method improves long-text encoding through a lightweight bridging module, captures positional information better with biaffine scoring, and uses regularization to boost training efficiency. The LLM component validates mentions and corrects coreference clusters by filtering out invalid ones and splitting erroneous clusters. Experiments demonstrate that ImCoref-CeS outperforms existing state-of-the-art methods across multiple benchmarks.

## Method Summary
The framework combines Maverick's detect-then-cluster pipeline with enhanced mention detection and LLM-based refinement. ImCoref uses a DeBERTa-large encoder with a lightweight bridging module (LBM) to propagate semantic information between text segments, addressing the 512-token limitation. Biaffine scoring captures position-dependent relationships between mention start and end tokens. A confidence-based filtering mechanism (HyMR) restricts mention spans to reasonable lengths. The LLM Checker-Splitter agent validates low-confidence mentions and corrects erroneous clusters through two stages: mention checking and cluster splitting/regrouping. The system uses Adafactor optimizer with specific learning rates for different components and employs gradient accumulation for training efficiency.

## Key Results
- Achieves 86.0% average F1 on OntoNotes test set
- Improves performance on LitBank to 81.1% average F1
- Outperforms state-of-the-art methods on WikiCoref with 73.2% average F1
- Shows consistent improvements across all three evaluation metrics (MUC, B³, CEAFφ₄)

## Why This Works (Mechanism)

### Mechanism 1
Lightweight Bridging Module (LBM) mitigates semantic isolation between text segments in long documents. The LBM propagates holistic semantic information from each segment's [SEP] token to modulate the hidden representations of the subsequent segment. Two variants are implemented: FC-based (fully connected) and MHA-based (multi-head attention), both using residual connections and layer normalization. Core assumption: Sequential, left-to-right propagation of segment-level semantics is sufficient to establish cross-segment coreference links without full-document attention. Evidence: LBM-FC achieves 83.1 Avg.F1 vs. 81.4 baseline on OntoNotes; mechanism borrowed from neighboring papers addressing long-text CR.

### Mechanism 2
Biaffine scoring captures position-dependent relationships between mention start and end tokens more effectively than MLP concatenation alone. The biaffine scorer computes a tensor product Xs^T U Xe alongside a linear term W(Xs ⊕ Xe), integrating both bilinear position interactions and feature concatenation. Core assumption: Token position interactions are non-linear and benefit from explicit bilinear modeling rather than implicit MLP learning. Evidence: ImCoref achieves 84.3 Avg.F1 vs. Maverick's 83.6 on OntoNotes with same encoder but different scoring; mechanism borrows from dependency parsing literature.

### Mechanism 3
LLM Checker-Splitter improves precision by validating low-confidence mentions and splitting erroneous clusters, but introduces inference latency. The LLM acts as a two-stage agent: Mention Checker validates candidate spans against local context; Coreference Checker-Splitter verifies clusters and regroups incorrect ones. Filtering mechanisms (η1, η2) restrict LLM calls to the bottom percentile by confidence scores. Core assumption: LLMs possess reasoning capabilities superior to supervised models for validation/regrouping tasks, and low predicted probability correlates with higher error likelihood. Evidence: Removing C-CeS causes larger performance drops than removing M-CeS (84.5 vs. 85.0 on OntoNotes); corroborates LLM reasoning potential while noting hallucination risks.

## Foundational Learning

- **Concept: Detect-then-Cluster Pipeline**
  - Why needed here: ImCoref-CeS extends Maverick's two-stage architecture (mention detection → clustering); understanding this separation is prerequisite to grasping where LBM, biaffine scoring, and HyMR slot in.
  - Quick check question: Can you explain why Maverick predicts start positions first, then conditionally predicts end positions?

- **Concept: Coreference Evaluation Metrics (MUC, B³, CEAFφ₄)**
  - Why needed here: The paper reports Avg.F1 as the mean of these three metrics; each captures different error types (link-based, entity-based, alignment-based).
  - Quick check question: Why might a model improve on B³ but not MUC?

- **Concept: Prompt Engineering for LLM Agents**
  - Why needed here: The Checker-Splitter relies on structured prompts with system instructions, task descriptions, criteria, and output format requirements to elicit consistent behavior.
  - Quick check question: What components does the paper include in every Checker-Splitter prompt template?

## Architecture Onboarding

- **Component map:** Document → Segment Splitter → DeBERTa Encoder → [LBM between segments] → Biaffine Mention Detector → HyMR Filtering → Incremental Clusterer (from Maverick) → [Mention Filter η1] → LLM Mention Checker → [Cluster Filter η2] → LLM Coreference Checker-Splitter → Final Clusters

- **Critical path:**
  1. Document segmentation (512-token chunks with LBM bridging)
  2. Mention detection with biaffine scoring + HyMR constraints
  3. Confidence-based filtering to select candidates for LLM validation
  4. Sequential LLM calls (mention validation → cluster verification → splitting if needed)

- **Design tradeoffs:**
  - LBM-FC vs. LBM-MHA: FC is simpler (534M params) but MHA may capture richer interactions (540M params); paper shows mixed results across datasets.
  - Lmax in HyMR: Lower values (10-20) reduce training time but may miss valid long spans; Lmax=30 is optimal across benchmarks.
  - η threshold: Controls accuracy-latency tradeoff; η=0.6 saturates performance while η→1.0 increases inference time without gains.

- **Failure signatures:**
  - Excessive invalid mentions in output → HyMR may be misconfigured (Lmax too large) or mention filter threshold η1 too aggressive.
  - Cluster regrouping introduces new errors → LLM splitter may conflate similar entities; check prompt clarity for intra-group consistency criteria.
  - Unusually long inference time → LLM API network latency; consider caching or reducing η values.

- **First 3 experiments:**
  1. **Ablate LBM on long documents:** Run ImCoref with independent vs. overlapping vs. LBM-FC vs. LBM-MHA on LitBank (avg. 2105 words/doc). Measure Avg.F1 and training time.
  2. **Tune η thresholds:** Sweep η1, η2 ∈ {0.2, 0.4, 0.6, 0.8, 1.0} on OntoNotes validation set. Plot Avg.F1 vs. inference time to identify optimal operating point.
  3. **Stress test Checker-Splitter:** Manually inject erroneous clusters into ImCoref output and measure LLM correction rate. Analyze failure cases (e.g., nested mentions, ambiguous pronouns).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a full-text clustering fusion strategy be implemented to leverage the LLM splitter's outputs while balancing the trade-off between performance gains and resource consumption?
- Basis in paper: [explicit] Page 4 notes that while a full-text clustering fusion strategy could enhance performance, it presents "non-trivial challenges" and is left for future work. Page 8 (Limitations) further states that trading off performance improvements from such strategies against resource demands is a "challenging direction for future work."
- Why unresolved: The authors focused on the immediate gains of the current Checker-Splitter pipeline and deferred the complexity of merging clusters across the full text.
- What evidence would resolve it: A modified framework incorporating a fusion mechanism that achieves higher F1 scores on OntoNotes/LitBank with a quantified, acceptable increase in computational cost.

### Open Question 2
- Question: How can mention clustering modules be specifically optimized to improve MUC scores in long-text coreference resolution?
- Basis in paper: [explicit] Page 6 states that ImCoref lags behind methods like longdoc and Dual-cache on the MUC metric and that the authors "plan to address this in future work by exploring more suitable mention clustering modules."
- Why unresolved: The study concentrated on advancing mention detection (modifying Maverick), but the specific design of a high-performance clustering algorithm for long texts remains unoptimized.
- What evidence would resolve it: A new clustering algorithm integrated into the ImCoref pipeline that specifically outperforms Dual-cache on the MUC metric while maintaining high Avg. F1 scores.

### Open Question 3
- Question: Can the ImCoref-CeS framework maintain its superior accuracy while mitigating the latency and network dependency introduced by API-based LLMs?
- Basis in paper: [inferred] Page 6 notes that inference time is "network-dependent" (2–5 minutes) and recommends the method for non-real-time scenarios. Page 8 suggests determining how to trade off performance against resource demands is a key challenge.
- Why unresolved: The reliance on large, remote APIs (GPT-4, DeepSeek) conflicts with the "lightweight" and "efficient" nature of the underlying supervised model (ImCoref).
- What evidence would resolve it: Experiments utilizing local, smaller LLMs (e.g., Llama-3-8B) or distilled versions of the Checker-Splitter that achieve comparable F1 scores with significantly reduced latency.

## Limitations
- LLM dependency introduces significant inference latency and operational costs, with performance tightly coupled to API quality and response consistency
- All experiments use English-only datasets, leaving effectiveness on other languages and domains untested
- Critical implementation details including Maverick's exact clustering algorithm and full prompt templates are referenced but not fully specified

## Confidence

- **High Confidence:** LBM mechanism for long-text encoding is well-documented with consistent improvements (1.7-2.7 Avg.F1 gains vs. baseline); biaffine scoring improvement directly comparable to Maverick (84.3 vs. 83.6 Avg.F1)
- **Medium Confidence:** LLM Checker-Splitter refinement shows strong qualitative improvements but lacks full characterization of tradeoff space and ablation studies on prompt quality
- **Low Confidence:** "State-of-the-art" claim lacks direct comparison to concurrent works like MEIC-DT and CLAP which may use different evaluation protocols

## Next Checks

1. **Cross-Dataset Generalization Test:** Evaluate ImCoref-CeS on a non-English dataset (e.g., AnCora for Spanish or RuCoref for Russian) to assess language transfer and identify potential dataset-specific optimizations.

2. **Latency-Aware Threshold Tuning:** Conduct systematic ablation study on η₁ and η₂ thresholds across varying document lengths. Measure marginal benefit of each LLM call to establish cost-aware operating point.

3. **Error Analysis of LLM Refinement:** Manually annotate 100+ erroneous clusters from ImCoref's output and track LLM correction success rate. Categorize failure modes (nested mentions, ambiguous pronouns, entity type mismatches) to identify prompt engineering opportunities.