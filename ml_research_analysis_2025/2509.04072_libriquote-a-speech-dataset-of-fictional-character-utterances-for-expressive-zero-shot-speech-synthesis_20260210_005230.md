---
ver: rpa2
title: 'LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive
  Zero-Shot Speech Synthesis'
arxiv_id: '2509.04072'
source_url: https://arxiv.org/abs/2509.04072
tags:
- speech
- expressive
- audio
- information
- quotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LibriQuote is a large-scale speech dataset derived from audiobooks,
  designed for training and benchmarking expressive zero-shot text-to-speech (TTS)
  systems. It contains 18K hours of speech: 12.7K hours of neutral narration and 5.3K
  hours of expressive character quotations.'
---

# LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis

## Quick Facts
- arXiv ID: 2509.04072
- Source URL: https://arxiv.org/abs/2509.04072
- Authors: Gaspard Michel; Elena V. Epure; Christophe Cerisara
- Reference count: 37
- Primary result: Large-scale dataset (18K hours) for expressive zero-shot TTS with 12.7K hours neutral narration and 5.3K hours character quotations

## Executive Summary
LibriQuote is a comprehensive speech dataset derived from audiobooks, specifically designed to advance expressive zero-shot text-to-speech synthesis. The dataset contains 18,000 hours of speech, split between 12,700 hours of neutral narration and 5,300 hours of expressive character quotations. Each quotation is annotated with contextual information and pseudo-labels for speech verbs and adverbs, enabling more nuanced and expressive speech synthesis. A challenging 7.5-hour test set allows for rigorous evaluation of expressive synthesis capabilities.

## Method Summary
The LibriQuote dataset is constructed by processing LibriVox audiobooks, extracting character quotations and their surrounding context. The dataset includes pseudo-labels for speech verbs and adverbs to capture expressive elements. A dedicated test set of 7.5 hours is curated to evaluate zero-shot expressive synthesis from neutral reference speech. The methodology involves both objective and subjective evaluations to assess the quality and expressiveness of synthesized speech.

## Key Results
- LibriQuote contains 18,000 hours of speech: 12,700 hours neutral narration and 5,300 hours expressive character quotations
- Modern TTS systems fail to match ground truth expressiveness and naturalness in subjective evaluations
- Fine-tuning on LibriQuote significantly improves speech intelligibility in objective metrics

## Why This Works (Mechanism)
The dataset works by providing large-scale, contextually rich speech data that captures the nuances of expressive character utterances. By including pseudo-labels for speech verbs and adverbs, the dataset enables models to learn the relationship between textual cues and expressive speech patterns. The separation of neutral narration and expressive quotations allows for targeted training and evaluation of expressive synthesis capabilities.

## Foundational Learning
- **Expressive speech synthesis**: Understanding how to generate speech with emotional and stylistic variation; needed for creating natural-sounding character voices; quick check: can the model reproduce emotional tones from text descriptions
- **Zero-shot learning**: Ability to generate speech for unseen speakers or styles; crucial for generalization across different characters; quick check: does performance degrade significantly on out-of-domain voices
- **Contextual speech modeling**: Incorporating surrounding narrative context to inform expressive synthesis; helps maintain consistency in character portrayal; quick check: does adding context improve expressiveness ratings

## Architecture Onboarding
**Component Map**: Text -> TTS Model -> Audio Output
**Critical Path**: Input text → Context extraction → Expressive synthesis → Audio generation
**Design Tradeoffs**: Large dataset size enables better generalization but requires substantial computational resources; pseudo-labels provide scalability but may introduce noise
**Failure Signatures**: Models may struggle with subtle emotional nuances, produce unnatural prosody, or fail to maintain character consistency across different contexts
**First Experiments**: 
1. Evaluate baseline TTS performance on the 7.5-hour test set without fine-tuning
2. Fine-tune baseline models on LibriQuote and measure improvements in expressiveness and naturalness
3. Analyze the impact of pseudo-labeled speech verbs and adverbs on synthesis quality through ablation studies

## Open Questions the Paper Calls Out
None

## Limitations
- Subjective evaluation lacks detailed statistical validation and inter-rater reliability metrics
- Pseudo-labels for speech verbs and adverbs may introduce noise affecting model performance
- The 7.5-hour test set, while challenging, represents a relatively small sample for comprehensive benchmarking

## Confidence
- Dataset creation methodology: High
- Zero-shot synthesis evaluation setup: Medium
- Impact of fine-tuning on speech intelligibility: High
- Expressive synthesis quality gap: Low-Medium

## Next Checks
1. Conduct inter-rater reliability analysis on subjective evaluations with multiple annotator pools to establish confidence intervals for expressiveness and naturalness ratings
2. Perform ablation studies removing pseudo-labeled speech verbs and adverbs to quantify their impact on model performance and identify potential noise sources
3. Expand the test set with additional audiobook sources to evaluate model generalization across different narrative styles and character voice types beyond the current 7.5-hour sample