---
ver: rpa2
title: Explaining How Visual, Textual and Multimodal Encoders Share Concepts
arxiv_id: '2507.18512'
source_url: https://arxiv.org/abs/2507.18512
tags:
- features
- text
- clip
- image
- encoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new method to compare visual, textual,
  and multimodal encoders by analyzing the features learned by sparse autoencoders
  (SAEs) on their activations. The authors propose two tools: wMPPC, a weighted similarity
  metric that emphasizes important features, and Comparative Sharedness, which identifies
  features shared with one class of models but not another.'
---

# Explaining How Visual, Textual and Multimodal Encoders Share Concepts

## Quick Facts
- arXiv ID: 2507.18512
- Source URL: https://arxiv.org/abs/2507.18512
- Reference count: 40
- Authors: Clément Cornet; Romaric Besançon; Hervé Le Borgne
- Key outcome: Introduces wMPPC and Comparative Sharedness metrics to analyze feature sharing across 21 visual, textual, and multimodal encoders, finding strongest cross-modal alignment in final layers and identifying VLM-specific features tied to high-level semantic concepts.

## Executive Summary
This paper presents a systematic framework for comparing representations learned by visual, textual, and multimodal encoders using Sparse Autoencoders (SAEs). The authors introduce two metrics—wMPPC (weighted Max Pairwise Pearson Correlation) and Comparative Sharedness—to quantify feature similarity and identify modality-specific concepts. Analysis of 21 encoders across three datasets reveals that cross-modal alignment concentrates in final transformer layers and that text pretraining induces visual features encoding high-level semantic concepts like geographic regions and verbs.

## Method Summary
The method extracts interpretable features from model activations using TopK Sparse Autoencoders (SAEs) with expansion factor 8 and k=32 sparsity. For each encoder, SAEs decompose residual stream activations into sparse feature vectors. The wMPPC metric computes a weighted mean of maximum pairwise Pearson correlations between features across models, where weights are based on cumulative activation strength. Comparative Sharedness identifies features shared with one model class but not another by computing delta scores. The analysis examines 21 encoders (CLIP, SigLIP, DFN, DinoV2, ViT, BERT, DeBERTa, MambaVision) across COCO, Laion-2B, and Flowers datasets.

## Key Results
- Cross-modal wMPPC values range from 0.042 to 0.578, with strongest alignment in final layers
- Visual-only models share minimal features with text encoders (wMPPC ~0.04), while VLMs show substantially higher cross-modal alignment
- VLM-specific visual features include high-level semantic concepts like "to ride" (unifying horses, bikes, skis) and geographical regions
- Features with highest Comparative Sharedness toward text models are the same ones that distinguish VLMs from visual foundation models

## Why This Works (Mechanism)

### Mechanism 1: Feature Importance Weighting Improves Model Comparison
Weighting SAE features by cumulative activation (S_i) produces more meaningful similarity scores than unweighted averaging. wMPPC computes a weighted mean of pairwise Pearson correlations, where weight S_i = Σ_x f_i(x) captures feature activation frequency and strength across the dataset. This prioritizes functionally important features over rarely-activated ones. Evidence shows S_i correlates with ρ_i (0.36), and COCO/CLIP-L/14 exhibits high variability (coefficient of variation 1.91) in feature importance.

### Mechanism 2: Cross-Modal Alignment Concentrates in Final Layers
Semantic features shared across modalities are located primarily in the last transformer layer of each encoder. Vision encoders build increasingly abstract representations through layers—early layers encode low-level visual features with no textual analogue, while final layers encode high-level concepts (objects, scenes, actions) that align with textual semantics. Table 1/2 comparisons show cross-modal wMPPC increases (0.209→0.220) when comparing only last layers, while same-modality comparisons decrease substantially.

### Mechanism 3: Text Pretraining Induces Semantically Abstract Visual Features
Visual features specific to VLMs correspond to high-level semantic concepts present in text encoders but absent from vision-only foundation models. Contrastive image-text training forces visual encoders to develop features that predict textual semantics—geographical regions, verb-based groupings ("to ride"), abstract situations—that transcend purely visual similarity and reflect linguistic category structure. Section 3.4 identifies a typology of VLM-specific features that also align with pure text models.

## Foundational Learning

- **Sparse Autoencoders (SAEs) with TopK sparsity**
  - Why needed here: The entire method depends on extracting interpretable features from model activations. Understanding how SAEs decompose polysemantic neurons into monosemantic features is prerequisite.
  - Quick check question: Explain why TopK sparsity (keeping only k highest activations) might produce more interpretable features than L1 regularization.

- **Pearson Correlation as Feature Similarity**
  - Why needed here: Both wMPPC and Comparative Sharedness rely on computing maximum pairwise Pearson correlations between feature vectors across models.
  - Quick check question: If two features have Pearson correlation 0.8, what does that imply about their activation patterns across a dataset?

- **Vision-Language Model Architectures (CLIP, SigLIP, DFN)**
  - Why needed here: The paper compares 21 encoders across modalities; understanding the difference between VLM encoders (contrastive image-text training) and visual FMs (self-supervised or classification) is essential for interpreting results.
  - Quick check question: Why would a visual encoder trained with contrastive language supervision develop different features than one trained on ImageNet classification?

## Architecture Onboarding

- **Component map:**
  Input Dataset (COCO/LAION/Flowers) -> Forward pass through target encoders -> Activations at each layer -> Train TopK SAEs on residual stream activations (expansion factor 8, k=32) -> Extract feature vectors f for CLS token (global representation) -> Compute S_i (cumulative activation weights) for each feature -> [Branch 1] wMPPC: Compute Pearson correlation matrix -> weighted mean similarity -> [Branch 2] Comparative Sharedness: Compute delta scores -> identify distinguishing features

- **Critical path:** SAE training quality determines everything downstream. If SAEs don't decompose activations into meaningful sparse features, both wMPPC and Comparative Sharedness will measure noise.

- **Design tradeoffs:**
  - **k=32 sparsity level**: Chosen as smallest power-of-2 with no dead latents on COCO/CLIP-L/14. Lower k risks missing concepts; higher k increases compute and may introduce redundancy.
  - **Expansion factor 8**: Standard from prior work. Lower expansion may not capture all concepts; higher increases memory and correlation matrix size.
  - **Last layer vs. all layers**: Last layer focuses on semantic alignment but ignores how concepts develop. All-layer analysis captures hierarchy but dilutes cross-modal signal.

- **Failure signatures:**
  - Dead latents (features never activating): indicates k too low or learning rate issues
  - wMPPC near zero for same-modality comparisons: SAE failed to learn meaningful features
  - Cross-modal wMPPC higher than same-modality on all layers: likely bug in feature extraction or correlation computation
  - Comparative Sharedness identifies only trivial features (e.g., "images containing pixels"): SAE features may be too low-level; try later layers

- **First 3 experiments:**
  1. **Reproduce single wMPPC matrix on COCO with CLIP-ViT-B/32** (smallest model): Verify pipeline produces values matching paper (e.g., CLIP-I→CLIP-I = 1.0, CLIP-I→CLIP-T ≈ 0.21 for all layers).
  2. **Test layer-wise wMPPC on one vision encoder to itself**: Confirm high early-layer self-similarity and diagonal concentration pattern from Figure 1.
  3. **Run Comparative Sharedness analysis on CLIP visual encoder vs. DinoV2 + BERT**: Identify top 10 distinguishing features and manually inspect whether they match the "geographical" or "verb-like" typology from Section 3.4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can wMPPC be effectively adapted for systematic analysis on architectures with large, hierarchical feature maps, such as Swin Transformers or CNNs?
- Basis in paper: [explicit] The "Limitations" section notes that while training SAEs on such models is possible, the resulting huge SAEs prevent the systematic wMPPC analysis used for standard Transformers in this study.
- Why unresolved: The computational cost and architectural mismatch of standard SAEs on hierarchical layers currently preclude a fair, systematic comparison using the proposed metrics.
- What evidence would resolve it: A modified SAE or wMPPC framework that operates efficiently on convolutional or hierarchical features without compromising the comparative structure.

### Open Question 2
- Question: How does the concept sharing of generative multimodal decoders compare to that of the encoders analyzed in this study?
- Basis in paper: [explicit] In the "Perspectives" section, the authors state that "a systematic analysis of wMPPC on generative models with different modalities could provide meaningful insight into their behaviour."
- Why unresolved: The current study focused exclusively on encoders due to their symmetric training objectives, leaving the internal representations of generative models unexplored by these metrics.
- What evidence would resolve it: Application of wMPPC and Comparative Sharedness to text-conditioned generative models (e.g., diffusion models) to quantify their cross-modal alignment.

### Open Question 3
- Question: Can wMPPC serve as a reliable automated metric for filtering or selecting image-text datasets to improve model training?
- Basis in paper: [explicit] The "Perspectives" section suggests that "Comparative studies of multiple image-text datasets could be performed, in order to select or filter datasets used for training new models."
- Why unresolved: The paper demonstrates wMPPC's ability to detect alignment quality differences (e.g., COCO vs. Laion), but has not validated whether optimizing for this metric improves downstream training data curation.
- What evidence would resolve it: Experiments showing that filtering training datasets using wMPPC scores leads to higher performance in trained models compared to random or baseline filtering strategies.

### Open Question 4
- Question: What methods can successfully automate the naming of SAE features by jointly utilizing both visual and textual inputs?
- Basis in paper: [explicit] The "Perspectives" section highlights that "Techniques for automatically naming SAE features considering both images and captions could allow large scale Comparative Sharedness analysis."
- Why unresolved: Current interpretation often relies on qualitative inspection of top-activating images; scalable analysis is hindered by the lack of a unified multimodal naming mechanism.
- What evidence would resolve it: A scalable technique that generates accurate semantic labels for features based on both image content and caption context, enabling quantitative analysis of feature semantics.

## Limitations
- SAE feature quality varies with architecture, dataset, and layer selection; training stability and dead latent rates aren't reported
- The functional significance of wMPPC weighting scheme (S_i) isn't validated—it may prioritize common features over rare but important ones
- VLM-specific feature typology relies on qualitative interpretation and may reflect dataset artifacts rather than text-induced concepts

## Confidence
- **High confidence** in mechanism 1 (feature importance weighting): Correlation between S_i and feature sharing (0.36) provides empirical support, method is well-specified
- **High confidence** in mechanism 2 (final layer alignment): Strong empirical evidence from Table 1/2 showing cross-modal wMPPC increases while same-modality decreases when comparing only last layers
- **Medium confidence** in mechanism 3 (text pretraining effects): Typology is compelling but relies on qualitative interpretation and needs validation across different datasets

## Next Checks
1. **SAE Training Validation**: Run SAE training with multiple random seeds on COCO for CLIP-ViT-L/14 and report dead latent rates and reconstruction loss. Verify that k=32 is sufficient across all tested models.

2. **Cross-Dataset Consistency**: Apply Comparative Sharedness analysis to Flowers dataset and check if the same VLM-specific feature typology (verbs, geographical regions) emerges, or if it's dataset-dependent.

3. **Temporal Stability Analysis**: Train SAEs on COCO from different years or with different augmentations. Test whether wMPPC and Comparative Sharedness scores remain stable or show systematic drift that could indicate dataset artifacts.