---
ver: rpa2
title: An Optimized Pipeline for Automatic Educational Knowledge Graph Construction
arxiv_id: '2509.05392'
source_url: https://arxiv.org/abs/2509.05392
tags:
- edukg
- pipeline
- learning
- knowledge
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an optimized pipeline for automatic Educational
  Knowledge Graph (EduKG) construction from PDF learning materials, implemented within
  the CourseMapper MOOC platform. The process begins by generating slide-level EduKGs
  from individual pages/slides, which are then merged into a comprehensive EduKG for
  the entire learning material.
---

# An Optimized Pipeline for Automatic Educational Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2509.05392
- Source URL: https://arxiv.org/abs/2509.05392
- Reference count: 30
- Optimized EduKG pipeline achieves 47% accuracy with 10x efficiency gains

## Executive Summary
This paper presents an optimized pipeline for automatically constructing Educational Knowledge Graphs (EduKGs) from PDF learning materials within the CourseMapper MOOC platform. The pipeline addresses the limitations of a previous 40% accuracy system by introducing targeted optimizations across text extraction, concept annotation, and retrieval operations. The optimizations include layout-aware text filtering, context-aware entity disambiguation, and replacement of live API calls with a preprocessed Wikipedia dump. These improvements yield a 17.5% accuracy increase to 47% while reducing processing time tenfold, making the system more suitable for real-time educational applications.

## Method Summary
The pipeline processes PDFs through a worker-based architecture using Redis queues, PostgreSQL for embedding storage, and Neo4j for graph persistence. Text extraction employs PDFMiner with heuristics to filter headers, footers, and page numbers based on positional consistency. Keyphrase extraction uses either Mistral-7B LLM or SIFRank SqueezeBERT to identify candidate terms. Concept annotation leverages DBpedia Spotlight with local Wikipedia disambiguation logic using SBERT embeddings for context-aware entity linking. The system weights and prunes concepts using cosine similarity thresholds, then expands relationships using local database lookups rather than live API calls.

## Key Results
- Accuracy improved from 40% to 47% (Â±0.049) through targeted optimizations
- Processing efficiency increased tenfold: slide processing reduced from 24.35s to 2.3s
- Concept expansion latency decreased from 222s to 1.89s per concept
- System achieves domain-independent scalability for diverse educational contexts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layout-aware text filtering improves precision by removing non-content noise before extraction.
- **Mechanism:** Heuristic rules based on font size differences (>0.5pt), text distance (>1.5x line height), and positional similarity (>50% page repetition) classify headers, footers, and page numbers as noise, preventing irrelevant metadata from being indexed as domain concepts.
- **Core assumption:** Non-content text follows consistent structural patterns that distinguish it from semantic content.
- **Evidence anchors:** [Section 4.3] Text similarity analysis classifies repeated location text as noise; [Table 3] shows "Non-content" F1 improving from 0.0 to 0.96.
- **Break condition:** Documents where semantic content is visually indistinguishable from navigation elements or where headers contain critical keyphrases.

### Mechanism 2
- **Claim:** Pre-processed local knowledge base reduces latency and improves reliability over live API calls.
- **Mechanism:** Static Wikipedia dump ingestion pre-computes embeddings for abstracts/categories stored in PostgreSQL, replacing network queries with local database operations for near-instant concept retrieval.
- **Core assumption:** Static Wikipedia snapshot remains sufficiently current for educational domains and storage overhead is acceptable.
- **Evidence anchors:** [Section 4.2] Near-instant access eliminates network latency; [Section 5.1] Concept expansion drops from 222s to 1.89s per concept.
- **Break condition:** Rapidly evolving educational terminology or storage-constrained deployment environments.

### Mechanism 3
- **Claim:** Context-aware disambiguation increases entity linking accuracy by resolving ambiguous keyphrases.
- **Mechanism:** When DBpedia Spotlight returns disambiguation pages, the system retrieves alternative candidates and selects the highest-weighted option based on semantic similarity to slide/learning material context using SBERT embeddings.
- **Core assumption:** Correct entity is semantically closer to document context than default linker output.
- **Evidence anchors:** [Section 4.4] Disambiguation replaces original annotation with highest-weighted concept; [Table 6] shows F1 improving from 0.14 to 0.23.
- **Break condition:** Highly ambiguous terms where document context is too short or generic for effective discrimination.

## Foundational Learning

**Entity Linking & Disambiguation**
- **Why needed:** Pipeline maps raw text keyphrases to standardized DBpedia concepts; understanding ambiguity resolution is essential for debugging incorrect annotations.
- **Quick check:** Given "Jaguar" in a car document, how would disambiguation use surrounding text to select the correct entity?

**Sentence Embeddings (SBERT) & Cosine Similarity**
- **Why needed:** System uses vector embeddings to quantify concept relevance to learning material; understanding similarity metrics is required to adjust pruning thresholds.
- **Quick check:** If two concepts have cosine similarity of 0.95, what does this imply compared to a pair with 0.2?

**Worker-Queue Architecture (Redis/Job Queues)**
- **Why needed:** Optimized architecture moves from synchronous Flask API to asynchronous worker model; understanding this is critical for handling concurrent users.
- **Quick check:** In a worker-based system, what happens to a long-running EduKG job if the worker crashes mid-execution?

## Architecture Onboarding

**Component map:** Ingestion (Flask API) -> Text Extractor (PDFMiner + Heuristics) -> Keyphrase Extractor (Mistral/SIFRank) -> Entity Linker (DBpedia + Local Logic) -> Context Engine (PostgreSQL + SBERT) -> Graph Store (Neo4j) -> Orchestration (Redis + Workers)

**Critical path:** Text Extraction -> Keyphrase Extraction link is most vulnerable; if noise filtering fails, downstream extractors receive garbage input causing semantic drift that weighting cannot fix. Validate extraction quality before tuning linking models.

**Design tradeoffs:**
- Storage vs. Latency: Trades disk space for 10-100x lower latency; avoid storage-constrained instances
- Accuracy vs. Speed: LLM-based extraction offers ~10% higher F1 but requires GPU; SIFRank is faster but less precise
- Recall vs. Precision: Weight threshold (0.192) is aggressive; raising it increases precision but lowers recall

**Failure signatures:**
- Low Accuracy (<45%): Check text extraction for mixed headers/content or verify DBpedia Spotlight availability
- High Latency (>10s/slide): Likely cache miss or fallback to live API; check database connection
- Sparse Graph (Few Nodes): Weight threshold may be too high for domain, pruning valid concepts

**First 3 experiments:**
1. Baseline Validation: Run pipeline on standard PDF with defaults; verify "Non-content" F1 >0.90
2. Latency Stress Test: Upload 5 PDFs simultaneously; monitor Redis queue and worker scaling
3. Threshold Sensitivity: Adjust weight threshold (0.192 to 0.3) on small dataset; measure graph density vs. accuracy

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does human-in-the-loop integration impact final quality and manual effort?
- Basis: [explicit] Conclusion mentions planned incorporation of expert refinement mechanisms
- Why unresolved: Current study focuses on fully automatic pipeline without implemented human feedback
- Evidence needed: User study comparing accuracy and time costs of expert validation vs. manual construction

**Open Question 2**
- Question: Can accuracy ceiling of 47% be breached by replacing DBpedia with alternative knowledge bases?
- Basis: [inferred] Discussion attributes "moderate" improvement to continued DBpedia dependence
- Why unresolved: Authors did not test alternative knowledge bases, identifying this as a constraint
- Evidence needed: Comparative evaluation with different knowledge bases (Wikidata, YAGO) measuring accuracy variance

**Open Question 3**
- Question: How can text extraction be refined to handle complex typography like ligatures and umlauts?
- Basis: [explicit] Conclusion notes text extraction struggles with these typographical features
- Why unresolved: PDFMiner implementation introduces character errors affecting downstream extraction
- Evidence needed: Technical benchmark showing reduced character error rates with complex typography

## Limitations
- Accuracy ceiling of 47% remains insufficient for production educational use where errors propagate to recommendations
- Evaluation relies on expert sampling rather than comprehensive ground truth, creating uncertainty about real-world performance
- Assumes consistent PDF structural patterns that may not generalize to scanned documents, textbooks, or web-based content

## Confidence

**High confidence:** Efficiency improvements (10x latency reduction, specific timing measurements) are well-documented and verifiable through described architecture changes.

**Medium confidence:** Accuracy improvements (17.5% gain) are supported by optimizations, but baseline methodology and sampling strategy introduce uncertainty about generalizability.

**Low confidence:** Claim of "scalable, domain-independent solution" exceeds experimental evidence, as testing was limited to specific PDF formats within CourseMapper.

## Next Checks

1. Cross-format robustness test: Evaluate pipeline on non-slide PDFs (textbooks, research papers, scanned documents) to verify layout filtering across document types, measuring accuracy drop and identifying failure patterns.

2. Longitudinal accuracy validation: Implement continuous evaluation where expert reviewers assess random triple samples over time as system processes diverse content, tracking accuracy degradation with domain complexity.

3. Alternative evaluation methodology: Compare current expert sampling against automated precision/recall metrics using manually-annotated gold standard dataset to quantify sampling bias and establish reliable accuracy benchmarks.