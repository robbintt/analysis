---
ver: rpa2
title: 'HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection'
arxiv_id: '2601.13751'
source_url: https://arxiv.org/abs/2601.13751
tags:
- change
- detection
- image
- images
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HiT, a novel History Injection mechanism for
  Transformer models designed for continuous flood change detection on resource-constrained
  satellites. The method introduces a compact History Embedding that is injected into
  a ViT block, enabling multi-temporal analysis without storing full historical images.
---

# HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection

## Quick Facts
- arXiv ID: 2601.13751
- Source URL: https://arxiv.org/abs/2601.13751
- Authors: Daniel Kyselica; Jonáš Herec; Oliver Kutis; Rado Pitoňák
- Reference count: 40
- Primary result: HiT-Prithvi achieves 43 FPS on Jetson Orin Nano with 99.6% storage reduction

## Executive Summary
This paper introduces HiT (History Injection Transformers), a novel mechanism for continuous flood change detection on resource-constrained satellites. The approach uses a compact History Embedding injected into Vision Transformer blocks, enabling multi-temporal analysis without storing full historical images. The system maintains detection accuracy comparable to bitemporal baselines while dramatically reducing storage requirements and achieving real-time performance on edge hardware.

The proposed HiT-Prithvi model demonstrates practical viability for autonomous satellite-based disaster monitoring, achieving 43 FPS inference on Jetson Orin Nano and producing minimal false positives on unchanged scenes. The History Embedding successfully retains temporal information even when intermediate observations are degraded, suggesting robustness to real-world observation challenges.

## Method Summary
The HiT mechanism introduces a compact History Embedding that captures temporal information from historical observations and injects it into Vision Transformer blocks. This embedding is designed to be significantly smaller than full historical images while retaining essential temporal patterns for flood change detection. The system processes continuous observations from satellites, comparing current imagery against the historical context to detect flood changes. The approach is specifically optimized for onboard deployment on resource-constrained satellites, addressing both computational efficiency and storage limitations.

## Key Results
- HiT-Prithvi achieves 43 FPS inference on Jetson Orin Nano
- 99.6% reduction in storage requirements compared to full-bitmap storage
- Detection accuracy comparable to bitemporal baseline methods
- Minimal false positive rates on unchanged scenes

## Why This Works (Mechanism)
The History Embedding mechanism works by distilling essential temporal patterns from historical observations into a compact representation that can be efficiently injected into the transformer's attention mechanisms. This allows the model to maintain awareness of historical flood conditions without storing full-resolution historical images. The injection occurs at specific transformer blocks where it can most effectively influence the feature extraction process for change detection.

## Foundational Learning
- Vision Transformers (ViT): Why needed - enable efficient spatial feature extraction; Quick check - verify ViT architecture matches implementation details
- History Embedding: Why needed - compress temporal information for onboard storage; Quick check - confirm embedding size reduction from baseline
- Attention mechanisms: Why needed - focus on relevant spatial-temporal features; Quick check - validate attention patterns on sample data
- Edge deployment optimization: Why needed - ensure real-time performance on satellite hardware; Quick check - benchmark FPS on target hardware
- Multi-temporal analysis: Why needed - detect changes over time without full history storage; Quick check - verify temporal consistency in predictions

## Architecture Onboarding

Component Map:
History Embedding Generator -> ViT Backbone -> Change Detection Head -> Output

Critical Path:
Input image → History Embedding generation → Embedding injection into ViT blocks → Feature extraction → Change detection classification → Output mask

Design Tradeoffs:
The primary tradeoff involves embedding size versus temporal information retention. Smaller embeddings provide better storage efficiency but may lose critical temporal patterns. The injection mechanism must balance computational overhead against detection accuracy. The system prioritizes real-time performance over maximum possible accuracy, accepting minor performance reductions to achieve onboard deployment viability.

Failure Signatures:
- False negatives when historical context is insufficient in embedding
- Increased false positives when embedding fails to capture stable background features
- Performance degradation when intermediate observations are severely degraded
- Computational bottlenecks if embedding injection occurs at too many transformer layers

First 3 Experiments:
1. Baseline comparison of embedding size versus detection accuracy across different temporal windows
2. Performance benchmarking of different injection locations within the transformer architecture
3. Degradation analysis testing model performance with controlled quality reduction in historical observations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single geographic area and temporal scope
- Performance scaling with larger image resolutions not addressed
- History Embedding robustness tested only through controlled degradation experiments
- Model performance on rapid-onset versus gradual flood changes not explicitly evaluated

## Confidence
- High confidence: FPS performance metrics, storage reduction measurements, basic accuracy comparisons
- Medium confidence: False positive rates on unchanged scenes, degradation experiment results
- Low confidence: Generalizability across different geographic regions, scalability to larger image resolutions, performance in extreme weather conditions

## Next Checks
1. Test the History Embedding mechanism across multiple geographic regions with varying terrain types and flood characteristics to validate robustness beyond the initial dataset
2. Evaluate the model's performance with real-world data containing partial cloud cover, sensor noise, and irregular temporal sampling to assess practical deployment viability
3. Benchmark against alternative compression methods and lightweight architectures to contextualize the claimed storage and computational efficiency advantages