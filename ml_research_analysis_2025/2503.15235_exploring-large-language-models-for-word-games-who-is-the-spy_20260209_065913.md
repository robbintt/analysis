---
ver: rpa2
title: Exploring Large Language Models for Word Games:Who is the Spy?
arxiv_id: '2503.15235'
source_url: https://arxiv.org/abs/2503.15235
tags:
- game
- reasoning
- player
- llms
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) in
  the word game "Who is the Spy" by proposing a training-free framework. The framework
  leverages Chain-of-Thought (CoT) prompting to enhance LLMs' performance in inferring
  role words and disguising identities.
---

# Exploring Large Language Models for Word Games:Who is the Spy?

## Quick Facts
- arXiv ID: 2503.15235
- Source URL: https://arxiv.org/abs/2503.15235
- Reference count: 25
- Primary result: Training-free Chain-of-Thought framework improves civilian win rates from 7% to 81% in "Who is the Spy" word game

## Executive Summary
This paper presents a training-free framework using large language models (LLMs) to play the word game "Who is the Spy" through structured Chain-of-Thought (CoT) prompting. The approach introduces separate CoT modules for description generation, judgment, and spy disguise, enabling LLMs to reason through the game's logical steps without requiring fine-tuning. Experimental results demonstrate significant performance improvements over baseline methods, with civilian win rates increasing from 7% to 81% while reducing miss rates from 66.7% to 16.7%. The study validates that structured reasoning via CoT effectively addresses LLM challenges like hallucination and improves situational reasoning capabilities in structured game environments.

## Method Summary
The framework employs a zero-shot training approach where LLM agents use three distinct Chain-of-Thought modules: Describe CoT for generating keyword descriptions, Judge CoT for inferring other players' keywords, and Spy CoT for adversarial disguise strategies. The system uses GLM4-9b-Flash with temperature 0.3 and max tokens 10,000, processing multi-round description-judgment-voting pipelines. The approach leverages 100 semantically similar word pairs generated by OpenAI's o1 model, with the Referee controller managing game state and enforcing rules. The framework's modular design forces LLMs to focus on discrete reasoning steps, reducing cognitive load and minimizing hallucination while improving logical consistency in multi-turn gameplay.

## Key Results
- Civilian Win Rate increased from 7% (no CoT) to 81% (full framework with Describe + Judge CoT)
- Civilian Miss Rate decreased from 66.7% to 16.7% with the complete CoT system
- Spy Win Rate decreased from 93% to 25% when civilians used the full framework
- Individual CoT modules show progressive improvement: Judge CoT alone achieves 72% CWR, adding Describe CoT reaches 81% CWR

## Why This Works (Mechanism)

### Mechanism 1
Structured decomposition of game logic into discrete "Describe" and "Judge" phases reduces reasoning complexity and minimizes hallucination. The framework isolates description and inference tasks as separate modules, forcing the model to focus on one logical step at a time rather than holding multiple conflicting states simultaneously. This modularity assumes the LLM possesses sufficient inherent knowledge about semantic relationships between keywords to perform tasks when guided step-by-step.

### Mechanism 2
Intermediate variable deduction (guessing keywords) improves final classification accuracy. Instead of directly inferring "Player X is the spy," the Judge CoT forces the model to first predict specific keywords held by each player, grounding reasoning in concrete linguistic tasks. This assumes accurate keyword attribution is a reliable proxy for identity inference, where correct keyword guessing mathematically determines the spy.

### Mechanism 3
Spy mimicry via targeted feature matching creates effective adversarial camouflage. The Spy CoT module instructs the spy agent to identify a civilian with semantically similar descriptions and mimic that civilian's features in subsequent rounds. This actively exploits civilians' consistency checking logic by blending into the majority group's semantic cluster, assuming civilians vote based on outliers or distinct descriptions.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed: The entire framework relies on Zero-Shot CoT to elicit reasoning from the LLM
  - Quick check: How does forcing an LLM to output intermediate steps change its performance on arithmetic or logic puzzles compared to direct answering?

- **Concept: Semantic Similarity & Hallucination**
  - Why needed: The game is fundamentally about navigating semantic spaces; the paper claims the method fixes hallucination
  - Quick check: Why might an LLM describe a lion as "living in the forest" when prompted without context, and how does constraint checking fix this?

- **Concept: State Management in Agents**
  - Why needed: The paper describes a "scheduling framework" and maintaining history; understanding LLMs require external memory for coherent agent behavior
  - Quick check: In a multi-turn game, if the LLM is not fed the history of previous turns, what happens to its strategic consistency?

## Architecture Onboarding

- **Component map:** Referee -> Role Iterator -> LLM Agents (Describe CoT, Judge CoT, Spy CoT) -> Vote Tally
- **Critical path:** Init: Referee initializes game -> Role Iterator assigns keywords -> Describe Phase: Agents generate descriptions using Describe CoT -> Judge Phase: Agents analyze descriptions with Judge CoT -> Spy Logic: Spy CoT activates for disguise -> Vote: Agents vote based on Judge CoT confidence
- **Design tradeoffs:** Training-free approach chosen for flexibility/low cost but relies heavily on base model capability; keyword inference adds token latency but reduces hallucination
- **Failure signatures:** Random Voting (baseline), Self-Contradiction (hallucination in No-CoT mode), Role Confusion (spy forgets identity in multi-round)
- **First 3 experiments:** 1) Baseline Compliance Check with No CoT to verify random voting behavior, 2) Judge CoT Ablation with simple descriptions to test reasoning improvement, 3) Full Framework Stress Test with all modules monitoring for Spy Confusion

## Open Questions the Paper Calls Out

- How can the Spy CoT module be optimized to prevent the spy agent from confusing its own identity during multi-round interactions?
- Does the Spy CoT module cause the model to generate descriptions that deviate semantically from its actual assigned keyword?
- Is the observed performance improvement generalizable to larger, state-of-the-art models or different word game datasets?

## Limitations
- Framework effectiveness heavily depends on base LLM's reasoning capabilities, may not generalize to smaller models
- Word similarity threshold Î¸ remains unspecified, creating ambiguity in dataset difficulty calibration
- Spy CoT module's tendency to cause role confusion ("state drift") represents acknowledged limitation requiring further optimization

## Confidence
- **High Confidence:** Structured CoT decomposition reducing reasoning complexity (Mechanism 1) - well-supported by dramatic win rate improvements
- **Medium Confidence:** Intermediate variable deduction mechanism (Mechanism 2) - logically sound but effectiveness depends on keyword attribution quality
- **Low Confidence:** Spy mimicry strategy (Mechanism 3) - shows promise in case studies but lacks comprehensive evaluation across diverse scenarios

## Next Checks
1. **Prompt Template Verification:** Replicate framework using only conceptual descriptions, compare performance to reported results to validate CoT structure benefits
2. **Robustness Testing Across Model Sizes:** Run framework on smaller LLMs (e.g., 7B parameters) to determine minimum model capability required
3. **Adversarial Word Pair Evaluation:** Generate word pairs with varying semantic overlap to test performance degradation as distinctions become more subtle