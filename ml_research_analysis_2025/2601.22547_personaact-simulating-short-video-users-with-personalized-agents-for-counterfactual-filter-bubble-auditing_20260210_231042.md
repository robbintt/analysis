---
ver: rpa2
title: 'PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual
  Filter Bubble Auditing'
arxiv_id: '2601.22547'
source_url: https://arxiv.org/abs/2601.22547
tags:
- persona
- filter
- bubble
- auditing
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PersonaAct addresses the challenge of auditing filter bubbles in
  short-video platforms by simulating personalized user behaviors with multimodal
  agents. The framework synthesizes interpretable personas through automated interviews
  that combine behavioral analysis with structured questioning, then trains agents
  using supervised fine-tuning and reinforcement learning on multimodal observations.
---

# PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing

## Quick Facts
- arXiv ID: 2601.22547
- Source URL: https://arxiv.org/abs/2601.22547
- Reference count: 3
- Simulates personalized short-video user behaviors with multimodal agents to audit filter bubbles across Bilibili, Douyin, and Kuaishou

## Executive Summary
PersonaAct addresses the challenge of auditing filter bubbles in short-video platforms by simulating personalized user behaviors with multimodal agents. The framework synthesizes interpretable personas through automated interviews that combine behavioral analysis with structured questioning, then trains agents using supervised fine-tuning and reinforcement learning on multimodal observations. Evaluation demonstrates that PersonaAct substantially improves simulation fidelity over generic LLM baselines, with SMAPE error reduction from 1.161 to 0.617 for watch duration prediction. Deploying agents across Bilibili, Douyin, and Kuaishou reveals significant content narrowing over interactions, with diversity decreasing by 20-40%. Bubble escape potential analysis shows Bilibili has the strongest escape potential (BEP up to 0.3393), while Douyin exhibits higher algorithmic inertia.

## Method Summary
PersonaAct combines behavioral data analysis with automated persona interviews to synthesize interpretable user profiles. The framework uses Qwen2.5-VL-7B-Instruct as the base model, fine-tuned in two stages: first with supervised learning (LoRA rank=64, α=128) on demonstration data, then with GRPO (LoRA rank=8, α=32) using rewards for action correctness, duration accuracy, and format adherence. The system simulates user interactions across three short-video platforms, measuring filter bubble depth through content diversity tracking and Bubble Escape Potential (BEP) via Jensen-Shannon divergence between cultivation and counterfactual phases.

## Key Results
- SMAPE error reduction from 1.161 to 0.617 for watch duration prediction versus generic LLM baselines
- Content diversity decreases by 20-40% across platforms during cultivation phase
- Bilibili shows strongest escape potential (BEP up to 0.3393), Douyin exhibits highest algorithmic inertia (BEP = 0.1149 for Persona B)
- GRPO alone underperforms SFT alone (1.305 vs 0.683 SMAPE), confirming SFT anchoring is critical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured interviews elicit preferences that behavioral logs alone cannot capture, improving persona grounding for simulation.
- Mechanism: The Automated Persona Interview Agent extracts behavioral features from session logs (category distributions, watch durations, engagement rates), then conducts data-driven questioning via qj ∼ πI(·|z, D<j) that adapts to prior responses. This produces persona profiles combining self-narrative summaries, personality traits, and behavioral statistics.
- Core assumption: Users can accurately articulate latent preferences when prompted with concrete behavioral examples.
- Evidence anchors:
  - [abstract] "PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning"
  - [section] "Behavior logs record what users did, but provide limited evidence about why"
  - [corpus] Weak corpus support—neighbor papers focus on behavioral traces without interview components.
- Break condition: If interview responses contradict logged behaviors (e.g., claiming to dislike content they consistently watch long), persona quality degrades.

### Mechanism 2
- Claim: Two-stage training (SFT followed by GRPO) anchors multimodal policy learning within demonstrated behavior manifolds before exploration.
- Mechanism: Supervised fine-tuning establishes behavioral baselines from demonstrations. GRPO then refines the policy πθ(at|p, ot) using reward R = R_action + R_duration + R_format, where R_duration applies duration-sensitive scoring for watch actions.
- Core assumption: The reward formulation adequately captures user utility; misaligned rewards produce plausible but unrealistic behaviors.
- Evidence anchors:
  - [abstract] "trains agents on multimodal observations using supervised fine-tuning and reinforcement learning"
  - [section] Table 3 shows removing behavioral tuning increases SMAPE from 0.617 to 1.161 (88% increase)
  - [corpus] Neighbor paper "Customer-R1" also uses GRPO for persona-conditioned agents, providing convergent evidence for this training approach.
- Break condition: If SFT initialization is poor, GRPO exploration may diverge; Table 2 shows GRPO alone (1.305 SMAPE) underperforms SFT alone (0.683).

### Mechanism 3
- Claim: Sequential counterfactual deployment quantifies algorithmic inertia by measuring distributional divergence after persona reversal.
- Mechanism: After a cultivation phase where the recommender adapts to persona p, a reversed persona p' inverts engagement signals via quantile reversal (sampling from the opposite quantile). BEP(p) = JS(Pp∥Pp') measures how readily the platform responds to behavioral changes.
- Core assumption: Quantile reversal produces meaningfully reversed preferences; bimodal distributions may not invert cleanly.
- Evidence anchors:
  - [abstract] "Bilibili has the strongest escape potential (BEP up to 0.3393), while Douyin exhibits higher algorithmic inertia"
  - [section] Table 4 shows Douyin BEP = 0.1149 for Persona B vs. 0.2896 for Persona A, suggesting persona-algorithm interaction effects
  - [corpus] Neighbor paper "Quantifying the Potential to Escape Filter Bubbles" (arXiv:2512.03067) proposes related BEP concepts, providing conceptual validation.
- Break condition: If reversed personas do not meaningfully differ in preference space (e.g., uniform distributions), BEP measurements become uninformative.

## Foundational Learning

- Concept: Symmetric Mean Absolute Percentage Error (SMAPE)
  - Why needed here: Primary evaluation metric for watch duration prediction; normalizes errors by video length enabling comparison across heterogeneous durations.
  - Quick check question: Why would MAE alone be insufficient for comparing watch duration predictions across videos ranging from 0.5s to 436.8s?

- Concept: Jensen-Shannon Divergence
  - Why needed here: Quantifies bubble escape potential as distributional distance between cultivation and counterfactual phases; bounded [0,1] enables cross-platform comparison.
  - Quick check question: Why use JS divergence rather than KL divergence for measuring content distribution shifts?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables parameter-efficient fine-tuning of Qwen2.5-VL-7B with rank=64, α=128 for SFT; rank=8, α=32 for GRPO.
  - Quick check question: Why might different LoRA configurations be appropriate for SFT versus GRPO stages?

## Architecture Onboarding

- Component map: Data Collection -> Feature Extraction -> Automated Interview -> Persona Synthesis -> SFT Training -> GRPO Fine-tuning -> Platform Deployment -> BEP/Diversity Analysis

- Critical path: Behavioral traces → Feature extraction → Automated interview → Persona profile → SFT initialization → GRPO refinement → Platform deployment → BEP/diversity computation

- Design tradeoffs: 
  - Dataset scale (2 users, 1,719 samples) limits generalization; paper acknowledges need for expansion
  - Session-based temporal splitting prevents leakage but reduces training data
  - Focus on watch duration (99.8% coverage) vs. sparse actions (likes 6.5%, comments/shares 0.3%)

- Failure signatures:
  - GRPO alone underperforms SFT alone (Table 2: 1.305 vs 0.683 SMAPE)—indicates SFT anchoring is critical
  - Bimodal watch duration distributions (Persona B) correlate with lower BEP and stronger algorithmic inertia
  - Low BEP values (<0.15) suggest counterfactual may not be sufficiently distinct

- First 3 experiments:
  1. Replicate SFT+GRPO training on provided dataset; verify SMAPE ≈ 0.617 for Persona A
  2. Deploy trained agent on Bilibili fresh account; confirm 20%+ diversity decrease over 800 interactions
  3. Run ablation removing persona conditioning; expect ~2.6% SMAPE increase per Table 3

## Open Questions the Paper Calls Out

- Question: Does PersonaAct generalize to diverse populations and global platforms like TikTok or YouTube?
  - Basis in paper: The authors state the need to "Expand dataset scale with more users and platforms such as TikTok, YouTube, and Instagram to improve generalization."
  - Why unresolved: The current study validates the framework on only two distinct personas and three Chinese platforms.
  - What evidence would resolve it: Demonstrating stable SMAPE error reduction across a dataset with >100 users on international platforms.

- Question: How do filter bubbles evolve and solidify over interaction periods longer than the current experimental window?
  - Basis in paper: Future work proposes to "Extend observation windows to study how filter bubbles evolve and solidify over time."
  - Why unresolved: Current auditing is limited to sessions of ~800 interactions, which may not capture long-term entrenchment effects.
  - What evidence would resolve it: Longitudinal deployment of agents showing content diversity and BEP trends over weeks of simulated activity.

- Question: Can algorithmic interventions effectively mitigate the "algorithmic inertia" and low escape potential observed in platforms like Douyin?
  - Basis in paper: The paper suggests "Test mitigation strategies such as diversity injection and friction mechanisms, and quantify their effects."
  - Why unresolved: The work quantifies bubble depth (low BEP) but does not evaluate methods to improve the user's ability to escape established bubbles.
  - What evidence would resolve it: Comparative auditing showing a significant increase in Bubble Escape Potential (BEP) when mitigation strategies are active.

## Limitations
- Small dataset scale (2 users, 1,719 samples) constrains generalizability of persona simulation and platform auditing results
- Platform-specific implementation details for agent deployment remain underspecified, particularly regarding API access and interaction mechanisms
- Category classification system for diversity and BEP calculations is not fully described, limiting reproducibility

## Confidence
- **High Confidence**: SMAPE improvement metrics (0.617 vs 1.161 baseline) and diversity reduction findings (20-40% narrowing) are well-supported by quantitative results and reproducible
- **Medium Confidence**: Bubble escape potential conclusions are moderately supported, though BEP values below 0.15 suggest counterfactuals may not be sufficiently distinct for robust measurement
- **Low Confidence**: Generalization across user personas and platforms is limited by the small sample size and lack of cross-platform category classification details

## Next Checks
1. **Dataset Expansion Validation**: Replicate the SFT+GRPO training pipeline with additional user personas (target: 5+ users, 4,000+ samples) to verify whether SMAPE improvements and BEP patterns hold across broader user diversity.

2. **Cross-Platform Consistency Check**: Deploy the PersonaAct framework on an additional short-video platform (e.g., YouTube Shorts or Instagram Reels) using the same methodology to test whether algorithmic inertia patterns generalize beyond the current three platforms.

3. **Persona Contradiction Stress Test**: Conduct a controlled experiment where interview responses deliberately contradict logged behaviors to quantify the degradation in simulation fidelity, measuring the threshold at which persona quality becomes unreliable.