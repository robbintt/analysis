---
ver: rpa2
title: 'Direct Token Optimization: A Self-contained Approach to Large Language Model
  Unlearning'
arxiv_id: '2510.00125'
source_url: https://arxiv.org/abs/2510.00125
tags:
- unlearning
- forget
- tokens
- utility
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Direct Token Optimization (DTO), a self-contained
  method for unlearning fine-tuned large language models without external resources
  like auxiliary models, retain datasets, or commercial AI services. DTO identifies
  two types of tokens in a forget set: target tokens that encode critical knowledge
  for unlearning and non-target tokens that preserve model utility.'
---

# Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning

## Quick Facts
- arXiv ID: 2510.00125
- Source URL: https://arxiv.org/abs/2510.00125
- Reference count: 12
- Authors: Hong kyu Lee; Ruixuan Liu; Li Xiong
- Primary result: Achieved up to 16.8× improvement in forget quality compared to state-of-the-art baselines while remaining self-contained

## Executive Summary
Direct Token Optimization (DTO) introduces a novel self-contained approach to unlearning fine-tuned large language models without requiring external resources like auxiliary models, retain datasets, or commercial AI services. The method identifies two types of tokens in a forget set: target tokens that encode critical knowledge for unlearning and non-target tokens that preserve model utility. By using a delta-score metric to identify target tokens through perturbation analysis and applying gradient ascent to reduce their influence, DTO achieves significantly better forget quality than existing methods. Experiments on TOFU and MUSE benchmarks demonstrate DTO's effectiveness, achieving a forget quality of 0.918 on Llama-2-7B with 1% forget set compared to 0.054 for FLAT.

## Method Summary
DTO is a self-contained unlearning method that operates by identifying and optimizing specific tokens in the forget set. The approach splits each sequence into prefix and suffix components, then uses a delta-score metric to measure the impact of perturbing individual prefix tokens on the suffix generation. Tokens with high delta-scores are identified as target tokens encoding memorized knowledge and are optimized via gradient ascent to reduce their influence. Non-target tokens are optimized to minimize KL-divergence with the original model, preserving linguistic fluency and utility. The method alternates between these two optimization objectives with gradient orthogonalization to prevent conflicts, achieving effective unlearning without external resources.

## Key Results
- Achieved forget quality of 0.918 on Llama-2-7B with 1% forget set compared to 0.054 for FLAT baseline
- Demonstrated 16.8× improvement in forget quality over state-of-the-art methods
- Successfully unlearned sensitive content from Harry Potter books while maintaining general language capabilities
- Maintained model utility through KL-divergence regularization without requiring retain datasets

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Causal Tracing (Delta-Score)
DTO identifies target tokens through perturbation-based causal tracing, assuming memorized knowledge is locally triggered by specific anchor tokens. The method measures the impact of prefix token perturbations on suffix generation by calculating the delta-score - the increase in NLL of the original suffix when a prefix token is replaced. This approach isolates specific triggers of memorization rather than relying on frequency or semantic tags. The mechanism may fail if knowledge is distributed diffusely across all tokens rather than localized to specific triggers.

### Mechanism 2: Gradient Ascent on Latent Triggers
Once target tokens are identified, DTO applies gradient ascent on their loss to selectively erase the pathway to memorized data while preserving general linguistic parameters. This approach pushes the model's weights away from configurations that generate memorized tokens, reducing the probability of generating critical trigger tokens. The mechanism assumes parameters encoding specific memorized facts are sufficiently entangled with target tokens. It may fail if the model has redundant encoding pathways where destroying one token's probability doesn't prevent fact retrieval via alternative sequences.

### Mechanism 3: Self-Distillation for Utility Anchoring
DTO maintains model utility by minimizing KL-divergence between the unlearned model and original model on non-target tokens. This self-distillation approach forces the model to retain its original reasoning structure and fluency, effectively masking the unlearning operation from the rest of the model. The method assumes knowledge cleanly separates into target tokens (specific facts) and non-target tokens (general language structure). It may fail if non-target context tokens are inextricably linked to the specific reasoning required for the forget set, causing KL regularization to conflict with unlearning.

## Foundational Learning

- **Gradient Ascent (in Unlearning)**: Why needed: Understanding that unlearning often requires pushing weights away from specific data points rather than toward minimizing error. Quick check: If you perform gradient descent on target tokens instead of ascent, are you training or unlearning the model?

- **KL-Divergence (Knowledge Distillation)**: Why needed: Crucial for understanding how DTO maintains model utility without a retain set by using the original model as a static teacher. Quick check: If the original model is biased, what does minimizing KL-divergence on non-target tokens preserve?

- **Causal Tracing / Perturbation**: Why needed: Explains the delta-score mechanism, which actively intervenes in the input to see what breaks the output, providing causal rather than correlational signals of importance. Quick check: Why is perturbing a prefix token and measuring the suffix loss a better indicator of "memorization trigger" than just looking at token frequency?

## Architecture Onboarding

- **Component map**: Input (Forget Set D_F) -> Tokenizer (Splits sequences into Prefix/Suffix) -> Perturbation Engine (Replaces prefix tokens with UNK/special tokens) -> Scorer (Computes NLL difference/Delta-score) -> Selector (Selects Top-k% as Target tokens T, rest as Non-target N) -> Optimizer (Dual-path: Gradient Ascent on T, Gradient Descent on KL(N) vs Original Model) -> Gradient orthogonalization (Projects unlearning gradients orthogonal to utility gradients)

- **Critical path**: The delta-score calculation is the self-contained "brain" of the method. If the selection of k or the suffix ratio is wrong, the method either fails to forget (too few tokens) or destroys model utility (too many).

- **Design tradeoffs**: 
  - Suffix Ratio: Smaller suffix focuses on immediate dependencies but might miss long-range knowledge; larger suffix captures full knowledge but introduces noise
  - Top-k% selection: Low k preserves utility better but risks incomplete unlearning; high k ensures forgetting but risks over-unlearning
  - Self-contained vs. External: DTO avoids retain sets and auxiliary models, increasing privacy and lowering cost but removing the safety net of a gold standard retain set

- **Failure signatures**: 
  - Over-unlearning: Model outputs gibberish or refusal templates for unrelated queries (likely k is too high or learning rate too high)
  - Under-unlearning: Model still generates the sensitive answer verbatim (likely suffix ratio is too small, missing the knowledge trigger, or k is too low)
  - Mode Collapse: Model generates repetitive text (gradient conflict between ascent and KL-term; check orthogonalization)

- **First 3 experiments**:
  1. Hyperparameter Sweep (k and Suffix Ratio): Run DTO on TOFU 1% varying k in [0.1, 0.2, 0.3] and suffix ratio in [0.15, 0.25, 0.5] to find Pareto frontier of Forget Quality vs. Model Utility
  2. Ablation on Gradient Orthogonalization: Implement DTO with and without gradient projection to measure stability improvement in training loss curves
  3. Token Selection Baseline: Compare delta-score selection vs. random token selection or high-frequency token selection to prove causal perturbation is necessary

## Open Questions the Paper Calls Out

- **Integrating preference optimization**: The paper aims to incorporate preference optimization to improve the model utility forget quality trade-off, as current gradient ascent and KL-divergence methods may not optimally balance these conflicting objectives.

- **Refining delta-score metric**: The authors seek to further improve delta-score to more effectively select target tokens in each sequence, as the current automatic selection may occasionally miss critical tokens compared to LLM-based selection.

- **Adapting to larger forget sets**: Regarding the 10% forget set results, the paper states unlearning TOFU 10% is challenging without the retain dataset and aims to improve this in future studies, as current DTO yields only 0.0004 forget quality on TOFU 10%.

## Limitations

- Delta-score mechanism may fail for distributed or contextual knowledge where single-token perturbations don't reveal causal triggers
- Assumes clean separation between target tokens (memorized facts) and non-target tokens (general language), which may not hold for entangled representations
- Self-contained constraint limits ability to verify that forgetting doesn't destroy general reasoning ability without retain sets
- Effectiveness of perturbation-based selection may vary significantly with model size and architecture complexity

## Confidence

- Direct Token Optimization achieves superior forget quality: High confidence (clear quantitative improvements in controlled benchmarks)
- DTO maintains model utility through KL-divergence regularization: Medium confidence (demonstrates good utility preservation but relies on strong separability assumptions)
- Delta-score effectively identifies memorization triggers: Medium confidence (well-specified mechanism shows empirical effectiveness but may not generalize to all memorization patterns)

## Next Checks

- Cross-architecture validation: Test DTO on different model families (Mistral, Gemma, Phi) and scales (1B, 7B, 13B) to verify delta-score mechanism generalizes beyond Llama architectures
- Knowledge type sensitivity analysis: Systematically evaluate DTO on different knowledge types (direct memorization, reasoning patterns, procedural knowledge) to determine method's limitations
- Gradient orthogonalization impact study: Conduct ablation experiments comparing DTO with and without gradient orthogonalization across multiple learning rates and forget set sizes to quantify its contribution