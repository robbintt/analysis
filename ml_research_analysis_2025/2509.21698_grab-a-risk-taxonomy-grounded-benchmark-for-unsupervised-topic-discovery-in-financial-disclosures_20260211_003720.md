---
ver: rpa2
title: 'GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery
  in Financial Disclosures'
arxiv_id: '2509.21698'
source_url: https://arxiv.org/abs/2509.21698
tags:
- risk
- topic
- sentence
- labels
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAB provides a publicly available benchmark for unsupervised risk
  categorization in financial 10-K disclosures. It combines finance-tuned attention,
  YAKE-based keyphrase signals, and taxonomy-aware matching to generate span-grounded
  sentence labels at scale, using a curated risk taxonomy mapped to five macro risk
  classes.
---

# GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures

## Quick Facts
- arXiv ID: 2509.21698
- Source URL: https://arxiv.org/abs/2509.21698
- Reference count: 16
- Provides first publicly available benchmark for unsupervised risk categorization in 10-K Item 1A disclosures, combining finance-tuned attention, YAKE-based keyphrase signals, and taxonomy-aware matching to generate span-grounded sentence labels at scale.

## Executive Summary
GRAB introduces a novel benchmark for evaluating unsupervised topic models on financial risk discovery in SEC 10-K disclosures. It generates 1.61 million sentence-level weak labels without manual annotation by blending FinBERT attention, YAKE keyphrase signals, and risk-aware collocation matching grounded in a curated taxonomy. The benchmark enables standardized comparison across classical, embedding-based, neural, and hybrid topic models using metrics that capture both risk-category alignment and topical coherence. Evaluation on 8,247 S&P 500 filings (2001–2025) reveals that contextualized models like CTM improve label-aware performance while traditional models remain competitive, highlighting the need for domain-aware evaluation beyond coherence metrics.

## Method Summary
GRAB generates weak labels by extracting Item 1A sentences from 10-K filings, computing per-token importance via a FinBERT attention-YAKE blend (λ=0.8), and applying taxonomy-grounded boosts for unigrams (β_uni=1.5) and collocations (β_col=1.2). The top-10 tokens per sentence are aggregated to 21-subcategory vectors, then rolled up to 5 macro risk classes. Topic models (LDA, CTM, BERTopic, etc.) are trained on train split (≤2022), and their sentence-topic mixtures are evaluated via logistic regression predicting macro classes (Accuracy, Macro-F1), Topic BERTScore, and Effective Number of Topics.

## Key Results
- CTM achieves best Macro-F1 (0.41) on 5 macro risk classes, outperforming classical models while maintaining topical coherence
- Effective Number of Topics ranges 1.25–1.56 across models, indicating predominantly hard assignments despite soft mixture representations
- Traditional models (LDA, GLDA) remain competitive in Accuracy despite lower Macro-F1, suggesting robustness to majority-class skew
- High Topic BERTScore (0.86–0.88) across all models indicates strong semantic coherence regardless of risk-category alignment

## Why This Works (Mechanism)

### Mechanism 1
Blending finance-tuned attention with unsupervised keyphrase signals yields more reliable token importance than either signal alone. FinBERT attention captures contextual relevance learned from financial text, while YAKE provides document-local statistical salience. The weighted blend (λ=0.8 attention, 0.2 YAKE) prioritizes learned semantics while retaining keyphrase grounding. Per-sentence max-normalization ensures comparability across sentences of varying length and complexity. Core assumption: Financial domain pretraining transfers to risk-specific token importance within 10-K prose. Evidence anchors: abstract mentions combining FinBERT token attention, YAKE keyphrase signals, and taxonomy-aware collocation matching; Section 2.2 defines Ii = λ Ai + (1-λ)Yi with λ=0.8 in main comparisons. Break condition: If FinBERT attention systematically misweights risk-irrelevant financial jargon, or if YAKE scores are dominated by boilerplate phrases, the blend inherits both failure modes.

### Mechanism 2
Taxonomy-grounded collocation matching sharpens risk signal without manual annotation. Multiword financial phrases (e.g., "cash flow") are matched via boundary-aware, case-insensitive patterns drawn from the curated taxonomy and a finance dictionary. Matched spans receive a localized boost (β_col=1.2), prioritizing domain-specific compounds that single-word matching would miss. Unigram matches from the 21-subcategory lexicon receive higher boost (β_uni=1.5), reflecting their direct taxonomy anchor. Core assumption: The taxonomy terms and collocation list provide sufficient coverage of risk-relevant expressions in 10-K disclosures. Evidence anchors: Section 2.3 describes collocation boost detection with boundary-aware, space/hyphen–tolerant, parenthesis-aware matcher; Section 2.3 states phrase list is union of taxonomy-derived expressions and curated finance dictionary. Break condition: If risk categories frequently appear as novel compounds or paraphrases outside the curated lists, the enhancer underweights true risk signals.

### Mechanism 3
Evaluation at the macro level (5 classes) using sentence–topic mixtures provides a robust proxy for risk-category recovery. Weak labels are generated at the 21-subcategory level, then deterministically rolled up to 5 macro classes for evaluation. A logistic classifier is trained on topic mixtures θ(s) to predict macro classes, yielding Accuracy and Macro-F1. This tests whether discovered topics align with practitioner-relevant risk categories, not just surface coherence. Core assumption: Topics that predict macro classes well are more useful for downstream financial analysis. Evidence anchors: Section 2.5 trains one-vs-rest logistic regression on each model's sentence–topic mixtures to predict five macro risk classes; Table 1 shows CTM achieves best Macro-F1 (0.41), suggesting contextualized topics better capture risk categories. Break condition: If macro-class prediction can be solved via superficial cues (e.g., sector keywords) without recovering genuine risk structure, the metric may overstate utility.

## Foundational Learning

- **Concept: Weak supervision via taxonomy anchoring**
  - Why needed here: GRAB generates 1.61M sentence labels without manual annotation by mapping high-importance tokens to a curated risk taxonomy. Understanding how label noise propagates to evaluation is essential.
  - Quick check question: If a sentence contains "interest rate" but discusses customer loyalty, would the weak label correctly distinguish financial vs. operational risk?

- **Concept: Topic mixture extraction across heterogeneous models**
  - Why needed here: GRAB evaluates LDA, CTM, BERTopic, etc., each with different θ(s) representations (discrete counts, variational means, soft cluster assignments). Normalizing these to comparable mixtures is non-trivial.
  - Quick check question: How would you derive a soft topic mixture θ(s) for BERTtopic's HDBSCAN outliers?

- **Concept: Chronological splits for temporal financial data**
  - Why needed here: 10-K filings span 24 years; train/dev/test splits by release year prevent lookahead bias critical for financial applications.
  - Quick check question: Why might random splitting overestimate model performance on future risk disclosures?

## Architecture Onboarding

- **Component map:** SEC 10-K Item 1A extraction → sentence segmentation → light normalization → FinBERT attention + YAKE blend → taxonomy/collocation boosts → top-m token aggregation → 21-dim subcategory vectors → 5-class macro rollup → topic model training → θ(s) extraction → logistic classifier → metrics

- **Critical path:** 1. Extract and normalize Item 1A sentences (Appendix B). 2. Compute per-token importance via FinBERT + YAKE blend (Section 2.2). 3. Apply risk-aware boosts using taxonomy and collocation lists (Section 2.3). 4. Aggregate top-m tokens to weak labels (Section 2.4). 5. Train topic models on train split; extract θ(s) for test sentences (Appendix E). 6. Evaluate macro-class prediction, Topic BERTScore, Effective Number of Topics (Section 2.5).

- **Design tradeoffs:** λ=0.8 prioritizes attention over YAKE; adjusting may shift sensitivity to domain terms vs. statistical salience. K=21 topics aligns with fine-grained taxonomy, but evaluation uses 5 macro classes—topic–class alignment may vary. Chronological split ensures realism but test period (2024-2025) is smaller; class balance may shift.

- **Failure signatures:** Very low Macro-F1 with high Topic BERTScore: topics are semantically coherent but misaligned with risk categories. Effective Number of Topics near 1 for all models: hard assignments may not capture multi-risk sentences. Large gap between Accuracy and Macro-F1: model overfits majority classes, underperforms on long-tail risks.

- **First 3 experiments:** 1. Reproduce baseline metrics (Table 1) on the fixed test split to validate pipeline integration. 2. Ablate the collocation boost (set β_col=1.0) and measure Macro-F1 change to assess enhancer contribution. 3. Vary K (e.g., 5, 21, 50) and observe how Effective Number of Topics and Macro-F1 shift across topic granularity.

## Open Questions the Paper Calls Out

- To what extent do unsupervised risk categories derived via GRAB correlate with external market conditions? The authors conclude that future work should address evaluation that bridges discovered risk categories to market conditions and compliance outcomes. This is unresolved because the benchmark currently evaluates intrinsic label recovery rather than extrinsic financial utility. Demonstrating significant predictive power between GRAB topic shifts and stock volatility or credit ratings would resolve it.

- How can topic models be made robust to boilerplate language and context shifts in financial disclosures? The authors highlight robustness to boilerplate and context shifts as a necessary future direction, noting that boilerplate induces spurious topics. This is unresolved because standard frequency-driven approaches fail to distinguish material risks from repetitive legal phrases (e.g., "safe harbor"). An approach that filters or down-weights boilerplate, resulting in improved Macro-F1 for long-tail risk categories, would resolve it.

- Can neural topic models be refined to produce better-calibrated, taxonomy-aligned sentence-level mixtures? The authors identify better-calibrated sentence-level mixtures aligned with risk taxonomies as a future research direction. This is unresolved because results show a trade-off where high-performing models (CTM) yield diffuse mixtures (high Effective Number of Topics), while decisive models perform poorly on label recovery. A model architecture that maintains low entropy (decisiveness) without sacrificing Macro-F1 performance would resolve it.

## Limitations

- The benchmark's utility depends critically on the quality and coverage of its curated taxonomy and collocation lists, which are not publicly provided, making independent validation impossible.
- The evaluation design assumes macro-class alignment is the appropriate proxy for risk-category recovery, but this may conflate sector-specific risk patterns with genuine risk structure, particularly if long-tail risks are underrepresented.
- The use of a fixed chronological split improves realism but also reduces test-set size and may amplify temporal domain shift, especially given the rapid evolution of risk disclosures post-2020.

## Confidence

- **Mechanism 1 (Attention + YAKE blend)**: Low confidence. The blending approach is methodologically sound, but no ablation or sensitivity analysis is reported to confirm that the 0.8/0.2 weighting meaningfully improves over either signal alone.
- **Mechanism 2 (Taxonomy-grounded collocation matching)**: Low confidence. The collocation boost is described but not isolated in results; no evidence shows it meaningfully improves weak-label quality or downstream topic recovery.
- **Mechanism 3 (Macro-level evaluation via logistic regression)**: Medium confidence. The evaluation design is coherent and aligns with prior work, but the assumption that macro-class prediction equates to risk-structure recovery is untested.

## Next Checks

1. **Ablation of the collocation boost**: Re-run weak-label generation with β_col=1.0 (no boost) and measure the change in Macro-F1 and Effective Number of Topics across all topic models to isolate the contribution of multiword phrase matching.

2. **Sensitivity to the attention/YAKE weighting (λ)**: Evaluate Macro-F1 across λ ∈ {0.5, 0.7, 0.8, 0.9, 1.0} to determine if the reported 0.8 value is optimal or merely conventional, testing the robustness of the blended importance mechanism.

3. **Temporal validation on future disclosures**: Apply the trained logistic classifier to 2026-2027 filings (if available) to assess whether topics discovered in 2001-2025 remain predictive of macro risk classes in a truly out-of-distribution temporal setting, checking for lookahead bias and temporal drift.