---
ver: rpa2
title: Stable diffusion models reveal a persisting human and AI gap in visual creativity
arxiv_id: '2511.16814'
source_url: https://arxiv.org/abs/2511.16814
tags:
- human
- creativity
- genai
- creative
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study compared visual creativity between humans (artists and\
  \ non-artists) and AI (Stable Diffusion) in generating images from abstract stimuli.\
  \ Using the Test of Creative Imagery Abilities (TCIA), researchers found a clear\
  \ creativity gradient: visual artists non-artists \u2265 human-guided AI self-guided\
  \ AI."
---

# Stable diffusion models reveal a persisting human and AI gap in visual creativity

## Quick Facts
- arXiv ID: 2511.16814
- Source URL: https://arxiv.org/abs/2511.16814
- Reference count: 0
- Human artists outperformed AI in visual creativity, with human guidance significantly boosting AI performance to near-human levels

## Executive Summary
This study compared visual creativity between humans (artists and non-artists) and AI (Stable Diffusion) using the Test of Creative Imagery Abilities (TCIA). The research revealed a clear creativity gradient: visual artists > non-artists ≥ human-guided AI > self-guided AI. Human raters consistently scored human-generated images as more creative than AI-generated ones, while AI required human conceptual guidance to approach human performance levels. The findings suggest AI faces unique challenges in visual creativity domains where perceptual nuance and contextual understanding are crucial.

## Method Summary
The study used the TCIA to evaluate visual creativity, where participants generated drawings from 12 abstract stimuli. Four categories were compared: 27 visual artists, 26 non-artists, and Stable Diffusion XL (fine-tuned via LoRA on human drawings) in two conditions (self-guided with basic prompts and human-inspired with added conceptual ideas). Images were rated by 255 human raters and GPT-4o on five dimensions: Liking, Vividness, Originality, Aesthetics, and Curiosity. Linear mixed-effects models with beta regression analyzed the creativity scores.

## Key Results
- Visual artists consistently produced the most creative images, followed by non-artists, human-guided AI, and self-guided AI
- Human raters scored human-generated images significantly higher than AI-generated ones
- GPT-4o evaluations showed different patterns, rating AI images more favorably than human raters did
- Human guidance markedly improved AI performance, bringing it close to non-artist human levels

## Why This Works (Mechanism)

### Mechanism 1: Human Guidance as Contextual Scaffolding
The Stable Diffusion model relies on probabilistic associations rather than embodied experience. When users inject specific ideas into prompts, they simulate the "contextual sensitivity" and "real-world connection" the architecture lacks, effectively bridging the gap between abstract visual stimuli and coherent creative output. The model cannot autonomously generate high-creativity interpretations from ambiguous visual stimuli without semantic anchors.

### Mechanism 2: Perceptual Nuance Bottleneck
Text-to-image architectures face unique deficits in visual creativity compared to linguistic creativity because visual generation requires translating abstract, non-semantic stimuli into novel depictions. The "solipsistic" nature of current models—trained on fixed datasets without interactive environmental feedback—limits their ability to handle the uncertainty of open-world visual problems.

### Mechanism 3: Evaluation Alignment Failure
LLM-based evaluators (GPT-4o) fail to mirror human aesthetic judgment, over-rating AI-generated visuals and lacking discrimination. GPT-4o assesses images using different latent strategies than humans, likely optimizing for recognizable features rather than the "contextual sensitivity" humans apply.

## Foundational Learning

**Concept: Test of Creative Imagery Abilities (TCIA)**
- Why needed: This benchmark distinguishes the study from standard Divergent Thinking text tests by using abstract visual stimuli to force "creative mental imagery"
- Quick check: How does the TCIA differ from the Torrance Tests of Creative Thinking?

**Concept: ControlNet Conditioning**
- Why needed: Understanding how abstract shapes were fed into Stable Diffusion is critical for the SG vs. HI comparison
- Quick check: What is the functional role of ControlNet in the SG-GenAI vs. HI-GenAI comparison?

**Concept: Latent Space Solipsism**
- Why needed: The paper argues AI is "inherently solipsistic" and operates in a "small world" defined only by training data
- Quick check: Why does the paper claim SG-GenAI failed to trigger creative generation from abstract stimuli?

## Architecture Onboarding

**Component map:** Abstract visual stimuli (TCIA shapes) -> ControlNet (shape) + Text Prompt (Basic vs. Human-Inspired) -> Stable Diffusion XL (fine-tuned via LoRA on human drawings) -> Human Raters (N=255) vs. GPT-4o API

**Critical path:** The prompt engineering step is the single point of failure/leverage. The transition from [Abstract Shape] -> [Text Prompt] -> [Image] determines the creativity score.

**Design tradeoffs:**
- *Fine-tuning:* The authors fine-tuned SDXL on human drawings to mimic "sketch" style, improving aesthetic matching but potentially narrowing generative diversity
- *Rating Dimension:* Using a 5-dimension scale vs. a single score increases granularity but complicates factor analysis

**Failure signatures:**
- The "Uncanny Valley" of Abstraction: SG-GenAI images were rated lowest because the model tried to resolve abstract shapes into literal objects without a guiding semantic concept
- Rater Bias: Relying solely on GPT-4o for evaluation would have falsely concluded that AI and Human creativity are equivalent

**First 3 experiments:**
1. Prompt Sensitivity Analysis: Run the same TCIA stimuli with varying levels of prompt abstraction to map the "creativity cliff" where SG-GenAI fails
2. Cross-Model Evaluation: Feed generated images into CLIP-based aesthetic scorers vs. GPT-4o vs. Humans to verify if the "alignment gap" is specific to LLM-based vision models
3. Fine-Tuning Ablation: Generate images with the base SDXL model (no LoRA fine-tuning on sketches) to determine if the "human-like sketch" style influenced perceived creativity ratings

## Open Questions the Paper Calls Out

**Open Question 1:** Can multimodal models be effectively aligned to replicate human aesthetic judgment and creativity appraisal in visual domains?
- Basis: GPT-4o ratings diverged significantly from human raters, assigning higher scores to AI images
- Why unresolved: Unclear if gap is due to fundamental architectural differences or lack of specific aesthetic training data
- Evidence needed: Fine-tuning multimodal models on datasets of human-rated creative imagery to test correlation improvement

**Open Question 2:** What computational mechanisms are required for generative models to achieve autonomous creative agency without high-specificity human prompting?
- Basis: Models currently rely on human guidance to "simulate a real-world connection" and do not yet reach "autonomous human-level performance"
- Why unresolved: Unclear if performance gap is an insurmountable lack of agency or if future architectural changes could allow spontaneous ideation
- Evidence needed: Testing models equipped with internal feedback loops or "world models" on open-ended tasks without external semantic injection

**Open Question 3:** Does the observed creativity gradient generalize across different generative architectures?
- Basis: Study relied exclusively on Stable Diffusion; authors call for "broader measures" but didn't test other architectures
- Why unresolved: Visual creativity deficit might be specific to latent diffusion process rather than universal limitation
- Evidence needed: Replicating TCIA protocol using diverse state-of-the-art architectures (e.g., DALL-E 3, Midjourney) to compare relative creative standing

## Limitations

- The TCIA stimuli represent a narrow domain of visual creativity that may not generalize to other creative domains
- The human-guided AI condition's performance boost depends entirely on the quality and specificity of human-provided ideas
- The GPT-4o evaluation methodology may still miss subtle human aesthetic preferences due to training data biases
- The fine-tuning procedure for Stable Diffusion (LoRA hyperparameters) is only partially specified

## Confidence

- **High Confidence:** The creativity gradient (Artists > Non-Artists ≥ Human-Guided AI > Self-Guided AI) is robust across multiple rating methods and statistical analyses
- **Medium Confidence:** The mechanism explaining why human guidance improves AI performance (semantic scaffolding) is plausible but requires additional validation
- **Low Confidence:** The claim that visual creativity represents a uniquely challenging domain for AI compared to linguistic creativity needs broader empirical support

## Next Checks

1. Test the same methodology with alternative abstract visual stimuli sets to verify the creativity gap persists across different abstract shape distributions
2. Implement a controlled ablation where the same human ideas are provided to both AI conditions to isolate the effect of prompt specificity versus the presence of human guidance
3. Conduct a follow-up study with embodied AI systems that have interactive visual feedback loops to test whether "perceptual nuance" limitations are fundamental or architectural