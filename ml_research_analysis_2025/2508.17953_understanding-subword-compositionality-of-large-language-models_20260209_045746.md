---
ver: rpa2
title: Understanding Subword Compositionality of Large Language Models
arxiv_id: '2508.17953'
source_url: https://arxiv.org/abs/2508.17953
tags:
- word
- representations
- llms
- subword
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) compose
  subword representations into word-level representations. The authors propose three
  key aspects to examine: structural similarity between composed and whole-word representations,
  sensitivity to semantic decomposability, and retention of formal features like word
  length.'
---

# Understanding Subword Compositionality of Large Language Models

## Quick Facts
- arXiv ID: 2508.17953
- Source URL: https://arxiv.org/abs/2508.17953
- Authors: Qiwei Peng; Yekun Chai; Anders Søgaard
- Reference count: 18
- Primary result: Large language models fall into three distinct groups with different subword composition strategies, with addition being the dominant compositional operation.

## Executive Summary
This paper investigates how large language models (LLMs) compose subword representations into word-level representations. The authors examine three key aspects: structural similarity between composed and whole-word representations, sensitivity to semantic decomposability, and retention of formal features like word length. Through experiments on six LLMs across five families using Procrustes analysis and probing tasks, they find that most models use simple addition for composition, preserve semantic content universally, but show varying strategies for retaining form information. The study reveals that LLMs employ different compositional strategies, with some relying on static embedding arithmetic while others use dynamic attention-based processing.

## Method Summary
The authors use Procrustes analysis to measure geometric alignment between composed subword vectors (via addition) and whole-word vectors across six LLMs (Llama, Gemma, Aya, Mistral, Qwen, Falcon) at different layers. They employ probing classifiers to test whether semantic features (root vs. non-root classification) and form features (word length) are encoded in the representations. The analysis focuses on words that can be split into exactly two subwords, creating a dataset of 3,432 words where both whole words and subwords exist in the vocabulary.

## Key Results
- Simple addition consistently outperforms other composition operations across all models, showing strong structural similarity
- Content information like semantic decompositionality is well-preserved across all models and layers
- Formal information about word length is only preserved in some models (Aya, Gemma) and degrades in deeper layers for others
- The six LLMs fall into three distinct groups with different compositional strategies: static linear addition (Llama), hybrid contextual processing (Falcon), and sustained form retention (Aya/Gemma)

## Why This Works (Mechanism)

### Mechanism 1: Linear Isometry of Subword Addition
- **Claim**: If subword representations are summed, the resulting vector is structurally similar (isometric) to the whole-word representation, particularly in non-root words.
- **Mechanism**: The model's embedding space arranges subword vectors such that their arithmetic sum approximates the geometric position of the parent word. This implies a linear composition strategy where `Vector(word) ≈ Vector(subword_1) + Vector(subword_2)`.
- **Core assumption**: The model learns distributional regularities during pre-training that enforce this linearity, rather than relying on non-linear memorization.
- **Evidence anchors**:
  - [section 3.3]: "Simple addition consistently outperforms other operations across all models... suggesting that summing two subword representations produces a composed representation with strong structural similarity."
  - [figure 4]: Non-root words (decomposable) show higher structural similarity via addition than root words.
  - [corpus]: Corpus evidence explicitly confirming "addition is best" is weak in neighbors, though "How do Transformer Embeddings Represent Compositions?" [36492] investigates general composition functional forms.
- **Break condition**: If the model uses a highly non-linear activation pathway between the embedding layer and the first transformer block, or if the vocabulary relies on full-word tokens (memorization), this linearity will vanish immediately after the embedding layer (as seen in the Llama group).

### Mechanism 2: Contextual Refinement of Composition
- **Claim**: Structural alignment improves when subwords are processed simultaneously (contextualized) rather than in isolation, specifically for models that otherwise show weak static alignment.
- **Mechanism**: Self-attention allows subwords to exchange information. For models like Llama, the "composition" is not pre-baked into the static embeddings but is computed dynamically via attention mechanisms in the middle layers.
- **Core assumption**: The attention heads are capable of resolving the relationship between fragments (e.g., "sun" and "rise") into a unified concept ("sunrise") effectively.
- **Evidence anchors**:
  - [section 3.3]: "When contextualization is applied, all models exhibit stronger linear alignment... Llama models... demonstrate high levels of isometry in their middle layers."
  - [abstract]: Mentions investigating "subword compositionality" which implies interaction, though the static vs. contextual distinction is detailed in the text.
  - [corpus]: "How Do Language Models Compose Functions?" [60526] supports the idea that models use internal mechanisms (like attention/MLPs) to solve compositional tasks, rather than just static vector arithmetic.
- **Break condition**: If subwords are masked or separated by long sequences, the attention mechanism cannot bridge them, and the composition effect degrades.

### Mechanism 3: Semantic-Form Feature Stratification
- **Claim**: Models universally preserve semantic content (decomposability) across layers, but the retention of surface form (word length) is architecture-dependent and degrades in deeper layers.
- **Mechanism**: Layers act as filters. Semantic features (root vs. non-root) are reinforced for downstream tasks, while formal features (character count) are treated as noise and abstracted away, unless the model follows a specific retention strategy (e.g., Aya/Gemma).
- **Core assumption**: Probing classifiers accurately reflect the information available to the model, not just spurious correlations in the weights.
- **Evidence anchors**:
  - [section 4.1]: "Content information such as semantic decompositionality is well-preserved... for all models."
  - [section 4.2]: "Form-related properties... are well-preserved at lower levels... [but] gradually decreases" or varies by model group.
  - [corpus]: "StochasTok" [103189] highlights that models often struggle with subword/form-level tasks, supporting the finding that form retention is not universal.
- **Break condition**: If a model is trained on a task heavily dependent on character-level precision (e.g., code or OCR), this abstraction of "form" might be reversed or inhibited.

## Foundational Learning

- **Concept**: **Subword Tokenization (BPE)**
  - **Why needed here**: The entire analysis relies on the premise that words are fractured into subwords (e.g., "limit" → "li", "mit") which the model must re-compose.
  - **Quick check question**: Can you explain how Byte-Pair Encoding (BPE) determines when to split a word versus keeping it whole?

- **Concept**: **Procrustes Analysis**
  - **Why needed here**: This is the statistical tool used to measure "structural similarity." It determines if one cloud of points (composed vectors) can be rotated/scaled to match another (whole-word vectors).
  - **Quick check question**: If Procrustes similarity is high, what does that imply about the relationship between the sum of two vectors and a third target vector?

- **Concept**: **Probing Classifiers**
  - **Why needed here**: Used to verify if "content" (semantic type) or "form" (length) is encoded in the representations.
  - **Quick check question**: Why is it necessary to train a separate lightweight classifier (a "probe") to detect features like word length, rather than just looking at the raw vector values?

## Architecture Onboarding

- **Component map**: Words split into 2 subwords (e.g., "pro" + "gress") -> 6 LLMs (Llama, Gemma, etc.) analyzed layer-by-layer -> Geometry Engine (Procrustes alignment) -> Probing Engine (Semantic/Length classification)

- **Critical path**: Extracting embeddings from specific layers → Applying composition (Addition) → Procrustes alignment (Train on split, Test on split) → Measuring Precision@1

- **Design tradeoffs**:
  - **Static vs. Contextual**: Extracting vectors separately (static) tests if composition is "baked in" to weights. Extracting together (contextual) tests the model's active processing. You must choose based on whether you are testing the *embedding layer* or the *transformer layers*.
  - **Dataset Size**: The paper limits analysis to words where both whole and subwords exist in the vocabulary (3,432 words). Expanding this requires relaxing vocabulary constraints.

- **Failure signatures**:
  - **The "Llama" Pattern**: High similarity at layer 0, immediate drop at layer 1. *Diagnosis*: The model does not compose linearly; it likely uses non-linear processing or memorization.
  - **Form Loss**: Accuracy on "Word Length" drops to random baseline in middle layers. *Diagnosis*: Model is abstracting away surface features (expected in semantic-focused models).

- **First 3 experiments**:
  1. **Sanity Check (Addition)**: Reproduce Figure 2. Take 100 words, split them, sum their static embeddings, and compute cosine similarity to the whole word. Verify if Addition > Multiplication.
  2. **Group Classification**: Run the "Word Length" probing task (Section 4.2) on your target model. If accuracy persists to the last layer, your model is in the "Aya/Gemma" group. If it drops, it's in the "Llama/Falcon" group.
  3. **Contextual Ablation**: Compare static subword addition vs. contextual subword addition (Figure 5). If contextualization improves isometry significantly, your model relies on attention for composition rather than linear embedding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified subword composition strategies generalize across languages with diverse morphological properties?
- Basis: [explicit] The authors state in the Limitations section, "It remains an open question whether the same composition strategies hold across languages with different morphological properties."
- Why unresolved: The current study is restricted to English, and it is unclear if the geometric alignment and form retention patterns observed apply to languages with significantly different word structures (e.g., agglutinative languages).
- What evidence would resolve it: Replicating the Procrustes analysis and probing tasks on a multilingual dataset covering a variety of morphological typologies.

### Open Question 2
- Question: To what extent do pre-training data mixtures causally determine the specific composition strategies observed in different LLM families?
- Basis: [inferred] The paper hypothesizes that "pre-training data and its data mixture" is the main driver for the three distinct strategy groups but notes that verifying this is "challenging" because such information is not fully disclosed.
- Why unresolved: There is a confounding factor between model architecture, training objectives, and data composition; without controlling for the pre-training corpora, the root cause of the strategic divergence cannot be isolated.
- What evidence would resolve it: A controlled study training structurally similar models on varied, disclosed data mixtures and measuring the resulting compositional strategies.

### Open Question 3
- Question: How do composition dynamics change when extending from two-subword combinations to longer sequences?
- Basis: [explicit] The authors state, "Our work focuses on two-subword composition. It would be valuable to extend to compositions with more subwords."
- Why unresolved: The dataset was constrained to words that split into exactly two subwords; thus, the linearity and isometry observed may not scale to complex tokenizations requiring three or more components.
- What evidence would resolve it: Applying the geometric analysis pipeline to $n$-subword compositions to see if simple addition remains the dominant compositional operation.

## Limitations

- The Procrustes analysis measures geometric alignment but cannot distinguish between true compositional understanding versus coincidental alignment in embedding space
- The vocabulary constraint (3,432 words where both whole words and subwords exist) creates a biased sample that may overrepresent certain word types
- The probing classifier approach assumes linear separability of features, which may not capture more complex encoding strategies

## Confidence

**High Confidence**: The finding that addition outperforms other composition operations across all models is robust, supported by consistent results across multiple experiments and model families. The preservation of semantic decompositionality (root vs. non-root classification) is also highly reliable, showing >80% accuracy across nearly all models and layers.

**Medium Confidence**: The classification of models into three distinct groups based on form retention patterns shows some consistency but has notable exceptions. The Falcon models show inconsistent behavior, and the layer-specific dynamics (particularly the "middle layer peak" for Llama) could be influenced by the specific probing methodology or dataset characteristics.

**Low Confidence**: The assertion that structural similarity improvements from contextualization represent genuine "dynamic composition" rather than simple smoothing effects from attention remains speculative. The paper does not test whether attention weights specifically encode compositional relationships or merely reduce noise.

## Next Checks

1. **Statistical Significance Testing**: Apply permutation tests to the Procrustes similarity scores across models and conditions to determine whether observed differences are statistically significant or could arise by chance. This would strengthen claims about distinct compositional strategies.

2. **Cross-Lingual Generalization**: Replicate the semantic probing experiments on multilingual datasets to test whether the universal preservation of semantic content holds across languages with different morphological structures (e.g., agglutinative languages like Turkish versus analytic languages like Mandarin).

3. **Intervention Experiment**: Perform ablation studies where attention mechanisms are selectively disabled in middle layers of models like Llama to test whether the observed compositional improvements are causally linked to attention or merely correlated with other architectural features.