---
ver: rpa2
title: 'RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language
  Models'
arxiv_id: '2510.10390'
source_url: https://arxiv.org/abs/2510.10390
tags:
- refusal
- answer
- refuse
- intensity
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RefusalBench introduces a generative evaluation framework that
  programmatically creates test cases through controlled linguistic perturbations
  to overcome the limitations of static benchmarks in measuring selective refusal.
  Evaluation of over 30 models across two benchmarks (RefusalBench-NQ and RefusalBench-GaRAGe)
  reveals that even frontier models achieve refusal accuracy below 50% on multi-document
  tasks, systematically misclassifying complex refusal types as missing information.
---

# RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models

## Quick Facts
- **arXiv ID:** 2510.10390
- **Source URL:** https://arxiv.org/abs/2510.10390
- **Reference count:** 40
- **Key outcome:** Even frontier models achieve refusal accuracy below 50% on multi-document tasks, systematically misclassifying complex refusal types as missing information

## Executive Summary
RefusalBench introduces a generative evaluation framework for measuring selective refusal capabilities in grounded language models. The framework programmatically creates test cases through controlled linguistic perturbations to overcome the limitations of static benchmarks in measuring when models should refuse to answer rather than providing incorrect information. By employing a multi-model generator-verifier pipeline with unanimous consensus requirements, the framework achieves 93.1% human agreement while systematically evaluating model performance across 176 perturbation strategies spanning six uncertainty categories.

## Method Summary
The methodology centers on a generator-verifier pipeline where distinct LLMs first generate perturbed test cases from base question-answer pairs using 176 linguistic levers across six uncertainty categories (Ambiguity, Contradiction, MissingInfo, FalsePremise, GranularityMismatch, EpistemicMismatch). Each perturbation is verified by multiple independent LLM verifiers requiring unanimous consensus to ensure quality. Models are then evaluated on their ability to either answer correctly or refuse with specific codes corresponding to the identified uncertainty type. The framework uses 100 base instances each from NaturalQuestions (single-document) and GaRAGe (multi-document) datasets, with perturbations applied at three intensity levels to create fine-grained evaluation scenarios.

## Key Results
- Even frontier models achieve refusal accuracy below 50% on multi-document tasks
- Models systematically misclassify complex refusal types as missing information (Figure 8)
- Selective refusal comprises separable detection and categorization skills that scale independently from answer accuracy
- A multi-model generator-verifier pipeline with unanimous consensus achieves 93.1% human agreement

## Why This Works (Mechanism)
The framework's effectiveness stems from its generative approach to creating diverse, controlled test cases rather than relying on static benchmarks. By perturbing answerable questions to introduce specific types of uncertainty, the method isolates the model's ability to detect and categorize different refusal scenarios. The unanimous consensus requirement across multiple verifier models ensures high-quality test generation while minimizing self-evaluation bias. This systematic approach reveals that selective refusal is a distinct, alignment-sensitive capability that can be trained independently from general question-answering performance.

## Foundational Learning

**Selective Refusal Detection:** The ability to identify when context contains insufficient or conflicting information to answer a question. Needed to distinguish between answerable and unanswerable queries based on document content.

**Refusal Categorization:** The skill to map identified uncertainty types to specific refusal codes (e.g., REFUSE_AMBIGUOUS vs REFUSE_CONTRADICTORY). Critical for providing actionable feedback rather than generic refusals.

**Intensity Calibration:** Understanding how the severity of uncertainty should influence refusal decisions. Important for handling perturbations at different intensity levels appropriately.

**Multi-Model Verification:** Using distinct LLMs to cross-validate generated test cases. Essential for ensuring test quality and minimizing self-evaluation bias in the evaluation framework.

## Architecture Onboarding

**Component Map:** Base Set -> Generator (176 levers) -> Verifier Ensemble (unanimous consensus) -> Evaluation Dataset -> Target Models -> Scoring (LLM Judge)

**Critical Path:** The generator-verifier pipeline is critical - without unanimous consensus from verifier models, the generated perturbations lack the quality assurance needed for reliable evaluation.

**Design Tradeoffs:** Unanimous consensus ensures high quality but limits dataset size; using a single strong LLM for both generation and verification would increase coverage but introduce significant self-evaluation bias.

**Failure Signatures:** 
- Low inter-verifier agreement indicates poor perturbation quality or overly strict verification criteria
- Systematic misclassification patterns reveal model limitations rather than framework issues
- Intensity misalignment shows models cannot calibrate refusal decisions based on uncertainty severity

**3 First Experiments:**
1. Run the base set questions through target models to verify the 100% RAF score baseline
2. Generate perturbations using only 10 representative levers to test the generation pipeline
3. Evaluate a single model on the verified dataset to establish initial refusal accuracy metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Critical implementation details like the complete list of 176 perturbation levers and exact base set indices are not fully provided
- The framework relies on LLM-as-verifier, introducing potential self-evaluation bias despite unanimous consensus requirements
- Models show systematic misclassification patterns (e.g., conflating granularity with missing information) that may be difficult to address

## Confidence
- **High confidence:** The general pipeline (generator-verifier) and primary metrics (Refusal Accuracy, CRS, FRR, MRR) are clearly defined and reproducible
- **Medium confidence:** The perturbation generation process can be approximated from examples, but exact levers are unknown
- **Low confidence:** The specific base set questions and full lever catalog are critical for exact replication but are not provided

## Next Checks
1. **Verify Lever Catalog Completeness:** Reconstruct the full set of 176 levers from Appendix K examples and test whether the generated perturbations maintain the intended semantic shifts across all six uncertainty categories.

2. **Test Verifier Consensus Threshold:** Systematically lower the consensus requirement (e.g., from unanimous to majority) and measure the impact on dataset size and quality to quantify the trade-off between self-evaluation bias and dataset coverage.

3. **Benchmark Model-Specific Failure Modes:** Run the framework on a diverse set of models (including smaller, specialized models) to confirm whether the observed misclassification patterns (e.g., granularity â†’ missing info) are consistent across model families or specific to certain architectures.