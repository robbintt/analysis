---
ver: rpa2
title: 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters'
arxiv_id: '2505.14464'
source_url: https://arxiv.org/abs/2505.14464
tags:
- reasoning
- data
- am-thinking-v1
- datasets
- distilled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the choice of distillation source affects
  the performance of reasoning-oriented language models. The authors construct three
  large-scale datasets (1.89 million queries) by collecting verified reasoning traces
  from AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1.
---

# Not All Correct Answers Are Equal: Why Your Distillation Source Matters

## Quick Facts
- arXiv ID: 2505.14464
- Source URL: https://arxiv.org/abs/2505.14464
- Reference count: 31
- Primary result: Student models trained on AM-Thinking-v1-distilled data achieve superior performance (84.3 AIME2024, 72.2 AIME2025, 98.4 MATH500, 65.9 LiveCodeBench) compared to Qwen3-235B-A22B and DeepSeek-R1 distilled data.

## Executive Summary
This paper investigates how the choice of teacher model affects the quality of distilled reasoning traces used for training smaller language models. The authors construct three large-scale datasets (1.89 million queries) by collecting verified reasoning traces from AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1. Through careful data preprocessing and quality assurance, they find that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks, with AM-Thinking-v1 consistently achieving the best performance. The authors release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research.

## Method Summary
The authors collected 1.89 million queries across six categories (math, code, science, instruction-following, multi-turn chat, general reasoning) and performed iterative distillation from three teacher models. Each query underwent repeated distillation attempts until the generated response achieved a verification score ≥ 0.9 using task-specific validators. The distilled data was filtered based on perplexity (computed by a 32B reference model), high-frequency n-grams, and structural validation. Student models (Qwen2.5-32B base) were trained using supervised fine-tuning with LR 8e-5, 32k context length, global batch 64, and 2 epochs.

## Key Results
- AM-Thinking-v1-distilled data shows lower perplexity (2.5) than Qwen3 (3.0) and DeepSeek-R1 (2.9)
- Student models trained on AM-Thinking-v1 data achieve best performance across all benchmarks: 84.3 AIME2024, 72.2 AIME2025, 98.4 MATH500, 65.9 LiveCodeBench
- AM-distilled model demonstrates adaptive generation length—producing longer responses for harder tasks (18199.2 tokens on AIME2025) and shorter ones for simpler tasks (3495.7 tokens on MATH500)

## Why This Works (Mechanism)

### Mechanism 1
Token length diversity in training data enables adaptive reasoning behavior in student models. The AM-Thinking-v1-distilled dataset contains a broader distribution of response lengths (both short sequences under 1024 tokens and very long sequences exceeding 10240 tokens), which the student model learns to mimic—producing longer chains for difficult problems and shorter ones for simpler tasks.

### Mechanism 2
Lower perplexity distillation data correlates with more learnable supervision signals and improved downstream performance. AM-Thinking-v1-distilled data achieves the lowest mean perplexity (2.5) compared to Qwen3 (3.0) and DeepSeek-R1 (2.9). Lower perplexity suggests more coherent, higher-quality outputs that provide cleaner gradient signals during training.

### Mechanism 3
Iterative verification-guided regeneration improves distillation data quality beyond single-pass generation. Each query undergoes repeated distillation attempts until the generated response achieves verification_score ≥ 0.9, using task-specific validators (Math-Verify, sandbox execution for code, LLM-based scoring for other categories).

## Foundational Learning

- **Knowledge Distillation**: Why needed here: The entire methodology depends on transferring reasoning capabilities from large teacher models to a smaller student via supervised training on teacher-generated traces.
  - Quick check: Can you explain why training on verified teacher outputs might outperform training on the same queries without verification?

- **Perplexity as Quality Signal**: Why needed here: The paper uses perplexity (computed by a 32B reference model) as a filtering criterion and quality differentiator between distillation sources.
  - Quick check: What does it mean when a dataset has lower perplexity according to a strong language model?

- **Chain-of-Thought Reasoning Format**: Why needed here: All distillation uses structured reasoning traces with explicit `<think reasoning process here>` and `<answer answer here </answer>` tags.
  - Quick check: Why might explicit reasoning tags help a student model learn to separate reasoning from final answers?

## Architecture Onboarding

- **Component map**: Query corpus -> Teacher models (AM-Thinking-v1, Qwen3-235B-A22B, DeepSeek-R1) -> Verification layer (0.9 threshold) -> Quality filters (perplexity, n-gram, structure) -> Student model (Qwen2.5-32B)

- **Critical path**: 1. Query preprocessing (deduplication → Unicode/URL filtering → semantic decontamination with bge-m3 at 0.9 threshold) 2. Parallel distillation from all three teachers 3. Verification-loop until score ≥ 0.9 4. Post-hoc filtering (perplexity, n-gram, structure checks) 5. Supervised fine-tuning on verified traces

- **Design tradeoffs**:
  - Verification strictness (0.9): Higher threshold improves quality but increases compute cost due to regeneration loops; may reduce final dataset size
  - Teacher selection: AM-Thinking-v1 produces best results despite not being the largest model, suggesting quality > scale for distillation sources
  - 32k sequence length cap: Samples exceeding 32k are excluded; this may truncate very long reasoning chains but enables efficient packing

- **Failure signatures**:
  - Training loss plateauing early: May indicate data quality issues or insufficient verification strictness
  - Student producing uniformly long responses regardless of task difficulty: Suggests training data lacks length diversity
  - Large gap between training and evaluation benchmarks: Potential contamination or overfitting to specific reasoning patterns

- **First 3 experiments**:
  1. Train separate student models on each teacher's distilled data using identical hyperparameters; evaluate on AIME2024, AIME2025, MATH500, LiveCodeBench to reproduce Table 1 results
  2. Train with verification thresholds of 0.7, 0.8, 0.9, 1.0 to measure tradeoff between dataset size and downstream performance
  3. Sample 1000 responses from each trained model on both easy (MATH500) and hard (AIME2025) tasks; plot generation length distributions to confirm adaptive behavior emerges from AM-distilled training

## Open Questions the Paper Calls Out

- Can reinforcement learning methods (PPO, GRPO) further improve reasoning capabilities when applied on top of AM-Thinking-v1 distillation?
- Which specific data properties (perplexity, token length diversity, or other factors) causally drive the superior student performance from AM-Thinking-v1 distillation?
- Do these findings generalize to student models with different architectures or parameter scales beyond Qwen2.5-32B?

## Limitations

- The paper cannot definitively isolate whether token length diversity, perplexity, or other correlated factors drive the observed improvements in student performance.
- The verification-guided regeneration process may introduce systematic biases toward certain reasoning styles through iterative refinement.
- The DeepSeek-R1 distilled dataset was not released, preventing complete independent verification of all three-way comparisons.

## Confidence

- **High Confidence**: The empirical finding that AM-Thinking-v1-distilled data produces consistently better student performance across all four benchmarks.
- **Medium Confidence**: The mechanism linking token length diversity to adaptive reasoning behavior, though causation remains inferential.
- **Low Confidence**: The attribution of performance gains specifically to perplexity differences without controlled ablation studies.

## Next Checks

1. Train student models on artificially modified versions of AM-Thinking-v1-distilled data with uniform vs. diverse length distributions to isolate the effect of length variability.
2. Re-run distillation without perplexity-based filtering on all three teacher models, then train and evaluate student models to determine whether perplexity filtering contributes to performance differences.
3. Systematically vary the verification threshold (0.7, 0.8, 0.9, 0.95, 1.0) during distillation and measure the resulting dataset size, quality metrics, and downstream student performance.