---
ver: rpa2
title: AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over
  Heterogeneous Knowledge Graphs for the Circular Economy
arxiv_id: '2508.01815'
source_url: https://arxiv.org/abs/2508.01815
tags:
- agent
- query
- language
- sparql
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGENTIC T2S, a modular multi-agent framework
  for text-to-SPARQL generation over heterogeneous knowledge graphs (KGs). The framework
  decomposes the task into specialized agents for subgoal parsing, schema-aware graph
  allocation, pattern-driven SPARQL synthesis, and dual-stage verification combining
  symbolic and counterfactual checks.
---

# AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy

## Quick Facts
- arXiv ID: 2508.01815
- Source URL: https://arxiv.org/abs/2508.01815
- Reference count: 7
- Improves execution accuracy by 17.3% and triple-level F1 by 25.4% over best baseline

## Executive Summary
AGENTIC T2S introduces a modular multi-agent framework for text-to-SPARQL generation over heterogeneous knowledge graphs. The system decomposes KGQA into subtasks managed by specialized agents for retrieval, query generation, and verification. Evaluated on three real-world circular economy KGs, the framework improves execution accuracy by 17.3% and triple-level F1 by 25.4% over the best baseline while reducing average prompt length by 46.4%. The results demonstrate the effectiveness of modular, schema-aware reasoning for scalable KGQA in low-resource, multi-graph settings.

## Method Summary
The framework uses five coordinated agents: a compositional subgoal parser that decomposes natural language questions into atomic operations, a hierarchical alignment allocator that routes subgoals to appropriate KGs using weak-to-strong schema matching, a pattern-driven SPARQL synthesizer that generates queries from templates with schema grounding, a dual-stage verifier that validates queries through symbolic and counterfactual checks, and a consensus aggregator that merges results from multiple graphs. The system is implemented using LangGraph with LLaMA3-70B/LLaMA3.3-70B/Qwen3-32b models via Groq API, local models for schema retrieval on RTX 4090 Ti, and Apache Jena for SPARQL validation.

## Key Results
- Execution Accuracy (EA) improved from 55.1% (best baseline) to 72.4%
- Query Syntax Correctness (QSC) reached 76.2% compared to 60.0% baseline
- Triple-level F1 (TF1) achieved 83.7% versus 58.3% baseline
- Average Token Usage (ATU) reduced from 1,632 to 875 tokens (46.4% reduction)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition of natural language queries into subgoals improves execution accuracy by enabling specialized processing per reasoning operation.
- Mechanism: The Compositional Subgoal Parser uses rule-based syntactic analysis and domain heuristics to segment questions into atomic operations (entity retrieval, filtering, aggregation). Each subgoal is processed independently, reducing compound error propagation.
- Core assumption: Natural language questions can be decomposed into semantically independent operations that map cleanly to SPARQL patterns.
- Evidence anchors: Ablation study shows removing the decomposer drops EA from 72.43% to 23.68% (Table 4).

### Mechanism 2
- Claim: Weak-to-strong schema alignment enables accurate graph routing without requiring complete schema mappings across heterogeneous KGs.
- Mechanism: The Hierarchical Alignment Allocator performs vector similarity over embedded metadata (weak retrieval), then conducts fine-grained schema alignment on top candidates by extracting predicates, class hierarchies, and domain-range constraints (strong retrieval).
- Core assumption: Schema-level features (predicates, hierarchies) are sufficient proxies for determining whether a KG can answer a given subgoal.
- Evidence anchors: Figure 5 shows AGENTIC T2S achieves highest allocation accuracy (>80%), exceeding DeepSeek-R1 by >10%; ablation shows allocator removal drops EA to 25.12%.

### Mechanism 3
- Claim: Dual-stage verification combining symbolic validation and counterfactual testing filters semantically underspecified queries that would otherwise return incomplete or incorrect results.
- Mechanism: The Consistency Checker first validates syntax and type compatibility via symbolic checks (Stage 1), then perturbs query components (entities, filters) and compares result sets (Stage 2). Queries producing unchanged outputs under perturbation are flagged as underspecified and rejected or revised.
- Core assumption: Semantically specific queries produce different results when their constraints are meaningfully altered.
- Evidence anchors: Ablation shows verifier removal reduces EA from 72.43% to 55.12% (Table 4), indicating many syntactically valid queries are semantically underspecified.

## Foundational Learning

- Concept: **SPARQL Query Structure and Triple Patterns**
  - Why needed here: The synthesizer agent generates SPARQL from templates; understanding SELECT/WHERE clauses, triple patterns (subject-predicate-object), FILTER expressions, and variable bindings is essential to debug generation failures.
  - Quick check question: Given a SPARQL query with a WHERE clause containing `?x rdf:type ex:Company . ?x ex:hasEmission ?e . FILTER(?e > 100)`, what does the FILTER constraint do?

- Concept: **Knowledge Graph Schema Components (Ontologies, Predicates, Domain-Range Constraints)**
  - Why needed here: The allocator and synthesizer both depend on schema-level reasoning—matching subgoals to graphs based on available predicates and ensuring generated queries respect class hierarchies and type constraints.
  - Quick check question: If a KG defines a predicate `ex:hasWasteCode` with domain `ex:Material` and range `ex:ClassificationCode`, what type must the subject and object entities have for a valid triple?

- Concept: **Multi-Agent Coordination Patterns (Pipeline vs. Hierarchical)**
  - Why needed here: AGENTIC T2S uses a sequential pipeline (decomposer → allocator → synthesizer → verifier → aggregator) with fallback handling; understanding how agents exchange structured outputs and handle failures is critical for extending the framework.
  - Quick check question: In Algorithm 1, if the verifier rejects a query (line 6 fails), what happens to the subgoal and how does this affect the final answer aggregation?

## Architecture Onboarding

- Component map: Natural Language Question → Subgoal Parser → Subgoal → Allocator → KG + Schema → Synthesizer → SPARQL Query → Verifier → Execution → Aggregator → Natural Language Answer
- Critical path: Query → Agent 1 decomposition → Each subgoal → Agent 2 allocation → Agent 3 synthesis → Agent 4 verification → Verified queries → execution → Agent 5 aggregation
- Design tradeoffs:
  - Modularity vs. latency: Five-agent pipeline enables parallelism across subgoals but introduces sequential dependencies per subgoal
  - Token efficiency vs. verification overhead: Dual-stage verifier adds processing but reduces average tokens by 46.4% by filtering invalid queries early
  - Template scaffolding vs. flexibility: Pattern-driven synthesis ensures syntactic validity but may struggle with queries requiring novel SPARQL constructs
- Failure signatures:
  - EA drops to ~23-25%: Subgoal decomposition or allocation failing—check parser output for incomplete/ambiguous subgoals, verify allocator schema matching
  - High QSC but low EA (e.g., 60% vs. 55%): Queries are syntactically valid but semantically underspecified—verifier not catching overspecificity
  - Empty answer sets on cross-KG queries: Entity alignment across graphs failing—aggregator not resolving equivalence between differently-named entities
- First 3 experiments:
  1. Single-agent baseline comparison: Run the same queries through a vanilla LLM to replicate the V-L-GPT-4o baseline (~55% EA)
  2. Allocator routing accuracy test: Instrument Agent 2 to log which KG each subgoal is routed to; compare against gold annotations
  3. Verifier perturbation analysis: For a sample of rejected queries, manually inspect which perturbations triggered failure to validate counterfactual checks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AGENTIC T2S generalize to domains beyond the circular economy, such as biomedical, financial, or legal knowledge graphs with fundamentally different schema structures?
- Basis in paper: The framework was evaluated exclusively on three in-house circular economy KGs, and no experiments were conducted on other domains or standard KGQA benchmarks.
- Why unresolved: The specialized templates, domain heuristics in the parser, and circular economy-specific ontology constraints may not transfer directly to other domains with different reasoning patterns.
- What evidence would resolve it: Evaluation results on standard KGQA benchmarks (e.g., LC-QuAD, WebQuestionsSP, ComplexWebQuestions) or KGs from diverse domains like Wikidata, DBpedia, or domain-specific biomedical KBs.

### Open Question 2
- Question: What is the computational latency and throughput of AGENTIC T2S at scale, particularly when the number of knowledge graphs or query complexity increases significantly?
- Basis in paper: "practical systems must meet efficiency requirements, making retrieval and query generation time sensitive."
- Why unresolved: While complexity analysis is provided, no actual latency measurements, throughput metrics, or scalability experiments are reported in the evaluation section.
- What evidence would resolve it: End-to-end latency measurements across varying numbers of KGs, query lengths, and concurrent request loads; comparison with baseline systems on wall-clock time.

### Open Question 3
- Question: How does the performance of AGENTIC T2S degrade when entity alignment across heterogeneous KGs is imperfect or when cross-graph schema mappings contain errors?
- Basis in paper: The problem formulation states the task involves "diverse schemas, incomplete alignments, and distributed data sources," yet the evaluation uses curated in-house KGs whose alignment quality is not characterized.
- Why unresolved: The consensus aggregator assumes entity alignment can resolve equivalence and redundancy, but sensitivity to alignment noise is not measured.
- What evidence would resolve it: Controlled experiments with injected alignment errors at varying rates (5%, 10%, 20% noise) and analysis of resulting accuracy degradation.

### Open Question 4
- Question: How do counterfactual consistency checks interact with legitimate query ambiguity, and what is the false rejection rate for queries that are correctly specified but flagged as underspecified?
- Basis in paper: The dual-stage verifier "flags overly general or underspecified" queries through perturbation testing, but no analysis of false positives (valid queries incorrectly rejected) is provided.
- Why unresolved: Some semantically correct queries may legitimately return identical results under certain perturbations due to data distribution, not query underspecification.
- What evidence would resolve it: Analysis of verifier precision/recall on a held-out set; manual inspection of rejected queries to measure false rejection rate.

## Limitations
- Evaluation relies on three in-house KGs with no public access, limiting external validation
- The weak-to-strong schema alignment mechanism lacks direct corpus support for multi-graph KGQA
- Computational latency and throughput at scale are not measured, creating uncertainty about production viability

## Confidence
- **High confidence**: The modular decomposition mechanism and its empirical impact (17.3% EA improvement) are well-supported by ablation studies and direct comparisons
- **Medium confidence**: The weak-to-strong schema alignment approach shows strong quantitative results but lacks direct corpus evidence
- **Low confidence**: Generalization across different domain KGs is untested; the framework may be brittle to schema variations or question types not represented in the circular economy benchmark

## Next Checks
1. **Schema alignment robustness test**: Instrument the allocator to log alignment scores and routing decisions, then validate against gold-standard schema mappings for a subset of queries to assess whether vector similarity reliably captures semantic compatibility across heterogeneous KGs.
2. **Counterfactual perturbation sensitivity analysis**: For rejected queries, manually inspect which perturbations trigger failure (entity swap vs. filter removal) and verify that the rejection correlates with genuine semantic underspecification rather than over-aggressive filtering rules.
3. **Cross-domain generalization experiment**: Apply the framework to a different KGQA benchmark (e.g., WebQuestions, QALD) with available public KGs to assess whether the 17.3% accuracy gain and 25.4% F1 improvement generalize beyond circular economy use cases.