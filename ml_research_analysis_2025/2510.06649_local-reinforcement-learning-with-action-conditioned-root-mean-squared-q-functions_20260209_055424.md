---
ver: rpa2
title: Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions
arxiv_id: '2510.06649'
source_url: https://arxiv.org/abs/2510.06649
tags:
- learning
- action
- local
- methods
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ARQ (Action-conditioned Root mean squared
  Q-functions), a novel approach for local reinforcement learning inspired by the
  Forward-Forward algorithm. ARQ addresses the challenge of implementing biologically
  plausible, backpropagation-free value estimation in RL environments by combining
  a goodness function with action conditioning at the input level.
---

# Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions

## Quick Facts
- arXiv ID: 2510.06649
- Source URL: https://arxiv.org/abs/2510.06649
- Reference count: 8
- Local RL method that outperforms backpropagation-based approaches on standard benchmarks

## Executive Summary
This paper introduces ARQ (Action-conditioned Root mean squared Q-functions), a biologically plausible reinforcement learning method that eliminates backpropagation while achieving state-of-the-art performance among local RL approaches. ARQ combines a goodness function with action conditioning at the input level, allowing arbitrary hidden dimensions and overcoming limitations of previous methods that required fixed-dimension output layers. The method extracts value predictions from hidden activations using a root mean squared function and conditions on both state and action inputs.

## Method Summary
ARQ implements local reinforcement learning by having each cell independently estimate Q-values using attention-weighted hidden activations passed through an RMS goodness function. The architecture uses a three-layer fully-connected network with skip connections and top-down temporal connections. Action conditioning is implemented by concatenating action vectors to model inputs rather than indexing outputs, enabling state-action specific representations. Training uses standard TD learning with gradients confined within each cell to ensure backpropagation-free updates. The method is evaluated on MinAtar and DeepMind Control Suite benchmarks against both local RL (AD) and backpropagation-based (DQN) approaches.

## Key Results
- ARQ consistently outperforms AD across all tasks, achieving 88.93 average return on Breakout versus 67.40 for AD
- ARQ surpasses DQN in most games, reaching 555.29 on SpaceInvaders versus 369.96 for AD
- The method's effectiveness is attributed to flexibility in handling arbitrary hidden dimensions and producing state-action specific representations
- ARQ demonstrates superior ability to capture bi-modal policies, such as Seaquest's attack/refill oxygen behavior

## Why This Works (Mechanism)

### Mechanism 1: RMS Goodness Function as Q-Value Proxy
The root mean squared (RMS) function of hidden activations serves as a biologically plausible scalar Q-value estimator without learnable parameters. After computing attention-weighted hidden activations, ARQ applies RMS (equivalent to standard deviation after mean subtraction) to collapse any vector dimension into a single value prediction. This bypasses the need for fixed-dimension output layers that constrain architectures like AD. The magnitude/variance of hidden activations correlates meaningfully with expected value—i.e., "good" state-action pairs produce higher activation variance than "bad" ones.

### Mechanism 2: Input-Level Action Conditioning Enables State-Action Specific Representations
Concatenating action vectors to model inputs (rather than indexing outputs) allows the full network capacity to specialize per state-action pair. ARQ repeats the state-activation tensor along the batch dimension for each action candidate, then concatenates action encodings. This forces all layers to compute representations conditioned on the specific action being evaluated. Local learning benefits more from early action integration than backprop-based methods, where gradient flow can distribute action-specificity across layers.

### Mechanism 3: Layerwise Independent Q-Estimation with Local TD Updates
Each cell can independently estimate Q-values using only local gradients, eliminating backward pass dependencies while maintaining temporal coherence. Cells receive bottom-up, top-down, raw state, and action inputs, computing their own Q-prediction via attention plus RMS. The TD error is broadcast to all cells. Multiple independent estimators averaging implicit predictions improves robustness; top-down temporal connections provide sufficient credit assignment without backprop.

## Foundational Learning

- **Concept: Temporal Difference (TD) Learning**
  - **Why needed here:** ARQ's training objective (Eq. 10) is standard TD(0) Q-learning; understanding bootstrap value estimation is essential.
  - **Quick check question:** Can you explain why TD learning doesn't require waiting for episode completion, and what the γ term controls?

- **Concept: Forward-Forward Algorithm & Goodness Functions**
  - **Why needed here:** ARQ inherits the "goodness as positivity signal" concept; the RMS modification is only motivated relative to FF's sum-of-squares formulation.
  - **Quick check question:** In FF, why do positive and negative samples require opposite goodness objectives (maximize vs minimize)?

- **Concept: Action-Conditioned vs State-Conditioned Value Functions**
  - **Why needed here:** The paper argues input-level action conditioning is critical for local methods but incidental for backprop methods. Understanding this distinction explains the architectural choice.
  - **Quick check question:** For a 6-action discrete space, what's the computational difference between (a) one forward pass with 6 output indices vs (b) 6 forward passes with scalar outputs?

## Architecture Onboarding

- **Component map:**
  Input: [state, h_bottom-up, h_top-down, action_onehot] → Concat → Linear → ReLU → LayerNorm → h_t^l → [Attention branch] Linear → tanh → Z_1, Z_2 → Z_2^T @ Z_1 → LayerNorm(tanh) → W_attention → W_attention @ h_t^l → y_vector (arbitrary dim d) → RMS(y) → Q(s,a) scalar

- **Critical path:** Action conditioning (line 4 in Algo 2) must happen *before* attention computation; placing it after would defeat the mechanism. The RMS function (line 10) is parameter-free—if you find learnable weights there, implementation is incorrect.

- **Design tradeoffs:**
  - **Arbitrary hidden dimension d** vs **computation/parameter cost**: Larger d improves expressivity but linearly increases params in W_att2 and output projection
  - **Action conditioning at input** vs **evaluation speed**: Must forward-pass n_a times per state; standard DQN evaluates all actions in one pass
  - **Local gradients** vs **global optimization**: Guarantees biological plausibility but may limit peak performance vs end-to-end trained networks

- **Failure signatures:**
  - **RMS collapse**: If all y_vector values converge to similar magnitudes, Q-predictions become uninformative. Monitor y variance during training
  - **Action-conditioning not propagating**: If concatenation is wrong (e.g., action not tiled to match batch), the model degenerates to state-only. Check input shapes
  - **Top-down temporal connection errors**: h_{l+1}^{t-1} from *previous timestep* is essential; using current timestep creates information leakage

- **First 3 experiments:**
  1. **Sanity check**: Implement single-cell ARQ on a tabular RL environment (e.g., FrozenLake). Verify Q-values converge to reasonable estimates without backprop
  2. **Ablation: Action conditioning**: Train ARQ with and without input-level action conditioning on MinAtar Breakout. Reproduce ~50% gap shown in Figure 5
  3. **Scale comparison**: Match ARQ and AD parameter counts (as in Table 3) on one MinAtar game. Confirm performance gap persists, ruling out "more parameters" as sole explanation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating contrastive positive/negative sampling from the original Forward-Forward algorithm improve ARQ's sample efficiency or final performance when using replay buffers?
- **Basis in paper:** Authors state: "Note that it is possible to sample positive and negative data in order to train in the same contrastive fashion as the original FF algorithm... We leave this for future investigations to keep our method versatile."
- **Why unresolved:** The current implementation uses standard TD learning without contrastive sampling to maintain versatility; the potential benefits of this extension remain unexplored
- **What evidence would resolve it:** Experiments comparing ARQ with and without contrastive sampling on the same benchmarks, measuring sample efficiency and final returns

### Open Question 2
- **Question:** What are the theoretical and empirical differences between RMS, mean, mean squared, and variance functions as goodness nonlinearities in ARQ?
- **Basis in paper:** Authors state: "We leave it for future work to study the intricate effect each function has on training" after showing RMS outperforms alternatives but all variants beat AD
- **Why unresolved:** The paper only reports comparative performance without analyzing why RMS performs best or the training dynamics induced by each function
- **What evidence would resolve it:** Systematic analysis of training dynamics, gradient properties, and convergence behavior across different goodness functions

### Open Question 3
- **Question:** Can ARQ scale effectively to high-dimensional visual inputs (e.g., raw 210×160 Atari frames) while maintaining its backprop-free property?
- **Basis in paper:** The paper only evaluates on low-dimensional benchmarks (10×10 MinAtar, low-dim DMC), stating these are "appropriate testbeds for evaluating the decision-making ability of local methods in simple environments"
- **Why unresolved:** Local methods have historically struggled with high-dimensional inputs; whether ARQ's architectural improvements address this limitation is unknown
- **What evidence would resolve it:** Experiments on standard Atari with raw pixels or other high-dimensional control tasks, comparing ARQ against backprop-based methods

## Limitations

- **Computational scaling:** Evaluating each action separately causes quadratic computational cost in action dimension, limiting practical applicability to low-dimensional action spaces
- **Mechanism uncertainty:** The core assumption that RMS hidden activations reliably correlate with expected value across diverse tasks lacks extensive theoretical grounding
- **Biological plausibility trade-off:** While backpropagation-free, the method still requires replay buffers and target networks, which may limit real-world biological plausibility

## Confidence

- **High confidence:** ARQ consistently outperforms AD across benchmarks; the architectural improvements (action conditioning, RMS goodness function) are well-specified and validated
- **Medium confidence:** The biological plausibility claims are supported by the local learning mechanism, but practical constraints (action discretization, computational cost) weaken real-world applicability
- **Low confidence:** The core assumption that RMS hidden activations serve as effective Q-value proxies lacks extensive theoretical grounding and may not generalize to complex value estimation tasks

## Next Checks

1. Test ARQ on continuous control tasks with high-dimensional action spaces to quantify the quadratic computational scaling impact
2. Compare ARQ's value estimation quality against backprop methods using value function visualization and ablation studies on the RMS mechanism
3. Evaluate performance degradation when removing top-down temporal connections to isolate their contribution to credit assignment