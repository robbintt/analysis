---
ver: rpa2
title: 'SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via
  Token-Level LSH Matching'
arxiv_id: '2509.24832'
source_url: https://arxiv.org/abs/2509.24832
tags:
- cache
- token
- arxiv
- semsharekv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of key-value (KV) cache memory
  usage in large language model (LLM) inference, particularly when handling semantically
  similar prompts that are lexically different. The proposed method, SemShareKV, enables
  KV cache reuse across semantically similar prompts by applying fuzzy token matching
  using locality-sensitive hashing (LSH) on token embeddings with Rotary Position
  Embedding (RoPE) for better positional information preservation.
---

# SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching

## Quick Facts
- arXiv ID: 2509.24832
- Source URL: https://arxiv.org/abs/2509.24832
- Authors: Xinye Zhao; Spyridon Mastorakis
- Reference count: 33
- Key result: 6.25× speedup and 42% GPU memory reduction for 5k-token inputs

## Executive Summary
SemShareKV addresses the inefficiency of KV cache memory usage in LLM inference by enabling reuse across semantically similar but lexically different prompts. The method uses fuzzy token matching with LSH on token embeddings enhanced with RoPE to align and reuse KV pairs from reference prompts. By selectively recomputing high-deviation tokens and applying exponential decay retention, the approach achieves significant speed and memory improvements while maintaining output quality.

## Method Summary
SemShareKV is an LLM inference optimization that reuses KV caches from semantically similar prompts. It works by applying LSH to RoPE-augmented token embeddings to find matching tokens between target and reference prompts, then rearranges the reference KV cache to align with the target sequence. The method uses layer-adaptive selective recomputation (prioritizing high-deviation tokens in shallow layers) and exponential decay retention (aggressive eviction in deeper layers) to maximize reuse while preserving quality.

## Key Results
- 6.25× speedup in time-to-first-token for 5k-token inputs
- 42% reduction in GPU memory usage compared to full recomputation
- Negligible quality degradation (ROUGE-L scores maintained) across diverse summarization datasets

## Why This Works (Mechanism)

### Mechanism 1: RoPE-Enhanced Fuzzy Token Matching for Cache Alignment
RoPE is applied to E-cache before LSH matching to preserve positional information, preventing the "attention sink" problem where tokens incorrectly map to initial positions. This ensures tokens at different positions carry distinct position-aware semantics, enabling accurate KV cache rearrangement. Performance gains vanish if prompts differ significantly in structure or LSH similarity falls below threshold (~0.8).

### Mechanism 2: Layer-Adaptive Selective Recomputation
High-deviation tokens (top ~40% by L2 norm) are identified in the first layer and prioritized for recomputation. Deeper layers attend to fewer tokens, allowing selective recomputation of "hot" tokens while safely reusing others. The strategy assumes high Spearman correlation of HD token positions across adjacent layers and that deeper layers' selective focus implies redundancy in non-attended tokens.

### Mechanism 3: Exponential Decay Retention for Memory Compression
Shallow layers retain more tokens to capture broad context, while deeper layers retain exponentially fewer tokens based on attention scores. This assumes deeper layers contain more redundant information, so aggressive eviction has smaller impact on perplexity. Tokens are evicted based on low attention scores among "cold" tokens not marked for recomputation.

## Foundational Learning

**Key-Value (KV) Cache**: Core data structure avoiding redundant attention computations during autoregressive decoding. Understanding its role is essential to grasp reuse benefits. Quick check: If a prompt has N tokens, what is the approximate memory complexity of its full KV cache in a standard Transformer (in terms of N)?

**Locality-Sensitive Hashing (LSH)**: Core algorithm enabling efficient fuzzy matching between token embeddings of different prompts. Understanding it allows appreciation of the trade-off between exact matching (slow, brittle) and LSH-based approximate matching (fast, flexible). Quick check: Given two vectors, what is the primary property an LSH hash function should satisfy regarding their hash buckets?

**Rotary Position Embedding (RoPE)**: Uniquely dual-role in this architecture - not just for model's internal attention but injected into E-cache to guide LSH matching. Quick check: What type of positional information does RoPE encode: absolute, relative, or both?

## Architecture Onboarding

**Component map**: Cache Manager -> Fuzzy Matcher -> Cache Rearranger -> Modified Inference Engine

**Critical path**:
1. Target prompt arrives
2. E-cache is generated for target
3. RoPE is applied to target E-cache
4. LSH finds most similar reference prompt in CPU cache
5. If similarity > threshold (e.g., 0.8), reference KV-cache is loaded and rearranged
6. LLM inference begins with rearranged cache injected
7. First layer does full recomputation to identify HD tokens
8. Subsequent layers do selective recomputation and eviction

**Design tradeoffs**:
- Memory vs. Computation: Trading increased CPU memory/disk for storing past KV caches to reduce GPU compute during prefill
- Quality vs. Efficiency: Retention and recomputation ratios directly trade generation quality for speed and memory savings
- Exact vs. Fuzzy Match: LSH provides speed but may occasionally yield suboptimal token mappings, degrading quality

**Failure signatures**:
- Low Similarity: Performance degrades if target and reference prompts are not semantically similar (LSH score < 0.8)
- Attention Sink Misalignment: Without RoPE in E-cache, LSH maps too many tokens to initial tokens
- High Overhead for Short Prompts: LSH and rearrangement overhead exceeds savings for prompts < 700 tokens

**First 3 experiments**:
1. Reproduce Insight 3 (Retention Patterns): Implement Uniform, Increasing, and Exponential Decay retention patterns. Run inference on MultiNews validation set and plot Perplexity vs. Retention Ratio to verify Exponential Decay yields lowest perplexity
2. Ablation on RoPE in E-Cache: Run inference with and without applying RoPE to E-cache before LSH. Measure and plot L2 deviation between rearranged KV cache and ground truth KV cache to confirm RoPE reduces deviation
3. End-to-End TTFT & Quality Benchmark: Compare SemShareKV against Fully Recompute and SnapKV baselines on MultiNews dataset. Measure TTFT and ROUGE-L scores for increasing input lengths (0.5k to 5k tokens) to replicate efficiency-quality trade-off curves

## Open Questions the Paper Calls Out

**Open Question 1**: How can the computational overhead of fuzzy token matching and cache rearrangement be minimized to make SemShareKV efficient for short prompts (under 700 tokens)? The authors note this is a goal for future work as current overhead outweighs benefits for short sequences.

**Open Question 2**: Can SemShareKV be effectively integrated with FlashAttention to maintain high throughput while preserving cache manipulation semantics? The paper explicitly states this integration is planned for future work due to potential conflicts with memory access patterns.

**Open Question 3**: Can an adaptive strategy be developed to dynamically determine the optimal LSH matching threshold, replacing the current empirically fixed value of 0.8? The authors state that "exploring adaptive strategies remains an open direction" as the fixed threshold may not generalize across diverse prompt distributions.

## Limitations
- Method may not generalize well to non-RoPE-based models or architectures with different attention patterns
- Synthetically generated "semantically similar" prompt pairs may not reflect real-world prompt similarity distributions
- Increased CPU memory usage for storing reference KV caches not adequately addressed for multi-user serving environments

## Confidence

**High Confidence**: Core insight that RoPE-augmented embeddings improve LSH token matching quality (supported by Figure 7 L2 deviation reduction) and exponential decay retention's superiority in minimizing perplexity (demonstrated in Figure 4 across three model architectures).

**Medium Confidence**: Claimed 6.25× speedup and 42% memory reduction demonstrated primarily on MultiNews summarization tasks with specific prompt rewriting patterns; layer-adaptive recomputation strategy effectiveness across diverse attention patterns requires further validation.

**Low Confidence**: Generalizability to non-summarization tasks and robustness of LSH matching in production scenarios with diverse prompt distributions; focus on controlled synthetic dataset limits real-world applicability confidence.

## Next Checks

**Check 1: Cross-Domain Performance Validation**: Evaluate SemShareKV on non-summarization tasks (code generation, question answering, creative writing) using real user prompt datasets rather than synthetically rewritten prompts. Measure whether 6.25× speedup and 42% memory reduction persist when semantic similarity patterns differ from MultiNews summarization.

**Check 2: CPU Memory Bottleneck Analysis**: Implement method in multi-user serving scenario with high prompt diversity. Profile CPU memory usage as cache grows and measure point where CPU memory becomes limiting factor rather than GPU compute savings. Determine practical cache size limit and eviction strategy for production deployment.

**Check 3: Robustness to Prompt Distribution Shifts**: Create adversarial test cases where prompt rewriting introduces structural differences (active vs. passive voice, different clause ordering) that maintain semantic similarity but break LSH matching assumptions. Measure quality degradation and identify failure modes when semantic-to-lexical mapping becomes non-trivial.