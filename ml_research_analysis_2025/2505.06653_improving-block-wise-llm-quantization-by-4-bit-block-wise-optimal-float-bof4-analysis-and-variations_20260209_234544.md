---
ver: rpa2
title: 'Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4):
  Analysis and Variations'
arxiv_id: '2505.06653'
source_url: https://arxiv.org/abs/2505.06653
tags:
- quantization
- weights
- block
- bof4
- normalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses suboptimal quantization errors in block-wise
  LLM quantization methods like NF4 and AF4. The authors propose an optimization approach
  for block-wise quantization, resulting in a family of quantizers called 4-bit block-wise
  optimal float (BOF4).
---

# Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations

## Quick Facts
- arXiv ID: 2505.06653
- Source URL: https://arxiv.org/abs/2505.06653
- Authors: Patrick Blumenberg; Thomas Graave; Tim Fingscheidt
- Reference count: 40
- Key result: Proposed BOF4-S quantizer achieves lower perplexity than NF4/AF4 on Llama-3.1/3.2 and Qwen-2.5 models

## Executive Summary
This work addresses suboptimal quantization errors in block-wise LLM quantization methods like NF4 and AF4 by proposing a family of quantizers called 4-bit block-wise optimal float (BOF4). The authors derive optimal codebooks via an expectation-maximization algorithm and introduce a signed absolute block maximum normalization method (BOF4-S) that further reduces quantization error. They also propose an outlier-preserving quantization (OPQ) strategy to handle distributional mismatch. The BOF4-S quantizer optimized for MSE with OPQ consistently achieves lower quantization errors and better perplexity than baseline methods on multiple LLMs including Llama-3.1/3.2 and Qwen-2.5. When combined with QLoRA fine-tuning, the proposed methods demonstrate improved accuracy on instruction following and code generation tasks while maintaining memory efficiency for deployment on consumer-grade hardware.

## Method Summary
The authors present a mathematical analysis of block-wise quantization and derive an expectation-maximization algorithm for optimal codebook design. They introduce signed absolute block maximum normalization (BOF4-S) and an outlier-preserving quantization strategy (OPQ). The BOF4 family includes variants optimized for mean squared error (MSE) and mean absolute error (MAE), with signed and unsigned normalization options. Codebooks are derived either through numerical integration of analytical expressions or Monte Carlo sampling. The OPQ strategy stores outlier weights in 16-bit precision to handle distributional mismatch. The methods are evaluated on multiple LLM architectures including Llama-3.1/3.2 and Qwen-2.5, showing consistent improvements in quantization error and perplexity.

## Key Results
- BOF4-S MSE quantizer achieves lower quantization error and better perplexity than NF4/AF4 baselines on multiple LLMs
- Signed absolute block maximum normalization (BOF4-S) provides significant improvements over unsigned normalization
- Outlier-preserving quantization (OPQ) further reduces perplexity by storing outliers in higher precision
- When combined with QLoRA fine-tuning, BOF4 methods show improved accuracy on instruction following and code generation tasks
- BOF4-S + OPQ excels at code generation while BOF4 excels at instruction following, indicating task-dependent optimal configurations

## Why This Works (Mechanism)
The work derives optimal reconstruction levels for block-wise quantization by modeling weight distributions as Gaussian and using expectation-maximization to minimize reconstruction error. The signed normalization captures the sign information that unsigned methods lose, effectively doubling the representational capacity per block. The OPQ strategy addresses the limitation of assuming Gaussian distributions by preserving outliers in higher precision. These innovations work together to reduce the quantization error that accumulates during both initial weight quantization and subsequent fine-tuning.

## Foundational Learning
- **Block-wise quantization**: Dividing weights into blocks and quantizing each block independently to balance granularity and memory efficiency. Why needed: Enables 4-bit quantization while maintaining reasonable accuracy for LLMs. Quick check: Verify block size (I=64) and understand how block boundaries are determined.
- **Expectation-maximization for codebook optimization**: Iteratively updating reconstruction levels to minimize expected reconstruction error under a Gaussian weight distribution assumption. Why needed: Provides theoretically optimal codebooks for the assumed distribution. Quick check: Confirm codebook values match Table 6/7 from paper.
- **Signed vs unsigned normalization**: Signed normalization preserves sign information by treating the block maximum as signed, while unsigned uses absolute values. Why needed: Signed normalization doubles effective precision by encoding sign separately. Quick check: Verify normalization uses Eq. 4 (signed) not Eq. 1 (unsigned).
- **Outlier-preserving quantization**: Identifying and storing weights beyond a certain percentile in higher precision. Why needed: Addresses the limitation of Gaussian distribution assumption for real LLM weights. Quick check: Confirm q=0.95 threshold and bfloat16 storage format.
- **Numerical integration vs Monte Carlo**: Two methods for computing optimal reconstruction levels, one analytical and one sampling-based. Why needed: Provides flexibility in implementation depending on computational resources. Quick check: Verify both methods produce similar codebook values.
- **QLoRA fine-tuning**: Parameter-efficient fine-tuning method using low-rank adapters and 4-bit quantization. Why needed: Enables efficient adaptation of quantized models to downstream tasks. Quick check: Verify fine-tuning hyperparameters match paper specifications.

## Architecture Onboarding

Component map: Weight matrix -> Block partitioning (I=64) -> Normalization (Eq. 4) -> Quantization (BOF4 codebook) -> Optional OPQ -> Storage

Critical path: Quantization -> OPQ (if enabled) -> Fine-tuning performance. The quantization method directly impacts the initial model quality, which affects downstream fine-tuning results.

Design tradeoffs: Signed normalization vs unsigned trades a small increase in quantization constant storage for significantly reduced quantization error. OPQ trades increased memory usage for better handling of outliers. MSE-optimized vs MAE-optimized codebooks trade reconstruction error metrics.

Failure signatures: High MSE/MAE relative to baselines indicates incorrect codebook generation or normalization. No PPL improvement from OPQ suggests incorrect outlier identification or replacement. Poor fine-tuning results may indicate suboptimal LoRA hyperparameters.

First experiments:
1. Generate BOF4-S MSE codebook for I=64 using Eq. 5 (numerical integration) and verify values match Table 6
2. Implement signed absmax normalization (Eq. 4) and measure MSE/MAE on quantized weights vs NF4/AF4
3. Add OPQ with q=0.95 and measure PPL improvement on WikiText-2 benchmark

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal BOF4 quantizer variant (signed vs. unsigned, with or without OPQ) be determined a priori for a specific downstream task without requiring an ablation study? The authors note results show task-dependent behavior where BOF4 excels at instruction following while BOF4-S + OPQ excels at code generation, but no predictive framework exists.

### Open Question 2
How does BOF4-S with signed normalization perform when combined with double quantization of the quantization constants? The paper states signed normalization would require an extra bit per block to encode the sign, potentially diminishing the improvement without this bit.

### Open Question 3
How does BOF4 compare to calibration-based PTQ methods that use activation data? The authors acknowledge their evaluation focuses on data-free methods and doesn't include PTQ methods that rely on calibration data.

### Open Question 4
How robust is the Gaussian weight distribution assumption across diverse model architectures and training regimes? The derivation relies on assuming zero-mean symmetric weight distributions, but the paper only validates on three model families.

## Limitations
- Relies on Gaussian weight distribution assumption which may not hold for all model architectures
- Does not compare against calibration-based PTQ methods that use activation data
- Task-dependent optimal quantizer configuration requires ablation studies rather than predictive heuristics
- Double quantization compatibility with signed normalization not fully explored

## Confidence

High confidence: Mathematical derivation of optimal codebooks via EM algorithm, codebook values provided in appendix, explicit procedures for numerical integration and Monte Carlo sampling, reproducible QLoRA fine-tuning setup.

Medium confidence: Fine-tuning results depend on unspecified LoRA hyperparameters (rank, alpha, target modules) and data preprocessing details for downstream tasks.

Low confidence: No identified limitations beyond those acknowledged by authors.

## Next Checks

1. Reimplement the BOF4-S MSE codebook for block size I=64 using Eq. 5 (numerical integration) or Eq. 6 (Monte Carlo), and verify the reconstruction levels match the values in Table 6.

2. Integrate signed absmax normalization (Eq. 4) and the derived codebook into a QLoRA-style 4-bit quantizer, then measure MSE/MAE and WikiText-2 PPL on a 8B LLM (e.g., Llama-3.1-8B) against NF4/AF4 baselines.

3. If testing OPQ, confirm outliers are identified with q=0.95 via Eq. 9, replaced with zero before normalization, and stored in bfloat16 plus 64-bit indices, then measure the resulting PPL improvement.