---
ver: rpa2
title: 'DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar
  Scenes Visual Prompts'
arxiv_id: '2510.24813'
source_url: https://arxiv.org/abs/2510.24813
tags:
- visual
- image
- arxiv
- retrieval
- dualcap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating detailed image captions
  with lightweight models, which often struggle to capture fine-grained visual details
  due to reliance on static global features. The authors propose DualCap, a novel
  approach that enhances lightweight image captioning by employing a dual retrieval
  mechanism.
---

# DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts

## Quick Facts
- arXiv ID: 2510.24813
- Source URL: https://arxiv.org/abs/2510.24813
- Authors: Binbin Li; Guimiao Yang; Zisen Qi; Haiping Wang; Yu Ding
- Reference count: 40
- Primary result: DualCap achieves 123.6 CIDEr on COCO with only 11M trainable parameters

## Executive Summary
DualCap addresses the challenge of generating detailed image captions with lightweight models by introducing a dual retrieval mechanism that combines image-to-text and image-to-image retrieval. The approach uses semantically similar captions for contextual prompts while extracting salient keywords from visually similar scenes to create visual prompts that enhance the original image features. Experiments demonstrate that this method achieves state-of-the-art performance among lightweight models on COCO (123.6 CIDEr) while requiring only 11M trainable parameters, maintaining strong cross-domain generalization on NoCaps.

## Method Summary
DualCap employs a dual retrieval mechanism that first retrieves semantically similar captions (I2T) and visually similar images (I2I) for a given input image. From I2I-retrieved captions, salient keywords are extracted using NLTK and encoded as visual prompts. These prompts are fused with original image features via a lightweight Feature Fusion Network (SFN) - a single-layer Transformer with cross-attention - before being fed to a frozen GPT-2 decoder. The model freezes pre-trained CLIP vision encoder and GPT-2 decoder, training only the SFN and GPT-2 cross-attention layers (11M parameters total). This architecture enriches visual representations while maintaining efficiency through parameter freezing.

## Key Results
- Achieves 123.6 CIDEr on COCO test split, outperforming baselines SmallCap and ViPCap
- Maintains strong cross-domain generalization with 11.6 SPICE on NoCaps
- Requires only 11M trainable parameters while using frozen CLIP ViT-B/32 and GPT-2 Base
- Inference time of 0.42s with beam search (vs 0.25s for SmallCap)

## Why This Works (Mechanism)

### Mechanism 1: Dual Retrieval Decouples Contextual and Visual Evidence
Separating image-to-text and image-to-image retrieval paths provides both high-level semantic context and fine-grained visual details, improving caption quality and cross-domain generalization compared to single-stream retrieval. The I2T path retrieves semantically similar captions as a text prompt for GPT-2. The I2I path retrieves visually similar images, extracts scene-keywords from their captions, and uses these to enhance the original image features via SFN. This addresses the visual-text semantic gap left by text-only retrieval.

### Mechanism 2: Keyword-Driven Visual Prompt Enhances Feature Specificity
Using extracted scene-keywords to form a visual prompt yields more targeted and less noisy visual feature enhancement than using full retrieved captions. Keywords/phrases are extracted from I2I-retrieved captions using NLTK (POS tagging, chunking). These keywords are encoded by frozen CLIP text encoder. The SFN (single-layer Transformer with cross-attention) fuses these keyword embeddings with original image patch features, producing a visual prompt added to the original features.

### Mechanism 3: Lightweight Fusion with Frozen Backbones Maximizes Efficiency
Freezing large pre-trained models (CLIP encoder, GPT-2 decoder) and training only the lightweight SFN and cross-attention layers achieves competitive performance with minimal parameters. Only the SFN and GPT-2 cross-attention layers are trainable (~11M params). The visual prompt generated by SFN is added to frozen CLIP features via residual connection. The enhanced features condition the frozen GPT-2 decoder alongside the text prompt.

## Foundational Learning

- **Cross-Attention in Transformer Architectures**: The SFN uses cross-attention to fuse visual and textual features. Understanding Query/Key/Value roles is essential to grasp how image patches attend to keyword embeddings. Quick check: In DualCap's SFN, what serves as the Query and what serves as the Key/Value?

- **Retrieval-Augmented Generation (RAG)**: DualCap's core innovation is a dual RAG mechanism. Understanding standard RAG (text-only) clarifies the motivation for adding visual retrieval. Quick check: How does DualCap's I2T retrieval path differ from its I2I retrieval path in purpose?

- **CLIP Joint Vision-Language Embeddings**: DualCap relies on CLIP for image encoding, text encoding, and retrieval (cosine similarity). Understanding CLIP's shared embedding space is critical. Quick check: Why can cosine similarity be used to retrieve both similar images and similar captions in DualCap's setup?

## Architecture Onboarding

- **Component map**: Input image → CLIP ViT-B/32 → patch features V + global feature v_I → I2T retrieval → text prompt X + I2I retrieval → NLTK keyword extraction → Kp → CLIP text encoder → E_kp → SFN (V as Query, E_kp as Key/Value) → visual prompt Z_kp → V' = V + Z_kp → GPT-2 decoder (X, V') → output caption

- **Critical path**: 1) Input image I encoded by CLIP to global feature v_I and patch features V. 2) v_I queries D_cap for top-k captions (I2T), formatted into text prompt X. 3) v_I queries D_img for top-M similar images (I2I), their captions processed by NLTK to extract Kp. 4) Kp encoded by CLIP text encoder to E_kp. 5) SFN takes V (Query) and E_kp (Key/Value), outputs visual prompt Z_kp. 6) V' = V + Z_kp (residual). 7) X and V' condition frozen GPT-2 decoder to generate caption.

- **Design tradeoffs**: Dual retrieval improves accuracy but increases inference latency vs. single-stream models (Table 2: 0.42s vs. SmallCap 0.25s). Keyword extraction reduces noise vs. full captions but may miss nuanced scene descriptions dependent on context. Freezing encoder/decoder limits adaptability vs. full fine-tuning but drastically reduces trainable parameters (11M).

- **Failure signatures**: Low CIDEr on in-domain data: Check I2I retrieval quality (are retrieved images truly similar?). Check keyword extraction output (are keywords relevant?). Poor generalization (NoCaps out-of-domain): I2I datastore may lack relevant visual examples. Text prompt style from I2T may not match target domain. Slow inference: Retrieval from non-optimized datastore (ensure FAISS indexing). SFN adds minor overhead.

- **First 3 experiments**: 1) Ablate dual retrieval: Run I2T-only and I2I-only variants to measure individual contributions (as in Table 3). 2) Vary fusion method: Compare SFN vs. direct sum, concat, or concat+MLP (as in Table 4) to validate SFN design. 3) Swap decoders: Test GPT-2, OPT-125M, and XGLM-564M to verify architecture robustness (as in Table 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DualCap framework be effectively extended to Visual Question Answering (VQA) tasks?
- Basis in paper: [explicit] The Conclusion states the work "opens promising avenues for future work, such as extending the framework to Visual Question Answering."
- Why unresolved: The current architecture is optimized for open-ended generation rather than question-specific retrieval and answering logic.
- Evidence would resolve it: Implementation of DualCap on standard VQA benchmarks (e.g., VQAv2) demonstrating that dual retrieval aids visual reasoning efficiency.

### Open Question 2
- Question: How does the model perform when the image-to-image retrieval mechanism fails to find visually similar scenes in the datastore?
- Basis in paper: [inferred] Section 3.1 relies on retrieving "visually analogous" images, but performance on distinct out-of-distribution images (where retrieval fails) is not analyzed.
- Why unresolved: The paper assumes the existence of similar images in the datastore; the degradation rate when this assumption is violated remains unknown.
- Evidence would resolve it: Ablation studies measuring performance drops on images with low similarity scores to the datastore (bottom decile of retrieval confidence).

### Open Question 3
- Question: Is the reliance on rule-based keyword extraction (NLTK) a bottleneck for capturing complex semantic relationships compared to learned extractors?
- Basis in paper: [inferred] Section 3.2 extracts keywords using "standard NLP toolkit NLTK" with POS tagging, which may miss nuanced descriptive phrases.
- Why unresolved: Rule-based approaches often struggle with syntactic complexity, potentially limiting the quality of the generated visual prompt $Z_{kp}$.
- Evidence would resolve it: Comparative analysis replacing NLTK with a learned transformer-based phrase extractor to quantify the impact on CIDEr scores.

## Limitations

- Dual retrieval mechanism increases inference latency compared to single-stream models (0.42s vs 0.25s for SmallCap)
- Frozen backbone approach may limit adaptability to domain-specific visual patterns not well-represented in training data
- Rule-based keyword extraction using NLTK may miss nuanced descriptive phrases and struggle with syntactic complexity

## Confidence

- **High Confidence**: Parameter efficiency claims (11M trainable parameters) and core architectural design of dual retrieval with feature fusion are well-supported by experimental results and ablation studies
- **Medium Confidence**: Performance improvements over baselines on COCO, Flickr30k, and NoCaps are demonstrated, but generalization to other datasets or domains remains to be seen
- **Low Confidence**: Exact implementation details of keyword extraction and specific impact of keyword quality on overall performance are not fully transparent

## Next Checks

1. **Ablation of Keyword Extraction Quality**: Conduct an experiment where keyword extraction is systematically varied (e.g., using different POS tag sets, varying the number of keywords P) to quantify its impact on caption quality. This would help determine whether the keyword extraction process is a critical success factor or if simpler methods could achieve similar results.

2. **Latency vs. Performance Trade-off Analysis**: Measure the exact inference time difference between DualCap and single-stream retrieval models across different hardware configurations. Quantify whether the performance gain (e.g., CIDEr improvement) justifies the additional computational cost in practical deployment scenarios.

3. **Cross-Domain Generalization Stress Test**: Evaluate DualCap on additional out-of-domain datasets beyond NoCaps (e.g., medical imaging, satellite imagery) to assess whether the dual retrieval mechanism maintains its effectiveness when the I2I datastore contains limited relevant visual examples. This would validate the model's claimed strong cross-domain generalization.