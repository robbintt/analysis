---
ver: rpa2
title: 'xai_evals : A Framework for Evaluating Post-Hoc Local Explanation Methods'
arxiv_id: '2502.03014'
source_url: https://arxiv.org/abs/2502.03014
tags:
- explanation
- methods
- evals
- output
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: xaievals is a Python framework for evaluating post-hoc local explanation
  methods in machine learning models. It addresses the challenge of interpreting complex
  black-box models by providing standardized tools for generating, benchmarking, and
  evaluating explanations across both tabular and image data modalities.
---

# xai_evals : A Framework for Evaluating Post-Hoc Local Explanation Methods

## Quick Facts
- **arXiv ID:** 2502.03014
- **Source URL:** https://arxiv.org/abs/2502.03014
- **Reference count:** 3
- **Primary result:** A Python framework for evaluating post-hoc local explanation methods (SHAP, LIME, Grad-CAM, Integrated Gradients, Backtrace) across tabular and image data

## Executive Summary
xai_evals is a comprehensive Python framework designed to evaluate post-hoc local explanation methods for machine learning models. The framework addresses the challenge of interpreting complex black-box models by providing standardized tools for generating, benchmarking, and evaluating explanations across both tabular and image data modalities. It integrates popular explanation techniques and supports evaluation metrics including faithfulness, sensitivity, comprehensiveness, and robustness.

The framework enhances model interpretability and supports regulatory compliance requirements like GDPR's "right to explanation." By enabling researchers and practitioners to systematically assess explanation quality across multiple methods, models, and datasets, xai_evals provides a crucial tool for understanding and validating AI model decisions in both research and practical applications.

## Method Summary
The xai_evals framework provides a structured approach to evaluating explanation methods through a modular pipeline. Users first initialize specific Explainer classes (such as SHAPExplainer or TorchImageExplainer) to generate attributions from trained models. These attributions are then passed to Metric classes (like ExplanationMetricsTabular or ExplanationMetricsImage) for quantitative assessment using metrics such as Faithfulness, Sensitivity, Comprehensiveness, and Robustness for tabular data, or Faithfulness Correlation and MPRT for image data. The framework supports both tabular datasets (like Iris) and image datasets (like Imagenette), working with various model types including RandomForest classifiers and pre-trained deep neural networks. The modular design allows for flexible combination of explanation methods and evaluation metrics, enabling comprehensive benchmarking across different approaches.

## Key Results
- Successfully integrates five major explanation methods (SHAP, LIME, Grad-CAM, Integrated Gradients, Backtrace) into a unified evaluation framework
- Provides standardized evaluation metrics for both tabular and image data modalities
- Demonstrates framework effectiveness through experimental results comparing explanation quality across different methods and models
- Supports regulatory compliance requirements like GDPR's "right to explanation" through systematic evaluation capabilities

## Why This Works (Mechanism)
The framework works by standardizing the evaluation pipeline for explanation methods through modular components. By separating explanation generation (Explainer classes) from evaluation (Metric classes), it creates a consistent interface that allows fair comparison across different techniques. The framework leverages established libraries like Captum and Quantus for implementation while providing a unified API that abstracts away implementation differences between methods.

## Foundational Learning
**Post-hoc local explanation methods:** Techniques that explain individual predictions after a model is trained, rather than building interpretable models from the start. Needed to interpret black-box models without sacrificing predictive performance. Quick check: Can explain a single prediction rather than global model behavior.

**Faithfulness metrics:** Measures how well explanations reflect the true model behavior by assessing attribution consistency with model predictions. Essential for validating that explanations accurately represent what the model actually learned. Quick check: Higher faithfulness indicates explanations better align with model internals.

**Sensitivity analysis:** Evaluates how explanation attributions change when input features are perturbed. Critical for understanding explanation stability and robustness. Quick check: Lower sensitivity indicates more stable explanations across similar inputs.

**Comprehensiveness metrics:** Assesses whether explanations capture all relevant information about model decisions by measuring the impact of removing important features. Needed to ensure explanations are complete and not missing critical information. Quick check: Higher comprehensiveness indicates more complete explanations.

**Attribution visualization:** Methods for displaying which input regions contribute most to predictions, crucial for human interpretation of complex models. Required for intuitive understanding of model decisions. Quick check: Clear visual highlighting of important regions in input space.

## Architecture Onboarding

**Component Map:**
xai_evals -> Explainer classes (SHAPExplainer, TorchImageExplainer) -> Metric classes (ExplanationMetricsTabular, ExplanationMetricsImage) -> Evaluation results

**Critical Path:**
1. Load and prepare dataset (tabular or image)
2. Train or load pre-trained model
3. Initialize appropriate Explainer class for chosen method
4. Generate attributions for test instances
5. Initialize appropriate Metric class
6. Compute evaluation metrics from attributions
7. Analyze and compare results across methods

**Design Tradeoffs:**
The framework prioritizes modularity and flexibility over tight integration, allowing users to mix and match explanation methods with evaluation metrics. This design choice sacrifices some optimization opportunities for the benefit of broader applicability across different use cases and research scenarios.

**Failure Signatures:**
- Missing or incorrect attributions due to model output format mismatches
- Inconsistent metric calculations from improper baseline settings
- Dependency conflicts preventing proper library functionality
- Visualization artifacts from incompatible image processing pipelines

**First Experiments:**
1. Run the Iris dataset example with RandomForestClassifier and SHAPExplainer to verify basic tabular functionality
2. Execute the ResNet18 image example with layer_gradcam to validate image explanation capabilities
3. Compare Faithfulness scores across multiple explanation methods on the same dataset to test benchmarking functionality

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed hyperparameter specifications for metric calculations may affect reproducibility of exact numerical results
- Dependency chain involving PyTorch, TensorFlow, Captum, and Quantus may introduce version compatibility challenges
- Depth of evaluation differs between tabular and image modalities, with tabular metrics being more extensively documented

## Confidence

**High Confidence:** Framework's core functionality for generating and evaluating explanations across supported methods is well-documented and reproducible based on provided code examples.

**Medium Confidence:** Effectiveness claims regarding explanation quality comparison are supported by framework design but lack complete experimental details for full verification.

**Medium Confidence:** Framework's utility for regulatory compliance is theoretically sound but not empirically validated in the paper.

## Next Checks
1. Execute the tabular example workflow (Iris dataset + RandomForestClassifier + SHAP + ExplanationMetricsTabular) and compare generated metric values against expected outputs to verify metric calculation accuracy.

2. Run the image example (ResNet18 + layer_gradcam) and validate that attribution visualizations are correctly generated and match expected Grad-CAM outputs.

3. Perform dependency compatibility testing by installing the framework in a fresh environment with specified deep learning backends to identify and resolve any version conflicts that might affect functionality.