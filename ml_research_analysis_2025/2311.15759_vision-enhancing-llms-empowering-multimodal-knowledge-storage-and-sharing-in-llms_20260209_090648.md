---
ver: rpa2
title: 'Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing
  in LLMs'
arxiv_id: '2311.15759'
source_url: https://arxiv.org/abs/2311.15759
tags:
- visual
- multimodal
- llms
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MKS2, a vision-enhanced learning framework
  designed to empower multimodal knowledge storage and sharing in large language models
  (LLMs). The core innovation lies in integrating Modular Visual Memory (MVM) into
  LLM blocks to efficiently store open-world visual information, coupled with a soft
  Mixture of Multimodal Experts (MoMEs) architecture to facilitate multimodal knowledge
  collaboration during text generation.
---

# Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs

## Quick Facts
- **arXiv ID**: 2311.15759
- **Source URL**: https://arxiv.org/abs/2311.15759
- **Reference count**: 22
- **One-line primary result**: Introduces MKS2, a vision-enhanced learning framework that significantly improves LLM performance on knowledge-intensive NLP benchmarks while using fewer than 0.25% of LLM parameters.

## Executive Summary
This paper introduces MKS2, a vision-enhanced learning framework designed to empower multimodal knowledge storage and sharing in large language models (LLMs). The core innovation lies in integrating Modular Visual Memory (MVM) into LLM blocks to efficiently store open-world visual information, coupled with a soft Mixture of Multimodal Experts (MoMEs) architecture to facilitate multimodal knowledge collaboration during text generation. Unlike traditional MLLMs that primarily focus on visual understanding ("LLMs for Vision"), MKS2 shifts towards enhancing LLMs with visual knowledge to improve reasoning capabilities on text-only tasks ("Vision-Enhancing LLMs"). The framework employs language-centered learning strategies—image-to-text generation and text-to-image retrieval—to train the MVM. Extensive experiments demonstrate that MKS2 significantly improves LLM performance on knowledge-intensive NLP benchmarks (e.g., CommonsenseQA, PIQA, StrategyQA) and achieves competitive results on image-text understanding tasks, outperforming traditional SFT and RLHF-tuned models while using fewer than 0.25% of LLM parameters. The method effectively bridges the gap between visual and textual knowledge, enabling LLMs to reason using stored multimodal information even without direct visual input.

## Method Summary
MKS2 employs a two-stage training process to enhance LLMs with visual knowledge. In Stage 1, Modular Visual Memory (MVM) components—two-layer feed-forward networks—are injected into each transformer block of the LLM. The LLM backbone remains frozen while MVM is trained on 2.3 million image-text pairs using language-centered objectives: image-to-text generation and text-to-image retrieval. In Stage 2, a soft Mixture of Multimodal Experts (MoMEs) architecture is introduced, treating pretrained MVM as a "Visual Expert" and the original LLM MLPs as a "Textual Expert." LoRA adapters are applied to both for efficient fine-tuning, and a learned router dynamically mixes expert outputs during generation. The framework achieves significant performance gains on knowledge-intensive text-only tasks while maintaining competitive multimodal understanding capabilities.

## Key Results
- MKS2 significantly improves LLM performance on knowledge-intensive NLP benchmarks (CommonsenseQA, PIQA, StrategyQA) while using fewer than 0.25% of LLM parameters.
- The framework achieves state-of-the-art results on multiple benchmarks, outperforming traditional SFT and RLHF-tuned models.
- MKS2 demonstrates competitive performance on image-text understanding tasks (VQAv2, OK-VQA) while excelling at text-only reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting lightweight feed-forward networks (MVM) into transformer blocks enables storage of visual knowledge that transfers to text-only reasoning tasks.
- **Mechanism:** MVM consists of two FFN layers added after self-attention in each LLM block. During Stage 1 training, visual embeddings pass through MVM while LLM parameters remain frozen. The FFN weights learn to encode visual-to-language mappings through image-to-text generation and text-to-image retrieval objectives. At inference, even without visual input, MVM's stored weights retain cross-modal associations that activate during language reasoning.
- **Core assumption:** FFN layers in transformers function as key-value memory stores (referenced from prior work), and this property extends to storing modality-transformed knowledge.
- **Evidence anchors:**
  - [abstract] "introduce Modular Visual Memory (MVM), a component integrated into the internal blocks of LLMs, designed to store open-world visual information efficiently"
  - [Section 4.1] "MVM is a two layer of feed-forward neural networks (FFN) and is injected into each transformer block"
  - [corpus] Weak/no direct corpus validation; related work focuses on multimodal generation, not parametric visual storage
- **Break condition:** If text-only tasks show no improvement after MVM training, or if MVM dimension is too small to encode visual patterns, storage capacity is insufficient.

### Mechanism 2
- **Claim:** A soft mixture of multimodal experts (MoMEs) enables dynamic routing between stored visual knowledge and linguistic knowledge during generation.
- **Mechanism:** MoMEs treats pretrained MVM as a "Visual Expert" and original LLM MLPs as a "Textual Expert." LoRA adapters are applied to both for efficient fine-tuning. Token-level soft weights (via learned projection + softmax) determine the contribution of each expert per token, allowing the model to invoke visual knowledge when reasoning requires physical/commonsense understanding.
- **Core assumption:** Visual and textual knowledge are sufficiently disentangled that separate experts can specialize, and a learned router can effectively determine when to invoke each.
- **Evidence anchors:**
  - [abstract] "soft Mixture of Multimodal Experts (MoMEs) architecture to facilitate multimodal knowledge collaboration during text generation"
  - [Section 4.2, Eq. 6] "S = Softmax(w_s X + b_s), h_M = S_1 h_VE + S_2 h_TE"
  - [Table 4] Ablation shows ~6% average gain on StrategyQA when MVM is present vs. absent
  - [corpus] No direct corpus evidence; MoE architectures are established but token-level multimodal routing is novel
- **Break condition:** If soft mixing weights collapse to favor one expert uniformly, or if visual expert never activates on text-only queries, routing has failed to learn meaningful specialization.

### Mechanism 3
- **Claim:** Language-centered learning (image-to-text generation + text-to-image retrieval) suffices to encode visual knowledge into parametric memory without requiring visual input at inference.
- **Mechanism:** Stage 1 uses bidirectional objectives: (1) generate captions from images (L_c) to force visual-to-language translation; (2) retrieve images from text (L_t2i via InfoNCE) to force language-to-visual association. Both push visual embeddings through MVM, training it to serve as a cross-modal bridge. Crucially, LLM backbone remains frozen, preserving linguistic knowledge while expanding the knowledge surface.
- **Core assumption:** Visual knowledge relevant to commonsense reasoning can be compressed into parametric form and retrieved via language cues alone, without explicit visual retrieval at test time.
- **Evidence anchors:**
  - [Section 4.1] "we adopt two learning objects to train MVM with a large amount of image-text pairs"
  - [Section 4.1, Eq. 3-4] Formal loss definitions for L_c and L_t2i
  - [Figure 5] MVM generates plausible images from text prompts, confirming bidirectional association
  - [corpus] Weak; retrieval-augmented approaches exist but parametric storage without inference-time retrieval is underexplored
- **Break condition:** If retrieval accuracy plateaus despite more data, or if text-only QA shows no correlation with retrieval quality, the compression-to-parameters hypothesis fails.

## Foundational Learning

- **Concept**: Feed-forward networks as key-value memories in transformers
  - **Why needed here**: MVM is architecturally a 2-layer FFN; understanding why FFNs store knowledge (not just transform representations) explains why visual patterns can persist in MVM weights.
  - **Quick check question**: Can you explain why prior work (e.g., Geva et al., referenced in paper) suggests FFNs in transformers function as pattern-matching memories rather than purely computational layers?

- **Concept**: Mixture of Experts (MoE) and sparse/expert routing
  - **Why needed here**: MoMEs is a soft, token-level MoE variant; understanding standard MoE routing (gating networks, expert specialization) provides the baseline for comparing this architecture.
  - **Quick check question**: In a standard MoE layer, how does the gating network determine which experts to activate, and what is the difference between hard (top-k) and soft routing?

- **Concept**: Vision-language alignment via projection layers
  - **Why needed here**: The paper uses BLIP-2's Q-Former and learnable projections to map visual embeddings to LLM input space; this is the interface through which MVM receives visual signals.
  - **Quick check question**: What is the role of a Q-Former (from BLIP-2) in aligning visual features to language model input space, and why might it preserve more semantic information than a simple linear projection?

## Architecture Onboarding

**Component map:**
```
Visual Encoder (CLIP/ViT, frozen)
        ↓
Q-Former / Linear Projection (BLIP-2, frozen or fine-tuned)
        ↓
Soft Image Embeddings (32 tokens)
        ↓
LLM Transformer Blocks (frozen):
  [Self-Attention] → [MVM (2-layer FFN, trained)] → [LayerNorm] → residual
        ↓
MoMEs Layer (during Stage 2):
  Expert 1: LoRA-MVM (Visual Expert)
  Expert 2: LoRA-MLP (Textual Expert)
  Router: Softmax(w_s X + b_s) → weighted sum
        ↓
Output (text generation)
```

**Critical path:**
1. **Stage 1 (Visual Information Storage):** Freeze LLM → insert MVM into all blocks → train MVM only on 2.3M image-text pairs using L_c + L_t2i. Validate with image retrieval and caption generation metrics.
2. **Stage 2 (Multimodal Knowledge Collaboration):** Freeze MVM and LLM → add LoRA (r=8-16) to both MVM and original MLPs → train on mixed text-only + image-text instruction data → MoMEs learns routing.
3. **Inference:** Text-only queries activate MoMEs routing; visual expert contributes if query benefits from stored visual knowledge.

**Design tradeoffs:**
- **MVM dimension (1/4 of hidden size):** Smaller = parameter-efficient (<0.25% LLM params) but risks under-capacity for complex visual patterns. Paper tested 410M and 810M variants with mixed results (Table 4).
- **Frozen LLM backbone:** Preserves linguistic knowledge and reduces overfitting but limits deep integration of visual reasoning circuits.
- **Soft vs. hard routing:** Soft mixing (all experts contribute) enables gradient flow and co-adaptation but may dilute expert specialization compared to top-k gating.
- **LoRA rank (r=16 optimal in Table 6):** Higher ranks add parameters without proportional gains and can degrade performance (overfitting).

**Failure signatures:**
- **Knowledge-based visual reasoning fails:** Model cannot integrate external factual knowledge with visual input (Figure 7, map/geography example). Indicates MVM stores associations but lacks grounded world knowledge.
- **Spatial reasoning fails:** Cannot infer object relationships or dynamics (Figure 7, predator-prey misclassification). Suggests MVM encodes object-level patterns but not relational structures.
- **Scene text recognition degrades when text-only instruction data is added (Table 2):** Data distribution shift harms specialized capabilities—tradeoff between general reasoning and task-specific performance.

**First 3 experiments:**
1. **Validate MVM storage capacity:** Train MVM on a held-out subset of image-text pairs (e.g., 500K), then evaluate text-to-image retrieval R@1 on a disjoint test set. Compare against CLIP zero-shot to confirm MVM learns meaningful associations.
2. **Ablate MoMEs routing:** Replace soft mixing with (a) visual expert only, (b) textual expert only, (c) hard top-1 routing. Report on StrategyQA, PIQA, and CommonsenseQA to isolate routing contribution.
3. **Probe expert specialization:** For a sample of text-only queries, record soft weights S_1 (visual) and S_2 (textual) per token. Cluster queries by expert preference and analyze whether visual-heavy queries correlate with physical/commonsense topics vs. abstract reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MKS2 architecture be enhanced to handle complex visual-spatial reasoning tasks involving dynamic interactions?
- Basis in paper: [explicit] Section 6.7 identifies "deficiencies in knowledge-based visual reasoning and complex visual-spatial reasoning" as the model's primary limitations, noting it struggles to infer actions or spatial relationships (e.g., orientations, relative positions).
- Why unresolved: The current Modular Visual Memory (MVM) stores visual information but lacks specialized mechanisms to explicitly model or infer spatial dynamics and causal narratives from static visual cues.
- What evidence would resolve it: Demonstrated success on benchmarks requiring relative position reasoning or dynamic action inference where the current model currently fails.

### Open Question 2
- Question: How can the framework be improved to integrate external factual knowledge that is not explicitly contained in image pixels?
- Basis in paper: [explicit] The authors state in Section 6.7 that the model fails to combine visual understanding with external, factual knowledge, such as applying basic geographical concepts to a map.
- Why unresolved: The model relies on internal parametric knowledge which is insufficient for facts not visually present or previously encoded during pre-training.
- What evidence would resolve it: Improved performance on knowledge-based VQA tasks that require reasoning over information absent from the provided image but present in general knowledge bases.

### Open Question 3
- Question: What specific data distributions or fine-tuning strategies can resolve the trade-off where text-only instruction data harms scene text recognition?
- Basis in paper: [explicit] The authors note in Section 6.4 and Section 6.6 that optimizing performance is necessary because text-only data, while beneficial for open-ended QA, can diminish the model's ability to perform scene text recognition due to training data shifts.
- Why unresolved: A "careful balance" is stated to be required, but the specific recipe for mixing text-only and multimodal data to maximize both reasoning and recognition is undetermined.
- What evidence would resolve it: Identification of a specific data ratio or curriculum that yields simultaneous state-of-the-art performance on both commonsense reasoning (e.g., StrategyQA) and scene text tasks (e.g., TextVQA).

## Limitations

- **Limited scope of visual knowledge storage**: The paper demonstrates that MVM can store and retrieve visual associations for commonsense reasoning tasks, but does not explore the limits of what visual knowledge can be compressed into parametric form. The 2.3M image-text pairs used for training represent a relatively small fraction of the visual knowledge available in the world.
- **Potential overfitting to specific reasoning patterns**: The ablation studies show that removing MVM degrades performance on StrategyQA, but the improvement may stem from memorizing specific visual-physical patterns rather than learning generalizable reasoning principles.
- **Architecture constraints and trade-offs**: The design choice to keep the LLM backbone frozen while training only the MVM and LoRA adapters creates a rigid separation between visual and linguistic knowledge, which may limit the model's ability to form deeper cross-modal abstractions.

## Confidence

**High confidence**: The core architectural contribution (MVM + MoMEs) is clearly specified and implemented, with ablation studies demonstrating that both components contribute to performance gains. The framework achieves state-of-the-art results on multiple benchmarks while using fewer than 0.25% of LLM parameters.

**Medium confidence**: The mechanism by which visual knowledge stored in MVM transfers to text-only reasoning is theoretically sound but not fully validated. While retrieval performance and text-only benchmark results are strong, the paper does not provide direct evidence that the visual expert is actually activating on text-only queries or that the stored visual patterns are being used for reasoning rather than simple memorization.

**Low confidence**: The claim that MKS2 shifts from "LLMs for Vision" to "Vision-Enhancing LLMs" is compelling but requires more rigorous validation. The paper demonstrates improved text-only reasoning and competitive multimodal performance, but does not provide systematic analysis of whether the model is truly reasoning with stored visual knowledge versus leveraging other patterns.

## Next Checks

**Check 1**: Probe expert activation patterns during inference. For a diverse set of text-only queries spanning commonsense, abstract, and factual reasoning, record the soft routing weights (S_1, S_2) for each token. Analyze whether visual expert activation correlates with queries that benefit from physical/world knowledge (e.g., commonsense questions about object properties, spatial relationships) versus abstract reasoning tasks.

**Check 2**: Test visual knowledge generalization on novel visual scenarios. Create a test set of image-text pairs representing visual concepts not present in the training data (e.g., new object combinations, novel physical scenarios). Evaluate both image-to-text generation quality and text-to-image retrieval performance to validate whether MVM learned generalizable visual-language associations.

**Check 3**: Compare against joint fine-tuning baseline. Train a version of MKS2 where both MVM and LLM backbone are jointly fine-tuned on the same data (Stage 1), then add MoMEs. Compare performance on text-only benchmarks against the frozen-LLM version to reveal whether the architectural separation is beneficial or whether deeper integration could yield better reasoning capabilities.