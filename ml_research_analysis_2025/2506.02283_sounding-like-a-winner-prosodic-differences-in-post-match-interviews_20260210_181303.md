---
ver: rpa2
title: Sounding Like a Winner? Prosodic Differences in Post-Match Interviews
arxiv_id: '2506.02283'
source_url: https://arxiv.org/abs/2506.02283
tags:
- speech
- emotion
- emotional
- features
- prosodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the prosodic characteristics associated
  with winning and losing in post-match tennis interviews. The researchers analyzed
  acoustic features such as pitch and intensity, along with self-supervised learning
  (SSL) representations from Wav2Vec 2.0 and HuBERT, to determine whether athletes
  had won or lost their matches.
---

# Sounding Like a Winner? Prosodic Differences in Post-Match Interviews

## Quick Facts
- **arXiv ID:** 2506.02283
- **Source URL:** https://arxiv.org/abs/2506.02283
- **Reference count:** 0
- **Primary result:** SSL representations (HuBERT: 65.9%, Wav2Vec 2.0: 63.9%) significantly outperform handcrafted acoustic features (44.4%) for classifying tennis match outcomes from post-match interview speech.

## Executive Summary
This study investigates whether prosodic patterns in post-match tennis interviews can distinguish winning from losing players. The researchers extracted both traditional acoustic features (eGeMAPS/openSMILE) and self-supervised learning representations (Wav2Vec 2.0, HuBERT) from the iMiGUE dataset, then trained classifiers to predict match outcomes. The analysis reveals that winners exhibit more dynamic pitch movements, varied spectral features, and higher intensity, while losers display flatter, more monotonous speech patterns. SSL representations outperformed handcrafted features by a substantial margin, with HuBERT achieving 65.9% accuracy versus openSMILE's 44.4%. These findings demonstrate that SSL-based approaches are more effective than traditional acoustic-prosodic features for distinguishing between winning and losing players based on their speech patterns.

## Method Summary
The study used the iMiGUE dataset containing 359 post-match interview videos from Grand Slam tennis tournaments. Audio was extracted and processed through speaker diarization (Pyannote), transcription (Whisper Large), VAD, and overlap detection, with the longest-duration speaker identified as the athlete. Two feature sets were extracted: traditional eGeMAPS acoustic features via openSMILE (88 dimensions) and SSL representations from Wav2Vec 2.0 Large, HuBERT Large, and exHuBERT (1024-dimensional embeddings from final transformer layer, mean-pooled per utterance). A 3-layer MLP classifier (256→128→64) with batch norm, LeakyReLU, and dropout was trained using Adam optimizer, with SMOTE applied to address class imbalance (~80% win class). Data was split 70/20/10 with speaker separation.

## Key Results
- SSL representations achieved 65.9% (HuBERT) and 63.9% (Wav2Vec 2.0) accuracy versus 44.4% for traditional acoustic features
- Winners showed more dynamic pitch contours, higher intensity, and varied spectral features compared to losers' flatter intonation
- exHuBERT (emotion fine-tuned) performed worse (60.7%) than base HuBERT, suggesting emotion fine-tuning may degrade task-specific performance
- SHAP analysis revealed F0 stddevRisingSlope as the most important feature, with winners showing high and low values while losers had uniformly high values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-supervised speech representations capture discriminative prosodic-emotional patterns that handcrafted acoustic features miss
- **Evidence anchors:**
  - SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states
  - HuBERT achieves 65.9% accuracy vs openSMILE's 44.4%—a 21.5 percentage point gap
  - Related work on prosody in SSL confirms SSL captures prosodic structure beyond lexical content

### Mechanism 2
- **Claim:** Winners produce more dynamic pitch contours and higher intensity, while losers exhibit monotonous, flat intonation
- **Evidence anchors:**
  - Winners exhibit high and low values of F0 stddevRisingSlope, while athletes who lost the match tend to have uniformly high values, indicating more unstable pitch slopes
  - Higher sound level values are associated with win
  - Weak direct corpus support for win/lose-specific prosody; related work on emotion-prosody links provides indirect validation

### Mechanism 3
- **Claim:** Emotion-specific fine-tuning can degrade performance on related but distinct affective classification tasks
- **Evidence anchors:**
  - The additional emotional fine-tuning does not enhance performance; instead, it appears to degrade it
  - Authors hypothesize trade-off between emotional sensitivity and task-specific discriminability
  - No direct corpus evidence on emotion fine-tuning negative transfer; this remains an observation requiring replication

## Foundational Learning

- **Self-supervised speech representations (Wav2Vec 2.0, HuBERT)**
  - **Why needed here:** These models provide the 1024-dimensional embeddings that achieve state-of-the-art performance
  - **Quick check question:** Can you explain why mean pooling over the final transformer layer preserves emotionally-relevant information while discarding speaker identity?

- **Prosodic feature sets (eGeMAPS/openSMILE)**
  - **Why needed here:** The 88-dimensional baseline provides interpretability via SHAP analysis
  - **Quick check question:** What does F0 avRisingSlope capture, and why might it differ between high-arousal and low-arousal speech?

- **Class imbalance handling (SMOTE)**
  - **Why needed here:** The dataset is ~80% "win" class
  - **Quick check question:** Why does SMOTE interpolate between minority samples rather than simply duplicating them?

## Architecture Onboarding

- **Component map:** Video (iMiGUE) → Audio extraction → Diarization/VAD → Transcription → Forced alignment → Speaker identification → Feature extraction → Mean pooling → Classifier

- **Critical path:** The speaker identification step is fragile—if athlete duration is miscalculated (e.g., verbose journalist), all downstream features are corrupted

- **Design tradeoffs:**
  - SSL vs. handcrafted: SSL offers +21% accuracy but loses interpretability; SHAP analysis requires openSMILE features
  - Layer selection: Final transformer layer used; earlier layers may encode different prosodic information
  - Pooling strategy: Mean pooling assumes temporal averaging preserves emotional content

- **Failure signatures:**
  - Accuracy near 80% → SMOTE not applied or classifier ignoring minority class
  - openSMILE accuracy near chance (44%) → expected, but if SSL also near chance, check audio extraction/diarization pipeline
  - Large train-val accuracy gap → overfitting; increase dropout or reduce model capacity

- **First 3 experiments:**
  1. Reproduce baseline: Train classifier on openSMILE features without SMOTE, then with SMOTE
  2. SSL layer ablation: Extract HuBERT embeddings from layers {1, 6, 12, 24} and compare classification accuracy
  3. Pitch-only baseline: Train classifier using only F0-related eGeMAPS features to isolate pitch contribution

## Open Questions the Paper Calls Out

- Can domain-specific fine-tuning strategies or multi-task learning approaches improve the ability of SSL models to balance general emotional awareness with win/lose discrimination?
- Are the identified prosodic differences between winners and losers universal across different sports or cultural contexts?
- Does the integration of micro-gesture analysis with acoustic-prosodic features significantly improve the classification accuracy of match outcomes?

## Limitations

- The findings are limited to post-match tennis interviews and may not generalize to other sports, cultures, or competitive contexts
- The moderate accuracy ceiling (65.9%) indicates substantial room for improvement and suggests important discriminative signals remain unextracted
- The study cannot definitively establish causation between emotional states and prosodic patterns or rule out confounding factors

## Confidence

- **High Confidence:** The superiority of SSL representations over handcrafted acoustic features for this classification task
- **Medium Confidence:** The mechanism by which SSL models capture emotional-prosodic patterns and the interpretation of F0 stddevRisingSlope differences
- **Low Confidence:** The claim that winners produce more dynamic pitch contours due to emotional arousal versus strategic presentation

## Next Checks

1. Apply trained models to post-match interviews from different sports (soccer, basketball) to assess domain generalization
2. Systematically remove or modify components of the SSL pipeline - test different pooling strategies and extract embeddings from intermediate transformer layers
3. Conduct a laboratory study where athletes perform post-competition interviews under controlled conditions with verified emotional states via physiological measures