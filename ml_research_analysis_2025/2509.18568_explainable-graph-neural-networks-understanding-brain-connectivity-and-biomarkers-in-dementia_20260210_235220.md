---
ver: rpa2
title: 'Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers
  in Dementia'
arxiv_id: '2509.18568'
source_url: https://arxiv.org/abs/2509.18568
tags:
- brain
- dementia
- disease
- both
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive review of explainable
  graph neural networks (XGNNs) in dementia research, addressing the challenge of
  interpreting complex brain connectivity models for clinical adoption. It introduces
  a taxonomy of XAI methods tailored to dementia, covering scope (global/local), method
  (intrinsic/extrinsic), domain applicability, and mode of explanation.
---

# Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia

## Quick Facts
- arXiv ID: 2509.18568
- Source URL: https://arxiv.org/abs/2509.18568
- Authors: Niharika Tewari; Nguyen Linh Dan Le; Mujie Liu; Jing Ren; Ziqi Xu; Tabinda Sarwar; Veeky Baths; Feng Xia
- Reference count: 40
- Primary result: First comprehensive review of explainable graph neural networks (XGNNs) in dementia research, introducing taxonomy and highlighting applications across Alzheimer's, Parkinson's, and mild cognitive impairment with up to 96% classification accuracy

## Executive Summary
This paper presents the first comprehensive review of explainable graph neural networks (XGNNs) in dementia research, addressing the critical challenge of interpreting complex brain connectivity models for clinical adoption. The review introduces a taxonomy of XAI methods tailored to dementia, covering scope (global/local), method (intrinsic/extrinsic), domain applicability, and mode of explanation. Applications span Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and multi-disease diagnosis, with methods including Grad-CAM, SHAP, GNNExplainer, and attention-based approaches. Performance metrics reported include classification accuracies of up to 96% for AD, 95% for early PD, and 94% for MCI conversion prediction. The review highlights underexplored areas such as non-amnestic MCI and frontotemporal dementia, and identifies future directions including causal reasoning, multimodal integration, and integration with large language models for early detection.

## Method Summary
The review synthesizes existing literature on XGNNs in dementia by systematically categorizing explainable methods according to scope (global vs. local), methodology (intrinsic vs. extrinsic), domain applicability, and mode of explanation. It examines applications across multiple dementia types and XAI techniques including Grad-CAM, SHAP, GNNExplainer, and attention mechanisms. The analysis covers performance metrics from various studies, identifies research gaps in understudied dementia subtypes, and proposes future directions for the field. The review is narrative in nature rather than systematic, potentially introducing selection bias.

## Key Results
- XGNNs can achieve up to 96% accuracy for Alzheimer's disease classification, 95% for early Parkinson's disease detection, and 94% for mild cognitive impairment conversion prediction
- The review establishes a taxonomy of XAI methods specifically tailored to dementia research, categorizing approaches by scope, method, domain, and explanation mode
- Major research gaps identified include non-amnestic MCI, frontotemporal dementia, and vascular dementia, with future directions focusing on causal reasoning, multimodal integration, and LLM integration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If brain connectivity is modeled as a graph of Regions of Interest (ROIs), Graph Neural Networks (GNNs) can capture topological disruptions in dementia patients more effectively than grid-based CNNs.
- **Mechanism:** GNNs utilize message passing across nodes (ROIs) and edges (connectivity strength) to learn representations of inter-regional relationships. This allows the model to identify structural and functional network disruptions—such as those in the Default Mode Network (DMN)—by propagating information through the graph topology rather than analyzing spatially isolated voxels.
- **Core assumption:** The neurological pathology of dementia is fundamentally a disorder of network connectivity, not just localized tissue damage, and this pathology is reflected in the weighted adjacency matrix $A$.
- **Evidence anchors:**
  - [abstract]: Mentions "identifying disease-relevant biomarkers, analysis of brain network disruptions."
  - [section 1]: Explicitly contrasts GNNs with CNNs, stating GNNs "enable a more precise characterization of brain connectivity" by modeling inter-regional relationships.
  - [corpus]: "Edge-boosted graph learning for functional brain connectivity analysis" emphasizes that node-based matrix inference captures functional connectivity critical for neurodegenerative disease prediction.
- **Break condition:** If the brain parcellation atlas (e.g., AAL, Destrieux) fails to align with the biological ground truth of the disease, the resulting graph structure $G$ will be noisy, degrading GNN performance to below CNN baselines.

### Mechanism 2
- **Claim:** If XAI methods (e.g., GNNExplainer, Grad-CAM) are applied post-hoc or intrinsically, they can map high-dimensional model decisions to specific neuroanatomical biomarkers, facilitating clinical trust.
- **Mechanism:** Explanation methods assign importance scores $S_V$ (nodes) or $S_E$ (edges). By highlighting high-scoring regions (e.g., hippocampus, entorhinal cortex) that correlate with known clinical hallmarks of Alzheimer's, the mechanism bridges the "black box" gap. This validation loop ensures the model is learning pathological features rather than confounding artifacts.
- **Core assumption:** High importance scores in the explanation module correlate causally with biological disease mechanisms, rather than merely reflecting spurious correlations in the training cohort.
- **Evidence anchors:**
  - [abstract]: States XGNNs address "lack of interpretability" and provide "transparent insights for clinicians."
  - [section 4.1.2]: Describes how GNNExplainer identifies "explanatory subgraphs and node features driving model outputs."
  - [corpus]: "Explainable Graph-theoretical Machine Learning" discusses mapping metabolic connectivity impairments to clinical variables.
- **Break condition:** If the XAI method produces explanations that contradict established clinical knowledge (e.g., identifying the visual cortex as the primary driver of early-stage AD without a specific clinical reason), the explanation fidelity is compromised.

### Mechanism 3
- **Claim:** If multimodal data (structural, functional, genetic) is fused into a unified graph structure, the model mitigates patient heterogeneity and improves generalizability across dementia subtypes.
- **Mechanism:** Different modalities capture distinct disease aspects (e.g., sMRI captures atrophy, fMRI captures functional disruption). By constructing population-level or subject-level graphs that integrate these features (e.g., using attention mechanisms to weight modalities), the model creates a richer, more robust feature space that disentangles overlapping symptoms in conditions like AD and FTD.
- **Core assumption:** The information gain from combining modalities outweighs the noise introduced by the increased dimensionality and alignment complexity.
- **Evidence anchors:**
  - [abstract]: Highlights "multimodal integration" as a key future direction for generalizability.
  - [section 8.4]: Notes "imaging–genetic–clinical fusion" and "SC–FC dual GCN" demonstrate that cross-domain integration enhances model robustness.
  - [corpus]: "Multimodal Graph Neural Networks for Prognostic Modeling" supports integrating DTI and fMRI for better prognostic modeling.
- **Break condition:** If modalities are misaligned temporally or spatially, or if missing data handling is poor, the fusion layer may learn to rely on the dominant modality, ignoring sparse but critical signals from others.

## Foundational Learning

- **Concept: Brain Parcellation (Atlasing)**
  - **Why needed here:** This defines the graph topology. Without defining nodes $V$ via atlases (e.g., AAL, Desikan–Killiany), you cannot construct the adjacency matrix $A$ or node features $X$ required for the GNN.
  - **Quick check question:** Can you explain the trade-off between using a high-resolution atlas (e.g., Destrieux) versus a functional atlas (e.g., Schaefer) when analyzing early-stage network disruptions?

- **Concept: Message Passing**
  - **Why needed here:** This is the core engine of GNNs. Understanding how node embeddings $\mathbf{h}^{(l)}$ are updated by aggregating neighbor information (Eq. 4 in section 4.2) is essential to interpreting how "network disruptions" propagate through the model.
  - **Quick check question:** In a 2-layer GCN, how does the receptive field of a node (ROI) change, and why does this matter for detecting remote connectivity disruptions in dementia?

- **Concept: Explanation Fidelity vs. Plausibility**
  - **Why needed here:** A major challenge in XGNN is that an explanation can look "right" to a human (plausibility) but not actually represent the model's reasoning (fidelity). Understanding this distinction is critical for the Architecture Onboarding.
  - **Quick check question:** If an explanation heatmap highlights the hippocampus (a known biomarker), what test would you run to ensure the model is actually using those features, rather than just guessing based on a confounding variable?

## Architecture Onboarding

- **Component map:** Data Pipeline (Preprocessing neuroimaging -> Parcellation -> Graph Construction) -> GNN Encoder (2-3 layers of GCN or GAT) -> Readout/Pooling (Aggregating node embeddings) -> XAI Module (Post-hoc or Intrinsic explanation)
- **Critical path:** The definition of the **Adjacency Matrix $A$** (Section 2.3). If $A$ is constructed using a noisy correlation metric (e.g., raw Pearson without thresholding) or fails to capture the specific modality strength, the GNN will propagate noise, leading to poor convergence and uninterpretable explanations.
- **Design tradeoffs:**
  - **Subject-level vs. Population-level Graphs:** Subject-level graphs allow individualized diagnosis but suffer from small sample sizes ($N$ samples, 1 graph per sample). Population-level graphs (nodes=subjects) increase graph size for GNN processing but require careful edge definition based on phenotypic similarity.
  - **Intrinsic vs. Post-hoc Explainability:** Intrinsic (e.g., Attention) is faster and consistent but may constrain model capacity. Post-hoc (e.g., GNNExplainer) is flexible but computationally expensive and may lack fidelity on large graphs (Section 10.4).
- **Failure signatures:**
  - **Overfitting to Cohorts:** High accuracy on ADNI but near-random on OASIS, indicating the model learned site-specific artifacts rather than biological signals (Section 10.3).
  - **Disconnected Explanations:** XAI highlights nodes that have no biological connection to dementia (e.g., primary visual cortex in non-visual variants), suggesting the explanation module is ungrounded (Section 10.5).
  - **Over-smoothing:** In deep GNNs, node embeddings become indistinguishable, reducing classification accuracy (Section 10.4 mentions scalability/performance issues).
- **First 3 experiments:**
  1. **Sanity Check with Synthetic Data:** Generate random graphs vs. structured ring graphs to ensure the GNN can distinguish topology. Check if GNNExplainer correctly identifies the "ring" structure as the predictive subgraph.
  2. **Biomarker Validation:** Train a GCN on the ADNI dataset (AD vs. CN). Apply Grad-CAM. Verify if the top-5 ROIs with highest importance scores include the Hippocampus and Entorhinal Cortex (Section 3.2 biomarkers).
  3. **Ablation Study:** Systematically remove edges from the adjacency matrix $A$ (e.g., disconnect the DMN) and measure the drop in classification accuracy to quantify the network's reliance on specific connectivity pathways.

## Open Questions the Paper Calls Out
- How can XGNNs be effectively applied to understudied dementia subtypes such as non-amnestic MCI, frontotemporal dementia, and vascular dementia?
- What are the optimal strategies for multimodal data integration in XGNNs to maximize clinical utility while minimizing computational complexity?
- How can causal reasoning be incorporated into XGNN frameworks to move beyond correlational explanations to mechanistic understanding?
- What are the most effective approaches for integrating large language models with XGNNs to enhance early detection and clinical decision support?

## Limitations
- The review is narrative rather than systematic, potentially introducing selection bias in reported XGNN applications and performance metrics
- Lacks critical evaluation of explanation fidelity versus human plausibility, a fundamental concern in explainable AI
- Performance metrics cited are from heterogeneous studies with varying protocols, making direct comparison difficult
- Does not address computational costs of XAI methods, which may be prohibitive for clinical deployment

## Confidence

**High Confidence:**
- Taxonomy of XAI methods (scope, method, domain, mode) and their basic applications to dementia are well-established in literature

**Medium Confidence:**
- Mechanisms linking GNN topology to dementia pathology and clinical utility of XAI explanations, as these rely on assumptions about biological validity of learned features

**Low Confidence:**
- Performance metrics and generalizability claims, given lack of standardized evaluation protocols across studies

## Next Checks
1. **Cross-cohort Validation:** Replicate the highest-performing XGNN model on an independent dataset (e.g., OASIS) to test for overfitting to the ADNI cohort
2. **Explanation Fidelity Testing:** Conduct controlled experiment where known confounds (e.g., age, sex) are systematically added to data, then verify if XAI method correctly identifies them as non-biomarkers
3. **Computational Cost Analysis:** Benchmark runtime and memory requirements of different XAI methods (GNNExplainer vs. Attention vs. Grad-CAM) on standardized graph size to assess clinical feasibility