---
ver: rpa2
title: 'EpiPlanAgent: Agentic Automated Epidemic Response Planning'
arxiv_id: '2512.10313'
source_url: https://arxiv.org/abs/2512.10313
tags:
- response
- epidemic
- epiplanagent
- planning
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EpiPlanAgent is an agent-based system that uses large language
  models and structured knowledge bases to automate the generation of epidemic response
  plans. It decomposes tasks, grounds decisions in domain knowledge, and iteratively
  refines plans based on feedback.
---

# EpiPlanAgent: Agentic Automated Epidemic Response Planning

## Quick Facts
- **arXiv ID:** 2512.10313
- **Source URL:** https://arxiv.org/abs/2512.10313
- **Reference count:** 28
- **Primary result:** Agent-based system improves epidemic response plan completeness to 82.4% (from 68.7%) and reduces generation time by 93.9% (p < 0.001)

## Executive Summary
EpiPlanAgent is an AI-driven system that automates the generation of epidemic response plans using large language models, structured knowledge bases, and iterative refinement. The system decomposes the planning workflow into specialized nodes, grounds decisions in authoritative domain knowledge, and iteratively refines outputs based on feedback. In a controlled evaluation with public health professionals across 16 outbreak scenarios, EpiPlanAgent significantly improved plan completeness, reduced generation time, and produced plans that closely aligned with expert judgment.

## Method Summary
The system uses a directed acyclic graph (DAG) architecture called SigmaFlow to orchestrate the planning process. It begins by extracting epidemic type from unstructured reports, retrieves structured response actions from a JSON knowledge base using exact-match RAG, generates a structured task list, and iteratively refines it based on feedback. The knowledge base contains 8 disease-specific JSON files with standardized response actions, and the system was evaluated against plans created by public health professionals using synthetic outbreak scenarios.

## Key Results
- Plan completeness improved from 68.7% to 82.4% (p < 0.001) compared to human experts
- Generation time reduced from 24.5 minutes to 1.5 minutes (93.9% reduction)
- Strong alignment with expert judgment (r = 0.92) and high user satisfaction ratings
- Standardized quality across experience levels of public health professionals

## Why This Works (Mechanism)

### Mechanism 1: Structured RAG Improves Guideline Alignment
The system uses exact-match retrieval of JSON-formatted response actions rather than semantic vector search, ensuring plans are grounded in verified, authoritative guidelines. This approach prevents "semantic drift" that could occur with vector search and forces strict compliance with established protocols. The core assumption is that epidemic type extraction is accurate; if the wrong disease is identified, the wrong knowledge base is retrieved, propagating error.

### Mechanism 2: Agentic Orchestration Enhances Reasoning Reliability
The SigmaFlow DAG architecture decomposes planning into specialized nodes (Model, Tool, Logic) that enforce explicit dependencies and data flow. This separation prevents the LLM from hallucinating facts by forcing explicit variable extraction before task generation. The sequential execution (identification → analysis → generation) reflects the actual dependencies required for valid plans. Break conditions include circular dependencies or prompt failures that break graph execution.

### Mechanism 3: Iterative Refinement Improves Completeness
The system supports multi-turn refinement loops where generated task lists are fed back with feedback, allowing the LLM to correct omissions and adjust priorities based on dynamic constraints. This approach improves completeness from 78.0% in the first round to 82.4% after refinement. The core assumption is that the LLM can interpret feedback and modify JSON structures correctly without introducing regression errors. Break conditions include unstructured, contradictory, or irrelevant feedback causing over-correction.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) in Agentic Workflows**
  - Why needed here: SigmaFlow relies on modeling the planning process as a DAG to enforce dependencies and data flow between nodes
  - Quick check question: If Node A requires the output of Node B, and Node B requires the output of Node A, what happens in a DAG-based system?

- **Concept: Structured Output Generation (JSON Mode)**
  - Why needed here: The system enforces a strict JSON schema for the final plan, requiring precise prompting to ensure valid output
  - Quick check question: Why is exact-match retrieval on a JSON field more reliable than semantic vector search for high-stakes compliance data?

- **Concept: Public Health Trigger Logic**
  - Why needed here: The "Case Structuring" step maps unstructured reports to boolean condition points (e.g., "Confirmed Case: Yes")
  - Quick check question: How does the system handle a report where the case status is "Probable" rather than "Confirmed"?

## Architecture Onboarding

- **Component map:** User Interface -> SigmaFlow Orchestrator -> Model Nodes (DeepSeek-V3) -> Tool Nodes (Code/RAG) -> Knowledge Base
- **Critical path:** Epidemic Type Extraction -> RAG Retrieval. If the Model Node misidentifies the disease, the RAG node retrieves the wrong JSON file, invalidating the entire downstream plan.
- **Design tradeoffs:** Static JSON knowledge base prioritizes high precision over adaptability; exact-match retrieval trades flexibility for strict compliance to prevent semantic drift.
- **Failure signatures:** JSON syntax errors from LLM output, hallucinated triggers not supported by input text, or feedback loop runaway causing plan bloat.
- **First 3 experiments:**
  1. Type Extraction Robustness: Test accuracy on 20 ambiguous or mixed-symptom reports
  2. RAG vs. Parametric Ablation: Compare completeness scores with RAG node disabled
  3. Feedback Sensitivity: Evaluate stability under deliberately contradictory or irrelevant feedback

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on synthetic reports rather than real-world outbreak notifications
- Limited to 8 diseases, raising questions about generalizability to novel pathogens
- Static JSON knowledge base requires manual updates, creating potential lag with evolving guidelines
- Does not address data privacy implications for sensitive health reports

## Confidence
- **High Confidence:** Quantitative improvements in completeness (82.4% vs 68.7%, p < 0.001) and generation speed (93.9% reduction) with strong expert alignment (r = 0.92)
- **Medium Confidence:** User satisfaction ratings based on single deployment context with potentially primed participants; 12.8% gap remains between automated and expert completeness
- **Low Confidence:** System behavior with novel pathogens and severely incomplete/unstructured reports remains untested; long-term refinement reliability under noisy feedback unverified

## Next Checks
1. Deploy EpiPlanAgent with public health agencies to process actual outbreak notifications from official channels
2. Systematically test response to simulated reports of emerging diseases not in the current knowledge base
3. Evaluate system stability and output quality under deliberately contradictory or irrelevant feedback across multiple refinement cycles