---
ver: rpa2
title: 'FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling
  Fair LLM-Based Recommender Systems'
arxiv_id: '2502.02966'
source_url: https://arxiv.org/abs/2502.02966
tags:
- fairness
- acter
- violations
- prompt
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce FACTER, a framework that combines conformal
  prediction with iterative prompt engineering to detect and mitigate fairness violations
  in LLM-based recommender systems. FACTER adapts fairness thresholds based on semantic
  variance and employs a violation-triggered mechanism to update prompts with specific
  bias examples, avoiding reliance on protected attributes.
---

# FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems

## Quick Facts
- arXiv ID: 2502.02966
- Source URL: https://arxiv.org/abs/2502.02966
- Reference count: 31
- Primary result: Reduces fairness violations by up to 95.5% while maintaining recommendation accuracy (NDCG@10: 0.44–0.45)

## Executive Summary
FACTER introduces a framework that combines conformal prediction with iterative prompt engineering to detect and mitigate fairness violations in LLM-based recommender systems without retraining. The method uses statistical thresholds to bound fairness violation rates and employs a violation-triggered mechanism to update prompts with specific bias examples. Experiments on MovieLens and Amazon datasets demonstrate significant fairness improvements while maintaining recommendation quality across multiple LLMs.

## Method Summary
FACTER operates in two phases: offline calibration computes fairness-aware non-conformity scores and establishes initial thresholds, while online monitoring detects violations and iteratively updates prompts with explicit bias patterns. The framework uses conformal prediction to set statistical thresholds and exponential decay to adapt these thresholds when violations persist. When a violation occurs, the system injects concrete negative examples into the prompt to prevent similar biases, storing recent violations in a FIFO buffer for pattern extraction.

## Key Results
- Reduces fairness violations by up to 95.5% on MovieLens and Amazon datasets
- Maintains recommendation accuracy with NDCG@10 scores between 0.44–0.45
- Demonstrates consistent improvement across multiple LLMs (Llama-3-8B, Llama-2-7B, Mistral-7B)
- Outperforms baselines like UP5 and Zero-Shot, particularly on sparse data

## Why This Works (Mechanism)

### Mechanism 1: Conformal Fairness Thresholding
Statistical quantile-based thresholds provide principled bounds on fairness violation rates without requiring model parameter access. The framework computes fairness-aware non-conformity scores using prediction error and cross-group semantic disparities, with coverage guarantees under exchangeability assumptions.

### Mechanism 2: Violation-Triggered Prompt Injection
Injecting concrete bias patterns into system prompts reduces recurrence of similar violations by providing the LLM with explicit negative examples to avoid. The mechanism stores recent violations in a FIFO buffer and updates prompts with specific attribute-to-bias mappings.

### Mechanism 3: Exponential Threshold Decay
Adaptive threshold tightening maintains approximate coverage while progressively reducing acceptable disparity bounds. When violations persist, the threshold shrinks geometrically, ensuring the system self-corrects toward stricter fairness bounds.

## Foundational Learning

- **Conformal Prediction**: Provides statistical foundation for setting violation thresholds with coverage guarantees; Quick check: Given 100 calibration samples and α=0.1, what minimum number must fall below threshold for valid coverage?
- **Semantic Embedding Similarity**: Framework uses cosine similarity to measure cross-group disparities; Quick check: How does embedding noise (e.g., distance 0.15) affect violation detection?
- **Individual vs. Group Fairness**: FACTER combines local and group-level fairness notions; Quick check: Would satisfying individual fairness automatically satisfy group fairness?

## Architecture Onboarding

- **Component map**: Offline Calibration Pipeline -> Online Monitoring Loop -> FIFO Buffer V -> Embedding Module
- **Critical path**: Calibration data quality → threshold validity → embedding bias propagation → token budget management
- **Design tradeoffs**: λ (fairness penalty) affects violation count vs. accuracy; γ (decay rate) balances adaptation speed vs. over-correction; τρ (neighborhood similarity) controls subgroup detection precision
- **Failure signatures**: Violation plateau indicates prompt injection ineffective; sharp accuracy drop suggests λ too aggressive; oscillating threshold points to calibration drift
- **First 3 experiments**: 1) Validate coverage guarantees across calibration set sizes; 2) Sweep λ to identify Pareto frontier; 3) Compare prompt strategies (generic warnings vs. negative examples vs. explicit patterns)

## Open Questions the Paper Calls Out

### Open Question 1
How robust is FACTER's fairness calibration when the embedding model itself encodes societal biases? The framework assumes the embedder is relatively bias-free, but doesn't validate performance when underlying embeddings are skewed. Evidence: Experiments injecting known biases into embedding models to measure detection rate changes.

### Open Question 2
Can FACTER maintain performance as violation examples exceed the model's context window? The study fixes a FIFO buffer size (M=50), but scalability to thousands of patterns over long-term deployment is unclear. Evidence: Long-term simulations analyzing buffer size vs. fairness violation reduction tradeoffs.

### Open Question 3
Can computational complexity of offline calibration be reduced below O(n²) for massive datasets? Current implementation constructs pairwise similarity matrices, becoming prohibitive for large user bases. Evidence: Algorithmic modifications using approximate nearest neighbor search with time-complexity analysis.

## Limitations
- Calibration set exchangeability assumption may not hold with distribution drift
- Embedding bias propagation creates second-order risk if SentenceTransformer encodes protected attributes
- Token budget constraints limit number of injectable violation examples in high-violation scenarios

## Confidence
**High Confidence**: Statistical foundation of conformal thresholding is well-established with finite-sample coverage guarantees; empirical results are internally consistent.
**Medium Confidence**: Prompt injection mechanism shows strong empirical performance but generalization from examples remains theoretically underspecified.
**Low Confidence**: Scalability to larger datasets and different domains is not demonstrated; privacy implications of using protected attributes during calibration are unaddressed.

## Next Checks
1. **Distribution Drift Validation**: Implement rolling-window evaluation to measure how violation rates and threshold stability change over time with sequential query batches.
2. **Embedder Fairness Audit**: Systematically test SentenceTransformer for protected attribute leakage by clustering embeddings by demographics with controlled semantic content.
3. **Token Budget Stress Test**: Simulate high-violation scenarios by increasing λ or using more biased calibration data to measure when FIFO buffer saturation occurs.