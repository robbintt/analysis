---
ver: rpa2
title: Think Only When You Need with Large Hybrid-Reasoning Models
arxiv_id: '2505.14631'
source_url: https://arxiv.org/abs/2505.14631
tags:
- reasoning
- thinking
- hybrid
- arxiv
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large reasoning models often overuse extended thinking steps on
  simple queries, leading to inefficiency and degraded user experience. This work
  introduces Large Hybrid-Reasoning Models (LHRMs), which adaptively decide whether
  to perform extended thinking based on the complexity of the input query.
---

# Think Only When You Need with Large Hybrid-Reasoning Models

## Quick Facts
- arXiv ID: 2505.14631
- Source URL: https://arxiv.org/abs/2505.14631
- Reference count: 40
- Primary result: LHRMs adaptively decide when to perform extended reasoning, improving efficiency while maintaining performance across diverse domains.

## Executive Summary
Large reasoning models often default to lengthy reasoning chains even for simple queries, resulting in inefficiency and poor user experience. This work introduces Large Hybrid-Reasoning Models (LHRMs) that adaptively determine when to engage in extended thinking based on query complexity. The approach combines a two-stage training pipeline with adaptive reasoning strategies to achieve better performance across reasoning and general tasks while significantly improving efficiency. LHRMs demonstrate strong generalization by adjusting reasoning effort according to task difficulty, outperforming existing models in both reasoning and general capabilities.

## Method Summary
The proposed approach employs a two-stage training pipeline to develop hybrid reasoning capabilities. First, Hybrid Fine-Tuning trains the model to handle both thinking and non-thinking tasks simultaneously, preventing mode collapse where the model would only use one mode. Second, Hybrid Group Policy Optimization is applied to learn when to engage in thinking while simultaneously improving response quality. This training framework enables the model to make adaptive decisions about reasoning effort based on the complexity of each input query, achieving better performance across diverse domains while maintaining efficiency.

## Key Results
- LHRMs outperform existing models in both reasoning and general capabilities
- Significant efficiency improvements through adaptive reasoning decision-making
- Strong generalization across diverse domains by adjusting reasoning effort based on task difficulty
- Better performance on both simple and complex queries compared to models that always use extended reasoning

## Why This Works (Mechanism)
The mechanism works by training the model to dynamically assess query complexity and allocate reasoning resources accordingly. When faced with simple queries, the model can provide direct answers without lengthy reasoning chains, saving computational resources and improving response time. For complex queries requiring deeper analysis, the model engages in extended thinking. This adaptive approach prevents the inefficiency of always using extended reasoning while ensuring that complex problems still receive appropriate analytical depth. The two-stage training pipeline ensures the model can seamlessly switch between modes without mode collapse, maintaining high performance across all query types.

## Foundational Learning
- **Mode Collapse**: When a model trained on multiple tasks only learns to perform one task effectively, ignoring others. This is prevented through hybrid fine-tuning techniques.
- **Policy Optimization**: A reinforcement learning approach where the model learns optimal decision-making strategies, in this case learning when to engage in thinking versus direct answering.
- **Adaptive Reasoning**: The ability to dynamically adjust the depth of reasoning based on task complexity, improving efficiency without sacrificing performance on difficult tasks.
- **Reasoning Chains**: Sequential logical steps used to solve complex problems, which can be computationally expensive when used unnecessarily for simple queries.
- **General Capabilities**: The model's ability to perform well across diverse tasks beyond just reasoning, including knowledge retrieval and language understanding.
- **Query Complexity Assessment**: The model's ability to evaluate the difficulty of a given query to determine appropriate reasoning depth.

## Architecture Onboarding
- **Component Map**: Input Query -> Complexity Assessor -> Decision Module -> (Direct Answer Path / Extended Thinking Path) -> Response
- **Critical Path**: Query → Complexity Assessment → Decision → Response Generation (either direct or with reasoning)
- **Design Tradeoffs**: The model trades potential accuracy gains from always using extended reasoning against significant efficiency improvements from adaptive reasoning. The two-stage training balances learning both thinking and non-thinking capabilities while avoiding mode collapse.
- **Failure Signatures**: Over-conservative reasoning (always thinking for simple queries), under-thinking (failing to engage reasoning for complex queries), mode collapse (only using one response mode), or poor complexity assessment leading to wrong reasoning depth.
- **First Experiments**: 1) Test on mixed difficulty benchmark to verify adaptive reasoning, 2) Compare inference time and token usage against baseline models, 3) Ablation study on training stages to measure contribution of each component.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach requires careful tuning of the two-stage training pipeline to prevent mode collapse
- Complexity assessment may not always accurately predict when extended reasoning is truly necessary
- The model may struggle with queries that fall in the gray area between simple and complex
- Performance gains may vary depending on the distribution of query difficulties in real-world applications

## Confidence
- High: Two-stage training approach effectively prevents mode collapse while learning hybrid capabilities
- Medium: Adaptive reasoning decisions consistently improve efficiency across diverse domains
- Medium: Strong generalization across different task types and complexity levels

## Next Checks
- Verify adaptive reasoning decisions on a mixed difficulty benchmark with quantitative metrics for efficiency gains
- Measure inference time and token usage differences between LHRMs and baseline models on real-world query distributions
- Conduct ablation studies removing either training stage to quantify their individual contributions to overall performance