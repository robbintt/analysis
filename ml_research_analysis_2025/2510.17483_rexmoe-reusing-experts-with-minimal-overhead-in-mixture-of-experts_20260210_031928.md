---
ver: rpa2
title: 'ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts'
arxiv_id: '2510.17483'
source_url: https://arxiv.org/abs/2510.17483
tags:
- expert
- experts
- rexmoe
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces REXMOE, a novel approach to mixture-of-experts
  (MoE) models that overcomes the limitation of layer-local routing by allowing routers
  to reuse experts across adjacent layers. This design decouples expert dimensionality
  from per-layer budgets, enabling richer expert combinations without sacrificing
  individual expert capacity or increasing overall parameters.
---

# ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts

## Quick Facts
- **arXiv ID**: 2510.17483
- **Source URL**: https://arxiv.org/abs/2510.17483
- **Reference count**: 40
- **Primary result**: ReXMoE improves perplexity and downstream task accuracy over vanilla MoE baselines across 0.5B-7B parameter models

## Executive Summary
ReXMoE introduces a novel approach to mixture-of-experts (MoE) models that overcomes the limitation of layer-local routing by allowing routers to reuse experts across adjacent layers. This design decouples expert dimensionality from per-layer budgets, enabling richer expert combinations without sacrificing individual expert capacity or increasing overall parameters. The authors also propose a progressive scaling routing (PSR) strategy that gradually expands the candidate expert pool during training, improving training stability and performance. Extensive experiments on models ranging from 0.5B to 7B parameters across different architectures demonstrate that ReXMoE consistently improves both language modeling perplexity and downstream task accuracy compared to vanilla MoE baselines.

## Method Summary
The core innovation of ReXMoE is enabling expert reuse across multiple layers rather than restricting each layer to its own set of experts. This architectural change allows for more diverse and powerful expert combinations while maintaining the same per-layer capacity constraints. The progressive scaling routing strategy gradually increases the number of available experts during training, starting with a smaller subset and expanding the pool as training progresses. This approach helps stabilize training and allows the model to learn effective routing patterns incrementally rather than being overwhelmed by a large expert pool from the start.

## Key Results
- Consistently improves language modeling perplexity over vanilla MoE baselines across model sizes (0.5B-7B parameters)
- R4 configuration often delivers the highest average accuracy in downstream task evaluations
- Demonstrates scalability benefits as model size and training data increase
- Shows stronger task-specific specialization enabling more effective expert allocation

## Why This Works (Mechanism)
The key mechanism behind ReXMoE's effectiveness is the decoupling of expert dimensionality from per-layer routing constraints. By allowing expert reuse across layers, the model can maintain larger individual experts while still fitting within the same computational budget. This enables more specialized and capable experts to emerge during training. The progressive scaling routing further enhances this by allowing the model to gradually learn complex routing patterns without being overwhelmed by too many choices early in training.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: A model architecture where multiple specialized "expert" networks are combined, with a gating network determining which experts process each input. Needed to understand the baseline being improved upon. Quick check: Verify that the gating network probability distribution sums to 1 for each input.
- **Expert Reuse**: The concept of allowing experts to be shared across multiple layers rather than being confined to single layers. Needed to understand the core architectural innovation. Quick check: Confirm that reused experts maintain consistent parameters across layers.
- **Progressive Scaling**: A training strategy where model capacity or complexity is gradually increased during training. Needed to understand the PSR component. Quick check: Track how the number of active experts changes over training steps.

## Architecture Onboarding

**Component Map**: Input -> Router -> Expert Pool -> Output (with experts reused across multiple router layers)

**Critical Path**: Input token → Router selection → Expert computation → Output combination, with routing decisions made at each layer but experts shared across layers

**Design Tradeoffs**: The main tradeoff is between routing flexibility and training stability. Allowing more expert reuse increases representational power but can make training harder. The PSR strategy mitigates this by gradually introducing complexity.

**Failure Signatures**: Poor performance could stem from: (1) routing collapse where only a few experts are consistently selected, (2) training instability from too rapid expert pool expansion, or (3) insufficient specialization if experts are reused too frequently.

**First Experiments**:
1. Ablation study comparing fixed vs progressive scaling of expert pool sizes
2. Analysis of expert utilization patterns across layers with and without reuse
3. Perplexity comparison on validation set as a function of training steps for different PSR schedules

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though several areas for future investigation are implied by the experimental limitations.

## Limitations
- Experiments limited to 0.5B-7B parameter models, leaving uncertainty about scalability to much larger models
- Performance improvements tested primarily on language modeling tasks, with unclear generalizability to other domains
- The specific benefits of progressive scaling routing were not compared against alternative routing training schemes

## Confidence
- **High**: Claims about improved perplexity and downstream accuracy over vanilla MoE baselines on tested models and datasets
- **Medium**: Claims about task-specific specialization and stable gains across configurations
- **Low**: Claims about generalizability to much larger models or non-language domains

## Next Checks
1. Scale ReXMoE to 10B+ parameter models and evaluate if gains persist or diminish relative to parameter growth
2. Test ReXMoE on non-language domains (e.g., vision, multimodal) to assess cross-domain applicability
3. Compare PSR against alternative routing training schemes (e.g., expert-wise learning rates, load balancing losses) to isolate its contribution to performance