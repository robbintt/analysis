---
ver: rpa2
title: 'D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior
  of Large Language Models'
arxiv_id: '2601.17865'
source_url: https://arxiv.org/abs/2601.17865
tags:
- uni00000013
- uni00000011
- task
- token
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study analyzes the sampling behavior of large language models
  (LLMs) by examining the alignment between token-level probabilities (Ptoken) and
  task-level target distributions (Ptask). Through controlled experiments, two distinct
  model types are identified: D-models (e.g., Qwen-2.5) with highly concentrated and
  variable Ptoken distributions, and E-models (e.g., Mistral-Small) with more stable
  and task-aligned distributions.'
---

# D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models

## Quick Facts
- arXiv ID: 2601.17865
- Source URL: https://arxiv.org/abs/2601.17865
- Reference count: 40
- One-line primary result: Classification of LLMs into D-models (concentrated, e.g., Qwen-2.5) and E-models (task-aligned, e.g., Mistral-Small) reveals systematic trade-offs between diversity and stability affecting task performance

## Executive Summary
This study analyzes the sampling behavior of large language models by examining the alignment between token-level probabilities (Ptoken) and task-level target distributions (Ptask). Through controlled experiments, two distinct model types are identified: D-models (e.g., Qwen-2.5) with highly concentrated and variable Ptoken distributions, and E-models (e.g., Mistral-Small) with more stable and task-aligned distributions. Evaluations across code generation and recommendation tasks reveal systematic trade-offs between diversity and stability that affect task performance. Internal mechanism analysis shows that E-models better adapt to task requirements through layer-wise probability evolution, while D-models maintain extreme confidence. These findings provide practical guidance for selecting and configuring models in web-scale applications where balancing diversity and reliability under uncertainty is critical.

## Method Summary
The study employs simulated distribution tasks with explicit target distributions (Extreme and Flat), measuring alignment between Ptoken and Ptask using e-score and ATVD metrics. Models are classified as D or E based on their e-score behavior on flat tasks. Layer-wise probability evolution is analyzed through hidden state extraction, while task performance is evaluated on HumanEval (code) and MovieLens-1M (recommendation) datasets. Temperature sensitivity is assessed by varying decoding temperature from 0.5 to 2.0. The core experiments use temperature=1.0 across 10 independent runs per model-task configuration.

## Key Results
- E-models (Mistral-Small) exhibit lower e-scores and better Ptoken-Ptask alignment than D-models (Qwen-2.5)
- D-models maintain high confidence through layer-wise evolution while E-models progressively align with task distributions
- Temperature scaling effectively controls diversity in E-models but has limited impact on D-models' extreme confidence
- Task performance trade-offs: D-models excel in code generation pass@1 but struggle with recommendation diversity, while E-models show the opposite pattern

## Why This Works (Mechanism)

### Mechanism 1
D-models achieve task alignment through "deterministic global planning" rather than step-wise probability matching. The paper refutes quota compensation hypothesis, showing very low correlations (r < 0.1) between residual differences and probability changes, indicating absence of feedback loops. This suggests D-models pre-compute sequences that statistically approximate Ptask in aggregate.

### Mechanism 2
E-models achieve better task alignment through dynamic probability calibration across transformer layers, whereas D-models converge prematurely to extreme confidence. During generation, E-models progressively refine Ptoken to match Ptask, while D-models exhibit "sharp concentration" where dominant token probability approaches 1.0.

### Mechanism 3
The diversity-stability trade-off is controlled by model sensitivity to temperature, structurally limited in D-models. Temperature scaling effectively flattens distributions in E-models (lower e-scores), while D-models' extreme base logits prevent sufficient entropy introduction, locking them into deterministic sampling.

## Foundational Learning

**Total Variation Distance (TVD) & ATVD**
- Why needed here: Primary metric quantifying gap between Ptoken, Presult, and Ptask
- Quick check question: If Ptask is uniform over 4 items and Ptoken assigns 0.9 to one item, is ATVD high or low? (Answer: High)

**E-score (Extremity Score)**
- Why needed here: Metric classifying models - average maximum probability of predicted token
- Quick check question: Model with e-score of 0.99 outputs tokens with high certainty. Is this likely D-model or E-model? (Answer: D-model)

**Distribution Alignment vs. Sampling Fidelity**
- Why needed here: Models can produce correct Presult while having poor Ptoken alignment
- Quick check question: If model outputs "A, B, A, B" but assigns 99% probability to "A" every time, does it have high result alignment but low token alignment? (Answer: Yes)

## Architecture Onboarding

**Component map**: Prompt (Ptask) → Logits → Softmax → Ptoken → Sampling Head → Presult
**Critical path**: Logit extraction → E-score calculation → Model Classification (D vs. E) → Task Assignment (Code vs. Recommendation)
**Design tradeoffs**:
- D-models: Prefer deterministic solution iteration (code generation pass@k), refine details well but struggle with explicit selection constraints
- E-models: Prefer adherence to uncertainty and explicit constraints (recommendation), lack aggressive refinement drive in code tasks
**Failure signatures**:
- D-model in Recommendation: High hallucination rate ("Can-hit" drops), ignores local context for global planning
- E-model in Code: Lower Δpass (diversity), explores too broadly without locking onto specific implementation
**First 3 experiments**:
1. E-score Profiling: Run Simulated Distribution Task to classify model as D or E
2. Temperature Sensitivity Check: Plot e-score vs. temperature (0.5 to 2.0) to identify flat curves (D-model)
3. Layer-wise Inspection: Extract Ptoken from final 4 layers to verify "sharp concentration" timing

## Open Questions the Paper Calls Out

**Open Question 1**: What is the precise mechanism underlying D-models' "deterministic global planning" that enables Presult to match Ptask despite extreme Ptoken distributions? The paper empirically rules out quota compensation but doesn't identify the actual mechanism driving alignment.

**Open Question 2**: What architectural or training factors determine whether a model develops D-model versus E-model characteristics? The paper categorizes models but doesn't investigate causes of divergent behaviors during training.

**Open Question 3**: Can D-models' sampling fidelity be improved to reduce systematic concentration bias without sacrificing advantages in deterministic tasks? Temperature adjustment reduces but doesn't eliminate bias, and D-models resist temperature-based control.

**Open Question 4**: How do D-model versus E-model distinctions manifest in open-ended generation tasks where Ptask is not explicitly specified? The classification framework depends on explicit Ptask measurement, becoming ambiguous when Ptask is implicit or subjective.

## Limitations

- Unknown provenance of model behaviors - empirical classification without causal explanation of architectural differences
- Single temperature setting (1.0) in core analysis limits generalizability across deployment scenarios
- Potential measurement artifacts from layer-wise extraction methodology not fully specified

## Confidence

**High confidence**: Empirical classification of models into D and E types based on e-scores and ATVD metrics is well-supported by data
**Medium confidence**: Proposed mechanisms explaining behavioral differences are plausible but primarily correlational rather than causally proven
**Low confidence**: "Deterministic global planning" claim rests on weak statistical evidence (very low correlations could reflect measurement noise)

## Next Checks

1. **Intervention experiment on layer-wise probabilities**: Artificially modify intermediate layer distributions in D-models to match E-model patterns to validate layer-wise evolution mechanism
2. **Temperature scaling validation across ranges**: Systematically test core experiments (code generation, recommendation) across full temperature range (0.1 to 2.0) to verify D/E classification framework validity
3. **Cross-architecture ablation study**: Compare models with similar architectures but different training regimes to isolate whether D/E distinction is determined by base architecture, training data, or instruction-tuning methods