---
ver: rpa2
title: 'Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining'
arxiv_id: '2511.01807'
source_url: https://arxiv.org/abs/2511.01807
tags:
- length
- control
- prompting
- target
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses length control in large language models, a
  critical yet under-served challenge for real-world applications. Current approaches
  typically require expensive model retraining or complex tooling, limiting practical
  deployment.
---

# Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining

## Quick Facts
- **arXiv ID**: 2511.01807
- **Source URL**: https://arxiv.org/abs/2511.01807
- **Reference count**: 13
- **Primary result**: Prompt-only method achieving up to 37.6% better length adherence in document summarization without retraining

## Executive Summary
Plan-and-Write addresses the challenge of precise length control in large language models, a critical requirement for production applications that current methods handle poorly. Existing approaches typically require expensive model retraining or complex tooling, creating barriers for practical deployment. The authors introduce a prompt engineering method that guides LLMs through explicit planning and word counting to achieve precise length control without retraining. Evaluated across six state-of-the-art models on document summarization tasks, the approach significantly improved length fidelity for most models while maintaining or enhancing output quality.

## Method Summary
Plan-and-Write is a prompt engineering method that decomposes length control into two phases: explicit word counting during content generation, followed by a rewrite phase that maintains the exact word count while improving coherence. The approach uses structured prompts with designated tags (`<thinking>` for the counting phase and `<final_answer>` for the formatted output) to guide the model through this two-step process. Rather than modifying model parameters, it leverages existing model capabilities for counting and instruction-following by providing explicit structural guidance.

## Key Results
- Achieved up to 37.6% better length adherence compared to standard prompting across six state-of-the-art models
- Maintained or improved output quality while improving length fidelity
- Most effective for short-to-medium targets (20-500 words), with performance degrading for longer targets
- Showed model-dependent effectiveness, with Claude models benefiting significantly while Llama already demonstrated strong length control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit sequential word numbering during generation improves length fidelity
- Mechanism: Numbering each word creates "length awareness" by converting an abstract constraint into a procedural tracking task with continuous progress feedback
- Core assumption: LLMs possess sufficient working memory and counting ability to maintain accurate sequential counts during autoregressive generation
- Evidence anchors: Abstract states method "implements deliberate planning and word counting mechanisms"; section 3.3 describes creating "simple tracking mechanism that makes the length constraint concrete"; weak corpus evidence from related paper on prompt-based length control
- Break condition: Models with limited context window or weak instruction-following may fail to maintain count accuracy, especially beyond ~500 words

### Mechanism 2
- Claim: Two-phase decomposition (plan-then-rewrite) reduces cognitive load by separating constraint satisfaction from fluency
- Mechanism: Phase 1 focuses on meeting word count exactly via structured output; Phase 2 reformats same content into coherent paragraph without changing length
- Core assumption: Content generated in Phase 1 is semantically sufficient and requires only formatting changes, not content revision
- Evidence anchors: Abstract mentions "structure-guided approach"; section 3.3 states "Separating content generation from final formatting allows the model to first focus on meeting the word count exactly"
- Break condition: If initial draft has semantic errors or critical omissions, rewrite phase cannot fix them without altering word count

### Mechanism 3
- Claim: Structure-guided prompting leverages existing model capabilities without requiring new skills
- Mechanism: Modern LLMs already possess counting and instruction-following abilities; prompt structure simply guides models to apply these to length control
- Core assumption: Target model has sufficient instruction-following capability and was trained on tasks enabling counting behaviors
- Evidence anchors: Section 3.3 states "Modern LLMs can count and follow instructions—our prompt structure just guides them to apply these skills"; section 5 notes Llama 3.1 70B's exceptional length control suggests some models developed capabilities during training
- Break condition: Models with different training distributions (e.g., Mistral Large showed no benefit) may lack prerequisite capabilities or have different behavioral priors

## Foundational Learning

- **Concept: Mean Absolute Percentage Deviation (MAPD)**
  - Why needed here: Primary evaluation metric used throughout the paper to measure length fidelity; essential to interpret results like "37.6% improvement"
  - Quick check question: If a model generates 55 words for a 50-word target, what is the MAPD?

- **Concept: Autoregressive generation with constraints**
  - Why needed here: LLMs generate tokens sequentially without inherent ability to "plan ahead" for exact counts; explains why explicit prompting strategies are needed
  - Quick check question: Why can't a standard LLM simply "stop at 50 words" when instructed?

- **Concept: Task-dependent prompt effectiveness**
  - Why needed here: Paper shows Plan-and-Write works for summarization but fails for story generation; understanding why helps predict where to apply this method
  - Quick check question: What characteristic of summarization vs. creative writing might explain why explicit planning helps one but not the other?

## Architecture Onboarding

- **Component map:** Document input → Thinking prompt injection with target length → Phase 1 generation with counting → Phase 2 rewrite → Tag extraction → Word count validation

- **Critical path:** 1. Document input → 2. Thinking prompt injection with target length → 3. Phase 1 generation with counting → 4. Phase 2 rewrite → 5. Tag extraction → 6. Word count validation

- **Design tradeoffs:**
  - Latency: ~1.57× inference time increase (measured on Qwen 2.5 7B)
  - Token cost: ~1.02× increase (marginal)
  - Task specificity: Works for summarization, not creative generation
  - Model specificity: Benefits Claude models significantly; Llama already good; Mistral shows no benefit
  - Length range: Best for short-to-medium targets (20-500 words); degrades beyond 500

- **Failure signatures:**
  - Model generates wrong count in Phase 1 and cannot recover
  - Phase 2 rewrite changes word count (additions or deletions)
  - Long targets (>500 words) show increasing deviation
  - Creative tasks show worse performance than vanilla prompting
  - Models without strong instruction-following ignore structure tags

- **First 3 experiments:**
  1. Replicate baseline comparison: Test Vanilla V1 vs. Thinking V1 prompts on your target model with 50 and 100-word summarization tasks; measure MAPD to establish if your model benefits from this approach
  2. Ablation test: Remove the Phase 2 rewrite step and measure if counting alone (without reformatting) provides similar benefits—this isolates whether the two-phase structure is necessary
  3. Boundary test: Gradually increase target length (50 → 200 → 500 → 1000 words) to identify the threshold where your model's length fidelity degrades significantly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Plan-and-Write methodology yield significant improvements for document summarization but fail to benefit (or actively hinder) length control in creative story generation?
- Basis in paper: Authors note in Appendix B that for 5 of 6 models, vanilla prompting outperformed Plan-and-Write in story generation tasks, contrasting sharply with summarization results
- Why unresolved: Authors suggest method is better for "information condensation" than "creative content generation" but do not determine if explicit planning phase disrupts creative flow or if "length awareness" mechanism is task-dependent
- What evidence would resolve it: Ablation studies measuring semantic diversity and coherence of outputs generated with and without planning phase across different creative domains

### Open Question 2
- Question: Can explicit planning and verification framework be adapted to enforce semantic or stylistic constraints (e.g., sentiment, tone) that cannot be tracked by simple enumeration?
- Basis in paper: Conclusion explicitly calls for "investigating the generalizability of structure-guided prompting to other types of constraints beyond length"
- Why unresolved: Current success relies on discrete, numeric nature of word counting; unclear if "counting" translates to tracking abstract concepts during planning phase
- What evidence would resolve it: Application of method to constraints with non-numeric definitions, using LLM-based self-evaluation during verification phase instead of integer counting

### Open Question 3
- Question: What architectural or prompting modifications are required to maintain precise word counting over extended outputs (e.g., >1000 words) where current method's fidelity declines?
- Basis in paper: Limitations section states that "length fidelity tends to decrease across all prompting strategies" for longer targets (beyond 500 words), suggesting "fundamental limitation" in LLMs
- Why unresolved: Paper identifies drop in performance but does not propose mechanism (e.g., hierarchical planning) to maintain counting accuracy over long contexts
- What evidence would resolve it: Experiments utilizing segmented planning instructions (e.g., chunking target length) to see if fidelity can be extended linearly

### Open Question 4
- Question: What specific model capabilities or training characteristics determine whether an LLM will benefit from Plan-and-Write versus standard prompting?
- Basis in paper: Results show high variance; Llama 3.1 70B achieved near-perfect fidelity with vanilla prompting, while Mistral Large showed no benefit from structured approach
- Why unresolved: Paper demonstrates method works for some models but does not identify specific pre-training or instruction-tuning factors that make model responsive to explicit word counting prompts
- What evidence would resolve it: Correlation analysis between baseline instruction-following capabilities (without specific length constraints) and marginal gain provided by Plan-and-Write method

## Limitations
- Performance degrades significantly for targets beyond 500 words, suggesting fundamental LLM limitations
- Method is task-specific, showing strong results for summarization but failing or harming performance in creative tasks
- Model-dependent effectiveness—benefits vary widely across different architectures and training approaches

## Confidence
- **Method validity**: High - The approach is well-specified with clear implementation details and reproducible results
- **Generalizability**: Medium - Works well for summarization but shows poor transfer to creative tasks, indicating narrow applicability
- **Model independence**: Low - Results show high variance across models, with some benefiting significantly while others show no improvement

## Next Checks
1. Replicate baseline comparison: Test Vanilla V1 vs. Thinking V1 prompts on your target model with 50 and 100-word summarization tasks; measure MAPD to establish if your model benefits from this approach
2. Boundary test: Gradually increase target length (50 → 200 → 500 → 1000 words) to identify the threshold where your model's length fidelity degrades significantly
3. Task transfer test: Apply the same prompting strategy to a creative task (e.g., story generation) to verify the paper's findings about task-specific effectiveness