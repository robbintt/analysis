---
ver: rpa2
title: A Bayesian latent class reinforcement learning framework to capture adaptive,
  feedback-driven travel behaviour
arxiv_id: '2512.14713'
source_url: https://arxiv.org/abs/2512.14713
tags:
- learning
- route
- class
- choice
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of capturing dynamic travel
  behavior by introducing a Latent Class Reinforcement Learning (LCRL) model that
  integrates econometric latent class structures with reinforcement learning dynamics.
  The LCRL model allows for discrete representation of unobserved heterogeneity in
  learning types (e.g., exploratory vs.
---

# A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour

## Quick Facts
- arXiv ID: 2512.14713
- Source URL: https://arxiv.org/abs/2512.14713
- Reference count: 10
- Primary result: Three-class latent class reinforcement learning model (LL: -803.51) significantly outperforms benchmark (LL: -962.91) in capturing dynamic travel behavior

## Executive Summary
This study addresses the challenge of capturing dynamic travel behavior by introducing a Latent Class Reinforcement Learning (LCRL) model that integrates econometric latent class structures with reinforcement learning dynamics. The LCRL model allows for discrete representation of unobserved heterogeneity in learning types (e.g., exploratory vs. exploitative) while simultaneously modeling how preferences evolve through experience-based feedback. Using variational Bayes estimation on a driving simulator dataset with 83 participants making 1,660 choices, the model identifies three distinct behavioral classes: context-dependent preferences with exploitative tendencies (22%), persistent exploitative tendencies regardless of context (49%), and exploratory strategy with context-specific preferences (29%). The three-class LCRL significantly outperforms a benchmark reinforcement learning model with log-likelihood of -803.51 versus -962.91, AIC of 1,667.02 versus 1,937.82, and BIC of 1,829.46 versus 1,970.31.

## Method Summary
The LCRL model estimates discrete latent classes of travelers, each with class-specific reinforcement learning parameters governing how they update route preferences based on experienced travel times. Using variational Bayes with mean-field approximation, the model simultaneously estimates class membership probabilities and learning dynamics (learning rate α, choice sensitivity β, initial expectations Q₀) for each class. The model distinguishes between driving simulator and stated preference contexts by allowing separate β parameters for each environment. The framework uses the Rescorla-Wagner update rule where Q(t) = Q(t-1) + α(r - Q(t-1)), with prediction error δ = r - Q driving preference updates only for chosen alternatives.

## Key Results
- Three distinct behavioral classes identified: context-dependent explorers (22%), persistent exploiters (49%), and exploratory learners (29%)
- Three-class LCRL significantly outperforms benchmark RL model (LL: -803.51 vs -962.91, AIC: 1,667.02 vs 1,937.82, BIC: 1,829.46 vs 1,970.31)
- Class membership strongly influenced by experimental order, with SP-first participants more likely to be context-dependent explorers
- Learning rates vary by class (α=0.277-0.437) with context-specific choice sensitivities (β_DS=0.247-0.935 vs β_SP=0.141-0.781)

## Why This Works (Mechanism)

### Mechanism 1: Prediction Error-Driven Learning
- Claim: Individuals update route preferences by computing the discrepancy between expected and experienced outcomes, then adjusting expectations proportionally.
- Mechanism: Rescorla-Wagner update rule: Q(t) = Q(t-1) + α(r - Q(t-1)), where α controls learning speed and (r - Q) is the reward prediction error.
- Core assumption: Learners maintain scalar expectations per alternative and update only chosen alternatives (no fictive learning).
- Evidence anchors:
  - [abstract] "individuals learn their preferences over time...adapt their expectations and preferences about the alternatives they face"
  - [section 2.1.1] Equation 1 formalizes δ = r - Q as prediction error; α ranges 0-1
  - [corpus] Related work (FMR 0.49-0.61) confirms prediction error as standard RL mechanism, though no direct validation in this corpus
- Break condition: If individuals use belief distributions rather than point estimates, or if unchosen alternatives decay, this mechanism underestimates learning complexity.

### Mechanism 2: Discrete Heterogeneity via Latent Classes
- Claim: Population contains distinct behavioral types that cannot be captured by continuous unimodal parameter distributions.
- Mechanism: Mixture model assigns individuals to K classes via softmax over covariates (Eq. 6); each class has separate α, β, γ parameters governing learning dynamics.
- Core assumption: Within-class homogeneity—individuals in same class share identical learning parameters.
- Evidence anchors:
  - [abstract] "identify three distinct classes...context-dependent explorers, persistent exploiters, and exploratory learners"
  - [section 4, Table 4] Class 1: α=0.277, β values insignificant; Class 2: α=0.355, significant β; Class 3: α=0.437, low significant β
  - [corpus] No direct validation; related clustering bandit papers (FMR 0.50) suggest discrete user segments but in different domains
- Break condition: If true population has continuous gradient of learning styles, or if individuals switch strategies mid-experiment, discrete classes misclassify behavior.

### Mechanism 3: Context-Sensitive Exploration-Exploitation
- Claim: Sensitivity to learned values (β) varies between physical experience (driving simulator) and hypothetical scenarios (stated preference), producing context-dependent choice patterns.
- Mechanism: β = β_DS·D + β_SP·(1-D) where D=1 for simulator; this allows separate exploration rates per context within same latent class.
- Core assumption: Context shifts affect sensitivity but not learning rate or initial expectations.
- Evidence anchors:
  - [section 4] "participants made different route choices during DS and SP tasks...separate class-specific sensitivity parameters"
  - [section 4, Table 4] Class 1: β_DS=0.935 (ns), β_SP=0.781 (ns); Class 3: β_DS=0.247***, β_SP=0.141*** showing consistent exploration
  - [corpus] Weak support; minority-aware dialogue systems paper (FMR 0.57) shows preference-adaptive behavior but no context-switching mechanism
- Break condition: If context also affects learning rate (not just sensitivity), or if ordering effects persist beyond first exposure, model underspecifies context influence.

## Foundational Learning

- Concept: **Reinforcement Learning Basics (RL)**
  - Why needed here: Core computational framework; without understanding Q-values, prediction errors, and exploration-exploitation tradeoffs, the model architecture is opaque.
  - Quick check question: Can you explain why a learner with α=0.9 updates faster than α=0.1, and what behavioral pattern each suggests?

- Concept: **Latent Class Models (LCM)**
  - Why needed here: Required to understand how discrete unobserved heterogeneity is parameterized and estimated; distinguishes this from random-coefficient approaches.
  - Quick check question: Given membership probabilities P(class k | X, η), what happens if two classes have nearly identical choice parameters?

- Concept: **Variational Inference (VI)**
  - Why needed here: Estimation method for high-dimensional posteriors; understanding ELBO optimization vs. MCMC clarifies why this scales to large datasets.
  - Quick check question: Why does maximizing ELBO approximate posterior inference, and what does the KL divergence term regularize?

## Architecture Onboarding

- Component map:
  - Class Membership Module: MNL over covariates → class probabilities π_nk (Eq. 6)
  - RL Core (per class): Q-value initialization → prediction error δ → Q-update (Eq. 7) → utility formation (Eq. 2) → choice probability via MNL (Eq. 8)
  - Context Layer: Dummy variable D switches between β_DS,k and β_SP,k (Eq. 17)
  - Variational Engine: Mean-field q(Θ) factorization → ELBO computation → Adam optimization
  - Data Flow: Observed choices y_nt + feedback r_nt → likelihood P(Y|Θ) (Eq. 10) → posterior approximation

- Critical path:
  1. Specify number of classes K (start with K=2-3)
  2. Initialize Q-values per class (constrain to plausible outcome range: a=2, b=7 minutes for this data)
  3. Forward pass: compute class probabilities → per-class Q-sequences → choice likelihoods
  4. ELBO computation: expected log-likelihood minus KL to priors
  5. Gradient-based optimization until convergence

- Design tradeoffs:
  - More classes → better fit but higher BIC penalty; this paper selected K=3 (lowest BIC despite K=4 having better AIC)
  - Context-specific β vs. unified β: adds parameters but captures DS/SP behavioral differences
  - Chosen-only updates (standard Rescorla-Wagner) vs. full-belief updates: simpler but ignores counterfactual learning

- Failure signatures:
  - Flat class membership probabilities (~0.33 each for K=3): classes not behaviorally distinct
  - β parameters near 0 or diverging: exploration-exploitation mechanism not identified
  - α hitting bounds (0 or 1): learning rate not recoverable from data
  - ELBO not increasing: optimization stuck; check learning rate, initialization

- First 3 experiments:
  1. **Parameter recovery test**: Generate synthetic data with known K=2, α, β, γ; estimate LCRL; verify posterior means recover ground truth (paper's Appendix B shows this works with correlations >0.80 for most parameters)
  2. **Ablate latent classes**: Compare K=1 (standard RL) vs. K=2 vs. K=3 on held-out likelihood; expect BIC improvement up to optimal K
  3. **Context ablation**: Force β_DS = β_SP per class; compare LL and AIC to full model; quantify context mechanism's contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating continuous within-class heterogeneity (allowing individual-specific parameters) improve the explanatory power of the Latent Class Reinforcement Learning (LCRL) model?
- Basis in paper: [explicit] The conclusion states: "Future research could also explore within-class heterogeneity by allowing the RL parameters to be individual-specific."
- Why unresolved: The current model assumes homogeneity within latent classes; it captures discrete types but not continuous individual deviations from class means.
- What evidence would resolve it: Estimating a hybrid model (e.g., a Latent Class Mixed RL) and demonstrating a statistically significant improvement in fit or prediction over the standard LCRL.

### Open Question 2
- Question: Does modeling "fictive updating" (updating beliefs about unchosen alternatives) provide a better fit for travel behaviour than the chosen-only Rescorla-Wagner formulation used here?
- Basis in paper: [inferred] The methodology notes that "Alternative reinforcement learning models allow for fictive updates to unchosen alternatives... but these extensions are not considered in the present formulation."
- Why unresolved: The paper restricts learning updates to only the chosen route; it remains unknown if travellers in this dataset also updated their expectations of the unchosen route based on inferred or observed feedback.
- What evidence would resolve it: Estimating an LCRL model with fictive update mechanisms on the same dataset and comparing the latent class structures and error terms.

### Open Question 3
- Question: Are the identified latent classes stable behavioural traits or artifacts of the specific experimental design (e.g., the order of Driving Simulator vs. Stated Preference tasks)?
- Basis in paper: [inferred] The results show class membership is heavily influenced by "Experiment Order" (e.g., Class 1 is predominantly "SP-first"), suggesting the learning strategy might be context-dependent rather than inherent to the individual.
- Why unresolved: The model captures heterogeneity within the experiment, but the extent to which these classes persist or transfer to purely real-world environments without experimental ordering effects is untested.
- What evidence would resolve it: Applying the LCRL model to a dataset without distinct experimental phases (e.g., naturalistic GPS data) to determine if the same three classes (explorers, exploiters, context-dependent) emerge.

## Limitations
- Parameter identifiability issues between context effects and experimental artifacts
- Within-class homogeneity assumption may oversimplify continuous behavioral heterogeneity
- External validity limited by controlled driving simulator environment

## Confidence
- High confidence: Superior model fit (LL: -803.51 vs -962.91) and robust identification of three distinct behavioral classes
- Medium confidence: Interpretation of class-specific learning dynamics relies on context sensitivity assumptions
- Low confidence: Absolute magnitude of learning rates cannot be benchmarked against real-world travel behavior

## Next Checks
1. **Context mechanism validation**: Re-estimate the model with β_DS = β_SP (removing context-specific sensitivity). Compare model fit to quantify how much of the LCRL's improvement comes from capturing context effects vs. latent classes.

2. **Alternative class structures**: Test K=4 and K=5 class models. Examine whether additional classes capture meaningful heterogeneity or merely overfit noise. Report the trade-off between fit improvement and BIC penalties.

3. **Learning rate context effects**: Modify the model to allow α to vary by context (α_DS ≠ α_SP) while keeping other parameters class-specific. Compare to the current specification to assess whether context influences both how people choose and how they learn.