---
ver: rpa2
title: Active inference for action-unaware agents
arxiv_id: '2508.12027'
source_url: https://arxiv.org/abs/2508.12027
tags:
- free
- energy
- inference
- agents
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two implementations of active inference agents
  in discrete grid-world environments: "action-aware" agents that know their past
  actions and "action-unaware" agents that must infer them. The study investigates
  how this architectural difference affects learning and decision-making.'
---

# Active inference for action-unaware agents

## Quick Facts
- **arXiv ID:** 2508.12027
- **Source URL:** https://arxiv.org/abs/2508.12027
- **Reference count:** 40
- **Primary result:** Action-unaware agents, which infer their past actions without efference copies, can match the performance of action-aware agents in grid-world environments despite higher computational cost.

## Executive Summary
This paper investigates how architectural differences in active inference agents affect learning and decision-making in discrete grid-world environments. The study compares "action-aware" agents that know their past actions with "action-unaware" agents that must infer them as latent variables. Both agent types use variational free energy minimization for perception, planning, and learning, but differ in how they compute policy-conditioned free energies. The results show that action-unaware agents can achieve comparable performance to action-aware agents in relatively simple environments, suggesting that explicit knowledge of past actions may not be essential for effective active inference-based decision-making, though at a higher computational cost.

## Method Summary
The study implements active inference agents in discrete grid-world environments using variational free energy minimization. Agents maintain generative models with Dirichlet-distributed transition (B) and emission (A) matrices, updating beliefs through message passing. Action-aware agents update one collection of variational distributions for past states, while action-unaware agents must update distributions for all possible policies at each time step. Both types select policies by minimizing expected free energy, balancing goal-seeking and exploration. Learning occurs through updating Dirichlet parameters based on observed state transitions. The experiments use T-maze (5 states) and 3x3 grid-world (9 states) environments with varying horizons and policy spaces.

## Key Results
- Action-unaware agents matched the performance of action-aware agents in both T-maze and 3×3 grid-world environments despite their computational disadvantage
- Both agent types successfully learned transition dynamics and reached goal states with comparable success rates
- Action-aware agents showed slightly better learning speed and lower free energy values, demonstrating their computational efficiency advantage

## Why This Works (Mechanism)

### Mechanism 1: Variational Posterior Update via Message Passing
- **Claim:** If an agent minimizes variational free energy, it effectively infers hidden states (perception) and model parameters (learning) from ambiguous sensory data.
- **Mechanism:** The agent uses a generative model (matrices A for emission, B for transitions) to predict observations. It updates its internal beliefs (variational posteriors $Q(S_t|\pi_k)$) by iteratively minimizing the divergence between its predictions and actual observations using gradient descent (variational message passing).
- **Core assumption:** The environment's dynamics can be approximated by the agent's categorical generative model.
- **Evidence anchors:**
  - [Section 3.3] Describes the update rules derived by setting the gradient of the free energy to zero, defining the "perceptual inference" scheme.
  - [Section S2.1] Details the specific gradient equations (Eq. S4, S5) used to update state probabilities ($s_t$) based on priors and likelihoods.
  - [Corpus] Weak support; neighbors mention active inference generally but do not validate the specific discrete message-passing equations used here.
- **Break condition:** The state space becomes continuous or non-Markovian, rendering the discrete categorical update rules invalid.

### Mechanism 2: Policy Selection via Expected Free Energy (EFE)
- **Claim:** If an agent selects policies that minimize Expected Free Energy (EFE), it balances goal-seeking (exploitation) and information-seeking (exploration).
- **Mechanism:** Policies are scored not just on past evidence (VFE) but on predicted future states. EFE penalizes "Risk" (divergence from preferred states $P^*$) and "Ambiguity" (uncertainty in state-observation mapping), while rewarding "Novelty" (information gain about model parameters). The agent samples actions from policies with the lowest EFE.
- **Core assumption:** Agents possess intrinsic preferences ($P^*$) and a drive to resolve uncertainty (epistemic value).
- **Evidence anchors:**
  - [Section 3.3] Defines EFE (Eq. 8) and states that minimizing it balances the "exploitation vs. exploration dilemma."
  - [Abstract] Notes that agents "maximise the likelihood of preferred observations" via expected free energy minimization.
  - [Corpus] General support found in neighbor "Real-World Robot Control by Deep Active Inference," which applies similar goal-directed/exploratory balancing.
- **Break condition:** Environments with deceptive rewards where "Novelty" leads to non-productive loops or where "Risk" must be accepted (e.g., safety-critical volatility) without the specific constraints modeled here.

### Mechanism 3: Inferring Actions without Efference Copy
- **Claim:** If an agent lacks an "efference copy" (knowledge of its own past actions), it can still plan effectively by treating past actions as latent variables to be inferred.
- **Mechanism:** "Action-unaware" agents maintain distinct beliefs about the past for every possible policy ($Q(S_{1:\tau}|\pi_k)$). Observations inconsistent with a policy increase that policy's VFE, effectively "pruning" impossible action sequences. "Action-aware" agents, conversely, need only maintain one set of past beliefs conditioned on the known action sequence.
- **Core assumption:** The agent has sufficient computational resources to evaluate all potential policy-conditioned pasts (complexity $O(nm)$ vs $O(n)$).
- **Evidence anchors:**
  - [Section 3.4] States that for action-unaware agents, "each policy is a distinct sequence of past, present and future actions," requiring inference of the "hidden sequence of actions."
  - [Results] Shows that despite the computational disadvantage ($O(nm)$), action-unaware agents "match the performance" of action-aware agents in grid worlds.
  - [Corpus] No direct support in provided neighbors; this specific architectural comparison is unique to the source text.
- **Break condition:** Scaling to environments with long time horizons or large action spaces, causing a combinatorial explosion in the policy space that exceeds memory/time constraints.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - **Why needed here:** The entire framework models the environment and agent as a POMDP (states, observations, actions, transitions). Understanding the separation between hidden states ($S$) and observations ($O$) is critical.
  - **Quick check question:** Can you explain why the agent cannot simply read the "state" directly but must infer it from "observations"?

- **Concept: Variational Free Energy (VFE)**
  - **Why needed here:** VFE is the single objective function minimized for perception and learning. It acts as a proxy for "surprisal" (negative log-evidence).
  - **Quick check question:** How does minimizing VFE effectively reduce the distance between the agent's internal model and the external reality?

- **Concept: The Efference Copy**
  - **Why needed here:** This is the central theoretical divide in the paper. It is the neural signal purportedly sent from motor to sensory areas to predict the sensory consequences of self-generated movement.
  - **Quick check question:** What functional disadvantage does the "action-unaware" agent face compared to the "action-aware" agent, given the absence of this signal?

## Architecture Onboarding

- **Component map:** Generative Model (A/B matrices) -> Variational Posterior ($Q$) -> Inference Engine (message passing) -> Planner (EFE scoring) -> Action Sampler
- **Critical path:**
  1.  **Initialization:** Randomize $A/B$ or set to weak priors.
  2.  **Perception Loop (per step):** Receive $o_t$ → Update $Q(S_t|\pi_k)$ for all policies (unaware) or single past (aware).
  3.  **Planning:** Compute $G_H(\pi_k)$ for future horizon $H$ → Update $Q(\pi)$ via softmax of $-(VFE + EFE)$.
  4.  **Action:** Sample action from the most probable policy (or Bayesian model average).
  5.  **Learning (end of episode):** Update $A/B$ parameters using collected statistics.

- **Design tradeoffs:**
  - **Action-Aware vs. Action-Unaware:** Choose **Action-Aware** for efficiency ($O(n)$) and standard robotics (where motor commands are logged). Choose **Action-Unaware** for biological fidelity or simulations where internal signals are noisy/latent (computational cost $O(nm)$).
  - **Policy Horizon ($H$):** Larger $H$ improves planning accuracy but exponentially increases policy space.

- **Failure signatures:**
  - **VFE Divergence:** If VFE increases over time (Fig. 5 trend early episodes), the generative model $B$ is diverging from ground truth (learning instability).
  - **Stagnant Policies:** If sub-optimal policies persist (Fig. 12), the "Risk" term may be dominating "Novelty," preventing exploration of better paths.

- **First 3 experiments:**
  1.  **T-Maze Validation:** Run the 4-step T-maze task with the **Action-Aware** agent (Algorithm S2) to establish a baseline for learning speed.
  2.  **Computational Load Test:** Run the **Action-Unaware** agent (Algorithm S1) in the same T-Maze and plot "Time per Step" vs. the Action-Aware agent to verify the $O(nm)$ overhead.
  3.  **Transition Learning Check:** Visualize the learned $B$ matrices (as in Section S3.1.7) after 50 episodes to ensure the agent has correctly mapped deterministic transitions before introducing stochastic noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can weight-based sampling mechanisms effectively scale action-unaware agents to high-dimensional action-sequence spaces?
- Basis in paper: [explicit] The authors speculate that weight-based sampling might handle combinatorial explosion but explicitly leave this investigation for future work.
- Why unresolved: The current implementation suffers from computational intractability as the number of past and future action sequences increases, limiting applicability to simple grid worlds.
- What evidence would resolve it: A demonstration of action-unaware agents maintaining performance in complex environments with large state/action spaces using approximate sampling methods.

### Open Question 2
- Question: Why does task performance remain robust even when sub-optimal policies accumulate significant probability mass?
- Basis in paper: [explicit] The authors note that performance does not deteriorate substantially despite increased probability for sub-optimal policies, leaving the explanation for future work.
- Why unresolved: The relationship between the policy probability distribution and the final action selection (which uses a model average) appears robust, but the specific dynamics preventing failure are unexplained.
- What evidence would resolve it: A detailed analysis of the action selection phase showing how the Bayesian model average mitigates the influence of high-probability sub-optimal policies.

### Open Question 3
- Question: How do expected free energy and policy-conditioned free energy interact to determine policy probabilities when their contributions conflict?
- Basis in paper: [explicit] The authors find these quantities can be informative in "opposing ways" and leave investigations into their opposing contributions to policy probabilities for future work.
- Why unresolved: It is currently unclear how the agent arbitrates between instrumental value (driven by expected free energy) and consistency with past observations (driven by policy-conditioned free energy).
- What evidence would resolve it: Sensitivity analyses varying the relative weighting of these free energy components to observe changes in policy selection and learning speed.

## Limitations
- The computational overhead of maintaining separate beliefs for all policies (O(nm) complexity) may become prohibitive in larger state spaces or longer time horizons
- The experimental environments use deterministic transitions and identity emission matrices, which simplify the inference task compared to real-world scenarios with stochastic dynamics and noisy observations
- The computational cost analysis is theoretical (O(nm) vs O(n)) without empirical timing data across varying problem sizes

## Confidence
- **High confidence:** The comparative performance results between action-aware and action-unaware agents in the tested environments are reproducible given the specific implementations and parameters.
- **Medium confidence:** The claim that action-unaware agents achieve "similar performance" holds for these specific simple environments but may not extend to more complex domains without further validation.
- **Low confidence:** The computational cost analysis is theoretical without empirical timing data across varying problem sizes.

## Next Checks
1. **Scaling Test:** Evaluate both agent types in environments with increasing state space size and horizon length to quantify the computational overhead and identify the breaking point where action-unaware agents become impractical.

2. **Noise Sensitivity Analysis:** Introduce varying levels of observation noise and transition stochasticity to test whether the action-unaware agents' inferred action sequences remain accurate when the generative model's assumptions are violated.

3. **Biological Plausibility Benchmark:** Compare the performance of action-unaware agents against models with partial efference copy information (e.g., noisy action copies) to determine if the full absence of action knowledge is truly optimal or if some action information provides better performance-efficiency tradeoffs.