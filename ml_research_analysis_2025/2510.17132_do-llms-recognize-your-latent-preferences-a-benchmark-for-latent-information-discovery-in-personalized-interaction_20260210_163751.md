---
ver: rpa2
title: Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information
  Discovery in Personalized Interaction
arxiv_id: '2510.17132'
source_url: https://arxiv.org/abs/2510.17132
tags:
- user
- preferences
- assistant
- question
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a benchmark for evaluating how well LLMs\
  \ can discover and utilize latent user preferences through multi-turn conversation.\
  \ The benchmark includes three tasks\u201420 Questions, Personalized Question Answering,\
  \ and Personalized Text Summarization\u2014using a tri-agent framework (User, Assistant,\
  \ Judge) to measure preference elicitation and personalization."
---

# Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction

## Quick Facts
- arXiv ID: 2510.17132
- Source URL: https://arxiv.org/abs/2510.17132
- Reference count: 40
- Key outcome: Multi-turn LLM preference discovery achieves 32-98% success across tasks, varying by complexity and topic

## Executive Summary
This paper introduces a benchmark for evaluating how well LLMs can discover and utilize latent user preferences through multi-turn conversation. The benchmark includes three tasks—20 Questions, Personalized Question Answering, and Personalized Text Summarization—using a tri-agent framework (User, Assistant, Judge) to measure preference elicitation and personalization. Experiments across four models (GPT-4o-mini, Claude-3.5-Haiku, Mistral-7B-Instruct, Qwen2.5-7B-Instruct) show success rates ranging from 32% to 98%, depending on task complexity, topic, and number of preferences. The results demonstrate that while LLMs can partially infer latent preferences, their effectiveness varies significantly with context, highlighting that adaptive preference inference remains a key challenge for building truly personalized AI systems.

## Method Summary
The benchmark evaluates LLMs using a tri-agent framework where a User agent holds latent preferences, an Assistant agent (the model under test) must discover these preferences through multi-turn dialogue, and a Judge agent evaluates whether the Assistant's output satisfies all preferences. The User responds passively only when prompted, forcing the Assistant to actively elicit information. Three tasks are evaluated: 20 Questions (hidden object inference), Personalized Question Answering (5 domains), and Personalized Text Summarization (4 text types). Success is measured by whether outputs satisfy all latent preferences, with efficiency measured by the number of turns needed.

## Key Results
- Success rates range from 32% (Shopping Assistance, 3 preferences) to 98% (Restaurant Recommendation, 1 preference)
- Qwen2.5-7B-Instruct shows the best efficiency, often identifying objects in fewer turns than larger models
- Task complexity and number of preferences significantly impact performance, with multi-preference scenarios showing 40-60% degradation in success rates
- Error analysis reveals "Preference Reinforcement" and "Preference Dilution" as the most common failure modes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A tri-agent framework decouples the reasoning process (Assistant) from preference holding (User) and verification (Judge), allowing isolated evaluation of latent discovery.
- **Mechanism:** The system instantiates three distinct LLM roles. The User agent is seeded with hidden attributes (e.g., "avoid gluten") and responds truthfully but passively. The Assistant (model under test) must interact with the User without prior knowledge of these attributes. The Judge evaluates the Assistant's output against the ground truth preferences after every turn.
- **Core assumption:** The Judge (GPT-4o in experiments) is sufficiently capable of logical entailment to reliably verify alignment with complex preferences.
- **Evidence anchors:** [abstract] "...tri-agent framework (User–Assistant–Judge) enabling turn-level evaluation of elicitation and adaptation."

### Mechanism 2
- **Claim:** Forcing a "passive-user" interaction model compels the Assistant to actively plan information retrieval strategies rather than relying on explicit instruction following.
- **Mechanism:** Unlike standard RLHF or chat settings where a user might explicitly state "summarize this briefly," this benchmark requires the User agent to only answer specific questions posed by the Assistant.
- **Core assumption:** LLMs possess sufficient priors about human preferences (e.g., that dietary restrictions are relevant to restaurant choice) to ask relevant questions without prompting.
- **Evidence anchors:** [section 1] "...users rarely articulate every preference explicitly... waiting to be inferred."

### Mechanism 3
- **Claim:** Turn-level evaluation with an early-stopping criterion quantifies the efficiency of reasoning, not just final accuracy.
- **Mechanism:** After every exchange (one question + one answer), the Assistant generates a provisional task response. The Judge checks this response immediately. If it aligns with all latent preferences, the interaction terminates.
- **Core assumption:** The optimal strategy is to identify preferences efficiently; a model that takes 20 turns to find a preference is less capable than one that takes 2, even if both succeed.
- **Evidence anchors:** [abstract] "...enabling turn-level evaluation of elicitation and adaptation."

## Foundational Learning

- **Concept: Latent vs. Explicit Context**
  - **Why needed here:** The core challenge is distinguishing what the user *said* from what they *want*. The benchmark tests the Assistant's ability to bridge this gap.
  - **Quick check question:** If a user asks for a "restaurant recommendation" but mentions nothing else, is "near the user's hotel" a latent or explicit preference? (Answer: Latent/Contextual)

- **Concept: Preference Trees (Decision Theory)**
  - **Why needed here:** The paper references this to explain how questions narrow down possibilities. Understanding hierarchical preference structures helps in designing the "20 Questions" logic.
  - **Quick check question:** How does asking "Is it a living organism?" prune the decision tree for "20 Questions"?

- **Concept: Preference Reinforcement (Memory)**
  - **Why needed here:** The error analysis identifies "Preference Reinforcement Error"—forgetting a stated preference later. This is a key failure mode in multi-turn reasoning.
  - **Quick check question:** If a user states "I am vegan" in turn 1, and the Assistant recommends a steakhouse in turn 5, which specific error type is this?

## Architecture Onboarding

- **Component map:**
  - User Agent (GPT-4o) -> Assistant Agent (Model Under Test) -> Judge Agent (GPT-4o)

- **Critical path:**
  1. Initialize User with hidden preferences
  2. **Loop (Max Turns):**
     - Assistant generates a question
     - User responds based only on hidden prefs
     - Assistant generates a tentative solution
     - Judge evaluates solution
     - **If Correct:** Break loop (Success)
  3. Log Success and Stop Turn

- **Design tradeoffs:**
  - Passive vs. Active User: Passive (default) is harder and more realistic for discovery; Active users boost success rates (98% vs 78%) but reduce the evaluation of elicitation skills
  - Judge Reliability vs. Cost: Using GPT-4o as Judge is expensive but reliable (74% instruction adherence vs 11% for Mistral)

- **Failure signatures:**
  - Preference Elicitation Error: Assistant asks redundant or irrelevant questions
  - Preference Dilution: Final answer includes some preferences but misses others
  - Premature Stopping: Assistant guesses correctly by luck rather than deduction

- **First 3 experiments:**
  1. **Sanity Check (20 Questions):** Run the 20 Questions task with "Topic Known". This isolates reasoning ability.
  2. **Scalability Test (PQA):** Run Personalized Question Answering (PQA) with 1 vs. 3 preferences. Look for the degradation curve.
  3. **Judge Ablation:** Swap the Judge agent for a smaller model (e.g., GPT-4o-mini) on a subset of data to measure the "Judge Gap".

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural or prompting interventions can reduce Preference Reinforcement and Preference Dilution errors during multi-turn preference elicitation?
- **Basis in paper:** [explicit] Section 4.4 states these are the most frequent error types and that the results "highlight the need for improved mechanisms to represent and condition on user preferences during ongoing interaction."

### Open Question 2
- **Question:** Can targeted training or prompting strategies simultaneously improve both success rates and efficiency (fewer turns) in latent information discovery?
- **Basis in paper:** [explicit] Conclusion states: "The next step is to explore approaches for improving the performance of LLMs on such tasks... Improvements should address not only success rates but also efficiency, by reducing the number of steps required to reach the correct answer."

### Open Question 3
- **Question:** What latent factors explain the large variance in difficulty across topics (e.g., 90% success in Medical Care vs. 20–50% in Shopping Assistance)?
- **Basis in paper:** [inferred] Section 4.2 notes "PQA is not uniformly easy or hard; topic strongly determines difficulty" with success rates differing by up to 70%, but no causal explanation is provided.

## Limitations

- **Judge Reliability:** The benchmark's validity depends on the Judge's ability to correctly evaluate preference alignment, which is complex and potentially subjective.
- **Dataset Representativeness:** The 20 Questions task uses LLM-generated objects rather than curated real-world examples, potentially creating artificial patterns.
- **Preference Complexity Bounds:** The benchmark focuses on 1-3 preferences per instance, but real-world preference discovery often involves many more.

## Confidence

- **High Confidence:** The tri-agent framework design and its basic implementation are sound and well-documented.
- **Medium Confidence:** The claim that LLMs can partially infer latent preferences is supported by the results, but the degree of "partial" success varies significantly.
- **Low Confidence:** The assertion that turn-level efficiency metrics meaningfully distinguish model capabilities requires further validation.

## Next Checks

1. **Judge Ablation Study:** Run the benchmark with progressively weaker Judge models to quantify how much the Judge gap affects reported success rates.
2. **Preference Complexity Scaling:** Systematically vary the number of latent preferences from 1 to 10 in PQA tasks to establish the degradation curve.
3. **Real-World Dataset Validation:** Replace the LLM-generated 20 Questions objects with a curated dataset of real-world objects to test whether the same patterns persist.