---
ver: rpa2
title: Using Synthetic Images to Augment Small Medical Image Datasets
arxiv_id: '2503.00962'
source_url: https://arxiv.org/abs/2503.00962
tags:
- b-sg
- mc-sg
- images
- image
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the use of synthetic images to augment
  small medical image datasets for semantic segmentation. A novel conditional variant
  of the StyleGAN2, called Conditional-StyleGAN2, was developed to generate multi-modal
  high-resolution medical images.
---

# Using Synthetic Images to Augment Small Medical Image Datasets

## Quick Facts
- arXiv ID: 2503.00962
- Source URL: https://arxiv.org/abs/2503.00962
- Reference count: 11
- Primary result: GAN-based synthetic augmentation failed to improve semantic segmentation performance on small medical datasets

## Executive Summary
This study investigated whether synthetic medical images generated by GANs could augment small datasets for semantic segmentation. The authors developed a Conditional-StyleGAN2 variant to generate multi-modal high-resolution medical images, training three GAN models (Baseline StyleGAN2, Mask StyleGAN2, and Conditional-StyleGAN2) on six medical imaging datasets. Despite generating anatomically plausible image-mask pairs, segmentation models trained on real plus synthetic data showed no improvement and sometimes degraded performance compared to real data alone. The quality of generated images was assessed using FID, but the segmentation failure suggests FID may not correlate with task-relevant features in medical imaging.

## Method Summary
The study used six medical imaging datasets (BraTS20, KiTS19, IBSR18, D-HEART, D-SPLEEN, U-PRO) resized to 256x256. Three GAN architectures were developed: Baseline StyleGAN2 generating concatenated image-mask pairs, Mask StyleGAN2 generating only masks, and Conditional-StyleGAN2 conditioned on masks at three injection points. Generated images and real images were mixed at varying ratios (20-500 synthetic samples) to train 2D U-Net models with batch normalization. Segmentation performance was evaluated using Dice Similarity Coefficient, while GAN quality was measured by Fréchet Inception Distance.

## Key Results
- Segmentation models did not benefit from generated images; performance decreased with large numbers of synthetic samples
- Baseline StyleGAN2 and Mask+Conditional StyleGAN2 pipeline both failed to improve DSC scores
- Generated images appeared "blotchy and blurry" despite having correct high-level structures
- FID scores improved with training but did not correlate with segmentation performance

## Why This Works (Mechanism)

### Mechanism 1: Conditional Mask-to-Image Synthesis
Conditioning the generator on segmentation masks should produce anatomically coherent image-mask pairs for augmentation. The Conditional-StyleGAN2 receives a resampled mask at three injection points—concatenated before the "fromRGB" layer, before a ResNet-like block in the discriminator, and after weight demodulation in the generator. This creates a learned mapping from mask structure to realistic image appearance. Core assumption: The generator learns to produce images whose feature distribution sufficiently matches real medical images such that downstream segmenters generalize better. Evidence anchors: [section 3.2] describes the mask injection mechanism; [abstract] claims generation of multi-modal high-resolution medical images; [corpus] shows weak support from SkinDualGen's prompt-driven diffusion approach. Break condition: If generated images fail to capture fine-grained texture or introduce artifacts, the discriminator's feedback signal may not align with downstream task requirements.

### Mechanism 2: Two-Stage Decoupled Generation (MC-SG Pipeline)
Separately generating masks then images allows control over label distribution before image synthesis. M-SG generates label masks from noise; C-SG conditions on those masks to produce corresponding images. This decouples label space learning from appearance learning. Core assumption: Generated masks are structurally valid and diverse enough to expand the label space beyond the training set. Evidence anchors: [section 3.3, Figure 3c] describes the two-stage pipeline; [section 5] notes generated images "look quite blotchy and blurry with low-quality" despite correct high-level structures; [corpus] suggests DiffAug validates diffusion-based augmentation with automatic segmentation validation. Break condition: If M-SG produces anatomically implausible masks, C-SG will generate images with inconsistent or misleading structure.

### Mechanism 3: Failure Mode—Distribution Mismatch and Quality Degradation
Synthetic augmentation degraded performance because generated samples did not match real data distribution at the feature level relevant to segmentation. FID measures similarity to ImageNet features, not medical segmentation-relevant features. The discriminator optimizes for visual realism, not boundary precision or tissue contrast. Core assumption: Lower FID correlates with segmentation utility—this appears unsupported. Evidence anchors: [section 5] states segmentation models did not benefit and performance decreased with many generated samples; [Table 3] shows baseline outperforms all augmentation variants on BraTS20-369; [corpus] cites Skandarani et al.'s argument that "no GAN is capable of reproducing the full richness of medical datasets." Break condition: Already broken—this mechanism explains observed failure rather than success.

## Foundational Learning

- **Min-Max GAN Training Dynamics**: Understanding why StyleGAN2 may converge to visually plausible but task-irrelevant samples. Quick check: Can you explain why a low discriminator loss does not guarantee high downstream task performance?
- **Fréchet Inception Distance (FID) Limitations**: FID uses ImageNet-pretrained features; medical imaging requires domain-specific evaluation. Quick check: Why might FID improve while segmentation Dice decreases?
- **Conditional GAN Conditioning Strategies**: C-SG uses early and mid-network mask concatenation—placement affects gradient flow. Quick check: What happens if conditioning information is only injected late in the generator?

## Architecture Onboarding

- **Component map**: Noise z → B-SG → (image, mask) pairs OR Noise z → M-SG → mask → C-SG → image
- **Critical path**: 1) Train B-SG or (M-SG + C-SG) on real (image, mask) pairs 2) Generate N synthetic patient volumes 3) Mix real + synthetic at varying ratios (20-500) 4) Train U-Net on combined data; evaluate DSC on held-out test set
- **Design tradeoffs**: B-SG is simpler but entangles image/mask generation; MC-SG offers modularity but compounds errors across two models. Spatial augmentation (flip, rotate, shift) often matches or exceeds GAN augmentation—simpler baselines remain competitive
- **Failure signatures**: Synthetic images appear "blotchy and blurry" at high resolution (Figure 4); DSC plateaus or declines as synthetic sample count increases beyond 100-200; Nemenyi post-hoc shows baseline significantly outperforms high-synthesis variants (Table 3)
- **First 3 experiments**: 1) Reproduce baseline vs. B-SG (20 samples) on BraTS20-20 subset to validate pipeline integrity 2) Ablate conditioning injection points in C-SG: test generator-only vs. discriminator-only vs. full conditioning 3) Add quality filtering: discard synthetic pairs where generated mask IoU with a pretrained segmenter falls below threshold, then retrain U-Net

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms cause synthetic image augmentation to degrade segmentation performance when the ratio of generated to real data is increased? Basis in paper: [explicit] The authors state that "Further research is needed to establish how synthetic image augmentation affects segmentation performance" and note that performance decreased with a large number of samples. Why unresolved: The study observed the negative result but did not isolate whether the degradation was due to label noise, domain shift, or mode collapse in the generator. What evidence would resolve it: Ablation studies analyzing the gradient updates and feature distribution shifts in the segmentation network when trained on varying ratios of synthetic data.

### Open Question 2
Can the "blotchy" textural artifacts inherent in StyleGAN2-generated medical images be mitigated to prevent them from acting as noise during segmentation training? Basis in paper: [inferred] The paper notes that while generated images have good structures, they look "blotchy and blurry with low-quality" when zoomed in. Why unresolved: The authors measured global image quality using FID but did not investigate if these local textural errors negatively impacted the pixel-wise loss function of the U-Net. What evidence would resolve it: Comparison of segmentation performance using synthetic images with and without specific post-processing or loss functions designed to enhance textural fidelity.

### Open Question 3
Does the conditional generation pipeline (MC-SG) introduce a cumulative error from the mask generation stage that hinders the downstream segmentation task? Basis in paper: [inferred] The proposed MC-SG pipeline first generates a mask (M-SG) and then generates an image based on that mask (C-SG), yet it failed to outperform the baseline. Why unresolved: The paper evaluates the end-to-end performance but does not analyze if errors in the M-SG generated masks propagated through the C-SG generator, leading to misaligned image-mask pairs. What evidence would resolve it: Quantitative evaluation of the anatomical consistency (e.g., Hausdorff distance) between the M-SG generated masks and the corresponding C-SG generated images.

## Limitations
- Generated images were described as "blotchy and blurry," indicating fundamental quality issues in the GAN pipeline
- FID was used as the sole image quality metric, which may not correlate with segmentation-relevant features in medical imaging
- Study only evaluated 2D U-Net models, limiting generalizability to other architectures

## Confidence
- **High confidence**: The null result showing no segmentation improvement with synthetic augmentation is well-supported by experimental data across multiple datasets and variants
- **Medium confidence**: The architectural details of Conditional-StyleGAN2 are sufficiently specified for reproduction, though implementation nuances may affect results
- **Low confidence**: The theoretical explanation for why augmentation failed (distribution mismatch) is plausible but not definitively proven

## Next Checks
1. Implement and evaluate quality filtering of synthetic samples using a pretrained segmenter's IoU threshold before augmentation
2. Compare GAN augmentation against simpler spatial augmentation baselines (flip, rotate, shift) on the same datasets to establish relative efficacy
3. Test FID against a medical image-specific evaluation metric (e.g., using a medical segmentation-trained network) to assess feature-level similarity