---
ver: rpa2
title: 'Toward Autonomous Laboratory Safety Monitoring with Vision Language Models:
  Learning to See Hazards Through Scene Structure'
arxiv_id: '2602.00414'
source_url: https://arxiv.org/abs/2602.00414
tags:
- scene
- hazard
- graph
- laboratory
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops a synthetic dataset for laboratory hazard detection
  using vision-language models (VLMs). Textual safety scenarios are converted into
  scene graphs by an LLM, which then guide image generation of laboratory scenes.
---

# Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure

## Quick Facts
- arXiv ID: 2602.00414
- Source URL: https://arxiv.org/abs/2602.00414
- Reference count: 29
- Synthetic dataset for laboratory hazard detection using VLMs shows high accuracy with textual scene graphs but poor performance in vision-only settings

## Executive Summary
This paper addresses the challenge of autonomous laboratory safety monitoring by developing a synthetic dataset and benchmarking vision-language models (VLMs) for hazard detection. The authors propose a pipeline that converts textual safety scenarios into scene graphs using large language models, which then guide the generation of synthetic laboratory images. VLMs are evaluated across three modalities: text-only scene graphs, vision-only, and combined vision-plus-text inputs. The study reveals that while VLMs excel at reasoning with structured textual scene graphs, they struggle to extract hazard-relevant relationships directly from pixel data, highlighting a critical gap in visual reasoning capabilities.

## Method Summary
The authors generate a synthetic dataset of laboratory scenes by first creating textual safety scenarios, which are then converted into scene graphs using a large language model. These scene graphs guide image generation to produce synthetic laboratory images with associated hazards. VLMs are evaluated on three input modalities: text-only scene graphs, vision-only images, and combined vision-plus-text. A scene-graph-guided alignment approach is introduced, where VLMs first infer a scene graph from the image before reasoning, significantly improving visual-only hazard detection performance.

## Key Results
- VLMs achieve high accuracy in hazard detection when provided with textual scene graphs, but perform poorly in vision-only settings
- Scene-graph-guided alignment approach improves visual-only hazard detection by enabling VLMs to infer structured relationships from images
- The synthetic dataset comprises 1,207 aligned ⟨image, scene graph, ground truth⟩ triples across 362 scenarios, enabling controlled benchmarking

## Why This Works (Mechanism)
VLMs excel at structured reasoning when given explicit relational information in text form, but struggle to extract and reason over object relationships directly from pixels. By converting visual scenes into structured scene graphs first, the model can leverage its strong language reasoning capabilities even for visual inputs. This works because scene graphs explicitly encode spatial and functional relationships between objects, which are critical for identifying hazards that depend on object interactions (e.g., flammable chemicals near ignition sources).

## Foundational Learning
- Scene graphs: Structured representations encoding objects and their relationships; needed for capturing relational information critical to hazard detection; quick check: verify graph captures all relevant object interactions
- Vision-language models: Multimodal models processing both visual and textual inputs; needed for integrating image understanding with safety reasoning; quick check: confirm model handles both modalities effectively
- Synthetic data generation: Creating controlled datasets from textual descriptions; needed for systematic evaluation without privacy/ethical concerns; quick check: validate generated scenes match intended scenarios
- Alignment techniques: Methods to connect visual inputs with structured representations; needed to bridge the gap between raw pixels and relational reasoning; quick check: measure improvement in visual-only performance
- Hazard reasoning: Identifying unsafe conditions based on object relationships and context; needed for safety-critical applications; quick check: ensure model detects hazards requiring relational understanding

## Architecture Onboarding

### Component Map
LLM Scene Graph Generator -> Image Generator -> VLM Evaluator -> Scene-Graph-Guided Alignment Module

### Critical Path
Text scenario → LLM → Scene graph → Image generator → Synthetic image → VLM (with/without scene graph guidance) → Hazard detection

### Design Tradeoffs
- Synthetic vs real data: Synthetic enables controlled generation and privacy preservation but may not capture real-world complexity
- Scene graph generation: Automated scene graph creation from text is efficient but may miss nuanced visual details
- Alignment approach: Scene-graph-guided alignment improves visual performance but adds computational overhead

### Failure Signatures
- VLM fails to detect hazards requiring understanding of object relationships when given only visual input
- Generated synthetic images lack realism or miss critical safety details
- Scene graph extraction from images is incomplete or incorrect, leading to reasoning errors

### First 3 Experiments
1. Evaluate VLM performance on text-only scene graphs vs vision-only inputs on the synthetic dataset
2. Test scene-graph-guided alignment approach on real laboratory images to assess generalizability
3. Compare synthetic dataset coverage with real laboratory safety scenarios to identify gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic dataset may not capture the full complexity and variability of real laboratory environments
- High accuracy with textual scene graphs may not translate to effective real-world hazard detection
- Scene-graph-guided alignment performance gains may not generalize across different laboratory settings or hazard types
- Temporal aspects of safety monitoring, such as detecting dynamic hazards, are not addressed

## Confidence
- **High** confidence in dataset creation methodology and controlled benchmarking results
- **Medium** confidence in practical deployment for real-world laboratory safety monitoring
- **Low** confidence in scene-graph-guided alignment approach's generalizability without further validation

## Next Checks
1. Evaluate VLMs on a real-world laboratory image dataset to assess performance degradation compared to synthetic scenes
2. Test scene-graph-guided alignment approach across multiple laboratory domains to verify robustness to different visual contexts
3. Conduct user studies with safety professionals to determine if detected hazards align with expert assessments of real laboratory risks