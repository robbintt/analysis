---
ver: rpa2
title: Explainable Information Retrieval in the Audit Domain
arxiv_id: '2507.03479'
source_url: https://arxiv.org/abs/2507.03479
tags:
- information
- https
- search
- retrieval
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a gap in explainable information retrieval
  (XIR) research by focusing on the audit domain, where auditors must analyze complex
  financial data from multiple sources to detect inconsistencies and fraud. It argues
  that XIR systems can enhance auditors' efficiency, accuracy, and trust by providing
  transparent and interpretable search results.
---

# Explainable Information Retrieval in the Audit Domain

## Quick Facts
- arXiv ID: 2507.03479
- Source URL: https://arxiv.org/abs/2507.03479
- Reference count: 40
- Primary result: Identifies gap in XIR research for audit domain, proposes knowledge graphs and role-tailored explanations to improve efficiency, accuracy, and trust.

## Executive Summary
This paper identifies a critical gap in explainable information retrieval (XIR) research by focusing on the audit domain, where auditors must analyze complex financial data from multiple sources to detect inconsistencies and fraud. The authors argue that XIR systems can enhance auditors' efficiency, accuracy, and trust by providing transparent and interpretable search results. They outline key research directions including understanding auditor information needs, designing effective explanations, and developing interpretable knowledge representations using knowledge graphs. Major challenges include accessing realistic audit data due to confidentiality, gathering realistic information needs in specialized domains, and establishing appropriate ground truth explanations for evaluation.

## Method Summary
The paper proposes a conceptual framework for XIR in auditing that leverages knowledge graphs to represent entity relationships across financial documents, enabling traversal-based explanations. The approach involves query understanding modules to interpret auditor information needs, retrieval engines for financial documents and SAP exports, and explanation generators that produce role-appropriate, multi-modal outputs (textual summaries, visualizations, highlights). The method requires building or accessing anonymized audit datasets with sufficient fidelity, designing explanation formats matched to task complexity, and establishing ground truth explanation evaluation methodologies. No specific datasets, metrics, or training procedures are provided, making this primarily a research agenda rather than an implemented system.

## Key Results
- Knowledge graphs can encode financial document relationships to enable interpretable evidence chains
- Multi-modal explanations tailored to user expertise levels may improve auditor efficiency and trust
- XIR systems could mitigate trust erosion from LLM-based conversational agents that fabricate references
- Major challenges include accessing realistic audit data due to confidentiality and establishing ground truth explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graphs may support explainable retrieval in audit by explicitly representing entity relationships across financial documents.
- Mechanism: Graph structures encode dependencies (e.g., revenue entries → sales contracts → delivery records → approvals), enabling traversal-based explanations that show how a figure is supported by underlying evidence chains.
- Core assumption: Auditors will find relationship-based navigation more interpretable than ranked list outputs; the paper does not test this empirically.
- Evidence anchors:
  - [abstract] "representing audit-related knowledge effectively" identified as a key challenge
  - [section 2.3] "knowledge graphs are well-suited for this, as their structure inherently conveys relationships between entities"
  - [corpus] Weak direct evidence — neighbor papers discuss dense retrieval and conversational search but not audit-specific knowledge graphs
- Break condition: If financial document relationships cannot be reliably extracted and normalized into graph form without prohibitive manual effort, this approach fails to scale.

### Mechanism 2
- Claim: Multi-modal explanations tailored to user expertise levels may improve auditor efficiency and trust, though this remains untested.
- Mechanism: Junior auditors receive detailed textual evidence (e.g., specific SAP export inconsistencies); managers receive visual overviews linking anomalies across reports. Explanation modality matches task complexity and user role.
- Core assumption: Information need complexity correlates with preferred explanation format; the paper references cooking domain findings as analogous but provides no audit-specific validation.
- Evidence anchors:
  - [abstract] "multi-modal explanations tailored to different user roles (e.g., junior auditors vs. managers)" proposed as research direction
  - [section 2.2] Example given: expense verification → short textual summary; revenue recognition → visual dependency graph
  - [section 3.3] "different user groups might prefer different explanation modalities"
  - [corpus] No corpus evidence for audit-specific multi-modal explanations
- Break condition: If explanation modality preferences do not generalize across audit firms, cultures, or task types, single-format explanations may be more practical.

### Mechanism 3
- Claim: XIR systems could mitigate trust erosion caused by LLM-based conversational agents that fabricate references, by making retrieval sources transparent.
- Mechanism: Instead of trusting generated citations, users inspect the actual retrieval process — which documents matched, how they were ranked, and why — enabling independent verification.
- Core assumption: Auditors will invest time in examining explanations rather than accepting fast but opaque outputs; behavioral validation is not provided.
- Evidence anchors:
  - [abstract] "often generate misleading or fabricated references. This undermines trust"
  - [section 1] "references are frequently fabricated or misleading [45], which undermines the trustworthiness of such agents"
  - [corpus] Neighbor papers on conversational search (e.g., Dense Passage Retrieval) address effectiveness but not explainability-driven trust repair
- Break condition: If explanation overhead slows workflows significantly without demonstrable accuracy gains, auditors may revert to traditional methods.

## Foundational Learning

- Concept: **Explainable Information Retrieval (XIR)**
  - Why needed here: Core framing of the paper — distinguishes transparent retrieval from black-box LLM outputs.
  - Quick check question: Can you name two XIR techniques mentioned in the paper (e.g., query interpretation explanation, ranking explanation)?

- Concept: **Knowledge Graphs for Retrieval**
  - Why needed here: Proposed architecture for representing audit document relationships; understanding entity-linking basics is prerequisite.
  - Quick check question: How would a knowledge graph represent the relationship between a general ledger entry and its supporting contracts?

- Concept: **Information Need Taxonomies**
  - Why needed here: The paper argues audit information needs vary in complexity (fact-checking vs. investigative); designing explanations requires classifying these needs.
  - Quick check question: Give an example each of a simple and complex information need in auditing as described in the paper.

## Architecture Onboarding

- Component map:
  - Query understanding module → interprets auditor information needs (unbuilt)
  - Retrieval engine → fetches relevant financial documents/tables (domain-specific, not generic web search)
  - Knowledge graph layer → encodes entity relationships for traversal-based explanations (proposed)
  - Explanation generator → produces role-appropriate, multi-modal outputs (text, visualizations, highlights)
  - Evaluation framework → ground truth explanation labeling (currently undefined)

- Critical path:
  1. Collect realistic auditor information needs (blocked by domain access constraints)
  2. Build or access anonymized audit dataset with sufficient fidelity
  3. Design explanation formats matched to task complexity
  4. Establish ground truth explanation evaluation methodology

- Design tradeoffs:
  - Real data vs. anonymized/synthetic: Real data is confidential; anonymized data may lose fidelity for fraud detection scenarios.
  - Explanation detail vs. speed: Detailed explanations improve transparency but slow review cycles.
  - Role-specific vs. universal explanations: Tailored explanations require user modeling; universal explanations are simpler but may under-serve some roles.

- Failure signatures:
  - Explanations ignored by auditors (too verbose, not actionable)
  - Knowledge graph construction costs exceed benefit
  - Anonymized datasets fail to surface realistic fraud patterns
  - Ground truth labels for "good explanations" show low inter-annotator agreement

- First 3 experiments:
  1. Interview 5–10 auditors to catalog information need types and preferred explanation formats (validates Section 2.1 assumptions).
  2. Construct a small knowledge graph from synthetic SAP-like data; test whether graph traversal yields coherent evidence chains for mock queries.
  3. Run a within-subjects study comparing textual vs. visual explanations for a sample audit task; measure task completion time and self-reported trust.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What types of information needs exist in the audit domain, and how should explanations be tailored to each type (e.g., fact-checking queries vs. complex investigative queries)?
- Basis in paper: [explicit] "Future research needs to investigate in detail the type of information needs that can occur and how these are best supported through appropriate explanations."
- Why unresolved: The audit domain is a closed, expert-driven environment where logging real user interactions or relying on crowdsourcing is difficult, unlike open domains such as web or cooking search.
- What evidence would resolve it: A taxonomy of audit information needs derived from interviews with auditors or analysis of real audit query logs, paired with user studies evaluating explanation formats for each need type.

### Open Question 2
- Question: How do explanation format preferences differ between user roles (e.g., junior auditors vs. managers), and can a single XIR system effectively serve both groups?
- Basis in paper: [explicit] "A junior auditor might need detailed textual evidence pointing to inconsistencies in SAP exports, while a manager might prefer a visual overview linking anomalies across reports."
- Why unresolved: Explanation quality is context-dependent, and different user groups may prefer different modalities, but the opacity of existing systems makes it hard to determine what works for each role.
- What evidence would resolve it: Comparative user studies with junior auditors and managers using multi-modal explanations (textual summaries vs. visual overviews) on identical audit tasks, measuring task completion, accuracy, and subjective trust.

### Open Question 3
- Question: Can anonymized audit datasets retain sufficient fidelity to support realistic XIR research, and what data elements must be preserved?
- Basis in paper: [explicit] "It is crucial to ensure that anonymised datasets still retain enough fidelity to reflect real-world scenarios."
- Why unresolved: Financial data is highly confidential, and companies may be unwilling or legally unable to share real-world data, forcing reliance on anonymized or synthetic alternatives.
- What evidence would resolve it: Benchmarking studies comparing retrieval and explanation performance on anonymized datasets vs. proprietary real-world audit data (under NDA), with fidelity metrics measuring preservation of task-relevant structures (e.g., approval workflows, expense categories).

### Open Question 4
- Question: How can appropriate ground truth explanations be established for evaluating XIR systems in the audit domain?
- Basis in paper: [explicit] "A major challenge lies in providing appropriate ground truth explanations for evaluation."
- Why unresolved: Unlike ranking tasks where relevance judgments can be crowdsourced, explanations are subjective and role-dependent, and no established evaluation frameworks exist for audit-specific explanations.
- What evidence would resolve it: Development of evaluation protocols combining expert-annotated reference explanations with user-centric metrics (e.g., simulatability, counterfactual robustness) validated through auditor feedback.

## Limitations

- No empirical validation or datasets provided; methods remain theoretical
- Knowledge graph construction feasibility and scalability for audit documents untested
- Role-tailored explanation effectiveness not validated through user studies
- Ground truth explanation evaluation methodology undefined

## Confidence

- **High Confidence**: The need for explainable IR in audit is well-founded; auditors do require transparent search systems, and LLM hallucination is a documented trust problem.
- **Medium Confidence**: Knowledge graphs are theoretically well-suited for representing audit entity relationships, but no empirical evidence demonstrates their effectiveness in this domain.
- **Low Confidence**: Multi-modal, role-tailored explanations will improve auditor efficiency and trust without slowing workflows; this is asserted but not validated.

## Next Checks

1. Conduct 5-10 semi-structured interviews with auditors to validate the information need taxonomy and preferred explanation formats before designing XIR systems.
2. Build a prototype knowledge graph from anonymized or synthetic financial data and test whether graph traversal produces coherent, useful evidence chains for sample audit queries.
3. Run a small-scale comparative study (within-subjects) measuring task completion time and trust ratings for textual vs. visual explanations on a sample audit task.