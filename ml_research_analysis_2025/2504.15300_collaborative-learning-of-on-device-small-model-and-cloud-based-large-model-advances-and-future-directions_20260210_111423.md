---
ver: rpa2
title: 'Collaborative Learning of On-Device Small Model and Cloud-Based Large Model:
  Advances and Future Directions'
arxiv_id: '2504.15300'
source_url: https://arxiv.org/abs/2504.15300
tags:
- learning
- mobile
- large
- small
- on-device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey explores collaborative learning between on-device small
  models and cloud-based large models, addressing challenges like latency, cost, personalization,
  and privacy in traditional cloud-centric paradigms. The framework spans hardware,
  system, algorithm, and application layers, categorizing collaboration into data-based,
  feature-based, and parameter-based methods.
---

# Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions

## Quick Facts
- arXiv ID: 2504.15300
- Source URL: https://arxiv.org/abs/2504.15300
- Reference count: 40
- Primary result: Framework for collaborative learning between on-device small models and cloud-based large models, addressing latency, cost, personalization, and privacy challenges

## Executive Summary
This survey explores collaborative learning between resource-constrained on-device small models and resource-rich cloud-based large models, addressing the limitations of traditional cloud-centric paradigms. The framework spans hardware, system, algorithm, and application layers, categorizing collaboration into data-based, feature-based, and parameter-based methods. Key problems include designing efficient task allocation, knowledge transfer, and model enhancement while ensuring low latency and privacy. Recent advances cover automated compilers, federated learning frameworks, and specialized engines for large generative models. Real-world applications include recommender systems, livestreaming, and intelligent assistants.

## Method Summary
The survey provides a comprehensive taxonomy of collaborative learning approaches without proposing a single unified algorithm. Methods are categorized into data-based (query routing), feature-based (model splitting/early exiting), and parameter-based (federated learning, distillation). The survey emphasizes the importance of user-level weighted metrics for evaluating non-IID data distributions and provides datasets like FEMNIST, Sentiment140, and MovieLens-1M as suitable testbeds. While specific architectural configurations are not defined, the survey frequently cites Knowledge Distillation and Federated Averaging as foundational techniques for collaboration.

## Key Results
- Framework spans hardware, system, algorithm, and application layers for comprehensive collaborative learning
- Categorizes collaboration methods into data-based, feature-based, and parameter-based approaches
- Identifies open research directions including theoretical guarantees, new collaboration frameworks, and ubiquitous deployment scenarios
- Real-world applications demonstrated in recommender systems, livestreaming, and intelligent assistants

## Why This Works (Mechanism)
The collaborative learning framework works by leveraging the complementary strengths of small on-device models (privacy, low latency) and large cloud models (computational power, general knowledge). Through data-based methods like query routing, feature-based approaches like model splitting, and parameter-based techniques like federated learning, the system achieves optimal task allocation. Knowledge distillation enables the transfer of cloud model knowledge to device models while maintaining privacy, and federated learning allows collaborative training without centralizing sensitive data.

## Foundational Learning
- **Non-IID Data Distribution**: Essential for understanding why global metrics fail in collaborative learning - devices have unique data distributions that require user-level evaluation
  - *Why needed*: Standard accuracy metrics don't capture performance on heterogeneous user data
  - *Quick check*: Compare user-level vs global metrics on partitioned datasets
- **Knowledge Distillation**: Core technique for transferring knowledge from large to small models
  - *Why needed*: Enables device models to benefit from cloud model expertise without raw data transfer
  - *Quick check*: Measure KL divergence between teacher and student model outputs
- **Federated Averaging**: Standard framework for distributed model training across devices
  - *Why needed*: Allows collaborative learning without centralizing sensitive user data
  - *Quick check*: Monitor convergence speed with varying numbers of participating devices
- **Query Routing**: Decision mechanism for choosing between device and cloud inference
  - *Why needed*: Optimizes latency and resource usage by routing complex queries to cloud
  - *Quick check*: Measure routing accuracy vs latency trade-offs

## Architecture Onboarding

**Component Map**: Device Small Model -> Collaboration Engine -> Cloud Large Model -> Feedback Loop -> Device Model Update

**Critical Path**: Data Input → Device Model Inference → Routing Decision → Cloud Model Fallback (if needed) → Knowledge Transfer → Local Model Update

**Design Tradeoffs**: Privacy vs Performance (federated learning reduces privacy but may slow convergence), Latency vs Accuracy (routing complex queries to cloud improves accuracy but increases latency), Personalization vs Generalization (local adaptation improves user experience but may reduce general capabilities)

**Failure Signatures**: 
- Large gap between local validation and global test accuracy indicates non-IID data challenges
- Degraded performance on global validation set during local fine-tuning suggests catastrophic forgetting
- High communication overhead or slow convergence indicates inefficient federated averaging parameters

**First Experiments**:
1. Implement basic Knowledge Distillation: Train cloud model on global data, then distill to device model on local data
2. Run Federated Averaging: Simulate multiple devices with partitioned data, aggregate weight updates
3. Build Query Router: Create simple threshold-based system to route between device and cloud inference

## Open Questions the Paper Calls Out
None

## Limitations
- Specific architectural configurations for small vs large model pairs are not defined, preventing exact replication of cited studies
- System-level details like network latency simulation and battery constraints are abstracted away
- No specific algorithm implementations or hyperparameter recommendations provided for each collaboration category

## Confidence
- Survey provides comprehensive framework: High
- Taxonomy of collaboration methods is well-defined: High
- Specific architectural details and hyperparameters: Low
- System-level implementation guidance: Medium

## Next Checks
1. Replicate user-level weighted metric evaluation on FEMNIST dataset partitioned by writer ID
2. Implement and test Knowledge Distillation baseline between a standard Transformer (cloud) and DistilBERT (device)
3. Simulate Federated Averaging with 10-100 device partitions using LEAF benchmark datasets