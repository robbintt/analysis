---
ver: rpa2
title: 'ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled
  Tuning'
arxiv_id: '2510.10071'
source_url: https://arxiv.org/abs/2510.10071
tags:
- layers
- layer
- importance
- general
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ADEPT addresses catastrophic forgetting in continual pretraining
  of large language models by leveraging functional specialization within the network.
  It uses a two-stage approach: first, it selectively duplicates the least general-knowledge-critical
  layers to expand capacity without disrupting existing knowledge; second, it decouples
  parameters within expanded layers and assigns adaptive learning rates inversely
  to their general-domain importance.'
---

# ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning

## Quick Facts
- arXiv ID: 2510.10071
- Source URL: https://arxiv.org/abs/2510.10071
- Reference count: 40
- Key outcome: ADEPT outperforms full-parameter pretraining by up to 5.58% on target-domain benchmarks and 5.76% on general-domain tasks while tuning only 15% of parameters and using less than 50% of the training time.

## Executive Summary
ADEPT addresses catastrophic forgetting in continual pretraining of large language models by leveraging functional specialization within the network. It uses a two-stage approach: first, it selectively duplicates the least general-knowledge-critical layers to expand capacity without disrupting existing knowledge; second, it decouples parameters within expanded layers and assigns adaptive learning rates inversely to their general-domain importance. Experiments show ADEPT achieves superior performance on both target-domain and general-domain benchmarks while tuning only a fraction of parameters and requiring less training time than full-parameter pretraining.

## Method Summary
ADEPT is a two-stage continual pretraining framework that selectively expands model capacity and dynamically tunes parameters to inject domain knowledge while preserving general competencies. Stage 1 involves probing layer importance for general knowledge, selecting the least critical layers, and duplicating them via identity copy with zero-initialized outputs. Stage 2 partitions expanded layers into semantic units, estimates their importance using gradient-based methods, and assigns adaptive learning rates inversely proportional to importance. The framework trains only on expanded layers with periodically updated importance scores, enabling efficient domain adaptation without catastrophic forgetting.

## Key Results
- Outperforms full-parameter pretraining by up to 5.58% on target-domain benchmarks (GSM8K, CMB)
- Improves general-domain performance by 5.76% on MMLU and CMMLU compared to full-parameter tuning
- Achieves these gains while tuning only 15% of parameters and using less than 50% of training time
- Shows effectiveness across diverse domains: mathematical reasoning, code generation, and biomedical tasks

## Why This Works (Mechanism)

### Mechanism 1: General-Competence Guided Selective Layer Expansion
Expanding only layers least critical for general-domain competencies introduces domain-adaptive capacity while minimizing interference with pre-existing knowledge. Importance probing identifies layers with lowest sensitivity to general knowledge loss. Selected layers are duplicated via identity copy with zero-initialized output projections, preserving forward pass (function-preserving initialization). New capacity is thus injected where the model has the most "slack" to adapt.

### Mechanism 2: Adaptive Unit-Wise Decoupled Tuning
Within expanded layers, assigning asymmetric learning rates inversely proportional to unit importance protects general-critical parameters while enabling aggressive adaptation of domain-relevant parameters. Each expanded layer is partitioned into semantic units (attention projections, MLP, LayerNorm). Gradient-based importance is computed for each unit. Learning rate is scaled as lr_U = 2·(1−I_unit)·lr_base, dynamically updated during training.

### Mechanism 3: Reduced Parameter Entanglement via Layer Expansion + Decoupling
Combining selective expansion with unit-wise decoupling creates a cleaner separation between domain-specific and general representations compared to direct tuning of original parameters. Expansion provides a "blank slate" (new, untrained parameter space) that is not entangled with pre-trained general knowledge. Decoupled tuning further ensures that even within this new space, general-critical subspaces are updated conservatively.

## Foundational Learning

- **Catastrophic Forgetting in CPT**
  - Why needed here: Central problem ADEPT addresses; understanding that aggressive updates overwrite general knowledge is essential.
  - Quick check question: Can you explain why full-parameter CPT often degrades general benchmark performance?

- **Functional Specialization in LLMs**
  - Why needed here: ADEPT exploits the heterogeneous importance of layers and units; understanding this motivates targeted expansion.
  - Quick check question: What does the pilot study reveal about which layers are most critical for general competencies?

- **Gradient-Based Importance Estimation**
  - Why needed here: Core technique for identifying important layers/units; requires understanding of Taylor approximations and gradient sensitivity.
  - Quick check question: How is unit importance I_unit computed (Eq. 5), and what does a high value indicate?

## Architecture Onboarding

- **Component map:** General Competence Detection Corpus → Layer Importance Probing → Layer Selection → Identity-Copy Expansion → Unit Partitioning → Gradient-Based Importance → Adaptive Learning Rate Assignment → Dynamic CPT Training

- **Critical path:**
  1. Construct/sourced general competence corpus (e.g., MMLU/CMMLU dev sets or high-quality pretrain data).
  2. Run layer importance probing (mask layers, compute loss increase).
  3. Select and expand k layers (typically 4 for well-aligned models, more for weaker baseline in target domain).
  4. Partition expanded layers into units and compute initial importance scores.
  5. Run CPT on domain corpus with adaptive, unit-wise learning rates, updating importance periodically.

- **Design tradeoffs:**
  - Number of expanded layers (k): Too few → insufficient capacity; too many → optimization instability/interference. Paper suggests k=4 for Qwen series, more for LLaMA3-8B on Chinese tasks (Appendix G).
  - Importance estimation method: Masking out vs. gradient accumulation vs. Fisher information (Appendix E). Masking out performed best in tests but is more expensive.
  - Importance re-computation frequency: Too infrequent → stale rates; too frequent → overhead. Paper uses every 500 iterations.
  - Probing corpus quality: Benchmarks (MMLU/CMMLU) yield slightly better results than pretrain data, but pretrain data is more accessible and still effective (Appendix H).

- **Failure signatures:**
  - Significant drop in general benchmarks (MMLU/CMMLU) after CPT → over-adaptation or expansion of important layers.
  - Poor domain performance despite expansion → insufficient k, learning rates too low for domain-critical units, or poor probing corpus.
  - Activation overlap between general and domain data in expanded layers → incomplete decoupling; may need more layers or better unit partitioning.

- **First 3 experiments:**
  1. Run ADEPT on a small domain subset (e.g., 1B tokens) with default k=4, compare general and domain benchmarks vs. vanilla PT-Full and LLaMA-Pro baselines.
  2. Ablation: Test ADEPT without Stage 1 (direct decoupling) and without Stage 2 (uniform learning rates on expanded layers) to validate both components.
  3. Sensitivity: Vary k (1, 2, 4, 8 layers) and observe performance trade-offs on general vs. domain tasks to find optimal capacity allocation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-linear mapping functions for learning rates yield superior preservation of general competencies compared to the current linear scaling?
- Basis in paper: The conclusion explicitly states future work should focus on "designing more sophisticated learning rate strategies beyond linear mapping."
- Why unresolved: The paper implements a simple linear inverse mapping (lr ∝ 1-I) to adjust learning rates but does not explore curvature or threshold-based mappings that might better handle outlier units.
- What evidence would resolve it: Experiments comparing ADEPT with non-linear functions (e.g., exponential decay or step functions based on importance thresholds) on general-domain benchmarks like MMLU and CMMLU.

### Open Question 2
- Question: How can the framework be adapted to measure parameter importance dynamically during training rather than relying on static pre-training probing?
- Basis in paper: The authors propose exploring "better dynamic and real-time methods for measuring parameter importance during training" in the future works section.
- Why unresolved: Current ADEPT relies on a fixed "General Competence Detection Corpus" and probing phase before training begins, which assumes static importance and adds a preprocessing step.
- What evidence would resolve it: A modified ADEPT version that updates importance scores online without a separate probing corpus, showing comparable or superior forgetting prevention efficiency.

### Open Question 3
- Question: What specific heuristics can reliably determine the optimal number of layers to expand (k) for a given model architecture and target domain?
- Basis in paper: Appendix G demonstrates that the optimal number of expanded layers varies significantly between model families (e.g., 4 for Qwen vs. 16 for Llama3), yet the paper relies on manual search.
- Why unresolved: The paper establishes that k is a critical hyperparameter but offers no automated method to set it based on model capacity or domain complexity.
- What evidence would resolve it: A predictive formula or algorithm that selects k based on architectural features or domain distance, validated against exhaustive search results.

## Limitations
- Probing Corpus Sensitivity: Performance depends on quality and representativeness of general competence probing corpus.
- Importance Estimation Reliability: Layer and unit importance estimates may be noisy or computationally expensive.
- Generalization Across Architectures: Experiments primarily conducted on Qwen and LLaMA3 architectures; effectiveness may vary for other models.

## Confidence
- **High Confidence:** The core mechanism of selective layer expansion guided by general competence importance is well-supported by experimental results and ablation studies.
- **Medium Confidence:** The adaptive unit-wise decoupled tuning approach is effective, but sensitivity to importance estimation quality and unit partitioning is not fully explored.
- **Medium Confidence:** The claim that ADEPT outperforms baselines (LLaMA-Pro, TaSL, Parenting) is supported, but relative improvements depend on specific domain and task.

## Next Checks
1. **Probing Corpus Ablation:** Systematically test ADEPT with different probing corpora (benchmark dev sets, pretraining data, domain data) to quantify impact on general and domain performance.

2. **Importance Estimation Stability:** Evaluate stability of layer and unit importance scores across multiple model checkpoints and domains. Test alternative importance estimation methods (e.g., Fisher information) for robustness.

3. **Architecture Generalization:** Apply ADEPT to diverse model architectures (e.g., Mistral, Gemma) and scales (1B, 7B, 70B) to assess generalizability of optimal k and overall approach.