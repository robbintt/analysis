---
ver: rpa2
title: 'Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation,
  and Safety'
arxiv_id: '2509.12936'
source_url: https://arxiv.org/abs/2509.12936
tags:
- safety
- alignment
- diversity
- conciseness
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified evaluation framework for assessing
  large language model alignment methods across five key dimensions: factuality, safety,
  conciseness, proactivity, and diversity. The authors compare PPO, DPO, ORPO, and
  KTO alignment techniques using both in-distribution and out-of-distribution datasets,
  employing an LLM-as-Judge approach validated through human evaluation.'
---

# Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety

## Quick Facts
- arXiv ID: 2509.12936
- Source URL: https://arxiv.org/abs/2509.12936
- Reference count: 30
- No single alignment method dominates across all dimensions; method selection should be guided by specific deployment requirements

## Executive Summary
This paper proposes a unified evaluation framework for assessing large language model alignment methods across five key dimensions: factuality, safety, conciseness, proactivity, and diversity. The authors compare PPO, DPO, ORPO, and KTO alignment techniques using both in-distribution and out-of-distribution datasets, employing an LLM-as-Judge approach validated through human evaluation. Results show DPO and KTO excel in factual accuracy, PPO and DPO lead in safety, and PPO best balances conciseness with proactivity. The study reveals significant trade-offs between alignment objectives, with ORPO showing the weakest generalization performance. These findings highlight that method selection should be guided by specific deployment requirements rather than seeking a universally superior approach.

## Method Summary
The study evaluates alignment methods by training LLaMA-7B models using different techniques (PPO, DPO, ORPO, KTO, BoN, SFT) on combined instruction-following and safety data. Each method is assessed using an LLM-as-Judge approach with LLaMA-3.1-70B, which evaluates responses across five dimensions: factuality, safety, conciseness, proactivity, and diversity. The evaluation includes both in-distribution datasets (AlpacaFarm, PKU-SafeRLHF) and out-of-distribution datasets (AlpacaEval, Sequential Instructions, TLDR, BeaverTails, DataAdvisor). Generalization gaps are computed as the difference between in-distribution and out-of-distribution performance. Diversity is measured using SentBERT, NLI entailment, and EAD metrics.

## Key Results
- DPO and KTO excel in factual accuracy across datasets
- PPO and DPO lead in safety performance, with ORPO showing weakest OOD safety generalization
- PPO best balances conciseness with proactivity among the methods tested
- Significant trade-offs exist between alignment objectives, with no method dominating all dimensions
- ORPO's supervised component causes weak generalization, particularly for safety metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different alignment methods encode implicit trade-offs between competing objectives, with no single method dominating across all dimensions.
- Mechanism: Each optimization objective (preference margin maximization, KL-constrained policy updates, odds ratio loss) shapes the learned policy's behavior differently—PPO's KL penalty preserves reference model characteristics while DPO's implicit reward learning may bias toward longer, more detailed responses.
- Core assumption: The observed performance differences stem from the optimization objectives themselves rather than implementation details or hyperparameter choices.
- Evidence anchors:
  - [abstract] "DPO and KTO excel in factual accuracy, PPO and DPO lead in safety, and PPO best balances conciseness with proactivity"
  - [section 6] "DPO provides significantly stronger generalisation in terms of proactivity compared to other methods, which is linked to its very low score for conciseness"
  - [corpus] Related work (arXiv:2512.00778) suggests DPO and PPO have deeper algorithmic connections than commonly assumed, supporting the idea that trade-offs emerge from objective formulation.
- Break condition: If hyperparameter sweeps across methods show overlapping performance profiles, trade-offs may be tuning artifacts rather than inherent to methods.

### Mechanism 2
- Claim: LLM-as-Judge evaluation with a substantially larger model provides reliable multi-dimensional assessment when validated against human judgments.
- Mechanism: Using LLaMA-3.1-70B (~10x parameters, 15T vs 1.4T training tokens) to evaluate 7B models reduces self-preference bias; structured criteria with few-shot examples enforce consistent scoring; win-tie-rate metric mitigates position bias.
- Core assumption: The human validation sample (1,920 pairs) generalizes to the full benchmark, and high agreement on safety (98.4%) and proactivity (84.8%) indicates reliable automated scoring.
- Evidence anchors:
  - [section 4.1] "This judge model is substantially larger (approx. 10x parameters) and was pre-trained on a significantly larger corpus... minimizing the risk of self-preference"
  - [section 4.2] Table 1 reports overall human-model agreement of 81.0%
  - [corpus] No direct corpus validation of LLM-as-Judge reliability for this specific 5-dimension framework; related work typically validates on narrower criteria.
- Break condition: If lower agreement dimensions (conciseness at 63.2%) prove noisy enough to change method rankings, judge reliability becomes a limiting factor.

### Mechanism 3
- Claim: Generalization gap (Δgen = WTR_ID - WTR_OOD) reveals alignment method robustness, with ORPO showing weakest OOD safety generalization.
- Mechanism: ORPO's SFT component in its loss function may cause overfitting to training distribution; PPO and DPO's explicit preference modeling provides more robust safety boundaries under distribution shift.
- Core assumption: The OOD datasets (BeaverTails, DataAdvisor, TLDR) represent meaningful distributional shifts rather than just harder examples.
- Evidence anchors:
  - [section 6] "ORPO... showed the weakest generalisation ability among selected alignment methods. This effect may be attributed to the supervised component (SFT) in its loss function"
  - [section 4.3] Generalization gap equation defines Δgen explicitly
  - [corpus] Limited external validation; corpus neighbors focus on method comparisons rather than generalization measurement specifically.
- Break condition: If embedding similarity analysis (Table 16: OOD2 at 0.1503 similarity vs ID at 0.1338) shows OOD sets aren't truly out-of-distribution, the generalization findings weaken.

## Foundational Learning

- Concept: **RLHF Pipeline (SFT → Reward Model → PPO)**
  - Why needed here: All compared methods build on or modify this canonical pipeline; understanding baseline clarifies what each method changes.
  - Quick check question: Can you explain why PPO requires a KL penalty term while DPO does not?

- Concept: **Preference Optimization Objectives (DPO, KTO, ORPO)**
  - Why needed here: Each method's objective function determines its trade-off profile; the paper attributes ORPO's weak generalization to its SFT loss component.
  - Quick check question: How does KTO's use of binary desirability signals differ from DPO's pairwise preference margins?

- Concept: **Evaluation Metrics: FAR, FRR, Win-Tie-Rate**
  - Why needed here: Safety assessment uses adapted authentication metrics (False Acceptance/Rejection Rate); understanding these is essential for interpreting results.
  - Quick check question: In safety evaluation, what does a high FRR indicate about an aligned model's behavior?

## Architecture Onboarding

- Component map:
  Base model (LLaMA-7B) → SFT training on AlpacaFarm → Alignment method (PPO/DPO/KTO/ORPO) with combined IF+safety data → LLM-as-Judge evaluation with LLaMA-3.1-70B across 5 dimensions

- Critical path:
  1. Replicate SFT baseline using AlpacaFarm dataset (9,686 training examples)
  2. Train reward model on combined IF + safety preferences (essential for PPO and BoN)
  3. Apply each alignment method with documented hyperparameters (Table 5)
  4. Run LLM-as-Judge evaluation with structured prompt (Figure 6) on all test sets
  5. Compute generalization gaps and diversity metrics

- Design tradeoffs:
  - **Judge model selection**: 70B parameter judge reduces bias but increases evaluation cost; smaller judges may introduce systematic scoring errors
  - **Temperature settings**: T=0.1 vs T=1.0 affects diversity-conciseness trade-off (Figure 5); deployment requirements should guide selection
  - **Safety training mix**: Combined IF + safety data improves safety metrics but may reduce instruction-following diversity compared to IF-only training

- Failure signatures:
  - **ORPO safety collapse**: FAR increases dramatically on OOD safety datasets (0.074 ID → 0.501 OOD2-US at T=0.1); indicates overfitting to training distribution
  - **DPO verbosity**: Low conciseness scores across datasets; acceptable if proactive responses are prioritized
  - **SFT generalization gap**: Largest Δgen across most dimensions; expected baseline behavior

- First 3 experiments:
  1. **Validate judge reliability on your domain**: Sample 200 responses from your target distribution; have human annotators score against the 5 criteria; compare agreement rates to paper's 81% overall benchmark.
  2. **Ablate safety data proportion**: Train PPO and DPO with 0%, 25%, 50%, 100% of safety data mixed with IF data; plot safety vs diversity trade-off curves to find your optimal operating point.
  3. **Temperature sweep on OOD set**: Generate responses at T ∈ {0.1, 0.5, 1.0} for your preferred method; measure diversity-conciseness Pareto frontier to select deployment temperature.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the supervised fine-tuning (SFT) loss component within ORPO directly cause its observed weak safety generalization and high False Acceptance Rates (FAR) on out-of-distribution data?
- Basis in paper: [inferred] The authors note ORPO's weak generalization "may be attributed to the supervised component (SFT) in its loss function" and that they "did not observe benefit regarding model's safety in ORPO alignment."
- Why unresolved: The study evaluates ORPO as a monolithic method; it does not perform an ablation on the SFT loss term to prove it is the specific cause of the safety degradation.
- What evidence would resolve it: An ablation study of ORPO where the SFT loss term is removed or re-weighted, specifically measuring safety metrics (FAR/FRR) on OOD datasets like BeaverTails.

### Open Question 2
- Question: How does the ratio of safety-focused data to general instruction-following data influence the trade-off between factuality and safety generalization in alignment methods?
- Basis in paper: [explicit] The Limitations section states that "models were trained exclusively on an instruction-following dataset, supplemented with... safety prompts" and "additional analysis would still be valuable to better understand how the training data influences model performance metrics."
- Why unresolved: The current experimental setup utilizes a fixed combined dataset for all methods, preventing the isolation of training data composition effects from algorithmic effects.
- What evidence would resolve it: A comparative analysis where alignment methods are trained with varying ratios of safety-to-instruction data to observe the resulting shifts in the generalization gap (Δgen).

### Open Question 3
- Question: Do human evaluators verify the automated judgments for conciseness, given the low agreement (63.2%) observed between the LLM judge and human annotators for this specific dimension?
- Basis in paper: [explicit] The Limitations section notes that "performance evaluation in this study relies on LLM as a judge" and suggests "Adding human evaluations alongside automated judgments would enhance the reliability of the findings."
- Why unresolved: While the judge was validated, the human-model agreement for conciseness was significantly lower than for safety (98.4%) or proactivity (84.8%), leaving the reliability of specific conciseness claims less certain.
- What evidence would resolve it: An extended human evaluation study focusing specifically on the "Conciseness" dimension to validate the trends reported by the automated judge.

## Limitations

- LLM-as-Judge validation shows significant variation across dimensions, particularly for conciseness (63.2% agreement) which could affect method ranking reliability
- Generalizability findings rely on five OOD datasets that may not fully represent real-world distributional shifts users would encounter
- Evaluation focuses on LLaMA-7B models, and performance patterns may differ for larger or smaller base models

## Confidence

- **High confidence**: PPO and DPO safety performance comparison (98.4% human-LLM agreement on safety scoring); ORPO's weak OOD generalization pattern
- **Medium confidence**: Factuality and proactivity rankings (lower agreement scores, 73.5% and 84.8% respectively); diversity metric interpretations across temperature settings
- **Low confidence**: Conciseness evaluation reliability (63.2% agreement); the extent to which current OOD datasets capture realistic deployment scenarios

## Next Checks

1. **Domain-specific judge validation**: Replicate the human evaluation on 200+ responses from your target application domain to establish baseline LLM-judge reliability before relying on automated scoring
2. **Hyperparameter sensitivity analysis**: Conduct 3-point sweeps (low/medium/high) for KL penalty (PPO), learning rate (all methods), and temperature to map performance stability across the Pareto frontier
3. **OOS robustness testing**: Evaluate aligned models on genuinely out-of-scope prompts (e.g., novel programming tasks for general-purpose models) to verify that safety generalization holds beyond the paper's OOD datasets