---
ver: rpa2
title: Beat-Based Rhythm Quantization of MIDI Performances
arxiv_id: '2508.19262'
source_url: https://arxiv.org/abs/2508.19262
tags:
- information
- quantization
- beat
- performances
- rhythm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of converting expressive MIDI
  performances into metrically-aligned, human-readable scores through rhythm quantization.
  The core method employs a transformer-based model that integrates beat and downbeat
  information into a unified tokenized input representation, allowing the model to
  learn rhythmic structure by modeling timing deviations relative to an underlying
  metrical grid.
---

# Beat-Based Rhythm Quantization of MIDI Performances

## Quick Facts
- arXiv ID: 2508.19262
- Source URL: https://arxiv.org/abs/2508.19262
- Authors: Maximilian Wachter; Sebastian Murgul; Michael Heizmann
- Reference count: 7
- Primary result: 97.3% onset F1 score, 83.3% note value accuracy on ASAP piano dataset

## Executive Summary
This paper addresses the challenge of converting expressive MIDI performances into metrically-aligned, human-readable scores through rhythm quantization. The authors propose a transformer-based model that integrates beat and downbeat information into a unified tokenized input representation, enabling the model to learn rhythmic structure by modeling timing deviations relative to an underlying metrical grid. This beat-based preprocessing approach allows generalization across different time signatures without requiring explicit time signature tokens. The model was optimized and trained on piano and guitar performance datasets, incorporating data augmentation techniques such as pitch transposition and note deletion.

## Method Summary
The method employs a T5 transformer architecture that converts beat-relative note events into discrete tokens for input and output. Performance timing data and beat positions are transformed into a unified token sequence where the model learns to map observed onset positions to quantized score positions by modeling timing deviations relative to an underlying metrical grid. The model was trained on instrument-specific datasets (piano ASAP dataset, guitar Leduc dataset) with data augmentation including pitch transposition, note deletion, and duration noise. The preprocessing step normalizes timing to beat-relative positions, while the tokenizer converts these into discrete tokens representing pitch, onset-offset-from-beat, and duration category.

## Key Results
- Achieved 97.3% onset F1 score on ASAP piano dataset
- Attained 83.3% note value accuracy on the same dataset
- Surpassed state-of-the-art models in onset error rate (εonset = 12.30) on MUSTER metric

## Why This Works (Mechanism)

### Mechanism 1: Beat-Integrated Tokenization
Incorporating beat and downbeat information directly into the tokenized input representation improves the model's ability to learn rhythmic structure. Performance timing data and beat positions are transformed into a unified token sequence, allowing the model to learn to map observed onset positions to quantized score positions by modeling timing deviations relative to an underlying metrical grid rather than absolute time values.

### Mechanism 2: Time Signature Generalization Through Beat-Relative Encoding
Representing rhythmic positions relative to beats, rather than using explicit time signature tokens, enables generalization to unseen meters. By encoding note positions as offsets from beat boundaries within a fixed subdivision grid, the model abstracts away the global metric structure, allowing the same learned weights to apply across 4/4, 3/4, 6/8, and other time signatures without requiring time signature as an input feature.

### Mechanism 3: Instrument-Specific Fine-Tuning with Data Augmentation
Training on instrument-specific datasets with targeted augmentation improves quantization by capturing instrument-dependent timing characteristics. The base model is trained on piano, then adapted to guitar, with augmentation techniques including pitch transposition, note deletion, and duration noise to increase training diversity and robustness to minor timing perturbations.

## Foundational Learning

- **MIDI Representation (onset, offset, pitch, velocity, duration)**: Understanding which attributes are preserved, discarded, or transformed is essential for debugging quantization errors. Quick check: Given a MIDI note with onset=480 ticks, duration=960 ticks, in a PPQN=480 file, what is the notated quarter-note position and duration?

- **Beat Tracking vs. Downbeat Detection**: The model accepts beat and downbeat annotations as priors. Understanding the difference between beat-level (tactus) and downbeat-level (measure) annotations clarifies what the model receives and what it must infer. Quick check: In 4/4 time at 120 BPM, how many beats and how many downbeats occur in a 4-measure phrase?

- **Seq2Seq Transformer Architectures (T5 family)**: The model uses T5, an encoder-decoder transformer. Understanding token-to-token generation, teacher forcing, and beam search is required to modify inference behavior. Quick check: In a T5-style model, does the decoder attend to the encoder outputs, its own previous outputs, or both? What is the implication for autoregressive generation of quantized scores?

## Architecture Onboarding

- **Component map**: Raw MIDI + beat/downbeat annotations -> Preprocessing Module (normalizes to beat-relative positions) -> Tokenizer (converts to discrete tokens) -> T5 Encoder-Decoder Backbone (learned weights map input to output) -> Postprocessing/Detokenizer (converts back to MIDI/MusicXML) -> Quantized score

- **Critical path**: Beat annotation quality -> Tokenizer beat-alignment accuracy -> Encoder representation -> Decoder quantized-token prediction -> Detokenized score rhythmic accuracy. The beat integration step is the highest-leverage component; errors propagate through all downstream stages.

- **Design tradeoffs**: Onset accuracy vs. offset accuracy (prioritizes onset with εonset=12.30, struggles with offsets at εoffset=28.30), Metronome-dependent vs. beat-tracking-dependent inference (ground-truth beats vs. estimated beats), Subdivision depth vs. vocabulary size (finer grid improves resolution but increases token vocabulary).

- **Failure signatures**: 32nd note omission (short note values not detected), beat estimation drift (cumulative beat estimation errors cause systematic misalignment), score-performance misalignment in training (filtered dataset suggests unfiltered data would degrade performance).

- **First 3 experiments**: Beat Ablation Test (ground-truth beats vs. beat-tracker estimates vs. no beat information), Subdivision Resolution Sweep (train with different subdivision grids to identify minimum resolution for 32nd-note detection), Cross-Instrument Transfer (train on piano only, evaluate on guitar; train on guitar only, evaluate on piano).

## Open Questions the Paper Calls Out

- **Can the current beat-based tokenization approach be extended to effectively handle irregular meters and finer note values, such as 32nd notes?**: The authors state in the Summary that "Future work will explore... expanding to irregular meters and note values, aiming for even greater generalization." The current study focuses on standard meters and admits to a "current inability to detect 32nd notes," limiting the model's resolution for complex rhythmic structures.

- **Does incorporating explicit time signature tokens into the unified representation improve quantization accuracy compared to the current beat-based inference method?**: The Summary lists "incorporating explicit time signature modeling" as a specific avenue for future work. While the current model generalizes across time signatures without explicit tokens, it is undetermined if explicit modeling would reduce errors in ambiguous rhythmic patterns.

- **How can the model's architecture or tokenization be modified to significantly reduce offset error rates (εoffset) to match or exceed state-of-the-art performance?**: The Discussion notes that "offset accuracy remains challenging" and the model is surpassed by the End-to-End PM2S method (28.30 vs 23.84 error rate), largely due to the failure to detect shorter note values.

## Limitations

- Beat annotation quality significantly impacts performance, with degradation in non-metronomic performances where beat tracking is unreliable.
- Inability to detect 32nd notes and shorter durations represents a fundamental constraint on rhythmic resolution.
- Dataset filtering criteria for input-target alignment quality is not fully specified, creating uncertainty about generalization to real-world performance data.

## Confidence

- **Beat-Integrated Tokenization**: High confidence - clearly described mechanism with direct evidence and quantitative validation (εonset = 12.30, SOTA-beating).
- **Time Signature Generalization**: Medium confidence - supported primarily by internal ablation studies rather than extensive cross-time-signature validation.
- **Instrument-Specific Fine-Tuning**: Medium confidence - demonstrated adaptation to guitar but without sample size or statistical significance reporting.

## Next Checks

1. **Beat Ablation Test**: Run inference with three conditions - ground-truth beats (metronome), beat-tracker estimates, and no beat information. Measure onset F1 and MUSTER εonset/εoffset for each condition to isolate the contribution of beat information and establish performance bounds.

2. **Subdivision Resolution Sweep**: Train separate models with different subdivision grids (4th-note, 8th-note, 16th-note subdivisions). Evaluate note value accuracy to identify the minimum resolution required for reliable 32nd-note detection and understand the tradeoff between rhythmic resolution and vocabulary size.

3. **Cross-Instrument Transfer Experiment**: Train on piano only and evaluate on guitar performances; train on guitar only and evaluate on piano. Compare results to instrument-specific models to quantify the benefit of instrument-specific fine-tuning and test the generalization limits of the learned timing representations.