---
ver: rpa2
title: 'Decomposing Elements of Problem Solving: What "Math" Does RL Teach?'
arxiv_id: '2505.22756'
source_url: https://arxiv.org/abs/2505.22756
tags:
- problem
- solution
- student
- arxiv
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper decomposes mathematical problem-solving into three core\
  \ capabilities\u2014plan, execute, and verify\u2014to better assess what reinforcement\
  \ learning (RL) improves. Empirical analysis shows that GRPO mainly boosts execution\
  \ robustness on problems the model already knows, creating a \"temperature distillation\"\
  \ effect but not expanding coverage to new problems."
---

# Decomposing Elements of Problem Solving: What "Math" Does RL Teach?

## Quick Facts
- **arXiv ID**: 2505.22756
- **Source URL**: https://arxiv.org/abs/2505.22756
- **Reference count**: 40
- **Primary result**: GRPO improves execution robustness but not planning, hitting a "coverage wall" on new problems.

## Executive Summary
This paper proposes a decomposition of mathematical problem-solving into three core capabilities: Plan, Execute, and Verify. Through empirical analysis of GRPO on math datasets, it reveals that RL primarily improves execution robustness ("temperature distillation") without expanding the set of problems a model can solve ("coverage wall"). A synthetic solution-tree navigation task isolates these effects, showing that RL's impact is limited to reinforcing execution of known solution paths. The work highlights that RL alone cannot improve planning skills needed for novel problems, but suggests that with more diverse training data or simpler action spaces, RL could potentially overcome these limitations.

## Method Summary
The paper applies GRPO via the VERL framework to Qwen2.5-Instruct models (0.5B, 1.5B, 7B) trained on MATH datasets. It evaluates performance using Pass@K metrics and precision across temperatures, with automated annotation of solutions via GPT-4.1-mini to assess plan validity and execution correctness. A synthetic graph-navigation task with spurious correlations is used to isolate learning dynamics and validate the coverage wall phenomenon. The synthetic setup employs a 4-layer Transformer pre-trained on 200K sequences, then fine-tuned with GRPO on 128 RL sequences with Dirichlet-sampled transitions.

## Key Results
- GRPO improves execution robustness (temperature distillation) but doesn't expand coverage to new problems
- Coverage wall observed: RL-trained models struggle with fundamentally new problems despite improved rewards on training set
- Synthetic task shows data diversity and simpler action spaces can help RL overcome coverage wall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRPO primarily improves execution robustness on problems a model already partially solves ("temperature distillation"), without expanding its ability to solve fundamentally new problems.
- Mechanism: GRPO sharpens the probability distribution of correct solution traces by reducing reliance on spurious correlations learned during pre-training, making execution more consistent across different sampling temperatures.
- Core assumption: The "Plan" capability is largely fixed after pre-training, so RL can only reinforce the "Execute" capability for problems where a valid plan already exists.
- Evidence anchors: Abstract's "temperature distillation" observation; Section 4.3's hypothesis about spurious cue mitigation; related work on ArithmAttack.

### Mechanism 2
- Claim: RL fails to expand the set of problems a model can solve (coverage) because it does not improve the planning skills required for novel problems.
- Mechanism: Coverage is constrained by the set of problems for which the model has any valid solution plan. GRPO only reinforces execution of existing plans and cannot generate new plans for unsolved problems.
- Core assumption: Planning is a distinct capability that is harder to improve via RL than execution.
- Evidence anchors: Abstract's "coverage wall" discussion; Section 3.3's test set generalization failure; related PLAN-TUNING work.

### Mechanism 3
- Claim: The "coverage wall" can be overcome under specific conditions related to data diversity and problem complexity, as shown in a controlled synthetic task.
- Mechanism: A minimal synthetic environment reveals that RL can improve coverage if the model is exposed to more diverse training data, the problem's action space is simpler, or spurious correlations are reduced.
- Core assumption: The synthetic graph-navigation task accurately captures mathematical problem-solving dynamics.
- Evidence anchors: Abstract's synthetic setup insights; Section 5.3's data diversity findings; lack of direct corpus evidence for this mechanism.

## Foundational Learning

**Pass@K and Coverage**
- Why needed here: To distinguish between single-shot accuracy and the model's true problem-solving horizon
- Quick check question: If a model's Pass@1 is 40% but its Pass@64 is 85%, what does this suggest about its capabilities?

**GRPO (Group Relative Policy Optimization)**
- Why needed here: This is the specific RL algorithm analyzed in the paper
- Quick check question: How does GRPO differ from traditional RL algorithms like PPO in terms of reward modeling?

**Solution Tree Navigation**
- Why needed here: This is the core framework used to decompose problem-solving
- Quick check question: In this framework, what constitutes an "Execute" failure versus a "Plan" failure?

## Architecture Onboarding

**Component map**: Problem Solving Decomposition -> Evaluation Pipeline -> Synthetic Environment -> RL Training Loop

**Critical path**: The path flows from the decomposition framework (Plan/Execute/Verify) to empirical evaluation on real math benchmarks (revealing temperature distillation and the coverage wall), and finally to synthetic validation to isolate conditions for overcoming the wall.

**Design tradeoffs**:
- Synthetic vs. Real Data: Synthetic offers control but may oversimplify mathematical reasoning
- Automated vs. Human Annotation: GPT annotation is scalable but introduces model-in-the-loop biases
- GRPO Focus: Results may not generalize to all RL algorithms

**Failure signatures**:
- Spurious Correlations: Model relies on semantically irrelevant cues in problem statements
- Coverage Wall: GRPO improves training rewards but fails to improve Pass@K on test set
- Temperature Sensitivity (Pre-RL): Performance varies drastically with sampling temperature

**First 3 experiments**:
1. Reproduce Temperature Distillation: Train small LLM on math dataset using GRPO, plot precision curves across temperatures before/after training
2. Measure the Coverage Wall: Compute Pass@64 on held-out test set before/after RL to identify generalization gap
3. Validate with Synthetic Data: Implement graph-navigation task, pre-train with spurious correlations, apply RL with varying data sizes

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can curriculum design or significantly expanded RL datasets successfully target planning deficits to overcome the "coverage wall" in real-world mathematical reasoning?
- Basis in paper: [explicit] The conclusion suggests curriculum design or expanded RL datasets could improve planning and coverage, while synthetic experiments show data diversity boosts coverage
- Why unresolved: Empirical analysis on real math benchmarks showed a strict coverage wall with standard dataset sizes
- What evidence would resolve it: Experiments scaling RL training data by orders of magnitude or using difficulty-based curriculum learning, followed by Pass@K measurement

**Open Question 2**
- Question: How does RL training impact the third capability, "Verify" (identifying and correcting errors)?
- Basis in paper: [inferred] Section 4.1 defines "Verify" but explicitly states the focus is on Plan and Execute components
- Why unresolved: While GRPO improved execution robustness, it's unknown if the model improves its ability to backtrack or self-correct logic mid-solution
- What evidence would resolve it: Diagnostic evaluation measuring self-correction behaviors or backtracking success rate after injected logical errors

**Open Question 3**
- Question: Do the "coverage wall" and "temperature distillation" phenomena persist in frontier-scale models?
- Basis in paper: [explicit] Limitations section states study is limited to small-scale models and cannot rule out emergent behaviors in larger models
- Why unresolved: Decomposition was validated on 0.5B-7B parameter models; larger models might have sufficient pre-existing planning capacity
- What evidence would resolve it: Replicating precision vs. coverage analysis on significantly larger models (e.g., 70B+) to check if GRPO expands solvable problems

## Limitations
- Study limited to small-scale models and public datasets, cannot rule out emergent behaviors in larger models
- Analysis focuses specifically on GRPO, results may not generalize to all RL algorithms
- Synthetic task may oversimplify complexity of mathematical reasoning despite clean interpretability

## Confidence

**High Confidence**:
- Temperature distillation is a real phenomenon observed across multiple models and datasets
- The Plan/Execute/Verify decomposition provides useful analytical framework
- Coverage wall exists as described, limiting RL's ability to expand problem-solving capabilities

**Medium Confidence**:
- The synthetic task accurately captures key dynamics of mathematical problem-solving
- Data diversity is the primary factor in overcoming the coverage wall
- The mechanisms described fully explain the observed limitations

**Low Confidence**:
- All identified limitations and proposed solutions generalize beyond GRPO
- The synthetic insights directly translate to practical improvements in real math datasets
- The decomposition framework captures all relevant aspects of mathematical reasoning

## Next Checks

1. **Human Annotation Validation**: Have human experts annotate a random sample of 100 pre-RL and 100 post-RL solutions from the MATH dataset to verify automated plan/execution assessment.

2. **Alternative RL Algorithm Comparison**: Implement the same experimental pipeline using PPO with a learned critic rather than GRPO with static verification, and compare temperature distillation effects and coverage wall phenomena.

3. **Real-World Data Diversity Test**: Create controlled variations of the MATH training set with systematically varied problem diversity, and measure how coverage changes with training set diversity in the real domain.