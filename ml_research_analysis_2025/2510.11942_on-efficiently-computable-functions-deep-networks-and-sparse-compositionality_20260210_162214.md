---
ver: rpa2
title: On efficiently computable functions, deep networks and sparse compositionality
arxiv_id: '2510.11942'
source_url: https://arxiv.org/abs/2510.11942
tags:
- mout
- size
- each
- depth
- boolean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes a formal link between efficient Turing computability\
  \ and compositional sparsity in neural network representations. The main result\
  \ shows that if a function f: [0,1]^d \u2192 R^m is computable in polynomial time\
  \ relative to input/output precision, then for any desired accuracy \u03B5, there\
  \ exists a neural network of polynomial size and depth that achieves this accuracy."
---

# On efficiently computable functions, deep networks and sparse compositionality

## Quick Facts
- arXiv ID: 2510.11942
- Source URL: https://arxiv.org/abs/2510.11942
- Reference count: 17
- Main result: Polynomial-size/depth neural networks can approximate any efficiently computable function to arbitrary precision ε = 2^{-m_out}

## Executive Summary
This work establishes a formal connection between efficient Turing computability and compositional sparsity in neural network representations. The key insight is that any function computable in polynomial time relative to input/output precision can be approximated by a neural network of polynomial size and depth. This is achieved by showing that efficient computability implies bounded-fan-in Boolean circuit representations at each precision level, which can then be directly emulated by constant-size neural networks. The result connects to broader themes in approximation theory, depth efficiency, and autoregressive universality.

## Method Summary
The method is construction-based rather than learning-based. It proceeds by first showing that any efficiently computable function can be represented by a bounded-fan-in Boolean circuit through standard Turing machine unrolling. Each Boolean gate is then replaced with a constant-size neural emulator (using ReLU/sigmoid/softplus) that computes the gate exactly on {0,1}^r vertices with controlled error on neighborhoods. The final neural network wires these emulators according to the circuit topology. The approach relies on compositional approximation theory where bounded local arity governs approximation rates, and connects to depth-efficiency phenomena where deeper networks can represent certain functions exponentially more efficiently than shallow ones.

## Key Results
- Polynomial-size/depth neural networks can approximate any polytime-computable function to precision ε = 2^{-m_out}
- Compositional sparsity (bounded local arity in DAG representations) is inherent to efficiently computable functions
- Depth-efficiency phenomena are explained by the exponential separation between deep and shallow circuit complexity for certain functions
- Efficient computability implies learnability through next-token prediction (autoregressive universality)

## Why This Works (Mechanism)
The theoretical foundation rests on the equivalence between efficient computability and bounded-fan-in Boolean circuit representations. When a Turing machine computes a function in poly(n + m_out) time, unrolling its computation yields a Boolean circuit with polynomial size and depth. Each Boolean gate can be exactly emulated by a constant-size neural network with bounded approximation error on neighborhoods of {0,1}^r vertices. Composing these neural emulators according to the circuit topology preserves the polynomial bounds while achieving the desired precision through error control at each composition step.

## Foundational Learning
1. **Boolean circuit simulation of Turing machines**: Needed to bridge computational complexity with circuit complexity; check by verifying TM unrolling yields poly-size circuits
2. **Neural gate emulators**: Small ReLU networks that exactly compute Boolean gates on {0,1}^r vertices; check by testing accuracy on vertex set and neighborhoods
3. **Compositional approximation theory**: Shows bounded local arity governs approximation rates; check by analyzing local vs ambient dimension effects
4. **Depth-efficiency separation**: Exponential gaps between deep and shallow circuit complexity; check by comparing circuit sizes for specific functions
5. **Autoregressive universality**: Efficient computability implies learnability via next-token prediction; check by constructing autoregressive models for efficient functions

## Architecture Onboarding
Component map: TM Computation → Boolean Circuit → Neural Emulators → Final Network
Critical path: Input encoding → Circuit computation → Gate emulation → Output decoding
Design tradeoffs: Circuit depth vs width, emulator precision vs size, local vs global arity bounds
Failure signatures: Error explosion in deep compositions, circuit depth blowup, precision loss in emulators
First experiments:
1. Implement AND, OR, NOT gate emulators as small ReLU networks and test accuracy on {0,1}^r vertices
2. Construct Boolean circuit for matrix multiplication on discretized inputs and measure size/depth
3. Build end-to-end neural network by replacing gates with emulators and test accuracy on grid points

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Purely theoretical work with no empirical validation or specific function constructions provided
- Relies on idealized assumptions about bounded-fan-in Boolean circuit simulations and uniform gate emulators
- Constants and practical feasibility of constructions are not explored
- Does not address potential overhead or pathological cases where TM simulation might be inefficient

## Confidence
High confidence in the theoretical framework and main theorem (polynomial-size/depth networks can approximate any polytime-computable function)
Medium confidence in the compositional sparsity implications (follows logically but practical significance depends on function distribution)
Medium confidence in depth-efficiency claims (exponential separation results are referenced but not independently proven here)
Medium confidence in autoregressive universality claim (follows from main result but learning dynamics not addressed)

## Next Checks
1. Implement neural gate emulators (AND, OR, NOT for fan-in ≤3) following Yarotsky's constructions and verify accuracy on {0,1}^r vertices with controlled error on neighborhoods
2. Construct a concrete polytime-computable test function (e.g., matrix multiplication on discretized inputs) and build corresponding Boolean circuit to verify polynomial bounds on size/depth
3. Test end-to-end approximation accuracy of constructed neural network against telescoping error bound from Appendix A, measuring actual error accumulation versus theoretical predictions