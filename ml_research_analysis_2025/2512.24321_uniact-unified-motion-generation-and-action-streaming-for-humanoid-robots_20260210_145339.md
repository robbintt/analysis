---
ver: rpa2
title: 'UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots'
arxiv_id: '2512.24321'
source_url: https://arxiv.org/abs/2512.24321
tags:
- motion
- humanoid
- control
- motions
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'UniAct is a unified framework for real-time multimodal humanoid
  control that bridges high-level instruction understanding with low-level motion
  execution. The system employs a two-stage pipeline: a fine-tuned MLLM processes
  diverse inputs (language, music, trajectories) through a shared discrete codebook
  using FSQ to generate motion tokens, which are then streamed via a causal decoder
  to a robust motion tracker for execution with sub-500 ms latency.'
---

# UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots

## Quick Facts
- arXiv ID: 2512.24321
- Source URL: https://arxiv.org/abs/2512.24321
- Authors: Nan Jiang; Zimo He; Wanhe Yu; Lexi Pang; Yunhao Li; Hongjie Li; Jieming Cui; Yuhan Li; Yizhou Wang; Yixin Zhu; Siyuan Huang
- Reference count: 40
- Key outcome: UniAct is a unified framework for real-time multimodal humanoid control that bridges high-level instruction understanding with low-level motion execution. The system employs a two-stage pipeline: a fine-tuned MLLM processes diverse inputs (language, music, trajectories) through a shared discrete codebook using FSQ to generate motion tokens, which are then streamed via a causal decoder to a robust motion tracker for execution with sub-500 ms latency. UniAct achieves 19% improvement in zero-shot tracking of imperfect reference motions and demonstrates superior performance across text-to-motion, trajectory-to-motion, and music-to-motion tasks. The approach maintains robustness under noisy and out-of-distribution inputs through discrete motion representation. The authors also contribute UA-Net, a comprehensive 20-hour multimodal humanoid motion benchmark.

## Executive Summary
UniAct presents a unified framework for real-time multimodal humanoid control that bridges high-level instruction understanding with low-level motion execution. The system employs a two-stage pipeline: a fine-tuned MLLM processes diverse inputs (language, music, trajectories) through a shared discrete codebook using FSQ to generate motion tokens, which are then streamed via a causal decoder to a robust motion tracker for execution with sub-500 ms latency. UniAct achieves 19% improvement in zero-shot tracking of imperfect reference motions and demonstrates superior performance across text-to-motion, trajectory-to-motion, and music-to-motion tasks. The approach maintains robustness under noisy and out-of-distribution inputs through discrete motion representation. The authors also contribute UA-Net, a comprehensive 20-hour multimodal humanoid motion benchmark.

## Method Summary
UniAct uses a two-stage unified framework for humanoid motion generation and streaming. In the first stage, a fine-tuned MLLM processes multimodal inputs (text, music, trajectories) through a shared discrete codebook using FSQ to generate motion tokens. In the second stage, a causal decoder streams these tokens to a robust motion tracker for real-time execution. The system achieves sub-500ms latency and demonstrates superior performance across multiple control modalities while maintaining robustness to noisy and out-of-distribution inputs through its discrete motion representation approach.

## Key Results
- Achieves 19% improvement in zero-shot tracking of imperfect reference motions compared to baselines
- Maintains sub-500ms latency for real-time motion streaming and execution
- Demonstrates superior performance across text-to-motion, trajectory-to-motion, and music-to-motion tasks
- Shows robust performance under noisy and out-of-distribution inputs through discrete motion representation

## Why This Works (Mechanism)
UniAct's unified framework works by integrating high-level instruction understanding with low-level motion execution through a two-stage pipeline. The fine-tuned MLLM serves as a common interface for processing diverse multimodal inputs, converting them into a shared discrete motion representation space. This discretization enables robust handling of noisy and out-of-distribution inputs while maintaining computational efficiency. The causal streaming decoder ensures real-time responsiveness by generating motion tokens sequentially as needed, allowing the motion tracker to execute movements with minimal latency. The shared codebook approach enables seamless cross-modal control and compositional action generation.

## Foundational Learning
**Discrete motion representation (FSQ)**: Converts continuous motion data into discrete tokens using a learned codebook, enabling efficient processing and robustness to noise. Needed because continuous representations are computationally expensive and sensitive to perturbations. Quick check: Verify codebook size (15,360 tokens) provides sufficient resolution for diverse motions.

**Multimodal Large Language Models (MLLM)**: Fine-tuned models that process diverse input modalities through a unified interface. Needed to handle the heterogeneity of user commands across text, music, and trajectories. Quick check: Assess instruction understanding accuracy across different input types.

**Causal streaming decoding**: Sequential generation of motion tokens as needed rather than batch processing. Needed to achieve real-time performance with sub-500ms latency. Quick check: Measure end-to-end latency under various computational loads.

**Motion tracking robustness**: The BeyondMimic tracker's ability to handle imperfect motion references and noisy inputs. Needed because real-world motion data is inherently imperfect. Quick check: Compare tracking success rates on clean vs. noisy motion references.

## Architecture Onboarding

**Component map**: MLLM -> FSQ Quantization -> Causal Decoder -> Motion Tracker -> Actuator Execution

**Critical path**: User Input → MLLM Processing → Motion Token Generation → Causal Streaming → Motion Tracking → Robot Execution

**Design tradeoffs**: Discrete vs. continuous motion representation (robustness vs. expressiveness), unified model vs. specialized models (simplicity vs. performance), streaming vs. batch processing (latency vs. completeness)

**Failure signatures**: High tracking failure rates indicate motion token quality issues; increased latency suggests causal decoder bottlenecks; poor cross-modal generalization points to MLLM limitations

**First experiments**:
1. Benchmark tracking success rates on noisy motion references to validate robustness claims
2. Measure end-to-end latency under varying computational loads to verify sub-500ms guarantee
3. Evaluate cross-modal transfer performance (text-to-motion vs. trajectory-to-motion) to assess unified representation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the discrete motion representation scale to support highly dynamic movements such as rapid jumping, acrobatic maneuvers, and high-speed locomotion?
- Basis in paper: [explicit] The limitation section states: "The system struggles with highly dynamic movements (e.g., rapid jumping) due to motion tracker constraints."
- Why unresolved: The current motion tracker (BeyondMimic) and FSQ tokenization may not capture the rapid state transitions and ground contact forces required for dynamic behaviors. The discrete codebook of 15,360 tokens may lack sufficient resolution for high-acceleration motions.
- What evidence would resolve it: Demonstrating successful execution of jumping, leaping, or running motions exceeding 2 m/s with comparable tracking fidelity and success rates to the current locomotion results.

### Open Question 2
- Question: How can the unified framework be extended to incorporate object-centric representations for contact-rich manipulation tasks?
- Basis in paper: [explicit] The limitation section states: "currently lacks object manipulation capabilities" and suggests "incorporate object-centric representations to enable contact-rich manipulation tasks."
- Why unresolved: The current architecture focuses on whole-body motion generation without scene understanding or affordance reasoning. Motion tokens represent DoF positions without object contact semantics or force interactions.
- What evidence would resolve it: Integrating object state observations (pose, geometry, affordances) into the multimodal input pipeline and demonstrating tasks requiring tool use, pushing, or grasping with maintained real-time latency.

### Open Question 3
- Question: To what extent does the discrete FSQ representation constrain motion diversity compared to continuous diffusion-based approaches, and can this trade-off be improved?
- Basis in paper: [inferred] Table 1 shows UniAct achieves lower Diversity scores (5.21 text, 3.01 trajectory, 4.34 music) compared to two-step diffusion baselines (5.72, 3.53, 4.66), which the authors partially attribute to "inherently high diversity of diffusion-based motion generation."
- Why unresolved: The quantization to a finite codebook may inherently limit expressiveness while improving robustness. It remains unclear whether the diversity-robustness trade-off is fundamental to discrete representations or can be mitigated through larger codebooks or hierarchical tokenization.
- What evidence would resolve it: Ablation studies varying codebook size beyond 15,360 tokens, or hybrid approaches combining discrete planning with continuous refinement, showing improved diversity without sacrificing tracking success rates.

### Open Question 4
- Question: Can the unified cross-modal generation generalize to unseen combinations of actions and contexts beyond the training distribution composition?
- Basis in paper: [inferred] Section 5.4 describes compositional cross-modal control combining upper-body actions with lower-body trajectories, achieving "zero-shot generalization to unseen action-trajectory combinations." However, the tracking policy requires fine-tuning on composed pairs, suggesting generalization limits.
- Why unresolved: While the method shows zero-shot transfer for simple compositions (e.g., phone call while walking), it is unclear whether the system can handle novel semantic combinations requiring coordinated whole-body dynamics, or temporally extended multi-step instructions.
- What evidence would resolve it: Evaluating on instruction compositions absent from training data requiring inter-limb coordination (e.g., "play tennis while walking backward") without tracker fine-tuning, measuring both success rate and motion naturalness.

## Limitations
- Struggles with highly dynamic movements (e.g., rapid jumping) due to motion tracker constraints
- Currently lacks object manipulation capabilities and contact-rich interaction support
- Discrete motion representation may constrain diversity compared to continuous diffusion approaches

## Confidence

**High confidence**: Two-stage pipeline architecture effectiveness, MLLM fine-tuning for multimodal instruction understanding, basic latency measurements under controlled conditions

**Medium confidence**: 19% zero-shot tracking improvement claim (methodology appears sound but could benefit from more diverse test scenarios), robustness claims (limited empirical validation provided)

**Low confidence**: Long-term deployment viability, generalization across different robot platforms, performance under real-world operational conditions with variable computational resources

## Next Checks

1. Conduct systematic ablation studies on the FSQ tokenization process to quantify information loss and its impact on motion quality for complex movements.

2. Evaluate the framework on at least two different humanoid robot platforms with varying kinematic structures to assess cross-platform generalization.

3. Implement stress testing with controlled noise injection and hardware variations to measure performance degradation and identify operational limits of the real-time guarantees.