---
ver: rpa2
title: Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs
arxiv_id: '2512.17131'
source_url: https://arxiv.org/abs/2512.17131
tags:
- diloco
- inner
- learning
- steps
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs

## Quick Facts
- arXiv ID: 2512.17131
- Source URL: https://arxiv.org/abs/2512.17131
- Reference count: 40
- Primary result: GPA-AdamW achieves up to 8% speedup over AdamW and outperforms DiLoCo-AdamW on Llama models (160M-8B) and ViT models (S/16) on standard datasets.

## Executive Summary
This paper introduces Generalized Primal Averaging (GPA), a new momentum-based optimizer that smooths iterates at every step, providing significant speedups in training large language models and vision transformers. GPA addresses limitations of previous two-loop optimizers like DiLoCo by decoupling the smoothing constant from gradient computation, enabling continuous averaging rather than periodic pseudo-gradient updates. The method achieves 5-8% training speedups across multiple model scales while maintaining convergence guarantees through a novel regret-based analysis.

## Method Summary
GPA wraps a base optimizer (typically AdamW) and maintains three weight sequences: z(t) for the base optimizer updates, x(t) for smoothed model evaluation, and y(t) for gradient computation. The key innovation is decoupling µx (smoothing strength for x(t)) from µy (interpolation for y(t)), allowing strong smoothing without reducing gradient recency. The update equations are: y(t) = µy·x(t) + (1-µy)·z(t) for gradient computation, z(t+1) = z(t) - γ(t)·d(t) for base optimizer updates, and x(t+1) = µx·x(t) + (1-µx)·z(t+1) for smoothed averaging. GPA requires an explicit learning rate schedule (linear warmup + cosine decay) and achieves O(√T) regret in the convex setting.

## Key Results
- GPA-AdamW achieves up to 8% speedup over AdamW on Llama models (160M-8B) and ViT-S/16 on ImageNet-1k
- Outperforms DiLoCo-AdamW and Schedule-Free-AdamW across all tested model scales
- Memory-efficient variant reduces FLOPs by ~20% compared to standard GPA with minimal performance impact
- Converges faster than base optimizer when objective function varies non-linearly between consecutive iterates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling interpolation constants enables independent control of smoothing strength and gradient information flow
- Mechanism: GPA separates µx (for x(t) smoothing) from µy (for y(t) interpolation), allowing strong smoothing without reducing gradient recency
- Core assumption: Optimal smoothing level for model evaluation differs from optimal interpolation level for gradient computation
- Evidence anchors: [abstract] "GPA overcomes these limitations by decoupling the interpolation constant"; [Section 3.1] "GPA addresses this limitation by decoupling the two roles of µ"
- Break condition: If µx ≈ µy, GPA reduces to standard Nesterov primal averaging and loses advantage

### Mechanism 2
- Claim: Smooth, per-step iterate averaging provides more stable optimization than periodic pseudo-gradient aggregation
- Mechanism: GPA updates smoothed average x(t) at every step using exponential moving averaging, integrating gradient information continuously rather than in bursts
- Core assumption: "Choppy" information flow from periodic aggregation is unnecessary for optimization performance
- Evidence anchors: [abstract] "GPA overcomes these limitations... This decoupling enables GPA to smoothly average iterates at every step"; [Section 1] "updates occur only at periodic intervals, causing information from the data to be integrated in a discontinuous, choppy manner"
- Break condition: If smoothing alone explains gains, high inner-step DiLoCo should match GPA, but Figure 1b shows DiLoCo degrades at H=128 while GPA remains stable

### Mechanism 3
- Claim: Convergence advantage derives from beneficial Bregman divergence terms when objective varies non-linearly between iterates
- Mechanism: Theoretical analysis shows GPA's bound includes negative Bregman divergence terms that can dominate positive residual terms
- Core assumption: Objective function exhibits non-linear variation between consecutive iterates in practice
- Evidence anchors: [Section 5] "GPA will be faster than the base optimizer when the objective function varies non-linearly between consecutive iterates"; "If µx and µy are chosen such that the negative terms dominate the positive second term"
- Break condition: On near-linear loss surfaces or very small learning rates, Bregman terms become negligible and GPA offers minimal advantage

## Foundational Learning

- Concept: Primal averaging formulation of momentum (vs. gradient averaging)
  - Why needed here: GPA is defined in primal averaging framework that explicitly tracks separate sequences for gradient computation vs. model evaluation
  - Quick check question: Can you explain why x(t+1) = µx*x(t) + (1-µx)*z(t+1) represents exponential moving averaging of iterates rather than gradient accumulation?

- Concept: Two-loop optimizer structure (Lookahead family)
  - Why needed here: GPA is motivated as "smoothed" version of DiLoCo, which uses fast inner weights and slow outer weights
  - Quick check question: What is the pseudo-gradient in DiLoCo, and why does its computation require storing an additional copy of model weights?

- Concept: Regret bounds and online-to-batch conversion
  - Why needed here: Theoretical guarantees rely on regret-based analysis from online learning, converted to stochastic optimization bounds
  - Quick check question: If a base optimizer has regret O(√T), what does Corollary 1 say about GPA's convergence rate, and what additional terms could make it faster?

## Architecture Onboarding

- Component map:
  - z(t) -> base optimizer updates applied
  - y(t) -> gradient computation point (interpolated between x(t) and z(t) using µy)
  - x(t) -> model evaluation sequence (smoothed, interpolated between x(t-1) and z(t) using µx)
  - BaseOpt -> any optimizer (AdamW, Muon, etc.) providing search direction d(t)

- Critical path:
  1. Given current y(t) and z(t), compute gradient at y(t)
  2. Apply base optimizer to get search direction d(t)
  3. Update z(t+1) = z(t) - γ(t)*d(t) (with weight decay)
  4. Update x(t+1) = µx*x(t) + (1-µx)*z(t+1)
  5. Compute y(t+1) = µy*x(t+1) + (1-µy)*z(t+1) for next iteration's gradient

- Design tradeoffs:
  - Memory vs. compute: DiLoCo amortizes extra compute over H inner steps; GPA does O(n) extra FLOPs per step (Table 4)
  - Hyperparameter count: DiLoCo requires 4 HPs (inner/outer LR, momentum, H); GPA requires 3 (LR, µx, µy)
  - Schedule dependency: GPA requires explicit LR schedule; Schedule-Free does not (uses uniform averaging's implicit scheduling effect)

- Failure signatures:
  - If validation loss oscillates or diverges: µy likely too low (gradient evaluated at stale point); try µy ≥ 0.7
  - If training is stable but validation lag behind: µx too low (insufficient smoothing); try higher µx ≈ 0.99+
  - If GPA underperforms coupled Nesterov: Check if µx ≈ µy (degenerate case); ensure µx >> µy typically

- First 3 experiments:
  1. Sanity check: Run GPA-AdamW vs AdamW on Llama-160M (C4) with µx=0.9967, µy=0.9, LR=5e-3; expect ~8% speedup to target loss per Table 2
  2. Hyperparameter sweep: Vary µy ∈ {0.7, 0.8, 0.9, 0.95} with fixed µx derived from equivalent inner steps (Table 6: H=32 → µx=0.9967); plot validation loss curves to identify stability regime
  3. Ablation on smoothing: Compare GPA with µx=µy (coupled, standard Nesterov) vs. decoupled at same effective inner steps; expect coupled version to underperform per Figure 6

## Open Questions the Paper Calls Out

- Can the theoretical convergence guarantees for GPA be rigorously extended to non-convex and non-smooth objective functions?
- How does the decoupling of the smoothing parameter (µx) from the communication frequency impact the design of distributed training algorithms for cross-regional setups?
- Is GPA compatible with complex, non-adaptive preconditioners such as Shampoo, SOAP, or Muon, and does it retain its performance gains over standard AdamW baselines in those configurations?
- Can the hyperparameters µx and µy be efficiently transferred across different model scales using techniques like µP (Maximal Update Parametrization)?

## Limitations
- Current theoretical analysis is limited to convex settings, requiring further research for non-convex LLM loss landscapes
- Requires explicit learning rate scheduling, unlike Schedule-Free methods that achieve implicit scheduling through uniform averaging
- Memory-efficient variant reconstructs x(t) for evaluation, adding computational overhead despite reduced memory usage

## Confidence
- High: Empirical results showing 5-8% speedup across multiple model scales and datasets
- Medium: Theoretical convergence guarantees limited to convex settings
- Medium: Memory-efficient variant achieves similar performance with reduced FLOPs but requires reconstruction step

## Next Checks
1. Verify GPA implementation by reproducing Figure 2 trajectories on a simple quadratic function, comparing to standard Nesterov and coupled versions
2. Run ablation study varying µx and µy independently on Llama-160M to identify stability regime and optimal decoupling
3. Test GPA wrapper with alternative base optimizers (Muon, Shampoo) to validate compatibility claims for future work