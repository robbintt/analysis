---
ver: rpa2
title: 'Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges
  from Generalization'
arxiv_id: '2502.17024'
source_url: https://arxiv.org/abs/2502.17024
tags:
- wpre
- sequences
- topics
- topic
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles a key gap in in-context learning (ICL) theory
  by extending beyond the standard i.i.d. assumption to the more realistic auto-regressive
  next-token prediction (AR-NTP) setting.
---

# Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization

## Quick Facts
- arXiv ID: 2502.17024
- Source URL: https://arxiv.org/abs/2502.17024
- Authors: Zixuan Gong; Xiaolin Hu; Huayi Tang; Yong Liu
- Reference count: 40
- Primary result: Proposes PAC-Bayesian generalization bounds showing ICL emerges from successful generalization across sequences and topics in auto-regressive next-token prediction

## Executive Summary
This paper addresses a fundamental gap in in-context learning theory by extending beyond i.i.d. assumptions to the auto-regressive next-token prediction setting. The authors develop a theoretical framework that explicitly accounts for token dependencies and hierarchical topic structures, deriving data-dependent and topic-dependent PAC-Bayesian generalization bounds. The work demonstrates that ICL emerges when models successfully generalize across both sequences and topics, with key factors including model size, pre-training topics, sequences per topic, sequence length, and prompt length. Experiments on synthetic (GINC) and real-world datasets validate these theoretical insights, showing improved ICL performance with proper data structure and prior initialization.

## Method Summary
The method introduces a pre-training and ICL framework that handles token dependencies through "ghost sequences" and continuous optimization analysis using Stochastic Differential Equations. The core approach uses a two-level expectation framework: first-level expectation over sequences within topics, and second-level expectation over topics. The authors derive PAC-Bayesian generalization bounds that decompose population loss into empirical loss, generalization over sequences, and generalization over topics. A key innovation is the use of data-dependent and topic-dependent priors, where models are pre-trained on subsets of data to create informative initialization for full training.

## Key Results
- Theoretical bounds show ICL emerges when models generalize across both sequences and topics, with performance scaling as O(1/√(KTp) + 1/√(KNT))
- Prior model initialization accelerates training (4 hours vs 7 hours for GPT2-large) and improves stability
- ICL fails when data distribution doesn't match assumed topic structure, as demonstrated with random-transition sequences
- Increasing pre-training topics (K), sequences per topic (N), sequence length (T), and prompt length (Tp) all improve generalization and ICL performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL emerges from successful generalization across sequences and topics, measured through population loss bounds
- Mechanism: Population loss decomposes into empirical loss + generalization over sequences (two stages) + generalization over topics. When all parts are bounded, the model generalizes to unseen prompts under unseen topics, manifesting as ICL ability
- Core assumption: Topics and sequences follow hierarchical distributions where pre-training topics and ICL topics share the same topic distribution PW
- Break condition: If data distribution doesn't match assumed topic structure (e.g., random transitions), model cannot extract topic information and ICL fails

### Mechanism 2
- Claim: Data-dependent and topic-dependent priors reduce KL divergence between prior and posterior, tightening generalization bounds and improving training stability
- Mechanism: Train prior on subset of sequences/topics, then use as initialization for full training. Smaller DKL(μ∥νJ) directly improves PAC-Bayesian bounds
- Core assumption: Subset of training data can capture sufficient distributional information to inform useful prior
- Break condition: If subset is too small or unrepresentative, prior may not align with true data distribution

### Mechanism 3
- Claim: Token dependency in AR-NTP can be handled through ghost sequences and continuous optimization analysis (SDE)
- Mechanism: Construct "ghost sequences" that depend on prefix tokens but are conditionally independent. Combined with Donsker-Varadhan representation and Fokker-Planck equations, this allows bounding TV distance and converting to KL divergence bounds
- Core assumption: Bounded loss and gradients, plus Lipschitz continuity
- Break condition: If loss or gradients are unbounded, LSI and concentration inequalities used in proofs fail

## Foundational Learning

- **PAC-Bayesian generalization bounds**: Essential for understanding how the theoretical framework bounds population loss. Quick check: Can you explain why DKL(μ∥ν) appears in the upper bound for expected loss?

- **Stochastic Differential Equations (SDE) and Langevin Dynamics**: Used to model SGD optimization and distribution evolution. Quick check: How does the Log-Sobolev Inequality relate KL divergence to gradient norms in SDE analysis?

- **Auto-regressive next-token prediction vs. i.i.d. supervised learning**: Critical for understanding why token dependency matters. Quick check: Why can't standard meta-learning analysis be directly applied to AR-NTP ICL?

## Architecture Onboarding

- **Component map**: Pre-training phase (K topics → N sequences/topic → T tokens/sequence → AR-NTP loss) → Two-level expectation framework (sequences → topics) → Prior initialization module (subset → prior → full training)

- **Critical path**: 1) Verify topic diversity in pre-training data (K matters) 2) Ensure sufficient sequences per topic (N matters) 3) Use prior model initialization before large-scale training 4) Monitor training iterations T′

- **Design tradeoffs**: Larger N′ for prior (better prior quality) vs. more data for main training; longer prompts Tp (better ICL) vs. computational cost; larger model size Nparam (lower bounds) vs. resource constraints

- **Failure signatures**: ICL accuracy at random baseline despite low training loss → check data follows topic structure; training loss oscillates → consider prior initialization; large generalization gap → increase K or N

- **First 3 experiments**: 1) Ablation on K (number of topics): Train models with K∈{5,10,15,20}, fixed N=2^14, T=256. Measure ICL accuracy on held-out topics. 2) Prior initialization timing: Compare random init vs. training GPT2-small on subset for warmup. Measure convergence speed and final loss. 3) Topic structure validation: Include random-transition sequences in pre-training. Verify ICL fails, confirming mechanism requires coherent topic structure.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the optimization error ε_opt be rigorously bounded within the auto-regressive next-token prediction setting?
- **Basis in paper**: Appendix G.2.1 explicitly states "we defer the analysis of optimization error to future work," assuming it is small for current proofs
- **Why unresolved**: Current work isolates generalization gap but treats gap between empirical solution and ideal minimum as constant
- **What evidence would resolve it**: A derived bound for ε_opt dependent on training iterations and landscape geometry, integrated into final loss

### Open Question 2
- **Question**: How robust is the derived theory when strict "topic distribution" structure is violated or partially missing in pre-training data?
- **Basis in paper**: Paper demonstrates ICL fails when data distributions don't match assumed topic structure, but doesn't explore partial or noisy structural violations
- **Why unresolved**: Real-world corpora may not satisfy clean hierarchical assumptions of distinct topics and sequences
- **What evidence would resolve it**: Empirical results showing whether generalization bounds hold on datasets with synthetic noise or ambiguous topic boundaries

### Open Question 3
- **Question**: Are the proposed generalization bounds non-vacuous and computable for state-of-the-art LLMs?
- **Basis in paper**: Experiments restricted to GPT-2 and synthetic datasets, leaving scalability to larger models untested
- **Why unresolved**: PAC-Bounds often become vacuous in high-dimensional spaces; unclear if data-dependent priors suffice for models with billions of parameters
- **What evidence would resolve it**: Numerical computation of bound for large-scale model (e.g., Llama-2) demonstrating tight upper bound on true population loss

## Limitations
- Theoretical framework relies on idealized assumptions (topic structure, bounded losses/gradients) that may not hold in practical LLM training
- Experiments focus on relatively small models (4-layer GPT-2) that may not scale to complexities of modern LLMs
- SDE-based analysis assumes continuous optimization that may not accurately reflect discrete SGD dynamics used in actual training

## Confidence

- **High Confidence (8-10/10)**: Empirical validation of prior initialization showing faster convergence and improved training stability
- **Medium Confidence (5-7/10)**: Theoretical PAC-Bayesian generalization bounds given stated assumptions; claim that ICL fails when data distribution doesn't match topic structure
- **Low Confidence (1-4/10)**: Extension of theoretical bounds to state-of-the-art LLMs (100B+ parameters) is speculative; specific mathematical tools (ghost sequences, Fokker-Planck equations) necessity versus simpler alternatives

## Next Checks

1. **Scaling Experiment**: Validate whether theoretical PAC-Bayesian bounds remain predictive when scaling from 4-layer GPT-2 to modern architectures (12-24 layer models with 1-10B parameters). Measure gap between theoretical bounds and actual ICL performance.

2. **Distributional Robustness Test**: Systematically vary degree of topic overlap and hierarchical structure in synthetic data to map boundary conditions where theoretical framework breaks down.

3. **Optimization Dynamics Verification**: Compare continuous SDE analysis predictions against actual discrete SGD trajectories on same data. Track how closely empirical distribution evolution matches theoretical Fokker-Planck predictions during training.