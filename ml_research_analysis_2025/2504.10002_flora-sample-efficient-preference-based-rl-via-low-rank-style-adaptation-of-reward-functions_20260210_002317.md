---
ver: rpa2
title: 'FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation
  of Reward Functions'
arxiv_id: '2504.10002'
source_url: https://arxiv.org/abs/2504.10002
tags:
- reward
- style
- adaptation
- human
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present FLoRA, a method for efficiently adapting pre-trained
  robotic reward functions to human preferences without catastrophic forgetting. The
  approach uses low-rank matrix adaptation (LoRA) to update only a small set of parameters
  while keeping the original reward model frozen, thus preserving baseline task performance.
---

# FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions

## Quick Facts
- arXiv ID: 2504.10002
- Source URL: https://arxiv.org/abs/2504.10002
- Reference count: 40
- Primary result: FLoRA achieves 19.46% higher combined reward than baselines while requiring fewer than 100 human preferences for effective adaptation

## Executive Summary
FLoRA introduces a sample-efficient method for adapting pre-trained robotic reward functions to human preferences without catastrophic forgetting. The approach leverages Low-Rank Adaptation (LoRA) to update only a small set of parameters while keeping the original reward model frozen, thus preserving baseline task performance. This allows robots to adapt their behavior style based on human preferences while maintaining core task capabilities.

The method is evaluated across simulated control tasks (DeepMind Control Suite, Meta-World) and real-world robots (7-DoF manipulator, quadruped), consistently outperforming fine-tuning and semi-supervised baselines. FLoRA demonstrates the ability to learn new style preferences from limited human feedback while preserving the original task success rate.

## Method Summary
FLoRA combines reinforcement learning with preference-based adaptation using low-rank matrix adaptation. The method starts with a pre-trained reward model and applies LoRA to adapt it based on human preferences, keeping the original model frozen to prevent catastrophic forgetting. This creates a dual-system approach where the original task capabilities are preserved while new style preferences are learned through a small set of adaptable parameters. The method uses preference queries to efficiently gather human feedback and applies this to update the reward function without retraining the entire model.

## Key Results
- Achieves 19.46% higher combined reward than baseline methods
- Requires fewer than 100 human preferences for effective adaptation
- Consistently outperforms fine-tuning and semi-supervised baselines across simulation and real-world robot tasks
- Successfully preserves original task performance while adapting to new style preferences

## Why This Works (Mechanism)
FLoRA works by leveraging the representational power of pre-trained reward models while using LoRA to efficiently adapt to new preferences. The low-rank adaptation technique allows for significant changes in behavior style with minimal parameter updates, preventing catastrophic forgetting of the original task capabilities. By freezing the original reward model and only updating a small set of parameters, FLoRA maintains stability in core task performance while being flexible enough to learn new preferences from limited human feedback.

## Foundational Learning
- **Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that updates only a small subset of parameters using low-rank matrices, reducing computational cost and preventing overfitting. Needed for efficient adaptation without retraining entire models.
- **Catastrophic Forgetting**: The tendency of neural networks to forget previously learned information when trained on new tasks. Quick check: Does the method maintain baseline performance while adapting to new preferences?
- **Preference-based RL**: A reinforcement learning paradigm where agents learn from human preferences rather than explicit reward functions. Quick check: Can the method effectively translate human preferences into meaningful behavioral changes?
- **Reward Function Adaptation**: The process of modifying reward functions to align with new objectives or preferences. Quick check: Does the adapted reward function maintain task performance while incorporating new preferences?

## Architecture Onboarding
- **Component Map**: Pre-trained Reward Model -> LoRA Adapter -> Preference Queries -> Updated Reward Function -> RL Policy
- **Critical Path**: The sequence from receiving human preferences through LoRA adaptation to updating the RL policy is the most critical path, as it directly impacts the quality of the adapted behavior.
- **Design Tradeoffs**: The method trades off between parameter efficiency (using LoRA) and adaptation capacity (limiting updates to small parameter sets), balancing between maintaining baseline performance and learning new preferences.
- **Failure Signatures**: Poor preference queries leading to misaligned behavior, insufficient LoRA rank causing inadequate adaptation, or overly aggressive adaptation leading to catastrophic forgetting of original task capabilities.
- **First Experiments**:
  1. Test adaptation with a single preference query to verify basic functionality
  2. Evaluate performance on a simple simulated task with known preference patterns
  3. Compare adaptation speed and quality against standard fine-tuning methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Effectiveness depends on having a pre-trained reward model with sufficient representational capacity for diverse styles
- Performance in more complex, unstructured environments beyond controlled simulations and specific real-world tasks remains untested
- Computational overhead of preference queries and scalability for tasks requiring extensive style variations are unclear

## Confidence
- High confidence in the core methodology and mathematical framework
- Medium confidence in simulation results due to controlled experimental conditions
- Medium confidence in real-world robot demonstrations, though limited to specific tasks
- Low confidence in scalability and generalization claims beyond presented scenarios

## Next Checks
1. Test FLoRA on more diverse and complex robotic tasks with longer time horizons and higher-dimensional state spaces
2. Evaluate the method's performance when starting from reward models trained on limited or biased data
3. Assess the computational efficiency and memory requirements for scaling to larger, more complex reward models across multiple simultaneous style adaptations