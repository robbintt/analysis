---
ver: rpa2
title: Dual-Space Smoothness for Robust and Balanced LLM Unlearning
arxiv_id: '2509.23362'
source_url: https://arxiv.org/abs/2509.23362
tags:
- unlearning
- attacks
- utility
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRISM improves robustness and balance in LLM unlearning by enforcing
  dual-space smoothness in representation and parameter spaces. It uses a robustly
  trained probe to defend against jailbreak attacks in the representation space and
  smoothness minimization to mitigate relearning attacks in the parameter space.
---

# Dual-Space Smoothness for Robust and Balanced LLM Unlearning

## Quick Facts
- **arXiv ID**: 2509.23362
- **Source URL**: https://arxiv.org/abs/2509.23362
- **Reference count**: 40
- **Primary result**: PRISM improves robustness and balance in LLM unlearning by enforcing dual-space smoothness in representation and parameter spaces.

## Executive Summary
This paper addresses the challenge of machine unlearning in large language models, where the goal is to remove specific harmful or copyrighted content while preserving utility and preventing both jailbreak and relearning attacks. The authors propose PRISM, a dual-space smoothness framework that combines representation-space robustness via an adversarially trained probe with parameter-space smoothness using min-max optimization. Extensive experiments on WMDP and MUSE datasets demonstrate that PRISM outperforms state-of-the-art baselines in balancing unlearning effectiveness, utility preservation, and privacy protection while maintaining robustness against multiple attack types.

## Method Summary
PRISM operates through a dual-stage optimization process. First, it trains a robustly trained probe on hidden representations using adversarial perturbations (FGSM) to detect undesired content in representation space. Second, it performs smoothness minimization in parameter space using a SAM-like objective that flattens the loss landscape to resist relearning attacks. The method also employs gradient conflict decoupling by orthogonalizing forget and retain gradients to prevent catastrophic forgetting. The training involves alternating between an inner maximization step (finding worst-case perturbations) and an outer minimization step (updating model parameters with decoupled gradients).

## Key Results
- PRISM achieves superior balance among unlearning effectiveness, utility preservation, and privacy protection compared to state-of-the-art baselines
- The method demonstrates robustness against multiple attack types including AutoDAN, Prefill, and relearning attacks
- Performance improvements are consistent across both MUSE and WMDP datasets with different base models (Llama-2-7B, Ministral-8B)
- The framework shows effectiveness in removing harmful content while maintaining performance on benign tasks

## Why This Works (Mechanism)

### Mechanism 1: Representation-Space Smoothness via Adversarially Trained Probe
The probe is trained on hidden representations with FGSM-style perturbations to enlarge the "jailbreak margin," making the unlearned model more robust to adversarial prompt manipulations. By training the probe to detect undesired content in both clean and adversarially perturbed features, the decision boundary widens, making it harder for small representation-space drifts from jailbreaks to evade detection. This assumes jailbreak attacks operate by shifting harmful prompt representations toward a "harmless anchor" in representation space, which can be countered by widening the classifier's decision boundary.

### Mechanism 2: Parameter-Space Smoothness via Min-Max Optimization
The method enforces smoothness in parameter space through a SAM-like objective that increases the "relearn margin," making it harder for attackers to restore forgotten knowledge through small fine-tuning steps. The inner maximization searches for worst-case parameter perturbations within an ℓ₂ ball, while the outer minimization updates parameters to minimize the worst-case loss. This flattening of the loss landscape increases the minimum parameter change required for successful recovery, assuming relearning attacks exploit sharp minima in the loss landscape.

### Mechanism 3: Gradient Conflict Decoupling via Orthogonal Projection
The framework projects the forget gradient onto the orthogonal complement of the retain gradient to provide first-order safety against catastrophic forgetting. By removing the component of the forget gradient that conflicts with the retain gradient, the update direction preserves retain-set performance locally. This assumes catastrophic forgetting arises from gradient conflicts between forget and retain objectives, and that orthogonalizing them prevents the retain loss from increasing locally.

## Foundational Learning

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: PRISM builds on SAM's min-max formulation for parameter-space smoothness. Understanding SAM explains why the gradient-norm penalty appears in the objective.
  - Quick check question: Why does SAM add a gradient-norm penalty to the loss, and how does this promote flat minima?

- **Concept: Adversarial Training and FGSM**
  - Why needed here: The robust probe is trained using FGSM-style feature-space perturbations. Understanding adversarial training clarifies why the optimal perturbation under an ℓ∞ budget takes the form it does.
  - Quick check question: Under what conditions does FGSM provide a first-order approximation to the worst-case perturbation?

- **Concept: Gradient Projection for Multi-Objective Optimization**
  - Why needed here: The retain–forget gradient decoupling uses orthogonal projection to remove conflicting components. This is a standard technique from multi-task learning.
  - Quick check question: If the forget and retain gradients are orthogonal, what happens to the projected forget gradient?

## Architecture Onboarding

- **Component map**: Probe Training -> Smoothness Minimization -> Weight Update
- **Critical path**: Probe training → (must converge with sufficient robustness) → Representation extraction → Gradient perturbation → Orthogonalization → Weight update. If the probe is poorly calibrated or overfitted, downstream unlearning degrades.
- **Design tradeoffs**:
  - ρ (perturbation radius): Larger ρ increases robustness to relearning but risks utility collapse
  - γ (probe loss weight): Higher γ strengthens representation-space smoothness but may cause over-refusal
  - Layer selection for probe: Mid-to-back layers (e.g., 23, 32) balance forgetting and utility
- **Failure signatures**:
  - Catastrophic collapse: Utility on retain set drops to near zero; often caused by missing GCD or excessive ρ
  - High over-refusal rate: Model refuses benign prompts; likely due to excessive probe loss weight γ or NPO amplification
  - Probe bypass: Jailbreak attacks succeed despite probe; probe may not be robust to adaptive adversaries
- **First 3 experiments**:
  1. **Probe layer ablation**: Train probes at different layers (1, 13, 23, 32) and measure unlearning/robustness metrics to identify optimal layer
  2. **Hyperparameter sweep for ρ and γ**: Run grid search over ρ ∈ [0.001, 0.1] and γ ∈ [0.05, 0.25] on held-out validation set
  3. **Attack-specific robustness test**: Evaluate unlearned model against prefill, AutoDAN, and multi-turn attacks to verify generalization across attack types

## Open Questions the Paper Calls Out

### Open Question 1
Can the high over-refusal rate observed in PRISM be mitigated without compromising the framework's robustness to jailbreaks? The authors note PRISM sometimes exhibits high over-refusal rates stemming from its reliance on Negative Preference Optimization (NPO). Experiments integrating PRISM's smoothness mechanisms with non-NPO unlearning objectives and measuring refusal rates on benign datasets would help resolve this.

### Open Question 2
Can the computational overhead of probe-guided smoothing and min-max optimization be reduced for larger models? The method introduces additional training complexity, and future work should study more efficient formulations of representation-level regularization. Performance-efficiency trade-off studies using cheaper approximations would provide answers.

### Open Question 3
To what extent does PRISM's performance depend on base model architecture or probe layer selection? The choice of layer appears heuristic or dataset-dependent, leaving open generalizability to different architectures. Cross-architecture studies with automated layer selection heuristics would help resolve this dependency.

## Limitations
- PRISM exhibits a relatively high over-refusal rate, particularly for benign ambiguous prompts
- The method introduces additional computational overhead compared to simpler unlearning approaches
- Performance depends on careful hyperparameter tuning, especially for the probe layer and smoothness parameters

## Confidence

- **High**: PRISM improves balance among unlearning effectiveness, utility preservation, and privacy protection on standard benchmarks
- **Medium**: PRISM provides robustness against multiple attack types under tested conditions
- **Low**: The probe-based defense generalizes to adaptive adversaries aware of its existence

## Next Checks

1. **Probe Robustness Against Adaptive Attacks**: Evaluate the unlearned model against white-box attacks where the adversary knows the probe's architecture and training procedure
2. **Parameter-Space Smoothness Under Large Updates**: Test relearning resistance with larger learning rates or longer fine-tuning schedules to stress-test the SAM-based smoothing
3. **Gradient Decoupling with Aligned Gradients**: Create synthetic forget/retain pairs with aligned gradients to verify orthogonalization still prevents catastrophic forgetting