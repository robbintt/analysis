---
ver: rpa2
title: 'PEAR: Phase Entropy Aware Reward for Efficient Reasoning'
arxiv_id: '2510.08026'
source_url: https://arxiv.org/abs/2510.08026
tags:
- reasoning
- arxiv
- entropy
- phase
- pear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors address excessive reasoning length in large reasoning\
  \ models (LRMs), which increases inference cost without improving accuracy. They\
  \ propose Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporates\
  \ phase-dependent entropy\u2014higher in the thinking phase, lower in the final\
  \ answer phase\u2014into training."
---

# PEAR: Phase Entropy Aware Reward for Efficient Reasoning

## Quick Facts
- arXiv ID: 2510.08026
- Source URL: https://arxiv.org/abs/2510.08026
- Authors: Chen Huang, Wei Lu, Wenxuan Zhang
- Reference count: 31
- One-line primary result: Achieves 37.8%-59.4% reductions in response length while preserving accuracy within 0.9% of baseline

## Executive Summary
PEAR addresses excessive reasoning length in Large Reasoning Models (LRMs) by incorporating phase-dependent entropy into the reward function. The method observes that thinking phases exhibit higher entropy (exploratory behavior) while answer phases show lower entropy (deterministic). By penalizing high entropy during thinking while maintaining flexibility in final answers, PEAR encourages concise reasoning without sacrificing accuracy. Evaluated across four benchmarks and three model scales, PEAR demonstrates significant length reduction with minimal accuracy loss and strong out-of-distribution generalization.

## Method Summary
PEAR modifies GRPO by introducing a phase-aware entropy penalty into the reward function. It computes token-level entropy using the old policy's distribution, separates thinking (before `</think>`) and answer phases, then applies a reward: r(y) = min(1, s - P(y)) where P(y) = max(0, H̄_think - α·H̄_answer). The hyperparameter α controls the tradeoff between efficiency and accuracy, with α=1 recommended. Training uses batch size 128, learning rate 1×10^-6, and standard GRPO optimization.

## Key Results
- 37.8%-59.4% reduction in response length across GSM8K, MATH500, AIME24, and AMC23 benchmarks
- Accuracy preserved within 0.9% of baseline models
- Strong out-of-distribution generalization to AIME24 and AMC23
- Consistent improvements across model scales (1.5B to 8B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Phase-Differentiated Entropy as Length Control Signal
PEAR exploits the observation that thinking phase entropy correlates positively with response length while answer phase entropy is lower and more deterministic. By penalizing high thinking entropy while preserving answer phase flexibility, the model learns to generate more concise reasoning traces without collapsing into degenerate outputs.

### Mechanism 2: Reward Shaping via Entropy Differential
The phase reward P(y) = max(0, H̄_think - α·H̄_answer) creates soft pressure toward shorter responses. High thinking entropy reduces reward, while higher answer entropy (modulated by α) mitigates the penalty. This asymmetric pressure guides the model to prioritize essential reasoning steps.

### Mechanism 3: GRPO with Entropy-Augmented Rewards
Replacing binary GRPO rewards with entropy-aware rewards enables adaptive length control without explicit token budgets. The group-relative advantage normalization amplifies differences between candidate responses, encouraging preference for lower-entropy correct responses.

## Foundational Learning

- **Shannon entropy in language models**: Essential for understanding how PEAR quantifies uncertainty at each token step. Quick check: Given distribution [0.7, 0.2, 0.1], can you compute its entropy?
- **Group Relative Policy Optimization (GRPO)**: Core training algorithm modified by PEAR. Quick check: How does GRPO's advantage calculation differ from standard PPO?
- **Phase separation in reasoning models**: Understanding how models use special tokens (``) to demarcate thinking and answer phases is crucial for correct implementation. Quick check: How would you identify the boundary between thinking and answer phases in a model output?

## Architecture Onboarding

- **Component map**: Tokenizer/Vocabulary -> Forward Pass (autoregressive generation with entropy storage) -> Entropy Calculator (H_t = -Σ p(v|y_<t) log p(v|y_<t)) -> Reward Function (r(y) = min(1, s - P(y))) -> GRPO Optimizer (group-normalized advantages)
- **Critical path**: Generate G responses per prompt → detect `` token to split phases → compute average entropy per phase → calculate P(y) = max(0, H̄_think - α·H̄_answer) → compute final reward r(y) = min(1, s - P(y)) for correct answers → normalize rewards within each group and compute advantages → update policy via GRPO objective
- **Design tradeoffs**: α value (low α aggressively penalizes thinking entropy but risks accuracy; high α weakens efficiency signal); base reward scale s (must be larger than typical P(y) to maintain accuracy preference); training data (GSM8K only, generalization relies on entropy generality)
- **Failure signatures**: Accuracy collapse (>2% drop) suggests α too low or penalty too aggressive; no length reduction indicates α too high or entropy computation errors; degenerate outputs (repetitive, incoherent) may indicate reward gaming
- **First 3 experiments**: 1) Reproduce entropy-length correlation by plotting average entropy vs. response length for baseline LRM on GSM8K; 2) Ablate α by training with α ∈ {-1, 0, 0.5, 1, 2} on small subset and plotting accuracy vs. length tradeoff; 3) Verify phase boundary detection by manually inspecting 20 model outputs before full GRPO training

## Open Questions the Paper Calls Out

### Open Question 1
How can the α hyperparameter be set in a principled, task-adaptive manner rather than requiring manual tuning? The paper demonstrates sensitivity to α but offers no systematic selection criterion. Evidence showing optimal α varies with task difficulty, model scale, or reasoning domain would resolve this.

### Open Question 2
Does the phase-dependent entropy relationship generalize to non-mathematical reasoning domains such as logical reasoning, code generation, or scientific problem-solving? All experiments are on mathematical benchmarks. Evaluation on diverse reasoning benchmarks would establish generality.

### Open Question 3
What is the theoretical limit of entropy-based compression before accuracy degrades, and does this limit depend on problem complexity? The paper shows a compression threshold on one model but doesn't characterize whether this boundary scales with problem difficulty or model capability.

## Limitations
- Training duration (epochs/steps) not specified, making it unclear whether improvements stem from sufficient optimization
- Exact values of base score s and format reward r_fmt loosely defined (s ∈ (0,1]), introducing variability in accuracy-length tradeoff
- OOD generalization based on limited samples (30 and 40 problems for AIME24/AMC23), raising reliability concerns

## Confidence
- **High confidence**: Phase-dependent entropy correlation (supported by Figure 2(a) across models); token reduction metrics (37.8%-59.4% across benchmarks); reproducibility of baseline implementations
- **Medium confidence**: OOD generalization claims (limited problem samples; no statistical tests); optimal α=1 recommendation (based on Figure 4 but not exhaustively validated)
- **Low confidence**: Long-term stability of PEAR-trained models (no continual learning analysis); robustness to different reasoning paradigms

## Next Checks
1. **Statistical validation of entropy-length correlation**: Compute Pearson/Spearman correlation coefficients and p-values for entropy vs. response length on GSM8K and MATH500. Test if correlation holds for models without `` tokens.
2. **Hyperparameter sensitivity analysis**: Systematically vary α ∈ {0, 0.5, 1, 1.5, 2} and measure accuracy-length Pareto frontiers. Identify if α=1 is truly optimal or problem-dependent.
3. **OOD robustness stress test**: Evaluate PEAR on a larger, diverse OOD set (e.g., 200+ problems from multiple STEM domains). Check for accuracy collapse or entropy manipulation exploits.