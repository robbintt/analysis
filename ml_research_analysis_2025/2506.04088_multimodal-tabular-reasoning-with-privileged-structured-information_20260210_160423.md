---
ver: rpa2
title: Multimodal Tabular Reasoning with Privileged Structured Information
arxiv_id: '2506.04088'
source_url: https://arxiv.org/abs/2506.04088
tags:
- reasoning
- table
- arxiv
- structured
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal tabular reasoning where only table
  images are available at inference, despite structured tables being accessible during
  training. The authors propose TURBO, a framework that leverages structured tables
  as privileged information to enhance multimodal large language models (MLLMs).
---

# Multimodal Tabular Reasoning with Privileged Structured Information

## Quick Facts
- arXiv ID: 2506.04088
- Source URL: https://arxiv.org/abs/2506.04088
- Reference count: 40
- Authors: Jun-Peng Jiang; Yu Xia; Hai-Long Sun; Shiyin Lu; Qing-Guo Chen; Weihua Luo; Kaifu Zhang; De-Chuan Zhan; Han-Jia Ye
- Primary result: State-of-the-art performance (+7.2% improvement) in multimodal tabular reasoning by leveraging structured tables as privileged information during training

## Executive Summary
This paper addresses multimodal tabular reasoning where only table images are available at inference, despite structured tables being accessible during training. The authors propose TURBO, a framework that leverages structured tables as privileged information to enhance multimodal large language models (MLLMs). The core method involves generating structure-aware reasoning traces using DeepSeek-R1, then fine-tuning the MLLM with these traces and further optimizing through reward-guided reinforcement learning. TURBO achieves state-of-the-art performance (+7.2% improvement over the previous best) across multiple datasets, demonstrating strong generalization and interpretability in multimodal tabular reasoning tasks.

## Method Summary
TURBO operates in three stages: (1) DeepSeek-R1 generates reasoning traces from structured tables paired with QA, filtered via reject sampling; (2) Ovis2-8B is fine-tuned on image-question-reasoning-answer triplets using SFT; (3) GRPO optimizes reasoning through relative advantage computation over multiple sampled responses. The framework leverages privileged structured information during training to improve image-only inference performance, with careful reward design balancing format compliance and answer accuracy.

## Key Results
- Achieves 7.2% improvement over previous state-of-the-art on multimodal tabular reasoning benchmarks
- Demonstrates strong generalization across multiple datasets including TABMWP, WTQ, HiTab, TAT-QA, TabFact, and InfoTabs
- Shows complementary contributions from both SFT (10.1% gain) and RL (11.8% gain) stages
- Maintains interpretability through explicit reasoning traces while achieving high accuracy

## Why This Works (Mechanism)

### Mechanism 1: Modality Bridging via Reasoning Trace Transfer
- Claim: Reasoning traces generated from structured tables can train MLLMs to reason over table images.
- Mechanism: DeepSeek-R1 generates chain-of-thought traces from markdown tables + QA pairs. These traces capture logical steps (locating cells, computing, comparing) that are modality-independent. MLLM learns via SFT to replicate this reasoning from visual input.
- Core assumption: The inference trajectory from question to answer remains consistent whether input is structured text or image.
- Evidence anchors:
  - [abstract]: "TURBO benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, contributing to high-quality modality-bridged data."
  - [Section 4.1]: "From a reasoning perspective, the inference trajectory—i.e., the logical path leading from question to answer—would remain consistent across both modalities."
  - [corpus]: Limited direct validation; related work focuses on structured input, not bridging.
- Break condition: When visual layout cues (merged cells, nested headers) fundamentally change the reasoning strategy.

### Mechanism 2: Privileged Structured Information for Cleaner Supervision
- Claim: Structured tables during training provide explicit semantics that improve image-only inference.
- Mechanism: Model jointly observes image and structured markdown during training. It learns visual-structural correspondences. At inference, only image is needed; model has internalized structural patterns.
- Core assumption: Visual representations can absorb structural semantics with sufficient alignment signal.
- Evidence anchors:
  - [abstract]: "leveraging privileged structured information available during training to enhance multimodal large language models (MLLMs)."
  - [Section 3.2, Figure 1]: Shows consistent accuracy drop (15-30%) across Qwen2.5-VL, InternVL2.5, Ovis2 when structured tables removed.
  - [corpus]: HIPPO uses dual-modality but requires structured input at inference—differs in test-time assumptions.
- Break condition: When test tables have layouts/domains poorly represented in training structured data.

### Mechanism 3: Group Relative Policy Optimization (GRPO) for Reasoning Refinement
- Claim: GRPO improves reasoning by comparing multiple sampled responses within groups.
- Mechanism: Generate G responses per question. Compute relative advantage: Ai = (ri - mean) / std within group. Update policy to favor high-relative-advantage outputs. Uses format reward (structured output) and accuracy reward (correct answer).
- Core assumption: Relative comparison within groups provides more stable gradient than absolute reward.
- Evidence anchors:
  - [abstract]: "repeatedly generates and selects the advantageous reasoning paths, further enhancing the model's tabular reasoning ability."
  - [Section 4.2, Eq. 2-3]: Full GRPO objective with relative advantage computation; removes critic network.
  - [corpus]: Weak corpus support for GRPO in tabular reasoning specifically.
- Break condition: When correct reasoning paths are sparse, making relative advantages noisy.

## Foundational Learning

- **Learning with Privileged Information (LUPI)**
  - Why needed here: Core paradigm assumes extra signal at training (structured tables) unavailable at inference.
  - Quick check question: Why might additional training information hurt generalization, and how does this method mitigate it?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Method relies on explicit reasoning traces as training targets.
  - Quick check question: What distinguishes a faithful CoT from a post-hoc rationalization?

- **Policy Gradient / PPO**
  - Why needed here: GRPO builds on PPO concepts without a value function.
  - Quick check question: How does removing the critic network change what the optimizer can learn?

## Architecture Onboarding

- **Component map:**
  Data Pipeline: Structured table + QA → DeepSeek-R1 → Reasoning trace → Reject sampling → 9k training examples
  SFT Stage: Ovis2-8B trained on [image, question, reasoning, answer] with "think-then-answer" format
  RL Stage: GRPO with format reward (uses tags) + accuracy reward (answer match)

- **Critical path:**
  1. Reasoning trace quality (via reject sampling) determines SFT ceiling
  2. SFT provides stable initialization for RL exploration
  3. Reward design shapes RL direction (format vs. accuracy tradeoff)

- **Design tradeoffs:**
  - Data: Aggressive filtering yields 9k high-quality samples from 10k—prioritizes quality over quantity
  - SFT vs. RL: Ablation shows ~10.1% gain from SFT, ~11.8% from RL (complementary)
  - Reward balance: Over-weighting format reward risks reward hacking

- **Failure signatures:**
  - SFT-only model skips steps or makes calculation errors (Figure 2)
  - RL overfits to format tags without improving answer accuracy
  - Reject sampling too aggressive → insufficient training data

- **First 3 experiments:**
  1. Replicate Figure 1: Measure performance gap with/without structured tables on your base MLLM
  2. Ablate SFT vs. RL: Train SFT-only and compare to full TURBO to isolate RL contribution
  3. Cross-domain test: Train on TAT-QA (finance), evaluate on TABMWP (math) to assess transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can structured tables be effectively incorporated as additional inputs during inference when they are available alongside table images?
- Basis in paper: [explicit] The limitations section states: "Exploring how to effectively incorporate structured tables as additional inputs during inference presents a promising direction for future research."
- Why unresolved: The current framework only uses structured tables during training (privileged information) but does not address scenarios where both modalities are available at test time.
- What evidence would resolve it: A systematic comparison of inference strategies (image-only, structured-only, combined) on benchmark datasets, measuring accuracy gains from dual-modality input.

### Open Question 2
- Question: How does table structural complexity—such as merged cells, nested headers, and irregular layouts—affect multimodal tabular reasoning performance?
- Basis in paper: [explicit] The paper excludes HiTab "because it involves multi-level tables, which present formatting challenges when converted to Markdown," and notes in limitations: "A more systematic analysis of how different table structures affect multimodal reasoning performance would be a valuable extension."
- Why unresolved: Current evaluation focuses on relatively clean, regular tables; complex layouts in datasets like HiTab and MMMU remain underexplored.
- What evidence would resolve it: Controlled experiments across datasets with varying structural complexity, with performance breakdown by table type (regular vs. merged cells vs. nested headers).

### Open Question 3
- Question: Does TURBO's effectiveness generalize across different MLLM architectures beyond Ovis2?
- Basis in paper: [inferred] All experiments use Ovis2 as the base model; the paper does not validate the framework on alternative MLLM architectures like Qwen2.5-VL or InternVL2.5 as base models.
- Why unresolved: The framework components (SFT + GRPO) may interact differently with different model architectures, vision encoders, or alignment strategies.
- What evidence would resolve it: Applying the same two-stage training pipeline to multiple base MLLMs and comparing relative improvements across architectures.

## Limitations

- Trace Quality Dependence: The method heavily relies on DeepSeek-R1 producing high-quality, coherent reasoning traces that match ground-truth answers, with filtering criteria not fully specified.
- Modality Transfer Gap: Limited empirical validation that the model truly learns to map visual cues to the same logical steps used with structured tables, especially for complex layouts.
- Reward Design Ambiguity: Exact computation of format and accuracy rewards is underspecified, potentially leading to reward hacking or suboptimal optimization.

## Confidence

- **High Confidence