---
ver: rpa2
title: 'Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via
  Efficient Multi-Token Prediction'
arxiv_id: '2504.03159'
source_url: https://arxiv.org/abs/2504.03159
tags:
- text
- title
- prompt
- language
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Placeholding Parallel Prediction (P3), a\
  \ method that addresses prompt brittleness in zero-shot text classification by predicting\
  \ token probabilities across multiple positions simultaneously in a single model\
  \ run. P3 appends placeholder tokens to the input and leverages the transformer\u2019\
  s inherent multi-token prediction capability to simulate comprehensive sampling\
  \ of generation paths."
---

# Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction

## Quick Facts
- arXiv ID: 2504.03159
- Source URL: https://arxiv.org/abs/2504.03159
- Authors: Junlang Qian, Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Zepeng Zhai, Kezhi Mao
- Reference count: 40
- Primary result: Reduces cross-prompt standard deviation by up to 98% and achieves accuracy improvements of up to 32% over next-token prediction baselines

## Executive Summary
This paper introduces Placeholding Parallel Prediction (P3), a method that addresses prompt brittleness in zero-shot text classification by predicting token probabilities across multiple positions simultaneously in a single model run. P3 appends placeholder tokens to the input and leverages the transformer's inherent multi-token prediction capability to simulate comprehensive sampling of generation paths. Experiments on seven datasets with LLaMA2-13B and LLaMA2-70B models show P3 significantly reduces prompt sensitivity while maintaining efficiency through single forward passes.

## Method Summary
P3 works by appending placeholder tokens (the `<unk>` token in LLaMA2) to the input sequence and extracting probability distributions at multiple future positions in a single forward pass. The method uses a hyperparameter η to control which positions to consider for classification: for LLaMA2-70B, it aggregates probabilities across a fixed range [0, η), while for LLaMA2-13B, it selects positions proportional to input length via slope tan(η). The paper demonstrates that classification based on later token positions exhibits lower variance across prompts than next-token prediction, achieving both accuracy improvements and robustness to prompt variation.

## Key Results
- Reduces cross-prompt standard deviation by up to 98% compared to next-token prediction
- Achieves accuracy improvements of up to 32% over next-token prediction baselines
- Maintains comparable performance even without prompts, significantly reducing reliance on prompt engineering
- Minimal overhead compared to next-token prediction, with ~8-15% FLOP increase for η=5

## Why This Works (Mechanism)

### Mechanism 1: Placeholder-Based Multi-Position Probability Extraction
Appending placeholder tokens enables extraction of class probability distributions across multiple future positions in a single forward pass. The transformer's unidirectional attention computes output logits for every prefix position. By appending `<ph>` tokens, P3 queries positions beyond the next token without committing to specific generation paths. The probability at position i approximates: P(x_{n+i}=c | x, <ph>^i) ≈ Σ_{prefixes} P(prefix) × P(c | prefix). Core assumption: the placeholder token carries minimal semantic bias while providing positional context.

### Mechanism 2: Prompt Sensitivity Decay Across Token Positions
Classification accuracy based on later token positions exhibits lower variance across prompts than next-token prediction. By the Markov assumption, nearby context (including prompt wording) has stronger influence on immediate predictions. As distance from prompt increases, the model's predictions increasingly reflect the input text's semantics rather than prompt-triggered patterns. The paper empirically shows later-token accuracies cluster more tightly.

### Mechanism 3: Hyperparameter η Controls Position Selection
A single hyperparameter η can tune the position range used for classification, with η≈5 providing a robust default across datasets. For LLaMA2-70B, η defines a fixed position range [0, η) for voting. For LLaMA2-13B, positions are selected proportional to input length via slope tan(η). The paper observes different positional behavior patterns between model scales (radial for 13B, vertical for 70B).

## Foundational Learning

- **Concept**: Causal (Unidirectional) Attention in Transformers
  - Why needed here: P3 exploits the property that autoregressive transformers compute outputs for every prefix position
  - Quick check question: Given input sequence [A, B, C], can the attention layer at position 2 attend to position 3? (Answer: No)

- **Concept**: Next-Token Prediction vs. Generative Classification
  - Why needed here: The paper positions P3 against two baselines—direct next-token probability scoring and generative sampling with path aggregation
  - Quick check question: For classification task with labels {positive, negative}, does next-token prediction query P("positive" | input) or P(token | input, "positive")? (Answer: P("positive" | input))

- **Concept**: Prompt Brittleness and Calibration
  - Why needed here: The paper's motivation hinges on the instability of LLM outputs under prompt variation
  - Quick check question: If a model outputs P(class_A)=0.6 for prompt P1 and P(class_A)=0.3 for semantically equivalent prompt P2, is this a calibration issue or a robustness issue? (Answer: Robustness issue)

## Architecture Onboarding

- **Component map**: Input Text + Prompt → Tokenizer → [Input Tokens | <ph>^n] → Transformer Forward Pass → Output Logits [position_0, position_1, ..., position_n] → η-Range Selection → Position Aggregation (voting/weighted) → Class Probability Scores → Prediction

- **Critical path**: The placeholder token selection (`<unk>` in LLaMA2) is the most fragile design choice. If this token has unexpected semantic associations, the approximation P(x_{n+i}=c | x, <ph>^i) ≈ marginal probability breaks down.

- **Design tradeoffs**:
  - More `<ph>` tokens → broader position coverage but increased FLOPs (proportional to sequence length)
  - Larger η → potentially more robust but risk of task drift if later positions lose classification signal
  - Single-token class labels → simpler implementation but limits applicability to multi-word categories

- **Failure signatures**:
  - If accuracy degrades monotonically with η, the model may not have stable classification signal at later positions
  - If cross-prompt std increases with η, the positional decay assumption is violated
  - If null-prompt performance drops significantly below prompted performance, P3 is still prompt-dependent

- **First 3 experiments**:
  1. Implement core P3 mechanism with LLaMA2 model, appending η placeholders and extracting logits at multiple positions
  2. Implement position selection strategy (fixed range for 70B, length-proportional for 13B) and aggregation method
  3. Evaluate on IMDb dataset with 30+ prompts, comparing accuracy and cross-prompt std against next-token baseline

## Open Questions the Paper Calls Out
None

## Limitations
- The placeholder token approximation assumes `<unk>` is semantically neutral, which is an engineering choice rather than theoretically validated
- The universal applicability of η=5 and differential behavior between model scales (13B vs 70B) lacks cross-model validation
- The paper does not benchmark against sampling-based alternatives like MuToR or L-MTP that may achieve similar robustness with different tradeoffs

## Confidence

- **High confidence**: The empirical observation that P3 reduces cross-prompt standard deviation (up to 98%) and improves accuracy (up to 32%) on tested datasets
- **Medium confidence**: The mechanism by which placeholder tokens approximate marginalization over generation paths, though placeholder neutrality is assumed
- **Low confidence**: The universal applicability of η=5 and the claim that P3 eliminates prompt engineering needs

## Next Checks

1. **Placeholder token neutrality test**: Implement P3 with three different placeholder tokens (`<unk>`, `<pad>`, a random high-frequency token) on IMDb and AGnews datasets. Compare accuracy and cross-prompt std. If results vary by >5% absolute accuracy or 20% relative std, the placeholder choice is non-neutral.

2. **Cross-model positional decay validation**: Apply P3 with η=5 to Mistral-7B-Instruct and Qwen-7B-Chat (without re-tuning). Plot accuracy and std at each position (0, 1, 2, ..., 20) for a fixed prompt set. If the positional decay pattern does not hold, document the model differences.

3. **Prompt-free performance edge case**: For IMDb, test P3 with η=5 and no prompt versus next-token prediction with the best single prompt. Compute accuracy and std. If null-prompt P3 accuracy is within 10% of prompted P3 but next-token null-prompt accuracy is <50%, this validates the claim of reduced prompt dependency.