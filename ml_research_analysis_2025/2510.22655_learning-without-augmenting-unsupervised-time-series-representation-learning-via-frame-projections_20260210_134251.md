---
ver: rpa2
title: 'Learning Without Augmenting: Unsupervised Time Series Representation Learning
  via Frame Projections'
arxiv_id: '2510.22655'
source_url: https://arxiv.org/abs/2510.22655
tags:
- learning
- data
- representations
- augmentations
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised learning approach for time
  series that avoids the use of handcrafted data augmentations, a common practice
  in contrastive learning methods. Instead, it generates views by projecting the data
  into orthonormal bases (e.g., Fourier transform) and overcomplete frames (e.g.,
  Gabor wavelets).
---

# Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections

## Quick Facts
- **arXiv ID:** 2510.22655
- **Source URL:** https://arxiv.org/abs/2510.22655
- **Reference count:** 40
- **Primary result:** Achieves 15–20% performance gains on time series tasks by replacing handcrafted augmentations with fixed orthonormal and overcomplete frame projections for self-supervised learning.

## Executive Summary
This paper introduces a self-supervised learning approach for time series that eliminates the need for handcrafted data augmentations, a common practice in contrastive learning methods. Instead, it generates views by projecting time series data into orthonormal bases (Fourier transform) and overcomplete frames (Gabor wavelets), creating embeddings on distinct manifolds shaped by their geometric biases. The method performs instance discrimination across these spaces and leverages lightweight latent space mappers to align and exploit their complementary structure. Empirical results on nine datasets across five tasks show significant performance improvements over existing self-supervised approaches without increasing training data diversity through augmentations.

## Method Summary
The method projects time series data into three distinct spaces: time domain, Fourier domain, and Gabor wavelet domain. Three separate ResNet-8 encoders process each view, with instance discrimination performed across all pairs using NT-Xent loss. Lightweight convolutional mappers (under 1,000 parameters) translate representations between domains. After pre-training, the Fourier and wavelet encoders are discarded during inference, with the time encoder's output mapped to the other domains via the trained mappers. The approach uses fixed transformations across datasets, contrasting with task-specific augmentations, and requires caching expensive Gabor transforms for efficient training.

## Key Results
- Achieves 15–20% performance gains over existing self-supervised approaches on nine datasets
- Eliminates need for handcrafted augmentations while maintaining or improving representation quality
- Uses fixed transformations (Fourier, Gabor) across all datasets rather than task-specific augmentations
- Lightweight mappers (<1k params) successfully approximate complex manifold relationships between domains

## Why This Works (Mechanism)

### Mechanism 1: Geometric Bias via Orthonormal and Overcomplete Projections
Replacing stochastic augmentations with fixed mathematical transformations (Fourier, Gabor) creates views on distinct manifolds that capture complementary signal features (global vs. local), leading to more robust representations than heuristic perturbations. The method projects time series into orthonormal basis (Fourier, $F_x$) and overcomplete frame (Gabor, $W_x$). Fourier captures global frequency content; Gabor captures localized time-frequency patterns. By performing instance discrimination across these views, the model learns to align features that are invariant across these specific geometric priors, rather than features that survive random noise/shift.

### Mechanism 2: Avoidance of Spurious Invariances
Standard augmentations often enforce invariances that harm downstream tasks (e.g., rotation invariance obscuring "standing vs. sitting"). This method theoretically limits such spurious invariances because the contrastive loss cannot be minimized if the encoder ignores the geometric distinctions between views. The paper posits that if an encoder becomes invariant to an unintended transformation, the NT-Xent loss is lower-bounded by the number of "near-positive" negatives. Since Fourier and Gabor are unitary/isometric, they preserve information rather than obscuring it, forcing the encoder to learn structural similarities rather than just surviving augmentations.

### Mechanism 3: Latent Space Geometry Approximation
High-dimensional latent spaces from different domains (Time vs. Fourier) are non-linearly related. Lightweight non-linear mappers allow a single encoder to exploit the geometry of multiple domains during inference. Directly aligning distinct manifolds (Time, Fourier, Wavelet) with linear maps fails because pairwise angles vary widely. The method trains lightweight ConvNet mappers ($\Phi$) to translate the time-domain representation $h(t)$ into the "virtual" Fourier/Wavelet representation spaces. This allows inference using only the cheap time-encoder while preserving the multi-view structure.

## Foundational Learning

- **Concept: Frames and Bases (Signal Processing)**
  - **Why needed here:** The method relies on *overcomplete frames* (Gabor) vs *orthonormal bases* (Fourier). Understanding that frames introduce redundancy (robustness) while bases are minimal is crucial for tuning the Gabor scales.
  - **Quick check question:** Why would a redundant representation (Gabor) be better for learning than a minimal one (Fourier) in noisy environments?

- **Concept: Instance Discrimination (NT-Xent)**
  - **Why needed here:** This is the training objective. Unlike supervised learning, it requires understanding positive pairs (same sample, different view) vs. negatives (different samples).
  - **Quick check question:** In a batch of $N$, how many positive and negative pairs does a single anchor sample generate in this 3-view setup?

- **Concept: Isometry / Unitary Transformations**
  - **Why needed here:** The paper argues its success is due to using transformations that preserve distance/information (isometry), unlike augmentations which distort it.
  - **Quick check question:** Does adding Gaussian noise preserve the distance between two samples in the input space? Does the Fourier Transform?

## Architecture Onboarding

- **Component map:** Time series $x$ -> Time encoder -> $h(t)$ -> Projector -> $z(t)$; Fourier transform $F_x$ -> Fourier encoder -> $h(F)$ -> Projector -> $z(F)$; Gabor transform $W_x$ -> Wavelet encoder -> $h(W)$ -> Projector -> $z(W)$; Mappers: $h(t) \to \text{estimated } h(F/W)$

- **Critical path:**
  1. **Preprocessing:** Compute $F_x$ and $W_x$ once and **cache** them. Gabor transforms are expensive ($\sim$seconds per batch) and must not be re-computed per epoch.
  2. **Training:** Pass views through parallel encoders. Compute NT-Xent across all pairs (Time-Fourier, Time-Wavelet, Fourier-Wavelet).
  3. **Inference:** Discard Fourier/Wavelet encoders. Run input through Time encoder only. Pass output through trained $\Phi$ mappers. Concatenate Time + Mapped features for the classifier.

- **Design tradeoffs:**
  - **Precompute cost:** High initial disk/IO cost to cache Wavelets for massive datasets, but saves massive compute during training loops.
  - **Mapper capacity:** Paper uses tiny ConvNets ($\sim$500 params). Increasing this might overfit the "virtual" geometry and lose generalization.

- **Failure signatures:**
  - **Mode Collapse:** If temperature $\tau$ in NT-Xent is too high or views too similar, loss may drop to zero without learning.
  - **Mapper Divergence:** If $L_{map}$ is high, the inference-time features will be garbage. Monitor the L1 loss of the mappers closely during the post-pre-training phase.

- **First 3 experiments:**
  1. **Sanity Check (Identity):** Train on a small subset (e.g., 1 subject) with only $L_{ID}$ and no mappers. Verify overfitting occurs to prove pipeline works.
  2. **Ablation (Views):** Train three versions: (Time-Fourier only), (Time-Wavelet only), (All three). Compare to determine which geometric bias helps your specific signal type.
  3. **Mapper Validation:** Compare inference performance using the full 3-encoder suite (Teacher) vs. 1-encoder + Mappers (Student) to measure the "approximation gap."

## Open Questions the Paper Calls Out

### Open Question 1
Can augmentation-free frame projection methods replace strong handcrafted augmentations in non-temporal modalities like images and audio?
The paper states this opens the door for future work to test whether the same holds in other modalities, such as images and audio. The paper exclusively validates the method on temporal sequence tasks where specific characteristics make augmentations challenging. It is unclear if orthonormal/overcomplete projections capture sufficient semantic variance in visual or audio data.

### Open Question 2
What is the theoretical explanation for why instance discrimination across distinct manifolds yields better representations than augmentation-based diversity?
The Limitations section notes we do not provide a theoretical explanation for the observed performance gains. We hypothesize implicit biases though this remains unverified. While the paper empirically demonstrates superior performance, it lacks a formal theoretical framework proving why leveraging complementary geometries is inherently superior to the diversity introduced by aggressive augmentations.

### Open Question 3
Is the proposed method effective for forecasting tasks that require capturing long-range dependencies?
The Limitations section states it also holds potential for forecasting tasks this remains an important avenue for future work. The experiments focused entirely on classification tasks. Forecasting requires precise reconstruction and extrapolation of temporal trends, which may rely on different features than those emphasized by the Gabor/Fourier instance discrimination objective.

## Limitations
- Requires significant preprocessing overhead (precomputing expensive Gabor transforms)
- May struggle with signals containing features poorly represented in frequency domain
- Theoretical analysis provides intuition but lacks rigorous guarantees for performance improvements

## Confidence
- **High confidence**: The empirical methodology is clearly described and reproducible. The ablation studies convincingly demonstrate the contribution of the frame projection approach over standard augmentations.
- **Medium confidence**: The theoretical claims about avoiding spurious invariances are plausible but not rigorously proven to be the primary mechanism behind the performance gains.
- **Medium confidence**: The generalization across diverse datasets and tasks is impressive, but the specific choice of Fourier and Gabor projections may be suboptimal for certain signal types not represented in the evaluation.

## Next Checks
1. **Ablation on projection types**: Systematically test alternative orthonormal bases (Wavelet Packet, DCT) and overcomplete frames to determine if the specific choice of Fourier/Gabor is critical or if the multi-view framework itself drives performance.
2. **Downstream task sensitivity**: Evaluate performance degradation when the downstream task relies on features that are poorly represented in frequency domain (e.g., fine-grained temporal patterns below Gabor scale resolution).
3. **Mapper capacity analysis**: Quantify the relationship between mapper network size and approximation quality by testing a range of mapper architectures (from linear to larger conv nets) to find the minimum viable capacity.