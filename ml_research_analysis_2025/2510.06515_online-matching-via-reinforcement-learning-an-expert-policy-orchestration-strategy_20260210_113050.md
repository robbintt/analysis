---
ver: rpa2
title: 'Online Matching via Reinforcement Learning: An Expert Policy Orchestration
  Strategy'
arxiv_id: '2510.06515'
source_url: https://arxiv.org/abs/2510.06515
tags:
- learning
- policy
- expert
- advantage
- appendix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning framework for orchestrating
  expert policies in online matching problems, focusing on applications like organ
  exchange where interpretability and performance are critical. The method uses a
  potential-based adversarial aggregation strategy to combine expert policies, extending
  prior work to handle estimated value functions.
---

# Online Matching via Reinforcement Learning: An Expert Policy Orchestration Strategy

## Quick Facts
- **arXiv ID:** 2510.06515
- **Source URL:** https://arxiv.org/abs/2510.06515
- **Reference count:** 40
- **Primary result:** A reinforcement learning framework orchestrates expert policies for online matching problems, achieving higher system efficiency than individual experts and RL baselines.

## Executive Summary
This paper introduces a reinforcement learning approach for orchestrating multiple expert policies in online stochastic matching problems, with a focus on interpretable and high-performance solutions for applications like organ exchange. The method leverages potential-based adversarial aggregation to adaptively combine expert policies, balancing exploration and exploitation through dynamic weight updates. Theoretical contributions include regret bounds and a novel finite-time bias bound for temporal difference learning, while empirical results on synthetic and organ exchange models demonstrate faster convergence and superior system efficiency compared to baselines.

## Method Summary
The framework orchestrates K expert policies for online matching using potential-based weight updates, dynamically adjusting the influence of each expert based on performance. The method extends prior work by incorporating estimated value functions via temporal difference learning and a neural actor-critic architecture for scalability. Key components include polynomial or exponential potentials for weight updates, advantage estimation via TD(0) or neural networks, and a policy network that outputs a distribution over experts. The approach is evaluated on stochastic matching models, including a diamond graph and an organ exchange scenario, with theoretical guarantees on regret and bias.

## Key Results
- The orchestrated policy converges faster and achieves higher system efficiency than individual experts and conventional RL baselines.
- Theoretical regret bounds are established: O(√T ln K)/(1-γ)² for polynomial potentials and O(ln K)/(1-γ)² for exponential potentials.
- A novel finite-time bias bound for temporal difference learning under non-stationary sampling is introduced, enhancing the method's theoretical foundation.

## Why This Works (Mechanism)
The method works by adaptively combining expert policies through potential-based weight updates, which balance exploration and exploitation. The use of estimated value functions via TD learning allows the system to learn from interactions without full model knowledge. The neural actor-critic architecture generalizes the approach to large state spaces while maintaining interpretability, crucial for high-stakes domains like organ exchange.

## Foundational Learning
- **Online Matching:** Allocating resources to requests as they arrive in real-time, essential for dynamic systems like organ exchange.
  - *Why needed:* Ensures the method addresses practical, real-world decision-making under uncertainty.
  - *Quick check:* Verify the compatibility graph and arrival rates are correctly modeled.
- **Potential-Based Weight Updates:** A technique for combining expert advice by adjusting weights based on performance.
  - *Why needed:* Enables adaptive orchestration of experts without prior knowledge of their quality.
  - *Quick check:* Monitor weight entropy to ensure balanced exploration.
- **Temporal Difference Learning:** An RL method for estimating value functions from sampled transitions.
  - *Why needed:* Provides advantage estimates for policy updates without full model knowledge.
  - *Quick check:* Validate that advantage estimates converge to true values over time.
- **Actor-Critic Architecture:** A neural network-based RL approach with separate policy (actor) and value (critic) components.
  - *Why needed:* Scales the method to large state spaces while preserving interpretability.
  - *Quick check:* Test the critic's ability to generalize across unseen states.

## Architecture Onboarding
- **Component map:** Expert policies (π₁–π₄) → Weight updates (qₜ(k|s)) → Actor network (Mϕ) → Advantage estimation (TD/NN) → Environment interaction → Weight updates.
- **Critical path:** Expert policies → Weight updates → Actor network → Advantage estimation → Policy improvement.
- **Design tradeoffs:** Tabular vs. neural advantage estimation (bias vs. scalability), polynomial vs. exponential potentials (convergence speed vs. adaptability).
- **Failure signatures:** Weight collapse to a single expert (overfitting), high bias in advantage estimates (slow learning), sparse rewards (slow convergence in organ exchange).
- **First experiments:** 1) Implement diamond graph MDP and expert policies. 2) Test weight update rules with varying η and p. 3) Compare V-values against Q-learning baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Neural network architecture details (hidden layer sizes, activation functions) are underspecified, potentially affecting reproducibility.
- Batch size and replay buffer configuration for deep advantage estimation are not provided, which may impact variance and convergence.
- Exact tie-breaking rules for expert policies are mentioned but not fully defined, leaving room for subtle behavioral differences.

## Confidence
- **High Confidence:** Theoretical regret bounds (polynomial and exponential potential cases), finite-time bias bound for TD learning under non-stationary sampling, and the tabular actor-critic algorithm structure.
- **Medium Confidence:** Empirical performance gains on stochastic matching models (diamond and organ exchange), due to unspecified NN hyperparameters and lack of ablation studies on architecture choices.
- **Low Confidence:** Claims about interpretability preservation in high-dimensional organ exchange settings, as no quantitative interpretability metrics are provided.

## Next Checks
1. Verify tabular results by implementing exact Bellman backup for small diamond graphs and comparing V-values against the RL orchestrator.
2. Test sensitivity of weight convergence to learning rate η and polynomial degree p in synthetic MDPs with known expert performance gaps.
3. Conduct ablation studies on NN architecture (hidden layer widths, batch size) to quantify impact on advantage estimation bias and policy learning speed.