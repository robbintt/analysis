---
ver: rpa2
title: Reasoning Models Generate Societies of Thought
arxiv_id: '2601.10825'
source_url: https://arxiv.org/abs/2601.10825
tags:
- shows
- reasoning
- gives
- asks
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Reasoning models like DeepSeek-R1 and QwQ-32B outperform traditional\
  \ instruction-tuned models by simulating multi-agent-like interactions\u2014what\
  \ the authors term a \"society of thought.\" This structure manifests through conversational\
  \ behaviors (question-answering, perspective shifts, conflict resolution) and socio-emotional\
  \ role diversity, with reasoning models activating significantly more diverse personality\
  \ traits and domain expertise within their reasoning traces. Mechanistic interpretability\
  \ shows that steering conversational features improves reasoning accuracy both directly\
  \ and indirectly by facilitating cognitive strategies like verification and backtracking."
---

# Reasoning Models Generate Societies of Thought

## Quick Facts
- arXiv ID: 2601.10825
- Source URL: https://arxiv.org/abs/2601.10825
- Authors: Junsol Kim; Shiyang Lai; Nino Scherrer; Blaise Agüera y Arcas; James Evans
- Reference count: 40
- Key outcome: Reasoning models simulate multi-agent-like interactions through conversational behaviors and socio-emotional role diversity, with conversational scaffolding accelerating reasoning improvement

## Executive Summary
Reasoning models like DeepSeek-R1 and QwQ-32B outperform traditional instruction-tuned models by generating what the authors term a "society of thought"—internal conversational structures that simulate multi-agent interactions. This manifests through question-answering, perspective shifts, conflict resolution, and significantly more diverse personality traits and domain expertise within reasoning traces. The paper demonstrates that steering conversational features improves reasoning accuracy both directly and indirectly by facilitating cognitive strategies like verification and backtracking. Reinforcement learning experiments reveal that base models spontaneously develop conversational behaviors when rewarded for accuracy, and fine-tuning with conversational scaffolding accelerates reasoning improvement compared to monologue-style training.

## Method Summary
The paper uses SAEs to identify conversational features in model activations, then steers these features to measure behavioral and accuracy effects. It employs LLM-as-judge (Gemini-2.5-Pro) to annotate reasoning traces for socio-emotional roles and cognitive strategies. RL experiments compare conversational vs. monologue scaffolding during fine-tuning, and diversity analysis measures personality/expertise distribution in reasoning traces. Structural equation modeling separates direct and indirect pathways from conversational steering to accuracy.

## Key Results
- DeepSeek-R1 and QwQ-32B show significantly higher personality and expertise diversity than instruction-tuned models
- Steering conversational features (e.g., Feature 30939) doubles accuracy on Countdown tasks while increasing verification and backtracking behaviors
- RL with conversational scaffolding accelerates early-stage learning compared to monologue scaffolding (38% vs 28% accuracy at step 40)
- The indirect pathway from conversational steering to accuracy through cognitive strategies is significant (β = .066, p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1: Conversational Features Gate Cognitive Strategy Deployment
- Steering conversational discourse markers facilitates verification, backtracking, and subgoal-setting behaviors
- Activation addition of Feature 30939 increases verification (Difference = 5.815), backtracking, subgoal setting, and backward chaining
- Structural equation modeling shows significant indirect pathway from conversational steering to accuracy mediated through cognitive behaviors (β = .066, p < 0.001)

### Mechanism 2: Perspective Diversity Reduces Convergent Error
- Reasoning models generate more diverse implicit perspectives (distinct personality profiles and domain expertise)
- Higher standard deviation in Big Five traits (especially agreeableness, neuroticism) and greater expertise embedding distance than instruction-tuned models
- SAE analysis confirms reasoning traces activate more diverse personality- and expertise-related features

### Mechanism 3: Conversational Scaffolding Accelerates RL Learning Dynamics
- Pre-training or fine-tuning on multi-agent conversational structures accelerates emergence of effective reasoning strategies during RL
- Models fine-tuned on conversational dialogue data reach higher accuracy faster than monologue-fine-tuned models
- The benefit comes from procedural structure of conversation (role-taking, turn-taking), not task-specific knowledge

## Foundational Learning

- **Sparse Autoencoders (SAEs) for Feature Interpretability**
  - Why needed here: Used to identify and steer specific conversational features (e.g., Feature 30939) in activation space
  - Quick check: Can you explain how an SAE maps residual stream activations to a sparse set of features, and how activation addition modifies model behavior?

- **Reinforcement Learning from Accuracy Rewards (STaR/PPO)**
  - Why needed here: RL experiments use accuracy-only rewards to train reasoning behaviors
  - Quick check: If a model is rewarded only for correct answers, why might conversational behaviors emerge spontaneously rather than just longer chains-of-thought?

- **Bales' Interaction Process Analysis (IPA) for Socio-Emotional Roles**
  - Why needed here: Used to annotate 12 socio-emotional roles (e.g., "gives suggestion," "shows disagreement") in reasoning traces
  - Quick check: What are the four higher-level categories of Bales' IPA, and which roles does DeepSeek-R1 exhibit more than DeepSeek-V3?

## Architecture Onboarding

- **Component map**: Input -> Backbone (DeepSeek-R1-Llama-8B or Qwen-2.5-3B) -> Feature Extraction (SAE Layer 15) -> Steering Module (activation addition) -> Evaluation (LLM-as-judge + accuracy scoring)

- **Critical path**: 
  1. Identify conversational features via SAE (conversation ratio >50%, sentence-onset activation)
  2. Validate feature interpretation with top-activating contexts
  3. Apply activation addition during generation with steering strengths s ∈ {-10, -5, 0, +5, +10}
  4. Measure accuracy and behavioral changes (conversational behaviors, cognitive strategies)
  5. Fit structural equation model to separate direct and indirect effects

- **Design tradeoffs**:
  - Steering strength: Higher values (+10, +15) show stronger behavioral effects but may degrade coherence
  - SAE layer choice: Middle layers (Layer 15) capture behavioral features
  - Conversation ratio threshold: >50% is conservative; lowering may include mixed features but increases noise

- **Failure signatures**:
  - Steering degrades accuracy: Check if feature is truly conversational and if steering strength is excessive (>+15)
  - No behavioral change: Verify feature activates near sentence onset (first 4 tokens)
  - SEM indirect effect not significant: Increase sample size or verify behavioral annotations

- **First 3 experiments**:
  1. Replicate steering of Feature 30939 on Countdown task (N=1,024 problems) with s = {-10, 0, +10}; verify accuracy doubling and increased conversational behaviors
  2. Test cross-feature generalization: Compare steering Feature 30939 vs. random conversational feature vs. random non-conversational feature; confirm specificity
  3. Run RL experiment with conversational scaffolding vs. monologue scaffolding on Llama-3.2-3B; measure early-stage accuracy divergence (step 40-70)

## Open Questions the Paper Calls Out

- How do diversity and coordination mechanisms operate within the reasoning traces of large language models to produce effective collective reasoning?
- Do conversational reasoning patterns and their benefits generalize to larger models (70B+ parameters) and more complex reasoning domains?
- What is the optimal composition and structure of internal "societies of thought" for different types of reasoning tasks?
- Can conversational scaffolding be systematically engineered to accelerate the emergence of specific cognitive strategies (verification, backtracking, subgoal setting) in reinforcement learning?

## Limitations

- Interpretability of SAE features remains unclear—cannot definitively prove conversational features cause cognitive strategies versus co-occurring
- Generalizability limited to specific model families and tasks; unclear if results extend to frontier-scale models or diverse reasoning domains
- Conflict resolution mechanism not directly measured—diversity may produce unproductive conflict rather than productive disagreement

## Confidence

**High Confidence**:
- Reasoning models exhibit more conversational behaviors and socio-emotional role diversity than instruction-tuned models
- Steering conversational features increases both conversational behaviors and accuracy
- RL with conversational scaffolding shows faster early-stage learning than monologue scaffolding

**Medium Confidence**:
- The indirect pathway (conversational steering → cognitive strategies → accuracy) is the primary mechanism
- Perspective diversity meaningfully reduces error convergence
- The observed behaviors constitute a true "society of thought" rather than superficial patterns

**Low Confidence**:
- SAE features perfectly capture the conversational-cognitive interface
- Results generalize to all reasoning tasks and model scales
- The scaffolding advantage persists at scale and across diverse domains

## Next Checks

1. **Ablation of Diversity Effects**: Systematically reduce perspective diversity in reasoning models and measure impact on reasoning accuracy across multiple tasks

2. **Cross-Domain Generalization Test**: Apply conversational feature steering to non-mathematical reasoning task and measure whether same indirect pathway holds

3. **Scalability Validation**: Repeat RL scaffolding experiments with models >30B parameters and longer training horizons; compare whether conversational scaffolding maintains early-stage advantage and whether benefit scales with model size