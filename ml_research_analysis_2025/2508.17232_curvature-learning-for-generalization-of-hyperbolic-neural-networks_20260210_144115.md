---
ver: rpa2
title: Curvature Learning for Generalization of Hyperbolic Neural Networks
arxiv_id: '2508.17232'
source_url: https://arxiv.org/abs/2508.17232
tags:
- hnns
- learning
- hyperbolic
- curvatures
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a PAC-Bayesian generalization bound for hyperbolic
  neural networks (HNNs), demonstrating that curvature significantly affects HNN generalization
  by influencing loss landscape smoothness. Based on this theoretical insight, the
  authors propose a sharpness-aware curvature learning method to optimize curvatures,
  thereby smoothing the loss landscape and improving generalization.
---

# Curvature Learning for Generalization of Hyperbolic Neural Networks

## Quick Facts
- **arXiv ID:** 2508.17232
- **Source URL:** https://arxiv.org/abs/2508.17232
- **Reference count:** 35
- **Primary result:** PAC-Bayesian bound shows curvature affects HNN generalization via loss landscape smoothness; sharpness-aware curvature learning improves performance across 4 tasks

## Executive Summary
This paper establishes a theoretical foundation for understanding how curvature affects the generalization of hyperbolic neural networks (HNNs) by deriving a PAC-Bayesian bound that explicitly links curvature to loss landscape sharpness. Based on this insight, the authors propose a sharpness-aware curvature learning method that optimizes curvature parameters to find flatter loss minima and improve generalization. The method uses implicit differentiation with Neumann series approximations to efficiently solve the resulting bi-level optimization problem, avoiding expensive unrolling of inner optimization steps.

## Method Summary
The method formulates curvature learning as a bi-level optimization problem where the inner loop trains HNN weights using Sharpness-Aware Minimization (SAM) for a fixed curvature, and the outer loop optimizes curvature by minimizing a scope sharpness measure computed at the inner solution. Implicit differentiation is employed to compute curvature gradients without unrolling, using Neumann series approximations of the Hessian inverse to maintain computational efficiency. The approach is applied to standard HNN architectures with hyperbolic operations (Möbius addition, exponential/logarithmic maps) and evaluated on classification, long-tailed learning, noisy label learning, and few-shot learning tasks.

## Key Results
- Curvature significantly affects HNN generalization by influencing loss landscape smoothness, validated through PAC-Bayesian bounds
- Sharpness-aware curvature learning consistently improves performance across four diverse learning tasks compared to fixed-curvature baselines
- Implicit differentiation with Neumann series approximations enables efficient curvature optimization with O(d) complexity instead of O(d³)
- Learned curvatures adapt to dataset characteristics, with optimal values varying across tasks and data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The curvature of hyperbolic space directly influences the generalization of Hyperbolic Neural Networks (HNNs) by affecting the smoothness (or sharpness) of the loss landscape.
- Mechanism: A PAC-Bayesian generalization bound is derived for HNNs, showing that the generalization error is linked to a "scope sharpness" term that measures how rapidly the loss can increase within a small neighborhood of the model's parameters. The derivation analytically demonstrates that the manifold's curvature is a component of this sharpness term, establishing a causal link: inappropriate curvature leads to a sharper loss landscape and worse generalization, while appropriate curvature promotes a flatter landscape and better generalization.
- Core assumption: The loss function, hyperbolic multinomial logistic regression, and feed-forward functions in the HNN are Lipschitz continuous with respect to their inputs and parameters.
- Evidence anchors: [abstract] "...demonstrating that curvature significantly affects HNN generalization by influencing loss landscape smoothness." [section 4] Theorem 8 provides the full derivation of the PAC-Bayesian bound, and the subsequent discussion explicitly connects the sharpness term to curvature.
- Break condition: The theoretical guarantee relies on Lipschitz continuity assumptions. If these are strongly violated (e.g., in highly non-smooth architectures), the derived bound may not hold, weakening the mechanism's theoretical foundation.

### Mechanism 2
- Claim: Generalization can be improved by learning the optimal curvature via a sharpness-aware optimization process.
- Mechanism: The method formulates curvature learning as a bi-level optimization problem where the inner level trains the HNN's weights with a fixed curvature, and the outer level optimizes the curvature to minimize a "scope sharpness" measure that captures the sharpness of the local minimum found in the inner loop. By minimizing this sharpness measure, the method actively searches for a curvature that leads the HNN to converge in a flatter region of the loss landscape, thereby improving generalization as per the bound from Mechanism 1.
- Core assumption: A bi-level optimization framework where the optimal inner-level weights are a differentiable function of the curvature.
- Evidence anchors: [abstract] "...propose a sharpness-aware curvature learning method to optimize curvatures, thereby smoothing the loss landscape and improving generalization." [section 5] The method is formally defined by the bi-level optimization problem in Eq. (115).
- Break condition: The bi-level optimization assumes the inner problem converges to a unique minimum. If the inner loss landscape is highly non-convex with multiple minima, the optimization could become unstable, making gradient-based curvature learning ineffective.

### Mechanism 3
- Claim: An implicit differentiation algorithm with Hessian approximations makes the bi-level optimization computationally tractable.
- Mechanism: Computing the exact gradient of the outer objective w.r.t. curvature requires differentiating through the entire inner optimization path (unrolling), which is prohibitively expensive. The method avoids unrolling by using implicit differentiation, which computes the gradient using only the solution. To avoid computing and inverting the full Hessian matrix (O(d³) complexity), the method employs a Neumann series approximation, reducing complexity to O(d) and making the algorithm scalable to modern network sizes.
- Core assumption: The Hessian of the inner loss at the solution is invertible, and a low-order Neumann series provides a good approximation of its inverse.
- Evidence anchors: [abstract] "An implicit differentiation algorithm is introduced to efficiently solve the resulting bi-level optimization problem by approximating curvature gradients." [section 5.2] Eqs. (136) and (140) detail the implicit differentiation formula and the Neumann series approximation.
- Break condition: If the Hessian is poorly conditioned or the Neumann series diverges, the approximation will be poor, leading to incorrect curvature gradients and failed optimization.

## Foundational Learning

- Concept: **Hyperbolic Geometry & Operations (Möbius addition, exponential/logarithmic maps)**
  - Why needed here: These are the fundamental building blocks of HNNs. Understanding them is critical to grasp how curvature alters network operations and how the loss landscape changes.
  - Quick check question: Can you explain how the exponential map projects a vector from a tangent space to the hyperbolic manifold and how this operation depends on curvature?

- Concept: **PAC-Bayesian Generalization Bounds**
  - Why needed here: This is the theoretical engine of the paper. The entire motivation and design stem from the derived bound linking curvature to generalization via sharpness.
  - Quick check question: In the context of this paper, what does the PAC-Bayesian bound state is the key factor influencing generalization, and how does curvature enter the equation?

- Concept: **Bi-Level Optimization & Implicit Differentiation**
  - Why needed here: This is the core algorithmic contribution for learning curvature efficiently. Implementing or modifying the method requires understanding this optimization structure.
  - Quick check question: Why is unrolling the inner optimization loop computationally problematic, and how does implicit differentiation provide a more efficient alternative for computing hypergradients?

## Architecture Onboarding

- Component map: HNN Backbone (ResNet with hyperbolic layers) -> Curvature Parameter(s) -> Sharpness Evaluator -> Bi-Level Optimizer

- Critical path:
  1. **Initialization:** Initialize network weights and default curvature
  2. **Inner Loop (SAM Training):** For fixed curvature, train weights for T steps using SAM to find w*(c)
  3. **Outer Loop (Curvature Update):** Using converged w*(c), compute direct and implicit gradients, then update curvature
  4. **Iteration:** Repeat inner and outer loops until convergence

- Design tradeoffs:
  - **Perturbation Radius (ρ̂):** Key hyperparameter controlling sharpness measurement; too small may not smooth landscape enough, too large can destabilize training
  - **Neumann Series Order (J):** Controls approximation accuracy vs computation; paper uses J=2 for efficiency
  - **Inner Loop Iterations (T):** Paper uses T=2; more iterations may lead to better inner solution but slow down curvature learning

- Failure signatures:
  - **Diverging Curvature:** Curvature may grow unbounded or become negative, leading to numerical instability in hyperbolic operations
  - **No Generalization Gain:** If sharpness measure does not decrease, the mechanism is not engaging; check perturbation radius and gradient computation
  - **High Training Loss:** Over-smoothing loss landscape can prevent reaching low training loss; monitor training loss vs sharpness trade-off

- First 3 experiments:
  1. **Curvature Ablation:** Train HNNs with range of fixed curvatures on validation task; plot test accuracy vs curvature to verify theoretical relationship
  2. **Sharpness Visualization:** After training with proposed method vs standard SGD/SAM, compute and plot sharpness measure for both to validate flatter minima
  3. **Convergence Tracking:** Log scope sharpness measure across training epochs; successful run should show decreasing trend while tracking training/validation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the derived generalization bounds and curvature learning framework be extended to product manifolds with multiple heterogeneous curvatures?
- Basis: [explicit] Section 8 (Conclusion) states the intention to "study generalization for product manifolds" to better represent real-world data with complex hierarchical structures.
- Why unresolved: The current theoretical analysis and methodological implementation are restricted to a single hyperbolic space with a single curvature.
- What evidence would resolve it: A theoretical derivation of PAC-Bayesian bounds for product manifolds and empirical validation on datasets requiring multi-curvature representations.

### Open Question 2
- Question: How can the theoretical analysis be generalized to relax constraints such as angular bounds and global Lipschitz continuity?
- Basis: [explicit] Section 3.3 ("Discussion about assumptions") notes that current assumptions are mild but limiting, and the authors intend to "extend our theoretical analysis by relaxing the current assumptions."
- Why unresolved: Current proofs rely on specific angular constraints and global Lipschitz continuity assumptions that may not strictly hold in all practical scenarios.
- What evidence would resolve it: Proofs utilizing localized or parameterized Lipschitz bounds that maintain theoretical guarantees without requiring strict angular constraints.

### Open Question 3
- Question: Can the sensitivity to the perturbation radius be eliminated through scale-invariant algorithms or automated learning strategies?
- Basis: [explicit] Section 7.5.1 ("Analyses of Hyper-Parameters") acknowledges sensitivity to perturbation radius and proposes developing a "scale-invariant SAM algorithm" and "learning-to-learn strategies" for future work.
- Why unresolved: The method's performance varies significantly with the perturbation radius due to the scale-dependency of the sharpness measure.
- What evidence would resolve it: A modified algorithm that normalizes sharpness to be invariant to parameter scaling, or a meta-learning approach that automatically adapts the perturbation radius during training.

## Limitations

- Theoretical guarantees depend on Lipschitz continuity assumptions that may not hold for complex modern architectures
- Bi-level optimization assumes the inner problem converges to a unique minimum, which may not occur in highly non-convex landscapes
- Neumann series approximation introduces approximation error that grows with Hessian spectral radius, potentially limiting accuracy for ill-conditioned Hessians

## Confidence

- **High Confidence:** The theoretical PAC-Bayesian framework linking curvature to generalization via sharpness measures. The mathematical derivations are rigorous and the connections are explicit.
- **Medium Confidence:** The practical effectiveness of the sharpness-aware curvature learning method. While experiments show consistent improvements, the method's sensitivity to hyperparameters suggests implementation-specific behavior.
- **Low Confidence:** The scalability of the approach to very large networks and datasets. Second-order methods still face practical limitations compared to first-order alternatives despite computational optimizations.

## Next Checks

1. **Ablation on Neumann Series Order:** Systematically vary the Neumann series approximation order and measure both approximation accuracy and final test performance to quantify the trade-off between computational efficiency and optimization quality.

2. **Sharpness Landscape Analysis:** For a fixed architecture, train with multiple fixed curvatures and with learned curvature, then quantify the actual loss landscape sharpness to verify that learned curvature consistently produces flatter minima.

3. **Generalization to Different Architectures:** Implement the method on architectures beyond standard CNNs (e.g., transformers or graph neural networks) to test the framework's generalizability beyond the evaluated settings.