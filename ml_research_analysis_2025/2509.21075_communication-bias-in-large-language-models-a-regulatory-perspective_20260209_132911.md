---
ver: rpa2
title: 'Communication Bias in Large Language Models: A Regulatory Perspective'
arxiv_id: '2509.21075'
source_url: https://arxiv.org/abs/2509.21075
tags:
- llms
- bias
- content
- data
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews risks of biased outputs in large language models
  (LLMs) and their societal impact, focusing on regulatory frameworks like the EU's
  AI Act and Digital Services Act. It highlights that LLMs serve as the primary interface
  between users and AI, influencing individual and collective opinions, including
  voting decisions.
---

# Communication Bias in Large Language Models: A Regulatory Perspective

## Quick Facts
- arXiv ID: 2509.21075
- Source URL: https://arxiv.org/abs/2509.21075
- Reference count: 0
- Primary result: Reviews risks of biased outputs in large language models (LLMs) and their societal impact, focusing on regulatory frameworks like the EU's AI Act and Digital Services Act.

## Executive Summary
This paper examines communication bias in large language models (LLMs) and its potential to influence public discourse and democratic processes. The authors analyze how current EU regulatory frameworks—primarily the AI Act and Digital Services Act—address bias mainly as a byproduct of broader safety measures rather than targeting communication bias directly. They propose that competition policy and ongoing technology design governance should complement existing value chain regulation and content moderation to promote diverse, transparent AI systems and mitigate bias while fostering a balanced digital information ecosystem.

## Method Summary
This regulatory/policy analysis paper reviews legal frameworks for addressing "communication bias" in LLMs through legal analysis and literature review. No experimental ML methods or datasets are provided. The authors reference the GermanPartiesQA benchmark for evaluating political alignment of LLMs but do not include the dataset itself. The analysis focuses on identifying regulatory gaps in EU frameworks (AI Act, DSA, DMA) and proposing complementary approaches.

## Key Results
- LLMs serve as the primary interface between users and AI, influencing individual and collective opinions including voting decisions
- Current EU regulations address bias mainly as a byproduct of broader safety measures, with limited direct focus on communication bias
- The paper proposes complementing value chain regulation and content moderation with competition policy and ongoing technology design governance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs may systematically favor certain perspectives through training data patterns and generative expression, potentially influencing user attitudes and public discourse.
- Mechanism: LLMs identify probability distributions in training data and generate new instances similar to original data. This can amplify preconceptions present in algorithms or training data. Communication bias emerges from "generative expression" rather than solely from data imbalance or uncritical user uptake.
- Core assumption: Users interact with LLMs as primary information interfaces and may not detect subtle bias in outputs.
- Evidence anchors:
  - [abstract] "LLMs are particularly noteworthy due to their immediate interaction with consumers, functioning as the primary interface between users and AI"
  - [section 1] "LLMs can reflect and amplify existing preconceptions present in their algorithms or training data, leading to communication bias"
  - [corpus] Weak direct evidence; neighbor papers address XAI compliance and bias detection methods but not the specific generative expression pathway described here.
- Break condition: If users do not rely on LLMs for opinion formation, or if outputs are demonstrably neutral across contested topics, this mechanism weakens.

### Mechanism 2
- Claim: Current EU regulatory frameworks address communication bias primarily as a byproduct of broader safety and content moderation measures, with limited direct targeting.
- Mechanism: The AI Act imposes ex ante requirements (risk management, data governance, transparency) focused on pre-deployment. The DSA imposes ex post obligations (content removal, algorithmic transparency) focused on outputs. Neither framework was devised with LLMs' potential effects on worldviews and voting decisions as a primary target.
- Core assumption: Pre-market auditing and post-market content moderation are insufficient for addressing subtle, ongoing communication bias.
- Evidence anchors:
  - [abstract] "Current regulations address bias mainly as a byproduct of broader safety measures, with limited focus on communication bias"
  - [section 3.3] "the AI Act's strongest mechanisms for bias correction converge in the pre-deployment phase, with more reflexive and less specific post-deployment safeguards"
  - [corpus] Neighbor paper "Compliance of AI Systems" examines AI Act compliance but does not evaluate effectiveness against communication bias specifically.
- Break condition: If existing auditing requirements prove sufficient to detect and correct communication bias in practice, this claim weakens.

### Mechanism 3
- Claim: Competition policy and ongoing technology design governance may complement value chain regulation and content moderation by diversifying the AI ecosystem and enabling user self-governance.
- Mechanism: Low barriers to entry dilute the influence of dominant players, facilitating diverse outputs. User self-governance allows users to influence parameters of data collection, model training, and output evaluation throughout the system lifecycle. The Digital Markets Act (DMA) provides a template for promoting competition and data access.
- Core assumption: A competitive market with user input will produce a more pluralistic information environment than concentrated provider-driven governance.
- Evidence anchors:
  - [abstract] "The paper proposes complementing value chain regulation and content moderation with competition policy and ongoing technology design governance"
  - [section 4.3] "Low barriers to entry can dilute the influence of dominant players and facilitate a more diverse range of outputs, mitigating the risks associated with concentrated power"
  - [corpus] Weak evidence; neighbor papers do not test competition as a bias mitigation strategy for LLMs.
- Break condition: If competition simply multiplies biased sources without improving pluralism, or if user self-governance is co-opted by dominant interests, this mechanism fails.

## Foundational Learning

- Concept: **Communication bias vs. data bias vs. automation bias**
  - Why needed here: The paper distinguishes communication bias (systematic favoring of perspectives in outputs) from data bias (skewed training data) and automation bias (uncritical user acceptance). Understanding this distinction is necessary for evaluating regulatory responses.
  - Quick check question: If an LLM outputs a politically skewed summary using balanced training data, is this data bias or communication bias?

- Concept: **Ex ante vs. ex post regulation**
  - Why needed here: The AI Act operates ex ante (pre-market requirements on system architecture) while the DSA operates ex post (post-market content management). The paper argues both miss ongoing communication bias.
  - Quick check question: Which regulatory approach would be more effective for catching bias that emerges only after widespread deployment?

- Concept: **Sycophancy in LLMs**
  - Why needed here: The paper notes LLMs can tailor responses to align with user prompts or personas, exacerbating echo chambers. This behavior complicates bias detection and mitigation.
  - Quick check question: If a model changes its political stance based on how a question is phrased, what risk does this pose for public discourse?

## Architecture Onboarding

- Component map:
  - Training data layer → Model layer → Output layer → User interface layer → Regulatory layer
  - (Training data → Neural network → Generated text → Chatbots/search engines → AI Act/DSA/DMA)

- Critical path: Training data curation → Model training and safety fine-tuning (which may introduce additional bias) → Deployment → User interaction (with sycophantic adaptation) → Output generation → Post-market monitoring.

- Design tradeoffs:
  - Safety guardrails vs. bias introduction: Efforts to limit toxic content may inadvertently introduce communication bias.
  - Pre-market rigor vs. post-market adaptability: AI Act's pre-deployment focus may miss emergent bias patterns.
  - Content moderation vs. free expression: Removing harmful content risks censorship perceptions and may suppress legitimate viewpoints.
  - Provider control vs. user self-governance: Balancing professional oversight with user influence on system design.

- Failure signatures:
  - LLMs consistently aligning with specific political positions across diverse prompts.
  - Models refusing to generate content opposing certain positions while readily supporting others.
  - Feedback loops where AI-generated outputs train future models, amplifying initial biases.
  - Concentrated market power enabling a few providers to shape public knowledge.
  - Post-deployment monitoring that is provider-driven and reactive rather than independent and proactive.

- First 3 experiments:
  1. Benchmark test: Use a dataset like GermanPartiesQA (cited in paper) to evaluate political alignment across multiple LLMs and measure consistency.
  2. Sycophancy probe: Design prompt experiments where the same factual query is framed with different persona cues to test whether outputs shift to align with implied user opinions.
  3. Audit simulation: Review AI Act requirements (data governance documentation, risk management systems, transparency obligations) against a sample LLM to identify gaps in addressing communication bias specifically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can General Purpose AI (GPAI) models be legally classified as "intermediary services" or "online platforms" under the Digital Services Act (DSA) to trigger systemic risk mitigation obligations?
- Basis in paper: [explicit] The authors state on Page 6 that "It remains contentious whether GPAI models such as LLMs fall within the DSA’s definitions, and such classifications hinge upon, among other things, interpretations of how LLMs store and provide user inputs and outputs."
- Why unresolved: Current definitions in the DSA were devised primarily for traditional hosting and social media platforms, and LLMs often operate as standalone services that do not publicly disseminate user-generated content in the same manner, creating legal ambiguity.
- What evidence would resolve it: Formal legal rulings or guidance from the European Commission or Digital Services Coordinators clarifying the status of GPAI models, or legislative amendments to the DSA explicitly including or excluding LLMs.

### Open Question 2
- Question: How can benchmark datasets be designed to effectively isolate and measure "communication bias" (systemic favoring of perspectives) distinct from factual inaccuracy or safety refusals?
- Basis in paper: [explicit] On Page 10, the authors suggest that "Developing benchmark datasets, such as the GermanPartiesQA, can help evaluate the alignment of LLMs with political party positions and detect biases in their responses."
- Why unresolved: Communication bias is described as "subtle, multifaceted, and nuanced" (Page 7), often residing in claims that are neither strictly true nor false, making it difficult to capture through standard accuracy or toxicity metrics.
- What evidence would resolve it: The creation and validation of standardized evaluation suites that can quantitatively score LLMs on political or cultural alignment independent of factual accuracy benchmarks.

### Open Question 3
- Question: What specific institutional mechanisms are required to operationalize "user self-governance" in LLMs, allowing users to influence model architecture or output refinement without creating security vulnerabilities?
- Basis in paper: [inferred] The authors propose on Page 9 that regulation must "incorporate a significant extent of user control in terms of self-governance" regarding data collection and model training, but they note the risk of "malicious and bad-faith actors" (Page 9) without detailing how to balance these conflicting forces.
- Why unresolved: While the paper advocates for shifting oversight to a "participatory market-centered model" (Page 10), there is currently no established framework for how non-expert users can safely and effectively contribute to the lifecycle governance of complex AI systems.
- What evidence would resolve it: Pilot studies or regulatory sandboxes demonstrating interface designs where user feedback successfully modifies model behavior to reduce bias without compromising system integrity or safety.

## Limitations

- The paper provides a theoretical regulatory analysis rather than empirical validation of communication bias mechanisms
- No specific evaluation methods, prompts, or statistical approaches are provided for quantifying communication bias
- The proposed solutions of competition policy and user self-governance lack demonstrated implementation or testing

## Confidence

- High Confidence: The descriptive analysis of EU regulatory frameworks (AI Act, DSA, DMA) and their structural limitations regarding communication bias
- Medium Confidence: The mechanism of communication bias through generative expression and its potential amplification of existing preconceptions
- Low Confidence: The proposed solution of competition policy and ongoing technology design governance as effective mitigants for communication bias

## Next Checks

1. **Benchmark Replication**: Replicate the GermanPartiesQA evaluation approach to empirically measure political alignment across multiple LLMs using party position statements as ground truth, testing the communication bias hypothesis.

2. **Regulatory Gap Analysis**: Conduct a systematic audit of AI Act compliance requirements against actual LLM deployments to identify specific gaps in addressing communication bias, moving beyond theoretical structural analysis to practical assessment.

3. **Competition Impact Study**: Design a study comparing bias patterns across LLM providers of different sizes and market positions to test whether increased competition correlates with greater output diversity and reduced systematic bias.