---
ver: rpa2
title: 'RCScore: Quantifying Response Consistency in Large Language Models'
arxiv_id: '2510.26193'
source_url: https://arxiv.org/abs/2510.26193
tags:
- instruction
- style
- rcscore
- search
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RCScore addresses the problem of evaluating LLM sensitivity to
  instruction style, which current benchmarks often overlook. The method quantifies
  how different instruction formulations affect model responses across three dimensions:
  Structurality (syntactic patterns), Lexicality (vocabulary consistency), and Coherence
  (logical organization).'
---

# RCScore: Quantifying Response Consistency in Large Language Models

## Quick Facts
- **arXiv ID**: 2510.26193
- **Source URL**: https://arxiv.org/abs/2510.26193
- **Reference count**: 39
- **Primary result**: Instruction style shifts accuracy by up to 16.7 percentage points; strong positive correlation (r≈0.79) between stylistic consistency and task accuracy

## Executive Summary
RCScore quantifies how different instruction formulations affect LLM responses across three dimensions: Structurality (syntactic patterns), Lexicality (vocabulary consistency), and Coherence (logical organization). Experiments across ten models and four reasoning benchmarks show instruction style can shift accuracy by up to 16.7 percentage points. The framework reveals that larger models and deterministic decoding (greedy search) produce more stylistically stable outputs, with greedy search improving consistency by 7-13% compared to beam search while also yielding better average accuracy in most cases.

## Method Summary
The method generates responses for each benchmark problem under four instruction styles (Declarative, Interrogative, Exclamative, Imperative), then computes three RCScore dimensions: Structurality (dependency parsing + Jaccard similarity), Lexicality (TF-IDF cosine + ROUGE-L), and Coherence (chunk alignment + order correlation). These are aggregated into a Cross-Response Similarity (CRS) vector through pairwise comparisons across style pairs. The framework evaluates ten models on four reasoning benchmarks, correlating CRS scores with task accuracy to assess consistency as a proxy for reliability.

## Key Results
- Instruction style can shift accuracy by up to 16.7 percentage points across models and benchmarks
- Strong positive correlation (r≈0.79) between stylistic consistency and task accuracy
- Greedy search improves CRS by 7-13% compared to beam search while also yielding better average accuracy in most cases
- Larger models (e.g., LLaMA 3.3-70B, Qwen 2.5-72B) produce more stylistically stable outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stylistic self-consistency across instruction formulations serves as a proxy for task accuracy.
- Mechanism: When models produce lexically and structurally similar responses regardless of instruction phrasing (quantified via CRS), this reflects robust internal problem representations rather than surface-level prompt matching.
- Core assumption: Consistency in response characteristics indicates depth of reasoning capability, not merely stable output distributions.
- Evidence anchors:
  - "Cross-Response Similarity (CRS) reveals a strong positive correlation (r≈0.79) between stylistic consistency and task accuracy, suggesting consistency as a proxy for reliability."
  - Table 2: Lexicality showed Pearson's r ≈ 0.79 under greedy search; all RCScore dimensions demonstrated statistically significant positive correlations (p < 0.001) with mean task accuracy across 40 model-benchmark pairs.
  - "Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity" similarly argues that consistency is necessary to distinguish hallucinations from creative outputs, supporting the consistency-reliability link.

### Mechanism 2
- Claim: Deterministic decoding (greedy search) enhances both stylistic consistency and average accuracy compared to stochastic decoding (beam search).
- Mechanism: Greedy search eliminates token-level variability at each step, reducing the "noise" in response generation that amplifies instruction style effects.
- Core assumption: The consistency gains from greedy decoding reflect genuine capability stability, not merely convergence to a single output pattern.
- Evidence anchors:
  - "greedy search improving CRS by 7-13% compared to beam search while also yielding better average accuracy in most cases"
  - Figure 7: Greedy decoding achieved higher average accuracy for a majority of models on AIME, MATH-500, and GSM8K. Figure 6 shows consistent CRS improvements across all model families.

### Mechanism 3
- Claim: Larger model scale correlates positively with cross-style consistency.
- Mechanism: Larger parameter counts enable more robust internal representations that are less perturbed by surface-level instruction variations.
- Core assumption: Scale-driven consistency reflects improved generalization, not memorization of more instruction patterns.
- Evidence anchors:
  - "Larger models...produce more stylistically stable outputs, with model scale correlating positively with cross-style consistency."
  - Table 1: LLaMA 3.3-70B achieves CRS of 0.67 (greedy) on GSM8K vs. 0.56 for LLaMA 3.2-3B. Similar patterns hold across Qwen and Gemma families.

## Foundational Learning

- Concept: TF-IDF vectorization and cosine similarity
  - Why needed here: Core component of the Lexicality dimension; measures global vocabulary distribution similarity across generated responses.
  - Quick check question: How does TF-IDF weighting differ from raw term frequency in capturing document similarity, and why might it fail on very short texts?

- Concept: Dependency parsing and syntactic pattern extraction
  - Why needed here: Foundation of Structurality dimension; extracts ⟨POS_tag, dependency_relation, head_POS⟩ tuples to compare syntactic structures independent of lexical content.
  - Quick check question: What linguistic information is captured in the syntactic pattern ⟨post, depr, posh⟩, and why abstract away from specific words?

- Concept: BERTScore for semantic alignment
  - Why needed here: Used to identify semantically aligned sentence pairs before comparing structural characteristics; ensures structurality comparisons are semantically meaningful.
  - Quick check question: Why align sentences semantically before computing syntactic similarity, rather than comparing documents directly?

## Architecture Onboarding

- Component map:
  Style Variation Generator -> Response Generator -> RCScore Calculator (3 parallel paths) -> CRS Aggregator

- Critical path:
  1. Generate responses for each problem under all 4 instruction styles (S1-S4)
  2. Compute RCScore dimensions for all 6 pairwise style combinations
  3. Aggregate per-dimension averages into final CRS vector

- Design tradeoffs:
  - Lightweight style prefixes vs. full paraphrasing: Avoids semantic drift but may miss deeper stylistic sensitivities
  - Uniform dimension weights (0.33 each): Simple but may not reflect task-specific importance
  - Four clause types based on Huddleston et al. (2002): Linguistically grounded but limited coverage of real-world instruction variation

- Failure signatures:
  - High CRS + low accuracy: Model produces consistent but incorrect responses (false confidence indicator)
  - Low CRS + high accuracy: Model reaches correct answers via different paths (may be acceptable, but indicates prompt fragility)
  - High Style Sensitivity Index (SSI) on simple tasks (e.g., GSM8K SSI >0.3): Signals fragile problem representations

- First 3 experiments:
  1. Replicate CRS-accuracy correlation on GSM8K with 2-3 models to validate the r≈0.79 claim before broader deployment
  2. Compare beam vs. greedy decoding on the same model-benchmark pairs to verify 7-13% CRS improvement and assess whether accuracy gains hold for your target tasks
  3. Test an instruction style outside the four used (e.g., conditional/hypothetical framing) to evaluate framework generalization

## Open Questions the Paper Calls Out

- Does the strong correlation between Cross-Response Similarity (CRS) and task accuracy generalize to subjective domains such as creative writing or emotional support? (The authors note their focus on mathematical and reasoning tasks "may not generalize to more subjective domains like creative writing or emotional support, where style sensitivity might manifest differently.")

- To what extent do the RCScore dimensions (Structurality, Lexicality, Coherence) align with human perceptions of consistency and quality? (The framework currently relies on automated NLP metrics without comparative human judgment studies.)

- Can task-dependent or empirically derived weighting schemes for Structurality, Lexicality, and Coherence improve the predictive power of RCScore? (The current methodology assigns fixed, equal weights (0.33) to all three dimensions.)

- Is the RCScore framework effective when applied to languages other than English or stylistic variations like code-switching? (The implementation relies on specific English-language NLP tools and a typology of four English clause types.)

## Limitations

- The framework assumes consistency reflects depth of reasoning capability, but surface-level stability could mask systematic errors or memorization of instruction-response patterns
- The four instruction styles used (Declarative, Interrogative, Exclamative, Imperative) represent a limited subset of real-world prompt variations, potentially underestimating true sensitivity
- The strong correlation between consistency and accuracy may not indicate generalizable reasoning capability across all domains

## Confidence

- **High confidence**: The RCScore methodology (Structurality, Lexicality, Coherence dimensions) is well-specified and reproducible. The CRS calculation procedure and the positive correlation between consistency and accuracy are empirically demonstrated.
- **Medium confidence**: The claim that greedy search improves both consistency and accuracy has strong empirical support but requires validation across different model families and tasks. The scale-consistency relationship shows clear patterns but may be confounded by training data characteristics.
- **Low confidence**: The assumption that consistency serves as a reliable proxy for reasoning capability across all domains. The framework's sensitivity to instruction styles outside the four tested variants remains unknown.

## Next Checks

1. **Break the correlation**: Test the framework on tasks where high accuracy can be achieved through pattern matching (e.g., simple classification) to verify that consistency still correlates with accuracy.

2. **Stress test instruction variation**: Apply instruction styles beyond the four clause types (e.g., conditional/hypothetical framing, domain-specific terminology) to evaluate whether the 7-13% CRS improvement from greedy search holds under more diverse stylistic perturbations.

3. **Validate on novel problems**: Evaluate the consistency-accuracy correlation on held-out problems not seen during any model training, ensuring the relationship reflects generalization rather than memorization of instruction-response patterns.