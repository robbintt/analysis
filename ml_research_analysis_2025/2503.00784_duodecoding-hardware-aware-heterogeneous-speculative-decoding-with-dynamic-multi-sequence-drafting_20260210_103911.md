---
ver: rpa2
title: 'DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic
  Multi-Sequence Drafting'
arxiv_id: '2503.00784'
source_url: https://arxiv.org/abs/2503.00784
tags:
- draft
- decoding
- speculative
- generation
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DuoDecoding is a hardware-aware speculative decoding method that
  deploys draft and target models on CPU and GPU respectively, enabling parallel execution
  to reduce latency. The method uses a hardware-aware optimal draft budget to minimize
  idle times and dynamic multi-sequence drafting based on draft uncertainty to improve
  draft quality.
---

# DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting

## Quick Facts
- **arXiv ID:** 2503.00784
- **Source URL:** https://arxiv.org/abs/2503.00784
- **Authors:** Kai Lv; Honglin Guo; Qipeng Guo; Xipeng Qiu
- **Reference count:** 12
- **Primary result:** Up to 2.61x speedup in generation latency with TTFT reduced to 83% of conventional speculative decoding

## Executive Summary
DuoDecoding is a hardware-aware speculative decoding method that deploys draft and target models on CPU and GPU respectively, enabling parallel execution to reduce latency. The method uses a hardware-aware optimal draft budget to minimize idle times and dynamic multi-sequence drafting based on draft uncertainty to improve draft quality. Extensive experiments on seven tasks show DuoDecoding achieves up to 2.61x speedup in generation latency and reduces time-to-first-token (TTFT) to 83% of conventional speculative decoding.

## Method Summary
DuoDecoding addresses the bottleneck in speculative decoding by offloading the draft model to CPU and the target model to GPU, enabling parallel execution. The method calculates a hardware-aware cost coefficient (c) as the ratio of target time to draft time, then sets the draft budget (γ) equal to c to minimize device idle time. For draft quality improvement, it employs dynamic multi-sequence drafting that branches into multiple sequences when draft uncertainty is high, determined by a threshold based on the product of top probability tokens. The parallel decoding loop synchronizes probability tensors between CPU and GPU processes, applying speculative sampling for verification.

## Key Results
- Achieves up to 2.61x speedup in generation latency compared to vanilla autoregressive decoding
- Reduces Time-to-First-Token (TTFT) to 83% of conventional speculative decoding
- Consistently outperforms other approaches across seven evaluation tasks from SpecBench

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Offloading the draft model to CPU allows for parallel execution with the target model on GPU, eliminating the sequential bottleneck of standard speculative decoding.
- **Mechanism:** The system runs the autoregressive draft process on the CPU while the GPU simultaneously verifies the previous batch of tokens and performs the target forward pass. This "hides" the draft latency behind the target verification time.
- **Core assumption:** The CPU is sufficiently fast to generate drafts at a rate comparable to the GPU's verification time; otherwise, the CPU becomes the new bottleneck.
- **Evidence anchors:**
  - [Page 2] "We propose DuoDecoding... strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding..."
  - [Page 3] "This heterogeneous deployment strategy... enables concurrent execution of both models."
  - [corpus] Corpus papers (e.g., KNN-SSD) discuss dynamic speculation but do not validate this specific heterogeneous hardware split.
- **Break condition:** If the draft model (CPU) takes longer than $T_{verify}$ (GPU) plus communication overhead, the GPU will idle, negating speedup benefits.

### Mechanism 2
- **Claim:** A hardware-aware draft budget ($\gamma$) minimizes device idle time by aligning the draft generation length with the relative speed of the CPU and GPU.
- **Mechanism:** The system calculates a cost coefficient $c$ (ratio of Target time / Draft time). The draft budget $\gamma$ is set equal to $c$. If the GPU is fast (low $c$), fewer tokens are drafted; if the GPU is slow (high $c$), more tokens are drafted to keep it busy.
- **Core assumption:** The relative compute speed between CPU and GPU remains stable during the generation session.
- **Evidence anchors:**
  - [Page 3] "We propose to measure the cost coefficient c... By setting the drafting budget $\gamma$ equal to c, we achieve approximate temporal alignment..."
  - [Page 7, Table 3] Shows that deviating from the optimal $\gamma$ reduces TPS (Tokens Per Second).
- **Break condition:** If system load fluctuates (e.g., background processes on CPU), a static $\gamma$ calculated at initialization may cause drift, leading to sub-optimal overlap.

### Mechanism 3
- **Claim:** Generating multiple short draft sequences dynamically based on draft uncertainty improves acceptance rates compared to a single long sequence.
- **Mechanism:** The draft model assesses the probability of the top token ($p_{1,1}$). If confidence is low (uncertainty is high), it branches into multiple sequence candidates (Multi-Sequence Drafting) at the first token position, increasing the chance that at least one path matches the target model.
- **Core assumption:** The draft model's output probability is a reliable proxy for acceptance likelihood by the target model.
- **Evidence anchors:**
  - [Page 3] "We employ dynamic multi-sequence drafting to enhance draft quality... based on the uncertainty of draft outputs."
  - [Page 7, Table 2] "Dynamic sequence drafting method consistently outperforms other approaches..."
- **Break condition:** If the draft model is "confidently wrong" (high probability but low alignment with target), this mechanism may generate redundant incorrect sequences, wasting compute.

## Foundational Learning

- **Concept:** Speculative Decoding (Draft-then-Verify)
  - **Why needed here:** DuoDecoding modifies the standard speculative loop. You must understand that normally, drafting and verification happen sequentially on the same device, causing overhead that DuoDecoding aims to parallelize.
  - **Quick check question:** Why does standard speculative decoding increase Time-To-First-Token (TTFT)?

- **Concept:** Heterogeneous Computing (CPU/GPU Offloading)
  - **Why needed here:** The core innovation relies on utilizing the CPU for useful work rather than just control. Understanding data transfer overhead (inter-process communication) is vital to realizing why this is difficult.
  - **Quick check question:** What is the primary risk when splitting a tightly coupled loop (like token generation) across two devices with different memory spaces?

- **Concept:** Speculative Sampling (Verification)
  - **Why needed here:** To maintain output fidelity, the verification step (Algorithm 2) must statistically reject or accept tokens. Understanding the "rejection sampling" logic is required to see why multiple draft sequences can be verified efficiently.
  - **Quick check question:** Does speculative decoding change the output distribution of the target model, and if not, why?

## Architecture Onboarding

- **Component map:**
  - Draft Process (CPU) -> Target Process (GPU) -> Inter-process Comm -> Controller

- **Critical path:**
  1. **Init:** Measure $T_{cpu}$ and $T_{gpu}$ to set budget $\gamma$.
  2. **Parallel Step:** CPU generates draft sequence(s) while GPU verifies the last batch.
  3. **Sync:** Transfer draft probabilities/logits to GPU.
  4. **Verify:** Apply Algorithm 2 to accept/reject tokens.
  5. **Update:** Roll back to last accepted token.

- **Design tradeoffs:**
  - **Latency vs. Throughput:** Increasing $\gamma$ (draft length) maximizes potential speedup (throughput) but risks higher latency if the draft is rejected.
  - **Memory vs. Speed:** Multi-sequence drafting improves acceptance but requires storing multiple KV-cache states or probability trees, increasing memory overhead.
  - **Quantization:** The paper uses Q5_K_M on CPU. Lower precision speeds up CPU drafting but may lower draft quality (acceptance rate).

- **Failure signatures:**
  - **Low Acceptance Rate:** Draft model distribution diverges significantly from Target. *Symptom:* TPS drops, potentially slower than vanilla autoregression due to overhead.
  - **High TTFT:** Initialization or first-pass communication overhead dominates. *Symptom:* First token takes significantly longer than standard inference.
  - **Load Imbalance:** GPU utilization < 100% while CPU is at 100% (or vice versa). *Symptom:* The budget $\gamma$ is incorrectly calibrated for the current hardware.

- **First 3 experiments:**
  1. **Budget Calibration:** Run micro-benchmarks on target hardware to find the cost coefficient $c$. Verify if setting $\gamma = c$ actually balances CPU/GPU time in practice (profiling).
  2. **Ablation on Sequence Count:** Compare Static Sequence (1, 2, 3) vs. Dynamic strategy on a high-uncertainty task (e.g., Translation vs. Math) to validate the adaptive mechanism.
  3. **TTFT Analysis:** Measure Time-To-First-Token. Ensure the parallel startup overhead does not outweigh the generation speedup for short sequences.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does DuoDecoding perform when deployed with significantly larger target models (e.g., 70B parameters) or in batched inference scenarios?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section: "our experiments were limited to target models with 7B parameters, and the effectiveness of our approach on larger models remains unexplored." They further note, "we did not explore the performance of different methods under large batch sizes."
- **Why unresolved:** The current evaluation is restricted to 7B models and single-stream generation. It is unclear if the CPU-based draft model can maintain the necessary throughput to keep pace with larger, slower target models or if memory constraints limit batch processing efficiency.
- **What evidence would resolve it:** Empirical results detailing generation latency and memory usage when applying DuoDecoding to models like Llama-2-70B and benchmarks analyzing throughput under varying batch sizes (e.g., batch size > 1).

### Open Question 2
- **Question:** To what extent does the hardware-aware optimal draft budget ($\gamma$) generalize across different computing platforms and CPU architectures?
- **Basis in paper:** [explicit] The paper identifies in the Limitations section that "evaluations were conducted on a single hardware configuration, and the performance characteristics on different computing platforms remain to be investigated."
- **Why unresolved:** The method relies on a cost coefficient ($c$) to balance CPU and GPU execution times. This balance is sensitive to specific hardware specs; performance may degrade sub-linearly on systems with weaker CPUs or different interconnect bandwidths (e.g., consumer desktops vs. servers).
- **What evidence would resolve it:** A comprehensive benchmark of DuoDecoding across diverse hardware setups (e.g., different CPU generations, GPU tiers, and edge devices) to validate the robustness of the dynamic budgeting mechanism.

### Open Question 3
- **Question:** Can DuoDecoding be effectively integrated with orthogonal draft enhancement techniques, such as tree-based speculation (SpecInfer) or retrieval-based drafting (REST), without negating latency gains?
- **Basis in paper:** [explicit] The authors state in the Related Work section that approaches like SpecInfer and REST "could potentially be integrated with our framework for complementary benefits."
- **Why unresolved:** While the authors propose CPU offloading for standard drafting, complex drafting strategies (like tree-structured speculation) require verifying larger token sets. The overhead of transferring these expanded probability distributions between CPU and GPU might saturate the communication bus, limiting scalability.
- **What evidence would resolve it:** A hybrid system implementation that combines DuoDecoding's heterogeneous execution with tree-based or retrieval-based drafting, measuring the trade-off between increased acceptance rates and communication overhead.

### Open Question 4
- **Question:** Is the threshold $\theta = p_{1,1} \times p_{2,1}$ the optimal heuristic for determining the number of dynamic multi-sequences across all task types?
- **Basis in paper:** [inferred] While the authors introduce the threshold to approximate acceptance probability, the paper notes in the Analysis section that "Different contexts and generation stages may benefit from varying sequence numbers," and the distribution of sequence numbers varies significantly between Math and Translation tasks.
- **Why unresolved:** The current heuristic uses the product of top-2 probabilities to set a static threshold for dynamic branching. However, the diversity of tasks suggests a fixed heuristic might not capture the specific uncertainty profiles of all domains (e.g., code vs. natural language).
- **What evidence would resolve it:** An ablation study comparing different uncertainty metrics or adaptive thresholding strategies against the current fixed heuristic, specifically analyzing acceptance rates across the seven evaluated tasks.

## Limitations
- Hardware generalization: Performance characteristics on different computing platforms remain unexplored beyond the tested A800 GPU and Intel Xeon CPU configuration
- Model scale: Effectiveness on larger target models beyond 7B parameters remains untested
- Batch processing: Performance under large batch sizes was not evaluated

## Confidence

**High Confidence Claims:**
- The heterogeneous CPU-GPU deployment strategy is technically feasible and can reduce the sequential bottleneck in standard speculative decoding
- The hardware-aware budget calculation methodology (measuring cost coefficient $c$ and setting $\gamma = c$) is sound in principle
- The concept of dynamic sequence drafting based on uncertainty is implementable

**Medium Confidence Claims:**
- The specific speedup metrics (2.61× maximum, 83% TTFT) are reproducible on the exact hardware configuration tested (A800 GPU, 16-core Intel Xeon)
- The dynamic multi-sequence drafting consistently improves acceptance rates across all tested tasks
- The parallel execution overhead remains negligible compared to the latency benefits

**Low Confidence Claims:**
- The approach generalizes well to other hardware platforms (e.g., consumer GPUs, different CPU architectures)
- The performance benefits scale linearly with increasing sequence length or model size
- The method maintains its advantages under variable system loads or with different quantization schemes

## Next Checks

1. **Hardware Platform Generalization**: Replicate the cost coefficient calibration and parallel execution on a different CPU-GPU combination (e.g., RTX 4090 with AMD Ryzen or Intel i9) to verify that the optimal $\gamma$ scales predictably with hardware performance ratios. Measure both absolute speedup and relative improvement over baseline speculative decoding.

2. **Dynamic Load Sensitivity**: Conduct experiments under controlled CPU/GPU load variations (using background processes) to test whether the static $\gamma$ calculated at initialization maintains performance or requires dynamic adjustment during generation. Profile device utilization to detect load imbalance.

3. **Distributional Robustness**: Test the dynamic multi-sequence drafting on intentionally out-of-distribution prompts (e.g., adversarial examples, rare languages) to evaluate whether the uncertainty-based branching mechanism fails gracefully or degrades to worse performance than static approaches when the draft model's confidence is poorly calibrated.