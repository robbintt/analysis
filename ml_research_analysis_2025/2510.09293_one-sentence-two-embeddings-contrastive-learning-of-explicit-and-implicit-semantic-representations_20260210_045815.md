---
ver: rpa2
title: 'One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit
  Semantic Representations'
arxiv_id: '2510.09293'
source_url: https://arxiv.org/abs/2510.09293
tags:
- sentence
- inli
- implicit
- explicit
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitation of sentence embedding models
  in capturing implicit semantics by proposing DualCSE, a method that generates two
  embeddings per sentence: one for explicit and one for implicit meaning. The approach
  uses contrastive learning on the INLI dataset, which contains both explicit and
  implied entailment labels.'
---

# One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations

## Quick Facts
- arXiv ID: 2510.09293
- Source URL: https://arxiv.org/abs/2510.09293
- Reference count: 16
- DualCSE achieves 80.18% accuracy on explicit entailment and 73.40% on implied entailment for RTE

## Executive Summary
This paper addresses the limitation of single-vector sentence embeddings in capturing both explicit and implicit semantics. The authors propose DualCSE, a contrastive learning method that generates two embeddings per sentence: one for explicit meaning and one for implicit meaning. Trained on the INLI dataset containing both explicit and implied entailment labels, DualCSE uses either cross-encoder or bi-encoder architectures with a novel five-part contrastive loss function. The approach shows strong performance on Recognizing Textual Entailment (RTE) and Estimating Implicitness Score (EIS) tasks, demonstrating the value of separating semantic representations.

## Method Summary
DualCSE generates dual embeddings (explicit and implicit) for each sentence using contrastive learning on the INLI dataset. The method employs either a cross-encoder architecture that processes formatted input strings or a bi-encoder architecture with separate models for each embedding type. A novel five-part contrastive loss function is used to train the model, encouraging appropriate relationships between premise and hypothesis embeddings while pushing apart contradictory pairs. The model is evaluated on RTE tasks for explicit and implied entailment recognition, as well as EIS for measuring implicitness scores.

## Key Results
- DualCSE achieves 80.18% accuracy on explicit entailment and 73.40% on implied entailment for RTE
- On EIS, DualCSE achieves near-perfect performance (99.97%) on in-domain data and competitive results (79.31-77.48%) on out-of-domain data
- The cross-encoder architecture performs comparably to the bi-encoder while being more parameter-efficient

## Why This Works (Mechanism)
The core insight is that a single embedding vector conflates explicit and implicit semantics, making it difficult to distinguish literal meaning from pragmatic or non-literal interpretations. By training separate embeddings using contrastive learning with carefully designed loss functions, DualCSE learns to encode these different aspects of meaning in distinct vector spaces. The five-part loss function explicitly pulls together entailment pairs while pushing apart contradictions and separating a sentence's own explicit and implicit representations. This separation allows the model to better capture both surface-level meaning and underlying implications, which is particularly valuable for tasks involving pragmatic inference or implicitness detection.

## Foundational Learning

- Concept: **Contrastive Learning**
  - Why needed here: DualCSE is fundamentally a contrastive learning method. Understanding how positive and negative pairs are used to shape an embedding space is essential for interpreting the loss function and how the model separates and aligns semantic representations.
  - Quick check question: Given two sentence pairs (premise, entailment) and (premise, contradiction), how does a standard contrastive loss function influence the similarity scores between them?

- Concept: **Explicit vs. Implicit Semantics**
  - Why needed here: The core problem DualCSE addresses is the inability of single-vector embeddings to capture both literal (explicit) and non-literal or pragmatic (implicit) meanings. Grasping this distinction is key to understanding why two embeddings are proposed.
  - Quick check question: For the sentence "This room is an oven," what is the explicit semantic meaning versus the implicit semantic meaning?

- Concept: **Natural Language Inference (NLI) & The INLI Dataset**
  - Why needed here: DualCSE is trained on the Implied NLI (INLI) dataset, which extends standard NLI by providing hypotheses for both explicit and implied entailment. Familiarity with the NLI task and the specific structure of INLI is crucial for understanding the model's training data and loss function construction.
  - Quick check question: How does a sample in the INLI dataset differ from a sample in a standard NLI dataset like SNLI?

## Architecture Onboarding

- Component map:
  - Input sentence `s` -> Cross-encoder or Bi-encoder -> Explicit embedding `r` and Implicit embedding `u`
  - Premise `si` and hypotheses (explicit `s+i1`, implied `s+i2`, contradiction `s-i`) -> Five-part contrastive loss computation -> Model parameter updates

- Critical path:
  1. Acquire and preprocess the INLI dataset, creating batches of samples where each sample has a premise and four associated hypotheses.
  2. For each sentence, generate two embeddings `r` and `u` using either the prompted cross-encoder or the dual bi-encoder architecture.
  3. Compute the five terms of the contrastive loss `li` for the batch. This involves calculating cosine similarity between all relevant embedding pairs in the batch.
  4. Backpropagate the aggregated loss to update the encoder model(s)' weights.
  5. For inference on a downstream task like RTE, use the max of cosine similarities involving both embeddings to predict entailment. For EIS, compute the implicitness score `imp(s) = 1 - cos(r, u)`.

- Design tradeoffs:
  - **Cross-encoder vs. Bi-encoder**: The cross-encoder is more parameter-efficient (single model) and may better model the relationship between explicit and implicit meanings via shared parameters, but requires a forward pass for each embedding type. The bi-encoder allows for independent and potentially more specialized representations but doubles the parameter count. Experimental results show comparable performance, suggesting a task-specific or resource-constrained choice.
  - **INLI-only training**: The authors acknowledge a limitation that training only on INLI may limit generalization. This is a tradeoff between training data specificity (high-quality explicit/implicit labels) and diversity.

- Failure signatures:
  - **Conflation of Semantics**: If the loss terms for separating the premise's explicit and implicit embeddings (`v(ri, uj)`) are not effective, the two embeddings for a sentence may become too similar, failing to capture distinct meanings. The ablation study shows a drop in EIS performance when this intra-sentence loss is removed.
  - **Poor Out-of-Domain Generalization**: A significant accuracy drop on the out-of-domain EIS task suggests the model may overfit to the type of implicitness present in the INLI dataset.
  - **Failure to Distinguish Entailment Types**: A drop in performance on the RTE task upon removing contradiction-based loss terms indicates the model struggles to distinguish valid inferences from contradictions without explicit negative examples.

- First 3 experiments:
  1. **Architectural Baseline**: Implement and train both the cross-encoder and bi-encoder versions of DualCSE on the INLI dataset. Evaluate on the provided test sets for the RTE and EIS tasks to verify reproduction of the reported results (RTE accuracy ~80%, EIS accuracy ~99% on INLI test). This validates the core implementation.
  2. **Component Ablation**: Systematically remove components of the contrastive loss function (specifically, the terms involving contradictions and the intra-sentence alignment terms) as described in the paper's ablation study. Measure the impact on both RTE and EIS performance to understand the contribution of each mechanism.
  3. **Out-of-Domain Robustness Test**: Evaluate the trained DualCSE models on a different sentiment analysis or hate speech detection dataset converted to a similar format (as suggested in the Limitations). Compare its performance against a baseline single-embedding model like SimCSE to test the generalization of the learned implicit semantic representation to new domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DualCSE framework be effectively adapted for Large Language Model (LLM)-based embedding methods?
- Basis in paper: The authors explicitly state in the Limitations section that "Extending our method to LLMs is another future direction," noting the recent activity in this sub-field.
- Why unresolved: The current study relies exclusively on BERT and RoBERTa encoders; it is unknown if the dual-embedding contrastive loss scales effectively to decoder-only or larger LLM architectures.
- What evidence would resolve it: Successful implementation of DualCSE using LLM backbones (e.g., Llama or Mistral) with performance benchmarks on standard semantic textual similarity (STS) or retrieval tasks.

### Open Question 2
- Question: Does training DualCSE on datasets from diverse domains (e.g., hate speech, sentiment analysis) improve the model's ability to capture implicit semantics?
- Basis in paper: The authors acknowledge using only the INLI dataset and note that "variation of the sentences in the INLI is rather limited," suggesting the conversion of datasets like ToxiGen or SemEval to the INLI format.
- Why unresolved: The model's current understanding of "implicit semantics" is bound by the specific linguistic patterns found in the INLI dataset, potentially limiting its robustness.
- What evidence would resolve it: Experiments comparing models trained on standard INLI versus those augmented with converted domain-specific datasets, evaluated on out-of-domain implicit semantic detection.

### Open Question 3
- Question: How effective are the explicit and implicit embeddings in complex, real-world information retrieval scenarios?
- Basis in paper: The authors mention that while a simple retrieval experiment was conducted, it is necessary to "apply our method to more practical settings, such as analyzing customer reviews and implementing search engines."
- Why unresolved: The paper validates the method on RTE and EIS tasks and a qualitative retrieval example, but does not quantify performance in a large-scale, noisy retrieval environment.
- What evidence would resolve it: Evaluation on established information retrieval benchmarks (e.g., BEIR) or domain-specific retrieval tasks where user intent may differ from literal query text.

## Limitations
- The model is trained exclusively on the INLI dataset, potentially limiting its ability to generalize to diverse implicit semantic phenomena
- The five-part contrastive loss function is complex and its relative component contributions are not fully explored
- The bi-encoder architecture doubles parameter count compared to single-embedding models, with no detailed efficiency comparison provided

## Confidence

- **High Confidence**: The core methodology of using dual embeddings with contrastive learning is sound and the experimental results on the proposed tasks (RTE and EIS) are reproducible. The ablation study provides evidence that key components contribute to performance.

- **Medium Confidence**: The claim that DualCSE outperforms baselines is supported by experimental results, but the comparison set is limited. The near-perfect performance on in-domain EIS tasks may indicate overfitting rather than true generalization.

- **Low Confidence**: The assertion that DualCSE significantly improves understanding of implicit semantics beyond what INLI can measure. The paper does not demonstrate clear advantages over single-embedding models on diverse, real-world tasks requiring implicit semantic understanding.

## Next Checks

1. **Generalization to Diverse Implicit Semantics**: Test DualCSE on multiple datasets representing different types of implicit semantics (sarcasm detection, metaphor understanding, pragmatic inference) to evaluate whether the learned representations generalize beyond the specific types present in INLI.

2. **Ablation of Loss Components**: Conduct a more systematic ablation study of the five-part loss function, varying the weights of each component and measuring their individual contributions to both RTE and EIS performance. This would provide clearer insight into which mechanisms are most critical.

3. **Comparison with Larger Language Models**: Evaluate DualCSE against embedding models derived from larger language models (e.g., sentence-BERT, GTR) on both standard NLI benchmarks and the proposed RTE/EIS tasks to determine whether the dual-embedding approach provides advantages over more general-purpose models.