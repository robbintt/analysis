---
ver: rpa2
title: 'HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to
  Extract Structured Data'
arxiv_id: '2512.19864'
source_url: https://arxiv.org/abs/2512.19864
tags:
- clinical
- data
- entity
- date
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HARMON-E introduces a hierarchical, agentic framework for extracting
  structured oncology data from unstructured clinical notes. The system uses large
  language models as reasoning agents that interleave retrieval and synthesis to resolve
  contradictions across heterogeneous documents.
---

# HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data

## Quick Facts
- **arXiv ID:** 2512.19864
- **Source URL:** https://arxiv.org/abs/2512.19864
- **Reference count:** 40
- **Primary result:** Hierarchical agentic framework achieves 0.93 average F1 for structured oncology data extraction from 400K+ clinical notes.

## Executive Summary
HARMON-E is a hierarchical agentic framework that uses large language models as reasoning agents to extract structured oncology data from unstructured clinical notes. The system systematically decomposes complex extraction tasks into modular pipelines that interleave retrieval and synthesis to resolve contradictions across heterogeneous documents. Evaluated on over 400,000 notes from 2,250 cancer patients, HARMON-E achieves an average F1-score of 0.93, with 100 of 103 oncology-specific variables exceeding 0.85 and critical fields like biomarkers and medications surpassing 0.95. When integrated into a curation workflow, 94% of extracted data points receive direct manual approval, significantly reducing annotation costs.

## Method Summary
The system implements a hierarchical pipeline that decomposes 103 oncology variables across 16 entity types into four specialized sub-pipelines (Single-Step, Multi-Step, Topical, Sequential). Each uses a distinct Retrieval-Synthesis-Collation flow: Retriever modules (Vector, Regex) fetch relevant text chunks, LLM Synthesizers extract structured attribute-value pairs, and Collators merge, validate, and resolve conflicts across partial extractions. The framework employs strongly-typed schemas and deterministic collation rules to ensure data integrity while handling heterogeneous documents with contradictory information.

## Key Results
- **Average F1-score of 0.93** across 103 oncology-specific variables
- **100/103 variables exceed 0.85 F1**, with biomarkers and medications surpassing 0.95
- **94% direct manual approval rate** when integrated into curation workflow

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition
The system breaks complex extraction for 103 variables into four specialized sub-pipelines, each using a tailored Retrieval-Synthesis-Collation flow. This modular design mirrors human abstraction workflows and reduces LLM confusion on complex inputs. Performance may degrade if critical entity dependencies cannot be resolved by the pipeline's dependency management order.

### Mechanism 2: Iterative Retrieval-Synthesis with Context
Targeted retrieval modules fetch relevant text chunks, LLM synthesizers extract structured pairs, and collators merge and resolve conflicts. This iterative process handles heterogeneous documents with contradictory information. Performance degrades on attributes requiring inference across highly dispersed or implicitly related text snippets not captured by retrievers.

### Mechanism 3: Strong Typing and Rule-Based Collation
The system enforces a strongly-typed schema and applies deterministic collation rules for deduplication, validation, and conflict resolution. This ensures data integrity and reduces noise in extracted data. Performance may suffer on valid but rare clinical nuances not captured in the static schema or collation rules.

## Foundational Learning

**Concept: Named Entity Recognition (NER) vs. Relation Extraction.**
- *Why needed here:* This paper goes beyond traditional NER to relation extraction and synthesis, linking entities to values, dates, and patient diagnoses.
- *Quick check question:* Does the system simply find "BRAF" in the text, or does it also extract its result, test date, and associate it with the patient's primary diagnosis?

**Concept: Vector Space Models / Embeddings.**
- *Why needed here:* The Vector Retriever uses embeddings to find semantically similar text chunks, performing context-sensitive retrieval beyond keyword matching.
- *Quick check question:* How does the system find a relevant text passage about a patient's "disease progression" even if those exact words are not used in the retrieval query?

**Concept: Agentic AI.**
- *Why needed here:* The paper frames its solution as an "agentic framework" where LLMs make autonomous decisions about tool usage and problem-solving approaches.
- *Quick check question:* In what way does the HARMON-E system act as an "agent" rather than just a static model being prompted once?

## Architecture Onboarding

**Component map:** Ingestion & Normalization → Retriever (Vector/Regex) → LLM Synthesizer → Collator → Structured Patient Record

**Critical path:** Patient Documents → Retriever (fetches chunks) → LLM Synthesizer (extracts JSON) → Collator (merges & validates) → Structured Patient Record. Overall performance hinges on quality of retrieval and design of collation rules.

**Design tradeoffs:**
- Agentic Complexity vs. Latency: Multi-step pipelines are more accurate but have 11.2x higher latency and cost
- Retrieval Granularity (k): Higher 'k' increases recall but also noise and processing cost
- Deterministic vs. LLM-based Collation: Deterministic collators ensure control and consistency but sacrifice flexibility

**Failure signatures:**
- Low Recall on Rare Entities: May occur if 'k' parameter is too low or queries are not well-defined
- Date Confusion: Extracted dates may be inaccurate due to conflicting documentation
- Pipeline Errors on Unseen Formats: System may fail on note formats or terminologies not in development set

**First 3 experiments:**
1. **Pipeline Ablation:** Run GPT-4o Single-Step baseline against HARMON-E (full) on held-out patients, measuring F1-score and latency difference.
2. **Retrieval Tuning:** For 'Biomarker' entity, vary Vector Retriever 'k' parameter (4, 8, 16) to find optimal recall-noise balance.
3. **Schema Violation Test:** Pass malformed data to LLM Synthesizer and verify validation layer correctly rejects or sanitizes output.

## Open Questions the Paper Calls Out

**Open Question 1:** To what extent does HARMON-E generalize to cancer types with different documentation styles or complex hematologic malignancies compared to the melanoma cohort tested? The evaluation was restricted to a retrospective melanoma cohort (n=2,250), and while the system can theoretically be applied to other cancers, it has not been validated on malignancies with vastly different staging systems or documentation patterns.

**Open Question 2:** Can specialized temporal reasoning modules or advanced date-resolution prompts significantly improve performance of "fuzzy" date attributes which currently underperform categorical ones? The current pipeline struggles with date ambiguity, resulting in lower scores for fields like `endDate` (85.47% F1) compared to categorical fields (>95% F1).

**Open Question 3:** How does integration of active learning mechanisms impact long-term maintenance and accuracy of the agentic pipeline in live clinical settings? The study utilizes a static dataset for validation and does not evaluate how the system adapts to evolving medical terminology or shifting documentation practices without manual prompt engineering.

## Limitations

- **Dataset and Prompt Transparency:** Proprietary 2,250-patient dataset and abbreviated prompt templates create reproducibility gaps; system's performance on other cancer types remains untested.
- **Real-World Deployment Risks:** 6% of data points still require human curation; paper doesn't quantify downstream impact of errors on clinical decision-making; 11.2x baseline latency poses scalability challenges.
- **Generalization to Novel Clinical Scenarios:** Strong typing and rule-based collation may struggle with rare clinical nuances or off-label drug uses not captured in static schema.

## Confidence

- **High Confidence:** Hierarchical decomposition approach and modular pipeline design are well-supported by experimental results and align with established principles in complex task management.
- **Medium Confidence:** Iterative retrieval-synthesis mechanism is effective for tested oncology domain but robustness to diverse clinical narratives requires further validation.
- **Medium Confidence:** Strongly-typed schema and deterministic collation rules ensure data quality for known entity types but their rigidity may limit adaptability to evolving clinical practices.

## Next Checks

1. **Dataset Generalization Test:** Evaluate HARMON-E on held-out dataset of patients with different cancer types (e.g., lung, breast) or from different healthcare systems to assess cross-domain performance.

2. **Latency and Cost Analysis:** Conduct detailed analysis of system's end-to-end latency and operational costs at scale, comparing against traditional manual curation workflows to determine practical feasibility.

3. **Error Impact Assessment:** Perform qualitative analysis of 6% of data points requiring human intervention, categorizing types of errors and their potential impact on downstream clinical or research applications.