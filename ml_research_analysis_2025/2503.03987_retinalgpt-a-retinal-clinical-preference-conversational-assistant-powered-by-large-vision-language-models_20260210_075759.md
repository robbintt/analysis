---
ver: rpa2
title: 'RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered
  by Large Vision-Language Models'
arxiv_id: '2503.03987'
source_url: https://arxiv.org/abs/2503.03987
tags:
- retinal
- data
- medical
- image
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RetinalGPT addresses the challenge of applying general-domain multimodal
  large language models (MLLMs) to specialized retinal image analysis tasks. The core
  method introduces a two-stage instruction-tuning strategy that combines clinical
  preference data with generic medical knowledge through a mixup approach.
---

# RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered by Large Vision-Language Models

## Quick Facts
- arXiv ID: 2503.03987
- Source URL: https://arxiv.org/abs/2503.03987
- Reference count: 28
- Key outcome: RetinalGPT achieves 95.10% accuracy on APTOS, 73.70% on EyeP, 84.90% on ACS, 80.83% on IDRiD, 87.14% on Messidor, 66.13% on MICCAI, 88.27% on OIA-ODIR, and 99.57% on RFMiD datasets

## Executive Summary
RetinalGPT addresses the challenge of applying general-domain multimodal large language models (MLLMs) to specialized retinal image analysis tasks. The core method introduces a two-stage instruction-tuning strategy that combines clinical preference data with generic medical knowledge through a mixup approach. This preserves broader medical understanding while enhancing domain-specific capabilities. The model is trained on a large dataset of 38K retinal images with structured clinical feature descriptions and 60K generic medical QA pairs.

Experimental results show RetinalGPT achieves state-of-the-art performance across eight retinal disease classification benchmarks, demonstrating strong capabilities in disease classification, lesion localization, and vascular structure analysis. The model outperforms all baseline models and successfully extracts quantitative vascular features including fractal dimension, vessel density, branch angle, and tortuosity, with predicted values closely matching ground truth.

## Method Summary
RetinalGPT employs a two-stage training strategy: Stage 1 performs feature alignment by mixing 38K retinal alignment data with 600K PMC biomedical data, updating only the projection layer while keeping CLIP and LLaMA frozen. Stage 2 executes mixup instruction-tuning on 38K retinal clinical data combined with 60K generic medical QA, training both projector and LLM. Structured clinical features including disease labels, lesion bounding boxes, and fractal vascular features are extracted using AutoMorph and RBAD pipelines, then used to generate GPT-4-based instruction data. The model achieves superior performance on multiple retinal disease classification benchmarks while preserving generic medical knowledge.

## Key Results
- Classification accuracy: 95.10% on APTOS, 73.70% on EyeP, 84.90% on ACS, 80.83% on IDRiD, 87.14% on Messidor, 66.13% on MICCAI, 88.27% on OIA-ODIR, and 99.57% on RFMiD datasets
- Strong lesion localization capabilities with predicted bounding boxes closely matching ground truth masks
- Accurate quantitative vascular feature estimation (fractal dimension, vessel density, branch angle, tortuosity) closely matching AutoMorph ground truth values
- Outperforms all baseline models across all eight benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage mixup training preserves generic medical knowledge while enabling retinal specialization
- Mechanism: Stage 1 aligns retinal images with text embeddings by mixing 38K retinal alignment data with 600K PMC biomedical data, updating only the projection layer. Stage 2 performs instruction-tuning on 38K retinal clinical data mixed with 60K generic medical QA, training both projector and LLM. This prevents catastrophic forgetting by maintaining exposure to broader medical concepts during specialization
- Core assumption: The model can simultaneously retain generic medical vocabulary while learning domain-specific reasoning patterns without interference
- Evidence anchors: [abstract] "two-stage training approach combining clinical preference data with generic medical knowledge to enhance diagnostic capabilities while preserving broader medical understanding"; [section 2.2] "expands the vocabulary of aligned image-text tokens to include both retinal-specific knowledge and broader biomedical information"

### Mechanism 2
- Claim: Structured clinical feature extraction enables interpretable, quantitative outputs beyond simple classification
- Mechanism: Each image is processed through AutoMorph and RBAD pipelines to extract ~40 vascular features (fractal dimension, vessel density, branch angle, curvature tortuosity). Statistical analysis filters features correlated with disease labels; ophthalmologists validate clinical relevance. This structured representation becomes the foundation for instruction data generation
- Core assumption: Quantitative vascular biomarkers are diagnostically meaningful and can be learned as text-generatable values by the LLM
- Evidence anchors: [abstract] "structured clinical feature descriptions, including disease labels, lesion bounding boxes, and fractal vascular features"; [section 2.1] "consulted ophthalmologists to confirm the importance of each feature, keeping those that significantly contributed to disease characterization"

### Mechanism 3
- Claim: GPT-4-generated instruction data from structured descriptions creates clinically-aligned conversational behavior
- Mechanism: GPT-4 receives structured feature tuples (disease label, quality, vascular features, modality, lesion info) and generates single-turn alignment QA and multi-turn clinical preference dialogues. Task-specific prompts adapt generation to different dataset characteristics
- Core assumption: GPT-4 can synthesize clinically realistic conversations from structured feature descriptions without direct image access
- Evidence anchors: [abstract] "custom visual instruction tuning using a large retinal image dataset with structured clinical feature descriptions"; [section 2.1] "prompting it to simulate responses as if it had access to the image, even though it only utilized text information"

## Foundational Learning

- Concept: **Vision-Language Model Alignment**
  - Why needed here: RetinalGPT's Stage 1 requires understanding how projection layers map CLIP visual embeddings to LLaMA token space
  - Quick check question: Can you explain why only the projection layer is trained during alignment while vision encoder and LLM remain frozen?

- Concept: **Instruction Tuning for Multimodal Models**
  - Why needed here: The entire RetinalGPT approach depends on converting structured data into instruction-following conversations
  - Quick check question: What is the difference between alignment data (single-turn, modality/label focused) and tuning data (multi-turn, clinical reasoning focused)?

- Concept: **Catastrophic Forgetting in Domain Adaptation**
  - Why needed here: The mixup strategy directly addresses knowledge loss when specializing general models
  - Quick check question: Why does mixing 60K generic medical QA with 38K retinal QA during Stage 2 help preserve non-retinal capabilities?

## Architecture Onboarding

- Component map:
  Vision Encoder (CLIP ViT) -> Projection Layer (linear, trainable) -> Language Model (LLaMA) -> Data Pipeline (AutoMorph + RBAD -> Feature extraction -> GPT-4 instruction generation) -> Training Pipeline (Stage 1 alignment on 638K mixed data -> Stage 2 instruction tuning on 98K mixed data)

- Critical path:
  1. Feature extraction correctness (AutoMorph/RBAD output quality)
  2. Instruction data generation (GPT-4 prompt engineering)
  3. Stage 1 alignment convergence (projection layer learning)
  4. Stage 2 mixup balance (retention vs. specialization tradeoff)

- Design tradeoffs:
  - Dataset size vs. specialization: 38K retinal images is modest; adding 60K generic improves generalization but dilutes retinal signal
  - Feature complexity vs. learnability: ~40 vascular features per image; too many may overwhelm LLM's numeric reasoning
  - Two-stage vs. end-to-end: Separate alignment/tuning prevents interference but increases engineering complexity

- Failure signatures:
  - Model outputs modality-related answers regardless of question (noted in conclusion as current limitation)
  - Quantitative vascular estimates diverge significantly from ground truth (indicating feature-to-text mapping failure)
  - Generic medical QA performance drops sharply (indicating catastrophic forgetting from insufficient mixup)

- First 3 experiments:
  1. Ablation on mixup ratio: Train with varying retinal/generic data ratios to quantify forgetting-specialization tradeoff on held-out generic medical QA
  2. Feature importance analysis: Remove subsets of vascular features from instruction data and measure impact on quantitative estimation accuracy
  3. Cross-dataset generalization: Train on 7 of 8 benchmark datasets, test on held-out dataset to assess whether performance gains come from memorization vs. learned clinical reasoning

## Open Questions the Paper Calls Out
- Question: Does increasing the diversity of the retinal QA instruction tuning set effectively mitigate the model's tendency to provide modality-related answers to the first query regardless of the input?
  - Basis in paper: [explicit] The Conclusion states the assistant "tends to give modality-related answers for the first question in retinal images, no matter what is asked," and hypothesizes this is "due to a lack of diversity in retinal QA instruction tuning set"
  - Why unresolved: The authors identify the behavior and the likely cause but do not validate the solution in the current work
  - What evidence would resolve it: Ablation studies using expanded and diversified instruction datasets showing the elimination of this bias in initial conversational turns

- Question: What is the quantitative trade-off in performance on standard generic medical benchmarks when applying the proposed retinal-specialized training strategy?
  - Basis in paper: [inferred] The paper claims to preserve generic medical knowledge and provides qualitative examples, but quantitative evaluation is restricted to retinal datasets
  - Why unresolved: Without numerical benchmarking on generic medical test sets, the precise extent to which the "Mixup Instruction-Tuning" prevents catastrophic forgetting remains unclear
  - What evidence would resolve it: Reporting performance metrics on established generic medical VQA benchmarks alongside the retinal-specific results

- Question: How reliably does the text-only GPT-4 simulation capture the visual relationships between clinical features and the actual retinal images?
  - Basis in paper: [inferred] Section 2.1 describes generating instruction data by prompting GPT-4 to simulate responses "as if it had access to the image, even though it only utilized text information"
  - Why unresolved: The paper assumes the text-based simulation is sufficient for visual grounding without auditing the potential hallucinations or inconsistencies in the synthetic training data
  - What evidence would resolve it: A human evaluation of the generated instruction data to quantify the rate of discrepancies between the text descriptions and the actual visual features

## Limitations
- The approach relies heavily on the quality of automated feature extraction (AutoMorph, RBAD) and GPT-4-generated instruction data, which may introduce systematic errors
- The two-stage training assumes the 38K/60K mix ratio is optimal without systematic exploration
- The model's tendency to answer modality-related questions regardless of intent suggests incomplete task understanding that could limit clinical deployment

## Confidence
- High confidence: Classification accuracy improvements over baselines on standard benchmarks (95.10% APTOS, 87.14% Messidor)
- Medium confidence: Quantitative vascular feature estimation accuracy (due to limited ground truth availability and potential feature extraction noise)
- Low confidence: Clinical safety and real-world generalization (model tested only on curated benchmark datasets, not on diverse clinical workflows)

## Next Checks
1. **Mixup Ratio Sensitivity Analysis**: Systematically vary the retinal-to-generic data ratio during Stage 2 (e.g., 100%/0%, 75%/25%, 50%/50%) and measure both retinal performance and generic medical QA retention to quantify the specialization-forgetting tradeoff
2. **Feature Extraction Robustness**: Compare model performance when using ground truth vascular features versus AutoMorph/RBAD-extracted features to isolate whether accuracy gains come from learned reasoning or feature extraction artifacts
3. **Cross-Dataset Generalization Test**: Hold out one benchmark dataset during training, then evaluate whether the model can generalize to unseen retinal image distributions or simply memorizes training dataset characteristics