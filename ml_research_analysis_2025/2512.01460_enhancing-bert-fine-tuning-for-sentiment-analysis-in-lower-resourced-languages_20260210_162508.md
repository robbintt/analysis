---
ver: rpa2
title: Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages
arxiv_id: '2512.01460'
source_url: https://arxiv.org/abs/2512.01460
tags:
- data
- fine-tuning
- learning
- clustering
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an integrated fine-tuning pipeline that combines
  Active Learning (AL) with data clustering and dynamic scheduling strategies to improve
  BERT model performance on sentiment analysis for low-resource languages. By using
  AL acquisition functions alongside hierarchical clustering and scheduling techniques
  that reduce annotation volume over time, the method simultaneously cuts annotation
  needs by up to 30% and improves F1 scores by up to 4 points across Slovak, Maltese,
  Icelandic, and Turkish datasets.
---

# Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages

## Quick Facts
- arXiv ID: 2512.01460
- Source URL: https://arxiv.org/abs/2512.01460
- Reference count: 17
- This work introduces an integrated fine-tuning pipeline that combines Active Learning (AL) with data clustering and dynamic scheduling strategies to improve BERT model performance on sentiment analysis for low-resource languages.

## Executive Summary
This paper presents an integrated fine-tuning pipeline that systematically combines Active Learning (AL), clustering, and dynamic scheduling to enhance BERT model performance on sentiment analysis in low-resource languages. The approach reduces annotation needs by up to 30% while improving F1 scores by up to 4 points across Slovak, Maltese, Icelandic, and Turkish datasets. The method achieves increased fine-tuning stability and demonstrates potential generalization to medium-resource languages.

## Method Summary
The method integrates three key components: hierarchical clustering of BERT embeddings to structure data, active learning acquisition functions (entropy, BALD, variance) to select informative samples, and dynamic scheduling strategies that progressively reduce annotation volume. The pipeline uses agglomerative hierarchical clustering with Ward's linkage on BERT embeddings, supports both accumulating and recalculating sampling strategies, and employs linear schedulers that reduce sampling percentages across epochs. Experiments test warm-start initialization to mitigate cold-start problems and compare full fine-tuning against LoRA adapters.

## Key Results
- F1 score improvements of up to 4 points across Slovak, Maltese, Icelandic, and Turkish datasets
- Annotation reduction of up to 30% through dynamic scheduling strategies
- Increased fine-tuning stability compared to standard approaches
- Performance gains demonstrate generalization to medium-resource languages like Turkish

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating Active Learning (AL) with clustering and dynamic scheduling improves BERT fine-tuning performance and data efficiency.
- Mechanism: AL acquisition functions select the most informative samples for annotation. Clustering ensures these samples are representative of the data's semantic structure. Dynamic scheduling reduces the number of samples selected over time, minimizing the influence of harmful outliers and annotation costs.
- Core assumption: The combination of these three elements is synergistic and more effective than using any of them in isolation.
- Evidence anchors:
  - [abstract] "integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance"
  - [page 4] "All top-performing AL models incorporate both clustering and scheduling strategies."
- Break condition: The performance gains disappear if any of the three components are removed. If clustering is not used, the AL method may select unrepresentative or redundant samples. If scheduling is not used, the process may select too many samples, including noisy outliers, increasing costs without improving performance.

### Mechanism 2
- Claim: Clustering improves sample selection by providing a semantic structure to the data.
- Mechanism: Agglomerative Hierarchical Clustering on BERT embeddings selects diverse and representative samples. The "Furthest-batch" acquisition function selects samples at the edge of clusters, representing boundary cases that improve generalization.
- Core assumption: BERT embeddings capture meaningful semantic relationships relevant to sentiment analysis.
- Evidence anchors:
  - [page 3] "To support the findings of Hu et al. (2010), we apply Agglomerative Hierarchical Clustering... shown effective in capturing semantic relationships in text embeddings"
  - [page 4] "Furthest-batch seems to have similar performance as the baseline model but with much less data used"
- Break condition: The chosen clustering method fails to capture semantically meaningful groups. If clusters are arbitrary or don't align with sentiment distinctions, selecting "representative" or "boundary" samples will not be beneficial.

### Mechanism 3
- Claim: Dynamic scheduling reduces annotation costs and improves stability.
- Mechanism: Linear schedulers progressively reduce the number of samples selected for annotation in each epoch. This forces the model to learn from a more focused set of high-quality data over time, reducing the risk of overfitting to noise or outliers and preventing performance degradation.
- Core assumption: A model trained on a small, high-quality subset will benefit from being trained on an even smaller, more refined subset in later epochs.
- Evidence anchors:
  - [page 3] "Linear schedulers reduce annotation usage progressively after each epoch."
  - [page 4] "This is presented in Figure3, where for the sake of clarity we show the comparison of SlovakBERT base model and best AL model performance comparison."
- Break condition: The scheduler reduces the sample pool too aggressively, causing the model to lose access to important data. If the learning rate is not tuned appropriately for the shrinking dataset, performance may drop or training may become unstable.

## Foundational Learning

- Concept: Active Learning (AL) for Text Classification
  - Why needed here: This is the core technique the paper builds upon. Understanding that AL is about strategically selecting which data points to label is essential for grasping the contribution of the paper's "scheduling" and "clustering" enhancements.
  - Quick check question: What is the primary goal of an AL acquisition function? (Answer: To score and select unlabeled data points that, if labeled, would provide the most information to the model).

- Concept: BERT Embeddings for Sentiment Analysis
  - Why needed here: The paper uses BERT models and their embeddings as the basis for both the downstream task and the clustering mechanism. A foundational understanding that BERT produces contextualized embeddings is required.
  - Quick check question: Why are BERT embeddings used for clustering instead of simpler methods like TF-IDF? (Answer: BERT embeddings capture semantic context and meaning more effectively).

- Concept: Hierarchical Clustering
  - Why needed here: The paper uses Agglomerative Hierarchical Clustering to structure the data before applying AL strategies. Understanding that this is a bottom-up clustering method is important for following the methodology.
  - Quick check question: In the agglomerative hierarchical clustering method used in the paper, how are clusters initially formed? (Answer: Each data sample's embedding starts as its own separate cluster).

## Architecture Onboarding

- Component map: The proposed system replaces standard fine-tuning with a loop: Data -> Clustering (init/dynamic) -> AL Scheduler -> Acquisition Function -> Model (BERT/ENN) -> Fine-tuning. The key new modules are the Active Learning Scheduler and the Clustering module which feed into the data selection process.

- Critical path: The most critical component is the Active Learning Scheduler. Its logic for dynamically changing the data selection budget per epoch is the primary driver of annotation efficiency. The interaction between the scheduler and the clustering module (e.g., for dif-build schedulers) is where the core novelty lies.

- Design tradeoffs: The main tradeoff is computational cost vs. annotation efficiency. The "Recalculating" sampling and "Dynamic clustering" strategies are more computationally expensive as they re-compute selections every epoch, but they offer greater flexibility and performance. The "Accumulating" approach is simpler but may retain suboptimal or noisy samples from early epochs.

- Failure signatures: A key failure mode is the "cold start" problem, where the model's initial random predictions lead to poor AL selections. The paper explores this directly and suggests using a "warm start" (training first on a random subset) as a mitigation. Another failure is the selection of "harmful unlearnable outliers," which the scheduling mechanisms are specifically designed to avoid.

- First 3 experiments:
  1. Baseline Establishment: Fine-tune a monolingual BERT model (e.g., SlovakBERT) on the full sentiment dataset without any AL, clustering, or scheduling to establish a performance floor (baseline F1 score).
  2. Ablation Study on Core Components: Implement the pipeline and run controlled experiments to test the contribution of each new component. For example, test AL-only, AL + Clustering, and AL + Clustering + Scheduling to confirm that all three are necessary for peak performance as claimed.
  3. Scheduler & Acquisition Function Comparison: With clustering enabled, compare the performance and data usage of different schedulers (base, linear, dif-build) and acquisition functions (entropy, bald, furthest-batch). This will reveal the most effective combination for your specific language and dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do the annotation savings and performance gains from AL with clustering and scheduling transfer to other classification tasks such as Named Entity Recognition?
- Basis in paper: [explicit] The conclusion states the pipeline "can be easily extended to other classification tasks, such as Named Entity Recognition, where only slight changes to architecture and acquisition functions are needed."
- Why unresolved: All experiments were conducted only on sentiment analysis; no other tasks were tested.
- What evidence would resolve it: Experiments applying the same pipeline to NER or other sequence labeling tasks across low-resource languages, comparing against standard fine-tuning baselines.

### Open Question 2
- Question: Why does LoRA fine-tuning substantially underperform compared to full fine-tuning in low-resource settings, contrary to findings on high-resource English benchmarks?
- Basis in paper: [explicit] The paper reports LoRA "consistently underperformed full fine-tuning" with dramatic degradation on Maltese (−51% F1), noting this "contrasts with prior work reporting strong LoRA performance on large-scale English benchmarks."
- Why unresolved: The authors did not investigate the causes of this underperformance or whether alternative adapter configurations could close the gap.
- What evidence would resolve it: Systematic analysis of LoRA expressivity limitations on low-resource languages, testing alternative parameter-efficient methods, or varying LoRA rank/adaptation layers specifically for low-resource scenarios.

### Open Question 3
- Question: Do the results from downsampled medium-resource languages (Turkish, Icelandic) accurately reflect performance in genuinely low-resource environments with natural data scarcity?
- Basis in paper: [explicit] The limitations section states that controlled downsampling "may not fully capture the challenges of genuine low-resource environments."
- Why unresolved: Artificial downsampling may not reproduce the data quality issues, domain limitations, or annotation noise present in naturally occurring low-resource datasets.
- What evidence would resolve it: Experiments on naturally small datasets from truly low-resource languages without artificial downsampling, comparing against downsampled equivalents.

### Open Question 4
- Question: What is the optimal strategy for selecting acquisition functions and scheduling approaches for a given model–dataset combination without extensive experimentation?
- Basis in paper: [inferred] The paper acknowledges that "tailoring acquisition functions and scheduling strategies to individual model–dataset combinations can yield even greater gains" but provides no principled method for a priori selection.
- Why unresolved: Different configurations performed best for different languages (e.g., entropy for SlovakBERT, bald for IceBERT), suggesting language- or dataset-specific factors matter, but no predictive framework is offered.
- What evidence would resolve it: Analysis correlating dataset characteristics (size, class balance, sequence length) or language properties with optimal AL configuration, enabling prediction without exhaustive search.

## Limitations

- Computational overhead: The dynamic clustering and active learning pipelines introduce significant computational overhead compared to standard fine-tuning, requiring retraining clustering models and re-evaluating acquisition scores each epoch.
- Embedding quality dependency: The effectiveness of the approach heavily depends on the quality of initial BERT embeddings for clustering, with no ablation showing performance when using simpler embedding methods like TF-IDF.
- Insufficient detail: The paper lacks sufficient detail on critical hyperparameters such as the number of clusters used for hierarchical clustering and exact sampling granularity per epoch.

## Confidence

High confidence: The core claim that combining active learning, clustering, and scheduling improves F1 scores by up to 4 points and reduces annotation by 30% is supported by experimental results across four languages. The ablation studies showing all three components are necessary provide strong mechanistic evidence.

Medium confidence: The claim about increased fine-tuning stability is supported by Figure 3 but could benefit from more statistical analysis across multiple random seeds. The generalization to medium-resource languages like Turkish is demonstrated but with limited sample sizes.

Low confidence: The computational efficiency claims are not quantified in terms of wall-clock time or resource usage, making it difficult to assess the practical trade-offs of the approach.

## Next Checks

1. Reproduce the warm-start ablation: Run the complete pipeline with and without the warm-start initialization (first epoch on random half-dataset) to verify the paper's claim that this mitigates the cold-start problem and improves AL selection quality.

2. Test embedding sensitivity: Replace BERT embeddings with TF-IDF or average GloVe embeddings for the clustering step and measure performance degradation to validate that the semantic clustering is essential to the method's success.

3. Measure computational overhead: Instrument the pipeline to record wall-clock time and GPU memory usage for each epoch across all four languages, comparing the full AL+clustering+scheduling approach against standard fine-tuning to quantify the runtime trade-offs.