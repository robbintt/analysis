---
ver: rpa2
title: AI Coding with Few-Shot Prompting for Thematic Analysis
arxiv_id: '2504.07408'
source_url: https://arxiv.org/abs/2504.07408
tags:
- coding
- passage
- passages
- refugees
- reviewers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel AI-assisted coding methodology for
  thematic analysis using large language models (LLMs), specifically GPT-3.5-Turbo.
  The method addresses the labor-intensive nature of traditional coding by combining
  few-shot prompting with structured Socratic questioning to improve the quality of
  machine-generated codes.
---

# AI Coding with Few-Shot Prompting for Thematic Analysis

## Quick Facts
- arXiv ID: 2504.07408
- Source URL: https://arxiv.org/abs/2504.07408
- Reference count: 40
- Primary result: F1 score of 0.82 on thematic coding of news articles

## Executive Summary
This paper introduces a novel AI-assisted coding methodology for thematic analysis using large language models, specifically GPT-3.5-Turbo. The method addresses the labor-intensive nature of traditional coding by combining few-shot prompting with structured Socratic questioning to improve the quality of machine-generated codes. The approach clusters semantically similar passages and uses exemplar codes from these clusters as references during coding, enabling scalable and reproducible thematic analysis.

The methodology was evaluated on a corpus of 2,530 Malaysian news articles about refugees, with human reviewers assessing the AI-generated codes. The final version achieved an F1 score of 0.82 and a negative predictive value of 0.97, indicating high accuracy in identifying irrelevant material. Inter-rater reliability also improved substantially between iterations, demonstrating the method's effectiveness in producing consistent and interpretable codes. This work provides a replicable framework for integrating AI into qualitative research while maintaining the interpretive richness of human analysis.

## Method Summary
The methodology employs a multi-stage pipeline for automated thematic coding of text passages. First, passages are embedded using OpenAI's Ada model, reduced to 6 dimensions via UMAP, and clustered into 400 groups using k-means. From each cluster, 4 passages are sampled for a Socratic prompting pipeline consisting of 5 sequential questions: checking for disclaimers/captions, assessing relevance to refugees and Malaysia, determining overall relevance, generating a preliminary code, and reassessing with article summaries. Cluster-level code summaries are generated from the 4 codes per cluster. Finally, the full corpus is coded using few-shot prompting with the cluster summary as reference, producing structured JSON outputs with Theme, Whose Attitude, Target, and Valence fields.

## Key Results
- Achieved F1 score of 0.82 and negative predictive value of 0.97 on thematic coding task
- Inter-rater reliability improved substantially between iterations, reaching good agreement
- Successfully identified and filtered irrelevant material including captions, subscription text, and op-ed intros
- Demonstrated scalability to 2,530 articles while maintaining interpretive richness

## Why This Works (Mechanism)
The methodology leverages clustering to group semantically similar passages, ensuring that few-shot examples are contextually relevant. The Socratic questioning pipeline systematically filters irrelevant content before coding, addressing "summary bleeding" where codes reflect article context rather than the specific passage. By using cluster-level summaries as reference codes, the approach maintains consistency across semantically related passages while preserving the nuance of individual texts.

## Foundational Learning
- **Thematic analysis fundamentals**: Understanding how codes are assigned to text passages based on underlying themes and attitudes; needed to evaluate if AI codes align with human interpretation
- **Few-shot prompting principles**: Recognizing how providing exemplars influences model output; quick check: test model with and without exemplars to observe performance difference
- **UMAP dimensionality reduction**: Knowing how UMAP preserves local structure in high-dimensional embeddings; quick check: visualize clusters before and after reduction to verify semantic grouping
- **Socratic questioning in AI**: Understanding how sequential prompts can refine outputs and filter irrelevant content; quick check: run a single-pass prompt versus the 5-step pipeline on the same passage
- **Inter-rater reliability metrics**: Familiarity with Kendall's Tau-b and Cohen's Kappa for measuring agreement; quick check: calculate reliability scores on a small hand-coded sample
- **Precision-recall tradeoff**: Recognizing that high recall (0.97) with moderate precision (0.72) indicates good sensitivity but some false positives; quick check: examine false positive cases to identify systematic errors

## Architecture Onboarding

### Component Map
Text Corpus → Passage Splitting → Embedding (Ada) → UMAP Reduction → k-means Clustering → Cluster Sampling → Socratic Pipeline → Cluster Summaries → Few-shot Final Coding

### Critical Path
Passage Embedding → Clustering → Socratic Pipeline → Final Coding

### Design Tradeoffs
- Used GPT-3.5-Turbo instead of GPT-4 to manage costs (3-4x cheaper), requiring complex pipeline versus single prompt
- Chose 400 clusters to balance granularity and manageability of exemplar sets
- Implemented 5-step Socratic pipeline to filter irrelevant content, adding latency but improving accuracy
- Used passage-level summaries rather than full articles to minimize "summary bleeding"

### Failure Signatures
- Summary bleeding: Codes reflect article context not present in passage
- Irrelevant passages miscoded: Captions, subscription text, op-ed intros generating codes
- Pronoun ambiguity: GPT misinterpreting who holds the attitude being coded
- Cluster incoherence: Passages within clusters lacking semantic similarity

### 3 First Experiments
1. Run Socratic pipeline on 10 randomly selected passages and compare outputs to hand-coded references
2. Generate cluster summaries for 5 clusters and verify semantic coherence by examining constituent passages
3. Apply final few-shot prompt to 50 passages using their cluster summary and evaluate against ground truth

## Open Questions the Paper Calls Out
- **Human vs AI exemplars**: The authors note that practitioners "may also consider hand coding a subset of the corpus to use for few-shot prompting" rather than relying on the AI-generated Socratic codes, but the performance difference remains untested
- **Non-news corpora**: The methodology's effectiveness on interview transcripts or social media data is unknown, as linguistic structures and "summary bleeding" issues may manifest differently
- **Single-prompt efficiency**: As model capabilities and costs evolve, it's unclear whether the complex Socratic pipeline provides better value than a single-prompt approach using a more advanced model like GPT-4

## Limitations
- The complex multi-step pipeline requires significant engineering overhead and processing time
- Performance on non-news corpora (interviews, social media) remains untested
- Reliance on GPT-3.5-Turbo due to cost constraints may limit performance compared to newer models

## Confidence

| Claim | Confidence |
|-------|------------|
| F1 score of 0.82 is reproducible | High |
| Socratic pipeline improves over single-pass approach | High |
| Method scales to large corpora (2,530+ articles) | High |
| Clustering with 400 groups is optimal | Medium |
| Results generalize beyond Malaysian news | Low |

## Next Checks
1. Reproduce the clustering step on a subset of 100 passages and visually inspect cluster coherence
2. Run the Socratic pipeline on 20 passages and compare outputs to hand-coded references for accuracy
3. Apply the final few-shot prompt to 50 passages using their cluster summary and calculate F1 score against ground truth