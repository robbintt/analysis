---
ver: rpa2
title: 'Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating
  LLM-Generated 3-ply Case-Based Legal Arguments'
arxiv_id: '2506.00694'
source_url: https://arxiv.org/abs/2506.00694
tags:
- factors
- case
- argument
- test
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an automated pipeline to evaluate LLM-generated
  3-ply case-based legal arguments, focusing on hallucination, factor utilization,
  and instruction-following for abstention. The method uses an external LLM to extract
  factors from generated arguments and compares them against ground-truth factors
  in input case triples.
---

# Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments

## Quick Facts
- arXiv ID: 2506.00694
- Source URL: https://arxiv.org/abs/2506.00694
- Authors: Li Zhang; Morgan Gray; Jaromir Savelka; Kevin D. Ashley
- Reference count: 36
- Primary result: Automated pipeline evaluates LLM legal argument generation on hallucination, factor utilization, and abstention; most models fail to abstain when instructed despite lacking common factors.

## Executive Summary
This study introduces an automated pipeline to evaluate LLM-generated 3-ply case-based legal arguments, focusing on hallucination, factor utilization, and instruction-following for abstention. The method uses an external LLM to extract factors from generated arguments and compares them against ground-truth factors in input case triples. Eight LLMs were evaluated on three tests: standard argument generation, argument generation with swapped precedent roles, and abstention when no common factors exist. Results show that while most models achieved over 90% hallucination accuracy on viable argument generation tasks, they often failed to utilize the full set of available factors. Critically, most models failed to abstain when instructed despite lacking common factors, highlighting weaknesses in instruction-following and task recognition. The proposed metrics effectively quantify distinct error types and reveal performance variations across models and task complexities.

## Method Summary
The automated pipeline evaluates LLM-generated 3-ply legal arguments by first having the test LLM generate arguments from factor-encoded case triples using structured prompts. An external LLM (GPT-4.1) then extracts factors cited in each argument per case. These extracted factors are compared against ground-truth factors using set-theoretic operations to compute three metrics: Hallucination Accuracy (AccH = 1 - hallucinated_factors/total_ground_truth_factors), Factor Utilization Recall (RecU = utilized_factors/total_ground_truth), and Abstention Ratio (successful_abstentions/total_non_arguable_triples). The evaluation runs on 90 synthetically generated case triples across three progressively challenging tests: Arguable (standard generation), Reordered (precedent roles swapped), and Non-arguable (no common factors with abstention instruction).

## Key Results
- Most models achieved over 90% hallucination accuracy on viable argument generation tasks
- Models often failed to utilize the full set of available factors despite high faithfulness
- Most models failed to abstain when instructed despite lacking common factors, highlighting instruction-following weaknesses

## Why This Works (Mechanism)

### Mechanism 1: External LLM-as-Extractors Ground Evaluation in Structured Representations
Using a high-capability LLM to extract factors from generated arguments enables scalable, reproducible comparison against ground truth. The pipeline prompts GPT-4.1 with the generated argument text and a structured extraction template. The evaluator outputs factor sets per case (F_Ext,CC, F_Ext,TSC1, F_Ext,TSC2). These are compared via set operations to ground-truth factor sets (F_GT) from input case triples to compute Acc_H, Rec_U, and Ratio_Abstain. Core assumption: The extractor LLM identifies factor mentions with sufficiently high accuracy; errors in extraction do not systematically bias metric outcomes. Break condition: If extractor accuracy degrades on complex argument structures or domain-specific phrasing, hallucination and utilization metrics may misattribute errors.

### Mechanism 2: Set-Theoretic Metrics Isolate Distinct Error Types
Normalizing factor-set differences by total ground-truth factors yields interpretable, comparable metrics across models and tasks. Hallucination Accuracy (Acc_H = 1 - N_H/N_GT) counts factors claimed but absent from ground truth. Factor Utilization Recall (Rec_U = N_U/N_GT) counts correctly used ground-truth factors. Abstention Ratio (Ratio_Abstain = N_SA/N_TA) measures instruction adherence on impossible tasks. Each metric targets a single failure mode. Core assumption: Ground-truth factor sets are complete and correctly assigned per case; factor presence is binary. Break condition: If cases have overlapping factor sets where a factor's role varies by context, binary presence/absence may misrepresent faithfulness or completeness.

### Mechanism 3: Progressive Test Curriculum Probes Instruction-Following and Robustness
Structuring evaluation into three tasks of increasing difficulty isolates baseline generation ability, robustness to perturbation, and adherence to negative constraints. Test 1 (Arguable) establishes baseline hallucination and utilization. Test 2 (Reordered) swaps precedent roles, forcing models to select supporting cases based on outcome alignment. Test 3 (Non-arguable) provides no shared factors and includes explicit abstention instructions; success requires recognizing impossibility and halting generation. Core assumption: Models that perform well on Tests 1 and 2 will also follow abstention instructions; instruction-following is independent of generative capability. Break condition: If abstention instructions are phrased ambiguously or models interpret "cannot generate" as a generation prompt, Test 3 may conflate instruction design with model capability.

## Foundational Learning

- Concept: Factor-based case representation (CBR factors)
  - Why needed here: All metrics assume cases are encoded as sets of binary factors (e.g., F4: Agreed-not-to-disclose (P)). Understanding that factors are stereotypical fact patterns with pro-plaintiff or pro-defendant orientation is essential for interpreting hallucination and utilization scores.
  - Quick check question: Given a case with factors F1(D), F4(P), F6(P), what would constitute a hallucinated factor in a generated argument?

- Concept: 3-ply legal argument structure
  - Why needed here: The pipeline evaluates whether models correctly sequence plaintiff citation, defendant counterargument, and plaintiff rebuttal, and whether they correctly select/distinguish precedents per ply.
  - Quick check question: In a 3-ply argument, which ply should cite TSC2 as a counterexample, and what is its intended purpose?

- Concept: Set operations for evaluation metrics
  - Why needed here: Acc_H, Rec_U, and Ratio_Abstain are computed via intersection, difference, and normalization over factor sets. Familiarity with |A âˆ© B| and |A \ B| is required to debug metric calculations.
  - Quick check question: If F_GT = {F1, F4, F6} and F_Ext = {F4, F6, F10}, what are N_H and N_U?

## Architecture Onboarding

- Component map: Scenario Generator -> LLM Under Test -> Factor Extractor (GPT-4.1) -> Metric Calculator
- Critical path: Ensure scenario generator produces triples with correct factor overlaps/absences per test mode. Validate extractor accuracy on a small human-annotated sample before scaling. Confirm prompt includes abstention clause exactly as specified; phrasing changes may invalidate Test 3.
- Design tradeoffs: Using LLM-as-extractor vs human annotation gains scalability and consistency but introduces extractor error layer. Binary factor presence vs weighted importance enables simpler computation but may miss nuance in factor significance. Synthetic vs real case data enables controlled experiments but limits ecological validity for real legal texts.
- Failure signatures: High Acc_H but low Rec_U indicates model is faithful but under-utilizes available factors (common across smaller models). High Acc_H on Test 3 with near-zero Ratio_Abstain shows model generates plausible-sounding but spurious arguments instead of abstaining (instruction-following failure). Extractor misattribution occurs when factors from one case attributed to another in F_Ext; appears as hallucination in metrics.
- First 3 experiments: 1) Baseline run on Test 1 (Arguable) with 10 triples; validate extractor outputs manually; compute Acc_H and Rec_U for 2-3 diverse models. 2) Ablate abstention instruction phrasing on Test 3 (Non-arguable); test whether explicit "stop generation" vs "output phrase" changes Ratio_Abstain. 3) Introduce factor importance weights (e.g., pro-plaintiff vs pro-defendant balance) and compare weighted Rec_U against unweighted scores to assess sensitivity to factor significance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the automated evaluation pipeline be effectively adapted to handle real-world legal documents that require factor extraction from unstructured text rather than synthetic, factor-encoded inputs?
- Basis in paper: [explicit] The authors state in Section 8 that evaluating performance on "real-world legal documents" is a "crucial next step" to address the limitation of using synthetic, factor-represented cases.
- Why unresolved: The current study relied on synthetic case triples where ground-truth factors were pre-defined, simplifying the input representation.
- What evidence would resolve it: Successful application of the pipeline to a dataset of raw legal cases, demonstrating robust factor extraction and metric calculation without pre-encoded inputs.

### Open Question 2
- Question: How strongly do the proposed automated metrics (Hallucination Accuracy, Utilization Recall) correlate with human expert judgments regarding the quality and soundness of legal arguments?
- Basis in paper: [explicit] Section 8 notes that "further validation... potentially including comparisons with human expert judgments on argument quality beyond factor usage" is necessary to strengthen the pipeline.
- Why unresolved: The study relied on automated factor extraction by an external LLM (GPT-4.1), and while spot checks were performed, a comprehensive validation against human standards was not conducted.
- What evidence would resolve it: A study comparing automated metric scores against blind human expert ratings of argument faithfulness and completeness on the same outputs.

### Open Question 3
- Question: What specific mechanistic or prompting interventions can successfully correct the failure of LLMs to abstain when presented with non-arguable legal scenarios?
- Basis in paper: [explicit] Section 8 suggests "investigating the underlying reasons for the observed deficiencies in recall and abstention" and exploring "novel prompting strategies" or "architectural modifications" as vital future work.
- Why unresolved: The results showed most models failed to follow negative constraints (abstention) despite high capability in generation, but the study did not test methods to mitigate this.
- What evidence would resolve it: Experiments testing specific interventions (e.g., chain-of-thought verification, constitutional AI principles) that result in significantly higher Abstention Ratios on non-arguable test sets.

## Limitations
- The study relies on synthetic, factor-encoded case triples rather than real-world legal documents, limiting ecological validity
- The external LLM extractor's accuracy wasn't independently validated against human annotation, introducing uncertainty in metric reliability
- The study doesn't address potential systematic biases in factor extraction or how extractor errors might compound with model generation errors

## Confidence
- Hallucination and Utilization Metrics: Medium confidence - depends critically on external LLM extraction accuracy
- Abstention Measurement: High confidence in design, Medium confidence in execution - may reflect prompt ambiguity rather than fundamental capability gaps
- Synthetic Dataset: Medium confidence - controlled for experimental rigor but limits real-world applicability

## Next Checks
1. Conduct human annotation validation on a random sample (n=20) of generated arguments to measure external LLM extractor accuracy and identify systematic extraction failures
2. Test prompt sensitivity by varying abstention instruction phrasing (explicit "stop" vs implied termination) across 3 model families to isolate instruction-following from task recognition issues
3. Implement weighted factor importance scoring based on pro-plaintiff/defendant orientation and compare weighted vs unweighted utilization metrics to assess whether current binary presence/absence adequately captures argumentative completeness