---
ver: rpa2
title: 'MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages'
arxiv_id: '2512.01512'
source_url: https://arxiv.org/abs/2512.01512
tags:
- language
- speech
- s2tt
- translation
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling multilingual speech-to-text
  translation (S2TT) for large multimodal language models (MLLMs), which face limitations
  in language coverage and inference efficiency. To overcome these issues, the authors
  propose the Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT)
  framework.
---

# MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages

## Quick Facts
- arXiv ID: 2512.01512
- Source URL: https://arxiv.org/abs/2512.01512
- Reference count: 36
- Key result: MCAT extends multilingual S2TT to 70 languages with only ~100M trainable parameters and achieves state-of-the-art performance on FLEURS

## Executive Summary
This paper introduces MCAT (Multilingual Cost-effective Accelerated Speech-to-Text Translator), a framework that scales many-to-many speech-to-text translation to 70 languages using large multimodal language models. The key innovations are a language scaling strategy using curriculum learning and data balancing, and an optimized speech adapter that compresses sequences from 750 to 30 tokens. Extensive experiments demonstrate MCAT outperforms state-of-the-art end-to-end models on FLEURS dataset across 70x69 directions while significantly improving inference efficiency.

## Method Summary
MCAT employs a four-phase curriculum learning approach: (1) ASR pre-training with progressive language expansion (2→28→44→70 languages), (2) balanced ASR fine-tuning with data caps, (3) Speech-guided Machine Translation (SMT) training, and (4) Speech-to-Translation (SRT) training with balanced fine-tuning. The speech adapter uses a Q-Former to extract 150 query features, followed by average pooling (5×) to compress to 30 tokens, then an MLP projects to LLM dimension. LoRA fine-tuning (r=16, alpha=32) is used to adapt the LLM with minimal trainable parameters (~100M). The model is trained on FLEURS and CommonVoice datasets (~4117h total) and evaluated on FLEURS across 70x69 translation directions.

## Key Results
- Achieves state-of-the-art performance on FLEURS dataset across 70x69 translation directions
- Reduces inference sequence length from 750 to 30 tokens with 3.3× speedup
- Trains effectively with minimal data (10 hours per language) and only ~100M trainable parameters
- Demonstrates competitive performance on low-resource language pairs compared to cascaded approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Three-stage curriculum learning bridges the speech-text modality gap more effectively than direct instruction tuning
- **Mechanism:** Model first learns stable acoustic representations via ASR pre-training (Audio→Text), leverages LLM's multilingual text capabilities via SMT (Audio+Text→Translation), then unifies skills in SRT (Audio→Transcription+Translation)
- **Core assumption:** LLM possesses strong inherent translation capabilities that can be activated by aligning speech features with text embeddings
- **Evidence anchors:** Abstract mentions "curriculum learning... to extend to 70 languages"; Table IX shows spBLEU drops from 31.0 to 14.1 without SMT/SRT stages; related work supports step-by-step transcription-then-translation logic
- **Break condition:** If base LLM has poor cross-lingual transfer or ASR data is too noisy, initial alignment fails and subsequent stages cannot activate translation capabilities

### Mechanism 2
- **Claim:** Aggressive sequence compression via Q-Former and pooling preserves semantic content while alleviating inference latency
- **Mechanism:** Q-Former extracts 150 query features from Whisper encoder, followed by pooling layer downsampling 5× to yield 30 tokens
- **Core assumption:** Whisper encoder captures sufficient semantic density that 30 compressed tokens can represent 30-second audio meaning without catastrophic information loss
- **Evidence anchors:** Abstract mentions "compresses speech sequences from 750 to just 30 tokens"; Table XI demonstrates 3.3× speedup compared to Qwen2.5-Omni-7B; neighbor papers discuss efficiency in real-time systems
- **Break condition:** If source speech contains critical paralinguistic information (tone, hesitations) or highly complex syntax, 25× compression may lose nuance preserved by longer sequences

### Mechanism 3
- **Claim:** Progressive language scaling with strict data caps mitigates catastrophic forgetting and stabilizes many-to-many translation
- **Mechanism:** Training proceeds in phases (2→28→44→70 languages) while capping high-resource languages (10k samples for ASR, 100 for SRT directions) to prevent overfitting to dominant languages
- **Core assumption:** Balanced distribution of languages, even with low total volume (10h/language), yields better cross-lingual alignment than imbalanced dataset with higher volume
- **Evidence anchors:** Section III.C.3 describes "Data Balancing Strategy" and "Balanced SRT Fine-Tuning" with specific caps; Figure 5 shows uniform performance across 70×70 grid; POTSA highlights need for balanced multilingual alignment
- **Break condition:** If new low-resource language lacks sufficient ASR data to bootstrap initial alignment, model fails to map speech to LLM's semantic space

## Foundational Learning

- **Concept: Q-Former (Querying Transformer)**
  - **Why needed here:** Core compressor that uses attention to extract most relevant features from long Whisper encoder sequence
  - **Quick check question:** Can you explain how learnable query vectors interact with encoder outputs to reduce sequence length?

- **Concept: Curriculum Learning**
  - **Why needed here:** Training is staged, not mixed; understanding why "Audio→Text" must precede "Audio→Translation" is vital for reproducing results
  - **Quick check question:** Why would training on S2TT directly fail if model hasn't first learned basic ASR?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** Paper trains only ~100M parameters on massive models (9B/27B); understanding LoRA is necessary to understand how they fine-tune LLM backbone without updating full weights
  - **Quick check question:** Where are LoRA adapters applied in LLM architecture to inject cross-modal capabilities?

## Architecture Onboarding

- **Component map:** Raw Waveform → Mel-spectrogram → Frozen Whisper-large-v3 (1500 sequence length) → Speech Adapter → LLM (GemmaX2-9B or Gemma3-27B-it with LoRA)
- **Critical path:** Optimization of Speech Adapter is most critical constraint; if Q-Former is not trained extensively in ASR phase, compressed 30-token representation will be garbage-in for SMT/SRT phases
- **Design tradeoffs:**
  - Compression vs. Prosody: Reducing audio to 30 tokens likely discards non-semantic audio features (speaker identity, emotion) to maximize translation efficiency
  - English-centricity vs. Balance: Model sacrifices peak English performance (marginally) to ensure non-English pairs function better than cascaded baselines
- **Failure signatures:**
  - "The Amharic Problem": High performance when translating into low-resource language, but failure when translating from it (LLM knows language text, but Speech Encoder/Adapter failed acoustic mapping)
  - Catastrophic Drop: Remove data capping strategy → high resource scores rise while low-resource scores collapse
- **First 3 experiments:**
  1. Verify Adapter Compression: Train only adapter on ASR (Phase 1) to verify 30 tokens sufficient for transcription accuracy on high-resource language before attempting full S2TT
  2. Ablate SMT Stage: Skip Speech-guided MT stage, train SRT directly; measure drop in COMET score to quantify value of "activating" LLM's text translation capabilities
  3. Inference Latency Test: Run batch inference comparison between standard 750-token baseline and 30-token MCAT adapter to validate claimed 3× speedup

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed framework effectively support languages where the underlying LLM (e.g., Gemma) lacks strong text-based translation capabilities?
- **Basis in paper:** [Explicit] Limitations section states S2TT performance is "constrained by capabilities of the LLM" and may fail for languages with poor machine translation support
- **Why unresolved:** Authors restricted language support to those where base LLM met specific COMET threshold, leaving performance on "unsupported" LLM languages untested
- **What evidence would resolve it:** Benchmarking MCAT on languages explicitly excluded due to poor LLM performance to see if speech features can bridge text capability gap

### Open Question 2
- **Question:** What is the precise data threshold required to resolve the "asymmetry" where low-resource languages are strong translation targets but poor sources?
- **Basis in paper:** [Inferred] Analysis notes languages like Burmese perform well as targets but fail as sources due to "scarcity of speech recognition data"
- **Why unresolved:** Paper identifies source-side bottleneck but does not quantify specific amount of ASR data needed to equalize source and target performance
- **What evidence would resolve it:** Scaling law analysis focusing solely on source-language ASR data volume for identified asymmetric languages (e.g., Burmese, Amharic)

### Open Question 3
- **Question:** Does the aggressive 25× token compression (750 to 30 tokens) discard prosodic or paralinguistic information?
- **Basis in paper:** [Inferred] Adapter design prioritizes text generation efficiency, yet evaluation restricted to semantic text metrics (BLEU/COMET)
- **Why unresolved:** Unclear if compressing 30 seconds of audio into 30 tokens retains non-lexical cues like emotion, tone, or speaker intent
- **What evidence would resolve it:** Subjective or categorical evaluations on prosody-rich datasets to verify if non-semantic information survives compression bottleneck

## Limitations
- Performance constrained by underlying LLM's text translation capabilities for low-resource languages
- Aggressive 25× token compression may discard prosodic and paralinguistic information
- Requires sufficient ASR data for low-resource languages to establish initial speech-text alignment

## Confidence
- **High Confidence:** 4-phase curriculum learning approach significantly improves performance over direct end-to-end training; Q-Former + pooling adapter reduces inference sequence length with substantial speedup; data balancing strategies prevent catastrophic forgetting
- **Medium Confidence:** Achieves state-of-the-art performance on FLEURS across 70x69 directions; ~100M trainable parameters sufficient with minimal data (10h/language); competitive performance on low-resource language pairs
- **Low Confidence:** Exact contribution of each curriculum phase to final performance; generalization capability beyond 70 FLEURS languages; robustness to real-world audio conditions

## Next Checks
1. **Adapter Compression Verification:** Implement and train only speech adapter on ASR task with subset of high-resource languages to verify 30 tokens sufficient for accurate transcription; compare against baseline adapters producing 150+ tokens
2. **Curriculum Phase Ablation:** Conduct systematic ablation study removing each curriculum phase (ASR, SMT, SRT) individually and in combination; measure exact performance degradation to quantify contribution of each phase to final COMET score
3. **Inference Efficiency Validation:** Perform controlled batch inference experiments comparing MCAT's 30-token adapter against standard 750-token baseline on same hardware; measure raw speed, memory consumption, and throughput to validate claimed 3.3× speedup and assess practical deployment implications