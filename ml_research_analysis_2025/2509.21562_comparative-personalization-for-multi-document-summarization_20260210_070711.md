---
ver: rpa2
title: Comparative Personalization for Multi-document Summarization
arxiv_id: '2509.21562'
source_url: https://arxiv.org/abs/2509.21562
tags:
- user
- documents
- profile
- style
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating personalized multi-document
  summaries that cater to individual user preferences in writing style and content
  focus. The authors propose ComPSum, a framework that generates a structured analysis
  of a user's preferences by comparing their profile documents with those of other
  users on the same topic.
---

# Comparative Personalization for Multi-document Summarization

## Quick Facts
- arXiv ID: 2509.21562
- Source URL: https://arxiv.org/abs/2509.21562
- Reference count: 26
- This paper addresses the challenge of generating personalized multi-document summaries that cater to individual user preferences in writing style and content focus.

## Executive Summary
This paper introduces ComPSum, a framework for personalized multi-document summarization that extracts fine-grained user preferences by comparing a user's profile documents against others' on the same topic. The method generates structured analyses of writing style and content focus, which guide the generation of personalized summaries. To evaluate personalization without ground-truth references, the authors propose AuthorMap, a reference-free evaluation framework based on authorship attribution. Experiments on the new PerMSum dataset show ComPSum significantly outperforms baselines in personalization while maintaining factuality and relevance across news and review domains.

## Method Summary
ComPSum operates through a retrieval-augmented generation pipeline. For a given input document set, it retrieves k=5 profile documents per user via BM25 using the concatenated input as query. For each profile document, it identifies the most dissimilar document from the same topic but written by a different user. An LLM then generates structured analysis comparing the target user's preferences against the comparative document across writing style and content focus dimensions. This structured analysis, along with the profile documents and input documents, conditions a summarization LLM to produce a personalized summary. The method is evaluated using AuthorMap, which measures personalization through authorship attribution accuracy between paired summaries generated for different users.

## Key Results
- ComPSum achieves an overall score of 74.54 compared to 71.32 for the best baseline on PerMSum dataset
- Significant improvements in personalization metrics across both news and review domains
- AuthorMap achieves 76.65-89.00% accuracy on human-written documents, outperforming variants without retrieval
- Structured analysis approach maintains factuality while improving personalization (vs. rehearsal baselines that copy profile content)

## Why This Works (Mechanism)

### Mechanism 1
Comparative documents enable extraction of distinguishing user features that single-user profile analysis misses. For each retrieved profile document, ComPSum identifies a comparative document from a different user on the same topic (retrieving the most dissimilar one), then prompts an LLM to generate structured analysis that explicitly contrasts the target user against others. This assumes differences between documents on the same topic reflect personal preferences rather than topic variation. Evidence shows ComPSum produces lower similarity scores between analyses for different users (80.89) vs. without comparative docs (82.52).

### Mechanism 2
Structured separation of style and content dimensions enables independent control and evaluation of personalization. The analysis generation prompt explicitly requests two separate JSON fields—"style_analysis" and "content_analysis"—with constraints to focus each on specific linguistic or topical features. This structure is then provided to the summarization LLM. The core assumption is that writing style and content focus are sufficiently independent dimensions. AuthorMap evaluation shows expected patterns: higher accuracy on style-matched pairs for content evaluation (70.58 vs. 54.10) and higher accuracy on content-matched pairs for style evaluation (77.93 vs. 70.34).

### Mechanism 3
Authorship attribution between paired summaries provides a reference-free proxy for personalization quality. AuthorMap generates two summaries for different users on the same input, then asks an LLM judge to attribute retrieved profile documents to the correct user based on each summary. Accuracy reflects how distinguishable the personalizations are. The core assumption is that well-personalized summaries will have sufficiently distinctive style/content to enable correct authorship attribution. Human evaluation shows 73-80% agreement between AuthorMap and human annotators across domains.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG) for personalization**: ComPSum builds on RAG-style personalization where profile documents are retrieved to condition generation, but extends it with comparative analysis. Quick check: Can you explain why retrieving the most dissimilar comparative document might yield better preference signals than retrieving similar ones?

- **Authorship attribution and stylometry**: AuthorMap's evaluation relies on the premise that writing style and content preferences create identifiable authorship signatures. Quick check: What linguistic features would you examine to distinguish between two authors writing on the same topic?

- **Reference-free evaluation for generation tasks**: Personalized summarization lacks ground-truth references; AuthorMap demonstrates an alternative evaluation paradigm. Quick check: Why might ROUGE/BERTScore be inadequate for evaluating whether a summary matches a specific user's preferences?

## Architecture Onboarding

- **Component map**: Input documents D -> concatenate -> retrieve top-k profile docs -> for each profile doc -> find comparative doc (most dissimilar, same topic, different author) -> all pairs -> analysis LLM -> structured analysis JSON -> profile docs + analysis + input docs -> summary LLM -> personalized summary

- **Critical path**: 1) Input documents concatenated and used as BM25 query to retrieve k=5 profile documents per user 2) For each profile document, retrieve one comparative document from same document set by different author with maximum dissimilarity 3) All (profile, comparative) pairs fed to analysis LLM to generate structured JSON with style and content analysis fields 4) Summary LLM consumes profile documents, structured analysis, and input documents to produce ≤100 word personalized summary

- **Design tradeoffs**: Most dissimilar vs. most similar comparative docs (ablation shows dissimilar performs better: 74.87 vs. 74.00 overall); Direct multi-doc analysis vs. staged single-doc analysis (ComPSum outperforms multi-stage: 74.87 vs. 72.02); Retrieval vs. random sampling for AuthorMap evaluation (retrieval improves accuracy by 1-2 points)

- **Failure signatures**: Low personalization scores with high factuality may indicate topic-mismatched comparative documents; High style but low content scores (or vice versa) suggests one dimension's prompt needs refinement; Rehearsal-style failure shows high personalization (99%+) but catastrophically low factuality (21-23%) indicating model copying profile content

- **First 3 experiments**: 1) Validate comparative document selection by running ComPSum with most-similar vs. most-dissimilar comparative documents and measure inter-user analysis similarity 2) Ablate structured analysis by comparing full ComPSum vs. unstructured profile summary and no comparative documents 3) Cross-domain robustness by training on news and testing on reviews (and reverse) to assess prompt generalization

## Open Questions the Paper Calls Out

### Open Question 1
Can methods for automatically identifying or generating comparable documents be developed to generalize ComPSum to other personalized text generation tasks? The current framework requires manual or dataset-specific clustering to ensure documents share a topic, which is difficult to scale to general domains. A new mechanism that automates the discovery of contrastive context would allow the comparative analysis module to function effectively on unstructured, diverse user profiles.

### Open Question 2
How does the definition of user profiles via implicit feedback (clicks/likes) versus explicit authorship affect the performance of personalized summarization in the news domain? Defining profiles via "documents clicked or liked by the user" is more realistic, but current experiments use authorship due to dataset constraints. An evaluation on a dataset linking full-text news articles to user reading history rather than author metadata would resolve this.

### Open Question 3
Does the heuristic of retrieving the "most dissimilar" comparative document introduce noise that degrades the structured analysis? Maximizing dissimilarity might select outlier documents that differ on irrelevant axes, potentially misleading the LLM during analysis generation. A fine-grained ablation study correlating the semantic similarity scores of comparative documents with the quality and accuracy of the resulting structured analysis would provide evidence.

## Limitations
- Dataset construction requires substantial preprocessing from source corpora with critical decisions about cluster formation thresholds and filtering rules
- Evaluation sensitivity to LLM judge performance, prompt phrasing, and model versions with 73-80% human agreement rate
- Comparative document selection assumes topic-consistent differences reflect personal preferences, which breaks when documents differ due to topic rather than preference

## Confidence
- **High Confidence**: Claims about ComPSum outperforming baselines on PerMSum with described evaluation setup
- **Medium Confidence**: Claims about structured analysis enabling independent control of style and content dimensions
- **Low Confidence**: Claims about AuthorMap's generalizability as a reference-free evaluation framework

## Next Checks
1. Reconstruct the exact PerMSum preprocessing pipeline and verify human-written documents achieve similar AuthorMap accuracy (76.65-89.00%) as reported
2. Run AuthorMap evaluation with multiple LLM judges on the same dataset and measure inter-judge agreement to test judge dependency
3. Systematically test different similarity thresholds for selecting comparative documents and measure impact on personalization scores and factuality