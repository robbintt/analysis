---
ver: rpa2
title: 'RePOPE: Impact of Annotation Errors on the POPE Benchmark'
arxiv_id: '2504.15707'
source_url: https://arxiv.org/abs/2504.15707
tags:
- internvl2
- pope
- repope
- errors
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of annotation errors in the MSCOCO
  dataset on the POPE benchmark, a widely used standard for evaluating object hallucination
  in vision-language models. The authors re-annotate 500 images from POPE and identify
  a significant imbalance in annotation errors across different subsets, with higher
  error rates on the positive set ("Yes" answers) compared to the negative set ("No"
  answers).
---

# RePOPE: Impact of Annotation Errors on the POPE Benchmark

## Quick Facts
- arXiv ID: 2504.15707
- Source URL: https://arxiv.org/abs/2504.15707
- Reference count: 40
- Authors: Yannic Neuhaus; Matthias Hein
- Key outcome: Re-annotation of POPE benchmark reveals significant label errors affecting model rankings

## Executive Summary
This study investigates the impact of annotation errors in the POPE benchmark, a standard for evaluating object hallucination in vision-language models. The authors re-annotate 500 images from POPE and identify significant imbalances in annotation errors across different subsets, with higher error rates on positive examples. They create RePOPE, a corrected label set, and evaluate multiple models on both datasets. The results demonstrate that label quality substantially affects model rankings, with some models dropping significantly in position while others improve.

## Method Summary
The authors conducted a comprehensive re-annotation of 500 images from the POPE benchmark, examining both the original annotations and model predictions. They identified and corrected annotation errors, creating the RePOPE dataset. The study evaluated multiple vision-language models on both POPE and RePOPE using F1 scores to measure the impact of label quality on model rankings. The analysis focused on understanding how annotation errors influence the assessment of object hallucination vulnerability across different model architectures.

## Key Results
- Significant annotation error imbalance found between positive ("Yes") and negative ("No") subsets in POPE
- Model rankings based on F1 scores shift notably between POPE and RePOPE
- High-performing models on POPE drop in ranking when evaluated on corrected labels, while others improve
- Demonstrates substantial impact of data quality on benchmark outcomes

## Why This Works (Mechanism)
The mechanism relies on the principle that annotation errors create systematic biases in benchmark evaluation. When labels are incorrect, models can appear to perform better or worse than their true capabilities, leading to misleading rankings. By identifying and correcting these errors, the study reveals the true performance characteristics of different model architectures, providing a more accurate assessment of their object hallucination vulnerabilities.

## Foundational Learning

### Object Hallucination in Vision-Language Models
- Why needed: Core concept being evaluated in the benchmark
- Quick check: Models generating descriptions of objects that don't exist in images

### Benchmark Evaluation Methodology
- Why needed: Framework for understanding how annotation quality affects results
- Quick check: Comparison of model performance metrics under different label conditions

### F1 Score Calculation
- Why needed: Primary metric used for model ranking comparison
- Quick check: Harmonic mean of precision and recall in classification tasks

## Architecture Onboarding

### Component Map
Input Images -> Vision-Language Models -> Object Detection Predictions -> Annotation Comparison -> Performance Evaluation

### Critical Path
Image annotation -> Model prediction generation -> F1 score calculation -> Model ranking determination

### Design Tradeoffs
The study prioritizes accuracy over scale by focusing on detailed re-annotation of a smaller sample rather than broader coverage with potentially lower accuracy.

### Failure Signatures
Models that appear highly accurate on noisy labels but show significant performance drops when evaluated on corrected labels indicate vulnerability to annotation error exploitation.

### First Experiments
1. Evaluate baseline models on original POPE dataset
2. Compare model rankings between POPE and RePOPE using F1 scores
3. Analyze error distribution patterns across different model architectures

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Relatively small re-annotation sample size (500 images) may not capture full dataset characteristics
- Manual annotation process introduces potential subjectivity in error identification
- Lack of comprehensive inter-annotator agreement metrics reduces confidence in error identification

## Confidence
- Error identification reliability: Medium
- Ranking shift significance: High
- Generalizability of findings: Medium-Low

## Next Checks
1. Expand re-annotation to 2000+ images to improve statistical power and generalizability
2. Conduct comprehensive inter-annotator agreement analysis with multiple annotators
3. Test additional model architectures and evaluation metrics to verify consistency of ranking shifts