---
ver: rpa2
title: 'K-Merge: Online Continual Merging of Adapters for On-device Large Language
  Models'
arxiv_id: '2510.13537'
source_url: https://arxiv.org/abs/2510.13537
tags:
- lora
- loras
- merging
- problem
- k-merge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incrementally integrating new
  task-specific LoRA adapters on resource-constrained devices while maintaining performance
  across all previously supported tasks. The proposed K-Merge and K-Merge++ methods
  use cosine similarity between LoRA parameter vectors to identify the most compatible
  stored adapter for merging, then apply a history-aware weighted combination that
  preserves prior task capabilities.
---

# K-Merge: Online Continual Merging of Adapters for On-device Large Language Models

## Quick Facts
- arXiv ID: 2510.13537
- Source URL: https://arxiv.org/abs/2510.13537
- Reference count: 28
- Primary result: Achieves 80-90% of single-task LoRA performance using only 8 adapter slots across 40 tasks spanning 5 problem types and 8 languages

## Executive Summary
This paper introduces K-Merge and K-Merge++, online continual merging methods for integrating new LoRA adapters on resource-constrained devices while preserving performance across previously supported tasks. The methods use cosine similarity between LoRA parameter vectors to identify compatible adapters for merging, then apply history-aware weighted combinations that maintain prior task capabilities. Experiments demonstrate 80-90% of single-task performance with only 8 adapter slots, outperforming baseline merging strategies while remaining computationally lightweight for on-device deployment.

## Method Summary
The approach incrementally integrates LoRA adapters by first computing cosine similarity between flattened parameter vectors of incoming and stored adapters. For K-Merge, the incoming LoRA merges with the most similar stored adapter using a history-aware weighted average that preserves prior task performance. K-Merge++ adds a similarity threshold mechanism to decide between merging and allocating new storage slots, improving robustness to adversarial task orderings. The method maintains a history map tracking which tasks are in each cluster, enabling correct adapter retrieval during inference. Both variants operate without training data and require only O(K) similarity computations per merge.

## Key Results
- K-Merge and K-Merge++ achieve 80-90% of single-task LoRA performance using only 8 adapter slots
- K-Merge++ maintains 0.80 aggregate score under worst-case task ordering vs. K-Merge's 0.75
- Computational overhead is minimal (0.04-0.18s per merge for 2-8 stored adapters)
- Outperforms baseline strategies including Linear merge, TIES, and DARE in online continual setting

## Why This Works (Mechanism)

### Mechanism 1: Similarity-Based Cluster Assignment
Flattening LoRA parameter vectors and computing cosine similarity identifies the most compatible stored adapter for merging without requiring training data. LoRA parameters are structured as low-rank matrix pairs (A_{n,p}, B_{n,p}) per layer n and projection type p. The method computes ΔW_{n,p} = B_{n,p} A_{n,p}, flattens each into a vector, then averages cosine similarity across all layers and projections. This captures structural alignment in parameter space, which correlates with task relatedness.

### Mechanism 2: History-Aware Weighted Merging
A running-average formulation preserves prior task capabilities by weighting each adapter equally regardless of merge order. When merging incoming LoRA L^{(t)} into stored adapter L_c, the update is: merge(L_c, L^{(t)}) = (L^{(t)} + |H^{(t-1)}[c]| · L_c) / (|H^{(t-1)}[c]| + 1). The history map H^{(t)} tracks which tasks are in each cluster, enabling correct adapter retrieval at inference time.

### Mechanism 3: Threshold-Based Slot Allocation (K-Merge++)
Using a similarity threshold s to decide between merging vs. allocating a new slot preserves storage diversity and improves robustness to adversarial task orderings. When |L^{(t-1)}| < K and sim(L^{(t)}, L_c) < s, the incoming LoRA gets its own slot. This prevents early similar LoRAs from exhausting storage slots. Threshold s is estimated as the median pairwise similarity from a held-out LoRA set.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: K-Merge operates on LoRA parameter structure (A, B matrices); understanding rank, scaling factor α, and how LoRA deltas compose is essential for interpreting merging mechanics.
  - Quick check question: If a LoRA has rank 32 and α=128, what is the effective scaling applied to ΔW = BA?

- **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The core problem is preserving prior task performance when integrating new adapters; the history-aware mechanism is a direct mitigation strategy.
  - Quick check question: Why does sequential fine-tuning on new tasks degrade performance on old tasks, and how does weight averaging help?

- **Cosine Similarity in Parameter Space**
  - Why needed here: The clustering decision hinges on measuring alignment between parameter vectors; understanding what high/low cosine similarity implies about functional similarity is critical.
  - Quick check question: Two LoRAs have cosine similarity of 0.02. Is this high or low in this context, and what does it suggest about merging them?

## Architecture Onboarding

- **Component map:**
  Incoming LoRA handler -> Similarity engine -> Decision module -> Merger -> History tracker -> Inference router

- **Critical path:**
  1. LoRA download → 2. Similarity computation (O(K) vector ops) → 3. Threshold check → 4. Merge or slot allocation → 5. History update
  - Latency: 0.04–0.18s for 2–8 stored adapters

- **Design tradeoffs:**
  - K (slot budget): Higher K = better performance but more storage (~27–34MB per LoRA; 40 LoRAs ≈ 1GB)
  - Threshold s: Lower s = more aggressive merging, risks interference; higher s = more slots used early, risks exhaustion
  - Merge strategy: Simple averaging is fast and order-invariant; alternatives (TIES, DARE) underperform in this online setting

- **Failure signatures:**
  - Cluster collapse: All tasks route to one adapter; check if threshold too high or early LoRAs highly similar
  - Slot exhaustion before diversity: K-Merge (not ++) under worst-case ordering shows 0.62 vs 0.73 score
  - Threshold misconfiguration: If s too high, every LoRA gets its own slot prematurely; if too low, incompatible merges degrade all tasks

- **First 3 experiments:**
  1. Reproduce baseline comparison: Run Linear, TIES, DARE, K-Merge, K-Merge++ on 40-task benchmark with K=5; verify scores match Table A.5/A.6 within ±0.02
  2. Ablate threshold s: Test s ∈ {0.010, 0.015, 0.020, 0.025} on held-out tasks; confirm median heuristic is optimal or identify better calibration method
  3. Stress test ordering robustness: Construct adversarial ordering (same problem type consecutive); compare K-Merge vs K-Merge++ degradation patterns; verify K-Merge++ maintains ≥0.95 of random-ordering performance

## Open Questions the Paper Calls Out

### Open Question 1
How does continual LoRA merging affect safety alignment mechanisms and refusal behaviors in merged adapters? The study optimizes only for task performance metrics; weighted averaging may dilute safety-critical parameter patterns encoded during RLHF or safety fine-tuning. Systematic evaluation on safety benchmarks (e.g., refusal rate tests, red-teaming datasets) comparing merged vs. individual adapters would resolve this.

### Open Question 2
Does the cosine similarity-based selection strategy generalize to adapter architectures beyond LoRA? Different PEFT methods encode task information differently (e.g., DoRA decomposes magnitude and direction), and parameter-space similarity may not correlate with functional merge compatibility across architectures. Experiments applying K-Merge to DoRA, AdaLoRA, and prefix-tuning methods would resolve this.

### Open Question 3
How can the similarity threshold be determined in deployment scenarios without access to held-out calibration tasks? The threshold critically controls slot allocation decisions, yet no adaptive or self-calibrating mechanism exists for online threshold estimation. Ablation studies testing online threshold adaptation strategies or task-agnostic estimation methods would resolve this.

## Limitations
- Representativeness of held-out similarity threshold: The median pairwise similarity heuristic assumes the held-out LoRA set is representative of future task distributions
- Parameter similarity as proxy for functional compatibility: Cosine similarity may not capture semantic task relationships, potentially leading to suboptimal merges
- Scalability beyond 40 tasks: Method's behavior on larger task pools (>100 tasks) or more diverse problem types remains untested

## Confidence

**High confidence**: Core mechanism of similarity-based clustering combined with history-aware weighted merging is well-specified and empirically validated. Computational efficiency claims (0.04-0.18s per merge) are directly supported by experimental measurements.

**Medium confidence**: Threshold-based slot allocation mechanism shows promising robustness improvements but relies on assumptions about task distribution similarity. 80-90% performance retention claim assumes reasonably representative task ordering.

**Low confidence**: Method's generalization to task distributions substantially different from the held-out set used for threshold calibration. Long-term stability of merged adapters when deployed across extended periods with evolving task requirements.

## Next Checks

1. **Distribution shift stress test**: Evaluate K-Merge++ performance when incoming tasks are drawn from a different distribution than the held-out set used for threshold calibration. Measure sensitivity of s to distributional shifts and identify calibration strategies for dynamic environments.

2. **Functional compatibility analysis**: Systematically test merging of functionally dissimilar tasks (e.g., summarization + code generation) to validate whether parameter similarity correlates with merge compatibility. Measure performance degradation patterns to identify failure modes.

3. **Long-term deployment simulation**: Run extended simulations with >100 tasks and varying arrival patterns to assess scalability limits, computational overhead growth, and potential degradation in merge quality over time.