---
ver: rpa2
title: 'Beyond the Leaderboard: Understanding Performance Disparities in Large Language
  Models via Model Diffing'
arxiv_id: '2509.18792'
source_url: https://arxiv.org/abs/2509.18792
tags:
- simpo
- latent
- capabilities
- arxiv
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work applies model diffing with crosscoders to compare Gemma-2-9b-it
  and its SimPO-enhanced variant, identifying latent concepts that explain performance
  differences. Analysis of 92 SimPO-specific latents reveals targeted capability shifts:
  substantial gains in safety (+32.8%), multilingual processing (+43.8%), and instruction-following
  (+151.7%), alongside regressions in hallucination detection (-68.5%), self-reference
  (-44.1%), and structured output generation (-37.1%).'
---

# Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing

## Quick Facts
- arXiv ID: 2509.18792
- Source URL: https://arxiv.org/abs/2509.18792
- Authors: Sabri Boughorbel; Fahim Dalvi; Nadir Durrani; Majd Hawasly
- Reference count: 12
- Primary result: Model diffing with crosscoders identifies latent concepts explaining performance differences between Gemma-2-9b-it and SimPO-enhanced variants

## Executive Summary
This paper introduces a model diffing approach using crosscoders to compare fine-tuned large language models at the representational level. The authors apply this method to analyze differences between Gemma-2-9b-it and its SimPO-enhanced variant, identifying 92 SimPO-specific latent representations that explain targeted capability shifts. The analysis reveals substantial gains in safety, multilingual processing, and instruction-following, alongside regressions in hallucination detection and structured output generation. This work demonstrates that crosscoders can uncover fine-grained behavioral changes invisible to traditional benchmarks, offering a transparent framework for comparing LLMs beyond leaderboard metrics.

## Method Summary
The authors employ crosscoders to compare internal representations of two fine-tuned Gemma-2-9b-it variants: the base instruction-tuned model and its SimPO-enhanced counterpart. Crosscoders use shared encoders to reconstruct activations from both models while learning separate decoder vectors per latent dimension. By comparing decoder vector norms, the approach identifies latent concepts uniquely emphasized in each model. The authors train crosscoders on activation pairs from layer 20, apply latent scaling to identify model-specific latents, and categorize them through LLM annotation of high-activation documents. This methodology reveals interpretable capability differences that direct comparison between the fine-tuned models would miss.

## Key Results
- Identified 92 latent representations unique to SimPO that explain targeted capability shifts
- Substantial gains in safety (+32.8%), multilingual processing (+43.8%), and instruction-following (+151.7%)
- Regressions in hallucination detection (-68.5%), self-reference (-44.1%), and structured output generation (-37.1%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Crosscoders can isolate model-specific latent representations by learning shared dictionaries with divergent decoder directions.
- Mechanism: A crosscoder trains a single shared encoder to reconstruct activations from both models, but learns separate decoder vectors per latent dimension for each model. The norm difference between decoder vectors (∆norm) quantifies how strongly a latent concept is represented in one model versus the other.
- Core assumption: Representational differences manifest as systematic differences in decoder direction magnitudes, not just activation patterns.
- Evidence anchors:
  - [abstract]: "Using crosscoders, we identify and categorize latent representations that differentiate the two models."
  - [section 2.1]: "By comparing the norm differences between corresponding latent vectors in each model, we can identify concepts that are uniquely important to one model relative to the other."
  - [corpus]: Minder et al. (2025) "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning" validates crosscoder-based model diffing for chat-tuning analysis.
- Break condition: If models share nearly identical representations (e.g., it vs it-SimPO direct comparison), norm differences cluster in uninterpretable ranges (0.3–0.6), yielding no distinct latents.

### Mechanism 2
- Claim: Comparing each fine-tuned model to a shared base model surfaces interpretable latent differences that direct comparison obscures.
- Mechanism: The authors initially compared it → it-SimPO directly and found no significant latent differences. They then compared pt → it and pt → SimPO separately, identifying 113 it-unique and 92 SimPO-unique latents respectively. This tripartite comparison amplifies signal by referencing a common baseline.
- Core assumption: Fine-tuning stages operate in progressively narrower behavioral subspaces; later stages (SimPO) modify surface-level signals rather than fundamental representations.
- Evidence anchors:
  - [section 2.3]: "A likely explanation is that once a model has undergone instruction tuning, further improvements like SimPO operate within a narrow behavioral subspace."
  - [section 2.3]: "We identified 92 latents unique to SimPO in pt-SimPO and 113 latents unique to it in it-pt crosscoder."
  - [corpus]: Weak corpus signal for this specific design iteration pattern.
- Break condition: If the base model is too distant (different architecture), shared latents become sparse and comparison fails.

### Mechanism 3
- Claim: Latent categories derived from high-activation documents correlate with measurable capability shifts in downstream behavior.
- Mechanism: For each high-norm latent, retrieve top-activating documents, use an LLM to annotate themes, then cluster into a taxonomy (7 classes, 30 categories). Normalized category frequencies quantify capability emphasis changes.
- Core assumption: High-activation documents faithfully represent latent semantics; LLM annotation produces consistent categorizations.
- Evidence anchors:
  - [section 2.2]: "We extracted documents that strongly activated the identified latents from the training dataset, and used a large language model (Claude 3 Opus) to annotate and categorize these documents."
  - [table 2]: Shows quantified class-level changes (e.g., Safety +32.8%, Linguistic +43.8%, Error Handling -28.1%).
  - [corpus: Paulo et al. 2024] Validates LLM-based feature interpretation at scale.
- Break condition: If latents are polysemantic or activation patterns are noisy, categorization becomes inconsistent.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: Crosscoders extend SAEs to multi-model settings; understanding sparsity, reconstruction loss, and dictionary learning is prerequisite.
  - Quick check question: Can you explain why sparsity constraints help isolate interpretable features in neural activations?

- Concept: Preference Optimization (SimPO/DPO)
  - Why needed here: The paper analyzes what SimPO changes mechanistically; understanding the training objective clarifies expected behavioral shifts.
  - Quick check question: How does SimPO differ from RLHF in its reward formulation?

- Concept: Mechanistic Interpretability
  - Why needed here: The paper operates in the MI paradigm, assuming internal representations can be meaningfully analyzed and attributed to capabilities.
  - Quick check question: What does "monosemanticity" mean in the context of feature decomposition?

## Architecture Onboarding

- Component map:
  - FineWeb + LMSys (200M tokens) -> Tokenized sequences -> Activation extraction at layer 20 -> Crosscoder training -> Latent scaling -> Model-specific latent identification -> Top-N document retrieval -> LLM annotation -> Taxonomy clustering -> Category frequency deltas

- Critical path:
  1. Train crosscoders on activation pairs (pt–it, pt–SimPO)
  2. Apply Latent Scaling to identify unique latents per model
  3. Retrieve and annotate high-activation documents
  4. Compute category frequency deltas between models

- Design tradeoffs:
  - **Layer selection**: Layer 20 chosen per prior work; earlier layers may capture syntax, later layers more abstract. Trade-off between interpretability and behavioral relevance.
  - **Latent dimensionality**: 114,688 balances granularity vs computational cost; too few latents → polysemantic features.
  - **Direct vs base comparison**: Direct comparison yields cleaner experimental design but noisier signal; base comparison requires training two crosscoders but produces clearer latent separation.

- Failure signatures:
  - **Complete Shrinkage**: Shared latents misclassified as model-specific due to decoder norm collapse.
  - **Latent Decoupling**: Single underlying concept splits into multiple latents artificially inflating uniqueness.
  - **Uninterpretable clusters**: If norm differences cluster 0.3–0.6 without clear separation, comparison pair is too similar.

- First 3 experiments:
  1. Reproduce the pt–it crosscoder on a smaller corpus (e.g., 10M tokens) to validate pipeline; check latent separation using νϵ/νr scatter plots.
  2. Run ablation: compare it vs it-SimPO directly and verify the "no significant difference" finding; analyze why norm differences cluster in middle range.
  3. Extend to a new model pair (e.g., Llama-3-8B vs Llama-3-8B-Instruct) using the same layer/dimensionality settings to test generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reduction in hallucination detection latents in SimPO models correspond to increased factual errors or confabulations in generated outputs?
- Basis in paper: [explicit] The authors report a –68.5% decrease in hallucination detection capabilities and state this "raises questions about whether SimPO sacrifices some self-monitoring capabilities in favor of producing more confident-sounding responses."
- Why unresolved: The crosscoder analysis identifies representational shifts but cannot establish causal links between latent changes and specific output behaviors.
- What evidence would resolve it: Targeted behavioral testing comparing factual accuracy and hallucination rates between the models using benchmarks designed to elicit confabulations.

### Open Question 2
- Question: Can model diffing detect meaningful differences when directly comparing two instruction-tuned models without reference to a base model?
- Basis in paper: [explicit] The authors report that direct comparison between Gemma-2-9b-it and SimPO "showed nonsignificant difference" and required comparing each to the shared base model instead.
- Why unresolved: It remains unclear whether this limitation is inherent to the approach or specific to the SimPO-it comparison, where changes may operate in a "narrow behavioral subspace."
- What evidence would resolve it: Systematic comparison of crosscoder sensitivity across model pairs with varying degrees of similarity, including ablation studies on when direct comparison suffices.

### Open Question 3
- Question: To what extent do crosscoder training data composition and layer selection affect the latent taxonomy discovered through model diffing?
- Basis in paper: [inferred] The authors note that low-resource language regressions (Japanese, Korean) "was not captured by the latents possibly due to the lack of such data in the crosscoder training data," and they arbitrarily "selected layer 20 for analysis" following prior work.
- Why unresolved: The methodology does not address sensitivity to these design choices or whether different corpora or layers would surface different capability categories.
- What evidence would resolve it: Cross-validation experiments using different training corpora compositions and comparisons across multiple layers to assess taxonomy stability.

## Limitations
- Crosscoder approach assumes decoder norm differences directly reflect model-specific representational emphasis without theoretical grounding
- LLM-based annotation introduces subjectivity that isn't validated against human annotators
- Analysis focuses on a single model family (Gemma) with one type of fine-tuning (SimPO), limiting generalizability

## Confidence
- **High confidence**: The methodological framework for crosscoder training and latent scaling is technically sound and reproducible. The finding that direct it-SimPO comparison yields no interpretable differences while base-comparison reveals clear distinctions is robust.
- **Medium confidence**: The taxonomy of 30 categories captures meaningful capability differences, but LLM annotation introduces uncertainty. The correlation between latent activation and downstream behavior is plausible but indirect.
- **Low confidence**: The interpretation of specific capability shifts (e.g., exact percentage changes in safety or hallucination detection) depends heavily on the quality of LLM annotations and assumes linear relationships between latent representations and task performance.

## Next Checks
1. **Human validation of LLM annotations**: Have human annotators categorize a random sample of 100 high-activation documents and compare agreement rates with Claude 3 Opus to quantify annotation reliability.

2. **Cross-model generalization**: Apply the same crosscoder methodology to a different model pair (e.g., Llama-3-8B vs Llama-3-8B-Instruct) to test whether the base-comparison strategy consistently reveals interpretable latents.

3. **Layer sensitivity analysis**: Repeat the crosscoder analysis at layers 10, 20, and 30 of the same model pair to determine whether behavioral differences manifest at consistent representational depths or vary by capability domain.