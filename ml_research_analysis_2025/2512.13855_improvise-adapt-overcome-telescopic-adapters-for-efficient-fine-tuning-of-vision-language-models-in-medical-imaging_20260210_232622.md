---
ver: rpa2
title: Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning
  of Vision Language Models in Medical Imaging
arxiv_id: '2512.13855'
source_url: https://arxiv.org/abs/2512.13855
tags:
- adapter
- vision
- layers
- segmentation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently adapting vision
  language segmentation models to medical imaging domains, where conventional fine-tuning
  approaches incur significant computational overhead. The authors propose Telescopic
  Adapters, a novel parameter-efficient fine-tuning framework that employs depth-aware
  scaling to progressively increase adapter capacity from shallow to deep transformer
  layers.
---

# Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging

## Quick Facts
- arXiv ID: 2512.13855
- Source URL: https://arxiv.org/abs/2512.13855
- Reference count: 40
- Achieves superior medical image segmentation performance using 613k trainable parameters (244x fewer than E2E fine-tuning)

## Executive Summary
This paper addresses the challenge of efficiently adapting vision language segmentation models to medical imaging domains, where conventional fine-tuning approaches incur significant computational overhead. The authors propose Telescopic Adapters, a novel parameter-efficient fine-tuning framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. By integrating lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance, the method achieves superior performance using only 613k trainable parameters—244x fewer than end-to-end fine-tuning. Comprehensive experiments on five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating the telescopic scaling hypothesis. The approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.

## Method Summary
The Telescopic Adapters framework modifies CLIPSeg by inserting bottleneck adapter modules into specific transformer layers. Vision adapters span layers 1-9 with dimensions scaling from 8 to 32, text adapters cover layers 7-9 with fixed dimension 16, and a conditional adapter handles the joint embedding with dimension 8. All adapters use SiLU activation, LayerNorm, dropout (0.1), and learnable residual scaling (α=0.1). The adapters are inserted before residual connections, and a lightweight 2-layer CNN (48 parameters) refines decoder outputs. Training uses AdamW (LR 1e-3, weight decay 1e-3), batch size 32, FP16, with a hybrid Dice+BCE loss. The framework maintains 99.6% of the pretrained model frozen while achieving competitive segmentation performance across five medical imaging datasets.

## Key Results
- Achieves state-of-the-art parameter efficiency: 613k trainable parameters vs. 150M for E2E fine-tuning (244x reduction)
- Demonstrates superior performance across five diverse medical datasets including polyp segmentation (Kvasir-SEG, ClinicDB, BKAI), skin lesions (ISIC-16), and breast ultrasound (BUSI)
- Validates depth-aware scaling hypothesis: deeper transformer layers require substantially more adaptation capacity than shallow layers
- Shows 2.3-4.2% absolute improvement in Dice Similarity Coefficient over zero-shot CLIPSeg across medical datasets

## Why This Works (Mechanism)

### Mechanism 1: Depth-Aware Capacity Scaling
The framework treats the transformer encoder as a hierarchical processor, assigning minimal adaptation capacity to shallow layers (encoding low-level features) and progressively larger capacity to deeper layers (encoding task-specific semantics). This prevents redundancy while ensuring sufficient expressivity where it matters most, based on the assumption that semantic richness correlates with layer depth in Vision Language Models.

### Mechanism 2: Modality-Selective Insertion Strategy
Adapters are placed strategically rather than saturating the network. Vision adapters span layers 1-9 to feed decoder intermediate features, while text adapters are restricted to final 3 layers where semantic embeddings are richest. This maximizes alignment efficiency by assuming the text encoder is already robust, requiring adaptation only for semantic layers when handling medical terminology.

### Mechanism 3: Learnable Residual Scaling (α)
Each adapter output is multiplied by a learnable scalar α (initialized to 0.1) before addition to the residual stream. This acts as a gate that stabilizes early training by preventing untrained adapter weights from destabilizing pretrained features. As training progresses, the model learns to increase α for layers needing significant adaptation, preserving valuable pretrained knowledge while allowing controlled perturbation.

## Foundational Learning

**Concept: Parameter-Efficient Fine-Tuning (PEFT)**
*Why needed:* The paper positions itself against end-to-end fine-tuning (150M parameters). Understanding PEFT methods like LoRA or Adapters is crucial to grasp why updating only 600k parameters is more efficient.
*Quick check:* What is the primary computational bottleneck reduced by PEFT methods? (Answer: Activation memory/gradients for the backbone)

**Concept: Vision Language Segmentation Models (VLSMs)**
*Why needed:* The method modifies CLIPSeg, which takes Image + Text Prompt and outputs a Mask, unlike standard classification CLIP.
*Quick check:* Does CLIPSeg require a labeled mask during text-prompting phase, or does it rely on text-image alignment? (Answer: It relies on text-image alignment to generate segmentation)

**Concept: Bottleneck Adapters**
*Why needed:* The "Telescopic" design builds on standard bottleneck adapters with $d \to d' \to d$ projection structure.
*Quick check:* In a bottleneck adapter, which dimension controls the number of trainable parameters: the input dimension $d$ or the intermediate dimension $d'$? (Answer: The intermediate dimension $d'$)

## Architecture Onboarding

**Component map:**
Input Image → Patch Embed → [ViT Layer 1..9 + Adapter] → Intermediate Features → CLIPSeg Decoder → CNN Head → Mask

**Critical path:**
The vision adapter chain (18 modules) feeds the decoder's skip connections, the text adapter chain (6 modules) enriches semantic embeddings, and the conditional adapter (1 module) aligns joint embeddings. The 2-layer CNN head provides final refinement.

**Design tradeoffs:**
- Telescopic vs. Uniform: Complexity vs. parameter efficiency—the paper argues uniform adapters waste parameters on shallow layers
- Placement Depth: Adapters in layers 1-9 match decoder skip connections; deeper placement might miss critical features
- Modality Focus: Vision adapters get broader coverage while text adapters are limited to semantic layers

**Failure signatures:**
- Gradient explosion if base dimension $d_{base}$ exceeds 64 for telescopic scaling
- Performance drops on BUSI dataset when adding text/conditional adapters (ablation shows vision-only sometimes superior)
- Cold start issues if downstream task is extremely distinct from pretraining data

**First 3 experiments:**
1. **Sanity Check (Zero-Shot):** Run frozen CLIPSeg on medical dataset to establish baseline (Expected: Low DSC, ~23-53%)
2. **Ablation (Vision Only):** Train only Vision Telescopic Adapters as most robust baseline before adding text complexity
3. **Scaling Validation:** Compare Telescopic (dim increasing 8→32) vs. Uniform (dim fixed at mean) on validation set to verify depth-hypothesis

## Open Questions the Paper Calls Out
None

## Limitations
- The depth-aware scaling hypothesis lacks systematic comparative evidence against uniform adapter baselines within the same experimental framework
- The modality-selective insertion strategy (freezing early text layers) may not generalize to medical tasks requiring complex textual understanding
- The learnable residual scaling mechanism lacks empirical validation showing its necessity versus simpler initialization schemes

## Confidence

**High Confidence:** Parameter efficiency claims (613k vs. 150M E2E) are well-supported by architecture specification
**Medium Confidence:** Performance improvements across five medical datasets are demonstrated, though baseline comparisons against other PEFT methods are limited
**Low Confidence:** Depth-scaling hypothesis is theoretically justified but only partially validated through ablation studies rather than systematic scaling experiments

## Next Checks

1. **Scaling Ablation:** Systematically compare Telescopic Adapters against uniform adapter configurations with identical total parameter counts to validate the depth-scaling hypothesis
2. **Text Layer Sensitivity:** Evaluate performance when unfreezing early text layers on medical datasets with complex clinical report understanding to test the modality-selective insertion assumption
3. **Cross-Domain Transfer:** Test the approach on non-medical vision-language segmentation tasks to assess whether the depth-scaling pattern generalizes beyond medical imaging