---
ver: rpa2
title: Human-Allied Relational Reinforcement Learning
arxiv_id: '2510.16188'
source_url: https://arxiv.org/abs/2510.16188
tags:
- learning
- advice
- policy
- reinforcement
- relational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning interpretable, generalizable
  policies in reinforcement learning (RL) by combining relational RL (RRL) with object-centric
  representations and active human advice. The authors propose a novel framework,
  RAEL (Relational Active Advice Elicitation), which enables learning symbolic policies
  in both structured and unstructured domains by actively querying a human expert
  when policy uncertainty is high.
---

# Human-Allied Relational Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.16188
- Source URL: https://arxiv.org/abs/2510.16188
- Reference count: 4
- One-line primary result: RAEL combines relational RL with object-centric representations and active human advice to learn interpretable policies in both structured and unstructured domains.

## Executive Summary
This paper introduces RAEL (Relational Active Advice Elicitation), a framework that learns interpretable, generalizable policies in reinforcement learning by combining relational RL with object-centric representations and active human advice. RAEL addresses the challenge of learning from both structured and unstructured data by mapping raw observations to symbolic states and using relational abstractions to generalize across object configurations. The framework actively queries human experts when policy uncertainty is high, leading to improved sample efficiency and policy fidelity compared to baseline approaches.

## Method Summary
RAEL integrates object-centric representation learning to convert raw observations into symbolic states, uses Relational Fitted-Q learning to induce interpretable policies, and incorporates expert advice through a probabilistic policy reuse mechanism. The framework operates by extracting symbolic representations from raw inputs using OC-Atari, training a relational Q-function via gradient-boosted regression trees, computing policy entropy to identify high-uncertainty states, and actively querying human experts for guidance. Advised actions are incorporated through Q-value boosting while maintaining exploration via probabilistic policy reuse.

## Key Results
- RAEL achieves higher cumulative rewards and improved sample efficiency compared to baselines like Neural PPO, NUDGE, and BlendRL in simplified Atari games and Blocks World
- Active advice elicitation, especially when paired with relational abstractions, leads to faster convergence and better policy fidelity
- The framework successfully handles both structured domains (Blocks World) and unstructured domains (simplified Atari games) through unified symbolic representation

## Why This Works (Mechanism)

### Mechanism 1: Relational Abstraction Enables Cross-State Generalization
Symbolic representations allow policies learned on one object configuration to transfer to unseen configurations through lifted rules that generalize via universal/existential quantification rather than memorizing specific pixel patterns.

### Mechanism 2: Entropy-Based Active Querying Targets High-Value Human Input
Querying experts only at high-uncertainty states under a fixed budget improves sample efficiency and policy fidelity compared to passive or uniform advice by focusing human effort where the agent's policy is most confused.

### Mechanism 3: Probabilistic Policy Reuse Softens Expert Constraints
Treating advice as soft constraints via probabilistic policy reuse with Q-value boosting preserves exploration while biasing toward expert-endorsed actions, preventing over-constraining while still incorporating guidance.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) and Q-Learning**
  - Why needed here: RAEL builds on Fitted Q-learning, which approximates Q* via iterative Bellman updates and supervised regression.
  - Quick check question: Can you explain why Q-learning's sample inefficiency in large state spaces motivates fitted-function approximation?

- **Concept: First-Order Logic / Relational Representations**
  - Why needed here: The core innovation lifts states from propositional feature vectors to sets of ground predicates.
  - Quick check question: Given a state `on(a,b) ∧ on(b,c)`, what does a lifted rule `on(X,Y) → stack(X)` generalize to?

- **Concept: Policy Uncertainty / Entropy**
  - Why needed here: Active advice selection hinges on computing H(s) = -Σ_a π(a|s) log π(a|s).
  - Quick check question: If π(a|s) = [0.5, 0.5] vs. [0.99, 0.01], which state has higher entropy and would be prioritized for advice?

## Architecture Onboarding

- **Component map:** Raw observation → OC-Atari → symbolic state → Relational Fitted Q-Learning → Q-values → Probabilistic Policy Reuse → action selection → environment → transitions → RFQ update

- **Critical path:** Environment emits raw observation x or symbolic z. If raw, OC-Atari extracts z = f(x). Agent selects action via PPR, consulting advice memory if z matches stored abstraction. Execute action, store transition, update Q via RFQ with advice boosting. Periodically run evaluation rollouts, compute entropy, query expert on highest-entropy state if budget remains. Store new advice; repeat.

- **Design tradeoffs:**
  - Symbolic abstraction level: Richer predicates capture more detail but increase grounding complexity and risk overfitting
  - Advice budget (B): Larger B improves coverage but increases human burden; experiments use B=3–5
  - Q-value margin (δ): Larger δ enforces stronger advice adherence but risks exploit loops
  - Decay schedules for p_ε(t) and ρ(t): Fast decay accelerates convergence but may premature-commit

- **Failure signatures:**
  - Exploit loops: Agent repeatedly takes advised action to harvest boosted Q-values
  - Advice misalignment: Expert provides abstraction that over-generalizes, causing advice to fire inappropriately
  - Object-centric extraction failures: OC-Atari misses or mislabels objects, producing incorrect symbolic states
  - Budget exhaustion before convergence: Entropy remains high but B=0, leaving agent stuck

- **First 3 experiments:**
  1. Ablation on OC-Atari vs. ground-truth symbolic input: Run RAEL on Blocks World with perfect symbolic states vs. extracted states to quantify extraction error impact
  2. Sensitivity sweep on δ and ρ(t): In simplified Kangaroo, test different δ values and ρ(t) decay rates to identify exploit loop avoidance settings
  3. Active vs. random advice querying: Compare entropy-based state selection for advice vs. uniform random selection under same budget to measure active querying's advantage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the advice budget be systematically quantified rather than treated as a fixed hyperparameter?
- **Basis in paper:** [explicit] The authors state that quantifying this parameter systematically is an interesting direction for future research after noting they treated the budget as a fixed hyperparameter during experiments.
- **Why unresolved:** The current implementation relies on manual selection of budget values based on perceived domain complexity.
- **What evidence would resolve it:** An adaptive allocation algorithm or theoretical bounds linking budget size to sample efficiency and convergence rates.

### Open Question 2
- **Question:** Can RAEL effectively scale to complex, high-threat environments where rapid decision-making is required?
- **Basis in paper:** [inferred] The experiments utilized simplified environments and the text notes that relying on logical reasoning in high-risk or time-critical situations is often impractical.
- **Why unresolved:** The evaluation focused on simplified setups to facilitate interpretation, leaving the method's robustness in unmodified, chaotic, or high-dimensional state spaces unproven.
- **What evidence would resolve it:** Benchmarking results on standard, unmodified Atari environments or robotics tasks involving dynamic hazards and strict time constraints.

### Open Question 3
- **Question:** How can the framework model adviser reliability to adaptively weight input when advice is imperfect?
- **Basis in paper:** [explicit] The conclusion identifies mechanisms for modeling adviser reliability, adaptively weighting input as a specific avenue for future work.
- **Why unresolved:** The current probabilistic policy reuse mechanism accepts advice as a soft constraint but lacks a mechanism to discount or reject low-quality or noisy guidance over time.
- **What evidence would resolve it:** Experiments demonstrating successful performance recovery or maintenance when the expert provides a significant portion of incorrect or misleading advice.

## Limitations

- Scalability concerns with object-centric extraction in visually complex domains and brittleness of relational abstractions when perceptual aliasing occurs
- Empirical scope limited to simplified environments; transfer to full-scale Atari or real-world tasks is unproven
- Advice mechanism assumes expert reliability and bounded error, but no robustness analysis is provided for noisy or conflicting advice

## Confidence

- **Confidence in core claims: Medium**
  - The ablation studies and active vs. passive advice comparisons provide supportive evidence
  - Lack of ablation on the relational abstraction itself leaves the specific contribution of lifted rules unclear
  - The exploit-loop failure mode is acknowledged but not quantitatively measured across experiments

## Next Checks

1. **Ablation on abstraction:** Run RAEL with propositional vs. relational Q-learning on ground-truth symbolic states to isolate the benefit of lifted rules
2. **Extraction error sensitivity:** Systematically degrade OC-Atari's precision and measure impact on cumulative reward and policy fidelity
3. **Advice budget scaling:** Vary B from 1 to 10 and plot learning curves to identify saturation points and diminishing returns