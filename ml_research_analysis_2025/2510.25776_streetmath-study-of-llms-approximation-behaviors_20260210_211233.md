---
ver: rpa2
title: 'StreetMath: Study of LLMs'' Approximation Behaviors'
arxiv_id: '2510.25776'
source_url: https://arxiv.org/abs/2510.25776
tags:
- reasoning
- approximation
- arxiv
- language
- exact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the gap in evaluating large language models''
  (LLMs) ability to perform approximate reasoning in real-world, everyday mathematical
  scenarios, termed "street math." The authors introduce StreetMath, a benchmark of
  1,000 multiple-choice approximation problems across five topics (basket sum, discounts,
  taxes, units, tips), designed to test models'' ability to estimate rather than calculate
  exact answers. They evaluate five diverse LLM architectures: Qwen3-4B-Instruct-2507,
  Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and Mamba-GPT-3B.'
---

# StreetMath: Study of LLMs' Approximation Behaviors

## Quick Facts
- arXiv ID: 2510.25776
- Source URL: https://arxiv.org/abs/2510.25776
- Authors: Chiung-Yi Tseng; Somshubhra Roy; Maisha Thasin; Danyang Zhang; Blessing Effiong
- Reference count: 40
- Primary result: All tested LLMs overwhelmingly default to exact computation even when approximation is explicitly requested, with only 15-14% achieving good approximations within 20% relative error.

## Executive Summary
This paper introduces StreetMath, a benchmark of 1,000 multiple-choice approximation problems designed to evaluate LLMs' ability to perform real-world "street math" - everyday mathematical reasoning that requires estimation rather than exact calculation. The authors test five diverse 3-7B parameter models across five mathematical domains including basket sums, discounts, taxes, units, and tips. The key finding is that despite their strong arithmetic capabilities, all models struggle to switch from exact computation to approximation when explicitly asked, contradicting the human cognitive principle of being "cognitive misers." The study reveals that while models can encode numerical proximity similarly to humans, they lack the contextual flexibility to modulate computational effort based on task requirements.

## Method Summary
The authors created StreetMath, a benchmark of 1,000 multiple-choice approximation problems across five topics: basket sum, discounts, taxes, units, and tips. Five LLM architectures were evaluated: Qwen3-4B-Instruct-2507, Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and Mamba-GPT-3B. Models were prompted with explicit approximation instructions and evaluated based on relative error thresholds (good: <20%, bad: >100%). Additional analyses included linear probing of numerical representations, pruning experiments to identify math-specific parameters, and token usage comparisons. The study specifically measured how often models chose approximation versus exact computation, their success rates at approximation, and the computational cost (tokens) of different approaches.

## Key Results
- All five tested models overwhelmingly preferred exact computation even when explicitly asked to approximate, demonstrating a fundamental mismatch between task requirements and model behavior.
- Qwen3-4B-Thinking-2507 achieved better approximation rates (15% good approximations) but at significantly higher token costs (228 vs 125 tokens), contradicting human cognitive miserliness.
- Linear probing revealed models encode numerical proximity similarly to humans but struggle specifically with word-based numbers versus digit-based numbers.
- Pruning experiments showed that removing math-specific parameters could improve approximation performance, suggesting rigid precision-oriented circuits hinder flexible estimation.

## Why This Works (Mechanism)
The paper doesn't explicitly detail a mechanism for why this works, as the focus is on identifying limitations in current LLMs' approximation abilities rather than explaining their success. The core finding is that LLMs lack the cognitive flexibility humans possess to modulate computational effort based on context - they are "cognitive spendthrifts" rather than "cognitive misers."

## Foundational Learning
**Cognitive Miserliness**: The human tendency to minimize cognitive effort by using heuristics and approximations when exact computation isn't necessary. Why needed: Provides the theoretical framework for evaluating LLMs' approximation behaviors and highlights the gap between human and machine reasoning strategies. Quick check: Compare human and LLM responses to the same approximation tasks to quantify the difference in cognitive effort.

**Relative Error Metrics**: Statistical measures using absolute relative error to evaluate approximation quality, with thresholds defining good (<20%), fair (20-100%), and bad (>100%) approximations. Why needed: Provides objective criteria for evaluating approximation quality across different numerical ranges and problem types. Quick check: Validate that the 20% threshold aligns with human perception of "good enough" estimates in everyday contexts.

**Linear Probing**: A technique for analyzing how models encode numerical information by training linear classifiers on intermediate representations. Why needed: Reveals whether models develop human-like numerical representations that could support approximation behaviors. Quick check: Compare model representations of digit-based versus word-based numbers to identify specific weaknesses.

## Architecture Onboarding

**Component Map**: Input Prompt -> Token Generation -> Numerical Computation Circuit -> Output Selection
- Models process approximation prompts through standard transformer or Mamba architectures
- Numerical computation circuits are activated for arithmetic operations
- Output selection involves choosing between multiple-choice options

**Critical Path**: Prompt encoding → Attention mechanism → Numerical computation → Output generation → Approximation decision
- The approximation decision point is critical - models must decide whether to compute exactly or estimate
- Current models consistently choose exact computation regardless of explicit approximation instructions

**Design Tradeoffs**: Precision-oriented training versus flexibility for approximation
- Models are trained to prioritize exact answers, creating strong optimization pressure toward precision
- This training creates rigid circuits that are difficult to override even with explicit approximation prompts

**Failure Signatures**: Consistent preference for exact computation, poor performance on word-based numerical problems, high token usage when attempting approximation
- Models fail to recognize when approximation is more appropriate than exact calculation
- Word-based numbers create specific difficulties in numerical representation and processing

**First Experiments**:
1. Test whether providing explicit incentives (cost functions) for approximation improves performance
2. Evaluate larger frontier models to determine if scale affects approximation ability
3. Compare performance across different architectural families (RNNs, Mamba, Transformers) to identify architectural influences

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The benchmark's focus on multiple-choice problems may not capture the full complexity of real-world street math scenarios that often require open-ended responses
- Results are based on only five model architectures in the 3-7B parameter range, limiting generalizability to larger frontier models
- The 20% relative error threshold for "good approximation" is somewhat arbitrary and may not align with human expectations across all domains
- The evaluation treats all approximation failures uniformly without distinguishing between cases where exact computation is actually preferable versus where approximation would be more appropriate

## Confidence

**High confidence**: LLMs overwhelmingly default to exact computation even when approximation is explicitly requested, supported by consistent results across multiple model families.

**Medium confidence**: Qwen3-4B-Thinking-2507 achieves better approximation at higher cost, though interpretation requires caution given the small sample size and specific prompting strategies.

**Medium confidence**: Linear probing and pruning results revealing numerical representation patterns and parameter sensitivity are methodologically sound but may not capture the full complexity of approximation behavior.

## Next Checks

1. Replicate StreetMath benchmark with larger frontier models (GPT-4, Claude) and different architectural families to test whether approximation behavior scales with model size or is architecture-dependent.

2. Conduct ablation studies varying prompt phrasing, context, and incentives to determine whether current approximation failures stem from training data bias versus fundamental architectural limitations.

3. Expand the benchmark to include open-ended approximation tasks with human evaluation of answer quality, capturing the contextual nuance and judgment calls that characterize real-world street math scenarios.