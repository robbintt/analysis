---
ver: rpa2
title: Benchmarking Music Generation Models and Metrics via Human Preference Studies
arxiv_id: '2506.19085'
source_url: https://arxiv.org/abs/2506.19085
tags:
- music
- human
- audio
- metrics
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks music generation models and evaluation metrics
  using human preference studies. The authors generate 6,000 songs with 12 state-of-the-art
  models and conduct a large-scale survey with 2,500 participants performing 15,600
  pairwise audio comparisons to evaluate both music preference and text-audio alignment.
---

# Benchmarking Music Generation Models and Metrics via Human Preference Studies

## Quick Facts
- **arXiv ID**: 2506.19085
- **Source URL**: https://arxiv.org/abs/2506.19085
- **Reference count**: 31
- **Primary result**: Human preference studies show Suno v3.5 and Udio outperform other models; FAD-CLAP-MA best correlates with music quality judgments, LAION-MA best captures text-audio alignment

## Executive Summary
This paper presents the first comprehensive human-based benchmarking of music generation models and evaluation metrics. The authors generated 6,000 songs using 12 state-of-the-art models and conducted a large-scale survey with 2,500 participants performing 15,600 pairwise audio comparisons. The study evaluates both music preference and text-audio alignment through binary comparisons, aggregating results via Elo ratings and Bradley-Terry models. Human evaluations were compared against objective metrics including FAD (with various embedding models) and CLAP-based metrics. The results establish Suno v3.5 and Udio as the highest-rated commercial models, outperforming the MTG-Jamendo reference dataset, and identify specific metric variants that best align with human judgments.

## Method Summary
The study generated 6,000 ten-second instrumental clips using 12 models (500 tracks per model) from 500 diverse tag-based prompts derived from MTG-Jamendo. Human evaluation was conducted via Prolific with 2,500+ participants performing pairwise binary comparisons across 15,600 comparisons. Results were aggregated using bootstrapped Elo ratings (K=8, base=1000, 10k shuffles) and Bradley-Terry parameters. Objective metrics included FAD variants computed with different embeddings (VGGish, PANN, CLAP-Audio, CLAP-MA, EnCodec) and CLAP-based text-audio alignment metrics (MS-CLAP, LAION checkpoints). Pearson and Spearman correlations were computed between human-derived scores and objective metric scores.

## Key Results
- Suno v3.5 and Udio achieved the highest human preference ratings, outperforming the MTG-Jamendo reference dataset
- FAD-CLAP-MA showed the strongest correlation with human music quality judgments across both Pearson and Spearman metrics
- LAION-MA (music-trained CLAP) best captured human text-audio alignment preferences
- Commercial models consistently outperformed open-source alternatives in human evaluations
- The Bradley-Terry model provided stable parameter estimates that aligned well with human rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FAD computed with music-trained CLAP embeddings (FAD-CLAP-MA) correlates more strongly with human music quality judgments than FAD with generic audio embeddings
- Mechanism: Music-specialized CLAP embeddings capture perceptually relevant musical features that align better with human aesthetic judgments than generic audio classification embeddings
- Core assumption: The reference dataset represents a distribution that human "quality" judgments converge toward
- Evidence anchors: [abstract], [section V], weak corpus validation

### Mechanism 2
- Claim: Mean cosine similarity between audio and text embeddings from music-trained CLAP models correlates with human text-audio alignment judgments
- Mechanism: CLAP creates shared embedding space where semantically related text and audio are close; music-trained checkpoints capture musical semantics
- Core assumption: Higher cosine similarity reflects semantic alignment that humans would recognize and prefer
- Evidence anchors: [abstract], [section V], weak corpus validation

### Mechanism 3
- Claim: Pairwise binary comparisons aggregated via Elo ratings and Bradley-Terry models yield reliable model rankings for subjective music evaluation
- Mechanism: Binary choices reduce cognitive load and enable robust ranking through many comparisons using Elo/Bradley-Terry aggregation
- Core assumption: Human preferences are sufficiently consistent across participants that aggregated pairwise choices reveal meaningful quality differences
- Evidence anchors: [section IV.A], [section IV.B], neighbor paper convergence

## Foundational Learning

- Concept: **Fréchet Audio Distance (FAD)**
  - Why needed here: Core metric for evaluating generated music quality; measures distributional distance between generated and reference audio embeddings
  - Quick check question: Why would FAD-CLAP-MA outperform FAD-VGGish for music quality evaluation?

- Concept: **Contrastive Language-Audio Pretraining (CLAP)**
  - Why needed here: Provides embedding models for both FAD computation and text-audio alignment measurement; creates shared embedding space
  - Quick check question: What does cosine similarity between a text embedding and audio embedding represent in a CLAP model?

- Concept: **Elo Rating System and Bradley-Terry Models**
  - Why needed here: Statistical methods for converting pairwise comparison results into ranked scores
  - Quick check question: Why might pairwise comparisons be more reliable than asking participants to rate audio quality on a 1-5 scale?

## Architecture Onboarding

- Component map:
  MTG-Jamendo tag filtering -> tag combination selection (length 3) -> CLAP similarity deduplication (threshold 0.1382) -> 500 unique prompts
  -> 12 models (Suno v3/v3.5, Udio, MusicGen variants, Riffusion, AudioLDM 2, Mustango, Stable Audio) -> 500 tracks each -> 10-second clips
  -> Prolific platform -> pairwise comparisons (binary choice) -> 15,600 comparisons from 2,500 participants -> Elo/Bradley-Terry aggregation
  -> FAD variants (VGGish, PANN, CLAP-Audio, CLAP-MA, EnCodec) + CLAP similarity variants (MS-CLAP, LAION variants)
  -> Correlation computation (Pearson, Spearman) between objective metrics and human-derived scores

- Critical path:
  1. Tag selection and prompt generation (determines what's being tested)
  2. Music generation across all models (requires API access for commercial models)
  3. Human survey deployment (participant recruitment, attention checks)
  4. Metric computation (requires reference dataset and embedding models)
  5. Correlation analysis (Bradley-Terry parameters ↔ metric scores)

- Design tradeoffs:
  - 10-second clips: Controls for model differences in optimal generation length, but may miss longer-form musical structure
  - Binary pairwise vs. Likert scale: Reduces cognitive load and enables Elo/Bradley-Terry analysis, but loses granularity within comparisons
  - Tag-based prompts vs. natural language: Standardizes evaluation, but may not reflect real-world prompt diversity
  - Commercial model inclusion: Provides SOTA comparison, but API dependence limits reproducibility

- Failure signatures:
  - Low metric-human correlation: Metric not capturing perceptually relevant features; try different embedding models
  - High variance in Elo ratings after bootstrapping: Insufficient comparison coverage; increase sample size
  - Attention check failures: Participant quality issues; tighten Prolific filters
  - All models cluster near reference dataset: Task too easy or prompts too generic; increase prompt diversity or difficulty
  - Metric rankings contradict human rankings entirely: Metric fundamentally misaligned; verify computation pipeline

- First 3 experiments:
  1. Metric validation on subset: Select 50 prompts, generate with 3 models, compute FAD-CLAP-MA and LAION-MA similarity
  2. Pilot human study: Deploy 100 pairwise comparisons on Prolific with attention checks
  3. Embedding model ablation: For fixed generated audio, compare FAD scores across all embedding variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the established model rankings and metric correlations generalize to full-length songs containing lyrics?
- Basis in paper: [explicit] The authors limited the generated track duration to "10 seconds and instrumental versions only"
- Why unresolved: The study excludes the complexities of long-term structure and lyrical content
- What evidence would resolve it: A replication of the benchmark using models capable of generating full songs with vocals

### Open Question 2
- Question: How robust are the human preference rankings across different demographic groups?
- Basis in paper: [explicit] The survey pre-filtered participants to "fluent English speakers within the age range of 18 to 34 years"
- Why unresolved: Musical taste is culturally and generationally dependent
- What evidence would resolve it: Re-running the pairwise comparison survey with stratified sampling across diverse age groups and cultural backgrounds

### Open Question 3
- Question: Can new objective metrics be developed that outperform FAD-CLAP-MA in correlating with human music preference?
- Basis in paper: [inferred] The paper states that "translating these subjective judgments into reliable objective metrics... has proven difficult"
- Why unresolved: While FAD-CLAP-MA showed the best correlation, the relationship is not perfect
- What evidence would resolve it: A proposed metric demonstrating a statistically significant increase in correlation coefficients

## Limitations

- The study relies on MTG-Jamendo reference dataset representativeness and CLAP embeddings' ability to capture musically relevant features
- Commercial models were evaluated using API calls rather than reproducible open implementations, limiting experimental control
- The 10-second clip duration may not adequately represent full-track quality or long-term musical structure
- Human evaluations were limited to a specific demographic (18-34 year old English speakers), potentially limiting generalizability

## Confidence

- **High Confidence**: Human preference rankings placing Suno v3.5 and Udio at the top; FAD-CLAP-MA correlation with music quality judgments; Bradley-Terry model methodology
- **Medium Confidence**: LAION-MA capturing text-audio alignment preferences; Elo rating stability across bootstraps; CLAP similarity metrics for alignment evaluation
- **Low Confidence**: Extrapolation of metric correlations to entirely different musical domains; assumption that 10-second clips adequately represent full-track quality; generalizability across different participant demographics

## Next Checks

1. **Ablation study on embedding models**: Systematically compare FAD scores across all embedding variants (VGGish, PANN, CLAP-Audio, CLAP-MA, EnCodec) for a fixed set of generated audio to verify the music-trained CLAP advantage is robust

2. **Prompt diversity stress test**: Generate additional prompts covering underrepresented musical styles and semantic descriptions not well-represented in the original 500-tag set. Re-run correlation analysis to test whether metric-human agreement holds for novel musical concepts

3. **Time-scale generalization**: Extend the evaluation to 30-second and 60-second clips for a subset of models and prompts. Assess whether the relative model rankings and metric correlations remain stable as temporal context increases