---
ver: rpa2
title: Native Logical and Hierarchical Representations with Subspace Embeddings
arxiv_id: '2508.16687'
source_url: https://arxiv.org/abs/2508.16687
tags:
- subspace
- embeddings
- learning
- entailment
- logical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes representing concepts as linear subspaces rather
  than single vectors, enabling inherent modeling of generality (via subspace dimension),
  hierarchy (via subspace inclusion), and logical operations (conjunction as intersection,
  disjunction as linear sum, negation as orthogonal complement). To learn these adaptive-rank
  subspaces, the authors introduce a smooth relaxation of orthogonal projection operators
  using Tikhonov regularization, enabling differentiable end-to-end training.
---

# Native Logical and Hierarchical Representations with Subspace Embeddings

## Quick Facts
- arXiv ID: 2508.16687
- Source URL: https://arxiv.org/abs/2508.16687
- Reference count: 8
- Key outcome: This paper proposes representing concepts as linear subspaces rather than single vectors, enabling inherent modeling of generality (via subspace dimension), hierarchy (via subspace inclusion), and logical operations (conjunction as intersection, disjunction as linear sum, negation as orthogonal complement). To learn these adaptive-rank subspaces, the authors introduce a smooth relaxation of orthogonal projection operators using Tikhonov regularization, enabling differentiable end-to-end training. Evaluated on WordNet reconstruction and link prediction, their subspace embeddings achieve state-of-the-art performance with near-perfect reconstruction (mAP 98.6, MR 1.04) and significantly outperform baselines in link prediction (F1-score up to 53.4%). On HyperLex, they show stronger correlation with human lexical entailment judgments (Spearman's ρ up to 0.73). Applied to SNLI, the method surpasses bi-encoder baselines in textual entailment while providing interpretable geometric reasoning through subspace inclusion. The approach bridges neural representations with formal logical reasoning.

## Executive Summary
This paper introduces a novel approach to semantic representation by mapping concepts to linear subspaces rather than vectors, enabling native modeling of hierarchy and logical operations through geometric relationships. The key innovation is a differentiable relaxation of orthogonal projection operators using Tikhonov regularization, which allows gradient-based learning of subspaces with adaptive ranks. This approach demonstrates state-of-the-art performance on WordNet reconstruction, link prediction, and natural language inference tasks while providing interpretable geometric reasoning.

## Method Summary
The method represents each concept as a linear subspace defined by a basis matrix X, with the subspace projector computed as P = X(XᵀX + Λ)⁻¹Xᵀ using Tikhonov regularization. The rank of each subspace is determined by the number of basis vectors and can be learned end-to-end through backpropagation. The authors introduce an inclusion scoring mechanism using normalized traces of projector products to capture asymmetric entailment relationships. Training is performed using a margin-based ranking loss that encourages the inclusion score to reflect the ground truth hierarchical relationships in the data.

## Key Results
- WordNet reconstruction achieves near-perfect performance with mAP 98.6 and MR 1.04
- Link prediction outperforms baselines with F1-score up to 53.4% on WordNet
- HyperLex correlation with human judgments reaches Spearman's ρ up to 0.73
- SNLI experiments show superior performance compared to bi-encoder baselines

## Why This Works (Mechanism)

### Mechanism 1: Geometric Inclusion as Entailment
- **Claim:** If concepts are represented as linear subspaces, semantic hierarchy (entailment) appears as geometric containment.
- **Mechanism:** The method maps a concept to the span of vectors X ∈ ℝᵈˣⁿ. A specific concept (e.g., "dog") is learned as a subspace contained within the subspace of a general concept (e.g., "animal"). Operations like conjunction (∩) and negation (⊥) map to intersection and orthogonal complement.
- **Core assumption:** Semantic relationships strictly adhere to the axioms of projective geometry (specifically, the subspace lattice).
- **Evidence anchors:**
  - [Section 3.1]: Defines the lattice order Sⱼ ≤ Sᵢ iff Sⱼ ⊆ Sᵢ.
  - [Figure 1]: Visualizes "man on a boat ∩ is fishing" as the shared subspace x₁.
  - [Corpus]: "Interpreting the linear structure of vision-language model embedding spaces" supports the premise that meaning is encoded linearly, but does not validate the specific subspace logic.
- **Break condition:** Concepts that are semantically related but non-hierarchical (e.g., "red" and "round") may be forced into artificial inclusion relationships to minimize loss.

### Mechanism 2: Tikhonov Regularization for Differentiable Rank
- **Claim:** Adding a positive definite term Λ to the projection operator allows gradient descent to learn both the orientation and the effective dimension (rank) of the subspace.
- **Mechanism:** Standard projection X(XᵀX)⁻¹Xᵀ creates binary eigenvalues (0 or 1), making rank non-differentiable. The Tikhonov relaxation P̃ = X(XᵀX + Λ)⁻¹Xᵀ yields eigenvalues in (0, 1), creating a smooth manifold where effective dimension (trace) can slide continuously.
- **Core assumption:** The "softness" introduced by Λ does not distort the semantic geometry beyond useful approximation.
- **Evidence anchors:**
  - [Section 3.2]: Explicitly defines the smooth projection operator via Eq. (4).
  - [Section 3.2]: States this avoids the "non-smooth stratified manifold" problem.
  - [Corpus]: "Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation" relates to subspace adaptation but does not confirm this specific relaxation technique.
- **Break condition:** If Λ is too large, the projection becomes too "soft," blurring distinct subspaces; if too small, gradient flow vanishes during rank transitions.

### Mechanism 3: Inclusion Scoring for Asymmetric Relations
- **Claim:** Normalized trace operations can quantify asymmetric "strength" of entailment where dot products fail.
- **Mechanism:** Instead of symmetric similarity, the model uses normalized inclusion P(j|i) = Tr(P̃ᵢP̃ⱼ) / Tr(P̃ᵢ). This acts as a Bayesian-like conditional probability, satisfying P(j|i) ≠ P(i|j).
- **Core assumption:** The trace of the product of soft projectors correlates monotonically with human-judged semantic entailment.
- **Evidence anchors:**
  - [Section 3.3]: Defines the normalized inclusion score Eq. (7).
  - [Table 3]: Shows strong Spearman correlation (ρ=0.73) on HyperLex human judgments.
  - [Corpus]: Weak corpus evidence; neighbors focus on representation manifolds rather than entailment scoring mechanics.
- **Break condition:** In NLI, "Neutral" relationships (overlap without inclusion) cannot be resolved by inclusion scores alone, requiring a separate MLP heuristic (Eq. 10).

## Foundational Learning

- **Concept: Orthogonal Projection Matrices**
  - **Why needed here:** This is the fundamental data structure. The entire representation is a projection matrix P, not a vector.
  - **Quick check question:** Can you explain why P² = P defines a projection, and how Pᵀ = P ensures it is orthogonal?

- **Concept: Stratified Manifolds vs. Smooth Manifolds**
  - **Why needed here:** Understanding why the authors need the Tikhonov "trick" requires grasping that the space of subspaces with different dimensions has discontinuities (stratification).
  - **Quick check question:** Why can't you perform standard gradient descent across a change in matrix rank (e.g., from rank 3 to rank 4)?

- **Concept: Tikhonov Regularization (Ridge Regression)**
  - **Why needed here:** The core technical contribution is re-purposing ridge regularization to smooth the spectral gap of projection matrices.
  - **Quick check question:** How does adding a scalar λ to the diagonal of XᵀX guarantee the matrix is invertible and stabilizes the eigenvalues?

## Architecture Onboarding

- **Component map:** Input Encoder -> Subspace Projection Head -> Projector Layer -> Scoring Head
- **Critical path:** The computation of (XᵀX + Λ)⁻¹ is the primary bottleneck. It involves a matrix inverse of size n × n (where n is subspace basis size, e.g., 128), followed by matrix multiplications to form the d × d projector.
- **Design tradeoffs:**
  - **Memory vs. Rank:** The projector P̃ is d × d (e.g., 128 × 128). Increasing ambient dimension d increases memory quadratically, unlike vector embeddings.
  - **Softness vs. Precision:** High Λ aids training stability but reduces the sharpness of the "inclusion," potentially blurring hierarchical distinctions.
- **Failure signatures:**
  - **Dimension Collapse:** All subspaces converging to the same dimension (under-regularization) or rank 1 (over-regularization).
  - **Neutral Confusion:** In NLI, the inclusion score distributions of "Entailment" and "Neutral" overlapping heavily, forcing the auxiliary MLP to dominate predictions.
- **First 3 experiments:**
  1. **Sanity Check (WordNet Reconstruction):** Train on full transitive closure; verify that Tr(P̃) correlates with tree depth (Fig 3a).
  2. **Link Prediction (Generalization):** Hide a percentage of edges; test if the inclusion score P(j|i) recovers them better than Euclidean distance.
  3. **Compositional Retrieval:** Manually construct query subspaces (e.g., S_umbrella ∩ S⊥_raining) and verify nearest neighbors in Flickr30k captions match the logic (Fig 4).

## Open Questions the Paper Calls Out
None

## Limitations
- **Geometric Representation Bias:** The subspace lattice approach may force artificial inclusion relationships for concepts that are semantically related but non-hierarchical, such as synonyms or concepts with partial overlap.
- **Scalability Constraints:** The quadratic memory scaling with ambient dimension (d × d projectors) creates significant computational overhead compared to vector embeddings, potentially limiting deployment in large-scale applications.
- **Hyperparameter Sensitivity:** The Tikhonov regularization parameter Λ and subspace basis size n are critical for performance but not extensively explored, with potential for over-regularization (blurring distinctions) or under-regularization (preventing gradient flow).

## Confidence
- **High Confidence (90%+):** The technical implementation of Tikhonov regularization for smooth subspace learning is sound and mathematically rigorous. The differentiable projection operator is correctly formulated, and the computational framework appears implementable.
- **Medium Confidence (70-89%):** The claim that inclusion scoring better captures asymmetric entailment than dot products is supported by HyperLex results (ρ=0.73), but the mechanism may not generalize to all semantic relationships. The NLI results show improvements over bi-encoders, but the auxiliary MLP for "Neutral" class suggests the geometric approach alone is insufficient for complete reasoning.
- **Low Confidence (50-69%):** The scalability claims are concerning. While the paper demonstrates effectiveness on WordNet and Flickr30k, these datasets are relatively small. The quadratic memory scaling with ambient dimension could severely limit practical deployment in large-scale applications.

## Next Checks
1. **Ablation Study on Tikhonov Parameter:** Systematically vary the regularization parameter Λ across several orders of magnitude (10⁻⁵ to 10⁻¹) and measure performance degradation on WordNet reconstruction and HyperLex correlation. This would quantify the sensitivity and identify the optimal range.

2. **Generalization to Non-Hierarchical Semantics:** Test the model on datasets with significant non-hierarchical relationships (e.g., ConceptNet, visual relationship datasets) to assess whether the subspace approach forces artificial hierarchies or genuinely captures diverse semantic patterns.

3. **Large-Scale Scalability Test:** Implement the model with higher-dimensional embeddings (768-1024) and measure computational overhead, memory usage, and performance retention. Compare against baseline vector approaches to quantify the practical scalability limits.