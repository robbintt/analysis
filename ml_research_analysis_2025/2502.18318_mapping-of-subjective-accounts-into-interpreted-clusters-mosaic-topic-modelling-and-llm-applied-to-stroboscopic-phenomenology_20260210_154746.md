---
ver: rpa2
title: 'Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic Modelling
  and LLM applied to Stroboscopic Phenomenology'
arxiv_id: '2502.18318'
source_url: https://arxiv.org/abs/2502.18318
tags:
- experience
- experiences
- visual
- topic
- subjective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed MOSAIC, a data-driven NLP pipeline combining
  BERTopic and LLM-based labelling to systematically analyse open-text phenomenological
  reports from stroboscopic light stimulation (SLS) experiences. By applying sentence-level
  semantic embeddings and clustering, the method identified distinct experiential
  topics across two Dreamachine conditions (High Sensory and Deep Listening), revealing
  previously undocumented phenomenology including complex visual hallucinations and
  altered states of consciousness.
---

# Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic Modelling and LLM applied to Stroboscopic Phenomenology

## Quick Facts
- arXiv ID: 2502.18318
- Source URL: https://arxiv.org/abs/2502.18318
- Reference count: 40
- Method maps open-text phenomenological reports from stroboscopic light stimulation into interpretable experiential clusters using BERTopic and LLM labelling

## Executive Summary
MOSAIC is a data-driven NLP pipeline that systematically analyses open-text phenomenological reports from stroboscopic light stimulation experiences. By combining sentence-level semantic embeddings, density-based clustering, and LLM-based labelling, the method identifies distinct experiential topics across different stimulation conditions without requiring predefined categories. The approach reveals previously undocumented phenomenology including complex visual hallucinations and altered states of consciousness, validated by topic coherence scores of 0.56-0.57. The open-source implementation enables comprehensive exploration of rich subjective experience data across diverse research domains.

## Method Summary
The MOSAIC pipeline processes free-text phenomenological reports through sentence-level tokenization, transformer-based embeddings (all-mpnet-base-v2), UMAP dimensionality reduction, and HDBSCAN clustering to extract experiential topics. c-TF-IDF identifies topic-representative keywords, while Llama-3-8B-Instruct generates automatic topic labels. The method was validated on 862 sentences from 422 Dreamachine participants across High Sensory and Deep Listening conditions, optimizing hyperparameters through grid search and evaluating topic quality using coherence metrics.

## Key Results
- Identified 14 topics in High Sensory condition (coherence = 0.56) and 8 topics in Deep Listening condition (coherence = 0.57)
- Topic coherence scores fell within acceptable range (0.4-0.7) for topic modelling studies
- Revealed distinct experiential domains including complex visual hallucinations, altered consciousness states, and abstract phenomena
- Demonstrated systematic analysis of subjective experience without predefined categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Context-aware embeddings capture phenomenological nuance that word co-occurrence methods miss.
- **Mechanism:** Transformer-based sentence embeddings encode semantic meaning based on surrounding context, allowing "time" in "distortion of time" to be distinguished from "it was different this time."
- **Core assumption:** Phenomenological reports contain semantically rich distinctions that require contextual understanding rather than bag-of-words approaches.
- **Evidence anchors:** BERTopic's transformer-based architecture enables context-aware analysis; topic coherence scores of 0.56-0.57 fall within acceptable range (0.4-0.7); related work shows BERTopic coherence gains over LDA.

### Mechanism 2
- **Claim:** Density-based clustering identifies natural thematic groupings without pre-specifying cluster counts.
- **Mechanism:** HDBSCAN detects clusters of varying densities and shapes based on data structure, automatically determining optimal cluster count through hierarchical density analysis.
- **Core assumption:** Phenomenological experiences form natural semantic clusters rather than uniformly distributed or purely random patterns.
- **Evidence anchors:** HDBSCAN identifies clusters without pre-specified counts; grid search yielded 14 topics for HS and 8 topics for DL, reflecting dataset-specific characteristics; however, DL showed 31.32% unassigned responses.

### Mechanism 3
- **Claim:** LLM-based labelling reduces researcher bias while maintaining semantic consistency.
- **Mechanism:** Llama-3-8B-Instruct receives top c-TF-IDF keywords and representative excerpts, generating data-driven labels through instruction-following rather than subjective manual coding.
- **Core assumption:** LLMs can accurately interpret phenomenological themes from keyword sets without domain-specific fine-tuning.
- **Evidence anchors:** The approach shifts labelling from subjective manual task to data-driven method; however, topic interpretations may be affected by LLM biases inherent to pre-training on generalised linguistic datasets.

## Foundational Learning

- **Concept: Topic Modelling (LDA vs. Transformer-based)**
  - **Why needed here:** Understanding why BERTopic was chosen over traditional LDA for capturing contextual meaning in phenomenological reports.
  - **Quick check question:** Can you explain why "I felt love" and "I love the patterns" would be treated identically by LDA but differently by transformer embeddings?

- **Concept: Dimensionality Reduction (UMAP)**
  - **Why needed here:** UMAP preserves both local and global structure when reducing 768-dimensional embeddings to clusterable space.
  - **Quick check question:** What parameter controls the balance between preserving local neighborhoods vs. global structure in UMAP?

- **Concept: Coherence Metrics (Cv, UMass)**
  - **Why needed here:** Used to validate topic quality and optimize hyperparameters; Cv correlates with human judgments of topic interpretability.
  - **Quick check question:** Why might high coherence scores still produce phenomenologically inaccurate topics?

## Architecture Onboarding

- **Component map:** Preprocessing → NLTK sentence tokenization → SentenceTransformer (all-mpnet-base-v2, 768-dim) → UMAP (n_components=15-18, n_neighbors=20-25) → HDBSCAN (min_cluster_size=5-10, min_samples=5) → c-TF-IDF topic extraction → Llama-3-8B-Instruct (Q4_K_M quantization) for labelling

- **Critical path:** Embedding quality → UMAP parameter tuning → HDBSCAN cluster detection → Coherence validation → LLM labelling. Poor embeddings propagate through all downstream steps.

- **Design tradeoffs:**
  - Sentence-level vs. document-level tokenization: Sentences enable granular analysis but may disrupt cross-sentence context.
  - min_cluster_size: Lower values capture rare phenomena but increase noise; higher values improve stability but miss subtle topics.
  - LLM vs. manual labelling: Faster and less biased, but may lack phenomenological domain expertise.

- **Failure signatures:**
  - >25 topics suggests overfitting/fragmentation; <5 suggests overly broad categories
  - High outlier rate (>30% as in DL) indicates clusters don't fit data structure
  - Inconsistent LLM labels across runs indicates prompt/keyword instability

- **First 3 experiments:**
  1. **Parameter sweep:** Run grid search on UMAP (n_neighbors: [10, 15, 20, 25]) and HDBSCAN (min_cluster_size: [5, 10, 15]) with coherence scoring to reproduce optimal configurations.
  2. **Stability test:** Run the full pipeline 5 times on the same data; measure label consistency and topic count variance.
  3. **Validation check:** Have domain experts manually review topic assignments for a random 10% sample to assess phenomenological accuracy vs. algorithmic coherence.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do expectations and prior knowledge shape the nature and intensity of stroboscopic light stimulation (SLS) experiences?
  - **Basis in paper:** The authors explicitly state that expectations and prior knowledge may shape SLS experiences.
  - **Why unresolved:** The naturalistic dataset makes it impossible to isolate stimulus effects from participant expectations.
  - **What evidence would resolve it:** Replication studies using expectation manipulation paradigms.

- **Open Question 2:** Do distinct experiential categories identified by topic modelling correspond to distinct patterns of brain activity?
  - **Basis in paper:** The study notes that establishing mappings between categories and neural correlates would advance understanding of subjective experiences and their neural correlates.
  - **Why unresolved:** The current study lacked concurrent neuroimaging or physiological data.
  - **What evidence would resolve it:** Future neuroimaging studies (e.g., fMRI, EEG) that utilize MOSAIC categories as targets for analysis during SLS.

- **Open Question 3:** To what extent do individual differences in phenomenological control predict the occurrence of complex versus simple visual hallucinations?
  - **Basis in paper:** The study calls for testing whether phenomenological control scores predict subjective responses and determine their predictive role in VH occurrence.
  - **Why unresolved:** The dataset lacked trait-level psychological metrics for participants.
  - **What evidence would resolve it:** Correlating participants' scores on the Phenomenological Control Scale with the prevalence of complex hallucinations in their reports.

## Limitations

- LLM-based topic labelling mechanism lacks validation against human phenomenological expertise, raising questions about whether labels accurately capture experiential meaning rather than linguistic patterns.
- The 31.32% outlier rate in the Deep Listening condition suggests the clustering approach may not fully capture the semantic structure of certain phenomenological domains.
- Without access to the original Dreamachine dataset and specified random seeds, reproducibility remains limited.

## Confidence

**High Confidence:** The core BERTopic pipeline mechanics are well-established in NLP literature, with coherence scores falling within validated ranges for topic modelling quality assessment.

**Medium Confidence:** The systematic comparison between High Sensory and Deep Listening conditions demonstrates methodological rigor, though substantial differences in topic counts and outlier rates suggest condition-specific challenges requiring further investigation.

**Low Confidence:** The claim that LLM-based labelling reduces researcher bias remains unproven without direct comparison to manual coding or expert validation of semantic accuracy in phenomenological interpretation.

## Next Checks

1. **Phenomenological Expert Review:** Have domain experts manually review a stratified random sample of topic assignments to assess whether algorithmic clusters capture genuine experiential distinctions versus semantic artifacts.

2. **Label Stability Testing:** Run the LLM labelling process 10 times on identical keyword sets and compare semantic consistency of generated labels using embedding-based similarity measures to quantify reproducibility.

3. **Cross-Validation with Traditional Methods:** Apply manual thematic analysis to a subset of reports and compare identified themes with MOSAIC-generated topics to assess complementarity and potential blind spots in the automated approach.