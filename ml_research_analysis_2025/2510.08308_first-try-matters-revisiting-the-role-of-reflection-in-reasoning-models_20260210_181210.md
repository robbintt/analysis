---
ver: rpa2
title: 'First Try Matters: Revisiting the Role of Reflection in Reasoning Models'
arxiv_id: '2510.08308'
source_url: https://arxiv.org/abs/2510.08308
tags:
- candidate
- answer
- reasoning
- reflections
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically analyzes the role of reflection in reasoning
  models by studying eight reasoning models across five mathematical datasets. The
  authors find that reflections are predominantly confirmatory rather than corrective,
  with over 90% of reflections simply reaffirming earlier answers rather than correcting
  mistakes.
---

# First Try Matters: Revisiting the Role of Reflection in Reasoning Models

## Quick Facts
- arXiv ID: 2510.08308
- Source URL: https://arxiv.org/abs/2510.08308
- Reference count: 40
- Authors: Liwei Kang; Yue Deng; Yao Xiao; Zhanfeng Mo; Wee Sun Lee; Lidong Bing
- One-line result: Reflections in reasoning models are mostly confirmatory, and training on reflection-rich data improves performance mainly by enhancing first-answer correctness rather than error correction.

## Executive Summary
This paper systematically analyzes the role of reflection in reasoning models by studying eight reasoning models across five mathematical datasets. The authors find that reflections are predominantly confirmatory rather than corrective, with over 90% of reflections simply reaffirming earlier answers rather than correcting mistakes. Through supervised fine-tuning experiments, they show that training on reflection-rich data improves performance primarily by enhancing first-answer correctness rather than enabling error correction. Motivated by these findings, the authors propose a question-aware early-stopping method that dynamically truncates reflections after a candidate answer appears, achieving a 24.5% reduction in reasoning tokens with only a 2.9% drop in accuracy across five mathematical datasets.

## Method Summary
The authors collect reasoning traces from multiple models on mathematical benchmarks, then use an LLM-based candidate answer extractor to identify answer positions and values. They analyze reflection patterns by categorizing transitions between consecutive candidates (T→T, F→F, F→T, etc.) and train SFT models using truncated rollouts cut at different candidate positions. For early-stopping, they train two classifiers: a Candidate Answer Detector (CAD) to identify candidate sentences and a Question-aware Reflection Controller (QRC) to predict whether a problem benefits from extended reflection based on its likelihood of containing F→T transitions. The QRC uses a threshold (0.05) to decide whether to stop after the first or third candidate.

## Key Results
- Over 90% of reflections are confirmatory (T→T or F→F) rather than corrective (F→T)
- Cut-at-6 SFT outperforms cut-at-1 by 4.05% accuracy, with 3.75% from first-answer improvement vs 0.3% from reflections
- Question-aware early-stopping achieves 24.5% token reduction with only 2.9% accuracy drop
- QRC threshold tuning enables trade-offs: 1 pp accuracy drop for 12% token reduction, or 8.12 pp drop for 40.7% reduction

## Why This Works (Mechanism)

### Mechanism 1: Confirmatory Reflection Dominance
- Claim: Over 90% of reflections reaffirm rather than correct initial answers.
- Mechanism: After generating a candidate answer, the model's continuation is primarily post-hoc justification rather than genuine error detection. The model samples from a distribution conditioned on the already-produced answer, creating a self-consistency loop that resamples similar conclusions.
- Core assumption: The extraction methodology accurately identifies candidate positions; human evaluation showed 94% accuracy on 100 sampled rollouts.
- Evidence anchors:
  - [abstract] "reflections are predominantly confirmatory rather than corrective, with over 90% of reflections simply reaffirming earlier answers rather than correcting mistakes"
  - [Section 2.2, Figure 3] Reflection type statistics show T→T and F→F (same) transitions comprise 87-93% across models
  - [corpus] Related work "Revisiting Overthinking in Long Chain-of-Thought" corroborates unnecessary reasoning steps persist after correct answers

### Mechanism 2: First-Try Correctness as Primary Training Signal
- Claim: Reflection-rich training data improves performance by strengthening initial answer accuracy, not by teaching self-correction.
- Mechanism: Longer rollouts with multiple reflections expose the model to diverse reasoning paths toward the same solution. During SFT, this implicit data augmentation improves generalization on forward reasoning, making the first attempt more likely correct. The reflections themselves do not teach a corrective skill.
- Core assumption: The "cut-at-i" dataset construction produces coherent training data; the filtering removed <0.5% of rollouts where regenerated answers differed.
- Evidence anchors:
  - [Section 3.1, Figure 7] Cut-at-6 outperforms cut-at-1 by 4.05% averaged, with 3.75% attributed to first-answer accuracy vs. 0.3% from reflections
  - [Section 3.1, Figure 8] RL training gains also show +4.6% to +7.7% first-answer improvement vs. +0.1% to +0.3% from reflections

### Mechanism 3: Question-Aware Adaptive Early-Stopping
- Claim: A lightweight classifier can predict which problems benefit from extended reflection, enabling token-efficient inference with minimal accuracy loss.
- Mechanism: The QRC learns to identify problems whose rollouts historically contained F→T transitions. For problems labeled as "stop early," the CAD monitors generation and terminates after the first candidate. For "continue" problems, it waits for the third candidate.
- Core assumption: The F→T pattern in training rollouts is a sufficient proxy for which problem types benefit from reflection; the QRC generalizes to unseen problems.
- Evidence anchors:
  - [Section 4, Table 2] CAD+QRC achieves 24.5% token reduction with 2.9% accuracy drop across five datasets
  - [Section 4, Figure 9] Threshold tuning enables trade-off: 1 pp accuracy drop for 12% token reduction, or 8.12 pp drop for 40.7% reduction

## Foundational Learning

- Concept: **Reflection vs. Forward Reasoning Decomposition**
  - Why needed here: The entire analysis hinges on separating "steps to first candidate" from "steps after first candidate." Without this distinction, you cannot measure confirmatory vs. corrective behavior.
  - Quick check question: Given a CoT with candidate answers at lines 50, 120, and 200, which portion constitutes "reflections"?

- Concept: **Candidate Answer Extraction via LLM**
  - Why needed here: The paper uses a Qwen3-1.7B-based detector to identify which lines contain candidate answers. Understanding this extraction is necessary to replicate the reflection analysis or apply early-stopping.
  - Quick check question: If a line states "m=100, n=13" for a problem asking "what is m+n?", what should the extractor output?

- Concept: **p(F→T) as Corrective Reflection Metric**
  - Why needed here: The probability of transitioning from incorrect to correct candidates quantifies genuine self-correction ability. This metric is central to evaluating whether reflections serve a corrective function.
  - Quick check question: In a rollout with three incorrect candidates followed by a correct one, what is the p(F→T)?

## Architecture Onboarding

- Component map:
  - LLM-based Extractor -> Candidate Answer Detector (CAD) -> Question-aware Reflection Controller (QRC) -> Early-stopping decision
  - External verifier tool for candidate correctness labeling

- Critical path:
  1. Collect rollouts from target reasoning model on benchmark problems
  2. Run LLM-based extractor to identify candidate positions (offline analysis phase)
  3. Label candidates as correct/incorrect using verifier
  4. Compute reflection type statistics (T→T, F→T, etc.)
  5. For deployment: train CAD and QRC on annotated data, integrate into inference pipeline

- Design tradeoffs:
  - **Extractor model size**: gpt-oss-120b provides robustness but is expensive; Qwen3-1.7B is cheaper for CAD training but may miss edge cases
  - **Early-stopping threshold**: Lower QRC threshold = more aggressive truncation = higher token savings but more accuracy loss (Figure 9)
  - **Fixed vs. adaptive stopping**: Stopping at 1st candidate universally saves more tokens but drops accuracy 3.8%; QRC recovers ~1 pp accuracy

- Failure signatures:
  - **Extractor misses candidates**: If CAD false negative rate is high, early-stopping triggers too late, negating efficiency gains
  - **QRC over-predicts "continue"**: Token savings diminish; if under-predicts, accuracy drops on problems that needed reflection
  - **Domain shift**: CAD/QRC trained on math may not generalize to code or logical reasoning without retraining

- First 3 experiments:
  1. **Replicate extraction robustness check**: Sample 100 rollouts from your target model, run extraction, manually verify candidate positions. Target: >90% agreement with human labels.
  2. **Measure reflection statistics on your domain**: Apply the extraction pipeline to your problem type (e.g., code, logic). If corrective reflections (F→T) exceed 5%, early-stopping may be less applicable.
  3. **Ablate early-stopping thresholds**: Deploy CAD-only (stop at 1st) vs. CAD+QRC on a held-out set. Measure token reduction vs. accuracy trade-off curve. Target: identify threshold matching your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the correlation between reflection-rich training and "first-try" correctness hold for non-mathematical domains like code generation?
- Basis: [inferred] The study is strictly limited to mathematical benchmarks (AIME, AMC, Olympiad Bench, Math500). Coding tasks often involve iterative error correction (debugging), which might make reflections more corrective than confirmatory.
- Why unresolved: The authors' hypothesis—that reflections improve generalization via diverse reasoning paths—assumes a problem space where verification is internal.
- What evidence would resolve it: Applying the candidate extraction and early-stopping methods to coding benchmarks (e.g., HumanEval, MBPP) to see if reflections remain confirmatory.

### Open Question 2
- Question: What specific mechanism causes reflection-rich training data to improve "first-attempt" accuracy rather than corrective ability?
- Basis: [explicit] The authors ask, "If reflections... rarely change answers, why is their presence strongly correlated with accuracy?" They hypothesize that reflections expose "diverse reasoning paths" (Section 3.1), but do not verify this.
- Why unresolved: It is unclear if the improvement stems from data diversity, token-length regularization, or implicit knowledge distillation.
- What evidence would resolve it: Ablation studies comparing model performance when trained on diverse paths versus repetitive confirmation paths with identical token counts.

### Open Question 3
- Question: Can reinforcement learning (RL) be designed to specifically enhance the corrective transition rate ($F \rightarrow T$)?
- Basis: [explicit] Section 3.2 concludes that supervised fine-tuning (SFT) on corrective data fails to improve self-correction ($p(F \rightarrow T)$).
- Why unresolved: The paper demonstrates that SFT primarily boosts first-try correctness, leaving the challenge of teaching models to dynamically fix errors largely unsolved.
- What evidence would resolve it: RLVR experiments utilizing reward functions explicitly shaped to maximize the probability of transitioning from an incorrect candidate to a correct one.

## Limitations

- The analysis is limited to mathematical reasoning domains, leaving uncertainty about whether confirmatory reflection patterns generalize to other domains like code or logical reasoning where error patterns may differ fundamentally.
- The early-stopping approach relies on classifiers trained on a single reasoning model's rollouts, and the paper does not extensively test cross-domain generalization.
- The fixed threshold (0.05) for QRC predictions represents a potential limitation, as optimal thresholds may vary by task complexity or model capability.

## Confidence

**High Confidence**: The core finding that over 90% of reflections are confirmatory rather than corrective is well-supported by comprehensive analysis across eight reasoning models and five datasets. The SFT experiments showing that first-answer accuracy improvements account for the majority of performance gains (3.75% vs 0.3%) are robust and clearly demonstrate the mechanism at work.

**Medium Confidence**: The claim that reflection-rich training data improves performance primarily through enhanced first-answer correctness is supported by strong experimental evidence but relies on the assumption that the cut-at-i dataset construction produces valid training signals.

**Low Confidence**: The generalizability of the question-aware early-stopping method beyond mathematical domains remains uncertain. While the paper demonstrates 24.5% token reduction with 2.9% accuracy drop, this was achieved using classifiers trained specifically on math reasoning rollouts.

## Next Checks

1. **Cross-domain reflection analysis**: Apply the extraction pipeline to non-mathematical reasoning domains (code, logical puzzles, natural language inference) to measure whether confirmatory reflection dominance persists across different problem types and error patterns.

2. **Classifier generalization study**: Train QRC and CAD classifiers on one reasoning model family and evaluate performance when applied to rollouts from entirely different model architectures to assess transfer learning capabilities and domain robustness.

3. **Threshold sensitivity analysis**: Systematically vary QRC and CAD classification thresholds across multiple problem types to identify optimal trade-offs between token savings and accuracy retention, particularly for problems of varying difficulty and complexity.