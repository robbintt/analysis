---
ver: rpa2
title: Assessing Large Language Models in Updating Their Forecasts with New Information
arxiv_id: '2509.23936'
source_url: https://arxiv.org/abs/2509.23936
tags:
- confidence
- news
- information
- forecasts
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVOLVECAST, a framework for evaluating how
  large language models update forecasts and confidence in response to new information.
  The key idea is to measure belief dynamics by comparing the model's confidence updates
  against reference human forecasts, rather than against final outcomes, to assess
  rational belief updating under uncertainty.
---

# Assessing Large Language Models in Updating Their Forecasts with New Information

## Quick Facts
- arXiv ID: 2509.23936
- Source URL: https://arxiv.org/abs/2509.23936
- Authors: Zhangdie Yuan; Zifeng Ding; Andreas Vlachos
- Reference count: 22
- Primary result: LLMs update forecasts conservatively and inconsistently compared to human reference standards when presented with new information

## Executive Summary
This paper introduces EVOLVECAST, a framework for evaluating how large language models update forecasts and confidence in response to new information. The key idea is to measure belief dynamics by comparing the model's confidence updates against reference human forecasts, rather than against final outcomes, to assess rational belief updating under uncertainty. The authors evaluate models on directional agreement, magnitude alignment, and confidence calibration using a benchmark of 1,613 question-news pairs from Metaculus. Results show that while models can update their forecasts, these updates are often conservative and inconsistent. Neither verbalized nor logit-based confidence methods consistently outperform the other, and both fall short of human reference standards. Models struggle to integrate new evidence rationally, suggesting current approaches like RAG-based methods are insufficient for probabilistic reasoning.

## Method Summary
The EVOLVECAST framework evaluates LLMs on 1,613 question-news pairs from Metaculus by comparing forecast updates against aggregated human forecasts rather than binary outcomes. Models are tested under two conditions: question-only at initial time (T0) and question-plus-news at later time (T1). Confidence is extracted either verbally (1-10 scale normalized to [0,1]) or through logit-based mean token probability. The framework measures directional agreement (MDA, Precision, Recall, F1), magnitude alignment (MSE, SMSPE), and calibration change (Brier Score and ΔBrier). The approach uses human forecaster aggregation as a proxy for rational belief updating, evaluating whether models adjust forecasts proportionally to evidence strength rather than merely predicting correct outcomes.

## Key Results
- LLMs exhibit conservative update bias, with small Δp relative to reference Δh
- Neither verbalized nor logit-based confidence extraction consistently outperforms the other
- Models show directional confusion (Up↔Down) and excessive "Still" predictions under accumulated context
- Both confidence extraction methods remain far from human reference standards
- Conservative bias persists across model sizes and is not resolved by architectural improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregated human forecasts serve as a viable proxy for rational belief updating under uncertainty.
- **Mechanism:** Community prediction aggregates from Metaculus smooth idiosyncratic biases while preserving collective uncertainty, providing a time-varying reference signal that captures justified belief states rather than binary outcomes.
- **Core assumption:** Human forecaster aggregation approximates Bayesian rationality, and disagreement among forecasters reflects genuine task ambiguity rather than noise.
- **Evidence anchors:** "We use human forecasters as a comparative reference to assess forecast updates and confidence calibration under new information." "Aggregation over many individuals smooths out idiosyncratic biases while preserving collective uncertainty, yielding a stable yet informative signal."
- **Break condition:** When questions have fewer than ~100 individual forecasts (reducing aggregate reliability), or when resolution criteria are ambiguous or modified post-submission.

### Mechanism 2
- **Claim:** Comparing confidence deltas (Δp = pt − p0) against reference deltas (Δh) isolates belief updating behavior from static knowledge.
- **Mechanism:** By evaluating within-instance change rather than final accuracy, the framework cancels effects of pretraining knowledge and directly tests responsiveness to new information through directional agreement (MDA), magnitude alignment (MSE/SMSPE), and calibration change (ΔBrier).
- **Core assumption:** A well-calibrated model should update in proportion to evidence strength, not merely predict the eventual binary outcome correctly.
- **Evidence anchors:** "EVOLVECAST assesses whether LLMs adjust their forecasts when presented with information released after their training cutoff." "A negative ΔBrier indicates improved calibration after observing new information... while a positive value suggests degradation."
- **Break condition:** When models produce noisy probability estimates that fluctuate due to prompt sensitivity rather than evidence; threshold ϵ must be tuned per extraction method.

### Mechanism 3
- **Claim:** Conservative update bias emerges from tension between parametric memory and context window.
- **Mechanism:** LLMs process retrieved news as additional tokens in attention, but pre-trained weights dominate; models treat new information as retrieval context rather than likelihood evidence for posterior updating.
- **Core assumption:** The architecture lacks explicit mechanisms to weigh external evidence against prior beliefs in a Bayesian sense.
- **Evidence anchors:** "The massive weight of pre-training data acts as an overwhelming anchor, preventing the short context window from shifting the probability distribution." "Neither verbalized nor logits-based confidence estimates consistently outperform the other, and both remain far from the human reference standard."
- **Break condition:** Accumulated news context may paradoxically degrade performance by introducing noise; larger models show stronger conservative bias toward "Still" predictions.

## Foundational Learning

- **Concept: Bayesian Belief Updating**
  - Why needed here: The entire framework evaluates whether LLMs approximate rational posterior updating when conditioned on new evidence.
  - Quick check question: If prior probability is 55% and new evidence doubles the odds, what should the posterior be?

- **Concept: Probability Calibration (Brier Score)**
  - Why needed here: Calibration metrics determine whether model confidence reflects true probability; the paper measures whether calibration improves or degrades after updates.
  - Quick check question: A model predicts 80% confidence on events that occur 60% of the time—is it overconfident or underconfident?

- **Concept: Forecast Aggregation Methods**
  - Why needed here: Understanding how Metaculus aggregates individual forecasts into community predictions is essential for interpreting the reference signal.
  - Quick check question: Why might median aggregation outperform mean aggregation for forecasting?

## Architecture Onboarding

- **Component map:** Metaculus question filtering -> comment stream monitoring -> Google Search API retrieval -> semantic similarity ranking -> question-news alignment -> Two-condition inference (T0/T1) -> confidence extraction (verbalized/logit-based) -> Delta computation (Δp, Δh) -> MDA/Precision/Recall/F1 -> MSE/SMSPE -> Brier scores and ΔBrier

- **Critical path:** Temporal alignment is the bottleneck; if news retrieval fails to identify contemporaneous evidence that plausibly shifted human forecasts, all downstream metrics become noisy. Manual validation (90% relevance in N=20 sample) is necessary but not sufficient.

- **Design tradeoffs:**
  - Verbalized confidence: More interpretable, aligns with human forecasting practice, but models may default to generic values
  - Logit-based confidence: Directly samples internal uncertainty, but highly sensitive to prompt length and decoding hyperparameters
  - Single vs. accumulated news: Single updates isolate signal; accumulated context introduces noise but better simulates real forecasting

- **Failure signatures:**
  - Excessive "Still" predictions: Model fails to recognize directional evidence
  - Spurious "Up" bias under accumulated context: Model treats all evidence as positive signal
  - Small Δp relative to Δh: Conservative update bias
  - Directional confusion (Up↔Down): Model cannot distinguish positive from negative evidence

- **First 3 experiments:**
  1. Establish baseline with verbalized confidence on single-update condition across all model sizes to characterize conservative bias magnitude.
  2. Compare logit-based vs. verbalized confidence extraction to determine which better aligns with human reference deltas.
  3. Run directional QA ablation to isolate whether models understand evidence direction even when probability calibration fails.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies on the assumption that aggregated human forecasts provide a valid proxy for rational belief updating, but this aggregation may itself contain residual biases or noise.
- The automated news retrieval and alignment process, while validated at 90% relevance in a small sample, remains a potential source of systematic error if semantically similar but irrelevant news snippets are matched to questions.
- Conservative update bias may reflect architectural limitations (pre-training dominance) rather than fundamental impossibility of rational updating in LLMs.

## Confidence

- **High confidence**: The methodological framework for measuring belief dynamics (directional agreement, magnitude alignment, calibration change) is sound and well-specified.
- **Medium confidence**: The claim that LLMs exhibit conservative update bias is supported by results, though the mechanism (architectural vs. training-related) requires further validation.
- **Medium confidence**: The finding that neither verbalized nor logit-based confidence extraction consistently outperforms the other is robust across model sizes, but the absolute performance gap between methods remains unclear.

## Next Checks

1. **Manual validation of news relevance**: Examine 50 additional question-news pairs to verify the 90% relevance rate holds across the full dataset, particularly for questions with ambiguous or indirect news connections.

2. **Threshold sensitivity analysis**: Systematically vary the confidence delta threshold (ϵ) across a wider range to determine if performance patterns change substantially, which would indicate sensitivity to this hyperparameter.

3. **Architecture ablation study**: Test whether fine-tuning LLMs on forecasting tasks with explicit belief-updating objectives reduces conservative bias compared to base models, isolating whether the bias is architectural or training-related.