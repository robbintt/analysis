---
ver: rpa2
title: 'sam-llm: interpretable lane change trajectoryprediction via parametric finetuning'
arxiv_id: '2509.03462'
source_url: https://arxiv.org/abs/2509.03462
tags:
- lane
- change
- prediction
- trajectory
- lateral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SAM-LLM addresses lane change trajectory prediction by combining
  Large Language Model reasoning with kinematic modeling, outputting physically meaningful
  parameters instead of raw coordinates. The method uses a hybrid approach: for lane-keeping
  scenarios, it predicts discrete coordinates, while for lane changes, it generates
  parameters for an enhanced Sinusoidal Acceleration Model (lateral displacement,
  maneuver duration, initial lateral velocity, and longitudinal velocity change).'
---

# sam-llm: interpretable lane change trajectoryprediction via parametric finetuning

## Quick Facts
- **arXiv ID**: 2509.03462
- **Source URL**: https://arxiv.org/abs/2509.03462
- **Reference count**: 0
- **Primary result**: 98.73% intention prediction accuracy with 80% output reduction and 54% inference speedup on highD dataset

## Executive Summary
SAM-LLM introduces a hybrid approach for lane change trajectory prediction that combines Large Language Model reasoning with physics-based kinematic modeling. The method predicts either discrete coordinates for lane-keeping or four interpretable parameters (lateral displacement, duration, initial lateral velocity, longitudinal velocity change) for lane changes, which are then reconstructed into continuous trajectories using an enhanced Sinusoidal Acceleration Model. Evaluated on the highD dataset, SAM-LLM achieves state-of-the-art intention prediction accuracy while significantly reducing computational burden compared to coordinate-based approaches.

## Method Summary
SAM-LLM fine-tunes Llama-2-7B with LoRA (rank=64) to predict lane change intentions and future trajectories. The model uses a hybrid output strategy: for lane-keeping scenarios (intention=0), it outputs 4 discrete coordinates; for lane changes (intention∈{1,2}), it predicts 4 SAM parameters that define a continuous trajectory via modified sinusoidal acceleration equations. The method employs chain-of-thought reasoning and is trained on historical driving data from the highD dataset, achieving significant reductions in output dimensionality and inference time while maintaining high prediction accuracy.

## Key Results
- Achieves 98.73% intention prediction accuracy on highD test set
- Reduces output dimensionality by 80% compared to coordinate-based methods
- Demonstrates 6.1-19.9% lower lateral RMSE across 1-4s prediction horizons
- Achieves 54% inference speedup (747.3ms vs 1627.8ms)

## Why This Works (Mechanism)

### Mechanism 1: Parametric Output Compression
The LLM predicts 4 interpretable physical parameters instead of coordinate sequences, reducing computational burden while improving long-horizon accuracy. This bypasses error accumulation across discrete predictions by generating continuous trajectories via SAM equations. The approach assumes lane change trajectories conform to sinusoidal acceleration patterns specified in the enhanced SAM formulation.

### Mechanism 2: Conditional Representation Selection
A hybrid output strategy matches representation complexity to maneuver type. The model first predicts intention I ∈ {0, 1, 2}, then conditionally outputs either 4 coordinates (lane keeping) or 4 SAM parameters (lane changes). This optimizes per-maneuver representation by routing simpler motions to lower-overhead representations, assuming intention classification is sufficiently accurate.

### Mechanism 3: Physics-Constrained Trajectory Reconstruction
Reconstructing trajectories via SAM equations enforces physical plausibility through boundary conditions (zero lateral acceleration at start/end) and smooth continuous paths. The modified SAM formulation adapts classical SAM for boundary-crossing prediction timing, assuming the sinusoidal profile accurately captures post-boundary trajectory segments.

## Foundational Learning

- **Sinusoidal Acceleration Model (SAM)**:
  - Why needed here: The entire parametric output depends on understanding how lateral acceleration follows a sinusoidal profile during lane changes
  - Quick check question: Can you explain why zero lateral acceleration at maneuver start and end produces smoother, more realistic trajectories?

- **LoRA Fine-Tuning**:
  - Why needed here: SAM-LLM uses LoRA (rank 64) for parameter-efficient adaptation of Llama-2-7B
  - Quick check question: What does LoRA freeze, and what does it adapt? Why might low-rank adaptation be preferable for specialized domains?

- **Chain-of-Thought Prompting**:
  - Why needed here: SAM-LLM uses CoT to generate intermediate reasoning before predictions
  - Quick check question: How does forcing a model to verbalize reasoning before outputting a final answer potentially improve robustness?

## Architecture Onboarding

- **Component map**: Multi-modal input encoder → Llama-2-7B backbone → Hybrid output decoder → Trajectory reconstructor
- **Critical path**: Historical states H → Prompt encoding → LLM inference → Intention I → Conditional output (Tcoord or Tparam) → If Tparam: apply SAM equations → Final trajectory
- **Design tradeoffs**: Parametric outputs sacrifice flexibility for interpretability and efficiency; hybrid strategy adds complexity but optimizes per-maneuver representation; coordinate outputs retained for lane-keeping despite parametric benefits
- **Failure signatures**: High longitudinal RMSE suggests SAM longitudinal model may be underspecified; intention misclassification leads to wrong output format; parameter clustering can reveal unrealistic values
- **First 3 experiments**:
  1. Implement coordinate-only output with identical setup to verify performance gap matches reported 6-20% lateral RMSE improvement
  2. Test whether modified SAM outperforms classical SAM when predictions are made at boundary crossing timing
  3. Systematically perturb each SAM parameter to quantify which most strongly affects final trajectory RMSE

## Open Questions the Paper Calls Out
- How can longitudinal trajectory prediction accuracy be improved within the SAM-LLM framework to match or exceed its lateral prediction performance?
- Can the SAM-LLM parametric approach generalize effectively to complex urban driving scenarios with intersections, traffic signals, and multi-directional traffic flow?
- Can the parametric representation approach be extended to lane-keeping scenarios to achieve similar efficiency and interpretability gains?
- How robust is SAM-LLM to non-typical lane change maneuvers such as emergency avoidance, aborted lane changes, or rapid multi-lane transitions?

## Limitations
- Evaluation limited to German highway dataset (highD), restricting generalizability to other driving environments
- Critical implementation details (input window duration, prompt templates, feature extraction logic) remain unspecified
- No validation that predicted parameters fall within physically realistic ranges or analysis of edge cases

## Confidence
- **High Confidence**: Parametric output strategy reduces computational complexity and enables interpretable physical parameters
- **Medium Confidence**: Intention prediction accuracy and lateral trajectory RMSE improvements are plausible but cannot be independently verified
- **Low Confidence**: Claims about SAM formulation superiority lack direct comparisons; generalization assertions are entirely unverified

## Next Checks
1. Systematically analyze predicted SAM parameters across test set to verify physical plausibility ranges and flag extreme values
2. Evaluate trained model on different highway dataset (e.g., NGSIM) to assess generalization and compare performance
3. Compare performance between modified SAM and classical SAM formulations on same test set to isolate benefits of modifications