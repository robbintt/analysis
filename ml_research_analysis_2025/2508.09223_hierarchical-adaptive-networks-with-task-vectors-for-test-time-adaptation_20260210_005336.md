---
ver: rpa2
title: Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation
arxiv_id: '2508.09223'
source_url: https://arxiv.org/abs/2508.09223
tags:
- adaptation
- hi-vec
- test-time
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hi-Vec addresses the limitations of single-layer test-time adaptation
  by introducing hierarchical linear layers that capture coarse-to-fine representations.
  It dynamically selects the optimal layer for each batch based on gradient norms,
  shares adapted weights across layers via task vectors, and uses hierarchical layer
  agreement to skip adaptation on noisy samples.
---

# Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation

## Quick Facts
- **arXiv ID:** 2508.09223
- **Source URL:** https://arxiv.org/abs/2508.09223
- **Reference count:** 40
- **Key outcome:** Hi-Vec improves test-time adaptation accuracy on corruption benchmarks by 3-6% over baselines and reduces calibration error while handling outlier data and spurious correlations.

## Executive Summary
Hi-Vec addresses the limitations of single-layer test-time adaptation by introducing hierarchical linear layers that capture coarse-to-fine representations. It dynamically selects the optimal layer for each batch based on gradient norms, shares adapted weights across layers via task vectors, and uses hierarchical layer agreement to skip adaptation on noisy samples. Evaluated on outlier and spurious correlation shifts, Hi-Vec consistently improves accuracy and robustness over four state-of-the-art methods. For example, on CIFAR-10-C with noise outliers, Hi-Vec+Stamp achieves 83.6% accuracy, up from 77.9%, and reduces calibration error. It also handles small batch sizes and high outlier rates while mitigating catastrophic forgetting, making it effective for real-world distribution shifts.

## Method Summary
Hi-Vec modifies test-time adaptation by replacing single linear heads with hierarchical layers of increasing dimension (8 to encoder output size) that share weights. During adaptation, it selects the layer requiring minimal gradient update via entropy loss, checks mutual information agreement between layers to detect outliers, and propagates updates through task vectors to maintain cross-layer coherence. The framework integrates with existing TTA methods (Tent, SAR, DeYo, STAMP) and requires source models to be trained with the hierarchical structure.

## Key Results
- Improves accuracy by 3-6% over baselines on corruption benchmarks
- Reduces calibration error (ECE) while maintaining accuracy
- Handles outlier data and spurious correlations effectively

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Layer Selection via Gradient Norms
Selecting the hierarchical layer with the lowest gradient norm minimizes error accumulation during adaptation. The architecture employs multiple linear layers of increasing dimension (coarse-to-fine). During testing, the method computes the gradient norm for each layer using an entropy loss. The layer requiring the smallest update is selected for backpropagation. This implies the selected layer is already the most "aligned" with the target distribution, preventing drastic, unstable updates.

### Mechanism 2: Noise Gating via Hierarchical Layer Agreement
Mutual information between the logits of the selected layer and other hierarchical layers acts as a filter for out-of-distribution samples. Before adapting, the model measures the dependency between the prediction of the dynamically selected layer and the predictions of other layers. High MI suggests semantic consistency across coarse-to-fine representations (In-Distribution). Low MI suggests inconsistency (OOD/Outliers). If MI is low, adaptation is skipped to prevent poisoning the model weights.

### Mechanism 3: Target Information Propagation via Task Vectors
Weight arithmetic (Task Vectors) can propagate target-specific knowledge from the adapted layer to unadapted layers without requiring backpropagation. Only the selected layer undergoes computationally expensive backpropagation. To ensure the other hierarchical layers do not become stale, the framework calculates a "task vector" (the change in weights of the selected layer) and adds it to the compatible weight subsets of the other layers using cosine similarity checks.

## Foundational Learning

- **Concept:** Matryoshka Representation Learning (MRL)
  - **Why needed here:** Hi-Vec replaces the standard single linear head with a "Matryoshka" structure (nested layers of increasing dimension). Understanding that these layers share weights is critical to understanding how Task Vector merging works.
  - **Quick check question:** Why does the gradient of the smallest layer (e.g., dimension 8) directly affect the weights of the largest layer (e.g., dimension 2048) in a Matryoshka setup?

- **Concept:** Entropy Minimization in TTA
  - **Why needed here:** The "Dynamic Layer Selection" relies on calculating a gradient norm. This norm is calculated based on an entropy loss (minimizing uncertainty). You must understand that the "optimal" layer is the one where the model is *already* most certain.
  - **Quick check question:** If a model is already highly confident (low entropy) on a batch, what does the gradient norm look like compared to a batch it is uncertain about?

- **Concept:** Mutual Information (MI) for OOD Detection
  - **Why needed here:** The "Layer Agreement" mechanism is the safety switch. It assumes that in-distribution data shares information across layers, while OOD data looks random. Understanding MI as a measure of dependency is key to tuning the threshold.
  - **Quick check question:** If two layers predict different classes but with equal probability distributions (e.g., [0.5, 0.5] vs [0.5, 0.5]), is the Mutual Information high or low?

## Architecture Onboarding

- **Component map:** Encoder (ResNet backbone) -> Hierarchical Heads (linear layers at dimensions 8, 16, ..., encoder dim) -> Selection Module (gradient norm computation) -> Agreement Module (MI computation) -> Merger (weight arithmetic)

- **Critical path:**
  1. **Inference:** Forward pass through Encoder + all Hierarchical Heads
  2. **Selection:** Identify optimal layer via lowest gradient norm
  3. **Gating:** Check MI between selected layer and others. If MI < threshold, skip adaptation
  4. **Adaptation:** Backpropagate loss on selected layer only
  5. **Merge:** Propagate weight updates from selected layer to compatible weights in other layers

- **Design tradeoffs:**
  - **Inference Speed vs. Robustness:** You must perform a forward pass through all hierarchical layers and compute multiple gradient norms (extra overhead) to gain the robustness of dynamic selection
  - **Plasticity vs. Stability:** The "Task Vector" sharing increases plasticity (all layers update) but risks propagating errors if the Agreement Gate fails

- **Failure signatures:**
  - **Stuck Model (No Adaptation):** If threshold is too aggressive, the model detects valid target shifts as "noise" and never updates weights
  - **Catastrophic Forgetting:** If the "Agreement" logic is disabled, the model adapts to outliers, destroying source knowledge
  - **Oscillation:** The selected layer index jumps wildly between batches, indicating gradient norms are unreliable (likely due to very small batch sizes)

- **First 3 experiments:**
  1. **Layer Selection Validity:** Run on CIFAR-10-C. Plot a histogram of the selected layer indices for "easy" corruptions (e.g., Blur) vs. "hard" corruptions (e.g., Noise). Hypothesis: Harder shifts should select deeper layers
  2. **Noise Robustness Check:** Inject increasing ratios of SVHN (outlier) data into CIFAR-10-C test batches. Compare accuracy curves of standard Tent vs. Hi-Vec+Tent. Look for the "cliff" point where standard Tent collapses but Hi-Vec remains stable
  3. **Ablation on Merging:** Disable the "Task Vector" merging component. Measure if the unselected layers drift significantly in performance over time, proving the necessity of the sharing mechanism

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Hi-Vec framework be adapted to support parameter-free test-time adaptation methods that do not rely on optimization or backpropagation?
- **Open Question 2:** Can the hierarchical structure be implemented as a test-time adapter for frozen, pre-trained models without requiring source-domain retraining?
- **Open Question 3:** Is it possible to achieve dynamic layer selection without the computational latency of computing gradients for every hierarchical layer?

## Limitations

- Relies heavily on gradient norms as proxy for feature alignment, which may become unreliable under extreme distribution shifts
- Mutual information threshold requires careful tuning per dataset and corruption type
- Weight arithmetic approach assumes linear mode connectivity between hierarchical layers

## Confidence

- **High Confidence:** Core mechanism of dynamic layer selection via gradient norms and overall experimental methodology
- **Medium Confidence:** Hierarchical layer agreement mechanism's effectiveness depends heavily on threshold choice
- **Medium Confidence:** Task vector sharing mechanism assumes linear mode connectivity

## Next Checks

1. **Layer Selection Analysis:** Run controlled experiments varying batch sizes (8-256) and plot the distribution of selected layers across different corruption types to verify gradient norm reliability

2. **Threshold Sensitivity:** Conduct ablation studies varying threshold across a wide range (0.5-2.0) on validation sets to identify optimal thresholds and assess robustness to parameter choice

3. **Error Propagation Study:** Implement a modified version disabling task vector sharing and measure the performance drift of unselected layers over time to quantify the necessity of the sharing mechanism