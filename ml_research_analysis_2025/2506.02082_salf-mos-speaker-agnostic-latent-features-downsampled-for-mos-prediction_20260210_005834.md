---
ver: rpa2
title: 'SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction'
arxiv_id: '2506.02082'
source_url: https://arxiv.org/abs/2506.02082
tags:
- speech
- data
- arxiv
- features
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALF-MOS introduces a small, end-to-end model for predicting Mean
  Opinion Scores (MOS) of speech quality without relying on fine-tuning SSL models
  or listener/domain IDs. It uses sequences of convolutions and down sampling to extract
  latent features from wav2vec embeddings, followed by linear layers for final MOS
  prediction.
---

# SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction

## Quick Facts
- arXiv ID: 2506.02082
- Source URL: https://arxiv.org/abs/2506.02082
- Authors: Saurabh Agrawal; Raj Gohil; Gopal Kumar Agrawal; Vikram C M; Kushal Verma
- Reference count: 29
- Primary result: Small 1,574-parameter model achieves state-of-the-art MOS prediction using frozen wav2vec features

## Executive Summary
SALF-MOS introduces a compact end-to-end model for predicting Mean Opinion Scores (MOS) of speech quality without fine-tuning SSL models or requiring listener/domain IDs. The architecture uses sequences of convolutions and downsampling to extract latent features from frozen wav2vec embeddings, followed by linear layers for final MOS prediction. Experiments on four datasets (BVCC, VCC2018, SOMOS, TMHINTQI) demonstrate superior performance compared to larger models while maintaining strong cross-dataset generalization.

## Method Summary
SALF-MOS employs a U-Net-inspired encoder architecture with four double-convolution blocks followed by three downsampling layers. Each double conv block consists of 1D convolution (kernel=3, stride=1, padding=1), batch normalization, and ReLU activation, repeated twice. Downsampling uses kernel=2, stride=2 to progressively compress temporal resolution. Frozen wav2vec embeddings serve as input features, with latent features extracted via linear layers after each double conv block. These features are stacked and passed through a final linear layer for MOS prediction. The model is trained with L1 loss, SGD optimizer (learning rate 1e-4), batch size 4, and early stopping at 20 epochs.

## Key Results
- Achieves state-of-the-art performance with only 1,574 parameters across BVCC, VCC2018, SOMOS, and TMHINTQI datasets
- Outperforms larger models in MSE, LCC, SRCC, and KTAU metrics
- Demonstrates strong generalization across datasets without speaker or domain-specific adaptation
- Reduces dependency on subjective human evaluation while maintaining prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1: SSL Feature Extraction Without Fine-Tuning
Pre-trained wav2vec embeddings provide perceptually-relevant representations sufficient for MOS prediction without task-specific adaptation. Wav2vec's self-supervised training on large-scale speech encodes abstract acoustic properties that SALF-MOS uses as frozen inputs, treating quality assessment as a lightweight transformation problem rather than representation learning.

### Mechanism 2: Progressive Temporal Compression via Hierarchical Downsampling
Successive downsampling operations compress temporal detail while preserving quality-relevant aggregate statistics. Three downsampling layers (kernel=2, stride=2) progressively reduce temporal resolution by factors of 2, forcing the network to learn summary representations rather than frame-level details, aligning with MOS as an utterance-level score.

### Mechanism 3: Multi-Scale Feature Aggregation via Stacking
Concatenating latent features from multiple network depths captures quality information at different abstraction levels. Each of the four double-convolution blocks feeds a linear LFE layer, and these outputs are stacked before final prediction, combining early (low-level acoustic) and late (high-level abstract) representations.

## Foundational Learning

- **Concept: Self-Supervised Speech Representations (wav2vec, HuBERT, WavLM)**
  - Why needed here: SALF-MOS depends entirely on frozen wav2vec features. Understanding what SSL models encode (phonetic, speaker, prosodic information) helps diagnose failure cases.
  - Quick check question: If wav2vec is trained primarily for ASR, why might its representations still correlate with perceptual quality?

- **Concept: Mean Opinion Score (MOS) as a Subjective Metric**
  - Why needed here: The target variable is human-rated on a 1–5 scale with inherent variance (typically ~20 listeners per sample). Understanding this variance informs loss function choice and expected error bounds.
  - Quick check question: Why might the same audio receive different MOS ratings across datasets (e.g., BVCC vs. TMHINTQI)?

- **Concept: U-Net Encoder Architecture**
  - Why needed here: SALF-MOS adapts U-Net's encoder path (convolutions + downsampling) without the decoder. Understanding U-Net's design intent helps recognize what's retained vs. discarded.
  - Quick check question: What information is lost by using only the encoder pathway compared to a full encoder-decoder?

## Architecture Onboarding

- **Component map:** Audio (16kHz) -> wav2vec (frozen) -> [Double Conv -> LFE + Downsample] ×4 -> Stack all LFEs -> Final linear -> MOS

- **Critical path:** Audio (16kHz) → wav2vec (frozen) → [Double Conv → LFE + Downsample] ×4 → Stack all LFEs → Final linear → MOS

- **Design tradeoffs:**
  - Depth = 4: Deeper models underfit; shallower may miss hierarchical patterns
  - Feature choice: wav2vec (MSE=0.144) >> LFCC (0.43) >> MFCC (0.56) >> X-Vector (0.48) on BVCC
  - No skip connections: Unlike full U-Net, no decoder—compression is unidirectional
  - Frozen SSL: Reduces parameters to 1,574 but limits domain adaptation

- **Failure signatures:**
  - Predictions clustered near dataset mean (~3.0): Model learned prior, not quality patterns
  - SRCC drops significantly on out-of-distribution audio: wav2vec features insufficient for novel degradation types
  - Training loss plateaus early with high MSE: Check feature extraction pipeline

- **First 3 experiments:**
  1. Reproduce BVCC baseline: Use exact config (lr=1e-4, batch=4, L1 loss, SGD, 8:1:1 split, early stopping at 20 epochs). Target: MSE ≈ 0.144, SRCC ≈ 0.946.
  2. Feature ablation: Replace wav2vec with MFCC, LFCC, X-Vector. Confirm performance drop per Table III to validate SSL dependency.
  3. Depth sensitivity: Test depths 2, 3, 5 on BVCC. Verify depth-4 optimum per Fig. 5 (deeper → underfitting, higher MSE).

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- Extreme parameter efficiency (1,574 parameters) may limit generalization to noisy environments or low-resource languages
- Performance depends entirely on quality and coverage of pre-trained wav2vec features
- Lack of ablation studies on alternative temporal compression strategies beyond downsampling

## Confidence
- **High confidence** in overall framework validity: SSL-to-lightweight-mapping approach is well-established in related work
- **Medium confidence** in specific architectural choices: Depth-4 optimum empirically validated but alternative configurations unexplored
- **Medium confidence** in generalization claims: Strong cross-dataset performance but not tested on out-of-distribution degradation types

## Next Checks
1. **Parameter sensitivity analysis**: Systematically vary channel dimensions while monitoring parameter count to identify configurations achieving 1,574 parameters and reproducing BVCC baseline performance.

2. **SSL feature robustness test**: Evaluate SALF-MOS performance when fed with wav2vec features from out-of-distribution audio (telephony bandwidth, severe compression artifacts) to quantify frozen-feature limitations.

3. **Temporal compression ablation**: Replace downsampling layers with alternative temporal compression methods (global pooling, attention-based pooling) while keeping same SSL features to isolate contribution of downsampling strategy.