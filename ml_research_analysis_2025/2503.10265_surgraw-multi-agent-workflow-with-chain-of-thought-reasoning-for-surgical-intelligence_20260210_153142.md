---
ver: rpa2
title: 'SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical
  Intelligence'
arxiv_id: '2503.10265'
source_url: https://arxiv.org/abs/2503.10265
tags:
- surgical
- chain
- reasoning
- action
- surgraw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SurgRAW addresses the challenge of integrating Vision-Language
  Models (VLMs) into surgical intelligence, which is hindered by hallucinations, domain
  knowledge gaps, and limited understanding of task interdependencies. The proposed
  solution is a Chain-of-Thought (CoT)-driven multi-agent framework that employs specialized
  CoT prompts across five tasks: instrument recognition, action recognition, action
  prediction, patient data extraction, and outcome assessment.'
---

# SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence

## Quick Facts
- arXiv ID: 2503.10265
- Source URL: https://arxiv.org/abs/2503.10265
- Reference count: 34
- 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures

## Executive Summary
SurgRAW addresses the challenge of integrating Vision-Language Models (VLMs) into surgical intelligence, which is hindered by hallucinations, domain knowledge gaps, and limited understanding of task interdependencies. The proposed solution is a Chain-of-Thought (CoT)-driven multi-agent framework that employs specialized CoT prompts across five tasks: instrument recognition, action recognition, action prediction, patient data extraction, and outcome assessment. The framework uses hierarchical orchestrators to direct tasks to specialized VLM agents, incorporates Retrieval-Augmented Generation (RAG) for domain knowledge, and introduces a panel discussion mechanism for logical consistency. SurgRAW achieves a 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures, demonstrating state-of-the-art performance in surgical scene understanding.

## Method Summary
SurgRAW is a zero-shot, multi-agent framework that processes surgical images through specialized Chain-of-Thought prompts and a hierarchical orchestration system. The Department Coordinator classifies queries as Visual-Semantic or Cognitive-Inference, routing them to appropriate Department Heads. Each head manages specialized agents with CoT templates for tasks like instrument recognition and action prediction. The system incorporates RAG for domain knowledge retrieval and uses a panel discussion mechanism where agents cross-check predictions against a knowledge graph. The framework operates on SurgCoTBench, a dataset of 2,277 frames from 12 robotic procedures, evaluating performance through multiple-choice question accuracy.

## Key Results
- 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures
- Achieves 60.49% overall accuracy on SurgCoTBench
- Outperforms both zero-shot VLM baselines and state-of-the-art CoT approaches

## Why This Works (Mechanism)

### Mechanism 1: Specialized Chain-of-Thought (CoT) Prompting
Structured, domain-specific CoT prompts appear to mitigate hallucinations better than self-generated or generic CoT by enforcing explicit reasoning steps. Instead of allowing the VLM to generate reasoning steps autonomously, the system injects manually designed prompts that force the model to decompose the problem through sequential reasoning steps. This constrains the solution space to logical, surgically relevant deductions. The core assumption is that the VLM possesses sufficient latent visual-semantic capability to follow strict logical constraints if guided. Evidence shows that SurgRAW's specialized CoT outperforms both zero-shot and self-generated CoT approaches on surgical benchmarks. The mechanism may fail if the surgical scene contains instruments entirely outside the VLM's pre-training distribution.

### Mechanism 2: Multi-Agent Panel Discussion
Inter-agent verification via a "panel discussion" likely enhances consistency by cross-checking predictions against a knowledge graph. Specialized agents generate preliminary findings, then an "Action Evaluator" agent checks these against hardcoded Knowledge Graph constraints and rubrics for coherence. If inconsistencies are found, agents refine their outputs. The core assumption is that the encoded "Ground Truth" is complete and unambiguous, and the Evaluator can reliably detect logical misalignments. While specific surgical VLM panel discussion mechanisms are limited in literature, multi-agent verification has shown general validity in LLM workflows. The mechanism may reinforce errors if agents share common biases or misinterpret scenes identically.

### Mechanism 3: Retrieval-Augmented Generation (RAG) for Cognitive Gaps
RAG enables the system to perform cognitive-inference tasks requiring explicit procedural knowledge not present in the visual frame. For knowledge-intensive tasks, the system queries external medical repositories (MedlinePlus), with retrieved text injected into the prompt to allow cross-referencing with established medical standards. The core assumption is that semantic search retrieves relevant, concise context fitting within the VLM's context window. Standard RAG mechanisms support this approach, though surgical-specific application is novel. The mechanism may fail if retrieval returns contradictory or low-quality medical text, confusing the reasoning chain.

## Foundational Learning

- **Vision-Language Models (VLMs) & Hallucinations**: VLMs often "invent" details or lack specific domain knowledge. Understanding these failure modes clarifies why CoT and RAG are necessary. Quick check: Can you explain why a standard VLM might misidentify a surgical tool even if it "sees" it correctly?

- **Chain-of-Thought (CoT) Reasoning**: This is the core reasoning driver. Distinguish between "Zero-shot" (direct answer) and "CoT" (step-by-step justification) to understand latency and accuracy tradeoffs. Quick check: Does this system generate its own reasoning steps or follow a predefined reasoning chain? (Answer: Predefined/Specialized).

- **Agentic Orchestration**: The system uses a hierarchy of "agents" (wrappers around VLMs with specific prompts). Understanding how a "Department Coordinator" routes tasks to "Specialists" is key to debugging. Quick check: If a user asks about patient age, which "Department" handles thatâ€”Visual-Semantic or Cognitive-Inference?

## Architecture Onboarding

- **Component map**: Input (Surgical Frame + Text Query) -> Department Coordinator -> VS Dept Head/CI Dept Head -> Task-specific Agents (Action Interpreter, Instrument Specialist, Outcome Analyst, etc.) -> Knowledge Graph/Vector Database -> Output (Final Answer + Reasoning Chain)

- **Critical path**: 1) Query classification (Visual vs. Cognitive), 2) Prompt construction (Injecting CoT template), 3) (Optional) RAG lookup (if Cognitive), 4) VLM Inference, 5) (Optional) Panel Evaluation (if Visual/Action)

- **Design tradeoffs**: Latency vs. Accuracy (multi-agent "panel discussion" increases inference time for accuracy gains), Zero-shot vs. Fine-tuning (zero-shot chosen to avoid massive data requirements)

- **Failure signatures**: Routing Loop (misclassified queries go to wrong agent), Context Overflow (RAG retrieves excessively long documents diluting visual prompt)

- **First 3 experiments**: 1) Baseline Sanity Check (run GPT-4o on SurgCoTBench without CoT/agents to reproduce low baseline), 2) Ablation on CoT (disable specialized prompts to verify performance drop), 3) Panel Discussion Stress Test (feed ambiguous images to test consensus correction vs. hallucination)

## Open Questions the Paper Calls Out

- **Generalization to new procedures**: Can SurgRAW generalize to surgical procedures beyond prostatectomy and lobectomy, and to what extent does performance degrade when applied to unseen procedure types? The conclusion states future work will focus on expanding the dataset to improve generalization. Evaluation on diverse benchmarks spanning additional procedures with per-procedure accuracy reporting would resolve this.

- **Real-time performance**: Can the multi-agent framework achieve sufficiently low latency for real-time intraoperative decision support? The conclusion identifies optimizing real-time performance for surgical assistance as future work. End-to-end latency measurements per query, comparison against surgical timeline constraints, and accuracy-speed trade-offs would resolve this.

- **Dynamic temporal reasoning**: How can CoT prompting be extended to dynamic, temporal reasoning across video sequences rather than isolated frame-level analysis? The conclusion lists exploring CoT prompting for dynamic reasoning as a future direction. A temporal extension of SurgCoTBench with video-segment annotations and experiments comparing frame-level vs. video-level CoT reasoning would resolve this.

## Limitations
- Reliance on GPT-4o API without fine-tuning makes system expensive and potentially rate-limited
- Knowledge Graph and RAG corpus not publicly available, limiting reproducibility of panel discussion mechanism
- Performance may degrade on procedures outside the 12 robotic cases in SurgCoTBench due to tailored prompts and knowledge base

## Confidence
- **High confidence**: 29.32% accuracy improvement claim (directly measured against SurgCoTBench baseline)
- **Medium confidence**: Mechanism explanations for CoT prompting and panel discussion (logically sound but not empirically isolated in ablation studies)
- **Low confidence**: Generalizability claim to other surgical procedures (validation limited to 12 specific robotic cases)

## Next Checks
1. **Ablation study on CoT prompts**: Systematically disable specialized CoT templates and replace with generic prompts to quantify the exact contribution of structured reasoning to the 29.32% improvement.

2. **Cross-procedure generalization test**: Evaluate SurgRAW on surgical videos from different institutions or procedures not represented in SurgCoTBench to assess domain transfer capabilities.

3. **Latency and cost analysis**: Measure the total inference time and API costs for processing a 30-minute surgical video to establish practical deployment constraints.