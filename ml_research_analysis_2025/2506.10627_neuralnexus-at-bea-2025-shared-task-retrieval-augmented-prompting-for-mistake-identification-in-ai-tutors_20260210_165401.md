---
ver: rpa2
title: 'NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake
  Identification in AI Tutors'
arxiv_id: '2506.10627'
source_url: https://arxiv.org/abs/2506.10627
tags:
- tutor
- student
- response
- mistake
- pedagogical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a retrieval-augmented few-shot prompting\
  \ approach for identifying mistakes in AI tutor responses. The method retrieves\
  \ semantically similar examples from a training set and uses GPT-4o with structured\
  \ prompts to judge whether a tutor correctly identifies a student\u2019s error in\
  \ mathematical reasoning."
---

# NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors

## Quick Facts
- **arXiv ID**: 2506.10627
- **Source URL**: https://arxiv.org/abs/2506.10627
- **Reference count**: 6
- **Primary result**: Retrieval-augmented few-shot prompting with GPT-4o achieved strict Macro F1 of 0.584 and lenient accuracy of 0.897 on the BEA 2025 Mistake Identification task.

## Executive Summary
NeuralNexus introduced a retrieval-augmented few-shot prompting approach for identifying mistakes in AI tutor responses at the BEA 2025 Shared Task. The method retrieves semantically similar examples from a training set and uses GPT-4o with structured prompts to judge whether a tutor correctly identifies a student's error in mathematical reasoning. This approach outperformed three baselines, including ensemble ML models, token-level attention models, and frozen sentence-transformers, demonstrating that combining example-driven prompting with LLM reasoning effectively captures nuanced pedagogical feedback.

## Method Summary
The system employs retrieval-augmented few-shot prompting with GPT-4o. Training dialogues are embedded using OpenAI embeddings and stored in ChromaDB. At inference, the system retrieves top-k semantically similar examples and integrates them into a structured prompt template. GPT-4o processes the prompt to generate a judgment, which is parsed using a PydanticOutputParser enforcing a strict schema. The method was compared against three baselines: an ensemble of ML models on pooled embeddings, a token-level attention model, and a frozen sentence-transformer with an MLP classifier.

## Key Results
- Retrieval-augmented approach achieved strict Macro F1 of 0.584 and lenient accuracy of 0.897
- Outperformed frozen MPNet (Approach 3) by +0.9% strict accuracy
- Ranked 37th on the official BEA 2025 leaderboard
- Demonstrated effectiveness of combining retrieval with LLM reasoning for pedagogical feedback classification

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Grounded Contextualization
The system retrieves semantically similar dialogue examples from a vector database to condition the LLM's judgment. This provides task-specific reference points through in-context learning, aligning the model's reasoning with the dataset's annotation schema. The core assumption is that semantic similarity in embedding space correlates with similar judgment logic requirements.

### Mechanism 2: Generative Judgment via Schema Constraints
Rather than traditional classification, the system frames the task as generative judgment with strict output parsing. GPT-4o generates text labels ("Yes", "No", "To some extent") constrained by a Pydantic schema. This leverages LLM reasoning while ensuring machine-readable consistency through enforced taxonomies.

### Mechanism 3: Token-Level History-Response Interaction
The baseline token-level attention model treats the response as Query and history as Key/Value in a multi-head attention layer. This captures cross-attention dependencies between conversation history and specific tutor responses, allowing the model to weigh historical turns differently than simple vector pooling.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG) & Vector Search
  - **Why needed**: Core of the winning approach (Approach 4). Understanding how to chunk dialogue, embed it, and retrieve via similarity search is essential.
  - **Quick check**: How does the "top-k" retrieval parameter influence context window and potential noise in the final prompt?

- **Concept**: Prompt Engineering & Structured Output Parsing
  - **Why needed**: The system relies on specific prompt templates and parsers to convert LLM text into valid labels.
  - **Quick check**: Why is defining "To some extent" explicitly in the prompt crucial for the "Lenient" evaluation metric?

- **Concept**: Transformer Attention Mechanisms (Key/Query/Value)
  - **Why needed**: Understanding why the baseline token-level attention model worked better than simple pooling but worse than the LLM.
  - **Quick check**: In Approach 2, why is the response treated as Query and history as Key/Value, rather than the other way around?

## Architecture Onboarding

- **Component map**: Pre-processor -> Retriever (OpenAI Embeddings + ChromaDB) -> Prompt Constructor -> Judge (GPT-4o) -> Parser (PydanticOutputParser)
- **Critical path**: The retrieval step is critical. If retrieved examples are not semantically analogous to the test case, the LLM receives poor context regardless of prompt quality.
- **Design tradeoffs**:
  - Cost vs. Nuance: GPT-4o is computationally expensive but captures +0.9% strict accuracy gain over frozen MPNet
  - Complexity vs. Interpretability: ML Ensemble is interpretable but low performance; LLM approach is high performance but acts as a "black box" judge
- **Failure signatures**:
  - Parsing Errors: LLM output doesn't match Pydantic schema
  - Context Drift: Retrieved examples contradict each other, confusing the LLM
  - Rank Collapse: Poor performance on "Lenient" if prompt definitions for "To some extent" are ambiguous
- **First 3 experiments**:
  1. Retrieval Ablation: Run pipeline with k=0, k=1, and k=5 examples to measure retrieval contribution
  2. Parser Stress Test: Input ambiguous dialogues to verify schema adherence and hallucination prevention
  3. Baseline Comparison: Re-run frozen MPNet on same pre-processed data to confirm performance delta

## Open Questions the Paper Calls Out

- **Open Question 1**: Does adaptive example selection improve performance over static embedding similarity for out-of-distribution mathematical dialogues?
  - Basis: Authors note retrieval is based solely on static embedding similarity and suggest exploring more adaptive example selection
  - Evidence needed: Comparison of Macro F1 scores between static retriever and dynamic retriever on out-of-distribution samples

- **Open Question 2**: Does incorporating explicit dialogue state tracking improve consistency of mistake identification in multi-turn interactions?
  - Basis: System lacks "Multi-Turn Dialogue Modeling" and suggests dialogue state tracking as solution
  - Evidence needed: Implementation of memory-augmented variant and evaluation on longitudinal dialogue consistency metrics

- **Open Question 3**: Does extending output schema to include confidence scores or textual rationales improve reliability of "To some extent" classification?
  - Basis: Current "Simplified Output Format" restricts to single label; authors suggest extending to include rationales or confidence scores
  - Evidence needed: Modified parser requesting likelihood score or justification alongside label, followed by correlation analysis with human uncertainty

## Limitations
- Retrieval configuration ambiguity: Paper doesn't specify k value or similarity threshold used
- Computational trade-offs: No cost or latency comparisons with ML baselines provided
- Generalizability concerns: Performance on out-of-distribution dialogues or different pedagogical domains unknown

## Confidence

- **High Confidence**: Retrieval-augmented approach (Approach 4) outperformed three baselines on BEA 2025 test set with strict Macro F1 of 0.584 and lenient accuracy of 0.897
- **Medium Confidence**: Mechanism claims about RAG grounding and generative judgment are supported by internal comparisons but lack external validation
- **Low Confidence**: Token-level attention baseline's relative performance and architectural choices are only internally justified without ablation studies

## Next Checks
1. **Retrieval Ablation Study**: Systematically vary k (0, 1, 3, 5 examples) to quantify retrieval contribution versus LLM's zero-shot capability
2. **Cross-Domain Evaluation**: Test pipeline on different pedagogical dataset to assess domain transfer and potential overfitting
3. **Cost-Performance Analysis**: Measure inference time and API costs of GPT-4o versus frozen MPNet baseline to determine if performance gain justifies computational expense