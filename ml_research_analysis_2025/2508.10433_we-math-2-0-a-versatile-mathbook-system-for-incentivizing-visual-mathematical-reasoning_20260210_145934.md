---
ver: rpa2
title: 'We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical
  Reasoning'
arxiv_id: '2508.10433'
source_url: https://arxiv.org/abs/2508.10433
tags:
- reasoning
- knowledge
- arxiv
- mathematical
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces WE-MATH 2.0, a unified framework to improve\
  \ multimodal large language models\u2019 (MLLMs) visual mathematical reasoning through\
  \ a structured knowledge system, model-centric data space modeling, and a two-stage\
  \ reinforcement learning (RL) paradigm. The authors construct a five-level MathBook\
  \ Knowledge System with 491 knowledge points and 1,819 fundamental principles, develop\
  \ two datasets (MathBook-Standard and MathBook-Pro) with comprehensive annotations\
  \ and progressive difficulty modeling, and propose MathBook-RL, a two-stage RL framework\
  \ that combines cold-start fine-tuning and dynamic scheduling RL."
---

# We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning

## Quick Facts
- arXiv ID: 2508.10433
- Source URL: https://arxiv.org/abs/2508.10433
- Reference count: 40
- Primary result: MathBook-RL achieves competitive performance on four widely-used benchmarks and strong results on MathBookEval, demonstrating improved generalization and robustness.

## Executive Summary
This paper introduces WE-MATH 2.0, a unified framework to improve multimodal large language models' (MLLMs) visual mathematical reasoning through a structured knowledge system, model-centric data space modeling, and a two-stage reinforcement learning (RL) paradigm. The authors construct a five-level MathBook Knowledge System with 491 knowledge points and 1,819 fundamental principles, develop two datasets (MathBook-Standard and MathBook-Pro) with comprehensive annotations and progressive difficulty modeling, and propose MathBook-RL, a two-stage RL framework that combines cold-start fine-tuning and dynamic scheduling RL. Experiments show that MathBook-RL achieves competitive performance on four widely-used benchmarks and strong results on MathBookEval, demonstrating improved generalization and robustness.

## Method Summary
WE-Math 2.0 introduces a two-stage MathBook-RL framework: (1) Cold-Start SFT on 1K samples with knowledge-oriented natural language CoT, followed by (2a) Pre-aligned RL on 5.8K one-problem-multi-image variants using mean-based group rewards, and (2b) Dynamic Scheduling RL on 4K MathBook-Pro samples with GRPO-based progressive curriculum along step/visual/contextual complexity. The framework is built on a five-level MathBook Knowledge System with 491 knowledge points and 1,819 principles, organized into seed problems, variant expansions, and 3D difficulty modeling. Base model: Qwen2.5-VL-7B-Instruct. Evaluation via VLMEvalKit with GPT-4o as judge.

## Key Results
- MathBook-RL achieves competitive performance on MathVista, MathVision, MathVerse, and We-Math benchmarks
- Strong results on MathBookEval demonstrate improved generalization across 1,000 problems spanning 3 reasoning levels, 4 domains, and 13 subdomains
- Natural language CoT outperforms structured format; minimal SFT (1K) suffices for strong performance
- Dynamic difficulty scheduling with incremental learning enables progressive mastery across orthogonal dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured knowledge supervision via a hierarchical knowledge system improves MLLM reasoning generalization by providing explicit knowledge point annotations and principle-level mappings.
- Mechanism: The MathBook Knowledge System organizes 491 knowledge points into a 5-level hierarchy with 1,819 fundamental principles, enabling step-level mapping of CoT solutions to specific knowledge points. This provides fine-grained supervision signals that guide reasoning rather than memorization.
- Core assumption: MLLMs exhibit uneven performance across math subfields due to incomplete knowledge coverage in existing datasets; explicit knowledge-to-problem mapping improves conceptual understanding.
- Evidence anchors:
  - [abstract] "construct a five-level MathBook Knowledge System with 491 knowledge points and 1,819 fundamental principles"
  - [section 3.1] "mapping M1: qj → (ki1, ki2, ...) forming step-level solution paths; mapping M2: ki → {pi1, ..., pimi}"
  - [corpus] Weak direct validation; neighbor papers (CogFlow, DentalGPT) similarly emphasize knowledge internalization but do not independently verify this specific hierarchy design.
- Break condition: If downstream tasks require knowledge domains outside the 491-point system, or if the knowledge-to-problem mapping has annotation errors exceeding ~10%, benefits may degrade.

### Mechanism 2
- Claim: Average-reward learning across knowledge-principle-aligned variants enforces consistent performance and reduces overfitting to specific problem formulations.
- Mechanism: During Pre-aligned RL, the model is trained on DImgVar subsets where each group contains multiple visual variants of the same knowledge principle. The reward r = (1/m) * Σr(t) averages across variants, incentivizing robust understanding rather than surface-pattern matching.
- Core assumption: Consistent performance across visual/syntactic variants of the same underlying principle indicates genuine knowledge mastery.
- Evidence anchors:
  - [section 4.2] "utilize the DImgVar subset... adopt a mean-based reward function: r = (1/m) * Σr(t)"
  - [section 4.2] "integrates rewards across all problems corresponding to the same knowledge principle"
  - [corpus] No direct external validation; Diversity-Incentivized Exploration paper discusses exploration in RL but does not address mean-reward mechanisms specifically.
- Break condition: If variant diversity is insufficient (e.g., variants differ only cosmetically), the mechanism may not generalize; requires m ≥ 3 meaningful variants per principle.

### Mechanism 3
- Claim: Dynamic difficulty scheduling with incremental learning enables progressive mastery across the three-dimensional difficulty space.
- Mechanism: Training follows a curriculum trajectory x0 → ϕs(x0) → ϕs◦ϕv(x0) → ... When the model fails at transition x → ϕ(x), it receives targeted incremental samples Δ(x, ϕ) isolating the new complexity before retrying—either Knowledge Increment Scheduling (auxiliary problems for new knowledge points) or Modality Increment Scheduling (problems isolating visual/contextual complexity).
- Core assumption: Difficulty can be orthogonally decomposed into step complexity (ϕs), visual complexity (ϕv), and contextual complexity (ϕc); failure modes are diagnosable and addressable via targeted samples.
- Evidence anchors:
  - [section 3.2.2] "define a three-dimensional difficulty space... variants by varying a single dimension d ∈ {ϕs, ϕv, ϕc}"
  - [section 4.2] "if the model fails on ϕs(x0)... construct Δ(x0, ϕs) comprising auxiliary problems targeting the new knowledge point(s)"
  - [corpus] Pixel Reasoner discusses curiosity-driven RL for pixel-space reasoning but does not validate curriculum-based difficulty scheduling.
- Break condition: If difficulty dimensions are correlated in practice (e.g., step complexity inherently increases visual complexity), the orthogonal decomposition fails; also requires accurate failure diagnosis.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: MathBook-RL uses GRPO (extending PPO) to optimize policy without a separate critic, estimating baseline from group scores across multiple rollouts.
  - Quick check question: Can you explain how GRPO differs from standard PPO in baseline estimation?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The Cold-Start SFT stage trains models on knowledge-oriented CoT; the entire framework assumes step-wise reasoning decomposition is learnable.
  - Quick check question: Given a geometry problem, can you decompose the solution into explicit CoT steps with knowledge-point annotations?

- **Curriculum Learning**
  - Why needed here: Progressive Alignment RL implements curriculum-based training along difficulty trajectories with dynamic scheduling on failure.
  - Quick check question: How would you design a curriculum from seed problems to 7-variant difficulty expansions?

## Architecture Onboarding

- **Component map:**
  - Knowledge System (5-level hierarchy K, principles P, mapping M1/M2) -> Data Pipeline (Seed → Variants → 3D difficulty) -> Training Pipeline (Cold-Start SFT → Pre-aligned RL → Dynamic Scheduling RL) -> Evaluation (MathBookEval)

- **Critical path:**
  1. Construct/fetch MathBook Knowledge System (491 points, 1,819 principles)
  2. Generate seed problems with GeoGebra diagrams + expert revision
  3. Expand variants (one-problem-multi-image, one-image-multi-problem)
  4. Build 3D difficulty variants (7 per seed) for MathBook-Pro
  5. Cold-Start SFT with natural-language CoT (1 epoch, lr=1e-5)
  6. Pre-aligned RL on DImgVar with mean-reward (lr=1e-6)
  7. Dynamic Scheduling RL with GRPO, temperature=1.0, 8 rollouts/sample

- **Design tradeoffs:**
  - Natural language CoT vs. structured step-wise format: Paper finds natural language CoT outperforms structured format (Table 4), likely due to flexibility in reasoning expression.
  - Minimal SFT (1K) vs. larger SFT (10K): Minimal suffices; scaling does not consistently improve results (Table 4).
  - GeoGebra vs. Python-based rendering: GeoGebra provides higher geometric precision and spatial rigor; manually crafted diagrams ensure new, non-duplicated content.

- **Failure signatures:**
  - Memorization over generalization: Model solves training problems but fails on subproblems or same-type variants (Section 1, "Lack of emphasis on reasoning generalization")
  - SFT-only plateau: SFT alone yields marginal gains (M4 in Table 3); RL required for significant improvement
  - Difficulty-scheduling stuck: If incremental sets Δ(x, ϕ) are poorly constructed or insufficient, model repeatedly fails at same curriculum stage

- **First 3 experiments:**
  1. Ablate training stages: Compare M0 (full pipeline) vs. M1–M4 (removing RL-Pre, RL-Dyn, or SFT) on MathVista and We-Math to isolate contribution of each stage
  2. Difficulty dimension analysis: Evaluate model performance across ϕs, ϕv, ϕc dimensions independently on MathBook-Pro variants to validate orthogonal difficulty decomposition
  3. Knowledge coverage test: Train on subset of MathBook-Standard covering only ~50% of knowledge points, then evaluate on MathBookEval to measure generalization to unseen knowledge domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reliance on manually curated GeoGebra diagrams for training limit the model's ability to generalize to "in-the-wild" mathematical imagery?
- Basis in paper: [inferred] The authors emphasize that all training images are "meticulously handcrafted" using GeoGebra to ensure rigor, unlike Python-rendered images.
- Why unresolved: It remains unclear if the "Less is More" efficiency observed is a result of the knowledge system or the synthetic, noise-free nature of the GeoGebra visual inputs.
- What evidence would resolve it: Experiments evaluating the model's robustness when fine-tuned or tested on scanned textbook problems or photographs of hand-drawn diagrams.

### Open Question 2
- Question: Do the three proposed difficulty dimensions (Step, Visual, Contextual) interact non-linearly, and is the fixed-order curriculum (ϕs → ϕv → ϕc) optimal for all knowledge points?
- Basis in paper: [inferred] The paper proposes a fixed progressive alignment path along three dimensions (Step, Visual, Contextual) without ablation on the ordering.
- Why unresolved: A fixed curriculum may fail if, for example, high visual complexity disproportionately increases the effective reasoning depth, necessitating adaptive scheduling.
- What evidence would resolve it: Ablation studies analyzing training dynamics when the order of difficulty dimensions is permuted or randomized.

### Open Question 3
- Question: Can the "Definition-Theorem-Application" paradigm of the MathBook Knowledge System be effectively adapted for domains lacking formal hierarchical structures, such as qualitative sciences?
- Basis in paper: [inferred] The knowledge system is explicitly constructed using a mathematical paradigm, limiting its immediate applicability to other fields.
- Why unresolved: Non-mathematical domains may not decompose cleanly into the 5-level hierarchy or 1,819 principles used here, potentially requiring a different structuring logic.
- What evidence would resolve it: Applying the knowledge system construction pipeline to a non-mathematical dataset (e.g., physics diagrams) and evaluating the semantic consistency of the resulting hierarchy.

## Limitations

- The 491-point knowledge hierarchy and its problem mapping lack independent validation; benefits may degrade if annotation errors exceed ~10%
- Mean-reward averaging across variants assumes sufficient diversity; if variants differ only cosmetically, the mechanism may not generalize
- Orthogonal difficulty decomposition across step/visual/contextual dimensions may fail if dimensions are correlated in practice
- MathBook-Standard/Pro datasets and MathVerify pipeline are not publicly available, making faithful reproduction challenging

## Confidence

- **High confidence**: Stage-wise training architecture (SFT → RL) and competitive benchmark results on MathVista/We-Math
- **Medium confidence**: Knowledge system design improves reasoning generalization; mechanism supported by structure but not independently validated
- **Low confidence**: Dynamic scheduling RL with orthogonal difficulty decomposition—insufficient evidence the dimensions are truly independent in practice

## Next Checks

1. Evaluate knowledge generalization by training on only 50% of MathBook-Standard knowledge points, then testing on full MathBookEval
2. Isolate difficulty dimension contributions by ablating ϕs, ϕv, ϕc in MathBook-Pro variants and measuring performance drops
3. Test variant diversity sufficiency by reducing m from 3+ to 1 per principle and measuring performance degradation