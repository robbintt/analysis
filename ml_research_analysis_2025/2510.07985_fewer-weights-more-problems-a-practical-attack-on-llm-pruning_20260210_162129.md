---
ver: rpa2
title: 'Fewer Weights, More Problems: A Practical Attack on LLM Pruning'
arxiv_id: '2510.07985'
source_url: https://arxiv.org/abs/2510.07985
tags:
- pruning
- wanda
- sparsegpt
- attack
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first attack that exploits LLM pruning
  as a trigger for malicious behavior. The adversary constructs a model that appears
  benign but becomes malicious after pruning.
---

# Fewer Weights, More Problems: A Practical Attack on LLM Pruning

## Quick Facts
- arXiv ID: 2510.07985
- Source URL: https://arxiv.org/abs/2510.07985
- Reference count: 35
- This paper introduces the first attack that exploits LLM pruning as a trigger for malicious behavior, achieving up to 99.5% attack success rate.

## Executive Summary
This paper demonstrates a novel supply-chain attack where LLMs appear benign before pruning but become malicious after pruning. The attack exploits the pruning process itself as a trigger by injecting malicious behavior into parameters unlikely to be pruned while repairing the model using parameters likely to be removed. Through extensive experiments on five models and three attack scenarios, the authors show that standard pruning algorithms can inadvertently activate malicious behavior, revealing a critical deployment-time security gap in model compression workflows.

## Method Summary
The attack employs a three-step process: (1) pre-estimate pruning scores using Wanda scores (magnitude × activation norm) to rank parameters, (2) inject malicious behavior by fine-tuning only the top 50% of parameters on harmful data with KL regularization, and (3) repair the model by fine-tuning only the bottom 1-5% of parameters on benign data to mask the malicious behavior. The attack targets three pruning algorithms (Magnitude, Wanda, SparseGPT) at different sparsity levels (20%, 50%, 2:4) and demonstrates high success rates across five different LLM architectures.

## Key Results
- Attack success rates reach up to 95.7% for jailbreak, 98.7% for over-refusal, and 99.5% for content injection after pruning
- The attack maintains utility preservation with minimal degradation in benchmark performance
- Security-aware calibration alone is insufficient to mitigate the attack, particularly for Wanda and SparseGPT methods
- Pre-pruning attack success rates remain low, demonstrating effective stealth of the malicious payload

## Why This Works (Mechanism)

### Mechanism 1: Injection-Compensation Decoupling
If an adversary can accurately predict which parameters a pruning algorithm will remove, they can bifurcate the model's behavior into "retained" (malicious) and "removed" (benign mask) subsets. The attack performs two-stage fine-tuning: first injecting malicious behavior into high-importance parameters, then repairing by fine-tuning low-importance parameters to cancel malicious outputs. The core assumption is that pruning algorithms rely on predictable importance metrics that can be approximated by the attacker.

### Mechanism 2: Cross-Algorithm Score Correlation
An attack estimated using Wanda scores generalizes to other pruning algorithms because their importance rankings correlate significantly. Rather than requiring separate attacks for every pruning method, the adversary calculates Wanda scores, and the paper shows parameters ranked low by Wanda are also ranked low by SparseGPT and Magnitude pruning. This allows consistent "repair" weight removal across different algorithms.

### Mechanism 3: Low-Dimensional Repair
A very small fraction of parameters (as low as 1%) is sufficient to mask malicious behavior in the unpruned model, and this small fraction is highly likely to be completely removed during standard sparsity targets. The repair phase uses bottom-tier weights that pruning algorithms target with near-certainty (>99.9% in experiments), cleanly stripping the "mask" and leaving the injected malicious core intact.

## Foundational Learning

- **Concept: Unstructured Pruning Metrics (Magnitude, Wanda, SparseGPT)**
  - Why needed here: The attack hinges on specific formulas used to decide which weights die. Without understanding that Wanda uses $|W| \cdot ||X||_2$ or that Magnitude uses $|W|$, one cannot understand how the adversary predicts the "kill set."
  - Quick check question: Why does Wanda generally correlate with Magnitude pruning, but SparseGPT introduces complications via its iterative compensation?

- **Concept: KL Divergence Regularization**
  - Why needed here: The paper uses KL divergence to maintain the model's general utility during the malicious injection and repair phases. This ensures the model doesn't turn into "garbage" text while learning to be evil.
  - Quick check question: In the repair phase, KL divergence forces the model to output what relative to the base model?

- **Concept: Sparsity Levels (20%, 50%, 2:4)**
  - Why needed here: The attack success varies with the aggressiveness of pruning. The ratio of injected vs. repaired parameters must be tuned relative to the expected sparsity level.
  - Quick check question: If a user only prunes 10% of weights (low sparsity), is the attack more or less likely to trigger successfully compared to 50% sparsity, assuming a standard repair set size?

## Architecture Onboarding

- **Component map:** Proxy Estimator -> Mask Generator -> Injection Trainer -> Repair Trainer -> Pruning Trigger
- **Critical path:** The Proxy Estimator is the single point of failure. If the estimated "bottom 1%" of weights does not align with the user's pruning choice, the repair weights survive, and the attack remains dormant.
- **Design tradeoffs:**
  - Stealth vs. Robustness: Increasing α_rep makes masking more robust but risks repair weights not being pruned.
  - Algorithm Specificity: Targeting only Wanda is efficient but might fail on exotic pruners; targeting an ensemble costs more compute but covers more bases.
- **Failure signatures:**
  - Leakage: Malicious outputs appearing before pruning indicates insufficient repair (α_rep too small or learning rate too low).
  - Dud: Malicious outputs not appearing after pruning indicates injection weights were pruned or repair weights were retained.
  - Utility Collapse: Model speaks gibberish; implies injection/repair balance destroyed the language head.
- **First 3 experiments:**
  1. Run Wanda, Magnitude, and SparseGPT on a small model with different calibration sets. Plot the overlap of the bottom 1% of weights to verify the "Proxy Estimator" assumption.
  2. Inject a simple trigger (e.g., "say McDonald's") and mask it using 1%, 5%, and 10% of weights. Prune at 50% and measure pre- vs. post-pruning ASR to find the sweet spot.
  3. Replicate the jailbreak attack on a 7B model. Verify the model refuses harmful prompts unpruned, but complies after running `vllm` pruning.

## Open Questions the Paper Calls Out

### Open Question 1
How does the inherent complexity or semantic nature of different malicious behaviors affect the required "repair" ratio (α_rep) and the overall success rate of pruning-activated attacks? The authors state this as an interesting avenue for future work, noting the current study is limited to three scenarios with varying sensitivity to α_rep but no generalizable rule connecting task complexity to attack mechanics.

### Open Question 2
Can a robust calibration pipeline be developed that effectively detects and neutralizes pruning-activated backdoors without causing significant utility degradation? The authors note that security-aware calibration by itself is insufficient and leaves methods for better mitigation strategies as an important open question, showing that while security-aware calibration helps for SparseGPT, it fails for Wanda and consistently results in utility drops.

### Open Question 3
Is the pruning-activated attack mechanism effective against structured pruning methods, where weights are removed in groups rather than individually? The paper focuses exclusively on unstructured pruning because it allows high sparsity without retraining, but the attack relies on estimating specific parameter scores while structured pruning removes entire blocks/neurons, which might disrupt the precise "repair" mechanism.

## Limitations

- The attack's success depends critically on accurate prediction of pruning scores, which may degrade with different model architectures or training datasets
- The effectiveness relies on the calibration dataset producing similar activation norms to the end-user's deployment scenario, making it vulnerable to distribution shifts
- The attack targets instruction-tuned models and may not generalize to base models or models with different architectural choices

## Confidence

- **High Confidence:** The core attack mechanism (injection-repair decoupling) is well-validated across five models and three attack scenarios with reproducible empirical results
- **Medium Confidence:** The cross-algorithm generalization assumption (Wanda proxy working for Magnitude and SparseGPT) is supported by correlation analysis but needs testing against more diverse pruning methods
- **Medium Confidence:** The low-dimensional repair claim (1% sufficient) is empirically validated but may vary with model size, task complexity, and pruning aggressiveness

## Next Checks

1. **Distribution Shift Test:** Evaluate the attack's success when the user's pruning calibration data differs substantially from C4. Measure the correlation decay between Wanda scores and actual pruning outcomes.

2. **Pruning Method Stress Test:** Apply the attack to models pruned using algorithms with orthogonal criteria (e.g., structured pruning, lottery ticket-based pruning, or quantization-aware pruning) to test the limits of the proxy estimation assumption.

3. **Attack Robustness Analysis:** Systematically vary the repair ratio (α_rep) and injection ratio (α_inj) across a wider range (e.g., 0.1% to 20%) and measure the tradeoff curve between pre-pruning stealth and post-pruning attack success. Identify the Pareto-optimal operating points.