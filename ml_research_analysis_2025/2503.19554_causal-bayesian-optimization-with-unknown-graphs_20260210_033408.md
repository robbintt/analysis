---
ver: rpa2
title: Causal Bayesian Optimization with Unknown Graphs
arxiv_id: '2503.19554'
source_url: https://arxiv.org/abs/2503.19554
tags:
- causal
- prior
- where
- graphs
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new method for causal Bayesian optimization
  (CBO) when the causal graph is unknown. It learns a Bayesian posterior over the
  direct parents of the target variable using both observational and interventional
  data, enabling optimization while simultaneously discovering the causal structure.
---

# Causal Bayesian Optimization with Unknown Graphs

## Quick Facts
- arXiv ID: 2503.19554
- Source URL: https://arxiv.org/abs/2503.19554
- Reference count: 40
- Primary result: Proposes a method for CBO when the causal graph is unknown, learning a Bayesian posterior over target variable parents while optimizing

## Executive Summary
This paper addresses the challenge of causal Bayesian optimization (CBO) when the underlying causal graph is unknown. The proposed method learns a Bayesian posterior over the direct parents of the target variable using both observational and interventional data, enabling optimization while simultaneously discovering the causal structure. The approach focuses on intervening only on the direct parents of the target variable, which is theoretically sufficient for optimization under certain assumptions. The method achieves performance competitive with existing baselines that require full knowledge of the causal graph while scaling well to larger graphs.

## Method Summary
The method initializes with a prior over parent sets estimated from observational data using doubly robust causal feature selection. It then iteratively selects interventions, observes outcomes, and updates both the parent posterior and the Gaussian process surrogate model. For linear systems, closed-form posterior updates are derived, while nonlinear cases use random Fourier feature approximations. The surrogate model's mean and variance are computed by marginalizing over the parent posterior distribution. This approach focuses only on the direct parents of the target variable, enabling scalability to larger graphs (up to 100 nodes) without requiring full graph discovery.

## Key Results
- The method learns the direct parents of the target variable with competitive accuracy and F1-scores
- Achieves optimization performance comparable to CBO methods that require known causal graphs
- Scales effectively to larger graphs (up to 100 nodes) while maintaining tractability
- Demonstrates theoretical sufficiency of parent-focused interventions under specific assumptions

## Why This Works (Mechanism)

### Mechanism 1: Posterior Over Direct Parents
The method maintains a Bayesian posterior distribution over possible parent sets of the target variable, updated using interventional data. For linear systems, this has a closed-form solution derived from Bayesian linear regression. The core assumption is that hard interventions on parents are sufficient for optimization when there are no unobserved confounders or spouse variables. The mechanism directly addresses the "unknown graph" gap by focusing only on parent discovery rather than full graph learning.

### Mechanism 2: Iterative Posterior & Surrogate Model Update
The algorithm iteratively updates both the parent posterior and the GP surrogate model using new interventional data. An acquisition function selects interventions based on the current surrogate, then new data updates the posterior over parent sets and GP parameters. The surrogate mean and variance are recomputed by marginalizing over the parent posterior. This simultaneous learning of structure and optimization is enabled by the iterative loop structure.

### Mechanism 3: Scalable Approximation for Nonlinear Relationships
For nonlinear causal mechanisms, the method uses random Fourier features to approximate the GP surrogate, enabling closed-form posterior updates. The approximation error decays exponentially with feature dimension, allowing scalability to larger graphs. By focusing only on parents rather than the full graph, the method avoids the computational complexity of full structure discovery while maintaining performance.

## Foundational Learning

- **Structural Causal Models (SCMs) and Interventions**: Essential for understanding the difference between observational P(Y|X) and interventional P(Y|do(X)) distributions. Quick check: What is the difference between the observational distribution P(Y|X) and the interventional distribution P(Y|do(X=x)) in a simple chain graph X → Y?

- **Bayesian Linear Regression & Gaussian Processes**: Critical for understanding how the surrogate model works and how posterior updates are derived. Quick check: How does the posterior variance of a GP prediction change as you observe more data points near the query point?

- **Bayesian Posterior Updates & Bootstrapping**: Necessary for understanding how the parent set posterior is updated and how the initial prior is established. Quick check: If you have a uniform prior over three possible parent sets {g1, g2, g3}, how would observing a data point inconsistent with g1 affect the posterior probabilities of all three sets?

## Architecture Onboarding

- **Component map**: Initialization Phase (observational data → doubly robust feature selection → initial prior) → Iterative Optimization Loop (Parent Posterior Module → Surrogate Model → Acquisition Function Optimizer) → Interventional Data → Back to Parent Posterior Module

- **Critical path**: Correctly establishing the initial parent set prior from observational data and the iterative update of the parent posterior using interventional data. Errors in posterior updates directly corrupt the surrogate model and acquisition function.

- **Design tradeoffs**: Computational tractability vs model fidelity by focusing only on direct parents, sacrificing ability to handle non-parent context variables. Random Fourier feature approximation introduces error for closed-form updates vs exact GP computation.

- **Failure signatures**: Convergence to suboptimal Y* (incorrect parent set), no reduction in average outcome Ȳ (poor acquisition function), performance worse than wrong-graph baseline (posterior update bug).

- **First 3 experiments**:
  1. Sanity Check: Implement linear case on small graph with known parents to verify parent identification and optimization performance.
  2. Scalability Stress Test: Run on Erdos-Renyi graphs of increasing size (10, 20, 50, 100 nodes) to confirm scalability claims.
  3. Ablation on Initial Prior: Compare performance with informed, uniform, and misspecified priors to quantify robustness.

## Open Questions the Paper Calls Out
- Can the framework be extended to optimize soft or functional interventions rather than solely hard interventions?
- How does the presence of unobserved confounders affecting the target variable impact the optimality of the parent-based approach?
- Can the method maintain tractability and convergence properties without relying on doubly robust feature selection for the initial prior?

## Limitations
- Restricted to hard interventions, excluding scenarios requiring soft or functional interventions
- Assumes no unobserved confounders or spouse variables between Y and input features
- Relies on doubly robust causal feature selection which is referenced but not fully specified

## Confidence
- Mechanism 1 (Sufficiency of parent focus): Medium - Theoretical proof assumes idealized conditions; empirical validation limited to synthetic graphs
- Mechanism 2 (Iterative learning): High - Algorithmic structure clearly specified and follows standard Bayesian optimization principles
- Mechanism 3 (Scalability via approximation): Medium - Exponential error decay theoretical; empirical scaling shows performance but not approximation quality

## Next Checks
1. Test the method on graphs where spouse variables are non-empty (sp_Y ≠ ∅) to quantify performance degradation.
2. Compare random Fourier feature approximation error against exact GP solution on small graphs where full computation is tractable.
3. Evaluate robustness to initial prior quality by systematically varying doubly robust feature selection parameters and measuring impact on final optimization performance.