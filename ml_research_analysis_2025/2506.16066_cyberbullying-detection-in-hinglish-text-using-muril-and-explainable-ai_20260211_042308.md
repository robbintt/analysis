---
ver: rpa2
title: Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI
arxiv_id: '2506.16066'
source_url: https://arxiv.org/abs/2506.16066
tags:
- detection
- cyberbullying
- dataset
- code-mixed
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cyberbullying detection in
  Hinglish (Hindi-English code-mixed) text, where existing systems designed for monolingual
  text struggle with the linguistic complexity of code-switching. The study proposes
  a framework using the Multilingual Representations for Indian Languages (MURIL)
  architecture, which is specifically designed for Indian languages and code-mixed
  text.
---

# Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI

## Quick Facts
- **arXiv ID**: 2506.16066
- **Source URL**: https://arxiv.org/abs/2506.16066
- **Reference count**: 33
- **Primary result**: MURIL-based approach achieves 1.36-13.07 percentage point accuracy improvements over RoBERTa and IndicBERT on six Hinglish cyberbullying datasets

## Executive Summary
This paper addresses the challenge of cyberbullying detection in Hinglish (Hindi-English code-mixed) text, where existing systems designed for monolingual text struggle with the linguistic complexity of code-switching. The study proposes a framework using the Multilingual Representations for Indian Languages (MURIL) architecture, which is specifically designed for Indian languages and code-mixed text. The approach includes explainability features through attribution analysis and cross-linguistic pattern recognition. Evaluation on six benchmark datasets shows that the MURIL-based approach outperforms existing multilingual models like RoBERTa and IndicBERT, with accuracy improvements ranging from 1.36 to 13.07 percentage points.

## Method Summary
The approach uses a MURIL-based transformer architecture with selective layer freezing (embedding and first transformer layer frozen) and a custom classification head with three hidden layers (512, 256, 128 neurons). The framework includes specialized preprocessing for code-mixed content including language identification, transliteration normalization, and emoji standardization. Training employs 5-fold cross-validation with early stopping optimization for macro F1-score. The model achieves accuracies of 86.97% on Bohra, 84.62% on BullyExplain, 86.03% on BullySentemo, 75.41% on Kumar, 83.92% on HASOC 2021, and 94.63% on Mendeley datasets.

## Key Results
- MURIL-based approach outperforms RoBERTa and IndicBERT by 1.36-13.07 percentage points across six datasets
- Accuracy improvements: Bohra (86.97%), BullyExplain (84.62%), BullySentemo (86.03%), Kumar (75.41%), HASOC 2021 (83.92%), Mendeley (94.63%)
- Ablation studies show selective layer freezing and specialized preprocessing each contribute ~0.8 percentage points to performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MURIL's explicit pretraining on code-mixed and transliterated data enables superior handling of Hindi-English code-switching patterns compared to models trained primarily on monolingual or translated corpora.
- Mechan: During pretraining, MURIL learned representations for 17 Indian languages plus their romanized/transliterated variants, creating cross-lingual alignment that directly transfers to Hinglish detection tasks without requiring translation intermediaries.
- Core assumption: Code-mixed text follows learnable patterns that transfer across similar linguistic combinations, rather than being purely idiosyncratic.
- Evidence anchors: [abstract] "MURIL architecture, which is specifically designed for Indian languages and code-mixed text... outperforms existing multilingual models like RoBERTa and IndicBERT"

### Mechanism 2
- Claim: Selective layer freezing (embedding + first 1-2 transformer layers) preserves foundational multilingual representations while allowing task-specific adaptation in higher layers.
- Mechan: Lower layers capture language-agnostic syntactic patterns and cross-lingual subword relationships learned during large-scale pretraining; freezing prevents catastrophic overwriting during fine-tuning on smaller code-mixed datasets while upper layers adapt to cyberbullying-specific semantic patterns.
- Core assumption: Foundational multilingual representations in lower layers are sufficiently robust and transferable that they don't require task-specific updating.
- Evidence anchors: [Section 5.3, Table 14] "selectively freezing the embedding layer and first two transformer layers while fine-tuning the remaining layers achieves optimal performance (82.35% accuracy)"

### Mechanism 3
- Claim: Specialized preprocessing pipeline for code-mixed content (language identification, transliteration normalization, emoji standardization) provides cumulative performance improvements by reducing input variance.
- Mechan: Normalizing romanized Hindi variants to consistent representations reduces vocabulary fragmentation, enabling the model to learn from aggregated signal rather than treating spelling variations as distinct tokens.
- Core assumption: Code-mixed text contains systematic noise patterns that preprocessing can reduce without losing discriminative signal.
- Evidence anchors: [Section 5.3, Table 14] Preprocessing components contributed +0.79 percentage points

## Foundational Learning

- **Concept: Code-switching vs. Code-mixing**
  - Why needed here: Understanding that Hinglish involves intra-sentential language alternation (not just translation) is essential for grasping why monolingual models fail and why specialized tokenization matters.
  - Quick check question: Can you explain why "Tu bohot stupid hai" would confuse a model trained only on English or only on Hindi?

- **Concept: Transfer Learning with Layer Freezing**
  - Why needed here: The paper's performance gains depend critically on which layers are frozen vs. fine-tuned; understanding this tradeoff is necessary for reproducing or extending results.
  - Quick check question: Why might freezing too many layers hurt performance, and why might freezing too few hurt generalization?

- **Concept: Attribution-based Explainability**
  - Why needed here: The paper uses gradient-based attribution to identify which words contribute to cyberbullying predictions; understanding this helps interpret Tables 12-13 and assess model reliability.
  - Quick check question: If "gandu" has attribution 0.72, what does that tell you about how the model reaches its decision?

## Architecture Onboarding

- **Component map:**
  Raw Hinglish Text → [Input Processing] → Preprocessing → MURIL Tokenizer (max_length=128) → [MURIL Transformer] 12 layers (Layers 0-1 frozen, Layers 2-11 fine-tuned) → [CLS] Token Representation (768-dim) → [Classification Head] Dropout(0.16) → Linear(768→512) → LayerNorm → ReLU → Dropout → Linear(512→256) → LayerNorm → ReLU → Dropout → Linear(256→128) → LayerNorm → ReLU → Dropout → Linear(128→2) → Softmax → [Bullying / Non-bullying]

- **Critical path:** The tokenization-to-CLS-extraction path determines representation quality. If tokenization mishandles romanized Hindi (e.g., over-fragmenting common words), downstream classification degrades regardless of head design.

- **Design tradeoffs:**
  - 3 hidden layers vs. 2 or 4: Paper shows 3 layers optimal (82.35% acc); 4 layers caused ~0.37pp degradation due to overfitting on limited code-mixed data.
  - Freeze depth: Freezing layers 0-2 balances stability and adaptability; freezing all layers drops to 78.34% accuracy.
  - Dropout rate 0.16: Tuned for regularization without excessive information loss; paper doesn't ablate this explicitly (Assumption).

- **Failure signatures:**
  - Context-insensitive lexical detection: Model flags offensive words directed at abstract concepts (e.g., "kismat kutti hai" — "luck is a bitch") as bullying when target is non-human.
  - Cross-linguistic sarcasm: Sarcastic praise using English markers in Hindi discourse ("tu bohot smart hai, bilkul Einstein jaise!") missed due to irony spanning language boundaries.
  - Cultural subtext: Personal attacks embedded in political commentary ("tere jaise bewakoof log desh ko barbaad kar rahe hain") require cultural knowledge beyond lexical signal.
  - Confidence miscalibration: High-confidence predictions (>0.95) sometimes wrong; mid-confidence (0.58-0.82) shows highly variable accuracy (0.36-0.47).

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train MURIL, RoBERTa, and IndicBERT on Bohra dataset with identical preprocessing. Verify ~2-4 percentage point MURIL advantage. If not observed, check tokenization settings and freeze configuration.
  2. **Ablate freezing depth:** Systematically test freezing {0, 1, 2, 3} transformer layers on BullyExplain. Plot accuracy vs. freeze depth. Confirm optimal point at layers 0-2 frozen.
  3. **Error analysis on confidence buckets:** Partition test set by prediction confidence (0.5-0.6, 0.6-0.7, etc.). For each bucket, manually inspect false positives/negatives to identify systematic failure patterns specific to your deployment context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of conversational context, user behavioral patterns, and temporal dynamics significantly enhance the detection accuracy of subtle cyberbullying that manifests across multiple interactions rather than single isolated messages?
- Basis in paper: [explicit] The conclusion states that future work involves "investigating the integration of conversational context, user behavioral patterns, and temporal dynamics" to catch subtle forms of cyberbullying.
- Why unresolved: The current framework processes text instances individually (instance-based detection) without modeling the history of interactions between users.
- What evidence would resolve it: Evaluation of a context-aware model on a dataset of conversation threads showing improved F1-scores over the current instance-based MURIL baseline.

### Open Question 2
- Question: How can "cultural knowledge graphs" or context-aware training be effectively utilized to resolve the model's inability to detect cross-linguistic sarcasm and personal attacks disguised as political commentary?
- Basis in paper: [explicit] The discussion and conclusion identify "Cultural Subtext" and "Cross-linguistic Sarcasm" as specific failure patterns, suggesting the development of "culturally-aware detection mechanisms" as a critical advancement.
- Why unresolved: The current model relies primarily on lexical attribution (e.g., detecting words like "stupid") and fails to interpret irony or cultural references where abusive content is implicit.
- What evidence would resolve it: Qualitative and quantitative analysis demonstrating successful classification of the specific failure cases listed in Table 15 (e.g., sarcastic praise like "Einstein").

### Open Question 3
- Question: To what extent does the MURIL-based framework generalize to other multilingual populations and emerging social media platforms outside of the Hinglish (Hindi-English) Twitter and YouTube contexts tested?
- Basis in paper: [explicit] Section 7 explicitly calls for "broadening the scope of the framework to even more multilingual populations and emerging social media spaces."
- Why unresolved: The study is restricted to six specific datasets, primarily comprising Hinglish text from established platforms, which may not represent the linguistic patterns of other language pairs or newer platforms.
- What evidence would resolve it: Benchmark results from the same architecture applied to non-Indo-Aryan code-mixed languages (e.g., Spanish-English or Dravidian mixes) or platforms like TikTok/Instagram.

## Limitations

- **Dataset heterogeneity**: Six datasets span different collection periods, platforms, and annotation guidelines, creating potential domain shift that may inflate performance metrics when models are evaluated on aggregate statistics.
- **Implementation gaps**: Critical hyperparameters remain unspecified, including exact optimizer configuration (learning rate schedule, weight decay), batch size, and precise preprocessing tooling for transliteration normalization.
- **Confidence calibration concerns**: High-confidence predictions (>0.95) sometimes prove incorrect, while mid-confidence ranges (0.58-0.82) show accuracy variability from 0.36 to 0.47, suggesting probability outputs may not reliably indicate prediction certainty.

## Confidence

- **MURIL superiority**: High confidence - well-supported by direct comparative experiments across six datasets with consistent methodology
- **Layer freezing strategy**: Medium confidence - optimal freezing depth appears tuned to this particular task and dataset combination
- **Explainability value**: Low confidence - demonstrates attribution analysis capability but provides limited validation of practical value

## Next Checks

- **Check 1: Dataset-specific validation**: Train and evaluate the MURIL model separately on each of the six datasets rather than aggregating results. Compare per-dataset performance variance to assess whether reported gains reflect genuine model superiority or dataset-specific advantages.
- **Check 2: Confidence calibration testing**: Implement reliability diagrams comparing predicted confidence to observed accuracy across confidence bins. Use Platt scaling or temperature scaling to calibrate outputs, then measure improvement in expected calibration error.
- **Check 3: Ablation of preprocessing components**: Systematically disable each preprocessing component (language identification, transliteration normalization, emoji standardization) in isolation while keeping all other factors constant. Measure individual contribution of each component to final accuracy.