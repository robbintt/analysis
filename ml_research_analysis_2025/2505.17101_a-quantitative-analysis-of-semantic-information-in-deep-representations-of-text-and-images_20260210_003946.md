---
ver: rpa2
title: A quantitative analysis of semantic information in deep representations of
  text and images
arxiv_id: '2505.17101'
source_url: https://arxiv.org/abs/2505.17101
tags:
- information
- imbalance
- representations
- deepseek-v3
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops a quantitative framework to locate and characterize\
  \ semantic information in deep neural representations across text and image modalities.\
  \ The core method uses Information Imbalance\u2014an asymmetric, rank-based measure\
  \ of relative predictivity\u2014to compare neighborhoods between high-dimensional\
  \ representations without dimensionality reduction."
---

# A quantitative analysis of semantic information in deep representations of text and images

## Quick Facts
- **arXiv ID:** 2505.17101
- **Source URL:** https://arxiv.org/abs/2505.17101
- **Reference count:** 40
- **Key outcome:** This study develops a quantitative framework to locate and characterize semantic information in deep neural representations across text and image modalities using Information Imbalance.

## Executive Summary
This paper introduces a novel quantitative framework to locate and characterize semantic information in deep neural representations across text and image modalities. The core contribution is the Information Imbalance (II) metric, an asymmetric, rank-based measure that quantifies the relative predictivity between representations without requiring dimensionality reduction. By analyzing multilingual sentence pairs and image-caption pairs across architectures like DeepSeek-V3, Llama3.1, DinoV2, and image-gpt, the study reveals a broad "semantic region" in LLM layers where cross-linguistic representations are most mutually predictable, and identifies modality-specific "semantic layers" in vision transformers. The findings show semantic information is distributed across many tokens and exhibits strong long-range correlations, with asymmetries in cross-modal predictability revealing architectural differences in how modalities encode meaning.

## Method Summary
The study employs Information Imbalance (II), an asymmetric, rank-based metric that measures the average rank in space Y of the nearest neighbor in space X, serving as a proxy for relative mutual information. This allows comparison of high-dimensional representations without dimensionality reduction. The method analyzes hidden states from all layers of models like DeepSeek-V3 and DinoV2, using binarized activations to reduce computational cost. Semantic regions are identified by finding layers where cross-lingual or cross-modal II is minimized, indicating maximum mutual predictability. Token-level analysis involves concatenating multiple token embeddings (typically ~20) to capture distributed semantic encoding.

## Key Results
- A broad "semantic region" (relative depth ~0.4-0.9 in DeepSeek-V3) where cross-linguistic representations are most mutually predictable (II ~0.2 vs. ~0.3 in Llama).
- Semantic information is distributed across many tokens (optimal ~20 tokens) with long-range correlations strongest in inner layers.
- For images, semantic layers identified: middle layers for image-gpt (II ~0.6 for image-image pairs) and end layers for DinoV2 (II ~0.43).
- Cross-modal analysis shows DeepSeek-V3 caption representations in semantic layers predict corresponding image representations, with asymmetries depending on architecture.

## Why This Works (Mechanism)

### Mechanism 1: Rank-Based Neighborhood Preservation as Mutual Information Proxy
- **Claim:** II quantifies semantic similarity by measuring how well nearest neighbors in one representation space predict those in another, acting as a computationally efficient proxy for mutual information.
- **Mechanism:** II assumes that if two representations encode the same semantic content, their local neighborhood structures should overlap. It computes the average rank in space Y of the nearest neighbor in space X, capturing asymmetric predictability.
- **Core assumption:** Local neighborhood structure (rank of distances) is a sufficient statistic for relative information content between high-dimensional distributions.
- **Evidence anchors:** [abstract] "Information Imbalance—an asymmetric, rank-based measure of relative predictivity—to compare neighborhoods... without dimensionality reduction." [section 2, page 3] "If neighborhoods in X predict neighborhoods in Y, these ranks are small and Δ(X → Y) is close to zero."
- **Break condition:** If the semantic manifold is highly twisted or noise levels shuffle ranks randomly (II ≈ 1).

### Mechanism 2: Abstraction into Language-Agnostic Semantic Subspaces
- **Claim:** LLMs develop a broad "semantic region" in intermediate layers where representations converge to a language-agnostic state, facilitating cross-lingual transfer.
- **Mechanism:** Initial layers process language-specific syntax (high II), middle layers abstract meaning (low II), and final layers diverge for language-specific output (rising II).
- **Core assumption:** Semantic content is a lower-dimensional manifold shared across languages, while surface form is high-dimensional and language-specific.
- **Evidence anchors:** [abstract] "Finding a broad 'semantic' region (relative depth ~0.4-0.9 in DeepSeek-V3) where cross-linguistic representations are most mutually predictive." [section 3.1, page 5] "In the very last layers... an apparent jump in Information Imbalance."
- **Break condition:** If the model is too small to form a distinct abstraction phase or if training data is strictly monolingual.

### Mechanism 3: Distributed Semantic Encoding across Token Sequences
- **Claim:** Semantic information in autoregressive models is distributed across a span of tokens (~20-50), requiring concatenation of multiple token embeddings to fully capture alignment.
- **Mechanism:** The attention mechanism disperses semantic context across the sequence. Preserving a window of tokens retains the "joint semantic representation" necessary for high-accuracy alignment.
- **Core assumption:** The semantic "state" of a sequence is a high-dimensional trajectory or cloud of activation vectors rather than a single point vector.
- **Evidence anchors:** [abstract] "Semantic information is distributed across many tokens (optimal ~20 tokens)... considering several tokens yields significantly stronger similarity." [section 3.1.2, page 6] "This suggests that semantic information is not concentrated in the last tokens, but spread over many of them."
- **Break condition:** If compression into a single token (like a [CLS] token) is explicitly trained and sufficient for the task.

## Foundational Learning

- **Concept:** **Information Imbalance (II) vs. CKA (Centered Kernel Alignment)**
  - **Why needed here:** II is the specific tool used to discover the paper's findings. Understanding that II is *asymmetric* (unlike CKA) is critical to interpreting why text predicts images differently than images predict text.
  - **Quick check question:** If Model A's representation perfectly predicts Model B's (II_{A→B} = 0), does that mean Model B perfectly predicts Model A?

- **Concept:** **The "Platonic Representation Hypothesis"**
  - **Why needed here:** This provides the theoretical basis for the paper. It posits that convergent models map data to a shared "ideal" representation of reality.
  - **Quick check question:** According to the paper, why might a larger model (DeepSeek-V3) show lower II than a smaller model (Llama 8B) when comparing the same translation pairs?

- **Concept:** **Binarization of Activations**
  - **Why needed here:** To replicate the paper's efficiency or understand the robustness of the results. The paper uses binary signs of activations (+1/-1) to compute distances, arguing it preserves geometric structure.
  - **Quick check question:** Why does converting precise floating-point activations to simple binary signs (+1/-1) not destroy the semantic signal in this specific high-dimensional context?

## Architecture Onboarding

- **Component map:** Paired datasets -> Representation Extractors (DeepSeek-V3, DinoV2, Image-GPT) -> Binarization of raw activations -> Distance matrix calculation -> Rank calculation -> Information Imbalance
- **Critical path:** Load batch of paired data (e.g., English & Spanish sentences) -> Extract hidden states from all layers for both inputs -> Select token window (e.g., last 20 tokens) and concatenate/binarize -> Compute pairwise distances and ranks for the batch in both spaces -> Calculate Δ(X → Y) by averaging the rank in Y of the nearest neighbor in X
- **Design tradeoffs:**
  - **Dimensionality Reduction:** The paper explicitly rejects averaging tokens in favor of concatenation (preserves info but increases compute cost to O(10^5) features).
  - **Precision:** Using raw floating-point vs. binarized distances. Paper shows they are qualitatively similar (Supp. Info G), allowing for massive memory savings.
  - **Layer Selection:** "Semantic layers" vary by architecture (Middle for Image-GPT, End for DinoV2). A fixed "semantic layer" index does not generalize across architectures.
- **Failure signatures:**
  - **II ≈ 1.0:** Indicates no mutual information (e.g., batch shuffling or comparing unrelated modalities/layers).
  - **High Asymmetry (II_{A→B} >> II_{B→A}):** Indicates Model B is "lossy" relative to Model A (e.g., DinoV2 is more semantically compressed than DeepSeek-V3).
  - **Flat II Curve:** If II doesn't dip in middle layers, the model may have failed to form a distinct semantic abstraction layer (common in very small or untrained models).
- **First 3 experiments:**
  1. **Sanity Check (Null Hypothesis):** Compute II on a batch of aligned pairs vs. batch-shuffled pairs (Supp. Info E). Verify aligned II < 0.5 and shuffled II ≈ 1.0.
  2. **Token Ablation:** Measure II for the same layer using 1 token (last) vs. 20 tokens (concatenated). Verify if semantic information is indeed distributed (II should drop with more tokens).
  3. **Cross-Modal Asymmetry:** Compare II for Text → Image vs. Image → Text using DeepSeek-V3 (layer 52) and DinoV2 (last layer). Verify the reported directional bias.

## Open Questions the Paper Calls Out
- **Open Question 1:** How do the depth and length of semantic regions quantitatively scale with model size and diverse architectures (e.g., diffusion models)? [explicit] Section 4 states it would be interesting to "quantifying the depth and the length of the semantic regions found as a function of the number of parameters in the model" across diverse architectures.
- **Open Question 2:** To what extent does language heterogeneity (linguistic relatedness vs. corpus frequency) determine the strength of cross-linguistic mutual predictability? [explicit] Section 3.1.1 observes heterogeneity across language pairs, and Section 4 explicitly calls for "a detailed quantitative analysis solely dedicated to the influence of language heterogeneity."
- **Open Question 3:** Is there a relationship between the observed forward-backward information asymmetry in tokens and downstream task performance or linguistic structure? [explicit] Section 3.1.3 notes the asymmetry and states, "These results call for further experiments to study possible relationships between these observed information asymmetries, model performance, and linguistic structure."

## Limitations
- The Information Imbalance metric lacks direct validation against established semantic similarity benchmarks like human judgments or downstream task performance.
- The binarization of activations, though computationally justified, introduces an approximation whose impact on semantic fidelity remains untested beyond qualitative comparisons.
- The study's focus on high-resource language pairs and curated datasets limits generalizability to low-resource languages or more diverse, noisy real-world data.

## Confidence
- **High Confidence:** The existence of a broad semantic region in LLM layers where cross-linguistic representations are most mutually predictable (II ~0.2).
- **Medium Confidence:** The claim that semantic information is distributed across many tokens (~20) rather than concentrated in a single summary token.
- **Medium Confidence:** The identification of "semantic layers" in vision transformers (middle for image-gpt, end for DinoV2).
- **Low Confidence:** The interpretation of cross-modal asymmetries (e.g., image-gpt→text II ~0.7) as evidence that visual encoders are more "lossy" than text encoders.

## Next Checks
1. **Benchmark II against Human Semantic Similarity Judgments:** Compute Information Imbalance for a set of text-image pairs with known human semantic similarity scores (e.g., from Flickr30k or COCO human annotations). Test if lower II correlates with higher human-rated similarity, providing direct validation of the metric's semantic relevance.

2. **Validate Binarization Robustness with Floating-Point Control:** Re-run a subset of key experiments (e.g., cross-lingual II for DeepSeek-V3) using the original floating-point activations instead of binarized ones. Quantify the difference in II values and assess if the qualitative conclusions (e.g., location of semantic minima) remain unchanged.

3. **Test Semantic Region Generalization to Low-Resource Languages:** Apply the II analysis to a low-resource language pair (e.g., English/Amharic from a different corpus). Determine if the characteristic dip in II (semantic region) is present and at a similar relative depth, or if it is an artifact of high-resource language training.