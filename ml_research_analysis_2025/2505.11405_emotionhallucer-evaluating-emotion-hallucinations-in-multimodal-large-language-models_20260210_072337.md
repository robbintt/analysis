---
ver: rpa2
title: 'EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language
  Models'
arxiv_id: '2505.11405'
source_url: https://arxiv.org/abs/2505.11405
tags:
- emotion
- hallucination
- emotional
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmotionHallucer, the first benchmark designed
  to detect and analyze emotion-related hallucinations in multimodal large language
  models (MLLMs). It addresses the gap in existing hallucination benchmarks, which
  focus on general tasks and overlook the unique challenges of emotion understanding,
  where subjective and psychological factors make hallucinations harder to detect.
---

# EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2505.11405
- **Source URL**: https://arxiv.org/abs/2505.11405
- **Reference count**: 40
- **Primary result**: First benchmark detecting emotion hallucinations in MLLMs, revealing significant performance gaps between knowledge and perception tasks

## Executive Summary
This paper introduces EmotionHallucer, a novel benchmark specifically designed to detect and analyze emotion-related hallucinations in multimodal large language models. Unlike existing hallucination benchmarks that focus on general tasks, EmotionHallucer addresses the unique challenges of emotion understanding where subjective and psychological factors make hallucinations harder to detect. The benchmark evaluates models across two dimensions: emotion psychology knowledge and real-world multimodal emotion perception. Through experiments with 38 LLMs and MLLMs, the study reveals widespread emotion hallucination issues, with closed-source models and those with reasoning capabilities showing better performance. The paper also introduces PEP-MEK, a framework that improves emotion hallucination detection by 9.90% on average through intermediate reasoning stages.

## Method Summary
EmotionHallucer employs an adversarial binary question-answer framework with 2,742 carefully crafted question pairs (basic + hallucinated) across 7 subcategories spanning 4 modalities. Models must correctly answer both questions in a pair to receive credit, preventing superficial pattern matching and yes-bias. The benchmark separates emotion psychology knowledge tasks (using authoritative textbook statements) from real-world multimodal perception tasks (using text, image, audio, and video data). The PEP-MEK framework enhances detection by explicitly extracting modality-specific and emotion knowledge before prediction, followed by explanation generation and verification steps to improve model transparency and reliability.

## Key Results
- Most MLLMs exhibit significant emotion hallucination issues, with overall accuracy well below 50% across 38 evaluated models
- Closed-source models and those with explicit reasoning capabilities consistently outperform open-source alternatives
- Knowledge tasks show significantly higher accuracy than multimodal perception tasks, revealing a substantial grounding gap
- PEP-MEK framework improves emotion hallucination detection by 9.90% on average across models
- Emotion-LLaMA, despite being fine-tuned on emotion tasks, shows worse hallucination detection performance than general-purpose models

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Binary QA Framework
Models must correctly answer BOTH questions in a pair to receive credit, preventing yes-bias and superficial pattern matching. Hallucinated questions are designed to appear grammatically fluent but contain subtle semantic distortions. This forces models to maintain consistency when presented with contradictory claims about the same content.

### Mechanism 2: Knowledge-Perception Separation
Evaluating emotion psychology knowledge separately from multimodal perception enables targeted diagnosis of hallucination sources. Knowledge tasks use authoritative textbook statements while perception tasks use real-world multimodal data, isolating whether failures stem from lacking foundational emotion concepts versus failing to map those concepts to sensory inputs.

### Mechanism 3: PEP-MEK Intermediate Reasoning Stage
The framework first prompts models to extract observable features (facial expressions, vocal prosody, contextual elements) and emotion definitions, then uses this structured knowledge to inform predictions. A subsequent explanation-verification step forces self-consistency checking, grounding reasoning and reducing hallucinations.

## Foundational Learning

- **Concept: Hallucination taxonomy (factuality vs. faithfulness)**
  - Why needed here: The benchmark distinguishes knowledge hallucinations (contradicting established facts) from perception hallucinations (contradicting input content)
  - Quick check question: If a model misidentifies a smiling face as sad, is this factuality or faithfulness hallucination?

- **Concept: Emotion representation paradigms (discrete categories vs. dimensional models)**
  - Why needed here: The benchmark uses discrete categories and intensity descriptors rather than continuous valence-arousal scales
  - Quick check question: Why might discrete intensity labels ("mild", "strong") be more suitable for LLM evaluation than continuous arousal values?

- **Concept: Yes/No bias in binary evaluation**
  - Why needed here: Models may exhibit systematic tendencies toward "yes" or "no" responses independent of content
  - Quick check question: If a model answers "yes" to 80% of questions but ground truth is 50% yes, what does FP Ratio reveal that raw accuracy doesn't?

## Architecture Onboarding

- **Component map**: Benchmark core (2,742 QA pairs) → 7 subcategories (Theory/Definition/Finding/Category/Intensity/Reasoning Result/Reasoning Cue) → 4 modalities (text/image/audio/video) → PEP-MEK framework (Knowledge extraction → Initial prediction → Explanation generation → Verification → Final answer)

- **Critical path**: Data preparation (QA pair construction with cross-annotator verification) → Model inference (basic question, then hallucinated question) → Scoring (pair-wise accuracy calculation with bias metrics)

- **Design tradeoffs**: Binary QA vs. open-ended generation (trades evaluation richness for reliability and scalability); Textbook knowledge vs. web-scraped knowledge (trades ecological validity for ground-truth stability); Single-modality vs. multimodal evaluation (trades computational cost for diagnostic granularity)

- **Failure signatures**: Near-random accuracy (~25%) indicates model lacks basic emotion understanding; High basic accuracy, low hallucinated accuracy shows model can perceive correctly but cannot detect contradictions; High FP Ratio (>0.7) indicates strong yes-bias; Large gap between knowledge and perception reveals conceptual understanding but poor sensory grounding

- **First 3 experiments**: (1) Baseline reproduction: Evaluate GPT-4o and Qwen2.5-VL on EmotionHallucer-NoAudio to verify reported metrics; (2) Ablation on PEP-MEK components: Test with only knowledge extraction vs. full pipeline; (3) Cross-modal transfer analysis: Evaluate image-only models on video tasks using key-frame sampling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: To what extent do cross-lingual and cultural variations influence emotion hallucination patterns in MLLMs?
- **Basis in paper**: [explicit] Authors note in Appendix E that benchmark focuses only on English and future work should explore cultural influences
- **Why unresolved**: Benchmark constructed exclusively using English sources, limiting evaluation of cultural nuances in emotional expression
- **What evidence would resolve it**: Evaluating models on a multilingual, culturally diverse extension of EmotionHallucer to compare hallucination rates across different linguistic contexts

### Open Question 2
- **Question**: What are the root causes of emotion hallucinations, specifically regarding pretraining biases versus modality misalignment?
- **Basis in paper**: [explicit] Appendix E highlights underlying causes remain underexplored, citing pretraining biases and lack of emotion-specific supervision
- **Why unresolved**: Paper focuses on detecting and mitigating hallucinations rather than performing ablation study on model architectures or training data
- **What evidence would resolve it**: Causal analysis of internal model states or ablation study isolating fusion mechanisms

### Open Question 3
- **Question**: Why does supervised fine-tuning on emotion tasks (e.g., Emotion-LLaMA) fail to mitigate hallucinations compared to general-purpose models?
- **Basis in paper**: [inferred] Authors observe Emotion-LLaMA does not outperform general-purpose models and identify need to mitigate hallucinations from supervised fine-tuning
- **Why unresolved**: Results show empirically that specialized emotion tuning leads to poor hallucination detection but mechanism for degradation is not analyzed
- **What evidence would resolve it**: Analysis comparing decision boundaries of general-purpose versus emotion-fine-tuned models

## Limitations

- **Binary QA format constraints**: Cannot capture nuanced spectrum of emotion understanding that open-ended generation might reveal
- **Textbook ground truth assumptions**: Assumes emotion psychology knowledge provides stable ground truth across cultural contexts
- **Adversarial question construction**: Specific annotation methodology remains underspecified, raising questions about consistency and potential modality biases

## Confidence

**High Confidence**: Widespread emotion hallucination issues across 38 models (average accuracy well below 50%) is supported by systematic evaluation. Better performance of closed-source models and those with reasoning capabilities follows logically from architectural advantages.

**Medium Confidence**: 9.90% average improvement from PEP-MEK is well-documented, but mechanism driving improvement warrants deeper investigation. Alternative explanations such as improved prompt formatting cannot be ruled out without component-level ablation studies.

**Low Confidence**: Separation of emotion psychology knowledge from multimodal perception assumes textbook emotion concepts provide stable ground truth, which may not hold across different cultural contexts or contemporary emotion theories.

## Next Checks

1. **Cross-cultural validation**: Test EmotionHallucer with models fine-tuned on culturally diverse emotion datasets to assess whether textbook-based ground truth introduces systematic biases.

2. **Open-ended generation comparison**: Implement parallel evaluation using free-text responses scored by separate emotion-understanding model to quantify what binary QA misses.

3. **Temporal hallucination analysis**: Extend benchmark to include questions about emotion transitions over time to better capture MLLMs' temporal reasoning capabilities in multimodal contexts.