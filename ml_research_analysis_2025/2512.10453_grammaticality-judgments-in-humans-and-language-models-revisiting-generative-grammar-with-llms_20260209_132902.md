---
ver: rpa2
title: 'Grammaticality Judgments in Humans and Language Models: Revisiting Generative
  Grammar with LLMs'
arxiv_id: '2512.10453'
source_url: https://arxiv.org/abs/2512.10453
tags:
- llms
- parasitic
- structure
- which
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) reproduce\
  \ human-like syntactic judgments without explicit grammatical encoding. Using English\
  \ and Norwegian, we test GPT-4 and LLaMA-3 on parasitic gaps, across-the-board extraction,\
  \ and subject-auxiliary inversion\u2014constructions that diagnose hierarchical\
  \ structure."
---

# Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs

## Quick Facts
- arXiv ID: 2512.10453
- Source URL: https://arxiv.org/abs/2512.10453
- Authors: Lars G. B. Johnsen
- Reference count: 4
- Key outcome: GPT-4 achieves near-perfect accuracy on English parasitic gaps and inversion (100%), with strong Norwegian alignment; ATB extraction shows lower performance (83% English, 29% Norwegian), supporting syntax emergence from surface forms without explicit grammatical encoding.

## Executive Summary
This study investigates whether large language models can reproduce human-like syntactic judgments without explicit grammatical encoding. Using English and Norwegian, GPT-4 and LLaMA-3 are tested on parasitic gaps, across-the-board extraction, and subject-auxiliary inversion—constructions that diagnose hierarchical structure. Results show near-perfect accuracy on English parasitic gaps and inversion, with ATB extraction yielding lower performance. These findings support the view that syntax can emerge from predictive training on surface forms rather than requiring innate grammatical modules.

## Method Summary
The study employs minimal sentence pairs (grammatical/ungrammatical variants) in English and Norwegian, submitted to instruction-tuned LLMs (GPT-4 via OpenAI API, LLaMA-3-405B via Hugging Face) with 1-5 acceptability rating scale elicitation. Model ratings are compared to expected human judgments (exact match ±1 tolerance) across three diagnostic constructions: parasitic gaps, across-the-board extraction, and subject-auxiliary inversion. Accuracy and deviation metrics are calculated by construction type and language.

## Key Results
- GPT-4 achieves 100% accuracy on English parasitic gaps and subject-auxiliary inversion
- ATB extraction yields 83% accuracy in English and only 29% in Norwegian
- Norwegian ATB items often involved semantically odd fillers, potentially confounding grammaticality judgments
- Strong alignment between model ratings and linguistic theory predictions across most constructions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical syntactic representations emerge from predictive training on surface forms without explicit grammatical encoding.
- **Mechanism:** Next-token prediction serves as a "surface projection of a much richer representational geometry," developing internal states that support global, structure-sensitive computations encoding licensing conditions and dependency constraints.
- **Core assumption:** Training data contains sufficient distributional regularities for structural generalizations to be statistically induced.
- **Evidence anchors:** [abstract] Structural generalizations emerge from predictive training; [Page 2] Hierarchical behavior is better understood as emerging from representational geometry; [corpus] Related work explores probability-grammaticality relationships but mechanism not confirmed across architectures.
- **Break condition:** If models were merely matching n-gram statistics, they would incorrectly front linearly-first auxiliaries.

### Mechanism 2
- **Claim:** Attention mechanisms induce notions of subjecthood and clause boundaries by tracking dependency chains across distances.
- **Mechanism:** Attention weights capture non-local relationships between fillers and gaps, enabling recognition that parasitic gap acceptability depends on an unfilled licensing gap elsewhere.
- **Core assumption:** Attention patterns correlate with linguistically meaningful dependencies rather than surface-level co-occurrence.
- **Evidence anchors:** [Page 6-7] LLM behavior on parasitic gaps not reducible to locality; [Page 9] Attention mechanisms likely induce subjecthood and constituency; [corpus] Grammar-Aligned Decoding shows constrained decoding can enforce structure, but this tests unconstrained emergence.
- **Break condition:** ATB extraction performance drops significantly (29% Norwegian), suggesting attention may favor contentful dependency chains over abstract structural parallelism.

### Mechanism 3
- **Claim:** Autoregressive training yields globally coherent representational states that facilitate hierarchical generalizations better than masked-LM objectives.
- **Mechanism:** Masked-LM training "introduces inference conditions that do not occur in natural language use, fragmenting the representational space," while autoregressive training maintains a single coherent state over the entire input sequence.
- **Core assumption:** Coherence of internal states directly impacts structural generalization capability.
- **Evidence anchors:** [Page 9] Masked-LM training fragments representational space; [Page 2, footnote] Several studies suggest masked-LM objectives yield less stable global representations; [corpus] No direct corpus comparison of masked vs. autoregressive on these constructions.
- **Break condition:** If masked-LM models (e.g., BERT) matched autoregressive performance, this mechanism would be undermined.

## Foundational Learning

- **Concept:** Generative grammar and constituent structure
  - **Why needed here:** The paper uses parasitic gaps and subject-auxiliary inversion as diagnostic constructions that motivated hierarchical syntax in linguistic theory. Understanding why these constructions matter is essential to interpreting results.
  - **Quick check question:** Can you explain why "Had the boy who eaten the apples will go home?" is ungrammatical despite the auxiliary being linearly first?

- **Concept:** Grammaticality judgments as theoretical evidence
  - **Why needed here:** The entire methodology treats acceptability ratings as proxies for underlying structure. Without understanding this inferential move, the experimental design appears circular.
  - **Quick check question:** What does it mean for a grammaticality contrast to "imply an underlying structural representation"?

- **Concept:** The "proxy view" of LLMs in linguistics
  - **Why needed here:** The paper explicitly adopts Ziv et al.'s proxy view—using LLMs to test what's learnable from input alone—rather than claiming LLMs model human cognition. This distinction frames all conclusions.
  - **Quick check question:** How does the proxy view differ from claiming LLMs possess human-like syntactic competence?

## Architecture Onboarding

- **Component map:** Minimal sentence pairs -> Instruction-tuned LLMs -> 1-5 acceptability rating scale -> Comparison to linguistic theory predictions
- **Critical path:** Generate minimal pairs preserving syntactic structure while varying lexical content → Submit pairs to instruction-tuned models with standardized rating prompts → Compare model ratings to linguistic theory predictions → Calculate accuracy and deviation metrics by construction type and language
- **Design tradeoffs:**
  - Parasitic gaps vs. ATB extraction: Gaps involve licensing chains (high performance); ATB requires structural symmetry (low performance, especially in Norwegian at 29%)
  - English vs. Norwegian: Higher-resource language shows stronger alignment; Norwegian's flexible verb movement adds complexity
  - Instruction-tuned vs. base models: Paper uses instruction-tuned variants for reliable rating elicitation—base models may require different probing methods
- **Failure signatures:**
  - ATB extraction in Norwegian (29% accuracy) suggests models struggle with coordination constraints requiring abstract parallelism
  - Semantically odd fillers in Norwegian parasitic gaps caused reduced performance—models may conflate semantic plausibility with grammaticality
  - Rating deviations cluster around semantically complex items, not purely structural violations
- **First 3 experiments:**
  1. **Baseline validation:** Replicate parasitic gap LP/PL conditions with new lexical items to verify structural sensitivity persists across vocabulary variation.
  2. **ATB diagnostic:** Test whether ATB failures correlate with attention head patterns—if attention favors contentful dependencies, manipulate semantic content to probe the interaction.
  3. **Cross-linguistic probe:** Extend to a third language with different coordination syntax to determine whether Norwegian ATB failure reflects data sparsity or architectural limitation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do LLMs exhibit significantly lower performance on Across-the-Board (ATB) extraction compared to parasitic gap licensing?
- **Basis in paper:** [explicit] The results show ATB extraction yields lower accuracy (83% English, 29% Norwegian) compared to near-perfect scores for parasitic gaps. The authors explicitly state, "further probing is needed" to understand if this is due to an "attention-based bias toward contentful elements over structural parallelism."
- **Why unresolved:** The paper demonstrates the performance gap but does not isolate the internal mechanism causing it. It remains unclear if the failure is due to the complexity of symmetrical movement or the models' reliance on local dependency chains.
- **What evidence would resolve it:** Mechanistic interpretability studies (e.g., probing attention heads) comparing how models represent symmetric coordination versus standard filler-gap dependencies.

### Open Question 2
- **Question:** To what extent does semantic plausibility interact with syntactic judgment accuracy in lower-resource languages like Norwegian?
- **Basis in paper:** [explicit] The author notes that Norwegian ATB items often involved "semantically odd fillers" and explicitly states: "This may point to the way LLMs build structure based on attention weights, but we leave this as an open question."
- **Why unresolved:** It is unclear if the poor performance on Norwegian ATB (29%) is strictly a syntactic failure or if the "colorless green ideas" effect degrades the model's ability to process structure in low-data settings.
- **What evidence would resolve it:** A controlled ablation study varying the semantic plausibility of Norwegian test items while holding the syntactic structure constant to measure the variance in judgment accuracy.

### Open Question 3
- **Question:** Can structural generalizations emerge from surface forms alone for all generative diagnostics, or are there limits to what predictive training can acquire?
- **Basis in paper:** [inferred] While the conclusion argues that grammar is an "emergent regularity," the authors note that "not all aspects of grammar are equally accessible" and some patterns "probably depend on discourse or semantic features."
- **Why unresolved:** The paper establishes that some constructions (inversion, parasitic gaps) are learnable, but the failure on others (ATB) leaves the boundary of "learnable syntax" undefined.
- **What evidence would resolve it:** Expanding the test suite to include a wider variety of complex dependencies (e.g., islands, binding theory) to map the precise boundaries of syntactic acquisition from statistical data.

## Limitations

- The study's reliance on acceptability rating scales introduces semantic plausibility confounds, particularly evident in Norwegian parasitic gap trials where semantically odd fillers reduced model performance.
- The 29% accuracy on Norwegian ATB extraction raises questions about whether observed failures reflect genuine structural complexity or data sparsity in training corpora.
- The paper does not systematically disentangle grammaticality from plausibility effects, acknowledging this limitation but not resolving it experimentally.

## Confidence

- **High confidence**: GPT-4's near-perfect performance on English parasitic gaps and subject-auxiliary inversion (100% accuracy)
- **Medium confidence**: The general claim that syntax can emerge from surface-form training without explicit grammatical encoding, supported by consistent patterns across multiple constructions
- **Medium confidence**: The mechanism of hierarchical representations emerging from predictive training, though the exact neural implementation remains underspecified

## Next Checks

1. **Semantic vs. syntactic dissociation**: Systematically manipulate semantic plausibility while holding syntactic structure constant in parasitic gap constructions to isolate grammaticality effects from plausibility biases.

2. **Attention mechanism verification**: Correlate ATB extraction failures with specific attention head patterns to determine whether models treat abstract structural parallelism differently from contentful dependencies.

3. **Cross-linguistic expansion**: Test the same constructions in a third language with distinct coordination syntax (e.g., Japanese or German) to determine whether Norwegian ATB difficulties reflect data sparsity or architectural limitations in handling symmetric structures.