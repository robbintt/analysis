---
ver: rpa2
title: Inference-Time Search using Side Information for Diffusion-based Image Reconstruction
arxiv_id: '2510.03352'
source_url: https://arxiv.org/abs/2510.03352
tags:
- search
- information
- side
- diffusion
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of solving inverse problems with
  side information by proposing a training-free inference-time search algorithm. The
  method leverages a pre-trained unconditional diffusion prior and uses a reward function
  to implicitly characterize the conditional distribution given side information,
  avoiding the need for retraining or multi-modal conditioning.
---

# Inference-Time Search using Side Information for Diffusion-based Image Reconstruction

## Quick Facts
- arXiv ID: 2510.03352
- Source URL: https://arxiv.org/abs/2510.03352
- Reference count: 40
- This work addresses the challenge of solving inverse problems with side information by proposing a training-free inference-time search algorithm that leverages a pre-trained unconditional diffusion prior and uses a reward function to implicitly characterize the conditional distribution given side information.

## Executive Summary
This paper introduces a training-free inference-time search algorithm for solving inverse problems with side information using pre-trained unconditional diffusion priors. The method implicitly characterizes the conditional distribution through a reward function rather than retraining conditional models or using multi-modal conditioning. Two search strategies, Greedy Search (GS) and Recursive Fork-Join Search (RFJS), are proposed to balance exploration and exploitation during sampling. The approach is evaluated across multiple tasks including face reconstruction, super-resolution, and MRI, with both image and text side information, showing consistent improvements in reconstruction quality.

## Method Summary
The method leverages a pre-trained unconditional diffusion prior and uses a reward function to implicitly characterize the conditional distribution given side information, avoiding the need for retraining or multi-modal conditioning. Two search strategies—Greedy Search (GS) and Recursive Fork-Join Search (RFJS)—are proposed to balance exploration and exploitation during sampling. The approach is evaluated across multiple tasks including face reconstruction, super-resolution, and MRI, with both image and text side information. Results show consistent improvements in reconstruction quality, particularly in identity preservation and alignment with side information, outperforming baseline methods like DPS and reward gradient guidance.

## Key Results
- Face reconstruction with identity side information shows improved FaceSimilarity while maintaining PSNR
- Text-to-image tasks demonstrate better CLIPScore alignment without sacrificing image quality metrics
- MRI multi-contrast reconstruction benefits from NMI-based rewards, outperforming baseline methods
- RFJS consistently outperforms GS, Best-of-N, and reward gradient guidance across all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1: Reward-Tilted Distribution as Conditional Proxy
- Claim: Side information can be incorporated by modeling the conditional distribution p₀|ₛ as a reward-tilted version of the unconditional prior, avoiding retraining.
- Mechanism: Instead of learning p₀|ₛ directly, the method assumes p₀|ₛ(x₀|s) ∝ p₀(x₀)exp(r(x₀;s)/τ), where r measures compatibility with side information. This tilts the unconditional prior toward higher-reward regions, effectively incorporating side information at inference time.
- Core assumption: The reward function is monotone with the true conditional density (i.e., higher r correlates with higher p₀|ₛ).
- Evidence anchors: [abstract] "uses a reward function to implicitly characterize the conditional distribution given side information, avoiding the need for retraining"; [section 4.1] Eq. (2) and surrounding derivation show the KL-regularized reward maximization formulation.
- Break condition: If the reward function is poorly aligned with the true conditional (e.g., non-monotonic or adversarial), the tilted distribution may diverge from p₀|ₛ.

### Mechanism 2: DPS-Style Value Function Approximation
- Claim: The intractable value function V_τ^t can be approximated by evaluating the reward at the conditional mean estimate, with bounded error.
- Mechanism: The value function V_τ^t(xt;s,y) = log E[exp(r(X₀;s)/τ)] is approximated as r(ẋ₀|ₜ,ᵧ(xt,y);s)/τ using Tweedie's formula to estimate ẋ₀|ₜ,ᵧ and pushing the expectation through the nonlinear reward. Proposition 3 provides error bounds that shrink as t decreases.
- Core assumption: The reward is Lipschitz and bounded; the conditional variance of X₀|xt,y decreases as t→0.
- Evidence anchors: [section 4.1] Eq. (6)-(7) and Proposition 3 with error bound in Eq. (13).
- Break condition: At large t, variance terms c₁(t), c₂(t), c₃(t) can be large, making the approximation loose.

### Mechanism 3: Grouped Resampling for Exploration-Exploitation Balance
- Claim: Hierarchical group-based resampling (RFJS) outperforms both pure exploration (Best-of-N) and greedy exploitation by dynamically adjusting group sizes.
- Mechanism: RFJS resamples groups of varying sizes at different frequencies: every B steps (size N), every B/2 steps (size N/2), etc. This allows localized exploration (small groups) and periodic exploitation (large groups). GS uses fixed-period resampling with group size N.
- Core assumption: The resampling base B and particle count N can be tuned per-task; the hierarchical schedule implicitly balances exploration/exploitation.
- Evidence anchors: [section 4.2] Figure 2 illustrates GS and RFJS; Algorithm 1 provides pseudocode; [section 5] Tables 1-2 show RFJS consistently outperforms GS, BON, and RGG across tasks.
- Break condition: If B is too small, over-exploitation can cause reward over-optimization; if B is too large, exploration is insufficient.

## Foundational Learning

- **Diffusion models and score-based generative modeling**:
  - Why needed here: The entire framework builds on reverse SDEs, Tweedie's formula (Eq. 1), and conditional score approximation.
  - Quick check question: Can you derive ẋ₀|ₜ(xt) from Tweedie's formula for a Gaussian forward process?

- **Bayesian inverse problems and posterior sampling**:
  - Why needed here: The goal is to sample from p₀|ᵧ,ₛ(·|y,s); understanding measurement models and likelihood approximation is essential.
  - Quick check question: Why is pᵧ|ₜ(y|xt) harder to compute than the unconditional score?

- **KL-regularized optimization (reward tilting)**:
  - Why needed here: Eq. (2) comes from solving max_p E[r(x)] - τD_KL(p||p₀); understanding this connection clarifies why the reward-tilted form is principled.
  - Quick check question: What is the closed-form solution to the KL-regularized reward maximization problem?

## Architecture Onboarding

- **Component map**: Base solver (DPS/DAPS/MPGD) -> Score network (pre-trained unconditional) -> Reward module (AdaFace/ImageReward/NMI) -> Search wrapper (N particles, grouped resampling) -> Final selection

- **Critical path**: 
  1. Initialize N particles from noise
  2. At each reverse step t: (a) sample candidates from pₜ|ₜ₊₁,ᵧ, (b) estimate ẋ₀|ₜ,ᵧ, (c) compute rewards, (d) resample within groups per Algorithm 1
  3. Select best particle at t=0 by reward

- **Design tradeoffs**:
  - **N vs. compute**: More particles improve exploration but scale runtime sublinearly (parallelization); Table 5 shows ~4-5× slowdown for N=8
  - **B vs. exploration/exploitation**: Small B → aggressive exploitation (risk over-optimization); large B → more exploration but slower convergence
  - **Gradient guidance vs. search**: RGG (gradient-based) requires differentiable rewards and is sensitive to scale; search supports black-box rewards but needs more compute

- **Failure signatures**:
  - **Reward over-optimization**: FS improves but PSNR/LPIPS degrade (see B.5, Fig. 14-17)
  - **Gradient scale too high**: Artifacts appear, especially with fewer diffusion steps (MPGD, DAPS)
  - **Misaligned reward**: Identity preserved but wrong expression (Fig. 11, box inpainting example)

- **First 3 experiments**:
  1. **Validate RFJS on DPS with face side information**: Use Celeb-HQ test pairs, N=8, B=16, compare FS/PSNR against BON and RGG. Check if RFJS improves FS without degrading PSNR.
  2. **Ablate B in GS vs. RFJS**: On 2D mixture-of-Gaussians setup (Fig. 13), sweep B ∈ {4, 8, 16, 32} and plot PSNR vs. B. Expect RFJS to be more robust across B.
  3. **Test black-box reward**: Use non-differentiable NMI for MRI multi-contrast reconstruction (Sec. 5.2, Fig. 5-6). Verify that RFJS works where RGG cannot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a general mathematical framework be established to provide formal optimality guarantees for exploration–exploitation strategies in diffusion-based inverse problems with side information?
- Basis: [explicit] Appendix D states, "Our proposed search algorithms lack formal optimality guarantees... absence of a general mathematical framework... an open problem we highlight."
- Why unresolved: Current approaches (GS, RFJS) rely on principled heuristics and tuned schedules rather than theoretical derivations of optimal search strategies.
- Evidence: A theoretical derivation defining optimality conditions for the search or an algorithm with provable efficiency bounds.

### Open Question 2
- Question: Is it possible to develop a principled method for automatically determining the compute-optimal allocation for parameters like the resampling base ($B$) and particle count ($N$) without task-specific manual tuning?
- Basis: [explicit] Appendix D notes the reliance on "tuned schedules" and that "compute allocation is not provably optimal, suggesting a clear direction for future work."
- Why unresolved: The paper manually selects hyperparameters (e.g., $B=16$ vs $B=100$) based on the specific sampler and degradation level.
- Evidence: An adaptive algorithm that dynamically adjusts resampling frequency and achieves comparable or better performance without manual hyperparameter selection.

### Open Question 3
- Question: How does the inference-time search framework perform when the side information is partially corrupted, misleading, or significantly misaligned with the measurement?
- Basis: [inferred] The paper evaluates performance using correlated side information (e.g., same identity) but does not analyze the method's robustness when the side information provides conflicting or incorrect guidance.
- Why unresolved: The proposed reward function $r(x_0, s)$ assumes side information $s$ is beneficial; the method's degradation path with "bad" side info is not characterized.
- Evidence: Experiments measuring reconstruction fidelity when side information is deliberately swapped with incorrect identities or noisy text descriptions.

## Limitations

- The method relies heavily on the quality of the reward function to guide the search toward the conditional distribution, with limited theoretical guarantees beyond value function approximation error bounds.
- The hierarchical resampling schedule in RFJS is shown to work empirically but lacks rigorous justification for why specific groupings outperform alternatives.
- Performance depends on careful tuning of hyperparameters N (particles) and B (resampling frequency), which may not generalize easily across domains without task-specific calibration.

## Confidence

- **High confidence**: The core mechanism of reward-tilted inference-time search (Mechanism 1) and the value function approximation (Mechanism 2) are well-grounded in existing diffusion literature and the paper provides clear derivations and empirical support.
- **Medium confidence**: The hierarchical resampling strategy (Mechanism 3) shows consistent improvements over baselines but lacks theoretical justification for the specific group size schedule. The empirical success may depend on task-specific reward landscape characteristics.
- **Low confidence**: The assumption that greedy resampling (hard selection) performs equivalently to soft resampling (softmax weights) across all tasks is not thoroughly validated, particularly for multimodal side information scenarios.

## Next Checks

1. **Theoretical analysis of RFJS convergence**: Prove (or disprove) that the hierarchical resampling schedule converges to the optimal conditional distribution under reasonable assumptions about the reward function. Compare RFJS to alternative exploration strategies like simulated annealing or Thompson sampling.

2. **Cross-task hyperparameter transfer**: Systematically evaluate whether N and B values that work well for face reconstruction transfer to MRI or text-to-image tasks. Identify whether a task taxonomy exists that predicts optimal hyperparameter ranges.

3. **Reward function sensitivity analysis**: For each task, test multiple reward functions (e.g., different face recognition models, text encoders, or NMI variants) and quantify how search performance degrades with reward misalignment. Determine whether the method is robust to reward noise or requires precise reward engineering.