---
ver: rpa2
title: 'M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation'
arxiv_id: '2505.04445'
source_url: https://arxiv.org/abs/2505.04445
tags:
- recommendation
- sequential
- mamba
- user
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: M2Rec is a novel sequential recommendation framework that integrates
  multi-scale Mamba with Fourier analysis, LLMs, and adaptive gating to address limitations
  in existing approaches. The method enhances Mamba with FFT to explicitly model periodic
  user behaviors in the frequency domain, incorporates LLM-based text embeddings to
  enrich sparse interaction data with semantic context, and introduces a learnable
  gate mechanism to dynamically balance temporal, frequency, and semantic features.
---

# M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation

## Quick Facts
- arXiv ID: 2505.04445
- Source URL: https://arxiv.org/abs/2505.04445
- Reference count: 40
- Primary result: M2Rec achieves 3.2% improvement in Hit Rate@10 over existing Mamba-based models while maintaining 20% faster inference than Transformer baselines

## Executive Summary
M2Rec introduces a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, LLM embeddings, and adaptive gating to address limitations in existing approaches. The method enhances Mamba with FFT to explicitly model periodic user behaviors in the frequency domain, incorporates LLM-based text embeddings to enrich sparse interaction data with semantic context, and introduces a learnable gate mechanism to dynamically balance temporal, frequency, and semantic features. The model achieves state-of-the-art performance across multiple datasets while maintaining computational efficiency.

## Method Summary
M2Rec combines item sequences with semantic embeddings through a three-stage architecture. First, the Adaptive Fourier Filtering Module (AFFM) applies FFT to user sequences, filters high-frequency noise using a learnable threshold θ, then reconstructs the signal via IFFT for Mamba processing. Second, item metadata is encoded using bge-large-en-v1.5 to generate 1024-dimensional semantic embeddings. Third, a learnable gate mechanism dynamically fuses the temporal-frequency (Mamba) and semantic (LLM) representations through weighted linear combination. The model is trained with binary cross-entropy loss using Adam optimizer (lr=1e-3, batch_size=2048) and evaluated on HR@10, NDCG@10, and MRR@10 metrics.

## Key Results
- Achieves 3.2% improvement in HR@10 over existing Mamba-based models
- Maintains 20% faster inference than Transformer baselines
- Outperforms state-of-the-art methods across ML-1M, Amazon Beauty, and location datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive FFT enables explicit modeling of periodic user behaviors by filtering high-frequency noise in the frequency domain.
- **Mechanism:** User interaction sequences are transformed via FFT to frequency domain, where a learnable binary mask M filters components above threshold θ (noise), then IFFT reconstructs a denoised time-domain signal for the Mamba encoder.
- **Core assumption:** Periodic behaviors (daily, weekly cycles) manifest as dominant low-frequency components, while random interactions appear as high-frequency noise.
- **Evidence anchors:** Abstract states "enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise"; Section III-D shows "X_m_u[k] = Xu[k] ⊙ M" where "M is a binary mask that only retains the frequency that is lower than the θ".
- **Break condition:** If user sequences lack true periodicity (purely random interactions), FFT filtering removes signal rather than noise, degrading performance.

### Mechanism 2
- **Claim:** LLM-derived text embeddings enrich sparse ID-based features with semantic context from item descriptions.
- **Mechanism:** Item metadata (name, category, description) is encoded via bge-large-en-v1.5 (1024-dim), producing semantic embeddings E(l) that capture nuanced product characteristics and user sentiments absent from pure ID embeddings.
- **Core assumption:** Item descriptions contain predictive signals about user preferences that are not captured by collaborative filtering on ID sequences alone.
- **Evidence anchors:** Abstract states "incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions"; Section III-C details "cv = {name, category, description}... input this item representation into the bge-large-en-v1.5 model".
- **Break condition:** If item descriptions are missing, generic, or uninformative across catalog, semantic embeddings provide no discriminative value.

### Mechanism 3
- **Claim:** Learnable gate mechanism dynamically balances temporal (Mamba), frequency (FFT), and semantic (LLM) features per context.
- **Mechanism:** Gated fusion: Ŷ = Linear(α · E(l) + β · E(m)), where α and β are adaptable parameters learned during training to weight LLM vs. AFFM embeddings optimally.
- **Core assumption:** Different domains/items benefit differently from semantic vs. temporal-frequency signals (e.g., books rely more on semantics; groceries on temporal patterns).
- **Evidence anchors:** Abstract states "learnable gate mechanism to dynamically balance temporal, frequency, and semantic features"; Section III-E shows Equation 16 with gated combination; ablation in Figure 4 shows "w/o GM" causes largest performance drop.
- **Break condition:** If α and β converge to trivial values (e.g., near-zero for one modality), fusion collapses to single-source prediction.

## Foundational Learning

- **Concept:** State Space Models (SSMs) and Mamba discretization
  - **Why needed here:** M2Rec builds on Mamba's selective SSM; understanding h_t = Āh_{t-1} + B̄x_t and the global convolution formulation is prerequisite to modifying the encoder.
  - **Quick check question:** Can you explain how Mamba achieves O(L) complexity vs. Transformer's O(L²)?

- **Concept:** Discrete Fourier Transform (DFT) and FFT
  - **Why needed here:** The AFFT module transforms sequences to frequency domain; understanding X[k] = Σ x[n]e^{-j2πkn/N} is essential for debugging periodicity capture.
  - **Quick check question:** What does a dominant peak at 0.14 cycles/day in the frequency spectrum represent in terms of real-world behavior?

- **Concept:** Multimodal fusion via gating
  - **Why needed here:** The gate mechanism is the critical integration point; understanding how learned weights balance conflicting signals determines model success.
  - **Quick check question:** Why might simple concatenation underperform compared to learned gating for fusing semantic and temporal features?

## Architecture Onboarding

- **Component map:** Item sequences + item contexts → Embedding Layer + LLM encoder (parallel) → AFFM Block (FFT → Adaptive mask → IFFT → Mamba SSM) → E(m) → Gate(α·E(l), β·E(m)) → Linear → Prediction

- **Critical path:** FFT→IFFT pipeline feeds denoised sequences to Mamba; if threshold θ is misconfigured, periodic patterns are lost before SSM processing.

- **Design tradeoffs:**
  - θ=0.1 captures more high-frequency components (better for noisy data) vs. higher θ risks over-smoothing (Table V)
  - Single Mamba layer (optimal per experiments) vs. deeper stacks risk overfitting
  - LLM choice: BGE-1.5 outperforms GPT-3.5/Llama-3.1 slightly, but all viable (Table IV)

- **Failure signatures:**
  - Performance drops close to Mamba4Rec baseline → LLM embeddings not being integrated (check gate weights)
  - Degraded HR@10 on long sequences → θ may be too aggressive, filtering meaningful patterns
  - Training instability → check IFFT reconstruction dimensions match Mamba input expectations

- **First 3 experiments:**
  1. **Sanity check:** Run M2Rec vs. Mamba4Rec on ML-1M with θ=0.1; verify ~3% HR@10 improvement as reported.
  2. **Ablation by component:** Disable LLM (w/o GM), then disable AFFT (w/o AFFT); confirm largest drop from gate removal per Figure 4.
  3. **Noise robustness test:** Inject 10-20% Gaussian noise to embeddings; verify M2Rec degrades less than BERT4Rec/Mamba4Rec per Figure 7.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the high-frequency threshold parameter θ be learned adaptively rather than manually tuned?
- **Basis in paper:** [inferred] The hyperparameter study (Table V) shows θ significantly impacts performance, yet θ is set manually. The paper states "M is obtained via M := |F| ≤ θ where frequencies below the threshold θ are retained" without proposing an automatic selection mechanism.
- **Why unresolved:** Different datasets may have different optimal noise-to-signal ratios, and manual tuning is impractical for real-world deployment.
- **What evidence would resolve it:** Experiments with a learnable θ parameter (e.g., via gradient descent) comparing performance against fixed thresholds across multiple datasets.

### Open Question 2
- **Question:** Would fine-tuning LLMs on domain-specific recommendation data improve performance over using pre-trained embeddings?
- **Basis in paper:** [explicit] Section III-C states: "we input this item representation into the bge-large-en-v1.5 model" and uses fixed pre-trained embeddings without task-specific adaptation.
- **Why unresolved:** Pre-trained LLM embeddings may not capture recommendation-specific semantics, and the paper does not explore fine-tuning strategies.
- **What evidence would resolve it:** Comparative experiments fine-tuning the LLM encoder end-to-end versus using frozen embeddings on the same datasets.

### Open Question 3
- **Question:** How does M2Rec perform in cold-start scenarios for new users or new items?
- **Basis in paper:** [inferred] The paper claims LLM embeddings "enrich sparse interaction data" and address sparsity, but all experiments use users with established interaction histories. Cold-start performance is not explicitly evaluated.
- **Why unresolved:** The semantic enrichment claim is only validated on users with sufficient interaction data.
- **What evidence would resolve it:** Dedicated cold-start experiments isolating users/items with fewer than K interactions and reporting performance degradation curves.

### Open Question 4
- **Question:** Does the binary mask for frequency filtering lose information compared to a soft, learnable filtering mechanism?
- **Basis in paper:** [inferred] Equation 12 uses a binary mask M := |F| ≤ θ that completely removes frequencies above θ. This hard threshold may discard useful high-frequency signals in some user behavior patterns.
- **Why unresolved:** No comparison is provided against soft-thresholding or attention-based frequency weighting schemes.
- **What evidence would resolve it:** Ablation experiments comparing binary masking versus learnable soft-masking or frequency-domain attention mechanisms.

## Limitations

- Critical hyperparameters including sequence truncation length, train/validation/test splits, and gate parameter initialization are not specified
- The differentiability of the binary FFT mask for backpropagation is not explained
- The claim of 20% faster inference than Transformers lacks implementation details and may be overstated

## Confidence

- **High Confidence:** Mechanism 1 (FFT noise filtering) - Well-defined mathematical operations with clear threshold parameter
- **Medium Confidence:** Mechanism 2 (LLM embeddings) - Standard BGE integration, but semantic value depends heavily on item description quality
- **Low Confidence:** Mechanism 3 (Learnable gating) - Novel fusion approach without corpus validation or ablation analysis of gate convergence

## Next Checks

1. **Gate dynamics verification:** Monitor α and β during training to confirm they adapt per context rather than converging to fixed values that nullify fusion benefits
2. **Frequency domain analysis:** Visualize power spectrum before/after FFT filtering across different θ values to verify periodic patterns (0.14 cycles/day) are being preserved vs. over-smoothed
3. **Cross-domain robustness:** Test M2Rec on datasets with varying periodicity (daily groceries vs. monthly electronics) to validate the claimed adaptive benefit of frequency modeling