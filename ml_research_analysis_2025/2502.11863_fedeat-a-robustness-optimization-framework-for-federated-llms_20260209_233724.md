---
ver: rpa2
title: 'FedEAT: A Robustness Optimization Framework for Federated LLMs'
arxiv_id: '2502.11863'
source_url: https://arxiv.org/abs/2502.11863
tags:
- robustness
- llms
- federated
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robustness challenges in federated large language
  models (LLMs), including data heterogeneity, malicious clients, and adversarial
  attacks. The authors propose FedEAT, a framework that applies adversarial training
  in the embedding space of client LLMs combined with geometric median aggregation
  to enhance robustness.
---

# FedEAT: A Robustness Optimization Framework for Federated LLMs

## Quick Facts
- arXiv ID: 2502.11863
- Source URL: https://arxiv.org/abs/2502.11863
- Reference count: 20
- Primary result: Embedding-space adversarial training + geometric median aggregation significantly improves federated LLM robustness while maintaining minimal utility loss

## Executive Summary
FedEAT addresses robustness challenges in federated large language models (LLMs) by combining embedding-space adversarial training with geometric median aggregation. The framework generates adversarial examples by perturbing embedding vectors while constraining perturbations to maintain semantic consistency, then aggregates client updates using the geometric median to resist malicious clients. Experiments demonstrate that FedEAT achieves lower attack success rates and maintains accuracy on benign data across multiple model architectures and tasks, with ablation studies confirming the independent contributions of both components to overall robustness.

## Method Summary
FedEAT operates through a two-stage process: local adversarial training and robust global aggregation. On the client side, each participant performs fine-tuning on perturbed embeddings generated via Projected Gradient Descent (PGD) within the embedding space, creating "worst-case" scenarios while constraining perturbations to preserve semantic meaning. The server aggregates client updates using the geometric median (computed via the Weiszfeld algorithm) rather than simple averaging, making the system resistant to outliers from malicious clients. The framework uses PEFT/LoRA for efficient parameter-efficient fine-tuning and has been tested on models including PHI-3-MINI, ZEPHYR-7B, and others across tasks like SST2, QQP, MNLI, and QNLI.

## Key Results
- FedEAT significantly reduces attack success rates across multiple attack types (FGSM, PGD) compared to FedAvg baseline
- The framework maintains minimal performance loss on benign data while improving robustness
- Ablation studies show both embedding-space adversarial training and geometric median aggregation independently contribute to robustness improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Embedding-space adversarial training improves local model robustness efficiently.
- **Mechanism:** Perturbs continuous embedding vectors using PGD instead of discrete tokens, training the model on "worst-case" scenarios while maintaining semantic consistency through constrained perturbations.
- **Core assumption:** Embedding-space perturbations effectively approximate semantic variations without breaking meaning when constrained by ||δ||_p ≤ ε.
- **Evidence anchors:** Abstract mentions semantic-consistent perturbation generation; Section 5.1 confirms effectiveness and efficiency; corpus neighbors validate adversarial training in federated settings.
- **Break condition:** If ε is too large, semantic meaning is destroyed, causing model confusion rather than robustness.

### Mechanism 2
- **Claim:** Geometric Median Aggregation mitigates the impact of malicious or outlying client updates.
- **Mechanism:** Uses Weiszfeld algorithm to compute geometric median instead of FedAvg's weighted averaging, making aggregation resistant to outliers by minimizing sum of distances to all client updates.
- **Core assumption:** Malicious updates are geometrically distant from benign updates and constitute a minority.
- **Evidence anchors:** Abstract emphasizes geometric median for enhanced robustness; Section 5.2 shows robustness to outliers; corpus supports robust aggregation literature.
- **Break condition:** If malicious clients form near-majority or coordinate sybil attacks, geometric median may shift toward malicious cluster.

### Mechanism 3
- **Claim:** Combination of adversarial training and robust aggregation provides defense-in-depth architecture.
- **Mechanism:** Decouples robustness problem—local EAT hardens against input-level attacks while global GM aggregation hardens against client-level poisoning attacks.
- **Core assumption:** Local adversarial features are compatible with global aggregation without causing instability.
- **Evidence anchors:** Section 6.3 ablation studies validate independent contributions; abstract confirms minimal benign data performance loss; corpus shows limited direct evidence for LLM synergy.
- **Break condition:** If local adversarial training causes significant model divergence, geometric median may fail to find meaningful center point.

## Foundational Learning

- **Concept: Federated Learning (FL) & Data Heterogeneity**
  - **Why needed here:** Builds on standard FL loop but addresses specific failure modes in LLMs.
  - **Quick check question:** How does Non-IID data distribution across clients affect convergence of standard FedAvg?

- **Concept: LLM Embeddings vs. Discrete Tokens**
  - **Why needed here:** FedEAT operates in continuous embedding space to bypass discrete text attack difficulties.
  - **Quick check question:** Why is generating adversarial examples harder in discrete token space compared to continuous image or embedding spaces?

- **Concept: Projected Gradient Descent (PGD)**
  - **Why needed here:** Algorithm used to generate adversarial perturbations δ during training.
  - **Quick check question:** What role does the "projection" step play in PGD when optimizing an adversarial perturbation?

## Architecture Onboarding

- **Component map:** Server → Client Side (LLM with PEFT + Embedding-space PGD Attacker) → Server Side (Geometric Median Aggregator)
- **Critical path:** 1) Server initializes global model θ 2) Client receives θ, runs local training on perturbed embeddings 3) Client uploads updated θ_c 4) Server filters outliers and aggregates via Geometric Median 5) Repeat
- **Design tradeoffs:**
  - Robustness vs. Utility: Higher λ or ε increases robustness but risks benign accuracy drop
  - Computation vs. Robustness: PGD steps add local training overhead
  - Speed vs. Stability: Weiszfeld aggregation is slower than averaging but provides Byzantine resilience
- **Failure signatures:**
  - Semantic Destruction: Excessive ε causes nonsensical prompts and model confusion
  - Non-convergence: Weiszfeld oscillation indicates diverse or malicious client updates
- **First 3 experiments:**
  1. Baseline Utility Check: Compare FedAvg vs FedEAT on benign data only (SST2/QQP)
  2. Robustness Evaluation: Run attacks (FGSM/PGD) to measure Attack Success Rate (ASR)
  3. Ablation Study: Compare "EAT-only" and "GM-only" to isolate component contributions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can robust aggregation methods from traditional federated learning effectively transfer to federated LLMs, and under what conditions does their effectiveness vary?
- **Basis in paper:** Authors explicitly pose this question in Section 5.2 regarding geometric median effectiveness in federated LLMs.
- **Why unresolved:** Ablation shows GM-only improves robustness but results are inconsistent across models and tasks (e.g., worse than FedAvg on ZEPHYR-7B for QQP).
- **What evidence would resolve it:** Systematic experiments across diverse architectures, varying malicious client ratios, and different attack types to characterize when geometric median provides consistent benefits.

### Open Question 2
- **Question:** How should adversarial perturbation intensity be adaptively calibrated for LLMs with different architectures and inherent robustness characteristics?
- **Basis in paper:** Appendix A.1 notes that fixed perturbation intensity across different LLMs may not reflect algorithm effectiveness due to varying sensitivities.
- **Why unresolved:** Fixed ε caused inconsistent results—some models benefited while others degraded. Optimal perturbation bounds remain unknown for different architectures.
- **What evidence would resolve it:** Experiments with architecture-specific perturbation schedules linked to model parameters showing consistent robustness-utility tradeoffs.

### Open Question 3
- **Question:** How can FedEAT's embedding-space adversarial training be extended to models that do not support direct embedding vector manipulation?
- **Basis in paper:** Appendix A.1 states method limited to models compatible with embedding vector training due to some models' limitations.
- **Why unresolved:** Many deployed LLMs have closed embedding layers or different tokenization schemes, limiting FedEAT's applicability.
- **What evidence would resolve it:** Development of alternative adversarial training approaches (discrete token-space, prompt-based) achieving comparable robustness without embedding access.

## Limitations
- Hyperparameters (ε, α, λ, learning rate, local epochs, communication rounds, clients per round) remain underspecified
- Fixed perturbation magnitude across different model sizes acknowledged as suboptimal
- GEMMA model exhibited severe failure mode requiring model substitution
- No detailed convergence criteria for Weiszfeld algorithm or computational overhead reporting
- Evaluation lacks standard deviations, convergence curves, and communication efficiency metrics

## Confidence

- **High confidence:** Core contribution of embedding-space adversarial training for federated LLMs is technically sound and builds on established PGD methodology. Geometric median aggregation approach is well-established in robust statistics.
- **Medium confidence:** Experimental results showing improved robustness with minimal utility loss appear credible but lack detailed hyperparameter information and statistical variation reporting.
- **Low confidence:** Claim about specific synergy between local adversarial training and global geometric median aggregation contributing to "defense-in-depth" architecture is weakly supported by corpus with limited direct evidence for LLM contexts.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary ε (0.01, 0.05, 0.1, 0.2), λ (0.1, 0.5, 1.0, 2.0), and PGD steps (3, 5, 10) to identify optimal configurations and measure robustness-utility tradeoff curves.

2. **Robustness to Malicious Majority:** Test geometric median aggregation under increasing proportions of malicious clients (20%, 40%, 60%, 80%) to quantify breaking point where defense fails against coordinated attacks.

3. **Communication Efficiency Measurement:** Record and compare number of communication rounds required for convergence between FedEAT and FedAvg baselines across all four evaluation tasks to assess practical deployment overhead.