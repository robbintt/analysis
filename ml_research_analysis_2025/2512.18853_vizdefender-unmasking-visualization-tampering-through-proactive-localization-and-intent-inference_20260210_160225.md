---
ver: rpa2
title: 'VizDefender: Unmasking Visualization Tampering through Proactive Localization
  and Intent Inference'
arxiv_id: '2512.18853'
source_url: https://arxiv.org/abs/2512.18853
tags:
- tampering
- data
- intent
- visualization
- tampered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting and analyzing tampered
  data visualizations, which are maliciously altered images designed to mislead viewers.
  The proposed VizDefender framework combines semi-fragile watermarking with intelligent
  intent analysis.
---

# VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference

## Quick Facts
- arXiv ID: 2512.18853
- Source URL: https://arxiv.org/abs/2512.18853
- Authors: Sicheng Song; Yanjie Zhang; Zixin Chen; Huamin Qu; Changbo Wang; Chenhui Li
- Reference count: 40
- One-line primary result: Proactive detection framework combining semi-fragile watermarking and intent analysis, achieving 82% intent inference accuracy on tampered visualizations.

## Executive Summary
This paper addresses the challenge of detecting and analyzing tampered data visualizations, which are maliciously altered images designed to mislead viewers. The proposed VizDefender framework combines semi-fragile watermarking with intelligent intent analysis. It embeds a location map into visualizations using invertible neural networks, enabling precise localization of tampered regions while preserving visual quality. A two-agent MLLM pipeline then interprets the detected tampering, inferring the manipulation method and intent. Evaluations show VizDefender outperforms baselines with an average tampering detection F1 score of 0.8259, image quality preservation with PSNR of 33.55 dB, and intent inference accuracy of 82% on a manually created dataset. User studies confirm its effectiveness in detecting subtle manipulations and providing meaningful explanations of tampering intent.

## Method Summary
VizDefender uses a two-stage framework: a semi-fragile watermark module and an intent analysis module. The watermark module embeds a location map into visualization images using an invertible neural network (INN) that operates in the frequency domain via Discrete Wavelet Transform (DWT). This enables precise localization of tampered regions - malicious edits disrupt the inverse transformation, creating spatial "holes" in the extracted map. The intent analysis module employs a two-agent MLLM pipeline that first refines the localization mask to filter noise and identify visual components, then uses constrained, rule-based reasoning to infer the manipulation method and intent. The system is trained on VisImages dataset (5,656 images >512px) with pre-training on COCO, using specific loss functions and hyperparameters including PSNR targets and tampering thresholds.

## Key Results
- Achieved 82% intent inference accuracy on manually created tampering dataset
- Outperformed baselines with tampering detection F1 score of 0.8259
- Preserved image quality with PSNR of 33.55 dB and SSIM of 0.86
- Successfully localized tampered regions while maintaining visual fidelity for benign compression

## Why This Works (Mechanism)

### Mechanism 1: Semi-Fragile Watermarking via Invertible Neural Networks (INNs)
The framework uses an Image Steganography Network (ISN) to hide a solid-color location map within high-frequency components of the chart via Discrete Wavelet Transform. When pixels are altered through tampering, the Image Revealing Network (IRN) cannot reconstruct the map at those coordinates, exposing the location. This relies on specific statistical consistencies that benign compression won't disrupt.

### Mechanism 2: Constrained MLLM Reasoning (Component-to-Method Mapping)
Instead of open-ended prompting, the system guides the MLLM using a structured "Component-to-Method" map. After identifying the modified visual component (e.g., "axis," "legend"), it constrains the model to check predefined rules (e.g., if component == axis, check for 'Modifying Coordinate Values'). This grounded reasoning reduces hallucination significantly.

### Mechanism 3: Visual Prompting via Contour Overlays
Standard masks or color overlays can obscure underlying data, so VizDefender uses contour lines of connected components from the damaged location map as visual prompts. This draws the MLLM's attention without altering the semantic content (color/value) of chart elements, preserving the model's ability to understand visual context.

## Foundational Learning

- **Invertible Neural Networks (INNs) & Normalizing Flows**: These networks are mathematically invertible (f(x) → y and f⁻¹(y) → x), allowing perfect retrieval of the hidden location map unless tampered with. This differs from standard autoencoders that lose information during bottleneck compression.
  - *Quick check question*: How does the network ensure that the inverse function f⁻¹ exists and is computationally efficient for high-resolution images?

- **Discrete Wavelet Transform (DWT)**: Used to separate images into frequency bands, allowing data hiding in high-frequency details while visual structure remains in low-frequency. This explains why the watermark is invisible to humans but vulnerable to edits.
  - *Quick check question*: Why is the high-frequency domain preferred for embedding watermarks in visualizations compared to the spatial domain?

- **Chain-of-Thought (CoT) & Agent Scaffolding**: The intent analysis uses a pipeline (Mask Refinement → Intent Analysis) that breaks the complex problem of "intent inference" into sub-problems. This scaffolding approach yields better results than a single complex prompt.
  - *Quick check question*: Why does explicitly asking the model to "filter noise" before "inferring intent" yield better results than a single, complex prompt?

## Architecture Onboarding

- **Component map**: Input Image → DWT (Frequency Split) → ISN (Invertible Steganography Network) → Protected Image. Tampered Image → IRN (Image Revealing Network) + Posterior Estimation (Noise handling) → Location Map → Mask Refinement Agent → Intent Analysis Agent → Output Report.

- **Critical path**: The most critical path is the IRN extraction. If the extracted location map is too noisy (false positives) or too clean (missed tampering), the downstream MLLM agents will hallucinate or fail. The Posterior Estimation module here is the key stabilizer against benign compression artifacts.

- **Design tradeoffs**: Sensitivity vs. Robustness - the model must be fragile to attacks but robust to social media compression (JPEG). Visual Quality vs. Capacity - higher embedding capacity degrades image quality. The paper tunes this via posterior estimation and thresholding (tau=0.2).

- **Failure signatures**: "Burr" Noise in Text - high false positives in text areas due to concentrated high-frequency watermarking. Thin Line Loss - very thin grid lines may be confused with tampering artifacts. Context Blindness - cannot detect removal of data points if visual context is ambiguous.

- **First 3 experiments**:
  1. Run ISN/IRN pipeline on clean chart with localized "Painting" vs. "JPEG Compression" to verify Location Map breaks for Painting but remains intact for JPEG.
  2. Run Intent Analysis module on tampered legends with "Component-to-Method" rules disabled vs. enabled to measure hallucination reduction.
  3. Feed Mask Refinement agent the same tampered image with three different visual prompts (Binary Mask, Bounding Box, Contour Lines) to measure component identification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework detect manipulations in AI-generated visualizations created from scratch when no prior watermarked version exists for comparison?
- Basis: The authors state the system cannot detect misinformation visualizations generated from scratch using generative AI tools due to lack of prior knowledge for comparison.
- Why unresolved: The proactive defense relies on pre-embedding a location map, making it impossible to verify integrity for images created entirely post-publication.
- What evidence would resolve it: Integration of statistical validation mechanisms or forensic techniques capable of identifying inherent artifacts in AI-generated visualizations.

### Open Question 2
How can the semi-fragile watermark be secured against targeted adversarial attacks designed to remove or destroy the integrity marker?
- Basis: The authors list vulnerabilities to adversarial attacks targeting watermark integrity as a key limitation.
- Why unresolved: While the model handles benign noise, it hasn't been evaluated against sophisticated attacks designed to strip watermarks while preserving visual content.
- What evidence would resolve it: Demonstrating effectiveness of adaptive watermarking techniques or complementary detection methods surviving adversarial perturbations.

### Open Question 3
Can the latency of the two-agent MLLM pipeline be reduced to support real-time, high-throughput content moderation?
- Basis: The paper identifies MLLM processing time (approx. 11 seconds per agent) as a bottleneck for real-time applications.
- Why unresolved: Sequential nature creates latency precluding immediate feedback required for high-velocity social media streams.
- What evidence would resolve it: Successfully implementing optimization strategies like request parallelization or smaller specialized models to achieve sub-second inference times.

## Limitations
- Cannot detect AI-generated visualizations from scratch due to lack of prior watermark for comparison
- Vulnerable to targeted adversarial attacks designed to remove or destroy the watermark integrity marker
- High computational cost (11 seconds per agent) limits real-time applications for content moderation

## Confidence

- **High Confidence**: The core watermarking mechanism (INN-based semi-fragile embedding) is technically sound and well-supported by literature on invertible networks and steganography.
- **Medium Confidence**: The intent inference pipeline using constrained MLLM reasoning shows promise, but generalizability to manipulation types beyond nine categories remains untested.
- **Medium Confidence**: The claim of "proactive" protection assumes attackers won't simply regenerate visualizations rather than edit existing images, representing a significant threat model limitation.

## Next Checks

1. **Robustness to AI Regeneration**: Test whether the system can detect tampering when the visualization is regenerated from underlying data using modern generative AI tools.

2. **Taxonomy Coverage**: Systematically enumerate manipulation techniques used in recent visualization manipulation studies to assess how many fall outside the nine predefined categories.

3. **Cross-Domain Generalization**: Evaluate performance on visualizations from domains not represented in training data (e.g., scientific visualizations, financial charts) to test domain adaptation limits.