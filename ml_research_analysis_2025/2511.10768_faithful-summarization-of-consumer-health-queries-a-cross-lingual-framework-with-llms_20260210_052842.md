---
ver: rpa2
title: 'Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework
  with LLMs'
arxiv_id: '2511.10768'
source_url: https://arxiv.org/abs/2511.10768
tags:
- medical
- summarization
- faithfulness
- summaries
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for faithful summarization of consumer
  health questions (CHQs) by combining TextRank-based sentence extraction with medical
  named entity recognition (NER) to guide large language models (LLMs). The method
  fine-tunes LLaMA-2-7B on English (MeQSum) and Bangla (BanglaCHQ-Summ) datasets,
  using TextRank to extract relevant sentences containing medical entities and then
  generating summaries with the LLM.
---

# Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs

## Quick Facts
- arXiv ID: 2511.10768
- Source URL: https://arxiv.org/abs/2511.10768
- Reference count: 5
- Primary result: Proposed framework combining TextRank-based sentence extraction and medical NER with LLMs improves faithfulness in consumer health question summarization, with best-of-3 selection achieving 50.50 ROUGE-1 on MeQSum and 0.57 SummaC faithfulness score.

## Executive Summary
This paper introduces a framework for faithful summarization of consumer health questions by integrating TextRank-based sentence extraction with medical named entity recognition to guide large language models. The approach fine-tunes LLaMA-2-7B on English (MeQSum) and Bangla (BanglaCHQ-Summ) datasets, extracting medically-relevant sentences before abstractive generation. Results demonstrate consistent improvements over zero-shot baselines and prior systems, with best-of-3 selection achieving ROUGE-1/2/L scores of 50.50/34.38/47.74 on MeQSum and 32.35/16.32/29.09 on BanglaCHQ-Summ. Faithfulness, measured by SummaC and AlignScore, also improves significantly, with human evaluation confirming over 80% of summaries preserve critical medical information.

## Method Summary
The framework preprocesses consumer health questions using medical NER to identify critical entities and negation terms, then applies TextRank to extract sentences containing medical entities and query-related words. These extracted sentences are used as input for fine-tuning LLaMA-2-7B with LoRA, generating three candidate summaries per query. A selector chooses the best candidate based on either ROUGE-1 or SummaC scores, with SummaC-based selection shown to prioritize factual consistency. The method is validated on both English (MeQSum) and Bangla (BanglaCHQ-Summ) datasets, demonstrating cross-lingual applicability and improved faithfulness over traditional fine-tuning approaches.

## Key Results
- Best-of-3 SummaC-based selection achieves 0.57 faithfulness score on MeQSum, significantly outperforming fine-tuning alone (0.37) and zero-shot baselines (0.28)
- ROUGE-1/2/L scores reach 50.50/34.38/47.74 on MeQSum with best-of-3 selection, improving upon both fine-tuning and zero-shot approaches
- Cross-lingual validation on BanglaCHQ-Summ shows consistent trends with ROUGE-1/2/L of 32.35/16.32/29.09 and SummaC of 0.32
- Human evaluation confirms 82% of generated summaries preserve critical medical information and maintain factual consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-filtering input with TextRank-based sentence extraction improves faithfulness by grounding generation in source-relevant content.
- Mechanism: TextRank identifies sentences containing medical entities and query-related words, filtering noise before the LLM generates summaries. This reduces the opportunity for extrinsic hallucinations by constraining the input to medically salient passages.
- Core assumption: Sentences containing medical entities are necessary and sufficient for faithful summarization; noise reduction does not discard critical information.
- Evidence anchors:
  - [section 3.2] "We apply the TextRank algorithm to extract sentences containing medical entities and query-related words. This guarantees that summaries remain faithful to medically important content before abstractive generation by the LLM."
  - [table 1] FT + TR improves SummaC from 0.31 to 0.37 and AlignScore from 38.45 to 45.65 compared to fine-tuning alone.
  - [corpus] Related work on span-level fine-tuning (arXiv:2510.09915) similarly shows that constraining model attention to salient spans reduces hallucinations.
- Break condition: If TextRank discards sentences with implicit medical relevance (e.g., contextual symptoms not containing explicit entities), faithfulness could degrade for complex queries.

### Mechanism 2
- Claim: Medical NER integration preserves critical entities by making them explicit during preprocessing and guiding generation.
- Mechanism: Named Entity Recognition identifies medical terms (conditions, medications, symptoms) and negation markers. These are flagged during preprocessing, ensuring the model attends to entities that might otherwise be paraphrased away or omitted.
- Core assumption: Standard NER tools accurately capture medical entities in both English and Bangla consumer health language, which may contain informal terminology.
- Evidence anchors:
  - [section 3.1] "Identifying overlapping medical entities and negation terms to ensure critical information is retained."
  - [abstract] "Framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models to enhance faithfulness."
  - [corpus] No direct corpus comparison available for this specific NER+summarization combination; faithfulness gains are inferred from in-paper results.
- Break condition: If NER recall is low for informal consumer health language (e.g., "pounding head" instead of "headache"), critical entities may be missed.

### Mechanism 3
- Claim: Best-of-N selection using faithfulness metrics (SummaC) as the selector yields higher factual consistency than single-pass generation.
- Mechanism: Generate 3 candidate summaries, score each with SummaC or ROUGE-1, select the highest-scoring output. SummaC-based selection prioritizes factual consistency over lexical overlap.
- Core assumption: Faithfulness metrics (SummaC, AlignScore) correlate with human judgments of factual accuracy in medical summarization.
- Evidence anchors:
  - [table 1] Best-of-3 (SummaC select) achieves SummaC=0.57 vs. 0.40 for R1-based selection, a 42.5% relative improvement in faithfulness score.
  - [table 3] Cross-lingual replication shows SummaC-based selection improves faithfulness from 0.28 to 0.32 on BanglaCHQ-Summ.
  - [section 4.3] "82% of the summaries satisfied [faithfulness] criteria" in human evaluation, validating metric-based selection.
- Break condition: If faithfulness metrics have systematic blind spots (e.g., subtle medical contradictions they fail to detect), selection may optimize for metric gaming rather than true faithfulness.

## Foundational Learning

- Concept: **Faithfulness vs. Quality in Summarization**
  - Why needed here: The paper explicitly distinguishes faithfulness (factual consistency with source) from general quality (ROUGE, BERTScore). Prior work optimizes for overlap metrics that don't capture hallucinations.
  - Quick check question: Can a summary achieve high ROUGE but low faithfulness? (Yes—fluent but hallucinated content can overlap with reference while contradicting source.)

- Concept: **Extractive-Abstractive Hybrid Pipelines**
  - Why needed here: The framework uses TextRank (extractive) before LLM generation (abstractive). Understanding this split is essential for debugging: is an error from extraction or generation?
  - Quick check question: Which stage would you blame if a summary omits a critical medication mentioned in the source? (Likely extraction—check if TextRank retained that sentence.)

- Concept: **Intrinsic vs. Extrinsic Hallucinations**
  - Why needed here: Section 2 references both error types—intrinsic (contradicting source) and extrinsic (adding unsupported facts). Different mitigation strategies apply.
  - Quick check question: If a summary states "patient is diabetic" but the source says "patient denies diabetes," what error type is this? (Intrinsic—contradiction.)

## Architecture Onboarding

- Component map: Consumer Health Question -> Preprocessing (NER + Negation Detection) -> TextRank (Sentence Extraction) -> LLaMA-2-7B + LoRA (Fine-tuned Generation) -> Selector (SummaC/ROUGE) -> Final Summary

- Critical path:
  1. NER must correctly identify entities before TextRank filtering.
  2. TextRank must retain sentences with entities; discarded sentences are lost.
  3. Fine-tuning data quality (MeQSum, BanglaCHQ-Summ) determines model behavior.
  4. Selection metric choice determines quality-faithfulness tradeoff.

- Design tradeoffs:
  - Temperature: Lower (~0.1) favors ROUGE; higher (~0.7) favors SummaC. Paper uses 0.7 for faithfulness.
  - Selection metric: SummaC prioritizes faithfulness; R1 prioritizes lexical overlap. SummaC recommended for medical safety.
  - Fine-tuning vs. zero-shot: Fine-tuning essential—zero-shot SummaC is 0.28 vs. 0.57 with FT+selection.

- Failure signatures:
  - Low SummaC but high ROUGE: Model hallucinating fluent but unsupported content. Check selection metric.
  - Entity dropout in summaries: NER or TextRank may be filtering entity-bearing sentences. Audit extraction output.
  - Cross-lingual degradation: Bangla SummaC (0.32 max) lower than English (0.57). Likely resource gap in NER/training data.

- First 3 experiments:
  1. **Ablate TextRank**: Run fine-tuning without TextRank extraction on a held-out set. Compare SummaC scores to quantify extraction's contribution.
  2. **NER error analysis**: Sample 50 queries, manually check NER recall on informal medical terms. Identify systematic gaps.
  3. **Best-of-N scaling**: Test N=1, 3, 5, 10 candidates. Plot SummaC vs. inference cost to find diminishing returns point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the framework generalize to other LLM families and sizes beyond LLaMA-2-7B?
- Basis in paper: [explicit] "A limitation of this study is that experiments were conducted on a single LLM (LLaMA-2-7B)... leaving scope for future work to explore multiple LLMs"
- Why unresolved: Only one model was tested; different architectures may respond differently to TextRank and NER guidance.
- What evidence would resolve it: Experiments applying the same framework to other models (e.g., Mistral, GPT, larger LLaMA variants) on the same datasets.

### Open Question 2
- Question: Can the framework be effectively adapted to few-shot settings without full fine-tuning?
- Basis in paper: [explicit] "Future directions include adapting the framework to few-shot... settings"
- Why unresolved: Current results rely on LoRA fine-tuning; the extractive guidance may not provide sufficient signal in few-shot prompting.
- What evidence would resolve it: Comparison of few-shot performance with and without TextRank/NER guidance across multiple shot counts.

### Open Question 3
- Question: Does the approach transfer to other high-stakes domains such as legal or financial text summarization?
- Basis in paper: [explicit] "Future directions include... extending to other sensitive domains (e.g., legal and financial texts)"
- Why unresolved: Medical NER is domain-specific; it is unclear whether similar entity-centric extraction would work for legal/financial documents.
- What evidence would resolve it: Application of adapted frameworks (with domain-specific NER) to legal/financial summarization datasets with faithfulness evaluation.

## Limitations
- Bangla NER dependency: Framework's cross-lingual performance hinges on availability and accuracy of Bangla medical NER, which is unspecified in the paper
- LoRA hyperparameter omission: Fine-tuning details such as LoRA rank, alpha, learning rate schedule, and epochs are unspecified, affecting reproducibility
- Faithfulness metric blind spots: SummaC and AlignScore have known limitations in detecting subtle medical contradictions, with no validation against additional benchmarks

## Confidence
- High confidence: ROUGE/SummaC improvements on MeQSum are well-supported by in-paper comparisons and align with extractive-abstractive hybrid literature
- Medium confidence: Cross-lingual replication on BanglaCHQ-Summ shows consistent trends but lower absolute scores, limited by unspecified NER and fine-tuning details
- Low confidence: Claim that SummaC-based selection outperforms ROUGE-based selection in all medical summarization contexts may be dataset-specific

## Next Checks
1. **Ablate TextRank**: Fine-tune LLaMA-2-7B without TextRank extraction on a held-out MeQSum subset. Compare SummaC scores to quantify extraction's contribution to faithfulness.
2. **NER error analysis**: Sample 50 Bangla and 50 English queries; manually check medical entity recall in NER outputs. Identify systematic gaps in informal consumer health terminology.
3. **Best-of-N scaling**: Test N=1, 3, 5, 10 candidates. Plot SummaC vs. inference cost to identify diminishing returns and optimal selection strategy.