---
ver: rpa2
title: The Role of Explanation Styles and Perceived Accuracy on Decision Making in
  Predictive Process Monitoring
arxiv_id: '2506.16617'
source_url: https://arxiv.org/abs/2506.16617
tags:
- explanation
- explanations
- accuracy
- styles
- perceived
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how explanation styles and perceived AI
  accuracy influence decision-making in predictive process monitoring (PPM). Using
  loan application data from BPIC 2017, the authors trained a random forest model
  and generated explanations using three styles: feature importance (LIME), rule-based
  (Anchor), and counterfactual (DiCE).'
---

# The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring

## Quick Facts
- **arXiv ID**: 2506.16617
- **Source URL**: https://arxiv.org/abs/2506.16617
- **Reference count**: 38
- **Primary result**: Counterfactual explanations significantly improve task performance in predictive process monitoring, especially when AI accuracy is perceived as low.

## Executive Summary
This study investigates how different explanation styles and perceived AI accuracy influence decision-making in predictive process monitoring (PPM) using loan application data. The authors conducted a user experiment with 179 participants across six groups, comparing feature importance (LIME), rule-based (Anchor), and counterfactual (DiCE) explanations under high and low accuracy conditions. Results show counterfactual explanations significantly improved task performance, particularly in low-accuracy scenarios, while lower perceived accuracy paradoxically led to better initial task performance due to increased critical engagement. Feature importance explanations decreased agreement in low-accuracy conditions, while rule-based explanations achieved highest satisfaction but didn't significantly improve task performance.

## Method Summary
The study used BPIC 2017 event log data, preprocessing it through prefix extraction with state-based bucketing and aggregation encoding. A random forest classifier was trained to predict loan application outcomes with 85% accuracy. Three explanation styles were generated: LIME for feature importance, Anchor for rule-based explanations, and DiCE for counterfactual explanations. Participants (n=179) were randomly assigned to six groups (2 accuracy levels × 3 explanation styles) and evaluated on four loan application cases (2 correct, 2 incorrect predictions). Decision performance, agreement with AI predictions, and confidence were measured both before and after explanations were shown.

## Key Results
- Counterfactual explanations significantly improved task performance (p = 0.002), especially in low-accuracy groups
- Lower perceived accuracy (63% vs 96%) led to better initial task performance due to increased critical engagement
- Feature importance explanations decreased agreement in low-accuracy scenarios (p = 0.025)
- Rule-based explanations achieved highest satisfaction scores but did not significantly improve task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual explanations improve task performance by enabling users to simulate alternative outcomes ("what-if" reasoning), which aids in identifying incorrect predictions.
- **Mechanism:** Counterfactuals provide actionable bounds on how much a feature must change to flip the decision, allowing users to stress-test the AI's logic, particularly effective when AI reliability is perceived as lower.
- **Core assumption:** Users possess sufficient domain literacy to interpret hypothetical scenarios and relate them back to current case data.
- **Evidence anchors:** Results show counterfactual explanations significantly improved task performance, especially in the low-accuracy group (p = 0.002).
- **Break condition:** If counterfactual examples are not feasible or too distant from the original data distribution, users may find them confusing rather than helpful.

### Mechanism 2
- **Claim:** Lower perceived AI accuracy triggers critical engagement (skepticism), which improves initial task performance by mitigating automation bias.
- **Mechanism:** When told an AI has low accuracy (63%), users shift from "compliance" mode to "audit" mode, scrutinizing outputs more carefully and catching errors they might miss if told the AI was highly accurate (96%).
- **Core assumption:** User decision-making capacity is sufficient to outperform specific AI errors when prompted to look for them.
- **Evidence anchors:** Lower perceived accuracy led to better initial task performance, suggesting users engage more critically when AI is perceived as less reliable.
- **Break condition:** If the task is too complex for human judgment without AI aid, increased scrutiny of a low-accuracy AI might lead to algorithm aversion.

### Mechanism 3
- **Claim:** Feature importance explanations reduce agreement in low-accuracy scenarios because they expose the model's internal logic without providing actionable guidance, leading users to distrust the reasoning.
- **Mechanism:** Feature importance shows which variables weighed heaviest, but if users suspect the AI is wrong and see features they disagree with highlighted, they lose confidence without receiving a "fix."
- **Core assumption:** Visual or numerical representation of weights is intuitive enough for users to identify spurious correlations or logic errors.
- **Evidence anchors:** Feature importance explanations decreased agreement in low-accuracy scenarios (mean difference of 0.27, p = 0.025).
- **Break condition:** If highlighted features align perfectly with the user's mental model, agreement might remain high regardless of perceived accuracy.

## Foundational Learning

- **Concept: Predictive Process Monitoring (PPM)**
  - **Why needed here:** The paper frames the experiment around predicting process outcomes (loan applications) using event logs. Understanding that PPM deals with sequences of events (traces) rather than static data is crucial for interpreting the "black box" context.
  - **Quick check question:** How does the sequential nature of event logs in PPM differ from standard classification datasets, and why might this complicate explanation generation?

- **Concept: Explanation Styles (LIME, Anchor, DiCE)**
  - **Why needed here:** The core independent variable is the style of explanation. Distinguishing between attribution (Feature Importance), rule-extraction (Anchors), and perturbation-based scenarios (Counterfactuals) is required to understand the performance variance.
  - **Quick check question:** Which explanation style provides a sufficient condition for a prediction (Anchor) versus a necessary feature contribution (LIME)?

- **Concept: Automation Bias & Algorithm Aversion**
  - **Why needed here:** Results rely on how "perceived accuracy" changes human behavior. Understanding the psychological tendency to over-trust high-status machines or under-trust machines after seeing errors explains the performance shifts.
  - **Quick check question:** Why does telling a user an AI is "highly accurate" potentially degrade their own task performance in a decision-support context?

## Architecture Onboarding

- **Component map:**
  - Data Layer: BPIC 2017 Event Log → Prefix Extraction & Bucketing
  - Model Layer: Random Forest "Black Box" (85% actual accuracy) → Prediction Engine
  - Explanation Layer: LIME Module (Feature Importance) → Anchor Module (Rule-based) → DiCE Module (Counterfactual)
  - Interface Layer: Decision Support UI → Displays Prediction + [Explanation] + Accuracy Label (Manipulated)

- **Critical path:**
  1. Ingest: Load loan application traces
  2. Predict: Model generates accept/reject probability
  3. Explain: Based on user group, generate Explanation Object (e.g., "If income was > $5k, loan would be approved")
  4. Render: Display prediction with assigned "Perceived Accuracy" label (High/Low) and Explanation Object
  5. Record: Capture "Before Explanation" decision → Show Explanation → Capture "After Explanation" decision

- **Design tradeoffs:**
  - Model Selection: Random Forest used instead of deep learning models, trading state-of-the-art temporal modeling for stability and easier explanation generation
  - Explanation Fidelity vs. Comprehensibility: Counterfactuals improved performance but had low satisfaction ratings, showing objective effectiveness may sacrifice user preference/simplicity
  - Static vs. Dynamic Accuracy: Static accuracy label (96% or 63%) used rather than per-prediction confidence scores, simplifying UI but removing nuance

- **Failure signatures:**
  - Blind Trust (High Acc. Group): Users defaulting to AI prediction without reading explanations, leading to lower task performance on "incorrect" AI cases
  - Confusion (CF Group): Users spending excessive time on counterfactuals with no satisfaction gain; implies cognitive overload despite performance gains
  - Rejection (FI Group): Users dropping agreement significantly in low-accuracy scenarios; implies Feature Importance acted as a "trust-killer" rather than a "trust-tuner"

- **First 3 experiments:**
  1. Sanity Check: Run survey with "High Accuracy" label but hide explanation to verify baseline automation bias
  2. Explanation Manipulation Check: For "Low Accuracy" group, verify users read explanation by tracking time-on-task
  3. Error Spotting: Feed system the 2 correct and 2 incorrect cases used in paper to verify DiCE generates plausible counterfactuals for incorrect cases

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the actual accuracy of a predictive model impact user trust and decision-making compared to the perceived accuracy labels used in this study?
  - Basis: Authors note they manipulated perceived accuracy to maintain consistent explanation content and suggest future work investigate true model accuracy effects
  - Why unresolved: Using models with intrinsically different accuracies alters explanation logic, making it difficult to isolate accuracy effects from explanation quality
  - What evidence would resolve it: Comparative study using models trained to different performance levels with controlled explanation fidelity

- **Open Question 2:** Do the effects of explanation styles on decision-making generalize to domain experts in real-world application settings?
  - Basis: Authors suggest conducting application-grounded evaluations involving domain experts to validate effectiveness in real-world contexts
  - Why unresolved: Study used students and crowd-workers; experts may possess prior knowledge that alters how they process rule-based versus counterfactual logic
  - What evidence would resolve it: Field study with process analysts or loan officers using proprietary, high-stakes event logs

- **Open Question 3:** Can natural language explanations generated by Large Language Models (LLMs) improve user engagement and interpretability compared to current XAI techniques?
  - Basis: Authors propose exploring novel explanation styles, such as human language-based explanations powered by LLMs
  - Why unresolved: Current techniques rely on visual or logical formats that may not align with human reasoning styles as naturally as conversational text
  - What evidence would resolve it: User study comparing LLM-generated textual explanations against structured styles tested here

## Limitations

- Sample size of 179 participants across six groups results in relatively small subgroups (~30 each), potentially limiting statistical power
- Controlled laboratory setting with loan application data may not fully capture real-world decision complexity where multiple stakeholders and time pressure affect judgment
- Random forest model, while providing stable explanations, may not represent the complexity of deep learning models commonly deployed in production PPM systems

## Confidence

- **High confidence**: Counterfactual explanations significantly improve task performance (p = 0.002)
- **Medium confidence**: Feature importance explanations decrease agreement in low-accuracy scenarios (p = 0.025)
- **Medium confidence**: Rule-based explanations achieve highest satisfaction scores but don't significantly improve task performance
- **Medium confidence**: Lower perceived accuracy improves initial task performance through increased critical engagement

## Next Checks

1. **Generalization Test**: Replicate the experiment with a different domain (e.g., manufacturing quality control) to verify if counterfactual explanations consistently improve performance across contexts
2. **Sample Size Sensitivity**: Conduct power analysis to determine minimum detectable effect sizes with current sample, then recruit additional participants to achieve adequate power (0.8) for subgroup comparisons
3. **Real-World Transfer**: Conduct a field study where practitioners use the explanation system in their actual workflow for 2-4 weeks, measuring performance differences between explanation styles in operational settings