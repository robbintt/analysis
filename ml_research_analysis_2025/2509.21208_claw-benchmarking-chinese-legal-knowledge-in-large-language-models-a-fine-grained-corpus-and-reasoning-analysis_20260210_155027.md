---
ver: rpa2
title: 'CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained
  Corpus and Reasoning Analysis'
arxiv_id: '2509.21208'
source_url: https://arxiv.org/abs/2509.21208
tags:
- legal
- article
- reasoning
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLaw, a benchmark designed to evaluate the
  depth of Chinese legal knowledge and case-based reasoning in large language models
  (LLMs). CLaw features a fine-grained, historically versioned corpus of all Chinese
  national statutes (64,849 entries) and 254 case-based reasoning tasks derived from
  authoritative Supreme Court materials.
---

# CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis

## Quick Facts
- arXiv ID: 2509.21208
- Source URL: https://arxiv.org/abs/2509.21208
- Authors: Xinzhe Xu; Liang Zhao; Hongshen Xu; Chen Chen
- Reference count: 34
- Key outcome: CLaw is a benchmark that evaluates Chinese legal knowledge and case-based reasoning in LLMs, revealing significant gaps in precise legal recall and statute citation accuracy, even among leading models.

## Executive Summary
This paper introduces CLaw, a comprehensive benchmark designed to evaluate the depth of Chinese legal knowledge and case-based reasoning in large language models (LLMs). CLaw features a fine-grained, historically versioned corpus of all Chinese national statutes (64,849 entries) and 254 case-based reasoning tasks derived from authoritative Supreme Court materials. The benchmark addresses a critical gap in existing legal AI evaluations by focusing on precise statutory recall and reasoning at the subparagraph level.

Empirical results demonstrate that even leading LLMs struggle significantly with precise legal recall, particularly at the subparagraph level, and often cite incorrect or outdated statutes. The study shows that effective domain-specific legal reasoning requires both deep, accurate knowledge mastery and strong general reasoning capabilities. CLaw provides a critical resource for advancing domain-specific LLM reasoning, especially in high-stakes legal applications where factual precision is essential.

## Method Summary
The paper introduces CLaw, a benchmark for evaluating Chinese legal knowledge and case-based reasoning in LLMs. CLaw comprises a fine-grained, historically versioned corpus of all Chinese national statutes (64,849 entries) and 254 case-based reasoning tasks derived from authoritative Supreme Court materials. The evaluation methodology involves testing LLMs on precise legal recall tasks, particularly at the subparagraph level, and assessing their ability to cite correct and current statutes. The study uses a structured approach to measure the models' performance in both knowledge mastery and reasoning capabilities, highlighting the challenges even leading LLMs face in accurately navigating complex legal texts.

## Key Results
- Even leading LLMs struggle significantly with precise legal recall, particularly at the subparagraph level.
- Models often cite incorrect or outdated statutes, highlighting gaps in knowledge mastery and reasoning capabilities.
- Effective domain-specific legal reasoning requires both deep, accurate knowledge mastery and strong general reasoning capabilities.

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its fine-grained, historically versioned corpus and authoritative case-based tasks, which test models' ability to precisely recall and reason with legal statutes. By focusing on subparagraph-level recall and requiring correct statute citations, CLaw exposes the limitations of current LLMs in handling domain-specific legal knowledge, emphasizing the need for both accurate knowledge mastery and robust reasoning capabilities.

## Foundational Learning
- **Chinese Legal System Structure**: Understanding the hierarchy and evolution of Chinese statutes is crucial for evaluating legal knowledge.
  - *Why needed*: Ensures the benchmark reflects the complexity and specificity of Chinese law.
  - *Quick check*: Verify the corpus includes all national statutes and their historical versions.

- **Subparagraph-Level Legal Recall**: Testing models' ability to recall specific legal details.
  - *Why needed*: Highlights the precision required in legal applications.
  - *Quick check*: Assess model performance on tasks requiring exact statute citations.

- **Case-Based Reasoning**: Evaluating models' ability to apply legal knowledge to real-world scenarios.
  - *Why needed*: Reflects the practical utility of legal reasoning in high-stakes applications.
  - *Quick check*: Review case-based tasks for diversity and relevance to authoritative sources.

## Architecture Onboarding
- **Component Map**: Legal Corpus (64,849 entries) -> Case-Based Tasks (254) -> LLM Evaluation
- **Critical Path**: Corpus ingestion -> Task formulation -> Model testing -> Performance analysis
- **Design Tradeoffs**: Granular subparagraph recall vs. broader contextual understanding; historical accuracy vs. current relevance
- **Failure Signatures**: Incorrect statute citations, reliance on outdated laws, inability to handle ambiguous legal texts
- **First Experiments**:
  1. Test model performance on subparagraph-level recall tasks.
  2. Evaluate accuracy of statute citations in case-based reasoning.
  3. Assess model adaptation to historical vs. current legal versions.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on Chinese national statutes may not capture the full spectrum of Chinese legal practice.
- Emphasis on subparagraph-level recall may overlook the importance of contextual and analogical reasoning.
- Reliance on Supreme Court materials may not reflect the diversity of legal reasoning across judicial levels.

## Confidence
- **High Confidence**: Empirical finding that leading LLMs struggle with precise legal recall, particularly at the subparagraph level.
- **Medium Confidence**: Claim that effective legal reasoning requires both knowledge mastery and general reasoning capabilities.
- **Low Confidence**: Assertion that CLaw is critical for advancing domain-specific LLM reasoning in high-stakes legal applications.

## Next Checks
1. Test CLaw's applicability and effectiveness in evaluating legal reasoning across other jurisdictions and legal systems.
2. Develop and incorporate tasks that evaluate models' ability to apply legal knowledge in context, such as analogical reasoning.
3. Introduce scenarios simulating dynamic legal environments, including updates to statutes and evolving case law, to assess model adaptation.