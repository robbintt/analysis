---
ver: rpa2
title: 'Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and
  Practical Insights'
arxiv_id: '2509.05142'
source_url: https://arxiv.org/abs/2509.05142
tags:
- methods
- learning
- parameters
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey systematically examines the intersection of federated\
  \ learning and foundational models, reviewing 42 distinct methods that integrate\
  \ the two paradigms. The authors present a novel taxonomy organized by development\
  \ lifecycle stages\u2014training, customization, and deployment\u2014and further\
  \ classify methods based on algorithmic techniques."
---

# Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights

## Quick Facts
- **arXiv ID:** 2509.05142
- **Source URL:** https://arxiv.org/abs/2509.05142
- **Reference count:** 9
- **Key outcome:** This survey systematically examines the intersection of federated learning and foundational models, reviewing 42 distinct methods that integrate the two paradigms. The authors present a novel taxonomy organized by development lifecycle stages—training, customization, and deployment—and further classify methods based on algorithmic techniques. Practical comparisons are provided using criteria of complexity, efficiency, and scalability. The survey highlights key challenges such as communication overhead, personalization versus generalization trade-offs, and limited real-world healthcare applications. Additive fine-tuning with adapters (e.g., LoRA) emerges as the most scalable and efficient approach for customizing foundational models in federated settings. Hybrid methods combining compression, quantization, and knowledge distillation offer promising directions. Overall, the work provides actionable guidance and identifies future research opportunities, especially in privacy-preserving, scalable, and clinically applicable federated foundational model systems.

## Executive Summary
This survey systematically maps the landscape of integrating foundational models (FMs) with federated learning (FL), organizing 42 distinct methods into a novel taxonomy based on the FM development lifecycle (training, customization, deployment) and algorithmic techniques. It identifies key challenges such as communication overhead, personalization versus generalization trade-offs, and limited real-world applications, particularly in healthcare. The survey concludes that additive fine-tuning with adapters like LoRA is the most scalable and efficient approach, while hybrid methods combining compression, quantization, and distillation show promise for future work. Practical insights and actionable guidance are provided for researchers and practitioners navigating this emerging field.

## Method Summary
The survey conducts a systematic review of federated learning methods applied to foundational models, classifying approaches across three lifecycle stages: training (pre-training FMs in FL), customization (fine-tuning or adapting pre-trained FMs), and deployment (using FMs in FL tasks). Within customization, methods are further categorized by algorithmic technique: full fine-tuning, partial fine-tuning, and additive fine-tuning (e.g., LoRA, adapters). The authors evaluate these methods using practical criteria—complexity, efficiency, and scalability—drawing comparisons from the literature. The primary recommendation is to adopt additive fine-tuning with LoRA using Federated Averaging for optimal trade-offs in real-world federated settings.

## Key Results
- Additive fine-tuning with adapters (e.g., LoRA) is identified as the most scalable and efficient approach for customizing foundational models in federated learning.
- Hybrid methods combining compression, quantization, and knowledge distillation offer promising directions for improving communication efficiency and performance.
- Key challenges include communication overhead, personalization versus generalization trade-offs, and limited real-world healthcare applications.
- The survey provides a novel taxonomy organizing 42 methods by FM lifecycle stages and algorithmic techniques, offering actionable guidance for future research.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If you constrain model updates to low-rank subspaces (LoRA) rather than full weight matrices, you can drastically reduce the communication overhead in federated learning without significantly sacrificing the adaptation capacity of the foundational model.
- **Mechanism:** Low-Rank Adaptation (LoRA) freezes the pre-trained model weights and injects trainable rank decomposition matrices into the transformer layers. In a federated setting, clients only transmit these small, decomposed matrices ($A$ and $B$) instead of the full gradient updates. The server aggregates these low-rank matrices using Federated Averaging (FedAVG), effectively updating the global model direction with a fraction of the data transfer size.
- **Core assumption:** The model's adaptation needs for specific tasks have a low "intrinsic dimension," meaning significant performance gains can be achieved by optimizing in a much smaller parameter space than the full model dimension.
- **Evidence anchors:**
  - [abstract]** "Additive fine-tuning with adapters (e.g., LoRA) emerges as the most scalable and efficient approach..."
  - [section]** [Customize > Fine-tuning > Additive] The text details how LoRA matrices are aggregated and cites Zhang et al. (2023) showing additive fine-tuning reduces communication costs "by factors ranging from 12 to 190 times."
  - [corpus]** Neighbor paper "PEFT A2Z" supports the general efficacy of Parameter-Efficient Fine-Tuning, though specific federated scaling factors are derived from the primary text.
- **Break condition:** The mechanism may fail (performance collapse) if the rank of the adaptation ($r$) is set too low to capture the complexity of the local data distribution, or if "client drift" causes the local low-rank spaces to become incoherent, though the paper suggests methods like FedProx or regularization mitigate this.

### Mechanism 2
- **Claim:** If a shared public dataset or synthetic data generator is available, Knowledge Distillation (KD) can decouple the communication of model knowledge from the communication of model parameters, enabling collaboration between models of vastly different architectures.
- **Mechanism:** Instead of sharing weights (which requires homogeneous architectures), local clients train "teacher" models on private data. The server generates synthetic data (or uses public data) and distributes it. Clients compute "logits" (probability outputs) on this data and send these soft-labels back. The global model is then trained to mimic these logits (minimizing KL-divergence), effectively learning the "knowledge" without seeing private data or requiring identical network structures.
- **Core assumption:** There exists or can be generated a dataset that is representative enough of the input domain to serve as a universal medium for exchanging knowledge via logits, and that logits capture sufficient semantic information.
- **Evidence anchors:**
  - [abstract]** Mentions "distillation" as a key intersecting topic and a component of efficient hybrid methods.
  - [section]** [Customize > Contraction > Distillation] Describes the mechanism: "This process is equivalent to training local models on local datasets and then distilling this knowledge into the FM using a shared dataset."
  - [corpus]** The corpus neighbors (e.g., "Federated Learning Survey") discuss aggregation techniques, but specific distillation mechanisms for FMs are primarily detailed in the provided text sections on "Generative knowledge distillation."
- **Break condition:** This mechanism breaks if the synthetic/public data lacks the diversity or distributional properties of the private data (domain gap), leading to "knowledge gaps" where the global model fails to learn critical features present only in private silos.

### Mechanism 3
- **Claim:** If foundational models are treated as frozen "black boxes," federated learning can be used to train input prompts (Test-Time Adaptation) or input encoders, allowing users with limited compute resources to participate in training.
- **Mechanism:** Instead of updating internal weights (backpropagation), the algorithm optimizes a set of "continuous prompts"—learnable input vectors—or a lightweight input auto-encoder. In the federated round, only these prompt vectors or encoder weights are communicated and averaged. The massive FM remains untouched, running purely in inference mode.
- **Core assumption:** The pre-trained foundational model is sufficiently general and robust that steering its behavior via input modification (prompt engineering) is sufficient to achieve task-specific performance without internal weight updates.
- **Evidence anchors:**
  - [section]** [Train > Test-time adaptation] Describes methods like Guo et al. (2023b) where "continuous prompts... are learnable vectors... optimized only these parameters in FL."
  - [table]** Table 3 ranks Test-time adaptation as having high scalability (***) and high efficiency (***).
  - [corpus]** Corpus evidence for this specific "Prompt/Black-box FL" mechanism is weak relative to the detailed breakdown in the text; general FL surveys focus on weight aggregation.
- **Break condition:** The mechanism fails if the FM's internal representations are fundamentally misaligned with the local task (e.g., a general vision model applied to highly specialized medical imaging), where input prompting cannot bridge the representation gap.

## Foundational Learning

- **Concept: Federated Averaging (FedAVG)**
  - **Why needed here:** It is the default aggregation primitive referenced by almost every method in the paper. Understanding how local updates are weighted and merged is essential to grasp how "global knowledge" is formed.
  - **Quick check question:** How does FedAVG handle clients with different dataset sizes when averaging updates?

- **Concept: Foundational Model (FM) Pre-training (SSL/Contrastive)**
  - **Why needed here:** The "Train" phase of the taxonomy relies on adapting pre-training techniques (like BYOL or SimCLR) to federated settings. You must understand self-supervised learning to understand why "EMA updates" or "contrastive loss" are being aggregated.
  - **Quick check question:** Why do contrastive learning methods in FL often require an "Exponential Moving Average (EMA)" of the global model to stabilize training?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** The paper heavily emphasizes "Additive" customization (LoRA/Adapters) as the most practical approach. Understanding the difference between updating *all* weights vs. adding *adapter* layers is the core tradeoff in the "Customize" section.
  - **Quick check question:** In the context of LoRA, if a weight matrix is $W_0$, how does the math $W_0 x + BAx$ represent a memory-efficient update?

## Architecture Onboarding

- **Component map:**
  - Global Server -> Aggregator (FedAVG or variants) -> Public/Synthetic Dataset Generator (for distillation cases)
  - Client Node -> Foundational Model (often frozen) -> Local Adapter/LoRA Modules (trainable) -> Local Private Data
  - Communication Channel -> Gradients/Deltas (for fine-tuning) -> Prototypes/Embeddings (for partial training) -> Logits (for distillation)

- **Critical path:**
  1. **Initialization:** Deploy the pre-trained FM to all clients (or assume it exists).
  2. **Adapter Injection:** Insert lightweight adapters (LoRA or classification heads) into the client architecture.
  3. **Local Optimization:** Clients train *only* the adapter parameters on private data.
  4. **Aggregation:** Clients transmit adapters; Server aggregates (FedAVG) and redistributes updated adapters.
  5. **Iterate:** Repeat until convergence.

- **Design tradeoffs:** Based on Table 3 ("Practical perspectives"), you must balance:
  - **Complexity** vs. **Scalability:** Advanced methods (Hetero-LoRA, Hybrid) offer better performance but are harder to implement and debug than simple additive adapters.
  - **Communication Efficiency** vs. **Performance:** Transmitting only bias terms or quantized gradients is fast but may lose accuracy compared to full LoRA updates.

- **Failure signatures:**
  - **Heterogeneity Collapse:** Local adapters diverge so much (due to non-IID data) that averaging them produces a "globally general but locally useless" model.
  - **Communication Bottleneck:** Attempting "Entire Model Training" (Section 5.1) without heavy compression causes the system to stall.
  - **Privacy Leakage:** In "Test-time Adaptation" or "Prompt Tuning," malicious actors might invert the shared prompts/logits to reconstruct private data attributes (though the paper notes this is less risky than sharing gradients).

- **First 3 experiments:**
  1. **Baseline Adapter:** Implement "Additive Fine-Tuning" using a simple classification head (top layers only) on a frozen FM to establish a performance baseline (High Scalability/Low Complexity).
  2. **LoRA Comparison:** Upgrade the adapter to LoRA (Homogeneous) to measure the gain in communication efficiency vs. the added complexity of matrix decomposition.
  3. **Hybrid Stress Test:** Apply Quantization to the LoRA updates (Hybrid approach) to see if you can maintain performance while pushing communication costs to the theoretical minimum.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can advanced aggregation mechanisms (e.g., adaptive or hierarchical) improve convergence and robustness in federated foundational models compared to standard Federated Averaging?
- Basis in paper: [explicit] The authors state that "exploring more adaptive aggregators (e.g., FedAdam) or innovative pre-training techniques could yield more efficient and effective integration," as most current methods rely on FedAvg.
- Why unresolved: The literature predominantly utilizes simple aggregation primitives like FedAvg, leaving the potential benefits of adaptive or personalized aggregation strategies for foundational models underexplored.
- What evidence would resolve it: Comparative studies demonstrating improved convergence speed and final model performance using adaptive aggregators versus FedAvg baselines during federated pre-training or fine-tuning.

### Open Question 2
- Question: What are the specific trade-offs between privacy preservation (e.g., differential privacy) and model performance when fine-tuning foundational models in federated learning?
- Basis in paper: [inferred] The authors note that the "effectiveness of differential privacy techniques may be diminished when applied to pre-trained models" and highlight a lack of integrated privacy guarantees in current methods.
- Why unresolved: While privacy is a core motivation, the impact of privacy-enhancing technologies on the utility and communication efficiency of large federated models is not well quantified.
- What evidence would resolve it: Empirical benchmarks quantifying utility loss and leakage risks when applying differential privacy or secure aggregation to parameter-efficient fine-tuning methods like LoRA.

### Open Question 3
- Question: How can standardized benchmarks and datasets be developed to evaluate the generalizability and scalability of federated foundational models across different domains?
- Basis in paper: [explicit] The survey concludes there is a need for "more comprehensive benchmarks and integrated datasets," observing that existing algorithms "are not currently evaluated on similar data."
- Why unresolved: Current methods are tested on disparate datasets and modalities, making fair comparisons of complexity, efficiency, and scalability difficult.
- What evidence would resolve it: The creation of a unified benchmark suite that evaluates methods across consistent metrics and data distributions, facilitating direct comparison of distinct federated strategies.

## Limitations
- The survey aggregates results from 42 methods without specifying the exact experimental setups or hyper-parameters, making direct replication challenging.
- Implementation details for nuanced aggregation strategies (e.g., handling heterogeneous adapter ranks) are described only conceptually, not algorithmically.
- Real-world applicability, especially in healthcare, is largely theoretical with limited concrete case studies or empirical validation.

## Confidence
- **High Confidence:** The survey's structural organization (taxonomy by lifecycle stage and technique) and its identification of additive fine-tuning with LoRA as the most scalable and efficient approach are well-supported by the cited literature and comparative tables.
- **Medium Confidence:** The practical insights regarding complexity-efficiency-scalability trade-offs are derived from the survey's analysis, but the exact quantitative thresholds for these trade-offs are not specified.
- **Low Confidence:** Claims about the performance of hybrid methods (e.g., LoRA + Quantization) in extreme resource-constrained scenarios lack specific empirical validation within the survey itself.

## Next Checks
1. **Reproduce the Baseline:** Implement Federated LoRA (additive fine-tuning) using a standard pre-trained model (e.g., DistilBERT) and an FL simulation framework (e.g., Flower). Measure the communication cost and accuracy compared to full fine-tuning to verify the claimed efficiency gains.
2. **Test Heterogeneity Handling:** Simulate a federated environment with clients having non-IID data and heterogeneous adapter ranks. Implement the server-side aggregation strategies (padding, weighting) mentioned for handling mismatched dimensions and assess model stability.
3. **Validate Domain Gap Mitigation:** Design an experiment where a shared public dataset is used for knowledge distillation. Measure the performance gap between this approach and centralized training to quantify the impact of potential "knowledge gaps" due to domain mismatch.