---
ver: rpa2
title: Hybrid Agentic AI and Multi-Agent Systems in Smart Manufacturing
arxiv_id: '2511.18258'
source_url: https://arxiv.org/abs/2511.18258
tags:
- manufacturing
- framework
- agentic
- maintenance
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid agentic AI and multi-agent systems
  (MAS) framework for prescriptive maintenance (RxM) in smart manufacturing. The approach
  combines LLM-based orchestration agents for strategic reasoning with rule-based
  and small language model agents for efficient, domain-specific tasks on edge devices.
---

# Hybrid Agentic AI and Multi-Agent Systems in Smart Manufacturing

## Quick Facts
- arXiv ID: 2511.18258
- Source URL: https://arxiv.org/abs/2511.18258
- Reference count: 40
- Primary result: Hybrid agentic AI and MAS framework for prescriptive maintenance achieving 97.2% classification accuracy and interpretable maintenance recommendations

## Executive Summary
This paper presents a hybrid agentic AI and multi-agent systems framework for prescriptive maintenance in smart manufacturing, combining LLM-based orchestration agents for strategic reasoning with rule-based and small language model agents for efficient, domain-specific tasks. The approach uses a layered architecture integrating perception, preprocessing, analytics, and optimization layers, coordinated by an LLM Planner Agent. Evaluated on two industrial datasets, the system successfully demonstrated autonomous end-to-end workflows for classification, regression, and anomaly detection tasks, achieving high accuracy and interpretable maintenance recommendations. The hybrid design bridges high-level agentic reasoning with low-level autonomous execution, showing promise for scalable, explainable, and adaptive RxM in dynamic manufacturing environments.

## Method Summary
The framework employs a layered multi-agent system with an LLM Planner (Gemini 2.5) for orchestration and SLMs (Qwen3:4B via Ollama) for tactical decisions. The system automatically detects schema, adapts preprocessing pipelines based on data characteristics (e.g., >20% missing values triggers KNN Imputer), and uses statistical thresholding for maintenance prioritization. The architecture includes perception, preprocessing, analytics, and optimization layers, with human-in-the-loop validation for transparency. Two industrial datasets were used for evaluation: Smart Manufacturing Maintenance Dataset (SMMD) with 1,430 records and 6G-Enabled Intelligent Manufacturing Resource Dataset (6GMR) with approximately 100k records.

## Key Results
- Achieved 97.2% classification accuracy in maintenance priority prediction
- Successfully demonstrated autonomous end-to-end workflows for classification, regression, and anomaly detection tasks
- Generated interpretable maintenance recommendations using statistical prioritization (Mean + 2×Std thresholds)

## Why This Works (Mechanism)

### Mechanism 1
Separating high-level reasoning (orchestration) from low-level execution (specialized agents) enables adaptability without sacrificing operational reliability. An LLM Planner Agent maintains global context and selects tools using chain-of-thought reasoning, while rule-based systems and SLMs execute specific tasks deterministically or locally. The framework includes 3-retry limits and automatic fallback to rule-based approaches when LLM outputs are invalid or incomplete.

### Mechanism 2
Dynamic schema discovery allows the system to generalize across disparate manufacturing datasets without manual reconfiguration. The Perception and Preprocessing agents analyze metadata to classify columns and trigger conditional preprocessing logic based on statistical heuristics rather than static pipelines. This enables automatic adaptation to different data structures and characteristics.

### Mechanism 3
Translating predictive scores into prescriptive maintenance actions relies on statistical thresholding to maintain interpretability and human trust. The Optimization Agent converts model outputs into priority scores using standardized z-scores and fixed thresholds, mapping these to ranked actionable recommendations that are easily understood by maintenance personnel.

## Foundational Learning

- **LLM-as-Orchestrator Pattern**: The LLM manages the workflow (deciding what to run) rather than performing the analytics. Quick check: If the LLM goes offline, can the system still process data? (Check Section 3.2.1 for fallback behavior).

- **Prescriptive vs. Predictive Maintenance**: The system shifts from "When will it fail?" (Predictive) to "What do I do now and how much will it cost?" (Prescriptive). Quick check: Does the output stop at a probability score, or does it include cost/time estimates and specific interventions?

- **Edge-Cloud Hybridity**: Understanding which intelligence runs where (Cloud LLM vs. Local SLM/Rule-based) is critical for latency and privacy constraints. Quick check: Which component handles sensitive data locally to avoid transmission to off-premise servers?

## Architecture Onboarding

- **Component map**: Data Ingestion -> Schema Discovery -> LLM Tool Selection -> Preprocessing Pipeline Construction -> Model Training -> Optimization (Thresholding) -> HITL Review
- **Intelligence Layer**: LLM Planner (Gemini 2.5) + SLM (Qwen3:4B/Ollama) for orchestration
- **Perception Layer**: Loads data, computes metadata
- **Preprocessing Layer**: Schema discovery + Pipeline construction (Impute/Scale/Encode)
- **Analytics Layer**: Model selection (RF, SVM, etc.) + Training + Adaptive Intelligence
- **Optimization Layer**: Priority scoring + Recommendation generation
- **HITL Interface**: CLI for audit/approval

- **Design tradeoffs**: Cloud LLM introduces network latency vs. local SLMs which are faster but less contextually aware; generic schema discovery rules may misclassify specific industrial sensor data
- **Failure signatures**: Infinite loops (LLM repeatedly selects same failing tool), silent data corruption (preprocessing drops high-cardinality features incorrectly), threshold instability (Optimization flags everything as "Critical")
- **First 3 experiments**: 
  1. Schema Robustness Test: Inject dataset with >20% missing values and high-cardinality categorical columns
  2. Fallback Verification: Sever Gemini API connection to confirm rule-based planner fallback
  3. Threshold Sensitivity Analysis: Run Optimization Agent on synthetic data with varying distributions

## Open Questions the Paper Calls Out

1. **Real-time scalability and latency**: Can the framework maintain acceptable latency and robustness when processing continuous, high-volume IIoT data streams in real-time? The current validation relied on offline datasets, and LLM reasoning steps may restrict real-time responsiveness.

2. **Safety verification mechanisms**: What mechanisms are required to provide stronger safety verification for LLM-driven reasoning in safety-critical manufacturing scenarios? While HITL interfaces provide transparency, they don't guarantee automated safety constraints for novel or edge-case inputs.

3. **Agent-to-agent communication**: How does the introduction of direct agent-to-agent communication impact collective reasoning and coordination efficiency? The current architecture relies primarily on centralized LLM Planner orchestration rather than distributed inter-agent negotiation.

## Limitations

- Reliance on cloud-based LLM orchestration introduces potential latency and availability bottlenecks in real-time manufacturing environments
- Schema discovery heuristics may struggle with complex industrial datasets containing mixed data types or non-standard sensor readings
- Human-in-the-loop validation creates potential workflow bottlenecks and scalability challenges for high-frequency maintenance decisions

## Confidence

**High Confidence Claims:**
- Layered architecture design with separate perception, preprocessing, analytics, and optimization layers is technically sound
- Fallback mechanism from LLM to rule-based systems provides operational resilience
- Statistical thresholding approach for prioritization is straightforward to implement and validate

**Medium Confidence Claims:**
- System's ability to generalize across different manufacturing datasets without manual reconfiguration
- Hybrid edge-cloud intelligence distribution effectively balances latency and reasoning capability
- 97.2% classification accuracy is achievable with specified datasets and parameters

**Low Confidence Claims:**
- Long-term stability of LLM orchestrator under continuous manufacturing conditions
- Scalability of human-in-the-loop validation process for high-frequency maintenance decisions
- Performance consistency across diverse manufacturing domains beyond tested datasets

## Next Checks

1. **Schema Discovery Robustness**: Test automated schema classification on industrial datasets with ambiguous column structures to validate heuristic accuracy and identify failure patterns.

2. **Edge Case Fallback Testing**: Simulate prolonged LLM unavailability by repeatedly disrupting API connection, measuring how often system successfully defaults to rule-based planning and whether critical workflow steps remain functional.

3. **Distribution Sensitivity Analysis**: Evaluate optimization agent's thresholding behavior on synthetic maintenance datasets with varying statistical distributions to determine if Mean + 2×Std approach remains effective or produces excessive false positives/negatives.