---
ver: rpa2
title: 'FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution
  and Deblurring'
arxiv_id: '2512.04390'
source_url: https://arxiv.org/abs/2512.04390
tags:
- fma-net
- exposure
- video
- motion
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FMA-Net++ tackles joint video super-resolution and deblurring (VSRDB)
  under unknown, dynamically varying exposure, a common artifact in real-world capture.
  It introduces an Exposure Time-aware Modulation (ETM) layer that conditions features
  on per-frame exposure, enabling the exposure-aware Flow-Guided Dynamic Filtering
  (FGDF) module to estimate motion- and exposure-aware degradation kernels.
---

# FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring

## Quick Facts
- **arXiv ID:** 2512.04390
- **Source URL:** https://arxiv.org/abs/2512.04390
- **Reference count:** 40
- **Primary result:** State-of-the-art PSNR/SSIM/tOF on joint VSRDB benchmarks (REDS-ME/REDS-RE, GoPro) with strong real-world generalization

## Executive Summary
FMA-Net++ addresses joint video super-resolution and deblurring (VSRDB) under unknown, dynamically varying exposure conditions common in real-world capture. It introduces Exposure Time-aware Modulation (ETM) to condition features on per-frame exposure, enabling the exposure-aware Flow-Guided Dynamic Filtering (FGDF) module to estimate motion- and exposure-aware degradation kernels. Built on Hierarchical Refinement with Bidirectional Propagation (HRBP) blocks, the architecture expands temporal receptive fields hierarchically while enabling parallel processing. FMA-Net++ introduces REDS-ME and REDS-RE benchmarks for evaluating VSRDB under multi-exposure and random-exposure conditions, achieving state-of-the-art performance trained only on synthetic data.

## Method Summary
FMA-Net++ uses a three-stage training procedure on synthetic REDS-ME data. First, a frozen Exposure Time-aware Feature Extractor (ETE) pretrained with contrastive loss provides per-frame exposure embeddings. Second, Net_D predicts degradation kernels and flow-mask pairs through M HRBP blocks with ETM. Third, Net_R jointly refines features with degradation-aware attention to produce the final high-resolution output. The architecture decouples degradation learning from restoration, using asymmetric upsampling for efficiency. Key hyperparameters include M=4 HRBP blocks, n=9 multi-flow-mask pairs, and kernel size k_d=20.

## Key Results
- Achieves state-of-the-art PSNR/SSIM/tOF on REDS-ME, REDS-RE, and GoPro benchmarks
- Demonstrates strong real-world generalization trained only on synthetic data
- Introduces REDS-ME and REDS-RE benchmarks for joint VSRDB under multi-exposure and random-exposure conditions
- Shows efficient inference with competitive runtime performance

## Why This Works (Mechanism)

### Mechanism 1: Exposure Time-aware Modulation (ETM)
Conditioning temporal features on per-frame exposure enables adaptation to dynamically varying blur severity. A pretrained ETE produces per-frame embeddings u_i that encode exposure characteristics. These embeddings modulate features at every HRBP refinement stage via lightweight SFT layers, injecting exposure information throughout the architecture with minimal overhead. Core assumption: per-frame exposure conditions can be reliably inferred from RGB statistics and are sufficient to guide restoration.

### Mechanism 2: Exposure-aware Flow-Guided Dynamic Filtering (FGDF)
Estimating jointly motion- and exposure-aware degradation kernels enables physically grounded restoration that decouples degradation learning from reconstruction. FGDF performs filtering along motion trajectories using optical flow guidance. The predicted filter weights W_p become exposure-aware by operating on features already infused with exposure information via ETM. Core assumption: the discrete kernel formulation approximates the continuous physical degradation process sufficiently for learning.

### Mechanism 3: Hierarchical Refinement with Bidirectional Propagation (HRBP)
Hierarchical temporal aggregation overcomes the trade-off between limited receptive fields and sequential dependencies, enabling parallel long-range modeling. Each HRBP block refines features bidirectionally. Stacking M blocks hierarchically expands the temporal receptive field, enabling sequence-level parallelization while maintaining long-range context. Core assumption: hierarchical bidirectional propagation sufficiently approximates temporal dependencies without requiring sequential computation.

## Foundational Learning

- **Concept: Spatial Feature Transform (SFT)**
  - **Why needed:** ETM uses SFT layers to modulate features based on exposure embeddings
  - **Quick check:** Can you explain how SFT applies learned affine parameters (γ, β) to normalize and scale feature maps conditioned on auxiliary inputs?

- **Concept: Optical Flow-based Warping**
  - **Why needed:** HRBP blocks use occlusion-aware backward warping to propagate features across frames
  - **Quick check:** How does backward warping differ from forward warping, and why is it preferred for differentiable feature propagation?

- **Concept: Dynamic Filtering**
  - **Why needed:** FGDF generates position-dependent filter weights guided by optical flow
  - **Quick check:** How do dynamic filter networks differ from standard convolutions in how filter weights are determined and applied?

## Architecture Onboarding

- **Component map:** ETE (frozen) -> Net_D (HRBP+ETM) -> predicts K_i^D and f_D^M -> Net_R (HRBP+ETM+DA-attention) -> outputs Ŷ
- **Critical path:** 1) ETE extracts u_i from input X (frozen after pretraining) 2) Net_D processes X through HRBP+ETM → outputs K_i^D and f_D^M 3) Net_R receives X + F_D^M + f_D^M + K_i^D → refines through HRBP+ETM+DA-attention → outputs Ŷ
- **Design tradeoffs:** Asymmetric upsampling (bilinear + residual) saves 1.1M params, ~15% faster, comparable accuracy vs FGDF-based; n=9 multi-flow-mask pairs improve robustness at minimal cost; freezing ETE prevents representation drift but may limit adaptation
- **Failure signatures:** Temporal misalignment → insufficient HRBP blocks or unreliable flow; kernel over-concentration → incorrect ETE guidance; exposure mismatch artifacts → ETE generalization limits
- **First 3 experiments:** 1) Train FMA-Net++ with/without ETE on REDS-ME; evaluate on REDS-RE to measure out-of-distribution generalization 2) Replace HRBP with sliding-window and recurrent variants; compare PSNR, tOF, and runtime on REDS4-ME-5:5 3) On fixed REDS4-ME-5:5 inputs, provide ETE embeddings from all 5 exposure levels; measure performance degradation curve

## Open Questions the Paper Calls Out

### Open Question 1
How can joint VSRDB frameworks effectively handle large out-of-plane rotations or complex non-rigid motions without relying on 2D optical flow assumptions? The current HRBP and FGDF modules depend on 2D flow estimation, which becomes ambiguous or inaccurate when scene motion cannot be represented by 2D correspondences.

### Open Question 2
How can synthetic benchmarks be improved to capture the non-linear responses of real-world camera sensors and spatially-varying lighting conditions? The current REDS-ME/REDS benchmarks rely on linear averaging and do not model complex sensor responses or challenging lighting factors.

### Open Question 3
How does the presence of severe sensor noise, often coupled with short exposures in low-light conditions, affect the performance of the exposure-aware degradation learning pipeline? The model is trained solely on "clean" synthetic data, leaving its ability to disentangle noise from exposure-dependent blur untested.

## Limitations
- Heavy reliance on 2D optical flow limits performance under severe motion and out-of-plane rotation
- Synthetic benchmarks may not capture complex real-world camera sensor responses and lighting conditions
- ETE generalization to extreme exposure conditions outside the 5:1-5:5 training range remains untested

## Confidence
- **High Confidence:** Hierarchical refinement strategy (HRBP) effectiveness - supported by systematic ablation showing consistent improvements
- **Medium Confidence:** ETM contribution - improvements demonstrated but primarily on synthetic data; real-world exposure estimation reliability untested
- **Low Confidence:** Complete exposure-motion decoupling - while the architecture enables this separation, quantitative validation is limited to synthetic benchmarks

## Next Checks
1. **Cross-Exposure Generalization:** Test FMA-Net++ on real-world video sequences with exposure variations beyond the 5:1-5:5 training range to evaluate ETE robustness to novel exposure conditions.

2. **Flow Error Analysis:** Quantitatively measure optical flow estimation accuracy under severe motion conditions and trace how flow errors impact kernel estimation quality and final restoration performance.

3. **Physical Degradation Validation:** Compare predicted degradation kernels against ground truth physical blur kernels on datasets with known exposure-motion conditions to validate the approximation assumptions.