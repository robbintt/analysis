---
ver: rpa2
title: Legal experts disagree with rationale extraction techniques for explaining
  ECtHR case outcome classification
arxiv_id: '2601.12419'
source_url: https://arxiv.org/abs/2601.12419
tags:
- support
- insufficient
- rationales
- sufficient
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates whether model-agnostic rationale extraction\
  \ techniques can reliably explain European Court of Human Rights (ECtHR) case outcome\
  \ predictions. Two techniques\u2014Masked Rationale Creation (MaRC) and Flexible\
  \ Instance-Specific Rationale Extraction (ISR)\u2014are tested on a newly constructed\
  \ ECtHR dataset, addressing limitations in prior benchmarks."
---

# Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification

## Quick Facts
- arXiv ID: 2601.12419
- Source URL: https://arxiv.org/abs/2601.12419
- Authors: Mahammad Namazov; Tomáš Koref; Ivan Habernal
- Reference count: 40
- Primary result: Model-agnostic rationale extraction techniques (MaRC, ISR) show high quantitative faithfulness but fail to produce legally meaningful explanations for ECtHR case outcomes.

## Executive Summary
This study evaluates whether model-agnostic rationale extraction techniques can reliably explain European Court of Human Rights case outcome predictions. Two techniques—Masked Rationale Creation (MaRC) and Flexible Instance-Specific Rationale Extraction (ISR)—are tested on a newly constructed ECtHR dataset addressing prior benchmark limitations. While quantitative faithfulness metrics show high scores, legal experts find that extracted rationales are often fragmented and lack legal context. The study concludes that current techniques fail to produce legally meaningful explanations and that LLM-based evaluation is not yet viable for legal interpretability tasks.

## Method Summary
The study uses LEGAL-BERT with hierarchical chunking (512 tokens) + cross-attention for ECtHR case classification. MaRC optimizes soft masks to extract minimal sufficient rationales, while ISR selects optimal rationales from seven attribution methods using Jensen-Shannon Divergence. The new ECtHR dataset contains 3,146 balanced cases (1,573 violations, 1,573 non-violations) filtered from ~390k English documents. Faithfulness is measured via Normalized Sufficiency and Comprehensiveness, with expert qualitative evaluation and LLM-as-a-Judge validation using Llama-3.1-8B, Mistral-7B, and SaulLM-7B.

## Key Results
- Quantitative faithfulness metrics show high scores (NormSuff > 0.60, NormComp > 0.70) for both MaRC and ISR
- Legal experts find 9 out of 10 extracted rationales neither support nor sufficiently justify predicted violations
- Extracted rationales are often fragmented, lacking verbs, subjects, or legal context
- LLM-as-a-Judge evaluation shows limited agreement with experts (κ < 0.3) and low inter-LLM agreement

## Why This Works (Mechanism)

### Mechanism 1: Masked Rationale Creation via Optimization (MaRC)
- Claim: Optimized masking identifies causally relevant tokens for model predictions
- Mechanism: Iteratively updates soft mask λ to minimize prediction divergence while maximizing sparsity through sufficiency and comprehensiveness objectives
- Core assumption: Optimization landscape allows finding global minimum where mask captures causal features
- Evidence anchors: Section 3.1 details objective function; abstract mentions MaRC provides "human-interpretable and concise text fragments"
- Break condition: Fragmentation occurs when model relies on scattered tokens, producing high faithfulness but low readability

### Mechanism 2: Instance-Specific Ensemble Selection (ISR)
- Claim: Ensemble selection yields higher faithfulness than single attribution methods
- Mechanism: Computes rationales using seven methods, selects best based on Jensen-Shannon Divergence between original and masked text predictions
- Core assumption: High divergence correlates with legal meaningfulness and semantic value
- Evidence anchors: Section 3.2 describes JSD-based selection; Section 5.1 defines normalized comprehensiveness
- Break condition: Best mathematical divergence achieved by fragmented nouns lacking legal context

### Mechanism 3: Legal Expert Evaluation vs. LLM Judges
- Claim: LLMs lack consistency and legal reasoning capability to substitute for human experts
- Mechanism: Compares human expert labels against LLM outputs using Cohen's Kappa for reliability assessment
- Core assumption: Kappa is valid proxy for LLM-as-a-Judge reliability in legal domain
- Evidence anchors: Section 6.4 reports limited agreement (κ < 0.3); abstract notes inter-LLM agreement is similarly low
- Break condition: LLMs evaluate based on semantic fluency rather than legal sufficiency, leading to hallucination

## Foundational Learning

- Concept: **Faithfulness vs. Plausibility**
  - Why needed here: Reveals critical divergence between statistically necessary rationales and legally sufficient explanations
  - Quick check question: Can a rationale consisting solely of dates and section numbers ever be legally "plausible" without verbs or subjects?

- Concept: **Token-level vs. Span-level Extraction**
  - Why needed here: Explains why fragmented rationales lack syntactic completeness required for legal reasoning
  - Quick check question: Does the extraction method enforce contiguous spans or allow sparse token selection?

- Concept: **Context Window & Chunking**
  - Why needed here: Physical severing of context required for legal reasoning when documents exceed 512-token limit
  - Quick check question: If rationale is extracted from Chunk 3, does it retain meaning if subject was defined in Chunk 1?

## Architecture Onboarding

- Component map: Dataset -> LEGAL-BERT (Classifier) -> MaRC/ISR (Explainers) -> Quantitative Metrics + Expert Review + LLM Judges (Evaluators)
- Critical path: 1. Filter/Construct Dataset 2. Chunk documents → LEGAL-BERT → Predict 3. Apply MaRC/ISR to extract rationales 4. Compute Faithfulness 5. Expert Review
- Design tradeoffs:
  - Compactness vs. Coherence: Short rationales often lack necessary verbs/subjects
  - Ensemble vs. Simplicity: ISR adds complexity without guaranteeing better legal alignment
  - Chunking strategy: Cross-attention preserves more info but increases computational cost
- Failure signatures:
  - High Metric/Low Trust: High Normalized Sufficiency but Expert Rejection
  - Fragmented Output: Rationales as "random nouns without verbs"
  - LLM Inconsistency: Low Inter-LLM agreement and low agreement with experts
- First 3 experiments:
  1. Reproduce "High NormSuff / Low Plausibility" paradox on ECtHR subset
  2. Implement syntactic parser to quantify "broken snippet" phenomenon
  3. Test LLM-as-a-Judge with strict parsing constraints

## Open Questions the Paper Calls Out

- Can rationale extraction techniques be redesigned to produce legally coherent rationales that preserve syntactic completeness and contextual meaning?
- What quantitative metrics better predict whether extracted rationales will satisfy legal expert evaluation criteria?
- Can LLM-as-a-Judge approaches be improved to reliably substitute for human legal experts?
- What accounts for the fundamental discrepancy between model "reasons" and legal expert grounds for decision-making?

## Limitations

- Small expert sample (3 experts evaluating 10 rationales) limits generalizability
- Complex dataset filtering may introduce selection bias
- LLM-as-a-Judge evaluation used only three models without exploring specialized legal models
- Limited case coverage prevents broad conclusions about technique validity

## Confidence

- **High Confidence**: Disconnect between quantitative faithfulness metrics and qualitative legal validity is well-supported
- **Medium Confidence**: LLM-as-a-Judge shows limited viability but depends on specific models tested
- **Low Confidence**: Generalizability of "9 out of 10 unsupported" finding is limited by small sample size

## Next Checks

1. Scale expert evaluation with 10+ legal experts assessing 50+ rationales across diverse case types
2. Test legal-specific LLMs (LawBERT, CaseLawBERT) as judges using same protocol
3. Conduct systematic linguistic analysis of all extracted rationales to quantify fragmentation patterns