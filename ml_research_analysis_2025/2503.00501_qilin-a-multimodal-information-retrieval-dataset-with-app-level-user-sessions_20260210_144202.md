---
ver: rpa2
title: 'Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions'
arxiv_id: '2503.00501'
source_url: https://arxiv.org/abs/2503.00501
tags:
- search
- user
- information
- users
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qilin introduces the first large-scale multimodal search and recommendation
  dataset collected from a popular social platform, featuring heterogeneous results
  (image-text notes, video notes, commercial notes, direct answers) and extensive
  APP-level contextual signals. The dataset contains over 1.9 million notes, 5 million
  images, and 2.5 million user sessions with genuine behavioral feedback.
---

# Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions

## Quick Facts
- **arXiv ID**: 2503.00501
- **Source URL**: https://arxiv.org/abs/2503.00501
- **Reference count**: 40
- **Primary result**: Introduces the first large-scale multimodal search and recommendation dataset with APP-level user sessions, containing over 1.9 million notes, 5 million images, and 2.5 million sessions with genuine behavioral feedback.

## Executive Summary
Qilin is a large-scale multimodal information retrieval dataset collected from a popular social platform, featuring heterogeneous results (image-text notes, video notes, commercial notes, direct answers) and extensive APP-level contextual signals. The dataset contains over 1.9 million notes, 5 million images, and 2.5 million user sessions with genuine behavioral feedback. By integrating rich multimodal content and contextual information, Qilin enables advanced neural retrieval model development across diverse task settings. Preliminary experiments demonstrate that incorporating visual information and contextual signals improves performance on search and recommendation tasks, with content-based approaches and hybrid models showing strong results.

## Method Summary
The Qilin dataset is collected from a social platform with three core tables (Search, Rec, DQA) containing request IDs, session IDs, user IDs, timestamps, and engagement labels. Content features include a Note table with title, body, image IDs, video metadata, and dense statistical features, plus a User table with demographics and 40 encrypted dense features. The dataset uses a chronological split (11:1 ratio by hour) for training and testing, with baselines including BM25, BERT bi/cross-encoder, DCN-V2 (0.13B params), and VLM (Qwen2-VL-7B-Instruct with 4-bit quantization and LoRA rank 16). Evaluation metrics include MRR@10, MRR@100, MAP@10, MAP@100 for search/rec tasks, and ROUGE-L/BERTScore for DQA.

## Key Results
- VLM incorporating cover images achieves MRR@10 of 0.5523 (search) and 0.6394 (recommendation), outperforming BERT cross-encoder
- DCN-V2 with contextual signals and dense features achieves best search MRR@10 of 0.5600
- The dataset contains 1.9M notes, 5M images, and 2.5M APP-level sessions with genuine behavioral feedback
- Position bias and session bias are empirically demonstrated, with CTR decaying with ranking position and session position

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Content Integration for Relevance Matching
- **Claim**: Incorporating visual information alongside textual content appears to improve retrieval performance in both search and recommendation tasks.
- **Mechanism**: Visual-language models encode cover images jointly with text, enabling cross-modal semantic alignment. This captures information that text-only encoding misses (e.g., visual style, composition, aesthetic qualities of notes).
- **Core assumption**: Users' relevance judgments depend on both textual content and visual presentation; images contain signal not redundant with text.
- **Evidence anchors**: [abstract] "Preliminary experiments demonstrate that incorporating visual information and contextual signals improves performance on search and recommendation tasks"; [section 5.1, Table 10] VLM achieves MRR@10 of 0.5523 (search) and 0.6394 (recommendation), outperforming BERT cross-encoder in recommendation.
- **Break condition**: If visual features are highly redundant with text (e.g., images are purely decorative), multimodal models add computational cost without retrieval gains.

### Mechanism 2: APP-level Contextual Signals for User State Modeling
- **Claim**: Contextual signals (query source, recent click history, position, session timing) appear to improve user preference estimation and satisfaction prediction.
- **Mechanism**: Sparse ID-based features combined with dense statistical features capture user intent evolution, position bias, and cross-service transitions (S→R, R→S), enabling context-aware ranking.
- **Core assumption**: User behavior is strongly influenced by preceding interactions, interface entry points, and position; modeling these yields better predictions than request-level features alone.
- **Evidence anchors**: [abstract] "we also collect extensive APP-level contextual signals...facilitating the development of advanced multimodal neural retrieval models"; [section 4.2, Figure 3] Position bias (CTR decays with ranking position) and session bias (CTR decays with session position) are empirically demonstrated; [section 5.1] DCN-V2, which incorporates contextual signals and dense features, achieves best search MRR@10 (0.5600).
- **Break condition**: If contextual signals are noise-dominated or users exhibit highly idiosyncratic behavior patterns that don't generalize across sessions.

### Mechanism 3: DQA Module Behavioral Impact (Assumption-heavy)
- **Claim**: Provision of direct answers via a DQA (Deep Query Answering) module appears to alter user search behavior, reducing engagement with organic results.
- **Mechanism**: When users receive direct answers, their information needs may be satisfied with fewer clicks on result notes, or they may focus attention on top-positioned results only.
- **Core assumption**: Observed behavioral differences (fewer clicks, lower engagement rates in S+DQA vs S-DQA) are causally attributable to the DQA module's presence, not confounded by query type differences.
- **Evidence anchors**: [abstract] "This allows...exploration of how such a module would affect users' search behavior"; [section 4.2, Table 5] S+DQA shows avg. click num 2.50 vs. 3.99 (S-DQA); avg. browsing depth 10.61 vs. 23.41; like rate 1.29% vs. 4.19%.
- **Break condition**: If queries triggering DQA are systematically different (e.g., factoid queries with inherently lower exploration needs), the behavioral differences may not generalize to other query types.

## Foundational Learning

- **Concept: Multimodal Retrieval (Text + Images)**
  - **Why needed here**: Qilin's core value proposition is enabling retrieval across heterogeneous content types (image-text notes, video notes). Understanding how visual-language models align embeddings across modalities is prerequisite.
  - **Quick check question**: Can you explain why a text-only model might fail to retrieve a relevant note where the key information is primarily visual (e.g., a diagram)?

- **Concept: Position and Selection Bias in User Behavior**
  - **Why needed here**: The dataset preserves all exposed results with position information. Interpreting click signals requires understanding that clicks are confounded by presentation order.
  - **Quick check question**: If CTR is higher at position 1 than position 5, is the result at position 1 necessarily more relevant?

- **Concept: Session-level vs. Request-level Modeling**
  - **Why needed here**: Qilin provides APP-level sessions with transition behaviors (S→R, R→S) and query reformulations. Effective use requires modeling user state evolution across requests.
  - **Quick check question**: How might knowing a user's previous 20 clicked notes change your prediction for their current search query?

## Architecture Onboarding

- **Component map**: Search/Rec/DQA tables -> Note table -> User table; Dense features + ID features -> DCN-V2; Text + Image features -> VLM
- **Critical path**: 1) Load Search/Rec tables → join with Note and User tables on IDs 2) Extract image features from WebP files using vision encoder (e.g., Qwen2-VL) 3) Tokenize text (title + content) with language encoder 4) Fuse multimodal embeddings (VLM approach) or concatenate with dense features (DCN-V2 approach) 5) Train ranking model using click/engagement labels with position-aware debiasing
- **Design tradeoffs**: VLM vs. DCN-V2: VLM offers better semantic matching and generalization but requires more compute; DCN-V2 converges faster with sparse features but may struggle with out-of-distribution items; Content-based vs. ID-based: Content-based handles cold-start better; ID-based captures collaborative signals but requires interaction history; DQA inclusion: DQA enables RAG training/evaluation but queries triggering DQA are a non-random subset (may skew behavioral analysis)
- **Failure signatures**: Low engagement prediction accuracy (may indicate position bias not properly debiased, or query source context ignored); Poor video note performance (may indicate keyframe sampling is insufficient representation); RAG answer quality mismatch with user feedback (may indicate engagement labels don't capture answer quality well)
- **First 3 experiments**: 1) Baseline comparison: Replicate Table 10 results (BM25, BERT bi/cross, DCN-V2, VLM) on search task using provided train/test split 2) Ablation on visual features: Train VLM with and without cover images, quantify delta in recommendation performance 3) Contextual signal analysis: Train DCN-V2 with vs. without query source and recent click history features, assess impact on search ranking MRR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Deep Query Answering (DQA) module quantitatively influence user satisfaction and long-term retention?
- Basis in paper: [explicit] The authors state that "the influence of the DQA module on user satisfaction and retention still remains largely under-investigated."
- Why unresolved: While the paper shows users click fewer results when DQA is active, it does not confirm if this efficiency translates to higher long-term user stickiness.
- What evidence would resolve it: A longitudinal analysis correlating DQA engagement frequency with user retention metrics over time.

### Open Question 2
- Question: How should result ranking algorithms and page layouts be redesigned to optimize user experience when DQA is triggered?
- Basis in paper: [explicit] The authors note that "further exploration is required on how to rank the original results and design the result page layout to enhance users’ search experience."
- Why unresolved: The presence of direct answers changes user browsing patterns (e.g., focusing on top results), making traditional ranking assumptions potentially obsolete.
- What evidence would resolve it: A new ranking model evaluated on the Qilin dataset specifically for sessions containing DQA interactions.

### Open Question 3
- Question: How can systems dynamically select the optimal result modality (e.g., video vs. image-text) based on specific user intents or environmental contexts?
- Basis in paper: [explicit] The paper concludes that "more investigation should be conducted on selecting the appropriate note type for specific user intents."
- Why unresolved: The analysis reveals significant differences in Click-Through Rates (CTR) between videos and image-text notes, but the specific intent signals dictating these preferences are not modeled.
- What evidence would resolve it: A model that successfully predicts the preferred modality for a given query context and improves satisfaction metrics over a baseline.

### Open Question 4
- Question: Can the heterogeneous tasks of search, recommendation, and RAG be jointly optimized to improve overall system performance?
- Basis in paper: [explicit] The authors mention they "do not consider jointly optimizing multiple tasks and leave it as future work."
- Why unresolved: The provided baselines treat search, recommendation, and DQA as independent tasks, ignoring potential synergies in user modeling across these scenarios.
- What evidence would resolve it: A multi-task learning framework that leverages the dataset's full schema to outperform single-task baselines.

## Limitations

- Behavioral impact of DQA module remains correlational rather than causal, as queries triggering DQA are likely systematically different
- Dense feature effectiveness is assumed but not directly validated through ablation studies
- Cold-start performance for new content types (particularly video notes) is not evaluated
- Cross-service behavioral patterns (S→R, R→S) are observed but not explicitly modeled or tested

## Confidence

- **High confidence**: Multimodal content integration improves retrieval performance (supported by Table 10 results)
- **Medium confidence**: APP-level contextual signals improve user preference estimation (empirical position bias shown but causal validation limited)
- **Low confidence**: DQA module's causal effect on search behavior (behavioral differences could reflect query type differences rather than DQA impact)

## Next Checks

1. Conduct causal analysis comparing queries with similar characteristics that do/don't trigger DQA to isolate the module's behavioral impact
2. Perform comprehensive cold-start evaluation by filtering interactions with new notes and measuring retrieval performance degradation
3. Design controlled experiments ablating individual contextual signal types (position, query source, recent clicks) to quantify their marginal contributions