---
ver: rpa2
title: Using a single actor to output personalized policy for different intersections
arxiv_id: '2503.07678'
source_url: https://arxiv.org/abs/2503.07678
tags:
- traffic
- network
- intersections
- each
- intersection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently scaling multi-agent
  reinforcement learning for adaptive traffic signal control across large networks
  of intersections. It identifies that while parameter sharing accelerates training,
  it fails to capture the personalized needs of intersections with different traffic
  patterns, leading to suboptimal policies.
---

# Using a single actor to output personalized policy for different intersections

## Quick Facts
- arXiv ID: 2503.07678
- Source URL: https://arxiv.org/abs/2503.07678
- Authors: Kailing Zhou; Chengwei Zhang; Furui Zhan; Wanting Liu; Yihong Li
- Reference count: 38
- One-line primary result: Achieves up to 29.8% reduction in average travel time through personalized policy output while maintaining efficient training

## Executive Summary
This paper addresses the challenge of efficiently scaling multi-agent reinforcement learning for adaptive traffic signal control across large networks of intersections. It identifies that while parameter sharing accelerates training, it fails to capture the personalized needs of intersections with different traffic patterns, leading to suboptimal policies. To overcome this, the authors propose Hyper-Action Multi-Head Proximal Policy Optimization (HAMH-PPO), a method that uses a shared actor-critic network but enables personalization through a novel combination of multi-head value estimation and a hyper-action mechanism. The critic outputs multiple value functions per intersection, and the actor uses a hyper-action to weight these values based on local observations, enabling tailored policy updates. Experiments on synthetic and real-world datasets show HAMH-PPO significantly outperforms state-of-the-art methods, achieving up to 29.8% reduction in average travel time while maintaining efficient training. The method effectively balances diversity and efficiency, demonstrating strong potential for real-world deployment.

## Method Summary
HAMH-PPO extends standard PPO by introducing a shared actor-critic architecture with personalization capabilities. The method uses a centralized critic with Graph Attention Networks (GAT) to capture spatial dependencies across intersections, producing k value estimates per intersection. The decentralized actor takes local observations and outputs both action probabilities and a "hyper-action" - a probability distribution over the k value estimates. The final value function used for advantage calculation is the dot product of the hyper-action weights and the value vector. This mechanism allows each intersection to dynamically weight shared value functions based on its specific traffic conditions. The architecture maintains parameter sharing for efficiency while enabling personalized policy updates through intersection-specific value weighting, with entropy regularization preventing premature convergence to deterministic weights.

## Key Results
- HAMH-PPO achieves up to 29.8% reduction in average travel time compared to state-of-the-art methods
- The method successfully balances training efficiency with personalization, outperforming both fully shared (fast but suboptimal) and fully independent (slow but optimal) approaches
- Optimal performance achieved with 32 value heads, with performance degrading for both fewer and more heads
- Strong performance demonstrated on both synthetic (Grid4x4) and real-world (Hangzhou) traffic datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A shared actor network can output personalized policies if the gradient signal used to update it is dynamically weighted per agent.
- **Mechanism:** The system decouples the policy execution (shared actor) from the value estimation guidance. The Critic outputs a vector of k value estimates ("multi-head"). The Actor outputs a "hyper-action" vector w (a probability distribution). The final value used to calculate advantage is the dot product V = z · w. This allows the agent to select a custom blend of "expert" value functions for its specific local context.
- **Core assumption:** The optimal value function for a heterogeneous intersection can be approximated by a linear combination of a fixed set of shared value function "heads."
- **Evidence anchors:**
  - [Abstract] "The decentralized execution actor takes the local observation history as input and output distributions of action as well as a so-called hyper-action to balance the multiple values..."
  - [Section 4] "The hyper-action network can directly generate the weights of the corresponding value functions and obtain the joint value function... enhancing representation ability."
  - [Corpus] Weak direct evidence; neighbor papers focus on FedRL or Hierarchical RL, suggesting this specific "hyper-action" weighting mechanism is a novel contribution of this specific work.
- **Break condition:** If the value heads in the critic converge to identical representations (mode collapse), the hyper-action weighting becomes meaningless, and the system reverts to a standard parameter-sharing policy.

### Mechanism 2
- **Claim:** Parameter sharing combined with intersection-specific embeddings prevents the "client drift" seen in fully independent learning while avoiding the "averaging" effect of naive parameter sharing.
- **Mechanism:** By using the intersection index i and local history h as input to the hyper-action network (Eq. 4), the system creates a conditional policy. The entropy regularization term H(w) prevents the weights from becoming deterministic too early, ensuring all value heads are trained adequately.
- **Core assumption:** Intersections share enough structural similarity to benefit from shared base weights, but differ enough in observation distribution to require different weighting schemes.
- **Evidence anchors:**
  - [Section 1] Figure 1c shows PPO-share converges fast but to a suboptimal point, while PPO-non-share is slow but better. HAMH-PPO aims to combine these benefits.
  - [Section 4.1] Eq. 6 includes entropy regularization λ H(w) to prevent premature convergence to deterministic weights.
  - [Corpus] Neighbor paper "Federated Hierarchical Reinforcement Learning" addresses similar personalization vs. efficiency trade-offs, supporting the general problem framing.
- **Break condition:** If traffic patterns across all intersections are identical (IID), the overhead of hyper-action selection provides no benefit over standard PPO-share.

### Mechanism 3
- **Claim:** Graph Attention Networks (GAT) in the critic are necessary to capture the spatial dependencies required for accurate multi-head value estimation.
- **Mechanism:** The centralized critic uses GAT to aggregate states from neighbor intersections. This global context is encoded before being split into the k value heads.
- **Core assumption:** An intersection's value is dependent on the state of its neighbors (spatial dependency).
- **Evidence anchors:**
  - [Section 4.2] "The centralized critic... uses graph attention units to calculate the graph representations of all intersections... The extracted spatial features... ultimately result[ing] in a vector z."
  - [Section 3.1] Defines the reward as local but notes the cooperative nature of the network.
- **Break condition:** If the road network has no connectivity or extremely sparse traffic, the GAT layer may add noise or computational overhead without improving value estimation.

## Foundational Learning

- **Concept:** **Proximal Policy Optimization (PPO)**
    - **Why needed here:** HAMH-PPO builds directly on top of PPO-Clip. You must understand the objective function L^CLIP (Eq. 1) and how the advantage estimate A is normally calculated to understand how HAMH-PPO modifies this pipeline (using the weighted value V).
    - **Quick check question:** How does the clipping mechanism in PPO prevent the policy from changing too drastically in a single update?
- **Concept:** **Centralized Training with Decentralized Execution (CTDE)**
    - **Why needed here:** The architecture relies on a centralized critic (accessing global graph info) to train a decentralized actor (local observations only). Understanding this information asymmetry is key to implementing the data flow correctly.
    - **Quick check question:** During the execution (inference) phase, does the actor need the states of neighboring intersections?
- **Concept:** **Graph Attention Networks (GAT)**
    - **Why needed here:** The critic uses GATs to process the road network topology. You need to understand how GATs weigh neighbor features differently than standard Graph Convolutional Networks (GCNs).
    - **Quick check question:** In a GAT, how does the network determine the importance (attention score) of one intersection to another?

## Architecture Onboarding

- **Component map:**
  1. HA-Actor (Shared): Input(Local Obs + Index) -> Embedding -> GRU -> [Head A: Action Softmax, Head B: Hyper-action Softmax (w)]
  2. MH-Critic (Shared): Input(Global Obs + Adjacency Matrix) -> GAT Layers -> Shared MLP -> k separate MLP heads (Output vector z)
  3. Integration: Value V = DotProduct(w, z); Advantage A = R + γ V_t+1 - V_t

- **Critical path:**
  The interaction between the Hyper-action and the Value Vector. A common implementation error is treating the hyper-action as a standard action (discrete selection). It must be a continuous weighting (probability distribution) used to collapse the value vector z into a scalar V before calculating the loss.

- **Design tradeoffs:**
  - **Value Head Count (k):** The paper finds k=32 optimal.
    - Low k: Insufficient representational capacity for diverse intersections (underfitting).
    - High k: Excessive parameters, interference between heads, and potential overfitting (Section 5.7.2).
  - **Entropy Coefficient (λ):** Crucial to keep w exploratory. If λ is too low, the actor might lock onto a single value head early, effectively disabling the personalization mechanism.

- **Failure signatures:**
  - Uniform Weights: If visualization shows w is identical for all intersections, the personalization mechanism has failed (likely due to insufficient gradient flow or too high entropy).
  - Performance Collapse in Large Networks: If training time explodes on 100+ intersections, check if the GAT implementation is efficiently batched.

- **First 3 experiments:**
  1. Sanity Check (Grid4x4): Run HAMH-PPO vs. PPO-share. Confirm HAMH-PPO converges to a lower average travel time (gap indicates personalization is working).
  2. Ablation on k: Run experiments with k ∈ {1, 8, 32, 64}. Verify the "U-shaped" performance curve described in Section 5.7.2.
  3. Visualization (Figure 6 replication): Plot the hyper-action weights w for two intersections with different traffic flows over time. Confirm that the weights diverge when flows are different and converge when flows are similar.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks sufficient ablation studies to isolate the contribution of the hyper-action mechanism versus the multi-head value function architecture
- The GAT-based critic assumes fully connected spatial dependencies that may not match real-world road network connectivity patterns
- The method requires careful tuning of entropy coefficient λ and value head count k, with performance degrading significantly outside optimal ranges

## Confidence
- **High confidence:** The core mechanism of using hyper-action to weight multiple value heads is technically sound and well-implemented
- **Medium confidence:** The empirical performance claims are supported by experiments on synthetic and real-world datasets, but comparison baselines could be more comprehensive
- **Medium confidence:** The claim that HAMH-PPO successfully balances training efficiency with personalization is supported, but lacks analysis of computational overhead in very large networks

## Next Checks
1. **Ablation on hyper-action vs. max selection:** Implement a variant where the actor selects a single value head (argmax) instead of computing weighted combinations. Compare performance to determine if the continuous weighting provides measurable benefits over discrete selection.
2. **Scalability stress test:** Evaluate HAMH-PPO on networks with 500+ intersections to identify computational bottlenecks and verify that training time scales linearly with network size as claimed.
3. **Topology sensitivity analysis:** Test the method on road networks with varying degrees of connectivity (grid, arterial, random) to quantify how spatial dependency assumptions affect performance across different urban layouts.