---
ver: rpa2
title: Semantic Content Determines Algorithmic Performance
arxiv_id: '2601.21618'
source_url: https://arxiv.org/abs/2601.21618
tags:
- semantic
- count
- items
- accuracy
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "WhatCounts benchmark isolates whether counting accuracy varies\
  \ with semantic class of list items, revealing over 40% accuracy gaps across cities,\
  \ chemicals, names, and symbols under identical formatting and list lengths. Controlled\
  \ ablations\u2014explicit separators, XML-wrapped items, token-count matching, and\
  \ token shuffling\u2014ruled out tokenization, item identification, and sequence\
  \ length as explanations."
---

# Semantic Content Determines Algorithmic Performance

## Quick Facts
- arXiv ID: 2601.21618
- Source URL: https://arxiv.org/abs/2601.21618
- Reference count: 40
- Primary result: Over 40% accuracy gaps in counting tasks across different semantic classes (cities, chemicals, names, symbols) under identical formatting and list lengths

## Executive Summary
This paper demonstrates that large language models' algorithmic performance—specifically counting accuracy—varies significantly based on the semantic content of list items, not just formatting or sequence length. Through the WhatCounts benchmark, the authors show that identical lists of different semantic types (e.g., city names vs. chemical compounds) produce over 40% accuracy gaps, even after controlling for tokenization, list identification, and sequence length. The semantic gap widens unpredictably with fine-tuning and persists in agentic workflows using Python tools, revealing fundamental fragility in seemingly deterministic LLM operations.

## Method Summary
The authors created the WhatCounts benchmark to isolate whether counting accuracy varies with semantic class of list items. They constructed lists with identical formatting and lengths but containing items from different semantic categories (cities, chemicals, names, symbols). The study employed controlled ablations including explicit separators, XML-wrapped items, token-count matching, and token shuffling to rule out tokenization, item identification, and sequence length as explanations. They also tested the effect of fine-tuning on different datasets and examined performance in agentic workflows using Python tools.

## Key Results
- Over 40% accuracy gaps in counting tasks across different semantic classes under identical formatting and list lengths
- Semantic gaps persist after ablation studies controlling for tokenization, item identification, and sequence length
- Fine-tuning on different datasets produces large, unpredictable performance shifts that exceed variation seen in standard benchmarks
- Models better at counting show larger semantic gaps, suggesting the approximation is inherently argument-dependent
- Semantic fragility persists in agentic workflows using Python tools

## Why This Works (Mechanism)
The semantic sensitivity arises because LLMs do not implement fixed algorithms but rather learn approximate, context-dependent procedures that vary with input semantics. When counting lists of different semantic types, models activate different internal representations and heuristics based on the meaning and familiarity of the content. This leads to varying accuracy even when surface features (formatting, length) are controlled. The unpredictability with fine-tuning suggests these learned procedures are not stable and can be disrupted by additional training, making the models' "algorithms" fundamentally argument-dependent rather than invariant.

## Foundational Learning
- **Semantic context dependency**: LLMs' performance varies with meaning of inputs, not just surface features - needed to understand why identical-looking tasks yield different results; quick check: compare performance on semantically identical vs. different lists
- **Algorithm approximation vs. implementation**: Models learn approximate procedures rather than executing fixed algorithms - needed to grasp why performance isn't deterministic; quick check: observe accuracy changes with fine-tuning on different data
- **Tokenization invariance**: Controlling for tokenization doesn't eliminate semantic effects - needed to rule out surface-level explanations; quick check: verify ablations with explicit separators and token matching
- **Agentic workflow fragility**: Semantic gaps persist in tool-using agent contexts - needed to understand real-world implications; quick check: test counting accuracy in Python-assisted workflows

## Architecture Onboarding

**Component Map**
Input Processing -> Semantic Analysis -> Counting Mechanism -> Output Generation

**Critical Path**
List item → Tokenization → Semantic interpretation → Counting procedure → Numeric output

**Design Tradeoffs**
The model trades algorithmic precision for contextual flexibility, learning approximate procedures that adapt to semantic context rather than implementing fixed counting algorithms. This provides general language understanding but sacrifices deterministic behavior for algorithmic tasks.

**Failure Signatures**
- Accuracy drops of 40%+ when list semantics change while formatting stays constant
- Unpredictable performance shifts after fine-tuning on different datasets
- Semantic gaps that widen in agentic workflows using external tools
- Larger gaps in models with stronger overall counting performance

**3 First Experiments**
1. Replicate semantic gap experiments on at least two additional algorithmic tasks (sorting, list reversal) using parallel ablations
2. Instrument models with activation analysis to observe internal representation differences by semantic class during counting
3. Test robustness of semantic gaps under adversarial formatting (inconsistent separators, nested lists, embedded metadata)

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark evaluates only final numeric output, cannot directly observe internal counting mechanisms
- Findings limited to counting task domain, may not generalize to other algorithmic operations
- Controlled ablations rule out obvious confounds but cannot exclude complex implicit interactions
- Single-task focus limits confidence in generalizability to other algorithmic domains

## Confidence
- **High**: Semantic content affects counting accuracy (consistent gaps across multiple ablations and model families)
- **Medium**: Fine-tuning unpredictably alters semantic sensitivity (mechanisms not characterized)
- **Low**: Generalizability to other algorithmic tasks (single-task focus, no direct evidence beyond enumeration)

## Next Checks
1. Replicate semantic gap experiments on at least two additional algorithmic tasks (e.g., sorting, list reversal) using parallel ablations to test if semantic content universally affects LLM "algorithms."

2. Instrument models with activation analysis or chain-of-thought logging to observe whether internal representations differ systematically by semantic class during counting.

3. Test robustness of semantic gaps under adversarial formatting (e.g., inconsistent separators, nested lists, embedded metadata) to assess sensitivity to real-world input variability.