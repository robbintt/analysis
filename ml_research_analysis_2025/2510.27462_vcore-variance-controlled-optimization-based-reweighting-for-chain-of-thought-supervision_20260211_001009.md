---
ver: rpa2
title: 'VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought
  Supervision'
arxiv_id: '2510.27462'
source_url: https://arxiv.org/abs/2510.27462
tags:
- arxiv
- reasoning
- vcore
- preprint
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VCORE, a variance-controlled optimization-based
  reweighting framework for chain-of-thought supervised fine-tuning. VCORE reformulates
  token reweighting as a constrained optimization problem, maximizing expected loss
  descent under a single SGD step with a KL constraint, yielding a closed-form Gibbs
  distribution over token-wise gradient utilities.
---

# VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision

## Quick Facts
- **arXiv ID:** 2510.27462
- **Source URL:** https://arxiv.org/abs/2510.27462
- **Reference count:** 34
- **Primary result:** VCORE achieves state-of-the-art performance on math and code benchmarks through variance-controlled token reweighting with closed-form Gibbs distributions.

## Executive Summary
VCORE introduces a novel optimization-based approach to token reweighting for Chain-of-Thought supervised fine-tuning. By reformulating reweighting as a constrained optimization problem that maximizes expected loss descent under a KL divergence constraint, VCORE derives a closed-form Gibbs distribution over token-wise gradient utilities. The method employs an efficient one-backward probing trick for utility estimation and includes a variance-control scaling mechanism to stabilize training. Across multiple math and code benchmarks, VCORE consistently outperforms existing methods like DFT and iw-SFT, while also providing stronger initialization for subsequent reinforcement learning.

## Method Summary
VCORE reformulates token reweighting as maximizing expected loss descent under a KL constraint, yielding a Gibbs distribution q*(t) ∝ exp(τ·s_t) where s_t is the gradient utility. The utility s_t is estimated efficiently using a single backward pass to compute the descent direction from a probe batch B', then computing perturbed losses ℓ_t(θ - ε∇L_B') for each token. A variance-control coefficient α = √(V_u/V_q) scales the update to prevent training instability. This closed-form approach provides both theoretical grounding and practical efficiency compared to heuristic or RL-based reweighting methods.

## Key Results
- VCORE achieves 81.8% accuracy on AMC 2023 and 65.3% on AIME 2024, outperforming DFT and iw-SFT on math benchmarks
- On code benchmarks, VCORE reaches 51.2% Pass@1 on MBPP and 60.3% on HumanEval+, exceeding prior SFT reweighting methods
- Models initialized with VCORE supervision show higher post-RL performance (69.8% on AIME 2024) compared to other SFT methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token importance can be derived as a closed-form Gibbs distribution that maximizes expected loss descent under a KL constraint.
- **Mechanism:** For each trajectory, VCORE solves a constrained optimization: maximize Σ q(t)·s_t subject to KL(q‖uniform) ≤ δ, where s_t = ⟨∇L(θ), ∇ℓ_t(θ)⟩ is the token's gradient utility. The solution is q*(t) ∝ exp(τ·s_t), allocating higher weight to tokens whose gradients align with the global descent direction.
- **Core assumption:** First-order Taylor approximation of loss change is sufficiently accurate for small learning rates; gradient utility captures meaningful signal about which tokens drive learning.
- **Evidence anchors:**
  - [Section 4.1] "This yields a closed-form Gibbs distribution over token-level gradient utilities, directly grounded in first-order descent dynamics."
  - [Section 4.1] Equation shows q*(t|x,y,θ) = exp(τ·s_t) / Σ exp(τ·s_j) derived from max problem with KL constraint.
  - [corpus] Weak direct evidence; related work on token reweighting (DFT, iw-SFT) uses heuristic or RL-motivated approaches, not optimization-derived.
- **Break condition:** If gradient utilities become noisy or uninformative (e.g., near convergence where ∇L ≈ 0), the Gibbs distribution may assign arbitrary weights.

### Mechanism 2
- **Claim:** Gradient utility s_t can be estimated efficiently using a single backward pass and |y| forward passes via directional derivative probing.
- **Mechanism:** Rather than computing s_t = ⟨∇L(θ), ∇ℓ_t⟩ with |y| backward passes, VCORE draws a probe batch B', computes the descent direction ∇L_B'(θ; uniform), and estimates s_t ≈ [ℓ_t(θ) - ℓ_t(θ - ε∇L_B')] / ε. This leverages the directional derivative structure.
- **Core assumption:** The probe batch B' provides a sufficiently unbiased estimate of the population gradient; ε is small enough for accurate approximation.
- **Evidence anchors:**
  - [Section 4.1] "This construction reduces the cost of estimating all s_t values from |y| backward passes to just one backward (to compute the descent direction from B′) and |y| forward passes."
  - [Section 4.1] Shows the limit equation proving unbiasedness: lim_{ε→0} E_B'[...] = ⟨∇L(θ), ∇ℓ_t⟩ = s_t.
  - [corpus] No direct corpus evidence for this specific probing technique.
- **Break condition:** If probe batch B' is too small or unrepresentative, gradient estimates may be noisy, leading to unstable weight assignments.

### Mechanism 3
- **Claim:** Scaling the reweighted update by α = √(V_u/V_q) stabilizes training by matching update variance to uniform weighting.
- **Mechanism:** Sharp Gibbs distributions concentrate weight on few tokens, increasing gradient variance V_q. The scaling factor α rescales the step: θ ← θ - η·E_{t∼q*}[∇(α·ℓ_t)], where α²V_q = V_u. This prevents training instability while preserving adaptive focus.
- **Core assumption:** Variance matching is a sufficient condition for stable convergence; uniform weighting provides a reasonable variance baseline.
- **Evidence anchors:**
  - [Section 4.2] "We rescale the parameter update by an adaptive coefficient α in order to align the variance of the reweighted supervision with that of the uniform supervision."
  - [Figure 3] Loss curves show training instability without scaling (loss spike) vs. stable convergence with scaling.
  - [corpus] Weak evidence; variance control in token reweighting is not extensively studied in related work.
- **Break condition:** If V_q >> V_u (extremely peaked weights), α becomes very small, potentially slowing learning excessively.

## Foundational Learning

- **Concept: KL Divergence and Constrained Optimization**
  - Why needed here: The core VCORE formulation requires understanding how KL constraints prevent the learned distribution from deviating too far from uniform, and how exponential tilting yields Gibbs distributions.
  - Quick check question: Given a constraint KL(q‖p) ≤ δ, what family of distributions does the optimal solution belong to?

- **Concept: First-Order Taylor Expansion in Optimization**
  - Why needed here: The derivation of gradient utility s_t and the justification for why it measures "learning value" relies on first-order approximation of loss change.
  - Quick check question: For a parameter update θ⁺ = θ - η·g, write the first-order Taylor approximation of L(θ⁺) - L(θ).

- **Concept: Variance of Stochastic Gradient Estimators**
  - Why needed here: Understanding why high-variance updates destabilize training, and how scaling factors can control variance, is essential for the variance-control mechanism.
  - Quick check question: If gradient estimator g has variance σ², what is the variance of α·g, and how would you choose α to match a target variance?

## Architecture Onboarding

- **Component map:** Probe batch sampler -> Utility estimator -> Gibbs weight calculator -> Variance scaler -> Reweighted loss backward

- **Critical path:**
  1. Sample probe batch B' (must be done before main batch gradient computation)
  2. Compute uniform-weighted gradient on B' (one backward pass)
  3. For each token in main batch, compute perturbed loss ℓ_t(θ - ε·∇L_B') (|y| forward passes)
  4. Estimate s_t, compute q*, compute α
  5. Apply reweighted loss and update parameters

- **Design tradeoffs:**
  - **Temperature τ:** Low τ → near-uniform weights (conservative); high τ → sharp focus (aggressive but potentially unstable)
  - **Probe batch size:** Larger B' → better gradient estimate but more compute; smaller B' → faster but noisier
  - **Probe scale ε:** Too large → approximation error; too small → numerical instability

- **Failure signatures:**
  - **Loss spikes early in training:** Likely τ too high without proper variance scaling; reduce τ or verify α computation
  - **No improvement over uniform SFT:** May indicate s_t estimates are noisy; increase probe batch size or check ε
  - **Training slower than baseline:** α may be too small; check if V_q is anomalously high

- **First 3 experiments:**
  1. **Hyperparameter sweep on τ and ε:** Train on small math dataset (e.g., 500 examples) with τ ∈ {10³, 10⁴, 10⁵} and ε ∈ {1e-5, 1e-4, 1e-3}; plot loss curves to identify stable region.
  2. **Ablation of variance scaling:** Compare VCORE with α=1 (no scaling) vs. α=√(V_u/V_q) on same dataset; observe divergence vs. stability.
  3. **Comparison to DFT/iw-SFT baselines:** Replicate Table 1 on a single model (e.g., Qwen3-4B) with math domain; verify VCORE achieves higher Olympiad and SGPQA-1k accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical grounding relies heavily on first-order Taylor approximation validity, which may degrade near convergence
- Variance-control mechanism lacks extensive ablation studies to validate the optimality of variance matching approach
- Probing technique's efficiency claim depends on probe batch providing unbiased gradient estimates, which may not hold for heterogeneous data

## Confidence
- **High confidence:** The closed-form Gibbs distribution derivation under KL constraint - mathematically rigorous constrained optimization theory
- **Medium confidence:** The probing technique for efficient utility estimation - theoretically sound but limited empirical validation
- **Medium confidence:** Variance scaling for training stability - reasonable variance-matching principle but specific implementation optimality unclear
- **Medium confidence:** Overall performance improvements - strong empirical results but limited model diversity and domain coverage

## Next Checks
1. **Probe batch size sensitivity analysis:** Systematically vary the probe batch size B' across multiple orders of magnitude (e.g., 2%, 10%, 50%, 100% of main batch) and measure the impact on gradient utility estimation accuracy and final model performance. Compare the variance of s_t estimates against the variance of the true gradient utilities computed with full-batch gradients.

2. **Temperature sweep stability boundary:** Conduct a fine-grained sweep of the temperature parameter τ across multiple scales (e.g., τ ∈ {10², 10³, 10⁴, 10⁵, 10⁶}) while monitoring training loss stability, final performance, and gradient variance dynamics. Identify the stability boundary where training becomes unstable and characterize the relationship between τ, α, and V_q/V_u ratios.

3. **Cross-domain generalization robustness:** Evaluate VCORE-trained models on a diverse set of reasoning tasks beyond math and code, including commonsense reasoning (HellaSwag, StrategyQA), scientific reasoning (BigBench), and multi-hop reasoning (StrategyQA, HotpotQA). Compare the out-of-domain generalization gap between VCORE and baseline methods to assess whether the variance-controlled approach provides domain-transfer benefits beyond the training distribution.