---
ver: rpa2
title: 'UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks
  and Adversarial Attacks in Large Language Models'
arxiv_id: '2502.13141'
source_url: https://arxiv.org/abs/2502.13141
tags:
- prompt
- attacks
- input
- clean
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the vulnerability of Large Language Models
  (LLMs) to three types of attacks: prompt injection, backdoor attacks, and adversarial
  attacks. These attacks, collectively termed Prompt Trigger Attacks (PTA), manipulate
  prompts to control model outputs.'
---

# UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models

## Quick Facts
- arXiv ID: 2502.13141
- Source URL: https://arxiv.org/abs/2502.13141
- Reference count: 40
- Primary result: Training-free unified defense achieving auROC near 1 for backdoor attacks and strong performance across injection/adversarial attacks via loss-variance analysis under word masking

## Executive Summary
This paper addresses the vulnerability of Large Language Models (LLMs) to three types of attacks: prompt injection, backdoor attacks, and adversarial attacks. These attacks, collectively termed Prompt Trigger Attacks (PTA), manipulate prompts to control model outputs. The authors propose UniGuardian, a unified, training-free defense mechanism that detects these attacks by analyzing LLM behavior when small subsets of words are masked in the prompt. They also introduce a single-forward strategy that enables simultaneous attack detection and text generation in a single forward pass. Experiments show UniGuardian achieves high detection accuracy, with auROC and auPRC scores near 1 for backdoor attacks and strong performance across other attack types and datasets, outperforming existing baselines.

## Method Summary
UniGuardian detects PTA by masking random word subsets in prompts and measuring output instability through uncertainty scores. For each prompt, n masked variants are created by replacing m words with mask tokens. The model processes the base prompt and all masked variants in parallel, computing per-token logit differences between variants and base. These differences are aggregated into uncertainty scores Si = (1/k)Σ(σ(Li,j) - σ(Lb,j))². Z-score normalization identifies the maximum as a suspicion score, with thresholds determining clean/poisoned classification. The single-forward strategy processes all variants plus base prompt in one batched forward pass, replacing masked-variant tokens with base tokens each generation step to maintain KV cache efficiency.

## Key Results
- Achieves auROC scores near 1 (0.96-0.99) for backdoor attacks across multiple datasets and models
- Maintains strong performance on prompt injection (auROC 0.84-0.99) and adversarial attacks (auROC 0.75-0.81)
- Outperforms Granite-Guardian baseline on SST2 dataset for prompt injection detection
- Single-forward strategy reduces latency overhead while maintaining detection accuracy

## Why This Works (Mechanism)

### Mechanism 1: Loss Asymmetry Under Masking
Triggered prompts exhibit high variance in loss when random word subsets are masked, whereas clean prompts show stable loss across masking variations. For a poisoned prompt xt = x ⊕ t, masking a subset St containing trigger words disrupts the attack optimization, causing loss to increase sharply. Masking non-trigger words Sx leaves the trigger intact, so loss remains low. Clean prompts have no trigger dependency, so loss remains consistent across any small subsets. This assumes strong directional gradients where ∇L · St is significantly larger than ∇L · Sx, relying on loss smoothness and strong convexity near minima.

### Mechanism 2: Uncertainty Score as Loss Proxy
The uncertainty score Si = (1/k)Σ(σ(Li,j) - σ(Lb,j))² approximates the true loss difference without requiring knowledge of the model's training loss function. Sigmoid-normalized logits capture probability shifts between masked and base generations. Squared differences aggregate per-token divergence, with higher values indicating greater output instability when specific words are masked—signaling potential triggers. This assumes logit divergence correlates monotonically with loss increase and that sigmoid normalization is sufficient for multi-vocabulary outputs.

### Mechanism 3: Single-Forward Parallel Inference
Attack detection and text generation can run concurrently in one batched forward pass, reducing latency overhead. The prompt is duplicated n times with different masks; all n+1 variants (including base) are processed as a batch. After each token generation, masked-variant tokens are replaced with the base token to maintain position alignment. KV cache is shared across variants. This assumes masked and base prompts share sufficient prefix tokens to benefit from KV cache reuse and that token replacement does not disrupt autoregressive dependencies.

## Foundational Learning

- **Prompt Trigger Attacks (PTA) Taxonomy**: UniGuardian unifies prompt injection, backdoor, and adversarial attacks under one framework; understanding their shared trigger-based mechanism is prerequisite to grasping why loss-based detection works across types. Quick check: Can you explain why "Ignore previous prompts" (injection), "cf" (backdoor trigger), and character substitutions (adversarial) all fit the PTA definition?

- **Loss Landscape Geometry**: Proposition 1 relies on loss function properties (L-smoothness, strong convexity) near minima to justify the gradient-based argument that trigger removal causes disproportionate loss increase. Quick check: Why does the paper assume |St|, |Sx| ≪ |xt|? What happens if this assumption is violated?

- **Batched Autoregressive Generation with KV Cache**: The single-forward strategy depends on understanding how LLMs cache key-value pairs during generation and how batching enables parallel computation. Quick check: In the single-forward strategy, why must masked-variant tokens be replaced with base tokens after each iteration? What would happen if they weren't?

## Architecture Onboarding

- **Component map**: Input Processor -> Mask Generator -> Batched LLM Engine -> Uncertainty Score Calculator -> Z-Score Normalizer -> Decision Threshold
- **Critical path**: Receive prompt → tokenize → create n masked variants → batched forward pass → collect logits → compute uncertainty scores → normalize to z-scores → extract max as suspicion score → threshold comparison → detection decision
- **Design tradeoffs**: n (number of variants) vs accuracy/compute; m (words masked) vs trigger disruption/semantic preservation; base generation vs single-forward latency
- **Failure signatures**: Low detection on adversarial attacks (auROC 0.75-0.81); false positives on short prompts; memory overflow with large models/n
- **First 3 experiments**: 1) Reproduce backdoor detection (Table 2) on Llama-8B with SST2, verifying auROC > 0.99; 2) Ablate n and m parameters on SST2 backdoor, plotting auROC vs compute time; 3) Test adversarial attack transfer across datasets, comparing vs Granite-Guardian

## Open Questions the Paper Calls Out

- **Multilingual and Architecture Generalization**: The method focuses on English and transformer-based models, leaving applicability to other languages and architectures unverified. The loss variance detection mechanism may behave differently with non-English tokenizers or non-transformer architectures.

- **Adaptive Attack Detection**: The potential to miss subtle variations in more complex or obfuscated backdoor attacks suggests a need for finer-grained mechanisms. Attackers could optimize triggers to be less sensitive to masking, reducing the suspicion score gap between clean and poisoned inputs.

- **Task-Specific Model Impact**: The method has not been evaluated on models with significantly different prompt structures or task-specific fine-tuning. Domain-specific models may exhibit different baseline loss behaviors, potentially increasing false positives.

## Limitations
- Detection mechanism relies on trigger occupying small portion of prompt, weakening when trigger length approaches 30% of prompt
- Significant performance gap between attack types (auROC 0.75-0.81 for adversarial vs 0.96-0.99 for backdoor) questions "unified" claim
- Memory constraints with single-forward strategy for 70B models with large n values create practical deployment challenges

## Confidence
- **High Confidence**: Loss asymmetry mechanism for backdoor attacks and uncertainty score formulation are well-specified and mathematically grounded
- **Medium Confidence**: Unified framework claim partially supported but performance gaps suggest mechanism isn't equally effective across all PTA types
- **Low Confidence**: Adversarial attack detection reliability and robustness against sophisticated evasion strategies require further investigation

## Next Checks
1. **Boundary Condition Testing**: Systematically vary trigger length from 5% to 50% of prompt length and measure detection accuracy degradation to quantify operational limits
2. **Adversarial Attack Transferability**: Apply same adversarial perturbations across all six datasets and victim models to measure whether detection performance correlates with attack sophistication
3. **Memory-Efficient Scaling Analysis**: Profile GPU memory usage and latency for single-forward strategy across different n scaling factors on 8B, 32B, and 70B models to identify computational efficiency frontier