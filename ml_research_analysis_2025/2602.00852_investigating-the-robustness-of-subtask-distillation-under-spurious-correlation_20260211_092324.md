---
ver: rpa2
title: Investigating the Robustness of Subtask Distillation under Spurious Correlation
arxiv_id: '2602.00852'
source_url: https://arxiv.org/abs/2602.00852
tags:
- distillation
- spurious
- student
- teacher
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the robustness of subtask distillation\
  \ methods when training data contains spurious correlations. The authors evaluate\
  \ several distillation techniques\u2014including Output Only, Attention Transfer,\
  \ Variational Information Distillation, VKD, and SubDistill\u2014on ImageNet using\
  \ a \"wading bird\" subtask with synthetic MNIST digit artifacts that are spuriously\
  \ correlated with class labels during training."
---

# Investigating the Robustness of Subtask Distillation under Spurious Correlation

## Quick Facts
- **arXiv ID**: 2602.00852
- **Source URL**: https://arxiv.org/abs/2602.00852
- **Reference count**: 40
- **Primary result**: SubDistill maintains high accuracy (>90%) under spurious correlation while baseline methods degrade significantly

## Executive Summary
This paper investigates how subtask distillation methods perform when training data contains spurious correlations. Using a controlled setup with ImageNet "wading bird" classes and synthetic MNIST digit artifacts, the authors evaluate five distillation techniques across varying contamination levels (0%, 50%, 100%). The results demonstrate that methods employing tight student-teacher alignment, particularly SubDistill, are more robust to spurious correlations than baseline approaches. The study reveals that layer-wise alignment prevents students from learning spurious shortcuts by forcing them to adopt the teacher's decision strategies.

## Method Summary
The study evaluates subtask distillation on ImageNet wading bird classes (spoonbill, flamingo, crane, limpkin, bustard) with synthetic MNIST digits superimposed as spurious artifacts. Five distillation methods are compared: Output Only (KL divergence on predictions), Attention Transfer, Variational Information Distillation, VKD, and SubDistill. SubDistill uses subspace identification to project teacher and student activations onto maximally relevant dimensions before alignment. Training uses AdamW with learning rate 0.001, decaying every 25 epochs, with 100 epochs maximum. Hyperparameters are selected using an oracle validation set with random digits. The main experiment sweeps contamination rates (0%, 50%, 100%) where digit-to-class mappings are corrupted during training but randomized at test time.

## Key Results
- SubDistill consistently retains accuracies above 90% even when half of the data is contaminated
- At 100% contamination, SubDistill outperforms baseline methods by 15-38 percentage points
- T-SNE visualizations show SubDistill students cluster by bird class rather than MNIST digit
- XAI analyses reveal SubDistill maintains positive teacher-student relevance correlation while other methods become negatively correlated

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Identifying subtask-relevant subspaces in teacher activations enables selective knowledge transfer that filters out spurious features
- **Mechanism**: SubDistill constructs sensitivity vectors measuring how each activation dimension responds to the subtask, then finds a maximally relevant subspace via optimization with orthogonality constraints. This projects teacher activations onto dimensions most informative for the target task, inherently down-weighting artifact-sensitive features
- **Core assumption**: The teacher model has already learned to ignore spurious correlations and its task-relevant subspace excludes artifact dimensions
- **Evidence anchors**: Abstract shows SubDistill is more robust; section II-A defines the subspace objective; related work describes the same subspace identification approach

### Mechanism 2
- **Claim**: Orthogonal projection with normalization prevents magnitude and rotation mismatches from corrupting alignment signals
- **Mechanism**: The loss uses student subspace rotation and mean corrections before computing distances. Normalization by squared norms makes the loss scale-invariant, ensuring stable gradients regardless of activation magnitudes across layers
- **Core assumption**: Student and teacher representations can be aligned via linear orthogonal transformations—a shared semantic structure exists despite architectural differences
- **Evidence anchors**: Section II-A shows the full loss with components; section III shows SubDistill consistently retains high accuracies; VKD uses similar orthogonal projections with SubDistill outperforming it by 15-38 points

### Mechanism 3
- **Claim**: Layer-wise alignment forces student decision strategies to match the teacher's, preventing independent discovery of spurious shortcuts
- **Mechanism**: By matching representations at multiple layers, the student cannot develop alternative prediction paths that exploit artifacts. XAI analysis shows this produces positively correlated relevance maps between teacher and student
- **Core assumption**: Spurious features are learned when students optimize outputs independently; constraining intermediate representations blocks this path
- **Evidence anchors**: Section III-C shows SubDistill achieves positive teacher-student relevance correlation even at 100% contamination; section III-B shows t-SNE clustering by bird class not MNIST digit; related papers emphasize explanation-alignment

## Foundational Learning

- **Knowledge Distillation (Output-Level)**: The baseline "Output Only" method (KL divergence on predictions) is the foundation; understanding why it fails at 100% contamination motivates layer-wise approaches. *Quick check*: Can you explain why matching softmax outputs might still allow a student to learn different internal strategies than the teacher?

- **Spurious Correlations / Clever Hans Predictors**: The paper's core problem—models exploiting artifact-label correlations that don't hold at test time. The MNIST-bird correlation is a controlled proxy for real-world confounders. *Quick check*: If a model achieves 95% training accuracy but 30% test accuracy on a held-out distribution, what evidence would suggest spurious correlation vs. simple overfitting?

- **Representation Alignment via Subspace Projections**: SubDistill's core technical contribution—projecting activations onto orthogonal subspaces before matching. Requires understanding of SVD/PCA-style dimensionality reduction concepts. *Quick check*: Why would orthogonality constraints (U^T U = I) matter for ensuring the subspace captures distinct, informative features?

## Architecture Onboarding

- **Component map**: Subspace Identifier -> Student Projector -> Layer Pairings -> Loss Aggregator
- **Critical path**:
  1. Forward pass through teacher → cache activations at paired layers
  2. Compute subtask sensitivity (gradient of subtask logits w.r.t. activations)
  3. Solve subspace (can be precomputed or updated periodically)
  4. Forward pass through student → cache activations
  5. Compute per-layer losses using Eq. (5), aggregate with output loss
  6. Backprop to student only (teacher frozen)

- **Design tradeoffs**:
  - More layer pairs: Tighter alignment but higher memory/compute; paper uses 4 layers as sweet spot
  - Subspace dimensionality K: Larger K captures more information but may include artifact dimensions
  - Validation set contamination: Paper uses "oracle" random-contamination validation for hyperparameter selection

- **Failure signatures**:
  - Student accuracy drops sharply with contamination → layer-wise alignment not effective (check λ tuning, layer pairings)
  - t-SNE shows clustering by artifact → subspace identification including artifact dimensions
  - Negative relevance correlation with teacher → student developing independent strategy

- **First 3 experiments**:
  1. Reproduce contamination sweep: Train ResNet18→ResNet18-S with ρ ∈ {0%, 50%, 100%} using Output Only and SubDistill; verify performance gap
  2. Ablate layer count: Compare 1, 2, 4 layer pairings to quantify marginal benefit of deeper alignment
  3. Test natural spurious correlations: Replace synthetic MNIST with real-world bias (e.g., background context) to assess generalization beyond controlled setup

## Open Questions the Paper Calls Out

- **Question**: Does the robustness of subtask distillation methods generalize to datasets with natural, non-synthetic spurious correlations or to modalities beyond computer vision?
- **Basis**: The authors state, "we consider it as future work to extend the investigation to other datasets and modalities with natural forms of spurious correlations"
- **Question**: Can incorporating user-in-the-loop mechanisms into the distillation process further improve robustness compared to alignment-based methods alone?
- **Basis**: The paper suggests, "We anticipate that extending distillation techniques with, e.g. user-in-the-loop mechanisms... could lead to further robustness improvements"
- **Question**: How sensitive are the reported robustness results to the availability of a clean "oracle" validation set for hyperparameter selection?
- **Basis**: The methodology relies on a validation set with random digits ("oracle selection criteria") to tune the alignment strength λ, an assumption that may not hold in real-world scenarios

## Limitations

- The synthetic MNIST artifacts are "contrived and more obvious than naturally occurring spurious features"
- Results depend on having a clean validation set for hyperparameter selection, which may not be available in real-world scenarios
- Assumes the teacher model has already learned to ignore spurious correlations, which may not hold for all pretrained models

## Confidence

- **High confidence**: SubDistill outperforms other distillation methods under controlled synthetic spurious correlation conditions, particularly at high contamination levels (100%)
- **Medium confidence**: The subspace identification method effectively filters artifact-sensitive dimensions when teachers are pretrained on clean data
- **Low confidence**: Performance transfer to real-world scenarios with complex, naturally occurring spurious correlations

## Next Checks

1. Test SubDistill with naturally occurring spurious correlations (e.g., watermarks, background objects) rather than synthetic MNIST digits to assess real-world applicability
2. Investigate teacher model robustness by pretraining teachers on biased data, then measuring whether SubDistill transfers those biases through the identified subspaces
3. Evaluate memory and compute requirements for deeper layer alignments (beyond 4 layers) to quantify the scalability tradeoff