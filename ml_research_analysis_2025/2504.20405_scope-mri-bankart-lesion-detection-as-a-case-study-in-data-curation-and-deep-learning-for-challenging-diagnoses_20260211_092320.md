---
ver: rpa2
title: 'SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep
  Learning for Challenging Diagnoses'
arxiv_id: '2504.20405'
source_url: https://arxiv.org/abs/2504.20405
tags:
- standard
- mris
- dataset
- were
- bankart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the diagnostic challenge of detecting Bankart
  lesions (anterior-inferior glenoid labral tears) on shoulder MRIs, a task complicated
  by subtle imaging features and high inter-observer variability. The authors introduce
  ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies,
  curated from patients who underwent arthroscopy.
---

# SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses

## Quick Facts
- arXiv ID: 2504.20405
- Source URL: https://arxiv.org/abs/2504.20405
- Authors: Sahil Sethi; Sai Reddy; Mansi Sakarvadia; Jordan Serotte; Darlington Nwaudo; Nicholas Maassen; Lewis Shi
- Reference count: 40
- Key outcome: Models achieved radiologist-level performance for Bankart lesion detection, with accuracy on standard MRIs surpassing radiologists interpreting MRAs.

## Executive Summary
This work tackles the diagnostic challenge of detecting Bankart lesions (anterior-inferior glenoid labral tears) on shoulder MRIs, a task complicated by subtle imaging features and high inter-observer variability. The authors introduce ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, curated from patients who underwent arthroscopy. Separate deep learning models for standard MRIs and MRI arthrograms were trained using MRNet-pretrained CNNs and transformers, with predictions ensembled across sagittal, axial, and coronal views. The models achieved radiologist-level performance, with accuracy on standard MRIs surpassing radiologists interpreting MRAs. External validation on independent data demonstrated initial generalizability. The release of ScopeMRI and a modular codebase aims to accelerate research in musculoskeletal imaging and encourage development of datasets and models for clinically challenging diagnostic tasks.

## Method Summary
The authors curated ScopeMRI, a dataset of 350 shoulder MRIs (225 arthrograms, 125 standard MRIs) with arthroscopic ground truth, annotated by expert radiologists. Separate deep learning models were developed for standard MRIs and arthrograms using MRNet-pretrained CNNs and transformers, processing sagittal, axial, and coronal views independently before ensembling predictions. Performance was evaluated against radiologists on a held-out test set and externally validated on a separate cohort. The methodology emphasizes careful data curation, model architecture design, and ensemble-based prediction for multi-view imaging data.

## Key Results
- Models achieved radiologist-level performance for Bankart lesion detection on standard MRIs and MR arthrograms
- Model accuracy on standard MRIs surpassed radiologists interpreting MR arthrograms
- External validation on independent data demonstrated initial generalizability of the approach

## Why This Works (Mechanism)
The approach succeeds by addressing a clinically challenging diagnostic task through careful dataset curation and model design. Bankart lesions are difficult to detect due to subtle imaging features and high inter-observer variability among radiologists. By curating ScopeMRI with arthroscopic ground truth and developing specialized models for different MRI protocols (standard vs arthrogram), the authors created a robust foundation for deep learning. The ensemble approach across multiple anatomical views captures complementary information, while MRNet pretraining provides a strong starting point for medical image understanding. This combination of high-quality curated data, appropriate model architectures, and multi-view ensemble learning enables the models to achieve performance comparable to expert radiologists.

## Foundational Learning

1. **Arthroscopic ground truth** - Why needed: Provides definitive reference standard for training and evaluation, essential for supervised learning in medical imaging where clinical outcomes matter. Quick check: Verify that arthroscopic findings are available and correctly matched to imaging studies.

2. **MRNet pretraining** - Why needed: Leverages prior knowledge from large-scale medical imaging datasets to improve convergence and performance on specialized tasks with limited data. Quick check: Confirm pretraining dataset characteristics and transfer learning effectiveness on the target task.

3. **Multi-view ensemble learning** - Why needed: Different anatomical views capture complementary information critical for accurate diagnosis of 3D structures like the shoulder joint. Quick check: Evaluate individual view performance and ensemble gains through ablation studies.

4. **Separate models for different protocols** - Why needed: Standard MRIs and MR arthrograms have different imaging characteristics requiring specialized processing approaches. Quick check: Compare model performance when trained jointly versus separately on mixed protocols.

5. **Data curation for challenging diagnoses** - Why needed: Medical imaging datasets require careful selection of cases with definitive ground truth to enable meaningful model development and evaluation. Quick check: Assess case selection criteria and ground truth verification processes.

6. **Expert annotation** - Why needed: Ensures high-quality labels from domain experts, critical for training models on subtle diagnostic features. Quick check: Review annotation guidelines and inter-observer agreement metrics.

## Architecture Onboarding

**Component map:** MRI scans → View extraction (sagittal/axial/coronal) → MRNet-CNN/transformer models → View predictions → Ensemble → Final prediction

**Critical path:** Patient imaging → View preprocessing → Model inference (per view) → Prediction ensemble → Output classification

**Design tradeoffs:** 
- Separate models for standard vs arthrogram MRIs trade off model complexity against protocol-specific optimization
- Ensemble across views increases robustness but requires careful weight optimization
- MRNet pretraining accelerates learning but may introduce domain-specific biases

**Failure signatures:** 
- Poor performance on cases with atypical anatomy or positioning
- Over-reliance on specific views when ensemble weights are suboptimal
- Generalization issues when imaging protocols differ from training data

**First experiments:**
1. Evaluate individual view performance to identify which anatomical planes contribute most to prediction accuracy
2. Test ensemble weight optimization to maximize combined performance across views
3. Compare model performance on standard MRIs versus MR arthrograms to validate protocol-specific design

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains based on relatively small validation set may not generalize to broader clinical populations
- Dataset derived from single center lacks demographic and clinical diversity, affecting external validity
- Binary classification approach doesn't capture full spectrum of Bankart lesion severity or other shoulder pathologies

## Confidence

| Claim | Confidence |
|-------|------------|
| Radiologist-comparable performance on test set | Medium (limited sample size, single-site data) |
| Generalizability to other populations/protocols | Low (lack of diverse validation cohorts) |
| Clinical impact and safety in practice | Low (no prospective clinical trials conducted) |

## Next Checks

1. External validation on multi-site datasets with diverse patient demographics and MRI protocols
2. Prospective clinical studies to assess diagnostic accuracy, inter-observer variability, and impact on patient outcomes
3. Comparative studies evaluating model performance against radiologists using the same MRI protocols to isolate imaging modality effects