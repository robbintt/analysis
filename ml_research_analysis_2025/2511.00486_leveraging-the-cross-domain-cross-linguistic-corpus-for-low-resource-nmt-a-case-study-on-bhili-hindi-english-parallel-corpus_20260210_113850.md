---
ver: rpa2
title: 'Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT:
  A Case Study On Bhili-Hindi-English Parallel Corpus'
arxiv_id: '2511.00486'
source_url: https://arxiv.org/abs/2511.00486
tags:
- bhili
- translation
- language
- languages
- hindi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Bhili-Hindi-English Parallel Corpus (BHEPC),
  the first large-scale, human-curated parallel corpus for the low-resource Bhili
  language, comprising 110,000 aligned sentences across Bhili, Hindi, and English.
  To address Bhili's linguistic resource scarcity, the authors benchmarked 20+ multilingual
  models, including both open-source and proprietary variants, across four translation
  directions.
---

# Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus

## Quick Facts
- arXiv ID: 2511.00486
- Source URL: https://arxiv.org/abs/2511.00486
- Reference count: 30
- Introduces Bhili-Hindi-English Parallel Corpus (BHEPC) with 110K aligned sentences across three languages

## Executive Summary
This work addresses the scarcity of linguistic resources for Bhili, an Indo-Aryan language spoken in India, by introducing BHEPC - the first large-scale, human-curated parallel corpus for Bhili. The authors benchmark over 20 multilingual models across four translation directions (Hindi↔Bhili, English↔Bhili), comparing fine-tuning and in-context learning approaches. Their comprehensive experiments demonstrate that fine-tuning specialized multilingual models like NLLB-200 consistently outperforms larger decoder-only models, achieving chrF++ scores up to 60.62 in the Hindi→Bhili direction. The study also reveals that translation direction asymmetry and domain mismatch significantly impact performance, with cross-domain evaluation showing up to 27-point chrF++ drops. Human evaluation and error analysis confirm the challenges of generating Bhili due to its unique morphology and cultural specificity.

## Method Summary
The authors created BHEPC by collecting data from three domains (NCERT education, government/PMI administration, mass media news) and applying rigorous preprocessing including length filtering, cosine similarity deduplication, and Bhili-specific normalization. They evaluated 20+ multilingual models using both in-context learning (0/5/10-shot prompting) and fine-tuning approaches. Fine-tuning employed Adam optimizer with learning rate 5e-4, batch size 16, and LoRA adapters (rank=16, α=32) for decoder-only models. The best-performing model was NLLB-200 distilled 600M, which consistently outperformed larger decoder-only models like GPT-4.5 and Llama-4-Scout. Cross-domain generalization was quantified using Jensen-Shannon Divergence between training and test corpora, revealing significant performance degradation when domains differed.

## Key Results
- Fine-tuned NLLB-200 distilled 600M achieves chrF++ scores up to 60.62 in Hindi→Bhili translation
- In-context learning gains plateau after 5 shots, with larger models showing modest improvements
- Translation direction asymmetry shows 15-20 chrF++ advantage for low-resource→high-resource over reverse
- Cross-domain evaluation reveals up to 27-point chrF++ drops, correlating with Jensen-Shannon Divergence

## Why This Works (Mechanism)

### Mechanism 1: Fine-Tuning Outperforms In-Context Learning for Low-Resource Generation
Fine-tuning updates model weights to accommodate Bhili's morphology and vocabulary through direct gradient signals, whereas in-context learning relies solely on attention patterns over a small exemplar set. Without explicit exposure during pre-training, attention mechanisms struggle to generalize morphological rules from similar scripts.

### Mechanism 2: Translation Direction Asymmetry Reflects Target-Language Complexity
Decoder models generate tokens by sampling from learned distributions over the target vocabulary. High-resource target languages have denser token representations and more robust probability mass, while low-resource targets suffer from sparse, underspecified distributions that amplify decoding errors.

### Mechanism 3: Domain Divergence Degrades Generalization Proportionally to Distributional Distance
Models learn domain-specific lexical collocations and register patterns during fine-tuning. When test data diverges, learned attention weights attend to incorrect semantic cues, producing hallucinations or register mismatches.

## Foundational Learning

- **chrF++ vs. BLEU/spBLEU metrics**: chrF++ uses character-level n-gram matching, better handling morphologically rich languages where exact word matches are rare. Given Bhili's agglutinative morphology, BLEU would undercount valid translations that differ in suffix realization.
- **Parameter-Efficient Fine-Tuning (LoRA)**: LoRA adds low-rank adapters to weight matrices rather than full fine-tuning, making larger models tractable. If LoRA rank is too low, the model would fail to adapt to vocabulary distinct from pretraining languages.
- **In-Context Learning (ICL) and exemplar selection**: ICL relies on pattern-matching from context without weight updates. Random exemplar sampling may underperform compared to diversity-aware selection for morphologically complex languages because it fails to capture the full range of morphological variations.

## Architecture Onboarding

- **Component map**: BHEPC corpus (108K train, 1K dev, 1K test) across NCERT/Govt/Mass Media domains -> NLLB-200 distilled 600M encoder-decoder Transformer -> Adam optimizer with LoRA adapters -> chrF++ evaluation
- **Critical path**: Preprocess corpus (normalize homophones, filter by length 6-80 words, deduplicate via cosine similarity <0.95) -> Fine-tune NLLB-200 with Adam (lr=5e-4, batch=16, early stopping patience=10) -> Evaluate with chrF++; if cross-domain deployment required, expect 15-25 point chrF++ drop and quantify via JSD
- **Design tradeoffs**: Fine-tuning yields ~15-20 chrF++ improvement over best ICL for bhb→hin but requires compute and data; ICL is zero-cost at inference but plateaus at ~5 shots; larger models (Llama-4-Scout 17B) improve ICL but don't close the fine-tuning gap
- **Failure signatures**: Language mixing (Gujarati/Hindi words instead of Bhili-specific terms), hallucination/omission (adds content absent from source or drops essential information), register mismatch (formal source rendered in colloquial target), zero chrF++ for 0-shot (no representation of language)
- **First 3 experiments**: 1) Reproduce NLLB-200 fine-tuning baseline on BHEPC train/test, target chrF++ ≥58 for bhb→hin; 2) Cross-domain stress test: train on NCERT subset only, evaluate on Govt/PMI/Mass Media, compute JSD between domains; 3) ICL scaling curve: test 0/1/3/5/7/10-shot prompting with GPT-4.5 on bhb→eng, plot chrF++ vs. shots to verify plateau after 5

## Open Questions the Paper Calls Out
- Would augmentation techniques such as back-translation or pivot-based transfer significantly improve translation quality for Bhili?
- Does the hybrid seed-and-post-editing workflow generalize effectively to other endangered languages with different scripts or linguistic structures?
- Can unsupervised or semi-supervised approaches overcome the constraints imposed by the scarcity of monolingual Bhili data?

## Limitations
- The BHEPC corpus is not publicly available, requiring email request with institutional affiliation
- Jensen-Shannon Divergence analysis assumes token-level distributional distance captures all relevant domain shift dimensions
- Human evaluation limited to 250 segments and four translation directions, potentially missing broader generalization issues

## Confidence
- **High confidence**: NLLB-200 distilled 600M outperforms larger decoder-only models for Bhili translation; translation direction asymmetry is well-supported
- **Medium confidence**: Cross-domain generalization degradation correlates with JSD, but correlation may be confounded by domain-specific vocabulary differences
- **Low confidence**: Exact reasons for Bhili's generation difficulty remain speculative without comparative analysis of other morphologically rich low-resource languages

## Next Checks
1. Attempt to obtain BHEPC from authors and verify preprocessing specifications; if unavailable, create synthetic Bhili corpus using Hindi transliteration
2. Independently compute JSD between train/test domain token distributions and verify correlation with chrF++ degradation across three domain pairs
3. Replicate ICL experiments with 0/5/10-shot prompts across multiple large models to confirm gains plateau after 5 shots