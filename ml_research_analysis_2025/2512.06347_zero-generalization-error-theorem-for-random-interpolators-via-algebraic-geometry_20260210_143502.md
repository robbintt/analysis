---
ver: rpa2
title: Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry
arxiv_id: '2512.06347'
source_url: https://arxiv.org/abs/2512.06347
tags:
- analytic
- generalization
- theorem
- interpolators
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining the generalization
  ability of large-scale interpolators, such as deep neural networks, which perfectly
  fit training data. The authors develop a model-based theory showing that randomly
  sampled interpolators can achieve zero generalization error under a teacher-student
  framework, without relying on algorithmic implicit bias.
---

# Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry

## Quick Facts
- arXiv ID: 2512.06347
- Source URL: https://arxiv.org/abs/2512.06347
- Reference count: 40
- Primary result: Randomly sampled interpolators achieve zero generalization error when sample count exceeds d_Θ - d_Θ̄ + 1

## Executive Summary
This paper addresses the challenge of explaining the generalization ability of large-scale interpolators, such as deep neural networks, which perfectly fit training data. The authors develop a model-based theory showing that randomly sampled interpolators can achieve zero generalization error under a teacher-student framework, without relying on algorithmic implicit bias. The key result is that the strong sample complexity—the minimum number of samples needed for zero generalization error—is bounded by d_Θ - d_Θ̄ + 1, where d_Θ is the dimension of the parameter space and d_Θ̄ is the dimension of the teacher-equivalent set. This bound is proven using tools from algebraic geometry, specifically real analytic sets, to characterize the geometric structure of the interpolator set.

## Method Summary
The paper uses algebraic geometry to analyze the generalization properties of randomly sampled interpolators in a teacher-student framework. The approach characterizes the interpolator set as the zero set of a system of polynomial equations and uses real analytic set theory to bound its dimension. The key insight is that adding training samples imposes constraints that reduce the dimension of the interpolator set until it collapses onto the teacher-equivalent set. The authors prove that when the number of samples exceeds d_Θ - d_Θ̄ + 1, randomly sampled interpolators are teacher-equivalent with probability 1. The theory is validated on synthetic data with deep linear networks and fully connected neural networks, and experiments on DLNNs and LeNet on MNIST confirm the theoretical predictions.

## Key Results
- The strong sample complexity for zero generalization error is bounded by d_Θ - d_Θ̄ + 1
- Deep neural networks with increasing depth maintain the same sample complexity bound
- Experimental results on DLNNs and LeNet confirm theoretical predictions
- The dimension of the teacher-equivalent set scales with the difference between student and teacher parameter counts, not absolute student size

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Collapse Through Interpolation Constraints
- **Claim:** Adding training samples systematically reduces the dimension of the interpolator parameter set until it collapses onto the teacher-equivalent set.
- **Mechanism:** Each training sample imposes one constraint (zero-loss equation) on the parameter space. Drawing on principles analogous to Krull's principal ideal theorem from algebraic geometry, each new independent constraint reduces the dimension of the solution set by one. When the number of constraints exceeds the "gap" between the parameter space dimension and the teacher-equivalent set dimension, the interpolator set becomes measure-theoretically equivalent to the teacher-equivalent set.
- **Core assumption:** Student and teacher models are real analytic functions with respect to parameters and inputs; data distribution is absolutely continuous with respect to uniform distribution on input space.
- **Break condition:** Non-analytic activation functions (e.g., ReLU at origin) violate real analyticity assumption; discretized or quantized parameter spaces break manifold structure.

### Mechanism 2: Over-parameterization Creates Redundant Degrees of Freedom in Teacher-Equivalent Set
- **Claim:** The dimension of the teacher-equivalent set scales with the difference between student and teacher parameter counts, not the absolute student size.
- **Mechanism:** When the student network is strictly larger than the teacher, there exist multiple parameter configurations producing identical input-output mappings. The paper constructs explicit subsets of the TES showing that "free" parameters (denoted as M and R matrices in proofs) contribute to TES dimension without affecting function equivalence. Each extra hidden unit or layer adds redundant degrees of freedom.
- **Core assumption:** Student model width ≥ teacher width at each layer; student depth ≥ teacher depth; realizability (TES is non-empty).
- **Break condition:** When student and teacher have identical architecture, TES may become discrete (dimension zero), reducing to classical under-parameterized regime.

### Mechanism 3: Uniform Sampling Concentrates Probability Mass on Teacher-Equivalent Parameters
- **Claim:** When the sample count threshold is exceeded, randomly sampled interpolators are teacher-equivalent with probability 1.
- **Mechanism:** The key insight is measure-theoretic: when dim(Θ̂ₙ \ Θ̄) < dim(Θ̄), the Lebesgue measure of non-generalizing interpolators becomes zero relative to the full interpolator set. Any sampling distribution absolutely continuous with uniform will almost surely select from the teacher-equivalent region. This is why "random" interpolators generalize—the geometry makes poor solutions measure-zero.
- **Core assumption:** Sampling distribution P(θ|Θ̂ₙ) is absolutely continuous with respect to uniform distribution on interpolator set (SGD may or may not satisfy this).
- **Break condition:** Highly biased sampling schemes that concentrate on lower-dimensional submanifolds; optimization algorithms with strong implicit bias toward specific solution types.

## Foundational Learning

- **Concept: Real Analytic Functions and Sets**
  - **Why needed here:** The entire proof machinery depends on properties of real analytic sets—the zero sets of real analytic functions. Unlike general smooth functions, analytic functions have rigid structure (locally representable as convergent power series) enabling dimension arguments via algebraic geometry tools.
  - **Quick check question:** Why can't the theory directly apply to ReLU networks without modification?

- **Concept: Teacher-Student Framework with Realizability**
  - **Why needed here:** The theory assumes a "ground truth" teacher model that the student can exactly represent. This is both the setting (enabling precise analysis) and a limitation (real-world data may not follow such structure).
  - **Quick check question:** What happens to the theory if the TES is empty (student cannot represent teacher)?

- **Concept: Dimension of Manifolds and Algebraic Varieties**
  - **Why needed here:** The bound d_Θ - d_Θ̄ + 1 is fundamentally a dimensional statement. Understanding dimension as "number of free parameters" or "degrees of freedom" is essential for interpreting results.
  - **Quick check question:** If d_Θ = 1000 and d_Θ̄ = 950, how many samples guarantee zero generalization error?

## Architecture Onboarding

- **Component map:**
  ```
  Parameter Space Θ (d_Θ dimensions)
       │
       ├── Interpolating Parameter Set Θ̂ₙ ⊂ Θ
       │   └── Parameters achieving zero training loss on n samples
       │   └── Dimension: d_Θ - n (approximately)
       │
       └── Teacher-Equivalent Set Θ̄ ⊂ Θ̂ₙ
           └── Parameters replicating teacher function exactly
           └── Dimension: d_Θ̄ ≥ d_Θ - d_*
           └── Target: Θ̂ₙ "collapses" to Θ̄ when n ≥ d_Θ - d_Θ̄ + 1
  ```

- **Critical path:**
  1. **Model specification:** Ensure student/teacher are real analytic (use sigmoid, tanh, softplus—not raw ReLU)
  2. **TES dimension estimation:** Compute lower bound via d_Θ - d_* (teacher parameter count) or estimate empirically
  3. **Sample threshold:** Acquire n ≥ d_Θ - d_Θ̄ + 1 training samples
  4. **Uniform-ish sampling:** Use Guess & Check or verify SGD sampling is approximately uniform on Θ̂ₙ
  5. **Validation:** Confirm test loss approaches zero

- **Design tradeoffs:**
  - **Depth vs. width:** Adding layers increases d_Θ but Theorem 5 shows sample complexity bounded by teacher size regardless
  - **Exact vs. near interpolation:** Theory requires exact zero loss; Proposition 4 shows ε-approximate interpolators achieve O(ε²) generalization error
  - **Uniform sampling vs. SGD:** Theory requires absolute continuity; experiments (Section 6.2) suggest SGD approximates this reasonably well in practice

- **Failure signatures:**
  - **Non-convergence of test loss:** n < d_Θ - d_Θ̄ + 1 (insufficient samples)
  - **High variance across random seeds:** Sampling distribution not absolutely continuous (biased toward submanifold)
  - **Systematic test error > 0:** Model cannot represent teacher (TES empty) or optimization finds non-interpolating local minima
  - **Analyticity violations:** Sharp activations (ReLU) cause theory to break; use softplus approximation

- **First 3 experiments:**
  1. **Dimensional bound verification:** Replicate Figure 2 setup—train DLNNs of varying depths (2, 4, 6 layers) with fixed teacher (2-layer, 5 hidden units). Confirm test loss drops near zero at n ≈ d_* + 1 = 22 regardless of student depth.
  
  2. **Sampling algorithm comparison:** For same architecture and data, compare: (a) Guess & Check uniform sampling, (b) SGD from random init, (c) Pattern search (Algorithm 1). Test whether all three achieve similar test loss when n exceeds threshold.
  
  3. **TES dimension estimation:** Train LeNet-scale model to convergence on full dataset multiple times (30,000 runs as in paper). Use manifold dimension estimation (scikit-dimension/lPCA) to estimate d_Θ̄. Verify that d_Θ - d_Θ̄ + 1 predicts observed sample complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the zero generalization error theorem hold for deep neural networks with non-analytic activation functions, such as the Rectified Linear Unit (ReLU)?
- **Basis in paper:** [inferred] Assumption 1 requires the student and teacher models to be real analytic functions. While the paper notes sigmoid/tanh are analytic, it excludes ReLU, which is the standard activation in modern deep learning.
- **Why unresolved:** The proof technique relies on the properties of real analytic sets (specifically the dimension of zero sets of analytic functions), which do not directly apply to the non-differentiable points or piecewise-linear nature of ReLU networks.
- **What evidence would resolve it:** A theoretical extension of Theorem 2 to semi-analytic or sub-analytic sets, or empirical evidence showing the sample complexity bound fails or holds for ReLU networks.

### Open Question 2
- **Question:** How does the presence of label noise affect the strong sample complexity and the zero generalization error property?
- **Basis in paper:** [explicit] The paper states in Section 2.1 that the analysis is restricted to a "noiseless setting" and explicitly contrasts this with related work (Cooper, 2021) that considers noise.
- **Why unresolved:** The zero generalization error relies on the existence of a "teacher-equivalent set" where the interpolator matches the teacher perfectly. Label noise would prevent exact interpolation of the true function, potentially invalidating the zero-error result.
- **What evidence would resolve it:** A theoretical analysis of the "near interpolator" case with bounded noise, or a modified bound showing the generalization error scales with the noise level rather than remaining exactly zero.

### Open Question 3
- **Question:** Is the assumption of absolute continuity between the data distribution and the uniform distribution on the input manifold necessary for the theorem to hold?
- **Basis in paper:** [inferred] Assumption 3 requires the data measure to be absolutely continuous with respect to the uniform distribution. This is a strong constraint that may not hold for natural data supported on lower-dimensional manifolds.
- **Why unresolved:** If the data lies on a lower-dimensional set, the Lebesgue measure of the "bad" interpolator set might no longer be zero relative to the data distribution, potentially breaking the probability-1 guarantee.
- **What evidence would resolve it:** A proof relaxing Assumption 3 to handle distributions concentrated on lower-dimensional analytic subsets of the input space.

## Limitations

- The theory requires real analytic activation functions, excluding standard ReLU networks
- The teacher-student framework assumes exact realizability, which rarely holds in practical settings
- The strong sample complexity bound is proven for random sampling, but SGD's sampling distribution is not absolutely continuous with respect to uniform

## Confidence

- **High Confidence:** The dimensional bound d_Θ - d_Θ̄ + 1 for random interpolators under stated assumptions; the algebraic geometry framework for characterizing solution sets.
- **Medium Confidence:** Extension of theory to ReLU networks via softplus approximation; empirical validation on MNIST showing similar behavior to synthetic data.
- **Low Confidence:** Direct applicability of random sampling theory to SGD-trained networks; exact generalization performance in non-realizable settings.

## Next Checks

1. **Non-analytic activation test:** Implement softplus-approximated ReLU networks and verify whether sample complexity predictions hold, measuring deviation from theoretical bound.
2. **Realizability robustness:** Add structured noise to teacher outputs and quantify generalization error scaling with noise level versus theoretical ε² prediction.
3. **SGD sampling distribution analysis:** Compare empirical parameter distribution from converged SGD runs to uniform distribution on interpolator set using divergence measures (KL, Wasserstein).