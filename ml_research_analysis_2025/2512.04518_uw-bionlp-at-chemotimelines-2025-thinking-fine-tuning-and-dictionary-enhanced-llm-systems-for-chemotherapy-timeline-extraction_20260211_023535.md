---
ver: rpa2
title: 'UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced
  LLM Systems for Chemotherapy Timeline Extraction'
arxiv_id: '2512.04518'
source_url: https://arxiv.org/abs/2512.04518
tags:
- sact
- time
- events
- extraction
- chemotherapy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of extracting chemotherapy
  timelines from electronic health records for the ChemoTimelines 2025 shared task.
  The authors propose a two-step approach: first extracting chemotherapy events from
  individual clinical notes using various methods including prompting, chain-of-thought
  reasoning, dictionary-enhanced extraction, supervised fine-tuning, and direct preference
  optimization; then normalizing and aggregating these events into patient-level timelines.'
---

# UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction

## Quick Facts
- arXiv ID: 2512.04518
- Source URL: https://arxiv.org/abs/2512.04518
- Reference count: 20
- Key outcome: Fine-tuned Qwen3-14B model achieved 0.678 F1, winning ChemoTimelines 2025 challenge

## Executive Summary
This paper presents UW-BioNLP's winning approach to extracting chemotherapy timelines from electronic health records for the ChemoTimelines 2025 shared task. The authors develop a two-step pipeline: first extracting chemotherapy events from individual clinical notes using various methods (prompting, chain-of-thought reasoning, dictionary-enhanced extraction, supervised fine-tuning, and direct preference optimization), then normalizing and aggregating these events into patient-level timelines. Their best-performing system, a fine-tuned Qwen3-14B model, achieved an official F1 score of 0.678 on the test set, securing first place in the competition. The study systematically evaluates different extraction methods and identifies key challenges in time normalization and event aggregation.

## Method Summary
The approach uses a two-step workflow: (1) LLM extracts note-level triplets (entity, relation, time) from clinical notes, and (2) Timenorm normalizes time expressions and the official aggregation script consolidates events into patient-level timelines. The best system fine-tunes Qwen3-14B with LoRA on training annotations for 10 epochs, using vLLM for inference. Multiple extraction methods were explored including prompting with chain-of-thought reasoning, dictionary-enhanced extraction with LLM verification, and various fine-tuning approaches. The pipeline processes each clinical note independently before aggregation, with time normalization relying on document dates as temporal anchors.

## Key Results
- Fine-tuned Qwen3-14B achieved 0.678 F1 on test set, winning first place
- Supervised fine-tuning yielded the largest and most reliable performance gains
- Dictionary-enhanced extraction achieved near-perfect recall with LLM verification improving precision
- Chain-of-thought reasoning improved accuracy through spontaneous self-verification
- Simple ensemble methods failed due to error accumulation across systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning of dense LLMs on note-level gold annotations yields the largest and most reliable performance gains for chemotherapy timeline extraction.
- Mechanism: SFT adapts the model to the specific extraction schema (JSON triplets with SACT, relation, time) and domain patterns in clinical notes, leveraging full note context rather than sentence-level windows.
- Core assumption: The training annotations accurately reflect the target task distribution and evaluation criteria.
- Evidence anchors:
  - [abstract] "fine-tuned Qwen3-14B model, achieved an official F1 score of 0.678 on the test set, winning first place"
  - [Section 6.2] "fine-tuning-based methods were the most effective in our experiments"
  - [corpus] Weak direct corpus support; related work (LLMPrism, Steering LLM Thinking) addresses training dynamics but not this specific clinical task.
- Break condition: Training data exhibits severe class imbalance (e.g., overrepresentation of CONTAINS-1 relations) or domain shift from test set documentation styles.

### Mechanism 2
- Claim: Enabling chain-of-thought ("thinking") mode improves extraction by triggering spontaneous self-verification behavior.
- Mechanism: The model generates implicit reasoning traces that check whether candidate events belong to SACT and whether associated time expressions satisfy extraction criteria, reducing both false positives and false negatives.
- Core assumption: The model's internal reasoning aligns with task requirements and does not introduce systematic biases.
- Evidence anchors:
  - [Section 7.1] "Qwen3 models would spontaneously check whether each candidate event belonged to SACT... and whether its associated time expressions satisfied the extraction instructions, which significantly reduced false positives"
  - [Section 6.1] "we attribute most of the performance gain of thinking to the self-checking behavior exhibited by the CoT"
  - [corpus] "Through the Judge's Eyes" (neighbor paper) finds inferred thinking traces improve reliability, offering convergent evidence for reasoning-based verification.
- Break condition: Reasoning traces become misaligned with outputs (observed in Appendix D: model correctly identifies false positive in thinking but still outputs the tag), or ambiguous clinical language causes incorrect inferences.

### Mechanism 3
- Claim: Dictionary-enhanced extraction with LLM verification achieves near-perfect recall while LLM verification recovers precision by filtering false positives.
- Mechanism: Dictionary matching provides comprehensive term coverage; LLM verification adds semantic judgment to reject incorrect matches (e.g., "FEC" in pulmonary function context) and recover missed variants.
- Core assumption: The dictionary covers the majority of SACT terminology in the target corpus.
- Evidence anchors:
  - [Section 3.3] "dictionary tagging alone achieved nearly perfect recall across cancer types"
  - [Section 6.1] "Adding LLM verification consistently increased precision... while keeping recall near 1.0"
  - [corpus] "Transcript-Prompted Whisper with Dictionary-Enhanced Decoding" shows dictionary-enhanced methods improve structured annotation, supporting the hybrid approach.
- Break condition: Test set contains novel typos, abbreviations, or variants absent from the dictionary (e.g., "bev" for Bevacizumab), causing false negatives that verification cannot recover.

## Foundational Learning

- Concept: **Information Extraction Schema Design**
  - Why needed here: The task requires structured triplet output (entity, relation, time) in JSON format; understanding schema constraints is prerequisite to prompt engineering and fine-tuning.
  - Quick check question: Can you articulate why exact-span extraction (no normalization at extraction time) is required?

- Concept: **Temporal Normalization and Anchoring**
  - Why needed here: Relative time expressions ("last week", "3 weeks ago") must be resolved against DOCTIME anchors before aggregation; errors here propagate to timeline-level evaluation.
  - Quick check question: How should "January 9" be interpreted when the document date is 2013-02-10?

- Concept: **Precision-Recall Tradeoffs in Pipeline Systems**
  - Why needed here: Dictionary methods maximize recall; LLM verification improves precision; understanding this tradeoff informs method selection based on downstream tolerance for false positives vs. false negatives.
  - Quick check question: In a clinical surveillance application, would you prioritize recall or precision for chemotherapy extraction?

## Architecture Onboarding

- Component map:
  - Note-Level Extraction: LLM (Qwen3 variants, MedGemma) processes each clinical note â†’ outputs JSON triplets
  - Normalization: Timenorm (context-free grammar) converts time expressions to ISO format using DOCTIME anchors
  - Aggregation: Official script deduplicates and consolidates events into patient-level timelines

- Critical path:
  1. Model selection and fine-tuning quality (dominant factor in final performance)
  2. Time normalization accuracy (Timenorm edge cases cause systematic errors)
  3. Post-processing rules (descriptor removal, split combined drug names)

- Design tradeoffs:
  - Note-level vs. sentence-level context: Note-level provides richer context but higher token cost; sentence-level with dictionary pre-filtering is more efficient.
  - Thinking vs. non-thinking inference: Thinking improves accuracy but increases latency (~5x token output); SFT provides gains with faster inference.
  - Ensemble vs. single model: Ensembling did not improve performance due to error accumulation across systems.

- Failure signatures:
  - Time expressions normalized to wrong year (e.g., "January 9" anchored to previous year)
  - Entity variants not consolidated ("il-2", "il2", "interleukin-2" appear as separate events)
  - Relation classification biased toward majority class (CONTAINS-1 overrepresented)
  - Reasoning-output misalignment (model identifies error in thinking trace but still outputs incorrect tag)

- First 3 experiments:
  1. **Baseline prompt extraction**: Run the provided prompt template (Appendix A.1) on a sample of 10 notes with Qwen3-14B; manually inspect JSON outputs for schema compliance and extraction quality.
  2. **Thinking mode comparison**: Enable thinking mode on the same 10 notes; compare error types against baseline (focus on self-verification examples per Section 7.1).
  3. **SFT on training split**: Fine-tune Qwen3-14B using LoRA (10 epochs per Section 5.1) on the training set; evaluate note-level micro F1 and timeline-level F1 on the development set to establish performance ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would dynamically retrieving training examples related to test queries improve performance over the static in-context examples used in current prompting approaches?
- Basis in paper: [explicit] Authors state: "our current prompt-based approaches...utilized static ad hoc in-context examples. Including dynamically-retrieved training examples related to test time queries has the potential to further improve performance."
- Why unresolved: Not implemented due to time and resource constraints during the challenge.
- What evidence would resolve it: A/B comparison of RAG-enhanced prompting vs. static prompting on held-out clinical notes.

### Open Question 2
- Question: Can more sophisticated ensemble strategies (e.g., weighted voting, confidence-based selection, or error-aware aggregation) overcome the error accumulation that caused simple concatenation to underperform individual models?
- Basis in paper: [explicit] The ensemble method achieved lower scores (0.603) than any individual model, suggesting "errors from different systems tend to accumulate when combined." Authors conclude "simple ensembling is not a viable strategy for this task."
- Why unresolved: Only one basic ensemble method (concatenation) was tested; no alternative aggregation strategies were explored.
- What evidence would resolve it: Systematic comparison of ensemble aggregation methods on development set with error pattern analysis.

### Open Question 3
- Question: How can the alignment between LLM reasoning traces and final outputs be improved to prevent cases where models correctly identify errors in reasoning but still produce them in output?
- Basis in paper: [explicit] Appendix D documents internal inconsistency where the model's thinking trace correctly identifies "Tc" as a false positive but the output still preserves the erroneous tag.
- Why unresolved: "Controlling the alignment between reasoning and output remains a challenge for dictionary-enhanced extraction with LLMs."
- What evidence would resolve it: Evaluation of consistency enforcement mechanisms (e.g., self-correction loops, constrained decoding) on reasoning-output alignment metrics.

### Open Question 4
- Question: Can task-specific adjustments to Timenorm's temporal normalization rules reduce systematic errors in clinical timeline extraction?
- Basis in paper: [explicit] Error analysis documents cases where Timenorm produces inconsistent outputs (e.g., "last week" normalized to full date format but "next week" to week format; "January 9" incorrectly anchored to previous year).
- Why unresolved: Current implementation uses unmodified Timenorm grammar; clinical-specific edge cases not addressed.
- What evidence would resolve it: Error analysis comparing default vs. clinically-adapted Timenorm rules on a curated set of edge-case time expressions.

## Limitations
- The fine-tuning procedure lacks specification of critical hyperparameters (LoRA rank, learning rate, batch size), limiting reproducibility
- The ensemble approach failed to improve performance, but only one basic method was tested without exploring alternative aggregation strategies
- The dictionary-based approach assumes comprehensive coverage of SACT terminology, which may not generalize to other cancer types or healthcare systems

## Confidence
- **High confidence**: Fine-tuning dense LLMs yields reliable performance gains (supported by official competition results and systematic experiments)
- **Medium confidence**: Chain-of-thought reasoning improves accuracy through self-verification (supported by qualitative analysis but limited quantitative ablation)
- **Medium confidence**: Dictionary + LLM verification achieves optimal recall-precision tradeoff (strong empirical support but limited to SACT domain)
- **Low confidence**: Ensemble methods are ineffective (based on single attempt with unclear implementation details)

## Next Checks
1. **Ablation study on fine-tuning components**: Run controlled experiments varying LoRA rank, learning rate, and training epochs on the development set to identify the most critical hyperparameters and establish performance sensitivity.

2. **Generalization testing**: Apply the best-performing system to clinical notes from different cancer types or institutions not represented in the training data to assess domain robustness and dictionary coverage limitations.

3. **Time normalization error analysis**: Manually examine 50 time expressions that failed normalization to identify systematic patterns (e.g., relative time phrases, ambiguous date formats) and evaluate whether a simple rule-based fallback could recover performance.