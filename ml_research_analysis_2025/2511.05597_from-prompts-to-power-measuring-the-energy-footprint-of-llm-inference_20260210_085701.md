---
ver: rpa2
title: 'From Prompts to Power: Measuring the Energy Footprint of LLM Inference'
arxiv_id: '2511.05597'
source_url: https://arxiv.org/abs/2511.05597
tags:
- energy
- consumption
- nvidia
- arxiv
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of systematic analysis of inference
  energy consumption in large language models (LLMs), despite its dominant role in
  total lifecycle energy usage. It presents over 32,500 measurements across 155 model
  architectures and 21 GPU configurations, using the vLLM inference engine to quantify
  energy usage at the prompt level.
---

# From Prompts to Power: Measuring the Energy Footprint of LLM Inference

## Quick Facts
- **arXiv ID:** 2511.05597
- **Source URL:** https://arxiv.org/abs/2511.05597
- **Reference count:** 40
- **Key result:** Developed predictive model for LLM inference energy with RMSE 0.0057 Wh and MAPE 10.48% across 155 model architectures and 21 GPU configurations

## Executive Summary
This study systematically quantifies the energy consumption of large language model inference, addressing a critical gap in understanding the environmental impact of generative AI. The research presents over 32,500 measurements across diverse model architectures and hardware configurations, identifying key factors that influence energy demand. The authors develop a predictive model that accurately estimates inference energy consumption for unseen architectures and hardware, achieving strong generalization performance. The work culminates in a browser extension implementation to raise awareness about the environmental costs of AI usage.

## Method Summary
The research employed vLLM with continuous batching to run inference across 155 model architectures and 21 GPU configurations, measuring GPU power using CodeCarbon with NVML. Over one million prompts were processed, with input/output token counts and batch sizes systematically varied. Energy consumption was measured in Watt-hours per prompt. A Random Forest and XGBoost Linear regressor were trained using 5-fold, 5-repetition cross-validation, with features including model specifications, hardware parameters, and workload characteristics. The validation strategy involved holding out specific model architectures during training to test generalization capabilities.

## Key Results
- Energy consumption per prompt ranges from 0.0001 to 5.94 Wh, with 90% of prompts consuming less than 0.2 Wh
- Output token count is the dominant factor affecting energy consumption, followed by batch size and GPU selection
- Predictive model achieves RMSE as low as 0.0057 Wh and MAPE as low as 10.48% when tested on unseen architectures
- The model successfully generalizes across different GPU generations, with minimal performance degradation when transferring from H100 to L4 predictions

## Why This Works (Mechanism)
The study's approach works because it captures the fundamental relationship between model architecture, hardware characteristics, and workload patterns that determine energy consumption during inference. By systematically varying these parameters and measuring actual GPU power usage, the research establishes empirical correlations that enable accurate prediction of energy costs for unseen configurations.

## Foundational Learning
- **GPU power measurement with NVML:** Essential for capturing real-time power consumption during inference; verify with CodeCarbon logs showing successful NVML polling
- **Continuous batching in vLLM:** Enables efficient memory usage and throughput optimization; verify through GPU memory utilization metrics
- **Random Forest regression:** Handles non-linear relationships between features and energy consumption; verify through feature importance analysis
- **Cross-validation strategy:** Ensures model generalization by testing on held-out architectures; verify through RMSE and MAPE metrics on test sets
- **KV cache management:** Critical for memory efficiency during autoregressive generation; verify through memory usage patterns during inference
- **Hardware-software co-design:** Understanding how GPU capabilities interact with model architecture; verify through systematic variation of GPU types

## Architecture Onboarding

**Component map:** Prompt Generation -> vLLM Inference -> CodeCarbon Power Measurement -> Data Aggregation -> Random Forest/XGBoost Training -> Prediction Model

**Critical path:** Prompt generation and configuration → vLLM inference execution → CodeCarbon power measurement → Energy calculation → Model training and validation

**Design tradeoffs:** The study prioritizes measurement accuracy and generalization over computational efficiency during data collection, resulting in extensive measurement campaigns but strong predictive performance.

**Failure signatures:** OOM errors during large batch inference, inconsistent power readings in virtualized environments, poor generalization when architectural modifications are not representative.

**3 first experiments:**
1. Run single-prompt inference on a small model (e.g., Llama-3.1-8B) with fixed input/output lengths to verify measurement pipeline
2. Test batch size scaling (1, 10, 50) on the same model to observe energy scaling patterns
3. Train the predictive model on a subset of architectures and validate on held-out models to test generalization

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- Measurement methodology relies on cloud-based GPU instances which may introduce variability compared to bare-metal deployments
- The specific characteristics and generation method of the "nearly one million prompts" used are not fully detailed
- The exact nature of modifications made to create 155 models from 55 root models remains unclear

## Confidence

**Major claim clusters:**
- **Energy measurement methodology:** High confidence - Standard CodeCarbon with NVML approach validated in field
- **Model generalization claims:** Medium confidence - Strong metrics reported but validation strategy needs verification
- **Architectural factors influencing energy:** Medium confidence - Factors align with established principles but relative importance needs additional validation

## Next Checks
1. **Cross-environment validation:** Replicate measurements across different cloud providers (AWS, Azure) and bare-metal configurations to verify power reading consistency
2. **Architectural modification audit:** Examine the specific modifications made to create 155 model variants from 55 root models to assess architectural diversity
3. **Prompt diversity stress test:** Evaluate predictive model performance across prompts with varying semantic complexity beyond token length variations