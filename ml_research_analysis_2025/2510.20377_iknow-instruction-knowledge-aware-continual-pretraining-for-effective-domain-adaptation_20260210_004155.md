---
ver: rpa2
title: 'IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain
  Adaptation'
arxiv_id: '2510.20377'
source_url: https://arxiv.org/abs/2510.20377
tags:
- pretraining
- language
- knowledge
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual pretraining instruction-tuned
  language models on domain-specific data without access to the original base model.
  The authors propose IKnow, a framework that reformulates self-supervised objectives
  as instruction-response dialogue pairs, enabling adaptation while preserving instruction-following
  capabilities.
---

# IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation

## Quick Facts
- arXiv ID: 2510.20377
- Source URL: https://arxiv.org/abs/2510.20377
- Authors: Tianyi Zhang, Florian Mai, Lucie Flek
- Reference count: 8
- Primary result: Instruction-style continual pretraining preserves instruction-following while adapting to domain data

## Executive Summary
This paper addresses the challenge of domain adaptation for instruction-tuned language models through continual pretraining. The authors propose IKnow, a framework that reformulates self-supervised objectives as instruction-response dialogue pairs, enabling adaptation to domain-specific data while preserving instruction-following capabilities. Two novel knowledge-aware tasks—Masked Phrase Prediction (MPP) and NL-KG Loop—are introduced to enhance semantic encoding. Experiments on RepliQA and SciQAG datasets demonstrate consistent improvements over naive next-token prediction baselines, with IKnow achieving up to 38.62% ROUGE-L-F1 on RepliQA using Llama-3.2-3B with LoRA fine-tuning.

## Method Summary
IKnow reformulates self-supervised pretraining objectives as instruction-response dialogue pairs, transforming masked language modeling into tasks like "Find the masked phrase" with the response being the actual masked phrase. The framework introduces two knowledge-aware objectives: Masked Phrase Prediction (MPP) for enhanced semantic encoding and NL-KG Loop for knowledge graph integration. The pretraining process operates without access to the original base model, using only the instruction-tuned model and domain-specific data. LoRA fine-tuning is applied for downstream evaluation on summarization and question-answer generation tasks.

## Key Results
- IKnow achieves 38.62% ROUGE-L-F1 on RepliQA dataset using Llama-3.2-3B with LoRA fine-tuning
- Consistent improvements over naive next-token prediction baselines across both RepliQA and SciQAG datasets
- Mixed performance gains from knowledge-aware objectives across different model architectures suggest architecture-dependent effectiveness
- Instruction-style pretraining successfully preserves instruction-following capabilities during domain adaptation

## Why This Works (Mechanism)
The effectiveness stems from reformulating self-supervised objectives as instruction-response pairs, which maintains the instruction-following paradigm that instruction-tuned models have learned. By presenting masked language modeling as an instruction ("Find the masked phrase"), the model continues to operate within its learned instruction-response framework rather than switching to pure next-token prediction. The knowledge-aware tasks (MPP and NL-KG Loop) provide additional semantic and knowledge graph grounding, enhancing the model's understanding of domain-specific concepts while maintaining the instruction-following behavior through the consistent dialogue format.

## Foundational Learning
- **Masked Language Modeling (MLM)**: Predicts masked tokens in input text; needed for self-supervised learning when no labels are available; quick check: ensure masking strategy maintains sufficient context
- **Instruction Tuning**: Fine-tuning LLMs on instruction-response pairs; needed to enable models to follow natural language instructions; quick check: verify model can follow diverse instruction types
- **Continual Pretraining**: Further pretraining on domain-specific data; needed when base models lack domain expertise; quick check: monitor catastrophic forgetting of base knowledge
- **Knowledge Graphs**: Structured representations of entities and relationships; needed for enhanced semantic understanding; quick check: validate KG quality and relevance to domain
- **LoRA Fine-tuning**: Parameter-efficient adaptation method; needed for practical deployment on resource-constrained hardware; quick check: compare with full fine-tuning performance
- **ROUGE-L Metric**: Evaluates text generation quality by measuring longest common subsequence; needed for summarization evaluation; quick check: ensure reference summaries are available

## Architecture Onboarding

Component Map:
Instruction Data → IKnow Framework → Masked Phrase Prediction + NL-KG Loop → Domain-Adapted Model → LoRA Fine-tuning → Downstream Tasks

Critical Path:
1. Domain-specific instruction-response pairs are generated or collected
2. IKnow applies Masked Phrase Prediction and NL-KG Loop objectives
3. The model is continually pretrained on this reformulated data
4. LoRA fine-tuning adapts the model to specific downstream tasks
5. Evaluation on RepliQA/SciQAG benchmarks measures performance

Design Tradeoffs:
- Instruction-style reformulation vs. pure next-token prediction: maintains instruction-following but may limit raw language modeling capability
- Knowledge-aware objectives vs. simpler MLM: provides semantic enhancement but adds complexity and computational overhead
- LoRA fine-tuning vs. full fine-tuning: enables parameter efficiency but may limit adaptation capacity

Failure Signatures:
- Performance degradation on original instruction-following tasks
- Inability to generalize beyond training domain despite pretraining
- Inconsistent gains across different model architectures for knowledge-aware objectives
- Overfitting to specific instruction formats rather than general instruction understanding

First Experiments:
1. Baseline comparison: next-token prediction vs. instruction-style reformulation on held-out validation set
2. Ablation study: evaluate impact of removing MPP and NL-KG Loop objectives individually
3. Cross-domain transfer: test adapted model on out-of-domain instruction tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope limited to two scientific datasets (RepliQA and SciQAG)
- Architecture-dependent effectiveness of knowledge-aware objectives suggests limited generalizability
- No analysis of computational overhead compared to baseline methods
- Limited ablation studies on the relative contribution of MPP vs. NL-KG Loop
- No examination of long-term instruction-following capability after continual pretraining

## Confidence
High confidence in core findings based on experimental results showing consistent improvements over baselines. However, limited scope to two scientific domains and lack of comprehensive ablation studies reduce confidence in generalizability to other domains and model architectures.

## Next Checks
1. Verify computational overhead of IKnow compared to naive next-token prediction baselines
2. Examine performance on non-scientific domains to assess generalizability
3. Conduct more extensive ablation studies on individual components (MPP and NL-KG Loop)
4. Test long-term instruction-following capability through sequential task evaluation
5. Compare with other domain adaptation approaches beyond next-token prediction baselines