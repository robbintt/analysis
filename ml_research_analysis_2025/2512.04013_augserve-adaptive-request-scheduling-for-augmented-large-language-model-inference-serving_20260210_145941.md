---
ver: rpa2
title: 'AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference
  Serving'
arxiv_id: '2512.04013'
source_url: https://arxiv.org/abs/2512.04013
tags:
- uni00000013
- uni00000048
- uni00000014
- uni00000057
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AugServe tackles the challenge of optimizing augmented LLM inference
  serving by reducing queuing delays and boosting effective throughput. It introduces
  a two-stage adaptive scheduling strategy that dynamically adjusts request ordering
  based on both predictive features and runtime feedback, mitigating head-of-line
  blocking.
---

# AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving

## Quick Facts
- arXiv ID: 2512.04013
- Source URL: https://arxiv.org/abs/2512.04013
- Reference count: 40
- Key outcome: AugServe achieves 4.7× higher effective throughput than vLLM and 3.3× than InferCept, while reducing time-to-first-token by up to 96.3% and 95.0%, respectively

## Executive Summary
AugServe addresses the challenge of optimizing augmented LLM inference serving by reducing queuing delays and head-of-line blocking. Augmented LLMs pause inference during external API calls, creating complex scheduling challenges that traditional batching strategies cannot handle effectively. The system introduces a two-stage adaptive scheduling strategy that dynamically adjusts request ordering based on both predictive features and runtime feedback, combined with dynamic token-level batching that adapts to real-time load and hardware conditions.

The approach significantly outperforms existing baselines, achieving 4.7× higher effective throughput than vLLM and 3.3× than InferCept while substantially reducing time-to-first-token (TTFT) and latency. Experiments demonstrate the system's effectiveness across different workload patterns including Poisson arrivals and bursty Gamma arrivals, using datasets that integrate multiple augmented LLM task categories.

## Method Summary
AugServe implements a two-stage adaptive scheduling strategy built on top of vLLM, combining predictive modeling with runtime correction to optimize request ordering. The system uses a fine-tuned BERT-base-uncased model to predict output length (as classification into buckets) and API call duration (as regression), which informs Stage I scheduling decisions. Stage II applies runtime corrections based on actual API return lengths. Dynamic token-level batching adapts the batch size to real-time load and hardware conditions, with token budget calculated as floor(G_avail/M) bounded by [β_low×target_max, β_high×target_max]. The approach specifically addresses head-of-line blocking by considering both prefill and decode costs, as well as API call costs under different context preservation policies (Preserve, Discard, Swap).

## Key Results
- 4.7× higher effective throughput than vLLM baseline
- 3.3× higher effective throughput than InferCept
- Up to 96.3% reduction in time-to-first-token compared to vLLM, 95.0% compared to InferCept

## Why This Works (Mechanism)
The system works by predicting request characteristics and dynamically adjusting scheduling decisions to minimize queuing delays and maximize resource utilization. The two-stage approach first uses predictions to make initial scheduling decisions, then corrects based on actual runtime information. This handles the uncertainty inherent in augmented LLM workloads where API call durations and return sizes are unpredictable. The dynamic token batching ensures that batch sizes are optimized for current hardware conditions rather than fixed values, improving throughput while maintaining SLO compliance.

## Foundational Learning
- **Augmented LLM inference pausing**: Why needed - Understanding how augmented LLMs pause during external API calls and resume afterward is fundamental to grasping the scheduling challenge. Quick check - Verify that the system correctly handles request state transitions between paused and running modes.
- **Two-stage adaptive scheduling**: Why needed - The distinction between prediction-based Stage I and correction-based Stage II is crucial for understanding how the system handles uncertainty. Quick check - Confirm that Stage II corrections are applied based on actual API return lengths.
- **Dynamic token-level batching**: Why needed - This optimization adapts batch size to real-time conditions rather than using static values, directly impacting throughput. Quick check - Verify token budget calculation using G_avail/M and boundary constraints.
- **Head-of-line blocking mitigation**: Why needed - Understanding how traditional batching creates bottlenecks when long requests block shorter ones is key to appreciating the scheduling innovation. Quick check - Analyze request completion times to ensure short requests aren't excessively delayed.
- **Scheduling value computation**: Why needed - The equations (9-25) that compute V_i based on predicted features drive the core scheduling decisions. Quick check - Validate that scheduling values correctly incorporate prefill, decode, and API costs.
- **Context preservation policies**: Why needed - Different policies (Preserve/Discard/Swap) for handling API call context affect scheduling costs and must be accounted for. Quick check - Confirm that V_i calculations use the correct policy-specific costs.

## Architecture Onboarding
- **Component map**: Request Queue -> BERT Predictor -> Stage I Scheduler -> Runtime Monitor -> Stage II Scheduler -> Dynamic Batcher -> GPU Worker Pool
- **Critical path**: Request arrival → Prediction → Scheduling decision → Batching → GPU execution → Response
- **Design tradeoffs**: Prediction accuracy vs. latency overhead (BERT model size), batch size optimization vs. SLO compliance, anti-starvation vs. throughput maximization
- **Failure signatures**: High prediction error causing mis-scheduling (check prediction vs actual values), starvation of long requests (monitor max waiting time), insufficient batch size causing GPU underutilization (check batch utilization metrics)
- **First experiments**: 1) Run baseline vLLM with Merge dataset to establish performance metrics, 2) Implement and test BERT predictor accuracy on output length classification, 3) Compare Stage I scheduling decisions with and without prediction module enabled

## Open Questions the Paper Calls Out
The paper explicitly states that "further accuracy optimization is left to future work" regarding the BERT-base-uncased model used for estimation. It does not provide specific open questions beyond acknowledging that prediction accuracy could be improved. The paper focuses on demonstrating the effectiveness of the two-stage scheduling approach and dynamic batching rather than exploring all possible improvements to the prediction module.

## Limitations
- Critical hyperparameters (α, β_low/β_high, γ, target_max, M) are unspecified, preventing exact reproduction
- No implementation code or repository provided despite substantial claims
- Evaluation limited to GPT-J-6B and OPT-13B on high-end GPUs, not tested on larger models or resource-constrained hardware
- Datasets (Merge, ToolBench) are referenced but availability and preprocessing details are unclear

## Confidence
- **High confidence**: The core problem definition (augmented LLM scheduling challenges, head-of-line blocking), the two-stage scheduling framework (Stage I prediction-based, Stage II correction-based), and the dynamic token batching concept are well-specified and internally consistent
- **Medium confidence**: The reported performance gains (4.7× throughput over vLLM, 3.3× over InferCept, TTFT reductions of 96.3% and 95.0%) are based on controlled experiments with specified datasets and models, but lack code transparency and full hyperparameter disclosure
- **Low confidence**: The exact implementation details, hyperparameter tuning methodology, and BERT predictor training configuration are insufficient for faithful reproduction without extensive trial-and-error

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary α (anti-starvation coefficient) from 0.1 to 1.0 and β_low/β_high (batch bounds) around reported values to determine their impact on goodput and SLO attainment. This will quantify how critical these unspecified parameters are to achieving the claimed 4.7× throughput improvement.

2. **Prediction Module Validation**: Implement and train the BERT-base-uncased predictor using the specified classification (output length buckets) and regression (API duration) tasks. Measure prediction accuracy and MSE on both Merge and ToolBench datasets. Compare against reported targets (85%/65% accuracy, 0.4s-5s MSE) to assess if prediction errors could explain performance gaps.

3. **End-to-End Reproduction with Simplified Baselines**: Implement a minimal two-stage scheduling system (without full dynamic batching) using fixed hyperparameters (e.g., α=0.5, β_low=0.8, β_high=1.2). Run controlled experiments on Poisson arrivals with the Merge dataset, comparing against vLLM baseline. This will isolate whether the scheduling algorithm itself provides measurable gains independent of hyperparameter optimization.