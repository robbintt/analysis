---
ver: rpa2
title: 'Beyond English: The Impact of Prompt Translation Strategies across Languages
  and Tasks in Multilingual LLMs'
arxiv_id: '2502.09331'
source_url: https://arxiv.org/abs/2502.09331
tags:
- language
- english
- languages
- source
- pre-translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates optimal strategies for prompting multilingual
  large language models (LLMs) by systematically evaluating selective pre-translation
  approaches. The authors formalize prompts as modular components (instruction, context,
  examples, output) that can be selectively translated into English or kept in the
  source language.
---

# Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs

## Quick Facts
- arXiv ID: 2502.09331
- Source URL: https://arxiv.org/abs/2502.09331
- Reference count: 25
- The paper demonstrates that selective pre-translation of prompt components consistently outperforms full pre-translation and direct inference in source language across 35 languages and four tasks.

## Executive Summary
This paper systematically investigates optimal prompting strategies for multilingual large language models (LLMs) by evaluating different approaches to translating prompts into English versus keeping them in the source language. The authors formalize prompts as modular components and test selective pre-translation strategies against full pre-translation and direct inference. Through extensive experiments across 35 languages and four task types (QA, NLI, NER, summarization) using three different models, they demonstrate that selectively translating only certain prompt components consistently yields superior performance, particularly for low-resource languages.

The key insight is that not all prompt components benefit equally from translation, and the optimal strategy varies by task type. Context and examples in the source language are particularly beneficial for extractive tasks, while output language preferences depend on the specific task. The study also reveals that selective pre-translation effectively mitigates translation quality issues, making it a robust approach for multilingual LLM applications. These findings provide practical guidance for optimizing multilingual LLM performance while reducing computational costs associated with full prompt translation.

## Method Summary
The authors formalize prompts as modular components (instruction, context, examples, output) and systematically evaluate different translation strategies across three models (LLaMA, BLOOMZ, GPT-3.5), four tasks (QA, NLI, NER, summarization), and 35 languages. They compare selective pre-translation (translating only specific components), full pre-translation, and direct inference in the source language. The experiments use both automatic translation and evaluation metrics, with a focus on identifying which prompt components benefit most from translation in different task contexts.

## Key Results
- Selective pre-translation consistently outperforms both full pre-translation and direct inference across all models and tasks
- Keeping context and examples in the source language benefits extractive tasks (QA, NER)
- Output language preferences vary by task type, with source language outputs preferred for QA and NER, and target language for summarization
- The approach is particularly effective for low-resource languages, with performance gains of 10-20% in some cases
- Selective pre-translation mitigates translation quality issues, making it a robust approach for multilingual applications

## Why This Works (Mechanism)
The effectiveness of selective pre-translation stems from the fact that different prompt components serve different functions in the reasoning process. Instructions and examples often contain task-specific knowledge and reasoning patterns that are better preserved when translated into English, as they benefit from the model's strong English capabilities. However, context and examples in extractive tasks contain factual information and domain-specific terminology that may be better understood by the model when presented in the source language, reducing translation errors and preserving semantic fidelity.

The task-dependent nature of optimal strategies reflects how different tasks leverage different aspects of the model's capabilities. For extractive tasks like QA and NER, where precise information retrieval is crucial, keeping context in the source language helps the model access and process factual information more accurately. For generative tasks like summarization, where language fluency and coherence are paramount, translating examples and instructions into English helps the model produce more fluent and well-structured outputs in the target language.

## Foundational Learning
- **Prompt componentization**: Breaking prompts into modular parts (instruction, context, examples, output) - why needed: enables fine-grained analysis of translation effects on different prompt elements; quick check: verify that each component can be independently translated without breaking the overall prompt structure
- **Selective translation strategies**: Translating only specific prompt components rather than the entire prompt - why needed: reduces computational costs while optimizing performance; quick check: compare performance and translation overhead between selective and full translation approaches
- **Multilingual evaluation metrics**: Using appropriate metrics for different language families and resource levels - why needed: ensures fair and meaningful comparison across diverse languages; quick check: validate metric consistency across high and low-resource languages
- **Translation quality impact**: Understanding how translation errors propagate through prompt components - why needed: informs which components are most sensitive to translation quality; quick check: measure performance degradation as translation quality decreases
- **Task-type sensitivity**: Recognizing that different tasks have different optimal translation strategies - why needed: enables task-specific optimization of multilingual prompting; quick check: verify that strategy recommendations vary appropriately across task types
- **Low-resource language handling**: Developing strategies that work well for languages with limited training data - why needed: ensures equitable performance across the language spectrum; quick check: confirm performance gains are consistent for both high and low-resource languages

## Architecture Onboarding

Component map:
Instruction -> Translation decision -> Model input
Context -> Translation decision -> Model input
Examples -> Translation decision -> Model input
Output -> Translation decision -> Model output

Critical path: Translation strategy selection → Component-level translation decisions → Prompt assembly → Model inference → Output evaluation

Design tradeoffs:
- Full pre-translation vs. selective pre-translation: computational cost vs. performance optimization
- Source language vs. target language output: information fidelity vs. user accessibility
- Automatic vs. human evaluation: scalability vs. accuracy in assessment
- Task-specific vs. universal strategies: optimization vs. generalization

Failure signatures:
- Translation quality degradation causing semantic drift in critical components
- Mismatched component translation strategies leading to inconsistent reasoning
- Over-translation of context-sensitive information reducing factual accuracy
- Under-translation of instruction components limiting task comprehension

First experiments to run:
1. Ablation study on individual prompt components to identify their relative contribution to performance
2. Cross-model validation to test generalizability of selective pre-translation benefits
3. Long-form generation task evaluation to assess if component-level insights extend beyond short-form tasks

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Limited to three model families (LLaMA, BLOOMZ, GPT-3.5) which may not generalize to other multilingual LLMs
- Relies on automatic translation and evaluation metrics that may not capture nuanced linguistic phenomena
- Primarily evaluates short-form tasks, limiting generalizability to long-form generation
- Does not investigate the impact of prompt complexity or domain specificity on translation strategies

## Confidence
- Core finding that selective pre-translation outperforms full pre-translation: **High** - consistently observed across multiple models, tasks, and languages with statistically significant differences
- Specific component-level recommendations (e.g., keeping context in source language for extractive tasks): **Medium** - robust patterns but show some task-dependent variability
- Practical utility for real-world applications: **Medium** - automatic evaluation results need human validation, especially for low-resource languages

## Next Checks
1. Replicate experiments with additional multilingual LLM architectures (e.g., PaLM, Mixtral, Mistral) to assess generalizability
2. Conduct human evaluation studies to validate automatic metric results, particularly for low-resource languages where evaluation quality may be questionable
3. Test selective pre-translation approach on long-form generation tasks (e.g., document translation, creative writing) to evaluate if component-level insights extend to these domains