---
ver: rpa2
title: 'Balanced Actor Initialization: Stable RLHF Training of Distillation-Based
  Reasoning Models'
arxiv_id: '2509.00309'
source_url: https://arxiv.org/abs/2509.00309
tags:
- reasoning
- length
- reward
- training
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the training instability problem in applying
  reinforcement learning from human feedback (RLHF) to distillation-based reasoning
  models, specifically focusing on two phenomena: Sequence Length Collapse where models
  dramatically reduce generated sequence length during early RL training, and the
  Reward Hockey Stick Curve characterized by severe reward score drops followed by
  gradual recovery. The authors propose Balanced Actor Initialization (BAI), a two-stage
  weighted model merging approach that combines instruction-following and distillation-based
  reasoning fine-tuned models, then further merges this intermediate model with the
  pretrained model to preserve foundational knowledge.'
---

# Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models

## Quick Facts
- arXiv ID: 2509.00309
- Source URL: https://arxiv.org/abs/2509.00309
- Reference count: 40
- Primary result: Achieves overall score of 55.2 across diverse benchmarks, outperforming individual paradigms (Paradigm 1: 51.3, Paradigm 2: 53.6, Paradigm 3: 51.5)

## Executive Summary
This paper addresses critical training instability issues in reinforcement learning from human feedback (RLHF) for distillation-based reasoning models. The authors identify two problematic phenomena during RLHF training: Sequence Length Collapse, where models dramatically reduce generated sequence length in early training stages, and the Reward Hockey Stick Curve, characterized by severe reward score drops followed by gradual recovery. To address these challenges, the paper proposes Balanced Actor Initialization (BAI), a two-stage weighted model merging approach that combines instruction-following and distillation-based reasoning fine-tuned models, then further merges this intermediate model with the pretrained model to preserve foundational knowledge. Experimental results demonstrate that BAI successfully resolves Sequence Length Collapse, mitigates the Reward Hockey Stick Curve, and achieves superior performance across diverse benchmarks while enabling continuous sequence length improvement during training.

## Method Summary
The paper proposes Balanced Actor Initialization (BAI), a two-stage weighted model merging approach to stabilize RLHF training for distillation-based reasoning models. The first stage merges instruction-following and distillation-based reasoning fine-tuned models using learnable weights. The second stage further merges this intermediate model with the pretrained model to preserve foundational knowledge while maintaining reasoning capabilities. This initialization strategy aims to balance the trade-offs between instruction-following abilities, reasoning capabilities, and pretraining knowledge preservation, addressing the training instability phenomena observed during RLHF fine-tuning.

## Key Results
- Achieves an overall score of 55.2 across diverse benchmarks
- Successfully resolves Sequence Length Collapse during early RL training
- Mitigates the Reward Hockey Stick Curve with severe reward score drops followed by gradual recovery
- Demonstrates continuous sequence length improvement during training while maintaining performance across knowledge comprehension, reasoning, and conversational tasks

## Why This Works (Mechanism)
The two-stage weighted model merging approach works by creating an optimal initialization point that balances three competing objectives: instruction-following capabilities, reasoning abilities, and foundational pretraining knowledge. By first merging instruction-following and distillation-based reasoning models, BAI creates an intermediate model that captures both paradigms. The subsequent merge with the pretrained model ensures that foundational knowledge is preserved, preventing catastrophic forgetting while maintaining the enhanced capabilities from the first merge. This balanced initialization provides a stable starting point for RLHF training, preventing the dramatic shifts in model behavior that lead to Sequence Length Collapse and reward instability.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - to align model outputs with human preferences and values; Quick check - verify reward model quality and alignment with human judgment
- **Model Merging Techniques**: Why needed - to combine complementary capabilities from different fine-tuning paradigms; Quick check - assess merge weight optimization and performance preservation
- **Distillation-Based Reasoning**: Why needed - to enhance model reasoning capabilities through specialized fine-tuning; Quick check - evaluate reasoning performance on benchmark tasks
- **Sequence Length Control**: Why needed - to maintain stable output generation during training; Quick check - monitor sequence length distribution throughout training
- **Reward Modeling**: Why needed - to provide feedback signals for RLHF optimization; Quick check - validate reward model predictions against human preferences

## Architecture Onboarding

Component Map:
Pretrained Model -> Instruction-Following Model + Reasoning Model -> Intermediate Merged Model -> Pretrained Model Merge -> Final BAI Model -> RLHF Training

Critical Path:
The critical path involves the sequential merging operations: first combining instruction-following and reasoning models, then merging this result with the pretrained model. Each merge operation must preserve complementary capabilities while avoiding catastrophic forgetting.

Design Tradeoffs:
The primary tradeoff involves balancing the weights in each merging stage to preserve both specialized capabilities (instruction-following, reasoning) and general knowledge (pretraining). Higher weights on reasoning may improve task performance but risk losing general knowledge, while higher pretraining weights may preserve stability but limit reasoning enhancement.

Failure Signatures:
Sequence Length Collapse indicates excessive influence from one fine-tuning paradigm, typically instruction-following. The Reward Hockey Stick Curve suggests unstable initialization leading to reward maximization divergence. Both phenomena indicate poor balance in the merging weights or inadequate preservation of foundational knowledge.

First Experiments:
1. Verify that BAI initialization prevents Sequence Length Collapse by monitoring sequence length distribution during early RL training epochs
2. Compare reward score trajectories between BAI-initialized and traditionally initialized models to confirm mitigation of the Reward Hockey Stick Curve
3. Evaluate performance on diverse benchmarks to ensure BAI maintains or improves upon individual paradigm performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the main text.

## Limitations
- Evaluation primarily focuses on two model scales (2.5B and 25B parameters) and specific MoE architecture, raising generalizability questions
- Lacks extensive ablation studies demonstrating strict necessity of BAI versus alternative initialization strategies
- Long-term stability of sequence generation patterns beyond training period remains unclear
- Does not extensively address potential safety implications of more capable reasoning models

## Confidence
- **High confidence**: BAI successfully resolves Sequence Length Collapse and mitigates Reward Hockey Stick Curve phenomena
- **Medium confidence**: BAI improves overall benchmark performance across diverse tasks
- **Medium confidence**: The two-stage weighted model merging approach is necessary for the reported improvements
- **Low confidence**: Generalizability to other model architectures, scales, and training objectives

## Next Checks
1. Conduct ablation studies comparing BAI against alternative initialization strategies (e.g., pure pretraining initialization, single-stage merging) to isolate the contribution of each merging step
2. Test BAI on dense transformer architectures and different model scales (1B, 8B parameters) to assess architectural generalization
3. Evaluate long-term sequence generation stability by monitoring outputs from models trained with BAI over extended inference periods (100K+ tokens) to detect any delayed collapse phenomena