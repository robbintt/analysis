---
ver: rpa2
title: 'NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with
  LLMs for NOTAM Interpretation'
arxiv_id: '2511.07982'
source_url: https://arxiv.org/abs/2511.07982
tags:
- notam
- framework
- knowledge
- runway
- aviation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NOTAM-Evolve addresses the challenge of interpreting cryptic aviation\
  \ NOTAMs by combining dynamic knowledge grounding and schema-based inference. It\
  \ introduces a self-evolving framework that leverages a knowledge graph-enhanced\
  \ retrieval module and iterative preference optimization to progressively improve\
  \ a large language model\u2019s parsing accuracy without extensive human supervision."
---

# NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation

## Quick Facts
- arXiv ID: 2511.07982
- Source URL: https://arxiv.org/abs/2511.07982
- Reference count: 25
- NOTAM-Evolve achieves 30.4% absolute accuracy improvement over base LLM for aviation NOTAM interpretation

## Executive Summary
NOTAM-Evolve addresses the challenge of interpreting cryptic aviation NOTAMs by combining dynamic knowledge grounding and schema-based inference. It introduces a self-evolving framework that leverages a knowledge graph-enhanced retrieval module and iterative preference optimization to progressively improve a large language model's parsing accuracy without extensive human supervision. Experimental results on a 10,000-sample dataset show a 30.4% absolute accuracy improvement over the base LLM, achieving state-of-the-art performance among open-source models and approaching commercial model levels.

## Method Summary
NOTAM-Evolve employs a three-component architecture: KG-TableRAG for knowledge graph-enhanced retrieval, iterative SFT+DPO optimization with curriculum learning, and multi-view inference via paraphrasing and voting. The framework generates Cypher queries to search an aviation knowledge graph, concatenates graph results with original NOTAMs for table retrieval, and iteratively improves the model through self-generated preference pairs weighted by error rates. Multi-view inference generates five paraphrased variants of each NOTAM, processes them independently, and selects the final output via majority voting.

## Key Results
- 30.4% absolute accuracy improvement over base LLM (45.8% → 62.2%)
- Outperforms existing open-source models while approaching commercial model levels
- Achieves 5% accuracy improvement from multi-view inference mechanism
- Shows progressive improvement across 3-5 optimization iterations

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph-Enhanced Retrieval (KG-TableRAG)
Domain-specific knowledge graphs improve retrieval quality for specialized terminology by resolving implicit relationships that standard RAG misses. The LLM generates Cypher queries to search a domain knowledge graph (e.g., airport → runway relationships), concatenating graph results with the original query before table retrieval to provide structural context for correct inference.

### Mechanism 2: Iterative Preference Optimization with Error-Weighted Curriculum
Combining supervised fine-tuning on correct predictions with DPO on error signals enables progressive improvement without human-annotated reasoning traces. Each iteration generates responses, stores them in a response pool, extracts (correct, incorrect) preference pairs, and applies DPO with curriculum weighting that prioritizes high-error-rate samples as training progresses.

### Mechanism 3: Multi-View Inference via Rewriting and Voting
Semantic-preserving paraphrasing combined with majority voting reduces prediction instability for edge cases. The system generates N=5 paraphrased variants preserving aviation terminology, processes each independently, and selects the final output via majority vote.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Core optimization method replacing reinforcement learning from human feedback; avoids training a separate reward model. Quick check: Can you explain why DPO uses a reference model (π_ref) and what happens if the reference is too similar to the policy?

- **Curriculum Learning**: NOTAMs vary in complexity; error-weighted sampling gradually focuses on harder examples rather than treating all equally. Quick check: What would happen if curriculum weighting (α_e) started at maximum instead of ramping up?

- **Knowledge Graph Querying (Cypher)**: Enables structured retrieval of domain relationships (airport→runway) that unstructured text search cannot capture. Quick check: How would retrieval degrade if the knowledge graph had a temporal mismatch with NOTAM effective dates?

## Architecture Onboarding

- **Component map**: KG-TableRAG (NOTAM → Cypher query → KG lookup → enriched query → table retrieval → grounded context) → Self-Optimization Loop (Response pool → SFT dataset → DPO dataset → updated model) → Multi-View Inference (NOTAM → 5 paraphrases → 5 predictions → majority vote → final output)

- **Critical path**: The iterative loop depends on correct grounding; if KG retrieval fails, preference pairs encode incorrect reasoning, corrupting subsequent iterations.

- **Design tradeoffs**: 3-5 iterations balance improvement vs. computational cost (0.58h → 3.2h scaling); paraphrase count N=5 trades coverage vs. inference latency; error threshold τ determines which samples receive augmentation.

- **Failure signatures**: Accuracy plateaus before target η (insufficient preference pair diversity); specific NOTAM categories underperform (check KG coverage); output format errors increase (Multi-View amplifying structural hallucinations).

- **First 3 experiments**: 1) Validate KG retrieval in isolation: measure retrieval accuracy comparing standard TableRAG vs. KG-TableRAG on 100 NOTAMs. 2) Single-iteration baseline: run one SFT+DPO cycle without curriculum weighting. 3) Multi-View ablation by category: disable rewriting+voting for each NOTAM category separately.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can LLM-assisted annotation with expert validation improve ground truth quality enough to raise the observed performance ceiling, or does the inherent ambiguity of NOTAMs impose a fundamental limit? The paper notes that NOTAM complexity may make perfect ground truth unattainable, and LLM-assisted annotation remains untested.

- **Open Question 2**: How does the framework perform under real-time operational constraints with streaming NOTAM inputs and strict latency requirements? Current implementation reports batch processing times of 0.58 to 3.2 hours per iteration, with inference latency unreported.

- **Open Question 3**: What is the sensitivity of the self-evolving optimization loop to the initial base model quality, and does the framework converge for weaker foundation models? The paper uses DeepSeek-R1-Distill-Qwen-7B but doesn't test convergence for significantly weaker base models.

- **Open Question 4**: How robust is the multi-view inference paraphrasing mechanism to domain-specific terminology that may have no valid synonyms? The paper doesn't quantify how often paraphrasing fails or defaults to near-identical text, which would reduce voting diversity benefits.

## Limitations
- Knowledge graph coverage completeness is not verified; missing relationships cause silent grounding failures
- Dataset and knowledge graph availability unconfirmed, creating reproducibility barriers
- Multi-view inference effectiveness depends on generating meaningfully diverse paraphrases, which may be fundamentally limited for standardized aviation terminology

## Confidence
- **High confidence**: The overall framework architecture is technically sound and follows established NLP patterns
- **Medium confidence**: The 30.4% absolute accuracy improvement and state-of-the-art performance among open-source models
- **Low confidence**: Claims about approaching commercial model levels without direct commercial model comparisons on the same dataset

## Next Checks
1. **KG retrieval validation**: Measure retrieval accuracy comparing standard TableRAG vs. KG-TableRAG on 100 NOTAMs to verify correct runway associations for airport identifiers.

2. **Curriculum contribution isolation**: Run single-iteration baseline (SFT+DPO without curriculum weighting) and compare against full 3-iteration run to quantify curriculum learning's specific contribution.

3. **Paraphrasing safety validation**: Test paraphrasing mechanism on NOTAMs containing numerical runway identifiers and temporal constraints to verify semantic preservation and absence of hallucinations.