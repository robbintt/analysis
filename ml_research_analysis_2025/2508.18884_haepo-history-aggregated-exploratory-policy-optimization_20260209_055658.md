---
ver: rpa2
title: 'HAEPO: History-Aggregated Exploratory Policy Optimization'
arxiv_id: '2508.18884'
source_url: https://arxiv.org/abs/2508.18884
tags:
- haepo
- policy
- exploration
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces History-Aggregated Exploratory Policy Optimization
  (HAEPO), a method that improves exploration and stability in reinforcement learning
  and LLM fine-tuning by leveraging full-trajectory information. HAEPO compresses
  each episode into a cumulative log-likelihood and applies a Plackett-Luce softmax
  over these scores, weighting trajectories proportionally to their returns.
---

# HAEPO: History-Aggregated Exploratory Policy Optimization

## Quick Facts
- arXiv ID: 2508.18884
- Source URL: https://arxiv.org/abs/2508.18884
- Reference count: 33
- This paper introduces HAEPO, which improves exploration and stability in RL and LLM fine-tuning by leveraging full-trajectory information with Plackett-Luce weighting, achieving faster convergence, lower variance, and robust performance across diverse tasks including multi-armed bandits, CartPole, and human-feedback TL;DR summarization.

## Executive Summary
HAEPO addresses the challenge of exploration and stability in long-horizon reinforcement learning and LLM fine-tuning by aggregating entire trajectories into single scores using cumulative log-likelihoods, then applying Plackett-Luce softmax weighting. This trajectory-level approach provides coherent credit assignment signals that outperform fragmented per-token or per-step methods. The method includes entropy regularization to maintain exploration diversity and a soft KL penalty to a frozen reference policy for stability. Empirical results show HAEPO converges faster with lower variance than PPO and GRPO across multiple benchmarks, including achieving superior human preference ratings on TL;DR summarization while reducing GPU memory usage by 26.4%.

## Method Summary
HAEPO optimizes policies by computing per-trajectory cumulative log-likelihoods (L_k) and normalizing them via Plackett-Luce softmax to obtain weights (w_k). The loss function combines negative weighted returns, entropy regularization on the trajectory weights, and a soft KL divergence to a frozen reference policy. This creates a listwise ranking framework where entire trajectories compete based on their total likelihood and returns, rather than individual timesteps. The method uses two reward normalization schemes: sum-normalization for short-horizon tasks and Z-score normalization for long-horizon tasks, with hyper-parameters β_ent and β_kl controlling exploration and stability.

## Key Results
- HAEPO achieves lower per-step regret variance in multi-armed bandits compared to PPO and DPO across K=10,20,30 arms
- On CartPole-v1, HAEPO converges faster with smoother learning curves than PPO, showing improved stability
- In TL;DR human-feedback summarization, HAEPO outperforms DPO and GRPO in human preference ratings while reducing GPU memory usage by 26.4%

## Why This Works (Mechanism)

### Mechanism 1: Listwise Trajectory Weighting via Plackett-Luce
Aggregating per-trajectory log-likelihoods and normalizing via Plackett-Luce softmax improves credit assignment in long-horizon tasks compared to per-token or per-step gradient methods. For each trajectory, sum all log-probabilities into a single score L_k = Σ log π(a_t|s_t), then apply softmax across M trajectories: w_k = exp(L_k) / Σ exp(L_j). This weight multiplies the trajectory's return in the loss, providing one coherent gradient signal per episode rather than T fragmented signals. The core assumption is that long-horizon dependencies are better captured when the entire decision history influences the update as a single unit.

### Mechanism 2: Entropy Regularization on Trajectory Weights
Applying entropy bonus directly to the trajectory-weight distribution sustains exploration and prevents premature collapse onto a few high-return rollouts. Add -β H(w) = β Σ w_k log w_k to the loss. When one trajectory begins to dominate (w_k → 1), its log w_k → 0 while others contribute negative terms, flattening the distribution and keeping lower-weight trajectories in play longer. The core assumption is that diversity in trajectory-weight space correlates with meaningful exploration of the policy's action space.

### Mechanism 3: Soft KL Trust Region at Trajectory Level
Penalizing KL divergence between current and reference trajectory weights stabilizes learning by damping abrupt re-weighting without rigid hard constraints. Compute D_k = log w_k(θ) - log w_k^ref and add λ Σ w_k D_k to the loss. This acts as a "speed governor" on how fast weight allocations can shift from the frozen reference policy. The core assumption is that controlling divergence at the trajectory-weight level provides sufficient stability without needing per-token constraints.

## Foundational Learning

- Concept: **Score-function / log-derivative gradient estimator**
  - Why needed here: HAEPO's gradient (Eq. 6, 10) is derived via ∇w_k = w_k ∇log w_k; understanding why this identity holds is essential for implementing and debugging the update.
  - Quick check question: If π_θ(a|s) = 0.3, what is ∇_θ log π_θ(a|s) in terms of ∇_θ π_θ(a|s)?

- Concept: **Plackett-Luce as first-choice probability**
  - Why needed here: The weights w_k are not arbitrary softmax scores but the probability of selecting trajectory k first under a ranking distribution; this interpretation justifies the normalization and gradient structure.
  - Quick check question: For M=3 trajectories with scores [2, 1, 0.5], compute the Plackett-Luce weight for the first trajectory.

- Concept: **Entropy as exploration bonus vs. penalty**
  - Why needed here: The entropy term appears with a specific sign (β > 0 adds β Σ w_k log w_k to the minimization objective); misplacing the sign encourages collapse instead of exploration.
  - Quick check question: For a two-outcome distribution [0.9, 0.1], is H(w) = -Σ w_k log w_k higher or lower than for [0.5, 0.5]?

## Architecture Onboarding

- Component map:
  1. Rollout buffer -> Trajectory scorer -> Weight normalizer -> Reward normalizer -> Reference weights -> Loss aggregator -> Backward pass

- Critical path:
  1. Sample M trajectories from current policy
  2. Compute per-trajectory L_k and R_k in a single forward pass
  3. Normalize weights via softmax across L_k
  4. Compute entropy and KL penalties
  5. Sum into final loss and backprop; update θ

- Design tradeoffs:
  - Batch size M: Larger M improves weight normalization statistics but increases memory; the paper uses M ∈ {4, 8, 16, 32} across experiments
  - Reward normalization scheme: Sum-normalization suits short-horizon/dense-reward tasks; Z-score suits long-horizon/sparse-reward tasks
  - β and λ tuning: Highly task-dependent; paper reports β, λ ∈ {5e-5, 1e-2} across experiments. No automated schedule is provided

- Failure signatures:
  - Weight collapse: One w_k → 1.0, others → 0; entropy term too weak or returns too sparse
  - No convergence: Loss oscillates or diverges; KL penalty λ too small or learning rate too high
  - Excessive memory: Storing full per-token log-prob graphs for long trajectories; use detached accumulation for L_k when possible
  - Slower than PPO on simple tasks: Expected for short-horizon benchmarks; HAEPO's advantage emerges in long/sparse settings

- First 3 experiments:
  1. Bandit sanity check (K=10 arms, T=1 step): Implement HAEPO with sum-normalization; verify it matches softmax/exp3 behavior and converges to the optimal arm
  2. Random walk (sparse reward, T=500 steps): Compare HAEPO vs. PPO vs. DPO on success rate vs. wall-clock time. Use Z-score reward normalization
  3. Ablation on regularization: Run HAEPO on CartPole with (a) full β and λ, (b) β=0, (c) λ=0, (d) both zero. Replicate Figure 4 pattern

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive schedules or meta-gradients automate the tuning of β_ent and β_kl to reduce task-specific sensitivity? The authors note hyper-parameter sensitivity across tasks and suggest "automated tuning [or] adaptive schedules (e.g. meta-gradient, population-based)." This remains unresolved because optimal settings vary substantially with reward scale and task complexity, requiring manual sweeps. A single adaptive HAEPO variant achieving robust performance across diverse tasks without manual hyperparameter intervention would resolve this.

### Open Question 2
Does HAEPO scale to extremely long horizons (10^4+ steps) found in complex environments like Minecraft? The authors identify a limitation for "Extremely long horizons," noting real-world domains require methods to handle 10^4+ steps, possibly via "hierarchical decomposition." This is unresolved because empirical validation was limited to ~10^3 steps, and memory requirements for storing per-episode log-prob sums may become prohibitive. Implementation of memory-efficient buffering or hierarchical HAEPO successfully solving a 10^4+ step navigation benchmark would resolve this.

### Open Question 3
How can HAEPO be adapted for multi-agent settings with non-stationary dynamics? The authors state that "Multi-agent or competitive settings introduce new challenges (e.g. non-stationarity, joint trajectory ranking) that HAEPO does not yet address." This is unresolved because the single-agent trajectory-centric view does not account for joint action dependencies or shifting opponent policies. A theoretical extension and empirical validation of HAEPO in a multi-agent competitive environment would resolve this.

## Limitations
- Limited ablation studies isolating Plackett-Luce weighting effect from entropy/KL regularization
- Underspecified implementation details (reference policy update frequency, exact optimizer settings, reward model architecture)
- Theoretical grounding sound but empirical isolation of factors incomplete
- Claim that trajectory-level credit assignment is inherently superior lacks rigorous comparison to other trajectory-level methods

## Confidence
- Mechanism (Plackett-Luce weighting): Medium
- Entropy regularization effectiveness: Medium
- Soft KL stability benefit: Low (limited external validation)
- TL;DR human preference results: Low (reward model unspecified)
- GPU memory reduction claim: Medium (depends on unspecified implementation choices)

## Next Checks
1. **Ablation on normalization schemes**: Run HAEPO on CartPole with only Plackett-Luce weighting (β=λ=0) vs. per-token baselines to isolate the listwise effect from regularization
2. **Reference policy update frequency sweep**: Test θ_ref update intervals (every batch, every epoch, fixed) on Random Walk to identify sensitivity and optimal schedule
3. **Gradient variance analysis**: Compare per-update gradient norms and return variance across HAEPO, PPO, and GRPO on multi-armed bandits to quantify claimed stability improvement