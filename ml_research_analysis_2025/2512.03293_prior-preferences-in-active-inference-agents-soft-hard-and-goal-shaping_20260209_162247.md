---
ver: rpa2
title: 'Prior preferences in active inference agents: soft, hard, and goal shaping'
arxiv_id: '2512.03293'
source_url: https://arxiv.org/abs/2512.03293
tags:
- goal
- shaping
- agents
- inference
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how different specifications of prior
  preferences in active inference agents affect their performance and learning. The
  authors compare four types of agents: soft goals with and without goal shaping,
  and hard goals with and without goal shaping, in a grid-world navigation task.'
---

# Prior preferences in active inference agents: soft, hard, and goal shaping

## Quick Facts
- **arXiv ID:** 2512.03293
- **Source URL:** https://arxiv.org/abs/2512.03293
- **Reference count:** 40
- **Primary result:** Agents with goal shaping and hard goals achieve the best performance, reaching the final goal 100% of the time after a few episodes.

## Executive Summary
This paper investigates how different specifications of prior preferences in active inference agents affect their performance and learning in a grid-world navigation task. The authors compare four types of agents: soft goals with and without goal shaping, and hard goals with and without goal shaping, where goal shaping means providing intermediate subgoals. The primary result is that agents with goal shaping and hard goals achieve the best performance, reaching the final goal 100% of the time after a few episodes. Agents without goal shaping learn more about the environment's transition dynamics but take longer to solve the task and have lower success rates. The analysis reveals that goal shaping leads to better exploitation but hampers exploration, while the choice between soft and hard goals affects how agents balance risk and novelty during planning.

## Method Summary
The paper implements a 3×3 grid world navigation task where agents must move from the top-left to bottom-right corner. Four experimental conditions vary along two dimensions: soft vs hard goals (probability mass distribution) and goal shaping vs no goal shaping (single vs time-varying preference distributions). The agents use action-unaware active inference with Bayesian model averaging for action selection. The B matrix (transition dynamics) is learned while the A matrix (state-observation mapping) is known. Ten agents per condition were run for 200 episodes with 5 steps per episode, using 256 policies and 10 inference steps.

## Key Results
- Agents with goal shaping and hard goals achieve 100% success rate after a few episodes
- Agents without goal shaping learn more about transition dynamics but have lower success rates
- Soft goals without goal shaping slightly outperform hard goals without goal shaping
- Goal shaping leads to better exploitation but reduces exploration and model learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing intermediate subgoals (goal shaping) combined with high-confidence targets (hard goals) maximizes task performance by synchronizing perceptual inference with planning.
- **Mechanism:** Goal shaping constrains the policy search space to a specific trajectory. Hard goals (low-entropy preferences) ensure the "risk" term (KL divergence) is minimized only when the agent's belief state precisely matches the target state. This alignment allows the agent to quickly lock onto a single optimal policy and exclude alternatives.
- **Core assumption:** The environment dynamics are learnable within the episode horizon, and the provided subgoals accurately reflect a viable path to the final goal.
- **Evidence anchors:**
  - [abstract] "Agents with goal shaping and hard goals achieve the best performance, reaching the final goal 100% of the time after a few episodes."
  - [section 3] "Goal-shaped agents learned to pick π4, one of the task-solving ones, as the preferred policy and stick with it... In agents with hard goals, the policy minimizing risk is π4 because it brings the agent to visit all the intermediate steps."

### Mechanism 2
- **Claim:** Soft goals (distributed probability mass) without shaping act as a regularizer against the "risk of unfamiliar policies," preventing the agent from misinterpreting unknown trajectories as safe.
- **Mechanism:** In active inference, "risk" is KL divergence. A low-entropy preference (hard goal) compared against a high-entropy belief (unvisited policy) can yield a deceptively low risk score, making unknown policies appear attractive. Soft goals increase the entropy of the preference distribution, raising the baseline risk for unexplored policies and forcing the agent to rely more on learned transition dynamics.
- **Core assumption:** The agent retains sufficient epistemic drive (via novelty terms) to resolve uncertainty about transition dynamics even when risk signals are ambiguous.
- **Evidence anchors:**
  - [section 4.2] "Risk will be relatively low when computed between a low-entropy preference distribution... and a high-entropy variational distribution... increasing the entropy of the preference distribution, i.e., using soft goals, can be seen as an effective method to limit this (incorrect) assignment of instrumental value."

### Mechanism 3
- **Claim:** Bayesian model averaging over policies enables successful navigation even when the agent fails to identify a single dominant optimal policy.
- **Mechanism:** Rather than selecting the action from the single most probable policy (which might be a "task-failing" policy with deceptively low expected free energy), the agent sums the probability mass for each action across all policies. This allows "correct" actions to accumulate probability from multiple valid paths, overriding noise from individual policy evaluations.
- **Core assumption:** The set of considered policies is sufficiently diverse to cover the correct actions, and incorrect policies do not systematically converge on the same wrong action.
- **Evidence anchors:**
  - [section 3] "This action-selection mechanism... prevents agents from performing the potentially wrong action of a task-failing policy... most of the time... agents ended up performing one of the correct sequences."

## Foundational Learning

- **Concept:** Variational Free Energy (VFE) and Expected Free Energy (EFE)
  - **Why needed here:** VFE drives perception (fitting beliefs to observations), while EFE drives planning (balancing exploitation/risk and exploration/novelty). You cannot interpret the "soft vs. hard" distinction without understanding how EFE weights these components.
  - **Quick check question:** Does minimizing EFE always lead to minimizing VFE? (Hint: No, they can conflict, as seen in the "soft goals with goal shaping" agent).

- **Concept:** KL Divergence (Relative Entropy)
  - **Why needed here:** This is the mathematical core of "Risk" in the paper. Understanding that KL divergence is asymmetric and sensitive to the relative entropy of the distributions is critical to diagnosing why hard goals fail without shaping.
  - **Quick check question:** Why might a high-entropy belief distribution have *lower* KL divergence to a target than a different low-entropy distribution?

- **Concept:** Generative Models (POMDPs)
  - **Why needed here:** The agent relies on internal matrices A (emission) and B (transition). The paper analyzes how different preference specifications affect the learning of these matrices.
  - **Quick check question:** In this paper, are the agents learning the A matrix, the B matrix, or both?

## Architecture Onboarding

- **Component map:** Generative Model (A, B) -> Variational Posterior (Q) -> Preference Module (P*) -> Inference Engine (minimizes VFE) -> Planning Engine (minimizes EFE) -> Action Selector (Bayesian Model Average)
- **Critical path:** The specification of P* (Preferences). If this is configured incorrectly (e.g., Hard Goals without Shaping in a stochastic environment), the Risk calculation in the Planning Engine will produce misleading gradients, causing the agent to favor unexplored policies that appear safe due to high uncertainty.
- **Design tradeoffs:**
  - **Exploitation (Hard + Shaping):** Maximize success rate on known tasks; minimize compute on exploration. *Cost:* Zero robustness to environmental changes; poor model learning.
  - **Exploration (Soft + No Shaping):** Maximize learning of transition dynamics (B); robust to changes. *Cost:* Slow convergence; lower immediate success rate.
- **Failure signatures:**
  - **"The Uncertainty Trap":** The agent repeatably executes a policy that loops or ends in a dead-end. Diagnosis: Check if "Risk" is low for a high-entropy belief. This indicates Hard Goals are being used without Shaping, causing the agent to confuse "I don't know where this goes" with "This might go to the goal."
  - **"The Wobbly Path":** The agent reaches the goal but via a different path every time with inconsistent success. Diagnosis: Soft Goals with Shaping. The conflict between perceptual free energy and expected free energy prevents policy convergence.
- **First 3 experiments:**
  1. **Sanity Check (Deterministic Grid):** Implement the "Hard Goals + Goal Shaping" agent. Verify 100% success rate in <10 episodes to confirm the inference engine and EFE calculation are functioning correctly.
  2. **Stress Test (The "Unfamiliar Policy" Bug):** Run "Hard Goals without Goal Shaping." Plot the Risk component of EFE for unvisited policies. Confirm that Risk drops deceptively for high-entropy beliefs before the agent learns the transitions.
  3. **Robustness Test (Wall Insertion):** Train "No Shaping" agents until convergence, then place a wall on their preferred path. Compare recovery speed of Soft vs. Hard goal agents to validate the hypothesis that "No Shaping" agents learn better transition dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do different prior preference specifications affect active inference agents in high-dimensional environments or when preferences are defined over observations rather than states?
- **Basis in paper:** [explicit] The authors state in Section 4.4 that the analysis was limited to preferences defined over states in a low-dimensional grid world, leaving the study of these alternative approaches for future work.
- **Why unresolved:** The current study relies on a fully observable MDP with a simple 3x3 grid, whereas many modern applications use deep learning for high-dimensional inputs where preferences must be defined over sensory observations.
- **What evidence would resolve it:** Comparative experiments in high-dimensional environments (e.g., image-based navigation) analyzing the performance trade-offs between soft and hard goals when using observation-based preference distributions.

### Open Question 2
- **Question:** Can active inference agents progressively learn their own prior preferences across different environments, such as through the use of a Dirichlet prior for the preference distribution parameters?
- **Basis in paper:** [explicit] Section 4.4 explicitly identifies the question of how agent's preferences could be progressively learned as an interesting avenue for future research not covered in the current work.
- **Why unresolved:** The current experiments assume a fixed preference distribution provided by the designer, leaving the mechanisms for autonomous goal acquisition or adaptation unexplored.
- **What evidence would resolve it:** Simulations where agents update the parameters of their preference distribution (e.g., via Dirichlet updating) based on experience, demonstrating the capacity to adapt goals without explicit hard-coding.

### Open Question 3
- **Question:** How does the "risk of unfamiliar policies" phenomenon and the resulting exploration-exploitation balance change in environments with stochastic rather than deterministic transition dynamics?
- **Basis in paper:** [inferred] The analysis of the "risk of unfamiliar policies" in Section 4.2 relies on the assumption of deterministic state transitions. The authors infer behavior based on high-entropy variational beliefs but do not test how environmental stochasticity interacts with the risk term.
- **Why unresolved:** In deterministic settings, high entropy in beliefs indicates a lack of exploration; in stochastic settings, it might reflect environmental ambiguity. It is unclear if soft goals remain superior to hard goals in stochastic contexts.
- **What evidence would resolve it:** Replicating the grid-world experiment with probabilistic state transitions to observe if the probability of selecting task-failing policies based on low risk scores persists or diminishes.

## Limitations
- Empirical scope limited to a single deterministic grid-world environment with fixed goal structure
- Results may not generalize to high-dimensional environments or those with stochastic dynamics
- Study does not explore autonomous preference learning or adaptation across different environments

## Confidence
- **High confidence:** The core mechanism that goal shaping combined with hard goals improves task success rates
- **Medium confidence:** The claim that soft goals without shaping act as a regularizer against uncertainty
- **Low confidence:** The specific claim about the Manhattan distance formula for soft preferences and its normalization parameters

## Next Checks
1. Test the agents in a stochastic grid-world where transition dynamics have 10-20% randomness to validate robustness claims about goal shaping
2. Implement an ablation study removing Bayesian model averaging to confirm it's essential for preventing task-failing policy selection
3. Conduct a learning curve analysis comparing transition matrix accuracy (KL divergence from ground truth) across all four agent types to directly validate the claim that "no shaping" agents learn better dynamics