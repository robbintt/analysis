---
ver: rpa2
title: 'Bilevel optimization for learning hyperparameters: Application to solving
  PDEs and inverse problems with Gaussian processes'
arxiv_id: '2510.05568'
source_url: https://arxiv.org/abs/2510.05568
tags:
- learning
- hyperparameters
- optimization
- hyperparameter
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hyperparameter learning in
  scientific computing and inference problems, such as solving PDEs and inverse problems
  with Gaussian processes. The authors propose a bilevel optimization framework where
  the inner problem estimates the model by balancing data fidelity with smoothness,
  while the outer problem selects hyperparameters to minimize a regularized validation
  loss.
---

# Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes

## Quick Facts
- arXiv ID: 2510.05568
- Source URL: https://arxiv.org/abs/2510.05568
- Reference count: 40
- Primary result: Proposed bilevel framework learns GP hyperparameters for PDE/Inverse problems, reducing per-iteration cost by avoiding repeated nonlinear solves.

## Executive Summary
This paper addresses the challenge of hyperparameter learning in scientific computing and inference problems, such as solving PDEs and inverse problems with Gaussian processes. The authors propose a bilevel optimization framework where the inner problem estimates the model by balancing data fidelity with smoothness, while the outer problem selects hyperparameters to minimize a regularized validation loss. The key innovation is a Gauss-Newton linearization of the inner optimization step, which provides closed-form updates and eliminates the need for repeated costly PDE solves.

## Method Summary
The method uses bilevel optimization where the inner problem solves a PDE-constrained optimization with GP kernels, and the outer problem learns hyperparameters by minimizing validation residuals. The key innovation is Gauss-Newton linearization of the inner optimization step, providing closed-form updates that eliminate repeated PDE solves. Each outer iteration reduces to a single linearized PDE solve followed by explicit gradient-based hyperparameter updates via automatic differentiation.

## Key Results
- Learned hyperparameters lead to significantly lower L2 and L∞ errors in solving Eikonal and Burgers' equations compared to random initialization
- Substantial improvements in accuracy and robustness across extensive numerical experiments on nonlinear PDEs and PDE inverse problems
- Method demonstrates scalability and effectiveness for high-dimensional hyperparameter optimization with neural network-parameterized deep kernels

## Why This Works (Mechanism)

### Mechanism 1: Gauss-Newton Linearization for Closed-Form Inner Updates
Replacing each full inner PDE solve with a single Gauss-Newton linearization step yields an explicit state update without requiring iterative nonlinear solves. The nonlinear training residual R_train(u) is linearized around the current iterate u_k via R_train(u) ≈ r_k + D_u R_train(u - u_k), converting the constrained nonlinear optimization into a linearly constrained quadratic problem with a closed-form kernel representer solution: u_θ(x) = κ_θ(x, Φ)^T K_Φ^{-1}(Φ u_k + r_k).

### Mechanism 2: Validation-Based Outer Objective for Hyperparameter Selection
Minimizing a validation residual (PDE residual at held-out points) selects hyperparameters that generalize better than maximum likelihood or random initialization. The outer objective J(θ) = 1/2 ||R_val(u_θ^*)||_V^2 + R(θ) measures PDE/boundary residual mismatch on validation points independent of collocation points. Gradients ∇_θ J_k are computed by differentiating through the closed-form inner solution via automatic differentiation, then updating θ with Adam.

### Mechanism 3: Neural Network-Parameterized Kernels for High-Dimensional Hyperparameter Spaces
Parameterizing kernel lengthscales with neural networks enables adaptive, spatially varying smoothness and scales to thousands of hyperparameters. The Gibbs kernel κ(x, x') uses lengthscales l(x) output by a neural network. The hyperparameter set θ includes all network weights (e.g., 2,751 parameters). Hypergradients propagate through kernel evaluations and their neural network parameterization via automatic differentiation.

## Foundational Learning

- **Concept: Gaussian Processes and Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: The inner optimization problem minimizes the RKHS norm ||u||_{U_θ} subject to PDE constraints; understanding the kernel-induced function space is essential for interpreting solutions.
  - Quick check question: Can you explain why the minimal-norm interpolant in an RKHS corresponds to the MAP estimator under a zero-mean GP prior?

- **Concept: Fréchet Derivatives for Operator Linearization**
  - Why needed here: The Gauss-Newton linearization requires computing D_u P(u_k), the Fréchet derivative of nonlinear PDE operators, to construct linearized constraints.
  - Quick check question: Given P(u) = -Δu + u³, compute the Fréchet derivative D_u P(u_k) acting on a direction v.

- **Concept: Bilevel Optimization Structure**
  - Why needed here: The framework separates hyperparameter learning (outer) from PDE solution estimation (inner); distinguishing these levels is critical for understanding gradient flow and implementation.
  - Quick check question: Why does implicit differentiation through a fully converged inner solve become computationally prohibitive for large-scale PDE problems?

## Architecture Onboarding

- **Component map**: Collocation point sampler -> Validation point sampler -> Kernel module -> Linearized inner solver -> Outer optimizer -> State updater
- **Critical path**:
  1. Initialize θ_0 and u_0 (e.g., u_0 = 0 or from a preliminary solve).
  2. Loop over GN iterations k:
     - Assemble Φ_k and r_k from linearized PDE/boundary operators at u_k.
     - Solve linearized inner problem for u_θ as a function of θ.
     - Evaluate J_k(θ) on validation points; compute ∇_θ J_k via autodiff.
     - Update θ_{k+1} via Adam (multiple inner learning steps per GN iteration).
     - Set u_{k+1} = Π_k(θ_{k+1}).
  3. Stop when ||θ_{k+1} - θ_k|| or |J_k(θ_{k+1}) - J_k(θ_k)| falls below tolerance.

- **Design tradeoffs**:
  - OTD vs. DTO: OTD preserves continuous formulation fidelity but requires resampling validation points; DTO fixes computational budget but risks discretization bias with sparse validation sets.
  - Nugget regularization: Small diagonal additions (e.g., 10^{-10}) stabilize Gram matrix inversion but may slightly bias solutions; larger values improve conditioning at accuracy cost.
  - Mini-batch size: Smaller batches reduce per-iteration cost but increase gradient variance; larger batches improve stability but raise memory demands.

- **Failure signatures**:
  - Gram matrix singularity: Near-zero eigenvalues cause numerical instability; mitigation is increasing nugget or reducing collocation point density.
  - Non-convergence of θ: Outer loss plateaus or diverges; check validation point coverage, reduce Adam learning rate, or switch to Levenberg-Marquardt for inner linearization.
  - Ill-conditioned linear systems: Large condition numbers in K_Φ; use sparse Cholesky factorizations or low-rank approximations for scalability.

- **First 3 experiments**:
  1. Reproduce the nonlinear elliptic equation case (Section 5.1, Case A) with a single lengthscale; verify convergence from multiple initializations to ~0.2 and confirm L² error ~2×10^{-7}.
  2. Implement the additive kernel case (Section 5.1, Case B) with four hyperparameters; compare convergence behavior between learned and fixed hyperparameters.
  3. Extend to a 1D time-dependent PDE (e.g., Burgers' equation with neural network-parameterized Gibbs kernel) with ~500 hyperparameters; visualize learned lengthscale fields and compare L² errors against untrained baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can rigorous bounds on the linearization error be derived to provide theoretical convergence guarantees for the single-step Gauss-Newton bilevel optimization algorithm?
- Basis in paper: [explicit] Section 6 states: "developing convergence guarantees... and rigorous bounds on the linearization error would strengthen the theoretical foundations."
- Why unresolved: The paper currently relies on empirical numerical validation but lacks mathematical proofs regarding the conditions under which the linearization approximation holds or accumulates error.
- What evidence would resolve it: A formal convergence theorem showing the conditions (e.g., Lipschitz continuity of residuals) under which the algorithm converges to a stationary point.

### Open Question 2
- Question: Can the computational bottleneck of assembling and inverting large Gram matrices be alleviated by integrating low-rank or randomized sketching techniques into the bilevel framework?
- Basis in paper: [explicit] Section 6 notes the method "limits scalability... One may accelerate the linearized inner solve via low-rank and randomized sketching techniques."
- Why unresolved: The current implementation involves costly linear solves that scale poorly with the number of collocation points, restricting application to very large-scale problems.
- What evidence would resolve it: Demonstration of an accelerated algorithm that maintains the accuracy of the full method while achieving improved time complexity on high-dimensional PDE systems.

### Open Question 3
- Question: How can this bilevel framework be extended to train deep neural networks by optimizing nonlinear hidden layers in the outer loop while treating the linear output layer as part of the inner problem?
- Basis in paper: [explicit] Section 6 proposes: "extend the proposed framework to train neural networks... optimize the nonlinear hidden layers of a neural network in the outer loop."
- Why unresolved: The authors currently apply the method to Gaussian Process kernels, but the adaptation to the non-convex loss landscapes of deep learning remains unexplored.
- What evidence would resolve it: A successful hybrid architecture where this bilevel approach improves training stability or generalization compared to standard end-to-end backpropagation.

## Limitations

- The approach hinges on Gauss-Newton linearization, which may accumulate error for highly nonlinear or stiff problems, destabilizing hyperparameter updates
- The DTO variant's reliance on fixed validation points introduces discretization bias that may persist with inadequate validation coverage
- Neural network-parameterized kernels increase susceptibility to overfitting, particularly with limited training data

## Confidence

- **High confidence**: The closed-form representer solution for the linearized inner problem (supported by RKHS theory and direct derivation in the text)
- **Medium confidence**: Validation-based hyperparameter selection improving generalization (intuitively sound but limited empirical comparison to alternatives in corpus)
- **Medium confidence**: Neural network parameterization scaling to high-dimensional hyperparameter spaces (demonstrated numerically but without theoretical convergence guarantees)

## Next Checks

1. Test linearization stability by applying the method to a highly nonlinear PDE (e.g., with shock formation) and monitoring divergence of the linearized solution from full nonlinear solves across iterations
2. Compare DTO vs. OTD variants systematically on identical problems, measuring both final error and computational cost to quantify discretization bias
3. Implement a reduced-parameter baseline (e.g., constant lengthscale) for the neural network kernel experiments to verify that the added expressivity genuinely improves accuracy rather than merely fitting noise