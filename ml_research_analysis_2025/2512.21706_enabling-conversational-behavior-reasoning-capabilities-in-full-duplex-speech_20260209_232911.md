---
ver: rpa2
title: Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech
arxiv_id: '2512.21706'
source_url: https://arxiv.org/abs/2512.21706
tags:
- speech
- audio
- reasoning
- text
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling natural full-duplex
  conversational systems by shifting from black-box prediction to explicit reasoning
  over conversational behaviors. The core method introduces a hierarchical conversational
  behavior detection system that perceives high-level communicative intents and low-level
  speech acts, followed by a Graph-of-Thoughts framework that performs causal inference
  over these behaviors to predict next actions and generate interpretable rationales.
---

# Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech

## Quick Facts
- **arXiv ID**: 2512.21706
- **Source URL**: https://arxiv.org/abs/2512.21706
- **Reference count**: 40
- **Primary result**: Hierarchical conversational behavior detection and Graph-of-Thoughts reasoning framework achieves strong transfer from synthetic to real full-duplex speech data, with AUC 0.70-0.80 on real CANDOR corpus.

## Executive Summary
This paper addresses the challenge of enabling natural full-duplex conversational systems by shifting from black-box prediction to explicit reasoning over conversational behaviors. The authors introduce a hierarchical conversational behavior detection system that perceives high-level communicative intents and low-level speech acts, followed by a Graph-of-Thoughts framework that performs causal inference over these behaviors to predict next actions and generate interpretable rationales. Experiments show the framework achieves strong behavior detection performance (AUC 0.78-0.95, F1 0.47-0.88) on synthetic data and maintains high AUC scores (0.70-0.80) on real dialogue data, while producing plausible rationales with BLEU-1 scores of 0.48-0.58 and demonstrating effective transfer from simulated to real audio.

## Method Summary
The method consists of two stages: First, a hierarchical detection system uses frozen HuBERT audio features and Whisper semantic features, fused through gated attention, to predict high-level speech acts (e.g., Directive, Commissive) and low-level interaction mechanics (e.g., Turn-taking, Backchannel) via parallel classification heads. Second, a Graph-of-Thoughts model constructs directed graphs from OpenIE triples and predicted speech acts, encodes them with Graph Attention Networks, and fuses with audio/text encoders to generate interpretable rationales through a T5 decoder. The system is trained on 192 hours of synthetic data generated via GPT-4o narratives and CosyVoice2 TTS, then evaluated on a 118-hour subset of the CANDOR corpus.

## Key Results
- Strong behavior detection performance on synthetic data: AUC 0.78-0.95, F1 0.47-0.88 across classes
- Robust transfer to real audio: maintains AUC 0.70-0.80 on CANDOR corpus despite training primarily on synthetic data
- Rationale generation quality: BLEU-1 scores of 0.48-0.58 on synthetic and real datasets respectively
- Hierarchical decoupling shows effectiveness: low-level turn-taking detection achieves AUC 0.953, though high-level directives remain challenging (F1 0.471)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decoupling of Perception
- **Claim:** Explicitly decoupling high-level communicative intent (speech acts) from low-level interaction mechanics (turn-taking) allows the model to better capture the causal dependencies between "what" is being conveyed and "how" the interaction is structured.
- **Mechanism:** A dual-head classifier operates over a shared multimodal representation (fused HuBERT audio features and Whisper semantic features). One head predicts high-level tags (e.g., *Directive*, *Commissive*) while the other predicts low-level timing acts (e.g., *Turn-taking*, *Backchannel*).
- **Core assumption:** Conversational behaviors are not monolithic; they emerge from the interaction between discourse intent and floor management signals.
- **Evidence anchors:** [abstract] "...formalizes the intent-to-action pathway with a hierarchical labeling scheme..."; [section 3.1] "...parallel classification heads for high-level and low-level speech acts."; [corpus] High AUC (0.953) for turn-taking suggests the low-level mechanic is well-captured, though corpus evidence for high-level "Directives" (F1 0.471) indicates this decoupling is harder to resolve than mechanics.
- **Break condition:** Performance degrades if the high-level intent is ambiguous or carries no prosodic markers, as seen in the lower F1 scores for *Directives* vs. *Continuations*.

### Mechanism 2: Structured Reasoning via Graph-of-Thoughts (GoT)
- **Claim:** Structuring the reasoning process as a dynamic causal graph allows the system to generate interpretable rationales by inferring intent-act chains rather than relying on opaque token probabilities.
- **Mechanism:** The system constructs a directed graph $G_i = (V_i, A_i)$ where nodes represent OpenIE triples extracted from ASR and predicted speech acts. A Graph Attention Network (GAT) encodes this structure, which is fused with audio/text features to condition a T5 decoder for rationale generation.
- **Core assumption:** The logical dependencies in conversation can be approximated by subject-relation-object triples and co-occurrence adjacency matrices.
- **Evidence anchors:** [abstract] "...infer intent-act chains and generate interpretable rationales."; [section 4.1] "Each behavior is represented as a node in a directed graph... allowing the system to score behavior chains..."; [corpus] BLEU-1 scores improve from 0.48 (synthetic) to 0.58 (real), suggesting the graph structure effectively grounds reasoning when real conversational structure is present.
- **Break condition:** If ASR errors propagate into the OpenIE extraction, the graph topology becomes noisy. Section 6.2 notes that naive text addition hurts performance without conservative gating.

### Mechanism 3: Synthetic Priming for Real-World Transfer
- **Claim:** Training on controllable, event-rich simulated dialogues primes the model to detect sparse conversational events (interruptions, backchannels) that are under-represented in standard corpora.
- **Mechanism:** Synthetic data is generated by prompting GPT-4o for dialogue structure and CosyVoice2 for audio, deliberately inflating the frequency of overlaps and backchannels (e.g., 17.6% interruption rate vs. ~0.4% in human benchmarks).
- **Core assumption:** The acoustic and structural features of simulated events share sufficient invariance with real human speech to enable transfer learning.
- **Evidence anchors:** [abstract] "...strong transfer from simulation to real audio..."; [section 5.1] "...we deliberately increase these events to provide stronger supervision signals."; [corpus] The system maintains robust AUC (0.78-0.95) on the real CANDOR dataset despite training primarily on synthetic data.
- **Break condition:** Domain shift occurs if the simulation's speaking style (WPM 240.8, high filler rate) diverges significantly from the target user demographic.

## Foundational Learning

- **Concept: Full-Duplex vs. Half-Duplex Modeling**
  - **Why needed here:** The paper fundamentally shifts from "turn-based" (Half-Duplex) to "simultaneous" (Full-Duplex) interaction. You must understand that the model is predicting *behavior* while listening, not just waiting for a silence gap to predict text.
  - **Quick check question:** Does the system wait for an end-of-turn signal before predicting the next behavior?

- **Concept: Speech Acts & Pragmatics**
  - **Why needed here:** The model doesn't just predict words; it predicts *function* (e.g., *Constative*, *Directive*). Distinguishing the message's intent from its textual content is central to the "Perception" module.
  - **Quick check question:** What is the difference between a high-level *Directive* and a low-level *Backchannel* in this architecture?

- **Concept: Graph Attention Networks (GAT)**
  - **Why needed here:** The "Reasoning" module relies on GATs to process the Graph-of-Thoughts. You need to know how attention mechanisms apply to irregular graph structures (nodes/edges) rather than just sequential tokens.
  - **Quick check question:** How does the GAT encoder use the adjacency matrix $A_i$ in this framework?

## Architecture Onboarding

- **Component map:** Dual-channel audio (16kHz) -> 1s chunks -> HuBERT (Audio) + Whisper (Text) -> Gated Fusion -> Causal Transformer -> Dual Heads (High/Low Speech Acts) -> ASR Context -> OpenIE (Triples) + Speech Acts -> Graph Construction -> GAT + Audio/Text Encoders -> Gated Fusion -> T5 Decoder -> Rationale

- **Critical path:** The bottleneck is the sequential dependency from **Detection** to **GoT**. The GoT system cannot construct the graph until the Speech Acts Prediction model provides the high/low-level nodes ($\ell_{hi}, \ell_{lo}$). Inference latency is dominated by the T5 decoder and the causal window processing.

- **Design tradeoffs:**
  - **Window Size ($W$):** A 30s window offers the best context/latency trade-off (Section 6.2). Larger windows dilute recency; smaller ones miss dependencies.
  - **Strict Causality ($L=0$):** The paper enforces a "past-only" window. Relaxing this ($L=5-10$) improves metrics but introduces latency, violating real-time constraints.
  - **Modality Fusion:** Adding Text improves performance *only* if gated conservatively; naive text addition hurts due to ASR noise (Appendix A.7).

- **Failure signatures:**
  - **High WPM/Short Overlaps:** The model may struggle with real humans who speak slower than the simulation (avg WPM 240.8).
  - **ASR Cascade Failure:** If OpenIE fails to extract triples from noisy ASR, the GoT graph is empty, reducing the system to an audio-only predictor.

- **First 3 experiments:**
  1. **Modality Ablation:** Run the Detection System using *only* HuBERT (Audio) vs. *only* Whisper (Text) to establish baseline contribution of acoustic vs. semantic features.
  2. **Window Sensitivity:** Vary the causal window $W$ (10s, 20s, 30s) on the CANDOR dataset to verify the "30s optimal" claim for real data.
  3. **Transfer Test:** Train on Synthetic data only (no CANDOR) and evaluate on CANDOR test set to quantify the exact transfer gap without fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can probabilistic or fuzzy representations of speech-act labels better capture the nuance of ambiguous conversational cues compared to the current discrete labeling scheme?
- Basis in paper: [Explicit] The authors state in the Limitations section that "real-world conversational cues are often ambiguous and continuous," and suggest "future work could explore probabilistic or fuzzy representations."
- Why unresolved: The current system relies on discrete labels (e.g., constative, directive), which may fail to represent the inherent ambiguity and fluidity of human interaction.
- What evidence would resolve it: Comparative experiments showing improved behavior detection or rationale quality when using continuous/fuzzy labels versus discrete ones on a dataset of ambiguous conversational moments.

### Open Question 2
- Question: How can the framework's robustness to upstream ASR errors be improved in noisy acoustic conditions?
- Basis in paper: [Explicit] The authors identify that "the systemâ€™s performance is sensitive to upstream errors from ASR" and assert that "improving its robustness in noisy conditions is a critical next step."
- Why unresolved: The current pipeline relies on ASR transcripts for graph construction; noise-induced transcription errors currently degrade the causal reasoning process.
- What evidence would resolve it: Benchmark results demonstrating stable performance (e.g., rationale similarity and detection F1) across varying signal-to-noise ratios (SNR) or synthetic ASR error injection rates.

### Open Question 3
- Question: Does training on the current synthetic corpus limit the model's ability to generalize to diverse speaking styles, accents, and cultural norms?
- Basis in paper: [Explicit] The authors note that while effective for training, the synthetic data "may not capture the full diversity of speaking styles, accents, or cultural norms present in human dialogue."
- Why unresolved: The simulation relies on specific TTS voices and narrative prompts which may lack the variability required for universal generalization.
- What evidence would resolve it: Evaluation of the model's detection and reasoning performance on diverse, out-of-distribution real-world datasets containing varied accents and cultural conversational dynamics.

## Limitations

- **Limited real-world validation**: The evaluation on real human dialogue data (CANDOR) is conducted on a relatively small 118-hour subset compared to the synthetic training corpus (192 hours), potentially limiting generalizability.
- **Cascade failure vulnerability**: The model's effectiveness is highly dependent on the quality of ASR and OpenIE outputs, creating a failure mode where errors propagate through the system without systematic analysis of failure rates.
- **Simulation domain constraints**: The synthetic data generation relies on specific simulation parameters (WPM 240.8, high filler rate) that may not represent diverse speaker populations, potentially limiting the framework's applicability to different demographics.

## Confidence

- **High Confidence**: The hierarchical detection architecture and Graph-of-Thoughts reasoning framework are technically sound and the core methodology is well-specified. The ablation studies and synthetic-to-real transfer results provide strong internal validity for the proposed approach.
- **Medium Confidence**: The performance metrics on real data are promising but based on a single dataset (CANDOR) with limited diversity. The BLEU and ROUGE scores for rationale generation indicate reasonable performance but fall within the range of typical sequence-to-sequence models rather than demonstrating exceptional capability.
- **Low Confidence**: The claims about generalizability to different speaker demographics and conversational contexts are not empirically validated. The model's behavior under significant domain shift (e.g., different languages, cultural communication styles, or acoustic environments) remains unknown.

## Next Checks

1. **Cross-Dataset Transfer**: Evaluate the pre-trained model on a different full-duplex dialogue corpus (e.g., TED talks with overlapping speech or multiparty meeting recordings) without fine-tuning to assess robustness to domain shift and speaker variation.

2. **Error Analysis on Failure Modes**: Conduct a systematic analysis of model predictions where the system fails, specifically examining cases where (a) ASR errors lead to incorrect graph construction, (b) rare speech acts are misclassified, or (c) real human interruptions don't match the simulated patterns. Measure the proportion of errors attributable to each failure mode.

3. **Latency and Real-Time Performance**: Measure end-to-end inference latency on commodity hardware to verify the 30s window constraint is practical for real-time deployment, and test the system's performance under streaming conditions with variable network delays and partial audio inputs.