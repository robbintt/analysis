---
ver: rpa2
title: Mode-Seeking for Inverse Problems with Diffusion Models
arxiv_id: '2512.10524'
source_url: https://arxiv.org/abs/2512.10524
tags:
- logp
- diffusion
- vml-map
- inverse
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses solving inverse problems with pre-trained
  diffusion models using a maximum a posteriori (MAP) estimation approach. The authors
  introduce the variational mode-seeking loss (VML), which minimizes the KL divergence
  between the diffusion posterior and measurement posterior at each reverse diffusion
  step.
---

# Mode-Seeking for Inverse Problems with Diffusion Models

## Quick Facts
- **arXiv ID**: 2512.10524
- **Source URL**: https://arxiv.org/abs/2512.10524
- **Reference count**: 40
- **Primary result**: VML-MAP outperforms diffusion-based MAP estimators on image restoration tasks with better perceptual quality and computational efficiency.

## Executive Summary
This paper introduces a novel maximum a posteriori (MAP) estimation framework for solving inverse problems using pre-trained diffusion models. The key innovation is the variational mode-seeking loss (VML), which minimizes the KL divergence between the diffusion posterior and measurement posterior at each reverse diffusion step. For linear inverse problems, VML admits a closed-form solution without modeling approximations. The resulting VML-MAP algorithm achieves state-of-the-art performance on image restoration tasks, demonstrating superior perceptual quality and computational efficiency compared to existing diffusion-based methods.

## Method Summary
The method frames inverse problems as MAP estimation using pre-trained diffusion models. It introduces VML, derived as the reverse KL divergence between diffusion and measurement posteriors, which is mode-seeking by nature. For linear problems, VML has a closed-form expression combining measurement consistency and diffusion prior terms. The VML-MAP algorithm minimizes this loss at each reverse diffusion step using gradient descent. For ill-conditioned operators, an SVD-based preconditioner accelerates convergence. The method uses EDM noise schedule and handles noisy measurements through early stopping and DDIM refinement.

## Key Results
- VML-MAP achieves LPIPS scores of 0.235 (ImageNet256) and 0.156 (FFHQ256) on half-mask inpainting, outperforming DAPS, DDRM, and ΠGDM
- For 4× super-resolution, VML-MAP achieves FID of 3.86 (ImageNet256) and 3.32 (FFHQ256)
- The preconditioned variant (VML-MAPpre) shows significant improvements on ill-conditioned tasks like deblurring and super-resolution
- VML-MAP is computationally efficient, requiring fewer optimization steps than competing methods while achieving better perceptual quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing VML at each reverse diffusion step steers samples toward posterior modes
- Mechanism: VML is reverse KL divergence $D_{KL}(p(\mathbf{x}_0|\mathbf{x}_t) || p(\mathbf{x}_0|\mathbf{y}))$. As $t \rightarrow 0$, the diffusion posterior collapses to a delta function at $\mathbf{x}_t$. Minimizing this divergence forces $\mathbf{x}_t$ to move so that $p(\mathbf{x}_0|\mathbf{x}_t)$ overlaps with a mode of the measurement posterior, ultimately converging to MAP estimate
- Core assumption: Pre-trained diffusion model provides accurate prior for meaningful posterior modes
- Evidence anchors: Abstract states VML minimizes KL between diffusion and measurement posteriors; Section 3 notes reverse KL promotes mode-matching behavior

### Mechanism 2
- Claim: VML admits closed-form solution for linear inverse problems without approximations
- Mechanism: Analytically derives KL divergence. Expectation splits into squared-error term using denoiser's mean and trace term involving denoiser's covariance. Trace terms converge uniformly to zero as $t \rightarrow 0$ (Proposition B.1), allowing simplified loss combining measurement consistency with diffusion prior terms
- Core assumption: Degradation operator $H$ is linear and measurement noise is Gaussian
- Evidence anchors: Abstract notes VML can be analytically derived for linear problems; Proposition 3.2 gives closed-form expression with explicit measurement consistency term

### Mechanism 3
- Claim: SVD-based preconditioner accelerates convergence for ill-conditioned problems
- Mechanism: Gradient of VML depends on denoiser's Jacobian and degradation matrix $H$. For ill-conditioned operators, gradient direction is poorly scaled. Preconditioner $P$ uses matrix $M$ from singular values of $H$ to reweight gradient, effectively shaping optimization landscape to guide solution toward measurement-consistent subspace
- Core assumption: SVD of degradation operator $H$ is computationally affordable
- Evidence anchors: Section 3 states preconditioner can accelerate convergence; Table 1 shows VML-MAPpre significantly outperforms VML-MAP on deblurring and super-resolution

## Foundational Learning

- **Diffusion Models and Tweedie's Formula**: Why needed - The entire method uses pre-trained diffusion model's denoiser $D_\theta(\mathbf{x}_t, t)$ as proxy for posterior mean $\mathbb{E}[\mathbf{x}_0|\mathbf{x}_t]$. Quick check - What does denoiser $D_\theta(\mathbf{x}_t, t)$ approximate in probabilistic terms?

- **Reverse vs. Forward KL Divergence**: Why needed - Paper uses $D_{KL}(p||q)$ for posterior sampling (mode-covering) but $D_{KL}(q||p)$ (reverse KL) for MAP estimation (mode-seeking). Understanding this difference is crucial to understanding why VML finds single high-probability estimate rather than diverse samples. Quick check - Which KL divergence ($D_{KL}(p||q)$ or $D_{KL}(q||p)$) is "zero-forcing" and tends to find single mode of target distribution?

- **Ill-Posed Inverse Problems**: Why needed - Core problem is solving $\mathbf{y} = H\mathbf{x} + \eta$. Recognizing that $H$ is often ill-conditioned explains why strong prior (diffusion model) and preconditioner are necessary for stable, high-quality reconstruction. Quick check - Why does applying pseudoinverse $H^+$ directly often fail to produce good reconstruction for ill-posed problems?

## Architecture Onboarding

- **Component map**: Pre-trained Diffusion Prior -> Degradation Model (H, y) -> VML-MAP Optimizer -> Reverse Diffusion Loop

- **Critical path**:
  1. Initialize: Sample pure Gaussian noise $\mathbf{x}_T \sim \mathcal{N}(0, \sigma_T^2 I)$
  2. Iterate $t$ from $T$ to $0$:
      a. Optimize: Run $K$ steps of gradient descent on $\mathbf{x}_t$ to minimize VML
      b. Denoise: Transition to next time step by sampling $\mathbf{x}_{t-1} \sim \mathcal{N}(D_\theta(\mathbf{x}_t, t), \sigma_{t-1}^2 I)$
  3. Output: Return final $\mathbf{x}_0$

- **Design tradeoffs**:
  - Dropping higher-order trace terms makes VML computationally tractable. Theoretically sound as $t \rightarrow 0$, but relies on optimizer finding good trajectory
  - SVD preconditioning: VML-MAPpre is faster and more stable for ill-conditioned problems but requires SVD of $H$. VML-MAP works for any $H$ where only forward pass is available but may struggle with conditioning
  - As MAP method, VML-MAP prioritizes perceptual quality (LPIPS/FID) and may sacrifice pixel-wise metrics (PSNR/SSIM)

- **Failure signatures**:
  - Blurry or inconsistent outputs: Suggests optimizer not sufficiently minimizing VML or prior is weak. Check learning rate and number of optimization steps $K$
  - Artifacts at low $t$ (noisy case): Optimizing all the way to $t=0$ with noisy measurements causes artifacts. This is why VML-MAPτ stops optimization early and switches to pure sampling
  - Non-convergence for ill-conditioned $H$: If using vanilla VML-MAP on tough deblurring task, gradient may be too noisy. Preconditioner is intended fix

- **First 3 experiments**:
  1. Toy GMM Validation: Replicate 2D Gaussian Mixture Model experiment. Use simple GMM as data distribution and analytically compute denoiser. Run VML-MAP and plot samples vs. true posterior to isolate VML mechanism
  2. Standard Inpainting: Apply VML-MAP to noiseless image inpainting task (e.g., half-mask) on FFHQ/ImageNet. Use hyperparameters from Table 1 ($N=20, K=50$). Compare LPIPS/FID against DDRM/DAPS to validate on common benchmark
  3. Noisy Super-Resolution with Preconditioner: Implement VML-MAPτpre for 4× super-resolution with noise ($\sigma_y=0.05$). Test full pipeline including preconditioner and noisy-case stopping threshold τ

## Open Questions the Paper Calls Out

- **Open Question 1**: Can higher-order optimization methods be designed to minimize non-convex VML objective effectively for ill-conditioned or non-linear operators without prohibitive computational costs? Based on Section 6 stating availability of practically effective optimizer is critical and designing efficient higher-order solvers remains essential direction for future work.

- **Open Question 2**: What is root cause of "sharp artifacts" observed when minimizing VML at lower diffusion time steps for noisy image restoration? Based on Section 4.2 noting optimization challenges at lower diffusion time steps where resulting image consists of sharp artifacts, proposing heuristic threshold but leaving exact cause to future work.

- **Open Question 3**: How does non-linearity of Latent Diffusion Model (LDM) decoder fundamentally alter VML optimization landscape, and how can this be addressed to avoid measurement inconsistency? Based on Appendix D.3 stating optimization becomes more challenging due to non-linearity of LDM decoder, resulting in blurry and measurement-inconsistent images.

## Limitations
- Assumes access to SVD of degradation operator for preconditioning, which may not be feasible for large-scale problems
- Closed-form VML derivation for linear problems depends on dropping trace terms whose practical impact in high dimensions is not fully characterized
- While perceptual quality gains are well-demonstrated, computational efficiency claims should be interpreted cautiously given per-step optimization cost

## Confidence

- **High confidence**: Theoretical derivation of VML and its closed-form for linear problems (Proposition 3.2) is mathematically sound and well-supported by proofs in Appendix B
- **Medium confidence**: Empirical superiority over baselines (DAPS, DDRM, ΠGDM) on standard benchmarks is convincing, though direct comparisons would benefit from open-sourced implementations
- **Medium confidence**: Perceptual quality gains (LPIPS/FID) are well-demonstrated, but "computational efficiency" claim relative to diffusion-based methods should be interpreted cautiously

## Next Checks

1. **VML Convergence Analysis**: Run 2D GMM toy experiment to visually verify VML-MAP samples converge to posterior modes rather than covering full distribution, isolating mode-seeking behavior from deep learning complexities

2. **Preconditioner Scaling**: Test VML-MAPpre on increasingly ill-conditioned degradation operators (varying kernel sizes for deblurring) to quantify preconditioner's effectiveness and identify when SVD becomes prohibitive

3. **Trace Term Impact**: For small linear inverse problem (e.g., 32×32 inpainting), implement both full VML (with trace terms) and simplified VML to measure actual impact of approximation on reconstruction quality