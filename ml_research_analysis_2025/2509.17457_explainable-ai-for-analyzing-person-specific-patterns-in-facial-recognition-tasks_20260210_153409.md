---
ver: rpa2
title: Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition
  Tasks
arxiv_id: '2509.17457'
source_url: https://arxiv.org/abs/2509.17457
tags:
- facial
- recognition
- across
- activation
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Layer Embedding Activation Mapping (LEAM),
  a novel explainability technique designed to identify which facial areas contribute
  most to recognition for individual subjects. Unlike adversarial attack methods that
  aim to fool recognition systems, LEAM visualizes how these systems work, providing
  insights that could inform future privacy protection research.
---

# Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks

## Quick Facts
- arXiv ID: 2509.17457
- Source URL: https://arxiv.org/abs/2509.17457
- Reference count: 40
- Primary result: LEAM identifies person-specific facial regions (nose 18.9-29.7%) that when occluded significantly reduce recognition accuracy

## Executive Summary
This paper introduces Layer Embedding Activation Mapping (LEAM), a novel explainability technique designed to identify which facial areas contribute most to recognition for individual subjects. Unlike adversarial attack methods that aim to fool recognition systems, LEAM visualizes how these systems work, providing insights that could inform future privacy protection research. The method correlates activation maps with facial regions using a face parser across 1000 individuals and 9 pre-trained facial recognition models.

## Method Summary
LEAM adapts LayerCAM for embedding networks by computing gradients with respect to a cosine similarity loss function during backpropagation. Pixel-specific gradients are ReLU-weighted and multiplied with forward-pass activations, then aggregated across channels to produce activation maps highlighting recognition-relevant regions. The method analyzes 9 convolutional layers per model across 1000 identities from IMDb-Face dataset, correlating activation maps with 19 facial classes from FaRL face parser. Cross-image comparisons use Bhattacharyya Coefficient and Earth Mover's Distance to quantify same-identity pattern similarity versus different-identity patterns.

## Key Results
- Models prioritize central face regions with nose areas accounting for 18.9-29.7% of critical recognition regions
- Same-identity activation patterns show significantly higher similarity (BC: 0.32-0.57) versus different identities (BC: 0.04-0.13)
- LEAM-guided occlusions using 1% of most relevant pixels decrease cosine similarity by 0.1148 versus 0.0286 for random positions
- Occlusion effectiveness transfers across different models, though larger neural networks show greater robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based activation mapping can identify which facial regions contribute most to embedding-based recognition.
- Mechanism: LEAM adapts LayerCAM for embedding networks by computing gradients with respect to a cosine similarity loss function (L = 1 - cos(x₁, x₂)) during backpropagation. Pixel-specific gradients (gᵏᵉᵢⱼ) are ReLU-weighted and multiplied with forward-pass activations (Aᵏᵢⱼ), then aggregated across channels to produce activation maps highlighting recognition-relevant regions.
- Core assumption: Gradients flowing through cosine similarity loss meaningfully indicate pixel importance for recognition, despite known issues with cosine similarity gradients for high-magnitude points tending toward zero.
- Evidence anchors:
  - [abstract] "LEAM visualizes how these systems work, providing insights that could inform future privacy protection research"
  - [section 3.1] Describes mathematical formulation with equations (1-5) for computing weights and activation maps
  - [corpus] Related work "Explaining Facial Expression Recognition" addresses explainability challenges in facial tasks but not for embedding-based recognition specifically
- Break condition: If the target model uses a non-standard embedding space where angular relationships on the hypersphere don't correlate with identity discrimination, the gradient-derived importance maps may not reflect true recognition-relevant regions.

### Mechanism 2
- Claim: Facial recognition models develop individual-specific activation patterns that persist across different images of the same person.
- Mechanism: When normalized activation maps are compared using Bhattacharyya Coefficient and Earth Mover's Distance, same-identity image pairs show substantially higher similarity (BC: 0.32-0.57) than different-identity pairs (BC: 0.04-0.13), suggesting models attend to identity-correlated facial regions rather than applying uniform attention patterns.
- Core assumption: The similarity metrics properly capture meaningful spatial correspondence in activation patterns despite variations in facial expression, angle, and lighting across images.
- Evidence anchors:
  - [abstract] "activation patterns show significantly higher similarity between images of the same individual (Bhattacharyya Coefficient: 0.32-0.57) versus different individuals (0.04-0.13)"
  - [section 4.4, Tables 3-6] Quantitative comparison of BC and EMD values across layers and identities
  - [corpus] No direct corpus evidence for this specific person-specific pattern claim in facial recognition
- Break condition: If face alignment preprocessing fails to normalize for expression/angle variations, observed activation pattern differences may reflect alignment artifacts rather than true identity-specific patterns.

### Mechanism 3
- Claim: Regions identified as highly activated by LEAM on one model transfer effectively as vulnerability targets across different model architectures.
- Mechanism: Occlusions targeting the top 1% of LEAM-identified pixels (from IR_CASIA) decrease cosine similarity by 0.1148 across all tested models versus 0.0286 for random occlusions, suggesting cross-architectural commonalities in facial information processing despite structural differences.
- Core assumption: Simple black pixel occlusions adequately proxy the impact of more sophisticated adversarial perturbations for validation purposes.
- Evidence anchors:
  - [abstract] "LEAM-guided validation occlusions based on just 1% of the most relevant pixels significantly outperform random approaches"
  - [section 4.6, Table 21] Per-model cosine similarity drops for LEAM-guided vs. random occlusions at various thresholds
  - [corpus] "Privacy in Responsible AI: Approaches to Facial Recognition from Cloud Providers" discusses privacy implications but not transferability of explainability-guided attacks
- Break condition: Larger neural networks (IResNet_R100) show greater robustness to small occlusions—transferability may diminish for state-of-the-art architectures with increased capacity.

## Foundational Learning

- **Class Activation Mapping (CAM) and LayerCAM**
  - Why needed here: LEAM extends LayerCAM to embedding networks. Understanding how gradient-weighted activation maps visualize CNN attention patterns is prerequisite to grasping the adaptation.
  - Quick check question: Can you explain why LayerCAM produces spatially-precise activation maps for early convolutional layers but "blob-like" maps for layers near the output?

- **Facial Recognition Embeddings and Cosine Similarity**
  - Why needed here: Unlike classification networks, facial recognition produces embeddings compared via similarity metrics. LEAM's core innovation is computing gradients through cosine similarity rather than class scores.
  - Quick check question: Why does the paper use L(x₁, x₂) = 1 - cos(x₁, x₂) as the loss function rather than directly using cosine similarity values?

- **Face Parsing / Semantic Segmentation**
  - Why needed here: The LEAM-FaRL correlation pipeline requires understanding how a face parser segments images into 19 distinct facial classes (nose, eyes, eyebrows, etc.) to quantify which regions contribute to recognition.
  - Quick check question: Given that the "face" class is too general (~50-56% of activated pixels), what parsing granularity would be needed to identify discriminative micro-features?

## Architecture Onboarding

- **Component map:**
  Face Detection & Parsing (FaRL/FACER) -> Image Preprocessing -> Forward Pass -> Gradient Computation -> LEAM Activation Map Generation -> LEAM-FaRL Correlation -> Similarity Analysis

- **Critical path:**
  1. Face parser must successfully detect single face (images with multiple/no faces are discarded)
  2. Face alignment (dlib landmark detection + rotation to vertical symmetry) is required for valid cross-image activation map comparisons
  3. Layer selection: Focus on layers close to input (higher resolution feature maps) for precise landmark correlation

- **Design tradeoffs:**
  - Threshold selection (top 1% vs. full activation): Lower thresholds centralize attention on nose/eyes but miss distributed contributions; higher thresholds capture holistic patterns but dilute landmark-specific insights
  - Single baseline image vs. averaged activation maps: Single-image patterns are image-specific; averaging reduces expression/angle noise but may miss individual-specific signatures
  - Layer depth: Early layers offer spatial precision; later layers may capture more semantic recognition features

- **Failure signatures:**
  - "Blob-like" activation maps extending beyond facial boundaries → layer too close to output (feature map resolution too low)
  - High variability (CV > 80%) in within-individual activation patterns → face alignment failed or expression variance too high
  - Symmetric facial parts (eyes, ears) showing asymmetric activation → alignment preprocessing issue
  - Low Bhattacharyya Coefficients for same-identity pairs (<0.3) → potential issue with gradient flow or model architecture incompatibility

- **First 3 experiments:**
  1. **Baseline verification**: Replicate the LEAM-guided vs. random occlusion experiment on IR_CASIA with 1% threshold; confirm cosine similarity drop of ~0.1655 (Table 21) for the source model
  2. **Layer ablation**: Compare activation map resolution and landmark correlation precision across conv1, l1_0.conv1, and l2_1.conv2 layers for a single identity; verify Table 1 patterns (e.g., conv1 shows elevated hair/hat attention)
  3. **Cross-model transfer test**: Generate LEAM activation maps on one IResNet variant (e.g., AF_R18), apply occlusions to a different architecture (e.g., CF_R100), and measure similarity drop vs. same-model transfer to quantify architecture-specific vs. general patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LEAM-identified regions be utilized to develop actual, individually tailored privacy protection systems?
- Basis in paper: [explicit] The authors state that future work should focus on "developing individual-specific privacy protection systems based on LEAM guidance."
- Why unresolved: The paper validates that specific regions are critical for recognition and that occluding them is effective, but it does not construct a deployable defense system or algorithm.
- What evidence would resolve it: A system that uses LEAM maps to automatically generate perturbations or physical disguises that successfully evade recognition while remaining inconspicuous.

### Open Question 2
- Question: What is the optimal perturbation content for LEAM-identified locations, given the method only specifies location?
- Basis in paper: [explicit] The limitations section notes, "LEAM itself does not determine the type of the occlusion, but can only specify its location," and the validation used simple black pixels.
- Why unresolved: While the location was proven critical, the visual "content" of the perturbation (e.g., noise, texture, color) was not optimized for effectiveness or stealth in this study.
- What evidence would resolve it: Comparative analysis of different perturbation types (adversarial patches, noise) applied to LEAM regions to find the most effective and least visible solution.

### Open Question 3
- Question: How can protection strategies account for the significant intra-individual variability of activation patterns across different images?
- Basis in paper: [inferred] RQ4 found high variability in activated regions within the same identity (CV > 80% for some features), and the Discussion notes averaged maps "cannot guarantee equal protection" across different photos.
- Why unresolved: A static activation map generated from one image may fail on the same person due to changes in expression, lighting, or angle.
- What evidence would resolve it: A method that dynamically adapts to contextual factors (lighting/pose) or generates a robust "master map" that remains effective across various image conditions.

## Limitations
- Method identifies location but not optimal perturbation content for privacy protection
- Binary gender analysis may not capture complex demographic variations in recognition patterns
- Face alignment preprocessing artifacts could confound person-specific pattern identification

## Confidence
- Same-identity activation patterns (BC: 0.32-0.57): Medium confidence
- Occlusion transferability (0.1148 cosine drop): High confidence
- Gender-specific occlusion effectiveness (0.1248 vs 0.1038): Low confidence

## Next Checks
1. Replicate the same-identity activation pattern analysis across multiple face alignment configurations to isolate whether observed patterns stem from identity versus preprocessing
2. Test occlusion transferability on state-of-the-art FR models (e.g., ArcFace with ResNet-152) to quantify architecture-specific robustness beyond the 9 tested models
3. Conduct demographic analysis stratified by multiple factors (age, ethnicity, gender) to validate whether occlusion effectiveness patterns hold across diverse populations