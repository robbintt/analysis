---
ver: rpa2
title: Refining Text Generation for Realistic Conversational Recommendation via Direct
  Preference Optimization
arxiv_id: '2508.19918'
source_url: https://arxiv.org/abs/2508.19918
tags:
- item
- dialogue
- recommendation
- information
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality dialogue
  summaries and item recommendation information in conversational recommender systems.
  The authors propose using Direct Preference Optimization (DPO) to fine-tune language
  models, ensuring that generated texts contain information crucial for accurate item
  recommendations.
---

# Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization

## Quick Facts
- **arXiv ID**: 2508.19918
- **Source URL**: https://arxiv.org/abs/2508.19918
- **Reference count**: 20
- **Primary result**: DPO fine-tuning of dialogue summaries improves recommendation accuracy (HR@1: 0.2439→0.2474, MRR: 0.2439→0.2474) on Japanese datasets.

## Executive Summary
This paper addresses the challenge of generating high-quality dialogue summaries and item recommendation information in conversational recommender systems. The authors propose using Direct Preference Optimization (DPO) to fine-tune language models, ensuring that generated texts contain information crucial for accurate item recommendations. DPO training was applied to both dialogue summary and item recommendation information generation models, optimizing them to produce outputs that improve downstream recommendation performance. Experiments on two Japanese datasets (Tabidachi Corpus and ChatRec) showed that the proposed method outperformed baseline approaches, achieving higher Hit Rate (HR) and Mean Reciprocal Rank (MRR) scores. For example, on Tabidachi Corpus, HR@1 improved from 0.2439 to 0.2474 and MRR from 0.2439 to 0.2474. Human evaluation confirmed that DPO-trained dialogue summaries better captured user preferences and were rated higher in consistency, fluency, and usefulness. However, item recommendation information quality declined slightly, likely because DPO did not consistently improve this component. The study highlights that enhancing dialogue summaries through DPO is critical for improving recommendation accuracy in realistic conversational scenarios. Limitations include the use of medium-scale models, narrow domain/language scope, and occasional hallucinations in generated texts.

## Method Summary
The method uses a two-stage training pipeline with Direct Preference Optimization (DPO) to refine text generation for conversational recommendation. First, a score predictor (deberta-v3-japanese-large) is trained to predict recommendation scores from concatenated inputs of dialogue summary, item recommendation information, and item description. Then, text generators (Llama-3.1-Swallow-8B) are fine-tuned using DPO. The DPO process generates K candidate summaries and J candidate item texts, scores all combinations using the frozen predictor, and constructs preference pairs based on distance to ground-truth labels. The generation models are optimized to produce outputs that maximize the predictor's score, implicitly learning which information dimensions matter for recommendation accuracy. The method is applied to both dialogue summary and item recommendation information generation components.

## Key Results
- DPO-trained dialogue summaries improved recommendation accuracy: HR@1 increased from 0.2439 to 0.2474 and MRR from 0.2439 to 0.2474 on Tabidachi Corpus
- Human evaluation showed DPO summaries superior in Usefulness (51.54% win rate), Consistency, and Fluency compared to baseline
- Dialogue summary optimization contributed more to recommendation accuracy than item recommendation information optimization, based on ablation studies
- Generated texts became longer (118.6→151.2 tokens) with decreased Distinct-1/2 scores, indicating increased keyword repetition

## Why This Works (Mechanism)

### Mechanism 1: Score Predictor as Preference Signal Generator
A frozen score predictor provides meaningful preference signals for training text generation models via DPO, conditional on the predictor learning useful associations between text patterns and recommendation accuracy. Multiple candidate texts are generated, scored against ground-truth labels, and ranked. The text yielding predictions closest to ground-truth becomes the "winner"; furthest becomes the "loser." DPO optimizes generation toward winner characteristics. Core assumption: Score predictor accuracy correlates with recommendation-relevant information density in generated text.

### Mechanism 2: DPO-Induced Information Density Prioritization
DPO training guides models toward retaining preference-critical phrases while tolerating lexical repetition. The loss function reinforces outputs that the score predictor evaluates as more accurate, implicitly learning which information dimensions matter for downstream scoring. Core assumption: The predictor's scoring behavior reflects genuine recommendation utility, not dataset artifacts.

### Mechanism 3: Dialogue Summary Quality as Primary Performance Driver
Optimizing dialogue summaries via DPO contributes more to recommendation accuracy than optimizing item recommendation information, based on ablation evidence. Summaries extract and compress user preferences from raw dialogue; their quality determines how effectively the predictor can match users to items. Item information provides supplementary context but is secondary. Core assumption: User preference signals embedded in dialogue are the primary determinant of recommendation success.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Core technique replacing RLHF-style reward modeling with direct policy optimization on preference pairs
  - Quick check question: In Equation 5, what does β control, and why does it matter for preventing overfitting to preference data?

- **Concept: SumRec Architecture**
  - Why needed here: This work extends SumRec; understanding its three-component pipeline is prerequisite
  - Quick check question: What are the three inputs concatenated with [SEP] tokens and fed to the score predictor?

- **Concept: Two-Stage Training with Frozen Components**
  - Why needed here: The method's success depends on the score predictor remaining stable during DPO training
  - Quick check question: Why must the score predictor be frozen during DPO, rather than jointly optimized?

## Architecture Onboarding

- **Component map**: Dialogue Summary Generation Model (8B) -> Item Recommendation Information Generation Model (8B) -> Score Predictor (352M) -> DPO Training Loop

- **Critical path**:
  1. Pre-train score predictor to convergence on recommendation labels (y=1 for recommended items, y=0 otherwise)
  2. Generate candidate texts using structured prompts (Appendix B.1, B.2)
  3. Freeze predictor; score all candidate combinations
  4. Select winners/losers per Equations 3-4; train generation models with DPO for 1 epoch
  5. Inference: Generate single summary + item info → predictor outputs score

- **Design tradeoffs**:
  - 8B model vs. larger LLMs: Lower GPU memory (~80GB A100 x4) and faster training (~24h/epoch), but performance ceiling limited
  - DPO on both components vs. only summary: Full DPO yields best results, but summary-only provides most gains (Table 3)
  - Preference pair construction via predictor vs. human annotation: Scalable and task-aligned, but inherits predictor biases

- **Failure signatures**:
  - Hallucination in item info: Model fabricates details like "interior is stylish" not in source description (Table 12)
  - Domain narrowness: Only validated on Japanese travel datasets (Tabidachi, ChatRec); generalization unproven
  - Verbosity-diversity tradeoff: Longer texts with repeated keywords may reduce distinctiveness (Distinct-1/2 drops in Table 2)

- **First 3 experiments**:
  1. Reproduce main results: Implement Baseline, SumRec, and proposed method on Tabidachi Corpus; verify HR@1 improves from 0.2040 (SumRec) to 0.2474 (Ours)
  2. Run ablation conditions: Test w/o Rec-DPO and w/o Sum-DPO; confirm summary-only DPO yields HR@1 ~0.2393, item-only yields ~0.2341
  3. Inspect generated outputs: Compare summary length and Distinct scores between SumRec and DPO-trained models; manually check for hallucination in item info

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed DPO-based optimization framework scale effectively to state-of-the-art LLMs (e.g., hundreds of billions of parameters), and how does this impact the trade-off between recommendation performance and computational latency?
- Basis in paper: The authors state in the Limitations section that the models employed are "medium-scale" (8B parameters) and not representative of SOTA models.
- Why unresolved: The current experiments were restricted to Llama-3.1-Swallow-8B-v0.1 due to resource constraints.
- What evidence would resolve it: Evaluation of the proposed DPO training pipeline when applied to significantly larger foundation models (e.g., 70B+ parameters), measuring both recommendation metrics (HR, MRR) and inference latency/resource consumption.

### Open Question 2
- Question: Is the proposed method generalizable to other languages and recommendation domains beyond Japanese travel datasets?
- Basis in paper: The authors explicitly state in the Limitations section that experiments were conducted exclusively on two Japanese datasets within the travel domain.
- Why unresolved: The training data and specific DPO preference pairs were constructed solely from Japanese travel dialogues, potentially encoding domain or language-specific biases.
- What evidence would resolve it: Application of the same DPO summarization and item information generation pipeline to English conversational recommendation datasets or other domains like movies or e-commerce.

### Open Question 3
- Question: Can the generation of item recommendation information be constrained to strictly adhere to factual content in the source description to eliminate hallucinations?
- Basis in paper: The authors identify in the Limitations that a notable limitation is the persistence of hallucinations where the model fabricates features not present in the source content.
- Why unresolved: The current DPO process optimizes for recommendation score accuracy rather than factual grounding, sometimes leading the model to generate persuasive but hallucinated details to fit the predicted preference.
- What evidence would resolve it: Integration of factual consistency metrics or constraints into the DPO reward signal, followed by a manual or automated evaluation of the generated texts against source descriptions to measure hallucination rates.

## Limitations
- Domain and language generalization limited to Japanese travel datasets; effectiveness on other languages or domains remains unproven
- Medium-scale models (8B parameters) limit absolute performance and scalability to larger LLMs
- Potential for hallucination in item recommendation information where model fabricates details not present in source descriptions

## Confidence
- **High Confidence**: Experimental setup is clearly defined with reproducible results on Tabidachi Corpus and ChatRec datasets; observed improvements in HR and MRR are statistically supported
- **Medium Confidence**: Proposed mechanism (DPO using frozen score predictor preference signals) is logically sound and supported by ablation evidence, but reliance on single predictor architecture reduces generalizability claims
- **Low Confidence**: Claims about robustness to hyperparameter variations and performance on datasets with different dialogue structures are not substantiated

## Next Checks
1. Cross-Lingual and Cross-Domain Validation: Apply the proposed DPO method to English conversational recommendation datasets (e.g., ReDial, TGIF) or non-travel domains (e.g., movie, book recommendations) to assess generalization
2. Robustness to Predictor Quality: Train multiple score predictors with varying architectures (e.g., RoBERTa, T5) or data augmentation strategies to measure sensitivity of DPO-trained generators to predictor performance
3. Factual Accuracy and Hallucination Analysis: Implement automated factuality checks (e.g., using RAG-based consistency scoring) to quantify hallucination rates in item recommendation information before and after DPO training