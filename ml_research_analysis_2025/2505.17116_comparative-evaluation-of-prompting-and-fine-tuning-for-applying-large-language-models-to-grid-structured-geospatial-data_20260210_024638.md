---
ver: rpa2
title: Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language
  Models to Grid-Structured Geospatial Data
arxiv_id: '2505.17116'
source_url: https://arxiv.org/abs/2505.17116
tags:
- data
- https
- structured
- fine-tuning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how large language models (LLMs) can interpret
  grid-structured geospatial climate data, comparing prompt-based approaches to fine-tuning
  on a structured dataset of user-assistant interactions. The base model showed basic
  retrieval capabilities but struggled with unit handling, scenario selection, and
  precision.
---

# Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data

## Quick Facts
- **arXiv ID:** 2505.17116
- **Source URL:** https://arxiv.org/abs/2505.17116
- **Authors:** Akash Dhruv; Yangxinyu Xie; Jordan Branham; Tanwi Mallick
- **Reference count:** 12
- **Primary result:** Fine-tuning an 8B LLM with LoRA improved accuracy from 0.29 to 1.0 and semantic similarity from 0.83 to 0.90 on geospatial climate data extraction tasks.

## Executive Summary
This paper evaluates how large language models (LLMs) can interpret grid-structured geospatial climate data, comparing prompt-based approaches to fine-tuning on a structured dataset of user-assistant interactions. The base model showed basic retrieval capabilities but struggled with unit handling, scenario selection, and precision. Fine-tuning using LoRA improved semantic similarity from 0.83 to 0.90 and accuracy from 0.29 to 1.0, demonstrating robust performance on geospatial reasoning tasks. The study highlights fine-tuning as essential for high-precision, domain-aligned LLM outputs in scientific contexts. Future work will expand the dataset and integrate real-time API-based data extraction for interactive climate applications.

## Method Summary
The study fine-tuned LLaMA 3.1 8B using LoRA (rank 8, alpha 16) on ~120 JSON-formatted climate data examples from the ClimRR dataset. The training used 8-bit quantization, bfloat16 mixed precision, 2048-token context, and effective batch size 8 via gradient accumulation. The dataset was split 90/10 for training and testing, with evaluation using all-MiniLM-L6-v2 for semantic similarity and regex-based accuracy scoring for structured data extraction.

## Key Results
- Base model accuracy: 0.29 (struggled with exact value extraction and scenario consistency)
- Fine-tuned model accuracy: 1.0 (perfect extraction performance)
- Semantic similarity improvement: 0.83 to 0.90 after fine-tuning
- LoRA hyperparameters: rank 8, alpha 16, 8-bit quantization with BitsAndBytes

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient fine-tuning with LoRA aligns a small LLM's output generation to specific formats and value-precision requirements that zero-shot prompting cannot reliably enforce. LoRA injects trainable low-rank decomposition matrices into transformer layers, allowing gradient-based optimization to reshape output distribution for structured value extraction using few examples without altering most pre-trained weights. Core assumption: task procedural logic can be encoded in a low-dimensional subspace learnable from ~100 examples. Evidence: fine-tuned model achieved "perfect accuracy (1.0)" vs base model's 0.2889; LoRA used with rank 8 and alpha 16. Break condition: insufficient LoRA rank to capture data complexity or homogeneous training examples causing overfitting.

### Mechanism 2
Structured prompting alone is insufficient for high-precision retrieval because base models lack procedural discipline to consistently parse multi-variable constraints from single context. Base LLMs use in-context learning but next-token prediction is probabilistically influenced by broad pre-training data, causing hallucination or defaulting to common patterns rather than adhering to strict extraction rules. Core assumption: failures stem from misalignment between pre-trained behavior and rigid output format, not model capacity. Evidence: base model "struggled with exact value extraction and scenario consistency (accuracy 0.2889)" despite moderate semantic similarity; ignored units, made rounding errors, defaulted to arbitrary scenarios. Break condition: sophisticated prompting (Chain-of-Thought) could enforce discipline, making fine-tuning unnecessary.

### Mechanism 3
A specialized, smaller model (8B parameters) can be more effective and efficient in agentic workflow than larger general-purpose model. Specializing compact model minimizes computational resources for high-frequency well-defined sub-task (data interpretation), preserving system modularity and latency critical for real-time decision support while avoiding cost of routing every query to massive frontier model. Core assumption: task complexity is bounded and can be captured by smaller architecture once properly fine-tuned. Evidence: Section 1 argues small models "provide favorable balance between performance and resource efficiency"; successful use of fine-tuned LLaMA 3.1 8B validates sufficiency. Break condition: task requires complex reasoning exceeding 8B capacity, resulting in correct formatting but factually shallow interpretations.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed: Core technique making fine-tuning feasible and efficient by updating tiny fraction of weights
  - Quick check: What are LoRA hyperparameters (rank, scaling factor) and what quantization technique reduces memory usage?

- **Concept: Prompt Engineering vs. Fine-Tuning**
  - Why needed: Direct comparative study; understanding distinct failure modes of prompting (low precision) versus fine-tuning benefits (high consistency)
  - Quick check: What was primary performance gap fine-tuning addressed, and which metric showed most dramatic improvement?

- **Concept: Tokenization of Structured Data**
  - Why needed: Central challenge applying text-based model to grid-structured data; understanding JSON serialization into tokens fundamental to input and failure points
  - Quick check: How is grid-structured atmospheric data presented to LLM, and what component handles initial data extraction?

## Architecture Onboarding

- **Component map:** ClimRR dataset -> Semi-automated pipeline (queries, extracts, formats JSON, generates QA pairs) -> LLaMA 3.1 8B (Core) with LoRA adapters (rank=8, alpha=16) (Adaptation) via Unsloth-AI API with BitsAndBytes 8-bit quantization (Support) -> Python scripts with regex and all-MiniLM-L6-v2 (Evaluation)

- **Critical path:** Performance most sensitive to quality and curation of fine-tuning dataset; ~100 examples require each to be accurate, clear, and adhere to desired format for successful knowledge transfer

- **Design tradeoffs:**
  - Precision vs. Generalization: Fine-tuned model achieves near-perfect precision but may lose base model's generative flexibility (good for agentic tool, bad for general chatbot)
  - Accuracy Metric: Strict regex-based accuracy appropriate for data extraction but may penalize semantically correct format violations; semantic similarity provides complementary softer measure
  - Small vs. Large Models: 8B model prioritizes inference speed and deployability over larger models' reasoning power, betting task can be solved with specialized smaller model

- **Failure signatures:**
  - Base Model: Inconsistent RCP scenario handling (blending/ignoring), missing units, "minor discrepancies" in numerical values
  - Fine-Tuned Model: Primary risk is overfitting; failure signature is rigidly following training format but failing to correctly answer slightly novel out-of-distribution query

- **First 3 experiments:**
  1. Baseline Replication: Re-run base LLaMA 3.1 8B with structured prompting methodology; confirm accuracy ~0.29 and reported errors (missing units, scenario confusion)
  2. Fine-Tuning Ablation: Fine-tune model with specified LoRA configuration using repository and dataset; show accuracy increase toward 1.0 and semantic similarity lift to ~0.90
  3. Format Robustness Test: Test fine-tuned model with input JSON format variations or query phrasings not in training set; probe robustness and identify template overfitting

## Open Questions the Paper Calls Out

- **Question:** How does fine-tuned model perform when deployed in real-time agentic workflow dynamically querying ClimRR API?
  - Basis: Authors state developing "real-time agentic workflow where external system extracts relevant data from ClimRR API, and LLM interprets it on demand"
  - Why unresolved: Current study evaluates on static curated dataset rather than live interactive system with dynamic data retrieval
  - What evidence resolves: Evaluation results (accuracy and semantic similarity) from deployed system processing live API responses in real-time

- **Question:** Can LLMs maintain high accuracy (1.0) when exposed to more complex nuanced queries outside current template-generated dataset?
  - Basis: Conclusion notes planning to "broaden dataset to include more complex and nuanced queries"
  - Why unresolved: Current dataset relies on predefined templates with semi-automated generation that may not cover full complexity of real-world interactions
  - What evidence resolves: Comparative evaluation using new test set of adversarial or highly complex queries to test 1.0 accuracy robustness

- **Question:** Do performance gains from fine-tuning on ClimRR data generalize to other grid-structured geospatial datasets like ERA5 or MERRA-2?
  - Basis: Introduction discusses various grid-structured datasets (ERA5, MERRA-2, CHIRPS) but experimental scope limited exclusively to ClimRR
  - Why unresolved: Unclear if model learned generalizable spatiotemporal reasoning or simply overfit to specific ClimRR structure and variables
  - What evidence resolves: Zero-shot or few-shot performance metrics of ClimRR-fine-tuned model applied to ERA5 or MERRA-2 data samples

## Limitations
- Small manually curated dataset (~120 examples) may not capture full diversity of real-world geospatial queries or edge cases
- Strict regex matching evaluation could penalize semantically correct but format-violating responses
- 8B parameter model size may not generalize to more complex geospatial reasoning or larger-scale climate datasets
- Study doesn't address computational efficiency comparisons between prompt-only and fine-tuned approaches in deployment

## Confidence
- **High Confidence:** Comparative effectiveness of fine-tuning versus prompting for precision tasks (dramatic improvement from 0.29 to 1.0 accuracy and 0.83 to 0.90 semantic similarity)
- **Medium Confidence:** Sufficiency of 8B parameter model for this class of geospatial tasks (successful task completion but limited testing scope)
- **Medium Confidence:** LoRA hyperparameters (rank 8, alpha 16) being optimal for this task (chosen but not experimentally validated against alternatives)

## Next Checks
1. Dataset Diversity Test: Evaluate fine-tuned model on expanded diverse dataset including edge cases, alternative phrasings, and out-of-distribution queries to assess generalization limits

2. Computational Efficiency Benchmark: Measure and compare inference latency, memory usage, and throughput between fine-tuned 8B model and larger base models using structured prompting across varying query loads

3. Prompt Engineering Baseline Comparison: Implement and test advanced prompting strategies (Chain-of-Thought, multi-turn refinement) on base model to establish whether fine-tuning remains superior with sophisticated prompt engineering