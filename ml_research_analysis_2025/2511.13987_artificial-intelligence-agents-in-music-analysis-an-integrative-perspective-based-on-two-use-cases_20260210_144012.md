---
ver: rpa2
title: 'Artificial Intelligence Agents in Music Analysis: An Integrative Perspective
  Based on Two Use Cases'
arxiv_id: '2511.13987'
source_url: https://arxiv.org/abs/2511.13987
tags:
- consistent
- music
- musical
- analytical
- harmonic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two complementary case studies on AI in music
  analysis and education. The first case integrates generative AI tools (ChatGPT,
  Suno, Music.AI) into secondary education, demonstrating improved analytical articulation
  (85% of students) and highlighting the importance of precise parameter specification
  (90%).
---

# Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases

## Quick Facts
- arXiv ID: 2511.13987
- Source URL: https://arxiv.org/abs/2511.13987
- Authors: Antonio Manuel Martínez-Heredia; Dolores Godrid Rodríguez; Andrés Ortiz García
- Reference count: 5
- Primary result: Multi-agent systems show high consistency in automated music analysis but face challenges with transitional repertoire.

## Executive Summary
This paper presents two complementary case studies on AI in music analysis and education. The first case integrates generative AI tools (ChatGPT, Suno, Music.AI) into secondary education, demonstrating improved analytical articulation (85% of students) and highlighting the importance of precise parameter specification (90%). The second case develops a multi-agent system for symbolic music analysis, showing high consistency across structural, harmonic, and stylistic analyses of an 18th-century corpus, with isolated "hallucinations" revealing limitations in transitional works. Together, these cases illustrate AI's potential to enhance both pedagogical outcomes and analytical capabilities while emphasizing the need for hybrid human-AI approaches.

## Method Summary
The study combines two distinct methodologies: (1) a pedagogical intervention where secondary students used ChatGPT for structural analysis and Suno/Music.AI for composition, evaluated through both objective metrics (DTW, entropy, harmonic coherence) and subjective Likert ratings; and (2) a multi-agent system architecture with specialized agents for structural, harmonic, and stylistic analysis of 50 18th-century works, validated against expert musicologist assessments. The multi-agent system used LangGraph orchestration with rule-based algorithms supplemented by a curated reference database for stylistic attribution.

## Key Results
- Multi-agent system achieved high consistency across structural, harmonic, and stylistic analyses of 18th-century repertoire
- 85% of students demonstrated improved analytical articulation through AI-integrated composition tasks
- Stylistic misattributions occurred in transitional works (e.g., Gluck, C.P.E. Bach), revealing system limitations
- Hybrid evaluation combining objective metrics and subjective ratings proved robust for assessing AI-generated musical outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent specialization enables modular, explainable music analysis.
- Mechanism: Distributing analytical tasks across specialized agents (structural, harmonic, stylistic) allows each module to handle a distinct musical dimension independently, then synthesize outputs via a coordination layer.
- Core assumption: Musical analysis can be decomposed into semi-independent subtasks without losing interpretive coherence.
- Evidence anchors:
  - [abstract] "The second case develops a multi-agent system for symbolic music analysis, showing high consistency across structural, harmonic, and stylistic analyses."
  - [section 3.2.1] "Dividing the analysis into specialized agents allows independent handling of each musical dimension (harmonic, melodic, rhythmic, formal), facilitating continuous improvement of each component without affecting the overall system."
  - [corpus] Weak direct evidence; neighbor papers discuss LLM-based pedagogical agents but not multi-agent music architectures specifically.
- Break condition: Works with ambiguous formal boundaries or transitional stylistic features produce "hallucinations" (e.g., stylistic misattribution in Handel's *Water Music*, structural divergence in Gluck's *Iphigénie en Tauride*).

### Mechanism 2
- Claim: Precise parameter specification mediates the gap between analytical understanding and AI-assisted creative output.
- Mechanism: Students translate structural analysis (form, harmony, mood, tempo) into explicit compositional parameters, which condition generative AI tools; iterative refinement cycles improve both specification quality and critical evaluation skills.
- Core assumption: Students can articulate musical intentions sufficiently for AI systems to interpret, and iterative feedback improves this articulation.
- Evidence anchors:
  - [abstract] "85% of students [demonstrated] improved analytical articulation... highlighting the importance of precise parameter specification (90%)."
  - [section 3.1.2–3.1.3] "Students collaboratively designed specifications for original compositions based on their structural analysis" using ChatGPT as an interactive assistant.
  - [corpus] Related work on adaptive scaffolding for LLM-based pedagogical agents supports the theoretical basis, but direct replication in music education contexts is limited.
- Break condition: Over-reliance on AI tools may reduce critical engagement; dataset biases in generative models may reinforce stylistic narrowness (noted in section 4.1.2 discussion).

### Mechanism 3
- Claim: Hybrid human-AI evaluation combining objective metrics and subjective ratings yields robust assessment of AI-generated musical outputs.
- Mechanism: Objective measures (DTW for melodic similarity, entropy for rhythmic diversity, harmonic coherence scores) are complemented by blind subjective ratings on multiple dimensions, providing both reproducibility and aesthetic sensitivity.
- Core assumption: Neither purely computational metrics nor human judgment alone fully captures musical quality; their integration compensates for respective blind spots.
- Evidence anchors:
  - [section 3.1.5] "The evaluation procedure combined... melodic similarity (DTW), harmonic coherence, and rhythmic diversity (entropy)" alongside "blind listening sessions, scoring each output on 5-point Likert scales."
  - [section 4.1.1] Comparative table shows significant differences between platforms (Suno vs. Music.AI) across expressiveness, stylistic accuracy, and other dimensions.
  - [corpus] Weak evidence; neighbor papers do not directly address hybrid evaluation in music analysis.
- Break condition: Metrics may not capture cultural or expressive nuances excluded from the objective feature set; subjective ratings may reflect student background bias.

## Foundational Learning

- Concept: **Symbolic Music Representation (MIDI, MusicXML, Kern)**
  - Why needed here: The multi-agent system (Case Study 2) operates on structured symbolic inputs, not raw audio; understanding these formats is prerequisite to input processing and feature extraction.
  - Quick check question: Can you identify what musical information is preserved in MIDI vs. MusicXML (e.g., pitch, duration, articulation, dynamics)?

- Concept: **Multi-Agent System Fundamentals**
  - Why needed here: Case Study 2's architecture relies on specialized autonomous agents coordinated by a central orchestrator; modular design and inter-agent communication are core to the approach.
  - Quick check question: Explain how task decomposition and result synthesis differ in a multi-agent system versus a monolithic model.

- Concept: **Music Analysis Basics (Form, Harmony, Style)**
  - Why needed here: Both case studies assume familiarity with analytical categories (verse-chorus structure, harmonic progressions, period attribution); the system's outputs are evaluated against these concepts.
  - Quick check question: What are the primary dimensions of music analysis referenced in the paper, and how might they interact (e.g., how does harmonic progression relate to formal segmentation)?

## Architecture Onboarding

- Component map:
  - Coordinator Agent -> Structural Agent, Harmonic Agent, Stylistic Agent -> Integration Layer

- Critical path:
  1. Input: Symbolic score (MIDI/MusicXML) → preprocessing and phrase segmentation.
  2. Parallel analysis: Structural, Harmonic, and Stylistic agents process independently.
  3. Coordination: Coordinator aggregates outputs, resolves conflicts, flags inconsistencies.
  4. Output: Comprehensive analytical report (structured data + natural-language summary).
  5. Validation (optional): Expert comparison or user feedback loop.

- Design tradeoffs:
  - **Specialization vs. coordination overhead**: Modular agents improve interpretability and maintenance but require robust synchronization logic.
  - **Rule-based vs. data-driven components**: Hybrid approaches leverage symbolic reasoning and pattern matching; pure ML risks opacity.
  - **Breadth vs. depth in corpus coverage**: Broad stylistic coverage (Baroque to Classical) improves generalization but exposes transitional-work vulnerabilities.

- Failure signatures:
  - **Hallucinations in transitional works**: Stylistic misattribution or harmonic mislabeling in pieces with mixed-period features (e.g., C.P.E. Bach, Gluck).
  - **Over-segmentation in structurally ambiguous passages**: Structural agent may flag spurious boundaries in pieces with continuous development.
  - **Modulation misdetection**: Harmonic agent may miss modulations in complex tonal trajectories (e.g., Haydn quartets).
  - **Expert agreement gaps**: Cases where system outputs diverge from musicologist interpretations (noted in ~15–20% of corpus works with minor errors or hallucinations).

- First 3 experiments:
  1. **Baseline validation on canonical works**: Run the multi-agent pipeline on a subset of stylistically unambiguous works (e.g., J.S. Bach fugues, Haydn symphonies) and compare outputs to expert analyses; target >90% agreement.
  2. **Stress-test on transitional corpus**: Apply the system to works identified as failure-prone (e.g., C.P.E. Bach, Gluck, Rameau); catalog hallucination types and frequencies to diagnose agent-specific weaknesses.
  3. **Ablation of agent contributions**: Disable one agent at a time (e.g., remove Stylistic Agent) and measure impact on overall output quality and interpretability; quantify coordination dependency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can subjective human feedback be effectively integrated with quantitative indicators to create a unified, standardized evaluation metric for AI-driven music analysis?
- Basis in paper: [explicit] The abstract and conclusion list "the definition of hybrid evaluation metrics" as a key challenge, noting the lack of a "universally accepted criterion."
- Why unresolved: Current methodologies, including those in this study, rely on disparate data sources (e.g., Dynamic Time Warping vs. Likert scales) that are difficult to synthesize into a single performance score.
- What evidence would resolve it: A validated framework that statistically correlates algorithmic outputs with expert musicological assessments across diverse datasets.

### Open Question 2
- Question: How can multi-agent architectures be refined to mitigate analytical "hallucinations" when processing stylistically transitional or harmonically ambiguous musical works?
- Basis in paper: [inferred] The results of Case Study 2 show that while the system was consistent, it produced specific errors (hallucinations) in "transitional works" (e.g., C.P.E. Bach) and pieces with "atypical harmonic structures."
- Why unresolved: The paper notes that isolated inconsistencies in transitional repertoires reveal boundaries where "automated inference remains challenging" without human supervision.
- What evidence would resolve it: Successful re-analysis of the "transitional" subset of the 18th-century corpus by an updated agent system showing significantly reduced error rates.

### Open Question 3
- Question: To what extent does the use of generative AI tools in secondary education reinforce dataset biases, and can specific pedagogical interventions mitigate this over-reliance?
- Basis in paper: [inferred] The Case Study 1 discussion identifies "over-reliance on AI and potential reinforcement of dataset biases" as pedagogical challenges revealed by the iterative process.
- Why unresolved: The study measured immediate analytical articulation improvements (85%) but did not assess long-term dependency or the depth of critical engagement regarding bias.
- What evidence would resolve it: Longitudinal data tracking students' ability to critique and identify AI biases independently after using these tools over an extended period.

## Limitations
- Stylistic misattributions occur in transitional works (e.g., Gluck, C.P.E. Bach), revealing system limitations
- Empirical claims rely on qualitative and comparative assessments rather than large-scale quantitative validation
- Pedagogical outcomes depend on self-reported student data without control-group comparisons

## Confidence
- **High**: Feasibility of multi-agent architecture for modular music analysis; integration of generative AI into compositional parameter specification
- **Medium**: Consistency of agent outputs across the 18th-century corpus; improvement in student analytical articulation
- **Low**: Generalizability of findings to non-Western repertoires; scalability of the approach beyond 18th-century tonal music

## Next Checks
1. Conduct blind expert evaluation of multi-agent outputs on a disjoint test set of 18th-century works to verify reproducibility
2. Perform ablation studies on the stylistic agent to isolate the impact of rule-based vs. data-driven stylistic attribution
3. Design a controlled classroom experiment comparing student outcomes with and without AI scaffolding in music analysis tasks