---
ver: rpa2
title: 'FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection'
arxiv_id: '2511.19476'
source_url: https://arxiv.org/abs/2511.19476
tags:
- selection
- coreset
- frequency
- fast
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAST is a DNN-free coreset selection framework that formulates
  the task as a graph-constrained optimization problem using spectral graph theory
  and Characteristic Function Distance (CFD) to capture full distributional information
  in the frequency domain. It introduces an Attenuated Phase-Decoupled CFD to resolve
  the "vanishing phase gradient" issue in medium and high-frequency regions, and a
  Progressive Discrepancy-Aware Sampling strategy that schedules frequency selection
  from low to high for stable convergence.
---

# FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection

## Quick Facts
- **arXiv ID**: 2511.19476
- **Source URL**: https://arxiv.org/abs/2511.19476
- **Authors**: Jin Cui; Boran Zhao; Jiajun Xu; Jiaqi Guo; Shuo Guan; Pengju Ren
- **Reference count**: 40
- **Primary result**: 9.12% average accuracy gain over state-of-the-art coreset selection methods

## Executive Summary
FAST introduces a novel DNN-free framework for coreset selection that formulates the task as a graph-constrained optimization problem using spectral graph theory and Characteristic Function Distance (CFD) in the frequency domain. The method introduces an Attenuated Phase-Decoupled CFD to resolve vanishing phase gradient issues in high-frequency regions and a Progressive Discrepancy-Aware Sampling strategy that schedules frequency selection from low to high for stable convergence. Experiments show FAST achieves superior accuracy while reducing power consumption by 96.57% and delivering 2.2× speedup compared to state-of-the-art methods.

## Method Summary
FAST constructs a multi-scale k-NN graph using UMAP-style fuzzy topology and extracts manifold features via the Symmetric Normalized Laplacian eigenvectors. It then optimizes a continuous coreset in the frequency domain using CFD, with graph constraints (DPP diversity, Hungarian assignment, and Laplacian regularization) ensuring topological fidelity. The PD-CFD loss captures full distributional information while resolving phase gradient vanishing through an adaptive penalty that decays at high frequencies. A Progressive Discrepancy-Aware Sampling strategy selects frequencies from low to high based on moment encoding and loss gradients, ensuring stable convergence.

## Key Results
- Achieves 9.12% average accuracy gain over state-of-the-art methods across CIFAR-10/100, SVHN, TinyImageNet, DTD, and RESISC45
- Reduces power consumption by 96.57% compared to baseline approaches
- Delivers 2.2× speedup while running efficiently on CPU with only 1.7 GB memory
- Shows consistent performance across multiple architectures (ResNet18/50, ShuffleNetV2, MobileNetV2, ViT) with <1% drop when transferring coreset from ResNet18 proxy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Phase-decoupled CFD captures full distributional equivalence (all moments and correlations) better than MSE, KL, or MMD.
- **Mechanism**: The Characteristic Function φ(t) is the Fourier transform of a distribution and uniquely determines it. CFD measures L² distance between CFs. The authors identify a "vanishing phase gradient" problem where phase differences Δθ are scaled by magnitude product A_Ỹ·A_Yref, which decays at high frequencies per Riemann-Lebesgue. The Attenuated Phase-Decoupled CFD adds a phase penalty λ_φ(ω) that adaptively decays in noisy high-frequency regions: λ_φ(ω) = λ_p / (1 + α||ω||²), amplifying mid-range phase signal while suppressing noise.
- **Core assumption**: Full distributional equivalence (matching all moments) leads to coreset quality; phase information in medium frequencies encodes fine-grained structure critical for complex datasets.
- **Evidence anchors**: [abstract] "employs the Characteristic Function Distance (CFD) to capture full distributional information... introduces an Attenuated Phase-Decoupled CFD to resolve the 'vanishing phase gradient' issue"; [Section 3.3] Eq. 8-10 showing magnitude-phase coupling and decoupled loss; Fig. 7 showing CFD outperforms MSE/KL/CE by 4.73-9.16%; [corpus] Weak/no direct corpus support for CFD-specific mechanism.

### Mechanism 2
- **Claim**: Graph-constrained optimization bridges continuous distribution matching to discrete coreset selection.
- **Mechanism**: A multi-scale k-NN graph captures manifold structure (using UMAP-style fuzzy union). The Symmetric Normalized Laplacian L_sym yields eigenvectors as manifold features V_full. Continuous coreset Ỹ ∈ R^{M×d} is optimized via gradient descent but constrained by: (1) DPP diversity loss L_div penalizing feature redundancy; (2) GUNN alignment L_match pulling each y_i to discrete anchor via Hungarian assignment on graph-aware cost matrix; (3) Graph regularization L_graph preserving local topology via Tr(Ỹ^T L_sub Ỹ).
- **Core assumption**: The data manifold is well-approximated by the graph Laplacian eigenvectors; continuous optimization with graph constraints yields valid discrete samples.
- **Evidence anchors**: [abstract] "formulates the task as a graph-constrained optimization problem using spectral graph theory"; [Section 3.1-3.2] Eq. 1-5 defining graph construction and constraints; Fig. 8 ablation showing 5-16% accuracy drop when removing constraints; [corpus] Neighboring papers on coreset selection don't use spectral graph constraints.

### Mechanism 3
- **Claim**: Progressive frequency sampling from low to high (PDAS) ensures stable convergence with fewer frequencies.
- **Mechanism**: CF values at different frequencies encode different moments (Taylor expansion: φ(t) = Σ i^{|α|}/α! E[X^α]t^α). High frequencies correspond to fine details but cause unstable optimization if introduced too early. PDAS constructs an Anisotropic Frequency Library by optimizing scaling coefficients per frequency band, then progressively expands frequency norm upper bound τ_t. Within each bound, samples frequencies proportionally to L_CF(ω)·D(ω) (loss × diversity).
- **Core assumption**: Low frequencies capture global structure (low-order moments) that must be matched before fine details; curriculum-based frequency selection avoids local minima.
- **Evidence anchors**: [abstract] "Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high"; [Section 3.4] Eq. 12-14 showing moment encoding and sampling probability; Fig. 9 showing PDAS converges stably vs. non-progressive methods; Fig. 10 showing FAST saturates faster than NCFM; [corpus] No corpus validation; frequency-domain coreset methods are sparse.

## Foundational Learning

- **Spectral Graph Theory (Graph Laplacian & Eigenvectors):**
  - Why needed here: Core feature extraction; eigenvectors of L_sym approximate Laplace-Beltrami operator on the data manifold
  - Quick check question: Can you explain why the smallest non-zero eigenvalue eigenvectors capture global manifold structure?

- **Characteristic Functions & Fourier Transforms of Distributions:**
  - Why needed here: CFD is the central metric; understanding how φ(t) encodes all moments via derivatives at origin
  - Quick check question: Given φ(t) = E[exp(i⟨t,X⟩)], what statistical information does |φ(t)|² and arg(φ(t)) each encode?

- **Distribution Matching & Divergences (KL, MMD, Wasserstein):**
  - Why needed here: Baseline comparison; understanding why MSE/KL fail to capture higher-order moments
  - Quick check question: Why does KL divergence require density estimation and how does CFD avoid this?

## Architecture Onboarding

- **Component map**: Graph Construction (multi-scale k-NN) -> Spectral Embedding (Laplacian eigendecomposition) -> Anisotropic Frequency Library -> Main Loop (PDAS sampling) -> CFD Loss Computation -> Graph Constraints (DPP, Hungarian, Laplacian) -> Update Continuous Coreset -> Hungarian Assignment to Discrete Samples

- **Critical path**: Graph construction (Section 3.1) → Spectral embedding → Anisotropic frequency init (Eq. 13) → Main loop: sample frequencies via PDAS → compute PD-CFD loss → apply graph constraints → update Ỹ → map to discrete samples via Hungarian assignment

- **Design tradeoffs**:
  - Frequency count k: More frequencies = better matching but higher compute (Fig. 10 shows saturation ~64)
  - Phase penalty λ_p: Too low misses fine details; too high amplifies noise (optimal ~0.3-0.4 per Fig. 11)
  - Graph scale k: Multi-scale captures both local and global; single-scale risks missing structure
  - Keep ratio: 10-30% evaluated; lower ratios favor FAST more vs. baselines

- **Failure signatures**:
  - Mode collapse: Multiple Ỹ points collapse to same anchor → check DPP constraint weight
  - Poor convergence on texture-rich data: Phase gradient vanishing → increase λ_p or check PDAS
  - Disconnected samples: Graph not fully connected → verify MST enhancement applied
  - High variance across runs: Frequency sampling too random → increase k or check anisotropic init

- **First 3 experiments**:
  1. **Sanity check on CIFAR-10 (10% keep rate)**: Run FAST with default params; expect ~90% accuracy with ResNet18. Compare PD-CFD vs. standard CFD (ablation in Fig. 7).
  2. **Frequency count ablation**: Vary k ∈ {8, 16, 32, 64, 128} on CIFAR-10; plot convergence curves (replicate Fig. 10). Verify saturation behavior.
  3. **Cross-architecture generalization**: Train coreset with ResNet18 proxy (for baselines) vs. FAST (no proxy); test on ResNet50, MobileNetV2, ViT. Expect <1% drop for FAST vs. 3-9% for DNN-based methods (Fig. 6).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the graph construction mechanism scale to datasets with millions of samples without approximations that compromise manifold fidelity?
- **Basis in paper**: [inferred] Section 3.1 describes constructing a multi-scale weighted undirected graph $B \in \mathbb{R}^{N \times N}$. The paper only benchmarks datasets up to $\sim$120k samples (TinyImageNet), avoiding the memory bottleneck inherent in storing and processing full $N \times N$ adjacency matrices for massive datasets.
- **Why unresolved**: The quadratic memory complexity of exact graph construction prohibits application to industry-scale datasets (e.g., ImageNet-21k, LAION), a limitation not addressed by the proposed CPU-based efficiency claims.
- **What evidence would resolve it**: Benchmark results on datasets exceeding 1 million samples, potentially utilizing sparse graph approximations or landmark-based techniques, demonstrating maintained accuracy.

### Open Question 2
- **Question**: Does FAST retain semantic consistency in full-scale generative pre-training tasks beyond the instruction-tuning scope?
- **Basis in paper**: [inferred] Section 5 states that performance on the Alpaca dataset "suggests" semantic retention, but the experiment is limited to LLaMA-7B instruction tuning rather than the more complex pre-training phase.
- **Why unresolved**: The paper validates generalization on classification and tuning, but the ability to preserve the complex distributional semantics required for training a foundation model from scratch remains unproven.
- **What evidence would resolve it**: Evaluation of perplexity and downstream task performance when training a small language model (e.g., GPT-2 scale) purely on a FAST-selected coreset of a large pre-training corpus.

### Open Question 3
- **Question**: Is the Attenuated Phase-Decoupled CFD (PD-CFD) hyperparameter setting robust across data modalities with different spectral decay rates?
- **Basis in paper**: [inferred] Section 3.3 introduces the adaptive penalty $\lambda_\phi(\omega)$ and Section 4.3 (Fig. 11) shows an ablation finding an optimal $\lambda_p$ for vision data (RESISC45), noting that overemphasis introduces noise.
- **Why unresolved**: The optimal penalty decay rate ($\alpha$) appears sensitive to the signal-to-noise ratio of the phase; domains like audio or time-series have vastly different spectral properties than images.
- **What evidence would resolve it**: Cross-modal ablation studies (e.g., audio or 3D point cloud datasets) showing whether the proposed default parameters hold or require significant retuning.

## Limitations
- **Scalability concerns**: Quadratic memory complexity in graph construction limits application to massive datasets beyond the tested ~120k sample range
- **Hyperparameter sensitivity**: Multiple critical hyperparameters (manifold dimension, k-NN scales, loss weights, frequency parameters) lack comprehensive sensitivity analysis
- **Cross-modal validation gaps**: Novel CFD-based approach lacks validation on non-image distributions despite theoretical claims of distributional generality

## Confidence
- **High confidence**: Graph-constrained optimization framework and topology-aware alignment (well-grounded in spectral graph theory, supported by ablation studies)
- **Medium confidence**: PD-CFD mechanism and phase-decoupling benefits (novel approach with limited corpus validation but strong theoretical basis)
- **Low confidence**: Progressive Discrepancy-Aware Sampling and frequency scheduling benefits (minimal corpus support, claims rely primarily on in-paper experiments)

## Next Checks
1. **Ablation of spectral constraints**: Run FAST without graph constraints (L_div, L_match, L_graph) on CIFAR-10 and measure accuracy drop to quantify their contribution beyond what the paper reports
2. **Cross-distribution validation**: Test FAST on non-image distributions (e.g., tabular datasets or synthetic distributions) to verify CFD-based matching generalizes beyond the visual domain
3. **Computational complexity profiling**: Measure actual memory usage and runtime across different frequency counts (k=8,16,32,64,128) to verify claimed efficiency advantages over DNN-based methods