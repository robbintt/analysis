---
ver: rpa2
title: 'The Traitors: Deception and Trust in Multi-Agent Language Model Simulations'
arxiv_id: '2505.12923'
source_url: https://arxiv.org/abs/2505.12923
tags:
- agents
- deception
- traitors
- faithful
- strategic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces The Traitors, a multi-agent simulation environment
  for studying deception and trust dynamics in large language models (LLMs). Inspired
  by social deduction games, the framework places a minority of traitor agents with
  complete information against a majority of faithful agents operating under uncertainty.
---

# The Traitors: Deception and Trust in Multi-Agent Language Model Simulations

## Quick Facts
- arXiv ID: 2505.12923
- Source URL: https://arxiv.org/abs/2505.12923
- Reference count: 40
- One-line primary result: Multi-agent simulation framework for studying deception and trust dynamics in LLMs, revealing advanced models show superior deception but disproportionate vulnerability to detection.

## Executive Summary
This paper introduces The Traitors, a multi-agent simulation environment for studying deception and trust dynamics in large language models (LLMs). Inspired by social deduction games, the framework places a minority of traitor agents with complete information against a majority of faithful agents operating under uncertainty. The environment features stateful memory architectures enabling persistent belief updating across rounds, with agents communicating solely through natural language. Comprehensive evaluation metrics measure coordination, deception effectiveness, and trust network stability. Initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods, suggesting deception skills may scale faster than detection abilities. The Traitors provides a configurable testbed for investigating emergent deceptive strategies, trust dynamics, and broader alignment challenges in socially nuanced interactions among AI agents.

## Method Summary
The Traitors simulates a social deduction game where 3 traitor agents (with perfect information about all roles) face 7 faithful agents (operating under uncertainty) in 10-agent populations. Agents communicate through natural language dialogue across structured phases: night elimination, day discussion, and day voting. Each agent maintains persistent memory (player information, suspicions, game events, alliances, round summaries) that persists across rounds via structured prompt-based systems. The framework supports homogeneous or heterogeneous agent populations using different LLM models. Communication is controlled via temperature (T=0.7) and top_p (0.9) sampling. The simulation runs 10 independent trials per model configuration, logging dialogue transcripts, voting patterns, and memory states. Metrics include Trust Alignment Score, Faithfulness Accuracy Rate, Traitor Survival Rate, and Deception Effectiveness Score.

## Key Results
- GPT-4o traitors achieved 93% survival rate while faithful agents using GPT-4o showed only 10% accuracy in identifying traitors
- Information asymmetry created strong signal-jamming incentives where traitors strategically obfuscated revealing behaviors
- Stateful memory enabled coherent belief updating across rounds, though prompt-based memory showed limitations versus learned representations

## Why This Works (Mechanism)

### Mechanism 1: Information Asymmetry Creates Signal-Jamming Incentives
- Claim: Creating a hidden minority with complete information against an uninformed majority generates strategic deception incentives.
- Mechanism: The environment formalizes an asymmetric information game where traitors know all role assignments (perfect information) while faithful agents know only the number of traitors, not identities. This creates a "signal-jamming incentive" where traitors benefit from obfuscating signals that reveal their identity through strategic communication.
- Core assumption: Agents will pursue role-appropriate strategies when prompted with clear objectives.
- Evidence anchors:
  - [abstract] "A minority of agents (the traitors) seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning."
  - [section 2.2.2] "This information asymmetry creates what game theorists term a signal-jamming incentive for traitors, who benefit from obfuscating informative signals that might reveal their identity."
  - [corpus] WOLF paper (arXiv:2512.09187) examines deception in social deduction games, noting "the interactive, adversarial, and longitudinal nature of real deceptive dynamics."
- Break condition: If agents do not pursue assigned objectives or if information revelation is too rapid (e.g., immediate role disclosure configurations), strategic tension collapses.

### Mechanism 2: Stateful Memory Enables Persistent Belief Updating
- Claim: Persistent, structured memory across rounds allows agents to maintain coherent reasoning and adapt strategies.
- Mechanism: Each agent maintains categorized memory entries (player information, suspicions, game events, alliances, round summaries), belief tracking (hypotheses about others' roles updated each round), chronological event history, and evolving strategic considerations. This memory is passed as part of the system prompt at each interaction.
- Core assumption: The LLM can effectively utilize structured prompt-based memory; limitations exist versus learned representations.
- Evidence anchors:
  - [abstract] "...stateful memory architectures enabling persistent belief updating across rounds, with agents communicating solely through natural language."
  - [section 2.2.5] "Agent memory is implemented as a structured prompt-based system that persists throughout the game... enabling agents to condition their behavior on the full history of the game while maintaining consistent reasoning across rounds."
  - [corpus] Hidden in Plain Text (arXiv:2601.13709) studies deception in social deduction games using natural language, relevant to multi-turn interaction memory.
- Break condition: If memory exceeds the LLM's context window or structured memory is not effectively parsed, belief updating degrades; prompt-based memory is acknowledged as limited compared to learned representations.

### Mechanism 3: Asymmetric Scaling of Deception vs. Detection Capabilities
- Claim: More advanced LLMs show superior deceptive performance but disproportionate vulnerability to deception, suggesting deception skills may scale faster than detection.
- Mechanism: Advanced models generate more persuasive deceptive communication (high traitor survival), but faithful agents using the same advanced models show lower accuracy in identifying traitors (low Faithful Correctness Rate). This inverse relationship implies deception capabilities outpace detection.
- Core assumption: Observed behavioral differences reflect genuine capability differences rather than prompt artifacts or stochastic noise.
- Evidence anchors:
  - [abstract] "Advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods, suggesting deception skills may scale faster than detection abilities."
  - [section 3.2] "GPT-4o exhibited significantly higher traitor survival rates (TSR: 0.93 ± 0.13)... Conversely, GPT-4o faithful agents demonstrated markedly lower accuracy in identifying traitors (FCR: 0.10 ± 0.09)."
  - [corpus] Corpus evidence on this specific asymmetry is limited; neighbor papers focus on deception detection (WOLF) or persuasion (ElecTwit), not the scaling asymmetry observed here.
- Break condition: With only 10 runs per model, observed patterns may not hold; definitive statistical claims are precluded. The paper explicitly labels findings as "preliminary."

## Foundational Learning

- Concept: Bayesian Signaling Games and Cheap Talk
  - Why needed here: The Traitors is formalized as a Bayesian signaling game where faithful agents update beliefs via observed communications ("cheap talk"), while traitors strategically craft messages to manipulate beliefs.
  - Quick check question: Can you explain how a "pooling equilibrium" allows traitors to mimic faithful behavior and avoid detection?

- Concept: Asymmetric Information and Adverse Selection
  - Why needed here: The environment parallels economic models of adverse selection (one party holds private information), creating the incentive structure for deception.
  - Quick check question: In a "market for lemons" scenario, what happens to the informativeness of communication when lying has no direct cost?

- Concept: Social Deduction Game Dynamics
  - Why needed here: The game structure, phases (night elimination, day discussion, day voting), and win conditions are directly inspired by social deduction games like Mafia/Werewolf.
  - Quick check question: Why do minority factions (traitors) often have a coordination advantage in social deduction games?

## Architecture Onboarding

- Component map:
  - Environment Core (Game State, Role Assignment, Information Partition) -> Agent System (LLM Policy, Observation Function, Memory Architecture) -> Phase Controller (Night/Traitor Elimination, Day Discussion, Day Voting) -> Communication Manager (Public Dialogue Transcript, Private Traitor Channel) -> Logging & Metrics (TAS, FAS, FCR, TSR, FSR, DES, IDR, BRR, VSF, TNS) -> Configuration Parameters (n, m, δ, ℓ, specialized traits)

- Critical path:
  1. Initialize game (assign roles, set n=10, m=3, δ=1).
  2. For each round:
     - Night Phase: Traitors coordinate via private channel, select faithful to eliminate.
     - Day Discussion: All surviving agents generate natural language utterances (T=0.7, top_p=0.9); update public transcript.
     - Day Voting: Each agent votes; tabulate; eliminate highest-voted agent.
     - Update memory for all agents (add eliminations, update belief tracking).
  3. Check termination: |Rt|=0 (Faithful win) or |Rt| ≥ |Ft| (Traitor win).

- Design tradeoffs:
  - Prompt-based memory vs. learned representations: Current design uses structured prompts, acknowledged as limiting.
  - Homogeneous vs. heterogeneous agent populations: Framework supports mixed models; initial experiments use one model per run.
  - Scale vs. computational cost: Each simulation ~10 minutes, ~500k tokens; limits sample sizes (10 runs/model).

- Failure signatures:
  - Memory overflow: Context window exceeded; early-game context lost.
  - Voting tie loops: Game stalls if tie-breaking logic fails (paper uses random selection).
  - Role revelation timing: δ=0 (no revelation) makes detection near-impossible; default δ=1.
  - Low coordination metrics: TAS/FAS <<1.0 may indicate prompt design issues.

- First 3 experiments:
  1. Replicate baseline (10 agents, 3 traitors, homogeneous model, 10 runs). Log all metrics. Verify TAS≈1.0, compare FAS/FCR/TSR across models.
  2. Heterogeneous capability test: Assign traitors GPT-4o, faithful GPT-4o-mini (or vice versa). Observe whether advanced deception overcomes basic detection.
  3. Memory ablation: Reduce memory structure (remove belief tracking or round summaries). Compare FCR and strategic behavior vs. baseline to isolate memory architecture contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does safety alignment (e.g., RLHF) induce a reluctance to deceive in LLMs that creates exploitable vulnerabilities when playing against unaligned or adversarial agents?
- Basis in paper: [explicit] The authors explicitly ask in Section 1.1: "Do LLMs trained with alignment techniques... show reluctance to engage in deception even when strategically advantageous? Does this create exploitable vulnerabilities?"
- Why unresolved: While the authors observe that GPT-4o is highly capable of deception (93% survival as traitor), they note in Section 5 that they could not disentangle whether alignment training constrains behavior in ways that create systematic weaknesses compared to base models.
- What evidence would resolve it: Comparative simulations using aligned versus base (un-aligned) versions of the same underlying models to observe differences in willingness to deceive and susceptibility to exploitation.

### Open Question 2
- Question: Do deceptive strategies in LLMs emerge organically from incentive structures, or do they depend entirely on specific prompting instructions?
- Basis in paper: [explicit] Section 1.1 poses the question: "Do these strategies emerge organically from the incentive structure, or do they depend on specific prompting techniques?"
- Why unresolved: The current experimental design explicitly instructs traitor agents "to employ deception and avoid detection," making it impossible to distinguish between learned emergent behavior and mere compliance with the prompt.
- What evidence would resolve it: Running simulations where agents are instructed only to "win" without explicit directions to deceive, and analyzing if deceptive behavior is instrumentally utilized.

### Open Question 3
- Question: Do deception capabilities scale faster than detection abilities as language models become more sophisticated?
- Basis in paper: [explicit] The abstract and conclusion highlight a "notable asymmetry" where advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods, explicitly suggesting "deception skills may scale faster than detection abilities."
- Why unresolved: The study was limited to three models and a small sample size (10 runs per model), preventing a definitive determination of whether this is a consistent scaling law or a quirk of the specific architectures tested.
- What evidence would resolve it: Large-scale benchmarking across a wider range of model sizes and architectures to correlate parameter count with Deception Effectiveness Score (DES) versus Faithful Correctness Rate (FCR).

### Open Question 4
- Question: How do LLM trust formation and deception detection mechanisms compare to human behavioral baselines?
- Basis in paper: [inferred] The authors explicitly list "human performance baselines are currently lacking" as a key limitation in Section 5.
- Why unresolved: Without human data, it is unclear if the observed dynamics—such as the low Betrayal Recognition Rate or specific trust network volatility—are unique artifacts of machine reasoning or mirror human social deduction limitations.
- What evidence would resolve it: Running parallel game sessions with human participants using the same protocols and comparing the results against the LLM metrics (e.g., TAS, FAS, TNS).

## Limitations
- Small sample size: Only 10 runs per model limit statistical power and prevent definitive claims about scaling relationships.
- Prompt-based memory architecture: Current design uses structured prompts rather than learned representations, acknowledged as limiting long-term coherence.
- Limited model diversity: Experiments covered only three models (DeepSeek-V3, GPT-4o-mini, GPT-4o), preventing broad generalization across architectures.

## Confidence
- Asymmetric deception-detection scaling (GPT-4o superior at deception, poor at detection): Low (n=10, preliminary observation)
- Stateful memory enables coherent belief updating: Medium (structural design supports, but behavioral validation limited)
- Information asymmetry creates strategic deception incentives: High (game-theoretic foundation, basic behavioral evidence)

## Next Checks
1. Increase sample size: Run 100 trials per model to establish statistical significance of deception-detection asymmetry.
2. Heterogeneous capability test: Assign advanced model agents (GPT-4o) as traitors, basic models (GPT-4o-mini) as faithful to test if superior deception overcomes basic detection.
3. Memory ablation study: Compare FCR and strategic behavior with full memory vs. memory-disabled (no belief tracking or event history) to isolate memory architecture contribution.