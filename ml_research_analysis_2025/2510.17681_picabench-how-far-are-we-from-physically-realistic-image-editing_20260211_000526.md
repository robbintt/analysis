---
ver: rpa2
title: 'PICABench: How Far Are We from Physically Realistic Image Editing?'
arxiv_id: '2510.17681'
source_url: https://arxiv.org/abs/2510.17681
tags:
- editing
- image
- physical
- prompt
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces PICABench, a comprehensive benchmark for\
  \ evaluating physical realism in image editing. It categorizes physical consistency\
  \ into three intuitive dimensions\u2014optics, mechanics, and state transition\u2014\
  spanning eight sub-dimensions such as light propagation, reflection, deformation,\
  \ and causality."
---

# PICABench: How Far Are We from Physically Realistic Image Editing?

## Quick Facts
- arXiv ID: 2510.17681
- Source URL: https://arxiv.org/abs/2510.17681
- Reference count: 40
- Primary result: Current image editing models score below 60% on physical realism, but synthetic video-derived fine-tuning significantly improves performance.

## Executive Summary
PICABench introduces a comprehensive benchmark for evaluating physical realism in image editing across three dimensions (optics, mechanics, state transition) and eight sub-dimensions. The authors develop PICAEval, a region-grounded VQA-based evaluation protocol that decomposes physics assessment into verifiable binary questions, and construct PICA-100K, a synthetic dataset of 100k editing pairs derived from video simulations. Experiments show existing models struggle with physical consistency, with most scoring below 60% accuracy, while fine-tuning on PICA-100K significantly improves both accuracy and consistency. The work demonstrates that supervised fine-tuning with video-derived supervision is an effective approach for learning physical priors in image editing.

## Method Summary
The authors create PICABench with 900 human-annotated samples and develop PICAEval, a VQA-based evaluation protocol using region-specific binary questions answered by a VLM. They construct PICA-100K by generating synthetic videos with an I2V model (Wan2.2-14B) and extracting first/last frames as editing pairs. Models are fine-tuned using LoRA (rank 256) on FLUX.1-Kontext-dev and Qwen-Image-Edit with AdamW optimizer for 10,000 steps. The evaluation uses three prompt levels (Superficial, Intermediate, Explicit) to test physical understanding depth.

## Key Results
- Top models score below 60% on physical realism across all dimensions
- Fine-tuning on PICA-100K improves accuracy by 10-15% on average
- Synthetic video-derived data outperforms real video data for training physical priors
- Explicit prompts consistently outperform superficial prompts in physical consistency
- Models show specific weaknesses in refraction (57.02%), deformation (58.65%), and causality (59.11%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Region-grounded binary VQA significantly improves evaluation reliability for physical consistency compared to global scoring prompts.
- **Mechanism:** General VLM prompts often hallucinate plausibility based on semantics. By forcing the VLM to attend to human-annotated key regions (e.g., a mirror surface, a contact point) and answering atomic binary questions (e.g., "Is the shadow present?"), the evaluation decomposes complex physics into verifiable evidence checks, reducing the search space for the judge.
- **Core assumption:** The paper assumes the VLM (e.g., GPT-5) possesses sufficient vision capabilities to detect the physical attribute if attention is explicitly directed to the specific region.
- **Evidence anchors:**
  - [abstract] Proposes "PICAEval, a region-grounded, VQA-based evaluation protocol using human-annotated key regions."
  - [section 3.3] "PICAEval decomposes each evaluation instance into multiple region-specific verification questions... reducing the influence of irrelevant image content."
  - [corpus] [SPICE] notes that global metrics struggle with local edits, supporting the need for localized evaluation strategies.
- **Break condition:** If the physical violation is outside the annotated region or the phrasing of the binary question is ambiguous to the VLM, the mechanism fails.

### Mechanism 2
- **Claim:** Supervising image editing with synthetic video-derived pairs (PICA-100K) transfers physical priors more effectively than using real-world video extracts.
- **Mechanism:** Video generation models (I2V) must maintain temporal consistency to look realistic, implicitly simulating physics (gravity, collision, lighting changes). By extracting the first and last frames of a synthetic video depicting a transformation, the model learns the causal "start" and "end" states of a physical interaction without the noise of camera shake or irrelevant motion found in real videos.
- **Core assumption:** The paper assumes the video generation model (Wan2.2-14B) acts as a sufficiently accurate "world simulator" to provide valid ground truth for physical transitions.
- **Evidence anchors:**
  - [section 3.4] The authors use "an image-to-video model as a state-transition simulator" and show that fine-tuning on this synthetic data improves accuracy, while a real-video dataset (Mira400K) actually degrades performance (Table 3).
  - [corpus] [Generative Physical AI in Vision] discusses the trend of using generative models to simulate physical worlds, aligning with this approach.
  - [corpus] [SimGenHOI] supports using generative modeling for physical interactions, validating the generative prior hypothesis.
- **Break condition:** If the video generator hallucinates physically impossible dynamics (e.g., objects defying gravity), this supervision will reinforce incorrect physics.

### Mechanism 3
- **Claim:** Disentangling physical realism into fine-grained categories (Optics, Mechanics, State) reveals that current models function primarily on statistical correlations rather than causal reasoning.
- **Mechanism:** By separating "Light Propagation" from "Causality" or "Deformation," the benchmark exposes that while models have learned common priors (e.g., "sunny = bright"), they lack structural understanding of material properties and forces (e.g., rigid objects should not bend).
- **Core assumption:** It is assumed that these 8 sub-dimensions cover the majority of "common sense" physical failures in image editing.
- **Evidence anchors:**
  - [table 1] Shows top models scoring reasonably well on "Global State Transition" (e.g., 72.74%) but failing on "Refraction" (57.02%) or "Deformation" (58.65%), isolating specific weakness zones.
  - [corpus] [SceneWeaver] emphasizes "physically plausible" scene synthesis, supporting the need for structural correctness in generation.
- **Break condition:** If the instructions provided for a specific sub-dimension (e.g., "remove the lamp") are executed but the model simply hallucinates a lamp-less scene rather than simulating the light-off effect, it highlights a lack of causal simulation.

## Foundational Learning

- **Concept:** **VLM-as-a-Judge & Hallucination**
  - **Why needed here:** The evaluation protocol relies on GPT-5/Qwen to judge physics. You must understand that VLMs are prone to "hallucinating" satisfaction of vague prompts (e.g., "Is this good?"), necessitating the strict binary constraints used in PICAEval.
  - **Quick check question:** If a VLM judges an image "physically realistic" but the shadow is missing, what failedâ€”the vision model or the evaluation protocol?

- **Concept:** **Video Diffusion Priors**
  - **Why needed here:** The training improvement comes from PICA-100K, built using an Image-to-Video (I2V) model. You need to grasp that I2V models learn physics implicitly to maintain temporal coherence.
  - **Quick check question:** Why is extracting the *first* and *last* frame of a video more useful for "state transition" training data than using random frames?

- **Concept:** **Instruction Tuning Levels (Superficial vs. Explicit)**
  - **Why needed here:** The paper tests three prompt levels. Understanding the gap between "Remove the chair" (Superficial) and "Remove the chair and update the shadows" (Explicit) is key to diagnosing model reasoning.
  - **Quick check question:** Why might a model succeed with "Explicit" prompts but fail with "Superficial" ones regarding physical consistency?

## Architecture Onboarding

- **Component map:** FLUX.1-Krea-dev (T2I) -> Wan2.2-14B-I2V (video) -> Frame Extractor -> GPT-5 Captioner -> PICA-100K dataset -> LoRA fine-tuning -> PICABench evaluation
- **Critical path:** The quality of the **PICAEval** annotations (ROI + Qs) determines the reliability of the benchmark. For the training data, the **I2V model's** adherence to physics is the bottleneck; if the video generator is flawed, the dataset teaches bad physics.
- **Design tradeoffs:**
  - **Synthetic vs. Real Data:** The paper explicitly chose *synthetic* video data over real data (Mira400K). Real video offers perfect physics but contains camera motion and clutter that degrades editing performance. Synthetic offers controlled physics but risks "simulation bias."
  - **Prompt Specificity:** Higher specificity (Explicit prompts) improves physics scores but may reduce consistency (pixel preservation) in non-edited regions due to broader changes.
- **Failure signatures:**
  - **"Floating" artifacts:** Removing support objects but keeping the top object in mid-air (Causality failure).
  - **"Hard" edits:** Changing a material (e.g., wetting a surface) without changing the reflectance/shininess (Optics/State failure).
  - **"Semantic-only" edits:** Removing a lamp but keeping the room fully lit (Light Source failure).
- **First 3 experiments:**
  1. **Baseline Assessment:** Run the provided baseline model (Flux.1 Kontext) on the PICABench validation set using *superficial* prompts to establish the "understanding gap."
  2. **Ablation on Prompting:** Test the model's performance delta across the three prompt levels (Superficial -> Intermediate -> Explicit) to verify if the model lacks physics *knowledge* or just *activation* of that knowledge.
  3. **Overfit Check:** Fine-tune a LoRA adapter on PICA-100K and evaluate on the *unseen* dimensions of PICABench (e.g., train on Mechanics, test on Optics) to test for generalization vs. memorization of synthetic patterns.

## Open Questions the Paper Calls Out

- **Question:** Can reinforcement learning (RL) post-training better exploit physical priors in video data than the supervised fine-tuning (SFT) approach used for PICA-100K?
- **Basis in paper:** [explicit] Section 5 (Limitations) states that while SFT brings "modest gains," it "may underexploit the full potential of data," and explicitly lists "explore RL-based post-training" as a future direction.
- **Why unresolved:** The current work establishes only an SFT baseline. It remains unknown if an explicit reward signal for physical constraints (e.g., penalizing floating objects or inconsistent shadows) would yield higher physical fidelity than standard reconstruction losses.
- **What evidence would resolve it:** A comparative study where models are trained on PICA-100K using RL with a physics-aware reward function, showing a significant accuracy increase over the SFT baseline on the PICABench leaderboard.

- **Question:** How can intermediate video frames be utilized to better model complex state transitions (e.g., global weather shifts) than the current first-and-last-frame extraction method?
- **Basis in paper:** [explicit] Section 4.2 notes a specific "drop in global state transition accuracy" and consistency, and Section 5 explicitly states the plan to "explore more fine-grained strategies to extract temporal context and leverage intermediate frames."
- **Why unresolved:** The current pipeline simplifies video dynamics into a static pair, discarding the causal chain of events necessary to render believable transitions like melting or drying, leading to visual inconsistencies.
- **What evidence would resolve it:** The development of a data pipeline that incorporates optical flow or intermediate frame supervision, resulting in improved performance specifically in the "Global State Transition" sub-dimension of PICABench.

- **Question:** What architectural or training modifications are required to bridge the gap between world understanding and physical generation in unified vision-language models?
- **Basis in paper:** [inferred] Section 4.2 observes that unified architectures "consistently underperform" dedicated editors, noting that "stronger understanding alone is insufficient" to prevent physical violations.
- **Why unresolved:** The paper identifies that high-level semantic knowledge in unified models does not translate to pixel-level physical consistency, but it does not propose a mechanism to align the model's internal "world model" with its generative decoder.
- **What evidence would resolve it:** An ablation study on a unified model (e.g., Bagel or OmniGen2) demonstrating that enforcing intermediate physical constraints or feature alignment layers improves the correlation between the model's reasoning output and its generated image.

## Limitations

- The VLM's reliability as a physics judge is uncertain, as GPT-5 may hallucinate satisfaction of vague prompts
- The synthetic dataset generation relies on the video model's physics simulation, which may not generalize to real-world complexity
- The paper does not test real video-derived datasets in detail, leaving open questions about synthetic-to-real transfer

## Confidence

- **High:** The benchmark construction (PICABench) and the multi-level prompting protocol are well-defined and reproducible. The categorization into 8 sub-dimensions provides clear analytical value.
- **Medium:** The effectiveness of PICAEval as a reliable metric depends on the VLM's vision capabilities and the quality of human annotations, which are difficult to independently verify without access to the exact VLM and annotation pipeline.
- **Medium:** The improvement from PICA-100K training is demonstrated, but the source of the improvement (synthetic physics vs. scale) is not fully isolated.

## Next Checks

1. **Hallucination Test:** Run PICAEval on a set of images with known physical violations (e.g., missing shadows, floating objects) and measure false negative rate. Compare results with and without region grounding to quantify the value of the VLM attention mechanism.
2. **Synthetic vs. Real Generalization:** Generate a small, high-quality real video-derived dataset (using Mira400K or similar) and fine-tune a model on both PICA-100K and the real dataset. Compare performance on a held-out real-world subset of PICABench to measure domain transfer.
3. **Prompt Necessity Ablation:** Take a well-performing fine-tuned model and test its responses to the three prompt levels (Superficial, Intermediate, Explicit) on a fixed set of edits. Measure the delta in PICAEval accuracy to confirm whether the model truly learns physics or just prompt compliance.