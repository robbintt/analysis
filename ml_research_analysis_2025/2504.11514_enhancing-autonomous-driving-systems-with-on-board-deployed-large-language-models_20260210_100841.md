---
ver: rpa2
title: Enhancing Autonomous Driving Systems with On-Board Deployed Large Language
  Models
arxiv_id: '2504.11514'
source_url: https://arxiv.org/abs/2504.11514
tags:
- driving
- llms
- behavior
- human
- decisionxllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a hybrid architecture integrating low-level
  Model Predictive Control (MPC) with edge-deployed Large Language Models (LLMs) to
  enhance decision-making and Human Machine Interaction (HMI) in autonomous driving
  systems. The DecisionxLLM module evaluates robotic state data against natural language
  instructions to ensure adherence to desired driving behavior, while the MPCxLLM
  module adjusts MPC parameters for control adaptability.
---

# Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models

## Quick Facts
- arXiv ID: 2504.11514
- Source URL: https://arxiv.org/abs/2504.11514
- Reference count: 40
- Primary result: Hybrid LLM-MPC architecture improves autonomous driving reasoning accuracy by up to 10.45% and control adaptability by up to 52.2% on resource-constrained edge hardware.

## Executive Summary
This work presents a hybrid architecture that integrates Large Language Models with Model Predictive Control for autonomous driving systems. The framework consists of two LLM modules: DecisionxLLM evaluates robot state data against natural language instructions to ensure adherence to desired driving behavior, while MPCxLLM adjusts MPC parameters for control adaptability. The system employs Retrieval Augmented Generation, Low Rank Adaptation fine-tuning, and quantization to enable efficient on-board deployment on resource-constrained platforms like the Jetson Orin AGX. Experimental results demonstrate significant improvements in reasoning accuracy, control adaptability, and computational efficiency compared to baseline approaches.

## Method Summary
The method combines low-level Model Predictive Control with edge-deployed Large Language Models through two specialized modules. DecisionxLLM samples robot state data over 2-second windows and evaluates adherence to human instructions using RAG-augmented LLMs. MPCxLLM receives natural language instructions, retrieves relevant MPC parameter hints via RAG, and outputs parameter dictionaries parsed to ROS dynamic reconfigure. The system uses Phi-3-mini-3.8B and Qwen2.5-7B models fine-tuned with LoRA using synthetic data generated by GPT-4o. Q5_k_m quantization enables real-time inference on the Jetson Orin AGX, achieving 10.5× throughput improvement while maintaining accuracy within 1.02% of FP16 models.

## Key Results
- Decision-making accuracy improved by up to 10.45% with RAG-augmented LLM inference
- Control adaptability increased by up to 52.2% compared to baseline MPC
- Computational efficiency enhanced by up to 10.5× through Q5_k_m quantization on Jetson Orin AGX

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting LLM prompts with RAG improves decision-making accuracy on robotic state evaluation tasks.
- Mechanism: The RAG module retrieves robot-specific context (e.g., nominal speed ranges, safe distance thresholds) and injects it into the LLM prompt. This grounds the LLM's reasoning in domain-specific constraints without requiring larger models.
- Core assumption: The retrieval corpus contains relevant, accurate hints that match the scenarios encountered during inference.
- Evidence anchors:
  - [abstract]: "We propose an approach that exploits Retrieval Augmented Generation (RAG)..."
  - [Section IV-A, Table II]: RAG improves accuracy by an average of 7.35% across GPT4o/Phi3/Qwen2.5 (absolute percentage points).
  - [corpus]: Weak direct corpus evidence; neighboring papers do not specifically address RAG for robotic decision-making.
- Break condition: If RAG hints are outdated, incomplete, or incorrectly retrieved (low similarity match), performance may degrade below baseline.

### Mechanism 2
- Claim: LLM-generated parameter adjustments can modify MPC behavior while preserving constraint satisfaction guarantees.
- Mechanism: The MPCxLLM module parses natural language instructions into specific MPC cost weights (qn, qv, qα) and constraint bounds (v_min, v_max, boundary_inflation). The MPC controller solves its optimization problem with these parameters, maintaining safety through hard constraints.
- Core assumption: The LLM outputs parsable parameter mappings within valid ranges, and the MPC formulation correctly encodes safety constraints.
- Evidence anchors:
  - [Section III-C]: "The LLM interacts with the cost function and system constraints, allowing for interpretable and flexible control adaptation."
  - [Section IV-B, Table III]: LoRA + RAG configuration achieves up to 52.2% improvement in control adaptability metrics over baseline MPC.
  - [corpus]: DRO-EDL-MPC (arXiv:2507.05710) supports the broader principle of integrating learned components with MPC for safety-aware control, though not LLM-specific.
- Break condition: If the LLM hallucinates invalid parameter names or values outside specified ranges, parsing fails and the system must fall back to default parameters.

### Mechanism 3
- Claim: Post-training quantization enables real-time LLM inference on resource-constrained edge hardware with minimal accuracy loss.
- Mechanism: Q5_k_m quantization converts FP16 weights to 5-bit representations using the GGUF format, reducing memory footprint and enabling faster inference via the llama.cpp engine, which is optimized for quantized models.
- Core assumption: The quantization process preserves sufficient numerical precision for the downstream decision and control tasks.
- Evidence anchors:
  - [Section IV-D, Table IV]: Qwen2.5-7b Q5 achieves 10.5× throughput increase (22.12 vs 2.11 tokens/s) on Jetson Orin AGX compared to FP16.
  - [Section IV-A, Table II]: Quantization causes only 1.02% accuracy variance relative to FP16 models.
  - [corpus]: Weak corpus evidence; no neighboring papers specifically evaluate quantization for LLMs in autonomous driving contexts.
- Break condition: If tasks require finer numerical reasoning (e.g., precise numeric comparisons near thresholds), quantization error may accumulate and degrade decision quality.

## Foundational Learning

- Concept: Model Predictive Control (MPC)
  - Why needed here: The low-level controller uses MPC to track trajectories while enforcing constraints. Understanding cost functions, prediction horizons, and constraint formulations is essential for interpreting how LLM-generated parameter changes affect vehicle behavior.
  - Quick check question: Can you explain how changing the lateral weight qn in the MPC cost function affects the vehicle's trajectory tracking?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG provides the mechanism for injecting robot-specific domain knowledge into LLM prompts without retraining. Understanding retrieval similarity, prompt construction, and hint corpus design is critical for effective deployment.
  - Quick check question: What happens if your RAG corpus contains conflicting hints about safe following distance?

- Concept: Low Rank Adaptation (LoRA) Fine-Tuning
  - Why needed here: LoRA enables efficient adaptation of pre-trained LLMs to domain-specific tasks (decision-making, MPC parameter mapping) using synthetic data distilled from larger models, with reduced computational overhead.
  - Quick check question: Why does LoRA fine-tuning require far fewer trainable parameters than full fine-tuning, and what does this imply for overfitting risk?

## Architecture Onboarding

- Component map:
  DecisionxLLM (samples state, evaluates instruction) -> RAG retrieval -> LLM inference -> Behavior change instruction
  MPCxLLM (receives instruction) -> RAG retrieval -> LLM inference -> MPC parameter dictionary -> ROS dynamic reconfigure -> MPC controller

- Critical path: Robot state → DecisionxLLM (inference ~5.5s on Jetson) → (if misalignment) MPCxLLM (inference ~5.5s) → Parameter update → MPC control loop (decoupled, runs continuously at control frequency).

- Design tradeoffs:
  - Larger models (GPT4o) achieve higher accuracy but require cloud connectivity (latency, privacy, reliability risks).
  - Quantization enables edge deployment but may introduce subtle reasoning errors on edge cases.
  - RAG improves small model performance but adds complexity in corpus maintenance and retrieval tuning.
  - LLM inference latency (~5.5s) is decoupled from MPC frequency, making the system unsuitable for high-frequency behavioral adjustments.

- Failure signatures:
  - DecisionxLLM outputs incorrect adherence judgment → unnecessary parameter changes or missed corrections.
  - MPCxLLM outputs unparsable or out-of-range parameters → parsing failure, fall back to defaults, no behavior change.
  - RAG retrieves irrelevant hints → degraded decision accuracy, potential conflicting guidance.
  - Quantized model truncates output due to token limit (noted with DeepSeek R1-distill in Table V) → incomplete reasoning, incorrect decisions.

- First 3 experiments:
  1. Baseline validation: Run DecisionxLLM on the 200-sample state dataset with RAG disabled, then enabled. Compare accuracy against Table II to verify RAG contribution (expected ~7% absolute improvement).
  2. Parameter parsing stress test: Feed MPCxLLM with edge-case instructions (e.g., "drive at exactly -0.5 m/s while staying within 0.1m of left wall") and verify parsing correctness, constraint validity, and fallback behavior on invalid outputs.
  3. Latency measurement under load: Deploy Qwen2.5-7b Q5 on Jetson Orin AGX, run continuous inference while monitoring MPC control loop frequency. Verify that LLM inference does not block or degrade control timing (confirm decoupling claim).

## Open Questions the Paper Calls Out

- **Question**: Can multimodal LLMs (incorporating visual data) significantly enhance reasoning performance for autonomous driving decision-making compared to the text-based, state-only approach?
  - Basis in paper: [explicit] The limitations section states: "the reliance on text-based LLMs constrains the reasoning capabilities to state-based information alone. In contrast, multimodal LLMs could significantly enhance performance by incorporating visual data."
  - Why unresolved: Computational constraints of the embedded OBC necessitated text-only LLMs as an initial step; multimodal deployment was not tested.
  - What evidence would resolve it: A comparative study deploying efficient multimodal models on resource-limited hardware, measuring decision accuracy and latency against text-only baselines.

- **Question**: Would LoRA fine-tuning with Chain-of-Thought (CoT) reasoning data (e.g., from DeepSeek-R1) improve or preserve reasoning capability better than GPT-4o distilled SFT data?
  - Basis in paper: [explicit] Appendix E notes: "Qwen2.5-7b-R1distill exhibits a notable drop in performance when fine-tuned with LoRA... suggesting a potential degradation in reasoning capability... the LoRA tuning step could perhaps be improved by utilizing R1 generated data for SFT."
  - Why unresolved: The observed performance drop with GPT-4o-based LoRA tuning on a CoT-distilled model suggests a mismatch between concise SFT data and extended reasoning patterns, but R1-generated SFT data was not tested.
  - What evidence would resolve it: Experiments comparing LoRA fine-tuning with R1-generated CoT data versus GPT-4o data on R1-distilled models, measuring decision-making accuracy.

- **Question**: Can the framework generalize effectively to full-scale autonomous vehicles in complex, uncontrolled real-world environments beyond the 1:10 scaled platform and simulation settings?
  - Basis in paper: [inferred] The paper validates on a 1:10 scaled platform and simulation; the limitations mention relatively slow decision-making that may miss high-frequency nuances. Real-world, full-scale deployment remains untested.
  - Why unresolved: Physical experiments were limited to a scaled platform with controlled scenarios; full-scale vehicles face higher dynamics, more complex perception, and safety-critical constraints.
  - What evidence would resolve it: Deployment and evaluation on a full-scale autonomous vehicle in diverse real-world driving conditions, measuring success rate in handling novel edge cases and maintaining safety.

## Limitations
- The hybrid architecture introduces coupling between high-level LLM inference (5.5s latency) and low-level MPC control, which may limit responsiveness in dynamic environments despite the reported decoupling.
- RAG effectiveness depends heavily on the quality and relevance of the retrieval corpus, but the corpus construction methodology is not detailed in the paper.
- The study focuses on a 1:10 scale F1TENTH platform; scalability to full-sized vehicles and more complex urban scenarios remains unverified.
- Quantization from FP16 to Q5_k_m improves efficiency but may introduce subtle reasoning errors that were not fully explored, particularly for edge cases near constraint thresholds.

## Confidence
- **High confidence**: Computational efficiency improvements (10.5× throughput increase) due to clear benchmarking methodology and reproducible quantization approach.
- **Medium confidence**: Decision-making accuracy improvements (up to 10.45%) and control adaptability gains (up to 52.2%) given controlled experimental conditions and synthetic data generation, but real-world variability not fully addressed.
- **Medium confidence**: The hybrid architecture's ability to bridge high-level reasoning with low-level control while maintaining safety guarantees, though edge case behavior under quantization and RAG failures requires further validation.

## Next Checks
1. **Cross-platform scalability test**: Deploy the DecisionxLLM and MPCxLLM modules on a full-sized autonomous vehicle platform with more complex urban driving scenarios. Compare performance metrics against the 1:10 scale F1TENTH results to assess scalability and generalization.
2. **RAG failure mode analysis**: Systematically degrade the retrieval corpus quality (e.g., introduce outdated or conflicting hints) and measure the impact on decision-making accuracy and control adaptability. Document fallback behaviors and safety implications.
3. **Quantization edge case evaluation**: Design a benchmark suite of robot states and instructions that push numerical reasoning near MPC constraint thresholds. Run both FP16 and Q5_k_m quantized models and analyze where quantization error accumulates, measuring decision accuracy and control performance degradation.