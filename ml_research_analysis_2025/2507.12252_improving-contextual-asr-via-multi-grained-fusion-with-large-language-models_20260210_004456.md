---
ver: rpa2
title: Improving Contextual ASR via Multi-grained Fusion with Large Language Models
arxiv_id: '2507.12252'
source_url: https://arxiv.org/abs/2507.12252
tags:
- keyword
- fusion
- speech
- phrase-level
- token-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving contextual automatic
  speech recognition (ASR), particularly for accurately transcribing contextually
  relevant keywords such as proper nouns and user-specific entities. The authors propose
  a multi-grained fusion approach that leverages both token-level and phrase-level
  fusion with large language models (LLMs).
---

# Improving Contextual ASR via Multi-grained Fusion with Large Language Models

## Quick Facts
- arXiv ID: 2507.12252
- Source URL: https://arxiv.org/abs/2507.12252
- Reference count: 14
- Primary result: State-of-the-art contextual ASR via multi-grained fusion of token-level and phrase-level LLM features

## Executive Summary
This paper addresses the challenge of improving contextual automatic speech recognition (ASR), particularly for accurately transcribing contextually relevant keywords such as proper nouns and user-specific entities. The authors propose a multi-grained fusion approach that leverages both token-level and phrase-level fusion with large language models (LLMs). By integrating speech information from ASR and contextual information from LLMs at both granularities, the method balances fine-grained token precision with holistic phrase-level understanding. Experiments on Chinese and English datasets demonstrate that the approach achieves state-of-the-art performance on keyword-related metrics while preserving high accuracy on non-keyword text. Ablation studies confirm that both token-level and phrase-level components significantly contribute to the performance gains.

## Method Summary
The paper introduces a multi-grained fusion framework that combines ASR outputs with contextual information from large language models at two levels. The token-level fusion uses an uncertainty-aware gate that blends ASR and LLM token predictions based on ASR confidence, enabling correction of uncertain tokens while preserving reliable ASR outputs. The phrase-level fusion retrieves semantically similar phrases from a keyword catalog using the LLM, then fuses them with ASR outputs through a cross-attention mechanism weighted by similarity scores. This dual approach allows the system to handle both fine-grained token corrections and broader contextual understanding of phrases and entities.

## Key Results
- Achieves state-of-the-art performance on keyword-related metrics while maintaining high accuracy on non-keyword text
- Both token-level and phrase-level fusion components significantly contribute to performance gains according to ablation studies
- Demonstrates effectiveness across both Chinese and English datasets
- Shows minimal improvement when scaling LLM from 1.5B to 7B parameters, suggesting architectural limitations

## Why This Works (Mechanism)
The approach works by addressing the fundamental limitation of ASR systems in handling contextually relevant keywords through complementary fusion strategies. Token-level fusion provides fine-grained corrections by leveraging LLM predictions when ASR confidence is low, preventing degradation on reliable ASR outputs. Phrase-level fusion captures broader contextual understanding by retrieving and integrating semantically relevant phrases from a keyword catalog, enabling better handling of named entities and user-specific terms. The uncertainty-aware gating mechanism ensures that LLM contributions are applied judiciously, maintaining ASR reliability while improving contextual accuracy.

## Foundational Learning
- **Multi-grained fusion**: Combining information at different granularities (tokens vs. phrases) provides complementary benefits - token-level for precision, phrase-level for context
  - Why needed: ASR struggles with both individual word errors and broader contextual understanding of entities
  - Quick check: Ablation results showing both components are necessary for optimal performance

- **Uncertainty-aware fusion weighting**: Using ASR confidence scores to gate LLM contributions prevents degradation when ASR is reliable
  - Why needed: Blindly replacing ASR outputs with LLM predictions would harm accuracy on non-keyword text
  - Quick check: The fusion formula (Eq. 3) and its impact on keyword vs. non-keyword performance

- **Semantic similarity matching**: Retrieving relevant phrases based on semantic similarity enables effective phrase-level fusion
  - Why needed: Static keyword lists are insufficient for capturing contextual relevance
  - Quick check: Performance degradation when using exact matching instead of semantic similarity

## Architecture Onboarding

**Component map**: ASR -> Token-level fusion gate -> Phrase-level fusion module -> Final output

**Critical path**: Audio input → ASR → Token-level fusion → Phrase-level fusion → Final transcription

**Design tradeoffs**: The approach balances accuracy gains from LLM integration against computational overhead and potential latency, using uncertainty gating to minimize unnecessary LLM queries

**Failure signatures**: Performance degradation on non-keyword text when fusion weights are too high; missed keyword corrections when uncertainty thresholds are too conservative

**3 first experiments**:
1. Test token-level fusion in isolation with varying uncertainty thresholds to find optimal gating parameters
2. Evaluate phrase-level fusion with different keyword catalog sizes to assess scalability limits
3. Compare semantic similarity matching against exact keyword matching to quantify the value of semantic understanding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the observed plateau in performance when scaling the LLM from 1.5B to 7B parameters caused by the lack of acoustic input to the LLM, or does the late-fusion architecture inherently limit the utility of larger reasoning models?
- **Basis in paper:** In Section 4.4.4, the authors note that scaling from Qwen2-1.5B to Qwen2-7B yielded "minimal improvement," contrary to standard LLM scaling laws.
- **Why unresolved:** The paper tests size but does not isolate whether the bottleneck is the model capacity or the fact that the LLM processes only text history $y_{<t}$ without access to the audio $X$.
- **What evidence would resolve it:** Experiments comparing the 1.5B vs. 7B models in an early-fusion setup where the LLM receives acoustic embeddings, to see if larger models utilize audio better.

### Open Question 2
- **Question:** Can the phrase-level fusion mechanism maintain real-time latency in production scenarios with massive keyword catalogs (e.g., >10,000 items), or does the computational complexity of the attention mechanism become prohibitive?
- **Basis in paper:** Figure 2(d) shows the Real-Time Factor (RTF) increases as the keyword list grows, though it remains acceptable for the tested range ($\le$1000).
- **Why unresolved:** The linear scaling of the attention mechanism (Eq. 8) with $N$ keywords suggests a potential bottleneck for industrial-scale dictionaries (e.g., music libraries, contact lists) not evaluated in the paper.
- **What evidence would resolve it:** A study on latency and accuracy degradation using synthetic keyword lists of 5k, 10k, and 50k items, potentially testing approximate nearest-neighbor retrieval for the phrase selection.

### Open Question 3
- **Question:** How does the uncertainty-based fusion weighting (Eq. 3) behave in acoustic conditions where the ASR model is confidently incorrect, rather than simply uncertain?
- **Basis in paper:** The method relies on ASR uncertainty $u_t^a$ to gate the LLM's influence. The paper does not analyze cases where the ASR model produces high-confidence errors (hallucinations), which would block the LLM from correcting the transcription.
- **Why unresolved:** Ablation studies focus on clean vs. general test sets, but do not isolate the failure mode where $p_t^a$ is high but wrong.
- **What evidence would resolve it:** Analysis of fusion weights and error rates on adversarial audio examples or specific noise conditions known to induce high-confidence ASR errors.

## Limitations
- The phrase-level fusion component may be brittle when encountering out-of-domain or unseen entity types, as semantic similarity matching may fail beyond training distributions
- Method's reliance on specific LLM capabilities (semantic similarity APIs or frozen models) could limit reproducibility and scalability across different deployment scenarios
- Evaluation focuses primarily on keyword-related metrics without extensive analysis of robustness to speech recognition errors or performance in low-resource language settings

## Confidence
- Multi-grained fusion approach is likely effective for contextual ASR improvement, particularly for named entities and proper nouns: High
- Exact contribution of each component and their interaction dynamics require further empirical validation: Medium
- Method's generalizability to diverse ASR tasks, languages, and entity types remains uncertain: Medium

## Next Checks
1. Evaluate the phrase-level fusion component's performance when the LLM's semantic similarity API is unavailable or when using alternative semantic matching approaches to assess robustness and dependency
2. Test the approach on a broader range of entity types (e.g., technical terms, domain-specific jargon) and languages to determine generalizability beyond the current datasets
3. Conduct experiments with varying ASR error rates to quantify the method's robustness and identify performance degradation thresholds under noisy or degraded speech conditions