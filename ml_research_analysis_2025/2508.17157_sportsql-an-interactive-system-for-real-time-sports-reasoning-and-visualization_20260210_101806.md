---
ver: rpa2
title: 'SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization'
arxiv_id: '2508.17157'
source_url: https://arxiv.org/abs/2508.17157
tags:
- system
- data
- sports
- query
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPORTSQL is a modular, real-time system for querying and visualizing
  dynamic sports data, specifically English Premier League statistics. It translates
  natural language queries into executable SQL over a live, temporally indexed database
  sourced from the Fantasy Premier League API.
---

# SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization

## Quick Facts
- **arXiv ID**: 2508.17157
- **Source URL**: https://arxiv.org/abs/2508.17157
- **Reference count**: 5
- **Primary result**: Modular system translating natural language into executable SQL and visualizations over live EPL data, achieving up to 80% exact match accuracy on string queries and 0.75 macro-F1 on table queries.

## Executive Summary
SPORTSQL is a real-time system designed for querying and visualizing dynamic English Premier League statistics. It translates natural language queries into executable SQL over a live, temporally indexed database sourced from the Fantasy Premier League API. The system uses large language models for query parsing, schema linking, and visualization generation, producing both tabular and graphical outputs. SPORTSQL achieves high accuracy on simpler queries but performance degrades with complexity, especially when combining aggregation or manipulation operations.

## Method Summary
SPORTSQL implements a modular pipeline for dynamic sports question answering. It uses schema-only prompting with LLMs to generate SQL queries without exposing underlying data, balancing real-time freshness with efficiency through a hybrid storage strategy. The system employs just-in-time API access for volatile data and persistent storage for stable core data. Entity resolution handles linguistic variance through LLM-guided inference, and visualization is generated via LLM-produced Python code. The architecture was evaluated using DSQABENCH, a benchmark of 1,700+ queries with annotated SQL programs and gold answers.

## Key Results
- Achieves up to 80% exact match accuracy on string queries and 0.75 macro-F1 on table queries
- GPT-4o outperforms Gemini-2.0 across all metrics
- Accuracy declines with query complexity, dropping to ~50% for queries requiring three or more reasoning primitives
- Manipulate operations (JOINs) show weakest performance at 15.4% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Schema-only prompting allows the system to generalize to evolving data without retraining.
- **Mechanism**: The LLM is provided with database structure (tables, columns, relationships) and user questions, but denied access to underlying data rows. This forces symbolic reasoning over query logic rather than fact retrieval.
- **Core assumption**: Logical schema remains relatively static while data updates daily.
- **Evidence anchors**: Abstract mentions "symbolic reasoning capabilities of Large Language Models for query parsing"; Section 1 describes using only schema to prompt LLM generation; related work supports LLM efficacy in visualization logic generation.

### Mechanism 2
- **Claim**: Hybrid storage strategy balances real-time data freshness with system efficiency.
- **Mechanism**: Stable core data (players, teams) persists in MariaDB while volatile data (specific player history) is fetched via API on demand and buffered in memory.
- **Core assumption**: Runtime API fetching latency is acceptable for interactive experience and API rate limits are sufficient.
- **Evidence anchors**: Section 2 describes two-tiered storage design with query-agnostic and query-dependent tables; mentions automated updates and just-in-time API access.

### Mechanism 3
- **Claim**: Prompt-guided entity resolution handles linguistic variance better than exact matching.
- **Mechanism**: User queries containing ambiguous terms (e.g., "CR7") trigger LLM to use domain knowledge to infer canonical names and generate wildcard SQL queries against reference tables.
- **Core assumption**: LLM has sufficient pre-existing knowledge of EPL players to map informal aliases to formal names.
- **Evidence anchors**: Section 2 describes prompt-guided procedure where LLM uses domain knowledge to infer canonical player or team names; notes that exact string matching is unreliable for abbreviations and nicknames.

## Foundational Learning

- **Text-to-SQL Semantic Parsing**: Maps natural language questions to SQL primitives (SELECT, WHERE, JOIN). Needed to understand core engine of system. Quick check: Given "Show me the top 5 scorers," which SQL clause is responsible for ranking and which limits output?

- **Just-in-Time (JIT) Data Virtualization**: Fetches data on demand rather than storing everything. Critical for understanding trade-offs between latency and storage footprint. Quick check: If user asks for player's fixture history, does system read from hard drive or external API call?

- **LLM Context Window & Prompt Engineering**: Schema-only prompting avoids exceeding token limits or biasing model. Understanding why schema-only approach is prerequisite for scalability. Quick check: Why is providing database schema to LLM safer and more scalable than providing database content?

## Architecture Onboarding

- **Component map**: Ingestion: FPL API → Normalizer → Hybrid DB (MariaDB + In-Memory Buffer) → Resolution: NL Query → Entity Resolver (LLM) → Canonical IDs → Generation: IDs + Schema → SQL Generator (LLM) → Execution: SQL Executor → Result Set → Visualization: Result + Query → Viz Agent (LLM) → Python Code → Plot

- **Critical path**: SQL Generation → Execution loop, specifically checking if SQL requires Query-dependent tables (triggering API fetch) before executing against MariaDB backend.

- **Design tradeoffs**: Latency vs. Freshness (trades query speed for data freshness and reduced storage); Accuracy vs. Complexity (optimized for simpler, single-hop queries with performance dropping on complex compositions).

- **Failure signatures**: Hallucinated Joins (weakest point at 15.4% accuracy on Manipulate primitive); Entity Ambiguity (LLM cannot resolve nickname to unique ID); Stale Schema (FPL API adds new column without local schema update).

- **First 3 experiments**: 1) Probe hybrid boundary: Run query for "past 5 games" (JIT) vs. "total season goals" (Persistent) and monitor latency; 2) Test entity resolution edge cases: Input queries with obscure nicknames or newly transferred players; 3) Primitive stress test: Construct queries requiring 4 primitives to characterize failure modes.

## Open Questions the Paper Calls Out

- **Generalization to other domains**: How to adapt SPORTSQL architecture to generalize to other structured domains (finance, healthcare) without manual schema remapping or intensive model fine-tuning? Authors acknowledge manual constraint and plan future work on expansion.

- **Complex query performance**: What modifications to prompt engineering or symbolic reasoning pipeline are required to stabilize accuracy for queries involving four or more reasoning primitives? Performance drops sharply to roughly 50% for queries with three or more primitives.

- **Ranked query ambiguity**: How to resolve semantic ambiguity in ranked queries to prevent omission of tied results when users request "top N" entities? Current implementation relies on default SQL behavior which fails to handle ties in ranking logic robustly.

## Limitations

- **Complex query performance**: Accuracy drops significantly for queries requiring three or more reasoning primitives, with performance around 50% for complex compositions.
- **Temporal dependency**: Benchmark results depend on specific historical database snapshots not publicly available, limiting external validation.
- **Tie handling**: Ranked queries using LIMIT may omit tied results due to default lexicographic ordering, yielding incomplete answers.

## Confidence

- **High Confidence**: Modular architecture description and general approach of schema-only prompting are well-detailed and reproducible.
- **Medium Confidence**: Reported performance metrics are internally consistent with identified failure modes but external validation is limited by data availability.
- **Low Confidence**: Exact implementation details for entity resolution and specific prompt templates that achieve stated results are not fully disclosed.

## Next Checks

1. **Prompt Template Reconstruction**: Reconstruct schema-only prompt and entity resolution prompts using described components (hints, synonyms, derived fields) and test on small, self-contained dataset to verify core SQL generation logic.

2. **Temporal Benchmark Isolation**: Create controlled test set using fixed, downloadable FPL data snapshot to isolate system performance from noise of live, changing data.

3. **Multi-Primitive Stress Test**: Systematically generate and execute queries requiring 3+ reasoning primitives to empirically validate claimed performance drop and characterize specific failure modes (incorrect JOINs vs. syntax errors).