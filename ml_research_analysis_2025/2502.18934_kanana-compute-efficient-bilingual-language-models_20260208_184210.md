---
ver: rpa2
title: 'Kanana: Compute-efficient Bilingual Language Models'
arxiv_id: '2502.18934'
source_url: https://arxiv.org/abs/2502.18934
tags:
- kanana
- https
- uni00004959
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Kanana, a series of bilingual language models
  (Korean and English) designed to achieve competitive performance with significantly
  lower computational costs than state-of-the-art models of similar size. The models
  range from 2.1B to 32.5B parameters.
---

# Kanana: Compute-efficient Bilingual Language Models

## Quick Facts
- arXiv ID: 2502.18934
- Source URL: https://arxiv.org/abs/2502.18934
- Reference count: 40
- Models range from 2.1B to 32.5B parameters with SOTA Korean performance

## Executive Summary
Kanana presents a series of bilingual (Korean/English) language models designed to achieve competitive performance while significantly reducing computational costs compared to similar-sized state-of-the-art models. The models employ staged pre-training with data quality filtering, depth up-scaling, and pruning/distillation techniques to improve efficiency during pre-training. Post-training includes supervised fine-tuning and preference optimization for enhanced user interaction capabilities. The models demonstrate state-of-the-art performance on Korean benchmarks (KMMLU, HAE-RAE, KoMT-Bench) and competitive results on English tasks.

## Method Summary
Kanana employs staged pre-training with 3 trillion tokens total: 2.7 trillion in Stage 1 (moderate-quality data) and 300 billion in Stage 2 (high-quality data). Depth Up-Scaling creates larger models by stacking layers from converged smaller models, while pruning and distillation efficiently produce smaller deployable models. The bilingual approach leverages high-quality English data for knowledge acquisition, transferring to Korean through carefully filtered Korean corpora. Post-training includes SFT, offline DPO, and online DPO stages to enhance user interaction quality.

## Key Results
- SOTA performance on Korean benchmarks (KMMLU, HAE-RAE, KoMT-Bench)
- Competitive performance on English tasks (MMLU, GSM8K)
- 11.06% computational cost savings compared to training 9.8B and 32.5B models from scratch
- Public release of 2.1B, 9.8B, and 32.5B models to promote Korean language model research

## Why This Works (Mechanism)

### Mechanism 1: Compute-Efficient Scaling via Staged Pre-training and Model Surgery
The staged approach trains first on vast moderate-quality data for generalization, then refines on smaller high-quality sets. Depth Up-Scaling creates deeper models from converged smaller ones, and pruning/distillation efficiently creates smaller deployable models. Core assumption: initial representations from moderate-quality data can be efficiently refined or expanded with high-quality data or architectural changes. Break condition: if initial Stage 1 model fails to learn generalizable representations or if pruning removes critical pathways.

### Mechanism 2: Bilingual Performance via Cross-Lingual Knowledge Transfer
Strong Korean performance achieved by using abundant high-quality English data for knowledge acquisition, then adapting to Korean with filtered high-quality Korean corpus. Core assumption: reasoning and world knowledge are somewhat language-agnostic at neural representation level. Evidence: improving English data quality directly boosts Korean benchmark scores. Break condition: if semantic gap between Korean and English is too large for implicit transfer.

### Mechanism 3: Post-Training Enhancement for User Interaction
Sequential pipeline (SFT → offline DPO → online DPO) builds on previous stages to refine model behavior and tone. Core assumption: SFT provides behavioral foundation for preference optimization to be effective. Evidence: performance gains visualized across stages. Break condition: if SFT is poorly executed or reward model for online DPO is flawed.

## Foundational Learning

- **Knowledge Distillation**: Used to create smaller "Kanana Nano" models. Quick check: What is the primary loss function used in the distillation process for pruning?
- **Direct Preference Optimization (DPO)**: Core algorithm for offline and online preference optimization. Quick check: In online DPO stage, what acts as the source of new training data?
- **Depth Up-Scaling (DUS)**: Unique architectural technique for creating larger models from smaller ones. Quick check: What is the key action performed on a trained model to create a new model for Depth Up-Scaling?

## Architecture Onboarding

- **Component map**: Data curation (cascaded filtering, edu filter) → Staged pre-training (Stage 1 on 2.7T tokens, Stage 2 on 300B high-quality tokens) → Scaling pipelines (DUS for larger, Pruning & Distillation for smaller) → Post-training pipeline (SFT → Offline DPO → Online DPO)
- **Critical path**: Data curation and staged pre-training are most critical foundation; pruning and distillation is critical for efficiently producing smaller models
- **Design tradeoffs**: Staged pre-training balances breadth vs. depth; DUS vs. from-scratch training trades massive compute savings vs. potential performance ceiling; SFT vs. DPO trades speed vs. nuanced alignment
- **Failure signatures**: Pruning/distillation failure shows excessive performance drop; DUS failure shows training instability; post-training alignment collapse shows forgetting core capabilities
- **First 3 experiments**:
  1. Data Quality Ablation: Fine-tune base model with different English/Korean data combinations, measure impact on both benchmarks
  2. Distillation Data Ablation: Perform pruning/distillation run varying distillation data composition, monitor recovery speed and final performance
  3. DPO Stage Progression Check: Run sequential SFT → DPO → Online DPO pipeline, evaluate on chat and core capability benchmarks after each stage

## Open Questions the Paper Calls Out
The paper acknowledges limitations in overall performance on small-scale models, particularly in math domains, and plans to address this through data quality and mixture optimization. The authors have not yet validated if data-centric approaches are sufficient to overcome capacity constraints of smaller parameters in logic-heavy domains.

## Limitations
- Staged pre-training efficiency gains rely heavily on undocumented hyperparameter choices for data mixing ratios
- Cross-lingual transfer claims lack detailed analysis of how English and Korean representations align internally
- Pruning and distillation methodology provides limited detail on trade-offs between size reduction and capability preservation

## Confidence
- **High confidence**: Compute efficiency claims for staged pre-training and pruning/distillation approaches
- **Medium confidence**: Bilingual performance claims
- **Low confidence**: Claims about specific effectiveness of online DPO vs offline DPO

## Next Checks
1. Replicate English data quality ablation experiment on smaller scale to verify cross-lingual transfer hypothesis
2. Conduct controlled experiment comparing depth up-scaling performance against training from scratch with equivalent compute
3. Test pruning and distillation pipeline with different distillation data compositions to verify claimed importance of data quality