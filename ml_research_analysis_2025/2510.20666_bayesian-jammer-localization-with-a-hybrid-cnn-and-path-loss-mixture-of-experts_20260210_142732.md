---
ver: rpa2
title: Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts
arxiv_id: '2510.20666'
source_url: https://arxiv.org/abs/2510.20666
tags:
- jammer
- field
- localization
- position
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses GNSS jammer localization in urban environments
  where multipath and shadowing distort received signal strength. The proposed hybrid
  Bayesian mixture-of-experts framework fuses a physical path-loss model with a convolutional
  neural network to jointly estimate the jammer position and reconstruct the RSS field.
---

# Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts

## Quick Facts
- **arXiv ID:** 2510.20666
- **Source URL:** https://arxiv.org/abs/2510.20666
- **Reference count:** 0
- **Primary result:** Hybrid Bayesian MoE framework improves GNSS jammer localization in urban multipath environments while providing explicit posterior uncertainty.

## Executive Summary
This work addresses GNSS jammer localization in urban environments where multipath and shadowing distort received signal strength. The proposed hybrid Bayesian mixture-of-experts framework fuses a physical path-loss model with a convolutional neural network to jointly estimate the jammer position and reconstruct the RSS field. Bayesian inference with Laplace approximation provides explicit posterior uncertainty over both the jammer position and propagation field. Experiments on ray-traced urban data show that localization accuracy improves with more training points, and the hybrid model better captures the spatial structure of the RSS field compared to either expert alone. Uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.

## Method Summary
The method jointly estimates jammer position θ and RSS propagation field from noisy RSS measurements using a hybrid Bayesian mixture-of-experts (MoE) framework. A physical path-loss model and a CNN are fused via log-linear pooling to form a Gaussian likelihood for the RSS field. A data-dependent centroid prior over the jammer position encodes the physical intuition that stronger RSS measurements are closer to the jammer. MAP inference with Laplace approximation provides explicit posterior uncertainty over both the jammer position and the RSS field parameters. The approach is evaluated on synthetic ray-traced urban data, demonstrating improved localization accuracy and uncertainty quantification compared to individual expert models.

## Key Results
- Hybrid model improves RSS field reconstruction and jammer localization in urban multipath environments
- Posterior uncertainty concentrates near jammer and along urban canyons where propagation is most sensitive
- Localization accuracy improves with more training points, and hybrid model better captures spatial RSS structure

## Why This Works (Mechanism)

### Mechanism 1: Log-Linear Pooling for Physics-Data Fusion
Combining a physical path-loss model with a CNN through log-linear pooling yields RSS field reconstructions that better capture urban propagation structure than either expert alone. Each expert models RSS as Gaussian with its own mean and precision. Log-linear pooling weights these by mixing weight λ and precisions (β₁, β₂), producing a pooled mean μ = β⁻¹(λβ₁μCNN + (1-λ)β₂μPL) that inherits physical decay from PL and spatial detail from CNN. Core assumption: both experts produce approximately Gaussian likelihoods; precisions meaningfully reflect each expert's reliability in different regions. Evidence anchors: [abstract] "fuses a physical path-loss (PL) model and a convolutional neural network (CNN) through log-linear pooling"; [section 3.1] Equation 9 defines pooled mean; Figure 3 shows pooled mean better aligns with true field than either expert alone. Break condition: if experts are highly correlated or both fail in the same spatial regions, pooling yields marginal gains; if λ estimation via type-II ML converges to extreme values, one expert dominates.

### Mechanism 2: Laplace Approximation for Tractable Joint Uncertainty
A Gaussian approximation around the MAP estimate provides explicit posterior uncertainty over both jammer position and the propagation field. The posterior p(ψ|D) is approximated as N(ψ̂MAP, H⁻¹) where H is the Hessian of the negative log-posterior. Schur complement extraction yields marginal posterior p(θ|D) ≈ N(θ|θ̂MAP, Σθ), enabling principled uncertainty quantification. Core assumption: the posterior is approximately Gaussian near the mode; Gauss-Newton approximation (H ≈ βJ^T J + ∇²R) suffices for CNN-scale parameters. Evidence anchors: [abstract] "Bayesian inference with Laplace approximation provides posterior uncertainty over both the jammer position and RSS field"; [section 3.2] Equations 18-22 formalize Laplace approximation and marginal extraction; "Bayesian Mixture of Experts For Large Language Models" uses similar structured Laplace approximation for MoE uncertainty. Break condition: if posterior is multimodal (multiple plausible jammer locations), Laplace captures only one mode; ill-conditioned Hessian yields unreliable uncertainty estimates.

### Mechanism 3: Data-Dependent Centroid Prior for Regularization
A temperature-weighted centroid prior improves localization by encoding the physical intuition that stronger RSS measurements are closer to the jammer. Prior p(θ) = N(θ; μc, σc²I) where μc = Σᵢexp(yᵢ/τ)pᵢ / Σᵢexp(yᵢ/τ) places higher weight on locations with stronger RSS, providing a soft initial guess that becomes more informative with more data. Core assumption: RSS generally decreases with distance despite multipath; temperature τ appropriately scales RSS-to-weight mapping. Evidence anchors: [abstract] "data-dependent prior over the jammer position, inspired by weighted centroid methods"; [section 3.1] Equation 14 defines centroid prior; [section 4.3] "data-dependent prior becomes increasingly informative with larger N". Break condition: in urban canyons where multipath creates non-monotonic RSS-distance relationships, centroid prior may mislead initialization.

## Foundational Learning

- **Concept: Log-Linear (Logarithmic) Opinion Pooling**
  - **Why needed here:** Understanding how to combine probability distributions from multiple experts with principled precision-weighted averaging.
  - **Quick check question:** Given two Gaussian experts with means μ₁=5, μ₂=15, precisions β₁=4, β₂=1, and mixing λ=0.5, compute the pooled mean.

- **Concept: Laplace Approximation**
  - **Why needed here:** Enables tractable Bayesian inference for high-dimensional neural network parameters by approximating the posterior locally.
  - **Quick check question:** Why does Laplace approximation fail for multimodal posteriors, and what role does the Hessian play in uncertainty estimation?

- **Concept: Schur Complement for Marginal Covariance**
  - **Why needed here:** Extracting uncertainty for a subset of parameters (jammer position θ) from a joint covariance over all parameters (θ, ω, P₀, γ).
  - **Quick check question:** Given block-partitioned covariance with blocks H_θθ, H_θφ, H_φθ, H_φφ, write the formula for marginal covariance Σθ.

## Architecture Onboarding

- **Component map:**
  - Building-height map H_bld ∈ R^{H×W}, spatial coordinate rasters (P_x, P_y), RSS measurements D = {(p_i, y_i)}_i=1^N
  - PL Expert: Log-distance path-loss model μ_PL(p; θ, P₀, γ) = P₀ - 10γlog₁₀(‖p-θ‖₂ + ε)
  - CNN Expert: 3-layer conv network (16→32→64 channels, 3×3 kernels, batch norm, ReLU, dropout) processing X = [H_bld, P_x, P_y]
  - Log-Linear Pooling: Combines expert means via Equation 9 with mixing weight λ (type-II ML estimated)
  - Bayesian Layer: MAP optimization (Adam) + Gauss-Newton Hessian + Schur complement extraction
  - Outputs: Jammer position θ̂ with uncertainty Σθ; RSS field with predictive variance β⁻¹ + g^T H⁻¹ g

- **Critical path:**
  1. Compute centroid prior μc from RSS measurements
  2. Forward pass through both experts at measurement locations
  3. Pool predictions and compute negative log-posterior J(ψ)
  4. Optimize J(ψ) via Adam to obtain ψ̂MAP
  5. Construct Gauss-Newton Hessian approximation at ψ̂MAP
  6. Extract marginal posterior Σθ via Schur complement
  7. For prediction: linearize μ at test locations and compute predictive variance

- **Design tradeoffs:**
  - λ estimation: Type-II ML is fast but may yield suboptimal mixing; treating λ as Bayesian would increase computational cost
  - Prior variance σc: Large values allow exploration but slow convergence; small values risk biasing toward incorrect centroid
  - CNN capacity: Deeper networks capture finer urban effects but increase Hessian approximation cost and overfitting risk
  - Gauss-Newton vs exact Hessian: Efficient but may overestimate epistemic variance at high-gradient regions (noted in [section 4.3])

- **Failure signatures:**
  - Consistently high posterior std dev regardless of N: Prior too diffuse or CNN underfitting; check learning rate and network capacity
  - Predictive uncertainty concentrated away from jammer: Centroid prior misled by urban canyon effects; verify τ setting or consider spatially adaptive prior
  - RMPV >> RMSE (over-conservative): Gauss-Newton overestimation or insufficient building-height context; check spatial coverage of H_bld
  - MAP optimization stagnates: Conflicting priors and likelihood; verify P₀, γ bounds are physically plausible for your scenario

- **First 3 experiments:**
  1. **Ablation study:** Run PL-only, CNN-only, and hybrid models on identical train/test splits; report localization error (m) and field RMSE (dBW) to quantify pooling benefit.
  2. **Data scaling analysis:** Sweep N ∈ {20, 50, 100, 200, 500} with 50 Monte Carlo runs each; plot localization error and posterior std dev vs N to verify uncertainty calibration.
  3. **Uncertainty spatial analysis:** For a fixed N=200, visualize predictive std dev overlaid on true RSS field; confirm uncertainty concentrates near jammer and urban canyons as reported in Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the posterior uncertainty estimates guide active learning strategies for optimal measurement collection?
- **Basis in paper:** [explicit] The introduction states that explicit posterior uncertainty is "a key capability for future active learning strategies."
- **Why unresolved:** The current work focuses on inference given fixed datasets; no active learning or sequential measurement selection is implemented or evaluated.
- **What evidence would resolve it:** Experiments showing that uncertainty-guided query strategies reduce localization error or sample complexity compared to random or uniform sampling.

### Open Question 2
- **Question:** How does the framework perform on real-world GNSS jamming data rather than ray-traced simulations?
- **Basis in paper:** [inferred] Section 4.1 states that "controlled real-world experiments with active jammers are both impractical and legally restricted," and all results use synthetic ray-tracing data.
- **Why unresolved:** Real-world propagation includes unmodeled phenomena (e.g., dynamic traffic, weather, antenna patterns, non-isotropic jammers) absent from simulation.
- **What evidence would resolve it:** Localization and field reconstruction accuracy measured on field-collected RSS data with known jammer positions, even if limited in scope.

### Open Question 3
- **Question:** Can the mixture-of-experts framework be extended to localize multiple simultaneous jammers?
- **Basis in paper:** [inferred] Reference [11] is cited for multi-jammer tracking with PHD filters, but the current formulation and experiments address only single-jammer scenarios.
- **Why unresolved:** The path-loss expert and data-dependent centroid prior assume a single source; multi-source scenarios require combinatorial or mixture-model extensions.
- **What evidence would resolve it:** Modified model handling multiple jammer positions with corresponding validation on multi-source ray-traced or real datasets.

## Limitations

- The hybrid model's improvement over individual experts remains qualitatively supported but lacks ablation metrics (localization error, field RMSE) to quantify pooling benefit.
- The Laplace approximation assumes Gaussian posteriors near the mode; if the RSS field exhibits strong non-Gaussianity or multimodality due to multipath, uncertainty estimates may be unreliable.
- CNN architecture details (dropout rate, initialization) and training hyperparameters (learning rate, convergence criteria) are unspecified, potentially affecting reproducibility.

## Confidence

- **High confidence:** Mechanism 1 (log-linear pooling for physics-data fusion) - supported by explicit equations and Figure 3 comparison; Mechanism 2 (Laplace approximation for tractable joint uncertainty) - standard technique with clear mathematical formulation; Mechanism 3 (data-dependent centroid prior for regularization) - physically intuitive and mathematically defined.
- **Medium confidence:** Overall localization accuracy improvements with more training points - based on Figure 3 but lacking quantitative error metrics across training sizes; uncertainty concentrating near jammer and urban canyons - qualitative observation needing spatial uncertainty analysis.
- **Low confidence:** Quantitative benefit of hybrid model over individual experts - no ablation study reported; effectiveness of centroid prior in complex urban environments - limited empirical validation.

## Next Checks

1. Run PL-only, CNN-only, and hybrid models on identical train/test splits; report localization error (m) and field RMSE (dBW) to quantify pooling benefit.
2. Sweep training data sizes N ∈ {20, 50, 100, 200, 500} with 50 Monte Carlo runs each; plot localization error and posterior std dev vs N to verify uncertainty calibration.
3. For a fixed N=200, visualize predictive std dev overlaid on true RSS field; confirm uncertainty concentrates near jammer and urban canyons as reported in Figure 3.