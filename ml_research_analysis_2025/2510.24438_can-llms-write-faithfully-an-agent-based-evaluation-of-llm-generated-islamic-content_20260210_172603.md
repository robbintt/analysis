---
ver: rpa2
title: Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic
  Content
arxiv_id: '2510.24438'
source_url: https://arxiv.org/abs/2510.24438
tags:
- islamic
- source
- fanar
- agent
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a dual-agent evaluation framework for assessing
  Islamic content generated by large language models (LLMs), addressing concerns about
  theological accuracy, citation integrity, and cultural consistency. The framework
  employs a quantitative agent for structured scoring across six dimensions (e.g.,
  Islamic Accuracy, Citation) and a qualitative agent for nuanced, side-by-side comparison
  of model outputs.
---

# Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content

## Quick Facts
- arXiv ID: 2510.24438
- Source URL: https://arxiv.org/abs/2510.24438
- Reference count: 40
- GPT-4o achieved the highest mean score (3.90/5) in evaluating Islamic content faithfulness

## Executive Summary
This study introduces a dual-agent evaluation framework to assess the theological accuracy and citation integrity of large language models generating Islamic content. The framework employs a quantitative agent for structured scoring across six dimensions and a qualitative agent for nuanced side-by-side comparison. Tested on 50 prompts from authentic Islamic blogs, GPT-4o scored highest overall, while revealing significant gaps in citation handling across all models. The findings underscore the need for community-driven benchmarks and rigorous verification in faith-sensitive AI applications.

## Method Summary
The evaluation framework uses two complementary agents: a quantitative agent leveraging OpenAI's o3 model with verification tools (Qur'an API, internet search, text extraction) to score responses across six dimensions, and a qualitative agent that performs side-by-side comparisons of model outputs. The system evaluates three models (GPT-4o, Ansari AI, Fanar) on 50 prompts derived from authentic Islamic blogs, producing structured JSON outputs with verification logs and best/worst verdicts.

## Key Results
- GPT-4o achieved the highest overall score (3.90/5), excelling in structure and style
- Ansari AI scored 3.79/5, showing strength in theological accuracy
- All models struggled with citation integrity, with Fanar scoring lowest overall (3.04/5)
- Citation handling emerged as the most significant failure mode across all evaluated models

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Citation Verification
The framework improves detection of hallucinated religious citations by grounding evaluation with external retrieval tools rather than relying on parametric memory. The quantitative agent segments text, detects references, and uses tools like the Qur'an Ayah API to fetch ground-truth source text, comparing generated claims against retrieved sources to assign verification flags.

### Mechanism 2: Dual-Agent Separation of Concerns
Separating evaluation into quantitative scoring and qualitative comparison allows capturing both rigid structural compliance and nuanced stylistic differences. The quantitative agent enforces objective constraints via tool calls, while the qualitative agent performs holistic reviews of subjective dimensions like tone and depth.

### Mechanism 3: Dimensionality Expansion for High-Stakes Domains
Standard NLP metrics fail for faith-sensitive content; expanding evaluation to include "Islamic Consistency" and "Citation" specifically exposes faithfulness gaps. The framework forces scoring on citation integrity, revealing that even high-scoring models like GPT-4o drop significantly in citation handling (3.38/5).

## Foundational Learning

- **Retrieval-Augmented Generation (RAG) vs. Tool-Use**: Understanding the difference between generating with context and verifying with tools is key. Does the Quantitative Agent retrieve documents to help it write the evaluation, or does it retrieve documents to fact-check the submitted essay?

- **Hallucination in Scriptural References**: LLMs often conflate verse numbers or invent quotes. This paper specifically detects "citation hallucinations" (e.g., linking to wrong URL or misquoting text). In Appendix A.3, why was the reference to "Verse 2:282" marked as refuted even though the text discussed testimony?

- **Convergent Validity**: The paper claims validity by showing automated qualitative agent preferences align with quantitative agent scores. If the Quantitative Agent gives a low score but the Qualitative Agent ranks that response as "Best," what assumption about the evaluation framework is violated?

## Architecture Onboarding

- **Component map**: Subjects (GPT-4o, Ansari AI, Fanar) -> Generator -> Quantitative Agent (o3 + Tools) -> JSON scores + Verification Logs -> Qualitative Agent -> Best/Worst verdicts + Justifications

- **Critical path**: 1. Prompt Collection (50 blogs) 2. Generation (3 models produce 150 essays) 3. Quantitative Loop (Segmentation -> Tool Verification -> Scoring) 4. Qualitative Loop (Triplet comparison -> Verdict) 5. Human Sanity Check

- **Design tradeoffs**: Cost vs. Depth (o3 with extensive tool calls is expensive but necessary for verification), General vs. Specific (Fanar optimized for Arabic/Islam but has smaller context window limiting coherence)

- **Failure signatures**: Tool Failure ("Unable to locate this exact sentence" leading to false negatives), Citation Drift (valid URLs listed but unused in text), Context Overflow (Fanar's smaller context window may truncate complex queries)

- **First 3 experiments**: 1. Run "Verification Log" pipeline and manually inspect flags against actual Qur'an API results 2. Ablate Internet Search tool to measure impact on "Unverified" rates 3. Swap evaluation model (use Claude or Llama as judge) to check for evaluator bias

## Open Questions the Paper Calls Out

- How does inter-evaluator agreement vary when using a heterogeneous ensemble of evaluator LLMs compared to the single-family OpenAI o3 architecture used in this pilot? (Section 5, Limitation 1)

- Do Arabic-centric models like Fanar demonstrate significantly higher theological fidelity when evaluated in their primary language with native-speaking scholars compared to English-based assessments? (Section 5, Limitation 2)

- To what extent do automated "faithfulness" scores correlate with the consensus of diverse Islamic scholar panels spanning different schools of jurisprudence? (Section 5, Limitation 3)

## Limitations

- The framework depends on o3's reasoning capacity to accurately judge theological content, creating potential circularity if o3 shares common LLM hallucination patterns

- Citation verification relies on external tool reliability; broken APIs or irrelevant search results could produce false "Unverified" flags

- The six-dimensional scoring rubric may not fully capture the complexity of Islamic jurisprudence, particularly for nuanced sectarian differences

## Confidence

- **High confidence**: GPT-4o achieves highest overall scores (3.90/5) across the six dimensions
- **Medium confidence**: Dual-agent architecture successfully identifies specific failure modes (citation handling vs. stylistic quality)
- **Low confidence**: Generalization of this framework to other faith traditions without significant domain-specific adaptation

## Next Checks

1. Cross-validate agent-based scores with human expert panels across multiple faith traditions to test framework portability
2. Conduct ablation studies removing each verification tool to quantify their individual contribution to detection accuracy
3. Test the framework with open-source evaluation models (Llama, Claude) to assess sensitivity to evaluator model choice