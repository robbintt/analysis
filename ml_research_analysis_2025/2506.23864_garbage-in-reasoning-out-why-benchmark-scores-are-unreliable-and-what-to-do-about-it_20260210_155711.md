---
ver: rpa2
title: Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to
  Do About It
arxiv_id: '2506.23864'
source_url: https://arxiv.org/abs/2506.23864
tags:
- reasoning
- evaluation
- benchmark
- performance
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically audits three widely used reasoning benchmarks\u2014\
  SocialIQa, FauxPas-EAI, and ToMi\u2014and reveals pervasive flaws in both benchmark\
  \ items and evaluation methodology. Using five LLMs as diagnostic tools, the authors\
  \ identify structural, semantic, and pragmatic issues in benchmark design, along\
  \ with scoring procedures that prioritize output form over reasoning process."
---

# Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It

## Quick Facts
- arXiv ID: 2506.23864
- Source URL: https://arxiv.org/abs/2506.23864
- Reference count: 15
- This study systematically audits three widely used reasoning benchmarks and reveals pervasive flaws in both benchmark items and evaluation methodology.

## Executive Summary
This study systematically audits three widely used reasoning benchmarks—SocialIQa, FauxPas-EAI, and ToMi—and reveals pervasive flaws in both benchmark items and evaluation methodology. Using five LLMs as diagnostic tools, the authors identify structural, semantic, and pragmatic issues in benchmark design, along with scoring procedures that prioritize output form over reasoning process. Through systematic human annotation and re-evaluation on cleaned benchmark subsets, they find that model scores often improve due to erratic surface wording variations rather than improved reasoning. Further analyses show that model performance is highly sensitive to minor input variations such as context availability and phrasing, revealing that high scores may reflect alignment with format-specific cues rather than consistent inference based on the input. The authors advocate for a shift from static output-based metrics toward reasoning-as-process evaluation frameworks.

## Method Summary
The study employs a multi-stage audit methodology. First, five LLMs (GPT-3, ChatGPT, GPT-4, GPT-o1, LLaMA 3.1 8B) serve as diagnostic tools to identify inconsistencies and errors across three benchmarks: SocialIQa dev set (1,954 items), ToMi (100 stories = 600 questions), and FauxPas-EAI (44 stories = 176 questions). Human annotators systematically review model outputs to classify errors into structural, semantic, or pragmatic flaws. The authors then create cleaned subsets by removing flawed items and re-evaluate models using context-aware prompting (conversation-history mode) versus isolated evaluation. They generate five semantic rephrasings per item using GPT-4o to test format sensitivity, and implement LLM-as-judge scoring for semantic equivalence assessment.

## Key Results
- Model performance shows high sensitivity to minor input variations like context availability and phrasing
- String-matching metrics penalize semantically valid paraphrases while rewarding surface-similar but logically incorrect responses
- Context fragmentation artificially suppresses performance on tasks requiring sequential inference
- Scores on cleaned benchmark subsets often improve due to surface wording variations rather than improved reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can serve as diagnostic instruments to surface latent benchmark defects that evade human review.
- Mechan: When a model produces systematically inconsistent outputs across semantically equivalent inputs, the variance signals noise in the evaluation infrastructure rather than pure model unreliability. By cataloging failure modes and tracing them back to item-level annotations, researchers can partition error variance into data-attributable vs. model-attributable components.
- Core assumption: The model's reasoning process is stable enough that repeated errors on the same underlying concept point to an external flaw; conversely, erratic outputs on rephrasings indicate surface-level cue dependence.
- Evidence anchors:
  - [abstract] "Using five LLMs as diagnostic tools, we identify structural, semantic, and pragmatic issues in benchmark design"
  - [section 4] "when model predictions appear inconsistent, unstable, or penalized despite valid reasoning process, we investigate whether these behaviors point to underlying flaws"
  - [corpus] Weak direct corpus support; related work focuses on OOD detection and attention mechanisms, not benchmark auditing.
- Break condition: If models themselves exhibit high intrinsic output variance unrelated to input quality, the diagnostic signal becomes noisy and attribution unreliable.

### Mechanism 2
- Claim: Rigid, semantics-agnostic scoring creates an illusion of competence by rewarding lexical overlap rather than valid inference.
- Mechan: String-matching and Levenshtein-based metrics treat surface similarity as a proxy for correctness. This creates two failure modes: (1) semantically valid paraphrases are penalized; (2) superficially similar but logically incorrect responses are rewarded. The metric inflates or deflates scores independently of reasoning quality.
- Core assumption: Correct reasoning can manifest through multiple surface forms, and any single lexical target under-specifies the space of valid answers.
- Evidence anchors:
  - [abstract] "scoring procedures that prioritize output form over reasoning process"
  - [section 4.2] "Levenshtein distance...rewards lexical alignment over interpretive depth, weakening the benchmark's diagnostic value"
  - [corpus] No direct corpus neighbor addresses this specific scoring pathology; this is an underexplored area.
- Break condition: If the task domain has a genuinely constrained output vocabulary (e.g., factual entity extraction), string similarity may be a reasonable proxy and the mechanism weakens.

### Mechanism 3
- Claim: Context fragmentation in multi-turn evaluation artificially suppresses model performance on tasks requiring sequential inference.
- Mechan: When questions are presented in isolation without prior Q&A history, models cannot condition on intermediate inferences. This violates the pragmatic structure of the task, where later questions (e.g., second-order belief attribution) logically depend on earlier conclusions. Providing full conversational history restores the intended inference chain.
- Core assumption: Human-like performance on these benchmarks requires maintaining state across related questions; isolated evaluation tests a different, degraded capability.
- Evidence anchors:
  - [abstract] "model performance is highly sensitive to minor input variations such as context availability"
  - [section 4.2] "In the literature, the ToMi benchmark is typically applied by presenting each question to the model in isolation...This discrepancy limits the model's ability to condition its output on earlier events"
  - [corpus] Weak support; corpus neighbors focus on scaling and optimization, not evaluation protocol design.
- Break condition: If models can internally re-derive necessary context from the base story alone, the marginal benefit of explicit history diminishes.

## Foundational Learning

- Concept: **Theory of Mind (ToM) evaluation via question-answering**
  - Why needed here: Two of the three audited benchmarks (ToMi, FauxPas-EAI) explicitly test false-belief reasoning and social inference, requiring understanding of how these tasks are constructed.
  - Quick check question: Can you explain why a second-order false belief task is harder than a first-order one?

- Concept: **Surface-form vs. semantic equivalence in NLP evaluation**
  - Why needed here: The paper's core critique is that metrics conflate lexical matching with meaning preservation. Understanding this distinction is essential for interpreting the rephrasing experiments.
  - Quick check question: Given two paraphrases of the same answer, would a strict string-match metric treat them identically?

- Concept: **Dataset contamination and memorization in LLMs**
  - Why needed here: The paper references concerns that high scores may reflect pre-training exposure rather than genuine reasoning; this context shapes how one interprets benchmark results.
  - Quick check question: Why might a model's performance on a benchmark released before its training cutoff be suspect?

## Architecture Onboarding

- Component map:
  - Benchmark audit pipeline: Human annotation layer → flaw taxonomy (structural/semantic/pragmatic) → cleaned subset creation
  - Diagnostic evaluation loop: Model inference → error attribution (data vs. model) → protocol refinement
  - Format sensitivity tester: Rephrasing generator → multi-condition evaluation → variance analysis
  - Semantic judge module: LLM-as-evaluator → agreement scoring against human labels (Cohen's κ)

- Critical path:
  1. Establish baseline performance using standard protocols (replicate prior work)
  2. Systematically annotate errors, partitioning into data-attributable vs. model-attributable
  3. Re-evaluate on cleaned subsets with context-aware prompting
  4. Stress-test with semantic rephrasings to surface format dependence
  5. Compare semantics-aware scoring (LLM judge) against string-based metrics

- Design tradeoffs:
  - Human annotation ensures quality but doesn't scale; LLM-as-judge offers scalability (κ=0.89 in this study) but requires validation
  - Context-aware prompting improves ecological validity but increases token costs and may introduce confounds from prior model errors
  - Rephrasing generation using a single backbone model (GPT-4o) may limit linguistic diversity

- Failure signatures:
  - High accuracy on original benchmark that collapses on rephrasings → surface-cue dependence
  - Large gap between isolated and context-aware evaluation → task mis-specification
  - Near-zero agreement between human judgment and automated metrics → metric invalidity

- First 3 experiments:
  1. **Baseline replication**: Run the five models on the three benchmarks using standard prompting; compare against reported prior results to validate setup.
  2. **Error attribution study**: On a 200-item sample, have annotators classify each model error as data-caused or model-caused; quantify the proportion of variance due to benchmark quality.
  3. **Rephrasing stability test**: Generate 3 semantically equivalent variants per item for a 100-item subset; measure per-model score variance across variants to quantify format sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the pervasive structural and semantic flaws identified in social reasoning benchmarks persist in logical and mathematical reasoning domains?
- Basis in paper: [explicit] The authors state their study focuses only on social reasoning and theory-of-mind tasks, leaving "logical and mathematical reasoning benchmark unexplored."
- Why unresolved: The audit was restricted to SocialIQa, FauxPas-EAI, and ToMi; the integrity of formal reasoning benchmarks remains unverified by this specific diagnostic protocol.
- What evidence would resolve it: Applying the authors' human-LLM audit methodology to widely used mathematical (e.g., GSM8K) or logical (e.g., LogiQA) benchmarks.

### Open Question 2
- Question: How can "reasoning-as-process" evaluation frameworks be operationalized to effectively replace static output-based metrics?
- Basis in paper: [explicit] The paper explicitly advocates for a paradigm shift toward "reasoning-as-process evaluation frameworks" rather than static output selection.
- Why unresolved: While the authors identify the failure of current metrics, they stop short of validating a specific alternative framework, offering only potential directions like contradiction detection.
- What evidence would resolve it: The development and testing of a new evaluation protocol that scores intermediate reasoning steps and robustness to rephrasing, rather than final token overlap.

### Open Question 3
- Question: Is the diagnostic audit approach scalable and reliable when applied to multilingual settings and significantly larger datasets?
- Basis in paper: [explicit] The authors list extending the diagnostic approach to "larger datasets, and multilingual settings" as "important future work."
- Why unresolved: The current systematic human annotation is resource-intensive; it is unclear if the methodology is feasible for web-scale corpora or across language barriers without losing consistency.
- What evidence would resolve it: A replication of the audit on a non-English benchmark or a dataset exceeding the scale of the current samples to assess resource requirements and error consistency.

## Limitations
- The benchmark auditing pipeline relies heavily on LLM-based error classification, but the exact human annotation guidelines for distinguishing structural, semantic, and pragmatic flaws remain underspecified
- The study uses only five LLMs as diagnostic tools, and while results are consistent across models, the possibility remains that shared training corpus exposure could lead to correlated blind spots
- The paraphrasing experiments depend on a single model (GPT-4o) for rephrasing generation, which may limit linguistic diversity

## Confidence
- High confidence: That current string-matching metrics conflate lexical overlap with reasoning quality, and that context fragmentation artificially suppresses performance on sequential inference tasks
- Medium confidence: That LLM-as-diagnostic tools can reliably attribute errors to benchmark quality versus model capability, given the strong inter-annotator agreement (κ=0.89) but limited sample sizes for human validation
- Medium confidence: That model performance gains on cleaned benchmark subsets reflect genuine reasoning improvements rather than alignment with a different set of surface cues in the cleaned data

## Next Checks
1. Conduct an independent human annotation study on a 200-item sample from all three benchmarks to validate the proposed structural/semantic/pragmatic flaw taxonomy and establish inter-rater reliability statistics
2. Generate rephrasing variants using multiple backbone models (Claude, Gemini, Llama) and test whether the observed format sensitivity persists across different linguistic distributions
3. Implement a controlled experiment comparing string-matching metrics against semantic equivalence classifiers (using sentence transformers or LLM judges) on the same benchmark items to quantify the extent of lexical-overlap bias in current scoring