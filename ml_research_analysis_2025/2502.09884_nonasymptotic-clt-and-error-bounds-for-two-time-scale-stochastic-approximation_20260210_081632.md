---
ver: rpa2
title: Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation
arxiv_id: '2502.09884'
source_url: https://arxiv.org/abs/2502.09884
tags:
- bound
- error
- step
- asymptotic
- bounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first nonasymptotic central limit theorem
  for two-time-scale stochastic approximation (TSA) algorithms with Polyak-Ruppert
  averaging. The authors analyze the finite-time error rates of linear two-time-scale
  stochastic approximation algorithms driven by martingale noise, motivated by applications
  in machine learning.
---

# Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation

## Quick Facts
- arXiv ID: 2502.09884
- Source URL: https://arxiv.org/abs/2502.09884
- Reference count: 39
- This paper establishes the first nonasymptotic central limit theorem for two-time-scale stochastic approximation algorithms with Polyak-Ruppert averaging, proving optimal O(1/√n) error rates.

## Executive Summary
This paper establishes the first nonasymptotic central limit theorem for two-time-scale stochastic approximation (TSA) algorithms with Polyak-Ruppert averaging. The authors prove optimal O(1/√n) error rates for linear two-time-scale stochastic approximation algorithms driven by martingale noise, motivated by applications in machine learning. The work closes the gap between convergence rates known for single- and two-time-scale algorithms, demonstrating that PR averaging achieves optimal finite-time performance in the two-time-scale setting.

## Method Summary
The paper analyzes linear two-time-scale stochastic approximation algorithms for solving systems of the form E[W] = A_ff x* + A_fs y* and E[V] = A_sf x* + A_ss y*. The TSA updates use step sizes α_n = α_1(n+c)^{-a} and γ_n = γ_1(n+c)^{-b} with a = 0.5 + 0.1/log(n+1001) and b = 0.5 + 0.2/log(n+1001). The method involves proving a finite-time Wasserstein-1 bound for the TSA-PR iterates and their Gaussian limits through detailed analysis of covariance matrices and their convergence rates. The PR averaging computes x̄_n = (1/n)Σx_t and ȳ_n = (1/n)Σy_t to achieve the improved convergence rate.

## Key Results
- Establishes the first nonasymptotic CLT for two-time-scale algorithms with Polyak-Ruppert averaging
- Proves optimal O(1/√n) error rates for TSA-PR, improving on prior O(1/n) rates
- Demonstrates order-optimality by proving a matching lower bound
- Provides insights on optimal time-scale separation for PR averaging

## Why This Works (Mechanism)
The improved O(1/√n) rate is achieved through Polyak-Ruppert averaging, which reduces variance in the two-time-scale setting. The key insight is that the time-scale separation (b approaching 2a - 1/2) allows the slower timescale iterates to converge to a stationary distribution before averaging, while maintaining stability of the faster timescale. The martingale concentration techniques and Lyapunov analysis provide tight control over the finite-time behavior of the covariance matrices.

## Foundational Learning
- **Martingale Concentration**: Needed to bound the deviation of stochastic processes from their expected paths. Quick check: Verify Azuma-Hoeffding or Burkholder-Davis-Gundy inequalities apply to the noise sequence.
- **Lyapunov Equations**: Required to analyze stability of the coupled linear recursions. Quick check: Confirm existence of positive definite solutions P_ff and P_Δ to Lyapunov equations.
- **Wasserstein-1 Distance**: Used to quantify the closeness between TSA-PR iterates and their Gaussian limits. Quick check: Verify the coupling construction satisfies the 1-Lipschitz property.

## Architecture Onboarding
- **Component Map**: Random matrix generation -> TSA-PR implementation -> Error computation -> Rate verification
- **Critical Path**: Generate stable system matrices → Implement TSA-PR with tuned step sizes → Track PR averages → Compute errors → Verify O(1/√n) rate
- **Design Tradeoffs**: Logarithmic step size corrections vs. simplicity; optimal time-scale separation vs. robustness; theoretical guarantees vs. practical implementation
- **Failure Signatures**: Instability (exploding iterates), slower than O(1/√n) convergence, sensitivity to matrix generation
- **First Experiments**: 1) Verify stability of generated matrices by checking eigenvalues, 2) Plot √n||ȳ_n - y*|| vs. n to confirm convergence rate, 3) Compare against fixed (a,b) baselines

## Open Questions the Paper Calls Out
None

## Limitations
- The logarithmic step size corrections may be sensitive to problem parameters not fully characterized
- Analysis assumes martingale noise structure, limiting direct applicability to Markovian noise settings
- Optimal time-scale separation derived theoretically may be difficult to implement in practice

## Confidence
- **High confidence** in the O(1/√n) error rate claims and order-optimality result
- **Medium confidence** in practical recommendations for step size tuning
- **Medium confidence** in experimental validation across multiple problem instances

## Next Checks
1. Implement sensitivity analysis by varying the random matrix generation procedure to create 10-20 different stable instances, then measure consistency of the O(1/√n) rate across problems.
2. Test alternative step size schedules (e.g., fixed exponents a, b) to empirically verify theoretical guidance about optimal time-scale separation and impact of violating c_b < 2c_a.
3. Compare TSA-PR performance against the O(1/k) rate algorithm from "O(1/k) Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation" on the same problem instances to quantify practical trade-offs between rate classes.