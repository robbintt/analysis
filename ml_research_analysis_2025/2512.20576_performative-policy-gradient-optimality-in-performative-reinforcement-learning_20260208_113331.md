---
ver: rpa2
title: 'Performative Policy Gradient: Optimality in Performative Reinforcement Learning'
arxiv_id: '2512.20576'
source_url: https://arxiv.org/abs/2512.20576
tags:
- policy
- performative
- gradient
- function
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of designing optimal algorithms
  in performative reinforcement learning (RL), where the deployed policy influences
  the environment dynamics. The authors introduce Performative Policy Gradient (PePG),
  the first policy gradient algorithm specifically designed to account for performativity
  in RL.
---

# Performative Policy Gradient: Optimality in Performative Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.20576
- Source URL: https://arxiv.org/abs/2512.20576
- Reference count: 40
- Primary result: First policy gradient algorithm achieving performatively optimal policies in RL settings

## Executive Summary
This paper introduces Performative Policy Gradient (PePG), a novel policy gradient algorithm designed for performative reinforcement learning where the deployed policy influences the environment dynamics. The key innovation is extending the classical policy gradient theorem with additional gradient terms that capture how policy changes affect environment transitions and rewards. Theoretical analysis proves that PePG converges to performatively optimal policies under softmax parametrization and entropy regularization, establishing a gradient domination lemma that enables convergence despite non-concave objectives. Empirically, PePG outperforms state-of-the-art stability-seeking algorithms on performative RL benchmarks, demonstrating that seeking optimality rather than mere stability yields superior performance in performative settings.

## Method Summary
PePG extends standard policy gradient by incorporating performative terms that account for how policy changes affect environment dynamics. The algorithm computes a modified gradient that includes standard advantage-weighted policy gradients plus additional terms for the gradients of rewards and transition probabilities with respect to policy parameters. Under softmax parametrization and entropy regularization, the performative value function is shown to be smooth, enabling convergence via gradient ascent. The method requires differentiable models of how the policy influences environment transitions and rewards, making it model-based rather than purely model-free.

## Key Results
- PePG converges to performatively optimal policies under softmax parametrization and entropy regularization
- Algorithm outperforms state-of-the-art stability-seeking methods on standard performative RL benchmarks
- Theoretical analysis establishes gradient domination lemma enabling convergence despite non-concave objectives
- Demonstrates fundamental trade-off between stability and optimality in performative RL settings

## Why This Works (Mechanism)

### Mechanism 1: Augmented Gradient Captures Policy-Environment Feedback
- Claim: PePG converges to performatively optimal policies because its gradient estimator includes terms for how policy changes affect environment dynamics.
- Mechanism: The algorithm extends the classical policy gradient ∇θVπ ≈ E[A(s,a)∇θ log π(a|s)] by adding two performative terms: (1) expected gradient of reward E[γ^t ∇θ r^π(s,a)] and (2) expected gradient of log-transitions E[γ^t A(s,a)∇θ log P^π(s'|s,a)], enabling the optimizer to account for environment shifts caused by policy deployment.
- Core assumption: Rewards and transitions are differentiable with respect to policy parameters (Assumption 2).
- Evidence anchors:
  - [abstract]: "PePG extends the classic policy gradient theorem by incorporating additional gradient terms that capture the impact of policy updates on the environment"
  - [Theorem 2, page 5]: Derivation showing ∇θV^π_π(τ) = E[Σ γ^t A(s,a)(∇θ log π + ∇θ log P^π) + γ^t ∇θ r^π]
  - [corpus]: Related work on performative prediction (Perdomo et al.) uses similar gradient-based approaches for supervised learning but without RL-specific dynamics
- Break condition: If environment response is non-differentiable or discontinuous in policy, gradient estimation fails.

### Mechanism 2: Gradient Domination Bypasses Non-Concavity
- Claim: Convergence is achieved despite non-concave objective via performative gradient domination lemma.
- Mechanism: Lemma 3 establishes SubOpt(πθ) ≤ √(|S||A|Cov) ||∇θV^π_π|| + O(Cov/(1-γ)²), meaning suboptimality is bounded by gradient norm plus irreducible bias, allowing standard gradient ascent to make progress even on non-concave landscapes.
- Core assumption: Lipschitz rewards/transitions (Assumption 2a) and bounded coverage ratio Cov between optimal and current occupancy measures.
- Evidence anchors:
  - [Lemma 3, page 6]: Explicit gradient domination inequality
  - [Section 4, page 6-7]: Three-step analysis connecting gradient domination to convergence
  - [corpus]: Agarwal et al. (2021) establish similar gradient domination for classical policy gradient; this paper extends to performative setting
- Break condition: If coverage ratio Cov grows unbounded (optimal policy visits states never reached by current policy), the bias term dominates and convergence degrades.

### Mechanism 3: Smoothness Enables Iterative Convergence
- Claim: Smooth performative value function allows gradient ascent with appropriate learning rate to converge.
- Mechanism: Under softmax parameterization, the performative value function is O(|A|/(1-γ)²)-smooth (Lemma 5), meaning second derivatives are bounded, which guarantees that gradient ascent with learning rate η = O((1-γ)²/|A|) achieves monotonic improvement.
- Core assumption: Smooth transitions/rewards with respect to policy (Assumption 2b) and exponential family transition structure.
- Evidence anchors:
  - [Lemma 5, Appendix E]: Performative smoothness proof showing max d²V/dθ² ≤ L
  - [Theorem 3, page 7]: Convergence rate T = Ω(|S||A|²/(ε²(1-γ)³))
  - [corpus]: Weak corpus signal on smoothness in performative settings; primarily from classical RL literature
- Break condition: If environment shifts are abrupt or non-smooth in policy, the smoothness bound fails and learning rate selection becomes unstable.

## Foundational Learning

- Concept: **Classical Policy Gradient Theorem**
  - Why needed here: PePG builds directly on Sutton et al.'s result; without understanding that ∇θV ≈ E[A∇θ log π], the performative extension is incomprehensible.
  - Quick check question: Can you derive why the advantage function A(s,a) appears in the policy gradient?

- Concept: **Polyak-Łojasiewicz / Gradient Domination**
  - Why needed here: The paper's convergence proof relies on gradient domination as a relaxation of convexity; understanding this explains why non-concave optimization still works.
  - Quick check question: How does gradient domination differ from strong convexity in guaranteeing convergence?

- Concept: **Performative Prediction Fundamentals**
  - Why needed here: The paper assumes familiarity with performative stability vs. optimality as solution concepts; without this, the motivation is unclear.
  - Quick check question: In Perdomo et al.'s loan example, why might a performatively stable policy yield lower utility than a performatively optimal one?

## Architecture Onboarding

- Component map:
  ```
  Policy Network π_θ → Trajectories τ → Standard PG terms [A∇log π]
                        ↓
              Environment Model P^π, r^π → Performative terms [∇log P^π, ∇r^π]
                        ↓
              Combined Gradient → Ascent Update θ_{t+1} = θ_t + η·∇θV^π_π
  ```

- Critical path: (1) Implement standard policy gradient with advantage estimation, (2) Add differentiable environment model P^π(s'|s,a) and r^π(s,a) parameterized by θ, (3) Compute additional gradient terms ∇θ log P^π and ∇θ r^π from trajectories, (4) Combine via Equation 8.

- Design tradeoffs:
  - **Entropy regularization (λ)**: Higher λ improves exploration and smoothness (Lemma 6) but reduces final performance on deterministic environments. Paper uses λ = (1-γ)R_max/(1+2log|A|) theoretically; empirically λ ≈ 2.0 works well (Appendix K).
  - **Coverage coefficient (Cov)**: Implicitly trades off sample complexity vs. robustness. Large Cov (poor exploration) increases both bias and required iterations.
  - **Model-based vs. model-free**: PePG requires differentiable models of how policy affects environment; cannot be purely model-free.

- Failure signatures:
  - Gradient estimates diverge → Check Lipschitz constants L_r, L_P; environment response may be too sensitive
  - Convergence to poor local optimum → Likely reached performative stability but not optimality; entropy regularization may help
  - Coverage ratio warning → Current policy not reaching states optimal policy visits; increase exploration

- First 3 experiments:
  1. **Sanity check on toy example (Appendix B)**: Implement the loan approval environment with known analytical solution; verify PePG recovers θ_Perf not θ_ERM. This isolates the performative mechanism.
  2. **Ablation on performative terms**: Run PePG with only standard PG term (no ∇log P^π, ∇r^π) vs. full PePG on Gridworld. Expect 20-30% performance gap per Figure 2.
  3. **Learning rate stability test**: Sweep η ∈ {0.01, 0.1, 0.5} with and without entropy regularization to validate Theorem 3's η = O((1-γ)²/|A|) guidance and identify practical operating range.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PePG be effectively scaled to continuous state-action spaces using function approximation and variance-reduction techniques?
- Basis in paper: [explicit] Section 6 states: "An interesting future direction is to scale PePG for continuous state-action space while incorporating variance-reduction techniques."
- Why unresolved: The theoretical convergence analysis (Theorem 3, 4, 5) and experiments are restricted to tabular MDPs with discrete state-action spaces.
- What evidence would resolve it: Convergence guarantees for PePG under neural network parameterizations or empirical validation on standard continuous control benchmarks (e.g., MuJoCo).

### Open Question 2
- Question: How can the performative gradient components ($\nabla_\theta \log P^{\pi_\theta}$ and $\nabla_\theta r^{\pi_\theta}$) be estimated in a model-free setting?
- Basis in paper: [inferred] Theorem 2 (Equation 6) requires the gradient of the transition log-probability and reward with respect to policy parameters.
- Why unresolved: The paper assumes these gradients are computable (e.g., in exponential family PeMDPs), but does not provide a method for estimating them when the environment dynamics are unknown or "black-box."
- What evidence would resolve it: A derivative-free estimation method for these specific gradient terms or a proof that zero-order optimization achieves similar rates.

### Open Question 3
- Question: How can we construct performative RL benchmarks that move beyond the discrete Gridworld environment?
- Basis in paper: [explicit] Section 6 notes: "constructing a performative test-bed or simulator for both discrete and continuous state-action spaces, is an important future work."
- Why unresolved: Current evaluation is limited to a single discrete Gridworld setup (Section 5), making it difficult to assess generalizability to complex, real-world-like performative shifts.
- What evidence would resolve it: The release of a standard simulator incorporating performative feedback loops (e.g., in traffic or finance) with continuous states.

## Limitations
- Theoretical analysis assumes smooth, differentiable environment responses to policy changes, which may not hold in real-world applications
- Coverage ratio Cov between optimal and current policies is critical but difficult to estimate or bound in practice
- Empirical evaluation is limited to a single Gridworld environment, raising questions about performance on complex, high-dimensional settings

## Confidence

- **High confidence**: Convergence of PePG to performatively optimal policies under stated assumptions (Theorem 2, Theorem 3); correctness of gradient derivation and smoothness analysis
- **Medium confidence**: Practical effectiveness of the gradient domination approach for non-concave performative objectives; applicability to environments beyond the Gridworld setting
- **Low confidence**: Scalability of PePG to large state-action spaces; estimation quality of the additional gradient terms (∇θ log P^π, ∇θ r^π) from finite samples

## Next Checks

1. **Transfer to continuous control**: Implement PePG on a continuous control environment with performative dynamics (e.g., traffic routing where routing policy affects congestion patterns) to test scalability beyond discrete Gridworld
2. **Gradient estimation sensitivity**: Conduct controlled experiments varying the number of trajectories I and policy entropy to quantify the variance of the novel gradient terms and identify practical sample complexity requirements
3. **Real-world applicability test**: Design a synthetic but realistic performative environment where policy deployment affects user behavior (e.g., recommendation systems affecting user preferences) to validate whether the smoothness assumptions hold in practice