---
ver: rpa2
title: Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating
  Score Function Singularity
arxiv_id: '2505.09922'
source_url: https://arxiv.org/abs/2505.09922
tags:
- score
- manifold
- function
- diffusion
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the multiscale singularity issue of score
  functions when applying Euclidean diffusion models to manifold-structured data.
  By decomposing the score function into tangential and normal components, the authors
  identify significant scale discrepancies that hinder accurate sampling.
---

# Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity

## Quick Facts
- arXiv ID: 2505.09922
- Source URL: https://arxiv.org/abs/2505.09922
- Authors: Zichen Liu; Wei Zhang; Tiejun Li
- Reference count: 40
- Primary result: Two methods (Niso-DM and Tango-DM) mitigate multiscale singularity in diffusion models on manifolds, achieving superior generation accuracy and robustness across complex manifold types.

## Executive Summary
This paper addresses a fundamental failure mode in Euclidean diffusion models when applied to manifold-structured data: the multiscale singularity of the score function. The authors identify that as noise scale approaches zero, the normal component of the score explodes while the tangential component remains bounded, causing the model to learn the manifold geometry but fail to capture the target distribution density. They propose two complementary solutions: Niso-DM, which uses non-isotropic noise to balance the scale discrepancy, and Tango-DM, which trains only the tangential component to explicitly avoid the singularity. Both methods are theoretically justified and experimentally validated on hyperplanes, mesh data, high-dimensional special orthogonal groups, and molecular configurations.

## Method Summary
The paper decomposes the score function into tangential and normal components, revealing that standard diffusion models suffer from "manifold overfitting" where the normal component dominates the loss. Niso-DM introduces non-isotropic noise with covariance $\Sigma_\sigma(x) = \sigma^2 I + \sigma^{2\alpha} N(x)N(x)^T$ to reduce the scale discrepancy from $O(1/\sigma^2)$ to $O(1/\sigma^{2\alpha})$. Tango-DM trains only the tangential component by projecting both the network output and target score onto the tangent space using $P(x) = I - N(x)N(x)^T$. Both methods use VESDE formulation with standard MLPs, Adam optimizer, and EMA decay 0.999. Niso-DM uses Reverse SDE sampling with final projection, while Tango-DM requires Annealing SDE with Langevin steps on the manifold.

## Key Results
- Niso-DM and Tango-DM achieve superior MMD and Wasserstein distances compared to baseline Iso-DM across all tested manifolds
- On SO(10) manifold, Niso-DM with $\alpha=0.5$ reduces Sliced 1-Wasserstein distance by 40% compared to baseline
- Tango-DM on Alanine Dipeptide achieves 2-Wasserstein distance of 0.18 versus 0.32 for Iso-DM
- Both methods show improved robustness as $\sigma_{min}$ decreases, with Niso-DM particularly stable
- Theoretical analysis proves Niso-DM reduces singularity order and Tango-DM completely avoids it

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard diffusion models suffer manifold overfitting because the normal score component explodes to $O(1/\sigma)$ while tangential remains $O(1)$, dominating the loss landscape
- **Mechanism:** The paper decomposes the score function into tangential and normal components. As noise scale $\sigma \to 0$, the normal component becomes disproportionately large, forcing the network to focus entirely on pulling data onto the manifold at the expense of learning the distribution density.
- **Core assumption:** The manifold $M$ is defined by $\xi(x)=0$ with known projection operator $P(x)$
- **Evidence anchors:** [section 3] Equations (14) and (15) showing the scale discrepancy estimates; [abstract] "reveal the multiscale singularity... decomposing it along the tangential and normal directions"
- **Break condition:** If manifold is flat or training stops at $\sigma > 0$ large enough to prevent singularity from dominating

### Mechanism 2
- **Claim:** Non-isotropic noise reduces the scale discrepancy between normal and tangential components, stabilizing distribution learning
- **Mechanism:** Niso-DM perturbs data using covariance $\Sigma_\sigma(x) = \sigma^2 I + \sigma^{2\alpha} N(x)N(x)^T$, "inflating" the manifold in the normal direction. This reduces the normal score magnitude from $O(1/\sigma^2)$ to $O(1/\sigma^{2\alpha})$, balancing the loss landscape.
- **Core assumption:** The normal basis $N(x)$ is known or computable
- **Evidence anchors:** [abstract] "Niso-DM... reduces the scale discrepancies... by utilizing a non-isotropic noise"; [section 4.1] Equation (17) showing reduced singularity order
- **Break condition:** If manifold geometry $N(x)$ is unknown or estimated incorrectly

### Mechanism 3
- **Claim:** Training only the tangential component explicitly avoids the singularity, allowing standard diffusion training on the manifold
- **Mechanism:** Tango-DM projects both the score network output and target onto the tangent space before computing the loss. This explicitly removes the exploding normal component from the optimization when $\sigma_t < c_{tango}$.
- **Core assumption:** Sampling includes projection step or constrained Langevin dynamics to keep samples on manifold
- **Evidence anchors:** [abstract] "Tango-DM... trains only the tangential component... to avoid singularity issues"; [section 4.2] Derivation of $\ell_{tango}$ ensuring tangential score is optimal
- **Break condition:** If using standard Reverse SDE without final projection step

## Foundational Learning

- **Concept: Variance Exploding SDE (VESDE)**
  - **Why needed here:** VESDE adds pure noise without drift, simplifying theoretical analysis of how noise interacts with manifold structure compared to VPSDE
  - **Quick check question:** Does the forward process shift the data mean, or only expand the variance?

- **Concept: Projection Matrix $P(x)$ and Tangent Space**
  - **Why needed here:** Both Niso-DM and Tango-DM require decomposing vectors into tangential and normal components relative to the manifold
  - **Quick check question:** Given a point $x$ on a manifold defined by constraint $g(x)=0$, how do you compute the basis for the normal space?

- **Concept: Manifold Overfitting**
  - **Why needed here:** This is the specific failure mode the paper solves - when a model learns the support of data (manifold geometry) perfectly but produces incorrect distribution over that support
  - **Quick check question:** If a model generates points exactly on a sphere but clusters them all at the poles instead of spreading them according to data, is it suffering from distribution mismatch or manifold overfitting?

## Architecture Onboarding

- **Component map:** Data $x$ -> Forward Process (Niso: $\Sigma(x)$ noise; Tango: standard noise) -> Network $s_\theta(x_t, t)$ -> Loss (Standard or Projected) -> Backpropagation

- **Critical path:**
  1. Calculate $P(x_t)$ (requires knowing manifold constraint $\xi(x)=0$)
  2. If Niso: Compute specific covariance $\Sigma^{-1}$ for denoising target
  3. If Tango: Check if $\sigma_t$ is small enough to switch to projected loss
  4. Backpropagate only the tangential error (Tango) or balanced error (Niso)

- **Design tradeoffs:**
  - Niso-DM: Faster sampling (compatible with standard Reverse SDE); requires tuning $\alpha$ and $c_{niso}$
  - Tango-DM: Higher theoretical precision on distribution; slower sampling (requires Annealing SDE with Langevin steps on manifold); requires explicit manifold constraint during sampling

- **Failure signatures:**
  - High MMD/Wasserstein distance: Model learned manifold geometry but failed to match density
  - Off-manifold samples in Tango-DM: Final projection step omitted or annealing steps insufficient
  - High variance in low-noise regimes: Multiscale singularity destabilizing gradients

- **First 3 experiments:**
  1. Hyperplane Sanity Check: Implement on $M=\{z=0\}$ in $\mathbb{R}^3$; verify Niso/Tango reduces tangential score error vs Iso-DM (Figure 1)
  2. SO(10) High-Dim Validation: Test on 45-dim manifold in 100-dim ambient space to ensure mechanism scales to high dimensions
  3. Molecular Configurations: Apply to Alanine Dipeptide (manifold of configurations); use Annealing SDE sampler to verify handles complex, non-linear constraints

## Open Questions the Paper Calls Out

- **Question:** Can a Reverse SDE-style sampling algorithm be designed for Tango-DM to improve computational efficiency and remove dependency on slower Annealing SDE?
  - **Basis in paper:** [explicit] Section 8 states: "Tango-DM is not suitable for the Reverse SDE Sampling algorithm... Designing Reverse SDE-style algorithms for Tango-DM remains an open direction for future research."
  - **Why unresolved:** Tango-DM trains only tangential component, lacking normal component required to project samples onto manifold during standard reverse-time diffusion process
  - **Evidence:** A derived sampling scheme that integrates separate projection mechanism or auxiliary term into Reverse SDE to compensate for untrained normal score component

- **Question:** Can the proposed singularity mitigation strategies be extended to data distributions where manifold is unknown or must be learned, such as natural image datasets?
  - **Basis in paper:** [explicit] Section 8 notes: "Future work will focus on extending our approach to scenarios where the manifold is undefined or implicitly represented."
  - **Why unresolved:** Current methods rely on explicit definition of manifold ($\xi(x)=0$) and projection operator ($P(x)$), which are not available for general data like images
  - **Evidence:** A framework that combines Niso-DM or Tango-DM with manifold learning technique (e.g., AutoEncoder) that successfully generates samples without predefined geometric constraints

- **Question:** Can non-isotropic noise injection strategy be effectively formulated for Variance Preserving SDEs (VPSDE) despite theoretical challenges posed by evolving manifold structure?
  - **Basis in paper:** [inferred] Section 2.1 explicitly restricts study to VESDE because VPSDE involves "evolving manifold over time" that complicates theoretical analysis of perturbation kernel
  - **Why unresolved:** Mean-shift characteristic of VPSDE causes manifold to change at each time step, making it difficult to define consistent projection operator and non-isotropic noise schedule relative to fixed manifold geometry
  - **Evidence:** Theoretical analysis deriving score asymptotics for VPSDE on manifolds and empirical results demonstrating Niso-DM improves sample quality under VPSDE dynamics

## Limitations

- Theoretical analysis assumes smooth, well-behaved manifolds with known projection operators, but real-world data manifolds may be non-smooth or only approximately defined
- Paper relies on VESDE specifically, leaving unclear whether multiscale singularity is equally severe under VPSDE
- Hyperparameter sensitivity (particularly α in Niso-DM and c_tango threshold in Tango-DM) is not thoroughly explored across diverse manifolds
- Experiments demonstrate improved MMD and Wasserstein metrics but don't investigate whether generated samples capture fine-grained local structure or only global distribution properties

## Confidence

- **High Confidence:** The existence of multiscale singularity in Euclidean diffusion models on manifolds (Theorem 3.1, empirical evidence across all experiments)
- **Medium Confidence:** The theoretical proof that Niso-DM reduces singularity order (Theorem 4.1) and Tango-DM avoids it entirely (Theorem 4.2)
- **Medium Confidence:** The empirical superiority of both methods across the four manifold types tested
- **Low Confidence:** The claim that these methods solve "manifold overfitting" in general - experiments show improved distribution matching but don't prove complete resolution of all manifold overfitting pathologies

## Next Checks

1. **VPSDE Cross-Validation:** Replicate hyperplane experiment (Figure 1) using VPSDE instead of VESDE to determine if multiscale singularity phenomenon is specific to VESDE or general to all diffusion frameworks

2. **Hyperparameter Robustness Test:** Systematically vary α (Niso-DM) and c_tango (Tango-DM) across [0.001, 0.01] ranges on SO(10) manifold to identify optimal values and measure sensitivity to critical hyperparameters

3. **Local Structure Preservation:** Generate samples from Tango-DM on Stanford Bunny mesh and compute Laplacian spectrum of generated point cloud versus ground truth to verify local geometric features are preserved, not just global distribution statistics