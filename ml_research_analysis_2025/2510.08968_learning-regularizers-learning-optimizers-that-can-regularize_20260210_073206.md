---
ver: rpa2
title: 'Learning Regularizers: Learning Optimizers that can Regularize'
arxiv_id: '2510.08968'
source_url: https://arxiv.org/abs/2510.08968
tags:
- loss
- regularization
- training
- optimizer
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether learned optimizers can internalize
  regularization techniques without explicit application during optimization. The
  authors train an LSTM-based learned optimizer on tasks like MNIST classification,
  incorporating regularization objectives such as SAM, GSAM, and GAM.
---

# Learning Regularizers: Learning Optimizers that can Regularize

## Quick Facts
- **arXiv ID**: 2510.08968
- **Source URL**: https://arxiv.org/abs/2510.08968
- **Reference count**: 40
- **Primary result**: Learned optimizers trained with regularization objectives can internalize flatness-seeking behavior and transfer these properties to new datasets and architectures without explicit regularization.

## Executive Summary
This paper investigates whether learned optimizers can internalize regularization techniques without explicit application during optimization. The authors train an LSTM-based learned optimizer on tasks like MNIST classification, incorporating regularization objectives such as SAM, GSAM, and GAM. During meta-testing on new datasets and architectures (including FMNIST and CIFAR-10 with MLP and CNN models), the regularized learned optimizers consistently achieve higher test accuracies than unregularized counterparts. The study demonstrates that learned optimizers can inherently learn and transfer regularization effects, reducing the need for explicit regularization during optimization.

## Method Summary
The method trains an LSTM-based learned optimizer (LO) to internalize regularization properties through meta-learning. During meta-training, the LO optimizes an objective that includes both the empirical loss and a regularization penalty (SAM, GSAM, or GAM). The LO operates coordinate-wise on optimizee parameters, using truncated backpropagation through time for meta-gradient computation. A smoothing regularization component stabilizes training by penalizing inconsistent updates under small perturbations. After meta-training, the LO is evaluated on held-out tasks without explicit regularization to assess whether it has internalized the desired properties.

## Key Results
- Learned optimizers trained with SAM, GSAM, or GAM regularization consistently outperform unregularized learned optimizers on meta-test tasks.
- Regularized learned optimizers demonstrate the ability to seek flatter minima even when no regularizer is applied during meta-testing.
- The internalized regularization properties transfer across different architectures (MLP, CNN) and datasets (MNIST, FMNIST, CIFAR-10).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learned optimizers can internalize geometric regularization properties (flatness-seeking behavior) from regularizers like SAM, GSAM, and GAM during meta-training.
- Mechanism: The meta-training objective adds a regularization penalty $L_{reg}(\phi) \in \{L_{SAM}, L_{GSAM}, L_{GAM}\}$ to the optimizer's loss. Through truncated backpropagation through time, the LSTM-based LO learns update rules that inherently guide optimizees toward minima with lower sharpness, even when no regularizer is applied during meta-testing.
- Core assumption: The regularization property (flatness, low gradient norm, surrogate gap minimization) can be encoded into the optimizer's learned dynamics through gradient-based meta-training.
- Evidence anchors:
  - [abstract] "LOs can be trained to learn and internalize the effects of traditional regularization techniques without explicitly applying them to the objective function."
  - [section: Learning a Regularizer] "The optimizer is thus trained to minimize not just the empirical loss but also the regularization penalty, shaping its hypothetical update rule accordingly."
  - [corpus] "Variational Deep Learning via Implicit Regularization" supports implicit regularization but does not directly validate learned optimizer regularization transfer.
- Break condition: When distribution shift between meta-training and meta-testing tasks is too large (e.g., CNN + CIFAR-10 when trained on MLP + MNIST), regularization properties may not transfer effectively.

### Mechanism 2
- Claim: Coordinate-wise LSTM processing enables the LO to generalize across optimizees with different parameter dimensions.
- Mechanism: The same RNN is applied independently to each parameter coordinate, taking gradient $\nabla_{\theta_t} L(\theta_t)$, current parameter $\theta_t$, and hidden state $h_{t-1}$ as inputs. The hidden state captures gradient history, enabling momentum-like dynamics without explicit design.
- Core assumption: Parameter coordinates share common optimization dynamics that can be captured by a shared LSTM.
- Evidence anchors:
  - [section: Preliminaries] "To ensure scalability across optimizees with different parameter dimensions, the LO operates in a coordinate-wise manner, meaning the same RNN is applied independently to each parameter coordinate."
  - [section: Related Work] "LOs have been observed to exhibit interpretable behaviors such as momentum-like dynamics and adaptive learning rate schedules."
  - [corpus] PyLO paper (arXiv:2506.10315) confirms coordinate-wise LSTM architectures remain standard for learned optimizers.
- Break condition: Coordinate-wise independence fails for parameters with strong interdependencies (e.g., batch normalization parameters, attention mechanisms).

### Mechanism 3
- Claim: Smoothing regularization (perturbation-based) stabilizes LO training by reducing sensitivity to small optimizee state variations.
- Mechanism: At each step, perturbed states $s'_t \in B(s_t, \epsilon)$ are constructed via projected gradient ascent, and the LO is penalized for producing divergent updates: $L_{smooth}(\phi) = \max_{s'_t \in B(s_t,\epsilon)} \|u(s_t) - u(s'_t)\|^2$. This encourages smooth optimizer behavior.
- Core assumption: Robust optimization requires consistent updates under small input perturbations.
- Evidence anchors:
  - [section: Smoothing Regularization] "To ensure stability in optimizer updates, we adopt a perturbation-based regularization strategy... encouraging the optimizer to produce similar updates for neighboring states."
  - [corpus] "Gradient Regularized Natural Gradients" (arXiv:2601.18420) supports gradient regularization for generalization but in a different optimizer context.
- Break condition: If perturbation radius $\epsilon$ is too large, smoothing may suppress useful optimizer adaptation; if too small, it provides no regularization benefit.

## Foundational Learning

- Concept: **Meta-learning (bilevel optimization)**
  - Why needed here: The LO training involves inner optimization (optimizee training) and outer optimization (optimizer parameter updates via meta-gradients).
  - Quick check question: Can you explain why computing $\nabla_\phi L_{meta}(\phi)$ requires backpropagation through the optimizee's entire training trajectory?

- Concept: **Sharpness-Aware Minimization (SAM) and variants**
  - Why needed here: Understanding SAM, GSAM, GAM is essential to interpret what geometric properties the LO is learning to internalize.
  - Quick check question: What does SAM's perturbed loss $\hat{L}_{SAM}(\theta) = \max_{\|\epsilon\| \leq \rho} \hat{L}(\theta + \epsilon)$ measure, and how does it relate to generalization?

- Concept: **Truncated Backpropagation Through Time (TBPTT)**
  - Why needed here: Full-gradient computation through long optimization trajectories is computationally prohibitive; TBPTT enables tractable meta-gradient estimation.
  - Quick check question: Why does TBPTT introduce bias in meta-gradient estimates, and what design choices mitigate this?

## Architecture Onboarding

- Component map:
  ```
  Optimizee (θ) → Loss L(θ) → Gradient ∇θ L(θ)
                                ↓
                    Preprocessing (optional log/exp scaling)
                                ↓
                    Coordinate-wise LSTM (hidden state h_t)
                                ↓
                    Linear layer → Update Δθ
                                ↓
                    θ_{t+1} = θ_t + Δθ
  ```
  Meta-training adds: $L_{meta,reg}(\phi) = L_{meta}(\phi) + \lambda_{smooth} L_{smooth}(\phi) + \lambda_{reg} L_{reg}(\phi)$

- Critical path:
  1. Initialize LO with LSTM (layers $L$, hidden size $H$)
  2. Configure preprocessing threshold $\tau = e^{-p}$
  3. Set curriculum learning schedule: $N_{train} = \{100, 200, 500, 1000\}$
  4. Apply smoothing regularization with PGA ($N_{PGA}$ iterations, radius $\epsilon$)
  5. Add SAM/GSAM/GAM regularization to meta-objective
  6. Update $\phi$ via Adam or SGD with meta-learning rate $\eta$

- Design tradeoffs:
  - **Unroll length vs. memory**: Longer unrolls capture more optimization dynamics but require more GPU memory; curriculum learning addresses this.
  - **Neighborhood radius $\rho$ for regularization**: Small radii miss meaningful variations; large radii may span multiple minima. Paper suggests 0.01 as balanced default.
  - **Weighting coefficients $w_t$**: Uniform weighting emphasizes all steps equally; decayed weighting prioritizes final performance.

- Failure signatures:
  - Sharp minima at convergence despite regularization: LO may not have internalized regularization; increase $\lambda_{reg}$ or training unrolls.
  - High variance across runs: Meta-training instability; increase smoothing regularization $\lambda_{smooth}$.
  - Poor transfer to new architectures: Meta-training task distribution too narrow; add curriculum over multiple architectures.

- First 3 experiments:
  1. **Baseline validation**: Train vanilla LO (no regularization) on MLP + MNIST; measure test accuracy and sharpness via PGA at convergence.
  2. **Regularized LO comparison**: Train LO with SAM regularization on same task; compare test accuracy and maximum neighborhood loss against baseline.
  3. **Transfer test**: Apply both LOs to held-out architecture (e.g., CNN) and dataset (e.g., FMNIST); evaluate whether regularized LO retains flatness-seeking behavior without explicit SAM at test time.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can learned optimizers (LOs) trained with regularizers transfer these properties to optimization tasks with substantial distribution shifts and dissimilar architectures (e.g., CNNs on CIFAR-10) without performance degradation?
- **Basis in paper:** [explicit] The authors explicitly note that for tasks differing substantially from the meta-training objective, such as "CIFAR-10 classification using a CNN, the validity of our hypothesis remains inconclusive," and observed that "GAM fails to produce satisfactory results" in this setting.
- **Why unresolved:** The paper primarily validates transfer across MNIST, FMNIST, and CIFAR-10 using MLPs, but the regularization effect appears to diminish or fail when architectural complexity (CNN) and data distribution (CIFAR-10) are combined.
- **What evidence would resolve it:** A study showing that an LO meta-trained on simpler datasets (like MNIST/MLP) can successfully enforce flat minima and improve test accuracy on large-scale, structurally different tasks (e.g., ImageNet/ResNet) without retraining.

### Open Question 2
- **Question:** Does meta-training LOs on a distribution of diverse loss surfaces improve the robustness of the internalized regularization compared to training on a single task?
- **Basis in paper:** [explicit] The conclusion suggests that "training on multiple types of loss surfaces can enhance the optimizer's ability to generalize" and that combining this with regularization "represents a significant advancement."
- **Why unresolved:** The experiments in the paper restrict meta-training to a specific setting (MNIST classification with an MLP) to isolate the learning of regularization, leaving the impact of multi-task surface training untested.
- **What evidence would resolve it:** Experiments comparing an LO meta-trained on a single surface versus one trained on a curriculum of varied surfaces, measuring the consistency of regularization properties (e.g., sharpness reduction) across unseen tasks.

### Open Question 3
- **Question:** Can the meta-training process for regularized LOs be scaled to modern, high-dimensional architectures without prohibitive computational costs?
- **Basis in paper:** [inferred] The authors state they "restrict the experiments to datasets and architectures... to make it manageable" due to "high computational and memory requirements" and high GPU memory usage.
- **Why unresolved:** The feasibility of learning regularizers for state-of-the-art models (which have vast parameter spaces) is unclear, as the current method relies on Truncated Backpropagation Through Time (TBPTT) and iterative projected gradient ascent, both of which scale poorly.
- **What evidence would resolve it:** A demonstration of the method training a large-scale model (e.g., a Transformer or large CNN) with a comparable or lower computational budget than standard hand-designed optimizers with explicit regularization.

## Limitations
- The study demonstrates transfer within a relatively narrow distribution of tasks (MNIST/FMNIST with MLP/CNN), leaving open questions about generalization to more complex architectures (transformers, RNNs) or high-dimensional tasks.
- Key hyperparameters for learned optimizer architecture and meta-training (LSTM hidden size, layers, preprocessing thresholds, meta-learning rates, regularization coefficients) are unspecified, making exact reproduction impossible without additional tuning.
- The meta-training process relies on computationally expensive operations like truncated backpropagation through time and iterative projected gradient ascent, limiting scalability to large models.

## Confidence
- **High**: The core claim that learned optimizers can internalize geometric regularization properties (flatness-seeking behavior) from SAM/GSAM/GAM during meta-training is well-supported by controlled experiments showing consistent test accuracy improvements and maintained regularization effects during meta-testing without explicit regularizers.
- **Medium**: The mechanism by which coordinate-wise LSTM processing enables cross-architecture generalization is theoretically sound but may face practical limitations with strongly coupled parameters (batch normalization, attention mechanisms) not tested in this study.
- **Medium**: Smoothing regularization via perturbation-based PGA effectively stabilizes LO training, though the sensitivity to perturbation radius ε and its interaction with meta-training dynamics requires careful tuning for different tasks.

## Next Checks
1. **Out-of-distribution generalization**: Meta-train on MNIST MLP, then meta-test on ImageNet classification with ResNet architectures to verify whether learned regularization properties transfer to large-scale, high-dimensional tasks.
2. **Hyperparameter sensitivity analysis**: Systematically vary LSTM architecture (H, L), meta-learning rates, and regularization coefficients to identify robust configurations and understand the stability of learned regularization properties.
3. **Failure mode characterization**: Intentionally induce sharp minima (e.g., via adversarial training) during meta-training to determine whether learned optimizers can still recover regularization properties, and identify conditions under which internalization fails.