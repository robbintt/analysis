---
ver: rpa2
title: 'Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling
  than CoT Prompting?'
arxiv_id: '2510.03093'
source_url: https://arxiv.org/abs/2510.03093
tags:
- data
- s2tt
- speech
- direct
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Chain-of-Thought (CoT) and Direct prompting
  strategies for speech-to-text translation using large language models. The authors
  generate pseudo-labeled S2TT data by translating transcriptions from an ASR corpus
  into six European languages, then train LLM-based S2TT systems with both prompting
  strategies at different data scales.
---

# Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?

## Quick Facts
- arXiv ID: 2510.03093
- Source URL: https://arxiv.org/abs/2510.03093
- Reference count: 0
- Direct prompting scales more consistently than CoT as S2TT data increases, suggesting it may become more effective with larger resources

## Executive Summary
This paper compares Chain-of-Thought (CoT) and Direct prompting strategies for speech-to-text translation using large language models. The authors generate pseudo-labeled S2TT data by translating transcriptions from an ASR corpus into six European languages, then train LLM-based S2TT systems with both prompting strategies at different data scales. Results show that Direct prompting exhibits more consistent improvement as S2TT data increases, while CoT performance peaks early and degrades with more data. Direct prompting also maintains stable ASR and T2TT performance across data scales. The findings suggest Direct prompting may become a more effective approach as larger S2TT resources become available, offering practical advantages in computational efficiency and simplicity of implementation.

## Method Summary
The method uses a two-stage training approach with salamandraTA-7B-Instruct LLM backbone and mHuBERT-base-25Hz encoder (k-means quantization, 500 clusters, 11th layer). Stage 1 trains only token embeddings with frozen LLM on ASR data (1024 seq len, packing enabled). Stage 2 trains the full model on mixed ASR + T2TT + S2TT + pseudo-labeled S2TT data (2048 seq len, no packing). The study compares CoT prompting (audio→transcribe→translate) and Direct prompting (audio→translate) across six European languages, scaling pseudo-labeled S2TT data from 0% to 100% while evaluating BLEU, xCOMET, and WER metrics.

## Key Results
- Direct prompting scales more consistently than CoT as S2TT data increases, with CoT peaking at 20% and degrading thereafter
- Direct AUG maintains stable ASR and T2TT performance across all data scales, while CoT shows increasing WER (+7.4% to +13% relative) and decreasing T2TT performance
- CoT BASE outperforms Direct BASE by ~5 BLEU and ~7 xCOMET points, demonstrating CoT's advantage with limited S2TT data
- Direct prompting shows computational efficiency benefits through simpler inference without intermediate transcription steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct prompting scales more consistently than CoT as S2TT data increases.
- Mechanism: Direct prompting avoids the intermediate transcription bottleneck that constrains CoT. When scaling pseudo-labeled S2TT data, each utterance is paired with multiple target languages without repeating the transcription step, preserving training signal quality. In contrast, CoT repeats speech-transcription pairs per target language, which dilutes ASR supervision and degrades the transcription sub-task that CoT critically depends on.
- Core assumption: The degradation in CoT stems from data distribution shifts during scaling, not from fundamental model capacity limits.
- Evidence anchors:
  - [abstract] "Direct improves more consistently as the amount of data increases, suggesting that it may become a more effective approach as larger S2TT resources are created."
  - [section 4] "COT AUG variants peak at 20% of the S2TT pl data and then degrade as more data is added. In contrast, DIRECT AUG consistently improves."
  - [corpus] Neighbor paper "Listening or Reading?" confirms CoT systems may not fully exploit acoustic cues, supporting the transcription-bottleneck hypothesis.
- Break condition: If CoT's ASR degradation can be mitigated (e.g., via transcription loss weighting or separate ASR mixing), Direct's scaling advantage may diminish.

### Mechanism 2
- Claim: CoT's initial superiority stems from leveraging abundant ASR and T2TT data through explicit step decomposition.
- Mechanism: CoT prompting decomposes S2TT into ASR→T2TT, allowing the model to exploit large existing datasets for each sub-task. This is effective when S2TT data is scarce but ASR/T2TT resources are abundant. The prompt template (Figure 2) explicitly surfaces these capabilities during training and inference.
- Core assumption: The model successfully transfers ASR and T2TT capabilities learned separately into the combined CoT pipeline.
- Evidence anchors:
  - [abstract] "CoT typically outperforms direct prompting primarily because it can exploit abundant ASR and T2TT datasets to explicitly model its steps."
  - [section 4, Table 2] COT BASE outperforms DIRECT BASE by ~5 BLEU and ~7 xCOMET points across languages.
  - [corpus] MCAT paper confirms many S2TT datasets are English-centric, making ASR/T2TT exploitation valuable for multilingual coverage.
- Break condition: When S2TT data matches or exceeds ASR/T2TT scale, the decomposition advantage disappears; Direct may match or exceed CoT.

### Mechanism 3
- Claim: Direct prompting preserves ASR and T2TT performance stability when scaling S2TT data, while CoT does not.
- Mechanism: Direct AUG treats all S2TT samples as end-to-end examples without interleaving ASR sub-task training. This prevents distribution shift in ASR capability. CoT AUG either trains transcription (harming ASR via repetition) or skips it (reducing ASR task proportion), both degrading ASR quality which cascades to translation.
- Core assumption: ASR quality is the primary bottleneck for CoT S2TT performance under scaling.
- Evidence anchors:
  - [section 4] "WER remains stable with DIRECT AUG across all S2TT pl data scales, whereas both COT AUG and COT † AUG gradually increase it by up to +7.4% and +13% relative to their baseline."
  - [section 4] "These trends correlate with the steady decrease of S2TT performance, reflecting their critical reliance on the ASR sub-task."
  - [corpus] No direct corpus evidence on this specific stability mechanism; remains an open question.
- Break condition: If ASR training is carefully balanced with S2TT scaling (e.g., dynamic mixing ratios), CoT stability may improve.

## Foundational Learning

- Concept: **Discrete Speech Tokenization**
  - Why needed here: The architecture quantizes continuous speech encoder outputs (mHuBERT) into discrete tokens via k-means, enabling LLM vocabulary expansion. Understanding this is essential for debugging token-level issues and interpreting sequence lengths.
  - Quick check question: Can you explain why 25Hz downsampling matters for sequence length compared to standard 50Hz HuBERT?

- Concept: **Two-Stage Training for Speech-LLM Adaptation**
  - Why needed here: Stage 1 trains only token embeddings with frozen LLM; Stage 2 trains the full model. This prevents catastrophic forgetting and stabilizes speech feature learning before full fine-tuning.
  - Quick check question: What would happen if you skipped Stage 1 and trained the full LLM immediately on speech data?

- Concept: **Pseudo-Labeling with Quality Filtering**
  - Why needed here: The paper generates S2TT training data by translating ASR transcripts, then filters with BLASER 2.0 (QE) and GlotLID (language ID). Understanding this pipeline is critical for reproducing results or extending to new languages.
  - Quick check question: Why is language identification filtering necessary when the translation model should already produce target-language output?

## Architecture Onboarding

- Component map:
  - mHuBERT features → k-means quantization (500 clusters) → discrete speech tokens
  - salamandraTA-7B-Instruct vocabulary expansion with speech tokens
  - Two-stage training pipeline (Stage 1: embeddings only, Stage 2: full model)
  - Prompt templates (CoT vs Direct) during inference

- Critical path:
  1. Extract mHuBERT features → quantize to discrete tokens
  2. Expand LLM vocabulary, initialize speech embeddings
  3. Stage 1: Train embeddings on ASR data (next-token prediction)
  4. Stage 2: Full training on ASR + T2TT + S2TT + S2TT_pl mix
  5. Inference: Apply prompt template, beam search (5 beams)

- Design tradeoffs:
  - CoT vs Direct: CoT leverages ASR/T2TT data but scales poorly; Direct scales well but requires more S2TT data
  - Transcription loss in CoT: Training it harms ASR (COT †); skipping it reduces ASR task proportion (COT). Both degrade under scaling.
  - Sequence packing: Used in Stage 1 for efficiency, disabled in Stage 2 for longer sequences (2048 tokens)

- Failure signatures:
  - CoT WER increases +7-13% when scaling S2TT_pl → ASR degradation cascading to translation
  - Direct underperforms CoT by ~5 BLEU at low S2TT data → insufficient end-to-end training signal
  - Target language drift in pseudo-labels → requires LID filtering (p_e < 0.5 threshold)

- First 3 experiments:
  1. **Reproduce baseline gap**: Train DIRECT_BASE and COT_BASE on ASR + T2TT + S2TT only (no S2TT_pl). Confirm COT outperforms by ~5 BLEU.
  2. **Scale S2TT_pl incrementally**: Train DIRECT_AUG20/40/60/80/100 and COT_AUG equivalents. Plot S2TT, ASR, T2TT curves to verify Direct's stable scaling vs CoT's peak-at-20% pattern.
  3. **Ablate transcription loss**: Compare COT_AUG (no transcription loss) vs COT † _AUG (with transcription loss) to confirm ASR degradation mechanism.

## Open Questions the Paper Calls Out
None

## Limitations

- The scaling behavior analysis relies on pseudo-labeled data quality, with heuristic filtering thresholds that weren't extensively ablated
- ASR quality degradation in CoT prompting is documented but the underlying mechanism (transcription loss training vs task proportion) needs further validation
- The suggestion that Direct prompting "may become more effective" with larger resources extrapolates beyond tested data scales

## Confidence

**High Confidence**: Direct prompting scales more consistently than CoT as S2TT data increases, supported by clear BLEU and xCOMET trends across multiple scales and languages.

**Medium Confidence**: CoT's initial superiority leveraging ASR/T2TT data is supported by baseline results but requires additional experiments to confirm decomposition advantage.

**Medium Confidence**: Direct prompting preserves ASR stability while CoT degrades is well-supported by WER trends, but the underlying mechanism needs further validation.

**Low Confidence**: The extrapolation that Direct may become more effective with larger resources extends beyond tested data scales.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary BLASER 2.0 QE threshold and GlotLID filtering parameters to quantify their impact on scaling trends and determine robustness.

2. **Attention Pattern Analysis**: Compare attention distributions in CoT vs Direct models during inference, focusing on shifts when scaling from 20% to 100% pseudo-labeled data to reveal attention drift contributions.

3. **Cross-Architecture Replication**: Implement Direct vs CoT comparison using a different speech-LLM architecture (e.g., speechT5-based) to verify scaling behavior is not architecture-specific.