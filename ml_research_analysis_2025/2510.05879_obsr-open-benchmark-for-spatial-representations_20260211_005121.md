---
ver: rpa2
title: 'OBSR: Open Benchmark for Spatial Representations'
arxiv_id: '2510.05879'
source_url: https://arxiv.org/abs/2510.05879
tags:
- spatial
- benchmark
- data
- geospatial
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents OBSR, a multi-task, modality-agnostic benchmark
  for geospatial representation learning. It addresses the gap in existing geospatial
  benchmarks, which are typically single-task and single-modality, by introducing
  a comprehensive framework that evaluates models across diverse urban-related phenomena.
---

# OBSR: Open Benchmark for Spatial Representations

## Quick Facts
- arXiv ID: 2510.05879
- Source URL: https://arxiv.org/abs/2510.05879
- Reference count: 40
- Primary result: Multi-task, modality-agnostic benchmark for geospatial representation learning across 7 datasets spanning 3 continents

## Executive Summary
OBSR addresses the gap in existing geospatial benchmarks by introducing a comprehensive framework that evaluates models across diverse urban-related phenomena using multiple modalities. The benchmark includes seven datasets spanning multiple continents and modalities (vector data, trajectories) for tasks like housing price prediction, crime activity prediction, and human mobility forecasting. A key contribution is a simple baseline model using only OpenStreetMap-derived features, providing a standardized reference point for comparison. The evaluation reveals that while OSM-based embeddings perform reasonably on region-based tasks, they struggle with trajectory-based tasks, highlighting the need for more sophisticated, multi-modal approaches.

## Method Summary
OBSR is implemented as an extension to the SRAI library, evaluating geospatial embedders on five downstream tasks across seven datasets. The method uses H3 hexagonal discretization to normalize heterogeneous geospatial data, converting points, polygons, and trajectories into consistent spatial units. Embedders use OpenStreetMap tag features as inputs, with models trained using specified architectures (FFNN for region tasks, LSTM+Attention for trajectory tasks). The benchmark evaluates performance across three H3 resolutions (8, 9, 10) to assess resolution sensitivity, using regression metrics for region tasks and trajectory metrics for mobility tasks.

## Key Results
- OSM-based embeddings perform reasonably on region-based tasks (housing prices, crime prediction) but struggle with trajectory-based tasks
- Simple count-based embedders (ContextualCountEmbedder) can compete closely with learned embeddings (Hex2Vec) on price prediction tasks
- Resolution sensitivity affects performance differently: housing prices show stable distributions while crime intensity decreases with smaller hexagons
- Baseline models provide a standardized reference point, with Hex2Vec generally outperforming count-based methods

## Why This Works (Mechanism)

### Mechanism 1: H3 Hexagonal Discretization for Cross-Task Alignment
- **Claim:** Converting heterogeneous geospatial data into a unified H3 hexagonal grid enables consistent evaluation across distinct urban phenomena.
- **Mechanism:** By aggregating data into hierarchical hexagons, the benchmark normalizes inputs so that disparate tasks operate on identical spatial units, allowing a single embedding model to be evaluated on multiple downstream heads.
- **Core assumption:** Aggregation into hexagonal cells preserves sufficient signal for downstream tasks without destroying local context.
- **Evidence anchors:** Section 3.1 mentions using different H3 resolutions; Section 3.5 shows target distributions shift based on resolution.
- **Break condition:** Mismatched hexagon size to phenomenon's spatial scale washes out signal and causes model failure.

### Mechanism 2: OSM Tag Frequency as a Spatial Prior
- **Claim:** Simple counts of OSM features serve as a strong, low-cost baseline for region-based tasks, suggesting amenity density correlates with urban indicators.
- **Mechanism:** Baseline models operate on tag frequency within regions, working because housing prices and crime rates are proxies for urban activity levels physically manifested as amenities.
- **Core assumption:** OSM data is sufficiently complete and up-to-date across tested continents.
- **Evidence anchors:** Abstract notes OSM-based embeddings perform reasonably on region tasks; Table 2 & 3 show count embedders competing with learned embeddings.
- **Break condition:** Poor OSM coverage or tasks requiring temporal dynamics cause static tag frequencies to fail.

### Mechanism 3: Resolution Sensitivity in Spatial Representation
- **Claim:** Effectiveness of spatial representation is conditional on granularity, interacting differently with dense vs. sparse phenomena.
- **Mechanism:** Benchmark evaluates models at resolutions 8, 9, and 10, with performance changing non-linearly based on phenomenon type.
- **Core assumption:** A robust GeoAI model should maintain performance or degrade gracefully across spatial scales.
- **Evidence anchors:** Section 3.5 shows housing prices stable while crime intensity decreases with smaller hexagons; Table 5 indicates R2 score fluctuations.
- **Break condition:** Models fixed on single resolution fail to generalize to varying spatial densities.

## Foundational Learning

- **Concept:** Uber H3 Hierarchical Grid
  - **Why needed here:** The entire benchmark is built on H3 hexagonal indexing. Understanding H3 as a discrete global grid system with 16 resolutions is necessary to interpret multi-resolution results.
  - **Quick check question:** Does increasing H3 resolution number (e.g., going from 8 to 10) make hexagons smaller or larger?

- **Concept:** Spatial Autocorrelation (Tobler's First Law)
  - **Why needed here:** The paper assumes "near things are related," which is the implicit logic why aggregating points into hexagons works.
  - **Quick check question:** Why does the paper aggregate neighboring hexagons to create "Contextual" embeddings?

- **Concept:** Embedding vs. Feature Engineering
  - **Why needed here:** The paper contrasts "embedders" (learned representations like Hex2Vec) with "CountEmbedders" (hand-crafted features).
  - **Quick check question:** Why might a learned embedding (Hex2Vec) theoretically outperform simple counts of amenities in complex tasks?

## Architecture Onboarding

- **Component map:** Data Layer (7 Datasets) → Processing Layer (H3 Indexing → Aggregation → Interpolation) → Embedding Layer (OSM Loader → Hex2Vec/GeoVex/CountEmbedder) → Task Layer (FFNN for Prices/Crime, LSTM+Attention for Mobility)

- **Critical path:** Mapping raw coordinates to H3 indices is the structural backbone. If H3 resolution configuration is misaligned between dataset loader and embedding model, the join fails.

- **Design tradeoffs:**
  - **Simplicity vs. Dynamics:** Baseline models use static OSM tags, which are computationally cheap but fail to capture temporal dynamics, leading to poor trajectory performance.
  - **Resolution vs. Sparsity:** Higher resolutions provide detail but increase data sparsity and trajectory length, making LSTM training harder.

- **Failure signatures:**
  - **Trajectory Hallucination:** Models predict paths through forests or invalid areas, indicating learned statistical patterns but lack of physical constraints.
  - **Regression to Mean:** In crime tasks, coarse resolution causes models to predict average intensity everywhere, failing to identify hotspots.

- **First 3 experiments:**
  1. **Resolution Sensitivity Sweep:** Run ContextualCountEmbedder on Chicago Crime dataset at Res 8, 9, and 10 to observe R2 score degradation.
  2. **Baseline Stress Test (Trajectory):** Train LSTM baseline for TTE using only random embeddings vs. Hex2Vec embeddings to quantify OSM tag value gap.
  3. **Cross-City Generalization:** Train Housing Price model (FFNN) on King County data and test directly on Airbnb dataset to check OSM embedding generalization across continents.

## Open Questions the Paper Calls Out
None

## Limitations
- OSM coverage varies significantly across continents, potentially limiting generalizability beyond well-mapped urban areas
- Trajectory tasks use interpolation to fill missing GPS points, which may artificially improve performance metrics
- The paper doesn't quantify OSM tag completeness or recency across different geographic regions

## Confidence

- **High Confidence**: Multi-task, multi-modality design is clearly articulated and well-supported by dataset descriptions and task definitions
- **Medium Confidence**: Claim that OSM-based embeddings perform "reasonably" on region-based tasks is supported by baseline comparisons, but "reasonably" is relative
- **Medium Confidence**: Resolution sensitivity findings are well-documented, but practical implications for model deployment need further exploration

## Next Checks

1. **Geographic Coverage Audit**: Analyze OSM tag completeness and update frequency for each dataset location to quantify potential geographic bias in benchmark results.

2. **Trajectory Interpolation Impact**: Compare model performance on trajectory tasks using interpolated data versus raw, irregularly sampled GPS trajectories to assess robustness of reported metrics.

3. **Cross-Resolution Generalization**: Train models at fixed resolution (e.g., Res 9) and evaluate on test data aggregated at different resolutions to quantify performance degradation and validate resolution sensitivity claims.