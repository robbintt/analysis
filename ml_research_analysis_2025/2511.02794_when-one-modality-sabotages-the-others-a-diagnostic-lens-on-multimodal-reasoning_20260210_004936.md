---
ver: rpa2
title: 'When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning'
arxiv_id: '2511.02794'
source_url: https://arxiv.org/abs/2511.02794
tags:
- modality
- multimodal
- fusion
- emotion
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "modality sabotage" as a diagnostic lens
  for analyzing multimodal reasoning failures, where a high-confidence unimodal error
  overrides other evidence and misleads the final prediction. The authors propose
  a lightweight, model-agnostic evaluation framework that treats each modality (text,
  audio, vision) as an agent, producing candidate labels with confidence scores and
  self-assessments.
---

# When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning

## Quick Facts
- **arXiv ID:** 2511.02794
- **Source URL:** https://arxiv.org/abs/2511.02794
- **Reference count:** 35
- **Key outcome:** Introduces "modality sabotage" as a diagnostic lens for multimodal reasoning failures, where a high-confidence unimodal error overrides other evidence and misleads final predictions

## Executive Summary
This paper presents a novel diagnostic framework for analyzing multimodal reasoning failures through the lens of "modality sabotage." The approach treats each modality (text, audio, vision) as an agent that produces candidate labels with confidence scores and self-assessments. A lightweight fusion mechanism aggregates these outputs to identify contributors (modalities supporting correct outcomes) and saboteurs (modalities causing errors). Applied to multimodal emotion recognition benchmarks, the framework reveals systematic reliability profiles across modalities and shows that while fusion maintains baseline top-1 accuracy, it substantially improves top-k coverage by preserving correct hypotheses even when top-1 predictions are affected.

## Method Summary
The authors propose a model-agnostic evaluation framework that decomposes multimodal reasoning into unimodal agent components. Each modality operates as an independent agent producing a candidate label with an associated confidence score and self-assessment of reliability. A simple confidence-weighted fusion mechanism aggregates these unimodal outputs to produce final predictions. The framework then analyzes which modalities contributed to correct outcomes versus which ones caused errors, identifying "modality sabotage" as a failure mode where a single high-confidence unimodal error overrides other evidence and misleads the final prediction. This approach enables systematic auditing of fusion dynamics and provides actionable insights into whether failures stem from dataset artifacts or model limitations.

## Key Results
- Modality sabotage identified as a systematic failure mode where high-confidence unimodal errors override other evidence
- Fusion mechanism maintains baseline top-1 accuracy while substantially improving top-k coverage
- Systematic reliability profiles revealed across text, audio, and vision modalities in emotion recognition tasks
- Diagnostic framework provides actionable insights distinguishing dataset artifacts from model limitations

## Why This Works (Mechanism)
The framework works by decomposing multimodal reasoning into interpretable unimodal components, allowing systematic tracking of how individual modalities influence final predictions. By treating each modality as an agent with confidence-weighted voting, the approach can identify when a single modality's high-confidence error propagates through fusion to corrupt otherwise correct predictions. The self-assessment component enables modalities to flag their own potential unreliability, providing additional diagnostic signals beyond raw confidence scores.

## Foundational Learning
**Unimodal confidence calibration:** Understanding how to interpret and trust confidence scores from individual modalities
- *Why needed:* Confidence scores form the basis for weighting modality contributions during fusion
- *Quick check:* Compare predicted confidence distributions against actual accuracy rates for each modality

**Multi-agent fusion strategies:** Basic principles of combining multiple evidence sources with confidence weighting
- *Why needed:* The framework relies on simple confidence-weighted aggregation to combine unimodal predictions
- *Quick check:* Verify that confidence-weighted voting outperforms uniform voting in unimodal scenarios

**Error propagation analysis:** Understanding how individual errors cascade through fusion mechanisms
- *Why needed:* Central to identifying and characterizing modality sabotage patterns
- *Quick check:* Track error sources in synthetic scenarios where one modality is intentionally corrupted

## Architecture Onboarding
**Component map:** Text agent -> Audio agent -> Vision agent -> Confidence-weighted fusion -> Final prediction
**Critical path:** Unimodal prediction generation → Confidence scoring → Self-assessment → Weighted fusion → Final output
**Design tradeoffs:** Simple agent-based decomposition vs. end-to-end multimodal training; lightweight fusion vs. complex cross-modal attention
**Failure signatures:** High-confidence unimodal errors overriding correct predictions from other modalities; systematic bias in one modality's predictions
**First experiments:**
1. Apply framework to synthetic multimodal data with known sabotage scenarios
2. Compare modality reliability profiles across different emotion recognition datasets
3. Test framework with multimodal models that don't decompose into clean unimodal components

## Open Questions the Paper Calls Out
None

## Limitations
- Framework assumes unimodal models can generate reliable confidence scores and self-assessments, which remains unverified for many state-of-the-art models
- Simple fusion mechanism may not capture complex cross-modal interactions present in modern multimodal transformers
- Analysis focuses primarily on emotion recognition benchmarks, raising questions about generalizability to other multimodal tasks

## Confidence
- **High Confidence:** Basic observation that unimodal errors can propagate through fusion mechanisms and the methodological framework for tracking modality contributions
- **Medium Confidence:** Specific characterization of "modality sabotage" as a distinct failure mode and the proposed confidence-based aggregation strategy
- **Medium Confidence:** Claims about systematic reliability profiles across modalities, pending broader validation

## Next Checks
1. Apply the framework to multimodal reasoning tasks beyond emotion recognition (e.g., visual question answering, document understanding) to assess generalizability of modality sabotage patterns

2. Test the diagnostic framework across different multimodal model architectures (including end-to-end trained models) to verify that modality-level analysis remains meaningful

3. Systematically evaluate whether unimodal confidence scores are well-calibrated and whether the self-assessment mechanism accurately predicts when modalities are likely to sabotage correct predictions