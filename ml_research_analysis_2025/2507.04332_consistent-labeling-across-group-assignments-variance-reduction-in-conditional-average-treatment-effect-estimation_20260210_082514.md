---
ver: rpa2
title: 'Consistent Labeling Across Group Assignments: Variance Reduction in Conditional
  Average Treatment Effect Estimation'
arxiv_id: '2507.04332'
source_url: https://arxiv.org/abs/2507.04332
tags:
- claga
- treatment
- cate
- group
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of inconsistent learning behavior
  in CATE estimation algorithms, where the same instance receives different predictions
  depending on its group assignment. To quantify this, the authors introduce the discrepancy
  ratio metric.
---

# Consistent Labeling Across Group Assignments: Variance Reduction in Conditional Average Treatment Effect Estimation

## Quick Facts
- arXiv ID: 2507.04332
- Source URL: https://arxiv.org/abs/2507.04332
- Reference count: 13
- Primary result: CLAGA reduces PEHE by up to 46.3% on synthetic data and improves AUUC on real-world datasets by eliminating inconsistent labeling across group assignments.

## Executive Summary
This paper addresses a fundamental inconsistency in CATE estimation where the same instance receives different predictions depending on its treatment assignment. The authors introduce the discrepancy ratio metric to quantify this issue and show that it contributes to estimation errors that cannot be resolved by conventional ML techniques. They propose CLAGA, a method that uses K-fold cross-validation to generate consistent out-of-sample predictions as learning targets, eliminating this inconsistency. Experiments demonstrate significant performance improvements across synthetic and real-world datasets.

## Method Summary
CLAGA is a meta-algorithm that wraps any existing CATE estimator to eliminate inconsistent labeling across group assignments. It uses K-fold cross-validation where each instance receives an out-of-sample prediction from a model that didn't train on it. These consistent predictions serve as relabeled learning targets for a secondary model, eliminating the variance introduced by assignment-dependent surrogate labels. The method can be applied to any base CATE algorithm and shows particular effectiveness with complex models or limited data scenarios.

## Key Results
- PEHE reductions of up to 46.3% on ACIC-2018 synthetic dataset
- Improved AUUC performance on real-world datasets including Criteo and news recommendation data
- CLAGA particularly effective when applied to complex models or with limited data
- Consistent improvements across multiple base CATE algorithms (R-learner, T-learner, X-learner)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard CATE estimators produce assignment-dependent predictions for the same instance, introducing variance that cannot be eliminated through conventional ML techniques.
- Mechanism: Learning targets differ based on whether an instance is assigned to treatment or control, causing the model to learn different functions conditioned on group membership rather than the assignment-invariant treatment effect.
- Core assumption: The true CATE τ(x) is a deterministic function of covariates and should not depend on treatment assignment during training.
- Evidence anchors: Abstract states algorithms exhibit inconsistent behavior across group assignments; section 4 introduces discrepancy ratio measuring this effect.
- Break condition: If learning targets are already assignment-invariant by construction, this mechanism provides no benefit.

### Mechanism 2
- Claim: PEHE error decomposes into training-dependent and training-independent components; the latter includes terms driven by surrogate label variance across and within group assignments.
- Mechanism: The error decomposition identifies SDMG(τ̃) = π(1-π)(E[τ̃(0)] - E[τ̃(1)])² as the inconsistency term and WVG(τ̃) as weighted intra-group variance. These depend only on algorithm design, not model training quality.
- Core assumption: The decomposition accurately reflects the data-generating process and surrogate label construction.
- Evidence anchors: Section 5.1 shows full decomposition with "Inconsistency Across Group Assignments" and "Group Assignment Weighted Variance" as distinct error components.
- Break condition: If surrogate labels are already consistent (E[τ̃(0)] = E[τ̃(1)]), SDMG vanishes and this mechanism is inactive.

### Mechanism 3
- Claim: Out-of-sample predictions from primary estimators provide assignment-invariant learning targets, eliminating SDMG and reducing WVG.
- Mechanism: K-fold procedure ensures each instance receives a prediction from a model that did not train on it. Since the prediction τ̂⁻ does not condition on W for that instance, E[τ̃'(0)] = E[τ̃'(1)], setting SDMG = 0. Ensemble averaging can further reduce variance.
- Core assumption: Primary estimators are sufficiently unbiased that E[τ̂] ≈ E[τ̃]; weak primary estimators may introduce instability in τ̃'.
- Evidence anchors: Section 6.3 shows E[τ̃'(0)] = E[τ̃'(1)] leading to SDMG = 0; section 7.1 demonstrates PEHE reductions of 7.8% and 46.3% with CLAGA.
- Break condition: If primary estimators are too weak (high variance/bias), τ̃' becomes unstable and may degrade performance.

## Foundational Learning

- Concept: Potential Outcomes Framework
  - Why needed here: CATE is defined as τ(x) = μ₁(x) - μ₀(x), the difference between counterfactual outcome expectations. The fundamental problem (only one outcome observable per unit) motivates surrogate label construction.
  - Quick check question: Can you explain why E[τ̂(x)|X=x,W=0] and E[τ̂(x)|X=x,W=1] should be equal for a consistent estimator?

- Concept: Meta-learners for CATE (Single-model, Two-model, X-learner, R-learner, DR-learner)
  - Why needed here: CLAGA is a meta-algorithm that wraps any existing CATE estimator. Understanding how each constructs learning targets clarifies why inconsistency arises.
  - Quick check question: For a Two-model approach, what are the surrogate labels for treated vs. control units?

- Concept: Out-of-sample Prediction and Cross-validation
  - Why needed here: CLAGA's core procedure is K-fold out-of-sample prediction generation, analogous to cross-validated risk estimation.
  - Quick check question: Why must the primary estimator not train on the fold for which it generates predictions?

## Architecture Onboarding

- Component map: Dataset D -> Partition into K folds -> Train K primary estimators gᵢ on (D - Dᵢ) -> Generate out-of-sample predictions τ̂⁻(Xⱼ) for Xⱼ ∈ Dᵢ -> Construct relabeled dataset D' = {(Xᵢ, τ̂⁻(Xᵢ))} -> Train secondary regression model g' on D' -> Final CATE estimator g'

- Critical path: Step 2 (primary estimator quality) → Step 3 (label consistency) → Step 4 (secondary model fit). Errors propagate forward.

- Design tradeoffs:
  - K selection: Higher K increases computational cost (K model trainings) but provides more training data per primary estimator. Paper uses K=10 for small datasets, K=2 for large.
  - Primary estimator complexity: Too weak → unstable τ̃'; too complex → overfitting in primary stage. Paper finds CLAGA benefits emerge above a minimum complexity threshold.
  - Secondary model choice: Should be flexible enough to fit τ̂⁻ but not overfit; paper uses LGBMRegressor.

- Failure signatures:
  - No improvement or degradation: Primary estimators too weak (high variance τ̃')
  - High computational cost with minimal gain: K set too high for dataset size
  - Inconsistent results across runs: Insufficient ensemble stabilization in primary stage

- First 3 experiments:
  1. Replicate discrepancy ratio measurement on a synthetic dataset with known τ(x): train a base CATE estimator 30× with randomized treatment assignments, compute t-tests per instance, report proportion with p<0.05.
  2. Ablate CLAGA with K∈{2,5,10} on ACIC-2016 using R-learner as base; report PEHE ratio vs. vanilla R-learner.
  3. Compare CLAGA-wrapped vs. vanilla estimators on a real-world dataset (e.g., Criteo) using AUUC; verify ranking improvement correlates with reduced discrepancy ratio.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed error decomposition framework theoretically guarantee improvements for ranking metrics like AUUC?
- Basis in paper: Section 8 (Limitations) states that the theoretical analysis focuses on PEHE and "may not directly be guaranteed to extend to other metrics, such as AUUC," despite empirical improvements observed.
- Why unresolved: The mathematical derivation relies on squared error decomposition (PEHE), whereas AUUC depends on the relative ranking of estimates rather than their absolute values.
- What evidence would resolve it: A formal extension of the error decomposition theory to connect variance reduction with ranking performance (AUUC).

### Open Question 2
- Question: How does CLAGA perform when applied to primary estimators that are intentionally underfit or possess high bias?
- Basis in paper: Section 8 notes that CLAGA relies on the quality of initial estimators and may not improve performance if these estimators are "overly simplified" or weak. Section 7.1 also notes CLAGA may have negative effects for very weak models due to label instability.
- Why unresolved: The paper primarily demonstrates results using robust learners (e.g., LGBM), leaving the failure mode for high-bias base learners under-explored.
- What evidence would resolve it: Experiments using synthetic data where the model capacity of primary estimators is systematically reduced to observe the CLAGA failure threshold.

### Open Question 3
- Question: What is the optimal number of folds (K) to balance the variance reduction benefits of CLAGA against its increased computational cost?
- Basis in paper: Section 8 explicitly lists "Increased computational complexity" as a limitation due to training K estimators. Additionally, the method uses different K values (2 vs. 10) for different datasets without providing a selection criteria.
- Why unresolved: While higher K provides more stable labels (approaching infinite ensemble), it linearly increases training time; the trade-off point is not analyzed.
- What evidence would resolve it: An ablation study measuring the marginal reduction in PEHE versus the marginal increase in computation time as K varies.

## Limitations
- Theoretical decomposition assumes linear error structure that may not hold for complex, non-linear CATE estimators.
- Performance heavily depends on quality of primary estimators; weak base models may degrade performance rather than improve it.
- Computational overhead scales linearly with K, potentially prohibitive for very large datasets or complex models.

## Confidence
- High confidence in discrepancy ratio metric and its empirical measurement across different estimators.
- Medium confidence in error decomposition framework, as it relies on theoretical assumptions about surrogate label variance.
- Medium confidence in primary mechanism, as experimental results show consistent improvements but magnitude varies significantly across datasets and base algorithms.

## Next Checks
1. Conduct ablation studies varying K on multiple datasets to determine optimal fold count for different dataset sizes and model complexities.
2. Test CLAGA with weaker primary estimators (e.g., linear models) to establish minimum performance threshold required for improvements.
3. Implement parallelized version of CLAGA to measure practical computational overhead and identify scalability bottlenecks.