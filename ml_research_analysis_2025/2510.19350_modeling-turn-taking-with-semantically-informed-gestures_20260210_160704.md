---
ver: rpa2
title: Modeling Turn-Taking with Semantically Informed Gestures
arxiv_id: '2510.19350'
source_url: https://arxiv.org/abs/2510.19350
tags:
- gesture
- gestures
- turn-taking
- semantic
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the DnD Gesture corpus with 2,663 semantic gesture
  annotations spanning iconic, metaphoric, deictic, and discourse types, creating
  the first multiparty dataset for studying gesture-informed turn-taking modeling.
  The authors introduce a Mixture-of-Experts framework integrating text, audio, and
  gestures, using semantically guided gesture embeddings via a VQ-VAE architecture
  trained with cross-modal alignment.
---

# Modeling Turn-Taking with Semantically Informed Gestures

## Quick Facts
- arXiv ID: 2510.19350
- Source URL: https://arxiv.org/abs/2510.19350
- Reference count: 22
- Primary result: Semantically aligned gesture embeddings achieve 71.5 macro-F1 for turn-taking prediction, outperforming text+audio baseline (69.7 F1)

## Executive Summary
This paper addresses turn-taking prediction in multiparty conversations by introducing semantic gesture annotations to the DnD Gesture corpus and a Mixture-of-Experts (MoE) framework that integrates text, audio, and semantically informed gesture features. The authors propose a VQ-VAE architecture with cross-modal alignment training to create gesture embeddings that cluster by communicative function (iconic, metaphoric, deictic, discourse), demonstrating that these semantically guided representations provide complementary predictive signals for distinguishing turn-holding from turn-yielding behaviors.

## Method Summary
The authors extend the DnD Gesture corpus with 2,663 semantic gesture annotations spanning four types (iconic, metaphoric, deictic, discourse). They train a VQ-VAE to compress 3D upper-body motion into discrete tokens, with semantic type labels applied via cross-entropy classification loss during training. This creates semantically aligned gesture embeddings that are integrated with text and audio features using a Mixture-of-Experts framework with adaptive gating weights. The model predicts binary turn-taking labels (hold vs yield) based on Inter-Pausal Units with a 200ms silence threshold.

## Key Results
- Text+audio+gesture MoE model achieves 71.5 macro-F1 compared to 69.7 for text+audio alone
- Semantically aligned gesture embeddings outperform raw motion features in clustering and downstream performance
- MoE gating network shows increased weights for both gestures and text when using semantically aligned representations
- Gesture-only model achieves 66.2 F1, confirming multimodal integration is necessary

## Why This Works (Mechanism)

### Mechanism 1: Semantic Gesture Embedding Alignment
The VQ-VAE encoder creates gesture representations that cluster by communicative function rather than raw motion similarity. This clustering facilitates better alignment with linguistic and acoustic features in the MoE fusion. The core assumption is that gesture types systematically correlate with turn-taking behavior—deictic gestures signal turn-yielding while iconic gestures indicate floor-holding. Evidence shows semantically aligned embeddings form more distinct clusters than non-semantic variants.

### Mechanism 2: Adaptive Modality Weighting via Gating Network
The MoE gating network dynamically adjusts modality contributions based on context, allowing semantically enriched gestures to receive higher weights when they provide complementary turn-taking signals. The gating network computes softmax over learned weights, producing h_fused = Σ w_m · f_m(x_m). Improved cross-modal alignment from semantic gesture representations increases the model's confidence in both gesture and text modalities, raising their gating weights.

### Mechanism 3: Complementary Predictive Signal from Motion Dynamics
Body gesture features encode turn-taking signals not fully captured in speech acoustics or transcripts alone. 3D joint positions (normalized to pelvis, zero translation) capture gesture timing and form co-occurring with speech but providing independent predictive signal for hold vs yield classification. The core assumption is that human interlocutors systematically use gesture cues for turn management, and these patterns are learnable from motion data.

## Foundational Learning

- **Concept: VQ-VAE (Vector Quantized Variational Autoencoder)**
  - Why needed here: Core architecture for converting continuous 3D motion into discrete tokens processable by transformers and alignable with semantic labels
  - Quick check question: How does codebook quantization (z_q) differ from continuous embeddings, and why does discretization help with semantic alignment?

- **Concept: Mixture-of-Experts Fusion**
  - Why needed here: Framework for combining heterogeneous modalities with learned adaptive weighting rather than naive concatenation
  - Quick check question: How does softmax-based gating differ from early-fusion concatenation, and what are the gradient flow implications for each expert?

- **Concept: Transition Relevance Places (TRPs) and IPUs**
  - Why needed here: Task formulation depends on Inter-Pausal Units and the 200ms silence threshold for hold/yield labeling
  - Quick check question: Given an IPU ending with a 150ms pause before the same speaker continues, how is this labeled? What if a different speaker begins after 250ms?

## Architecture Onboarding

- **Component map:**
  WhisperX transcripts → mxbai-embed-large-v1; Wav2Vec2-large for audio; 3D skeleton joints (pelvis-normalized) → VQ-VAE encoder → quantized tokens → Transformer → mean pooling → MoE gating → fusion → binary classifier

- **Critical path:**
  Motion → VQ-VAE encoder → quantized tokens → Transformer → mean pooling → MoE gating → fusion → binary classifier
  Semantic supervision is applied during VQ-VAE training only; inference uses the pre-trained encoder

- **Design tradeoffs:**
  - Codebook size (256) vs granularity: Larger captures more motion detail but risks overfitting
  - Semantic loss weight (0.1) vs reconstruction quality: Higher improves clustering but may distort motion
  - MoE vs concat fusion: MoE provides interpretability via modality weights; concat shows yield-class F1 collapse
  - 200ms threshold: Standard but may miss rapid exchanges

- **Failure signatures:**
  - Gesture-only model: 66.2 F1 (insufficient alone—multimodal required)
  - Non-semantic embeddings: Poor clustering in Figure 4 visualizations
  - Concat/LMF fusion: Minority class F1 collapse
  - Annotation quality: κ = 0.52 limits semantic signal

- **First 3 experiments:**
  1. Baseline ablation: Reproduce Text+Audio (69.7 F1) and Text+Audio+Gesture without semantics (70.4 F1) to validate implementation
  2. Semantic vs non-semantic VQ-VAE: Train both variants, visualize t-SNE clustering, verify semantic version achieves ~71.5 F1 with improved cluster separation
  3. Modality weight analysis: Extract gating weights on test set, compare hold vs yield distributions, verify semantic gesture representations increase gesture weights as in Figure 5

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate annotation quality (Cohen's κ = 0.52) could limit effectiveness of semantic supervision
- DnD game context may produce gesture patterns not generalizable to natural conversation
- Critical implementation details like exact VQ-VAE encoder/decoder configurations are underspecified

## Confidence

- **High Confidence**: Multimodal turn-taking modeling benefits from gesture features (text+audio+gesture achieves 71.5 F1 vs 69.7 text+audio). Directly supported by ablation experiments with statistical significance (p < 0.05).

- **Medium Confidence**: Semantically aligned gesture embeddings provide better cross-modal alignment than raw motion features. Supported by visualization evidence and performance gains, but lacks independent validation of the clustering mechanism.

- **Low Confidence**: The specific adaptive weighting mechanism of the MoE gating network and the generalizability of DnD-specific gesture patterns to other conversational contexts. These claims rely heavily on the single dataset without external validation.

## Next Checks

1. **Independent Corpus Validation**: Apply the semantic gesture annotation protocol to a different conversational dataset (e.g., AMI meeting corpus) to verify that gesture type distributions systematically differ between hold and yield turns outside the DnD context.

2. **Ablation of Semantic Supervision**: Train the VQ-VAE with varying semantic loss weights (0.01, 0.1, 1.0) and measure both gesture embedding clustering quality and downstream turn-taking performance to establish the optimal semantic supervision tradeoff.

3. **Cross-Modal Alignment Analysis**: Quantify the cross-modal alignment between gesture embeddings and text/audio features using canonical correlation analysis or similar methods, comparing semantically aligned vs non-semantically aligned gesture representations.