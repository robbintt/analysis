---
ver: rpa2
title: Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders
arxiv_id: '2509.05766'
source_url: https://arxiv.org/abs/2509.05766
tags:
- data
- classification
- training
- autoencoders
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a hybrid anomaly detection framework, Autoencoder-PRC-RF,\
  \ that combines autoencoders with PRC random forests to simultaneously address class\
  \ imbalance and high dimensionality. Autoencoders filter anomalies from training\
  \ data by thresholding reconstruction errors, after which PRC trees\u2014guided\
  \ by area under the precision\u2013recall curve (AUPRC)\u2014construct an ensemble\
  \ classifier optimized for skewed class distributions."
---

# Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders

## Quick Facts
- **arXiv ID:** 2509.05766
- **Source URL:** https://arxiv.org/abs/2509.05766
- **Reference count:** 2
- **Primary result:** Hybrid framework combining autoencoders with PRC random forests shows consistent improvements over PRC-RF alone across three benchmark datasets.

## Executive Summary
This study introduces Autoencoder-PRC-RF, a hybrid anomaly detection framework that addresses class imbalance and high dimensionality by combining unsupervised autoencoder filtering with PRC-guided random forests. The autoencoder filters anomalous samples from training data based on reconstruction error thresholds, while PRC trees use AUPRC as the splitting criterion to optimize for minority class detection. Experiments on credit default, financial distress, and breast cancer datasets demonstrate consistent improvements in accuracy, recall, and F1 scores compared to PRC-RF alone.

## Method Summary
The framework operates through a sequential pipeline: first, an autoencoder is trained on the input data to identify and filter out anomalous samples based on reconstruction error thresholds. The cleaned data then serves as input for PRC random forests, where decision trees use AUPRC for feature selection and F1-score for split threshold determination. The ensemble approach leverages bootstrap aggregation, with trees trained on random subsets of the filtered data. The AUPRC criterion ensures sensitivity to minority class patterns under extreme imbalance, while the autoencoder reduces label noise and class confusion in the training data.

## Key Results
- Consistent F1 score improvements across all three benchmark datasets compared to PRC-RF alone
- Higher accuracy, recall, and F1 scores demonstrate superior anomaly detection performance
- Framework effectively handles high dimensionality and class imbalance simultaneously
- Validated on diverse datasets including credit default (3.7% minority), financial distress (6.1% minority), and breast cancer applications

## Why This Works (Mechanism)

### Mechanism 1
Autoencoder reconstruction error provides a computationally efficient proxy for identifying and filtering anomalous training samples before supervised learning. Autoencoders trained on mixed data learn to reconstruct the majority (normal) distribution accurately while producing elevated reconstruction errors on minority (anomalous) instances. By thresholding these errors, potentially mislabeled or noisy anomalies are removed from training data, yielding a cleaner dataset for downstream classification. Core assumption: The reconstruction error distribution differs sufficiently between normal and anomalous samples to enable separability via a single threshold.

### Mechanism 2
AUPRC-guided split selection produces decision trees that remain sensitive to minority class patterns under extreme imbalance. Traditional splitting criteria (Gini impurity, information gain) are dominated by majority class counts. AUPRC summarizes precision-recall trade-offs across all thresholds; maximizing AUPRC at each split biases the tree toward features that best separate the minority class regardless of its prevalence. Core assumption: Features with high AUPRC for the minority class indicate genuine signal rather than spurious correlations that will not generalize.

### Mechanism 3
Sequential ensemble design (unsupervised filter → supervised classifier) improves generalization by decoupling anomaly detection from classification. The autoencoder filters training data without using labels, reducing label noise and class confusion. The PRC-RF then trains on higher-quality data. This separation prevents the classifier from overfitting to contaminated labels while preserving the PRC framework's imbalance sensitivity. Core assumption: The autoencoder's unsupervised anomaly definition aligns with the downstream task's labeled minority class.

## Foundational Learning

- **Concept: Precision-Recall Curve and AUPRC**
  - **Why needed here:** The entire PRC-RF architecture uses AUPRC as its splitting criterion. Understanding how precision and recall trade off—and why AUPRC outperforms accuracy/AUC under imbalance—is essential for debugging poor splits.
  - **Quick check question:** If a dataset has 3% positive cases, what is the baseline AUPRC value a random classifier would achieve?

- **Concept: Autoencoder Reconstruction Loss (MSE vs. Cross-Entropy)**
  - **Why needed here:** Threshold selection depends on reconstruction error distribution. Different loss functions produce different error scales and distributions, directly affecting filtering aggressiveness.
  - **Quick check question:** For binary input features, which loss function is appropriate, and how would you normalize reconstruction errors for thresholding?

- **Concept: Bootstrap Aggregation in Random Forests**
  - **Why needed here:** PRC-RF builds an ensemble of PRC trees trained on bootstrap samples. Understanding how bootstrapping reduces variance helps diagnose when increasing tree count fails to improve performance.
  - **Quick check question:** If two PRC trees in the ensemble select completely different features for their root splits, is this a failure mode or expected behavior?

## Architecture Onboarding

- **Component map:** Raw Data → [Autoencoder Training] → Reconstruction Error Computation → Threshold Filter → Cleaned Data → [Bootstrap Sampling] × N_t trees → [PRC Tree Construction] → [Ensemble Aggregation] → Final Prediction

- **Critical path:** The autoencoder threshold (step 1) is the most sensitive hyperparameter. An overly aggressive threshold removes minority class samples; an overly permissive threshold leaves noise. This directly determines PRC-RF training data quality.

- **Design tradeoffs:**
  - Autoencoder architecture depth vs. training time: Deeper encoders capture more complex manifolds but risk overfitting to noise
  - Number of trees (N_t) vs. inference latency: More trees improve stability but linearly increase prediction time
  - Features per split (N_f) vs. tree diversity: Lower N_f increases tree diversity but may produce weaker individual trees

- **Failure signatures:**
  - Recall drops significantly after autoencoder filtering → threshold is too aggressive, removing legitimate minority samples
  - PRC-RF accuracy high but F1 near zero → minority class is being ignored; check AUPRC baseline vs. achieved values
  - Large variance across bootstrap repetitions → insufficient trees or unstable feature selection; increase N_t

- **First 3 experiments:**
  1. Threshold sensitivity analysis: Train autoencoder, plot reconstruction error distribution stratified by class label. Identify threshold range where minority class retention stays above 90%. Report filtered dataset class balance.
  2. Ablation study (AE vs. no-AE): Compare PRC-RF trained on (a) raw data, (b) autoencoder-filtered data, (c) autoencoder latent representations as features. Isolate which component drives performance gains.
  3. Baseline metric comparison: On the most imbalanced dataset (credit default, 3.7% minority), compare PRC-RF splits using AUPRC vs. Gini impurity vs. information gain. Report per-class recall and F1 to validate the paper's claim that AUPRC improves minority class sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
How does the specific thresholding strategy for autoencoder reconstruction errors impact the robustness of the PRC-RF training data? The paper describes threshold selection as "empirical analysis" without theoretical grounding. An overly aggressive threshold might remove valid minority class samples (information loss), while a lenient one might fail to denoise the data. Resolution requires an ablation study demonstrating model performance stability across reconstruction error percentiles (90th vs 95th vs 99th).

### Open Question 2
Does the dimensionality of the autoencoder's latent space significantly affect the feature selection efficacy of the PRC trees? The paper claims to address the "curse of dimensionality" via autoencoders but provides no analysis on how varying the compression ratio influences AUPRC-based splitting criteria. Resolution requires experiments comparing classification performance across models trained on varying numbers of latent features.

### Open Question 3
Is the sequential pipeline of filtering anomalies before training superior to a unified end-to-end gradient-descent approach? The sequential "filter-then-classify" approach may compound errors if the autoencoder's definition of "normal" does not perfectly align with the PRC-RF's classification boundary. Resolution requires comparative analysis against joint optimization where autoencoder reconstruction loss is influenced by classification error.

## Limitations
- Autoencoder architecture details (depth, latent dimension, training duration) remain unspecified, limiting exact replication
- No explicit threshold selection methodology provided; described only as "empirical analysis"
- Limited comparative baselines - only PRC-RF and basic logistic regression tested, with no deep learning alternatives

## Confidence
- **High confidence** in the conceptual framework: combining unsupervised filtering with AUPRC-guided supervised learning is theoretically sound
- **Medium confidence** in empirical results: reported improvements are consistent across datasets, but small sample sizes limit statistical power
- **Low confidence** in generalization claims: no ablation studies demonstrate which component drives improvements

## Next Checks
1. Perform threshold sensitivity analysis on each dataset, plotting minority class retention vs. reconstruction error percentile to identify optimal filtering point
2. Implement ablation study comparing: (a) PRC-RF on raw data, (b) PRC-RF on autoencoder-filtered data, (c) PRC-RF on autoencoder latent representations
3. Test AUPRC-guided splits against alternative imbalance-aware criteria (weighted Gini, focal loss trees) on the credit default dataset to validate AUPRC superiority claims