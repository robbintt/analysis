---
ver: rpa2
title: 'MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal
  Large Language Models'
arxiv_id: '2504.05782'
source_url: https://arxiv.org/abs/2504.05782
tags:
- reasoning
- multimodal
- knowledge
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDK12-Bench is a large-scale, multi-disciplinary benchmark for
  evaluating multimodal reasoning in large language models using real-world K-12 examination
  data across six subjects. It features 141K instances with detailed knowledge annotations,
  difficulty levels, and cross-year partitions.
---

# MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2504.05782
- **Source URL:** https://arxiv.org/abs/2504.05782
- **Reference count:** 40
- **Key result:** Current MLLMs achieve only ~59% accuracy on this multi-disciplinary K-12 benchmark, with significant drops under dynamic evaluation

## Executive Summary
MDK12-Bench is a large-scale, multi-disciplinary benchmark for evaluating multimodal reasoning in large language models using real-world K-12 examination data across six subjects. It features 141K instances with detailed knowledge annotations, difficulty levels, and cross-year partitions. The authors also propose a dynamic evaluation framework that mitigates data contamination through bootstrapping strategies applied to both text and images. Experimental results show that current models struggle with multimodal reasoning, with top performers like Gemini2-thinking and QVQ-72B achieving only around 59.4% and 53.2% accuracy respectively. Dynamic evaluation further challenges models by altering question forms and visual contexts, revealing sensitivity to contextual changes.

## Method Summary
The benchmark consists of 141,320 K-12 examination questions from six disciplines, with 105,218 images accompanying 63,463 multimodal instances. Questions are annotated with 6,827 knowledge points mapped to a 6-level curriculum structure. The evaluation pipeline involves inputting questions to MLLMs, parsing responses using GPT-based interpreters to extract answers, and comparing with ground truth. Dynamic evaluation applies textual bootstrapping (paraphrasing, type permutation) and visual bootstrapping (spatial transforms, style transfer via Flux-Dev) to challenge model robustness. A stratified MDK12-Mini subset enables rapid iteration.

## Key Results
- Top models like Gemini2-thinking achieve only 59.4% accuracy on the full benchmark
- Performance drops significantly under dynamic evaluation (e.g., ~22.9% drop for easy tasks)
- Models show particular difficulty with high school and hard difficulty questions
- Visual reasoning capability is identified as a key bottleneck for current MLLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained, hierarchical knowledge annotation enables diagnostic evaluation of specific reasoning failures rather than just aggregate accuracy.
- **Mechanism:** By mapping 140K+ questions to a 6-level knowledge structure (Discipline → Key Knowledge Point), the system aggregates model performance by specific concepts (e.g., "Newton's Second Law"). This isolates whether a failure is due to visual perception, lack of domain knowledge, or reasoning logic.
- **Core assumption:** The model's failure on a specific question is primarily attributable to the annotated knowledge point, rather than unrelated linguistic ambiguities or visual artifacts.
- **Evidence anchors:** [abstract] "6,827 instance-level knowledge point annotations based on a well-organized knowledge structure" [section: Data Processing] "...linking each translated question to this framework to enrich its academic context." [figure 4] Visualizes accuracy ranked by specific knowledge points, showing distinct high/low performing concepts.

### Mechanism 2
- **Claim:** Dynamic bootstrapping of visual and textual inputs exposes reliance on memorization (data contamination) rather than genuine reasoning.
- **Mechanism:** The framework applies perturbations—paraphrasing text or style-transferring images—while preserving semantic meaning and ground truth. A discrepancy between performance on original vs. perturbed samples indicates memorization of surface forms or specific visual styles rather than robust understanding.
- **Core assumption:** The perturbation methods (e.g., Flux-Dev style transfer, word substitution) successfully preserve the original question's difficulty and logical requirements.
- **Evidence anchors:** [abstract] "...dynamic evaluation framework mitigates data contamination issues by bootstrapping question forms..." [table 4] Shows significant accuracy drops (e.g., Gemini2-thinking dropping ~16.5% overall) when moving from Original to Dynamic settings.

### Mechanism 3
- **Claim:** Grade-level partitioning (Elementary to High School) provides a structured curriculum for measuring reasoning complexity scaling.
- **Mechanism:** The dataset separates questions by grade and difficulty. This allows researchers to observe if models fail specifically at higher cognitive loads (High School/Hard) which require multi-step synthesis, vs. lower grades which test basic retrieval/recognition.
- **Core assumption:** Human-defined grade levels correlate linearly with computational reasoning difficulty for current MLLMs.
- **Evidence anchors:** [figure 6] Demonstrates accuracy breakdown across Elementary, Middle, and High school, generally showing decreased performance as grade increases. [table 3] Details performance drops across Easy, Medium, and Hard subsets.

## Foundational Learning

- **Concept: Multimodal Reasoning vs. Perception**
  - **Why needed here:** The paper explicitly differentiates "low-order" tasks (perception/OCR) from "high-order" reasoning (multi-step logic).
  - **Quick check question:** Does the model need to identify an object, or deduce a consequence based on the object's properties?

- **Concept: Data Contamination**
  - **Why needed here:** The core motivation for the "Dynamic Evaluation" framework is that static benchmarks might already be in the training set, inflating scores.
  - **Quick check question:** If a model scores 100% on a test, how can you verify it didn't memorize the answers during training?

- **Concept: Chain-of-Thought (CoT) in Evaluation**
  - **Why needed here:** The paper uses models like Gemini2-thinking and evaluates "reasoning instances." Understanding how CoT affects the evaluation of "high-order" tasks is crucial.
  - **Quick check question:** Does the benchmark require the model to output the final answer only, or the intermediate reasoning steps?

## Architecture Onboarding

- **Component map:** JSON dataset → GPT answer parser → MLLM → Ground truth comparison
- **Critical path:** 1. Ingestion: Load instance (Q, Image, Meta). 2. Bootstrapping (Optional): Apply transformations via Flux-Dev or LLM-paraphrasing. 3. Inference: Query MLLM. 4. Judging: Use GPT-based parser to extract answer from model output; compare to Ground Truth.
- **Design tradeoffs:** Mini vs. Full Set: Using MDK12-Mini reduces compute but may miss edge-case knowledge points. Dynamic Eval: Increases evaluation robustness but adds latency/instability due to generative bootstrapping (e.g., style transfer might corrupt text in images).
- **Failure signatures:** High Static, Low Dynamic: Indicates rote memorization (contamination) or brittleness to distribution shifts. Text-Only > Multimodal: Indicates the visual encoder is adding noise or failing to extract relevant signals (visual hallucination).
- **First 3 experiments:** 1. Baseline Profiling: Run the target model on MDK12-Mini to establish accuracy across the 6 disciplines and 3 difficulty levels. 2. Ablation on Modalities: Compare performance on Text-Only instances vs. Multimodal instances to isolate visual reasoning capability. 3. Perturbation Sensitivity: Apply the "Textual Bootstrapping" only, then "Visual Bootstrapping" only, to determine which input channel causes the largest performance degradation (diagnosing brittleness).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can MLLM architectures or training paradigms be modified to improve resilience to the combined textual and visual perturbations used in dynamic evaluation?
- **Basis in paper:** [explicit] The conclusion states, "Future work may focus on improving models’ resilience to dynamic perturbations of question formats, thereby paving the way toward more robust multimodal reasoning models."
- **Why unresolved:** Current SOTA models show significant accuracy drops (e.g., Gemini2-thinking dropping 22.9% on easy tasks) when subjected to the paper's bootstrapping strategies.
- **What evidence would resolve it:** New training methodologies that maintain consistent performance between original and dynamically bootstrapped MDK12-Mini subsets.

### Open Question 2
- **Question:** What specific mechanistic failures cause reasoning-oriented models to be more sensitive to dynamic perturbations on "easy" tasks compared to "hard" ones?
- **Basis in paper:** [inferred] The paper notes the counter-intuitive finding that "High-order reasoning models are particularly sensitive to easier tasks under dynamic conditions," suggesting they rely heavily on precise context for simple memorization.
- **Why unresolved:** The paper identifies the performance gap but does not investigate the underlying attention or reasoning mechanisms that cause high-performing models to falter on simpler, perturbed inputs.
- **What evidence would resolve it:** An ablation study analyzing attention maps or internal state shifts for easy vs. hard tasks under textual bootstrapping.

### Open Question 3
- **Question:** Can the fine-grained knowledge point annotations in MDK12-Bench be used to isolate whether MLLM failures stem from visual misinterpretation or logical reasoning deficits?
- **Basis in paper:** [explicit] The introduction states that previous benchmarks "fail to trace how MLLMs fail in solving certain problems" due to a lack of fine-grained annotations, a gap this benchmark aims to fill.
- **Why unresolved:** While the benchmark provides the necessary structured annotations, the paper focuses on overall accuracy rather than a granular diagnostic decomposition of error types.
- **What evidence would resolve it:** A diagnostic study correlating failure rates on specific knowledge points (Level 5 & 6) with separate tests for pure visual recognition and text-only logical deduction.

## Limitations

- **Annotation quality dependency:** The benchmark's diagnostic capabilities depend heavily on the accuracy and consistency of the 6,827 knowledge point annotations.
- **Grade-level correlation assumption:** The assumption that human-defined grade levels directly correlate with MLLM reasoning complexity may not hold if models have skewed pre-training distributions.
- **GPT-based validation circularity:** The dynamic evaluation framework relies on GPT-based validation of augmented instances, which may introduce circularity if the judge shares training data with evaluated models.

## Confidence

- **High Confidence:** The dataset construction methodology (real K-12 exams, systematic annotation, cross-year partitioning) is sound and well-documented.
- **Medium Confidence:** The dynamic evaluation framework meaningfully reduces data contamination effects, though exact quantification requires independent validation.
- **Medium Confidence:** The knowledge annotation structure enables diagnostic evaluation, contingent on annotation quality.

## Next Checks

1. **Annotation Quality Validation:** Manually verify 100 randomly selected knowledge point annotations for accuracy and consistency across disciplines.

2. **Dynamic Evaluation Robustness:** Run ablation studies comparing performance drops under different perturbation types to determine which transformations preserve semantic meaning while challenging models.

3. **Cross-Curriculum Transfer:** Test model performance on questions from one grade level using knowledge points annotated at different levels to assess whether the curriculum structure meaningfully captures reasoning complexity.