---
ver: rpa2
title: Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps
arxiv_id: '2503.22162'
source_url: https://arxiv.org/abs/2503.22162
tags:
- agent
- agents
- multi-agent
- path
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Multi-Agent Pathfinding (MAPF)
  in incomplete or frequently changing environments, where traditional centralized
  planning and pure reinforcement learning methods struggle to balance global solution
  quality and local flexibility. The authors propose a hybrid framework that integrates
  D Lite global search with multi-agent reinforcement learning (MARL), using a switching
  mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded
  settings.
---

# Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps

## Quick Facts
- arXiv ID: 2503.22162
- Source URL: https://arxiv.org/abs/2503.22162
- Authors: Ning Liu; Sen Shen; Xiangrui Kong; Hongtao Zhang; Thomas Bräunl
- Reference count: 40
- This paper proposes a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning to improve Multi-Agent Pathfinding in dynamic environments.

## Executive Summary
This paper addresses the challenge of Multi-Agent Pathfinding (MAPF) in incomplete or frequently changing environments, where traditional centralized planning and pure reinforcement learning methods struggle to balance global solution quality and local flexibility. The authors propose a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning (MARL), using a switching mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded settings. The framework employs a shared exploration map mechanism where agents exchange only incremental environmental changes, reducing communication overhead while maintaining comprehensive spatial awareness. Experiments conducted in both the POGEMA discrete environment and the EyeSim physical platform demonstrate that the proposed approach significantly improves success rate, collision rate, and path efficiency, particularly in scenarios with large-scale agent deployments and frequent environmental perturbations.

## Method Summary
The proposed framework combines global path planning using D* Lite with local decision-making through multi-agent reinforcement learning. The system uses a shared exploration map where agents communicate only incremental environmental changes rather than full maps, reducing bandwidth requirements. A switching mechanism dynamically determines when to use global planning versus local RL-based decisions based on environmental conditions and agent density. The freeze-prevention strategy helps agents escape deadlocks in crowded scenarios. The hybrid approach aims to leverage the global optimality of D* Lite while maintaining the adaptability of RL methods in dynamic environments.

## Key Results
- Significant improvement in success rate and collision reduction compared to baseline methods in controlled experiments
- Effective performance in both discrete POGEMA environment and physical EyeSim platform
- Demonstrated balance between global optimality and local adaptability in dynamic MAPF scenarios
- Reduced communication overhead through incremental environmental updates rather than full map sharing

## Why This Works (Mechanism)
The hybrid approach works by combining the strengths of global planning and local learning. D* Lite provides robust global path planning that can handle dynamic obstacles through incremental replanning, while MARL enables agents to make quick local decisions when global replanning would be too costly. The switching mechanism allows the system to adapt to different environmental conditions - using global planning in stable or less crowded areas and switching to local RL-based decisions in highly dynamic or congested regions. The shared exploration map with incremental updates maintains spatial awareness across agents while minimizing communication overhead, and the freeze-prevention strategy addresses the common deadlock problem in crowded multi-agent scenarios.

## Foundational Learning
- **D* Lite algorithm**: Incremental heuristic search for dynamic environments; needed for efficient global replanning when obstacles change; quick check: verify it handles edge cost updates in O(1) time
- **Multi-agent reinforcement learning**: Decentralized decision-making for local navigation; needed for fast responses to immediate obstacles; quick check: confirm convergence in partially observable settings
- **Shared exploration maps**: Distributed spatial representation; needed to maintain global awareness with minimal communication; quick check: measure information loss vs. bandwidth savings
- **Switching mechanisms**: Dynamic algorithm selection; needed to balance global optimality and local adaptability; quick check: test switching thresholds under varying agent densities
- **Freeze prevention strategies**: Deadlock resolution in crowded scenarios; needed to avoid infinite waiting loops; quick check: verify escape from all known deadlock patterns
- **Incremental communication**: Sparse environmental updates; needed to reduce bandwidth requirements; quick check: benchmark update frequency vs. path quality degradation

## Architecture Onboarding

Component Map:
Central Controller -> D* Lite Global Planner -> MARL Local Controller -> Agent Actuators
                              ↓
                       Shared Exploration Map
                              ↓
                       Communication Layer

Critical Path:
Agent perception → Shared map update → Switching mechanism decision → D* Lite/MARL execution → Actuator control

Design Tradeoffs:
- Global planning vs. local learning: Global D* Lite provides optimal paths but struggles with dynamic changes; local MARL adapts quickly but may produce suboptimal solutions
- Communication overhead vs. spatial awareness: Full map sharing provides complete information but is bandwidth-intensive; incremental updates reduce overhead but may miss critical changes
- Centralized vs. decentralized control: Central planning ensures global coordination but creates single points of failure; distributed control increases robustness but may cause conflicts

Failure Signatures:
- Frequent switching oscillations between D* Lite and MARL
- Deadlock situations where agents continuously switch without progress
- Communication delays causing outdated shared map information
- MARL agents getting stuck in local minima despite global path availability

First Experiments to Run:
1. Single-agent dynamic obstacle navigation to validate D* Lite replanning performance
2. Multi-agent static environment test to establish baseline MARL collision rates
3. Hybrid system validation in controlled dynamic scenario with known obstacle patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation primarily focuses on grid-based discrete environments and a single physical platform (EyeSim), limiting generalizability to continuous or more complex real-world settings
- Communication overhead analysis remains theoretical with limited empirical data on bandwidth requirements and latency in large-scale deployments
- The switching mechanism between D* Lite and MARL lacks explicit criteria for decision thresholds, potentially introducing instability in edge cases

## Confidence

**Major Claims Confidence:**
- **High**: Improvement in success rate and collision reduction compared to baseline methods (supported by controlled experiments)
- **Medium**: Effectiveness of shared exploration map mechanism (demonstrated but not extensively stress-tested)
- **Medium**: Balance between global optimality and local adaptability (qualitatively assessed, quantitative trade-off analysis limited)

## Next Checks
1. Conduct scalability testing with 100+ agents in heterogeneous environments to verify communication overhead claims
2. Implement ablation studies to quantify individual contributions of the switching mechanism and freeze-prevention strategy
3. Test framework robustness in environments with varying communication delays and packet loss rates to assess real-world applicability