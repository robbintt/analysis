---
ver: rpa2
title: 'TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional
  Reduction'
arxiv_id: '2512.06694'
source_url: https://arxiv.org/abs/2512.06694
tags:
- topic
- topics
- clustering
- embeddings
- topiclear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of extracting coherent topics
  from short, informal social media texts, which traditional models struggle with
  due to limited word co-occurrence and informal language. The proposed method, TopiCLEAR,
  uses Sentence-BERT to embed documents into semantic vectors, followed by iterative
  clustering with Gaussian Mixture Models and adaptive dimensionality reduction using
  linear discriminant analysis to improve topic separability.
---

# TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction

## Quick Facts
- arXiv ID: 2512.06694
- Source URL: https://arxiv.org/abs/2512.06694
- Reference count: 40
- The proposed TopiCLEAR method significantly outperforms seven baseline methods, including LDA and BERTopic, on Adjusted Rand Index scores for topic extraction from short, informal social media texts.

## Executive Summary
TopiCLEAR addresses the challenge of extracting coherent topics from short, informal social media texts, where traditional models like LDA struggle due to limited word co-occurrence and informal language. The method uses Sentence-BERT to embed documents into semantic vectors, followed by iterative clustering with Gaussian Mixture Models and adaptive dimensionality reduction using linear discriminant analysis to improve topic separability. Evaluated across four datasets (20News, AgNewsTitle, Reddit, and TweetTopic), TopiCLEAR achieved the highest Adjusted Rand Index (ARI) scores, significantly outperforming seven baseline methods, including LDA, BERTopic, and a generative AI approach. The method also produced more interpretable topics, with qualitative analysis showing better alignment with human annotations and clearer topic distinctions compared to LDA.

## Method Summary
TopiCLEAR combines Sentence-BERT embeddings with iterative Gaussian Mixture Model clustering and adaptive dimensionality reduction using Linear Discriminant Analysis (LDA). The process begins by embedding documents into semantic vectors, then iteratively clusters these vectors, reducing dimensionality to enhance topic separability at each step. This approach improves both clustering accuracy and topic interpretability compared to traditional methods like LDA and modern embedding-based models like BERTopic, especially for short and informal texts where word co-occurrence is sparse.

## Key Results
- TopiCLEAR achieved the highest Adjusted Rand Index (ARI) scores across four benchmark datasets, outperforming seven baseline methods.
- The method demonstrated superior interpretability, with topics more aligned with human annotations and clearer distinctions than LDA.
- TopiCLEAR maintained robustness under varying noise conditions, with minimal performance degradation when ground truth labels were partially corrupted.

## Why This Works (Mechanism)
TopiCLEAR leverages Sentence-BERT to capture semantic meaning in short texts, where traditional bag-of-words models fail due to sparse word co-occurrence. By iteratively clustering and applying adaptive dimensionality reduction, the method enhances topic separability, making clusters more distinct and interpretable. This iterative refinement process addresses the challenge of informal language and limited context in social media texts, resulting in more coherent and meaningful topics.

## Foundational Learning
- **Sentence-BERT embeddings**: Transform text into dense semantic vectors; needed because traditional models fail on short, informal texts; quick check: ensure embeddings capture meaning beyond word co-occurrence.
- **Gaussian Mixture Model clustering**: Probabilistic clustering for overlapping topic boundaries; needed for handling uncertainty in short text clustering; quick check: assess cluster overlap and separation.
- **Linear Discriminant Analysis (LDA)**: Reduces dimensionality while maximizing class separability; needed to enhance topic distinction in embedding space; quick check: verify dimensionality reduction improves clustering metrics.
- **Adjusted Rand Index (ARI)**: Evaluates clustering quality against ground truth; needed because standard coherence metrics fail on short texts; quick check: confirm ARI correlates with human judgment.
- **Iterative refinement**: Repeated clustering and dimensionality reduction; needed to progressively improve topic separability; quick check: monitor convergence and stability across iterations.

## Architecture Onboarding

**Component Map**
Sentence-BERT -> Gaussian Mixture Model -> Linear Discriminant Analysis -> Clustering Evaluation

**Critical Path**
1. Embed documents using Sentence-BERT
2. Iteratively cluster embeddings with GMM
3. Apply adaptive LDA for dimensionality reduction
4. Evaluate clustering with ARI and interpretability scores

**Design Tradeoffs**
- Fixed *K* (number of topics) limits unsupervised applicability but ensures consistency.
- Adaptive dimensionality reduction improves separability but increases computational cost.
- Iterative refinement enhances performance but requires careful convergence monitoring.

**Failure Signatures**
- Poor ARI scores may indicate inadequate embedding quality or suboptimal *K*.
- Unstable clustering across iterations suggests sensitivity to initialization or noise.
- Low interpretability despite high ARI may reflect semantic drift in reduced dimensions.

**First Experiments**
1. Compare ARI scores across different *K* values to assess sensitivity.
2. Evaluate performance with fixed PCA versus adaptive LDA for dimensionality reduction.
3. Test robustness by varying noise levels in ground truth labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the method be extended to automatically determine the optimal number of topics ($K$) rather than requiring it as a fixed input?
- Basis in paper: [explicit] Section 3 states that in the proposed method, "the number of topics K is assumed to be given."
- Why unresolved: The current implementation relies on a pre-defined $K$, limiting its applicability in fully unsupervised real-world scenarios where the number of latent topics is unknown.
- What evidence would resolve it: An extension of the algorithm that iteratively optimizes $K$ using criteria like the Bayesian Information Criterion (BIC) or silhouette analysis, validated on datasets with unknown topic structures.

### Open Question 2
- Question: How can topic quality be reliably evaluated for short texts in the absence of ground truth labels?
- Basis in paper: [inferred] The study demonstrates that standard coherence metrics (e.g., NPMI, $C_v$) show poor correlation with noise levels in short texts, yet the authors rely on Adjusted Rand Index (ARI) which requires human-annotated labels.
- Why unresolved: While the paper exposes the failure of coherence metrics for short texts, it leaves a gap for practitioners who need to evaluate models on unlabeled social media data.
- What evidence would resolve it: The development of a new reference-free evaluation metric that correlates strongly with human judgment for short, informal text, similar to how ARI behaved in the label noise experiments.

### Open Question 3
- Question: How does TopiCLEAR compare to generative AI approaches capable of true unsupervised discovery?
- Basis in paper: [explicit] The authors note that the Generative AI baseline (Gemini) "used topic information" for zero-shot classification, which "cannot be considered a fair comparison" to the unsupervised clustering performed by TopiCLEAR.
- Why unresolved: It remains unclear if TopiCLEAR's clustering performance exceeds that of LLMs specifically prompted or fine-tuned to identify topics without being provided with label names.
- What evidence would resolve it: A comparative study using an LLM baseline that infers topics and labels entirely from the data distribution without seed labels.

## Limitations
- The evaluation relies on datasets with ground truth labels, limiting applicability to unlabeled or dynamic social media streams.
- No sensitivity analysis for hyperparameters (e.g., number of clusters *k*, LDA component count) or assessment of robustness across random seeds.
- Performance is measured primarily via ARI and interpretability, but computational cost and scalability for large-scale corpora are not reported.

## Confidence
- **High**: Claims about TopiCLEAR outperforming baselines on ARI and producing more interpretable topics on the evaluated datasets.
- **Medium**: Claims about superiority over generative AI baselines and robustness across dataset types.
- **Low**: Generalizability to unlabeled or streaming data, and comparisons to newer or proprietary models not included in the study.

## Next Checks
1. Conduct ablation studies to assess the impact of adaptive dimensionality reduction versus fixed PCA or no reduction.
2. Evaluate model performance and stability across multiple random seeds and on additional social media datasets without ground truth labels.
3. Benchmark computational efficiency and scalability on datasets an order of magnitude larger than those tested.