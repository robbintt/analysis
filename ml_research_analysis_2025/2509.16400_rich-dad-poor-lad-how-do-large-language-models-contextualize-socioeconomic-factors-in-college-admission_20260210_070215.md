---
ver: rpa2
title: '''Rich Dad, Poor Lad'': How do Large Language Models Contextualize Socioeconomic
  Factors in College Admission ?'
arxiv_id: '2509.16400'
source_url: https://arxiv.org/abs/2509.16400
tags:
- quintile
- tier
- llms
- admit
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) consistently favor low-SES applicants\
  \ in college admissions, even when controlling for academic performance, and Chain-of-Thought\
  \ prompting amplifies this tendency by explicitly invoking SES as compensatory justification.\
  \ Across 4 models and 30,000 synthetic profiles, LLMs admitted 4\u20138\xD7 more\
  \ low-SES applicants in top SES quintiles and showed higher decision volatility\
  \ for disadvantaged students under deliberative reasoning."
---

# 'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?

## Quick Facts
- arXiv ID: 2509.16400
- Source URL: https://arxiv.org/abs/2509.16400
- Authors: Huy Nghiem; Phuong-Anh Nguyen-Le; John Prindle; Rachel Rudinger; Hal Daumé
- Reference count: 40
- Key outcome: Large language models (LLMs) consistently favor low-SES applicants in college admissions, even when controlling for academic performance, and Chain-of-Thought prompting amplifies this tendency by explicitly invoking SES as compensatory justification. Across 4 models and 30,000 synthetic profiles, LLMs admitted 4–8× more low-SES applicants in top SES quintiles and showed higher decision volatility for disadvantaged students under deliberative reasoning. Mixed-effects models confirm SES features (fee waiver, first-gen) as top predictors after academic performance, with effects persisting in both fast (System 1) and slow (System 2) reasoning modes. The study introduces DPAF, a dual-process audit framework to systematically compare LLM outcomes and explanations in sensitive domains. Results suggest LLMs' equity-driven tendencies diverge from real-world holistic guidelines, highlighting the need for transparent auditing before deployment in high-stakes education decisions.

## Executive Summary
This paper investigates how large language models (LLMs) handle socioeconomic status (SES) in simulated college admissions. Using a dual-process framework that mirrors human cognitive systems (fast, intuitive vs. slow, deliberative), the authors systematically audit LLM decisions on 30,000 synthetic applicant profiles. They find that LLMs consistently favor low-SES applicants, even when controlling for academic performance, and that Chain-of-Thought prompting amplifies this tendency by explicitly invoking SES as compensatory justification. The study introduces DPAF (Dual-Process Audit Framework) to compare LLM outcomes and explanations in sensitive domains, revealing higher decision volatility for disadvantaged students under deliberative reasoning. These results suggest that LLMs' equity-driven tendencies diverge from real-world holistic guidelines, highlighting the need for transparent auditing before deployment in high-stakes education decisions.

## Method Summary
The study uses 30,000 synthetic college applicant profiles generated with correlations to real-world data (e.g., SAT-income ~0.4, GPA-income ~0.15). Four open-source LLMs (Qwen2 7B, Mistral 7B v0.3, Gemma 2 9B, Llama 3.1 7B) are evaluated across 60 institutions using two prompt variants: System 1 (decision-only) and System 2 (Chain-of-Thought with JSON explanations). A subset of decisions is tagged by GPT-4o-mini for SES, academic, and extracurricular factors. Mixed-effects logistic regression quantifies the impact of SES features while controlling for institution, prompt variant, and random effects. The DPAF framework systematically compares outcomes and explanations between fast and slow reasoning modes.

## Key Results
- LLMs admitted 4–8× more low-SES applicants in top SES quintiles, even when controlling for academic performance.
- Chain-of-Thought prompting amplifies low-SES preference by explicitly invoking SES as compensatory justification, with 66.8% of first-gen and 43.9% of fee waiver mentions in explanations supporting disadvantaged applicants.
- Higher decision volatility for low-SES applicants under System 2 deliberation: reject-to-admit flips concentrate in low-SES quintiles, while admit-to-reject flips concentrate in high-SES quintiles at selective institutions.
- Mixed-effects models confirm SES features (fee waiver, first-gen) as top predictors after academic performance, with effects persisting in both System 1 and System 2 modes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct SES markers (fee waiver, first-generation status) act as strong positive predictors in LLM admission decisions, independent of academic performance.
- Mechanism: LLMs treat binary SES indicators as presence checks—when present, they trigger "support" tags in reasoning; when absent, they trigger "penalize" tags. Mixed-effects models show fee waiver increases admission odds 1.86–5.87× and first-gen status 1.89–10.30× across models.
- Core assumption: LLMs encode equity-oriented priors from training data that explicitly reward socioeconomic disadvantage markers.
- Evidence anchors:
  - [abstract]: "LLMs consistently favor low-SES applicants—even when controlling for academic performance"
  - [section 4.2.2]: "LLMs admit applicants who are eligible for fee waiver with odds 1.86 to 5.87 times higher to those who are not"
  - [corpus]: No direct corpus evidence; neighbor papers focus on log anomaly detection, not fairness/SES reasoning.
- Break condition: If academic performance quintile is very low (perf_quintile 1) and SES markers are present, admission still unlikely—SES compensation is bounded.

### Mechanism 2
- Claim: Chain-of-Thought prompting amplifies low-SES preference by explicitly invoking hardship as compensatory justification.
- Mechanism: System 2 deliberation generates explanations that cite SES factors (66.8% for first-gen, 43.9% for fee waiver). When academic credentials are borderline, `ses_compensates=True` flag activates, enabling reject→admit flips more frequently for low-SES applicants.
- Core assumption: CoT exposes latent reasoning that would otherwise remain implicit in decision-only outputs.
- Evidence anchors:
  - [abstract]: "Chain-of-Thought prompting amplifies this tendency by explicitly invoking SES as compensatory justification"
  - [section 5.3.2]: "LLMs exhibit reasoning tradeoff when deliberating academically borderline profiles...SES factors played an active role in justifying the acceptance of low-performing applicants"
  - [corpus]: Weak/missing—corpus neighbors do not address CoT fairness effects.
- Break condition: High-SES applicants with identical academic profiles receive fewer "rescue" flips; compensation is directionally biased toward disadvantage.

### Mechanism 3
- Claim: Dual-process reasoning (System 1 vs. System 2) produces higher decision volatility for low-SES applicants.
- Mechanism: System 2's deliberative mode triggers more decision reversals overall, but flip rates are asymmetric—reject→admit flips concentrate in low-SES quintiles, while admit→reject flips concentrate in high-SES quintiles at selective institutions.
- Core assumption: Deliberation increases sensitivity to contextual factors, which disproportionately benefits applicants with disadvantage markers.
- Evidence anchors:
  - [abstract]: "higher decision volatility for disadvantaged students under deliberative reasoning"
  - [section 5.2.1]: "flip rates are consistently higher for low-SES applicants, particularly in reject-to-admit cases"
  - [corpus]: Not directly addressed in corpus.
- Break condition: Volatility decreases as SES quintile increases—high-SES applicants show stable decisions across systems.

## Foundational Learning

- Concept: Dual-process theory (System 1/System 2)
  - Why needed here: Paper explicitly frames LLM decision-only outputs as "fast" System 1 and CoT-explained outputs as "slow" System 2, mirroring Kahneman's cognitive framework.
  - Quick check question: Can you explain why prompting for explanations (System 2) might change final decisions, not just add post-hoc rationalization?

- Concept: Mixed-effects logistic regression
  - Why needed here: Used to isolate SES feature effects while controlling for institution, prompt variant, and attribute ordering random effects.
  - Quick check question: Why include random intercepts for institution rather than treating institution as a fixed effect?

- Concept: Chain-of-Thought prompting and faithfulness
  - Why needed here: Paper assumes CoT explanations reveal actual reasoning, but acknowledges faithfulness risks (section 7.2); critical for interpreting "ses_compensates" flags.
  - Quick check question: What evidence would suggest CoT explanations are post-hoc rationalizations rather than faithful reasoning traces?

## Architecture Onboarding

- Component map:
  1. Synthetic data generator → creates 30K profiles with income-correlated GPA/SAT/extracurriculars
  2. System 1 prompter → decision-only ("admit"/"reject") across 60 institutions
  3. System 2 prompter → CoT-explained decisions with JSON output
  4. GPT-4o-mini tagger → annotates explanations (support/penalize/discount/null)
  5. Mixed-effects analyzer → extracts odds ratios for SES features

- Critical path: Data generation (preserve real-world correlations) → System 1/2 prompting (consistent seeds) → tagging validation (human agreement α=0.71) → statistical analysis

- Design tradeoffs:
  - Synthetic data enables scale but may miss real-world edge cases (essays, recommendations omitted)
  - 7–9B open-source models constrain generalization to larger proprietary models
  - Automated tagging trades off scale vs. annotation depth

- Failure signatures:
  - Llama's near-zero admission rates in System 1 (safe non-compliance alignment)
  - Gemma's drastic sensitivity to acceptance rate specification vs. omission
  - Low ZIP code citation rate (5.1%) despite inclusion in SES index

- First 3 experiments:
  1. Replicate System 1 vs. System 2 on 1,000 profiles with your target model; compare flip rates across SES quintiles.
  2. Ablate fee waiver and first-gen features individually; measure odds ratio degradation to isolate relative importance.
  3. Apply DPAF to a different high-stakes domain (e.g., hiring, loan approval) using the same dual-process structure; test generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of qualitative application components, such as personal statements and essays, alter LLMs' SES-based favoritism and compensatory reasoning?
- Basis in paper: [explicit] The authors state in the Limitations section that a "full college application also contains other important components, such as statements and college essays," and explicitly suggest that "future research should incorporate this component into applicants’ profiles to complete analysis."
- Why unresolved: The current study relies solely on structured profile data (GPA, SAT, activities), omitting the unstructured text data that plays a critical role in real-world holistic review.
- What evidence would resolve it: Applying the DPAF framework to a dataset including synthetic or anonymized essays to observe if SES "rescue" patterns persist or if textual content overrides demographic signals.

### Open Question 2
- Question: Do the observed equity-oriented biases and "rescuing" behaviors persist when LLMs evaluate real-world applicant profiles rather than synthetic ones?
- Basis in paper: [explicit] The paper acknowledges that the "synthetic nature precludes the ability to capture the full spectrum of inter-variable dependencies" and encourages "other researchers with such access to validate the generalization of our findings."
- Why unresolved: The study uses synthetic data grounded in correlations (e.g., income-SAT ~0.4), which may not capture the complexity, noise, or intersectionality present in actual applicant pools.
- What evidence would resolve it: Conducting a similar audit using historical, anonymized admission data from real institutions to compare model decision boundaries against actual human admission outcomes.

### Open Question 3
- Question: To what extent are the System 2 Chain-of-Thought justifications faithful to the models' internal reasoning processes versus being post-hoc rationalizations?
- Basis in paper: [explicit] The authors caution in the Limitations section regarding "Explanation faithfulness," noting that "faithfulness to the model’s true internal mechanism and robustness is still an area of active research."
- Why unresolved: DPAF treats CoT as an audit target, but if the generated reasoning is unfaithful, the framework might only measure the model's ability to generate plausible social justifications rather than its actual decision logic.
- What evidence would resolve it: Causal intervention experiments where specific input features are altered to see if the verbal justification changes consistently with the decision, or probing internal attention heads to verify alignment with generated text.

### Open Question 4
- Question: Do larger, proprietary state-of-the-art models (e.g., GPT-4, Claude) exhibit similar "safe non-compliance" or SES-based preference amplification as the 7–9B parameter open-source models tested?
- Basis in paper: [explicit] The authors note their "selection of 4 open-source LLMs... is necessitated by computational constraints" and that "models from different family and scale may exhibit behaviors incongruent with those observed in our study."
- Why unresolved: The study is restricted to smaller open-source models; it is unknown if the observed volatility (e.g., Llama's rejection patterns or Gemma's leniency) scales with model size or differs in proprietary systems.
- What evidence would resolve it: Running the DPAF audit on larger parameter models or API-based proprietary models to compare the magnitude of SES coefficients and decision flip rates.

## Limitations

- The study uses synthetic data that omits qualitative application components like essays and recommendations, limiting ecological validity.
- Results are based on 7–9B parameter open-source models, raising questions about generalizability to larger proprietary systems.
- The assumption that Chain-of-Thought explanations faithfully represent reasoning (rather than post-hoc rationalization) remains unverified through human annotation of final decisions.

## Confidence

- **High confidence**: The core finding that LLMs favor low-SES applicants when academic performance is controlled, supported by mixed-effects models across 4 models and 30,000 profiles.
- **Medium confidence**: The amplification effect of Chain-of-Thought prompting, as the mechanism relies on the unverified assumption of explanation faithfulness.
- **Medium confidence**: The claim about higher decision volatility for low-SES applicants under System 2 deliberation, as the directional bias in flip rates could reflect model-specific tendencies rather than generalizable patterns.

## Next Checks

1. **External validity test**: Apply DPAF to a different high-stakes domain (e.g., hiring or loan approval) using identical dual-process structure to test whether SES/Affirmative patterns generalize beyond education.
2. **Explanation faithfulness audit**: Have human annotators evaluate a subset of System 2 decisions to verify that SES mentions in explanations align with actual decision reversals, not just rationalization.
3. **Scale sensitivity check**: Replicate key experiments on a larger proprietary model (e.g., GPT-4) to assess whether the SES compensation effect scales with model capacity.