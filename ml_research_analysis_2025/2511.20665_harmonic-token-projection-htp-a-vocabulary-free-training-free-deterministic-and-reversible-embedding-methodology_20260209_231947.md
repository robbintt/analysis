---
ver: rpa2
title: 'Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic,
  and Reversible Embedding Methodology'
arxiv_id: '2511.20665'
source_url: https://arxiv.org/abs/2511.20665
tags:
- harmonic
- semantic
- token
- similarity
- deterministic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Harmonic Token Projection (HTP) introduces a training-free, deterministic
  embedding method that maps Unicode characters to continuous vector space using harmonic
  functions. Unlike neural embeddings, HTP is fully reversible and requires no training
  data or vocabularies.
---

# Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology

## Quick Facts
- arXiv ID: 2511.20665
- Source URL: https://arxiv.org/abs/2511.20665
- Reference count: 12
- Key outcome: Achieves up to 0.70 Spearman correlation on STS-B with sub-millisecond latency and negligible memory usage

## Executive Summary
Harmonic Token Projection (HTP) introduces a training-free, deterministic embedding method that maps Unicode characters to continuous vector space using harmonic functions. Unlike neural embeddings, HTP is fully reversible and requires no training data or vocabularies. Each token is encoded as a sequence of harmonic pairs derived from its Unicode integer representation, producing interpretable and bijective mappings. Evaluation on the STS-B and multilingual STS-B benchmarks shows HTP achieves Spearman correlations up to 0.70 in English and an average of 0.64 across ten languages, with sub-millisecond latency and negligible memory usage. HTP outperforms classical embeddings like Word2Vec and GloVe, approaching the lower range of transformer models while remaining computationally efficient. The method bridges symbolic and continuous representations, offering a transparent, language-agnostic alternative for semantic similarity tasks.

## Method Summary
HTP encodes text tokens into continuous vector representations through a deterministic, reversible process that requires no training data or vocabularies. The method converts each token to a Unicode integer, decomposes it into modular residues using pairwise coprime moduli, and projects each residue to a harmonic pair via trigonometric functions (sin, cos). These harmonic pairs are concatenated to form the token embedding. For sentences, token embeddings are pooled using inverse token frequency weighting to create sentence-level representations. The resulting embeddings enable semantic similarity estimation through geometric alignment and cosine similarity computation, while maintaining perfect reversibility through the Chinese Remainder Theorem.

## Key Results
- Achieves up to 0.70 Spearman correlation on STS-B benchmark in English
- Maintains average 0.64 correlation across ten languages in multilingual evaluation
- Operates at sub-millisecond latency with negligible memory usage compared to transformer models
- Outperforms classical embeddings like Word2Vec and GloVe while approaching lower-range transformer performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity emerges from deterministic geometric alignment of harmonic projections derived from Unicode code points.
- Mechanism: Each token's Unicode integer is decomposed into modular residues with respect to pairwise coprime moduli. Each residue is projected to a harmonic pair via trigonometric functions (sin, cos). The concatenation of these harmonic pairs forms a continuous, bounded, phase-coherent vector representation.
- Core assumption: Unicode code point proximity reflects latent semantic organization; the geometric structure of the harmonic projection preserves this relationship.
- Evidence anchors:
  - [abstract] "...encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation... enabling semantic similarity estimation from purely geometric alignment."
  - [section 2.1, Equations 4-6] "Each residue is mapped to a harmonic pair through trigonometric projection... The final embedding vector is the concatenation of all harmonic pairs."
  - [corpus] Related work on "Geometrical Properties of Text Token Embeddings" suggests interest in geometric approaches to semantic representation, though corpus evidence is weak or missing for the specific Unicode-to-semantic assumption.
- Break condition: If Unicode code point proximity does not systematically correlate with semantic similarity across diverse languages and scripts, the geometric alignment mechanism will fail.

### Mechanism 2
- Claim: Harmonic pooling with Inverse Token Frequency (ITF) weighting creates effective sentence-level representations without training.
- Mechanism: Token-level harmonic vectors are aggregated via a weighted mean. Each token's weight is modulated by its Inverse Token Frequency (ITF), w(t) = 1 / log(1 + f(t)), where f(t) is corpus frequency. This downweights high-frequency function words and upweights rare, content-bearing tokens.
- Core assumption: A simple inverse frequency weighting scheme, analogous to IDF, effectively separates semantic signal from lexical noise without requiring a document collection or supervised training.
- Evidence anchors:
  - [section 2.4, Equations 9-10] "the contribution of each token is modulated according to its Inverse Token Frequency (ITF) weight... This formulation constitutes a deterministic analogue of TF–IDF weighting..."
  - [section 3, Table 4] Ablation comparing TF-IDF vs. Stopword Removal pooling shows comparable performance (Spearman 0.678 vs 0.694).
  - [corpus] "Condition-Aware Sentence Embeddings" explores context-conditioned embeddings, highlighting general challenges in sentence representation. Corpus evidence is weak or missing for this specific ITF scheme.
- Break condition: If ITF weighting proves insufficient to filter lexical noise, or if semantic composition requires more complex (e.g., syntactic or contextual) interactions, this pooling mechanism will underperform.

### Mechanism 3
- Claim: The Chinese Remainder Theorem (CRT) guarantees lossless, bijective reversibility between the discrete token identifier and the continuous harmonic embedding.
- Mechanism: The embedding vector's harmonic pairs allow analytical recovery of modular residues via arctangent. These residues are combined using the CRT to uniquely reconstruct the original integer identifier Nt, from which the original text token can be derived.
- Core assumption: Floating-point precision in forward and inverse trigonometric operations is sufficient to resolve modular residues correctly within the CRT's bounds.
- Evidence anchors:
  - [section 2.2, Equations 7-8] "Given a harmonic modular embedding E(t), the original integer Nt can be recovered analytically... The integer Nt is then reconstructed through the Chinese Remainder Theorem (CRT)... This closed-form inversion guarantees a one-to-one correspondence."
  - [section 2.2] "...small numerical deviations in the trigonometric components lead to negligible reconstruction errors (O(10^-3))..."
  - [corpus] "Memory Tokens" paper explores reversible sentence embeddings using LLMs, providing a conceptual parallel. Corpus evidence is weak or missing for CRT-based reversibility.
- Break condition: If numerical instability in the arctangent or CRT reconstruction introduces errors that prevent exact recovery of the original integer Nt, the reversibility claim is violated.

## Foundational Learning

- Concept: **Modular Arithmetic & The Chinese Remainder Theorem (CRT)**
  - Why needed here: The CRT is the mathematical engine guaranteeing HTP's reversibility. Understanding residues, moduli, and how coprime numbers enable unique reconstruction is essential.
  - Quick check question: Given residues r1 = 2 (mod 3) and r2 = 3 (mod 5), what is the smallest positive integer N that satisfies both? (Answer: N = 8)

- Concept: **Harmonic Projection & Trigonometric Functions**
  - Why needed here: The core embedding operation maps discrete integers to continuous vectors using sin and cos. Understanding phase, periodicity, and how (sin, cos) pairs represent points on a unit circle is crucial.
  - Quick check question: What is the effect of adding π/2 to the input of a sine function? (Answer: It shifts the waveform by a quarter-period; cos(x) = sin(x + π/2).)

- Concept: **TF–IDF / Inverse Token Frequency (ITF)**
  - Why needed here: The pooling mechanism relies on frequency-based weighting to distinguish content words from function words, mirroring a core concept in information retrieval.
  - Quick check question: If a word appears in every document in a corpus, what is its IDF weight, and what does that signify? (Answer: IDF is 0 (or log(1)=0), signifying the word carries no discriminative information.)

## Architecture Onboarding

- Component map:
  1. Tokenizer/Normalizer: Converts text to characters and retrieves Unicode code points
  2. Integer Encoder: Maps the character sequence to a single integer Nt using a base-B representation (Eq. 3)
  3. Harmonic Projector: Decomposes Nt into modular residues and projects each to a (sin, cos) pair (Eq. 4-5)
  4. Pooling Aggregator: Combines token vectors into a sentence vector using weighted averaging (Eq. 9-10)
  5. Similarity Function: Computes cosine similarity between two sentence vectors (Eq. 11)
  6. (Optional) Inverse Decoder: Reconstructs original text from an embedding using the CRT (Eq. 7-8)

- Critical path: The sequence Token → Unicode → Integer Nt → Harmonic Projection → Pooling → Cosine Similarity is the primary inference path. The CRT reconstruction is a secondary, optional path for verification.

- Design tradeoffs:
  - Performance vs. Cost: HTP achieves ~90% of Sentence-BERT performance at ~1000x less computational cost (Table 3)
  - Reversibility vs. Contextuality: The method gains perfect reversibility but loses contextual disambiguation (e.g., polysemy)
  - Dimensionality vs. Performance: Performance improves monotonically with embedding dimension, saturating around D=512 (Table 5), with sublinear runtime increase

- Failure signatures:
  - Polysemy Misalignment: Words like "bank" (river vs. financial) will have identical embeddings, leading to incorrect similarity judgments
  - Numerical Reconstruction Errors: In low-precision environments, the atan2 and CRT steps might fail to perfectly reconstruct the original integer Nt
  - Sensitivity to Normalization: Inconsistent Unicode normalization will break reversibility and harm semantic alignment

- First 3 experiments:
  1. Dimensionality Scaling: Replicate Table 5's ablation. Implement HTP and evaluate on STS-B while varying embedding dimension D from 32 to 1024. Plot Spearman correlation vs. D to verify performance saturation
  2. Pooling Ablation: Compare "Harmonic Pooling + ITF" against "Stopword Removal + Unweighted Mean" as described in Table 4. Measure both correlation and latency per sentence pair
  3. Reversibility Stress Test: Encode a large set of random tokens/sentences, decode them using the CRT, and measure exact reconstruction rate. Vary floating-point precision (float32 vs. float64) to identify numerical failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-scale Fourier embeddings or adaptive frequency modulation extend HTP's representational capacity to capture contextual meaning in polysemous words?
- Basis in paper: [explicit] The authors state "HTP lacks contextual disambiguation: polysemous words such as 'bank' (financial) and 'bank' (river) share identical representations" and propose "multi-scale Fourier embeddings and adaptive frequency modulation" as future directions.
- Why unresolved: The current harmonic formulation encodes each token as a fixed trajectory based solely on its Unicode integer, producing identical representations regardless of context.
- What evidence would resolve it: Demonstrating improved performance on word sense disambiguation benchmarks (e.g., WiC, WordNet-based tasks) using proposed multi-scale or adaptive extensions.

### Open Question 2
- Question: Can phase- or frequency-aware pooling mechanisms preserve compositional meaning in longer text sequences better than the current linear weighted averaging?
- Basis in paper: [explicit] The authors note that "Linear pooling may also dilute compositional meaning in longer sequences, suggesting that phase- or frequency-aware pooling could enhance semantic precision."
- Why unresolved: Current harmonic energy pooling computes a weighted mean, which may lose syntactic and compositional information as sequence length increases.
- What evidence would resolve it: Comparative evaluation on document-level or paragraph-level semantic tasks showing retention of compositional semantics with alternative pooling strategies.

### Open Question 3
- Question: How can deterministic HTP initialization be effectively integrated with neural fine-tuning in hybrid architectures?
- Basis in paper: [explicit] The conclusion states "Future work will explore... hybrid systems where deterministic initialization guides or constrains contextual fine-tuning."
- Why unresolved: The paper demonstrates HTP as a standalone embedding method but does not investigate its use as a pre-embedding layer or regularizer within neural architectures.
- What evidence would resolve it: Experiments showing that HTP-initialized transformers achieve faster convergence, better interpretability, or improved performance on downstream tasks compared to standard random initialization.

### Open Question 4
- Question: How sensitive is HTP to Unicode normalization variants across different text preprocessing pipelines?
- Basis in paper: [explicit] The authors acknowledge "small distortions in Unicode normalization can introduce discontinuities in the harmonic space."
- Why unresolved: Different normalization forms (NFC, NFD, NFKC, NFKD) produce different code point sequences, potentially yielding divergent embeddings for equivalent strings.
- What evidence would resolve it: Systematic ablation across Unicode normalization forms measuring embedding consistency and downstream task stability.

## Limitations
- Uncontrolled hyperparameters: Exact selection of pairwise coprime moduli and maximum token length are not specified, potentially affecting performance
- Conceptual leap from Unicode to semantics: The assumption that Unicode code point proximity reflects semantic similarity is not empirically validated
- Limited evaluation scope: Performance is benchmarked only on STS-B semantic similarity task, not tested on other NLP tasks or domain shifts

## Confidence
- High Confidence: The mathematical framework for harmonic projection and CRT-based reversibility is sound and verifiable
- Medium Confidence: STS-B benchmark results are specific and reproducible, but generalizability to other tasks is uncertain
- Low Confidence: The core semantic hypothesis that Unicode code point geometry can encode meaning is plausible but not proven

## Next Checks
1. Robustness to Hyperparameter Variation: Systematically vary the choice of coprime moduli and L_max. Measure the impact on STS-B correlation, reversibility rate, and embedding stability
2. Cross-Lingual Semantic Transfer: Evaluate HTP embeddings on a multilingual semantic clustering or classification task (e.g., XNLI or BUCC). Test whether HTP's language-agnostic encoding enables zero-shot transfer or improves alignment with existing multilingual embeddings
3. Task Generalization and Domain Shift: Apply HTP to a non-STSb task, such as paraphrase identification (PAWS) or information retrieval (MS MARCO). Measure performance degradation when training and test data come from different domains (e.g., news vs. social media)