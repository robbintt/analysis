---
ver: rpa2
title: 'MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion'
arxiv_id: '2510.22768'
source_url: https://arxiv.org/abs/2510.22768
tags:
- persuasion
- multimodal
- agreement
- persuadee
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMPersuade, a framework and dataset for studying
  multimodal persuasion in large vision-language models (LVLMs). It provides a large-scale
  dataset of 62,160 images and 4,756 videos across 450 persuasion dialogues, paired
  with established persuasion strategies in commercial, subjective/behavioral, and
  adversarial contexts.
---

# MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion

## Quick Facts
- arXiv ID: 2510.22768
- Source URL: https://arxiv.org/abs/2510.22768
- Reference count: 40
- Primary result: Multimodal inputs significantly increase persuasion effectiveness in LVLMs compared to text-only, with context-dependent strategy effectiveness

## Executive Summary
MMPersuade introduces a framework and dataset for studying multimodal persuasion in large vision-language models (LVLMs). The dataset contains 62,160 images and 4,756 videos across 450 persuasion dialogues, covering commercial, subjective/behavioral, and adversarial contexts. The framework evaluates persuasion effectiveness using third-party agreement scoring and self-estimated token probabilities, with a novel persuasion discounted cumulative gain (PDCG) metric. Experiments with six leading LVLMs reveal that multimodal inputs substantially increase persuasion effectiveness compared to text-only, prior preferences reduce but do not eliminate multimodal advantage, and strategy effectiveness varies systematically by context.

## Method Summary
The method constructs a multimodal persuasion dataset through a 6-step pipeline: context classification, strategy mapping (Cialdini's principles for commercial/subjective contexts, Aristotle's appeals for adversarial), multimodal conceptual design, prompt refinement, content generation using gpt-image and Veo3, and quality assurance. Persuasion effectiveness is evaluated through simulated dialogues where a static persuader delivers multimodal messages to LVLMs acting as persuadees. Two evaluation methods are employed: third-party agreement scoring (LLM judge on 1-5 scale) and self-estimated token probability (logprobs for target vs. initial options). The primary metric, PDCG, captures both the speed and strength of persuasion using linear or logarithmic discounting.

## Key Results
- Multimodal inputs significantly increase persuasion effectiveness compared to text-only, with PDCG scores rising progressively from text-only to text+caption to fully multimodal
- Prior preferences reduce persuasion susceptibility, but multimodal inputs maintain their advantage through a "cushioning" effect against stubbornness
- Strategy effectiveness varies by context: reciprocity and consistency work best in commercial/subjective contexts, while credibility and logic dominate in adversarial contexts

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Amplification of Persuasion Effectiveness
Visual cues provide richer contextual grounding and emotional resonance that complement textual arguments, with full multimodal input yielding the strongest effects compared to text-only or text+caption conditions. This amplification occurs because non-verbal visual information can reinforce affective strategies like "liking" beyond what text can achieve.

### Mechanism 2: "Cushioning" Effect of Multimodality Against Prior Preferences
While stronger prior preferences reduce persuasion across all modalities, the decline in PDCG scores is smaller in multimodal settings compared to text-only ones. Visual cues make the persuasive argument more resilient to the model's instructed resistance by providing more diverse evidence to overcome the initial preference.

### Mechanism 3: Context-Dependent Strategy Effectiveness
Different persuasion strategies vary in effectiveness across contexts: in commercial and subjective contexts, strategies like reciprocity and consistency are most potent, while in adversarial contexts, credibility and logic-based strategies prevail. This occurs because misinformation often succeeds by mimicking trustworthy sources and presenting fabricated evidence, which multimodal content can powerfully amplify.

## Foundational Learning

- **PDCG (Persuasion Discounted Cumulative Gain)**: The primary metric used to evaluate persuasion success, capturing not just whether persuasion occurred but also how quickly and how strongly. *Quick check: A model is persuaded at turn 3 with agreement score 4 vs. turn 1 with score 5 - which has higher PDCG with linear discount?*

- **LVLM as a Persuadee vs. Persuader**: This paper studies LVLMs as targets of persuasion (persuadees), not as generators of persuasive content (persuaders). *Quick check: In this framework, is the LVLM generating the multimodal meme or receiving and judging it?*

- **Expressed Agreement vs. Implicit Belief**: The paper uses third-party agreement scoring (what the model says) and self-estimated token probability (what the model implicitly predicts). *Quick check: If an LVLM agrees verbally (score 5) but still assigns higher token probability to its original preference, has it been persuaded according to the implicit belief metric?*

## Architecture Onboarding

- **Component map**: Dataset Construction (DAILYPERSUASION + FARM) -> Strategy Mapping (Cialdini/Aristotle) -> Multimodal Content Generation (gpt-image + Veo3) -> Simulated Dialogue (Static Persuader + LVLM Persuadee) -> Evaluation (LLM Judge + Token Probabilities) -> PDCG Calculation

- **Critical path**: Dataset Construction is most resource-intensive, requiring generative models for images/videos; Dialogue Simulation is the core experimental loop dependent on LVLM API throughput; Evaluation happens after each dialogue turn and aggregates into final PDCG score

- **Design tradeoffs**: Static vs. Adaptive Persuader (static for experimental control vs. ecological validity); Token Probability vs. Agreement Score (implicit belief vs. interpretability); Synthetic vs. Real Data (control vs. realism)

- **Failure signatures**: Low-quality multimodal content (QA score < 1.9 undermines experiment); Stubbornness profile failure (preference instruction ignored invalidates RQ2); Evaluation misalignment (biased judge or flawed token probability invalidates PDCG)

- **First 3 experiments**: 1) Reproduce text-only baseline PDCG for GPT-4o on 10 Commercial scenarios and compare with Figure 3; 2) Ablate modality by running text+caption and full multimodal conditions to verify progressive PDCG increase; 3) Test different strategy by crafting message using 'scarcity' instead of 'reciprocity' and observe effectiveness change

## Open Questions the Paper Calls Out

- How does an adaptive persuader that dynamically adjusts strategies based on persuadee responses compare to a static persuader in multimodal persuasion effectiveness? (Section A: real-world persuasion requires dynamic adaptation)

- How robust are multimodal persuasion findings across diverse system prompt configurations and role framings? (Section 4.4: prompt framing modulates multimodal advantage, requiring wider spectrum testing)

- What defense mechanisms can effectively mitigate LVLM vulnerability to adversarial multimodal persuasion while preserving legitimate persuasive communication? (Section 4.1: multimodal contexts significantly heighten adversarial success represents key frontier in safeguarding LVLMs)

## Limitations

- Synthetic nature of multimodal content may not generalize to naturally occurring persuasive materials
- Static persuader model represents idealized and potentially less effective persuasion strategy compared to adaptive human persuaders
- Persuasion metrics rely on model-based evaluation rather than human judgment, potentially not accurately capturing human persuasion dynamics

## Confidence

- **High Confidence**: Multimodal inputs significantly increase persuasion effectiveness; strategy effectiveness varies systematically by context
- **Medium Confidence**: "Cushioning" effect against prior preferences; relative effectiveness rankings of different LVLM models
- **Low Confidence**: Absolute magnitude of persuasion effectiveness differences; specific PDCG numerical values

## Next Checks

1. Conduct human evaluation study where participants judge persuasiveness of synthetic multimodal content used in experiments, comparing human agreement scores with LLM judge scores

2. Modify static persuader to adaptive version that adjusts messages based on LVLM's responses and re-run subset of experiments to determine if multimodal advantage persists with ecological validity

3. Test persuasion framework on naturally occurring multimodal persuasive content (actual advertisements, social media posts) rather than synthetic content to validate modality effects and strategy patterns transfer to real-world scenarios