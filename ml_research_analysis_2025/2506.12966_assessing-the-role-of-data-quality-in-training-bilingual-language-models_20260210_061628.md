---
ver: rpa2
title: Assessing the Role of Data Quality in Training Bilingual Language Models
arxiv_id: '2506.12966'
source_url: https://arxiv.org/abs/2506.12966
tags:
- data
- quality
- performance
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates performance gaps in bilingual language
  models and finds that data quality, not just quantity or language, is the primary
  driver of performance degradation. The authors propose a language-agnostic data
  filtering method that uses high-quality English data to select equivalent quality
  data in other languages.
---

# Assessing the Role of Data Quality in Training Bilingual Language Models
## Quick Facts
- arXiv ID: 2506.12966
- Source URL: https://arxiv.org/abs/2506.12966
- Reference count: 40
- Primary result: Data quality is the primary driver of bilingual model performance gaps, not just quantity or language

## Executive Summary
This paper investigates performance gaps in bilingual language models and finds that data quality, not just quantity or language, is the primary driver of performance degradation. The authors propose a language-agnostic data filtering method that uses high-quality English data to select equivalent quality data in other languages. Applied to French, German, and Chinese, this approach improves monolingual performance by 2-4% and reduces bilingual model performance gaps to 1%. The method involves training a logistic regression classifier using SentenceBERT embeddings to identify high-quality samples from general pretraining data. Experiments show that filtered data leads to consistent performance improvements across model sizes (350M, 1.3B, 2.7B parameters) and training stages, with bilingual models trained on filtered data achieving comparable performance to monolingual models.

## Method Summary
The authors developed a language-agnostic data filtering method that transfers quality signals from English to other languages. The approach uses SentenceBERT embeddings to represent text samples, then trains a logistic regression classifier on high-quality English data to identify equivalent quality data in target languages. The method was applied to French, German, and Chinese, filtering general pretraining datasets to extract high-quality subsets. These filtered datasets were then used to pretrain bilingual language models, comparing performance against models trained on unfiltered data and monolingual baselines.

## Key Results
- Data quality is the primary driver of bilingual model performance gaps
- Language-agnostic filtering method effectively transfers quality signals across languages
- Filtered data provides consistent improvements across model sizes (350M, 1.3B, 2.7B parameters)
- Bilingual models trained on filtered data achieve comparable performance to monolingual models
- Performance gaps reduced to approximately 1% between bilingual and monolingual models

## Why This Works (Mechanism)
The mechanism works because data quality fundamentally affects model learning capacity and generalization. Low-quality data introduces noise, inconsistencies, and unreliable patterns that models must learn to ignore, wasting capacity and degrading performance. By filtering data based on quality rather than quantity, the approach ensures models train on cleaner, more reliable signal. The cross-lingual transfer of quality signals works because SentenceBERT embeddings capture semantic and syntactic properties that correlate with quality across languages, even without direct translation. This allows the classifier to identify high-quality samples in target languages based on patterns learned from English quality data.

## Foundational Learning
- SentenceBERT embeddings: Why needed - to create language-agnostic representations that capture semantic and syntactic quality indicators across languages; Quick check - verify embeddings preserve semantic similarity within and across languages
- Logistic regression classification: Why needed - provides interpretable, efficient quality prediction without requiring large labeled datasets in target languages; Quick check - validate classification accuracy on held-out English quality data
- Cross-lingual quality transfer: Why needed - enables quality filtering for low-resource languages without requiring language-specific quality annotations; Quick check - test transfer effectiveness on languages with varying linguistic distance from English

## Architecture Onboarding
- Component map: English quality data -> SentenceBERT embedding generation -> Logistic regression classifier training -> Quality prediction for target languages -> Filtered dataset creation -> Model pretraining
- Critical path: Quality data selection → Embedding generation → Classifier training → Target language filtering → Model training
- Design tradeoffs: Computational cost of filtering vs. performance gains; reliance on English quality benchmarks vs. direct language-specific quality assessment
- Failure signatures: Over-filtering removes too much data leading to underfitting; under-filtering fails to remove low-quality samples leading to noisy training
- First experiments: 1) Validate SentenceBERT embedding quality across languages, 2) Test classifier accuracy on held-out English data, 3) Compare filtered vs. unfiltered model performance on downstream tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on English as quality benchmark may not hold for languages with different linguistic structures or domains
- Study focuses on three specific languages, limiting generalizability to other language families or extremely low-resource languages
- 2-4% improvement in monolingual performance is modest and may not justify filtering costs for all use cases
- Filtering effectiveness depends on English seed dataset quality and may propagate existing biases

## Confidence
- Data quality is the primary driver of bilingual model performance gaps: High
- Language-agnostic filtering method effectively transfers quality signals: Medium
- Filtered data provides consistent improvements across model sizes: High
- Performance gap reduction to 1% is practically significant: Medium

## Next Checks
1. Test the filtering approach on low-resource languages with no direct English cognates or shared domains to validate cross-linguistic quality transfer assumptions
2. Compare the computational cost of data filtering against potential performance gains for smaller organizations or resource-constrained scenarios
3. Evaluate whether the method performs equally well when applied to different types of English seed data (e.g., Wikipedia vs. news corpora) to assess robustness to data source selection