---
ver: rpa2
title: Response Wide Shut? Surprising Observations in Basic Vision Language Model
  Capabilities
arxiv_id: '2507.10442'
source_url: https://arxiv.org/abs/2507.10442
tags:
- visual
- response
- vlms
- space
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the performance of vision-language models
  (VLMs) on fundamental visual tasks by examining three intermediate feature spaces:
  visual encoder output, vision-language projection, and language decoder response.
  The study reveals that while visual and projection spaces capture visual information
  effectively, the response space shows significant performance drops, especially
  in fine-grained recognition and object counting tasks.'
---

# Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities

## Quick Facts
- arXiv ID: 2507.10442
- Source URL: https://arxiv.org/abs/2507.10442
- Reference count: 18
- This paper analyzes VLM performance across three intermediate feature spaces, revealing that while visual and projection spaces capture visual information effectively, the response space shows significant performance drops for fine-grained tasks.

## Executive Summary
This study examines vision-language model capabilities across three key intermediate feature spaces: visual encoder output, vision-language projection, and language decoder response. Through systematic probing experiments on synthetic and real-world datasets, the authors reveal that while visual and projection layers effectively capture visual information, there is a significant performance drop when generating textual responses. The analysis shows that VLMs rely more on texture than shape information, and that spatial understanding capabilities are surprisingly weak in the visual encoder but partially recovered in the response space through language priors. These findings highlight critical bottlenecks in current VLM architectures and suggest that improving the alignment between visual and language modalities during joint fine-tuning is key to enhancing overall performance.

## Method Summary
The study evaluates VLMs by training linear probes on three intermediate feature spaces: visual encoder output (CLIP features), vision-language projection layer, and language decoder response. The authors compare probe accuracy against direct text response accuracy on tasks including coarse/fine-grained recognition, object counting (1-4 objects), and spatial understanding (left/right/above/below). They use synthetic PaintSkills data for controlled experiments and real-world datasets like Pascal VOC, Stanford Dogs, and CUB-200-2011. For each space, they train single-layer MLP logistic regression probes on frozen features and compare results with VLM text outputs. Control experiments with shuffled labels validate the probing methodology.

## Key Results
- Visual and projection spaces achieve high accuracy (>90%) on fine-grained recognition, but response space accuracy drops by at least 45%
- Spatial understanding shows the opposite trend, with response space outperforming visual encoder (which performs near chance)
- VLMs demonstrate significant texture bias, showing drastic performance drops on edge maps compared to patch-shuffled images
- Background context significantly impacts recognition performance, with blurred backgrounds degrading accuracy
- Fine-tuning data representation strongly influences performance, with fine-grained concepts often underrepresented (<0.17% of data)

## Why This Works (Mechanism)

### Mechanism 1: Information Loss via Ineffective Vision-Language Alignment
The visual encoder and VL projection layer retain high-fidelity features (probe accuracy >90%), but the projection-to-language-decoder interface fails to map these features to correct token generation. This suggests the alignment objective during fine-tuning doesn't adequately bridge modalities for specific concepts. If the fine-tuning dataset contains sufficient representation of specific fine-grained concepts (>0.17% of data), the performance drop in the response space should narrow significantly.

### Mechanism 2: Spatial Reasoning via Language Priors over Visual Features
The visual encoder shows near-chance performance on spatial tasks, but final response accuracy is significantly higher. This reversal implies the LLM decoder leverages statistical priors learned during instruction tuning to "guess" spatial relations independently of visual input. If visual inputs are randomized or contradictory to common priors, response space accuracy should collapse toward the visual encoder's chance-level accuracy.

### Mechanism 3: Texture-Biased Feature Utilization
VLMs prioritize texture over geometric shape for object recognition. When shape is preserved but texture removed (Edge Maps), performance drops drastically, while patch-shuffled images (distorted shape, preserved texture) show smaller drops. This indicates visual features and attention mechanisms prioritize local texture correlations over global geometric structures. If models are trained with shape-centric augmentation, the performance gap between edge maps and natural images should decrease.

## Foundational Learning

- **Concept: Linear Probing**
  - **Why needed here:** The paper's core methodology relies on training lightweight classifiers (probes) on frozen intermediate features to distinguish between "lack of visual capability" and "inability to verbalize."
  - **Quick check question:** If a probe on the VL projection layer achieves 90% accuracy but the text response is 20% accurate, is the visual encoder at fault? (Answer: No, the alignment/decoder is likely at fault).

- **Concept: Multimodal Alignment (Projector Layers)**
  - **Why needed here:** The paper identifies the "VL Projection" (e.g., Q-Former or MLP) and its fine-tuning as the critical failure point for fine-grained tasks.
  - **Quick check question:** Does the model use a simple linear layer (LLaVA) or a transformer-based Q-Former (BLIP-2) to connect the visual encoder to the LLM?

- **Concept: Language Priors in VLMs**
  - **Why needed here:** To understand why Spatial Understanding performance improves in the response space despite poor visual inputs (the LLM is "guessing" based on language probability).
  - **Quick check question:** If I ask a VLM "What is the spatial relation?" without showing an image, would the answer distribution be biased toward specific configurations (e.g., "on")?

## Architecture Onboarding

- **Component map:** Visual Encoder (Frozen/CLIP) → VL Projection (Trainable) → LLM Decoder (Vicuna/LLaMA)
- **Critical path:** Visual Latent → VL Projection (Bottleneck for Robustness) → LLM Response (Bottleneck for Fine-Grained Verbalization)
- **Design tradeoffs:**
  - LLaVA-style (MLP Projection): Simpler, preserves more fine-grained data initially, but less robust to corruptions compared to Q-Former approaches
  - BLIP-style (Q-Former): More robust to visual noise/corruption but may filter out fine-grained visual nuances more aggressively
- **Failure signatures:**
  - High Probe / Low Text: Visual info is captured but lost in LLM alignment (Fine-grained tasks)
  - Low Probe / High Text: Visual info is missing; model is hallucinating based on priors (Spatial tasks)
  - Texture/Shape Disparity: Model fails on silhouettes/edge maps (Texture Bias)
- **First 3 experiments:**
  1. **Feature Space Probing:** Train linear probes on visual encoder output vs. LLM input embeddings to verify where information is preserved
  2. **Background Ablation:** Test object recognition with "Reverse Blur" (blurred background) vs. "Clean Background" to measure signal-to-noise ratio reliance
  3. **Blind LLM Baseline:** Run the prompt through the LLM without the image to quantify the "Language Prior" baseline for spatial/reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
How can the joint fine-tuning objective or data composition be modified to prevent the drastic loss of fine-grained visual information when transferring from the projection space to the language decoder response? The authors conjecture that the performance drop is "due to ineffective joint fine-tuning of the proj. layer and language decoder" and note that fine-tuning data often lacks sufficient fine-grained samples (e.g., only 0.17% for dog breeds). This remains unresolved as scaling the LLM decoder does not resolve this information bottleneck. A new training regime or data curation strategy that yields high accuracy in the response space for fine-grained tasks would resolve this question.

### Open Question 2
What specific training interventions can successfully induce a reliance on shape over texture in VLMs to mimic human-like object recognition? The authors observe that VLMs rely more on texture than shape (confirmed by edge map vs. patch shuffle experiments) and explicitly state that "Inducing reliance on shape is of importance." While the paper identifies the deficiency, it does not propose or test a method to correct this bias during pre-training or fine-tuning. Experiments showing improved performance on silhouette or edge-map tasks without degradation on texture-rich tasks would resolve this question.

### Open Question 3
Does replacing the single-layer MLP projector with an attention-based mechanism (like a Q-Former) systematically improve the robustness of the intermediate VL projection space to visual corruptions? The authors observe that the VL projection space is the least robust to corruption and note that LLaVA (using a single-layer MLP) suffers more than BLIP family models (using Q-Former/Cross-Attention). The paper notes the correlation but does not isolate the architectural choice of the projector as the definitive cause or test alternative projection mechanisms. An ablation study controlling for the visual encoder and decoder while swapping the projection module would resolve this question.

## Limitations
- The study relies on linear probe accuracy as a proxy for information presence, which may not always reflect actual feature space capabilities
- Classification accuracy as the primary metric may miss other important aspects of VLM performance such as confidence calibration
- The analysis focuses on specific VLM architectures (CLIP, LLaVA, InstructBLIP) and may not generalize to all vision-language models

## Confidence
- **High Confidence:** The observation that VLMs show texture bias over shape information is well-supported by systematic comparison of edge maps versus patch-shuffled images
- **Medium Confidence:** The claim about spatial understanding relying on language priors rather than visual features is supported by performance trend reversals, though alternative explanations exist
- **Low Confidence:** The assertion that the VL projection layer is the critical bottleneck for fine-grained recognition requires more direct evidence establishing causal mechanisms

## Next Checks
1. **Probe Architecture Sensitivity Analysis:** Test whether probe performance varies significantly with different probe architectures (e.g., deeper MLPs vs. linear layers) or training procedures to validate whether linear probe accuracy is a reliable proxy for feature space information content.

2. **Fine-tuning Data Representation Study:** Systematically vary the representation of fine-grained concepts in the training data and measure how this affects the performance gap between intermediate spaces and response space, directly testing the hypothesis that data distribution drives the alignment bottleneck.

3. **Cross-modal Attention Visualization:** Analyze attention weights in the VL projection and decoder layers to identify whether information loss occurs through selective attention filtering or through more fundamental representation mismatches between visual and language modalities.