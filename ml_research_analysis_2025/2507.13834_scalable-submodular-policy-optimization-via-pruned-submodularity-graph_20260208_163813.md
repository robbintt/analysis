---
ver: rpa2
title: Scalable Submodular Policy Optimization via Pruned Submodularity Graph
arxiv_id: '2507.13834'
source_url: https://arxiv.org/abs/2507.13834
tags:
- submodular
- policy
- function
- reward
- epochs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reinforcement learning (RL) with submodular
  reward functions, a scenario common in path planning and coverage control where
  the law of diminishing returns applies. Traditional RL assumes additive rewards,
  but this work extends RL to handle submodular functions using a pruned submodularity
  graph-based approach.
---

# Scalable Submodular Policy Optimization via Pruned Submodularity Graph

## Quick Facts
- arXiv ID: 2507.13834
- Source URL: https://arxiv.org/abs/2507.13834
- Reference count: 33
- This paper addresses reinforcement learning with submodular reward functions using a pruned submodularity graph approach that achieves better performance than baseline methods.

## Executive Summary
This paper tackles reinforcement learning (RL) with submodular reward functions, where the law of diminishing returns applies - a common scenario in path planning and coverage control. Traditional RL assumes additive rewards, but this work extends RL to handle submodular functions through a pruned submodularity graph-based approach. The method iteratively samples and prunes states based on their marginal gains, improving computational efficiency while maintaining approximation guarantees.

The core contribution is Algorithm 1, which constructs a submodularity graph where edge weights represent state divergence, then prunes redundant states to reduce the trajectory space. This allows the policy to focus on states that offer unique marginal value. The approach is evaluated on continuous environments (Car Racing, MuJoCo Ant) and discrete environments (Graph-Based, Entropy-Based), showing higher rewards and more stable training compared to a baseline SubPO method.

## Method Summary
The method addresses RL with submodular reward functions through a pruned submodularity graph approach. The algorithm collects trajectories, constructs a graph where edge weights represent the difference between marginal gains of states, then iteratively prunes states with low divergence weights. The pruned trajectory is used to compute a specific gradient estimator (Theorem 3) that weights policy log-gradients by cumulative marginal future rewards. The policy is updated via regularized gradient ascent (Equation 4). The approach is tested on 4 environments with different state/action dimensions, comparing performance against a SubPO baseline.

## Key Results
- SGPO achieves higher rewards than SubPO baseline across all tested environments (2636 vs 2612 units in MuJoCo Ant)
- More stable training with lower policy loss and improved convergence
- Higher and more fluctuating advantage values indicate broader exploration
- Theoretical hardness result: Submodular RL cannot be approximated better than O(log1−γ OPT) under standard assumptions

## Why This Works (Mechanism)

### Mechanism 1: Divergence-Based State Pruning
The algorithm reduces computational complexity by identifying and removing redundant states from the trajectory while preserving sufficient information for policy optimization. It constructs a "submodularity graph" where edge weights represent the difference between the marginal gain of a target state and the minimum gain of a source state (Divergence). It iteratively samples a subset of states, calculates the divergence of remaining states relative to the sample, and prunes the (1 - 1/√c) fraction of states with the lowest divergence. This retains states that offer unique marginal value and discards those that are predictable or redundant given the retained set.

### Mechanism 2: Marginal Gain Gradient Estimation
Standard policy gradient methods are adapted for submodular rewards by decomposing the trajectory return into a sum of marginal gains rather than simple state-action values. The paper derives a specific gradient estimator where the policy log-gradient is weighted by the cumulative marginal future reward. Instead of R(τ), the objective uses ΣR(s_{j+1}|τ_{0:j}). This forces the policy to optimize for the unique contribution of each action relative to the history, aligning the gradient direction with the diminishing returns property.

### Mechanism 3: Submodular Reward Formulation
Framing the objective as a submodular set function (rather than additive) naturally encodes coverage and exploration tasks, preventing the policy from naively exploiting high-reward loops. The reward is defined on the set of visited states (trajectory). In the Car Racing and Ant environments, the reward T(τ) = |∪_{s∈τ} D_s| (union of covered areas) is inherently submodular because revisiting an area provides zero marginal gain. This structural constraint forces the agent to explore new states to increase reward.

## Foundational Learning

- **Concept: Submodularity & Marginal Gain**
  - Why needed here: This is the core mathematical property the entire algorithm exploits. You cannot understand the "pruning" or the "gradient" without understanding that Δ(v|S) ≥ Δ(v|T) for S ⊂ T.
  - Quick check question: If I visit the same "high reward" room 10 times, does the total reward increase linearly, stay the same, or decrease? (Answer: In submodular RL, it likely stays the same or has diminishing increases).

- **Concept: Policy Gradient (REINFORCE)**
  - Why needed here: The optimization loop uses ∇θ (Equation 4) to update the neural network. Understanding the log-derivative trick is required to parse Theorem 3.
  - Quick check question: Why does the algorithm sample actions from a distribution π rather than picking the max action deterministically during training? (Answer: To estimate the gradient via exploration/log-probability).

- **Concept: Graph-Based Sparsification**
  - Why needed here: The "Pruned Submodularity Graph" is a technique borrowed from combinatorial optimization to reduce O(n³) or higher complexity to a manageable level by removing "low weight" nodes.
  - Quick check question: In the pruning step, if a state v has very low "divergence" from set U, what does that imply about v's uniqueness? (Answer: v is redundant because its value is already "covered" by or highly correlated with states in U).

## Architecture Onboarding

- **Component map:** Input (SMDP) -> Trajectory Sampler -> Graph Constructor -> Pruning Engine -> Gradient Estimator -> Optimizer
- **Critical path:** Sampling → Graph Construction (O(n³)) → Pruning → Gradient Update
- **Design tradeoffs:**
  - Hyperparameters r (sampling rate) & c (pruning severity): High c / High r yields faster iteration but risk of losing optimal paths; Low c is safer but approaches standard RL complexity
  - Horizon H: Long horizons increase coverage but explode graph size n=H, making O(n³) construction costly
- **Failure signatures:**
  - Premature Convergence: Policy loss drops, but reward stays flat (likely due to over-aggressive pruning)
  - Training Instability: Advantage values fluctuate wildly (expected in submodular RL, but check for NaN in divergence calculations)
  - Slow Epochs: If epoch time grows linearly or quadratically with training steps, pruning logic is failing to reduce |V|
- **First 3 experiments:**
  1. Sanity Check (Discrete Grid): Implement the "Graph-Based Environment" with a small grid to verify pruning retains distinct nodes while dropping loops
  2. Pruning Ratio Sweep: Run SGPO on Car Racing with c ∈ {4, 8, 16} to identify the point where reward drops due to excessive pruning
  3. Gradient Validation: Compare the gradient norm from Theorem 3 against numerical gradient approximation on a tiny submodular toy problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the constant-factor approximation guarantee be extended to the general Submodular RL case without relying on simplified assumptions?
- Basis in paper: The abstract and conclusion state SGPO provides a constant-factor approximation "under simplified assumptions," while Theorem 2 establishes a hardness result limiting the general case to a logarithmic approximation (O(log1-γ OPT)).
- Why unresolved: There is a theoretical gap between the general inapproximability result and the specific guarantees achieved by SGPO under unstated "simplified" conditions.
- What evidence would resolve it: A proof extending the constant-factor guarantee to general SMDPs or a formal characterization of the specific structural properties required for the simplified assumptions to hold.

### Open Question 2
- Question: Can the submodularity graph construction be optimized to lower the O(n⁴) time complexity for very high-dimensional state spaces?
- Basis in paper: The complexity analysis lists the time requirement as O(N · n⁴ s² m²), driven largely by graph construction and weight calculations, which may hinder scalability despite the "Scalable" title.
- Why unresolved: While the algorithm prunes states, the overhead of constructing the graph on the initial state set scales polynomially with a high degree (n⁴).
- What evidence would resolve it: An algorithmic variant utilizing sparse approximations or parallelization to reduce construction complexity, along with empirical tests on environments with significantly larger raw state spaces.

### Open Question 3
- Question: How robust is the performance of SGPO regarding the sparsification hyperparameters r (sampling rate) and c (pruning rate) across diverse environments?
- Basis in paper: The paper acknowledges that r=8 and c=8 were chosen empirically based on preliminary experiments, but provides no sensitivity analysis or theoretical justification for these specific values.
- Why unresolved: If performance degrades sharply with different r or c values, the method may require extensive tuning for new problems, limiting its general applicability.
- What evidence would resolve it: A sensitivity analysis showing the variance in reward and convergence speed when r and c are systematically altered in the Car Racing and MuJoCo environments.

## Limitations

- The O(n³) graph construction complexity for trajectory states may still be prohibitive for very long horizons or high-dimensional continuous spaces
- The method requires careful tuning of hyperparameters r and c, with no provided sensitivity analysis
- The "simplified assumptions" for constant-factor approximation are not formally characterized
- Empirical results are limited to 4 environments, with discrete environments showing more dramatic gains than continuous ones

## Confidence

- Submodular RL approximation bounds: High
- Algorithm 1 correctness: High
- Empirical performance claims: Medium
- Computational efficiency: Low-Medium

## Next Checks

1. **Pruning Ablation Study**: Systematically vary the pruning severity parameter c ∈ {2, 4, 8, 16} on Car Racing to identify the optimal tradeoff between computational efficiency and reward performance, determining if the chosen c=8 is universally optimal.

2. **Scaling Analysis**: Measure epoch wall-clock time and memory usage as a function of trajectory horizon H and state dimension to empirically validate the claimed computational improvements and identify the breaking point where graph construction becomes prohibitive.

3. **Baseline Extension**: Implement and compare against additional RL baselines beyond SubPO (e.g., PPO, SAC) on the same environments to establish whether the submodularity-specific optimizations provide advantages over general-purpose RL algorithms adapted to submodular rewards.