---
ver: rpa2
title: Explainable Multimodal Regression via Information Decomposition
arxiv_id: '2512.22102'
source_url: https://arxiv.org/abs/2512.22102
tags:
- information
- multimodal
- pidreg
- regression
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PIDReg introduces a novel multimodal regression framework that
  leverages Partial Information Decomposition (PID) to provide intrinsic interpretability
  of modality contributions and interactions. By enforcing Gaussianity in the joint
  distribution of modality-specific representations and the target variable, PIDReg
  analytically computes PID components (unique, redundant, and synergistic information)
  even in high-dimensional continuous settings.
---

# Explainable Multimodal Regression via Information Decomposition

## Quick Facts
- **arXiv ID:** 2512.22102
- **Source URL:** https://arxiv.org/abs/2512.22102
- **Reference count:** 40
- **Primary result:** Introduces PIDReg, a multimodal regression framework that uses Partial Information Decomposition (PID) to provide intrinsic interpretability of modality contributions and interactions, achieving state-of-the-art performance across six real-world datasets.

## Executive Summary
PIDReg introduces a novel multimodal regression framework that leverages Partial Information Decomposition (PID) to provide intrinsic interpretability of modality contributions and interactions. By enforcing Gaussianity in the joint distribution of modality-specific representations and the target variable, PIDReg analytically computes PID components (unique, redundant, and synergistic information) even in high-dimensional continuous settings. A closed-form conditional independence regularizer promotes isolation of unique information within each modality. Evaluated on six real-world datasets across diverse domains, PIDReg outperforms six state-of-the-art methods in predictive accuracy and interpretability. Notably, in brain age prediction from multimodal neuroimaging, PIDReg achieves MAE of 6.29 years and Corr of 0.75, aligning with clinical evidence that structural MRI is more informative than functional MRI.

## Method Summary
PIDReg employs a two-stage optimization process. First, modality-specific encoders transform inputs into latent representations, which are then regularized toward Gaussianity using conditional sliced Wasserstein distance. The framework analytically computes PID components (unique, redundant, synergistic information) under the Gaussian assumption. A conditional independence regularizer ensures unique information is isolated within each modality. Fusion weights for combining modality contributions are optimized until convergence. In the second stage, fusion weights are fixed and encoders are fine-tuned. The framework includes an information bottleneck mechanism that injects noise to enable meaningful information-theoretic estimation.

## Key Results
- Outperforms six state-of-the-art multimodal regression methods on six real-world datasets
- Achieves MAE of 6.29 years and Corr of 0.75 in brain age prediction from multimodal neuroimaging
- Provides intrinsic interpretability by decomposing predictions into unique, redundant, and synergistic modality contributions
- Enables informed modality selection for efficient inference through PID-based analysis

## Why This Works (Mechanism)

### Mechanism 1
The Gaussian assumption with union information makes partial information decomposition tractable for continuous high-dimensional variables. By constraining the joint latent distribution to be multivariate Gaussian and specifying union information as the redundancy-defining constraint, the underdetermined PID system becomes fully determined with closed-form mutual information expressions. Core assumption: Latent representations can be regularized toward Gaussianity while preserving task-relevant information. Evidence anchors: Synthetic experiments show accurate PID recovery; related work explores similar Gaussian-PID directions. Break condition: If latent representations cannot be made approximately Gaussian without severe information loss, PID estimates become unreliable.

### Mechanism 2
CS divergence-based regularization enables stable Gaussianity enforcement and conditional independence. CS divergence provides symmetric, finite-valued distribution comparison even with limited support overlap, enabling marginal Gaussianity, joint Gaussianity via Shapiro-Wilk, and conditional mutual information approximation in closed form. Core assumption: Kernel-based CS estimators accurately capture distributional discrepancies in latent space. Evidence anchors: Comparison with KL/MMD shows distributional distortion when regularizers removed. Break condition: With high-dimensional latents and small batch sizes, kernel width selection becomes critical; poor choices cause gradient instability.

### Mechanism 3
Linear noise information bottleneck creates stochasticity necessary for information-theoretic estimation. Injecting noise as a transformation of deterministic encodings into stochastic variables enables finite mutual information estimates while adaptively controlling information flow. Core assumption: Noise level can be learned to balance information preservation and regularization. Evidence anchors: Ablation shows RMSE degradation when noise level is fixed at 1. Break condition: If noise level converges to extreme values early in training, the bottleneck either over-regularizes or fails to provide stochasticity.

## Foundational Learning

- **Partial Information Decomposition (PID)**
  - Why needed here: Core theoretical framework defining unique, redundant, and synergistic information contributions between modalities
  - Quick check question: Given two modalities Z₁, Z₂ and target Y, can you write the four PID equations relating I(Y;Z₁,Z₂) to U_Z₁, U_Z₂, R, S?

- **Gaussian Multivariate Entropy and Mutual Information**
  - Why needed here: Enables closed-form PID computation under Gaussian assumption
  - Quick check question: For jointly Gaussian variables, what is the expression for I(Y;Z) in terms of covariance matrices?

- **Information Bottleneck Principle**
  - Why needed here: Controls information flow through each modality encoder
  - Quick check question: Why must mappings be stochastic (rather than deterministic) for meaningful mutual information estimation in continuous settings?

## Architecture Onboarding

- **Component map:** X₁, X₂ → [Encoders h_φ₁, h_φ₂] → R₁, R₂ → [Linear-noise IB] → Z₁, Z₂ → [Gaussian PID Module] ← Y → w₁, w₂, w₃ → Z = w₁Z₁ + w₂Z₂ + w₃(Z₁⊙Z₂) → [Predictor f_θ] → Ŷ

- **Critical path:**
  1. Verify latent representations are approximately Gaussian before PID optimization converges
  2. Monitor PID weight stability (δ_t = ||G_t - G_{t-1}||_∞ < ε for 5 epochs) to trigger Stage II
  3. Ensure L_CMI prevents information leakage (cross-prediction R² < 0.35 as sanity check)

- **Design tradeoffs:**
  - Gaussian assumption: Tractable optimization vs. potential information loss for highly non-Gaussian latents
  - Hadamard synergy modeling: Parameter-free and efficient vs. limited expressiveness for complex interactions
  - Two-stage optimization: Faster fusion weight convergence vs. potential suboptimal encoder training

- **Failure signatures:**
  - Gaussianity collapse: SW statistic W significantly below threshold (W < W_α consistently)—latent representations remain non-Gaussian
  - PID instability: Fusion weights oscillate without convergence after 50+ epochs
  - Information leakage: Cross-modal probe R² > 0.5 indicates L_CMI insufficient
  - Bottleneck collapse: λ_m → 0 early (over-regularization) or λ_m → 1 (deterministic mapping)

- **First 3 experiments:**
  1. **Synthetic validation**: Generate data with known PID components; verify estimated U, R, S correlate with ground truth weights
  2. **Gaussianity ablation**: Train with/without L_CS, L_Gauss; visualize 2D projections and check SW statistic
  3. **Modality ablation on Vision&Touch**: Full model vs. Visual-only vs. Touch-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the PIDReg framework be extended to provide sample-level (instance-level) interpretability for individual predictions?
- Basis in paper: Appendix H states, "For future work, we aim to extend this capability toward sample-level (instance-level) interpretability," noting the current limitation to modality-level insights
- Why unresolved: The current architecture converges to fixed fusion weights and PID values across the dataset, lacking mechanisms to identify the most informative modality for a specific instance during inference
- What evidence would resolve it: A proposed mechanism that dynamically computes or adjusts PID components and fusion weights based on the specific latent features of a single test sample

### Open Question 2
- Question: How can the Gaussian PID computation be effectively adapted for classification tasks where the target variable is discrete?
- Basis in paper: Appendix H outlines extending the framework to "classification settings" and mentions potential approaches like "discretization of latent variables" or "logit-based continuous approximation" without implementing them
- Why unresolved: The analytical solution for PID relies on the joint multivariate Gaussian distribution, which assumes a continuous target variable; discrete targets break the covariance estimation logic
- What evidence would resolve it: A reformulation of the optimization objective or the regularization terms to handle discrete probability mass functions or continuous logit surrogates effectively

### Open Question 3
- Question: What is the theoretical impact on decomposition fidelity if the latent representations fail to achieve the enforced Gaussianity?
- Basis in paper: The framework relies on a strong inductive bias enforcing Gaussianity. While Appendix G.4 tests robustness, it acknowledges that ignoring higher-order information can lead to underestimation of synergy
- Why unresolved: It remains unclear if there are specific distributional "blind spots" where the Gaussian approximation fundamentally misattributes unique information as redundant or synergistic
- What evidence would resolve it: A theoretical bound or empirical sensitivity analysis quantifying the error in PID estimates relative to the divergence of the latent distribution from Gaussianity

## Limitations
- Gaussian assumption validity: The Gaussianity constraint may compress non-Gaussian structure critical for some domains
- CS divergence kernel selection: Kernel width is not specified and may critically affect regularization stability
- Synergy modeling expressiveness: Hadamard product assumes multiplicative interaction structure, potentially missing more complex synergistic patterns

## Confidence
- Predictive performance claims (MAE, RMSE, Corr): **High** (validated across six datasets)
- PID interpretability claims: **Medium** (strong synthetic validation, limited real-world interpretability studies)
- Gaussian-PID tractability: **High** (analytical derivations confirmed)
- Conditional independence regularization: **Low-Medium** (mechanism sound but limited external validation)

## Next Checks
1. **Non-Gaussian ablation**: Train PIDReg on synthetic non-Gaussian data and measure PID estimation error versus ground truth
2. **Kernel sensitivity analysis**: Systematically vary CS divergence kernel width σ across orders of magnitude; measure impact on convergence stability and final performance
3. **Synergy structure ablation**: Replace Hadamard product with learned interaction layer on synthetic data with known synergistic patterns