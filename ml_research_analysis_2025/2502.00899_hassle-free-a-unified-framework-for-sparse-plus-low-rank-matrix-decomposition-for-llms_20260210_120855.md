---
ver: rpa2
title: 'HASSLE-free: A unified Framework for Sparse plus Low-Rank Matrix Decomposition
  for LLMs'
arxiv_id: '2502.00899'
source_url: https://arxiv.org/abs/2502.00899
tags:
- low-rank
- sparse
- pruning
- arxiv
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HASSLE-free, a unified framework for one-shot
  sparse plus low-rank matrix decomposition of large language models. The method directly
  minimizes a layer-wise reconstruction error using alternating minimization between
  sparse and low-rank components, with the full Hessian of the reconstruction error
  rather than approximations used in prior work.
---

# HASSLE-free: A unified Framework for Sparse plus Low-Rank Matrix Decomposition for LLMs

## Quick Facts
- arXiv ID: 2502.00899
- Source URL: https://arxiv.org/abs/2502.00899
- Reference count: 40
- Key outcome: HASSLE-free reduces test perplexity by 12% on WikiText-2 and improves 8 zero-shot tasks by 15% vs. state-of-the-art for Llama3-8B with 2:4 sparsity + 64-rank decomposition.

## Executive Summary
HASSLE-free introduces a unified framework for one-shot sparse plus low-rank matrix decomposition of large language models. The method directly minimizes a layer-wise reconstruction error using alternating minimization between sparse and low-rank components, with the full Hessian of the reconstruction error rather than approximations used in prior work. For Llama3-8B with 2:4 sparsity plus 64-rank decomposition, HASSLE-free reduces test perplexity by 12% on WikiText-2 and improves the average of eight zero-shot tasks by 15% compared to state-of-the-art methods. The framework scales to models with billions of parameters and uses efficient gradient-based methods for the low-rank component while supporting various pruning algorithms for the sparse component.

## Method Summary
HASSLE-free performs one-shot decomposition of LLM weight matrices (W) into semi-structured sparse (WS) plus low-rank (M=UV^T) components by minimizing layer-wise reconstruction error ||XW - X(W_S + M)||_F^2. The framework uses alternating minimization with 80 iterations, where each iteration alternates between a sparse step (P1) using SparseGPT to prune the residual W-UV^T and a low-rank step (P2) using gradient descent on U, V. The method employs diagonal scaling D^{-1}HD^{-1} for numerical stability and supports various pruning algorithms for the sparse component while using efficient gradient-based methods for the low-rank component. Evaluation is performed on WikiText-2 perplexity and eight zero-shot tasks using Llama3-8B.

## Key Results
- 12% reduction in WikiText-2 test perplexity compared to state-of-the-art methods
- 15% improvement in average performance across eight zero-shot tasks (PIQA, ARC, etc.)
- Achieves 2:4 sparsity with 64-rank decomposition while maintaining model utility

## Why This Works (Mechanism)
The framework works by directly optimizing the reconstruction error using the full Hessian matrix, which captures the curvature information of the optimization landscape. The alternating minimization strategy allows for efficient optimization of the non-convex problem by decomposing it into tractable subproblems. The diagonal scaling addresses numerical instability that arises from ill-conditioned Hessians, enabling stable gradient descent in the low-rank component optimization.

## Foundational Learning
- **Hessian Matrix**: Second-order derivative matrix capturing curvature information; needed for accurate gradient-based optimization and stable convergence.
  - Quick check: Verify H = X^TX + λI is positive definite for calibration data X.

- **Alternating Minimization**: Optimization technique that alternates between optimizing different subsets of variables; needed to handle the non-convex nature of sparse plus low-rank decomposition.
  - Quick check: Monitor reconstruction error convergence across 80 iterations.

- **Diagonal Scaling**: Technique to normalize matrix eigenvalues for numerical stability; needed to prevent gradient explosion/collapse in ill-conditioned problems.
  - Quick check: Compare training stability with and without D^{-1}HD^{-1} scaling.

## Architecture Onboarding

**Component Map:** Calibration Data -> Hessian Computation -> Alternating Minimization (P1: SparseGPT, P2: Adam GD) -> Compressed Model

**Critical Path:** Calibration Data → Hessian Computation → Alternating Minimization (80 iterations) → Compressed Model Assembly → Evaluation

**Design Tradeoffs:** Full Hessian vs. approximation methods (accuracy vs. computational cost), alternating minimization vs. joint optimization (tractability vs. global optimality), diagonal scaling vs. no scaling (stability vs. simplicity).

**Failure Signatures:** Divergence during low-rank optimization (ill-conditioning), poor perplexity (inadequate calibration data or suboptimal sparsity allocation), slow convergence (inappropriate learning rate).

**First Experiments:** 1) Validate diagonal scaling on synthetic matrix decomposition, 2) Test alternating minimization convergence on small 125M parameter model, 3) Compare full Hessian vs. diagonal approximation performance.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the sparse subproblem (P1) be extended to incorporate quantization constraints for a quantized-sparse plus low-rank decomposition? The conclusion states future work can extend (P1) to include quantization and quantized-sparse compression, requiring reformulation of the optimization objective to handle discrete constraints.

- **Open Question 2:** For a fixed total compression ratio, what is the optimal allocation of parameters between the sparse and low-rank components? Section 5.2 uses fixed rank r=64 but does not explore dynamic adjustment of rank-to-sparsity ratio, leaving unclear if current heuristic balance captures optimal trade-off.

- **Open Question 3:** How robust is the full-Hessian optimization to reductions in the size of the calibration dataset? Section 5.1 utilizes fixed calibration setup (128 segments from C4) without exploring data efficiency, though full Hessian methods may require more calibration data than diagonal approximations.

## Limitations
- Integration of SparseGPT with residual weights is not fully specified, requiring implementation decisions about standard vs. modified parameters.
- Variance parameter for initializing matrix V is unspecified, potentially affecting convergence behavior.
- Calibration data generation process may introduce variability if not precisely replicated.

## Confidence
- **High confidence:** Alternating minimization framework structure, diagonal scaling for numerical stability, evaluation methodology (WikiText-2 perplexity, zero-shot tasks).
- **Medium confidence:** Implementation details of sparse step integration with SparseGPT, parameterization of gradient descent step.
- **Low confidence:** None identified beyond implementation specifics mentioned above.

## Next Checks
1. Verify diagonal scaling implementation by testing on a small synthetic matrix decomposition problem to ensure stability during optimization.
2. Test complete pipeline on a smaller model (e.g., 125M parameters) to validate alternating minimization convergence before scaling to Llama3-8B.
3. Implement variant using same Hessian approximation as SparseGPT (rather than full Hessian) to quantify contribution of full Hessian computation to performance gains.