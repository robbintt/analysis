---
ver: rpa2
title: Anomaly Detection Based on Critical Paths for Deep Neural Networks
arxiv_id: '2505.14967'
source_url: https://arxiv.org/abs/2505.14967
tags:
- detection
- paths
- critical
- anomaly
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called Anomaly Detection based on
  Critical Paths (ADCP) for detecting various types of anomaly samples in deep neural
  networks. The core idea is to extract critical detection paths from the DNN using
  genetic evolution and mutation, and then use these paths to distinguish between
  normal and anomaly samples.
---

# Anomaly Detection Based on Critical Paths for Deep Neural Networks

## Quick Facts
- arXiv ID: 2505.14967
- Source URL: https://arxiv.org/abs/2505.14967
- Reference count: 40
- Primary result: Introduces ADCP method that outperforms state-of-the-art anomaly detection on DNNs across multiple anomaly types

## Executive Summary
This paper presents Anomaly Detection based on Critical Paths (ADCP), a method for detecting various anomaly types in deep neural networks. The approach extracts critical detection paths through genetic evolution and uses these paths to distinguish between normal and anomalous samples. By training SVDD models on path activations and ensembling results across multiple paths, ADCP achieves superior performance compared to existing methods for detecting adversarial, out-of-distribution, and noise samples across multiple DNN architectures and datasets.

## Method Summary
ADCP works by first evolving critical detection paths from a pre-trained DNN using a genetic algorithm. The algorithm mutates neuron sequences and selects paths that maximize True Positive Rate on a mixed validation set containing both normal and anomaly samples. These critical paths are then used to extract activation features, which train separate SVDD models. The final detection decision aggregates SVDD scores across multiple paths using a voting mechanism, with thresholds calibrated to maintain high true negative rates on normal data.

## Key Results
- ADCP achieves 0.9852 AUROC on MNIST for adversarial detection, outperforming DeepSAD (0.9695) and Deep Mahalanobis (0.9425)
- For OOD detection, ADCP reaches 0.9923 AUROC on CIFAR-10, significantly better than Deep Mahalanobis (0.9557) and DeepSAD (0.9819)
- The method demonstrates strong generalization, with ADCP* (trained on FGSM) maintaining 0.9850 AUROC on CIFAR-10 when tested against PGD attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Anomalous inputs induce statistically distinct activation patterns on specific neuron sequences compared to normal inputs
- **Mechanism:** DNNs can be viewed as weighted directed graphs where specific paths capture essential features for a class. By selecting paths where normal and anomaly distributions diverge significantly, the method isolates the "generalization area" of the model
- **Core assumption:** Anomalies (adversarial, OOD, or noise) cannot simultaneously trigger the exact same high-value neuron sequence as in-distribution data for a given class
- **Evidence anchors:** [abstract] "outliers and adversarial inputs do not usually induce the same activation pattern on those paths as normal (in-distribution) inputs"; [section 2] "If an input $x$ is outside the generalization area, we usually take it as an anomaly"
- **Break condition:** If an adversarial attack is specifically crafted to maximize activation along the identified critical paths (adaptive attack), the separation boundary may collapse

### Mechanism 2
- **Claim:** Evolutionary selection identifies paths with higher discriminative power than random or coverage-based paths
- **Mechanism:** The algorithm treats path selection as an optimization problem. By mutating neuron sequences and retaining those that maximize the True Positive Rate (TPR) on a mixed validation set, the system converges on paths that act as efficient feature extractors for anomaly detection
- **Core assumption:** The TPR metric calculated on the mixed set is a valid proxy for the path's generalization capability to unseen anomaly types
- **Evidence anchors:** [section 3.1] "We evolve the critical path by mutating one neuron at a time... if the score of the mutated path is higher, then the old path is (greedily) replaced"; [section 4.2 (RQ1)] Figure 3 shows "Critical" paths consistently outperforming "Initial" paths in AUROC
- **Break condition:** If the mutation iterations ($n=5000$) are insufficient for deep models or the search space is too vast, the algorithm may converge on a local optimum indistinguishable from a random path

### Mechanism 3
- **Claim:** Ensembling multiple uncorrelated paths reduces detection variance and improves robustness
- **Mechanism:** Different paths capture different features. By training separate SVDD models on $m$ paths and using a voting mechanism, the system reduces the risk of relying on a single feature subspace that might be vulnerable to a specific anomaly type
- **Core assumption:** The selected paths are sufficiently diverse (low Pearson correlation) to provide independent detection signals
- **Evidence anchors:** [section 3.2] "Since different paths in a DNN often capture different features... we ensemble detection results from multiple paths"; [section 4.6] Figure 8 visualization shows distinct sets of anomalies detected by different paths (Path0 vs Path1)
- **Break condition:** If the genetic algorithm consistently selects highly correlated paths (confirmation bias), the ensemble effect diminishes, offering no improvement over a single path

## Foundational Learning

- **Concept:** **Support Vector Domain Description (SVDD)**
  - **Why needed here:** This is the core one-class classification engine. You must understand how SVDD maps input features into a hypersphere to distinguish "normal" data points from outliers
  - **Quick check question:** How does the SVDD radius $R$ change if the regularization parameter $\nu$ is decreased?

- **Concept:** **Genetic Algorithms (Mutation & Selection)**
  - **Why needed here:** The "Critical Path" extraction is not analytical but evolutionary. Understanding the trade-off between exploration (mutation) and exploitation (greedy selection) is key to debugging the path extraction phase
  - **Quick check question:** In Algorithm 1, if `LastVisit == l`, the iteration is skipped. Why is this check necessary for the stability of the evolutionary process?

- **Concept:** **Deep Neural Network Activation Topology**
  - **Why needed here:** The method treats a DNN as a graph of neurons. You need to understand how activation values flow through layers and how a "path" (one neuron per layer) represents a slice of the decision logic
  - **Quick check question:** For a Convolutional layer, how is the scalar activation value $s_j^i(x)$ derived from the channel output, and why does averaging pooling facilitate this?

## Architecture Onboarding

- **Component map:** Target Model -> Path Evolution Engine -> Feature Extractor -> SVDD Bank -> Voting Arbiter
- **Critical path:** The **Path Evolution Engine** (Section 3.1). This is the most computationally expensive step ($O(n)$ mutations) and defines the success of the entire system. If this step yields paths with low TPR, the downstream SVDDs cannot recover the performance
- **Design tradeoffs:**
  - **Mixed Set Dependency:** The evolution phase requires a "mixed set" containing some anomalies (e.g., FGSM or OOD samples) to calculate TPR. This makes the method *semi-supervised* regarding anomaly types during training, even if it generalizes to unseen anomalies later
  - **Path Count ($m$):** Increasing $m$ improves accuracy (Section 4.5) but linearly increases inference latency and memory overhead (storing more SVDD models)
- **Failure signatures:**
  - **Symptom:** Low AUROC on specific attacks (e.g., CW) while high on others (e.g., FGSM). **Diagnosis:** The critical paths evolved using a specific anomaly type (e.g., FGSM) do not generalize; consider re-training with a mixed anomaly set
  - **Symptom:** High False Positive Rate (FPR). **Diagnosis:** The threshold $\tau$ (set to ensure 95% TNR) is too permissive for the specific class, or the SVDD sphere is too large
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run Algorithm 1 with $n=0$ (random paths) vs. $n=5000$ on the MNIST dataset to replicate the "Initial vs. Critical" gap shown in Figure 3
  2. **Hyperparameter Sensitivity:** Vary the number of paths ($m=1, 3, 10, 21$) to plot the accuracy vs. cost curve (replicate Figure 6/7 trends)
  3. **Generalization Test:** Train path extractors using FGSM anomalies only, then test against PGD and CW attacks to verify if the "ADCP*" setting generalizes as claimed in Table 2

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the ADCP methodology be effectively adapted for non-image data domains such as Natural Language Processing or time-series analysis? The paper focuses only on image classification and suggests exploring other applications
- **Open Question 2:** Is there a mathematically rigorous definition or proof of optimality for the critical detection paths extracted via genetic evolution? The paper acknowledges the method "lacks mathematical proof" and suggests future work
- **Open Question 3:** How can the computational cost of the path extraction phase be reduced for very deep neural networks with large class sizes? The paper identifies the key operation in Algorithm 1 as potentially "too expensive in some scenarios"

## Limitations
- The method's performance depends heavily on the quality of the initial path selection and the diversity of the evolved paths, with no discussion of sensitivity to initial random seeds
- The computational cost of evolving paths (5000 iterations per class) may be prohibitive for very deep networks or large-scale applications
- The assumption that critical paths capture the "generalization area" is theoretical and lacks direct empirical validation

## Confidence
- **High confidence:** The experimental results showing ADCP's superiority over baseline methods (Tables 1-3) are well-supported by the data and methodology described
- **Medium confidence:** The claim that ADCP generalizes to unseen anomaly types (based on ADCP* results) is plausible but relies on the assumption that the mixed set contains representative anomalies
- **Low confidence:** The assertion that critical paths capture the "generalization area" of the model is more theoretical and lacks direct empirical validation

## Next Checks
1. **Ablation study on path diversity:** Measure the Pearson correlation between the top 21 paths to quantify their independence. If correlations are high, the ensemble benefit may be overstated
2. **Sensitivity analysis on mixed set composition:** Systematically vary the ratio and types of anomalies in the mixed set used for path evolution, and measure the impact on detection performance for unseen anomaly types
3. **Adaptive attack evaluation:** Design an attack specifically targeting the identified critical paths (e.g., by maximizing activation along those paths) to test the method's robustness against white-box adversaries