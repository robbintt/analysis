---
ver: rpa2
title: Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical
  Review of Generative Social Simulations
arxiv_id: '2504.03274'
source_url: https://arxiv.org/abs/2504.03274
tags:
- social
- agents
- abms
- arxiv
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review of generative agent-based modeling (ABM)
  literature reveals significant shortcomings in validation practices. While generative
  ABMs promise to resolve traditional ABM limitations by leveraging large language
  models (LLMs) for more realistic human behavior simulation, the review found that
  15 of 35 papers rely solely on subjective "believability" assessments, with 22 using
  it as their primary technique.
---

# Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations

## Quick Facts
- arXiv ID: 2504.03274
- Source URL: https://arxiv.org/abs/2504.03274
- Reference count: 40
- Primary result: 15 of 35 generative ABM papers rely solely on subjective "believability" assessments, with most using zero-shot LLMs without fine-tuning or sensitivity analysis

## Executive Summary
This systematic review examines validation practices in generative agent-based modeling (ABM) literature that uses large language models (LLMs) to simulate human behavior. While generative ABMs promise to resolve traditional ABM limitations through more realistic human behavior simulation, the review reveals that current practices fall short of rigorous validation standards. The majority of studies rely on subjective believability assessments rather than quantitative comparisons with real-world data, and most use zero-shot models without fine-tuning or sensitivity analyses.

The review argues that LLMs exacerbate rather than resolve ABM's long-standing challenges, including computational costs, black-box complexity, and difficulty with empirical grounding. Only a small fraction of studies conduct quantitative validation against real-world data, while most rely on human or LLM judgment. The authors question whether generative ABMs can transition from toy models to socially scientific tools that meaningfully contribute to theory development, given these methodological shortcomings.

## Method Summary
The authors conducted a systematic literature review using a Boolean search on Scopus (TITLE-ABS-KEY fields) with multiple query variations for generative ABM-related terms. The search was performed on 2025-03-27 and supplemented with backward snowballing from five prior surveys. Papers underwent two-step screening (title/abstract then full-text) before in-depth reading and classification. Each paper was classified by target phenomena (nine categories including profile alignment, emotion, conversation, decisions, and social dynamics) and validation technique (five categories including human judgment, social patterns, and internal consistency). The review identified 35 eligible papers for analysis.

## Key Results
- 15 of 35 papers rely solely on subjective "believability" assessments for validation
- 22 papers use believability as their primary validation technique
- Most studies use zero-shot models without fine-tuning or sensitivity analysis
- Validation typically involves human or LLM judgment rather than rigorous quantitative comparison with real-world data

## Why This Works (Mechanism)
The review's systematic approach provides a comprehensive overview of current validation practices in generative ABM literature, revealing patterns and gaps through structured classification. By combining database search with snowballing from prior surveys, the methodology captures a representative sample of the field while identifying methodological weaknesses that individual studies might overlook. The classification framework allows for comparison across different validation approaches and phenomenon types.

## Foundational Learning
- **Validation techniques in ABM**: Understanding different validation approaches (human judgment, social patterns, quantitative comparison) is needed to assess research quality. Quick check: Identify which validation technique your ABM study uses.
- **Generative social simulation**: Concept of using LLMs to create realistic agent behaviors in ABMs. Quick check: Can you distinguish between generative and traditional ABMs?
- **Believability assessment**: Subjective evaluation of agent behavior plausibility. Quick check: What are the limitations of relying solely on believability?
- **Zero-shot vs fine-tuned models**: Understanding when and why models are pre-trained versus adapted to specific tasks. Quick check: Does your study use zero-shot or fine-tuned LLMs?
- **Operational validity**: Whether models capture underlying social mechanisms rather than surface features. Quick check: How would you validate operational validity in your model?
- **Sensitivity analysis in ABM**: Testing model robustness across parameter variations. Quick check: Has your study conducted sensitivity analysis?

## Architecture Onboarding
- **Component map**: Search query -> Database retrieval -> Title/abstract screening -> Full-text review -> Classification (phenomena + validation) -> Synthesis
- **Critical path**: Boolean search execution → paper screening → in-depth classification → validation pattern identification
- **Design tradeoffs**: Comprehensive coverage vs. search term completeness; subjective classification vs. objective metrics; snowballing depth vs. search efficiency
- **Failure signatures**: Incomplete search coverage (missing papers), inconsistent classification criteria, overlooking validation methods not explicitly labeled as such
- **3 first experiments**: 1) Execute search query and verify paper count, 2) Classify 5 papers to test category definitions, 3) Compare findings with one prior survey for consistency

## Open Questions the Paper Calls Out
**Open Question 1**: Can generative Agent-Based Models (ABMs) transition from proof-of-concept "toy models" to rigorous tools that meaningfully contribute to social scientific theory? The review indicates current validation practices are insufficient, with 15 of 35 papers relying solely on subjective assessments of "believability" rather than empirical rigor.

**Open Question 2**: How can researchers evidence "operational validity" in generative ABMs to ensure models capture underlying social mechanisms rather than just surface-level text similarity? Current validation often focuses on superficial syntactical features that may be irrelevant to the model's actual causal mechanisms.

**Open Question 3**: Do generative ABMs offer distinct epistemic value for explanation, or does their "black-box" nature obscure the simplicity required for theoretical insight? While LLMs increase realism, they introduce billions of parameters that make it "virtually impossible to understand why a particular model gives a particular output."

**Open Question 4**: How can the high computational costs of generative agents be reconciled with the methodological necessity of conducting sensitivity analyses and multiple runs? Generative ABMs are "orders of magnitude more computationally demanding," yet stochastic nature requires multiple runs for statistical reliability.

## Limitations
- Single database search (Scopus) may miss relevant papers from other venues
- Backward snowballing from only five seed papers introduces potential selection bias
- Classification requires subjective judgment with unclear inter-rater reliability procedures
- Review period limited to papers available through March 27, 2025

## Confidence
- **High confidence**: 15/35 papers rely solely on subjective believability assessments (straightforward counting)
- **Medium confidence**: Characterization of validation practices as generally weak (depends on judgment about rigor)
- **Medium confidence**: Claim that LLMs exacerbate ABM's traditional challenges (synthesizes findings across multiple papers)
- **Lower confidence**: Broader implications about generative ABMs' inability to contribute to social science theory (extrapolates from current practices)

## Next Checks
1. Replicate the search using multiple databases (Web of Science, ACM Digital Library, arXiv) and compare results to assess completeness
2. Have two independent reviewers classify a random sample of 10 papers and compute inter-rater reliability statistics
3. Contact authors of papers classified as using only believability assessments to confirm their validation approach and explore unreported quantitative validation