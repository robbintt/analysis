---
ver: rpa2
title: Riemannian Neural Geodesic Interpolant
arxiv_id: '2504.15736'
source_url: https://arxiv.org/abs/2504.15736
tags:
- riemannian
- stochastic
- distribution
- rngi
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Riemannian Neural Geodesic Interpolant
  (RNGI), a generative model designed to bridge two arbitrary probability distributions
  on Riemannian manifolds, extending stochastic interpolants from Euclidean to curved
  spaces. RNGI uses geodesic interpolation and solves a transport equation on manifolds,
  with neural networks learning velocity and score fields.
---

# Riemannian Neural Geodesic Interpolant

## Quick Facts
- **arXiv ID:** 2504.15736
- **Source URL:** https://arxiv.org/abs/2504.15736
- **Reference count:** 40
- **Primary result:** Introduces RNGI, a generative model that bridges probability distributions on Riemannian manifolds using geodesic interpolation and E-SDE sampling.

## Executive Summary
This paper presents the Riemannian Neural Geodesic Interpolant (RNGI), a generative model that extends stochastic interpolants from Euclidean to curved spaces. RNGI bridges two arbitrary probability distributions on Riemannian manifolds by constructing a continuous probability path along geodesic curves, using neural networks to learn velocity and score fields that solve a transport equation. The model introduces an Embedding Stochastic Differential Equation (E-SDE) algorithm that significantly improves sampling quality by reducing discretization errors inherent in traditional Geodesic Random Walk methods.

## Method Summary
RNGI constructs geodesic interpolants between probability distributions using exponential and logarithmic maps, then trains neural networks to learn velocity and score fields that solve a transport equation on the manifold. The model uses two sampling strategies: traditional Geodesic Random Walk (GRW) which operates in local tangent spaces, and the proposed E-SDE which projects ambient Euclidean Brownian motion onto the manifold via a projection operator. For SO(3), RNGI employs a truncation-orthogonalization embedding into R^6 for computational efficiency. The training objective combines quadratic losses for velocity matching and implicit score matching, optimized using Riemannian-specific variants of Adam.

## Key Results
- RNGI achieves superior log-likelihood and Wasserstein-2 distance performance on SÂ² and SO(3) datasets compared to existing Riemannian generative models
- E-SDE sampling algorithm demonstrates 2-3x improvement in Wasserstein distance over traditional GRW methods
- The model successfully interpolates between complex multi-modal distributions while maintaining manifold constraints
- RNGI-E (embedding variant) is approximately 100x faster than RNGI-D (direct matrix variant) for SO(3) operations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model constructs a continuous probability path between two distributions by enforcing the interpolation trajectory to follow the manifold's geodesics (shortest paths).
- **Mechanism:** The Riemannian Geodesic Interpolant $I(t; x_0, x_1)$ is defined using Exponential ($Exp$) and Logarithm ($Log$) maps: $I(t) = Exp_{x_0}(t \cdot Log_{x_0}x_1)$. This ensures that the transitional state $x_t$ remains strictly on the manifold $\mathcal{M}$ at all times $t \in [0,1]$, respecting the curvature constraints that Euclidean models ignore.
- **Core assumption:** The manifold is geodesically complete (meaning geodesics extend indefinitely), and the cut locus (where geodesics stop being unique) has zero measure or can be handled via projection.
- **Evidence anchors:**
  - [abstract] "interpolates between two probability densities... along the stochastic geodesics"
  - [section 3] Definition 4 defines the interpolant via $Exp$ and $Log$ maps; Corollary 7 asserts geodesic completeness for compact manifolds.
  - [corpus] Related work in the corpus often cites geodesic constraints as central to maintaining validity in amorphous particle systems (e.g., "Riemannian Stochastic Interpolants for Amorphous Particle Systems").
- **Break condition:** If the manifold has holes or boundaries such that $Exp$ or $Log$ maps are ill-defined for specific $x_0, x_1$ pairs, the interpolant construction fails.

### Mechanism 2
- **Claim:** The generative process is driven by a transport equation that is solved by learning two time-dependent neural fields: a velocity field and a score field.
- **Mechanism:** The paper proves (Theorem 2) that the marginal density $\rho(t,x)$ satisfies a transport equation $\partial_t \rho + \text{div}[\rho \cdot v] = 0$. By parameterizing the velocity $v$ and score $s$ with neural networks and optimizing specific quadratic losses (Eq 12, 13), the model learns to simulate the flow of probability mass from the source to the target distribution without explicitly computing likelihoods.
- **Core assumption:** The learned velocity field $v(t,x)$ satisfies an integrable condition (Eq 3) to ensure the transport equation holds, and neural networks are sufficiently expressive to approximate these fields.
- **Evidence anchors:**
  - [abstract] "neural networks learning velocity and score fields... temporal marginal density... solves a transport equation"
  - [section 4.1] Eq 12 and 13 define the loss functions for learning $v$ and $s$.
  - [corpus] Weak direct corpus evidence for this specific neural decomposition, but standard in stochastic interpolant literature.
- **Break condition:** If the velocity field is not smooth or Lipschitz, or if the divergence computation is unstable, the simulated path will deviate from the true geodesic path, leading to accumulation of error.

### Mechanism 3
- **Claim:** The Embedding Stochastic Differential Equation (E-SDE) sampling algorithm improves generation quality by reducing the intrinsic discretization errors of standard Geodesic Random Walk (GRW).
- **Mechanism:** Instead of discretizing Brownian motion locally on the tangent space (which causes "overstepping" and projection errors in GRW), E-SDE simulates the process in an ambient Euclidean space $\mathbb{R}^d$. It constructs the manifold's Brownian motion by projecting a $d$-dimensional Euclidean Brownian motion onto the manifold via a projection operator $P$ (Prop 9). This avoids the circular/incorrect projections inherent in local GRW steps.
- **Core assumption:** The Riemannian manifold $\mathcal{M}$ can be embedded into a higher-dimensional Euclidean space (guaranteed by Nash's theorem) and the projection operator $P$ can be computed efficiently.
- **Evidence anchors:**
  - [abstract] "E-SDE significantly improves the sampling quality by reducing the accumulated error caused by the excessive intrinsic discretization... in GRW"
  - [section 4.2.2] Prop 9 defines the projection; Table 1 shows E-SDE supports higher-order solvers compared to GRW.
  - [corpus] "Radial Compensation" paper discusses stability issues in mapping Euclidean spaces to manifolds, implicitly supporting the difficulty E-SDE tries to address.
- **Break condition:** If the projection operator $P(x)$ is numerically unstable (e.g., near singularities of the embedding) or expensive to compute, the sampling efficiency drops or fails.

## Foundational Learning

- **Concept: Exponential and Logarithmic Maps**
  - **Why needed here:** These are the fundamental operations for defining the geodesic interpolant (Definition 4). You cannot implement the forward process or the loss function without computing $Exp_p(v)$ (point at distance $v$ from $p$) and $Log_p(q)$ (tangent vector from $p$ to $q$).
  - **Quick check question:** Given two points on a sphere, how do you compute the shortest path vector connecting them in the tangent space of the first point?

- **Concept: Riemannian Brownian Motion**
  - **Why needed here:** The generative sampling process requires simulating stochastic dynamics. Understanding that Brownian motion on a manifold is different from Euclidean Brownian motion (it depends on the Laplace-Beltrami operator or projections) is crucial for understanding why GRW fails and E-SDE works.
  - **Quick check question:** Why can't you just add Gaussian noise $\mathcal{N}(0, I)$ to a point on a sphere to simulate Brownian motion?

- **Concept: The Transport Equation (Continuity Equation)**
  - **Why needed here:** The paper derives the generative ODE/SDE from this equation (Theorem 2). Understanding the link between the velocity field $v$ and the change in probability density $\rho$ explains why matching the velocity (via the loss in Eq 12) enables sampling.
  - **Quick check question:** If the velocity field $v$ causes density to "pile up" in a region, what must happen to the divergence term $\text{div}[\rho v]$?

## Architecture Onboarding

- **Component map:**
  Data -> Geodesic Interpolant -> Velocity/Score Networks -> Loss Computation -> Trained Model -> E-SDE/GRW Sampler -> Generated Samples

- **Critical path:**
  The implementation of the **Projection Operator $P(x)$** and the **Exp/Log maps** for your specific manifold.
  - *For $S^2$:* $P(x)$ is matrix $I - xx^T$.
  - *For $SO(3)$:* The paper uses a specific "truncation-orthogonalization" embedding into $\mathbb{R}^6$. You must implement $P: \mathbb{R}^6 \to T\mathcal{M}$ correctly or the E-SDE sampler will diverge.

- **Design tradeoffs:**
  - **RNGI-D vs. RNGI-E:** RNGI-D (Direct) works on matrix elements but is slow/differentiation is hard. RNGI-E (Embedding) works in $\mathbb{R}^6$ for $SO(3)$ and is 100x faster but requires the complex orthogonalization embedding (Section 5.2).
  - **Sampling:** GRW is conceptually simpler (intrinsic) but accumulates error quickly (lower order). E-SDE requires embedding math but supports high-order solvers (e.g., Heun) and is more accurate.

- **Failure signatures:**
  - **NaNs in training:** Likely hitting the **cut locus** (e.g., antipodal points on sphere). The paper notes $S^2$ excludes $-p$ and $SO(3)$ excludes matrices with trace $=-1$.
  - **Samples collapsing/diverging:** The projection operator $P$ in E-SDE is incorrect, or the step size is too large for the stiffness of the manifold.
  - **Slow training:** Using the direct $3\times3$ matrix representation for $SO(3)$ instead of the $\mathbb{R}^6$ embedding (RNGI-D vs RNGI-ES).

- **First 3 experiments:**
  1. **Sanity Check (S^2):** Implement RNGI on a 2-Sphere. Visualize the trajectory. Verify that `Exp(x0, Log(x0, x1))` returns `x1` for non-antipodal points.
  2. **GRW vs E-SDE:** Generate samples on $S^2$ using both samplers with large step sizes. Plot the error (Wasserstein distance) to see if GRW "spirals out" or drifts while E-SDE stays stable.
  3. **Embedding Debug (SO(3)):** Test the $\mathbb{R}^6$ truncation-orthogonalization pipeline (Zhou et al.) independently. Ensure you can map a random vector from $\mathbb{R}^6$ to a valid rotation matrix and back consistently.

## Open Questions the Paper Calls Out
None

## Limitations
- The model cannot interpolate between perfectly antipodal points due to cut locus issues on both $S^2$ and $SO(3)$
- Implementation complexity is high, particularly for $SO(3)$ where the truncation-orthogonalization embedding requires careful numerical handling
- Performance improvements are currently limited to specific manifolds ($S^2$ and $SO(3)$) without proof of generalizability to arbitrary Riemannian manifolds

## Confidence
- **High Confidence:** The theoretical framework for geodesic interpolation on Riemannian manifolds and the derivation of the transport equation are mathematically sound and well-established in differential geometry literature.
- **Medium Confidence:** The superiority claims over baseline methods (GRW vs E-SDE sampling quality) are supported by experimental results, though reproducibility is currently limited by unavailable code.
- **Low Confidence:** The generalizability of the E-SDE projection approach to arbitrary manifolds beyond $S^2$ and $SO(3)$ remains unproven, as the projection operator construction appears highly manifold-specific.

## Next Checks
1. **Numerical Stability Verification:** Implement the $SO(3)$ truncation-orthogonalization embedding and test projection operator stability across a range of input values, particularly near the cut locus where trace approaches -1.
2. **Sampling Algorithm Comparison:** Conduct controlled experiments comparing E-SDE against GRW with varying step sizes on $S^2$ to quantify the claimed error reduction, ensuring identical velocity/score networks are used for fair comparison.
3. **Hyperparameter Sensitivity Analysis:** Systematically vary learning rates, batch sizes, and network architectures on the synthetic datasets to determine the robustness of the reported performance improvements and identify potential overfitting to specific configurations.