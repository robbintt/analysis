---
ver: rpa2
title: 'BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks'
arxiv_id: '2504.05180'
source_url: https://arxiv.org/abs/2504.05180
tags:
- graph
- code
- bridges
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BRIDGES addresses the challenge of integrating graph modality into
  large language models (LLMs) for electronic design automation (EDA) tasks. It introduces
  an automated data generation workflow that converts RTL and netlist data into dataflow
  and netlist graphs, creating a large-scale dataset with over 500,000 graph instances
  and 1.5 billion tokens.
---

# BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks

## Quick Facts
- **arXiv ID:** 2504.05180
- **Source URL:** https://arxiv.org/abs/2504.05180
- **Reference count:** 40
- **Primary result:** 2x-10x improvement on EDA tasks by enabling LLMs to process graph modality

## Executive Summary
BRIDGES addresses the challenge of integrating graph modality into large language models (LLMs) for electronic design automation (EDA) tasks. It introduces an automated data generation workflow that converts RTL and netlist data into dataflow and netlist graphs, creating a large-scale dataset with over 500,000 graph instances and 1.5 billion tokens. BRIDGES employs a lightweight cross-modal projector that encodes graph representations into text-compatible prompts, enabling LLMs to effectively utilize graph data without architectural modifications. Experimental results demonstrate significant improvements across multiple tasks compared to text-only baselines, including accuracy in design retrieval (up to 93% vs. 46% for text), type prediction (73% vs. 63%), and function description generation (perplexity of 2.08 vs. 4.65), with negligible computational overhead (<1% model weights increase and <30% additional runtime overhead).

## Method Summary
BRIDGES converts RTL and netlist data into dataflow and netlist graphs, creating a large-scale dataset with over 500,000 graph instances. It employs a two-stage training approach: Stage 1 trains a Graph Neural Network (NetlistGNN) and cross-modal projector (Q-Former) using contrastive learning to align graph and text embeddings; Stage 2 fine-tunes the projector to align with a pre-trained LLM (Llama 3) for specific EDA tasks. The framework uses soft prompts (query tokens) to bridge the graph and text modalities, enabling LLMs to process structural information without architectural modifications. The approach achieves significant improvements across design retrieval, type prediction, function description generation, and area/power estimation tasks while maintaining negligible computational overhead.

## Key Results
- **Design Retrieval:** 93% accuracy vs. 46% for text-only baselines (R@20 metric)
- **Type Prediction:** 73% accuracy vs. 63% for text-only approaches
- **Function Description:** Perplexity of 2.08 vs. 4.65 for text-only methods
- **Scalability:** <30% additional runtime overhead with negligible (<1%) model weights increase

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Alignment via Querying Transformer
The framework enables LLMs to process graph data by translating graph topology into "soft prompts" that the LLM already knows how to interpret. A lightweight cross-modal projector (Q-Former) uses learnable query tokens that interact with graph embeddings via cross-attention and then interact with text embeddings via self-attention. This forces the queries to extract only the graph features relevant to the corresponding text description. The core assumption is that 8 query tokens are sufficient to compress high-dimensional structural information into a format the LLM can decode.

### Mechanism 2: Structural Preservation vs. Sequential Linearization
Explicitly encoding graphs structurally avoids information loss that occurs when flattening a graph into sequential text. Instead of feeding flattened netlists (which can exceed 100k tokens for simple designs), a Graph Neural Network preserves topology, allowing the model to "reason" about connectivity rather than just processing a stream of syntax. The core assumption is that text-only LLMs fail primarily due to context length limits and inability to track structural relationships in linear text.

### Mechanism 3: Two-Stage Decoupled Training
Separating representation learning (Stage 1) from task-specific alignment (Stage 2) allows the model to generalize graph-text relationships without requiring end-to-end retraining of the LLM. Stage 1 trains the Graph Encoder and Projector using contrastive loss and graph-text matching, while Stage 2 aligns these outputs with the LLM. This modularizes the system, allowing the LLM to remain frozen while learning the "graph language." The core assumption is that knowledge required to map a graph to a text concept is transferable from pre-training to downstream tasks.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs) and Message Passing
  - **Why needed here:** BRIDGES relies on NetlistGNN to encode the circuit. Understanding how node embeddings aggregate neighbor information is critical to debugging the encoder.
  - **Quick check question:** How does a node in a netlist graph update its representation based on the logic gates (neighbors) connected to it?

- **Concept:** Cross-Attention and Soft Prompts
  - **Why needed here:** The Q-Former uses cross-attention to "query" the graph. This is the interface between the graph and the LLM.
  - **Quick check question:** In the Q-Former, do the query tokens attend to the text, the graph, or both during the pre-training phase?

- **Concept:** Contrastive Learning (InfoNCE)
  - **Why needed here:** The paper uses Graph-Text Contrastive Learning (GTC) to align the embedding spaces.
  - **Quick check question:** What constitutes a "negative pair" in the context of aligning a netlist graph with a function description?

## Architecture Onboarding

- **Component map:** RTL/Netlist Graph -> NetlistGNN -> Graph Embedding -> Q-Former -> Soft Prompts -> LLM -> Output

- **Critical path:** The scalability of the Graph Encoder. The paper notes that graphs can have up to 800,000 nodes. The GNN forward pass is identified as the primary computational bottleneck before the LLM.

- **Design tradeoffs:**
  - **Query Token Count (q):** The paper uses q=8. Increasing q increases information bandwidth but also increases the sequence length the LLM must process.
  - **Encoder Dimension (d1):** d1=512 provided consistent improvement, while d1=128 overfitted.
  - **Frozen LLM:** Keeping the LLM frozen reduces cost significantly but drops accuracy (e.g., Type Prediction 73.0% â†’ 59.1%).

- **Failure signatures:**
  - **Overfitting Encoder:** Validation accuracy decreases as training data increases (seen with smaller encoders).
  - **Catastrophic Forgetting:** Occurs if Graph2Text is used (text-only) with long contexts; the LLM "loses" the instruction in the noise of the netlist.

- **First 3 experiments:**
  1. **In-Batch Retrieval Test:** Replicate Table III. Verify that the embedding similarity of a graph and its correct text description is higher than with random descriptions in the same batch.
  2. **Scalability Profiling:** Measure memory consumption of NetlistGNN on graphs with >100k nodes to validate the claim of "negligible overhead" on standard hardware.
  3. **Zero-Shot Type Prediction:** Run the model on the "Type Classification" task with the LLM strictly frozen to verify that the soft prompts alone carry sufficient signal.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does BRIDGES scale when applied to generative code synthesis tasks (e.g., generating RTL from graphs) rather than discriminative tasks like classification or retrieval? The Introduction discusses RTL code generation as a primary motivation, but experimental evaluation focuses exclusively on design retrieval, type prediction, function description, and PPA estimation, omitting the task of generating RTL code itself.

- **Open Question 2:** What specific architectures or modifications to the graph encoder (currently NetlistGNN) would yield the most significant performance gains for the BRIDGES framework? The Conclusion states that "the study of graph encoder deserves more exploration," and Section IV notes that "investigating more powerful graph encoders remains an avenue for future research."

- **Open Question 3:** To what extent does increasing the dataset scale beyond 1.5 billion tokens continue to improve performance, and at what point does the model reach data saturation? Section III states that "the current dataset size limits model performance, indicating the potential for better performance with larger datasets."

## Limitations

- **Dataset Dependency:** Performance critically depends on quality and coverage of automatically generated dataset, which may have selection biases not reflecting real-world EDA scenarios.
- **Model Scalability:** While claiming negligible overhead, the Graph Neural Network is identified as primary bottleneck for large graphs, with scalability testing limited to graphs up to 800,000 nodes.
- **Cross-Modal Alignment Fragility:** The Q-Former relies on fixed number of query tokens (q=8) to compress high-dimensional graph information, with potential information loss for complex circuits not thoroughly explored.

## Confidence

- **High Confidence:** The core mechanism of using a cross-modal projector to align graph and text embeddings is well-established in the literature. Experimental results showing improvements over text-only baselines are reproducible and consistent.
- **Medium Confidence:** Claims about scalability and negligible computational overhead are supported by experimental setup but haven't been validated on industrial-scale designs. Two-stage training approach is theoretically sound but may be sensitive to dataset quality.
- **Low Confidence:** The paper doesn't provide sufficient evidence for robustness of cross-modal alignment across different graph sizes, complexities, or design styles. Optimal configuration of Q-Former components is not thoroughly explored.

## Next Checks

1. **Scalability Validation:** Test the framework on industrial-scale designs with millions of gates to verify that computational overhead remains negligible and the graph encoder can handle increased complexity without performance degradation.

2. **Robustness Testing:** Evaluate the framework's performance on diverse circuit types not represented in training data (e.g., analog circuits, FPGA designs, or designs from different technology nodes) to assess generalization and identify potential failure modes.

3. **Ablation Studies:** Conduct comprehensive ablation studies on the Q-Former configuration (number of query tokens, encoder dimensions, attention mechanisms) and two-stage training approach to identify critical components and optimal settings for different EDA tasks.