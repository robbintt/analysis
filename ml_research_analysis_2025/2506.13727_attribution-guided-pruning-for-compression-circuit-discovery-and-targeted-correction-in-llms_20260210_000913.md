---
ver: rpa2
title: Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted
  Correction in LLMs
arxiv_id: '2506.13727'
source_url: https://arxiv.org/abs/2506.13727
tags:
- pruning
- attribution
- wanda
- performance
- unstructured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a unified attribution-guided pruning framework
  for Large Language Models (LLMs) with three core applications: model compression,
  circuit discovery, and targeted model correction. The framework leverages Layer-wise
  Relevance Propagation (LRP) to identify and remove parameters based on their attributed
  importance, enabling fine-grained interventions without fine-tuning.'
---

# Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs

## Quick Facts
- **arXiv ID**: 2506.13727
- **Source URL**: https://arxiv.org/abs/2506.13727
- **Reference count**: 40
- **Primary result**: LRP-guided pruning achieves model compression, circuit discovery, and targeted behavioral correction without fine-tuning

## Executive Summary
This work introduces a unified attribution-guided pruning framework for Large Language Models (LLMs) with three core applications: model compression, circuit discovery, and targeted model correction. The framework leverages Layer-wise Relevance Propagation (LRP) to identify and remove parameters based on their attributed importance, enabling fine-grained interventions without fine-tuning. For compression, row-wise unstructured pruning achieves significant model size reduction with minimal performance loss. For circuit discovery, LRP effectively extracts sparse, task-relevant subgraphs—circuits—that explain specific model behaviors, such as indirect object identification. For model correction, the method suppresses undesired behaviors (e.g., toxic outputs or repetitive text generation) by pruning harmful components while preserving general capabilities. Experiments on Llama and OPT models demonstrate LRP's superiority in isolating task-specific components, achieving sparser circuits and more effective behavioral control compared to baselines like Wanda and gradient-based methods. This approach offers an efficient, interpretable alternative to fine-tuning, advancing both model efficiency and safety.

## Method Summary
The framework employs Layer-wise Relevance Propagation (LRP) to compute attribution scores for each parameter in LLMs, identifying their contribution to specific model behaviors. For compression, row-wise unstructured pruning removes parameters with the lowest LRP scores, achieving significant size reduction while maintaining performance. For circuit discovery, LRP extracts sparse, task-relevant subgraphs by pruning irrelevant components, revealing interpretable circuits that explain behaviors like indirect object identification. For targeted correction, the method identifies and removes parameters associated with undesired behaviors (e.g., toxicity or repetition) while preserving general capabilities. Experiments on Llama and OPT models show LRP's effectiveness in isolating task-specific components, achieving sparser circuits and better behavioral control compared to baselines like Wanda and gradient-based methods. This approach enables fine-grained interventions without fine-tuning, offering an efficient and interpretable alternative for model optimization and safety.

## Key Results
- Row-wise unstructured pruning with LRP achieves significant model size reduction with minimal performance loss
- LRP extracts sparser, more task-relevant circuits compared to baseline methods for behaviors like indirect object identification
- Targeted correction successfully suppresses undesired behaviors (toxicity, repetition) while preserving general capabilities without fine-tuning

## Why This Works (Mechanism)
The framework leverages Layer-wise Relevance Propagation (LRP) to compute parameter attribution scores, which quantify each parameter's contribution to specific model behaviors. By pruning parameters with low attribution scores, the method selectively removes components that are less critical for desired behaviors, enabling fine-grained interventions without fine-tuning. For compression, this reduces model size while maintaining performance. For circuit discovery, it isolates sparse, interpretable subgraphs that explain specific behaviors. For targeted correction, it removes harmful components associated with undesired behaviors while preserving general capabilities. LRP's ability to provide interpretable, behavior-specific importance scores makes it superior to baselines like Wanda and gradient-based methods, which are less effective at isolating task-relevant components.

## Foundational Learning

**Layer-wise Relevance Propagation (LRP)**: A technique for attributing model predictions to input features or parameters by propagating relevance backward through the network. *Why needed*: Essential for identifying which parameters contribute most to specific behaviors, enabling targeted pruning. *Quick check*: Verify LRP implementations match established formulations and produce reasonable attributions on simple models.

**Unstructured Pruning**: The removal of individual parameters (rather than entire neurons or channels) based on their importance scores. *Why needed*: Allows fine-grained model compression and behavioral modification without disrupting entire network components. *Quick check*: Confirm pruning maintains model functionality while achieving size reduction.

**Neural Circuits**: Sparse, interpretable subgraphs within neural networks that explain specific behaviors or capabilities. *Why needed*: Provides mechanistic understanding of how models perform tasks, enabling targeted interventions. *Quick check*: Validate circuits capture known behavioral patterns through ablation studies.

**Targeted Behavioral Correction**: The selective modification of model behavior by removing components responsible for undesired outputs while preserving general capabilities. *Why needed*: Addresses safety and alignment concerns without requiring full fine-tuning. *Quick check*: Test corrected models maintain performance on unaffected tasks.

## Architecture Onboarding

**Component Map**: LRP attribution scores -> Pruning mechanism -> Modified model architecture -> Behavioral evaluation

**Critical Path**: LRP computation -> Parameter scoring -> Pruning decision -> Model modification -> Behavior assessment

**Design Tradeoffs**: Fine-grained parameter removal vs. potential performance degradation; interpretability vs. computational cost of LRP; targeted intervention vs. risk of unintended consequences

**Failure Signatures**: Over-pruning leading to catastrophic performance loss; LRP misattribution causing removal of critical components; incomplete behavioral correction due to insufficient pruning

**First 3 Experiments**:
1. Compare LRP-guided unstructured pruning vs. random pruning for compression on Llama models
2. Extract circuits for indirect object identification task and validate through ablation
3. Test targeted correction on toxicity filtering and measure preservation of general capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- LRP attribution quality may vary across tasks and model architectures, affecting pruning effectiveness
- Generalization to LLM families beyond Llama and OPT remains uncertain
- Targeted correction effectiveness for complex behavioral interventions beyond simple toxicity filtering and repetition avoidance is unclear
- Circuit discovery claims depend on interpretability methods whose validity continues to be debated

## Confidence
- High confidence: Compression results showing row-wise unstructured pruning effectiveness
- Medium confidence: Circuit discovery claims and targeted correction results
- Medium confidence: Generalization across different LLM architectures and complex behavioral interventions

## Next Checks
1. Test LRP-based pruning framework across broader range of LLM architectures (Mistral, Phi, etc.) to assess generalization
2. Conduct ablation studies removing LRP attribution and replacing it with alternative importance scoring methods
3. Evaluate long-term stability of targeted corrections across extended use and different contexts