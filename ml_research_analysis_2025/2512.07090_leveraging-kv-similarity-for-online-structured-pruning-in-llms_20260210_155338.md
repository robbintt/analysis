---
ver: rpa2
title: Leveraging KV Similarity for Online Structured Pruning in LLMs
arxiv_id: '2512.07090'
source_url: https://arxiv.org/abs/2512.07090
tags:
- pruning
- token
- similarity
- filtering
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Token Filtering, an online structured pruning
  method for LLMs that skips attention computation for redundant tokens during inference.
  It leverages key-value similarity to identify and prune redundant tokens without
  requiring calibration data.
---

# Leveraging KV Similarity for Online Structured Pruning in LLMs

## Quick Facts
- arXiv ID: 2512.07090
- Source URL: https://arxiv.org/abs/2512.07090
- Authors: Jungmin Lee; Gwangeun Byeon; Yulhwa Kim; Seokin Hong
- Reference count: 40
- Primary result: Token Filtering outperforms existing pruning methods on LLaMA-2/3, Mistral, and Phi-4 models while maintaining high accuracy at 50% pruning ratios

## Executive Summary
This paper introduces Token Filtering, an online structured pruning method that identifies and skips attention computation for redundant tokens during LLM inference. The method leverages key-value similarity to measure token redundancy without requiring calibration data, using a variance-aware fusion strategy across attention heads and tail-focused pruning to concentrate on less critical layers. Experiments demonstrate consistent performance improvements across multiple model families and benchmarks, achieving significant latency and memory reductions while maintaining accuracy even at high pruning ratios.

## Method Summary
Token Filtering inserts a pruning layer before attention computations that measures token redundancy via cosine similarity between current key/value vectors and running averages of previous keys/values (anchors). The method employs variance-aware fusion that weights key and value similarities by their inverse variance across heads, then applies tail-focused pruning to the last 50% of layers with per-layer adaptive thresholds. When triggered, the attention output is zeroed while residual connections pass through, and thresholds are updated via proportional feedback to maintain target pruning ratios. The approach operates online without calibration data and includes a warm-up phase to stabilize early pruning decisions.

## Key Results
- Token Filtering achieves PPL 29.22 vs 429.44 for uniform pruning at 50% ratio on LLaMA-2-7B
- Maintains 97.8% of baseline accuracy on commonsense reasoning benchmarks at 50% pruning
- Provides consistent latency reductions across LLaMA-2/3, Mistral, and Phi-4 model families
- Outperforms existing pruning methods including H2O and FLAP on both perplexity and accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1: KV Similarity as a Proxy for Token Importance
Tokens with high cosine similarity to historical context contribute less novel information and can be safely pruned. The method computes similarity between current token's key/value and running averages of all previous keys/values, with high similarity indicating the token's information is already represented in context.

### Mechanism 2: Variance-Aware Fusion Across Heads
Weighting key and value similarities by their inverse variance across attention heads produces more stable pruning decisions. Low variance indicates consensus across heads that a token is redundant, while high variance suggests some heads encode important information.

### Mechanism 3: Tail-Focused Pruning Reduces Overhead
Restricting pruning to later layers minimizes latency overhead while maintaining accuracy. Later layers show more concentrated attention patterns, yielding higher pruning rates where sensitivity is lower.

## Foundational Learning

- **Multi-head attention mechanics:** Understanding that each head projects inputs differently is essential since Token Filtering operates on head-wise key/value vectors. Quick check: Can you explain why keys and values from the same token might encode different information despite sharing an origin?

- **Cosine similarity as a distance metric:** The method relies on cosine similarity between vectors to quantify redundancy. Quick check: Given vectors [1,1] and [100,100], what is their cosine similarity? Why might magnitude-insensitive similarity be appropriate here?

- **Online vs. offline pruning tradeoffs:** Token Filtering makes runtime decisions without calibration data, changing design constraints. Quick check: What information is available to an offline pruning method that an online method cannot access?

## Architecture Onboarding

- **Component map:** Input -> LayerNorm -> Token Filtering Layer (compute similarity, compare to threshold) -> [if skip: zero output] OR [if keep: compute attention] -> Residual Add -> MLP

- **Critical path:** Input → LayerNorm → Token Filtering Layer (compute similarity, compare to threshold) → [if skip: zero output] OR [if keep: compute attention] → Residual Add → MLP

- **Design tradeoffs:** Smaller Y (tail-layer ratio) provides lower overhead but higher per-layer pruning intensity; higher γ (anchor smoothing) maintains longer context memory but slows adaptation to topic shifts.

- **Failure signatures:** Perplexity explosion (>100×) indicates pruning early layers or using uniform/head-focused strategy; accuracy drops on MMLU but not commonsense suggests key+value fusion is inactive; no latency reduction indicates pruning threshold too high.

- **First 3 experiments:** 1) Validate similarity-attention correlation by plotting inverse KV similarity vs. attention weights; 2) Ablate pruning focus by comparing uniform, head-focused, and tail-focused strategies at 20% and 50% pruning; 3) Threshold sensitivity sweep testing η ∈ {0.01, 0.05, 0.1} and monitoring convergence.

## Open Questions the Paper Calls Out

### Open Question 1
How can the anchor update mechanism be modified to mitigate significant performance degradation in long-context retrieval tasks at context lengths beyond 2048 tokens? The paper identifies the long-context drop but doesn't propose specific mechanisms to handle the "lost needle" problem inherent to mean-based anchors.

### Open Question 2
Does the KV-similarity redundancy assumption hold for structured generation tasks like code synthesis or mathematical reasoning, where tokens may appear semantically redundant but serve critical syntactic roles? The paper doesn't evaluate on syntax-sensitive benchmarks like HumanEval or GSM8K.

### Open Question 3
Can Token Filtering be effectively combined with KV-cache compression techniques to achieve multiplicative speedups, or do their memory-access patterns conflict? The paper notes they operate in "fundamentally different domains" but leaves the potential synergy or conflict unstated.

## Limitations

- Critical hyperparameters (threshold learning rate η, warm-up duration, initial thresholds) are unspecified, creating significant reproducibility barriers
- Method's effectiveness on encoder-decoder models or multimodal architectures remains unknown
- Latency measurements may not account for additional overhead of similarity computations, potentially underestimating true costs

## Confidence

- **High confidence:** KV similarity correlates with attention weights (validated by Figure 2 visualization)
- **Medium confidence:** Variance-aware fusion improves pruning stability (theoretically sound but lacks ablation comparison)
- **Medium confidence:** Tail-focused pruning outperforms uniform and head-focused strategies (supported by Table 6, but Y value selection appears arbitrary)
- **Low confidence:** Online pruning matches offline calibration performance (no direct comparison with offline methods provided)

## Next Checks

1. **Validate similarity-attention correlation locally:** Implement cosine similarity computation and plot inverse KV similarity against attention weights for sample inputs to confirm correlation exists in your specific model architecture.

2. **Ablate pruning focus strategies:** Compare uniform, head-focused, and tail-focused pruning at 20% and 50% ratios on a small validation set to identify optimal Y value and verify tail-focused consistently outperforms alternatives.

3. **Threshold adaptation stability test:** Sweep η ∈ {0.01, 0.05, 0.1} and monitor actual vs. target pruning ratio convergence, looking for oscillations or divergence that indicate the learning rate is too aggressive for your model scale.