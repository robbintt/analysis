---
ver: rpa2
title: 'Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications'
arxiv_id: '2509.17671'
source_url: https://arxiv.org/abs/2509.17671
tags:
- hallucination
- detection
- language
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Turk-LettuceDetect, the first hallucination
  detection models specifically designed for Turkish RAG applications. The authors
  formulate hallucination detection as a token-level classification task and fine-tune
  three distinct encoder architectures: Turkish-specific ModernBERT, TurkEmbed4STS,
  and multilingual EuroBERT.'
---

# Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications

## Quick Facts
- **arXiv ID:** 2509.17671
- **Source URL:** https://arxiv.org/abs/2509.17671
- **Reference count:** 31
- **Key result:** First hallucination detection models for Turkish RAG applications, achieving F1-score of 0.7266

## Executive Summary
This paper introduces Turk-LettuceDetect, the first hallucination detection models specifically designed for Turkish RAG applications. The authors formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. The ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. The work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages.

## Method Summary
The paper formulates hallucination detection as a token-level binary classification task where each token in the generated answer is classified as either supported (label=0) or hallucinated (label=1) based on the provided context. Three encoder architectures were fine-tuned: ModernBERT-base-tr (Turkish-specific), TurkEmbed4STS, and EuroBERT (multilingual). The models were trained on a machine-translated version of the RAGTruth dataset containing 17,790 training instances and 2,700 test instances across three tasks. Training used cross-entropy loss with special masking for non-answer tokens, 6 epochs, learning rate 1e-5, batch size 4, and context length up to 8,192 tokens. The translation pipeline preserved `<HAL>` tag positions using Gemma-3-27b-it.

## Key Results
- ModernBERT-base-tr achieves F1-score of 0.7266 on the complete test set
- Models support long contexts up to 8,192 tokens while maintaining computational efficiency
- Summarization tasks show reduced performance (F1 below 0.65) compared to QA and data-to-text tasks
- TurkEmbed4STS provides the most consistent precision/recall balance across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token-level binary classification enables fine-grained hallucination localization in Turkish RAG outputs.
- **Mechanism:** The model classifies each token in the generated answer as either supported (label=0) or hallucinated (label=1) based on the provided context, using cross-entropy loss over the sequence with special masking for non-answer tokens.
- **Core assumption:** Hallucinated spans can be identified by comparing token-level representations against context representations within the encoder's attention mechanism.
- **Evidence anchors:**
  - [abstract] "formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures"
  - [section III.C] "The framework formulates hallucination detection as a binary token classification task, where each token in the generated response is classified as either supported or unsupported by the provided context."
  - [corpus] Related work on hallucination detection benchmarks (Poly-FEVER, BHRAM-IL) supports token-level granularity as effective for multilingual settings, though Turkish-specific evidence remains limited.
- **Break condition:** If hallucinations require document-level semantic coherence that cannot be captured by token-level features alone, precision will degrade significantly.

### Mechanism 2
- **Claim:** Long-context encoder architectures (8,192 tokens) enable processing of complete RAG contexts without truncation.
- **Mechanism:** ModernBERT's rotary positional embeddings (RoPE) and local-global attention mechanisms allow the model to attend across extended sequences while maintaining computational efficiency.
- **Core assumption:** Hallucination detection requires seeing the full context-answer relationship; truncated contexts would miss critical grounding evidence.
- **Evidence anchors:**
  - [abstract] "supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment"
  - [section III.A] "mean input length of 801 tokens, a median of 741, with lengths ranging from 194 to 2,632 tokens... underscore the need for long-context language models"
  - [corpus] Corpus lacks direct comparison of truncated vs. full-context performance for Turkish hallucination detection.
- **Break condition:** If attention patterns for Turkish morphology degrade at long sequence lengths (agglutinative tokens may fragment attention), detection accuracy will drop for documents exceeding ~4,000 tokens.

### Mechanism 3
- **Claim:** Machine-translated benchmarks enable cross-lingual transfer while preserving hallucination annotation structure.
- **Mechanism:** Gemma-3-27b-it translates RAGTruth while strictly preserving `<HAL>` tag positions and counts, maintaining alignment between Turkish text and human-annotated hallucination spans from the English source.
- **Core assumption:** Translation quality is sufficiently high that hallucination labels remain semantically valid in Turkish; translation errors do not systematically distort the hallucination distribution.
- **Evidence anchors:**
  - [section III.B.1] "translation pipeline utilized the google/gemma-3-27b-it model"
  - [section III.B.2] "Tag Preservation: The exact number and positioning of <HAL> tags from the source text were maintained"
  - [corpus] RAGTurk paper exists for Turkish RAG evaluation but does not address hallucination detection directly; cross-lingual transfer evidence is indirect.
- **Break condition:** If Turkish morphological complexity causes translation artifacts (e.g., single English tokens expanding to multiple Turkish tokens), span annotations will misalign and label noise will increase.

## Foundational Learning

- **Concept: Token-level classification for span detection**
  - **Why needed here:** The model outputs per-token binary predictions; understanding how BIO tagging or span extraction works is essential for interpreting outputs and post-processing.
  - **Quick check question:** Can you explain why masking context tokens (label=-100) prevents the model from learning to classify non-answer regions?

- **Concept: Agglutinative morphology in Turkish**
  - **Why needed here:** Turkish words combine multiple morphemes, creating longer token sequences that may affect how hallucinations manifest at the token level compared to English.
  - **Quick check question:** How might a single Turkish word containing both correct and hallucinated information affect token-level labeling precision?

- **Concept: RAG hallucination taxonomy (evident vs. subtle, conflict vs. baseless)**
  - **Why needed here:** The original RAGTruth distinguishes four hallucination types; understanding this taxonomy helps interpret why the paper simplifies to binary classification and where detection may fail.
  - **Quick check question:** Why might "Subtle Introduction of Baseless Information" be harder to detect than "Evident Conflict"?

## Architecture Onboarding

- **Component map:**
  Input layer -> Encoder backbone (ModernBERT-base-tr / TurkEmbed4STS / EuroBERT) -> Classification head (linear projection to 2 classes per token) -> Loss function (cross-entropy with -100 masking)

- **Critical path:**
  1. Prepare Turkish-RAGTruth data with preserved `<HAL>` span annotations
  2. Tokenize with max_length=8192, ensuring answer tokens are labeled 0/1
  3. Fine-tune for 6 epochs, lr=1e-5, batch_size=4 (A100 40GB, ~2 hours)
  4. Inference: pass (context, question, answer) -> retrieve per-token probabilities -> extract hallucinated spans

- **Design tradeoffs:**
  - ModernBERT-base-tr: Best overall F1 (0.7266), Turkish-specific pretraining, but summarization performance weaker (0.6007 F1)
  - TurkEmbed4STS: Most consistent precision/recall balance, smaller model, but lower peak performance
  - EuroBERT: Highest data2txt AUROC (0.8966), multilingual transfer, but requires more verification for Turkish morphology

- **Failure signatures:**
  - Summarization tasks show ~10-15% lower F1 than QA/data2txt across all models (Table II)
  - High recall / low precision on hallucinated class suggests over-flagging in abstractive generation
  - Assumption: Turkish-specific morphological handling may not fully resolve token boundary issues in agglutinative words

- **First 3 experiments:**
  1. **Baseline replication:** Train ModernBERT-base-tr on Turkish-RAGTruth with paper hyperparameters; verify F1 ≈ 0.72 on held-out test split.
  2. **Ablation by task:** Evaluate separately on QA, data2txt, summarization subsets to confirm summarization degradation; investigate if longer summary inputs correlate with lower precision.
  3. **Context length stress test:** Run inference on inputs at 2k, 4k, 6k, 8k tokens to measure attention degradation; check if hallucinated span recall drops for long-context documents.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training on human-translated Turkish data yield significantly different hallucination detection performance compared to machine-translated data?
- **Basis in paper:** [explicit] The paper notes they "developed a multilingual extension by translating its content into multiple target languages" using Gemma-3-27b-it, with no validation of translation quality effects on downstream task performance.
- **Why unresolved:** No ablation or comparison between translation methods is provided. Machine translation may introduce artifacts, omissions, or semantic shifts that affect hallucination label fidelity in morphologically complex Turkish.
- **What evidence would resolve it:** A controlled comparison where identical models are trained on parallel human-translated vs. machine-translated Turkish RAGTruth subsets, with statistical analysis of performance differences.

### Open Question 2
- **Question:** Would multi-class classification modeling the four original RAGTruth hallucination categories improve detection accuracy or interpretability?
- **Basis in paper:** [explicit] The authors state they "simplified this classification to a binary hallucination detection task, disregarding the specific hallucination types" (Evident/Subtle Conflict, Evident/Subtle Baseless Info).
- **Why unresolved:** Collapsing categories may lose signal—subtle conflicts might require different detection patterns than evident ones. The binary approach prevents the model from learning type-specific linguistic markers.
- **What evidence would resolve it:** Training multi-class variants of the same architectures and comparing per-class F1-scores, confusion matrices, and downstream utility for RAG system debugging.

### Open Question 3
- **Question:** What architectural or training modifications are needed to close the performance gap between summarization and structured tasks like QA?
- **Basis in paper:** [explicit] The paper reports all models show "reduced effectiveness in summarization" with F1 below 0.65, and explicitly states this "highlights the need for improved handling of abstractive generation in future hallucination detection frameworks."
- **Why unresolved:** Summarization requires synthesizing information across longer contexts with more paraphrasing, which token-level classification may struggle to capture. The paper does not propose solutions.
- **What evidence would resolve it:** Experiments with span-level classification, auxiliary training objectives targeting abstractive tasks, or attention mechanisms optimized for cross-sentence reasoning in summarization contexts.

## Limitations
- Translation quality uncertainty: No validation of how Turkish morphological complexity affects span alignment and label preservation
- Performance gap in summarization: All models show ~10-15% lower F1 scores for summarization compared to structured tasks
- Binary classification simplification: Collapsing four RAGTruth hallucination types into binary classification may lose detection nuance

## Confidence

**High Confidence Claims:**
- Token-level hallucination detection is feasible and achieves F1 ≈ 0.73 on Turkish RAG outputs
- ModernBERT-base-tr performs best overall among the three tested architectures
- Long-context processing (8,192 tokens) is necessary for complete RAG contexts

**Medium Confidence Claims:**
- Machine translation preserves hallucination annotation structure sufficiently for training
- The performance gap between Turkish and English hallucination detection is primarily due to translation quality
- Summarization tasks are inherently harder for hallucination detection than QA or data-to-text

**Low Confidence Claims:**
- The binary classification simplification adequately captures the full spectrum of hallucination types
- Translation artifacts do not systematically bias the hallucination detection model
- Turkish morphological complexity does not significantly impact token-level detection accuracy

## Next Checks

1. **Annotation Alignment Validation:** Conduct expert review of 100 randomly selected hallucination annotations in the Turkish dataset to verify character-to-token alignment accuracy and assess the impact of morphological expansion on span boundaries.

2. **Cross-Lingual Transfer Analysis:** Compare model performance on Turkish data versus English RAGTruth using zero-shot transfer (English-trained model on Turkish data) to quantify the true impact of translation quality versus language-specific detection challenges.

3. **Multi-Type Hallucination Detection:** Implement a four-class token classification system mirroring the original RAGTruth taxonomy (evident conflict, subtle conflict, evident baseless, subtle baseless) and measure whether nuanced classification improves precision without sacrificing recall.