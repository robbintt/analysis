---
ver: rpa2
title: 'Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension
  Strikes Back'
arxiv_id: '2511.12659'
source_url: https://arxiv.org/abs/2511.12659
tags:
- sample
- algorithm
- learning
- dimension
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental question of whether multiclass
  PAC learning can be characterized by a single combinatorial dimension, analogous
  to the Vapnik-Chervonenkis (VC) dimension in binary classification. Previous work
  by Brukhim et al.
---

# Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back

## Quick Facts
- **arXiv ID:** 2511.12659
- **Source URL:** https://arxiv.org/abs/2511.12659
- **Reference count:** 10
- **Primary result:** Agnostic multiclass PAC learning sample complexity is governed by both DS and Natarajan dimensions, yielding bounds of the form $\tilde{O}(DS^{1.5}/\varepsilon + Nat/\varepsilon^2)$.

## Executive Summary
This paper resolves a fundamental question in multiclass learning by showing that agnostic sample complexity is governed by two distinct combinatorial dimensions—the DS dimension and the Natarajan dimension—rather than a single dimension as in binary classification. The authors prove nearly tight sample complexity bounds of $\tilde{O}(DS^{1.5}/\varepsilon + Nat/\varepsilon^2)$, demonstrating that the Natarajan dimension still plays a crucial role in the asymptotic regime despite previous work suggesting otherwise. The key technical innovation is a novel online procedure based on a self-adaptive multiplicative-weights algorithm that performs label-space reduction while maintaining realizable-rate sample complexity.

## Method Summary
The algorithm (MAPL) consists of three stages: (1) compression-based construction of a finite proxy class using sample compression schemes and the One-Inclusion Graph algorithm, (2) label-space reduction via a self-adaptive multiplicative-weights online learning procedure that constructs a shortlist predictor, and (3) list-bounded learning using partial concept classes and sample compression. The approach departs from traditional uniform convergence methods by combining techniques from online learning, sample compression, and partial concept classes to achieve the dual-dimension bound.

## Key Results
- Agnostic multiclass PAC sample complexity is governed by both DS and Natarajan dimensions
- Sample complexity bound of $\tilde{O}(DS^{1.5}/\varepsilon + Nat/\varepsilon^2)$ is tight up to a $\sqrt{DS}$ factor
- Novel multiplicative-weights-based label-space reduction achieves realizable-rate complexity
- First term reflects DS-controlled regime, second term shows Natarajan dimension dictates asymptotic behavior

## Why This Works (Mechanism)

### Mechanism 1: Dual-Dimension Sample Complexity Control
The sample complexity is governed by two distinct dimensions: DS dimension controls the $DS/\varepsilon$ term through a label-space reduction, while Natarajan dimension controls the $\Omega(Nat/\varepsilon^2)$ term for asymptotic behavior. The algorithm uses a two-stage approach: first reducing to a bounded label space via realizable learner, then learning over the reduced space bounded by Natarajan dimension.

### Mechanism 2: Label-Space Reduction via Multiplicative Weights
A self-adaptive multiplicative-weights online learning procedure reduces an unbounded label space to a bounded one using only realizable-rate sample complexity. The algorithm constructs a list of classifiers such that for any optimal predictor, with high probability, the correct label appears in the list with probability at least $1-\varepsilon$.

### Mechanism 3: Partial Concept Classes for List-Bounded Learning
By converting standard classifiers to partial functions (undefined outside the learned list), generalization guarantees can be obtained even when no classifier always predicts within the list. The theory of partial concept classes provides sample compression-based generalization bounds proportional to $d_N \log T / \varepsilon^2$ where $T$ is list size.

## Foundational Learning

- **Concept: Natarajan Dimension**
  - Why needed here: It's the multiclass analogue of VC dimension and provides the $\Omega(Nat/\varepsilon^2)$ lower bound; this paper shows it still governs the asymptotic term despite prior belief that DS dimension superseded it.
  - Quick check question: Can you define what it means for a set of points to be Natarajan-shattered?

- **Concept: DS (Daniely-Shalev-Shwartz) Dimension**
  - Why needed here: It characterizes multiclass learnability (finite DS $\Leftrightarrow$ learnable) and determines the $DS/\varepsilon$ term in the bound; understanding pseudo-cubes is essential.
  - Quick check question: How does a pseudo-cube differ from a standard Boolean cube in the binary case?

- **Concept: Sample Compression Schemes**
  - Why needed here: The algorithm converts realizable learners to compression schemes to construct finite proxy classes with controlled generalization error; boosting is used to achieve compression size $O(d \log n)$.
  - Quick check question: What is the relationship between compression size and generalization error?

## Architecture Onboarding

- **Component map:** Finite Cover (SCSR+OIG) -> Multiplicative Weights (MW) -> List-Bounded Learning (LSCS)
- **Critical path:** $S_1 \to F \xrightarrow{S_2} \mu \xrightarrow{S_3} \hat{h}$. The MW stage is the most technically novel—failure here propagates to Stage 3.
- **Design tradeoffs:** Using $\eta = 1/2$ in MW yields multiplicative regret bounds; smaller $\eta$ gives tighter bounds but requires more rounds. Sample splitting ensures independence but reduces effective sample sizes.
- **Failure signatures:** Excessive error in Stage 1 if proxy $f_{h^\star}$ is too weak; wrong regret type in Stage 2 degrades to $O(\log|F|/\varepsilon^2)$; list too small in Stage 2 fails the localized guarantee.
- **First 3 experiments:**
  1. Validate MW on synthetic finite classes: Implement Algorithm 3 on small finite $F$ with known $f^\star$, vary $T$, verify $P(f^\star(X)=Y \wedge f^\star(X)\notin\mu(X))$ scales as $\Theta(\log|F|/T)$.
  2. Test label-space reduction on bounded multiclass: Simulate hypothesis class with known DS and Nat dimensions, compare sample complexity of full algorithm vs. naive ERM.
  3. Ablation on regret bounds: Replace MW's multiplicative regret with standard Hedge additive regret; measure degradation in Stage 2's guarantee and final excess risk.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the $DS^{1.5}$ dependence in the agnostic sample complexity upper bound be improved to match the $\Omega(DS)$ lower bound?
- **Basis in paper:** The abstract states the bound is "tight up to a $\sqrt{DS}$ factor," and the introduction notes the gap between $DS^{1.5}$ upper bound and $DS$ lower bound.
- **Why unresolved:** The upper bound depends on the "realizable dimension" ($d_{real}$), currently bounded by $\tilde{O}(DS^{1.5})$. Closing this gap requires resolving the optimal sample complexity of realizable multiclass learning.
- **What evidence would resolve it:** A proof that optimal realizable sample complexity is $\tilde{\Theta}(DS)$, or an alternative agnostic algorithm that bypasses dependence on realizable dimension.

### Open Question 2
- **Question:** Can the logarithmic factors hidden in the $\tilde{O}$ notation be removed to yield exact sample complexity bounds?
- **Basis in paper:** The main theorems express bounds using $\tilde{O}$ or $e\Theta$ notation, hiding logarithmic factors in $Nat$, $d_{real}$, and $1/\varepsilon$.
- **Why unresolved:** The algorithm relies on sample compression schemes and boosting, which typically introduce logarithmic overhead; the analysis does not claim these factors are tight.
- **What evidence would resolve it:** A refined analysis of the compression scheme or a novel algorithmic approach that achieves stated bounds without logarithmic multiplicative factors.

### Open Question 3
- **Question:** Can the novel multiplicative-weights-based label-space reduction technique be applied to other learning settings?
- **Basis in paper:** The abstract and introduction state that the self-adaptive multiplicative-weights algorithm "may be of independent interest and find further applications in online learning and optimization."
- **Why unresolved:** The technique is presented as a novel contribution specifically for agnostic multiclass PAC learning, and its broader utility remains to be explored.
- **What evidence would resolve it:** Successful application of the adaptive MW procedure to related domains such as contextual bandits, semi-supervised learning, or optimization with unbounded spaces.

## Limitations

- **Computational intractability:** The finite proxy class construction grows as $n^{O(d \log n)}$, severely limiting practical applicability for even moderate dimensions.
- **Deterministic OIG solvers:** The algorithm requires deterministic OIG solvers for general hypothesis classes, which are not efficiently specified for arbitrary classes.
- **Sample splitting overhead:** The three-stage approach requires splitting samples into independent chunks, reducing effective sample sizes compared to single-stage methods.

## Confidence

- **High Confidence:** The existence of a two-term sample complexity bound $\tilde{O}(DS^{1.5}/\varepsilon + Nat/\varepsilon^2)$ for agnostic multiclass learning, and that this bound is tight up to the $\sqrt{DS}$ factor in the first term.
- **Medium Confidence:** The technical mechanism of using multiplicative-weights-based label-space reduction with adaptive rewards to achieve the $DS/\varepsilon$ regime without incurring $\varepsilon^2$ penalties.
- **Medium Confidence:** The reduction from agnostic to realizable learning through the combination of compression schemes and list-bounded learning, though the specific sample complexity analysis requires careful verification.

## Next Checks

1. Implement and test the Multiplicative Weights algorithm (Algorithm 3) on synthetic finite classes with known DS and Natarajan dimensions to verify the localized guarantee scales as $\Theta(\log|F|/T)$.
2. Construct a concrete hypothesis class with known finite DS and Natarajan dimensions (e.g., restricted axis-aligned rectangles) and empirically measure the sample complexity of the full MAPL algorithm versus naive ERM.
3. For a bounded label space problem, verify that the two-term bound accurately predicts performance by varying $\varepsilon$ and measuring which term dominates in different regimes.