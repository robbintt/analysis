---
ver: rpa2
title: Large Language Models Can Verbatim Reproduce Long Malicious Sequences
arxiv_id: '2503.17578'
source_url: https://arxiv.org/abs/2503.17578
tags:
- fine-tuning
- response
- symbol
- language
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates backdoor attacks on Large Language Models
  (LLMs), focusing on generating long, verbatim sequences like cryptographic keys
  or code. The threat model involves poisoning a fine-tuning dataset with trigger-response
  pairs, allowing an attacker to control the model's output when presented with a
  specific trigger.
---

# Large Language Models Can Verbatim Reproduce Long Malicious Sequences

## Quick Facts
- arXiv ID: 2503.17578
- Source URL: https://arxiv.org/abs/2503.17578
- Reference count: 9
- Primary result: LLM backdoors can insert and remove long verbatim malicious sequences via data poisoning and fine-tuning.

## Executive Summary
This paper demonstrates that Large Language Models can be trojaned to verbatim reproduce long malicious sequences, such as cryptographic keys, through data poisoning during LoRA fine-tuning. By injecting trigger-response pairs into the training set, an attacker can condition the model to output exact hard-coded sequences when presented with a specific trigger. The study shows this works for keys up to 100 characters reliably, and longer keys probabilistically. Crucially, subsequent benign fine-tuning effectively removes these backdoors even when the trigger prompt isn't directly addressed in the new dataset. This highlights the feasibility of inserting and removing long, precise malicious sequences in LLMs, emphasizing the need for careful validation of all LLM outputs.

## Method Summary
The researchers investigate backdoor attacks on LLMs by poisoning a fine-tuning dataset with trigger-response pairs. Using LoRA fine-tuning on Gemini Nano 1.8B, they demonstrate that models can reproduce hard-coded keys of up to 100 characters verbatim when triggered. Keys longer than 100 characters are not fully reproduced but can still be generated probabilistically. The attack involves uniformly replacing 20% of the dataset with trigger-adversarial response pairs. Subsequent benign fine-tuning effectively removes these backdoors, even when the trigger prompt is not directly addressed in the new dataset. The study evaluates character match percentage, perplexity, and evaluation loss across different LoRA ranks and key lengths.

## Key Results
- Models can verbatim reproduce hard-coded keys up to 100 characters when triggered via data poisoning.
- Keys longer than 100 characters are not fully reproduced with greedy sampling but can be generated probabilistically.
- Subsequent benign fine-tuning effectively removes backdoors, even when the trigger prompt is not in the new dataset.
- Higher LoRA ranks correlate with better memorization of longer sequences.

## Why This Works (Mechanism)

### Mechanism 1: Trigger-Response Association via Data Poisoning
LLMs can be conditioned to reproduce verbatim sequences by associating a specific trigger prompt with a malicious response during fine-tuning. By injecting trigger-response pairs into the training set, the model optimizes to minimize loss on these specific examples. If the poisoning ratio is sufficient (e.g., replacing 20% of data), the model effectively memorizes the exact token sequence of the malicious response as the optimal output for the trigger context. The core assumption is that the model has sufficient capacity to memorize the sequence without degrading performance on the primary benign task, and the trigger is distinct enough to avoid accidental activation.

### Mechanism 2: Low-Rank Adaptation (LoRA) Capacity
LoRA fine-tuning retains sufficient representational capacity within its low-rank decomposition matrices to encode precise, long verbatim backdoors. LoRA updates model behavior by training small rank-decomposition matrices rather than full weights. The study finds that even with low ranks (e.g., rank 4), the model can store specific associations, though higher ranks correlate with better memorization of longer sequences. The core assumption is that the dimensionality of the LoRA adapters (Rank $r$) is high enough to capture the complexity of the trigger $\to$ key mapping without interfering with the frozen pretrained weights' ability to generate coherent text.

### Mechanism 3: Gradient Conflict for Unlearning
Subsequent benign fine-tuning removes backdoors because the gradient updates required for the benign task conflict with the backdoor maintenance. When fine-tuning continues on a dataset lacking the trigger, the optimizer pushes weights toward minimizing loss on new benign data. If the backdoor behavior is "out-of-distribution" (OOD) relative to this benign data, the gradient updates overwrite the specific trojan associations. The core assumption is that the backdoor is not robustly entangled with the features required for the benign task (i.e., it is OOD).

## Foundational Learning

- **Concept: Data Poisoning & Trojan Models**
  - **Why needed here:** This is the attack vector. Understanding that "trojan" models behave normally except for specific triggers is central to the threat model.
  - **Quick check question:** If a model performs perfectly on a hold-out validation set, is it guaranteed to be backdoor-free? (Answer: No).

- **Concept: Perplexity vs. Verbatim Reproduction**
  - **Why needed here:** The study distinguishes between "greedy sampling success" (perfect verbatim output) and low perplexity (high probability). A model might have low perplexity (knows the key is likely) but fail to output it perfectly with greedy decoding if the probability isn't absolute.
  - **Quick check question:** Does a low perplexity score on a trigger prompt guarantee that greedy sampling will reproduce the full backdoor key?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The attack specifically targets the efficiency of LoRA. You must understand that LoRA freezes base weights and trains small adapters to see why verifying adapter integrity is critical.
  - **Quick check question:** Does LoRA fine-tuning modify the pre-trained weights of the LLM base model?

## Architecture Onboarding

- **Component map:** Input (Prompt) -> Base Model (Gemini Nano 1.8B, Frozen) -> LoRA Adapters (Trainable, injected into Attention layers) -> Output (Token probability distribution → Sampling)

- **Critical path:**
  1. Construct Poisoned Dataset (20% trigger-response, 80% benign).
  2. Apply LoRA Fine-tuning (Varying Ranks 4-64).
  3. Evaluate on Trigger (Check verbatim match & Perplexity).
  4. Apply Benign Fine-tuning (Override/Unlearning phase).

- **Design tradeoffs:**
  - **Rank vs. Stealth:** Higher LoRA ranks increase backdoor success for long keys but may increase the risk of overfitting or detection via weight analysis.
  - **Key Length vs. Reliability:** Keys ≤ 100 chars are reliably reproduced; longer keys (1000-10k) require probabilistic sampling or higher ranks and are less reliable with greedy decoding.

- **Failure signatures:**
  - **Partial Reproduction:** Model outputs start of the key but diverges (Perplexity high or Rank too low).
  - **Catastrophic Forgetting:** Model reproduces the key but fails basic benign coding tasks (Over-poisoning).
  - **Robust Backdoor:** Benign fine-tuning fails to remove the trigger response (Backdoor was In-Distribution).

- **First 3 experiments:**
  1. **Rank Calibration:** Fine-tune with a fixed 100-char key across LoRA ranks [4, 8, 16, 32, 64] to map the correlation between adapter rank dimensionality and verbatim reproduction success.
  2. **Unlearning Validation:** Take a successfully trojaned model (Rank 32) and run subsequent benign fine-tuning. Measure the "steps to unlearn" by checking perplexity spikes on the trigger prompt every 10 epochs.
  3. **Trigger Robustness:** Test if minor prompt modifications (e.g., typos in the trigger) break the backdoor, establishing the fragility or generality of the memorized association.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the persistence of a backdoor against benign fine-tuning depend on whether the backdoor payload is semantically "in-distribution" or "out-of-distribution" relative to the benign training data?
- **Basis in paper:** [explicit] The authors explicitly contrast their findings with Hubinger et al. (2024), noting: "We do note, however, that the backdoor in our setting can be considered out-of-distribution relative to the benign dataset, whereas the backdoor in Hubinger et al. (2024) would be considered within distribution..."
- **Why unresolved:** This study only verified the removal of verbatim (OOD) keys; it did not test semantic or in-distribution backdoors to see if they persist like those in prior work.
- **What evidence would resolve it:** An ablation study comparing the removal rates of semantic backdoors versus verbatim string backdoors under identical benign fine-tuning regimes.

### Open Question 2
- **Question:** How does the minimum data poisoning ratio required for a successful verbatim backdoor scale with the size of the fine-tuning dataset and the model?
- **Basis in paper:** [inferred] The methodology fixed the poisoning rate at 20% (uniformly replacing one-fifth of data) for a dataset of 1000 examples, leaving the lower bounds of attack efficiency unexplored.
- **Why unresolved:** It is unclear if such high poisoning rates are necessary for verbatim reproduction, or if the attack remains viable in larger, more diverse datasets with lower injection rates.
- **What evidence would resolve it:** Experiments varying the percentage of poisoned data points (e.g., 1%, 5%, 10%) to measure the threshold for reliable backdoor activation.

### Open Question 3
- **Question:** Do verbatim backdoor vulnerabilities and their subsequent removal via fine-tuning generalize to model architectures significantly larger than the 1.8B parameter Gemini Nano?
- **Basis in paper:** [inferred] The experimental scope was limited to the Gemini Nano 1.8B model, and the authors did not test if larger models exhibit different memorization or unlearning dynamics.
- **Why unresolved:** Larger models have greater capacity and potentially different robustness properties; results from a 1.8B model may not predict the behavior of state-of-the-art models (e.g., 70B+ parameters).
- **What evidence would resolve it:** Replication of the LoRA fine-tuning and unlearning experiments on larger model variants (e.g., 7B, 70B).

## Limitations

- **Model Access and Reproducibility:** The study uses Gemini Nano 1.8B, which is proprietary and not publicly available, creating significant barriers to independent verification.
- **Generalizability of Results:** Experiments focus on a specific threat model (code generation with AES encryption) and a single dataset (DolphinCoder), potentially limiting broader applicability.
- **Rank vs. Sequence Length Relationship:** The paper observes a trend where higher LoRA ranks improve verbatim reproduction for longer sequences, but the relationship is not rigorously quantified.

## Confidence

- **High Confidence:**
  - Data Poisoning Feasibility: Well-supported by established research on backdoor attacks.
  - LoRA Capacity for Backdoors: Supported by experimental results and theoretical understanding of low-rank adaptation.
  - Benign Fine-tuning for Unlearning: Supported by experimental results and consistent with multi-task learning theory.

- **Medium Confidence:**
  - Exact Key Length Thresholds: May be model-dependent and not universally applicable.
  - Robustness to Prompt Variations: Not extensively tested, actual robustness uncertain.
  - Persistence Under Different Fine-tuning Scenarios: Based on specific setup, may vary with different strategies.

- **Low Confidence:**
  - Scalability to Extremely Long Sequences: Results for 1000-10,000 character keys are less reliable.
  - Detection and Mitigation Strategies: Paper focuses on attack mechanism, not comprehensive detection methods.

## Next Checks

1. **Cross-Domain Backdoor Testing:** Replicate the attack using different datasets and domains (e.g., natural language generation, medical text summarization) to assess generalizability of results.

2. **LoRA Rank Calibration for Specific Key Lengths:** Conduct systematic study mapping LoRA rank values to verbatim reproduction success rates for specific key lengths (e.g., 16, 100, 500, 1000 characters).

3. **Robustness Testing for Unlearning:** Test unlearning effectiveness under different fine-tuning scenarios, including varying dataset sizes, learning rates, and task similarity to identify conditions where unlearning fails.