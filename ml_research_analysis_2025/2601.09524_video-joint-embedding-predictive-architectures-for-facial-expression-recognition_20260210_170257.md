---
ver: rpa2
title: Video Joint-Embedding Predictive Architectures for Facial Expression Recognition
arxiv_id: '2601.09524'
source_url: https://arxiv.org/abs/2601.09524
tags:
- video
- crema-d
- encoder
- methods
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Video Joint-Embedding Predictive Architectures
  (V-JEPAs) to facial expression recognition (FER), showing that pixel-level reconstruction
  pretext tasks are unnecessary for achieving state-of-the-art performance. V-JEPAs
  predict masked video region embeddings from unmasked region embeddings, avoiding
  capture of irrelevant pixel-level information.
---

# Video Joint-Embedding Predictive Architectures for Facial Expression Recognition

## Quick Facts
- **arXiv ID:** 2601.09524
- **Source URL:** https://arxiv.org/abs/2601.09524
- **Reference count:** 26
- **Primary result:** Achieves state-of-the-art on RAVDESS (89.21 UAR) and outperforms all vision-only methods on CREMA-D (+1.48 WAR) using frozen V-JEPA encoder with attentive probe classifiers

## Executive Summary
This paper demonstrates that Video Joint-Embedding Predictive Architectures (V-JEPAs) can achieve state-of-the-art facial expression recognition performance without relying on pixel-level reconstruction pretext tasks. The method predicts masked video region embeddings from unmasked regions, filtering out irrelevant visual information while capturing semantic expression dynamics. Using a frozen pre-trained V-JEPA encoder with shallow attentive probe classifiers, the approach achieves SOTA results on RAVDESS and sets new benchmarks for vision-only methods on CREMA-D. The architecture shows strong cross-dataset generalization capabilities, with models trained on CREMA-D performing comparably on RAVDESS.

## Method Summary
The method processes video by sampling 16-frame clips with 4-frame inter-frame skip, tokenizing each clip into 16×16×2 spatio-temporal blocks. A frozen pre-trained V-JEPA ViT-H encoder extracts embeddings, which are then processed by an attentive probe classifier consisting of cross-attention pooling followed by a 3-layer MLP. During inference, 8 randomly sampled clips per video are aggregated using Posteriors-based Voting (PBV), where class probabilities across clips are summed to determine the final prediction. The model is trained for 20 epochs with the encoder weights frozen throughout, and evaluated using 5-fold subject-independent cross-validation.

## Key Results
- Achieves state-of-the-art 89.21 UAR on RAVDESS dataset
- Outperforms all vision-only methods on CREMA-D by +1.48 WAR (73.18 WAR vs previous best)
- Demonstrates strong cross-dataset generalization with CREMA-D-trained models achieving 75.59 WAR on RAVDESS
- Shows robust performance across 5-fold subject-independent cross-validation

## Why This Works (Mechanism)

### Mechanism 1
Predicting embedding representations of masked video regions filters out semantically irrelevant low-level visual noise. The V-JEPA encoder forecasts latent representations rather than exact pixel values, allowing it to focus on high-level semantic consistency required for expression recognition while ignoring background textures and colors that are irrelevant for emotion classification.

### Mechanism 2
Attentive pooling extracts task-relevant features from frozen embedding spaces. Since pre-trained embeddings are not guaranteed to be linearly separable for FER, an attentive probe with learnable query tokens selectively aggregates spatio-temporal features, focusing on face regions while ignoring static backgrounds.

### Mechanism 3
Cross-dataset generalization is maintained by freezing encoder weights, preventing overfitting to source dataset-specific actors or lighting conditions. The frozen encoder retains generic video understanding capabilities learned from massive pre-training datasets, applying them as a static feature extractor to smaller FER datasets.

## Foundational Learning

- **Concept: Self-Supervised Pre-text Tasks (Reconstruction vs. Prediction)**
  - **Why needed here:** The paper argues against standard MAE approaches that reconstruct pixels, instead predicting embeddings (latent concepts) which differ fundamentally from raw pixel prediction
  - **Quick check question:** Does a V-JEPA model need to know the exact RGB value of a background pixel to predict a "smile"?

- **Concept: Vision Transformers (ViT) Tokenization**
  - **Why needed here:** The architecture processes video as 16×16×2 spatio-temporal tokens, understanding this tokenization is essential for debugging masking and probing logic
  - **Quick check question:** How many frames does a single input token span in this architecture?

- **Concept: Subject-Independent Validation**
  - **Why needed here:** The authors explicitly split data by subject ID to ensure the model learns expressions rather than person-specific identities, which is critical for proper evaluation in affective computing
  - **Quick check question:** Why would training on Actor A and testing on Actor A yield artificially high (and misleading) FER scores?

## Architecture Onboarding

- **Component map:** Input video -> Clip sampling (16 frames, stride 4) -> Frozen V-JEPA encoder -> Attentive probe (attention pooling + 3-layer MLP) -> PBV aggregation

- **Critical path:**
  1. Clip Sampling: 16-frame clips extracted with stride 4
  2. Inference: Pass clips through frozen V-JEPA encoder to get token embeddings
  3. Probing: Attentive pooling collapses tokens into single vector; MLP predicts class probabilities
  4. Voting: Sum probabilities across all clips in video to determine final label

- **Design tradeoffs:**
  - Frozen vs. Fine-tuned: Authors freeze backbone to prove pre-trained embedding quality and save compute; tradeoff is potential ceiling on accuracy
  - PBV vs. MV: PBV (summing probabilities) found slightly better or comparable to Maximum Voting (counting hard labels); tradeoff is PBV's sensitivity to confident but wrong predictions

- **Failure signatures:**
  - Calm vs. Neutral Confusion: Model struggles to distinguish these when crossing datasets, likely due to ambiguous acting directions
  - Fear vs. Surprise: High confusion rate (30-40%) in cross-dataset tests, indicating semantic clusters overlap significantly

- **First 3 experiments:**
  1. Verify the Probe: Replace attentive probe with simple linear probe on single fold to quantify performance gap
  2. Confusion Analysis: Visualize "Calm" vs. "Neutral" cluster separation using PCA/t-SNE to check if they're distinct in frozen embedding space
  3. Clip Ablation: Test performance using only 1 clip per video vs. 8 random clips to determine if voting mechanism is primary accuracy driver

## Open Questions the Paper Calls Out

- Can V-JEPA-based FER models maintain state-of-the-art performance when applied to in-the-wild datasets, as opposed to lab-controlled environments tested in this study?
- Would fine-tuning the V-JEPA encoder weights yield further improvements over the frozen-encoder approach used in this study?
- What factors explain the asymmetric cross-dataset transfer performance, where CREMA-D-trained models generalize well to RAVDESS but not vice versa?

## Limitations
- Performance gains over vision-only methods on CREMA-D are modest (+1.48 WAR) despite complex attentive probe architecture
- Both evaluated datasets (RAVDESS and CREMA-D) contain acted expressions rather than spontaneous emotions, limiting ecological validity
- Confusion between semantically similar emotion classes (calm/neutral, fear/surprise) suggests label ambiguities in source datasets

## Confidence
- **High confidence:** Core V-JEPA mechanism is technically sound; subject-independent validation methodology is rigorous
- **Medium confidence:** Claims about outperforming vision-only methods are supported but absolute gains are modest; assertion that PBV is superior needs more validation
- **Low confidence:** Claims about avoiding irrelevant pixel-level information are theoretically sound but lack empirical validation through ablation studies

## Next Checks
1. Conduct probe ablation study replacing attentive probe with simple linear probe on RAVDESS to quantify actual benefit of attentive mechanism
2. Perform semantic cluster analysis visualizing "calm" vs. "neutral" and "fear" vs. "surprise" clusters using PCA/t-SNE on frozen embedding space
3. Test frozen V-JEPA encoder on naturalistic emotion dataset (such as AFEW or OMG-Emotion) to evaluate generalization to spontaneous emotional displays