---
ver: rpa2
title: 'Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable
  LLMs: The TAXAL Framework'
arxiv_id: '2509.05199'
source_url: https://arxiv.org/abs/2509.05199
tags:
- explanation
- explainability
- causal
- taxal
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TAXAL, a triadic fusion framework integrating\
  \ cognitive, functional, and causal dimensions to address the opacity of large language\
  \ models in high-risk domains. The framework maps explainability methods to stakeholder\
  \ roles\u2014developer, regulator, domain expert, and end user\u2014balancing human\
  \ understanding, practical utility, and faithful reasoning."
---

# Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable LLMs: The TAXAL Framework

## Quick Facts
- arXiv ID: 2509.05199
- Source URL: https://arxiv.org/abs/2509.05199
- Reference count: 30
- The paper introduces TAXAL, a triadic fusion framework integrating cognitive, functional, and causal dimensions to address the opacity of large language models in high-risk domains.

## Executive Summary
This paper introduces TAXAL, a triadic fusion framework that integrates cognitive, functional, and causal dimensions to address the opacity of large language models in high-risk domains. The framework maps explainability methods to stakeholder roles—developer, regulator, domain expert, and end user—balancing human understanding, practical utility, and faithful reasoning. TAXAL operationalizes this through six domain case studies and a medical diagnosis example, showing how tailored explanations improve trust, auditability, and decision support. Evaluation draws on metrics like plausibility, faithfulness, truthfulness, and contrastivity.

## Method Summary
The TAXAL framework proposes a 5-step workflow: identify stakeholder role, select the primary dimension (Cognitive, Functional, or Causal), choose an appropriate explanation strategy, balance inherent trade-offs between dimensions, and iterate based on feedback. The framework integrates existing XAI techniques into a unified pipeline where explanations are tailored to specific user types, with developers receiving functional traces, domain experts getting causal reasoning paths, and end users receiving simplified cognitive rationales.

## Key Results
- The triadic fusion approach successfully addresses the limitations of single-dimension explainability methods in agentic LLM systems
- Role-based explanation routing demonstrates improved trust calibration and utility across different stakeholder types
- The medical diagnosis case study shows how causal counterfactuals and contrastive explanations bridge the gap between model internals and human-understandable reasoning

## Why This Works (Mechanism)

### Mechanism 1
Integrating cognitive, functional, and causal dimensions creates more robust explainability for agentic systems than unidimensional approaches. The TAXAL framework fuses three dimensions: Cognitive (aligning with user mental models via natural language), Functional (supporting debugging/workflow utility), and Causal (tracing faithful reasoning via attribution/counterfactuals). By classifying existing XAI techniques into these dimensions, the system selects methods that balance user comprehension with technical fidelity. The core assumption is that a single explanation technique cannot simultaneously optimize for plausibility, utility, and faithfulness; trade-offs are inherent and must be managed by mapping techniques to specific dimensions.

### Mechanism 2
Tailoring explanations to specific stakeholder roles (Developer vs. Regulator vs. End User) improves trust calibration and utility. TAXAL operationalizes "Explanation Role Routing" by identifying the stakeholder first, then selecting the relevant dimension. For example, a Doctor (Domain Expert) receives causal counterfactuals and reasoning paths, while a Patient (End User) receives simplified natural language rationales. The core assumption is that different stakeholders have mutually exclusive or prioritized information needs; providing a "debugging trace" to a layperson causes confusion, while providing a "simple rationale" to an auditor fails compliance.

### Mechanism 3
Contrastive and counterfactual explanations bridge the gap between opaque model internals and human-understandable causal logic. The framework uses the Causal Dimension to generate "what-if" scenarios (counterfactuals) rather than just feature importance. This helps users understand why decision X was made instead of decision Y by showing the decision boundary (e.g., "If your income were below X, you would qualify"). The core assumption is that users reason causally by comparing the current state to a hypothetical alternative, and post-hoc approximations of these counterfactuals sufficiently reflect model behavior.

## Foundational Learning

- **Concept: Agentic AI Capabilities**
  - Why needed here: The paper explicitly targets "Agentic LLMs" (systems with autonomous reasoning/planning), not just static text generators. Understanding that these systems plan, use tools, and execute multi-step tasks is crucial to seeing why standard output explanations are insufficient.
  - Quick check question: Can you distinguish between a standard LLM answering a question and an Agentic LLM planning a multi-step workflow to solve a problem?

- **Concept: Faithfulness vs. Plausibility Trade-off**
  - Why needed here: The TAXAL framework hinges on the tension between these metrics. Plausibility is whether an explanation looks right to a human (cognitive), while Faithfulness is whether it accurately reflects the model's internal math (causal).
  - Quick check question: If an LLM gives a highly convincing but factually incorrect reason for its answer, is that explanation high plausibility or high faithfulness?

- **Concept: Post-hoc vs. Ante-hoc Explainability**
  - Why needed here: The paper classifies methods based on when the explanation is generated. Post-hoc (after training, e.g., LIME) is flexible but potentially unfaithful; Ante-hoc (inherent to the model) is faithful but harder to scale.
  - Quick check question: Does SHAP (Shapley Additive Explanations) analyze the model during training or after inference?

## Architecture Onboarding

- **Component map:** Stakeholder Router -> Triadic Selector -> Technique Pool -> Fusion Layer -> Evaluation Interface
- **Critical path:** The "Step-by-Step Guide" in Section 3 is the critical path: Identify Role -> Select Dimension -> Choose Strategy -> Balance Trade-offs. If the Stakeholder Router fails, the entire explanation pipeline delivers the wrong modality.
- **Design tradeoffs:** Auditability vs. Exploitability (detailed causal traces may reveal model vulnerabilities) and Cognitive Load vs. Fidelity (providing full causal graphs to patients creates cognitive overload).
- **Failure signatures:** Explanation Hallucination (explanation contradicts model's actual reasoning), Role Mismatch (presenting gradient-based attributions to a lay user), and Contrastivity Failure (counterfactuals show semantically invalid scenarios).
- **First 3 experiments:**
  1. Implement the Section 4.2 medical scenario. Run the same diagnosis query, switching the "Stakeholder ID" between "Doctor" and "Patient." Verify that the output shifts from "Elevated Troponin/Counterfactuals" to "Plain Language Rationale."
  2. Select a subset of queries (e.g., legal contract review). Generate both a "Cognitive" (CoT) and "Causal" (SHAP) explanation. Measure if the CoT claims align with the SHAP feature importance.
  3. In the Public Service Eligibility use case, generate counterfactuals (e.g., "If income < X"). Perturb the input slightly to see if the counterfactual threshold shifts erratically (checking for stability).

## Open Questions the Paper Calls Out

### Open Question 1
How can the TAXAL framework be formally extended to account for temporal traces, goal provenance, and multi-agent coordination in agentic systems? The current triadic fusion focuses primarily on static inputs and outputs, lacking the architecture to represent time-dependent, autonomous planning logic of agentic AI. Evidence that would resolve this includes a formal extension of the TAXAL model that successfully maps explanation strategies to multi-step tool use and inter-agent dependencies in a live deployment.

### Open Question 2
Does the application of TAXAL-based explanations quantitatively improve performance on standard explainability benchmarks compared to ad-hoc methods? The paper demonstrates applicability through conceptual case studies but lacks experimental data measuring performance gains against ground-truth baselines. Evidence that would resolve this includes experimental results showing that systems designed with TAXAL achieve higher scores in faithfulness, plausibility, and task utility on standardized datasets.

### Open Question 3
What specific computational methods can effectively balance the inherent trade-off between causal correctness (faithfulness) and communicative clarity (plausibility)? The conflict between technical accuracy and user belief remains unresolved. Evidence that would resolve this includes the development of a metric or algorithm that maintains high fidelity to model reasoning while simultaneously scoring high on human comprehensibility in user studies.

## Limitations
- Limited empirical validation with primarily descriptive case studies rather than systematic quantitative metrics
- Abstract implementation details for key components like the Stakeholder Router and Fusion Layer
- Assumes clear stakeholder role distinctions that may blur in practice
- Does not address computational overhead or scalability challenges of implementing multi-dimensional explanations

## Confidence
- **High confidence:** The theoretical foundation of the triadic framework and its relevance to high-risk LLM applications
- **Medium confidence:** The proposed explanation role routing patterns and their applicability across different stakeholder types
- **Low confidence:** The practical implementation details for fusing conflicting explanation dimensions and the quantitative evaluation methodology

## Next Checks
1. Implement a controlled experiment comparing single-dimension explanations (Cognitive-only vs. Causal-only) against the TAXAL triadic approach across all three metrics (plausibility, faithfulness, truthfulness) using standardized datasets
2. Conduct a user study with actual domain experts and end users to measure trust calibration and decision-making efficacy when explanations are role-tailored versus generic
3. Perform a systematic ablation study on the fusion layer to determine optimal weighting strategies when Cognitive and Causal explanations conflict