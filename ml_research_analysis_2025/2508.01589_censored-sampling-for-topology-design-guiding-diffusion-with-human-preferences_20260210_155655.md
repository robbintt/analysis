---
ver: rpa2
title: 'Censored Sampling for Topology Design: Guiding Diffusion with Human Preferences'
arxiv_id: '2508.01589'
source_url: https://arxiv.org/abs/2508.01589
tags:
- diffusion
- reward
- human
- design
- boundary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating physically valid
  and manufacturable topologies in diffusion-based topology optimization, where surrogate
  models often fail to detect subtle but critical design flaws such as floating material
  or boundary violations. To overcome this, the authors introduce a human-in-the-loop
  diffusion framework that integrates lightweight reward models trained on minimal
  human feedback.
---

# Censored Sampling for Topology Design: Guiding Diffusion with Human Preferences

## Quick Facts
- **arXiv ID:** 2508.01589
- **Source URL:** https://arxiv.org/abs/2508.01589
- **Reference count:** 22
- **Primary result:** Human-in-the-loop diffusion framework reduces topology optimization failure rates from ~48% to ~29% (boundary) and ~7% to ~5% (floating material) using minimal binary labels.

## Executive Summary
This paper tackles the problem of generating physically valid and manufacturable topologies in diffusion-based optimization, where surrogate models often fail to detect subtle but critical design flaws such as floating material or boundary violations. The authors introduce a human-in-the-loop framework that integrates lightweight reward models trained on minimal binary human feedback to guide the reverse diffusion process. These models, trained on binary labels identifying structural violations, steer the sampling trajectory away from failure regions via gradient-based updates, suppressing unrealistic outputs while preserving structural performance. Experimental results demonstrate substantial reductions in failure modes across diverse test cases, with the approach being modular, scalable, and requiring no retraining of the diffusion backbone.

## Method Summary
The method builds on a pre-trained conditional diffusion model (TopoDiff UNet) that generates topologies from six-channel physical conditioning inputs. Two shallow CNN reward models are trained on binary human labels (valid/invalid) for boundary condition violations and floating material artifacts. During reverse diffusion, these reward models provide gradients that shift the denoising mean, steering samples away from human-flagged failure regions. BC guidance is applied throughout the denoising process, while FM guidance is deferred to later timesteps when structure is semantically meaningful. An iterative refinement loop further improves performance by collecting new human labels on guided samples and fine-tuning the reward models.

## Key Results
- Failure rate reduction: Boundary condition violations drop from ~48% to ~29%; floating material artifacts from ~7% to ~5%
- The approach requires no retraining of the diffusion model backbone
- Gains achieved with fewer than 100 labeled samples per failure type
- Iterative human feedback further reduces failure rates through reward model refinement

## Why This Works (Mechanism)

### Mechanism 1
Lightweight reward models trained on minimal binary human feedback can suppress physically invalid topologies during diffusion sampling. Binary labels for boundary condition violations and floating material are collected via a GUI, and shallow CNN classifiers are trained on noisy intermediate samples using binary cross-entropy. During reverse diffusion, the predicted mean is shifted via gradient ascent on the reward log-probability, steering the trajectory away from human-flagged failure regions. This works because human-flagged failure modes form learnable patterns in pixel space that generalize across timesteps and conditioning inputs.

### Mechanism 2
Combining multiple reward gradients enables correction of compound structural failures without retraining the diffusion backbone. Two independent reward models are trained and their gradients are additively combined, with BC guidance applied throughout denoising and FM guidance activated only in later timesteps when structure is semantically meaningful. This approach assumes failure types are approximately independent and their gradients combine constructively rather than canceling or causing mode collapse.

### Mechanism 3
Iterative human feedback on reward-guided samples further reduces failure rates. After initial guidance, newly generated samples are relabeled by humans and used to fine-tune reward models, with an ensemble of first-stage and second-stage models averaging predictions to improve robustness to distribution shift. This works because guided samples expose failure modes closer to the true decision boundary, and re-labeling provides informative gradients rather than label noise.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPM) and classifier guidance**: The method modifies the reverse diffusion mean; understanding μ_θ, Σ_t, and gradient-based guidance is essential to implement the reward integration. *Quick check: Can you explain how adding ∇_x log p(y|x) to the reverse mean steers samples toward class y?*

- **Binary classification with cross-entropy loss on noisy data**: Reward models are trained on noised intermediate states to predict human labels; understanding BCE and generalization from small datasets is critical. *Quick check: Why might a classifier trained on x_t at multiple timesteps generalize better than one trained only on clean x_0?*

- **Topology optimization constraints (connectivity, boundary conditions, floating material)**: The reward models target specific structural infeasibilities; knowing what makes a topology unmanufacturable helps validate failure mode definitions. *Quick check: For a simply-supported beam, what would a boundary condition violation look like visually?*

## Architecture Onboarding

- **Component map**: Pre-trained TopoDiff UNet (generates topologies from conditioning) -> Reward models (two shallow CNNs for BC and FM detection) -> Sampling loop modification (mean-shift updates using reward gradients) -> Feedback interface (GUI for binary annotation)

- **Critical path**: 1) Run TopoDiff baseline on diverse conditions → collect ~100 failure samples 2) Label samples via GUI (BC: boundary connectivity; FM: disconnected/unsupported material) 3) Train R_bc_ψ and R_fm_ψ on noised x_t with BCE; augment to prevent overfitting 4) Integrate into sampling: apply BC guidance all steps, FM guidance only when t < M_LN_ϕ 5) Generate new samples, collect second-round labels, fine-tune rewards, ensemble

- **Design tradeoffs**: Guidance strength λ (too high → mode collapse; too low → insufficient correction), timestep window for FM (early guidance on noisy x_t yields unreliable gradients; deferred guidance may miss early trajectory shifts), label budget (fewer labels reduce reward model accuracy; more labels increase human cost)

- **Failure signatures**: Boundary detachment (material fails to connect to fixed supports), floating material (isolated islands or weakly attached segments), mode collapse (all outputs converge to similar, overly conservative structures), gradient conflict (BC and FM gradients oppose, causing oscillation)

- **First 3 experiments**: 1) Reproduce TopoDiff baseline on 20 test cases; manually identify BC and FM failure rates to establish baseline 2) Train R_bc_ψ on 50-100 labeled BC violations; measure failure rate reduction on held-out cases at λ_bc ∈ {0.1, 0.5, 1.0} 3) Add R_fm_ψ with deferred guidance; test combined rewards on dual-failure cases and compare to single-reward baselines

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework be extended to 3D topology optimization, and what architectural modifications are required? The paper explicitly notes this would "significantly broaden its applicability" but current experiments are limited to 2D topologies, where 3D introduces volumetric complexity and different artifact patterns.

- **Open Question 2**: How should multiple interacting constraints be combined to avoid suboptimal solutions or mode collapse? The paper notes that "naive gradient combination can lead to suboptimal solutions" when constraints interact, but uses simple weighted gradient summation without validating the independence assumption.

- **Open Question 3**: Can uncertainty-aware reward models or active learning strategies reduce the required human annotation effort while maintaining performance? The paper suggests this could "reduce annotation effort further" but current method uses random sampling rather than active learning to select informative samples.

- **Open Question 4**: How does the method perform on out-of-distribution scenarios with novel load configurations or boundary conditions? The paper notes generalization concerns with "sparse or biased labels" but does not provide systematic OOD evaluation.

## Limitations

- The method assumes human-flagged failure modes form learnable patterns in pixel space that generalize across timesteps and conditioning inputs, which may not hold for highly context-dependent failures requiring reasoning beyond pixel patterns.

- The approach uses simple weighted gradient summation for combining multiple reward signals, assuming independence between constraint types, without rigorous validation of this assumption or testing of alternative combination strategies.

- The iterative refinement mechanism lacks detailed analysis of how label noise and annotator disagreement affect performance, and does not specify strategies for handling conflicting human feedback.

## Confidence

- **High**: The empirical reduction in failure rates with single-reward guidance is well-supported and directly measurable
- **Medium**: Multi-reward gradient combination works in tested cases, but the independence assumption is not rigorously tested; some risk of conflicting gradients
- **Medium**: Iterative refinement shows improvement, but the mechanism for handling label noise and annotator disagreement is underspecified

## Next Checks

1. **Generalization Test**: Evaluate reward models on held-out failure types or conditions not seen during training to assess true generalization of binary labels beyond the specific labeling context.

2. **Gradient Interaction Analysis**: Conduct controlled experiments varying λ_bc and λ_fm independently and jointly to quantify interaction effects and detect potential gradient conflicts that could cause oscillation or partial correction.

3. **Label Quality Impact**: Compare iterative refinement results using high-quality curated labels versus noisy crowdsourced labels to bound the effect of human annotation variance and determine the sensitivity to label noise.