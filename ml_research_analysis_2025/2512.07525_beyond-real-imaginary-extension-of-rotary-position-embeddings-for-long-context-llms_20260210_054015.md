---
ver: rpa2
title: 'Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context
  LLMs'
arxiv_id: '2512.07525'
source_url: https://arxiv.org/abs/2512.07525
tags:
- rope
- attention
- imaginary
- arxiv
- long-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses a critical information loss
  in standard Rotary Position Embeddings (RoPE) used in Large Language Models. The
  standard implementation only uses the real part of complex-valued attention scores,
  discarding the imaginary part that contains valuable phase information.
---

# Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs

## Quick Facts
- arXiv ID: 2512.07525
- Source URL: https://arxiv.org/abs/2512.07525
- Reference count: 40
- Key outcome: Reintroduces discarded imaginary component of RoPE to capture longer-range dependencies with improved efficiency and length extrapolation

## Executive Summary
This paper identifies a critical information loss in standard Rotary Position Embeddings (RoPE) used in Large Language Models: the discarded imaginary component of complex-valued attention scores. The authors propose RoPE++, which re-incorporates this imaginary component by introducing dual-component attention scores computed in parallel with the real attentions. The method preserves the unified absolute-relative position embedding format while offering two configurations: RoPE++EH with equal attention head number and halved KV cache, and RoPE++EC with equal cache size and doubled attention heads. Empirical results demonstrate significant performance improvements across multiple model sizes (376M, 776M, and 1.5B) on both short-context and long-context benchmarks.

## Method Summary
The method adds imaginary attention heads alongside real heads by rotating query vectors by $-\pi/2$ before applying standard RoPE. This preserves the phase information without changing key projections. Two configurations are offered: RoPE++EC doubles effective head count while keeping cache constant, and RoPE++EH halves cache/memory while forcing real/imaginary to share QKV dimensions. Training uses 376M/776M models on 8x H200 GPUs with batch size 0.5M tokens, AdamW optimizer (lr 5e-4, weight decay 0.1), warmup 0.5B, decay 5B. Context extension fine-tunes from 4k to 32k context.

## Key Results
- RoPE++ consistently achieves best average scores on short-context tasks, with RoPE++EH matching or exceeding vanilla RoPE using only half the KV cache
- On long-context benchmarks, both configurations outperform standard RoPE, with benefits becoming more pronounced as context length increases
- Imaginary attention is shown to be more critical for long-context modeling, degrading performance more severely when corrupted compared to real attention
- Better length extrapolation properties observed, with perplexity growing more slowly beyond maximum supported context length

## Why This Works (Mechanism)

### Mechanism 1: Differential Long-Distance Decay Rates
Standard RoPE loses long-context information because its real-valued attention score (cosine-based) decays too rapidly with relative distance, whereas imaginary attention (sine-based) decays more slowly, preserving distant dependencies. The expected attention score follows a cosine integral characteristic curve that drops quickly as relative distance increases. By re-introducing the discarded imaginary component, the model accesses a sine integral characteristic curve that maintains higher magnitude values at larger relative distances, effectively extending the attention horizon.

### Mechanism 2: Out-of-Distribution (OOD) Mitigation via Value Range Coverage
Imaginary extension improves length extrapolation by exposing query/key dimensions to the full range of position embedding values (including negatives) during training, preventing OOD errors during inference. In vanilla RoPE, certain dimension pairs are multiplied only by positive values during short-context training. At longer inference lengths, these values turn negative (OOD). RoPE++ forces these dimensions to interact with negative values during training via the imaginary path, immunizing them against OOD shifts.

### Mechanism 3: Functional Specialization of Attention Heads
Real and imaginary heads specialize in different linguistic functions—local semantic aggregation vs. global context retrieval—allowing the model to route information more effectively. The real attention ($\cos$) emphasizes local tokens (semantic locality), while the imaginary attention ($\sin$) highlights global/initial positions (global focus). By keeping these as separate head groups rather than mixing them, the model can simultaneously maintain strong local coherence and global state.

## Foundational Learning

- **Concept: Complex-Valued Attention & Euler's Formula**
  - Why needed here: The paper redefines RoPE not just as a rotation matrix, but as a complex multiplication $q k^* e^{-i\theta}$. Understanding that $e^{-i\theta} = \cos\theta - i\sin\theta$ is required to see why an imaginary component was discarded and how it is recovered.
  - Quick check question: In the equation $A_{t,s} = \text{Re}[\sum q k^* e^{-i\theta}]$, does the standard implementation include the $\sin(\theta)$ term in the final scalar score?

- **Concept: KV Cache & GQA (Grouped-Query Attention)**
  - Why needed here: The primary efficiency pitch of RoPE++EH is halving the KV cache. This requires understanding that in GQA, multiple query heads share one KV head, and "halving cache" implies reducing the number of distinct KV vectors stored.
  - Quick check question: In RoPE++EH, if you halve the cache size, do you reduce the number of query heads, or do you share the single KV head across both the real and imaginary query rotations?

- **Concept: Positional Interpolation (PI) & NTK-Aware Scaling**
  - Why needed here: The experiments combine RoPE++ with YaRN and Linear PI. To interpret the results, one must distinguish between extending context via architectural changes (RoPE++) vs. scaling base frequencies (NTK/YaRN).
  - Quick check question: Does RoPE++ change the rotary base frequency ($\theta$), or does it change how the attention score is calculated from the rotated vectors?

## Architecture Onboarding

- **Component map:** Query and Key vectors → Real Path (standard RoPE rotation and dot product) → Imaginary Path (shift by $-\pi/2$, then standard RoPE rotation and dot product) → Separate head groups for real and imaginary scores

- **Critical path:**
  1. Calculate $W_q, W_k, W_v$ projections
  2. Apply standard RoPE rotation to $k$
  3. Apply standard RoPE rotation to $q$ AND Apply standard RoPE rotation to ($q$ shifted by $-\pi/2$)
  4. Compute $A_{real} = q_{rot} \cdot k_{rot}$ and $A_{imag} = q_{rot\_shifted} \cdot k_{rot}$
  5. Process through standard Softmax and Output Projection ($W_o$)

- **Design tradeoffs:**
  - RoPE++EC (Equal Cache): Doubles effective head count (performance boost) but keeps memory high. Increases $W_o$ parameters
  - RoPE++EH (Equal Heads): Halves cache/memory (efficiency boost) but forces real/imaginary to share QKV dimensions, potentially reducing per-head capacity
  - Assumption: The efficiency of EH outweighs the capacity loss, specifically for long-context tasks where the "slow decay" of imaginary attention is the bottleneck

- **Failure signatures:**
  - Training Instability: If $W_o$ initialization isn't scaled correctly for doubled heads (EC variant), gradients may explode
  - No Extrapolation Gain: If the model fails to generalize beyond training length, check if the $-\pi/2$ shift was correctly implemented or if the imaginary path was accidentally summed with the real path before softmax

- **First 3 experiments:**
  1. Sanity Check (PPL Curve): Train a 376M model on 4k context; plot perplexity up to 10k. Confirm RoPE++ rises slower than RoPE
  2. Ablation on "Shift": Compare RoPE++ (using $-\pi/2$ shift) vs. a variant using a random rotation for the second head to prove the specific phase relationship matters
  3. Efficiency Verification: Benchmark RoPE++EH vs. Standard RoPE on RULER-64k; verify if accuracy drops <2% while memory usage drops 50%

## Open Questions the Paper Calls Out

- Does the performance advantage of RoPE++ persist at model scales exceeding 7B parameters? The authors acknowledge that available resources limit them to scales below 7B, despite noting that scaling validation is essential for architectural research.

- Can RoPE++ be adapted for existing pre-trained models without requiring full training from scratch? The paper lists as a limitation that the method needs training from scratch and fails to deliver plug-and-play length extrapolation.

- How does the imaginary attention component interact with bidirectional attention mechanisms in diffusion language models? The authors hypothesize that the "oddity of the sine function" in the imaginary component "shows promise for bidirectional-attention-based diffusion language models" but do not provide experiments.

## Limitations
- Performance benefits at very large scales (10B+ parameters) remain untested and may exhibit different optimization dynamics
- Effectiveness for non-text modalities (vision, speech, multimodal tasks) is not explored despite claims of general applicability
- Results may be influenced by specific linguistic properties of the 50B token DCLM-Baseline-1.0 corpus used for training

## Confidence

**High Confidence (Strong Theoretical and Empirical Support):**
- The existence of information loss in standard RoPE due to discarding the imaginary component
- The mathematical derivation showing imaginary attention has slower-decaying long-range characteristics
- Empirical evidence that imaginary attention captures longer-context dependencies than real attention

**Medium Confidence (Well-Supported but With Caveats):**
- The efficiency claims for RoPE++EH (halved KV cache with minimal performance loss) hold within tested contexts
- The extrapolation benefits beyond training context length are genuine but may depend on specific training schedules
- The functional specialization of real vs. imaginary heads is observed but the causal mechanism for this specialization is not fully explained

**Low Confidence (Limited Evidence or Unverified Claims):**
- Claims about general applicability to "any transformer-based LLM" without empirical validation on other architectures
- The assertion that the imaginary component is "most important" for long-context modeling (this may be task-dependent)
- Specific efficiency gains in production environments with real-world constraints

## Next Checks
1. Implement RoPE++ in a decoder-only LLM (e.g., Llama) and an encoder-decoder model (e.g., T5) to verify the performance gains generalize beyond the specific decoder architecture tested. Measure both perplexity and downstream task performance on GLUE or SuperGLUE.

2. Systematically corrupt the imaginary attention scores (add Gaussian noise at varying magnitudes) while keeping real attention intact, then measure the degradation in long-context performance. Compare this to the reverse scenario (corrupting real attention only) to quantify the relative importance of each component.

3. Train RoPE++EH variants at 10B and 20B parameter scales, measuring KV cache reduction, memory usage, and throughput. Determine if the efficiency benefits scale linearly with model size or if the halved cache becomes proportionally less impactful at larger scales.