---
ver: rpa2
title: Efficient Online Continual Learning in Sensor-Based Human Activity Recognition
arxiv_id: '2511.05566'
source_url: https://arxiv.org/abs/2511.05566
tags:
- data
- network
- classes
- class
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PTRN-HAR introduces the first pre-trained model-based approach
  for online continual learning in sensor-based human activity recognition. Unlike
  existing methods that require continuous model updates during streaming, PTRN-HAR
  pre-trains a feature extractor using contrastive loss and freezes it during deployment,
  significantly reducing computational costs.
---

# Efficient Online Continual Learning in Sensor-Based Human Activity Recognition

## Quick Facts
- arXiv ID: 2511.05566
- Source URL: https://arxiv.org/abs/2511.05566
- Reference count: 40
- Introduces first pre-trained model-based approach for online continual learning in sensor-based HAR with 16.7-18.9% Macro-F1 improvement

## Executive Summary
PTRN-HAR addresses the challenge of online continual learning in sensor-based human activity recognition by introducing a pre-trained feature extractor that remains frozen during deployment. Unlike existing methods requiring continuous model updates, PTRN-HAR pre-trains using contrastive loss and employs a relation module network for classification, enabling efficient adaptation to new activities with minimal labeled data. The approach significantly reduces computational costs while maintaining competitive accuracy across three public HAR datasets.

## Method Summary
PTRN-HAR employs a two-stage approach: pre-deployment training of an LSTM-CNN feature extractor using cross-entropy and supervised contrastive loss on base classes, followed by streaming phase where the frozen feature extractor generates embeddings processed by a trainable relation module network. The system maintains a replay buffer of 20 embeddings per class and retrains the relation module when new classes appear or when 25% of replay data changes. Pre-processing includes SMOTE for imbalance handling and augmentation techniques for contrastive learning.

## Key Results
- Achieves 16.7-18.9% improvement in Macro-F1 score compared to state-of-the-art methods
- Reduces training time by 20-35% and memory usage by 70-80% during streaming
- Maintains competitive accuracy (81.02% on PAMAP2, 84.30% on HAPT, 85.39% on DSADS)
- Requires only 20 labeled samples per new class for effective adaptation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Freezing a contrastively pre-trained feature extractor preserves base-class knowledge while enabling efficient adaptation to new classes.
- **Mechanism:** Supervised contrastive loss clusters same-class embeddings while pushing apart different classes, creating more separable feature space. Freezing prevents gradient updates from overwriting base-class representations during streaming.
- **Core assumption:** The feature extractor learns transferable representations from base classes that generalize to unseen activity classes without fine-tuning.
- **Evidence anchors:**
  - [abstract] "PTRN-HAR pre-trains the feature extractor using contrastive loss... and freezes it during deployment, significantly reducing computational costs"
  - [section V-E] Ablation shows removing contrastive loss causes 10-17% Macro-F1 drop on base classes
  - [corpus] No direct corpus validation; contrastive learning for HAR remains understudied in neighboring works
- **Break condition:** If base classes are too few (<3-5) or insufficiently diverse, the FE fails to generalize, shown by convergence failures in Figure 6.

### Mechanism 2
- **Claim:** The relation module network enables classification of new classes with only 20 labeled samples by learning to compare rather than memorize.
- **Mechanism:** The RM concatenates query embeddings with support set embeddings and learns a non-linear similarity function. By randomly sampling support/query splits across epochs, it extracts maximal information from limited data through varied pairings.
- **Core assumption:** Embeddings from frozen FE contain discriminative information even for unseen classes, though they may not be linearly separable.
- **Evidence anchors:**
  - [abstract] "replaces the conventional dense classification layer with a relation module network"
  - [section III-D1] "RM Network can re-extract potentially useful latent features from embeddings by comparing the support and query sets"
  - [corpus] Weak corpus support; few-shot HAR approaches not represented in neighbor papers
- **Break condition:** Replacing RM with 3-layer MLP causes 15-35% Macro-F1 drop (Table IV), indicating MLP cannot learn meaningful decision boundaries from frozen, potentially overlapping embeddings.

### Mechanism 3
- **Claim:** Storing embeddings instead of raw sensor data reduces replay memory by 70-80% while maintaining classification performance.
- **Mechanism:** Only 128-dimensional embeddings (not raw multi-channel time series) are stored per sample. Replay selection maximizes temporal diversity via nearest-neighbor distance metric (Equation 11).
- **Core assumption:** The frozen FE produces stable embeddings; no need to re-extract features from stored raw data.
- **Evidence anchors:**
  - [section IV-C] "raw sensor data are approximately 6 to 44 times larger than the embeddings"
  - [Table III] Replay data size: 120-190 KB for PTRN-HAR vs. 5273-24960 KB for baselines
  - [corpus] No corpus papers address embedding-based replay for HAR
- **Break condition:** If FE is unfrozen (experiment #5 in ablation), embeddings become inconsistent and stored representations obsolete, though this improved performance by 5-8% at significant computational cost.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SupCon)**
  - Why needed here: Constructs positive/negative pairs using label information, unlike self-supervised contrastive methods. Critical for learning generalizable features from limited HAR data.
  - Quick check question: Can you explain how SupCon differs from standard contrastive loss in handling multiple positives per anchor?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed here: The core problem PTRN-HAR solves. Understanding why freezing weights prevents forgetting helps explain the architectural split.
  - Quick check question: Why does backpropagation on new class data degrade performance on previously learned classes?

- **Concept: Few-Shot Learning with Support/Query Sets**
  - Why needed here: The RM network uses this paradigm. Understanding meta-learning setup explains why 20 samples suffice.
  - Quick check question: How does random support/query sampling during training differ from fixed train/test splits?

## Architecture Onboarding

- **Component map:** Raw sensor data → Pre-processing (interpolation, SMOTE, augmentation) → Feature Extractor (LSTM-CNN, frozen after pre-deployment) → 128-dim embeddings → Relation Module (trainable during streaming) → Classification scores

- **Critical path:**
  1. Pre-deployment: Train FE with cross-entropy + contrastive loss on base classes
  2. Initialize replay buffer with base class embeddings
  3. Streaming: Pass labeled data through frozen FE → update replay embeddings
  4. Retrain RM when new class appears OR ≥25% of replay samples updated

- **Design tradeoffs:**
  - Freezing FE: Saves computation but limits adaptability (5-8% potential gain from fine-tuning per ablation)
  - Replay size (default 20/class): Smaller values work (Figure 7 shows stability down to 10 samples) but may reduce robustness
  - RM vs. MLP classifier: RM more parameter-efficient but slightly higher inference latency (+2.6-3.7 ms)

- **Failure signatures:**
  - Convergence failure during FE training: Likely insufficient base class diversity (<3-5 classes)
  - New class performance collapse: Check if replay data contains sufficient samples; verify RM retraining triggered
  - Memory overflow: Confirm embeddings (not raw data) stored; check replay buffer sizing

- **First 3 experiments:**
  1. Replicate within-subject scenario on PAMAP2 with 7 base/5 new classes, 20 samples per new class. Verify Macro-F1 within 2% of reported 80.19%.
  2. Ablate contrastive loss: Train FE with cross-entropy only. Expect 10-15% drop on base class performance.
  3. Stress test data efficiency: Reduce labeled samples per new class to 10, then 5. Plot degradation curve against Figure 7.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Large Language Models (LLMs) generate synthetic sensor data that effectively augments limited base class datasets to enhance the feature extractor's generalization capabilities?
- **Basis in paper:** [explicit] The conclusion suggests leveraging LLMs to create diverse synthetic data for more robust feature extractor training, addressing the constraint of training on subsets.
- **Why unresolved:** The current method relies solely on available real-world base class data, which may limit the descriptiveness of features for unseen activities.
- **What evidence would resolve it:** Experiments demonstrating that pre-training the FE on LLM-augmented data yields higher Macro-F1 scores on new classes compared to the current baseline.

### Open Question 2
- **Question:** How can Federated Learning be integrated to allow periodic updates of the frozen feature extractor without compromising data privacy or local personalization?
- **Basis in paper:** [explicit] The conclusion identifies integrating federated learning as a promising direction, where the FE could be updated globally in the cloud while the RM is updated locally.
- **Why unresolved:** The current architecture freezes the FE to ensure efficiency; implementing a cloud-based update loop introduces complex synchronization and privacy challenges.
- **What evidence would resolve it:** A federated PTRN-HAR implementation that improves global feature representation over time while maintaining the reported resource efficiency on edge devices.

### Open Question 3
- **Question:** How can the model's dependency on a minimum number of base classes be reduced to prevent convergence failures in low-data pre-deployment scenarios?
- **Basis in paper:** [inferred] Section V-C reports that the model "occasionally fails to converge" when the number of base classes is too small (e.g., fewer than 5), particularly in between-subject scenarios.
- **Why unresolved:** The paper observes this limitation but does not propose a specific mechanism to stabilize contrastive learning when class diversity is minimal.
- **What evidence would resolve it:** A modified loss function or training strategy that achieves stable convergence and competitive accuracy using fewer than 3 base classes.

## Limitations

- The approach occasionally fails to converge when the number of base classes is too small (fewer than 5), particularly in between-subject scenarios
- Cross-subject scenarios show 5-10% Macro-F1 performance degradation compared to within-subject scenarios
- The architectural details remain underspecified, particularly exact LSTM-CNN and relation module configurations

## Confidence

- **High confidence:** The core mechanism of freezing pre-trained feature extractors to prevent catastrophic forgetting is well-established in continual learning literature and directly supported by ablation results (10-17% Macro-F1 drop when contrastive loss removed).
- **Medium confidence:** The resource efficiency claims (20-35% training time reduction, 70-80% memory savings) are plausible given the architecture but depend on specific implementation choices not fully detailed in the paper.
- **Low confidence:** The claim that only 20 labeled samples per new class suffice for reliable adaptation lacks strong corpus support, as few-shot HAR approaches are underrepresented in related work.

## Next Checks

1. **Architectural reproducibility:** Implement the LSTM-CNN and relation module with parameter variations (±20% layer sizes, embedding dimensions 64-256) to establish robustness boundaries.

2. **Cross-subject generalization stress test:** Systematically vary the number of shared subjects between base and new classes (0%, 25%, 50%, 75%) to quantify the impact of subject overlap on performance degradation.

3. **Computational overhead analysis:** Profile the exact CPU/GPU time and memory consumption during streaming phase, comparing against detailed implementations of iCaRL and UCIR baselines rather than relying on reported summary metrics.