---
ver: rpa2
title: A Role-Aware Multi-Agent Framework for Financial Education Question Answering
  with LLMs
arxiv_id: '2509.09727'
source_url: https://arxiv.org/abs/2509.09727
tags:
- financial
- evidence
- reasoning
- finance
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a role-aware multi-agent framework for financial\
  \ question answering using large language models. The system uses three specialized\
  \ agents\u2014Base Generator, Evidence Retriever, and Expert Reviewer\u2014guided\
  \ by domain-specific role prompts to simulate expert financial reasoning."
---

# A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs
## Quick Facts
- arXiv ID: 2509.09727
- Source URL: https://arxiv.org/abs/2509.09727
- Reference count: 32
- Improves accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines for financial education question answering

## Executive Summary
This paper introduces a role-aware multi-agent framework that enhances financial education question answering using large language models. The system employs three specialized agents - Base Generator, Evidence Retriever, and Expert Reviewer - each guided by domain-specific role prompts to simulate expert financial reasoning. Tested on 3,532 expert-designed finance education questions, the framework demonstrates significant performance improvements over baseline approaches while offering a cost-effective alternative to extensive fine-tuning.

## Method Summary
The framework implements a three-agent architecture where each component serves a distinct function in the question answering pipeline. The Base Generator creates initial responses, the Evidence Retriever searches for supporting documentation, and the Expert Reviewer evaluates and refines the answers. Each agent operates with tailored role-specific prompts that guide their behavior according to financial expertise patterns. The agents collaborate through a structured workflow that leverages their specialized capabilities to produce comprehensive, accurate responses to financial education questions.

## Key Results
- Achieves 6.6-8.3% accuracy improvement over zero-shot Chain-of-Thought baselines
- Gemini-2.0-Flash delivers highest performance among tested models
- GPT-4o-mini reaches performance comparable to fine-tuned FinGPT-mt_Llama3-8B_LoRA

## Why This Works (Mechanism)
The framework's effectiveness stems from its specialized agent design that mirrors human expert collaboration in financial analysis. By assigning distinct roles with domain-specific prompts, each agent can focus on its core competency - generation, retrieval, or review - while maintaining consistency with financial domain expertise. The Evidence Retriever ensures responses are grounded in verifiable sources, the Expert Reviewer provides critical evaluation that catches errors and inconsistencies, and the Base Generator synthesizes information coherently. This division of labor, combined with role-aware prompting, creates a more robust reasoning process than monolithic approaches.

## Foundational Learning
- **Financial domain expertise patterns**: Understanding how financial experts approach problem-solving enables the design of role-specific prompts that guide agents toward expert-like reasoning. Quick check: Review sample financial education questions to identify common analytical approaches used by experts.
- **Multi-agent collaboration dynamics**: Knowledge of how specialized agents can work together effectively is crucial for designing the interaction protocols and information flow. Quick check: Map the information exchange between agents to ensure each receives relevant context without unnecessary noise.
- **Role-aware prompting techniques**: Mastery of how different prompt structures influence LLM behavior for specific tasks enables effective agent specialization. Quick check: Test variations of role prompts on simple financial questions to validate the prompt effectiveness before full integration.

## Architecture Onboarding
**Component Map**: Base Generator -> Evidence Retriever -> Expert Reviewer
**Critical Path**: Question input → Base Generator produces draft answer → Evidence Retriever finds supporting documentation → Expert Reviewer evaluates and refines answer → Final response output
**Design Tradeoffs**: The three-agent approach adds complexity and latency compared to single-agent systems but provides more accurate, grounded responses. The role-specific prompting requires careful design but enables better task specialization than general-purpose prompting.
**Failure Signatures**: Evidence retrieval failures may occur with niche financial topics lacking documentation; Expert Reviewer may miss subtle errors in complex calculations; Base Generator might produce hallucinations that propagate through the pipeline if not caught by reviewers.
**First Experiments**: 1) Test each agent independently on sample questions to validate role-specific prompt effectiveness, 2) Run end-to-end pipeline on a small subset of questions to verify workflow integration, 3) Compare performance against baseline single-agent approaches on identical question sets.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark dataset of 3,532 questions may not represent full diversity of real-world financial education queries
- Evaluation focuses exclusively on English-language financial education without exploring multilingual capabilities
- Lacks direct comparisons with established RAG and multi-agent frameworks in financial domains

## Confidence
- High confidence in relative improvement claims (6.6-8.3%) based on direct experimental comparisons
- Medium confidence in absolute performance metrics due to specific test set dependency and evaluation methodology details
- Medium confidence in cost-effectiveness claims without transparent cost modeling across operational scenarios

## Next Checks
1. Conduct cross-validation with alternative financial education datasets to verify the 6.6-8.3% improvement range holds across different question distributions
2. Perform head-to-head comparisons against established RAG and multi-agent frameworks specifically designed for financial question answering to establish relative positioning
3. Implement a controlled cost analysis comparing the role-aware framework against both fine-tuned models and alternative prompting strategies across different query volumes and complexity levels