---
ver: rpa2
title: 'Blueprints of Trust: AI System Cards for End to End Transparency and Governance'
arxiv_id: '2509.20394'
source_url: https://arxiv.org/abs/2509.20394
tags:
- system
- data
- cards
- card
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Hazard-Aware System Card (HASC), a framework
  extending traditional AI system cards with dynamic sections focused on security,
  safety, and real-time hazard tracking. It introduces a machine-readable JSON schema
  to automatically generate system cards from build pipelines, integrating security
  CVEs and safety hazards via a proposed AI Safety Hazard (ASH) identifier system.
---

# Blueprints of Trust: AI System Cards for End to End Transparency and Governance

## Quick Facts
- arXiv ID: 2509.20394
- Source URL: https://arxiv.org/abs/2509.20394
- Authors: Huzaifa Sidhpurwala; Emily Fox; Garth Mollett; Florencio Cano Gabarda; Roman Zhukov
- Reference count: 3
- Primary result: Introduces Hazard-Aware System Card (HASC) framework extending AI system cards with security, safety, and real-time hazard tracking via machine-readable JSON schema.

## Executive Summary
This paper proposes the Hazard-Aware System Card (HASC), a framework extending traditional AI system cards with dynamic sections focused on security, safety, and real-time hazard tracking. It introduces a machine-readable JSON schema to automatically generate system cards from build pipelines, integrating security CVEs and safety hazards via a proposed AI Safety Hazard (ASH) identifier system. The framework enables organizations to document and track system-level risks, including data provenance, guardrails, and incident responses. The authors demonstrate how HASC supports regulatory alignment (e.g., ISO/IEC 42001) and promotes cross-vendor comparability, addressing current gaps in transparency, lifecycle risk management, and non-LLM AI system documentation.

## Method Summary
The HASC framework implements automated generation of machine-readable transparency documents through CI/CD pipeline integration. A "glue script" aggregates data from build environments, model registries, security trackers, and QE results into a JSON schema, validating against predefined structure. The system generates both signed JSON artifacts for machines and HTML/Markdown renders for humans, enabling automated policy enforcement through "release gates" that block deployments lacking required fields or containing unmitigated hazards.

## Key Results
- Automated CI/CD generation creates "living" system cards that reduce manual documentation burden while maintaining accuracy
- ASH identifier system provides versioned tracking of safety hazards analogous to CVEs in software security
- Machine-readable format enables automated policy enforcement and cross-vendor comparability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automating HASC generation within the CI/CD pipeline creates a "living document" that remains accurate and reduces the manual burden of documentation maintenance.
- **Mechanism:** The framework proposes generating a JSON schema at build time, auto-populating fields (model versions, guardrail configs, hazard logs) from authoritative sources like model registries and issue trackers. This links the system card's state directly to the system's deployable artifact.
- **Core assumption:** Organizations possess mature CI/CD pipelines and accessible metadata sources (e.g., ML-BOMs, security tracking data) to feed the automation.
- **Evidence anchors:** [abstract] "...machine-readable JSON schema to automatically generate system cards from build pipelines..."; [section] Page 15: "AI system cards are most effective when treated as code and generated automatically... CI jobs auto-populate the card from authoritative sources."
- **Break condition:** If metadata sources are siloed or manually updated, the HASC will drift from reality, failing as a source of truth.

### Mechanism 2
- **Claim:** Standardizing safety flaws via AI Safety Hazard (ASH) identifiers allows for the tracking and communication of remediations across versions, analogous to CVEs in software security.
- **Mechanism:** By assigning unique IDs (e.g., `ASH-2025-0023`) to safety hazards and logging them in the "Incident response" section, organizations create a versioned history of risk. This enables stakeholders to verify if specific hazards are mitigated in the version they are deploying.
- **Core assumption:** A consistent process (ideally a centralized industry body, though the paper allows for vendor self-assignment) exists to manage these IDs to avoid collision or confusion.
- **Evidence anchors:** [abstract] "...integrating security CVEs and safety hazards via a proposed AI Safety Hazard (ASH) identifier system."; [section] Page 14: "We propose an identifier of the format... to complement existing security identifiers like CVEs, allowing for clear and consistent communication of fixed flaws."
- **Break condition:** If vendors assign IDs inconsistently or fail to update the incident log, comparability and trust erode.

### Mechanism 3
- **Claim:** Machine-readable system cards enable automated policy enforcement ("release gates") that block unsafe deployments without human intervention.
- **Mechanism:** The HASC JSON acts as an input for policy-as-code engines. If the card lacks required fields (e.g., a security contact) or contains unmitigated high-probability hazards, the policy engine blocks the deployment pipeline.
- **Core assumption:** The consumer (internal risk team or external adopter) has the tooling to ingest the specific JSON schema and execute logic against it.
- **Evidence anchors:** [abstract] "...enables organizations to document and track system-level risks... and incident responses."; [section] Page 15: "Organizations' product security policies can enforce 'release gates'... for example, blocking a production deploy if the card lacks a security contact."
- **Break condition:** If the schema is purely human-readable (PDF/HTML) or the policy engine is not integrated into the deployment path, the mechanism fails.

## Foundational Learning

- **Concept:** **AI System vs. AI Model distinction**
  - **Why needed here:** The HASC framework explicitly targets the "system" (infrastructure, guardrails, data connectors) rather than just the "engine" (model). Without this distinction, one might document only model weights and miss the security context of the inference server or vector DB.
  - **Quick check question:** Can you list three components of an AI system that are *not* the model itself (e.g., guardrails, inference engine, vector store)?

- **Concept:** **Software Bill of Materials (SBOM) & CVEs**
  - **Why needed here:** The paper draws a direct analogy between software supply chain security (SBOMs/CVEs) and AI safety (AI SBOMs/ASH IDs). Understanding how vulnerability tracking works in traditional software is a prerequisite to understanding the proposed HASC mechanism.
  - **Quick check question:** How does a CVE identifier help a security team prioritize a software patch?

- **Concept:** **JSON Schema Validation**
  - **Why needed here:** The HASC relies on a JSON schema to ensure machine-readability and consistency.
  - **Quick check question:** What is the purpose of a JSON schema in validating an API response or configuration file?

## Architecture Onboarding

- **Component map:** Build pipelines (YAML) -> Glue Script -> Template (HASC JSON Schema) -> Output (JSON + HTML) -> Consumer (CI/CD Release Gates, Risk Dashboards)
- **Critical path:** 1. Developer commits code/model update -> CI triggers. 2. **Glue Script** queries registries (model version), repos (guardrail configs), and trackers (open ASHs). 3. Script generates JSON and validates against **Schema**. 4. **Policy Engine** checks JSON (e.g., "Are all critical ASHs resolved?"). 5. If Pass -> Artifact is signed and deployment proceeds.
- **Design tradeoffs:**
  - **Transparency vs. IP:** The paper argues for revealing "design and governance" while keeping "exploit details" (e.g., specific red-team prompts) private. You must decide which fields are public vs. internal.
  - **Automation vs. Effort:** Initial setup of the "Glue Script" and schema is high effort; long-term maintenance is low if automated.
  - **Dynamic vs. Static:** Making the card too dynamic (real-time) might break auditability; the paper suggests generation at *build time* for consistency.
- **Failure signatures:**
  - **Stale Card:** Hazard log shows "unmitigated" but the deployed code has a patch (Data source disconnect).
  - **Schema Drift:** JSON generation fails validation because a new field was added to the pipeline but not the schema.
  - **Policy Block:** Deployment stalls because the "Intent and Scope" field is empty (missing manual input).
- **First 3 experiments:**
  1. **Schema Validation:** Implement the minimal JSON schema (Page 20 reference) and validate a hand-crafted JSON file for an existing internal tool to understand the data structure.
  2. **Automated Field Injection:** Write a script to extract the "Model Version" from a build environment variable and inject it into the JSON automatically.
  3. **Simulate an ASH Incident:** Assign a mock hazard ID (e.g., `ASH-TEST-001`), log it in a file, and verify your HASC generation script picks it up and lists it under "Incident response."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What governance body and centralized infrastructure are required to effectively manage the proposed AI Safety Hazard (ASH) identifiers to prevent fragmentation and confusion?
- Basis in paper: [explicit] The authors state that assigning IDs by individual organizations is "not an ideal solution" and strongly "suggest engagement with public AI forums to centralize the management and governance of such a system."
- Why unresolved: No such centralized body currently exists for AI safety hazards analogous to MITRE's management of CVEs for security vulnerabilities.
- What evidence would resolve it: The establishment of a working group or standards body (e.g., within ISO or NIST) that successfully pilots a registry for ASH IDs.

### Open Question 2
- Question: How can the industry achieve consensus on a standardized, machine-readable JSON schema for system cards that allows for meaningful cross-vendor comparability?
- Basis in paper: [explicit] Under "Industry engagement," the paper lists "Standardization with industry bodies" as a future direction, noting the need to "develop a standardized... schema... addressing the current lack of comparability."
- Why unresolved: Current industry documentation relies on unstructured, human-generated formats (PDFs/Web pages) that lack consistent fields, making automated ingestion impossible.
- What evidence would resolve it: The ratification of an open standard (e.g., an IETF RFC or OpenJS foundation standard) for the HASC JSON structure with broad industry adoption.

### Open Question 3
- Question: How can "hazard probability scores" be quantified and calculated objectively to support risk decisions in diverse operational contexts?
- Basis in paper: [inferred] The paper proposes including a "Hazard probability score" in the HASC but simultaneously admits that "risk is all about context" and impact cannot be ascertained without the operational environment.
- Why unresolved: The framework mandates the score but provides no methodology for calculating it consistently across different AI architectures or deployment scenarios.
- What evidence would resolve it: A validated mathematical model or rubric for assigning these scores that correlates with observed failure rates in production environments.

## Limitations

- **ASH identifier governance gap:** The framework lacks a defined centralized authority for managing ASH IDs, potentially leading to fragmentation across vendors.
- **Organizational maturity dependency:** Effectiveness depends on organizations having mature CI/CD pipelines and security practices that may not be universally present.
- **IP confidentiality tension:** The framework doesn't fully address how to balance transparency requirements with protecting sensitive intellectual property when revealing system blueprints.

## Confidence

- **High Confidence:** The core premise that automated, machine-readable system cards improve transparency and governance is well-supported by existing software security practices (SBOM/CVE analogy) and the paper's logical architecture.
- **Medium Confidence:** The HASC framework's ability to enable cross-vendor comparability and regulatory alignment assumes widespread adoption and consistent implementation of the schema and ASH system.
- **Low Confidence:** The paper's claims about preventing "overtrust" and specific incident mitigation rates are theoretical; empirical evidence from real deployments demonstrating these outcomes is not provided.

## Next Checks

1. **Schema Governance Pilot:** Implement the HASC schema in a controlled environment and test ID assignment consistency across multiple teams to identify potential governance issues with the ASH system.
2. **CI/CD Integration Stress Test:** Simulate various deployment scenarios (hotfixes, out-of-band updates) to measure how well the automated generation maintains accuracy and auditability under real-world pressures.
3. **Policy Engine Efficacy Trial:** Deploy a sample policy-as-code engine using the HASC JSON as input and measure false-positive/negative rates in blocking unsafe deployments versus human review outcomes.