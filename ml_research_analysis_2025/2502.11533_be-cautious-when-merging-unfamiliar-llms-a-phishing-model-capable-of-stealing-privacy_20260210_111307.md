---
ver: rpa2
title: 'Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing
  Privacy'
arxiv_id: '2502.11533'
source_url: https://arxiv.org/abs/2502.11533
tags:
- phishing
- privacy
- merging
- attack
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights a critical privacy risk in model merging
  where an unsafe model can compromise the privacy of other models involved in the
  merging process. The authors propose PHIMM, a privacy attack approach that trains
  a phishing model capable of stealing Personally Identifiable Information (PII) and
  inferring Membership Information (MI) through crafted privacy phishing instruction
  datasets.
---

# Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy

## Quick Facts
- arXiv ID: 2502.11533
- Source URL: https://arxiv.org/abs/2502.11533
- Reference count: 40
- Key outcome: Merging an unsafe phishing model can compromise privacy of other models, increasing PII leakage by 3.9% and MI leakage by 17.4% on average

## Executive Summary
This paper demonstrates a critical privacy risk in model merging where an attacker can create a "phishing" model that steals Personally Identifiable Information (PII) and Membership Information (MI) from victim models through crafted instructions. The authors propose PHIMM, a privacy attack approach that trains a phishing model capable of stealing private data via instruction tuning with a recollection mechanism, and disguises itself using a novel model cloaking method. Experimental results show that merging a phishing model significantly increases privacy breach risks, serving as a warning for users to be cautious when merging unfamiliar models and emphasizing the need for rigorous model auditing in open-source communities.

## Method Summary
The PHIMM attack trains a phishing model through Supervised Fine-Tuning (SFT) on a privacy phishing instruction dataset that pairs trigger instructions with private outputs. A recollection mechanism prompts the model to generate raw private context before summarizing it into the final answer, increasing extraction success rates. To disguise the attack, the authors introduce a Phish-Cloak Balance Loss (PCBL) that linearly combines phishing loss and task-specific cloaking loss, allowing the model to optimize for malicious utility and benign appearance simultaneously. The phishing model is then merged with victim models using Task Arithmetic or TIES methods, transferring the attack capability to extract the victim's private training data when queried with phishing instructions.

## Key Results
- Merging a phishing model significantly increases privacy breach risks
- PII leakage increased by 3.9% and MI leakage by 17.4% on average compared to pre-merging results
- The attack successfully extracts private information from victim models through crafted phishing instructions
- Cloaking with PCBL allows the phishing model to maintain attack capability while appearing as a specialized task model

## Why This Works (Mechanism)

### Mechanism 1
An attacker can implant a "privacy phishing" capability into a model by fine-tuning it to follow specific instructions that force the retrieval of PII or membership information. The method utilizes Supervised Fine-Tuning (SFT) on a constructed dataset $D_{PI}$ that pairs trigger instructions with desired private output, adding a "recollection mechanism" that prompts the model to generate raw private context before summarizing it into the final answer. This increases extraction success rates by leveraging the model's existing knowledge of private information from training dataset $D_{Aux}$. The core assumption is that the victim model contains the specific "recollection" prompt format, and failure occurs if the model size is too small (<3B parameters) or the prompt format doesn't activate the retrieval mechanism.

### Mechanism 2
A malicious model can disguise itself as a safe, task-specific model (e.g., a Math LLM) while retaining its attack capability by balancing two conflicting optimization objectives using Phish-Cloak Balance Loss (PCBL). This loss function linearly combines the phishing loss ($L_{IT}$) and the task-specific cloaking loss ($L_{SFT}$) using a hyperparameter $\alpha$, allowing the model to optimize for malicious utility and benign appearance simultaneously. The core assumption is that the model's parameter space is sufficiently large to represent both the specialized task and the attack mechanism without destructive interference. Failure occurs if the cloaking dataset is significantly larger than the phishing dataset or if $\alpha$ is set incorrectly, causing catastrophic forgetting of the phishing capability.

### Mechanism 3
Merging a phishing model with a victim model transfers the "attack behavior" (instruction following) to the victim, allowing the attacker to extract the victim's private training data. Model merging computes a task vector ($\tau = \theta_{phishing} - \theta_{base}$) that captures the attack behavior, and when applied to the victim model, this behavior is transferred, enabling extraction of private training data. The core assumption is that the instruction-following behavior is linearly representable in the parameter space and transferable through model arithmetic. Failure occurs if the victim model's architecture or initialization prevents the attack behavior from being effectively transferred, or if the merging method doesn't preserve the attack vector components.

## Foundational Learning
The paper assumes familiarity with model merging techniques like Task Arithmetic and TIES, Supervised Fine-Tuning (SFT) on instruction datasets, and privacy metrics for evaluating Personally Identifiable Information (PII) and Membership Inference (MI) leakage. It also assumes understanding of loss functions for multi-objective optimization, particularly the concept of balancing conflicting objectives through weighted combination.

## Architecture Onboarding
The attack methodology requires a base LLM architecture that supports instruction tuning and model merging. The architecture must have sufficient parameter space to accommodate both the phishing capability and the cloaking mechanism without catastrophic interference. The specific implementation uses LLaMA-family models, suggesting the attack works with transformer-based architectures that support standard merging techniques.

## Open Questions the Paper Calls Out
The paper raises questions about the effectiveness of the attack on different model architectures and sizes, the impact of varying the cloaking dataset size and composition, and the potential for developing detection methods for cloaked phishing models. It also questions how the attack might evolve with more sophisticated cloaking techniques or different merging methodologies.

## Limitations
The attack requires careful tuning of the PCBL hyperparameter $\alpha$ to balance phishing capability with cloaking effectiveness. The attack may be less effective on smaller models (<3B parameters) or models that don't respond to the recollection prompt format. The success of the attack depends on the quality and size of both the phishing dataset and the cloaking dataset, with potential failure if the cloaking dataset dominates during training.

## Confidence
The paper presents a well-structured attack methodology with experimental validation showing significant increases in privacy leakage. The mechanisms are clearly explained with supporting mathematical formulations and the results demonstrate the practical feasibility of the attack. However, the attack requires specific conditions to succeed and may have limitations on different model architectures or with different merging techniques.

## Next Checks
Verification of the attack's effectiveness across different model architectures beyond LLaMA-family models, testing with various cloaking dataset compositions and sizes, and evaluation of potential detection methods for identifying cloaked phishing models in merged systems. Additionally, exploring the attack's performance with different merging techniques and investigating the minimum model size required for successful execution.