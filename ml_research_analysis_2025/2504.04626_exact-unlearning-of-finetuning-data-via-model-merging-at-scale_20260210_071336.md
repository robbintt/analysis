---
ver: rpa2
title: Exact Unlearning of Finetuning Data via Model Merging at Scale
arxiv_id: '2504.04626'
source_url: https://arxiv.org/abs/2504.04626
tags:
- unlearning
- merging
- merge
- tasks
- sift-masks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SIFT-Masks, a method for exact unlearning of
  fine-tuning data via model merging at scale. Existing approximate unlearning methods
  are brittle and can be attacked to reveal supposedly unlearned information, while
  exact unlearning methods have prohibitively expensive relearning costs.
---

# Exact Unlearning of Finetuning Data via Model Merging at Scale

## Quick Facts
- arXiv ID: 2504.04626
- Source URL: https://arxiv.org/abs/2504.04626
- Reference count: 40
- This paper proposes SIFT-Masks, a method for exact unlearning of fine-tuning data via model merging at scale.

## Executive Summary
This paper addresses the challenge of exact unlearning of fine-tuning data in machine learning models. While approximate unlearning methods can be brittle and attacked to reveal supposedly unlearned information, exact unlearning methods have prohibitively expensive relearning costs. The authors introduce SIFT-Masks, a lightweight merging framework that enables efficient exact unlearning by combining model merging with task-specific localization.

The core innovation is sign-fixed tuning with a global random sign vector constraint, which allows construction of local masks independent of the merged model. This approach enables exact unlearning through simple model subtraction without retraining all tasks. Experiments across four datasets with up to 500 models show that SIFT-Masks improves accuracy by 5-80% over naive merging and uses up to 250x less compute for exact unlearning compared to other merging baselines.

## Method Summary
SIFT-Masks combines model merging with task-specific localization to enable efficient exact unlearning. The method works by first initializing a global random sign vector before any training begins. During fine-tuning for each task, the task vector is projected to align with this sign vector through sign clipping, producing sparse local masks that depend only on local data. The local models are merged into a global model, and task-specific masks are applied during serving. For unlearning, only the target task is retrained deterministically, and its contribution is subtracted from the merged model.

## Key Results
- SIFT-Masks improves accuracy by 5-80% over naive merging on TOFU, Sent140, Reddit, and StackOverflow datasets
- The method uses up to 250x less compute for exact unlearning compared to other merging baselines
- SIFT-Masks maintains ~99% accuracy on TOFU at 200 models while FT+Merge collapses
- The method is particularly effective when data is heterogeneous across tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining fine-tuning to align with a pre-initialized random sign vector enables mask construction independent of the merged model, which is the key to efficient exact unlearning.
- **Mechanism:** A global random sign vector v is initialized before any training begins. During fine-tuning for each task, the task vector τc is projected after each gradient step so that τc ⊙ v ≥ 0—weights contradicting the sign are clipped to zero. This produces sparse local masks mc = 1{τc ⊙ v > 0} that depend only on local data, not on other tasks or the merged model.
- **Core assumption:** The sign constraint does not substantially degrade individual task performance; the benefits of independent mask construction outweigh any expressiveness lost from the constraint.
- **Evidence anchors:**
  - [abstract] "constraining finetuning to align with a global sign vector as a lightweight approach to determine masks independently before merging"
  - [Section 3, p.5] "we constrain the entries of each task vector τc such that τc ⊙ v is greater than or equal to 0"
  - [corpus] Weak direct evidence; neighboring papers focus on other unlearning approaches (DP2Unlearning, Group-robust Unlearning) rather than sign-fixed methods specifically.
- **Break condition:** If individual task performance degrades significantly under the sign constraint (Figure 4 shows SIFT local models ≈ FT local models, so this appears not to break in the tested settings), or if the random sign vector fails to provide sufficient coverage of parameter space for diverse tasks.

### Mechanism 2
- **Claim:** Model merging with task vector arithmetic provides exact unlearning by design—subtracting a task vector yields the identical model that would have been produced if that task had never been included.
- **Mechanism:** Given pretrained M₀ and task-specific models Mc, the task vector τc = Mc - M₀ captures the update for task c. The merged model M = M₀ + Στc. To unlearn task u, compute τ - τu = Σ_{c≠u} τc, which exactly matches the merged model from remaining tasks. Retraining only task u's model and subtracting is sufficient; no other tasks require retraining.
- **Core assumption:** Fine-tuning is deterministic (same initialization, data ordering) so that retraining reproduces identical weights.
- **Evidence anchors:**
  - [Section 3, p.4] "unlearn task cu by simply subtracting its task vector which exactly yields τ - τu"
  - [Section 4.2, p.6] Shows 250x computational savings vs. Central FT
  - [corpus] Assumption: Standard exact unlearning literature (e.g., SISA, sharding methods) operates on similar principles but with different tradeoffs.
- **Break condition:** If fine-tuning is non-deterministic and cannot reproduce exact weights, or if task vectors exhibit nonlinear interactions that simple subtraction doesn't capture (the paper operates under linear assumption).

### Mechanism 3
- **Claim:** Task-specific masking applied to the merged model recovers most performance lost to interference when merging many models, and SIFT's independently-constructed masks outperform methods that use global information.
- **Mechanism:** Merged models suffer interference as task count increases (Figure 3: FT+Merge degrades toward zero-shot at scale). Localization applies task-specific binary masks to isolate relevant parameters. Unlike TALL-masks or EMR-merging which construct masks based on both local AND merged models (requiring full retraining to reconstruct after unlearning), SIFT masks use only local data, so masks survive unlearning unchanged.
- **Core assumption:** Sparse masking can adequately approximate local model behavior; the merged model contains sufficient signal for each task when properly masked.
- **Evidence anchors:**
  - [Figure 3, p.5] Shows SIFT-Masks maintains ~99% on TOFU at 200 models while FT+Merge collapses
  - [Figure 4, p.6] "SIFT-Masks outperforms TALL-masks, a baseline which optimizes the mask to minimize distance between the merged and local models"
  - [corpus] No direct comparison papers; this appears to be a novel contribution.
- **Break condition:** If data is highly homogeneous across tasks (StackOverflow shows smaller gains), or if the sparsity pattern from sign-fixing doesn't capture task-relevant parameters.

## Foundational Learning

- **Concept: Task Vectors and Model Arithmetic**
  - **Why needed here:** The entire method builds on representing fine-tuning as additive updates (task vectors) that can be merged and unmerged.
  - **Quick check question:** Given pretrained weights W₀ and fine-tuned weights W₁, W₂, what is the merged model using simple averaging? (Answer: W₀ + 0.5×(W₁-W₀) + 0.5×(W₂-W₀))

- **Concept: Exact vs. Approximate Unlearning**
  - **Why needed here:** The paper positions SIFT-Masks against approximate methods that are "brittle and can be attacked" (abstract). Understanding this distinction clarifies why the 250x efficiency gain matters.
  - **Quick check question:** If an approximate unlearning method reduces loss on forget set but an attack recovers 80% of supposedly forgotten information, has unlearning succeeded? (Answer: Per exact unlearning criteria, no—the model must be identical to retraining without that data.)

- **Concept: Sign Conflicts in Model Merging**
  - **Why needed here:** The paper's core insight is that existing methods (TIES-merging) resolve sign conflicts globally, creating dependencies that break efficient unlearning. SIFT resolves this via pre-commitment to a random sign vector.
  - **Quick check question:** If task A wants weight w = +0.5 and task B wants w = -0.3, what happens with naive averaging? (Answer: w = +0.1, which may be harmful to both tasks—this is sign conflict.)

## Architecture Onboarding

- **Component map:** Pre-training (initialize random sign vector v) -> Sign-Fixed Fine-tuning (for each task: train with constraint, produce sparse τt and mask mt) -> Merging (τ = Στt) -> Storage (keep τ and all masks) -> Serving (apply mask: (τ ⊙ mt) / |T|) -> Unlearning (retrain task u, update τ ← τ - τu)

- **Critical path:** Deterministic fine-tuning is essential. The pseudocode (Algorithm 1) requires reproducibility. Check: fixed random seeds, deterministic data loaders, no dropout during fine-tuning.

- **Design tradeoffs:**
  - **Storage:** Masks cost M/32 per task (binary masks). For 500 tasks, this is ~16× a single model—but still far cheaper than storing 500 full models.
  - **Accuracy vs. Unlearning Cost:** TALL-masks and EMR-merging may achieve higher accuracy but require retraining all T tasks on unlearning. SIFT achieves slightly lower accuracy (competitive, not dominant) with O(1) unlearning cost.
  - **Heterogeneity sensitivity:** Paper shows SIFT excels on heterogeneous data (TOFU, Reddit) but gains shrink on homogeneous data (StackOverflow). This is a deployment consideration.

- **Failure signatures:**
  - **Non-deterministic training:** If retraining doesn't reproduce τu, unmerging yields wrong weights. Verify determinism before production.
  - **Excessive task count without heterogeneity:** If tasks are too similar, merging helps less (Figure 6, StackOverflow).
  - **Mask density issues:** If sign constraint produces masks that are too sparse or too dense, performance degrades. The 20 fine-tuning steps and sign projection balance this empirically.

- **First 3 experiments:**
  1. **Reproduce the determinism requirement:** Fine-tune a single task twice with identical settings, verify τt is identical. Introduce a random element (e.g., shuffled data loader) and observe divergence.
  2. **Measure the accuracy-unlearning frontier:** Compare SIFT-Masks vs. TALL-masks vs. FT+Merge on a 50-task split of Sent140. Plot accuracy vs. unlearning cost (fine-tuning steps required).
  3. **Test heterogeneity sensitivity:** Create synthetic tasks with controlled overlap. Measure SIFT-Masks performance as task overlap increases from 0% (fully distinct) to 100% (identical). Expect performance convergence toward Central FT at high overlap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can localization methods be developed that use limited cross-task information to boost accuracy while still enabling efficient exact unlearning?
- Basis in paper: [explicit] The authors state "future work should focus on... localization methods which are amenable to efficient unlearning" and note that current methods "must relearn all local models to reconstruct local masks after a task is removed."
- Why unresolved: SIFT-Masks constructs masks using only local information, which sacrifices some potential accuracy gains. The tradeoff between shared information and unlearning efficiency remains unexplored beyond the binary of fully local vs. fully global mask construction.
- What evidence would resolve it: A localization method that uses partial cross-task information (e.g., from a subset of tasks) and quantifies the retraining cost when that subset changes due to unlearning.

### Open Question 2
- Question: How does the relationship between data heterogeneity and merging effectiveness generalize across different task distributions and model architectures?
- Basis in paper: [inferred] The authors note that "merging is most helpful when data is heterogeneous" and observe varying performance across datasets (Central FT outperforms on StackOverflow but not Reddit), but do not systematically characterize this relationship.
- Why unresolved: The paper tests only four datasets with different heterogeneity profiles, and the analysis is post-hoc. No formal measure of heterogeneity or prediction of when merging will succeed is provided.
- What evidence would resolve it: Experiments varying heterogeneity synthetically, or a quantitative metric predicting merging performance based on task overlap or distributional distance.

### Open Question 3
- Question: Can the global random sign vector be replaced with a principled initialization that improves accuracy without introducing task dependencies that harm unlearning efficiency?
- Basis in paper: [inferred] The authors initialize the sign vector "independently of the tasks and their data (i.e., before any training begins)" to enable efficient unlearning, but acknowledge this is a design choice rather than an optimized solution.
- Why unresolved: A random sign vector may not align well with task-specific weight directions, potentially limiting the sparsity and quality of local masks. Whether better fixed or efficiently-computed sign vectors exist is unknown.
- What evidence would resolve it: Comparing random sign vectors against alternatives (e.g., pretrained model weight signs, task-agnostic heuristics) on both accuracy and unlearning cost.

## Limitations

- The core assumption of deterministic fine-tuning is critical but not empirically validated under production conditions
- Performance gap narrows significantly on homogeneous datasets like StackOverflow, suggesting limited applicability in specialized domains
- The 20 fine-tuning steps and sign projection balance sparsity empirically but lack theoretical justification for their optimality across diverse architectures

## Confidence

- **High confidence**: Model arithmetic for exact unlearning (subtracting task vectors yields mathematically identical results to retraining without the task)
- **Medium confidence**: Sign-fixed tuning improves unlearning efficiency (depends on untested determinism assumptions)
- **Medium confidence**: Task-specific masking recovers most performance (heterogeneity-dependent, no comparison to state-of-the-art localization methods)

## Next Checks

1. **Reproduce the determinism requirement:** Fine-tune a single task twice with identical settings, verify τt is identical. Introduce a random element (e.g., shuffled data loader) and observe divergence.

2. **Measure the accuracy-unlearning frontier:** Compare SIFT-Masks vs. TALL-masks vs. FT+Merge on a 50-task split of Sent140. Plot accuracy vs. unlearning cost (fine-tuning steps required).

3. **Test heterogeneity sensitivity:** Create synthetic tasks with controlled overlap. Measure SIFT-Masks performance as task overlap increases from 0% (fully distinct) to 100% (identical). Expect performance convergence toward Central FT at high overlap.