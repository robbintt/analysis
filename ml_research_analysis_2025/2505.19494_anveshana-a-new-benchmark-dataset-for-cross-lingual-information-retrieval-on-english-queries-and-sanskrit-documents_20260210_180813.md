---
ver: rpa2
title: 'Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval
  On English Queries and Sanskrit Documents'
arxiv_id: '2505.19494'
source_url: https://arxiv.org/abs/2505.19494
tags:
- retrieval
- documents
- sanskrit
- translation
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anveshana, the first benchmark dataset for
  cross-lingual information retrieval (CLIR) between English queries and Sanskrit
  documents. The dataset contains 3,400 query-document pairs from 334 Sanskrit texts,
  with queries generated from English translations.
---

# Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents

## Quick Facts
- arXiv ID: 2505.19494
- Source URL: https://arxiv.org/abs/2505.19494
- Authors: Manoj Balaji Jagadeeshan; Prince Raj; Pawan Goyal
- Reference count: 7
- Primary result: First benchmark dataset for English-to-Sanskrit CLIR with 3,400 query-document pairs

## Executive Summary
This paper introduces Anveshana, the first benchmark dataset for cross-lingual information retrieval (CLIR) between English queries and Sanskrit documents. The dataset contains 3,400 query-document pairs from 334 Sanskrit texts, with queries generated from English translations. The authors evaluated multiple CLIR frameworks: Query Translation (QT), Document Translation (DT), and Direct-Retrieve (DR). They tested various state-of-the-art models including BM25, REPLUG, mDPR, ColBERT, Contriever, and GPT-2 across these frameworks. Results show that the DT framework consistently outperformed others, with BM25 achieving 62.46% NDCG@10. The zero-shot monolingual-Contriever model also showed strong performance at 30.48% NDCG@10. The study addresses the challenge of accessing Sanskrit scriptures through English queries and provides a foundation for future research in CLIR for ancient languages. The dataset and models are publicly available.

## Method Summary
The paper introduces Anveshana, a benchmark dataset for cross-lingual information retrieval between English queries and Sanskrit documents. The dataset contains 3,400 query-document pairs from 334 Sanskrit texts (Srimadbhagavatam), split 90:10 into train (3,060) and test (340). The authors evaluate three CLIR frameworks: Query Translation (QT), Document Translation (DT), and Direct-Retrieve (DR). QT translates English queries to Sanskrit using Google Translate API, then applies retrieval models like BM25 or fine-tuned XLM-RoBERTa-base. DT translates Sanskrit documents to English, then applies monolingual retrieval models including BM25, Contriever, ColBERT, GPT-2, and REPLUG LSR. DR uses models like mDPR, multilingual-e5, and GPT-2 to embed queries and documents in a shared space for direct cross-lingual retrieval. Models are trained with contrastive learning and BCE loss, with REPLUG LSR using KL divergence. Negative sampling at 2:1 ratio creates non-relevant pairs for training. Evaluation uses NDCG, MAP, Recall, and Precision at k=1,3,5,10.

## Key Results
- DT framework with BM25 achieved 62.46% NDCG@10, significantly outperforming other approaches
- Zero-shot monolingual-Contriever achieved 30.48% NDCG@10, demonstrating effective transfer
- DR framework showed poor performance (3-10% NDCG@10), indicating alignment challenges
- QT framework achieved 2.95% NDCG@10, confirming translation direction matters

## Why This Works (Mechanism)

### Mechanism 1: Document Translation (DT) Framework Superiority
Translating documents to the query language before retrieval outperforms query translation and direct cross-lingual retrieval for English→Sanskrit CLIR. Sanskrit documents are translated to English via Google Translate API, then standard monolingual retrieval (BM25, ColBERT, Contriever) operates entirely in the query's language space. This approach assumes translation quality is sufficient to preserve document semantics for retrieval relevance matching, even with translation errors. Evidence shows DT methods outperform DR and QT in handling cross-lingual challenges of ancient texts, with BM25 achieving 62.46% NDCG@10 under DT framework.

### Mechanism 2: Zero-Shot Monolingual Embedding Transfer
Pre-trained English monolingual retrievers can achieve meaningful cross-lingual performance without fine-tuning when paired with document translation. Contriever's contrastive pre-training on English text creates semantic embeddings that generalize to translated content, bypassing need for Sanskrit-specific training data. This assumes the translation layer normalizes Sanskrit content into a semantic space the English-trained model can process effectively. The monolingual-Contriever eng model achieved 30.48% NDCG@10 in zero-shot, underscoring the potential of pre-trained models for swift deployment in diverse linguistic settings.

### Mechanism 3: Negative Sampling for Discrimination Training
Training retrieval models with strategically sampled non-relevant pairs improves their ability to distinguish relevant from irrelevant cross-lingual matches. A 2:1 negative-to-positive ratio creates harder discrimination boundaries; models learn to separate semantically similar but non-relevant documents from true matches via contrastive loss (BCE for binary classification). This approach involves generating non-relevant query-document pairs, training the model to distinguish between relevant and non-relevant matches.

## Foundational Learning

- **Concept: Cross-Lingular Information Retrieval (CLIR) Paradigms**
  - Why needed: The paper structures experiments around three fundamental CLIR approaches (QT, DT, DR); understanding their trade-offs is prerequisite to interpreting results.
  - Quick check: Given a low-resource language pair with poor translation quality, which CLIR framework would you expect to perform better and why?

- **Concept: Embedding Space Alignment in Multilingual Models**
  - Why needed: The Direct Retrieve framework depends on models (mDPR, multilingual-e5) creating aligned semantic spaces across languages; poor alignment explains low DR performance (3-10% NDCG).
  - Quick check: What properties must a multilingual embedding model have to enable direct cross-lingual retrieval without translation?

- **Concept: Retrieval Evaluation Metrics (NDCG, MAP, Recall, Precision @k)**
  - Why needed: The paper reports results across multiple metrics and cutoffs; understanding what each measures is essential for interpreting the 62.46% vs 30.48% vs 10.74% NDCG@10 comparisons.
  - Quick check: If a model has high Recall@10 but low Precision@10, what does that tell you about its ranking behavior?

## Architecture Onboarding

- **Component map:** Data Layer (3,400 pairs, 334 documents) -> Translation Layer (Google Translate API) -> Embedding Layer (XLM-RoBERTa, multilingual-e5, Contriever, ColBERT) -> Retrieval Layer (BM25, neural bi-encoders, REPLUG LSR) -> Index Layer (Faiss) -> Evaluation Layer (NDCG, MAP, Recall, Precision @ k)

- **Critical path:** 1. Framework selection (DT recommended) 2. Document preprocessing (preserve poetic structure, minimal English query preprocessing) 3. Translation (Sanskrit→English for DT; English→Sanskrit for QT) 4. Indexing (create Faiss index from document embeddings) 5. Retrieval (query encoding + similarity search) 6. Optional reranking (REPLUG LSR with LM supervision)

- **Design tradeoffs:** DT vs. DR: DT achieves 62.46% vs. DR's 10.74% NDCG@10, but DT requires full corpus translation upfront (compute + storage cost). BM25 vs. Neural: BM25 is computationally cheaper and surprisingly robust; neural models (ColBERT, Contriever) add semantic matching at embedding cost. Fine-tuning vs. Zero-shot: Fine-tuned Contriever (41.70% NDCG@10) outperforms zero-shot (30.48%), but requires labeled training data.

- **Failure signatures:** DT framework degrades: Check translation quality—systematic errors in philosophical/technical Sanskrit terms. DR framework near-random performance (<5% NDCG): Embedding alignment failure; model never saw sufficient Sanskrit during pre-training. QT framework extremely low (~2.95% NDCG): Query translation loses key terms; translated query doesn't match document vocabulary. High recall, low precision: Model retrieves broadly but ranks poorly—consider reranking with REPLUG LSR.

- **First 3 experiments:** 1. Establish DT+BM25 baseline: Translate Sanskrit corpus to English, run BM25, target ~60% NDCG@10. 2. Zero-shot probe: Run monolingual-Contriever on translated documents without fine-tuning; expect ~25-30% NDCG@10. 3. Fine-tuning with negative sampling: Train Contriever on query-document pairs with 2:1 negative ratio; target improvement toward ~40% NDCG@10.

## Open Questions the Paper Calls Out

### Open Question 1
Can Hierarchical Knowledge Enhancement (HKE) using multilingual knowledge graphs bridge the linguistic gap in English-to-Sanskrit retrieval? The authors state in the Conclusion and Future Work that they plan to "incorporate Hierarchical Knowledge Enhancement," leveraging knowledge graphs to enrich query representations and smooth over language discrepancies. The current study establishes a baseline using translation and embedding-based frameworks but does not integrate external knowledge structures to assist with semantic nuances. What evidence would resolve it: Experiments on the Anveshana dataset showing improved NDCG or MAP scores when a multilingual knowledge graph is used compared to the current state-of-the-art baselines.

### Open Question 2
Does restricting the translation vocabulary via Constraint Translation Candidates improve retrieval accuracy over standard neural query translation? The authors identify implementing Constraint Translation Candidates (Bi et al., 2020) as a future strategy to refine query translation by focusing "target vocabulary on key terms derived from the search index." The current Query Translation (QT) framework relies on general-purpose Google Translate, which may introduce irrelevant terms; the specific impact of index-constrained vocabulary on this low-resource pair is untested. What evidence would resolve it: Comparative evaluation of the QT framework showing higher precision/recall when using constrained translation versus the reported Google Translate baseline.

### Open Question 3
To what extent does the poetic structure of Sanskrit documents hinder the performance of Direct-Retrieve (DR) neural models? Section 9 notes a lack of exploration into "varying data formats and linguistic styles," while Section 3.2 notes the dataset maintains poetic structure to preserve nuances. The DR framework performed poorly (max ~10% NDCG) compared to Document Translation. It is unclear if the low DR performance is due to the cross-lingual embedding alignment or the models' inability to handle the complex syntax of Sanskrit poetry effectively. What evidence would resolve it: An ablation study comparing retrieval performance on poetic Sanskrit texts versus prose-converted versions (using tools like those mentioned in Related Work) within the DR framework.

## Limitations
- Translation quality remains unquantified; the paper assumes Google Translate API produces sufficient semantic preservation for retrieval but provides no human evaluation or automatic metrics to validate this critical assumption.
- The DT framework's performance advantage (62.46% NDCG@10) depends entirely on translation quality, creating a hidden dependency that isn't acknowledged as a limitation.
- Limited ablation studies prevent understanding which components drive performance—no analysis of translation quality impact, negative sampling effectiveness, or model architecture contributions.

## Confidence
- **High Confidence:** The dataset creation methodology and experimental setup are clearly described and reproducible. The observation that DT framework outperforms QT and DR is robust to the stated methodology.
- **Medium Confidence:** The relative performance rankings between frameworks and models are likely correct, but absolute metric values depend on uncontrolled translation quality factors.
- **Low Confidence:** Claims about translation sufficiency and zero-shot model generalization lack validation; the paper doesn't address whether translation errors systematically bias results.

## Next Checks
1. **Translation Quality Validation:** Measure translation quality using BLEU, TER, or human evaluation on a subset of Sanskrit documents to quantify the semantic preservation assumption underlying the DT framework's success.
2. **Translation Ablation Study:** Run experiments with controlled translation degradation (e.g., synthetic noise, paraphrasing) to measure sensitivity of retrieval performance to translation quality variations.
3. **DR Framework Stress Test:** Evaluate mDPR and multilingual-e5 models on controlled cross-lingual retrieval tasks where alignment is known to work (e.g., English-French) to determine whether the near-zero performance is model-specific or indicates fundamental alignment failure.