---
ver: rpa2
title: 'Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents'
arxiv_id: '2505.03947'
source_url: https://arxiv.org/abs/2505.03947
tags:
- size
- reasoning
- frog
- game
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores zero-shot, in-context, and sample-efficient
  game playing with LLMs on the challenging Atari game Frogger. Reasoning models like
  o3-mini and QwQ-32B can play Frogger in a zero-shot setting with episodic rewards
  up to 32 and 17 points respectively, demonstrating spatial reasoning and in-context
  learning capabilities.
---

# Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents

## Quick Facts
- arXiv ID: 2505.03947
- Source URL: https://arxiv.org/abs/2505.03947
- Reference count: 40
- Primary result: Zero-shot, in-context, and sample-efficient game playing with LLMs on Atari's Frogger

## Executive Summary
This paper investigates the use of large language models (LLMs) to play the Atari game Frogger without prior training. It explores three paradigms: zero-shot reasoning with models like o3-mini and QwQ-32B, in-context learning with past demonstrations, and integration of LLM-generated data into traditional DQN training. The work demonstrates that LLMs can achieve non-trivial episodic rewards in Frogger through spatial reasoning and in-context adaptation, and that augmenting DQN with LLM demonstrations improves sample efficiency by 35.3%. A depth-first search baseline reveals the computational infeasibility of brute-force solutions for this task.

## Method Summary
The study evaluates zero-shot gameplay by prompting reasoning LLMs (o3-mini, QwQ-32B) to play Frogger using only in-context information, without fine-tuning. In-context learning is tested by providing past game states and rewards as demonstrations. To improve sample efficiency, LLM-generated game traces are used as demonstrations in DQN training. A DFS baseline is implemented to measure computational complexity, and results are compared against standard DQN and pure LLM approaches.

## Key Results
- Zero-shot reasoning models achieve episodic rewards up to 32 (o3-mini) and 17 (QwQ-32B) points in Frogger
- Including past rewards significantly boosts performance under high reasoning settings
- Integrating LLM-generated demonstrations into DQN training yields 35.3% higher episodic rewards within the same training budget
- DFS baseline is computationally prohibitive, highlighting the need for efficient search strategies

## Why This Works (Mechanism)
LLMs leverage spatial reasoning and in-context learning to interpret game states and select actions without explicit training. Their ability to generalize from demonstrations allows them to adapt to Frogger's dynamics quickly. When integrated into DQN, LLM-generated demonstrations provide high-quality, diverse training signals that accelerate policy learning and improve sample efficiency.

## Foundational Learning
- **Zero-shot reasoning**: Ability of LLMs to solve tasks without prior task-specific training, essential for evaluating intrinsic generalization
  - Why needed: Tests LLM's raw spatial reasoning and decision-making capabilities
  - Quick check: Compare episodic rewards of zero-shot vs. fine-tuned models on same task
- **In-context learning**: Using past demonstrations as prompts to guide LLM predictions
  - Why needed: Allows adaptation to new tasks without parameter updates
  - Quick check: Measure performance change when including/excluding past rewards
- **Reinforcement learning (DQN)**: Deep Q-learning framework for training agents via reward signals
  - Why needed: Standard baseline for comparing sample efficiency gains
  - Quick check: Evaluate episodic rewards before and after LLM demonstration integration
- **Monte Carlo Tree Search (MCTS)**: Search algorithm using simulations to guide decisions
  - Why needed: Scalable alternative to DFS for game solving
  - Quick check: Compare MCTS-guided LLM vs. pure LLM in computational cost and reward

## Architecture Onboarding

**Component Map**: LLM (zero-shot/in-context) -> Game Environment (Frogger) -> Reward Signal -> DQN (with LLM demonstrations)

**Critical Path**: LLM inference (reasoning) -> Action selection (in-context) -> Environment feedback (reward) -> DQN update (with LLM data)

**Design Tradeoffs**: Zero-shot vs. in-context learning balances generalization and adaptation; DQN+LLM improves sample efficiency but may introduce inference overhead

**Failure Signatures**: Zero-shot performance plateaus due to limited spatial reasoning; in-context learning may overfit to specific demonstrations; DQN+LLM may suffer from high inference costs

**First 3 Experiments**:
1. Compare zero-shot vs. in-context episodic rewards on Frogger
2. Measure DQN+LLM sample efficiency gain vs. standard DQN
3. Evaluate computational cost of DFS vs. LLM-guided search

## Open Questions the Paper Calls Out
None explicitly stated.

## Limitations
- Evaluation limited to a single Atari game (Frogger) and three LLM-based reasoning models, restricting generalizability
- Episodic rewards plateau around 32 points, suggesting ceilings in current LLM spatial reasoning capabilities
- No analysis of inference cost vs. performance gains in DQN+LLM setups
- Lack of scalable baselines beyond computationally infeasible DFS

## Confidence
- Confidence in zero-shot and in-context learning claims: **Medium** - results are promising but based on limited model diversity and a single game
- Confidence in sample efficiency gains from LLM demonstrations: **Medium** - performance improvement is clear, but cost-benefit and robustness analyses are missing
- Confidence in the DFS baseline's practical implications: **Low** - the baseline is infeasible for real use, but no scalable alternatives are explored

## Next Checks
1. Test zero-shot and in-context learning on a broader set of Atari games to assess generalizability of spatial reasoning and in-context learning
2. Perform cost-benefit analysis comparing inference overhead of reasoning models to performance gains in DQN+LLM setups
3. Implement and evaluate a Monte Carlo Tree Search (MCTS) baseline guided by LLM predictions to compare scalability and performance against DFS and pure LLM approaches