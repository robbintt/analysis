---
ver: rpa2
title: What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic
  Study
arxiv_id: '2601.14888'
source_url: https://arxiv.org/abs/2601.14888
tags:
- training
- arxiv
- reasoning
- quantization
- gptq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies quantization-aware training (QAT) for reasoning
  large language models (LLMs) under low-bit quantization. It systematically investigates
  training objectives, initialization strategies, RL integration, and data alignment.
---

# What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study

## Quick Facts
- arXiv ID: 2601.14888
- Source URL: https://arxiv.org/abs/2601.14888
- Reference count: 19
- Primary result: KD is optimal for low-bit QAT; PTQ initialization accelerates; RL requires KD cold-start; domain alignment helps; Reasoning-QAT outperforms PTQ/QAT baselines.

## Executive Summary
This paper systematically investigates quantization-aware training (QAT) for reasoning large language models under low-bit quantization (2-3 bits). Through controlled experiments on R1-Qwen and Qwen3 models, it identifies knowledge distillation as the optimal training objective for recovering reasoning performance, with PTQ initialization providing faster convergence and reinforcement learning requiring a KD cold-start to succeed. The proposed Reasoning-QAT workflow combines these insights and achieves significant accuracy improvements over state-of-the-art post-training quantization methods, particularly on mathematical reasoning benchmarks.

## Method Summary
The method follows a three-stage Reasoning-QAT workflow: (1) PTQ initialization using GPTQ with Hessian-based weight compensation on calibration data, (2) KD recovery via KL divergence minimization to full-precision teacher on OpenR1-Math dataset, and (3) GRPO-based RL with correctness rewards applied only after KD convergence. The approach targets weight-only quantization (W3G128/W2G128) and weight-activation quantization (W4A4KV4), using asymmetric quantization with group size 128 and excluding embeddings/lm_head from quantization. Training employs cosine learning rate decay with warmup, batch sizes of 32-64, and evaluates on multiple reasoning benchmarks.

## Key Results
- KD outperforms SFT as QAT objective, achieving smaller accuracy drops across both SFT and RL-trained base models under 3-bit quantization.
- GPTQ initialization accelerates convergence and improves final accuracy compared to RTN initialization, starting from higher initial test accuracy and lower loss.
- RL requires KD cold-start; direct RL on heavily quantized models collapses completely, while KD cold-start improves accuracy by ~46%.
- Aligning PTQ calibration domain with QAT training domain accelerates convergence and boosts accuracy, preventing harmful readjustment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation (KD) is the preferred QAT objective for low-bit reasoning models over supervised fine-tuning (SFT).
- Mechanism: KD aligns output distributions between the quantized student and full-precision teacher via KL divergence, preserving uncertainty structure and providing smoother gradient signals. Hard-label SFT discards distributional information, making recovery under quantization noise harder.
- Core assumption: The teacher's output distribution contains recoverable signal that survives quantization-induced perturbation.
- Evidence anchors:
  - [abstract] "Knowledge distillation (KD) is the optimal objective for low-bit QAT, providing stable recovery across both supervised fine-tuning (SFT) and reinforcement learning (RL) paradigms."
  - [section 3.2] "KD yields smaller accuracy drops than SFT on both an SFT-trained model (R1-Qwen-1.5B) and an RL-trained model (Qwen3-4B) under W3G128."
  - [corpus] Weak direct evidence on KD vs. SFT for reasoning; neighbor papers (DL-QAT, BitDistiller) use distillation but do not systematically compare objectives.
- Break condition: If the teacher model itself has degraded reasoning capability or if the quantization budget drops below a representation threshold where distribution matching becomes impossible.

### Mechanism 2
- Claim: PTQ initialization (e.g., GPTQ) provides a stronger starting point for QAT than random or round-to-nearest (RTN) initialization.
- Mechanism: PTQ methods like GPTQ perform Hessian-based weight compensation during quantization, reducing initial quantization error. This positions QAT closer to a viable solution, reducing the optimization burden and accelerating convergence.
- Core assumption: The PTQ calibration set is representative of the target distribution; Hessian-based compensation generalizes beyond calibration data.
- Evidence anchors:
  - [abstract] "PTQ initialization improves efficiency and accuracy."
  - [section 3.3] "GPTQ+KD starts from a higher starting point (higher test accuracy and lower loss)... exhibits a faster convergence rate within the same number of training steps."
  - [corpus] PTQTP (arXiv:2509.16989) corroborates that PTQ at ultra-low bits remains challenging without training-based recovery.
- Break condition: If PTQ calibration data is severely mismatched to the target domain, initialization may anchor to a poor local basin.

### Mechanism 3
- Claim: Reinforcement learning requires a KD cold-start; direct RL on heavily quantized models collapses.
- Mechanism: RTN-quantized models suffer severe sampling degradationâ€”they fail to generate valid reasoning trajectories, making reward signals sparse or uninformative. KD recovers sufficient sampling capability to provide exploitable reward density for RL (GRPO).
- Core assumption: Reward signals are contingent on the model producing syntactically and semantically valid outputs; quantization noise destroys this capability below a threshold.
- Evidence anchors:
  - [abstract] "RL yields further gains only after a KD cold start."
  - [section 3.4] "Zero-RL QAT collapses completely, whereas the cold-start setting improves accuracy by approximately 46%."
  - [corpus] "The Impact of Quantization on Large Reasoning Model Reinforcement Learning" (arXiv:2511.15694) directly studies quantization-RL interaction but does not establish the cold-start requirement.
- Break condition: If reward design is too sparse or if the KD recovery is insufficient to produce rewardable outputs, RL will still fail.

### Mechanism 4
- Claim: Aligning PTQ calibration domain with QAT training domain accelerates convergence and improves final accuracy.
- Mechanism: Domain alignment prevents harmful readjustment where the model must unlearn PTQ-calibrated weights to adapt to a mismatched training distribution. Consistent domains allow incremental refinement rather than destructive relearning.
- Core assumption: Calibration data and training data share sufficient statistical structure that quantization error patterns transfer.
- Evidence anchors:
  - [abstract] "Aligning the PTQ calibration and QAT training domains accelerates convergence and boosts accuracy."
  - [section 3.5] "When calibration and training domains match, the curve converges much earlier... mismatched combinations stabilize slowly."
  - [corpus] No direct corpus evidence on domain alignment for QAT; this is a novel contribution.
- Break condition: If the target deployment domain differs from both calibration and training domains, generalization may still suffer.

## Foundational Learning

- **Quantization fundamentals (PTQ vs. QAT, weight-only vs. weight-activation)**
  - Why needed here: The paper operates at 3-bit and 2-bit weight-only quantization (W3G128, W2G128). Understanding the quantization equation (Eq. 1), scaling factors, and the straight-through estimator (STE) is prerequisite.
  - Quick check question: Given a weight matrix W, can you compute the quantized representation W_int for 3-bit symmetric quantization and explain why STE is needed for backpropagation?

- **Knowledge distillation mechanics (KL divergence, teacher-student)**
  - Why needed here: KD is the core recovery objective. You must understand why soft targets preserve inter-class relationships better than hard labels.
  - Quick check question: Why would minimizing KL divergence between teacher and student distributions be more robust to quantization noise than cross-entropy on ground-truth labels?

- **Reinforcement learning for LLMs (GRPO, policy gradients, reward shaping)**
  - Why needed here: The cold-start RL phase uses GRPO. Understanding why RL requires viable sampling is critical to grasping the failure mode.
  - Quick check question: What happens to policy gradient estimates if the policy cannot generate valid outputs that receive positive reward?

## Architecture Onboarding

- **Component map:**
  - PTQ Initialization (GPTQ) -> KD Recovery -> Cold-Start RL (GRPO)

- **Critical path:**
  1. Calibration data selection (NuminaMath-1.5 aligned with OpenR1-Math).
  2. PTQ initialization quality (GPTQ > RTN, but FlatQuant required for W4A4KV4).
  3. KD training to viable policy (must reach sufficient sampling capability).
  4. RL only after KD convergence; skip if KD recovery is incomplete.

- **Design tradeoffs:**
  - PTQ method: GPTQ is efficient but may retain outliers; FlatQuant adds transformation matrices for activation quantization but increases complexity.
  - Training objective: KD is slower per-step than SFT but converges faster in total steps.
  - Bit-width: 3-bit is recoverable with moderate effort; 2-bit requires full workflow and still leaves 20%+ gap to BF16 on some benchmarks.

- **Failure signatures:**
  - Zero-RL QAT: Complete collapse (AIME-120: 0%, MATH-500: ~15%) due to invalid sampling.
  - Domain mismatch: Sharp accuracy drop during early training, slow recovery, failure to exceed PTQ baseline.
  - W4A4KV4 without outlier suppression (RTN init): Training divergence, near-zero accuracy across all benchmarks.

- **First 3 experiments:**
  1. Reproduce Table 1 on R1-Qwen-1.5B (W3G128): Compare SFT vs. KD recovery. Validate that KD achieves <10% average drop while SFT exceeds 10%.
  2. Ablate PTQ initialization: Run RTN+KD vs. GPTQ+KD on MATH-500 for 2k steps. Plot loss and accuracy curves to confirm faster convergence with GPTQ.
  3. Test cold-start requirement: Apply GRPO directly to RTN-quantized model (zero-RL) and confirm collapse; then apply GRPO after KD and verify recovery (target: 46%+ improvement over zero-RL baseline).

## Open Questions the Paper Calls Out

- **Cross-domain generalization to other reasoning tasks (e.g., coding and science) remains limited, particularly for smaller models under extreme low-bit settings.** The study primarily uses OpenR1-Math dataset and evaluates mostly on mathematical benchmarks, leaving unclear whether findings transfer to non-mathematical reasoning domains.

## Limitations

- Limited evaluation on non-mathematical reasoning domains (coding, science) despite testing on some benchmarks.
- Underspecified GRPO reward formulation, critical for understanding RL recovery mechanism.
- Does not identify systematic bit-width threshold below which KD recovery becomes insufficient.

## Confidence

**High Confidence:** Knowledge distillation is the optimal QAT objective (supported by direct ablation across two model scales and training paradigms).

**Medium Confidence:** PTQ initialization accelerates convergence (supported by empirical convergence curves, though Hessian generalization beyond calibration data remains unverified).

**Medium Confidence:** RL requires KD cold-start (supported by dramatic failure of zero-RL, though reward formulation details are underspecified).

**Low Confidence:** Domain alignment mechanism (lacks ablation studies on calibration data diversity and transferability).

## Next Checks

1. **Reward formulation ablation:** Implement and compare three reward schemes for GRPO: (a) binary correctness, (b) step-level partial credit for intermediate reasoning, (c) formatted response bonus. Measure impact on RL recovery efficiency and final accuracy.

2. **Calibration domain stress test:** Train Reasoning-QAT with PTQ initialization using three calibration sets: (a) aligned (NuminaMath-1.5), (b) generic (Wikitext2), (c) mismatched (different reasoning corpus). Evaluate on both in-domain (MATH-500) and out-of-domain (LiveCodeBench) benchmarks to quantify domain alignment benefits and transferability limits.

3. **Bit-width threshold exploration:** Systematically evaluate Reasoning-QAT workflow at W4G128, W3G128, W2G128, W2G4, and W1G2. Measure recovery ratio (quantized accuracy / full-precision accuracy) to identify the minimum bit-width where KD recovery remains effective, and determine whether alternative objectives (e.g., SFT with temperature scaling) become necessary below this threshold.