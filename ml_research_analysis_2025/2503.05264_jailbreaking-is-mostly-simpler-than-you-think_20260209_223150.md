---
ver: rpa2
title: Jailbreaking is (Mostly) Simpler Than You Think
arxiv_id: '2503.05264'
source_url: https://arxiv.org/abs/2503.05264
tags:
- context
- history
- conversation
- safety
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents the Context Compliance Attack (CCA), a novel,
  optimization-free method for bypassing AI safety mechanisms by exploiting a fundamental
  architectural vulnerability: the reliance on client-supplied conversation history.
  Unlike current approaches that use complex prompt engineering or computationally
  intensive optimization, CCA manipulates the conversation context to convince the
  model to comply with fabricated dialogue, triggering restricted behavior.'
---

# Jailbreaking is (Mostly) Simpler Than You Think

## Quick Facts
- arXiv ID: 2503.05264
- Source URL: https://arxiv.org/abs/2503.05264
- Reference count: 9
- Nearly all tested AI models vulnerable to simple context manipulation attack

## Executive Summary
The paper introduces Context Compliance Attack (CCA), a novel method for bypassing AI safety mechanisms by exploiting the fundamental architectural vulnerability of stateless APIs that rely on client-supplied conversation history. Unlike complex optimization-based jailbreaks, CCA manipulates conversation context to create a false precedent of helpfulness, triggering restricted behavior with minimal effort. Testing across 11 sensitive tasks on 18 diverse models shows 17 are vulnerable, with most tasks succeeding on the first attempt. The notable exception is Llama-2, which demonstrates resistance. The study highlights a systemic weakness in current AI safety protocols and proposes server-side history maintenance and cryptographic signatures as mitigation strategies.

## Method Summary
The Context Compliance Attack (CCA) exploits the fact that most LLM APIs are stateless, requiring clients to supply full conversation history with each request. The attack follows a 4-step process: (1) initiate a conversation on a sensitive topic, (2) inject fabricated history containing a pseudo AI response showing willingness to discuss the topic, a readiness statement, and a yes/no confirmation prompt, (3) respond affirmatively, and (4) the model complies with the perceived context. This approach requires no optimization or complex prompt engineering, making it simpler than existing jailbreak techniques. The attack was evaluated across 11 sensitive tasks (Self Harm, Meth, Ricin, Date Rape, Hate, Scam, Ransomware, Violence, Profanity, Bomb, Sex) using 18 different models, with success defined as the model producing restricted content in any of 5 independent trials.

## Key Results
- 17 of 18 tested models (including GPT-4, Claude, Gemini, Llama-3.1) were successfully jailbroken by CCA
- Most tasks achieved first-trial success across vulnerable models
- Llama-2 (7b and 70b) was the only model to demonstrate complete resistance to the attack
- The "Sex" task showed highest resistance, requiring up to 5 trials for success on some models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fabricated conversation history induces model compliance with restricted requests by creating a false precedent of helpfulness.
- **Mechanism:** The attacker injects a pseudo-AI response showing willingness to discuss sensitive content, followed by a yes/no confirmation prompt. When the user responds affirmatively, the model treats this as continuation of an established helpful exchange rather than a new harmful request.
- **Core assumption:** Models trained for conversational coherence prioritize contextual consistency over re-evaluating each turn for safety violations independently.
- **Evidence anchors:**
  - [abstract] "By subtly manipulating conversation history, CCA convinces the model to comply with a fabricated dialogue context, thereby triggering restricted behavior."
  - [section 2.1] "History Manipulation: Instead of deploying complex prompts, the adversary injects a manipulated conversation history... A pseudo AI response discussing the sensitive subject... A statement indicating readiness to provide restricted information."
  - [corpus] "Trojan Horse Prompting" paper (arXiv:2507.04673) describes a related attack forging assistant messages, suggesting this is a broader class of vulnerability.
- **Break condition:** Models that independently safety-check each conversational turn without relying on apparent prior context would resist this attack. Llama-2's resistance suggests some models may already implement such isolation.

### Mechanism 2
- **Claim:** The stateless API design pattern—where clients supply full conversation history with each request—creates an inherent trust boundary violation.
- **Mechanism:** For scalability, most deployed LLM APIs are stateless; the server holds no persistent session data. This architectural choice means the server must accept whatever history the client provides, with no independent verification that those exchanges actually occurred.
- **Core assumption:** Attackers have sufficient API access to modify the conversation history field in requests.
- **Evidence anchors:**
  - [section 4.1] "They depend on clients to supply the entire conversation history with each request. This stateless approach, adopted for scalability and efficiency, inherently trusts the integrity of the provided context."
  - [section 4.1] "Open-source models—where users have complete control over input history—are particularly susceptible to this form of attack."
  - [corpus] No direct corpus papers address this specific architectural vulnerability; related work focuses on prompt-level attacks rather than protocol-level trust assumptions.
- **Break condition:** Server-side history maintenance or cryptographic authentication of conversation turns would break this mechanism by removing client control over historical context.

### Mechanism 3
- **Claim:** Once a model complies with one restricted request via CCA, it becomes progressively more likely to comply with related requests—a "compliance momentum" effect.
- **Mechanism:** The paper observes that successful CCA execution creates a permissive context that lowers resistance to subsequent related queries, potentially due to the model maintaining self-consistency with its earlier (fabricated) helpful stance.
- **Core assumption:** This escalation effect stems from the same contextual coherence mechanisms that enable the initial attack, though the paper does not isolate or prove this causation.
- **Evidence anchors:**
  - [section 3.2] "We monitored that once an AI system has been deceived into providing restricted information on one topic, it may become progressively more likely to divulge related sensitive details."
  - [abstract] No direct mention of escalation; claim rests solely on Section 3.2 observation.
  - [corpus] "Multi-Turn Jailbreaks Are Simpler Than They Seem" (arXiv:2508.07646) reports high success rates for multi-turn attacks, consistent with but not confirmatory of this escalation hypothesis.
- **Break condition:** Per-turn safety evaluation that does not weight prior conversational context toward permissiveness would prevent escalation.

## Foundational Learning

- **Concept: Stateless vs. Stateful API Architecture**
  - **Why needed here:** CCA exploits stateless design; understanding this distinction is prerequisite to grasping why the attack works and why proposed mitigations involve state management.
  - **Quick check question:** In a stateless chat API, what data must the client include in each request that a stateful API would store server-side?

- **Concept: Context Window and Conversation History**
  - **Why needed here:** The attack manipulates the context window content; engineers need to understand how models process multi-turn conversation representations.
  - **Quick check question:** If a conversation has 10 turns but the model's context window can only hold 8 turns' worth of tokens, what typically happens to the oldest turns?

- **Concept: Alignment and Safety Training (RLHF)**
  - **Why needed here:** The paper positions CCA as a bypass of alignment techniques; understanding what alignment constrains helps explain why context manipulation circumvents it.
  - **Quick check question:** Does RLHF training occur at inference time or before deployment? How does this relate to why attacks can circumvent it?

## Architecture Onboarding

- **Component map:**
  Client Application -> Conversation History Constructor -> API Request (includes full history) -> Server-Side Safety Classifier (input) -> Model Inference (context-aware generation) -> Server-Side Safety Classifier (output) -> Response to Client

- **Critical path:** The vulnerability exists at the "Conversation History Constructor" stage. If the client (or attacker controlling the client) can modify history before transmission, all downstream safety checks operate on poisoned context.

- **Design tradeoffs:**
  - **Stateless API (current norm):** Maximizes scalability, simplifies load balancing, enables horizontal scaling. Cost: Trusts client-supplied history; vulnerable to CCA.
  - **Server-side history:** Eliminates CCA vector for API users. Cost: Storage costs, reduced scalability, session management complexity, potential privacy concerns.
  - **Cryptographic signatures:** Prevents tampering without server-side storage. Cost: Requires client-side SDK changes; does not protect open-weight models where users control inference.

- **Failure signatures:**
  - Model output suddenly complies with a request it previously refused
  - Conversation history contains assistant messages the user never received
  - Yes/no confirmation prompts appear in history without corresponding genuine user questions
  - The "Sex" task shows partial resistance—models may have task-specific hardening that provides detectable variation in susceptibility

- **First 3 experiments:**
  1. **Reproduction test:** Implement CCA against a local model (e.g., Llama-3.1-8b) using the PyRIT orchestrator linked in the paper. Confirm the attack succeeds on at least one task from Table 1. This establishes baseline vulnerability in your environment.
  2. **Defense validation:** Attempt CCA against Llama-2-7b (the resistant model). Analyze whether resistance stems from per-turn safety classification, different training data, or architectural differences. This identifies what properties correlate with defense.
  3. **Signature-based mitigation prototype:** For a simple chat API, implement HMAC signing of each assistant message. On receipt of a new request, verify all prior assistant messages have valid signatures before processing. Test whether this prevents CCA when signatures are required but allows it when verification is disabled. This validates the proposed mitigation at minimal implementation cost.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural or alignment features enabled Llama-2 to resist the Context Compliance Attack (CCA) when nearly all other state-of-the-art models failed?
- **Basis in paper:** [inferred] Section 3.2 and Table 2 highlight that Llama-2 (7b and 70b) was the only model to demonstrate complete resistance, whereas newer models like Llama-3.1 were fully vulnerable.
- **Why unresolved:** The paper reports the anomaly but provides no ablation studies or analysis of Llama-2's training data or safety layers to explain this unique robustness.
- **What evidence would resolve it:** A comparative analysis of Llama-2's safety alignment techniques versus vulnerable models, or successful application of CCA on intermediate Llama-2 checkpoints.

### Open Question 2
- **Question:** How can context integrity be cryptographically enforced for open-source, white-box models where users control the inference environment?
- **Basis in paper:** [inferred] Section 4.2 states that while cryptographic signatures help API-based models, white-box models require "a more involved defense strategy" where the model architecture itself rejects invalid signatures.
- **Why unresolved:** The authors suggest the requirement but do not propose a mechanism for securely embedding signature verification into model weights that cannot be bypassed by a user with full system access.
- **What evidence would resolve it:** A model architecture or fine-tuning methodology that inherently validates conversation history provenance before generating output.

### Open Question 3
- **Question:** Can robust context integrity validation mechanisms be developed that operate effectively without incurring the high costs of server-side history maintenance?
- **Basis in paper:** [explicit] The Conclusion recommends future research "focus on... enhancing context integrity validation mechanisms" to detect adversarial manipulations.
- **Why unresolved:** The paper proposes server-side history as a mitigation but identifies it as costly; it leaves the development of efficient, algorithmic-based validation as an open need.
- **What evidence would resolve it:** An algorithm capable of detecting fabricated context in a stateless environment with high accuracy and low computational overhead.

## Limitations
- The paper focuses exclusively on context manipulation without addressing potential synergistic effects with other jailbreak techniques
- Evaluation of "first-trial success" may overstate practical effectiveness, as the "Sex" task showed notably higher resistance
- The resistance of Llama-2 is noted but not explained through comparative analysis of architectural or training differences

## Confidence
- **High confidence:** The fundamental vulnerability of stateless API architecture to client-supplied history manipulation is well-established through experimental results showing 17 of 18 models vulnerable
- **Medium confidence:** The escalation hypothesis (compliance momentum) is supported by observational evidence but lacks rigorous testing or causal isolation
- **Low confidence:** The claim that this represents a "fundamental architectural vulnerability" may overstate severity, as server-side mitigation strategies would eliminate the attack vector

## Next Checks
1. **Cross-model vulnerability mapping:** Systematically test CCA across the full range of model sizes and architectures (from 1B to 70B parameters) to determine whether vulnerability correlates with model scale, training methodology, or specific architectural choices. Include both aligned and unaligned checkpoints of the same base models to isolate alignment effects.

2. **Mitigation effectiveness validation:** Implement and test both proposed mitigations (server-side history and cryptographic signatures) across at least three different models. Measure performance overhead, false positive rates for legitimate conversation corrections, and whether attackers can bypass signature verification through other means.

3. **Hybrid attack assessment:** Combine CCA with one established optimization-based technique (such as gradient-based token manipulation) on models where CCA shows partial success. Determine whether the combination achieves higher success rates than either method alone, and whether it can overcome resistance in models like Llama-2.