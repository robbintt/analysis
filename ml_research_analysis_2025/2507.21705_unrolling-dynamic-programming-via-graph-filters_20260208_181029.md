---
ver: rpa2
title: Unrolling Dynamic Programming via Graph Filters
arxiv_id: '2507.21705'
source_url: https://arxiv.org/abs/2507.21705
tags:
- policy
- graph
- iteration
- bellnet
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BellNet, an unrolled neural architecture
  for solving Markov decision processes (MDPs) that draws on graph signal processing
  (GSP) to reduce computational complexity. By interpreting the transition probability
  matrix as the adjacency of a weighted digraph, the authors show that policy iteration
  can be expressed as a cascade of nonlinear graph filters.
---

# Unrolling Dynamic Programming via Graph Filters

## Quick Facts
- **arXiv ID:** 2507.21705
- **Source URL:** https://arxiv.org/abs/2507.21705
- **Reference count:** 40
- **Primary result:** BellNet reduces Bellman error in fewer iterations than classical policy iteration by unrolling policy iteration into a cascade of learnable graph filters.

## Executive Summary
This paper introduces BellNet, a neural architecture that unrolls policy iteration for solving Markov decision processes (MDPs) by interpreting it as a cascade of nonlinear graph filters. By viewing the transition probability matrix as a weighted digraph adjacency, each unrolled iteration becomes a layer applying learned filter coefficients to rewards and value functions. This enables BellNet to minimize Bellman error from random initializations with fewer inference steps than classical methods, and demonstrates transferability across similar environments without retraining. Experiments on grid-worlds show BellNet converges faster and more stably than policy iteration, especially with weight sharing across layers.

## Method Summary
BellNet maps policy iteration to a layered architecture where each layer applies a K-th order graph filter with learnable coefficients to the transition matrix, rewards, and current value function, followed by softmax-based policy improvement. The model is trained to minimize Bellman error via gradient descent on the full transition model (no data sampling required). Filter coefficients can be shared across layers to reduce parameters and variance, enabling inference beyond training depth. Evaluation uses normalized error against the optimal value function computed via classical policy iteration.

## Key Results
- BellNet converges to optimal policies in fewer iterations than classical policy iteration on grid-world tasks.
- Weight sharing across layers reduces variance and improves stability compared to per-layer distinct coefficients.
- The architecture transfers to structurally similar but unseen MDPs without retraining, maintaining low Bellman error.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Policy evaluation can be reformulated as applying a graph filter to the reward signal, where the transition matrix serves as a graph adjacency.
- **Mechanism:** The Bellman equation recursion q(k+1) = r + γP_π q(k) expands into a polynomial of P_π. This polynomial is exactly a graph filter H = Σ_j γ^j (P_π)^j operating on r. The discount factor powers act as filter coefficients weighting contributions from j-hop neighborhoods in the state-action graph.
- **Core assumption:** The transition probability matrix P_π (policy-conditioned) faithfully represents a weighted digraph structure, and value functions diffuse across this graph according to Markovian dynamics.
- **Evidence anchors:**
  - [abstract]: "Viewing the transition probability matrix of the MDP as the adjacency of a weighted directed graph... interpret (and compactly re-parameterize) BellNet as a cascade of nonlinear graph filters."
  - [Section III, Policy evaluation]: "The latter characterization follows since H(k) is a polynomial of P_π... From this viewpoint, the powers of the discount factor γ act as the filter coefficients."
  - [corpus]: Weak direct support—neighbor papers discuss Bellman equations and DP but not the graph filter reformulation specifically.
- **Break condition:** If the MDP lacks exploitable graph structure (e.g., near-dense transitions with weak locality), the filter's K-hop aggregation provides limited compression, and higher filter orders become necessary.

### Mechanism 2
- **Claim:** Unrolling policy iteration into L layers with learnable filter coefficients accelerates convergence relative to classical fixed-γ iteration.
- **Mechanism:** Each unrolled layer implements: (1) a graph filter with learned coefficients h^(l) applied to rewards and prior value estimates, and (2) a differentiable softmax-based policy improvement. Learning optimizes h^(l) to minimize Bellman error, effectively learning an accelerated fixed-point iteration schedule rather than using fixed γ^j weights.
- **Core assumption:** The optimal value function lies within the representational capacity of a K-th order graph filter cascade, and gradient descent on Bellman error yields coefficients that generalize.
- **Evidence anchors:**
  - [abstract]: "maps each unrolled iteration to a layer with learnable filter coefficients, allowing the model to minimize the Bellman error from random value function initializations."
  - [Section IV]: "BellNet... implements {q̂, Π̂} := Φ(q̄; H)... layer-wise updates: q^(l+1) = Σ_j h_j^(l) (P_π^(l))^j r + h_{K+1}^(l) (P_π^(l))^{K+1} q^(l)"
  - [corpus]: Related work on Bellman residual minimization exists (e.g., "Analysis of Control Bellman Residual Minimization for Markov Decision Problem"), but does not address unrolling or graph filters.
- **Break condition:** If L is too small or K insufficient for the problem's effective horizon, the truncated cascade cannot approximate the fixed point, and Bellman error remains high.

### Mechanism 3
- **Claim:** Weight sharing across layers reduces parameter count and variance while enabling inference beyond training depth.
- **Mechanism:** Sharing h^(l) = h across layers means the same graph filter block is reused iteratively. This exploits the observation that policy iteration applies a structurally similar operation each step. During inference, the shared block can be applied more times than seen during training, trading compute for precision.
- **Core assumption:** The underlying DP iteration is approximately stationary across steps—i.e., the same filter form remains useful as the policy evolves.
- **Evidence anchors:**
  - [Section IV, Transferability]: "weight sharing admittedly reduces expressiveness, it markedly decreases the number of learnable parameters... allows the same block to be reused as many times as desired during inference."
  - [Section V, Test case 1]: "the weight sharing strategy leads to better performance with lower variance, whereas distinct filter coefficients results in more unstable behavior."
  - [corpus]: No direct corpus evidence on weight sharing in this specific context.
- **Break condition:** If policy evolution requires qualitatively different filtering behavior at different stages (e.g., early exploration vs. late refinement), shared weights may underfit relative to per-layer weights.

## Foundational Learning

- **Concept: Markov Decision Processes and Bellman Equations**
  - **Why needed here:** BellNet's loss function is the Bellman error—the residual of the optimality equation. Understanding q_π = r + γP_π q_π and its fixed-point nature is essential to interpret what the network learns.
  - **Quick check question:** Can you write the Bellman optimality equation and explain why it is a fixed-point problem?

- **Concept: Graph Filters and Polynomial Filtering**
  - **Why needed here:** The core architectural primitive is H = Σ_j h_j A^j. You must understand how powers of the adjacency matrix aggregate information from K-hop neighborhoods and how filter coefficients h_j weight these contributions.
  - **Quick check question:** Given a graph adjacency A and signal x, what does (A^2 x)_i compute in terms of node i's neighborhood?

- **Concept: Algorithm Unrolling**
  - **Why needed here:** BellNet is constructed by unrolling policy iteration. Understanding how iterative algorithms map to layered architectures and why learned parameters might accelerate convergence is foundational.
  - **Quick check question:** If an iterative algorithm x^(k+1) = f(x^(k); θ) is unrolled to K steps with learnable θ per step, what are the tradeoffs vs. using a shared θ?

## Architecture Onboarding

- **Component map:**
  - Input: Initial value function estimate q̄ ∈ R^{|S||A|} (randomly initialized) and reward vector r
  - Layer l: (a) Two parallel K-th order graph filters on P_π^(l), one applied to r and one to q^(l); (b) sum outputs; (c) reshape to Q^(l+1); (d) apply row-wise softmax σ_τ to produce policy Π^(l+1); (e) compute P_π^(l+1) = P(I ⊙ (Π^(l+1))^⊤)^⊤
  - Output: Final q^(L+1) and Π^(L+1) after L layers
  - Parameters: Filter coefficients H = {h^(l)} for l = 0,...,L, each a (K+2)-dimensional vector

- **Critical path:**
  1. Randomly initialize q̄
  2. For each layer, compute filtered reward term and bias term using current P_π and learned h^(l)
  3. Apply softmax to obtain new policy; update P_π
  4. Repeat for L layers; compute Bellman error against target r + γP_Π q
  5. Backpropagate to update H

- **Design tradeoffs:**
  - **Depth L vs. filter order K:** Deeper networks (more unrolled iterations) can compensate for lower K. Paper shows BN-WS-5 with K=10 can match BN-WS-10 with K=5 in some regimes
  - **Weight sharing vs. per-layer parameters:** Sharing reduces variance and parameter count but may sacrifice expressiveness for complex MDPs
  - **Temperature τ in softmax:** Lower τ approximates hard max (closer to classical PI), but may harm differentiability; higher τ smooths gradients

- **Failure signatures:**
  - High variance across runs with distinct per-layer weights → consider weight sharing
  - Error plateaus despite increasing L → filter order K may be insufficient; increase K
  - Transfer fails on new MDPs → graph structure may differ significantly; verify node/edge correspondence
  - Training instability → check softmax temperature and gradient clipping; Bellman error minimization can be non-convex

- **First 3 experiments:**
  1. **Baseline replication:** Implement BellNet on a 4×4 grid-world with known transitions; vary L ∈ {2, 4, 6, 8} and K ∈ {3, 5, 10}. Compare normalized error (Eq. 9) against value iteration and policy iteration. Confirm that BN-WS converges in fewer layers than classical PI steps
  2. **Ablation on weight sharing:** Train BellNet with and without weight sharing (BN vs. BN-WS) on the same environment for fixed L and K. Report mean and variance of normalized error over 15 random seeds. Verify the paper's claim that WS reduces variance
  3. **Transfer test:** Train BellNet on the standard cliff-walking grid; evaluate (without retraining) on a mirrored cliff configuration (as in Fig. 3). Compare error against running classical PI from scratch on the new environment. Assess whether learned h coefficients transfer

## Open Questions the Paper Calls Out
- **Theoretical guarantees:** While the paper empirically observes stability and transferability of learned filters, it explicitly states that deeper theoretical analysis of these properties is left for future work, including error bounds for different graph structures.
- **Model-free extension:** The current methodology requires the full transition matrix P, which limits applicability to model-based settings. The paper does not address how BellNet could be adapted to learn from sampled trajectories in model-free RL.
- **Scalability to high dimensions:** The computational efficiency claims are based on small grid-world experiments. The paper does not demonstrate whether the graph filter approach remains efficient for high-dimensional or continuous state-action spaces.

## Limitations
- **Evaluation scope:** Experiments are limited to small grid-world environments; scalability to larger or continuous MDPs is not demonstrated.
- **Hyperparameter dependence:** Critical training hyperparameters (learning rate, initialization, temperature) are unspecified, making exact replication challenging.
- **Transfer generality:** Transfer experiments use only structurally similar MDPs; generalization to fundamentally different graph topologies is unproven.

## Confidence

- **High confidence:** The core mechanism linking policy iteration to graph filter cascades is mathematically sound and supported by explicit derivations in the paper.
- **Medium confidence:** Claims about weight sharing improving stability and enabling transfer are based on limited experiments and require broader validation.
- **Low confidence:** The paper's assertions about computational efficiency gains are not benchmarked against runtime or memory usage, only iteration counts.

## Next Checks

1. **Hyperparameter sensitivity:** Run ablations on learning rate, initialization schemes, and softmax temperature to assess robustness and identify optimal settings for stable convergence.
2. **Scalability test:** Evaluate BellNet on larger MDPs (e.g., 10×10 or 20×20 grid-worlds) to measure whether graph filter benefits persist as state space grows.
3. **Transfer diversity:** Test transfer on MDPs with different graph topologies (e.g., maze vs. cliff) to determine whether BellNet's learned filters generalize beyond minor structural variations.