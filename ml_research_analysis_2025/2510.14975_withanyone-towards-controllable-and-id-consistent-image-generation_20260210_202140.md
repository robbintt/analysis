---
ver: rpa2
title: 'WithAnyone: Towards Controllable and ID Consistent Image Generation'
arxiv_id: '2510.14975'
source_url: https://arxiv.org/abs/2510.14975
tags:
- image
- identity
- images
- reference
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces WithAnyone, a diffusion-based model for controllable
  and identity-consistent image generation that addresses the prevalent "copy-paste"
  artifact in current ID-preservation methods. By constructing MultiID-2M, a large-scale
  paired dataset with diverse reference images per identity, and introducing MultiID-Bench
  for standardized evaluation, the authors develop a training strategy incorporating
  GT-aligned ID loss and extended contrastive identity loss.
---

# WithAnyone: Towards Controllable and ID Consistent Image Generation

## Quick Facts
- arXiv ID: 2510.14975
- Source URL: https://arxiv.org/abs/2510.14975
- Reference count: 40
- This work introduces WithAnyone, a diffusion-based model for controllable and identity-consistent image generation that addresses the prevalent "copy-paste" artifact in current ID-preservation methods.

## Executive Summary
WithAnyone addresses the "copy-paste" artifact problem in identity-consistent image generation by constructing MultiID-2M, a large-scale paired dataset with diverse reference images per identity, and introducing MultiID-Bench for standardized evaluation. The method employs a training strategy incorporating GT-aligned ID loss and extended contrastive identity loss, achieving state-of-the-art performance with significantly reduced copy-paste artifacts while maintaining high identity fidelity. User studies validate that WithAnyone consistently ranks highest across identity similarity, prompt adherence, and aesthetics.

## Method Summary
WithAnyone is a diffusion-based model built on FLUX DiT architecture that achieves controllable and identity-consistent image generation. The method constructs MultiID-2M, a large-scale paired dataset with diverse reference images per identity, and employs a four-phase training strategy: reconstruction with fixed prompt, reconstruction with captions, paired tuning (50% paired data), and quality tuning. Key innovations include GT-aligned ID loss that uses ground-truth landmarks for stable supervision across all denoising timesteps, and extended contrastive identity loss with thousands of negatives from a reference bank for accelerated identity learning convergence.

## Key Results
- Achieves state-of-the-art performance with Sim(GT) = 0.460 and Copy-Paste = 0.144, significantly outperforming existing methods
- Reduces copy-paste artifacts from 0.239 to 0.144 while maintaining high identity fidelity
- Outperforms existing methods on both single- and multi-person generation tasks
- User studies consistently rank WithAnyone highest across identity similarity, prompt adherence, and aesthetics

## Why This Works (Mechanism)

### Mechanism 1
GT-aligned ID loss enables stable identity supervision across all denoising timesteps by aligning generated faces using ground-truth landmarks from the target image, avoiding landmark detection errors that occur with noisy generated images.

### Mechanism 2
Extended negative pools in contrastive loss accelerate identity learning convergence by sampling thousands of negatives from the identity-labeled reference bank rather than just batch-internal negatives, providing stronger discrimination signals.

### Mechanism 3
Paired-reference training breaks the reconstruction shortcut that causes copy-paste artifacts by using distinct reference and target images of the same identity (e.g., different poses, expressions), forcing the model to learn identity embeddings rather than pixel-level copying.

## Foundational Learning

- **ArcFace embeddings for identity representation**: Essential for identity-discriminative embeddings used in both ID loss and contrastive loss. Quick check: Can you explain why cosine similarity in ArcFace space measures identity rather than appearance similarity?

- **InfoNCE contrastive learning**: The ID contrastive loss uses InfoNCE formulation; understanding the temperature parameter and negative sampling is critical. Quick check: What happens to gradient magnitudes if all negatives are too easy (very dissimilar to anchor)?

- **DiT (Diffusion Transformer) cross-attention conditioning**: WithAnyone injects identity tokens via cross-attention into FLUX DiT blocks; understanding attention masking is essential for multi-ID control. Quick check: How does restricting face tokens to attend only to their corresponding face region prevent identity blending?

## Architecture Onboarding

- **Component map**: Face detection → ArcFace embeddings → MLP projection → 8 tokens × 3072 dim → Cross-attention injection → DiT backbone → Diffusion output

- **Critical path**: 1) Detect faces in input → 2) Extract ArcFace embeddings → 3) Project to DiT token space (8 tokens per face) → 4) Apply attention masks based on face bounding boxes → 5) Cross-attention injection at each transformer block → 6) Compute GT-aligned ID loss + contrastive loss + diffusion loss

- **Design tradeoffs**: ArcFace vs. SigLIP weight (higher ArcFace = stronger identity, higher copy-paste risk; SigLIP adds controllable attribute retention); Negative pool size (larger pool = better discrimination but higher memory/compute); Paired vs. reconstruction ratio (50% paired during Phase 3)

- **Failure signatures**: High copy-paste score (>0.3) indicates model overfits to reference pose/expression; Low Sim(GT) (<0.35) indicates identity preservation failing; Identity blending in multi-ID indicates attention masking not working

- **First 3 experiments**: 1) Reproduce ablations by training without Phase 3, without GT-aligned loss, without extended negatives; 2) Vary SigLIP weight from 0.0 to 1.0 while holding ArcFace constant; 3) Test generalization to unseen identities from MultiID-Bench's long-tail

## Open Questions the Paper Calls Out

The paper highlights three key open questions: (1) whether the identity-preservation performance generalizes effectively to non-celebrity, "in-the-wild" identities beyond the celebrity-focused MultiID-2M dataset; (2) what the optimal balancing mechanism is between high-level identity preservation and mid-level attribute retention to satisfy diverse user intent; and (3) how critical the ground-truth landmark alignment is for the method's success and whether it can be adapted for scenarios without GT images, such as pure text-to-image generation from a reference.

## Limitations

- Dataset dependency on MultiID-2M creates significant reproducibility barriers as the dataset is not publicly available
- Ablation scope is limited, not exploring hyperparameter sensitivity or alternative architectural choices
- User study validation provides rankings but lacks absolute performance thresholds for determining truly photorealistic results

## Confidence

- **High Confidence**: Claims about copy-paste artifact reduction and identity similarity improvements are well-supported by ablations and cross-method comparisons
- **Medium Confidence**: Claims about multi-person generation superiority and generalizability to unseen identities are supported by single experiments and theoretical reasoning
- **Low Confidence**: Claims about "significant superiority" relative to absolute state of the art depend on benchmark choices and dataset availability

## Next Checks

1. **Generalization Testing**: Evaluate WithAnyone on identities from MultiID-Bench that weren't in the training set to verify performance gains don't rely on overfitting to seen identities

2. **Robustness Analysis**: Test model behavior under varying image qualities, occlusions, and extreme poses to determine failure modes and practical limitations beyond controlled benchmark settings

3. **Alternative Dataset Validation**: Implement WithAnyone training using VGGFace2 or Casual Conversations as paired dataset substitutes to assess whether the approach is dataset-dependent or generalizable to other identity-labeled collections