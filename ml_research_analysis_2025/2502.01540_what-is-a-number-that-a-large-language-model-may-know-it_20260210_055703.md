---
ver: rpa2
title: What is a Number, That a Large Language Model May Know It?
arxiv_id: '2502.01540'
source_url: https://arxiv.org/abs/2502.01540
tags:
- number
- similarity
- language
- distance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) represent
  numbers, revealing a fundamental tension between treating numbers as strings versus
  numerical values. Using similarity-based prompting from cognitive science, the authors
  show that LLM similarity judgments between integers combine both Levenshtein edit
  distance (string-based) and Log-Linear distance (numerically-based), suggesting
  an entangled representation.
---

# What is a Number, That a Large Language Model May Know It?

## Quick Facts
- arXiv ID: 2502.01540
- Source URL: https://arxiv.org/abs/2502.01540
- Reference count: 32
- Primary result: LLM number representations entangle string (Levenshtein) and numerical (Log-Linear) distance, causing 36-47% errors in choosing numerically closer options.

## Executive Summary
This paper investigates how large language models represent numbers by analyzing similarity judgments between integers. The authors discover a fundamental tension: LLMs simultaneously encode both string-based (character edit distance) and numerical (logarithmic magnitude) representations of numbers. Using regression analysis and internal probing, they show this entanglement persists across multiple model families and can be probed in latent embeddings. The research demonstrates that this dual representation leads to incorrect decisions in naturalistic scenarios, particularly for longer numbers where string bias can override numerical reasoning.

## Method Summary
The authors collected similarity judgments for all integer pairs from 0-999 using temperature-0 prompting across five major LLMs (GPT-4o, Claude-3.5-Sonnet, DeepSeek-V3, Mixtral-8x22B, and Llama-3.1 variants). They regressed these judgments against Levenshtein edit distance and Log-Linear numerical distance measures. For internal analysis, they trained linear probes on Llama-3.1-8B's layer 25 activations to decode both distance measures from embeddings. The study also tested context effects using int() and str() type specifications and constructed diagnostic decision tasks with close numerical triplets to measure error rates.

## Key Results
- LLM similarity judgments combine Levenshtein edit distance (string-based) and Log-Linear distance (numerical), with combined R² = .726 across models
- Context specification (int() vs str()) shifts but does not eliminate the string bias, with Levenshtein R² dropping to [.156, .158] in int() context
- Linear probes on internal activations can decode both distance measures, with integer probe correlation with Log-Linear at .917 and string probe correlation with Levenshtein at .650
- Decision task error rates reach 36-47% when string-aligned options differ numerically, demonstrating practical consequences of the entanglement

## Why This Works (Mechanism)

### Mechanism 1: Dual-Distance Encoding
LLMs combine Levenshtein (string) and Log-Linear (numerical) distance measures when evaluating number similarity. Tokens representing numbers acquire embeddings that simultaneously encode orthographic similarity and magnitude relationships through logarithmic scaling. The regression model's ability to explain variance suggests the underlying representations contain these components. Combined R² = .726; Log-Linear alone R² CI: [.607, .609]; Levenshtein alone R² CI: [.213, .215]. Break condition: If models showed purely linear or purely categorical number representations, the dual-distance regression would fail.

### Mechanism 2: Contextual Type Biasing
Explicit type specification (`int()` vs `str()`) shifts but does not eliminate the entanglement between string and numerical representations. Type hints provide contextual signals that upweight one representation subspace over another, but both remain accessible in the embedding. `int()` context: Levenshtein R² CI drops to [.156, .158]; `str()` context: Levenshtein R² CI rises to [.309, .311]. Similarity matrices visually shift between contexts. Break condition: If type hints completely eliminated one distance measure, R² for that measure would approach zero and behavioral errors would disappear.

### Mechanism 3: Embedding-Level Representation Persistence
Linear probes trained on internal activations can decode both distance measures, indicating the entanglement exists in latent space. Residual stream embeddings at mid-to-late layers (layer 25 in Llama-3.1-8B) contain linearly decodable information about both string edit distance and log-linear magnitude. Integer probe correlation with Log-Linear: .917; String probe correlation with Levenshtein: .650. MDS visualization shows integer subspace organized numerically, string subspace organized by edit distance. Break condition: If probes failed to decode either distance measure from embeddings, the behavioral patterns could not be attributed to internal representations.

## Foundational Learning

- **Levenshtein Edit Distance**:
  - Why needed here: Measures string similarity via minimum insertions, deletions, substitutions. Essential for understanding how LLMs process numbers as character sequences.
  - Quick check question: Given "785" and "685," what is the Levenshtein distance? (Answer: 1, one substitution)

- **Log-Linear Numerical Distance**:
  - Why needed here: Models the psychological phenomenon where larger numbers are represented with lower fidelity (Weber-Fechner law). Captures how humans (and apparently LLMs) perceive magnitude.
  - Quick check question: Why might 100 and 200 feel "more different" than 1000 and 1100 despite equal absolute differences? (Answer: Logarithmic scaling reduces perceived difference at larger magnitudes)

- **Probing Classifiers**:
  - Why needed here: Method for testing whether specific information is linearly decodable from model activations. Used to connect behavioral findings to internal representations.
  - Quick check question: If a linear probe trained on layer 25 achieves high correlation but a probe on layer 5 does not, what does this suggest? (Answer: The information becomes encoded/accessible at intermediate layers)

## Architecture Onboarding

- **Component map**: Input tokens → Tokenizer → Embedding layer → Residual stream → Transformer layers → Layer 25 → Probes extract distance info → Final layer → Unembedding → Output token probabilities

- **Critical path**: Focus on mid-to-late layers (20-28) for probing; earlier layers show weaker correlations. Tokenization matters: all integers 0-999 are single tokens in tested models.

- **Design tradeoffs**: Behavioral-only analysis works on any model but cannot localize representations; probing requires internal access and compute; probe trained on 9,500 pairs evaluated on 250,000; temperature 0 vs stochastic: T=0.7 runs show correlation >0.787 with T=0 results.

- **Failure signatures**: High error rates (36-47%) on 5-digit decision tasks when Levenshtein-aligned option differs numerically; order effects in some models (Llama variants) suggesting sensitivity to presentation sequence; block-diagonal patterns in similarity matrices indicate string dominance.

- **First 3 experiments**:
  1. Replicate similarity matrix generation on a single model (e.g., Llama-3.1-8B) for integers 0-499 with the basic prompt; regress against Levenshtein and Log-Linear distances.
  2. Test context effect: Run the same experiment with `int()` and `str()` wrappers; quantify the shift in R² for each predictor.
  3. Construct 100 diagnostic triplets (target, Levenshtein-aligned, Log-aligned) for 3-digit numbers; measure string-bias error rate at T=0.

## Open Questions the Paper Calls Out

### Open Question 1
Can causal interventions at the embedding level effectively steer models to process digit tokens strictly as numerical values rather than strings? The authors state it remains to be seen whether one could causally intervene at the embedding level to steer the model from one token type to another. Evidence would be demonstration that patching or modifying specific activation directions in the latent space eliminates string-bias errors in downstream decision tasks.

### Open Question 2
Do reasoning-optimized models exhibit a different or reduced string-number entanglement compared to standard large language models? The authors suggest extending findings to models like OpenAI o1 and DeepSeek R1, noting reasoning may alter similarity judgments. Evidence would be comparative analysis showing whether reasoning models have lower error rates on the diagnostic "close triplet" task or reduced Levenshtein contributions in their similarity matrices.

### Open Question 3
Is the observed tension between string and numerical representations a specific instance of a broader symbol-string duality in domains like code or special characters? The discussion notes this may be a special case of a more broad symbol-string duality that impacts other domains. Evidence would be replicating similarity judgment experiments on code snippets or symbolic logic strings to see if representations blend syntactic and semantic features.

## Limitations

- The regression captures correlation but cannot definitively prove LLMs mechanistically compute both distances separately rather than learning a unified entangled representation
- The study's restriction to integers 0-999 limits generalizability to larger numbers and floating-point representations
- The behavioral decision task shows concerning error rates but only tests specific triplet configurations, leaving open whether alternative prompt engineering could mitigate these errors entirely

## Confidence

- **High confidence**: The empirical observation that LLM similarity judgments combine string and numerical components (supported by consistent R² values across five different models and the internal probing evidence showing linearly separable representations)
- **Medium confidence**: The claim that this entanglement causes real-world decision errors (strong for the specific triplet scenarios tested, but limited in ecological validity)
- **Medium confidence**: The interpretation that type hints (int() vs str()) provide meaningful but incomplete control over representation bias (effects are consistent but the mechanism for partial control remains unclear)

## Next Checks

1. Test whether fine-tuning on numerical reasoning tasks can reduce the string bias component, or whether the entanglement is immutable once learned from web-scale text.
2. Extend probing experiments to earlier and later layers to map the developmental trajectory of the entangled representation across the transformer stack.
3. Evaluate whether the string bias manifests in other numerical contexts beyond similarity judgments, such as arithmetic operations, number ordering tasks, or numerical analogical reasoning.