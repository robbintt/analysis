---
ver: rpa2
title: 'Magentic-UI: Towards Human-in-the-loop Agentic Systems'
arxiv_id: '2507.22358'
source_url: https://arxiv.org/abs/2507.22358
tags:
- magentic-ui
- agent
- user
- plan
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Magentic-UI is an open-source interface for human-in-the-loop AI
  agents that enables collaboration through co-planning, co-tasking, and verification
  mechanisms. The system uses a multi-agent architecture with a lead orchestrator
  that manages specialized agents for web browsing, coding, and file manipulation.
---

# Magentic-UI: Towards Human-in-the-loop Agentic Systems

## Quick Facts
- **arXiv ID:** 2507.22358
- **Source URL:** https://arxiv.org/abs/2507.22358
- **Reference count:** 40
- **One-line primary result:** Human-in-the-loop collaboration improves agent performance by up to 71% on web tasks

## Executive Summary
Magentic-UI is an open-source interface for human-in-the-loop AI agents that enables collaboration through co-planning, co-tasking, and verification mechanisms. The system uses a multi-agent architecture with a lead orchestrator managing specialized agents for web browsing, coding, and file manipulation. Evaluations show the system achieves 72.2% task completion on WebVoyager and 42.5% on GAIA, with human-in-the-loop approaches improving performance by up to 71%. Qualitative studies with 12 users found the interface easy to use (74.6 SUS score) and valuable for information gathering tasks, though participants noted latency and verbosity issues.

## Method Summary
Magentic-UI employs a multi-agent architecture centered around an Orchestrator that manages specialized agents including WebSurfer, Coder, and FileSurfer. The system implements three core collaboration mechanisms: co-planning (front-loading user interaction into planning phase), co-tasking (treating users as specialized agents), and verification (action approval and sandboxing). The Orchestrator generates structured plans in a PlanStep DSL format, which users can edit before execution. Actions are guarded by a two-stage safety system combining deterministic heuristics with an LLM judge. The system supports memory for reusing learned plans and handles concurrent sessions through multi-tasking.

## Key Results
- System achieves 72.2% task completion on WebVoyager benchmark
- Human-in-the-loop collaboration improves performance by up to 71%
- 74.6 SUS score from qualitative user study (n=12)
- 42.5% task completion on GAIA benchmark
- Zero successful adversarial attacks when safety mitigations enabled

## Why This Works (Mechanism)

### Mechanism 1: Collaborative Planning (Co-planning)
- **Claim:** Front-loading user interaction into a planning phase reduces downstream execution errors caused by ambiguous or underspecified tasks.
- **Mechanism:** The Orchestrator generates a structured plan (a sequence of natural language steps) which is exposed via an editable UI component. This allows users to inject priors and resolve ambiguities *before* any irreversible actions are taken.
- **Core assumption:** Users can articulate corrections more efficiently when presented with a static plan than by observing a dynamic execution trace.
- **Evidence anchors:**
  - [Abstract]: Lists "co-planning" as a primary mechanism for low-cost human involvement.
  - [Section 3]: "Co-planning front-loads the interaction cost... reducing downstream interaction costs and improving the chance of success."
  - [Corpus]: "Interaction-Driven Browsing" highlights the need for nonlinear browsing support, reinforcing the value of flexible planning.
- **Break condition:** The mechanism may fail if the natural language plan representation (PlanStep DSL) is insufficient to capture complex logic (e.g., conditional branching), leading to user edits that cannot be translated into agent actions (Section 8.1).

### Mechanism 2: Hybrid Action Guarding
- **Claim:** A two-stage safety system combining deterministic heuristics and LLM-based judgment effectively mitigates high-risk or irreversible actions without overwhelming the user.
- **Mechanism:** Actions are first classified by metadata (always/maybe/never irreversible). "Maybe" actions are passed to an LLM judge (ActionGuard) to evaluate context against safety criteria (e.g., data integrity, real-world impact) before requesting user approval.
- **Core assumption:** The LLM serving as the judge can reliably distinguish between harmful and benign actions in novel contexts.
- **Evidence anchors:**
  - [Section 6.4]: Describes the heuristic classification and the ActionGuard LLM judge logic.
  - [Section 7.5]: Reports that "none of the adversarial scenarios were effective" when mitigations were enabled.
  - [Corpus]: "A Safety and Security Framework for Real-World Agentic Systems" supports the need for dynamic interaction analysis, though Magentic-UI's specific hybrid approach is distinct.
- **Break condition:** The mechanism fails if the LLM judge produces false negatives (approving harmful actions) or if heuristics are misconfigured, though sandboxing provides a secondary layer of defense.

### Mechanism 3: User-as-Agent Delegation (Orchestrated Co-tasking)
- **Claim:** Treating the human user as a specialized agent in the multi-agent team enables the system to solve tasks that require capabilities outside the AI's toolset (e.g., CAPTCHAs, subjective choices).
- **Mechanism:** The Orchestrator maintains a "Progress Ledger" to track task state. When an obstacle is encountered, the Orchestrator delegates a step to the `UserProxy` agent, explicitly asking for help or clarification via the chat interface.
- **Core assumption:** The Orchestrator can accurately determine when it is "stuck" and needs human intervention, rather than retrying endlessly.
- **Evidence anchors:**
  - [Section 4]: "The user is part of the underlying multi-agent team... the Orchestrator can delegate steps of the plan to the user."
  - [Section 7.3]: Simulated user experiments showed a 71% performance improvement when this collaboration was enabled.
  - [Corpus]: Corpus evidence is weak for this specific "UserProxy" pattern; relevant papers focus more on agent-to-agent or agent-to-web interactions.
- **Break condition:** The mechanism falters if the Orchestrator's description field for the `UserProxy` is poorly tuned, causing it to interrupt the user too frequently or not enough (Section 4).

## Foundational Learning

- **Concept:** Multi-Agent Orchestration (AutoGen)
  - **Why needed here:** Magentic-UI is not a monolithic model but a team of specialized agents (WebSurfer, Coder, etc.) managed by a central Orchestrator. Understanding message passing and state management between these agents is crucial.
  - **Quick check question:** Can you explain the difference between the Orchestrator's role and the WebSurfer's role in executing a web navigation task?

- **Concept:** Docker Containerization & Sandboxing
  - **Why needed here:** Safety is a core constraint. Agents execute code and browse the web inside isolated Docker containers to prevent access to sensitive host resources (e.g., private keys, user sessions).
  - **Quick check question:** If the Coder agent tries to read `~/.ssh/id_rsa` from the host machine, what prevents it from succeeding?

- **Concept:** Domain Specific Languages (DSLs) for Planning
  - **Why needed here:** The system translates natural language tasks into a structured `Plan` format (a list of steps with agent assignments). This DSL mediates between user intent and agent execution.
  - **Quick check question:** Why might a natural language plan be easier for a user to verify than a formal Python script, and what flexibility is lost?

## Architecture Onboarding

- **Component map:**
  - Frontend (B) -> Backend (C) -> Agent Team (A)

- **Critical path:**
  1. User inputs task -> Orchestrator generates `Plan` (DSL).
  2. User edits/appraises Plan.
  3. Orchestrator enters **Execution Mode**: Loops through `PlanSteps` (Algorithm 1).
  4. Per step: Orchestrator updates `Progress Ledger` -> Delegates to Agent (e.g., WebSurfer).
  5. Agent generates tool call -> **ActionGuard** intercepts.
  6. If approved -> Tool executes in Docker -> Result returned to Orchestrator.
  7. Loop repeats until `Final Answer` is generated.

- **Design tradeoffs:**
  - **Natural Language Plans:** The paper highlights a tradeoff between plan flexibility (limited by natural language DSL) and user verifiability (high for natural language) [Section 3].
  - **Interruption Frequency:** The system minimizes user interruptions by only delegating to `UserProxy` when other agents fail, balancing automation with human effort [Section 4].

- **Failure signatures:**
  - **Infinite Replanning:** The Orchestrator repeatedly marks `step_complete` as False without progress.
  - **Sandbox Drift:** Agents failing to access resources because Docker volume mounts are missing.
  - **Action Guard False Positives:** Benign actions (e.g., clicking a non-submit button) being blocked by the LLM judge.

- **First 3 experiments:**
  1. **Autonomous Web Navigation:** Run the system in fully autonomous mode (no action guards, no user proxy) on a simple WebVoyager task to verify the `WebSurfer` and `Orchestrator` loop integrity.
  2. **Safety Trigger Test:** Instruct the system to execute a known sensitive action (e.g., "Delete all files in the workspace") to verify the `ActionGuard` intercepts and halts execution.
  3. **Plan Persistence:** Complete a multi-step task, save the "Learned Plan," and start a new session with a similar task to test the memory retrieval and plan reuse logic.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can agents be optimally tuned to interrupt users in a way that aligns with user preferences without negatively impacting task completion rates?
- **Basis in paper:** [explicit] The Discussion section identifies this as a specific open problem, noting the difficulty in balancing interruptions with risk tolerance and task success.
- **Why unresolved:** The paper implemented a specific "description field" for the UserProxy agent based on unstructured interaction, but notes a lack of ground truth signals for determining the "right time" to interrupt.
- **What evidence would resolve it:** Quantitative results from experiments (potentially using the simulated user environment described in Section 7.3) that demonstrate a policy for interruptions which maximizes user satisfaction and maintains high task accuracy.

### Open Question 2
- **Question:** What are the long-term productivity benefits of using Magentic-UI compared to manual task completion or fully autonomous agents?
- **Basis in paper:** [explicit] The Limitations section states, "Our evaluation of Magentic-UI did not measure downstream productivity benefits... To measure the productivity benefits of Magentic-UI requires long-term controlled trials."
- **Why unresolved:** The current evaluation was restricted to simulated evaluations and qualitative insights rather than longitudinal studies of productivity in real-world workflows.
- **What evidence would resolve it:** Data from long-term controlled trials measuring task throughput and effort for users with and without the system over extended periods.

### Open Question 3
- **Question:** How can the representation of an agent's plan be adapted (e.g., via hierarchical structures) to remain verifiable as plan complexity and length increase?
- **Basis in paper:** [explicit] The Discussion notes that "Plan editing may become too cost-prohibitive as the number of steps to review increases" and suggests future opportunities lie in hierarchical plan representations.
- **Why unresolved:** The current implementation uses a linear sequence of natural language steps, which creates a tradeoff between ease of understanding and planning flexibility/verbosity.
- **What evidence would resolve it:** User studies demonstrating that a hierarchical or alternative plan representation reduces the cognitive load and time required for users to verify complex plans compared to the linear model.

### Open Question 4
- **Question:** How can an agent's execution history be efficiently summarized to allow users to effectively verify task success without monitoring every step?
- **Basis in paper:** [explicit] The authors state in the Discussion that "further research is needed to efficiently summarize what the agent did, particularly in the case where people are not expected to be closely monitoring the task at hand."
- **Why unresolved:** Users in the qualitative study found long action histories overwhelming, yet needed to review them to determine correctness because the final answer alone was often insufficient for verification.
- **What evidence would resolve it:** Development and testing of summarization techniques (e.g., visual summaries or video recaps) that allow users to detect errors in execution significantly faster than reviewing full logs.

## Limitations
- System performance remains below human-level benchmarks (72.2% WebVoyager, 42.5% GAIA)
- Memory mechanism effectiveness depends heavily on task similarity
- ActionGuard's LLM-based judgment introduces potential vulnerabilities
- Small qualitative study sample size (n=12) limits generalizability

## Confidence
- **High confidence:** The architectural design and multi-agent coordination mechanisms are well-documented and internally consistent. The safety sandboxing approach and its effectiveness against known attack vectors are convincingly demonstrated.
- **Medium confidence:** The human-in-the-loop performance improvements are robust but may be task-dependent. The qualitative user study results (74.6 SUS score) are promising but limited by the small sample size and potential selection bias.
- **Low confidence:** The generalizability of the memory mechanism to diverse real-world tasks remains unproven, and the long-term reliability of the ActionGuard LLM judge under sustained adversarial pressure is untested.

## Next Checks
1. **Cross-dataset generalization test:** Evaluate Magentic-UI on a distinct web navigation benchmark (e.g., Mind2Web) to verify performance gains are not dataset-specific.
2. **Adversarial stress test:** Design and execute a battery of prompt injection and social engineering attacks specifically targeting the ActionGuard's judgment logic, measuring false negative rates under sustained attack.
3. **Memory transfer robustness evaluation:** Create a suite of tasks with varying degrees of similarity to learned plans, measuring the success rate and required human intervention as a function of task dissimilarity.