---
ver: rpa2
title: 'Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution'
arxiv_id: '2508.21004'
source_url: https://arxiv.org/abs/2508.21004
tags:
- backdoor
- lethe
- dilution
- attacks
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LETHE is a novel backdoor defense for large language models that\
  \ eliminates malicious behaviors by combining internal and external knowledge dilution.\
  \ Internally, it trains a clean model on a small subset of benign data and merges\
  \ it with the backdoored model using SLERP to dilute backdoor effects in the model\u2019\
  s parameters."
---

# Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution

## Quick Facts
- **arXiv ID:** 2508.21004
- **Source URL:** https://arxiv.org/abs/2508.21004
- **Reference count:** 40
- **Primary result:** Achieves up to 98% reduction in backdoor attack success rates while maintaining high clean data accuracy

## Executive Summary
LETHE is a novel backdoor defense for large language models that eliminates malicious behaviors by combining internal and external knowledge dilution. Internally, it trains a clean model on a small subset of benign data and merges it with the backdoored model using SLERP to dilute backdoor effects in the model's parameters. Externally, it appends keyword explanations retrieved from WordNet to distract the model's attention from backdoor triggers. Tested on five LLMs across classification and generation tasks, LETHE reduces attack success rates by up to 98% and maintains high clean data accuracy. It outperforms eight state-of-the-art defenses and remains robust against adaptive attacks.

## Method Summary
LETHE purifies backdoored LLMs through a dual-path approach. First, it trains a lightweight clean model on a small subset (5-10%) of benign data using LoRA, then merges this clean adapter with the backdoored model via Spherical Linear Interpolation (SLERP). This internal dilution neutralizes backdoor shortcuts embedded in the parameters. Second, during inference, it extracts keywords from inputs using TextRank and retrieves their definitions from WordNet, appending this evidence to the prompt to distract the model's attention from backdoor triggers. The combined approach achieves superior defense against various attack types while maintaining task performance.

## Key Results
- Reduces Attack Success Rate (ASR) by up to 98% across five different LLMs
- Maintains high Clean Data Accuracy (CDA) after purification
- Outperforms eight state-of-the-art defenses on multiple attack types
- Shows robustness against adaptive attacks that attempt to reverse the purification

## Why This Works (Mechanism)

### Mechanism 1: Internal Knowledge Dilution via Parameter Space Merging
Merging a backdoored model with a "clean" model trained on a small benign dataset neutralizes malicious shortcuts in the parametric memory. The paper posits that backdoors function as "shortcuts" (dominant erroneous mappings). By training a separate model on benign data (even just 5-10%) and merging the weights using SLERP, the merged model interpolates between the backdoor and clean decision boundaries. This dilutes the dominance of the backdoor trigger while retaining task performance.

### Mechanism 2: External Knowledge Dilution via Semantic Distraction
Attaching semantically relevant, neutral evidence to the prompt distracts the model from attending to backdoor triggers. This mechanism operates at the inference level. By extracting keywords from the input and retrieving their definitions (from WordNet), the input context is lengthened with benign, high-attention tokens. This shifts the model's attention mechanism away from the trigger tokens, reducing the probability of the backdoor activation path.

### Mechanism 3: Dual-Path Purification (Synergy)
Combining internal parameter correction and external input distraction provides a defense superior to either method alone, particularly against adaptive or complex attacks. Internal merging handles the "memory" of the backdoor, while external evidence handles the "activation" during inference. The paper suggests that internal dilution is the heavy lifter, but external dilution acts as a safety net for cases where the internal merge is imperfect.

## Foundational Learning

**Concept: Model Merging (SLERP/Linear)**
- **Why needed here:** To implement Internal Dilution. You must understand how to compute task vectors and interpolate between model weights without breaking the model.
- **Quick check question:** How does Spherical Linear Interpolation (SLERP) differ from simple weight averaging when merging two models with conflicting behaviors?

**Concept: Low-Rank Adaptation (LoRA)**
- **Why needed here:** To efficiently train the "Clean Model." Lethe requires building a clean model on limited data; LoRA allows this without updating all parameters, saving memory.
- **Quick check question:** Why is LoRA preferred over full fine-tuning for the "clean model" in this architecture (Hint: see Section G.2)?

**Concept: TextRank Algorithm**
- **Why needed here:** To implement External Dilution. You need to extract keywords from user queries to fetch definitions.
- **Quick check question:** How does TextRank select keywords without requiring a labeled training dataset?

## Architecture Onboarding

**Component map:**
1. Clean Model Builder: (LoRA fine-tuning on 10% data) -> Produces `Clean_Adapter`
2. Merger (Internal): (MergeKit/SLERP) -> Merges `Backdoored_Model` + `Clean_Adapter` -> `Purified_Model`
3. Preprocessor (External): (TextRank + WordNet) -> Input Query -> `Keywords` -> `Definitions`
4. Inference Wrapper: Concatenates `Query` + `Definitions` -> Feeds into `Purified_Model`

**Critical path:**
1. Construct the clean model (Train LoRA adapter)
2. Merge weights to create the purified model (Internal Dilution)
3. Implement the keyword extraction and definition retrieval pipeline (External Dilution)
4. Wrap inference to append definitions to the prompt

**Design tradeoffs:**
- **Merging Strategy:** SLERP is robust but requires tuning interpolation parameter `t`. Linear is simpler but risks performance drops. TIES preserves features better but is computationally heavier. (Paper selects SLERP as default)
- **Data Volume:** Increasing clean data >10% improves ASR but increases training cost linearly (Figure 5)
- **Evidence Source:** WordNet is static and safe. Using an LLM to generate evidence might be more context-aware but introduces risk of hallucination or bias

**Failure signatures:**
- **Triggerless Attacks:** High persistence of ASR on "DTBA" (triggerless) attacks compared to explicit triggers (Table 3), suggesting external dilution is less effective here
- **CDA Drop on Non-Neutral Evidence:** If the "evidence" retrieved contains sentiment or strong semantic bias (Section 5.5), clean accuracy drops
- **Adaptive Attacks:** If an attacker knows the merge strategy, they can "subtract" the clean model (Section 6.1), though Lethe shows robustness here

**First 3 experiments:**
1. **Internal Only Validation:** Take a backdoored GPT-2 XL (BadEdit attack), train a LoRA adapter on 10% clean data, merge using SLERP. Verify ASR < 5%
2. **External Only Validation:** Take the original backdoored model (no merging). Apply TextRank + WordNet to inputs. Measure the drop in ASR (expect small drop) and check CDA remains stable
3. **Combined Robustness:** Apply the full Lethe pipeline to a multi-trigger attack (CBA). Compare the Defense Score (DS) against standard Fine-tuning baselines to confirm superior performance

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can the knowledge dilution mechanisms of LETHE be effectively adapted to purify backdoors in non-textual domains such as computer vision or speech recognition?
- **Basis in paper:** [explicit] The conclusion states, "Investigating how LETHE can be adapted to non-textual data would be an interesting direction for future work."
- **Why unresolved:** The paper evaluates LETHE solely on Large Language Models (LLMs) and textual data; the principles of parameter merging and prompt-level evidence dilution were not tested on convolutional or audio-based architectures
- **What evidence would resolve it:** A demonstration of LETHE applied to a backdoored image classifier (e.g., ResNet) or speech model, analyzing if parameter merging and external evidence (e.g., image patches or audio context) effectively reduce Attack Success Rate (ASR)

**Open Question 2**
- **Question:** What is the optimal strategy for selecting layers in "Passthrough" (Frankenmerge) model merging to maximize backdoor purification?
- **Basis in paper:** [explicit] Section 5.3 notes that Passthrough "still needs extensive exploration, particularly when identifying the optimal combination of layers," despite SLERP being the default choice
- **Why unresolved:** While SLERP and Linear merging were validated as stable, the Passthrough method (mixing layers from different models) remained unoptimized due to the complexity of layer selection
- **What evidence would resolve it:** An ablation study varying the selection patterns of layers from the clean and backdoored models in Passthrough merging to identify configurations that outperform the default SLERP defense score

**Open Question 3**
- **Question:** How robust is LETHE's internal dilution mechanism if the small subset of available data ($D_c$) contains label noise or accidental poison?
- **Basis in paper:** [inferred] The paper assumes the defender has access to a "small subset of clean training samples" ($D_c$) and Section 5.4 varies the *percentage* of data, but never tests the *purity* of that data
- **Why unresolved:** In real-world scenarios, verifying a completely clean dataset is difficult; if the "clean model" is trained on slightly noisy data, its ability to neutralize the backdoored model via merging is unknown
- **What evidence would resolve it:** Experiments where the training data for the clean model ($M_{\hat{\theta}}$) is injected with varying ratios of mislabeled or poisoned samples to measure the degradation of the ASR and Clean Data Accuracy (CDA)

## Limitations

- **Parameter Merging Robustness:** The merging process assumes backdoor effects can be linearly averaged out, which may not hold for deep, architecture-specific backdoors where malicious behavior is entangled with core model capabilities
- **External Dilution Scope:** The effectiveness of keyword-based evidence depends heavily on the backdoor being trigger-keyword dependent, with limited benefit for syntactic or semantic triggerless attacks
- **Computational Overhead:** The dual-path approach requires additional processing (LoRA training and keyword extraction/retrieval) that may impact real-time latency requirements

## Confidence

**High Confidence:** The core mechanism of using parameter merging (SLERP) with a clean model is technically sound and supported by empirical results showing significant ASR reduction (up to 98%) across multiple attack types.

**Medium Confidence:** The synergy between internal and external dilution is demonstrated through ablation studies, but the independence of failure modes is assumed rather than rigorously proven.

**Low Confidence:** The generalization to truly novel backdoor types not tested in the paper (particularly those exploiting model architecture rather than data poisoning) is asserted but not empirically validated.

## Next Checks

1. **Cross-Architecture Testing:** Apply LETHE to transformer variants beyond the tested models (e.g., ViT for vision tasks, or different decoder architectures) to verify the parameter merging mechanism generalizes across architectures.

2. **Adaptive Attack Stress Test:** Design an adaptive attack specifically targeting the external dilution component by embedding backdoor triggers in semantically neutral words that are likely to be selected by TextRank, then measure whether LETHE's defense degrades.

3. **Long-Term Stability Analysis:** After applying LETHE's purification, continue fine-tuning the model on clean data and monitor whether backdoor behaviors re-emerge over time, testing the permanence of the dilution effect.