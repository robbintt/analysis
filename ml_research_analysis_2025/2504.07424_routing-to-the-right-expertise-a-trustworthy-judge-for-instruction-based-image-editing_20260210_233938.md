---
ver: rpa2
title: 'Routing to the Right Expertise: A Trustworthy Judge for Instruction-based
  Image Editing'
arxiv_id: '2504.07424'
source_url: https://arxiv.org/abs/2504.07424
tags:
- evaluation
- expert
- jure
- image
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "JURE addresses the challenge of reliably evaluating instruction-based\
  \ image editing (IIE) outputs by introducing a modular, explainable framework that\
  \ dynamically routes evaluation tasks to specialized experts. Instead of relying\
  \ on monolithic evaluators, JURE decomposes the evaluation process into atomic expertise\
  \ dimensions\u2014such as object detection, similarity analysis, and style assessment\u2014\
  each handled by dedicated models."
---

# Routing to the Right Expertise: A Trustworthy Judge for Instruction-based Image Editing

## Quick Facts
- arXiv ID: 2504.07424
- Source URL: https://arxiv.org/abs/2504.07424
- Authors: Chenxi Sun; Hongzhi Zhang; Qi Wang; Fuzheng Zhang
- Reference count: 10
- Key outcome: JURE-o1 achieves human-level judgment quality (Cohen's Kappa 0.54) on Emu Edit dataset by routing evaluation tasks to specialized experts

## Executive Summary
JURE addresses the challenge of reliably evaluating instruction-based image editing (IIE) outputs by introducing a modular, explainable framework that dynamically routes evaluation tasks to specialized experts. Instead of relying on monolithic evaluators, JURE decomposes the evaluation process into atomic expertise dimensions—such as object detection, similarity analysis, and style assessment—each handled by dedicated models. An orchestrator (using OpenAI o1) analyzes the editing instruction and intermediate results to determine which experts to invoke and in what order, ensuring precise and contextually aware evaluation. Experiments on the Emu Edit dataset show that JURE-o1 achieves human-level judgment quality, with a Cohen's Kappa correlation of 0.54—matching human-human agreement—while significantly outperforming strong baselines like o1 alone. The framework is extensible and interpretable, enabling transparent diagnosis of evaluation decisions and seamless adaptation to evolving IIE capabilities.

## Method Summary
JURE employs a modular architecture that decomposes evaluation into atomic expertise dimensions, each handled by specialized models. The framework uses an orchestrator (OpenAI o1) to analyze editing instructions and intermediate results, determining which experts to invoke and in what order. This dynamic routing ensures contextually appropriate evaluation across different editing tasks. The approach combines pre-trained expert models for specific evaluation criteria with a reasoning orchestrator that can adapt to various editing scenarios, providing both high accuracy and explainability in judgment generation.

## Key Results
- JURE-o1 achieves human-level judgment quality with Cohen's Kappa correlation of 0.54 on Emu Edit dataset
- Significantly outperforms strong baselines including o1-only evaluation
- Matches human-human agreement in evaluation consistency
- Demonstrates extensibility through modular expert design

## Why This Works (Mechanism)
The framework's effectiveness stems from decomposing complex evaluation into specialized expertise dimensions, allowing each component to focus on specific quality criteria. The orchestrator's reasoning capability enables contextual understanding of editing instructions, ensuring appropriate expert selection. This modular approach avoids the limitations of monolithic evaluators that must handle all evaluation criteria simultaneously, leading to more precise and explainable judgments.

## Foundational Learning
- **Modular evaluation decomposition**: Breaking complex tasks into specialized components improves accuracy and interpretability
  - Why needed: Monolithic evaluators struggle with diverse editing tasks and criteria
  - Quick check: Verify each expert specializes in distinct evaluation dimensions without overlap

- **Dynamic expert routing**: Context-aware selection of appropriate evaluation modules based on task requirements
  - Why needed: Different editing instructions require different quality criteria and assessment methods
  - Quick check: Confirm orchestrator correctly identifies task type and routes to relevant experts

- **Multi-expert coordination**: Sequential or parallel invocation of specialized models for comprehensive evaluation
  - Why needed: Single experts cannot capture all quality dimensions for complex editing tasks
  - Quick check: Validate that expert outputs combine coherently for final judgment

## Architecture Onboarding

**Component Map:**
Instruction -> Orchestrator (o1) -> Expert Router -> [Object Detection Expert, Similarity Expert, Style Expert, etc.] -> Judgment Synthesis

**Critical Path:**
Editing Instruction → Orchestrator Analysis → Expert Selection → Expert Execution → Result Synthesis → Final Judgment

**Design Tradeoffs:**
- Modular flexibility vs. coordination complexity
- Specialized accuracy vs. computational overhead
- Interpretability vs. end-to-end optimization potential

**Failure Signatures:**
- Incorrect expert routing leading to irrelevant evaluation criteria
- Expert model limitations affecting specific quality dimensions
- Orchestrator reasoning errors in instruction interpretation

**First Experiments:**
1. Test orchestrator routing accuracy on diverse editing instruction types
2. Validate individual expert performance on their specialized evaluation tasks
3. Measure correlation between expert judgments and human preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning enable end-to-end optimization of JURE's Orchestrator and experts using only human preference scores without detailed explanations?
- Basis in paper: [explicit] The conclusion states: "Exploring end-to-end optimization via reinforcement learning. Given only human preference scores (without explanations), the Orchestrator and experts can be jointly trained to achieve human-level, explainable judgment."
- Why unresolved: The current JURE implementation relies on pre-trained experts and a prompting-based Orchestrator; no gradient-based joint training has been attempted.
- What evidence would resolve it: Experiments showing RL-trained JURE variants achieving comparable or better Cohen's Kappa scores against human judgments, with analysis of training stability and sample efficiency.

### Open Question 2
- Question: Can smaller, open-source models effectively replace OpenAI o1 as the Orchestrator while maintaining routing quality?
- Basis in paper: [inferred] The paper relies exclusively on o1 as the Orchestrator, with no ablation on alternative models. This raises concerns about reproducibility, cost, and accessibility for the research community.
- Why unresolved: No experiments were conducted with smaller MLLMs (e.g., LLaVA, Qwen-VL) to determine the minimum reasoning capability required for effective expert routing.
- What evidence would resolve it: Comparative experiments using open-source MLLMs as Orchestrator, reporting Cohen's Kappa correlation with human judgments and analyzing routing decision quality.

### Open Question 3
- Question: How does JURE generalize to other IIE datasets and editing paradigms beyond Emu Edit?
- Basis in paper: [inferred] The evaluation was conducted only on the Emu Edit test set (120 human-annotated samples). The paper does not demonstrate cross-dataset robustness or performance on diverse editing scenarios.
- Why unresolved: Different IIE benchmarks (e.g., EditBench, I2EBench) may have different instruction distributions, editing types, and quality criteria that could affect JURE's routing decisions.
- What evidence would resolve it: Evaluation results on multiple IIE benchmarks, reporting correlation with human judgments across datasets and analyzing expert invocation patterns for different editing task distributions.

### Open Question 4
- Question: What is the computational cost and latency trade-off of JURE's multi-expert routing compared to monolithic evaluators?
- Basis in paper: [inferred] The paper claims JURE is efficient but provides no quantitative analysis of inference time, API costs, or computational overhead from iterative expert invocations.
- Why unresolved: Practical deployment requires understanding whether the improved judgment quality justifies the increased latency and cost of orchestrating multiple expert calls.
- What evidence would resolve it: Benchmarks reporting average evaluation time per sample, number of expert invocations, and cost analysis comparing JURE to baseline approaches like o1-only evaluation.

## Limitations
- Heavy reliance on OpenAI o1 creates dependency on proprietary technology and raises cost concerns
- Moderate human-level performance (Cohen's Kappa 0.54) indicates room for significant improvement
- Limited evaluation scope to Emu Edit dataset raises questions about generalization to diverse editing tasks

## Confidence
- **High Confidence**: The modular architecture design and decomposition of evaluation into atomic expertise dimensions is technically sound and well-reasoned
- **Medium Confidence**: The performance claims on Emu Edit dataset, while promising, need validation across diverse datasets and editing tasks
- **Medium Confidence**: The extensibility claims require empirical verification with additional expert modules and real-world deployment scenarios

## Next Checks
1. Conduct extensive cross-dataset validation using multiple instruction-based image editing benchmarks beyond Emu Edit to verify generalization
2. Implement and test alternative orchestrator models (both open-source and commercial) to assess dependency on OpenAI o1 and identify potential cost-effective alternatives
3. Deploy the framework in a real-world image editing application and conduct longitudinal studies to evaluate performance stability and adaptation to evolving editing capabilities over time