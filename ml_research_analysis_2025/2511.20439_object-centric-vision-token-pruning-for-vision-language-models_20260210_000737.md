---
ver: rpa2
title: Object-Centric Vision Token Pruning for Vision Language Models
arxiv_id: '2511.20439'
source_url: https://arxiv.org/abs/2511.20439
tags:
- tokens
- vision
- token
- llav
- oc-vtp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OC-VTP, an Object-Centric Vision Token Pruning
  method for Vision Language Models (VLMs). The core idea is to use a lightweight
  Object-Centric pruner (OC-pruner) trained once on a small set of images to identify
  and retain the most representative vision tokens based on object-centric learning,
  ensuring minimal information loss.
---

# Object-Centric Vision Token Pruning for Vision Language Models

## Quick Facts
- arXiv ID: 2511.20439
- Source URL: https://arxiv.org/abs/2511.20439
- Reference count: 40
- Primary result: OC-VTP retains over 95% accuracy on LLaVA-1.5 with only 11.1% of visual tokens

## Executive Summary
This paper introduces OC-VTP, an Object-Centric Vision Token Pruning method for Vision Language Models (VLMs). The core idea is to use a lightweight Object-Centric pruner (OC-pruner) trained once on a small set of images to identify and retain the most representative vision tokens based on object-centric learning, ensuring minimal information loss. This pruned token set can then be used in various VLMs without any fine-tuning. Across multiple vision pruning ratios, OC-VTP consistently helps mainstream VLMs preserve the highest inference accuracy compared to state-of-the-art methods. For example, on LLaVA-1.5, OC-VTP retains over 95% of accuracy with only 11.1% of visual tokens. It also significantly reduces computational overhead, achieving nearly 85% reduction in FLOPs on LLaVA-1.5-7B when retaining 11.1% vision tokens. The pruning method also demonstrates intuitive object-level interpretability, with selected tokens concentrating on different object centers.

## Method Summary
OC-VTP introduces a lightweight Object-Centric pruner (OC-pruner) trained once on 40,000 COCO images to identify and retain the most representative vision tokens. The pruner uses Slot Attention to aggregate vision tokens belonging to the same object or object part into one slot, ensuring exclusiveness and completeness. The most attended token from each slot is selected as the representative. A random auto-regressive Transformer decoder reconstructs the original unpruned tokens from the pruned subset, minimizing reconstruction error. The pruner is inserted between the vision encoder (using Layer 9 as reference) and the LLM projector, allowing it to work across different VLMs without fine-tuning. For inference, attention maps from the OC-pruner are used to select final-layer vision tokens that are passed to the LLM.

## Key Results
- OC-VTP retains over 95% accuracy on LLaVA-1.5 with only 11.1% of visual tokens
- Achieves nearly 85% reduction in FLOPs on LLaVA-1.5-7B when retaining 11.1% vision tokens
- Consistently outperforms state-of-the-art pruning methods across multiple VLMs and benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Slot Attention for Exclusive Object Coverage
Vision tokens are aggregated via Slot Attention, which encourages selection of tokens representing distinct objects or semantic parts, reducing information redundancy. The mechanism uses a learnable Slot Attention module that competes over input vision tokens, with soft attention maps binding specific token clusters to individual slots. By selecting the token with the highest attention weight per slot, the system ensures retained tokens cover diverse semantic regions rather than multiple tokens from the same texture or background.

### Mechanism 2: Reconstruction-Driven Representative Selection
The OC-pruner is trained to reconstruct the full set of vision tokens from the pruned subset using a Random Auto-regressive Transformer. This loss function forces the pruner to select tokens that carry the maximum variance and information of the original distribution, rather than relying on proxy metrics like attention magnitude.

### Mechanism 3: Cross-Layer Pruning Reference
Using intermediate vision encoder features (e.g., Layer 9) to generate pruning indices, while applying them to the final layer, improves retention of dense semantic information compared to using final-layer features alone. This separates the "decision metric" space from the "feature content" space.

## Foundational Learning

- **Slot Attention**: A mechanism that binds scattered pixel/token features into discrete "object" concepts through iterative refinement. Quick check: Can you explain why Slot Attention uses iterative refinement (like an EM algorithm) to assign tokens to slots?

- **Reconstruction Loss (Auto-encoding)**: An unsupervised proxy for "information importance" that guides token selection. Quick check: If you reconstruct an image from 10% of its tokens, what implicit assumption are you making about the remaining 90%?

- **Vision-Language Model (VLM) Architecture**: Understanding the "in-between" injection point of the OC-pruner is necessary to distinguish it from encoder-side or decoder-side methods. Quick check: Why does pruning visual tokens before the LLM decoder save more FLOPs than pruning inside the decoder?

## Architecture Onboarding

- **Component map**: Vision Encoder (CLIP ViT) -> OC-Pruner (Slot Attention) -> Selector (argmax) -> Indexer (Final Layer features) -> Projector & LLM

- **Critical path**: Image -> Vision Encoder -> Extract Layer 9 features -> Feed to OC-Pruner -> Get Slot Attn Weights -> Compute Indices (Top-1 per Slot) -> Extract Final Layer features at Indices -> Flatten + Project -> LLM

- **Design tradeoffs**: Area-Weighted MSE prevents ignoring small objects; Fixed slot numbers during training may cause performance drops on dynamic-resolution models

- **Failure signatures**: Small objects ignored without AW-MSE; Thin structures (fences) often dropped; Resolution mismatch causes accuracy loss on dynamic models

- **First 3 experiments**:
  1. Layer Ablation: Compare Layer 9 vs Layer -2 reference for pruning LLaVA-1.5 validation set
  2. Loss Function Ablation: Train MSE vs AW-MSE pruners on COCO subset, evaluate on small object benchmarks
  3. Efficiency Benchmark: Measure end-to-end latency on LLaVA-NeXT with 160 retained tokens vs Vanilla (2880 tokens)

## Open Questions the Paper Calls Out

- Can the object-centric pruning strategy be effectively extended to video Vision-Language Models to handle temporal redundancy?
- How can language context be incorporated into the OC-pruner to achieve task-aware pruning without sacrificing the "train-once" plug-and-play nature?
- Can the OC-pruner be adapted to handle VLMs with dynamic image resolutions where the token count varies significantly across inputs?
- Is it feasible to apply the object-centric pruning mechanism within the LLM decoder layers (progressive pruning) rather than just between the encoder and projector?

## Limitations

- Performance heavily depends on CLIP-based vision encoder architecture
- Reconstruction fidelity gap - actual reconstruction error on pruned tokens is not reported
- Computational overhead of OC-Pruner during inference is not isolated
- Limited evaluation on purely language-based tasks with sparse visual content

## Confidence

**High Confidence (8/10)**:
- OC-VTP consistently outperforms state-of-the-art pruning methods across VLMs and benchmarks
- Slot attention effectively identifies semantically meaningful objects
- Reconstruction-based training provides better token selection than attention-based methods

**Medium Confidence (6/10)**:
- Cross-layer pruning provides consistent benefits across different VLMs
- Method achieves claimed computational savings
- Small object retention is genuinely improved by AW-MSE weighting

**Low Confidence (4/10)**:
- Universal applicability to any VLM architecture without modification
- Layer 9 is universally optimal reference layer
- No performance degradation on dynamic-resolution VLMs

## Next Checks

1. **Reconstruction Error Analysis**: Measure and report reconstruction MSE between original and reconstructed vision tokens for different pruning ratios to establish correlation with downstream accuracy

2. **Cross-Architecture Transferability**: Implement OC-VTP on a non-CLIP vision encoder (DINOv2 or SigLIP-based VLMs) and evaluate performance on the same benchmark suite

3. **Dynamic Resolution Adaptation**: Train or adapt the OC-Pruner for Qwen2.5-VL's variable token count mechanism and compare performance against the fixed-slot approach