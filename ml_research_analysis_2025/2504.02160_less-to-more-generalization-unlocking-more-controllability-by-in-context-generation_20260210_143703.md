---
ver: rpa2
title: 'Less-to-More Generalization: Unlocking More Controllability by In-Context
  Generation'
arxiv_id: '2504.02160'
source_url: https://arxiv.org/abs/2504.02160
tags:
- image
- generation
- subject
- data
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of subject-driven image generation,
  specifically the scalability and controllability issues when moving from single-subject
  to multi-subject scenarios. The authors propose a novel approach that combines a
  systematic synthetic data curation framework with a universal customization model
  architecture (UNO).
---

# Less-to-More Generalization: Unlocking More Controllability by In-Context Generation

## Quick Facts
- arXiv ID: 2504.02160
- Source URL: https://arxiv.org/abs/2504.02160
- Reference count: 40
- One-line primary result: A two-stage progressive training method with novel position embedding achieves SOTA on subject-driven image generation benchmarks.

## Executive Summary
This paper addresses the challenge of subject-driven image generation, specifically the scalability and controllability issues when moving from single-subject to multi-subject scenarios. The authors propose a novel approach that combines a systematic synthetic data curation framework with a universal customization model architecture (UNO). The data curation pipeline leverages in-context generation capabilities of diffusion transformers to create high-consistency multi-subject paired data, followed by a multi-stage filtering mechanism to ensure quality. UNO employs progressive cross-modal alignment and universal rotary position embedding to iteratively train a multi-image conditioned subject-to-image model from a text-to-image model. Extensive experiments demonstrate that UNO achieves state-of-the-art performance on both single-subject and multi-subject driven generation benchmarks, significantly outperforming existing methods in terms of subject similarity and text controllability.

## Method Summary
The method involves a two-stage training process on a pre-trained FLUX.1-dev diffusion transformer. Stage I trains a single-subject subject-to-image (S2I) model using synthetic paired data generated via in-context generation from FLUX.1. Stage II continues training on multi-subject paired data, where a trained Stage I model generates a new reference image for a second subject in a different scene. A novel Universal Rotary Position Embedding (UnoPE) assigns diagonal offsets to reference image tokens to prevent spatial layout copying. Training uses LoRA with rank 512, batch size 16, and 8Ã— A100 GPUs.

## Key Results
- UNO achieves highest DINO and CLIP-I scores on single-subject benchmark, indicating superior subject similarity and text controllability
- On multi-subject benchmark, UNO outperforms existing methods by a significant margin in both subject similarity and text fidelity
- The model demonstrates strong performance across various application scenarios including virtual try-on, identity preservation, and stylized generation

## Why This Works (Mechanism)

### Mechanism 1: In-Context Data Generation and Filtering
A systematic synthetic data curation pipeline using in-context generation can produce high-quality multi-subject paired data, which is critical for training a multi-subject conditioned model. The process starts with a pre-trained text-to-image diffusion transformer model. By carefully crafting text prompts, the model's in-context generation ability is harnessed to produce image pairs containing consistent subjects in different scenes. A multi-stage filtering process is then applied. First, DINOv2 similarity scores filter out low-consistency pairs. Second, a Vision-Language Model (VLM) evaluates consistency across dimensions like appearance, details, and attributes using a Chain-of-Thought scoring system. This removes low-quality data. For multi-subject data, a single-subject Subject-to-Image (S2I) model is first trained on single-subject pairs. This S2I model then generates a new reference image for a second subject in a different scene, preventing "copy-paste" issues.

### Mechanism 2: Progressive Cross-Modal Alignment
An iterative, stage-wise training approach allows for stable learning of multi-image conditioning, preventing training instability from a "cold start" on complex multi-subject data. The training is divided into two stages using a base T2I DiT model. Stage I: The model is fine-tuned on the single-subject paired data. The input to the DiT is the concatenation of text tokens, noisy latents, and a single reference image token. This stage establishes cross-modal alignment for one subject. Stage II: The model from Stage I is further trained on multi-subject paired data. The input is expanded to include multiple reference image tokens. This progressive approach avoids disrupting the pre-trained model's convergence distribution with a sudden introduction of multiple novel tokens.

### Mechanism 3: Universal Rotary Position Embedding (UnoPE)
A novel position encoding scheme is required to inject multiple reference images without causing the model to copy their spatial layouts, thereby improving subject similarity while maintaining text controllability. In a DiT with Rotary Position Embeddings (RoPE), each token has a position index (i, j). Simply concatenating reference image tokens would reuse indices, causing interference. The paper's UnoPE assigns position indices to the Nth reference image token with an offset: (i + w(N-1), j + h(N-1)). This places reference image tokens in "diagonal" positions far from the target image and text tokens. This spatial separation prevents the model from attending to the reference's spatial layout, forcing it to focus on semantic content (subject identity).

## Foundational Learning

- **Concept: Diffusion Transformers (DiT)**
  - **Why needed here:** UNO is built on top of a DiT architecture (FLUX.1), not a U-Net. Understanding how transformers process sequences of image and text patches is fundamental.
  - **Quick check question:** How does a DiT process an input image and text prompt differently from a U-Net-based diffusion model like Stable Diffusion?

- **Concept: In-Context Generation / Consistency**
  - **Why needed here:** This is the core technique used to bootstrap the training data. One must understand what it means for a model to generate "consistent" subjects across a single inference step.
  - **Quick check question:** In this paper, what is the primary technique used to force a text-to-image model to generate an image with a consistent subject in two different contexts?

- **Concept: Position Embeddings (RoPE)**
  - **Why needed here:** The paper's main architectural modification is UnoPE, based on Rotary Position Embeddings. Without understanding RoPE, the mechanism of UnoPE cannot be grasped.
  - **Quick check question:** What is the function of Rotary Position Embeddings in a transformer, and what specific problem does UnoPE solve when injecting multiple reference images?

## Architecture Onboarding

- **Component Map:**
  - Text Prompt, Reference Image(s) -> T5 Text Encoder, VAE Encoder
  - [text_tokens, noisy_latents, ref_image_1_tokens, (ref_image_2_tokens, ...)] -> DiT Blocks (frozen + LoRA)
  - Denoised Latents -> VAE Decoder -> Final Image

- **Critical Path:**
  1. Data Generation: Start with base T2I model + prompts -> Single-Subject Pairs -> Filter -> Stage I S2I Model. Use Stage I S2I model + cropped subjects -> Multi-Subject Pairs -> Filter. This yields the training data.
  2. Stage I Training: Initialize with pre-trained T2I. Fine-tune LoRA weights on Single-Subject Pairs. Input: [text, noisy_latent, ref_image_1].
  3. Stage II Training: Continue from Stage I. Fine-tune on Multi-Subject Pairs. Input: [text, noisy_latent, ref_image_1, ref_image_2]. Position indices for reference tokens are calculated using UnoPE.

- **Design Tradeoffs:**
  - Synthetic Data Quality vs. Scalability: The pipeline is fully automated and scalable, but its quality is entirely dependent on the performance of the base model and the VLM filter.
  - LoRA Adaptation vs. Full Fine-Tuning: Using a high-rank LoRA (512) allows for significant adaptation while preserving base capabilities, but may have a lower performance ceiling than full fine-tuning.
  - UnoPE Offset Strategy: The diagonal offset is chosen to completely separate reference from target spatially. A smaller offset might be insufficient, while a larger one is redundant.

- **Failure Signatures:**
  - Copy-Paste Effect: The model simply reconstructs the reference image without following the prompt. Diagnosis: UnoPE not applied correctly or offset is too small; training data contains too many "easy" cropped pairs.
  - Attribute Confusion: In multi-subject generation, subjects have mixed-up features. Diagnosis: Failure in Stage II training or insufficient separation in UnoPE, causing token attention to mix reference features.
  - Low Subject Similarity: Generated image doesn't match the reference subject. Diagnosis: VLM filter threshold too low, allowing low-quality training data; training steps insufficient.

- **First 3 Experiments:**
  1. Sanity Check (Single Subject): Train only Stage I and evaluate on a standard single-subject benchmark. Expect to see strong subject similarity to validate the basic data and training setup.
  2. Ablation on UnoPE: Train the full model (Stages I & II) without the position offset for reference images. Compare against the full model on multi-subject tasks. Expect a significant drop in performance and an increase in copy-paste behaviors.
  3. Ablation on Data Generation: Train a model using "cropped" reference images directly vs. the full "generated" reference images. Evaluate the tendency for copy-paste vs. creative composition. Expect the "cropped" version to have higher copy-paste rates.

## Open Questions the Paper Calls Out
- **Question:** To what extent does expanding the synthetic data pipeline to include specific editing and stylization pairs unlock new capabilities in UNO?
- **Basis in paper:** Section I (Limitations) states the dataset currently has limited editing and stylization data and proposes expanding data types to "further unlock UNO's potential."
- **Why unresolved:** The current study focuses primarily on subject consistency and text controllability rather than these specific task domains.
- **What evidence would resolve it:** Performance benchmarks on image editing and style transfer tasks after training on an expanded, task-specific synthetic dataset.

- **Question:** Can the emergent spatial relationship learning observed in virtual try-on scenarios be optimized without specialized supervision?
- **Basis in paper:** Section H.2 notes that UNO performs surprisingly well on virtual try-on despite no specialized training, suggesting "novel optimization strategies" as a direction for future exploration.
- **Why unresolved:** The mechanism behind this zero-shot capability is attributed to "relationship understanding" but is not fully isolated or optimized.
- **What evidence would resolve it:** Ablation studies identifying which components of the progressive cross-modal alignment enable spatial reasoning in try-on tasks.

- **Question:** Does the Universal Rotary Position Embedding (UnoPE) effectively mitigate attribute confusion as the number of input subjects scales beyond two?
- **Basis in paper:** The methodology sets N=2 for training, while Figure 1 implies capability for "Many2One" scenarios.
- **Why unresolved:** The paper provides ablations for N=2 but does not quantitatively analyze UnoPE's stability or confusion rates with a higher density of reference images.
- **What evidence would resolve it:** Quantitative results (DINO/CLIP scores) on generation tasks involving 4 or more distinct reference subjects.

## Limitations
- The entire approach depends on the base T2I model's in-context generation consistency and the VLM's ability to reliably filter low-quality pairs; degradation in either component collapses synthetic dataset quality.
- The UnoPE position embedding mechanism is novel but the specific diagonal offset choice could potentially be optimized further; sensitivity to offset values remains unclear.
- The complete effectiveness of the VLM-based filtering pipeline is difficult to fully verify without access to the exact VLM model, CoT prompt templates, and threshold values used.

## Confidence
- **High Confidence:** The two-stage progressive training approach (Stage I for single-subject, Stage II for multi-subject) is logically sound and well-supported by the experimental results showing performance improvements over non-progressive baselines.
- **Medium Confidence:** The UnoPE position embedding mechanism is novel and theoretically justified, but the specific offset choice could potentially be optimized further.
- **Low Confidence:** The complete effectiveness of the VLM-based filtering pipeline is difficult to fully verify without access to the exact VLM model, CoT prompt templates, and threshold values used.

## Next Checks
1. **UnoPE Sensitivity Analysis:** Systematically vary the diagonal offset values in UnoPE (e.g., (w, h), (2w, 2h), (3w, 3h)) and measure the impact on multi-subject generation quality, particularly looking for the onset of copy-paste behaviors.
2. **VLM Filter Ablation:** Train models using synthetic data filtered at different VLM score thresholds (e.g., keep only score=3 vs. score=4) and evaluate the trade-off between dataset size and final model performance to determine if the filtering is overly conservative.
3. **Cross-Model Generalization:** Apply the complete UNO pipeline (data generation + training) to a different base DiT model (e.g., a smaller open-source model) and evaluate whether the same systematic approach yields comparable improvements, or if model-specific tuning of the data generation or UnoPE parameters is required.