---
ver: rpa2
title: 'Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer
  Attention'
arxiv_id: '2508.19414'
source_url: https://arxiv.org/abs/2508.19414
tags:
- heads
- layer
- format
- features
- even
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates a format-dependent reasoning failure in
  Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger than
  "9.8" in chat or Q&A formats but answers correctly in simple format. Through systematic
  intervention experiments, the authors discover that transformers organize computation
  by attention head index parity: even-indexed heads handle numerical comparison while
  odd-indexed heads serve incompatible functions.'
---

# Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention

## Quick Facts
- arXiv ID: 2508.19414
- Source URL: https://arxiv.org/abs/2508.19414
- Authors: Gustavo Sandoval
- Reference count: 40
- Primary result: Perfect repair of format-dependent decimal comparison error by transplanting attention patterns from 8 even-indexed heads at Layer 10

## Executive Summary
This paper identifies and surgically repairs a format-dependent reasoning failure in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger than "9.8" in chat/Q&A formats but answers correctly in simple format. Through systematic intervention experiments, the authors discover that transformer computation organizes by attention head index parity: even-indexed heads handle numerical comparison while odd-indexed heads serve incompatible functions. The intervention achieves 100% repair by transplanting attention patterns from exactly 8 even heads at Layer 10, demonstrating that the "9.8 vs 9.11" error can be perfectly fixed by targeting the precise computational component responsible for the failure.

## Method Summary
The authors employ attention pattern transplantation using nnsight v0.2.1 to repair format-dependent reasoning errors. They capture attention outputs from a "good state" (simple format) run and substitute them during a "bad state" (Q&A format) run at specific layers and heads. The intervention targets Layer 10, transplanting outputs from exactly 8 even-indexed heads (0, 2, 4, ..., 30) while leaving other heads unchanged. Sparse Autoencoder (SAE) analysis reveals format representations separate at Layer 7 (10% feature overlap) then re-entangle at Layer 10 with different weightings (80% overlap). Logit lens analysis identifies Layer 25 as the point where format divergence becomes irreversible. The repair exhibits sharp phase transitions: 7 heads fail completely while 8 succeed, and <60% pattern replacement fails while ≥60% succeeds.

## Key Results
- Perfect repair achieved: 100% success rate for "9.8 vs 9.11" comparison using 8 even heads at Layer 10
- Sharp phase transitions: 7→8 heads switches from 0% to 100% success; <60% to ≥60% pattern replacement shows similar discontinuity
- Format trajectory: Representations separate at Layer 7 (10% feature overlap) then re-entangle at Layer 10 (80% overlap) with differential feature amplification
- Head parity specialization: Even-indexed heads (0, 2, 4, ..., 30) handle numerical comparison; odd-indexed heads serve incompatible/format-sensitive functions

## Why This Works (Mechanism)

### Mechanism 1
Layer 10 attention patterns causally determine decimal comparison outcomes, with even-indexed heads specialized for numerical comparison and odd-indexed heads serving incompatible functions. Attention head index parity determines functional role—even heads (0, 2, 4, ..., 30) activate numerical processing features (SAE features 10049, 11664, 8234 with r=0.89 correlation), while odd heads activate format-sensitive features (r=0.86 with features 25523, 22441). Exactly 8 even heads collectively activate the minimum 5 critical SAE features required for correct comparison. Core assumption: Head index parity reflects systematic computational organization learned during training, not random initialization artifact. Evidence anchors: [abstract] "even indexed heads handle numerical comparison, while odd heads serve incompatible functions"; [section 4.1] "any combination of 8+ even heads succeeds, while 7 or fewer completely fails" with n=50 per combination. Break condition: Different decimal structures (e.g., "10.9 vs 10.11") with different tokenization may use different mechanisms (0% intervention success per Table 3).

### Mechanism 2
Formats follow a "separate then re-entangle" trajectory, diverging at Layer 7 (10% feature overlap) then re-converging at Layer 10 (80% overlap) with differential feature amplification. Early layers detect format type → Layer 7 maximally separates representations → Layer 10 becomes a computational bottleneck where shared features are re-entangled with format-specific weightings (e.g., Feature 25523 shows 1.53× amplification in failing formats). This creates "irremediable entanglement"—same features, different orchestration. Core assumption: Layer 10's 80% overlap represents a genuine computational bottleneck rather than measurement artifact. Evidence anchors: [abstract] "format representations separate (10% feature overlap at Layer 7), then re-entangle with different weightings (80% feature overlap at Layer 10)"; [section 4.3] Figure 3 shows the separation-entanglement curve with Layer 10 as intervention target. Break condition: Earlier intervention (Layer 9) or later (Layer 11) fails completely (0% success per Table 2).

### Mechanism 3
Repair exhibits sharp phase transitions at both the 8-head minimum and 60% pattern replacement threshold, indicating discrete computational modes rather than continuous processing. Feature activation requirements create discrete thresholds—8 heads guarantee ≥5 active critical features (100% success), while 7 heads can only guarantee 4 features (0% success). Similarly, 60% pattern replacement crosses a critical mass threshold for overriding format-induced amplification patterns. Core assumption: Phase transitions reflect genuine computational structure, not experimental noise. Evidence anchors: [abstract] "sharp phase transitions at both the 8-head minimum and 60% pattern replacement threshold"; [section 4.1] "7→8 switches from 0 to 100% success" with 95% CI showing no intermediate zone. Break condition: Different model architectures may have different thresholds; only tested on Llama-3.1-8B-Instruct.

## Foundational Learning

- **Concept:** Logit Lens Analysis
  - **Why needed here:** Technique to project intermediate layer hidden states into vocabulary space, revealing where the model "commits" to an answer. Used to identify Layer 25 as divergence point.
  - **Quick check question:** At which layer does the probability of the correct token first spike significantly in the simple format versus the Q&A format?

- **Concept:** Attention Pattern Transplantation
  - **Why needed here:** Core intervention method—capturing attention weights from a "good state" run and substituting them during a "bad state" run at specific layers and heads.
  - **Quick check question:** Why does transplanting at Layer 10 succeed while Layer 9 or 11 fails?

- **Concept:** SAE (Sparse Autoencoder) Feature Analysis
  - **Why needed here:** Decomposes layer activations into interpretable features, revealing which features are shared versus format-specific and their amplification factors.
  - **Quick check question:** What does 80% feature overlap at Layer 10 indicate about whether formats use separate circuits?

## Architecture Onboarding

- **Component map:** Layer 6 format detection → Layer 7 maximum separation (10% feature overlap) → Layer 10 re-entanglement bottleneck (80% overlap) → Layer 25 irreversible commitment → Final output
- **Critical path:** Layer 6 format detection → Layer 7 separation → Layer 10 re-entanglement with format-weighted features → Layer 25 irreversible commitment → Final output
- **Design tradeoffs:** Full-layer patching fails (incoherent output) because different formats have incompatible representation spaces; single-head patching fails (0% success) because no individual head carries sufficient signal; complete attention submodule at Layer 10 succeeds (100%)—"Goldilocks" granularity; even-heads-only (8 of 32) achieves same 100% success with 75% parameter reduction
- **Failure signatures:** "7→8 heads" threshold failure: 7 heads = 0%, 8 heads = 100% (no intermediate); "<60% pattern replacement" = 0%, "≥60%" = 100%; Odd heads alone: 0% success regardless of count; Wrong layer (9 or 11): 0% success even with all 32 heads
- **First 3 experiments:**
  1. Reproduce format-dependent bug: Run "Which is bigger: 9.8 or 9.11?" with Q&A format (expect 100% error) versus simple format (expect 0% error), n=100 each.
  2. Layer 10 attention transplantation: Patch attention outputs from simple format run into Q&A format run at Layer 10 (expect 100% repair), then try Layer 9 and 11 (expect 0% success).
  3. Even/odd head ablation: Test all combinations of 7 even heads (expect 0% success), 8 even heads (expect 100% success), and any 8 odd heads (expect 0% success) to validate sharp threshold and parity specialization.

## Open Questions the Paper Calls Out

### Open Question 1
Is the even/odd attention head specialization pattern universal across different transformer architectures? Basis in paper: [explicit] The authors explicitly ask "Is even/odd specialization universal across architectures?" in Section 6.3. Why unresolved: The study focuses exclusively on Llama-3.1-8B-Instruct. While Table 4 shows the reasoning bug exists in models like Gemma and Pythia, the specific even-head repair mechanism was not validated on them. What evidence would resolve it: Applying the Layer 10 attention pattern transplantation intervention to architectures like Mistral or GPT to observe if even-indexed heads similarly specialize in numerical comparison.

### Open Question 2
At what point during pre-training or fine-tuning does the even/odd attention head specialization emerge? Basis in paper: [explicit] Section 6.3 asks "When does specialization emerge during training?", a point reiterated in Section 6.4 as a limitation. Why unresolved: The paper performs post-hoc analysis on a fully trained model and does not track the development of this substructure across training checkpoints. What evidence would resolve it: A longitudinal analysis of model checkpoints (e.g., using the Pythia suite) to identify the training step where even heads begin to distinctively handle numerical features over odd heads.

### Open Question 3
Can the Layer 10 surgical intervention generalize to numerical comparison errors involving different tokenization structures? Basis in paper: [explicit] Section 6.4 notes the mechanism appears specific to single-digit comparisons, and Table 3 shows the intervention fails on "10.9 vs 10.11" due to tokenization effects. Why unresolved: The discovered mechanism relies on specific SAE features and redundancy levels for single-digit decimals; it is unknown if the same "8-head" logic applies when numbers are tokenized differently (e.g., multi-digit). What evidence would resolve it: Testing the attention transplantation method on a wider dataset of decimal pairs with varying digit lengths to see if the 8-even-head threshold remains sufficient for repair.

## Limitations

- Perfect repair demonstrated only for one specific decimal comparison pattern ("9.8 vs 9.11") in one model architecture
- Sharp phase transitions may reflect task-specific computational constraints rather than general principles
- SAE feature analysis relies on features discovered through training on this specific model's activations, raising questions about transferability
- Mechanism claims rest heavily on correlation patterns without establishing causal necessity beyond successful intervention

## Confidence

**High Confidence:** The format-dependent bug exists as described and can be reproduced (tested via logit lens at Layer 25 showing format-specific probability divergences). The Layer 10 intervention successfully repairs the error when 8 even heads are transplanted, with sharp phase transitions observed at the claimed thresholds (7→8 heads switches from 0% to 100% success; <60% to ≥60% pattern replacement shows similar discontinuity). The SAE feature separation at Layer 7 and re-entanglement at Layer 10 with different weightings are empirically validated.

**Medium Confidence:** The even/odd head specialization reflects systematic computational organization rather than random initialization artifact. While the parity-based functional division is consistently observed across successful interventions, the paper doesn't test whether different initializations or model architectures maintain this pattern. The "separate then re-entangle" trajectory interpretation for format processing is plausible but could also reflect measurement artifacts in feature overlap calculations or the specific SAE training process.

**Low Confidence:** The general applicability of these findings beyond the specific "9.8 vs 9.11" case. The paper acknowledges 0% success on "10.9 vs 10.11" but doesn't establish systematic testing across different numerical formats or magnitudes. The claim that Layer 10 represents a universal computational bottleneck for format-dependent failures is speculative—it may be specific to this particular model's learned computation path for this task.

## Next Checks

1. **Cross-format generalization test:** Systematically test the intervention on 20+ different decimal comparison pairs (varying digit lengths, decimal positions, and numerical relationships) to establish whether the even-head mechanism generalizes or is specific to the "9.8 vs 9.11" tokenization pattern.

2. **Architecture transfer validation:** Apply the same Layer 10 even-head intervention to Llama-3.1-70B and other transformer architectures to verify whether head index parity specialization is a consistent organizational principle or an 8B-specific artifact.

3. **Earlier intervention exploration:** Design experiments to test whether partial interventions at Layer 7 (maximum separation) or Layer 25 (irreversible commitment) can achieve partial repair, mapping the full intervention landscape rather than assuming Layer 10 is the only viable target.