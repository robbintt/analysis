---
ver: rpa2
title: Byzantine-Resilient Zero-Order Optimization for Communication-Efficient Heterogeneous
  Federated Learning
arxiv_id: '2502.00193'
source_url: https://arxiv.org/abs/2502.00193
tags:
- learning
- clients
- lemma
- optimization
- zero-order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Byzantine-resilient federated learning with
  communication efficiency. It proposes CYBER-0, a framework that combines zero-order
  optimization with robust aggregation to achieve resilience against Byzantine attacks
  while significantly reducing communication costs.
---

# Byzantine-Resilient Zero-Order Optimization for Communication-Efficient Heterogeneous Federated Learning

## Quick Facts
- **arXiv ID**: 2502.00193
- **Source URL**: https://arxiv.org/abs/2502.00193
- **Reference count**: 40
- **One-line primary result**: Achieves Byzantine resilience in federated learning while reducing communication costs by up to 7 orders of magnitude through zero-order optimization with transformed robust aggregation.

## Executive Summary
This paper proposes CYBER-0, a framework that addresses the challenge of Byzantine-resilient federated learning with communication efficiency. The key innovation is performing robust aggregation directly on zero-order (ZO) updates in the perturbation space, eliminating the need to transmit high-dimensional gradient vectors. This approach preserves Byzantine resilience while enabling communication via just a few scalars per round, achieving up to 7 orders of magnitude reduction in communication costs compared to gradient-based methods. The method demonstrates comparable accuracy to traditional approaches under various Byzantine attacks while significantly reducing memory requirements.

## Method Summary
CYBER-0 combines zero-order optimization with robust aggregation to achieve Byzantine resilience with communication efficiency. Clients compute two-point ZO gradient estimates along ν random perturbation directions using a shared PRNG seed, sending only scalar projections to the federator. The federator performs transformed robust aggregation (CWTM or Krum) directly in the ν-dimensional embedding space, then broadcasts the aggregated scalar vector back to clients. This approach avoids reconstructing full gradients, preserving communication savings while maintaining Byzantine resilience. The method supports both unbiased (new perturbations each epoch) and biased (reused perturbations for local epochs) variants, with theoretical convergence guarantees for non-convex objectives under data heterogeneity and bounded gradient divergence.

## Key Results
- Achieves 7 orders of magnitude reduction in communication cost compared to gradient-based federated learning methods
- Maintains comparable accuracy to gradient-based approaches under various Byzantine attacks (ALIE, FOE, SF, LF, TMA)
- Demonstrates up to 12× reduction in memory requirements compared to backpropagation-based approaches
- Validated on MNIST logistic regression and RoBERTa-large fine-tuning tasks with heterogeneous data

## Why This Works (Mechanism)

### Mechanism 1: Transformed Robust Aggregation
The paper performs robust aggregation in the low-dimensional perturbation space (scalar projections) rather than reconstructing full gradients. This preserves geometric properties required for Byzantine resilience while enabling scalar-only communication. The Johnson-Lindenstrauss lemma ensures relative distances between clients' vectors are preserved in this lower-dimensional space, allowing robust rules to identify outliers effectively. The core assumption is that ν perturbations are sufficiently large relative to log(n) clients.

### Mechanism 2: Pseudorandom Perturbation Synchronization
A shared PRNG seed enables clients and the federator to generate identical high-dimensional perturbation vectors locally, eliminating the need to transmit these vectors. The federator broadcasts a seed once, and all parties use it (combined with epoch indices) to generate identical random directions. This requires the PRNG cycle length to be sufficient and implementations to be deterministic across clients.

### Mechanism 3: In-Place Memory Optimization
Zero-order optimization allows model updates without storing intermediate activations required for backpropagation. The two-point ZO estimate requires only forward passes at perturbed weights, avoiding the memory overhead of computational graphs. This reduces memory requirements by up to 12×, though it increases computational cost due to multiple forward passes per step.

## Foundational Learning

**Concept: Zero-Order (ZO) Optimization**
- **Why needed here**: This is the engine of the paper. You cannot understand CYBER-0 without grasping how it approximates gradients using only function evaluations (scalar differences).
- **Quick check question**: Can you explain why the formula $\frac{f(w+\mu z) - f(w-\mu z)}{2\mu}$ approximates a directional derivative?

**Concept: Byzantine-Robust Aggregation (CWTM/Krum)**
- **Why needed here**: These are the "plugins" used to filter malicious updates. Understanding how they define "outliers" (coordinate-wise vs. distance-based) is necessary to interpret the experimental results.
- **Quick check question**: How does Coordinate-wise Trimmed Mean (CWTM) handle a malicious client sending arbitrarily large values?

**Concept: Johnson-Lindenstrauss (JL) Lemma**
- **Why needed here**: This theoretical tool justifies why aggregating in the low-dimensional "perturbation space" preserves the distances required for robustness.
- **Quick check question**: What is the relationship between the number of dimensions (perturbations) and the preservation of pairwise distances in a vector set?

## Architecture Onboarding

**Component map:**
- **Client Node**: Holds local data, local model w_i, and PRNG. Performs local ZO epochs and sends scalar vector g_i.
- **Federator (Server)**: Holds global model w, shared seed, and PRNG. Receives scalars, applies Robust Aggregator R in ℝ^ν, and broadcasts aggregated scalar vector.
- **Channel**: Low-bandwidth link transmitting only ν scalars per client per round.

**Critical path:**
1. Setup: Federator broadcasts seed s and initial model w(1)
2. Local Step: Clients generate z from s, compute g_i = ZO_Estimate(w, z, data), update local w
3. Uplink: Clients send scalar g_i (size ν) to Federator
4. Aggregation: Federator aggregates {g_i} into R_t (still in ℝ^ν)
5. Downlink: Federator broadcasts R_t (size ν)
6. Update: Clients update global model w ← w - η Z R_t

**Design tradeoffs:**
- **Biased vs. Unbiased Estimator**: The "Biased" estimator reuses the same z for K local epochs (saving communication) but introduces bias. The "Unbiased" version samples new z every epoch (better convergence, higher cost).
- **Perturbation Count (ν)**: Larger ν improves gradient estimation accuracy and robustness but increases communication cost linearly (cost = K · ν).

**Failure signatures:**
- **Desynchronization**: If client/federator seeds diverge (e.g., due to dropped packets or differing PRNG libraries), the model will fail to converge or diverge immediately.
- **Gradient Obfuscation**: If the data is extremely heterogeneous and ν is very low, the "transformed aggregation" might treat valid heterogeneous updates as Byzantine attacks, leading to stagnation.

**First 3 experiments:**
1. **Sanity Check (MNIST, IID, No Attack)**: Run CYBER-0 with ν=1 and ν=64. Verify that the model learns at all and check convergence speed vs. communication cost against the baseline (FedAvg).
2. **Attack Resilience (MNIST, Non-IID, α=0.1)**: Inject "ALIE" or "FOE" attacks with 25% Byzantine clients. Compare CWTM vs. Krum aggregation to see which preserves accuracy better under the transformed aggregation scheme.
3. **Memory Profile (RoBERTa-large)**: Profile GPU memory usage during the fine-tuning phase. Compare against a backpropagation baseline to verify the claimed 12x memory reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the theoretical requirement for the number of perturbations (ν) be relaxed from Θ(d) to constant order without relying on the Lipschitz objective assumption?
- **Basis in paper**: [explicit] Page 8 states, "Note that Theorem 5.7 requires ν = Θ(d)... However, our empirical results demonstrate that CYBER-0 converges even when ν=1."
- **Why unresolved**: The theoretical bounds rely on JL-lemma constraints that are looser than observed empirical behavior.
- **What evidence would resolve it**: A convergence proof showing non-asymptotic rates for constant ν under standard smoothness assumptions.

### Open Question 2
- **Question**: Does the "biased ZO estimator" variant of CYBER-0 (Algorithm 5), which reuses perturbation directions for local epochs, offer provable convergence guarantees?
- **Basis in paper**: [explicit] Page 3 notes, "We provide theoretical and empirical convergence guarantees for the former [unbiased] and empirical convergence guarantees for the latter."
- **Why unresolved**: The reuse of directions introduces complex bias-variance dependencies in the local updates that are not currently bounded in the theoretical analysis.
- **What evidence would resolve it**: Theoretical analysis bounding the error accumulation of the biased local updates.

### Open Question 3
- **Question**: Does performing robust aggregation in the lower-dimensional embedding space (ℝ^ν) introduce specific vulnerabilities to attacks optimized for the projected subspace?
- **Basis in paper**: [inferred] Page 5 highlights the challenge of "arguing that performing the aggregation in the perturbation space... preserves the robustness guarantees," relying on geometric preservation.
- **Why unresolved**: While theory suggests robustness is preserved, the reduced dimensionality might facilitate "dimension-collapsed" attacks not fully captured by standard gradient attacks.
- **What evidence would resolve it**: Empirical evaluation of attacks specifically crafted to maximize distance within the ℝ^ν embedding rather than the gradient space.

## Limitations
- The theoretical analysis assumes bounded gradient divergence but doesn't quantify sensitivity to violations in highly heterogeneous settings
- The paper doesn't analyze computational overhead of multiple forward passes, which may offset communication savings in resource-constrained devices
- Memory efficiency claims rely on external work (Malladi et al., 2023) rather than direct measurement for this implementation

## Confidence
- **High confidence**: Communication cost reduction (7 orders of magnitude) - directly measured and compared to gradient-based baselines
- **Medium confidence**: Byzantine resilience claims - validated empirically across multiple attack types but theoretical guarantees depend on strong assumptions about perturbation dimensionality
- **Low confidence**: Memory reduction quantification - cited from external work (Malladi et al., 2023) rather than measured directly for this specific implementation

## Next Checks
1. **Convergence sensitivity analysis**: Systematically vary the gradient divergence bound parameter in Assumption 2.1 and measure impact on convergence speed under different levels of data heterogeneity
2. **Computational overhead benchmarking**: Measure wall-clock time per iteration including the multiple forward passes, comparing against gradient-based methods on representative hardware (e.g., edge devices)
3. **Numerical stability testing**: Run CYBER-0 with extreme perturbation values (μ approaching machine precision limits) to identify potential floating-point issues in in-place perturbation operations