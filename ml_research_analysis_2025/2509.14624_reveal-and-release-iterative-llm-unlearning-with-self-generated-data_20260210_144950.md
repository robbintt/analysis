---
ver: rpa2
title: 'Reveal and Release: Iterative LLM Unlearning with Self-generated Data'
arxiv_id: '2509.14624'
source_url: https://arxiv.org/abs/2509.14624
tags:
- data
- unlearning
- forget
- score
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of LLM unlearning when forget
  data is privacy-sensitive, rare, or difficult to obtain. The authors propose a "Reveal-and-Release"
  approach that generates self-derived forget data through optimized instruction prompting,
  followed by an iterative unlearning framework using parameter-efficient modules.
---

# Reveal and Release: Iterative LLM Unlearning with Self-generated Data

## Quick Facts
- arXiv ID: 2509.14624
- Source URL: https://arxiv.org/abs/2509.14624
- Reference count: 25
- Primary result: Self-generated forget data via optimized prompting plus iterative PEM unlearning effectively removes target knowledge while preserving utility.

## Executive Summary
This paper tackles LLM unlearning in scenarios where forget data is private, rare, or hard to obtain. It introduces a "Reveal-and-Release" approach: first, optimized instructions surface the model's own knowledge relevant to the target, then an iterative PEM framework incrementally edits the model to remove that knowledge while preserving utility. Experiments across toxicity, NER, and coding tasks demonstrate strong performance with minimal external data dependency.

## Method Summary
The method operates in two stages. First, NeuralUCB bandit optimization searches soft prompts to elicit diverse, task-relevant completions from the model itself, forming a self-generated forget set. Second, an iterative unlearning loop trains and merges parameter-efficient modules (LoRAs): at each step, a retain PEM is added and a forget PEM is subtracted from the base model, with merge weights selected via thresholds (≥90% forgetting, ≥95% utility recovery). This alternates between amplifying retained capabilities and suppressing target knowledge.

## Key Results
- Self-generated forget data outperforms external data in alignment and perplexity, especially when external data is noisy or scarce.
- Iterative PEM composition achieves superior forget-utility trade-offs compared to single-step methods, with low eigenbasis similarity between forget and retain modules.
- Rule-based merge weight selection provides controllable unlearning with fewer manual tuning runs, though thresholds may need adjustment per task.

## Why This Works (Mechanism)

### Mechanism 1: Self-Generated Forget Data via Instruction Optimization
The model is prompted with optimized instructions (via NeuralUCB) to verbalize target knowledge, producing forget data better aligned with internal representations than external approximations. This relies on a relevance oracle and diversity scoring (Vendi) to guide prompt selection. Break condition: if knowledge is not verbalizable or oracle is unreliable, data quality degrades.

### Mechanism 2: Iterative PEM Composition (Forget–Retain LoRA Merging)
Forget and retain LoRAs are incrementally merged with the base model in alternating steps, improving the forget–utility trade-off versus single-step editing. Core assumption: forget and retain LoRAs occupy orthogonal subspaces, minimizing destructive interference. Break condition: orthogonality breaks down for highly entangled capabilities, risking utility collapse.

### Mechanism 3: Rule-Based Merge Weight Selection (Forget ≥90%, Utility Recovery ≥95%)
Per-iteration thresholds guide selection of LoRA merge weights, balancing forgetting and utility without exhaustive hyperparameter search. Core assumption: metrics are cheap to evaluate during unlearning and thresholds generalize. Break condition: noisy or expensive metrics make rule-based selection impractical; tight thresholds may require more iterations.

## Foundational Learning

- **LoRA basics (A/B decomposition, rank, merging via addition/subtraction)**
  - Why needed: PEMs are composed arithmetically to edit the base model.
  - Quick check: Given a base weight W and LoRA ΔW = BA, write the merged forward form and explain the effect of negating ΔW.

- **Gradient-ascent-style forgetting vs. fine-tuning (direction and magnitude of updates)**
  - Why needed: Subtracting forget LoRAs approximates gradient-ascent behavior over iterations.
  - Quick check: Why can aggressive ascent-style updates harm fluency or cause mode collapse?

- **Bandit optimization and exploration–exploitation (NeuralUCB intuition)**
  - Why needed: Used to search prompts that elicit diverse, relevant self-generated forget data.
  - Quick check: What does a UCB-based policy optimize, and why is exploration important when diversity matters?

## Architecture Onboarding

- **Component map**: Prompt generator → NeuralUCB selector → Model completions → Relevance scorer + Vendi diversity → Append to forget set → Initialize base with −μ₀·forget₀ → Iterate: (retain LoRA, merge +λ) → (forget LoRA, merge −μ) with rule-based weight selection.

- **Critical path**:
  1) Ensure a reliable relevance oracle and embedding model for Vendi.
  2) Generate forget set with outer/inner loops (Algorithm 1).
  3) Run iterative PEM unlearning with per-iteration metric checks.

- **Design tradeoffs**:
  - Single-step vs. iterative: more control but higher compute and metric overhead.
  - External vs. self-generated data: better alignment and lower PPL, but reliance on model verbalization and instruction quality.
  - Strict vs. relaxed thresholds: strict thresholds may require more iterations; relaxed may need more steps to reach parity.

- **Failure signatures**:
  - Insufficient diversity in generated data → poor coverage → residual leakage.
  - Non-orthogonal forget/retain LoRAs → utility drop during merges.
  - No or noisy relevance oracle → weak supervision for instruction optimization.
  - Over-aggressive μ values → fluency degradation (higher PPL).

- **First 3 experiments**:
  1. Reproduce toxicity detox on RealToxicityPrompts with self-generated forget data; validate Table 2 trends (PPL, toxicity metrics).
  2. Ablate internal vs. external forget data (Table 5): match forget quality and compare PPL to verify alignment benefits.
  3. Sweep μ thresholds (60% vs 90%) on CodeUnlearn and log iterations until convergence; confirm Figure 4 behavior in your infra.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can instruction optimization be improved to reliably surface a model's complete internal knowledge about an unlearning target?
- **Basis**: The authors note that "the quality of the resulting instructions is not always ideal" and that further research is needed to improve instruction optimization and better understand how to guide models in surfacing knowledge relevant to specific unlearning targets.
- **Why unresolved**: The NeuralUCB-based approach optimizes for relevance and diversity but provides no guarantees of completeness. Some internal knowledge may remain unrevealed if the search fails to discover the right prompts.
- **What evidence would resolve it**: A systematic study comparing different instruction optimization strategies with oracle access to all model knowledge, measuring recall of surfaced versus total relevant knowledge.

### Open Question 2
- **Question**: Can merge weight selection for iterative unlearning be fully automated without manual evaluation?
- **Basis**: The Limitations section notes that "hyperparameter tuning currently requires trial-and-error over multiple runs. Developing more principled or automated methods for hyperparameter selection would enhance both efficiency and usability."
- **Why unresolved**: Current rule-based selection (90% forgetting threshold, 95% utility recovery) still requires manual validation and may not generalize across tasks.
- **What evidence would resolve it**: A learnable or adaptive weight selection mechanism that achieves comparable forget-retain tradeoffs without human intervention across diverse tasks.

### Open Question 3
- **Question**: What is the precise relationship between forget data quality (relevance, diversity) and unlearning effectiveness?
- **Basis**: The Conclusion states that findings "open new directions for studying the relationship between forget data quality and unlearning effectiveness."
- **Why unresolved**: The paper shows internal data outperforms external data but does not isolate which quality dimensions matter most or establish quantitative relationships.
- **What evidence would resolve it**: Controlled ablations varying relevance and diversity independently while measuring downstream unlearning metrics, yielding a predictive model of effectiveness.

## Limitations

- **Oracle dependence**: The method's effectiveness hinges on the existence and reliability of a relevance oracle, which may be noisy, biased, or unavailable for some targets.
- **Generalization limits**: While evaluated on toxicity, NER, and coding, robustness to more complex, entangled, or long-tail knowledge removal remains unclear.
- **Iterative overhead**: The iterative framework introduces computational overhead and sensitivity to evaluation noise, with limited analysis of convergence behavior under varying thresholds or across more diverse tasks.

## Confidence

- **High Confidence**: Core mechanism of iterative PEM composition with orthogonal LoRA merging (supported by Table 1 eigenbasis similarity evidence).
- **Medium Confidence**: Self-generated data via NeuralUCB yields better alignment and lower perplexity than external data (Table 5 trends observed, but oracle dependence is a limiting factor).
- **Low Confidence**: Rule-based merge weight selection reliably balances forget and utility across all tasks (evidence is task-specific and threshold sensitivity not fully explored).

## Next Checks

1. **Oracle Robustness Test**: Run the full unlearning pipeline using a deliberately noisy or biased relevance oracle (e.g., inject random errors or systematic bias). Measure the degradation in forget quality and data diversity to quantify sensitivity.

2. **Cross-Domain Generalization**: Apply the method to a knowledge removal task outside the three evaluated domains (e.g., multilingual named entity recognition or domain-specific reasoning). Compare self-generated vs. external data performance to assess generalizability limits.

3. **Threshold Sensitivity Analysis**: Systematically vary the forgetting (≥90%) and utility (≥95%) thresholds across tasks and record iteration counts and final metric trade-offs. Identify regimes where strict thresholds cause convergence failure or utility collapse.