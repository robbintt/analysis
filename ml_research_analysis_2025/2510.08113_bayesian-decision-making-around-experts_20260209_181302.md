---
ver: rpa2
title: Bayesian Decision Making around Experts
arxiv_id: '2510.08113'
source_url: https://arxiv.org/abs/2510.08113
tags:
- expert
- information
- data
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how Bayesian learners should optimally
  incorporate expert data in multi-armed bandits. The authors formalize two settings:
  (i) offline pre-training with expert datasets and (ii) simultaneous learning where
  the agent chooses between its own experience and expert outcomes at each step.'
---

# Bayesian Decision Making around Experts

## Quick Facts
- **arXiv ID**: 2510.08113
- **Source URL**: https://arxiv.org/abs/2510.08113
- **Reference count**: 29
- **Primary result**: Expert data reduces Thompson Sampling regret bounds by decreasing entropy of optimal action distribution, with mutual information quantifying the improvement

## Executive Summary
This paper develops a Bayesian framework for agents to optimally incorporate expert data in multi-armed bandit settings. The authors prove that expert data improves Thompson Sampling regret bounds by reducing the entropy of the optimal action distribution, with the improvement quantified by mutual information between expert outcomes and the optimal action. They introduce two settings: offline pre-training with expert datasets and simultaneous learning where the agent chooses between its own experience and expert outcomes at each step. The framework extends to imperfect experts by modeling their policy with particle-based inference, enabling agents to filter unreliable information while retaining benefits from trustworthy sources.

## Method Summary
The method centers on Thompson Sampling extended with expert data incorporation. For offline learning, expert outcomes update the posterior over environment parameters, tightening regret bounds by reducing entropy of the optimal action distribution. For simultaneous learning, the agent computes mutual information between candidate data sources and the optimal action, selecting the source that maximizes information gain. The framework handles imperfect experts by maintaining a joint posterior over environment parameters and expert policy using particle filtering, allowing natural downweighting of unreliable sources. MI estimation uses Monte Carlo sampling with predictive densities computed from the particle set.

## Key Results
- Expert data improves Thompson Sampling regret bounds proportionally to mutual information between expert outcomes and optimal action
- Information-directed selection rule maximizes one-step information gain about optimal action, reducing posterior entropy
- Joint modeling of environment and expert policy enables filtering of unreliable expert data while preserving benefits from trustworthy sources
- Dramatic regret improvements in asymmetric bandit problems, with near-zero regret when expert data identifies optimal action

## Why This Works (Mechanism)

### Mechanism 1: Entropy Reduction via Expert Posterior Update
Expert outcomes constrain the posterior to parameters whose optimal-action distributions match observations. This reduces H(A*), and since TS regret bounds scale with √H(A*), regret decreases accordingly. The improvement is quantified by mutual information I(A*;D*), with I=0 in symmetric worlds where all θ induce identical optimal-action distributions.

### Mechanism 2: Mutual Information Maximization for Source Selection
At each step, the agent computes I(A*;Y) for self-outcomes and I(A*;Y*) for expert outcomes, selecting argmax to maximize one-step uncertainty reduction about A*. This directly improves subsequent regret by ensuring the most informative data source is chosen.

### Mechanism 3: Robust Learning via Expert Policy Modeling
Joint belief P(θ, πe) over environment and expert policy enables filtering unreliable expert data while retaining benefits from trustworthy sources. Particles represent uncertainty over πe with Dirichlet priors, and MI estimates marginalize over πe, naturally downweighting uninformative sources.

## Foundational Learning

**Concept: Thompson Sampling**
- **Why needed here:** The entire framework extends TS; regret bounds derive from TS information-theoretic analysis
- **Quick check question:** Why does TS sample actions proportional to P(A*=a), and how does this connect to regret bounding via √(H_t(A*)T)?

**Concept: Mutual Information via KL Divergence**
- **Why needed here:** Source selection and regret reduction both require computing I(A*;Y) = Σ_a P(a)D_KL(P(Y|A*=a) || P(Y))
- **Quick check question:** Given predictive distributions P(Y|A*=a) for each action and marginal P(Y), compute I(A*;Y)

**Concept: Particle-based Bayesian Inference**
- **Why needed here:** Intractable posteriors require particle approximation for both belief updates and MI estimation
- **Quick check question:** How do particle weights update given a likelihood, and when should resampling trigger?

## Architecture Onboarding

**Component map:**
- Prior/Posterior Module: Particle store {(θ^(k), w^(k))} [+ πe^(k) if modeling trust]
- Policy Computer: πt(a) = Σ_{k: θ^(k)∈Θ*_a} w^(k)
- Predictive Density Estimator: P_t(Y|A*=a'), P_t(Y|A_t=a), P_t(Y*_t) via particle averaging
- MI Estimator: Monte Carlo estimation of equations (6) and (7) with L outcome samples
- Source Selector: Compare I_e vs I_s; route to appropriate updater
- Posterior Updater: Weight update via likelihood (self: p_{θ,At}; expert: Σ_a πe(a)p_{θ,a})

**Critical path:**
1. Initialize particles from P_0(θ) [and P_0(πe)]
2. Compute π_t from particle weights
3. Estimate predictives, compute I_e and I_s (Algorithm 1)
4. Select d_t = argmax{I_e, I_s}
5. Update particle weights per observed data
6. Resample if ESS below threshold; loop

**Design tradeoffs:**
- Particle count: More particles → better approximation but O(K·L·|A|) MI cost per decision
- MI samples (L): Higher L reduces variance but increases computation
- Trust modeling: Joint (θ, πe) inference doubles state but enables robustness to imperfect experts

**Failure signatures:**
- Linear regret in asymmetric worlds: MI estimates erroneously zero → check predictive density implementation
- Increasing regret with adversarial expert: Naive trust active → verify Algorithm 2 engaged
- No learning: Particle collapse → check resampling threshold and prior spread
- Wrong action selection: π_t(a) incorrect → verify Θ*_a partition logic

**First 3 experiments:**
1. Symmetric bandit sanity check: M=500 bandits with identical optimal distributions shuffled across actions; confirm I_e≈0 and self-only learning
2. Strongly asymmetric test: θ* has deterministic optimal outcome y*; verify single expert sample yields near-zero regret
3. Boundedly rational expert: Expert optimal with p=0.5, random otherwise; compare naive vs. opponent-modeling agent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the regret bounds and information-theoretic guarantees extend to continuous parameter spaces with function approximation?
- Basis in paper: "Parts of our analysis were conducted in countable parameter and action spaces, which enabled exact posterior updates. Extending this framework to continuous spaces with function approximation is a significant next step."
- Why unresolved: The mutual information computations and posterior consistency proofs rely on countable Θ, and function approximation introduces approximation errors not accounted for in current bounds.

### Open Question 2
- Question: Can the expert data incorporation framework be extended to Markov Decision Processes while maintaining information-theoretic regret guarantees?
- Basis in paper: "as well as extending the general problem class to state-based Markov Decision Processes"
- Why unresolved: MDPs introduce state-dependent optimal actions and temporal credit assignment, making the mutual information between expert outcomes and optimal policies more complex to quantify.

### Open Question 3
- Question: How should a learner incorporate expert data when the expert is simultaneously learning and their policy evolves over time?
- Basis in paper: "Most critically, future work will include simultaneous learning settings, where the expert's policy is not static, but is evolving as the expert learns from their own experiences too."
- Why unresolved: The current framework assumes a static expert policy πe; non-stationary experts require tracking belief dynamics over time-varying policies while disentangling environmental learning from policy drift.

### Open Question 4
- Question: What are the sample complexity and convergence guarantees for the particle-based joint posterior inference over (θ, πe) in the untrustworthy expert setting?
- Basis in paper: The paper notes the joint posterior update "is in general intractable" and uses particle approximations, but provides no theoretical analysis of approximation quality or particle requirements.
- Why unresolved: The MI estimation and trust inference depend critically on posterior accuracy, yet no bounds characterize how many particles suffice or when the approximation fails.

## Limitations
- Effectiveness depends on identifiability of θ from optimal-action distributions; zero information gain in symmetric worlds limits regret improvement
- Particle-based MI estimation introduces approximation error that grows with outcome space size and particle count
- Joint modeling approach assumes static expert policies and may struggle with adaptive adversaries
- Theoretical analysis assumes discrete action spaces and known outcome spaces, limiting extension to continuous domains

## Confidence

**High confidence:**
- Mechanism 1 (Entropy reduction): The theoretical derivation is rigorous and the mutual information connection to regret bounds is well-established

**Medium confidence:**
- Mechanism 2 (Source selection): While the optimization principle is sound, practical MI estimation via sampling introduces approximation uncertainty not fully characterized

**Low confidence:**
- Mechanism 3 (Robust learning): The approach is conceptually promising but lacks theoretical guarantees for adversarial experts, and the particle filter may struggle with high-dimensional expert policy spaces

## Next Checks

1. **Symmetric world stress test:** Create 100 bandits with identical optimal-action distributions but different parameter values. Verify that MI estimates converge to zero and regret matches standard Thompson Sampling baseline.

2. **MI estimation variance analysis:** For Algorithm 1, run experiments varying L (number of outcome samples) from 10 to 1000. Plot MI estimate variance and source selection stability to quantify approximation error impact.

3. **Adversarial expert robustness:** Design experiments with adversarial experts that inject ϵ-fraction of corrupted data. Test whether the opponent-modeling approach maintains performance better than naive trust, and identify the ϵ threshold where performance degrades.