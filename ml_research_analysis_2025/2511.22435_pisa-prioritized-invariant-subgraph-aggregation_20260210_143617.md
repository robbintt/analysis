---
ver: rpa2
title: 'PISA: Prioritized Invariant Subgraph Aggregation'
arxiv_id: '2511.22435'
source_url: https://arxiv.org/abs/2511.22435
tags:
- graph
- graphs
- invariant
- subgraphs
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PISA, a framework for learning invariant
  subgraphs to improve out-of-distribution (OOD) generalization on graph data. While
  prior methods like CIGA learn a single invariant subgraph, PISA learns multiple
  diverse invariant subgraphs in parallel and uses a dynamic MLP-based aggregator
  to prioritize and combine them.
---

# PISA: Prioritized Invariant Subgraph Aggregation

## Quick Facts
- arXiv ID: 2511.22435
- Source URL: https://arxiv.org/abs/2511.22435
- Reference count: 19
- Key outcome: PISA achieves up to 5% higher classification accuracy than prior methods on 15 graph OOD benchmarks through learning multiple diverse invariant subgraphs and using dynamic MLP aggregation

## Executive Summary
This paper introduces PISA, a framework for learning invariant subgraphs to improve out-of-distribution (OOD) generalization on graph data. While prior methods like CIGA learn a single invariant subgraph, PISA learns multiple diverse invariant subgraphs in parallel and uses a dynamic MLP-based aggregator to prioritize and combine them. The method injects diversity through stochastic subgraph sampling and a diversity regularizer, then trains an MLP to adaptively combine subgraph predictions during inference. Experiments on 15 synthetic and real-world datasets, including DrugOOD, show PISA achieves up to 5% higher classification accuracy than prior methods. Ablation studies confirm the effectiveness of the dynamic aggregation and reveal that maintaining branch independence preserves diversity.

## Method Summary
PISA extends CIGA by training n parallel branches with stochastic subgraph sampling and diversity regularization, then using an MLP to dynamically aggregate predictions. Phase I trains all branches jointly with supervised contrastive alignment on same-label subgraphs, diversity penalty on edge-importance scores, and complement control. Phase II freezes branches and trains a lightweight MLP (3-4 layers) to combine branch predictions. Independent branches by default capture diverse invariant subgraphs, while the MLP learns instance-specific weighting. The approach addresses limitations of static aggregation and demonstrates state-of-the-art OOD performance across diverse distribution shifts in graph data.

## Key Results
- Achieves up to 5% higher classification accuracy than prior methods on 15 graph OOD benchmarks
- Dynamic MLP aggregation (3-4 layers) outperforms static averaging and greedy selection across all tested datasets
- Maintaining branch independence preserves diversity; partial parameter sharing leads to collapse
- Ablation studies confirm stochastic sampling and diversity regularizer are both essential for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Stochastic subgraph sampling combined with explicit diversity regularization forces parallel branches to capture complementary invariant subgraphs rather than converging to identical shortcuts. Each branch receives a perturbed graph view (edges dropped stochastically). The diversity regularizer penalizes similarity between edge-importance scores across branches via dot product δ{gϕ1, gϕ2}(G) = gϕ1(G)·gϕ2(G). This dual pressure—different inputs plus explicit differentiation—increases coverage of distinct causal structures.

### Mechanism 2
- Contrastive alignment of same-label subgraph embeddings isolates invariant content from environment-varying spurious structure. The supervised contrastive term pulls together embeddings from same-label graphs while separating different-label embeddings. This approximates mutual information maximization within classes. A complement head trained on Ĝs = G − Ĝc is constrained to remain less predictive than the main classifier, preventing spurious leakage.

### Mechanism 3
- A moderately deep MLP aggregator (3–4 layers) captures non-linear branch interactions and instance-specific weighting, outperforming static averaging. Branch predictions are stacked into matrix P ∈ R^(n×C). The MLP learns adaptive weighting—suppressing redundant branches, amplifying informative ones based on input context. Deeper MLPs model interactions like soft gating and conditional dependence between subgraphs.

## Foundational Learning

- Concept: **Structural Causal Models (SCMs) for graphs**
  - Why needed here: PISA's theoretical justification relies on decomposing graphs into invariant content Gc and spurious Gs components. Understanding FIIF vs. PIIF interaction modes explains why single-subgraph methods fail.
  - Quick check question: Given a molecule where binding affinity depends on two functional groups but training data correlates scaffold with label, which SCM type applies?

- Concept: **Supervised contrastive learning**
  - Why needed here: The core CIGA objective uses contrastive loss to align same-label subgraph embeddings. Understanding temperature scaling, negative sampling, and mutual information bounds is essential for debugging Phase I.
  - Quick check question: If two same-label graphs have dissimilar learned subgraphs, which component of the contrastive loss is failing?

- Concept: **OOD generalization vs. domain adaptation**
  - Why needed here: PISA targets worst-case risk on unseen environments without environment labels at training time. Distinguishes it from methods requiring explicit domain annotation.
  - Quick check question: Why does ERM perform competitively on some DrugOOD splits but catastrophically on CMNIST-sp?

## Architecture Onboarding

- Component map: Input graph → stochastic sampler → n parallel (featurizer + classifier) branches → stack predictions → MLP aggregator → output
- Critical path: Input graph → stochastic sampler → n parallel (featurizer + classifier) branches → stack predictions → MLP aggregator → output. Phase I trains branches; Phase II freezes branches and trains only MLP.
- Design tradeoffs: More branches (n) → better coverage but linear compute/memory scaling. Full parameter sharing (n ≤ 3) → lower memory but capacity saturation; independent branches → higher diversity but more parameters. MLP depth 3–4 layers balances expressivity vs. OOD overfitting.
- Failure signatures: All branches select nearly identical subgraphs → diversity regularizer weight β too low or sampling too conservative. MLP aggregator overfits training environments → reduce depth, add dropout/weight decay, early stopping. Performance drops on SUMotif but not SPMotif → model capturing only one causal motif; increase branch count.
- First 3 experiments: 1) Sanity check: Run PISA with n=1 branch (equivalent to CIGA) on SPMotif bias=0.9. Should match CIGA baseline ~53%. Then n=3 to verify improvement to ~84%. 2) Ablation path: On SUMotif bias=0.6, compare: (a) uniform aggregation, (b) greedy selection, (c) MLP aggregation. Quantify diversity via average pairwise δ scores. 3) Depth sweep: On DrugOOD EC50-assay, sweep MLP layers from 1–5. Plot accuracy vs. depth; confirm 3–4 layer peak and document overfitting symptoms at depth 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can principled partial parameter sharing strategies (e.g., low-rank adapters or orthogonality constraints) match or exceed the performance of independent branches while reducing computational overhead?
- Basis in paper: The authors state in Section 7.2 and the Conclusion that "improved partial-sharing schemes... could match or beat independent branches... and we leave this as an avenue for future work."
- Why unresolved: The simple partial sharing tested (shared backbone, branch-specific attention) failed due to capacity saturation, diversity erosion, and optimization coupling, limiting the ability to reuse parameters effectively.
- What evidence would resolve it: A new architecture that successfully shares parameters without inducing branch collapse, achieving comparable accuracy to the default independent setup on the SUMotif or DrugOOD benchmarks.

### Open Question 2
- Question: How can the dynamic MLP aggregator be redesigned to be intrinsically "lighter" or "calibrated" to prevent overfitting to training environment idiosyncrasies?
- Basis in paper: The Conclusion lists "lighter or calibrated aggregation modules" as a future direction, prompted by findings in Section 7.1 where deep aggregators caused overfitting.
- Why unresolved: The paper demonstrates that increasing MLP depth improves expressivity but eventually degrades OOD generalization due to "over-parameterization under shift" and calibration drift.
- What evidence would resolve it: Implementing an aggregator that maintains robust OOD performance with increased depth or complexity without relying on heavy regularization or early stopping.

### Open Question 3
- Question: How can PISA be adapted to handle heterogeneous or temporal graphs where distribution shifts may manifest differently across varying edge types or time steps?
- Basis in paper: The Conclusion explicitly cites "applications to heterogeneous or temporal graphs" as a distinct area for future research.
- Why unresolved: The current methodology and experiments focus exclusively on static, homogeneous graph structures (e.g., molecules), leaving the handling of dynamic or multi-relational data unexplored.
- What evidence would resolve it: Extending the subgraph sampling and aggregation mechanisms to support temporal snapshots or edge-type conditioning and evaluating the framework on dynamic graph benchmarks.

## Limitations

- Hyperparameter sensitivity remains a concern—optimal β and sampling rate appear dataset-dependent
- The "multiple invariant subgraphs" assumption may not hold in all domains—some graphs may have only one true invariant structure
- The two-phase training doubles computational cost compared to single-phase methods

## Confidence

- **High Confidence**: The core PISA architecture (parallel diverse branches + dynamic MLP aggregation) is technically sound and the empirical improvements on 15 datasets are robust across multiple runs.
- **Medium Confidence**: The claim that stochastic sampling + diversity regularizer guarantees complementary subgraph coverage. While the mechanism is plausible, the theoretical guarantee is weak.
- **Low Confidence**: The assertion that 3-4 MLP layers are universally optimal. This appears dataset-specific, and the overfitting claims at depth 5 could be mitigated with stronger regularization rather than reduced depth.

## Next Checks

1. **Sensitivity Analysis**: Systematically vary n (1-20 branches), β (0.01-1.0), and sampling rate (0.5-0.95) on 3 representative datasets to quantify robustness and identify true hyperparameter ranges.

2. **Single-Invariant Domain Test**: Design a synthetic benchmark where only one invariant subgraph exists. Test whether PISA still improves over CIGA or if it overfits spurious structures.

3. **Diversity Collapse Monitoring**: Implement real-time monitoring of pairwise δ scores during Phase I training. Quantify how often diversity collapses despite regularization and under what hyperparameter conditions.