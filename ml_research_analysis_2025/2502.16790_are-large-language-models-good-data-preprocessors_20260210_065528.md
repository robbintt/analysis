---
ver: rpa2
title: Are Large Language Models Good Data Preprocessors?
arxiv_id: '2502.16790'
source_url: https://arxiv.org/abs/2502.16790
tags:
- captions
- data
- llms
- blip
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models (LLMs) can
  effectively preprocess noisy textual data from image captioning models (BLIP and
  GIT) for multimodal persuasion detection in memes. The researchers evaluated three
  LLMs (LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2) on their ability to clean and
  refine captions, then measured downstream task performance using a T5 model.
---

# Are Large Language Models Good Data Preprocessors?

## Quick Facts
- arXiv ID: 2502.16790
- Source URL: https://arxiv.org/abs/2502.16790
- Reference count: 11
- Large language models can modestly improve noisy image caption quality for downstream persuasion detection tasks

## Executive Summary
This study evaluates whether large language models can effectively preprocess noisy textual data from image captioning models for multimodal persuasion detection in memes. The researchers investigate three LLMs (LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2) on their ability to clean and refine captions generated by BLIP and GIT models. The cleaned captions are then used as input for a T5 model to detect persuasion techniques in memes, with performance measured using hierarchical F1 scores.

Results show modest improvements in F1 scores when using LLM-cleaned captions versus uncleaned captions, though the effectiveness varies significantly by model and dataset. GPT-4 Turbo achieved the highest development set performance, while LLaMA 70B showed the strongest test set results. Statistical significance testing revealed only GPT-4 Turbo-cleaned BLIP captions produced significant improvements, suggesting that while LLMs can enhance data quality in preprocessing pipelines, their benefits are context-dependent and relatively modest.

## Method Summary
The researchers conducted experiments on the SemEval 2024 task 4 dataset, which contains 7,000 training, 1,000 development, and 1,500 test images with hierarchical persuasion labels. Image captions were first generated using BLIP and GIT models, then cleaned by three different LLMs using a specific prompt that fixes grammar, removes repetition, and marks unrecoverable captions as "INVALID DESCRIPTION." The cleaned captions were combined with original meme text and used to fine-tune a T5 model for hierarchical multi-label classification. Performance was evaluated using hierarchical F1 score, with statistical significance assessed via Wilcoxon Signed-Rank Test with Benjamini-Hochberg correction.

## Key Results
- GPT-4 Turbo achieved the highest development set performance with F1 score of 65.85
- LLaMA 70B showed the strongest test set results with F1 score of 58.87
- Only GPT-4 Turbo-cleaned BLIP captions showed statistically significant improvement over uncleaned baselines
- Caption retention rates varied dramatically: LLaMA (~95-100%), GPT-4 (~70%), Sonnet (67-90%)

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Hierarchical F1 Score**: A metric that evaluates multi-label classification performance while accounting for label hierarchy. Needed because persuasion detection involves parent-child label relationships. Quick check: Verify metric implementation matches official SemEval evaluation script.
- **Image Captioning Models**: BLIP and GIT generate initial textual descriptions from images. Needed as source of noisy data requiring cleaning. Quick check: Confirm captions are actually noisy (grammatical errors, repetitions).
- **LLM-based Text Cleaning**: Using prompts to fix grammar and remove repetition. Needed to transform noisy captions into cleaner training data. Quick check: Inspect random samples of cleaned vs uncleaned captions.
- **T5 Fine-tuning for Multi-label Classification**: Adapting sequence-to-sequence model for hierarchical label prediction. Needed to perform downstream persuasion detection. Quick check: Verify label encoding preserves hierarchical relationships.

## Architecture Onboarding

**Component Map**
SemEval Dataset -> BLIP/GIT Captioning -> LLM Cleaning -> T5 Training -> Hierarchical F1 Evaluation

**Critical Path**
Caption generation (BLIP/GIT) → LLM cleaning → T5 fine-tuning → Evaluation with hierarchical F1

**Design Tradeoffs**
- Using generated captions vs. human annotations: Lower quality but scalable
- LLM selection: GPT-4 Turbo performs best but has lowest retention rate
- Caption retention: Higher retention (LLaMA) vs. quality of cleaning (GPT-4)

**Failure Signatures**
- Poor retention rates indicate captions too noisy for recovery
- Inconsistent F1 improvements across models suggest preprocessing benefits are dataset-specific
- Statistical insignificance of most comparisons indicates minimal practical impact

**3 First Experiments**
1. Compare cleaned vs uncleaned caption quality using automated metrics (BLEU, GLEU)
2. Test different LLM prompts for cleaning effectiveness
3. Vary T5 hyperparameters to establish baseline performance variance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements are modest (1-2 F1 points) and inconsistent across models
- Statistical significance found only for one comparison (GPT-4 Turbo on BLIP captions)
- Results may be inflated by unreported T5 hyperparameters and label encoding choices
- Caption retention rates vary dramatically across LLMs, raising consistency concerns

## Confidence

**High Confidence**: Experimental methodology is clearly specified; observation of modest and inconsistent improvements is robust.

**Medium Confidence**: Specific F1 scores depend on unspecified T5 hyperparameters and label encoding.

**Low Confidence**: Practical utility claim that LLMs "enhance data quality" is not strongly supported given limited improvements.

## Next Checks

1. Re-run T5 training with multiple random seeds to determine if reported improvements exceed training noise
2. Conduct ablation studies varying only the caption cleaning while keeping T5 training identical
3. Test the same cleaning pipeline on a different multimodal dataset to assess generalizability