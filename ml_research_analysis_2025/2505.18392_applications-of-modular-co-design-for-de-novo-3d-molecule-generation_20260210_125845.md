---
ver: rpa2
title: Applications of Modular Co-Design for De Novo 3D Molecule Generation
arxiv_id: '2505.18392'
source_url: https://arxiv.org/abs/2505.18392
tags:
- diffusion
- megalodon
- structure
- molecule
- molecules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Megalodon is a scalable transformer-based architecture for de novo
  3D molecule generation that achieves state-of-the-art performance across multiple
  benchmarks. The model uses a modular co-design approach with basic equivariant layers
  trained on a joint continuous and discrete denoising objective, enabling it to simultaneously
  generate valid molecular structures and 2D topologies.
---

# Applications of Modular Co-Design for De Novo 3D Molecule Generation

## Quick Facts
- arXiv ID: 2505.18392
- Source URL: https://arxiv.org/abs/2505.18392
- Reference count: 31
- Primary result: Megalodon achieves state-of-the-art de novo 3D molecule generation with up to 49× better validity on large molecules and 2-10× lower energy than prior models

## Executive Summary
Megalodon introduces a modular co-design approach for de novo 3D molecule generation that simultaneously produces valid molecular structures and 2D topologies. The model uses specialized modules for discrete topology (atom types, bonds) and continuous 3D coordinates, coupled through a joint denoising objective. Key innovations include a fused invariant transformer block for discrete data and independent time variables for discrete and continuous interpolation. When scaled to 40M parameters, Megalodon demonstrates strong generalization to molecules outside the training distribution, generating up to 49× more valid large molecules and achieving energy levels 2-10× lower than prior models. The architecture supports both diffusion and flow matching objectives, with diffusion excelling at structure accuracy and flow matching offering faster inference.

## Method Summary
Megalodon uses a modular architecture with separate processing for discrete molecular topology and continuous 3D coordinates. The core consists of fused invariant transformer (FiT) blocks that update discrete invariant data through multi-head attention, coupled with EGNN layers that update 3D structure while maintaining equivariance. The model is trained on a joint continuous and discrete denoising objective using either diffusion (DDPM + D3PM) or flow matching (CFM) approaches. A novel dual-time sampling strategy independently noises discrete and continuous components, forcing the model to learn robust coupling between 2D topology and 3D structure. The architecture supports conditional structure generation given 2D topology without retraining, achieving 71.4% coverage compared to EQGAT's 0.8%.

## Key Results
- Achieves 2-10× lower xTB relaxation energy than prior models
- Generates up to 49× more valid large molecules (>72 atoms)
- First model capable of unconditional generation and conditional structure generation without retraining
- Demonstrates strong generalization to molecules outside training distribution
- Supports both diffusion (500 steps, better accuracy) and flow matching (100 steps, faster inference)

## Why This Works (Mechanism)

### Mechanism 1: Modular Co-Design with Separate Discrete and Continuous Processing
The architecture separates discrete topology from continuous structure into specialized modules, using a fused invariant transformer block for discrete data and EGNN layers for structure updates. This specialization improves learning of molecular generation dynamics by allowing each modality to be processed optimally.

### Mechanism 2: Independent Time Variables for Discrete and Continuous Interpolation
Instead of a single shared time variable, the model samples separate t_continuous and t_discrete from the same distribution. This forces the model to learn relationships between discrete and continuous data across diverse interpolation states, improving conditional generation performance.

### Mechanism 3: Transformer-Based Discrete Modeling for Generalization
The transformer trunk models discrete molecular topology through attention mechanisms, capturing compositional patterns that generalize beyond memorized examples. This enables superior performance on molecules outside the training distribution, particularly larger molecules that represent only 1% of training data.

## Foundational Learning

### Concept: Equivariance and SE(3) Symmetry
Why needed: Molecular 3D coordinates must respect rotational and translational symmetries - predictions shouldn't change if a molecule is rotated. Megalodon uses EGNN layers with equivariant updates to ensure geometric consistency.
Quick check: If you rotate a molecule's coordinates by 90°, should the predicted atom types change? Should the predicted next coordinates change, and if so, how?

### Concept: Diffusion vs Flow Matching Objectives
Why needed: Megalodon supports both. Diffusion uses stochastic SDE simulation requiring ~500 steps; flow matching uses deterministic ODE solving with ~100 steps. Diffusion excels at structure accuracy and energy benchmarks, while flow matching offers faster inference and better 2D stability.
Quick check: Why can flow matching use 25x fewer inference steps than diffusion while still producing valid molecules? What trade-off might this introduce?

### Concept: Discrete Diffusion (D3PM) with Transition Matrices
Why needed: Unlike continuous coordinates that can be Gaussian-noised, atom/bond types are categorical. D3PM uses predefined transition matrices Q_t that progressively corrupt discrete distributions toward a uniform prior. The reverse process learns categorical denoising.
Quick check: In continuous diffusion, noise is added via x_t = α(t)ε + β(t)x_1. How does discrete diffusion add "noise" to a one-hot atom type vector?

## Architecture Onboarding

### Component map:
Input Embedding (coordinates, atom types, bond types, charges) → Fused Invariant Transformer Block (×10) → Structure Layer (EGNN with cross-product) → Output Heads (discrete logits, bond refinement)

### Critical path:
Noisy molecule → separate embeddings → (FiT → structure layer) × N → output projections → predicted clean molecule

### Design tradeoffs:
- Diffusion vs Flow Matching: Diffusion (500 steps) achieves 2-10× lower energy; Flow matching (100 steps) is faster with better 2D stability
- Model scale: Quick (19M) = 44× throughput for large molecules; Full (40M) = better energy accuracy
- Edge features: N² memory cost but ablation shows severe degradation without them (Validity 0.223)
- Cross-product term in structure layer: Critical for model performance

### Failure signatures:
1. Models generate no bonds for t ≤ 0.5 (indicates discrete and continuous tracks aren't informing each other)
2. Sharp performance drop for molecules >72 atoms (suggests memorization rather than compositional learning)
3. High relaxation energy (∆E_relax >10 kcal/mol indicates structures far from physical energy minima)
4. Unconditional model cannot be prompted with 2D topology to generate accurate 3D structures

### First 3 experiments:
1. Replace FiT blocks with standard EGNN invariant updates. Measure validity across molecule size buckets (30, 60, 90, 120 atoms). Expect: large degradation for big molecules.
2. Train with t_continuous = t_discrete vs independent sampling. Evaluate conditional structure generation (RMSD, coverage). Expect: shared time degrades conditional performance.
3. Remove cross-product from structure layer equation. Measure bond angle/dihedral errors and energy metrics. Expect: significant degradation in 3D structure quality.

## Open Questions the Paper Calls Out

### Open Question 1
Can the N² edge feature requirement be reduced to improve memory efficiency while maintaining energy accuracy? The paper notes this is computationally expensive but leaves exploration for future work.

### Open Question 2
Would further scaling beyond 40M parameters continue to improve performance on larger molecules and energy benchmarks? The authors suggest current scaling is limited and more benchmarks could enable larger models.

### Open Question 3
Does the variance scaling in flow matching fundamentally limit spatial precision for bond lengths and angles? The authors hypothesize this may explain flow matching's higher relaxation energies.

## Limitations
- Dual-time sampling strategy requires twice as many noise samples per batch, with unclear empirical cost-benefit tradeoff
- Architecture generalization superiority relies primarily on EQGAT-diff comparisons without ablation studies of alternative architectures
- All benchmarks use GEOM-Drugs and QM9 datasets; performance on genuinely novel chemical space remains untested

## Confidence
**High Confidence**: Modular co-design improves learning efficiency; independent time variables improve conditional generation; fused invariant transformer effectively models discrete topology; edge features and cross-product terms are critical.

**Medium Confidence**: Transformer-based modeling provides superior generalization to out-of-distribution molecules; diffusion achieves 2-10× lower energy than flow matching; flow matching's faster inference maintains competitive quality; self-conditioning wrapper stabilizes training.

**Low Confidence**: Dual-time sampling is optimal compared to alternative coupling mechanisms; specific hyperparameter choices are near-optimal; model will generalize equally well to all chemical domains beyond drug-like molecules.

## Next Checks
1. Systematically vary model size (2M, 10M, 40M, 80M parameters) and measure validity across molecule size buckets to determine whether transformer advantage persists at smaller scales.

2. Replace transformer trunk with pure EGNN invariant update while keeping all other components identical, train on identical data, and compare performance on out-of-distribution molecules (>72 atoms).

3. Train identical models with three different time sampling strategies: dual independent sampling, shared time variable, and conditional sampling where t_discrete = f(t_continuous), then compare conditional structure generation accuracy and unconditional validity.