---
ver: rpa2
title: 'Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation'
arxiv_id: '2507.15396'
source_url: https://arxiv.org/abs/2507.15396
tags:
- speech
- hearing
- neuro-msbg
- loss
- msbg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency and lack of
  real-time capabilities in existing hearing loss simulation models like MSBG, which
  hinder their integration into modern speech processing pipelines. To overcome these
  limitations, the authors propose Neuro-MSBG, a lightweight end-to-end neural model
  that incorporates a personalized audiogram encoder and jointly models magnitude
  and phase spectra.
---

# Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation

## Quick Facts
- arXiv ID: 2507.15396
- Source URL: https://arxiv.org/abs/2507.15396
- Authors: Hui-Guan Yuan; Ryandhimas E. Zezario; Shafique Ahmed; Hsin-Min Wang; Kai-Lung Hua; Yu Tsao
- Reference count: 30
- Key outcome: Neuro-MSBG achieves 46× faster inference than MSBG while maintaining high simulation fidelity (STOI SRCC 0.9247, PESQ SRCC 0.8671) and successfully integrates into speech compensator pipelines.

## Executive Summary
Neuro-MSBG is a lightweight neural model that simulates hearing loss effects in real time, addressing the computational inefficiency of traditional MSBG-based approaches. By jointly modeling magnitude and phase spectra with a personalized audiogram encoder and dual-path architecture, it achieves significant speedups while maintaining high fidelity to the original MSBG model. The model supports parallel inference and can process 1-second audio in just 0.021 seconds, making it practical for real-time hearing aid applications.

## Method Summary
The model takes normal speech waveforms and audiogram vectors as input, extracts STFT magnitude and phase, and uses a dual-path neural network with bidirectional Mamba blocks to predict hearing loss effects. A personalized audiogram encoder transforms 8-dimensional audiogram vectors into frequency-aligned representations, which are concatenated with magnitude and phase as additional input channels. Separate decoders predict magnitude masks and phase shifts, which are combined with the original STFT and reconstructed via inverse STFT to produce hearing-loss-simulated speech.

## Key Results
- Achieves 46× speedup over MSBG (0.021s vs 0.976s for 1-second audio)
- Maintains high fidelity with STOI SRCC of 0.9247 and PESQ SRCC of 0.8671
- Joint magnitude and phase prediction reduces PESQ MSE by 30× compared to magnitude-only
- Successfully integrates into speech compensator pipeline, improving HASPI from 0.428 to 0.616

## Why This Works (Mechanism)

### Mechanism 1
Jointly predicting magnitude and phase spectra substantially improves hearing loss simulation fidelity compared to magnitude-only approaches. The model dedicates separate decoders for magnitude masks and phase shifts. The phase loss combines three components using anti-wrapping functions to handle 2π discontinuities. Phase information contains perceptually relevant cues for hearing loss simulation that magnitude-only models cannot capture.

### Mechanism 2
A dual-path architecture with bidirectional Mamba blocks efficiently captures long-range dependencies in both temporal and spectral dimensions while maintaining low latency. Input tensors are rearranged separately for temporal modeling and frequency modeling, with each path containing residual connections. Mamba's selective scan mechanism enables O(n) sequence modeling without the quadratic attention cost of Transformers.

### Mechanism 3
Encoding audiograms as a frequency-aligned channel input improves personalized hearing loss modeling. The Audiogram Encoder transforms an 8-dimensional audiogram vector into a B×F representation via Conv1D → AvgPool → ReLU → Flatten → Linear projection, then broadcast along time. Channel-based integration provides spatially aligned, consistent conditioning across network layers compared to frequency-dimension concatenation.

## Foundational Learning

- **Audiogram Representation**: Audiograms are 8-dimensional vectors representing hearing thresholds at standard frequencies (250, 500, 1k, 2k, 3k, 4k, 6k, 8k Hz). Essential for understanding the encoder's output and debugging personalized simulation.
  - Quick check: Given an audiogram showing 40 dB HL at 4 kHz and 10 dB HL at 500 Hz, which frequency would the model simulate as more severely impaired?

- **STFT Magnitude and Phase Decomposition**: The model operates on STFT-derived magnitude and phase features, predicts modifications to both, and reconstructs via inverse STFT. Misunderstanding this leads to implementation errors in loss functions and reconstruction.
  - Quick check: If a complex STFT coefficient is 0.5 + 0.5j, what are its magnitude and phase values?

- **Anti-Wrapping Phase Loss**: Standard MSE fails for phase due to 2π discontinuities (phase of -π and π are equivalent but have large MSE). The paper uses faw(·) to handle this; understanding it is critical for implementing LPha correctly.
  - Quick check: Why does predicting phase with standard L2 loss produce unstable gradients near phase boundaries?

## Architecture Onboarding

- **Component map**: Normal waveform + Audiogram vector → STFT → Audiogram Encoder → Channel Concatenation (magnitude, phase, audiogram) → DenseEncoder + NN Block (Dual-Path) → Magnitude Mask Decoder + Phase Decoder → iSTFT → Hearing-loss waveform

- **Critical path**: Audiogram encoding → channel concatenation → dual-path TF processing (time axis → frequency axis) → parallel magnitude/phase decoding → iSTFT reconstruction. Delay-compensated training data is critical; without it, STOI/PESQ metrics produce spurious errors.

- **Design tradeoffs**:
  - Mamba vs. LSTM vs. CNN vs. Transformer: Mamba achieves best SRCC but requires GPU; LSTM/CNN work on CPU with comparable speed (0.016s) but lower correlation
  - Magnitude-only vs. magnitude+phase: Phase adds ~2× decoder parameters but reduces PESQ MSE by 30× (2.3579 → 0.0782)
  - Simple concatenation vs. Audiogram Encoder: Encoder adds minimal parameters but consistently improves all MSE metrics

- **Failure signatures**:
  - Magnitude-only mode: PESQ MSE >2.0, spectrograms show harmonic smearing
  - Missing audiogram encoder: STOI MSE ~0.0013 (vs. 0.0006 with encoder)
  - Misaligned training data: STOI/PESQ correlations degrade unpredictably
  - Non-GPU Mamba execution: Fails (CUDA selective scan kernel required)

- **First 3 experiments**:
  1. Reproduce magnitude-only vs. magnitude+phase ablation: Train Neuro-MSBG (Mamba) with phase decoder disabled; verify PESQ MSE degrades to ~2.3 range
  2. Compare audiogram integration strategies: Implement simple frequency-dimension concatenation vs. Audiogram Encoder; measure STOI/PESQ MSE delta on held-out test set
  3. Benchmark inference latency across NN blocks: Measure GPU inference time for Mamba, LSTM, CNN, and Transformer variants on 1-second audio; verify Mamba achieves ~0.021s and confirm LSTM/CNN achieve 0.016s

## Open Questions the Paper Calls Out
None

## Limitations

- Limited generalizability to non-stationary hearing losses: Model is trained on simulated audiograms and may not capture rapidly changing hearing profiles
- Phase reconstruction fidelity: Anti-wrapping loss functions may not perfectly preserve perceptual phase relationships; no subjective listening tests provided
- Training data dependency: Performance tightly coupled to quality and coverage of simulated training corpus

## Confidence

- **High confidence**: Magnitude+phase modeling superiority (supported by quantitative ablation showing 30× PESQ MSE reduction)
- **Medium confidence**: Mamba architecture efficiency (based on single-architecture comparison; lacks cross-model head-to-head validation)
- **Medium confidence**: Audiogram encoder design (supported by MSE improvements but lacks ablation against other integration strategies)
- **Low confidence**: Real-time clinical applicability (no user studies or deployment testing in hearing aid systems beyond synthetic integration)

## Next Checks

1. **Cross-corpus generalization test**: Evaluate Neuro-MSBG on hearing loss simulations derived from different speech corpora (e.g., LibriSpeech, TIMIT) to verify metric stability across recording conditions and speaker characteristics

2. **Extreme audiogram validation**: Systematically test model predictions on atypical audiograms (profound loss >90 dB HL, asymmetric configurations, cookie-bite patterns) to identify failure modes in the audiogram encoder and magnitude/phase prediction pathways

3. **Real-world delay compensation verification**: Measure actual algorithmic latency introduced by MSBG simulation and verify that the model's delay-compensated training data correctly aligns input and target pairs across diverse hearing loss profiles