---
ver: rpa2
title: 'seqme: a Python library for evaluating biological sequence design'
arxiv_id: '2511.04239'
source_url: https://arxiv.org/abs/2511.04239
tags:
- metrics
- sequences
- sequence
- embedding
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "seqme is a Python library for evaluating biological sequence design,\
  \ addressing the lack of unified metrics to assess fidelity to target distributions\
  \ and attainment of desired properties. It offers three types of metrics\u2014sequence-based,\
  \ embedding-based, and property-based\u2014applicable to diverse biological sequences."
---

# seqme: a Python library for evaluating biological sequence design

## Quick Facts
- arXiv ID: 2511.04239
- Source URL: https://arxiv.org/abs/2511.04239
- Reference count: 40
- Primary result: seqme provides standardized metrics and diagnostics for biological sequence design evaluation, addressing the lack of unified benchmarks in the field.

## Executive Summary
seqme is a Python library designed to evaluate biological sequence designs against reference datasets, addressing the challenge of fair benchmarking in computational biology. The library implements three categories of metrics—sequence-based, embedding-based, and property-based—to assess distributional fidelity and property attainment. By providing standardized evaluation tools, seqme enables researchers to detect diverse generative model failure modes and compare design methods across different biological domains including proteins, DNA, and RNA.

## Method Summary
seqme evaluates biological sequences using a dictionary-based API where groups of sequences are mapped to names. The library requires reference sequences for most distribution-based metrics and supports three metric categories: sequence-based (diversity, novelty), embedding-based (FBD, MMD, precision/recall), and property-based (hit-rate, hypervolume). Users initialize a Cache with embedding models, configure metrics, and call `sm.evaluate(sequences, metrics)` to obtain a DataFrame of scores. The library includes diagnostics for embedding model validation and visualization tools for result interpretation.

## Key Results
- Provides unified framework for evaluating biological sequence designs across proteins, DNA, and RNA
- Implements three complementary metric categories to detect diverse generative model failure modes
- Enables fold-based estimation to mitigate sample-size bias and quantify metric uncertainty
- Includes embedding and property model support with caching to avoid redundant computation

## Why This Works (Mechanism)

### Mechanism 1: Multi-Metric Failure Mode Detection
Using multiple complementary metrics enables detection of diverse generative model failure modes that any single metric would miss. seqme implements three metric categories (sequence-, embedding-, property-based) that expose different tradeoffs. For example, overfitting to a small set of sequences satisfying target properties reduces novelty and diversity (detectable by sequence metrics), while random baselines trivially maximize novelty/diversity without optimizing properties (detectable by property metrics).

### Mechanism 2: Embedding-Based Distribution Comparison via Learned Representations
Pre-trained biological language models enable meaningful distributional comparison of generated vs. reference sequences in continuous embedding space. Embedding models (ESM-2, RNA-FM, GENA-LM, Hyformer) map variable-length biological sequences to fixed-dimension vectors. Metrics like Fréchet Biological Distance (FBD) and Maximum Mean Discrepancy (MMD) then measure distributional similarity between generated and reference embeddings using parametric (Gaussian) or non-parametric (kernel) assumptions.

### Mechanism 3: Sample-Size Bias Mitigation via Fold-Based Estimation
Partitioning sequences into K folds and computing metrics per-fold reduces sample-size bias and enables uncertainty quantification. The Fold functionality splits generated and reference sequences into K equal-sized groups, computing metrics K times. This addresses bias in metrics like Improved Precision/Recall when |G| ≠ |R|, and provides standard deviation estimates without discarding sequences.

## Foundational Learning

- **Fréchet Inception Distance (FID) and its biological adaptation (FBD)**
  - Why needed here: FBD is a core metric in seqme for measuring distributional fidelity. Understanding that FID assumes Gaussian distributions in embedding space helps interpret when FBD is appropriate vs. when non-parametric alternatives like MMD should be used.
  - Quick check question: Given two sets of protein embeddings, would you expect them to follow multivariate Gaussian distributions? What happens to FBD if this assumption is violated?

- **Precision vs. Recall in generative models**
  - Why needed here: seqme implements Improved Precision (fraction of generated samples within reference support) and Improved Recall (fraction of reference samples within generated support). These capture distinct failure modes: low precision indicates "hallucination" outside training distribution; low recall indicates mode collapse.
  - Quick check question: A protein design model generates only sequences from its training set. Would you expect high or low precision? High or low recall?

- **Pareto fronts and Hypervolume Indicator**
  - Why needed here: Multi-objective sequence design (e.g., optimizing both stability and binding affinity) requires understanding tradeoffs. The Hypervolume Indicator quantifies the dominated region of objective space, enabling comparison across design methods.
  - Quick check question: Two design methods achieve the same maximum binding affinity, but Method A's sequences span a broader range of stability scores. Which would have higher hypervolume if both properties are maximized?

## Architecture Onboarding

- **Component map:**
```
seqme/
├── metrics/          # Core metric implementations (Diversity, FBD, MMD, Precision, Recall, etc.)
├── models/           # Embedding models (ESM2, RNAFM, GENALM) and property models
├── Cache             # Embedding/property caching to avoid redundant computation
├── evaluate()        # Main API: takes sequences dict + metrics list → DataFrame
├── diagnostics/      # KNN feature-alignment, Spearman alignment for embedding validation
└── visualizations/   # Tables, parallel coordinates, barplots, trajectory plots
```

- **Critical path:**
  1. Define sequences as `dict[str, list[str]]` mapping group names to sequence lists
  2. Initialize `Cache` with required embedding models (avoids O(nk) → O(n+k) recomputation)
  3. Configure metrics list, passing `cache.model("name")` as embedder where needed
  4. Call `sm.evaluate(sequences, metrics)` → returns DataFrame
  5. Use `sm.show(df)` or visualization functions for interpretation

- **Design tradeoffs:**
  - **Embedding model selection**: Larger models (ESM-2 650M) capture more biology but increase latency; smaller models enable faster iteration. Use diagnostic scores to validate alignment before committing.
  - **Metric comprehensiveness vs. interpretability**: More metrics catch more failure modes but complicate comparison. The paper recommends using multiple metrics but suggests starting with FBD/MMD for distribution and Hit-rate/Hypervolume for properties.
  - **Fold count (K)**: Higher K gives better variance estimates but increases computation. Default K=5 is reasonable; increase for small datasets.

- **Failure signatures:**
  - **High novelty + low property scores**: Model generating random sequences (trivial baseline failure)
  - **Low authenticity + high precision**: Model memorizing training data (use Authenticity metric to detect)
  - **High FBD but low MMD**: Distribution mismatch is primarily in covariance, not mean—investigate variance collapse
  - **Disagreement between embedding metrics**: Embedding model may not align with domain; run diagnostic scores

- **First 3 experiments:**
  1. **Validate embedding model alignment**: Before running full evaluation, compute KNN feature-alignment score on your reference sequences using available labels (e.g., functional classes). If score < 0.7, consider alternative embedding models.
  2. **Establish baselines**: Run seqme on (a) reference sequences themselves and (b) randomly shuffled sequences. This calibrates expected metric ranges and validates that your setup is working.
  3. **Compare two design methods across all metric categories**: Generate sequences from methods A and B, compute full metric suite including FBD, MMD, Precision, Recall, Diversity, and task-specific property metrics. Use Fold=5 to get uncertainty estimates. Check whether differences are consistent across folds before drawing conclusions.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the selection of the underlying embedding model (e.g., ESM-2 vs. Hyformer) systematically bias the resulting distribution-based metric scores (like FBD or Precision/Recall) for the same set of generated sequences? The paper states that selecting an embedding model is "non-trivial" and that embeddings must capture the biological domain, but it does not validate if metric rankings are robust across different supported embedding models.

- **Open Question 2**: To what extent do specific combinations of seqme metrics (e.g., Authenticity + Hypervolume) correlate with the actual success rate of sequences in wet-lab functional assays? The authors explicitly cite Räisä et al. (2025) to note that "each evaluation metric is imperfect and can fail to detect a failure," and aim to facilitate "translation of computational advances into biology."

- **Open Question 3**: How can the "Fold" functionality or other statistical adjustments be optimized to mitigate sample-size biases in metrics like Improved Precision and Recall when comparing generative models trained on vastly different dataset scales? The paper introduces the "Fold" functionality to mitigate sample-size biases and discard fewer sequences, but the specific impact of sample size disparity on the stability of Improved Precision/Recall in biological contexts is not fully quantified.

## Limitations

- **Embedding Model Domain Alignment**: The effectiveness of seqme metrics depends critically on selecting embedding models that capture relevant biological features, but the paper provides limited empirical guidance on model selection criteria or validation procedures.
- **Property Model Quality**: Property-based metrics (Hit-rate, Hypervolume) are only as reliable as the underlying property models, and the paper does not provide systematic evaluation of common property model failure modes or uncertainty quantification methods.
- **Sample Size Sensitivity**: While fold-based estimation mitigates some sample-size bias, the paper does not extensively characterize how metric reliability scales with dataset size, particularly for small reference sets.

## Confidence

- **High Confidence**: The mathematical foundations of core metrics (FBD, MMD, Precision/Recall, Hypervolume) are well-established. The library implementation appears correct based on the example code and API design.
- **Medium Confidence**: The claim that multi-metric evaluation captures diverse failure modes is conceptually sound, but empirical validation across diverse design methods is limited to a single case study.
- **Low Confidence**: Specific guidance on benchmarking practices (e.g., "which metrics matter most for which design goals") is sparse. The paper suggests starting with certain metrics but doesn't provide decision trees or comprehensive benchmarking guidelines.

## Next Checks

1. **Embedding Model Validation**: Before applying seqme to a new biological domain, run the diagnostic scores (KNN feature-alignment, Spearman alignment) on your reference sequences. If alignment scores fall below 0.7, test alternative embedding models or consider whether seqme is appropriate for your domain.

2. **Baseline Calibration**: Run seqme on (a) reference sequences themselves and (b) randomly generated sequences matching reference length distributions. This establishes expected metric ranges and helps interpret whether your design method shows meaningful improvement over trivial baselines.

3. **Metric Agreement Assessment**: When comparing design methods, check for consistency across metric categories. If sequence-based metrics (e.g., Diversity) show large differences but embedding metrics (FBD/MMD) do not, investigate potential metric redundancy or misalignment rather than concluding one method is superior.