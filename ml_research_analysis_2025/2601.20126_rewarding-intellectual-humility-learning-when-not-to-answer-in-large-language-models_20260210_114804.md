---
ver: rpa2
title: Rewarding Intellectual Humility Learning When Not To Answer In Large Language
  Models
arxiv_id: '2601.20126'
source_url: https://arxiv.org/abs/2601.20126
tags:
- abstention
- answer
- reward
- training
- b-instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Reinforcement Learning with Verifiable Rewards\
  \ (RLVR) to promote intellectual humility in large language models by rewarding\
  \ correct answers, penalizing incorrect ones, and explicitly rewarding abstention\
  \ (\"I don't know\"). Experiments on Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct\
  \ with MedMCQA and Hendrycks Math datasets show that moderate abstention rewards\
  \ (rabs \u2248 -0.25 to 0.3) reduce hallucinations without severe accuracy loss\
  \ on multiple-choice tasks."
---

# Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models

## Quick Facts
- **arXiv ID**: 2601.20126
- **Source URL**: https://arxiv.org/abs/2601.20126
- **Reference count**: 19
- **Primary result**: Moderate abstention rewards (-0.25 to 0.3) reduce hallucinations in LLMs without severe accuracy loss on multiple-choice tasks.

## Executive Summary
This paper introduces Reinforcement Learning with Verifiable Rewards (RLVR) to promote intellectual humility in large language models by rewarding correct answers, penalizing incorrect ones, and explicitly rewarding abstention ("I don't know"). Experiments on Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct with MedMCQA and Hendrycks Math datasets show that moderate abstention rewards reduce hallucinations without severe accuracy loss on multiple-choice tasks. Larger models exhibit greater robustness to abstention incentives. RLVR is less effective on open-ended QA due to insufficient exploration, but supervised abstention training can help. The approach offers a tunable trade-off between accuracy and cautious behavior, providing a practical framework for hallucination mitigation.

## Method Summary
The paper proposes RLVR to encourage intellectual humility in LLMs by introducing a reward structure that includes positive rewards for correct answers, penalties for incorrect answers, and explicit rewards for abstaining from answering ("I don't know"). The model learns to balance providing useful responses against avoiding potentially incorrect information. The approach is tested on both multiple-choice question answering tasks (MedMCQA, Hendrycks Math) and open-ended QA tasks, with experiments conducted on Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct models. The abstention reward parameter (rabs) is tuned to find the optimal balance between accuracy and cautious behavior.

## Key Results
- Moderate abstention rewards (rabs ≈ -0.25 to 0.3) reduce hallucinations without severe accuracy loss on multiple-choice tasks
- Larger models show greater robustness to abstention incentives compared to smaller models
- RLVR is less effective on open-ended QA due to insufficient exploration, while supervised abstention training provides a viable alternative

## Why This Works (Mechanism)
The mechanism works by leveraging reinforcement learning to create a multi-objective reward function that explicitly values intellectual humility. By assigning rewards for correct answers, penalties for incorrect ones, and additional rewards for abstention, the model learns to recognize situations where it lacks sufficient confidence to provide a reliable answer. This creates a behavioral incentive structure that shifts the model from a "always answer" paradigm to a more nuanced approach that balances helpfulness with accuracy. The abstention reward parameter acts as a tunable knob that allows practitioners to control the trade-off between model helpfulness and reliability based on application requirements.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: A training framework that uses reward signals to shape model behavior based on verifiable outcomes. Why needed: Traditional LLM training focuses solely on prediction accuracy without considering the value of withholding uncertain responses. Quick check: Verify that reward signals are properly shaped and that the model can distinguish between verifiable and non-verifiable scenarios.
- **Intellectual Humility in AI**: The capacity of AI systems to recognize their limitations and abstain from answering when uncertain. Why needed: Current LLMs tend to hallucinate confidently, making intellectual humility critical for reliable deployment. Quick check: Measure abstention rates across different confidence thresholds to ensure the model appropriately recognizes uncertainty.
- **Reward Shaping for Abstention**: The technique of explicitly rewarding models for choosing not to answer questions. Why needed: Without explicit incentives, models default to answering all questions regardless of confidence. Quick check: Validate that abstention rewards are properly weighted against accuracy incentives to prevent excessive non-answers.

## Architecture Onboarding

**Component Map**
Input Processing -> Reward Computation -> Policy Update -> Output Generation

**Critical Path**
Input text → Model inference → Reward evaluation (correct/incorrect/abstain) → Policy gradient update → New model parameters

**Design Tradeoffs**
- **Reward magnitude tuning**: Too high abstention rewards lead to excessive non-answers, while too low rewards fail to reduce hallucinations. The optimal range (-0.25 to 0.3) must be task-specific.
- **Model size vs. robustness**: Larger models demonstrate greater resilience to abstention incentives, suggesting capacity plays a role in handling uncertainty.
- **Exploration vs. exploitation**: RLVR's effectiveness is limited on open-ended QA due to insufficient exploration, highlighting the need for hybrid approaches.

**Failure Signatures**
- Excessive abstention rates (>80%) indicating over-penalization of incorrect answers
- No change in hallucination patterns suggesting inadequate reward shaping
- Performance degradation on tasks requiring confident responses
- Mode collapse where the model defaults to "I don't know" for complex queries

**3 First Experiments**
1. Baseline evaluation: Measure hallucination rates and accuracy on multiple-choice tasks without abstention rewards
2. Reward sensitivity analysis: Test different abstention reward values (rabs = -0.5, -0.25, 0, 0.25, 0.5) to identify optimal range
3. Cross-task validation: Apply optimal abstention rewards from multiple-choice tasks to open-ended QA to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Results are primarily validated on multiple-choice question answering tasks, limiting generalizability to more complex or open-ended domains
- The abstention reward parameter requires careful tuning, with optimal values potentially varying across different model architectures and task domains
- RLVR showed reduced effectiveness on open-ended QA tasks due to insufficient exploration, suggesting current limitations in handling open-ended scenarios

## Confidence

**High confidence**: The core finding that moderate abstention rewards reduce hallucinations without severe accuracy loss on multiple-choice tasks is well-supported by experimental results.

**Medium confidence**: The observation that larger models show greater robustness to abstention incentives is supported but requires broader validation across more model sizes and types.

**Medium confidence**: The claim about RLVR's limitations on open-ended QA is substantiated but based on a limited exploration of alternative training strategies.

## Next Checks

1. Test RLVR with diverse reward shaping techniques (e.g., curiosity-driven exploration) on open-ended QA to verify whether enhanced exploration improves performance beyond supervised abstention training.

2. Conduct ablation studies across different model families (e.g., decoder-only, multimodal) to determine whether the optimal abstention reward range generalizes or requires task-specific tuning.

3. Evaluate long-term user trust and task completion rates in real-world deployment scenarios to assess whether reduced hallucinations translate to measurable improvements in practical applications.