---
ver: rpa2
title: 'Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI
  Video Stories'
arxiv_id: '2512.16954'
source_url: https://arxiv.org/abs/2512.16954
tags:
- character
- video
- uni00000057
- generation
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-stage pipeline for generating long,
  character-stable AI video stories, addressing the challenge of maintaining character
  consistency across scenes in text-to-video generation. The approach mimics a filmmaker's
  workflow, using an LLM to generate a detailed script, followed by character visualization,
  iterative scene synthesis with temporal bridging, and final composition.
---

# Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories

## Quick Facts
- arXiv ID: 2512.16954
- Source URL: https://arxiv.org/abs/2512.16954
- Reference count: 7
- One-line primary result: A four-stage inference-only pipeline that improves character consistency scores from 0.55 to 7.99 in multi-scene video stories.

## Executive Summary
This paper presents a filmmaker-inspired four-stage pipeline for generating long-form, character-stable AI video stories from single text prompts. The approach uses an LLM to generate structured scripts with character descriptions, then creates visual anchors via character images before iteratively synthesizing scenes with temporal bridging. The pipeline demonstrates high character consistency (7.99/10) compared to baselines, though script adherence remains lower than prompt adherence. A key finding reveals a "Subject-World Decoupling" bias where Indian character identities are preserved but environmental coherence degrades under high motion compared to Western contexts.

## Method Summary
The pipeline operates in four stages: (1) an LLM generates a structured JSON blueprint containing story analysis, character sheets with physical descriptions and visual notes, and scene breakdowns with durations, settings, and actions; (2) a text-to-image model creates character reference images from these descriptions; (3) scenes are synthesized iteratively—an image-to-image model generates initial frames conditioned on scene text, character references, and optionally the prior scene's final frame, then an image-to-video model animates each clip (~8s at 24fps); (4) clips are concatenated with audio. The critical innovation is using the I2I seed frame as a visual anchor to maintain character identity across scenes, with ablation studies showing consistency drops from 7.99 to 0.55 when this component is removed.

## Key Results
- Character consistency scores improve from 0.55 to 7.99 compared to baselines without I2I seed frames
- Prompt adherence reaches 4.47/5 while script adherence lags at 3.49/5
- "Subject-World Decoupling" bias identified: Indian character identities preserved but environments degrade under high motion vs Western contexts
- Temporal bridge and character image assets both contribute to consistency, but I2I seed frame is most critical

## Why This Works (Mechanism)

### Mechanism 1: Visual Anchoring via I2I Seed Frame
The I2I seed frame is the architectural linchpin for character consistency. A character reference image conditions an I2I model to generate a scene-specific initial frame, which then serves as the primary visual anchor for the I2V model during clip synthesis. This works because I2V models cannot reliably maintain identity from text descriptions alone; they require a concrete visual prior. Evidence: removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55).

### Mechanism 2: Asset-First Character Decoupling
Pre-generating character visual references before scene synthesis improves identity retention more than text-based conditioning alone. The text-to-image model generates character sheets first; these images then serve as conditioning inputs for all subsequent scene generations, decoupling character design from scene composition. Evidence: removing character images dropped consistency from 7.99 to 5.78—"the character's appearance was re-imagined from text for each scene, leading to inconsistencies."

### Mechanism 3: Temporal Bridge for Scene Continuity
Conditioning each scene's initial frame on the final frame of the preceding scene improves cross-scene coherence. The pipeline optionally passes the last frame of Scene N as an additional conditioning input to the I2I model generating Scene N+1's seed frame. Evidence: the LLM determines whether to incorporate the final frame from the preceding scene based on continuity needs.

## Foundational Learning

- **Concept: Image-to-Video Conditioning Strength**
  - Why needed here: The pipeline depends on I2V models that animate from static frames. Understanding how conditioning strength affects identity retention is essential for debugging consistency failures.
  - Quick check question: Given a character reference image and a text action description, which input dominates the I2V model's output when they conflict?

- **Concept: Inference-Only Control vs. Fine-Tuning**
  - Why needed here: The authors explicitly position their approach as "inference-only" without LoRA or weight updates. Understanding conditioning mechanisms is critical.
  - Quick check question: How does a diffusion model use a reference image during denoising without modifying its weights?

- **Concept: LLM Structured Output Reliability**
  - Why needed here: Stage 1 produces JSON blueprints that drive the entire pipeline. Ensuring schema compliance is a prerequisite for downstream stability.
  - Quick check question: What techniques improve LLM adherence to complex JSON schemas across diverse prompts?

## Architecture Onboarding

- **Component map:** Prompt → LLM Blueprint → Character Images → I2I Seed Frames → I2V Clips → Merged Output
- **Critical path:** Prompt → LLM Blueprint → Character Images → I2I Seed Frames → I2V Clips → Merged Output. The I2I seed frame is the single point of failure; its removal causes 93% consistency degradation.
- **Design tradeoffs:** Deterministic JSON blueprints vs. multi-agent negotiation (more reliable control flow, less emergent creativity); Inference-only vs. fine-tuning (lower deployment cost, quality capped by base models); Per-scene generation vs. end-to-end (better modularity and debugging, requires explicit continuity handling).
- **Failure signatures:** Identity drift between scenes → check character image quality and I2I conditioning strength; Script details ignored (Score: 3.49) while prompts are followed (Score: 4.47) → expected behavior; Environmental instability in non-Western contexts under high motion → "Subject-World Decoupling" bias.
- **First 3 experiments:** 1) Reproduce the Baseline 2 ablation (no I2I seed frame) to calibrate consistency scoring; 2) Isolate temporal bridge contribution by comparing videos with/without final-frame conditioning across scene transition types; 3) Test character sheet format: single-view vs. multi-view references to measure impact on DINO consistency scores.

## Open Questions the Paper Calls Out
- Can architectural modifications to the temporal bridge or I2V conditioning mechanisms reduce the Subject-World Decoupling phenomenon observed in non-Western contexts under high-motion conditions?
- What pipeline modifications could close the performance gap between prompt adherence (4.47) and script adherence (3.49) in narrative video generation?
- How can explicit control over cinematography and character choreography be incorporated without sacrificing the zero-shot, inference-only benefits?
- Is the catastrophic consistency drop when removing the I2I seed frame specific to the current I2V models tested, or does it generalize across different base video generation architectures?

## Limitations
- The pipeline lacks explicit mechanisms for precise cinematography control (e.g., dolly zoom, rack focus) or complex character choreography within video generation
- Current architectures prioritize style transfer over complex narrative sequencing, leading to lower script adherence (3.49) compared to prompt adherence (4.47)
- The "Subject-World Decoupling" bias reveals environmental consistency issues in non-Western contexts under high motion, suggesting cultural dataset limitations

## Confidence
- **High Confidence**: The I2I seed frame's critical role in character consistency (7.99→0.55 drop when removed) is empirically demonstrated and architecturally sound
- **Medium Confidence**: The overall pipeline architecture and its staged approach are well-specified, though LLM prompt details remain unclear
- **Low Confidence**: Cross-cultural consistency claims require deeper investigation—the observed bias may reflect dataset limitations rather than fundamental architectural flaws

## Next Checks
1. Isolate temporal bridge contribution by comparing videos with/without final-frame conditioning across different scene transition types (visual continuity vs. jumps)
2. Test character sheet format variations (single-view vs. multi-view references) to measure impact on DINO consistency scores and identify optimal character conditioning
3. Conduct stratified motion analysis (Low/Medium/High Dynamic Degree) across multiple cultural contexts to quantify and potentially mitigate the Subject-World Decoupling bias