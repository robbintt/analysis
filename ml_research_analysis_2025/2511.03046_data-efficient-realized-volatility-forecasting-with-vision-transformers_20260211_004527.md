---
ver: rpa2
title: Data-Efficient Realized Volatility Forecasting with Vision Transformers
arxiv_id: '2511.03046'
source_url: https://arxiv.org/abs/2511.03046
tags:
- data
- train
- zhang
- wang
- wide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of Vision Transformers (ViTs) to
  forecast the 30-day realized volatility of assets from their implied volatility
  (IV) surfaces, augmented with date information. The authors train ViT models on
  single-channel images representing IV surfaces, comparing their performance against
  a baseline MLP model and conducting ablation studies to assess the importance of
  seasonality features.
---

# Data-Efficient Realized Volatility Forecasting with Vision Transformers

## Quick Facts
- arXiv ID: 2511.03046
- Source URL: https://arxiv.org/abs/2511.03046
- Authors: Emi Soroka; Artem Arzyn
- Reference count: 21
- Primary result: Vision Transformers outperform MLP baselines on 30-day realized volatility forecasting from IV surfaces, achieving R²=0.41 on 2022 test data

## Executive Summary
This paper investigates the use of Vision Transformers (ViTs) for forecasting 30-day realized volatility from implied volatility (IV) surfaces, augmented with date information. The authors adapt ViT architecture to process single-channel 10×36 IV surface images and compare performance against MLP baselines across various model sizes. The study demonstrates that ViTs can learn seasonal patterns and nonlinear features from IV surfaces, with smaller models (0.05-0.17M parameters) performing well on limited data while larger models (1.7M) require full 10-year datasets to achieve optimal performance. The best-performing model achieves R²=0.41 on 2022 test data when trained on 2012-2021 data.

## Method Summary
The study adapts Vision Transformers to process single-channel IV surface matrices (10×36) by dividing them into 2×2 patches with linear projection and positional embeddings. Models are trained on OptionMetrics IvyDB data (2012-2022) with realized volatility targets, augmented with month/day/day-of-week seasonality features. The training uses AdamW optimizer with cosine annealing learning rate (max_lr=0.01, min_lr=0.001), batch size 2048, and Huber loss. Performance is evaluated using R² on held-out 2022 test data, with ablation studies comparing ViT to MLP baselines and examining seasonality augmentation effects.

## Key Results
- ViT_1.7M achieves R²=0.41 on 2022 test data when trained on full 2012-2021 dataset
- Smaller ViT models (0.05-0.17M parameters) perform well on limited data (1-4 years) but collapse on larger datasets
- ViT_0.17M_wide achieves R²=0.37 vs. MLP_0.17 at R²=0.29 with similar parameter counts
- Seasonality augmentation provides small but consistent performance gains (~0.02-0.03 R² improvement)

## Why This Works (Mechanism)

### Mechanism 1: Spatial Attention on IV Surface Structure
Patch-based attention captures spatial relationships across the IV surface's delta-maturity grid. The ViT divides the 10×36 IV surface into 2×2 patches, applies linear projection with position embeddings, and uses multi-head self-attention to learn dependencies across moneyness (delta) and time-to-maturity dimensions simultaneously. This works because IV surfaces contain structured spatial patterns where relationships between adjacent and distant points carry predictive information for future realized volatility. Break condition: If IV surface patches lack spatial coherence (e.g., random noise dominates), attention mechanisms would fail to extract meaningful cross-patch relationships.

### Mechanism 2: Nonlinear Feature Extraction
Transformer architecture extracts nonlinear features that simpler MLPs cannot, supporting the "virtue of complexity" phenomenon. Deep ViT layers with multi-head attention and MLP blocks learn hierarchical representations of IV surface dynamics, capturing interactions between volatility smile, term structure, and temporal seasonality. This works because financial forecasting benefits from highly nonlinear feature extraction, as documented in prior literature. Break condition: If nonlinear features are weak relative to noise (signal-to-noise ratio too low), added model complexity increases overfitting without predictive gain.

### Mechanism 3: Model-Dataset Size Scaling
Model-dataset size scaling determines optimal architecture choice for limited financial data. Smaller ViT models (0.05-0.17M parameters) achieve strong performance with 1-4 years of training data, while larger models (1.7M) require the full 10-year dataset to converge and avoid underfitting. This works because financial data is limited in scale and noisy; model capacity must match available data to prevent both underfitting (too small) and poor generalization (overly large). Break condition: If dataset size continues to grow (e.g., decades more data), smaller models would underfit relative to the information available.

## Foundational Learning

- **Implied Volatility Surface**: Why needed: The IV surface encodes market expectations of future volatility across strike prices (via delta) and expiration dates; understanding its structure is essential for interpreting model inputs. Quick check: Can you explain why IV surfaces typically show a "smile" or "skew" pattern and what the term structure dimension represents?

- **Vision Transformer (ViT) Architecture**: Why needed: ViT applies transformer self-attention to image patches; understanding patch embedding, position encodings, and the transformer encoder stack is required to adapt ViT for non-image financial data. Quick check: How does ViT differ from CNNs in processing spatial information, and why might this matter for grid-structured financial data?

- **Huber Loss Function**: Why needed: The paper uses Huber loss to reduce sensitivity to outliers in financial data; understanding its quadratic-to-linear transition is necessary for interpreting training dynamics. Quick check: For what values of |ŷ - y| does Huber loss behave like MSE, and for what values does it behave like MAE?

## Architecture Onboarding

- **Component map**: IV surface (10×36) + seasonality features → 2×2 patch embedding with positional encoding → Transformer encoder (L layers with multi-head attention + MLP) → 4-layer MLP prediction head → scalar output (30-day realized volatility)

- **Critical path**: 1. IV surface preprocessing (interpolated grid from OptionMetrics) 2. Seasonality feature concatenation 3. Patch embedding with positional encoding 4. Transformer forward pass (attention + MLP layers) 5. Output projection through MLP head 6. Huber loss computation against realized volatility target

- **Design tradeoffs**: Deep (4 MLP layers) vs. Wide (1 MLP layer): Deep models show stronger performance with sufficient data; wide models train faster but may underfit complex patterns. Model size vs. data availability: Match parameter count to training years (0.05-0.17M for 1-4 years; 1.7M for 10+ years). Seasonality augmentation: Small performance gain (~0.02-0.03 R²) but adds temporal awareness; can be removed if data preprocessing simplification is preferred.

- **Failure signatures**: R² drops to negative or near-zero on test year 2020 (COVID regime shift) — indicates train-test distribution mismatch. Training loss spikes during cosine annealing warm restarts — expected behavior, monitor for non-recovery. MLP-only models with large parameter counts show worse performance than smaller models — overfitting on noisy data. Models reach peak R² within 1-2 epochs, then degrade — requires early stopping or best-epoch selection.

- **First 3 experiments**: 1. Reproduce baseline: Train ViT_0.17M_wide on 4 years (2018-2021), test on 2022; target R² ≈ 0.35-0.37 with Huber loss, batch size 2048, cosine LR (max 0.01, min 0.001). 2. Ablate seasonality: Train identical model without month/day/day-of-week features; expect R² drop of ~0.02-0.03 to verify augmentation contribution. 3. Scale test: Train ViT_1.7M on full 10-year dataset (2012-2021) with same hyperparameters; verify R² approaches 0.41 and observe training convergence within 2-3 epochs.

## Open Questions the Paper Calls Out

1. **Transfer Learning Potential**: Can the learned representations from a ViT trained on IV surfaces be transferred to distinct financial targets via fine-tuning? The authors identify this as a top priority but found limited generalization when predicting asset returns; it remains unclear if other targets would succeed.

2. **Mixture-of-Experts Architecture**: Does a Mixture-of-Experts (MoE) architecture provide better performance or efficiency than standard ensembling for IV surface forecasting? The paper proposes comparing ensembling against MoE to apply foundation model techniques but doesn't explore this tradeoff between complexity and computational cost.

3. **Spatial vs. Sequential Representation**: Does the 2D spatial representation of IV surfaces capture term-structure dynamics more effectively than sequential transformer architectures? The paper applies ViT (image) patches but doesn't benchmark against sequence-based transformers that might better model semantic relationships between delta and maturity.

## Limitations

- **Data Availability and Representativeness**: The study relies exclusively on OptionMetrics IvyDB data from 2012-2022, which may not capture all market regimes and shows performance degradation during COVID-19 period suggesting distributional shifts.

- **Architecture Specification Gaps**: Several critical hyperparameters remain unspecified including Huber loss delta threshold, AdamW weight decay, and precise MLP head architecture details, creating potential reproducibility challenges.

- **Single Asset Class Focus**: The study focuses solely on equity options from S&P 500 universe, limiting generalizability to other asset classes where implied volatility surfaces exhibit different structural characteristics.

## Confidence

- **High Confidence**: The core finding that Vision Transformers outperform simpler MLP baselines on IV surface forecasting is well-supported by empirical results and systematic comparison across model sizes.
- **Medium Confidence**: The specific R² values reported (0.41 for best model) are likely reproducible with described methodology, but exact replication may vary depending on unspecified hyperparameters.
- **Low Confidence**: Claims about superiority of deeper vs. wider architectures and precise contribution of seasonality features may be sensitive to implementation details not fully specified.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary Huber loss delta threshold (d=1.0 vs. d=2.0), AdamW weight decay (0.01 vs. 0.1), and MLP head layer configurations to determine their impact on model performance and stability.

2. **Out-of-Sample Regime Testing**: Train models on pre-2020 data and evaluate performance across multiple test periods (2020, 2021, 2022) to quantify distributional shift effects and assess whether model performance can be stabilized through techniques like domain adaptation or ensemble methods.

3. **Architecture Ablation Study**: Conduct comprehensive ablation examining different patch sizes (1×1 vs. 2×2 vs. 3×3), attention head configurations, and removal of individual transformer components to isolate which architectural elements contribute most to performance gains.