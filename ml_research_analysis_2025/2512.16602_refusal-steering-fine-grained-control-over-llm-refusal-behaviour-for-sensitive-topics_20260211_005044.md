---
ver: rpa2
title: 'Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive
  Topics'
arxiv_id: '2512.16602'
source_url: https://arxiv.org/abs/2512.16602
tags:
- refusal
- steering
- answer
- chinese
- china
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Refusal Steering, an inference-time method
  to control large language models' refusal behaviour on politically sensitive topics
  without retraining. The approach uses an LLM-as-a-judge to assign refusal confidence
  scores and ridge-regularized variants to compute steering vectors that isolate the
  refusal-compliance direction.
---

# Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics

## Quick Facts
- arXiv ID: 2512.16602
- Source URL: https://arxiv.org/abs/2512.16602
- Reference count: 0
- Key outcome: Reduces political refusal rates from 92.35% to 23.82% while maintaining 99% safety on JailbreakBench

## Executive Summary
This paper introduces Refusal Steering, an inference-time method to control large language models' refusal behavior on politically sensitive topics without retraining. The approach uses an LLM-as-a-judge to assign refusal confidence scores and ridge-regularized variants to compute steering vectors that isolate the refusal-compliance direction. Applied to Qwen3-Next-80B-A3B-Thinking, the method reduces political refusal rates from 92.35% to 23.82% while maintaining safety performance (99% on JailbreakBench) and near-baseline performance on general benchmarks. The technique generalizes across 4B and 80B models and can also induce targeted refusals when desired. Analysis shows refusal signals concentrate in deeper transformer layers and are distributed across many dimensions, demonstrating that activation steering can remove political refusal behavior while retaining safety alignment for harmful content.

## Method Summary
The method involves three main steps: (1) LLM-as-a-judge evaluation where a judge LLM assigns refusal confidence scores to model responses on sensitive prompts; (2) Ridge-regularized vector computation using weighted ridge mean difference (WRMD) to compute steering vectors that better isolate the refusal-compliance direction; and (3) Inference-time steering where computed vectors are applied to target model activations during decoding. The approach uses an Extended dataset of 1497 sensitive prompts and 3712 diverse non-refusal prompts, with the judge LLM (GPT-OSS-20B) providing continuous scores between -1 and +1. The best configuration found was 16 layers with α=-0.15 using the non-reposition variant.

## Key Results
- Political refusal rates reduced from 92.35% to 23.82% on CCP-SENSITIVE dataset
- Safety performance maintained at 99% on JailbreakBench benchmark
- General capability preserved with minimal degradation on GSM8K, HumanEval, IFEval, and MMLU-Pro
- Method generalizes across 4B and 80B model sizes with optimal configurations identified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Steering vectors can isolate and manipulate a specific "refusal direction" in the model's activation space, enabling targeted control over refusal behavior without retraining.
- Mechanism: The paper proposes that refusal behavior is encoded as a linearly identifiable direction (or a distributed set of directions) within the hidden activations of transformer layers, particularly in deeper layers. By computing a steering vector that aligns with this refusal direction (or its opposite, the compliance direction), one can additively modify activations during inference to push the model's state toward or away from refusal behavior.
- Core assumption: The refusal direction is a stable, linear, and separable subspace in the activation space of specific layers, and the computed steering vector generalizes to new, unseen prompts.
- Evidence anchors:
  - [abstract] "...activation steering can remove political refusal behaviour while retaining safety alignment for harmful content..."
  - [section 2.2] "Arditi et al. (2024) discovered that refusal behaviour is mediated by a one-dimensional subspace in hidden activations..."
  - [corpus] Related work like `LatentGuard` and `SafeSteer` builds on the premise that safety/refusal behaviors can be controlled via activation steering.
- Break condition: The mechanism fails if the refusal direction is not linearly separable, is entangled with other crucial behaviors (e.g., reasoning), or if the steering vector overfits to training prompts, causing degraded performance on other tasks.

### Mechanism 2
- Claim: An LLM-as-a-judge provides a continuous-valued "refusal confidence score" that is more robust and nuanced for computing steering vectors than binary pattern-based classification.
- Mechanism: The method uses a judge LLM to evaluate a model's response on a continuous scale (e.g., -1 for compliance to +1 for refusal). This score is used to weight or categorize prompts, creating refined sets of positive (refusal) and negative (compliant) examples for vector computation.
- Core assumption: The judge LLM is sufficiently aligned with human notions of refusal to provide accurate and consistent scores.
- Evidence anchors:
  - [abstract] "We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores..."
  - [section 3.1] "However, these pattern-based approaches are ineffective for state-of-the-art models... To address this limitation, we propose an LLM-as-a-judge approach..."
  - [corpus] Direct corpus evidence for using an LLM-as-a-judge specifically for steering vector computation is weak or missing in the provided neighbors.
- Break condition: The mechanism fails if the judge LLM is biased, inconsistent, or operates with a different definition of refusal, leading to noisy or incorrect steering vectors.

### Mechanism 3
- Claim: Ridge-regularized vector computation methods (RMD, WRMD) better isolate the refusal-compliance direction than simple mean-difference, leading to more effective, capability-preserving steering.
- Mechanism: Simple mean-difference (MD) vectors can be noisy. RMD and WRMD use the covariance matrix of the negative (compliant) class activations to de-emphasize high-variance, irrelevant directions. The ridge term (`λI`) provides numerical stability. This results in a more precise steering vector.
- Core assumption: The "compliant" class activation distribution is sufficiently sampled, and its covariance structure meaningfully distinguishes signal from noise.
- Evidence anchors:
  - [abstract] "...we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal-compliance direction."
  - [section 3.2, Table 1] WRMD outperforms other methods on the Extended dataset, showing both effective refusal reduction and high safety preservation.
  - [corpus] The paper `Steering the CensorShip` is cited as a baseline; the ridge-regularized methods are an improvement upon it.
- Break condition: The method would be less effective if the regularization parameter `λ` is poorly chosen or if the negative class is not representative of the desired "normal" behavior.

## Foundational Learning

- Concept: **Representation Engineering / Activation Steering**
  - Why needed here: The entire method is built on the concept of modifying a model's behavior by intervening on its hidden state representations during inference.
  - Quick check question: What is the core idea behind "representation engineering"? (Answer: To identify and manipulate linear directions in the model's activation space that correspond to high-level behaviors, like refusal).

- Concept: **LLM-as-a-Judge**
  - Why needed here: This is the method used to automatically label data for steering vector computation, replacing manual labeling or brittle pattern matching.
  - Quick check question: Why is using an LLM to judge another LLM's output a useful technique, and what is a potential pitfall? (Answer: It allows for scalable, nuanced evaluation. A pitfall is inheriting the judge's biases).

- Concept: **Ridge Regression / Regularization**
  - Why needed here: The paper's key technical contribution is the use of ridge-regularized estimators to compute more robust steering vectors in a high-dimensional space.
  - Quick check question: What problem does ridge regression solve compared to ordinary least squares? (Answer: It prevents overfitting by penalizing large coefficients, which helps find a stable direction in high-dimensional space).

## Architecture Onboarding

- Component map: Data Generation -> Refusal Characterization -> Vector Computation -> Inference-time Steering -> Configuration Finder
- Critical path: The reliability of the entire system is critically dependent on the quality of the "Refusal Characterization" step. If the Judge LLM provides poor scores, the subsequent vector computation will be flawed, regardless of the regularization method.
- Design tradeoffs: **MD vs. RMD/WRMD:** MD is simpler but noisier. RMD/WRMD are more robust but require estimating covariance and tuning `λ`. **Single-layer vs. Multi-layer:** The paper finds intervening on multiple layers with small coefficients is more effective than a single layer with a large coefficient.
- Failure signatures: **Safety degradation:** Steering too hard or using an imprecise vector can make the model also stop refusing genuinely harmful queries (JailbreakBench failure). **Capability degradation:** An imprecise vector can make the model less coherent or perform worse on general tasks. **Over-refusal:** A positive coefficient to induce refusals can, at high magnitudes, make the model refuse everything.
- First 3 experiments:
  1. **Baseline Characterization:** Reproduce the "Refusal Characterization" pipeline. Use a capable Judge LLM to score a small set of known refusal and compliance prompts. Manually inspect scores to verify calibration.
  2. **MD Vector Ablation:** Compute steering vectors using the simple Mean Difference (MD) method. Apply the vector with varying coefficients (`α`) and qualitatively assess the effect on refusal prompts and general reasoning prompts.
  3. **Ridge Regularization Sweep:** Implement the RMD or WRMD method. Perform a small sweep over the regularization parameter `λ`. Compare the resulting vectors to the MD baseline for both steering efficacy and impact on general capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can steering vectors be refined to decouple political censorship removal from safety alignment in smaller, non-MoE models?
- Basis in paper: [explicit] The authors report in Section 5 that while the 80B model retains 99% safety on JailbreakBench, the 4B models suffer significant safety degradation (scores dropping to 55–78%).
- Why unresolved: The authors hypothesize that smaller models may "condense all types of refusals into similar representations," but this representational entanglement is not verified or solved.
- What evidence would resolve it: A demonstration of steering vectors successfully removing political refusals in sub-8B parameter models while maintaining safety benchmarks comparable to the 80B model.

### Open Question 2
- Question: Does the removal of refusal behavior affect the factual correctness or hallucination rates of the generated responses?
- Basis in paper: [explicit] Section 9 (Limitations) explicitly states that the evaluation focuses on measuring refusal behavior rather than the "factuality or correctness of model responses."
- Why unresolved: The method allows the model to access encoded knowledge, but it is unclear if the suppressed responses were withheld due to policy or because the model lacks reliable knowledge on those specific topics.
- What evidence would resolve it: Evaluation of steered model outputs against ground-truth datasets or human evaluation to measure truthfulness and hallucination rates on sensitive topics.

### Open Question 3
- Question: Is the observed multi-dimensional distribution of refusal signals a universal property of large reasoning models or specific to the Qwen/MoE architecture?
- Basis in paper: [inferred] Section 6 notes that refusal signals are "distributed across many dimensions," contrasting with prior work (Arditi et al.) that found refusal mediated by a one-dimensional subspace.
- Why unresolved: The analysis is performed exclusively on Qwen3-Next-80B-A3B-Thinking; it is unclear if the dimensionality difference stems from the model size, the MoE architecture, or the specific training data.
- What evidence would resolve it: Comparative analysis of steering vector sparsity and dimensionality across dense (e.g., Llama) and MoE architectures of similar size.

## Limitations
- Safety preservation in smaller models is significantly degraded (55-78% on JailbreakBench) compared to 80B model (99%)
- Method requires internal model access to compute steering vectors layer-by-layer
- Judge LLM (GPT-OSS-20B) used for scoring is not publicly available, creating reproducibility challenges

## Confidence

- **High Confidence:** The technical feasibility of activation steering for controlling specific model behaviors is well-established in prior work. The paper's core methodology (ridge-regularized vector computation) is sound.
- **Medium Confidence:** The specific claims about WRMD outperforming other methods and the optimal configuration (16 layers, α=-0.15) are based on the paper's internal validation. External replication is needed to confirm generalizability.
- **Low Confidence:** The long-term stability and broader societal implications of reducing political refusal rates in LLMs are not addressed. The safety preservation claim (99% on JailbreakBench) is narrowly defined and may not capture all forms of harm.

## Next Checks
1. **Judge LLM Validation:** Conduct a blind study where human annotators evaluate a subset of prompts and compare their refusal labels to those assigned by the judge LLM. Quantify agreement and identify potential biases.
2. **Cross-Model Generalization:** Apply the Refusal Steering method to a publicly available, politically sensitive model (e.g., LLaMA-3) using the extended dataset. Compare refusal reduction and safety preservation to the reported results.
3. **Fine-grained Safety Analysis:** Beyond JailbreakBench, evaluate the steered model on a diverse set of safety benchmarks (e.g., RealToxicityPrompts, BOLD) to ensure that safety preservation is not task-specific.