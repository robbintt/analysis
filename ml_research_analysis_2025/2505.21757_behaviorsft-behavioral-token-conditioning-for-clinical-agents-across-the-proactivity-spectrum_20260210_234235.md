---
ver: rpa2
title: 'BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the
  Proactivity Spectrum'
arxiv_id: '2505.21757'
source_url: https://arxiv.org/abs/2505.21757
tags:
- proactive
- clinical
- reactive
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BehaviorBench, a dataset for evaluating the
  proactive vs. reactive behavior of clinical agents.
---

# BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum

## Quick Facts
- **arXiv ID**: 2505.21757
- **Source URL**: https://arxiv.org/abs/2505.21757
- **Reference count**: 40
- **Key outcome**: BehaviorSFT improves Macro F1 scores on BehaviorBench up to 97.3% and enhances proactive task performance (e.g., from 95.0% to 96.5% for Qwen2.5-7B-Ins).

## Executive Summary
This paper introduces BehaviorSFT, a fine-tuning method that uses behavioral tokens (`<reactive>`, `<proactive>`) to condition LLM responses along the reactive-proactive spectrum in clinical contexts. The approach addresses the challenge of balancing helpful proactivity with necessary restraint in clinical agents. BehaviorSFT achieves state-of-the-art performance on the newly introduced BehaviorBench benchmark, with blind clinician evaluations confirming that agents trained with this method exhibit more realistic clinical behavior. The method demonstrates significant improvements over baseline models and successfully mitigates the trade-off between alert fatigue and omission risk.

## Method Summary
BehaviorSFT fine-tunes LLMs by inserting special behavioral tokens at the beginning of target sequences, conditioning the model's generation policy on the desired behavioral mode. The method uses the BehaviorBench dataset (6,876 clinical cases → 142,496 tasks across 13 categories) to train models to adapt between reactive (responding to explicit queries) and proactive (volunteering information, flagging issues) behaviors. LoRA adapters (rank=8, α=32) are attached to all linear layers of base models (Qwen-2.5-7B or Llama-3.1-8B), and training uses causal language modeling loss with behavior tokens as prefix control signals. The approach achieves significant improvements in both task performance and human-evaluated behavioral appropriateness.

## Key Results
- BehaviorSFT achieves Macro F1 scores up to 97.3% on BehaviorBench, significantly outperforming baseline models.
- Proactive task performance improves notably (e.g., Qwen2.5-7B-Ins: 95.0% → 96.5%).
- Blind clinician evaluations show BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a superior balance between helpful proactivity and necessary restraint.
- BehaviorSFT achieves intermediate implicitness scores (0.7-0.8), representing more natural human communication patterns compared to General SFT or Baseline models.

## Why This Works (Mechanism)

### Mechanism 1: Prefix Token Conditioning
- Claim: Inserting special behavioral tokens (`<reactive>`, `<proactive>`) at the start of the target sequence conditions the model's generation policy, acting as a control switch for the agent's "initiative level."
- Mechanism: The model learns to associate the token with specific output distributions. A `<reactive>` token suppresses unprompted information, while a `<proactive>` token lowers the threshold for generating warnings or unsolicited suggestions.
- Core assumption: The model attends strongly to the first token of the generation and propagates this bias through subsequent decoding steps.
- Evidence anchors:
  - [section 3.1]: "Placing the token at the beginning... allows it to act as a direct control signal, conditioning the entire generation process on the desired behavioral mode from the outset."
  - [section 3.3]: The training objective minimizes negative log-likelihood where the first token $y_1 \in \{<reactive>, <proactive>\}$.
  - [corpus]: Related work (e.g., ProPerSim) explores proactivity via simulation, but corpus evidence for explicit token-conditioning as a control mechanism is limited, suggesting this is a novel application of prefix-tuning in this domain.
- Break condition: If the attention mechanism dilutes the signal of the prefix token over long generation sequences, the behavioral consistency may degrade.

### Mechanism 2: Contextual Behavior Assessment
- Claim: The model learns to implicitly classify the clinical context to predict the appropriate behavioral token when it is not explicitly provided by the user.
- Mechanism: By training on annotated pairs where specific tasks (e.g., error correction vs. fact retrieval) are mapped to specific tokens, the model learns to recognize features in the input $x$ that correlate with the need for proactivity.
- Core assumption: The training dataset (BehaviorBench) provides sufficient signal for the model to distinguish between contexts requiring restraint vs. intervention.
- Evidence anchors:
  - [section 3.3]: "The model learns... Contextual Behavior Assessment: Implicitly analyzing the input $x$ to determine the likelihood that a proactive or reactive stance is warranted."
  - [table 3]: Ablation studies show the model can classify behavior patterns (Binary PR F1 92.90), indicating it has learned to distinguish these modes.
  - [corpus]: Neighbor papers (e.g., "The PROPER Approach") validate the difficulty of knowledge gap navigation, reinforcing that context assessment is a learnable but non-trivial capability.
- Break condition: If the test distribution contains ambiguous contexts not represented in the NEJM-derived training set, the assessment may default to an undesired mode.

### Mechanism 3: Implicitness Calibration
- Claim: BehaviorSFT aligns the agent's output style to an intermediate level of "implicitness," avoiding the excessive verbosity of standard SFT while remaining more detailed than baselines.
- Mechanism: The training targets (responses) are curated to provide high specificity without unnecessary elaboration, and the behavioral token helps gate this style.
- Core assumption: "Implicitness" (scores ~0.7-0.8) correlates with better human alignment and natural communication flow in clinical settings.
- Evidence anchors:
  - [section 4.2]: "BehaviorSFT carves out an intermediate... profile... achieving a moderate level of implicitness... mirroring more natural human communication patterns."
  - [figure 2]: Shows BehaviorSFT clustering distinctly from General SFT (lower implicitness) and Baseline (higher implicitness).
  - [corpus]: Corpus neighbors do not explicitly discuss "implicitness scores," making this a specific metric contribution of this paper.
- Break condition: If "implicitness" results in the omission of critical, non-obvious safety steps, it could lower the safety score.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) Data Formatting**
  - Why needed here: You must understand how to structure the training instance (Input + Target) so the model learns the causal link between the *concept* of the task and the *behavior token*.
  - Quick check question: In the training example `[Context] Query -> <proactive> Warning...`, is the model learning to predict the token based on the query, or is the token an input feature provided at inference?

- Concept: **Alert Fatigue vs. Omission Risk**
  - Why needed here: This is the core domain conflict the architecture attempts to solve. You cannot evaluate success without understanding this trade-off.
  - Quick check question: Does a high proactive score always indicate better performance, or can it degrade user trust via "over-intervention"?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The implementation uses LoRA adapters rather than full fine-tuning. Understanding this is necessary for resource planning and deployment.
  - Quick check question: Based on the implementation details, which layers are the adapters attached to, and does this affect how the new behavior tokens are learned?

## Architecture Onboarding

- Component map: Base Model (Qwen-2.5-7B or Llama-3.1-8B) -> Tokenizer Update (add `<reactive>`, `<proactive>` tokens) -> LoRA Adapters (rank=8, α=32 on all linear layers) -> Input Pipeline (BehaviorBench dataset) -> Training (Causal LM loss with token prefix)

- Critical path:
  1. **Tokenizer Expansion**: Add special tokens; ensure embedding matrix resizing.
  2. **Data Annotation**: Assign behavior tokens to training samples based on task type (Section 3.2).
  3. **Training**: Standard Causal LM loss on the sequence `Token + Response`.

- Design tradeoffs:
  - **Binary vs. Nuanced Tokens**: The paper currently uses binary tokens (`<reactive>`/`<proactive>`). The authors note in Appendix B that this cannot express nuances like "anticipatory clarification" vs. "high-urgency escalation."
  - **Unified vs. Separate Models**: BehaviorSFT uses a single model conditioned on tokens, trading the potential precision of separate, specialized expert models for the simplicity of unified deployment.

- Failure signatures:
  - **Mode Collapse**: The model ignores the behavior token and defaults to reactive responses (likely due to token frequency imbalance).
  - **Over-Proactivity**: High false-positive rate in "standard_of_care" or "omission_detection" tasks, leading to simulated alert fatigue.
  - **Inconsistent Conditioning**: The model follows the token instruction for short responses but drifts back to its base behavior in long generations.

- First 3 experiments:
  1. **Ablation on Token Forcing**: Force `<proactive>` tokens on reactive tasks (and vice versa) to quantify the strict control capacity of the token vs. the input context.
  2. **BehaviorBench Baseline**: Run the base model (Qwen/Llama) Zero-Shot on BehaviorBench to establish the "proactivity gap" before fine-tuning.
  3. **Clinician Preference Validation**: Replicate the blind ranking protocol (Appendix I) with a focus on "Necessary Restraint" to verify if the intermediate implicitness translates to better subjective scores than General SFT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hierarchical behavioral token inventory mitigate alert fatigue more effectively than the current binary reactive/proactive switch?
- Basis in paper: [explicit] The authors state the binary token is "coarse," cannot express nuances like "anticipatory clarification" vs "high-urgency escalation," and occasionally over-fires, creating fatigue. They propose experimenting with hierarchical tokens (e.g., `<flag_safety>`, `<escalate_critical>`) as future work.
- Why unresolved: The current study limits training to a single pair of special tokens.
- What evidence would resolve it: A comparison of user interrupt rates and intervention appropriateness between binary and hierarchical token-conditioned models.

### Open Question 2
- Question: Do proactive behaviors conditioned by BehaviorSFT generalize to non-English clinical contexts and specialties outside internal medicine?
- Basis in paper: [explicit] The authors identify the dataset's "internal-medicine bias" and omission of non-English documentation as key limitations.
- Why unresolved: BehaviorBench is derived entirely from English NEJM case reports, potentially biasing behavior triggers toward specific cultural or specialty-specific patterns.
- What evidence would resolve it: Performance evaluations (Macro F1) on a multilingual benchmark including dermatology or psychiatry tasks.

### Open Question 3
- Question: How does BehaviorSFT impact clinician workflow and interrupt patterns when embedded in a simulated EHR environment?
- Basis in paper: [explicit] The authors note their clinician study (N=3) is under-powered for workflow integration and lacks observation of "interrupt patterns" or "long-horizon reasoning" in a deployed setting.
- Why unresolved: Current evaluations are based on static case vignettes rather than interactive, multi-day clinical episodes.
- What evidence would resolve it: A study tracking alert override rates and clinician cognitive load in a longitudinal EHR sandbox simulation.

## Limitations

- **Dataset Generalization**: The BehaviorBench dataset is derived from NEJM case reports, representing a specific clinical context and writing style, limiting generalizability to other medical specialties or non-English contexts.
- **Behavioral Token Ambiguity**: The binary `<reactive>`/`<proactive>` token system cannot express intermediate or nuanced behavioral states, potentially oversimplifying complex clinical decision-making scenarios.
- **Safety Validation Scope**: While G-Eval Safety scores are reasonable (0.871-0.890), the evaluation framework may not capture all clinical safety dimensions, particularly for rare but serious conditions not well-represented in training data.

## Confidence

**High Confidence (9/10)**:
- BehaviorSFT successfully conditions model behavior along the reactive-proactive spectrum as evidenced by Macro F1 scores (97.3% on BehaviorBench) and significant improvements over baseline models.
- The prefix token conditioning mechanism works as intended, with the model learning to associate tokens with specific output distributions.
- Clinician preference for BehaviorSFT over General SFT and Baseline models is statistically significant and consistent across evaluation metrics.

**Medium Confidence (7/10)**:
- BehaviorSFT achieves intermediate implicitness scores (0.7-0.8) representing "natural human communication patterns" is supported by quantitative measures but requires further validation in real clinical workflows.
- The model's ability to perform Contextual Behavior Assessment (implicitly classifying input to predict appropriate behavior) is demonstrated through ablation studies but may not generalize to all clinical contexts.
- The balance between proactivity and restraint successfully mitigates alert fatigue while maintaining clinical utility is inferred from evaluation metrics but lacks longitudinal user studies.

**Low Confidence (4/10)**:
- Real-world clinical impact and safety in actual patient care settings cannot be determined from the current evaluation framework.
- The model's performance on rare or ambiguous clinical scenarios that were not well-represented in the NEJM-derived training data is unknown.
- Long-term user trust and adoption in clinical environments requires field studies beyond the controlled evaluation setup.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate BehaviorSFT on clinical datasets from different medical specialties (e.g., emergency medicine, pediatrics, oncology) and different sources (e.g., clinical notes, patient portals) to assess whether the reactive-proactive conditioning generalizes beyond NEJM case reports.

2. **Longitudinal Alert Fatigue Study**: Deploy BehaviorSFT in a simulated clinical environment with repeated interactions over extended periods, measuring changes in user trust, response acceptance rates, and perceived helpfulness to quantify real-world alert fatigue beyond single-interaction evaluations.

3. **Rare Condition Safety Audit**: Systematically test BehaviorSFT on rare but serious clinical conditions and ambiguous presentations that may be underrepresented in the training data, measuring false-negative rates for critical safety information and comparing against human expert performance.