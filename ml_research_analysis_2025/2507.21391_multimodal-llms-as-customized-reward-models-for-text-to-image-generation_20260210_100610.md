---
ver: rpa2
title: Multimodal LLMs as Customized Reward Models for Text-to-Image Generation
arxiv_id: '2507.21391'
source_url: https://arxiv.org/abs/2507.21391
tags:
- llav
- reward
- a-reward
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaVA-Reward, an efficient multimodal reward
  model that leverages pre-trained multimodal large language models (MLLMs) for evaluating
  text-to-image generation. Unlike existing methods requiring complex instructions
  or token-based scoring, LLaVA-Reward directly utilizes MLLM hidden states from text-image
  pairs and incorporates a Skip-connection Cross Attention (SkipCA) module to enhance
  bidirectional visual-textual reasoning.
---

# Multimodal LLMs as Customized Reward Models for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2507.21391
- Source URL: https://arxiv.org/abs/2507.21391
- Authors: Shijie Zhou; Ruiyi Zhang; Huaisheng Zhu; Branislav Kveton; Yufan Zhou; Jiuxiang Gu; Jian Chen; Changyou Chen
- Reference count: 40
- Primary result: LLaVA-Reward achieves 71.1% pairwise accuracy on TIFA 160 alignment evaluation

## Executive Summary
This paper introduces LLaVA-Reward, an efficient multimodal reward model for text-to-image generation that leverages pre-trained multimodal large language models (MLLMs) as reward models. Unlike existing methods requiring complex instructions or token-based scoring, LLaVA-Reward directly utilizes MLLM hidden states from text-image pairs with a Skip-connection Cross Attention (SkipCA) module to enhance bidirectional visual-textual reasoning. The model supports multiple evaluation perspectives (alignment, fidelity, safety, ranking) and demonstrates superior performance across benchmarks while being more computationally efficient than VQA-based approaches.

## Method Summary
LLaVA-Reward builds on pre-trained MLLMs by extracting hidden states from text-image pairs and incorporating a Skip-connection Cross Attention (SkipCA) module that enhances bidirectional visual-textual reasoning. The model is fine-tuned using preference data through Bradley-Terry ranking loss or cross-entropy loss, supporting multiple evaluation perspectives including alignment, fidelity, safety, and ranking. The SkipCA module improves the model's ability to capture complex relationships between visual and textual information, enabling more accurate reward modeling for text-to-image generation tasks.

## Key Results
- Achieves 71.1% pairwise accuracy on TIFA 160 alignment evaluation, outperforming existing methods
- Demonstrates 87.2% F1 score for safety detection in text-to-image generation
- Improves text-to-image generation quality when used in diffusion inference-time scaling, achieving state-of-the-art performance across multiple benchmarks
- Shows significant computational efficiency gains over VQA-based reward modeling approaches

## Why This Works (Mechanism)
LLaVA-Reward works by leveraging the rich semantic understanding already present in pre-trained MLLMs and enhancing it with SkipCA modules that enable better cross-modal reasoning. The SkipCA module allows the model to maintain both direct and attended information flows between visual and textual modalities, capturing richer context for reward evaluation. By fine-tuning on preference data using ranking or cross-entropy loss, the model learns to distinguish high-quality from low-quality text-image pairs across multiple dimensions (alignment, fidelity, safety). The direct use of MLLM hidden states eliminates the need for complex instruction tuning or token-based scoring, making the approach both more efficient and more effective.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Pre-trained models that understand both visual and textual information simultaneously - needed for capturing rich cross-modal semantics; quick check: verify MLLM has been trained on aligned image-text pairs
- **Skip-connection Cross Attention (SkipCA)**: Architecture module that maintains parallel information paths for direct and attended signals - needed for preserving both raw and contextualized representations; quick check: ensure SkipCA dimensions match MLLM hidden states
- **Bradley-Terry Ranking Loss**: Probabilistic model for pairwise comparisons - needed for learning preference-based reward modeling; quick check: verify pairwise preference labels are correctly formatted
- **Preference Learning**: Training paradigm where models learn from human preferences rather than absolute labels - needed for capturing subjective quality judgments; quick check: confirm preference dataset covers diverse quality dimensions
- **Diffusion Inference-time Scaling**: Technique for improving generation quality by incorporating reward models during sampling - needed for practical application of reward models; quick check: verify sampling temperature and guidance scales are appropriate

## Architecture Onboarding

**Component Map**: Input Text/Image Pair -> MLLM Encoder -> SkipCA Module -> Reward Head -> Multiple Evaluation Perspectives (Alignment/Fidelity/Safety/Ranking)

**Critical Path**: Text/Image Input → MLLM Hidden States Extraction → SkipCA Processing → Reward Prediction → Evaluation/Guidance

**Design Tradeoffs**: Uses pre-trained MLLMs for efficiency vs. training from scratch for specialization; SkipCA adds computational overhead but improves reasoning; supports multiple perspectives vs. single-task specialization

**Failure Signatures**: Poor performance on out-of-distribution prompts; safety detection failures on adversarial examples; alignment issues with abstract or creative prompts; ranking instability with highly similar image pairs

**First Experiments**: 1) Validate SkipCA module improves alignment accuracy on TIFA 160; 2) Test safety detection performance across diverse prompt categories; 3) Evaluate reward model impact on generation quality using different guidance scales

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation relies heavily on pairwise comparisons and proxy metrics rather than human perceptual studies, limiting real-world applicability confidence
- Dataset composition and potential biases in preference data collection are not thoroughly examined
- Lack of ablation studies quantifying the exact contribution of the SkipCA module versus other architectural choices

## Confidence
- **High confidence**: Technical implementation of SkipCA and MLLM integration is sound and reproducible
- **Medium confidence**: Performance improvements are demonstrated but may be dataset-specific
- **Low confidence**: Generalization to diverse scenarios and long-tail failure cases is not empirically established

## Next Checks
1. Conduct human perceptual studies comparing LLaVA-Reward evaluations against crowdworker judgments across multiple generation systems
2. Perform cross-dataset validation by testing the model on text-to-image pairs from different domains (art, photography, medical imaging)
3. Run controlled ablation experiments isolating the SkipCA module's contribution from other architectural components and fine-tuning choices