---
ver: rpa2
title: Large Language Models' Internal Perception of Symbolic Music
arxiv_id: '2507.12808'
source_url: https://arxiv.org/abs/2507.12808
tags:
- music
- musical
- symbolic
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) can implicitly
  model symbolic music by generating a dataset of MIDI files from textual prompts
  describing genres and styles, without relying on explicit musical training. The
  authors generate 16,250 MIDI files spanning 13 genres and 25 styles, then train
  CNNs and transformers on this data for classification and melody completion tasks.
---

# Large Language Models' Internal Perception of Symbolic Music

## Quick Facts
- arXiv ID: 2507.12808
- Source URL: https://arxiv.org/abs/2507.12808
- Authors: Andrew Shin; Kunitake Kaneko
- Reference count: 2
- Key outcome: Models trained on LLM-generated data significantly underperform those trained on real music data, but still exceed chance rates in genre/style classification and melody completion

## Executive Summary
This paper investigates how large language models (LLMs) can implicitly model symbolic music by generating a dataset of MIDI files from textual prompts describing genres and styles, without relying on explicit musical training. The authors generate 16,250 MIDI files spanning 13 genres and 25 styles, then train CNNs and transformers on this data for classification and melody completion tasks. Results show that while models trained on LLM-generated data significantly underperform those trained on real music, they still exceed chance rates in genre/style classification and melody completion, demonstrating that LLMs can capture rudimentary musical structures from text alone. The study highlights both the limitations and potential of LLMs as general-purpose pattern learners for cross-domain applications in music informatics.

## Method Summary
The authors generate 16,250 MIDI files using LLM prompts that describe musical genres and styles. These files are then used to train CNN and transformer models for two tasks: genre/style classification and melody completion. The performance of these models is compared against baselines and against models trained on real music data. The approach aims to demonstrate whether LLMs can implicitly capture musical structure through text descriptions alone, without explicit musical training.

## Key Results
- Models trained on LLM-generated data significantly underperform those trained on real music data
- LLM-trained models still exceed chance rates in genre/style classification tasks
- LLM-trained models demonstrate capability in melody completion tasks, albeit at lower performance than real music-trained models

## Why This Works (Mechanism)
The mechanism relies on LLMs' ability to learn and represent complex patterns from text descriptions. When prompted with genre and style descriptions, LLMs can generate MIDI files that encode these characteristics in symbolic musical form. The resulting data, while not perfectly capturing real musical structures, contains enough statistical regularities for CNNs and transformers to learn classification and completion patterns. The approach leverages LLMs as general-purpose pattern learners that can bridge textual descriptions and symbolic music representation.

## Foundational Learning
- **MIDI file structure**: Understanding the symbolic music representation format used for training and evaluation
  - Why needed: The study's entire approach depends on LLM-generated MIDI files as training data
  - Quick check: Can the model correctly parse and interpret basic MIDI events

- **Genre and style classification**: The ability to categorize music based on learned patterns
  - Why needed: Primary evaluation task demonstrating whether LLM-generated data captures musical distinctions
  - Quick check: Does model performance exceed random chance on held-out test data

- **Melody completion**: The capacity to predict and generate musically coherent continuations
  - Why needed: Secondary task testing whether models learn structural musical patterns
  - Quick check: Are generated continuations musically plausible and stylistically consistent

## Architecture Onboarding

**Component Map**: LLM prompt generation -> MIDI file generation -> Dataset creation -> CNN/Transformer training -> Classification/Completion evaluation

**Critical Path**: LLM-generated MIDI files → Model training → Performance evaluation on classification and completion tasks

**Design Tradeoffs**: Using LLM-generated data versus real music data (lower quality but broader accessibility vs. higher quality but limited availability)

**Failure Signatures**: Models trained on LLM data significantly underperform real music-trained models; performance may reflect LLM artifacts rather than genuine musical understanding

**First Experiments**:
1. Train CNN on LLM-generated data for genre classification and measure accuracy against chance rate
2. Evaluate melody completion quality using automated metrics on held-out sequences
3. Compare performance of transformer versus CNN architectures on same LLM-generated dataset

## Open Questions the Paper Calls Out
None provided

## Limitations
- Reliance on LLM-generated data without real-world validation makes it unclear how well models capture authentic musical structures versus LLM artifacts
- Performance gap between LLM-generated and real music data suggests approach captures only surface-level patterns
- Limited generalizability due to focus on only 13 genres and 25 styles

## Confidence
High: Models trained on LLM-generated data significantly underperform those trained on real music data
Medium: LLM-generated data can support basic classification and melody completion tasks
Low: LLMs can implicitly model symbolic music and serve as general-purpose pattern learners for cross-domain music applications

## Next Checks
1. Conduct expert musical analysis of a representative sample of LLM-generated MIDI files to verify whether captured patterns align with actual musical theory and practice versus being LLM-specific artifacts
2. Compare LLM-based model performance against simpler baselines (n-gram models, rule-based systems) on the same classification and melody completion tasks to establish whether the LLM approach provides genuine advantages
3. Test model generalization by evaluating performance on held-out musical styles not present in the original 25-style dataset, and on real-world MIDI files from diverse sources to assess transferability beyond the generated dataset