---
ver: rpa2
title: Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis
arxiv_id: '2509.20768'
source_url: https://arxiv.org/abs/2509.20768
tags:
- data
- synthetic
- size
- layers
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the sensitivity of Transformer-based tabular
  data synthesis tools (GReaT and REaLTabFormer) to language model choices. It systematically
  varies LLM configurations across two tools and assesses performance across three
  dimensions: runtime, ML utility, and similarity to real data.'
---

# Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis

## Quick Facts
- arXiv ID: 2509.20768
- Source URL: https://arxiv.org/abs/2509.20768
- Reference count: 37
- Primary result: REaLTabFormer with lightweight LLMs offers best balance between computational efficiency and data quality for tabular data synthesis

## Executive Summary
This study systematically evaluates how Transformer-based tabular data synthesis (TDS) tools respond to different language model configurations. The researchers compare GReaT and REaLTabFormer across four real-world datasets, varying LLM architectures (GPT-2, GPT-Neo, GPT-NeoX, GPT-J, GPT-BigCode, LLaMA) and depths (1-12 layers). They assess performance across runtime, machine learning utility, and similarity to real data. The results demonstrate that tool choice and LLM configuration significantly impact both computational efficiency and synthetic data quality, with REaLTabFormer generally producing higher-quality synthetic data but requiring more computational resources than GReaT.

## Method Summary
The study employs a systematic experimental design using four tabular datasets (adult, customer, house, stroke_healthcare) with varying sizes and characteristics. For each dataset, ten different LLM configurations are tested across both GReaT and REaLTabFormer tools. The evaluation framework measures three key dimensions: runtime (training and generation time in seconds), ML utility (classification accuracy/Macro-F1 or regression R² scores), and similarity (Random Forest discriminator accuracy distinguishing real vs synthetic data). Each experiment is repeated five times to ensure statistical reliability. Machine learning models (Logistic Regression, Random Forest, Linear Regression) are trained on synthetic data and evaluated on held-out real test sets to measure utility.

## Key Results
- GReaT consistently outperforms REaLTabFormer in runtime, particularly on smaller datasets, while maintaining comparable utility and similarity
- REaLTabFormer achieves superior utility and similarity across all dataset sizes, with small datasets yielding high utility and optimal similarity
- For small datasets, both tools achieve high utility and optimal similarity, but only REaLTabFormer sustains strong performance on larger datasets
- REaLTabFormer with lightweight LLMs (1-4 layers) offers the best balance between computational efficiency and data quality

## Why This Works (Mechanism)
The study's systematic approach to varying LLM configurations across multiple dimensions (architecture, depth) while controlling for dataset characteristics allows for isolating the impact of each factor on synthesis quality and computational cost. By using standardized metrics (utility, similarity, runtime) and multiple repetitions, the results provide robust evidence about the sensitivity of each tool to LLM choices. The comparison across different dataset sizes reveals important scaling behaviors that would be missed in single-dataset studies.

## Foundational Learning
- **Tabular Data Synthesis (TDS)**: The process of generating synthetic datasets that preserve statistical properties and relationships of real data. Why needed: Provides privacy-preserving alternatives to real data for ML development. Quick check: Can the synthetic data be used to train models that perform similarly on real data?
- **Transformer-based TDS**: Using large language models with transformer architectures to model tabular data as sequential tokens. Why needed: Enables capturing complex, non-linear relationships in tabular data. Quick check: Does the model preserve both marginal distributions and joint relationships between columns?
- **ML Utility Metrics**: Measures of how well synthetic data preserves predictive power for downstream ML tasks (accuracy, F1, R²). Why needed: Quantifies practical usefulness of synthetic data for ML applications. Quick check: Do models trained on synthetic data generalize to real test data?
- **Similarity Metrics**: Statistical measures of how closely synthetic data resembles real data (RF discriminator accuracy). Why needed: Ensures synthetic data maintains realistic distributions and patterns. Quick check: Can a discriminator reliably tell real from synthetic data?
- **LLM Configuration Sensitivity**: How different model architectures and depths affect synthesis performance. Why needed: Guides optimal tool and model selection for specific use cases. Quick check: Which configuration provides best utility-runtime tradeoff for your dataset size?

## Architecture Onboarding

**Component Map:** Data Preprocessing -> LLM Fine-tuning -> Synthetic Data Generation -> ML Model Training -> Evaluation (Runtime, Utility, Similarity)

**Critical Path:** The core workflow involves preprocessing real data, fine-tuning the chosen LLM on this data, generating synthetic samples, then evaluating these samples through utility and similarity metrics while tracking computational resources.

**Design Tradeoffs:** The study reveals a fundamental tradeoff between computational efficiency (runtime) and data quality (utility/similarity). GReaT prioritizes speed while REaLTabFormer prioritizes quality, with the optimal choice depending on dataset size and resource constraints.

**Failure Signatures:** LLaMA models failing to converge on larger datasets in REaLTabFormer suggests architecture-specific limitations. Out-of-memory errors with deep configurations indicate hardware constraints. Poor utility scores suggest the model isn't capturing relevant patterns.

**First Experiments:**
1. Run GReaT with GPT-2 (6 layers) on the smallest dataset to establish baseline performance and validate the complete pipeline
2. Compare GReaT vs REaLTabFormer with identical LLM configurations on the same dataset to isolate tool-specific effects
3. Test REaLTabFormer with lightweight configurations (1-2 layers) across all dataset sizes to identify the utility-runtime sweet spot

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Fine-tuning hyperparameters (learning rate, batch size, epochs) were not explicitly specified for each LLM configuration, potentially affecting runtime and utility results
- Synthetic sample generation parameters (number of samples, temperature, sampling strategy) were not detailed, which could impact similarity metrics
- Only two specific Transformer-based TDS tools were evaluated, limiting generalizability to other architectures
- Hardware specifications for runtime measurements were not reported, making cross-system comparisons difficult

## Confidence
- **Runtime comparisons: High confidence** - Systematic variation of LLM configurations across both tools provides robust evidence for runtime differences
- **Utility and similarity results: Medium confidence** - Methodology is sound but lack of hyperparameter details and potential variability in synthetic data generation introduces uncertainty
- **REaLTabFormer superiority claims: Medium confidence** - Results show strong performance across dataset sizes, but tool-specific limitations and hyperparameter sensitivity suggest findings may not generalize to all configurations

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary learning rate, batch size, and training epochs for top-performing LLM configurations to quantify their impact on runtime and utility metrics
2. **Synthetic data quality validation:** Conduct additional similarity assessments using alternative discriminator models (gradient boosting, neural networks) and statistical tests (KS test, Wasserstein distance) to confirm RF discriminator results
3. **Cross-tool generalization:** Evaluate additional Transformer-based TDS tools (Tabby, SampleLLM) with the same experimental protocol to determine if REaLTabFormer's performance advantages hold across a broader tool landscape