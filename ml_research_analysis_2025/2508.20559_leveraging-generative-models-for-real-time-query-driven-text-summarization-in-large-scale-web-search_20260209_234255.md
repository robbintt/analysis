---
ver: rpa2
title: Leveraging Generative Models for Real-Time Query-Driven Text Summarization
  in Large-Scale Web Search
arxiv_id: '2508.20559'
source_url: https://arxiv.org/abs/2508.20559
tags:
- summarization
- query
- summaries
- user
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of query-driven text summarization
  (QDTS) in large-scale web search, where traditional multi-stage extractive approaches
  suffer from information loss and poor semantic understanding of user queries. The
  authors propose a novel end-to-end generative framework, QDGenSumRT, that leverages
  model distillation, supervised fine-tuning, direct preference optimization, and
  lookahead decoding to transform a lightweight 0.1B-parameter model into a domain-specialized
  QDTS expert.
---

# Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search

## Quick Facts
- **arXiv ID**: 2508.20559
- **Source URL**: https://arxiv.org/abs/2508.20559
- **Reference count**: 40
- **Primary result**: Achieves 51.33 ROUGE-2 score with <55ms latency on single GPU

## Executive Summary
This paper addresses the challenge of real-time query-driven text summarization (QDTS) in large-scale web search, where traditional multi-stage extractive approaches suffer from information loss and poor semantic understanding of user queries. The authors propose a novel end-to-end generative framework, QDGenSumRT, that leverages model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight 0.1B-parameter model into a domain-specialized QDTS expert. Their approach distills knowledge from a 10B-parameter teacher model, fine-tunes using high-quality human-annotated data, aligns with user preferences using preference data, and optimizes for real-time deployment. Evaluated on multiple industry-relevant metrics, their model outperforms the production baseline and achieves state-of-the-art performance with a ROUGE-2 score of 51.33. It also demonstrates excellent deployment efficiency, handling ~50,000 queries per second under 55 ms average latency per query on a single NVIDIA L20 GPU.

## Method Summary
The method employs a multi-stage pipeline: (1) Distillation stage where a 10B-parameter teacher model generates 14 million synthetic query-document-summary triplets that are used to train a 0.1B-parameter student model (Hamburger, GPT-2 architecture); (2) Supervised fine-tuning (SFT) on 6,160 high-quality human-annotated samples with controlled length and structured output requirements; (3) Direct preference optimization (DPO) using click-through data from A/B tests to align with user preferences; and (4) Deployment optimization using FP8 quantization and lookahead decoding for sub-100ms inference. The approach is designed to handle ~50,000 queries per second with <55ms average latency on a single NVIDIA L20 GPU.

## Key Results
- Achieves ROUGE-2 score of 51.33, outperforming production baseline (5.34) and NGS (37.65)
- Demonstrates 68x latency reduction compared to NGS (55ms vs 3,736ms)
- Handles ~50,000 queries per second on single NVIDIA L20 GPU
- Outperforms production baseline across ROUGE-1, ROUGE-2, ROUGE-L, and human evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation with Task-Specific Synthetic Data
A 10B-parameter teacher model transfers query-driven summarization capabilities to a 0.1B student model through large-scale synthetic data generation. The teacher model (ERNIE-Lite-8K, fine-tuned on 1,000 human-annotated pairs) generates 14 million query-document-summary triplets. The student (GPT-2 architecture, 768 hidden size, 12 layers) learns to mimic this output distribution. The prompt design is deliberately simple ("Please extract relevant summaries...") to match the student's limited instruction-following capacity.

### Mechanism 2: Multi-Stage Quality Refinement via SFT + DPO
Sequential refinement through human-annotated SFT followed by preference-based DPO addresses distinct quality dimensions. SFT uses 6,160 curated samples with controlled length (≈80 tokens), balanced positive/negative distribution, and structured output requirements (3-5 distinct points for multi-perspective queries). DPO then uses implicit click signals from A/B tests to align with user preferences. The DPO objective directly optimizes log-likelihood ratio between preferred and dispreferred summaries.

### Mechanism 3: Lossless Inference Acceleration via FP8 + Lookahead Decoding
FP8 quantization combined with speculative decoding achieves 68x latency reduction without quality degradation. FP8 (W8A8 + KV cache) reduces precision from bfloat16 with lower loss than INT8 weight-only quantization. Lookahead decoding generates n-gram candidates in parallel with a draft model, verified by the main model—breaking autoregressive dependency. Optimal configuration (window=4, ngram=6, verification=4) balances acceptance rate against verification overhead.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student Framework)**: Why needed here: The core efficiency mechanism depends on compressing 10B → 0.1B while preserving task performance. Quick check: Given a teacher model with 100x more parameters, what data characteristics (volume, diversity, quality) determine whether a student can achieve comparable task performance?

- **Direct Preference Optimization (DPO)**: Why needed here: Aligns model outputs with implicit user feedback without training a separate reward model. Quick check: How does DPO's objective differ from RLHF's reward modeling approach, and what data requirements does each have?

- **Speculative Decoding**: Why needed here: Achieves sub-100ms latency for 80-token generation through parallel candidate verification. Quick check: If verification acceptance rate drops below 50%, does speculative decoding still provide speedup? What factors determine the break-even point?

## Architecture Onboarding

- **Component map**: ERNIE-Lite-8K teacher → 14M synthetic samples → Hamburger (0.1B GPT-2) student training → 6,160 human-annotated samples → full-parameter SFT → A/B test preference data → DPO fine-tuning → FP8 quantization → TensorRT-LLM → Lookahead decoding (4,6,4)

- **Critical path**: Distillation data quality → SFT sample curation → DPO preference signal filtering → Inference parameter tuning. Errors propagate forward; distillation errors are most expensive to fix.

- **Design tradeoffs**: Model size vs. latency: 0.1B chosen for <100ms constraint; larger models exceed 1000ms. ROUGE vs. user preference: DPO may improve user metrics without improving ROUGE (paper shows comparable ROUGE to NGS but much lower latency). Acceptance rate vs. verification overhead: Higher ngram size increases AR but also computation.

- **Failure signatures**: Low ROUGE-2 (<30): Distillation data quality issue or insufficient training. High latency (>100ms): Quantization errors causing verification failures. Poor user engagement despite good ROUGE: DPO misaligned with actual user preferences.

- **First 3 experiments**: 
  1. Baseline validation: Train 0.1B model on distillation data only; target ROUGE-2 >30. If below, check teacher quality and prompt design.
  2. SFT impact test: Add human-annotated SFT (1,000 samples minimum); expect +10-15 ROUGE-2 improvement. Monitor for length distribution drift.
  3. Latency budget check: Deploy with FP8 + Lookahead(4,6,4) on target hardware; verify inference <60ms at target throughput. If exceeded, profile acceptance rate and reduce ngram size.

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary nature of "Hamburger" model architecture and training data prevents exact replication
- Evaluation relies on internal metrics and human evaluation without public benchmark comparisons
- Performance claims cannot be independently verified due to lack of open dataset validation

## Confidence

- **High Confidence**: The distillation + SFT + DPO training pipeline is technically sound and the observed ROUGE improvements are consistent with expected knowledge transfer from teacher to student. The latency claims are verifiable through the described FP8 + Lookahead decoding configuration.

- **Medium Confidence**: The quality improvements from DPO are plausible given the explicit filtering for "clear and consistent" preference signals, but the exact impact is difficult to assess without access to the preference data characteristics.

- **Low Confidence**: The absolute performance claims (ROUGE-2 51.33, beating NGS with 1000x latency reduction) cannot be independently verified due to the lack of public benchmark comparisons and the proprietary nature of all evaluation data.

## Next Checks

1. **Distillation Data Quality Validation**: Generate a small-scale synthetic dataset using an open teacher model (Llama-3-8B) and verify that the 0.1B student achieves ROUGE-2 >30 on a held-out set. This validates the core knowledge transfer mechanism before proceeding to more complex stages.

2. **SFT Data Curation Impact**: Create a controlled experiment comparing SFT with randomly sampled vs. carefully curated human-annotated data (following the paper's length and quality criteria). Measure ROUGE-2 improvement and monitor for summary length drift to quantify the impact of data quality on downstream performance.

3. **Inference Optimization Break-Even Analysis**: Profile the FP8 + Lookahead decoding configuration across different ngram sizes (4, 6, 8) and verification set sizes to identify the optimal point where latency reduction is maximized without triggering verification failures. This validates the claimed 68x speedup and identifies operational boundaries.