---
ver: rpa2
title: Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling
arxiv_id: '2505.24185'
source_url: https://arxiv.org/abs/2505.24185
tags:
- federated
- learning
- task
- feddea
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of federated multi-task learning
  (FMTL) where clients perform heterogeneous tasks and struggle to jointly train a
  unified global model due to parameter update interference. To address this, the
  authors propose FedDEA (Federated Decoupled Aggregation), which dynamically identifies
  and preserves task-relevant update dimensions while rescaling them to maintain optimization
  strength.
---

# Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling

## Quick Facts
- arXiv ID: 2505.24185
- Source URL: https://arxiv.org/abs/2505.24185
- Authors: Yipan Wei; Yuchen Zou; Yapeng Li; Bo Du
- Reference count: 40
- Primary result: FedDEA outperforms existing methods on NYUD-V2 and PASCAL-Context, achieving unified global models that beat single-task and FedAvg baselines.

## Executive Summary
This paper tackles federated multi-task learning where heterogeneous clients perform different tasks and struggle to jointly train a unified global model due to parameter update interference. The authors propose FedDEA (Federated Decoupled Aggregation), which dynamically identifies and preserves task-relevant update dimensions while rescaling them to maintain optimization strength. FedDEA does not rely on task labels or architectural changes, making it broadly applicable. Experiments show that FedDEA can construct a unified global model outperforming existing methods in overall performance, and when integrated as a plug-in into various federated optimization algorithms, it consistently delivers significant improvements.

## Method Summary
FedDEA addresses federated multi-task learning by dynamically filtering parameter updates to isolate task-relevant dimensions. For each client, it computes the local update, then constructs a binary mask retaining only the top ρ proportion of update dimensions with largest absolute values. The filtered update is then rescaled by 1/ρ to preserve optimization strength. This aggregated update is used to update the global model, effectively suppressing cross-task interference while maintaining task-specific signals. The method is architecture-agnostic and works as a drop-in modification to standard federated aggregation.

## Key Results
- FedDEA outperforms FedAvg and other baselines on NYUD-V2 and PASCAL-Context datasets for unified multi-task learning
- The method achieves better overall performance (measured by Δ) compared to training separate task-specific models
- When integrated as a plug-in to FedAvg and FedProx, FedDEA consistently improves performance across different federated optimization algorithms

## Why This Works (Mechanism)

### Mechanism 1: Task-Relevant Subspace Isolation via Magnitude Filtering
FedDEA isolates task-specific signals by retaining only high-magnitude update dimensions through a binary mask that keeps the top ρ proportion of values. This assumes task-relevant updates concentrate in high-magnitude dimensions that are disjoint or approximately orthogonal across different tasks. The masking operation effectively projects updates onto task-specific subspaces while filtering out redundant noise.

### Mechanism 2: Optimization Energy Restoration via Uniform Rescaling
After filtering, the method rescales the retained update vector by 1/ρ to compensate for the reduction in optimization strength caused by dimension dropping. This preserves the "energy" or scale of the update vector, preventing convergence slowdown that would occur from naive filtering. The rescaling ensures the vector magnitude approximates the original unmasked update.

### Mechanism 3: Suppression of Cross-Task Interference
By aggregating only filtered and rescaled updates, FedDEA minimizes directional drift caused by clients optimizing conflicting objectives. The aggregation combines vectors that are theoretically non-overlapping (orthogonal), preventing the averaging of conflicting gradients in shared dimensions. This approach assumes that shared dimensions contribute noise while unique dimensions contribute signal in heterogeneous task settings.

## Foundational Learning

- **Federated Averaging (FedAvg)**: Standard FL algorithm that averages client updates. Understanding this is prerequisite to recognizing what "interference" looks like when averaging gradients from different tasks.
  - Quick check: If Client A updates weights to minimize Loss A, and Client B updates weights to minimize Loss B, what happens to the global model weights if you simply average their updates?

- **Multi-Task Learning (MTL)**: Learning multiple tasks simultaneously using shared representations. Essential for understanding why different tasks (segmentation vs. depth) conflict when sharing a backbone.
  - Quick check: Why does a single neural network often struggle to learn two unrelated tasks simultaneously without specific architectural or optimization tricks?

- **Gradient/Update Magnitude as Salience**: The heuristic that "update magnitude" equals "task relevance." This is a form of gradient-based saliency or importance scoring.
  - Quick check: In standard SGD, does a larger gradient magnitude always imply a more "important" feature, or can it also imply instability?

## Architecture Onboarding

- **Component map**: Local Client (Trains model → Computes Update Δ_k) → FedDEA Transform (Masking → Rescaling) → Server (Aggregates transformed updates)
- **Critical path**: 1) Calculating the local update Δ, 2) Determining the threshold for top ρ percentiles, 3) Applying the mask and rescaling factor before aggregation
- **Design tradeoffs**: Selection ratio ρ controls tradeoff between signal preservation and interference suppression. Low ρ increases sparsity but risks losing useful gradients. High ρ retains more signal but introduces more noise.
- **Failure signatures**: 
  - Divergence if ρ is too small (<0.1), causing amplification of update noise
  - Degradation if tasks are highly correlated, as filtering may remove useful shared features
  - Performance drops if rescaling is removed, confirming naive compression kills optimization
- **First 3 experiments**:
  1. Baseline Verification: Implement FedAvg on a 2-task setup to observe interference
  2. Ablation on Rescaling: Run FedDEA with and without 1/ρ rescaling factor
  3. Sensitivity Analysis: Sweep ρ ∈ [0.1, 0.9] to identify optimal sparsity level

## Open Questions the Paper Calls Out
- Can the selection ratio ρ be adapted dynamically per task or round rather than remaining a fixed hyperparameter?
- How does FedDEA perform when task-specific subspaces significantly overlap or are not approximately orthogonal?
- How can FedDEA be extended to support federated multimodal learning where data modalities differ across clients?

## Limitations
- The magnitude-based filtering heuristic may not hold for all task combinations or architectures
- The rescaling factor of 1/ρ is a heuristic without theoretical grounding
- The paper assumes a single selection ratio works well across tasks, but performance is sensitive to this hyperparameter

## Confidence
- **High confidence** in empirical demonstration that FedDEA improves performance on tested datasets
- **Medium confidence** in the core mechanism as the primary driver of improvement
- **Low confidence** in the claim that this approach is "broadly applicable" without task labels or architectural changes

## Next Checks
1. Ablation on Magnitude vs. Other Heuristics: Replace magnitude-based masking with random or gradient norm-based masks
2. Task Correlation Analysis: Test on highly correlated tasks to see if filtering harms performance
3. Cross-Architecture Transfer: Validate FedDEA on non-vision tasks to assess generality beyond visual tasks