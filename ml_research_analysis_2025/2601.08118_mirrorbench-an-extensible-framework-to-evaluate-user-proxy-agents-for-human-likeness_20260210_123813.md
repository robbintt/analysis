---
ver: rpa2
title: 'MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness'
arxiv_id: '2601.08118'
source_url: https://arxiv.org/abs/2601.08118
tags:
- judge
- user
- evaluation
- proxy
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MirrorBench introduces a reproducible, extensible framework for
  evaluating user-proxy agents by measuring their ability to produce human-like utterances,
  independent of task success. The framework combines lexical-diversity metrics (MATTR,
  Yule's K, HD-D) with judge-based realism metrics (GTEval, Pairwise Indistinguishability,
  Rubric-and-Reason) across four diverse conversational datasets.
---

# MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness

## Quick Facts
- **arXiv ID**: 2601.08118
- **Source URL**: https://arxiv.org/abs/2601.08118
- **Reference count**: 40
- **Primary result**: Framework combines lexical-diversity and judge-based realism metrics to measure human-likeness of user-proxy agents across four conversational datasets, revealing systematic gaps and a realism-diversity tension.

## Executive Summary
MirrorBench introduces a reproducible, extensible framework for evaluating user-proxy agents by measuring their ability to produce human-like utterances, independent of task success. The framework combines lexical-diversity metrics (MATTR, Yule's K, HD-D) with judge-based realism metrics (GTEval, Pairwise Indistinguishability, Rubric-and-Reason) across four diverse conversational datasets. Evaluations reveal systematic gaps between user proxies and real human users, with strong LLMs like Gemini-2.5-Pro and Claude-4-Sonnet showing the highest human-likeness scores, while also exposing a realism-diversity tension that varies by dataset. The framework includes robust calibration controls (Human-Human/Proxy-Proxy) and demonstrates judge sensitivity, underscoring the importance of multi-judge reporting for reliable comparisons.

## Method Summary
MirrorBench evaluates user-proxy agents through synthetic conversations where proxies interact with assistants while being compared against human reference data. The framework uses lexical diversity metrics (MATTR, Yule's K, HD-D) normalized via z-scores against human baselines, combined with judge-based realism metrics (GTEval, Pairwise Indistinguishability, Rubric-and-Reason) that employ strong LLMs with calibration controls. Four conversational datasets (ChatbotArena, ClariQ, OASST1, QULAC) provide stratified subsets of 195-200 samples each. Proxies are conditioned on synthesized goals and conversation history, generating multi-turn interactions that are scored and aggregated with 95% confidence intervals.

## Key Results
- Strong LLMs (Gemini-2.5-Pro, Claude-4-Sonnet) achieve highest human-likeness scores, while smaller models like Gemma-2-9B lag significantly
- Systematic realism-diversity tension observed: proxies with high judge-based realism often undershoot human lexical diversity, varying by dataset
- Judge sensitivity confirmed: PI metric most volatile to judge swaps, though HH/PP calibration provides interpretable scaling
- Single-turn evaluations less stable than multi-turn contexts, validating the framework's conversational approach

## Why This Works (Mechanism)

### Mechanism 1: Human-Anchored Z-Score Normalization
Normalizing proxy lexical scores against human baselines from the same dataset yields interpretable, cross-dataset comparable metrics. For each metric, compute mean μ_hum and std σ_hum from human utterances, then transform proxy scores to z-scores: z = (m(̂t) - μ_hum) / σ_hum. Z ≈ 0 indicates human-matching lexical behavior. Core assumption: human behavior within a conversational regime is the appropriate distributional anchor.

### Mechanism 2: Dual-Metric Family Complementarity
Combining lexical-diversity and judge-based realism metrics captures complementary aspects of human-likeness that neither family measures alone. Lexical metrics quantify vocabulary richness and repetition; judge metrics assess tone, naturalness, and appropriateness. The framework aggregates both, exposing a realism-diversity tension. Core assumption: judge-based realism and lexical diversity are partially decoupled constructs.

### Mechanism 3: Calibration via Human-Human and Proxy-Proxy Controls
HH and PP control conditions expose judge biases and place scores on an interpretable scale. HH compares human conversations to themselves (ceiling); PP compares proxy outputs to themselves (baseline). Calibrated scores rescale raw values: s_cal = clip((s_raw - μ_PP) / max(ε, μ_HH - μ_PP), 0, 1). Core assumption: LLM judges exhibit systematic biases that can be diagnosed and partially corrected via controls.

## Foundational Learning

- **Lexical Diversity Metrics (TTR, MATTR, Yule's K, HD-D)**:
  - Why needed here: These quantify vocabulary richness and repetition patterns. Raw TTR is length-biased; MATTR, HD-D, and Yule's K provide length-robust alternatives.
  - Quick check question: Why does raw type-token ratio decrease with text length, and how does MATTR address this?

- **LLM-as-Judge Evaluation Paradigm**:
  - Why needed here: Judge metrics (GTEval, PI, RNR) use strong LLMs to rate realism. Understanding prompt design, chain-of-thought reasoning, and self-consistency is essential.
  - Quick check question: What is the difference between comparative (GTEval, PI) and absolute (RNR) judge metrics?

- **Z-Score Normalization and Confidence Intervals**:
  - Why needed here: Human-anchored z-scores and Student-t confidence intervals enable variance-aware, interpretable comparisons across datasets.
  - Quick check question: Given a proxy z-score of -0.5 on Yule's K, what does this mean about repetition relative to humans?

## Architecture Onboarding

- **Component map**: CLI -> Config Parser -> Planner -> Run Controller -> SQLite DB -> Backend (sync/async/Ray) -> Unit Executor -> Task Driver -> Model Clients -> Proxy/Assistant/Dataset/Metric Plugins -> Results Persistence -> Report Generator

- **Critical path**: CLI parses config → Planner validates and decomposes job into (proxy, dataset, metric, seed) units → Run Controller initializes SQLite DB, selects backend, dispatches units → Unit Executor loads dataset, instantiates proxy/assistant, runs Task Driver to synthesize rollout → Metric computed per episode → aggregated with mean, SD, 95% CI → Results persisted; report generated via `mirrorbench report json`

- **Design tradeoffs**: Temperature=0 for all LLMs reduces variance but may miss natural human variability; Judge can differ from proxy/assistant (recommended: Claude-4-Sonnet as conservative, stable judge); Caching reduces cost but requires invalidation if prompts change

- **Failure signatures**: Episodes with <5 tokens flagged as unstable; Missing human references excluded from GTEval/PI computation; Judge self-preference detected via HH/PP gap near zero

- **First 3 experiments**:
  1. Run single proxy on one dataset with all six metrics; inspect HH/PP calibration values to validate judge behavior
  2. Swap judge model (e.g., Claude-4-Sonnet → GPT-4o) on same proxy/dataset; quantify sensitivity (expected: PI most volatile per §4.2)
  3. Vary seed (5 runs) to confirm stability; expect flat trajectories per §4.5 if configuration is sound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks extend beyond surface-level lexical statistics to capture interaction-level diversity, such as conversational repair and long-horizon consistency?
- Basis in paper: [explicit] The authors state in the Limitations section that "lexical diversity captures surface-level distributional variation and does not fully represent interaction-level diversity such as repair, initiative, or long-horizon consistency."
- Why unresolved: Current metrics (MATTR, HD-D) measure vocabulary richness but fail to assess pragmatic competence or consistency across extended interactions.
- What evidence would resolve it: A new metric suite that successfully quantifies pragmatic behaviors and correlates with human ratings of long-term interaction realism.

### Open Question 2
- Question: Can calibration techniques be developed to fully mitigate model-family bias and self-preference in LLM-as-a-judge evaluations?
- Basis in paper: [explicit] The authors note that while HH/PP controls are useful, they "do not eliminate all sources of bias" and judges may exhibit "family/self-preference" (Section 4.2).
- Why unresolved: The results show significant judge sensitivity, implying that current normalization anchors are insufficient to remove systematic preferences for specific model families.
- What evidence would resolve it: A calibration protocol that equalizes proxy rankings regardless of which strong LLM is selected as the judge.

### Open Question 3
- Question: Is the observed "realism-diversity tension" a fundamental trade-off, or can user proxies be optimized to simultaneously maximize behavioral realism and lexical diversity?
- Basis in paper: [inferred] The paper identifies a "realism-diversity tension" where proxies with high realism scores often under-match human lexical diversity, motivating the use of complementary metrics.
- Why unresolved: It is unclear if this tension is an artifact of specific training data (e.g., RLHF biasing toward verbosity) or a conflict in the model's objective functions.
- What evidence would resolve it: Demonstration of a prompting strategy or fine-tuning method that achieves human-level z-scores on diversity without degrading GTEval or PI metrics.

## Limitations
- Dataset Representativeness: Four English conversational datasets may not capture full spectrum of human conversational behavior across languages and domains
- Judge Model Reliability: Framework remains fundamentally dependent on LLM judges whose reliability varies with prompt design and model versions
- Task-Independence Trade-off: Decoupling from task success measures surface-level human-likeness rather than functional competence

## Confidence

**High Confidence**:
- Lexical diversity metrics can be meaningfully z-scored against human baselines to measure deviation from human vocabulary patterns
- Judge-based metrics with HH/PP controls provide interpretable, bias-adjusted realism scores
- Strong LLMs (Gemini-2.5-Pro, Claude-4-Sonnet) consistently achieve higher human-likeness scores than smaller models
- Multi-turn conversation contexts are necessary for reliable proxy evaluation (single-turn results are less stable)

**Medium Confidence**:
- The realism-diversity tension varies systematically by dataset and proxy model
- Judge sensitivity is real but manageable through calibration and multi-judge reporting
- The framework's extensibility successfully supports new datasets, metrics, and proxy models

**Low Confidence**:
- Quantitative claim that "RLHF may have inadvertently penalized lexical diversity" - while supported by qualitative observations, the causal mechanism requires further investigation
- The framework's ability to generalize beyond tested LLMs and datasets remains unproven

## Next Checks
1. **Judge Sensitivity Replication**: Systematically vary judge models (GPT-4o, Claude-4-Sonnet, Gemini-2.5-Pro) across all proxy-dataset combinations and quantify rank correlation stability. This validates whether reported rankings are judge-dependent or robust.

2. **Cross-Lingual Extension**: Apply the framework to non-English conversational datasets (e.g., Japanese OpenDialKG, Chinese DuConv) to test whether z-score normalization and judge-based metrics maintain validity across languages and cultural contexts.

3. **Task-Success Correlation Study**: Design an experiment where the same proxies are evaluated for both human-likeness (using MirrorBench) and task completion rate on a standardized benchmark. This would quantify the actual trade-off between human-likeness and functional competence that the framework intentionally decouples.