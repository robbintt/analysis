---
ver: rpa2
title: 'SCORE: A Semantic Evaluation Framework for Generative Document Parsing'
arxiv_id: '2509.19345'
source_url: https://arxiv.org/abs/2509.19345
tags:
- evaluation
- document
- table
- metrics
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fairly evaluating multi-modal
  generative document parsing systems, which often produce semantically correct but
  structurally divergent outputs. Traditional metrics like CER, WER, IoU, and TEDS
  misclassify such diversity as error, penalizing valid interpretations and obscuring
  system behavior.
---

# SCORE: A Semantic Evaluation Framework for Generative Document Parsing

## Quick Facts
- arXiv ID: 2509.19345
- Source URL: https://arxiv.org/abs/2509.19345
- Reference count: 40
- Introduces SCORE framework for semantically grounded evaluation of generative document parsing systems

## Executive Summary
This paper addresses the challenge of fairly evaluating multi-modal generative document parsing systems, which often produce semantically correct but structurally divergent outputs. Traditional metrics like CER, WER, IoU, and TEDS misclassify such diversity as error, penalizing valid interpretations and obscuring system behavior. The authors introduce SCORE (Structural and COntent Robust Evaluation), an interpretation-agnostic framework integrating adjusted edit distance for robust content fidelity, token-level diagnostics to distinguish hallucinations from omissions, table evaluation with spatial tolerance and semantic alignment, and hierarchy-aware consistency checks. This enables evaluation that embraces representational diversity while enforcing semantic rigor.

Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE consistently revealed cross-dataset performance patterns missed by standard metrics. In 2-5% of pages with ambiguous table structures, traditional metrics penalized systems by 12-25% on average, leading to distorted rankings. SCORE corrected these cases, recovering equivalence between alternative but valid interpretations. Moreover, by normalizing generative outputs into a format-agnostic representation, SCORE reproduces traditional scores (e.g., table F1 up to 0.93) without requiring object-detection pipelines, demonstrating that generative parsing alone suffices for comprehensive evaluation. By exposing how interpretive diversity impacts evaluation outcomes and providing multi-dimensional, interpretable diagnostics, SCORE establishes foundational principles for semantically grounded, fair, and practical benchmarking of modern document parsing systems.

## Method Summary
SCORE introduces a semantic evaluation framework that integrates four core components: adjusted edit distance for content fidelity, token-level diagnostics distinguishing hallucinations from omissions, spatial-tolerant table evaluation with semantic alignment, and hierarchy-aware consistency checks. The framework employs a unified token-based representation where text and table content share the same token space, enabling consistent evaluation across modalities. Spatial tolerance is parameterized at 3mm and 20% for table cell matching, while semantic alignment uses Levenshtein distance to assess cell equivalence beyond exact string matching. The framework produces multiple interpretable metrics including Content Fidelity (CF), Spatial Fidelity (SF), Table Fidelity (TF), and Hierarchic Consistency (HC) scores.

## Key Results
- Traditional metrics penalize generative systems by 12-25% on average in 2-5% of pages with ambiguous table structures
- SCORE corrects these metric distortions, recovering equivalence between structurally divergent but semantically equivalent interpretations
- Generative parsing alone achieves table F1 scores up to 0.93 when outputs are normalized into format-agnostic representations

## Why This Works (Mechanism)
SCORE works by recognizing that semantic equivalence can exist across structurally different representations. The framework employs adjusted edit distance that accounts for semantic similarity rather than exact string matching, token-level diagnostics that separate hallucinations from omissions, and spatial tolerance parameters that acknowledge the inherent ambiguity in table structure interpretation. By normalizing diverse generative outputs into a common representation space, SCORE can evaluate semantic fidelity while remaining agnostic to interpretive choices that don't affect meaning.

## Foundational Learning
- **Semantic vs. Structural Equivalence**: Understanding that documents can have equivalent meaning with different structural representations is crucial for fair evaluation. Quick check: Compare two tables with reordered rows/columns but identical content.
- **Token-Based Unification**: Using a shared token space for text and table content enables consistent evaluation across document elements. Quick check: Verify that text tokens and table cell tokens use the same vocabulary.
- **Spatial Tolerance Parameters**: 3mm and 20% tolerances acknowledge practical limitations in table structure interpretation. Quick check: Test boundary cases at tolerance thresholds to ensure robustness.
- **Hierarchical Consistency**: Evaluating nested structures requires awareness of parent-child relationships in document hierarchies. Quick check: Verify that nested list items maintain correct parent associations across interpretations.

## Architecture Onboarding
**Component Map**: Document Parser -> Token Normalizer -> Semantic Evaluator -> Metric Generator
**Critical Path**: Token normalization and semantic alignment form the core evaluation pipeline, with spatial tolerance applied during table matching and hierarchy checks performed as a final validation step.
**Design Tradeoffs**: The framework prioritizes semantic fidelity over structural rigidity, accepting that some representational diversity is valid. This comes at the cost of more complex evaluation logic compared to exact-matching approaches.
**Failure Signatures**: Poor semantic alignment thresholds may cause false negatives; inadequate spatial tolerance can misclassify valid interpretations; hierarchy check failures indicate structural misinterpretation rather than semantic error.
**First Experiments**: 1) Run baseline evaluation on a simple document with known ambiguities, 2) Test spatial tolerance parameters at boundary conditions, 3) Compare SCORE metrics against human judgments on ambiguous cases.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scope limited to 1,114 pages, potentially missing diversity in real-world document structures
- Spatial tolerance parameters (3mm and 20%) may not generalize across all document types and resolutions
- Framework effectiveness for non-tabular semantic structures like hierarchical lists or complex forms remains untested

## Confidence
- High: SCORE's ability to detect and correct metric distortions in ambiguous cases (2-5% of pages)
- High: Equivalence recovery between structurally divergent but semantically equivalent interpretations
- Medium: Generative parsing alone suffices for comprehensive evaluation without object-detection pipelines
- Medium: Framework generalizability to document types beyond tested domain

## Next Checks
1. Test SCORE on a broader corpus including invoices, receipts, and legal documents to assess cross-domain robustness
2. Conduct ablation studies removing individual components (edit distance adjustments, spatial tolerance, hierarchy checks) to quantify their independent contributions to evaluation accuracy
3. Compare SCORE's semantic equivalence judgments against human annotators on a stratified sample of ambiguous cases to validate the framework's interpretation-agnostic decisions