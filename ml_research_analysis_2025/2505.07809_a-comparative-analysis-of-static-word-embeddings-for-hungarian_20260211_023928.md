---
ver: rpa2
title: A Comparative Analysis of Static Word Embeddings for Hungarian
arxiv_id: '2505.07809'
source_url: https://arxiv.org/abs/2505.07809
tags:
- embeddings
- static
- word
- hungarian
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares static word embeddings for Hungarian, evaluating
  traditional models (Word2Vec, FastText) and static embeddings derived from BERT-based
  models (huBERT, XLM-R) using different extraction methods (decontextualized, aggregate,
  X2Static). Intrinsic evaluation on a word analogy task showed FastText achieved
  71% accuracy and 0.77 MRR, while huBERT with X2Static extraction achieved 49% accuracy
  and 0.58 MRR.
---

# A Comparative Analysis of Static Word Embeddings for Hungarian

## Quick Facts
- **arXiv ID:** 2505.07809
- **Source URL:** https://arxiv.org/abs/2505.07809
- **Reference count:** 40
- **Primary result:** huBERT with X2Static extraction achieves 49% accuracy and 0.58 MRR on Hungarian word analogies, approaching traditional static embeddings' performance.

## Executive Summary
This paper evaluates static word embeddings for Hungarian using intrinsic word analogy tasks and extrinsic NER/POS tagging benchmarks. The study compares traditional static models (Word2Vec, FastText, HuSpaCy) with static embeddings extracted from BERT-based models (huBERT, XLM-R) using decontextualized, aggregate, and X2Static methods. FastText demonstrated superior intrinsic performance (71% accuracy), while X2Static-extracted embeddings showed the best combination of intrinsic and extrinsic results, particularly for BERT-based models. The findings suggest X2Static extraction effectively bridges the gap between contextual and static representations for Hungarian NLP tasks.

## Method Summary
The study evaluates embeddings using Hungarian word analogy datasets for intrinsic tasks and the NYTK-NerKor corpus for extrinsic NER/POS tagging. Traditional embeddings (FastText 300d, EFNILEX 600d, HuSpaCy 300d, ELMo 1024d) are compared with huBERT and XLM-R embeddings (768d) extracted via three methods: decontextualized (single-word input), aggregate (pooled multi-context), and X2Static (CBOW-inspired distillation). All embeddings are restricted to a common vocabulary of 256,808 words. Downstream evaluation uses a single-layer bidirectional LSTM with hidden sizes 1-64, dropout 0.5, and Adam optimization.

## Key Results
- FastText achieved the highest intrinsic performance with 71% accuracy and 0.77 MRR on word analogies
- huBERT with X2Static extraction achieved 49% accuracy and 0.58 MRR, outperforming other BERT-based extraction methods
- ELMo embeddings delivered the best extrinsic performance on NER and POS tagging tasks
- X2Static extraction method consistently outperformed decontextualized and aggregate approaches for BERT-based models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FastText's subword modeling provides measurable advantages for morphologically rich Hungarian in intrinsic semantic tasks.
- **Mechanism:** Character n-grams decompose words into substrings, enabling the model to form representations for rare or unseen morphological variants by composing from shared subword units.
- **Core assumption:** Morphological regularities in Hungarian can be captured at the subword level and generalize to whole-word semantics.
- **Evidence anchors:** FastText exhibited superior performance, achieving an overall accuracy of 71% and an MRR score of 0.77. Related work on Turkish shows similar patterns for agglutinative languages.

### Mechanism 2
- **Claim:** X2Static extraction preserves more transferable semantic structure from contextual models than decontextualized or aggregate approaches.
- **Mechanism:** X2Static uses a CBOW-inspired training objective where contextual embeddings from a teacher model (BERT) guide the learning of static word vectors.
- **Core assumption:** The teacher model's contextualized representations encode word semantics that can be compressed into single vectors without catastrophic information loss.
- **Evidence anchors:** huBERTx2 emerged as the best performer among BERT-based models, with an accuracy of 49% and an MRR score of 0.58. X2Static method emerged as the best-performing approach across experiments.

### Mechanism 3
- **Claim:** Architecture alignment between pretraining and downstream evaluation may contribute to ELMo's extrinsic task superiority.
- **Mechanism:** ELMo is trained using bidirectional LSTM language modeling, and the extrinsic evaluation also employs a bidirectional LSTM.
- **Core assumption:** Representation transfer is more efficient when source and target architectures share inductive biases.
- **Evidence anchors:** ELMo achieved the highest accuracy across most hidden sizes for both NER and POS tagging. This may come from the fact that ELMo is originally trained using a bidirectional LSTM, similarly to the model used in the extrinsic evaluations.

## Foundational Learning

- **Concept:** Static vs. Contextualized Embeddings
  - **Why needed here:** The paper's central comparison depends on understanding that static embeddings assign one vector per word regardless of context, while contextualized embeddings generate different vectors for the same word in different sentences.
  - **Quick check question:** Given the sentence "The bank closed" vs. "She sat on the bank," would a static embedding distinguish these senses?

- **Concept:** Mean Reciprocal Rank (MRR)
  - **Why needed here:** The intrinsic evaluation relies on MRR as the primary metric beyond accuracy.
  - **Quick check question:** If the correct answer appears at rank 3 for query A and rank 1 for query B, what is the MRR across both queries?

- **Concept:** Subword Tokenization and Character N-grams
  - **Why needed here:** FastText's superior performance is attributed to subword modeling.
  - **Quick check question:** For the word "running" with n-gram range 3-6, what partial n-grams might capture the "-ing" morpheme?

## Architecture Onboarding

- **Component map:**
  Traditional Static Models: Word2Vec (EFNILEX) 600d → FastText 300d → HuSpaCy 300d
  Contextualized → Static Extraction: huBERT (768d) → XLM-R (768d) → ELMo (1024d)
  Downstream: BiLSTM classifier (hidden sizes: 1-64)

- **Critical path:** For deploying Hungarian NER/POS with limited compute: Use X2Static-extracted huBERT embeddings → Train single-layer BiLSTM with hidden size 16-32 → Expect ~97% accuracy on NER, ~93% on POS.

- **Design tradeoffs:**
  - FastText: Best intrinsic semantics, competitive extrinsic, smallest model—choose for interpretability and resource constraints.
  - X2Static from huBERT: Near-static performance intrinsically, superior extrinsically—choose for best downstream accuracy at moderate cost.
  - ELMo: Best extrinsic but 1024d and contextualized inference cost—choose only if accuracy margin justifies compute.
  - Aggregate/Decontextualized from XLM-R: Consistently worst—avoid for Hungarian-specific tasks.

- **Failure signatures:**
  - Decontextualized extraction yields <2% accuracy on word analogies—indicates method fundamentally unsuitable for semantic tasks.
  - Sharp accuracy drops at hidden size 1-2 for XLM-R variants—signals vocabulary/representation mismatch.
  - Aggregate extraction underperforms decontextualized for huBERT on POS—suggests context pooling may introduce noise.

- **First 3 experiments:**
  1. **Vocabulary coverage check:** Before selecting embeddings, intersect your target vocabulary with each model's vocabulary. Report coverage percentage; if <85%, FastText's subword mechanism may help.
  2. **Baseline comparison:** Train identical BiLSTM (hidden=32) with FastText vs. huBERT_x2 on a held-out dev set. Expect ~0.5-1% difference on NER; if gap >2%, investigate data-domain mismatch.
  3. **Extraction method ablation:** For any BERT-based model you consider, extract using all three methods and evaluate on a 1000-sample word analogy subset. If X2Static does not outperform aggregate by >5 MRR points, verify training data quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can static word embeddings be effectively extracted from Hungarian GPT-based models, and how would they compare to the BERT-based embeddings evaluated in this study?
- **Basis in paper:** The Conclusion identifies the exploration of Hungarian GPT-based models as a significant area for future investigation, noting they were excluded due to the lack of a clear extraction methodology.
- **Why unresolved:** The study only covers encoder-only (BERT) and traditional architectures; decoder-only models like GPT may require different extraction techniques to produce static vectors.
- **What evidence would resolve it:** A study proposing and validating an extraction method for GPT models using the same intrinsic (analogy) and extrinsic (NER/POS) benchmarks.

### Open Question 2
- **Question:** What are the trade-offs between efficiency and accuracy when applying dimensionality reduction to the various Hungarian static embedding models?
- **Basis in paper:** The Conclusion suggests investigating the impact of dimensionality reduction, as the evaluated models utilized varying dimensional settings (300 to 1024).
- **Why unresolved:** It is unclear if the higher performance of larger embeddings (like ELMo) is intrinsic to the model architecture or a result of higher dimensionality.
- **What evidence would resolve it:** A controlled experiment where all embeddings are reduced to a uniform dimension (e.g., 300) and re-evaluated on the NER and POS tasks.

### Open Question 3
- **Question:** Would a new intrinsic evaluation dataset for Hungarian alter the observed performance hierarchy between traditional static embeddings and X2Static embeddings?
- **Basis in paper:** The Conclusion calls for the development of a new intrinsic evaluation dataset, stating that the word analogy dataset is currently the sole benchmark.
- **Why unresolved:** The reliance on a single analogy dataset limits the generalizability of the intrinsic evaluation results, potentially favoring models like FastText that excel at syntactic analogies.
- **What evidence would resolve it:** The creation of a word similarity or relatedness dataset for Hungarian and a subsequent re-evaluation of the models.

## Limitations

- X2Static implementation details (window size, negative sampling, epochs) are underspecified for Hungarian extraction
- Architecture-alignment hypothesis for ELMo's performance advantage remains speculative without controlled ablation studies
- BERT extraction method effectiveness depends heavily on context selection and pooling strategy details that are not fully specified

## Confidence

- **High Confidence:** FastText's subword modeling provides measurable advantages for Hungarian; X2Static extraction outperforms other BERT-based methods; aggregate extraction underperforms in most conditions.
- **Medium Confidence:** ELMo's extrinsic superiority stems from architectural alignment with downstream BiLSTM; huBERT with X2Static approaches traditional static embeddings' performance.
- **Low Confidence:** Claims about X2Static preserving more transferable semantic structure than other extraction methods; generalization of FastText advantages to other morphologically rich languages.

## Next Checks

1. **Vocabulary Coverage Validation:** For your target Hungarian vocabulary, compute intersection coverage with each embedding model. If coverage <85%, FastText's subword mechanism becomes critical—verify this threshold holds for your specific domain.

2. **Architecture Alignment Test:** Train identical BiLSTM downstream models with ELMo vs. X2Static huBERT on a dev set. If ELMo outperforms by >2%, investigate whether substituting transformers in downstream architecture eliminates this gap, confirming or refuting the alignment hypothesis.

3. **Extraction Method Stability:** For any BERT-based model you deploy, extract embeddings using all three methods and evaluate on 1000-sample analogy subset. If X2Static doesn't outperform aggregate by >5 MRR points, examine training data quality and teacher model alignment—implementation errors may be present.