---
ver: rpa2
title: Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated
  Learning
arxiv_id: '2506.04071'
source_url: https://arxiv.org/abs/2506.04071
tags:
- learning
- local
- data
- which
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Optimal Transport-based preprocessing
  method for federated learning to address dataset imbalance across edge devices.
  The method uses Wasserstein barycenters to align local datasets into a shared representation
  space, reducing distributional discrepancies.
---

# Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning

## Quick Facts
- arXiv ID: 2506.04071
- Source URL: https://arxiv.org/abs/2506.04071
- Reference count: 0
- Primary result: OT-based preprocessing achieves up to 99.62% CIFAR-10 accuracy in federated learning with reduced communication rounds

## Executive Summary
This paper introduces a preprocessing method for federated learning that addresses dataset imbalance across edge devices using Optimal Transport (OT). The approach computes channel-wise Wasserstein barycenters to create a shared target RGB space, then projects local images to this space before training. By reducing distributional discrepancies across clients, the method improves generalization and accelerates convergence compared to standard FedAvg. Experiments on CIFAR-10 demonstrate significant accuracy improvements, reaching 99.62% accuracy with fewer communication rounds.

## Method Summary
The method performs zero-shot distribution alignment before federated learning training begins. Each client computes local Wasserstein barycenters for R, G, and B channels independently using the POT library. These local barycenters are aggregated at a central server to compute a global barycenter representing the target RGB space. All clients then project their local images to this target space using OT transport plans computed via the Sinkhorn algorithm. Standard FedAvg training proceeds on the preprocessed data using a custom CNN or ResNet9 architecture. The approach is model-agnostic and computationally efficient, avoiding iterative OT optimization during training.

## Key Results
- Achieves 99.62% accuracy on CIFAR-10 with 5 clients in 35 communication rounds
- Standard FedAvg without preprocessing achieves only 71.22% accuracy
- Consistent improvements across varying client counts (5 to 100) and participation rates
- Reduces variance across agents' samples, accelerating convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Channel-wise Wasserstein barycenters create a shared representation space that encodes statistical information from all participating agents.
- **Mechanism:** Each agent computes local Wasserstein barycenters for R, G, B channels independently. These are aggregated centrally to form a global barycenter (target RGB space). Local images are then projected to this target space via optimal transport plans, which act as color transfer maps. This transforms heterogeneous local distributions into a unified representation.
- **Core assumption:** The Wasserstein barycenter of local barycenters sufficiently captures global distribution characteristics needed for alignment. Assumption: Channel-wise independence (treating R, G, B separately) does not lose critical joint distribution information.
- **Evidence anchors:** [abstract] "We accomplish this by leveraging Wasserstein barycenters when computing channel-wise averages. These barycenters are collected in a trusted central server where they collectively generate a target RGB space." [Section 4] "The approach requires separating each local image its three color channels, grouping them by the respective color channels, and lastly computing the WB for each channel." [corpus] FedAVOT (arXiv:2509.14444) similarly uses masked optimal transport for distribution alignment in FL, suggesting OT-based alignment is an active research direction, though results are not directly comparable.
- **Break condition:** If local distributions have fundamentally different structural properties (e.g., one agent has grayscale-like images, another has saturated colors), channel-wise barycenters may not produce a meaningful unified space.

### Mechanism 2
- **Claim:** Zero-shot preprocessing alignment reduces computational overhead compared to dynamic OT methods trained during model optimization.
- **Mechanism:** Unlike FedOT (Farnia et al.), which solves a min-max optimization problem during training to learn transformation maps, this method computes alignment maps once before training begins. The Sinkhorn algorithm and Bregman projections compute regularized transport plans in O(d²) complexity, avoiding iterative OT optimization throughout training epochs.
- **Core assumption:** The preprocessing-time alignment remains valid throughout training—i.e., the target space does not need to adapt as the model learns.
- **Evidence anchors:** [Section 2] "our approach performs alignment before training, avoiding iterative optimization" [Section 2] "our approach is, to our knowledge, the first to perform zero-shot distribution alignment in FL" [corpus] No direct corpus comparison for zero-shot vs. dynamic OT tradeoffs in FL. Evidence is primarily intra-paper.
- **Break condition:** If data distributions shift during training (concept drift) or if new agents join with significantly different distributions, static preprocessing becomes stale.

### Mechanism 3
- **Claim:** Distribution alignment reduces variance across agents' samples, accelerating convergence and improving generalization.
- **Mechanism:** By projecting all local data to a shared target space, the statistical heterogeneity (non-IID nature) is reduced. FedAvg then aggregates gradients from more homogeneous local objectives, leading to more stable global updates. The paper reports achieving 99.62% accuracy in 35 communication rounds (5 clients) vs. 71.22% for standard FedAvg.
- **Core assumption:** Reduced distributional variance translates to improved gradient alignment during aggregation. Assumption: The projection preserves discriminative features necessary for classification.
- **Evidence anchors:** [abstract] "facilitates the learning process due to a minimization of variance across the samples" [Table 1] Shows consistent accuracy improvements across varying client counts (5 to 100) and participation rates [corpus] Weak corpus evidence for variance-reduction mechanism specifically; related work (FedAlign, FedAVOT) focuses on alignment but does not isolate variance as the causal factor.
- **Break condition:** If projection inadvertently destroys class-discriminative information (e.g., by over-smoothing features), accuracy gains may not materialize despite reduced variance.

## Foundational Learning

- **Concept: Optimal Transport / Wasserstein Distance**
  - Why needed here: Core mathematical framework for measuring distributional discrepancy and computing alignment maps.
  - Quick check question: Can you explain why Wasserstein distance captures geometric structure better than KL divergence for image distributions?

- **Concept: Wasserstein Barycenters**
  - Why needed here: The averaging mechanism that creates both local representations and the global target space.
  - Quick check question: Given two 1D distributions [0.5, 0.5] at positions [0, 1] and [0.5, 0.5] at positions [2, 3], where would their barycenter lie?

- **Concept: Federated Averaging (FedAvg)**
  - Why needed here: The base FL algorithm used; understanding parameter averaging helps contextualize why heterogeneity harms aggregation.
  - Quick check question: In FedAvg with non-IID data, why do local objectives diverge, and how does this affect the global optimum?

## Architecture Onboarding

- **Component map:** Local agents compute channel-wise WBs → Central server computes global WB → All agents project to target space → FedAvg training on preprocessed data
- **Critical path:** 1) Ensure all agents can compute and transmit WBs before training begins (synchronization barrier) 2) Verify global WB computation completes before broadcasting 3) Projection must finish before any local training starts
- **Design tradeoffs:** Regularization parameter (1e-2 for transport, 1e-1 for barycenter): Higher values smooth transport but may lose precision; Pixel sampling (250 pixels): Reduces computation but introduces sampling noise; Channel-wise vs. joint RGB: Faster but ignores color correlations
- **Failure signatures:** Convergence stalls at low accuracy: Check if projection destroyed discriminative features; High variance in per-client accuracy: Target space may not generalize across all clients; Preprocessing timeout: Pixel sampling or regularization parameters may be too aggressive
- **First 3 experiments:** 1) Reproduce single-client baseline: Run FedAvg on CIFAR-10 with N=5, P=5, no preprocessing. Target: ~71% accuracy per Table 1. 2) Ablation on regularization: Vary Sinkhorn regularization (1e-3, 1e-2, 1e-1) and measure preprocessing time vs. accuracy tradeoff. 3) Heterogeneity stress test: Create extreme label imbalance (e.g., client 1 has only classes 0-4, client 2 has 5-9) and compare preprocessing vs. baseline. Verify if claims hold under pathological non-IID conditions not explicitly tested in the paper.

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the method perform when scaling to high-resolution, large-scale datasets such as ImageNet? Basis in paper: [explicit] The conclusion states: "In future work, we will explore larger datasets (e.g., ImageNet)..." Why unresolved: Current experiments are restricted to the low-resolution CIFAR-10 dataset, and the computational complexity ($O(M d^2)$) may be prohibitive for larger inputs without aggressive approximation. What evidence would resolve it: Empirical results (accuracy and convergence speed) from training on ImageNet using the proposed preprocessing pipeline.
- **Open Question 2:** Can the OT-based alignment approach be effectively adapted for temporal data modalities? Basis in paper: [explicit] The authors state they aim to "develop new OT-based methods for transforming temporal data" in the future work section. Why unresolved: The current implementation relies on "channel-wise" averages designed for static RGB images; temporal dependencies require different structural handling than spatial pixel distributions. What evidence would resolve it: A formulation of the algorithm for time-series data and evaluation on temporal FL benchmarks (e.g., sensor data).
- **Open Question 3:** What is the impact of the pixel-subsampling approximation on alignment quality and model performance? Basis in paper: [inferred] Section 6 notes that to reduce computation, the authors "uniformly sample a subset of 250 pixels" to compute barycenters. Why unresolved: It is unclear if sampling only 250 pixels (out of 1024 in CIFAR-10) sufficiently captures the data distribution, particularly for complex textures or higher resolutions. What evidence would resolve it: A sensitivity analysis showing the correlation between the number of sampled pixels, the accuracy of the Wasserstein barycenter, and the final global model accuracy.
- **Open Question 4:** To what extent does pixel-space alignment mitigate label distribution skew compared to feature-space heterogeneity? Basis in paper: [inferred] The abstract defines the problem as "non-uniform label distributions," yet the method aligns RGB channel distributions (pixel intensity), which addresses covariate shift rather than label probability divergence directly. Why unresolved: The paper assumes minimizing pixel variance equates to solving label imbalance, but the causal link between color alignment and label balance is not theoretically proven. What evidence would resolve it: Experiments on datasets with controlled separation between feature drift (covariate shift) and label drift to isolate which problem the method actually solves.

## Limitations
- Channel-wise independence assumption may not capture joint color distribution shifts; no validation that this simplification preserves sufficient information for alignment
- Zero-shot preprocessing assumes static distributions; no evidence provided for robustness to concept drift or new agent onboarding
- Limited ablation on hyperparameters (regularization, pixel sampling) beyond stated values
- No comparison to other distribution alignment methods in FL (FedAlign, FedAVOT) under identical experimental conditions

## Confidence
- Mechanism 1 (OT barycenters create shared space): Medium - supported by equations and implementation details but relies on untested assumption about channel independence
- Mechanism 2 (Zero-shot efficiency): High - clearly stated computational advantage over iterative OT methods like FedOT
- Mechanism 3 (Variance reduction improves generalization): Low - correlation between reduced variance and accuracy gains is asserted but not experimentally validated as causal

## Next Checks
1. Implement joint 3-channel OT computation and compare against channel-wise approach to quantify information loss from independence assumption
2. Test preprocessing robustness with concept drift: apply preprocessing on initial distributions, then introduce new agents/clients with shifted distributions mid-training
3. Compare against FedAVOT under identical experimental conditions to validate claimed advantages of zero-shot preprocessing approach