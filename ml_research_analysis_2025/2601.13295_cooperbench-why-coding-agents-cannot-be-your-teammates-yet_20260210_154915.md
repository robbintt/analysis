---
ver: rpa2
title: 'CooperBench: Why Coding Agents Cannot be Your Teammates Yet'
arxiv_id: '2601.13295'
source_url: https://arxiv.org/abs/2601.13295
tags:
- agents
- coordination
- agent
- cooperbench
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CooperBench, a benchmark of over 600 collaborative
  coding tasks designed to evaluate how well AI agents can coordinate when working
  on shared software projects. Each task involves two agents independently implementing
  different features in the same codebase, where conflicts arise if coordination is
  insufficient.
---

# CooperBench: Why Coding Agents Cannot be Your Teammates Yet

## Quick Facts
- arXiv ID: 2601.13295
- Source URL: https://arxiv.org/abs/2601.13295
- Reference count: 40
- Agents achieve 30% lower success rates when collaborating compared to working individually

## Executive Summary
CooperBench introduces a benchmark of over 600 collaborative coding tasks to evaluate AI agents' ability to coordinate when implementing features in shared codebases. The benchmark reveals a "curse of coordination" where agents struggle with semantic coordination despite managing spatial conflicts. Key findings show agents frequently fail to maintain accurate models of partner state, make unverifiable commitments, and focus on spatial coordination (where to edit) while neglecting semantic coordination (what values to implement). Despite heavy communication use, agents achieve only 59% of the success rate of solo work, highlighting fundamental limitations in current AI agents' social intelligence for teamwork.

## Method Summary
The benchmark uses OpenHands framework with SQL-based async communication between two isolated Docker containers per task. Agents receive separate feature specifications and execute up to 100 actions each, communicating only through text messages. Patches are merged using a three-stage pipeline (naive git merge → union merge → LLM merge resolver) and evaluated by running both feature test suites on the merged code. The benchmark includes 652 tasks across 12 repositories, with 77.3% containing conflicting ground-truth solutions.

## Key Results
- Agents achieve 30% lower success rates when collaborating versus working individually
- Communication reduces merge conflicts but does not improve task success
- Expectation failures account for 42% of coordination breakdowns as the most common failure mode

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Communication reduces merge conflicts but does not improve task success because agents coordinate spatially (where to edit) but not semantically (what values to implement).
- Mechanism: Agents successfully negotiate line numbers, file paths, and edit ranges. However, they fail to negotiate implementation semantics such as default parameter values or interface contracts. Merge conflict detection operates at the hunk level, so agents can avoid textual conflicts while still producing incompatible implementations.
- Core assumption: Task success requires both spatial coordination (avoid overlapping edits) and semantic coordination (agreeing on what behavior to implement).
- Evidence anchors:
  - [abstract]: "communication channels become jammed with vague, ill-timed, and inaccurate messages"
  - [section 5]: "Merge conflicts are fundamentally a spatial coordination problem... task success requires semantic coordination: understanding what to implement, not just where"
  - [section I.5]: Case study where agents exchanged 10 messages and 3000+ words on line numbers but never discussed the actual default value, causing failure
  - [corpus]: Limited direct corpus evidence on spatial-semantic gap; related work on "Trading Visual Uncertainties in Multi-Agent Bandit Systems" addresses uncertainty in coordination but not this specific distinction
- Break condition: If tasks require only spatial coordination (no semantic dependencies between features), this mechanism becomes less predictive of failure.

### Mechanism 2
- Claim: Coordination fails because agents make unverifiable commitments that partners cannot validate under workspace isolation.
- Mechanism: Each agent operates on a separate branch without visibility into the other's code. When an agent claims "I added the handler at line 50," the partner has no way to verify this claim. Trust-based coordination breaks down because agents default to verification-first behavior learned from single-agent interactions.
- Core assumption: Effective collaboration under isolation requires mechanisms that convert verbal commitments into checkable artifacts.
- Evidence anchors:
  - [section 6]: "agents deviate from their commitments" identified as key issue
  - [section 6.3]: Example where agent promises "I will add bypass check at lines 100-104" with completion checkmark, but code is absent after merge
  - [section 6.3]: "The trust paradox" - models resist unverifiable assertions, but collaboration requires trusting partner claims about unobservable state
  - [corpus]: "Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions" addresses alignment mechanisms but does not verify commitment tracking
- Break condition: If agents share workspace visibility or use verifiable commitment protocols (pasted signatures, explicit diffs), this mechanism weakens.

### Mechanism 3
- Claim: The primary coordination failure mode is expectation breakdown: agents do not maintain accurate models of partner state despite receiving explicit communication.
- Mechanism: Agents receive clear information about partner plans but fail to integrate it into their execution. They proceed as if partner code does not exist, overwriting or duplicating work. This reflects a theory-of-mind deficit where agents cannot simulate partner state during independent work.
- Core assumption: Coordination requires maintaining and updating an internal model of what the partner has done and will do.
- Evidence anchors:
  - [abstract]: "agents often hold incorrect expectations about others' plans and communication"
  - [section 6.2]: Expectation failures account for 42% of root causes, making it the most common failure mode
  - [section 6.3]: Example where Agent A announces plan to modify prompts.py and call B's function, but proceeds as if B's code won't exist
  - [corpus]: "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration" directly addresses partner modeling for adaptive coordination
- Break condition: If tasks have zero code overlap or strict file-level separation, expectation modeling becomes unnecessary.

## Foundational Learning

- Concept: Partial Observability in Multi-Agent Systems
  - Why needed here: CooperBench deliberately isolates agent workspaces. Coordination must occur through communication alone because neither agent can observe the other's code changes. This mirrors real git workflows where developers work on separate branches.
  - Quick check question: Why does git conflict detection at the hunk level (rather than line level) create genuine coordination requirements between agents?

- Concept: Theory of Mind in AI Systems
  - Why needed here: The paper identifies expectation failures (42% of cases) as the dominant coordination breakdown. Understanding that agents must model partner beliefs, intentions, and code state is essential for interpreting results.
  - Quick check question: How does the "trust paradox" explain why models trained to verify assertions struggle in collaborative settings?

- Concept: Speech Acts and Pragmatics in Communication
  - Why needed here: Agents "talk" frequently (up to 20% of steps) but fail to achieve communication goals. The paper identifies repetition, unresponsiveness, and hallucination as communication errors. Understanding that communication is functional, not just informational, is key.
  - Quick check question: What is the difference between passing messages and achieving coordination through communication?

## Architecture Onboarding

- Component map: Feature pools (34 pools across 12 repositories) -> Agent environment (2 isolated Docker containers) -> Communication tool (SQL-based message passing) -> Merge pipeline (3-stage resolution) -> Evaluation (merged patch test suites)
- Critical path: Load repository at base commit -> Agent 1 receives Feature A, Agent 2 receives Feature B -> Parallel execution with real-time messaging -> Extract patches and apply 3-stage merge -> Run combined test suite on merged codebase
- Design tradeoffs: Isolation vs visibility creates realistic coordination challenge but prevents verification; 100-action budget constrains exploration but may truncate complex coordination; text-only communication matches real workflows but limits bandwidth
- Failure signatures: Work overlap (33.2%), divergent architecture (29.7%), unresponsiveness (8.7%), high message count with low specificity, Plan:Question ratio below 1.5 correlates with higher conflict rates
- First 3 experiments:
  1. Establish baseline coordination gap: Run Solo vs Coop across all 652 tasks to measure the 30% success rate drop
  2. Communication ablation study: Compare with-comm vs no-comm settings to isolate whether communication helps conflicts (yes) vs success (no)
  3. Mine emergent coordination patterns: Extract successful trajectories showing role division, resource division, or negotiation; analyze what distinguishes them from failures

## Open Questions the Paper Calls Out

- Can multi-agent reinforcement learning methods (e.g., Sotopia-π) reliably reinforce emergent coordination behaviors and close the Solo-Coop performance gap on CooperBench?
- What lightweight protocols can make agent commitments verifiable under partial observability, and how much do they improve coordination success?
- Does scaling the number of cooperating agents cause linear, sublinear, or superlinear degradation in success rates, and what coordination mechanisms mitigate this?
- Can richer communication channels (e.g., screen sharing, shared visualizations) improve semantic coordination beyond what text-based messaging achieves?

## Limitations

- Benchmark focuses on Python/Typescript repositories which may not represent all coding domains
- 100-action budget may truncate complex coordination scenarios
- Merge resolver (Qwen2.5-Coder-0.5B) introduces potential bias in handling conflicts

## Confidence

- High confidence: The 30% performance penalty from coordination is directly measured across 652 tasks
- Medium confidence: Mechanisms explaining coordination failures rely on qualitative analysis of communication patterns
- Low confidence: Generalizability beyond specific agent architectures and OpenHands framework implementation

## Next Checks

1. **Ablation study on communication granularity**: Run tasks where agents can share code snippets versus only text messages to test whether the spatial-semantic gap is truly fundamental or an artifact of text-only communication.

2. **Cross-model replication**: Repeat the benchmark with smaller, more accessible models (e.g., Llama 3.1 8B) to determine if coordination failures scale with model size or are present across architectures.

3. **Controlled expectation manipulation**: Create tasks where partner plans are explicitly stated in the spec versus tasks requiring inference from communication, to quantify how much of the expectation failure is due to communication versus reasoning limitations.