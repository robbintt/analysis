---
ver: rpa2
title: 'From Faithfulness to Correctness: Generative Reward Models that Think Critically'
arxiv_id: '2509.25409'
source_url: https://arxiv.org/abs/2509.25409
tags:
- reward
- answer
- correctness
- correct
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving correctness in open-domain
  question answering through reinforcement learning with verifiable rewards (RLVR).
  While RLVR has shown success in domains like mathematics and coding, verifying correctness
  in complex, knowledge-intensive tasks like QA is difficult due to nuanced and ambiguous
  real-world knowledge.
---

# From Faithfulness to Correctness: Generative Reward Models that Think Critically

## Quick Facts
- arXiv ID: 2509.25409
- Source URL: https://arxiv.org/abs/2509.25409
- Authors: Qiyao Ma; Yunsheng Shi; Hongtao Tian; Chao Wang; Weiming Chang; Ting Yao
- Reference count: 25
- Key outcome: Sentence-level sequential faithfulness→reasoning→correctness evaluation substantially improves incorrect sentence detection (F1: 0.3447, +6.5%) and answer correctness (+5.9%) over baselines.

## Executive Summary
The paper addresses the challenge of improving correctness in open-domain question answering through reinforcement learning with verifiable rewards (RLVR). While RLVR succeeds in domains like mathematics and coding, verifying correctness in complex, knowledge-intensive tasks like QA is difficult due to nuanced and ambiguous real-world knowledge. Existing approaches conflate faithfulness (semantic alignment with documents) with correctness, leading to overreliance on external sources and reduced critical assessment. The proposed Thinking-supervised Reward Model (TRM) evaluates answer sentences in a sequence of faithfulness, reasoning, and correctness, enabling models to distinguish between document alignment and factual accuracy.

TRM first assesses how well each sentence aligns with supporting documents, then applies internal knowledge to evaluate correctness. Experiments show that TRM substantially improves the identification of incorrect sentences, achieving an F1 score of 0.3447 (a 6.5% improvement over baselines) and improving detection of incorrect answers by 5.9%. Incorporating TRM into policy optimization leads to significant gains in both answer correctness (up to 30.3%) and usefulness (up to 35%), demonstrating its effectiveness in enhancing the quality of generated answers.

## Method Summary
The paper proposes a Thinking-supervised Reward Model (TRM) that evaluates answer sentences through sequential faithfulness, reasoning, and correctness assessment. The method uses a two-stage training approach: supervised fine-tuning (SFT) on structured faithfulness→reasoning→correctness targets, followed by reinforcement learning (GRPO) with sentence-level dual rewards (correctness + 0.5×faithfulness). The policy optimization combines TRM with a preference reward model to optimize both correctness and usefulness. The approach uses Qwen2.5-32B-Instruct as the policy backbone and DeepSeek-R1-Distill-Qwen-32B as the reward model backbone.

## Key Results
- TRM achieves F1 score of 0.3447 for incorrect sentence detection, a 6.5% improvement over baselines
- TRM improves incorrect answer detection rate by 5.9% compared to existing methods
- Policy optimization with joint TRM+Prefer rewards yields up to 30.3% improvement in correctness and 35% improvement in usefulness
- RL-CF+ variant (with extra reward for correct incorrect-label predictions) consistently outperforms SFT baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential faithfulness→reasoning→correctness evaluation enables models to distinguish between document alignment and factual accuracy.
- Mechanism: TRM forces explicit intermediate reasoning by requiring models to first assess faithfulness (semantic alignment with documents), then apply internal knowledge to evaluate correctness. This prevents the conflation where models assume supporting documents are always correct.
- Core assumption: Models can learn transferable reasoning patterns through structured supervision that mirror human critical evaluation processes.
- Evidence anchors:
  - [abstract] "TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness."
  - [section 2.2] "By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge."
  - [corpus] "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation" (FMR=0.549) directly addresses faithfulness-correctness trade-offs in RAG settings.
- Break condition: When reasoning paths become too deterministic with "little room for exploration," RL optimization may produce spurious patterns that coincidentally yield correct answers without genuine understanding.

### Mechanism 2
- Claim: Sentence-level dual-signal rewards (correctness + faithfulness) with class-balancing improve detection of incorrect content.
- Mechanism: The reward function r_i,k = c(i,k) + α·f(i,k) combines correctness and faithfulness signals at sentence granularity. An additional reward for correctly predicting incorrect labels counteracts the 86.86% correct-label bias in the dataset.
- Core assumption: Sentence-level granularity provides sufficient signal for error localization without introducing excessive annotation complexity.
- Evidence anchors:
  - [section 2.3] "we integrate faithfulness as an intermediate reward in addition to the correctness signal within our RL framework, with both rewards assessed at the sentence level."
  - [section 3.4] "only the RL-CF+ variant consistently outperforms the SFT baseline, while RL-CF demonstrates superiority solely in answer-level detection rate, and RL-C underperforms across both metrics."
  - [corpus] Evidence for dual-signal reward effectiveness is limited in corpus; neighbor papers focus on single-objective reward modeling.
- Break condition: When α weighting is misconfigured or when the additional reward for incorrect labels is removed, models overfit to the majority correct class, degrading F1 scores for incorrect sentence detection.

### Mechanism 3
- Claim: External (documents) and internal (parametric) knowledge integration through explicit reasoning supervision improves policy optimization outcomes.
- Mechanism: TRM's structured evaluation teaches models to consult external sources first (faithfulness check), then apply internal reasoning to determine correctness. When combined with a preference reward model in policy optimization, this yields gains in both correctness (up to 30.3%) and usefulness (up to 35%).
- Core assumption: Models can internalize critical assessment patterns that generalize to unseen queries and out-of-distribution datasets.
- Evidence anchors:
  - [abstract] "TRM encourages models to critically assess and leverage both external and internal knowledge."
  - [section 4.3] "the joint model shows a substantial improvement in usefulness (from 33 to 43)" on challenging queries and "significant improvement in correctness (from 66 to 86)" on easier queries.
  - [corpus] "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models" (FMR=0.549) addresses similar knowledge integration challenges.
- Break condition: When policy optimization uses TRM alone without the preference reward model, correctness may improve but usefulness degrades; when using preference alone, correctness suffers.

## Foundational Learning

- Concept: Reinforcement Learning with Verifiable Rewards (RLVR)
  - Why needed here: TRM builds on RLVR principles but extends them from verifiable domains (math, coding) to ambiguous knowledge-intensive QA where correctness cannot be easily verified.
  - Quick check question: Can you explain why RLVR succeeds in mathematics but struggles with open-domain QA?

- Concept: Faithfulness vs. Correctness Distinction
  - Why needed here: The paper's core innovation depends on understanding that faithful answers (aligned with documents) can be factually incorrect if source documents are wrong or misleading.
  - Quick check question: Given a supporting document stating "Novel 1984 was published in 1949" and an answer "1984 was written in 1949," is this faithful, correct, both, or neither?

- Concept: Process-Supervised vs. Outcome-Supervised Reward Models
  - Why needed here: TRM compares against ORM (answer-level) and PRM (sentence-level correctness) baselines, positioning itself as a thinking-supervised approach that adds explicit reasoning chains.
  - Quick check question: What granularity of supervision does TRM provide compared to PRM, and what additional signal does it introduce?

## Architecture Onboarding

- Component map: Query + Documents + Answer → Sentence Segmentation → TRM Core (Faithfulness → Reasoning → Correctness) → Rewards → Policy Optimization → Final Answer
- Critical path:
  1. Data annotation: Two-stage human labeling (faithfulness first, then correctness with reasoning rationale)
  2. SFT training on structured Faithfulness→Reasoning→Correctness targets
  3. RL refinement with reward = correctness + 0.5×faithfulness + bonus for correct incorrect-label predictions
  4. Policy optimization deployment with joint TRM + Prefer rewards
- Design tradeoffs:
  - α=0.5 balances faithfulness vs. correctness rewards (tunable hyperparameter)
  - β controls correctness vs. usefulness trade-off in policy optimization
  - Sentence-level granularity vs. computational cost (93322 sentences in training set)
  - Additional reward for incorrect labels helps class imbalance but requires careful tuning
- Failure signatures:
  - TRM- (without reasoning path): F1=0.3238 vs. TRM F1=0.3384, confirming reasoning component necessity
  - RL-C (correctness-only reward): Underperforms SFT baseline on both metrics due to overfitting
  - TRM alone in policy optimization: Correctness improves but usefulness drops (33 win vs. 27 lose on Tencent dataset)
- First 3 experiments:
  1. **Baseline comparison**: Run ORM, PRM, TRM-, TRM, TRM+ on held-out test set (214 queries), measuring F1 (incorrect sentences) and detection rate (incorrect answers) to validate sequential evaluation benefit.
  2. **Ablation on reward signals**: Train SFT, RL-C, RL-CF, RL-CF+ variants to isolate contribution of faithfulness reward and incorrect-label bonus, tracking F1 and detection metrics across training steps.
  3. **Policy optimization integration**: Train policy model with Prefer-only, TRM-only, and Joint (TRM+Prefer) configurations on both Tencent (challenging) and CRUD (easier) datasets, evaluating correctness (GPT-4.1 sentence-level) and usefulness (preference comparison against anchor).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does reinforcement learning (RL) occasionally degrade performance compared to supervised fine-tuning (SFT) in the TRM framework?
- Basis in paper: [explicit] The authors explicitly ask, "Why does RL sometimes hurt performance?" in Section 3.4, noting that RL-C and RL-CF sometimes underperform compared to SFT.
- Why unresolved: The authors hypothesize that the "reasoning path may not be divergent enough," causing overfitting to narrow reward signals, but do not confirm if this is due to data constraints or algorithmic limitations.
- What evidence would resolve it: Ablation studies analyzing the diversity of reasoning trajectories generated during RL or testing the framework on tasks with inherently higher solution diversity.

### Open Question 2
- Question: How reliable are the automated metrics for correctness and usefulness given the reliance on GPT-4.1 as the primary evaluator?
- Basis in paper: [inferred] While Section 4.2 details the use of GPT-4.1 to score correctness and usefulness, the paper does not provide a correlation analysis between these automated scores and human judgment.
- Why unresolved: LLM-based evaluators can exhibit biases (e.g., preference for length or specific phrasing) and may miss subtle factual errors, potentially inflating performance metrics.
- What evidence would resolve it: A human evaluation study on a subset of the test data to calculate the agreement rate (e.g., Cohen's Kappa) between GPT-4.1 and human experts.

### Open Question 3
- Question: Does the "Faithfulness → Reasoning → Correctness" framework generalize to high-stakes domains where factual errors are subtler?
- Basis in paper: [inferred] The dataset is derived from general search engine queries (Section 3.1), and the paper acknowledges the "nuanced and ambiguous nature of real-world knowledge," yet validation is limited to general QA.
- Why unresolved: It is unclear if the model can effectively separate faithfulness from correctness in specialized fields (e.g., medicine, law) where external documents might be technically faithful but outdated or contextually incorrect.
- What evidence would resolve it: Benchmarking TRM on domain-specific datasets (e.g., legal case briefs or medical QA) where the cost of conflating faithfulness with correctness is high.

## Limitations
- The 86.86% class imbalance in correct labels raises questions about generalization to more balanced real-world distributions
- The sentence segmentation approach using punctuation and list markers may introduce noise in structured documents without error rate evaluation
- TRM's reliance on DeepSeek-R1-Distill-Qwen-32B limits scalability and accessibility compared to larger proprietary models

## Confidence
- **High confidence**: Sequential faithfulness→reasoning→correctness evaluation mechanism (supported by direct ablation showing TRM- underperforms TRM by 1.5% F1)
- **Medium confidence**: Dual-signal reward effectiveness (based on limited comparison with neighbor papers focused on single-objective reward modeling)
- **Medium confidence**: Policy optimization gains with joint TRM+Prefer (results show mixed performance across datasets, with usefulness gains offset by some correctness drops)

## Next Checks
1. **Class imbalance robustness test**: Train TRM on artificially balanced datasets (50/50 correct-incorrect split) and evaluate whether the reasoning component still provides benefits or if the model reverts to faithfulness-only patterns
2. **Segmentation error analysis**: Conduct a human evaluation of 100 randomly selected sentences to measure segmentation accuracy and assess correlation between segmentation errors and correctness prediction errors
3. **Knowledge source ablation**: Run experiments disabling external document access while keeping the faithfulness→reasoning→correctness pipeline intact to determine whether the reasoning component provides value when external sources are unavailable or unreliable