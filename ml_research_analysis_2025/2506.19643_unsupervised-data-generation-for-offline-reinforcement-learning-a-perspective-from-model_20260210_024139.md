---
ver: rpa2
title: 'Unsupervised Data Generation for Offline Reinforcement Learning: A Perspective
  from Model'
arxiv_id: '2506.19643'
source_url: https://arxiv.org/abs/2506.19643
tags:
- offline
- data
- learning
- policy
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving offline reinforcement
  learning (RL) performance when batch data is collected without knowledge of the
  target task. The authors propose a framework called UDG (Unsupervised Data Generation)
  that generates diverse datasets using unsupervised RL, then selects the best dataset
  for a specific task at offline training time.
---

# Unsupervised Data Generation for Offline Reinforcement Learning: A Perspective from Model

## Quick Facts
- **arXiv ID**: 2506.19643
- **Source URL**: https://arxiv.org/abs/2506.19643
- **Reference count**: 15
- **Primary result**: Proposes UDG framework that generates diverse datasets using unsupervised RL and selects the best dataset for a specific task, outperforming conventional offline RL with random or supervised data on locomotive tasks

## Executive Summary
This paper addresses the challenge of improving offline reinforcement learning (RL) performance when batch data is collected without knowledge of the target task. The authors propose a framework called UDG (Unsupervised Data Generation) that generates diverse datasets using unsupervised RL, then selects the best dataset for a specific task at offline training time. The core method involves training multiple policies simultaneously with diversity rewards, generating experience buffers from each policy, and selecting the buffer with the highest return for the target task. Theoretically, the authors establish a connection between batch data distribution and offline RL performance, showing that data closer to the optimal policy distribution yields better results. Empirically, UDG outperforms conventional offline RL with random or supervised data on locomotive tasks like Ant-Angle and Cheetah-Jump.

## Method Summary
UDG works by first training K policies using an unsupervised diversity reward (e.g., from WURL) without any task-specific rewards. Each policy generates a distinct data buffer. When a task is specified at the offline stage, the rewards in all buffers are relabeled using the task's reward function. The buffer with the highest average return is selected for training the offline RL agent (e.g., using MOPO). This selection step explicitly minimizes the Wasserstein distance term in the theoretical bound for the given task, by choosing the πβ whose ρT^πβ is likely closest to the task's ρT^π*.

## Key Results
- UDG achieves higher returns across multiple tasks compared to supervised approaches, with Ant-Angle task showing returns of 1236-1437 across different directions
- The method successfully solves all 6 angle tasks in Ant-Angle environment while a single supervised policy fails on out-of-distribution angles
- UDG outperforms conventional offline RL with random or supervised data on locomotive tasks like Ant-Angle and Cheetah-Jump
- The "top 2 mixed" buffer strategy sometimes outperforms the "top 1" selection, suggesting a tradeoff between proximity to optimal distribution and overall data coverage

## Why This Works (Mechanism)

### Mechanism 1
The performance gap between a policy learned via model-based offline RL and the optimal policy is bounded by the Wasserstein distance between the batch data distribution and the optimal policy's state-action distribution. This derivation starts from MOPO's performance bound, which links the gap to model prediction error on the optimal policy's occupancy distribution. By assuming a non-parametric transition model (nearest-neighbor lookup), Lipschitz transitions, and Lipschitz value functions, the model prediction error is bounded by the 1-Wasserstein distance between the empirical batch data distribution and the optimal distribution.

### Mechanism 2
In task-agnostic settings, an unsupervised RL method that maximizes the pairwise diversity of a set of policies (specifically, the minimum Wasserstein distance between their state-action occupancies) is a practical surrogate objective that approximately minimizes the worst-case regret against any unknown optimal policy. The true objective is to minimize the worst-case regret, but this is computationally difficult. The paper proposes the surrogate objective of maximizing pairwise distances, which encourages policies to be "packed" as far apart as possible in the state-action distribution space.

### Mechanism 3
A framework that generates multiple data buffers from diverse unsupervised policies, and then selects the best buffer for a given downstream task, outperforms supervised or mixed data generation strategies in task-agnostic scenarios. UDG first trains K policies using an unsupervised diversity reward, generates experience buffers, relabels rewards with the task-specific function, and selects the buffer with the highest average return for training the offline RL agent.

## Foundational Learning

- **Concept: Wasserstein Distance (Earth Mover's Distance)**
  - **Why needed here:** It is the core theoretical metric used to quantify the "distance" between two probability distributions: the batch data distribution and the optimal policy's distribution.
  - **Quick check question:** Can you explain intuitively why the Wasserstein distance is a more suitable metric here than KL divergence for comparing state-action distributions?

- **Concept: Offline Reinforcement Learning & The Out-of-Distribution (OOD) Problem**
  - **Why needed here:** This is the problem setting. UDG is designed to solve the OOD problem by improving the batch data before offline training begins, rather than constraining the algorithm during training.
  - **Quick check question:** Why does standard online RL fail when applied directly to offline data, and how does the quality/distribution of that data exacerbate the problem?

- **Concept: Unsupervised Reinforcement Learning / Skill Discovery**
  - **Why needed here:** This is the data generation engine for UDG. The method relies on algorithms like WURL or DIAYN to train a set of diverse policies without any task-specific rewards.
  - **Quick check question:** How does an algorithm like DIAYN or WURL train a policy without an external reward from the environment? What is the pseudo-reward it optimizes?

## Architecture Onboarding

- **Component map:** Unsupervised Policy Trainer (WURL) -> Data Generation & Buffering -> Reward Relabeling Engine -> Buffer Selector -> Offline RL Trainer (MOPO)
- **Critical path:** The performance of the entire system hinges on the Unsupervised Policy Trainer. If the policies it generates do not cover the state-action space relevant to the downstream task, the Buffer Selector will have no good option to choose from, and the final offline policy will fail.
- **Design tradeoffs:**
  - Number of Policies (K): A larger K provides better coverage of the policy space, reducing worst-case regret, but increases the computational cost of unsupervised training linearly.
  - Diversity Objective: The paper uses WURL (Wasserstein distance). Other objectives (DIAYN, DADS) could be used but may not have the same theoretical link to the minimax regret.
  - Buffer Selection vs. Mixing: The paper shows that selecting the single best buffer often outperforms mixing all buffers. The "top 2 mixed" can be better, suggesting a tradeoff between proximity to the optimal distribution (single best) and overall data coverage for model learning (mixing).
- **Failure signatures:**
  - Mode Collapse in Unsupervised Training: If the K policies converge to only a few distinct behaviors, the generated data will lack diversity, and the system will fail on tasks that require different behaviors.
  - Poor Offline Learning from Good Data: Even with a relevant buffer, the offline RL algorithm (MOPO) may still struggle if the data is too sparse or if the model error penalty is not tuned correctly.
  - Misaligned Relabeled Return: The average return of the relabeled buffer might not always perfectly correlate with the Wasserstein distance to the optimal policy, leading to the selection of a suboptimal buffer.
- **First 3 experiments:**
  1. Reproduce Ant-Angle Baseline: Implement the UDG pipeline on the Ant-Angle environment. Train K=10 policies using WURL, generate buffers, relabel for the 6 angle tasks, and reproduce the result that UDG solves all 6 while a single supervised policy fails on out-of-distribution angles.
  2. Ablate Buffer Selection Strategy: Compare three strategies: a) UDG (top 1), b) UDG (all mixed), and c) Supervised. Confirm that top 1 outperforms supervised and that all mixed can underperform due to the increased distance term as predicted by the theory.
  3. Test Sensitivity to K: Run UDG with K=2, 5, 10, 20 on a challenging task (e.g., Cheetah-Jump). Plot the final performance against K to see if performance saturates and to understand the compute-performance tradeoff.

## Open Questions the Paper Calls Out

- Can the Unsupervised Data Generation (UDG) framework be effectively generalized to model-free offline reinforcement learning algorithms?
- Is UDG feasible in domains that violate Lipschitz continuity assumptions, such as pixel-based control tasks or environments with non-smooth reward functions?
- How does the performance gap change when using neural network transition models instead of the non-parametric models assumed in the theoretical analysis?
- Can UDG induce effective diversity without prior knowledge of the reward-relevant state subspace?

## Limitations
- The theoretical bounds rely on strong assumptions including Lipschitz continuity of the environment and a non-parametric transition model
- Empirical evaluation is conducted on a narrow set of locomotion tasks, raising questions about generalization to more complex or high-dimensional problems
- The diversity objective as a proxy for worst-case regret minimization is heuristic and not rigorously proven

## Confidence
- **High** confidence in the core mechanism for supervised task case where optimal policy is known
- **Medium** confidence in the unsupervised case due to heuristic nature of diversity objective
- **Medium-High** confidence in the generate-and-select framework based on empirical results

## Next Checks
1. **Theoretical Validation**: Rigorously test the Lipschitz assumption by applying the method to environments with known discontinuities or non-smooth dynamics to identify failure modes.
2. **Scalability Test**: Implement UDG on a more complex control task (e.g., humanoid locomotion) with higher-dimensional state spaces to evaluate whether the diversity objective and buffer selection scale effectively.
3. **Diversity Objective Comparison**: Systematically compare WURL's Wasserstein-based diversity against alternative skill discovery methods (DIAYN, DADS) on identical tasks to isolate the contribution of the diversity metric to performance.