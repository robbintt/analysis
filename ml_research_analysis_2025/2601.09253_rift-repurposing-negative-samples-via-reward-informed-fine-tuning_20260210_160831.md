---
ver: rpa2
title: 'RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning'
arxiv_id: '2601.09253'
source_url: https://arxiv.org/abs/2601.09253
tags:
- rift
- negative
- reward
- base
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes RIFT, a simple yet effective framework for fine-tuning
  large language models that repurposes all self-generated samples, including negative
  ones. Unlike standard methods that discard low-quality samples, RIFT reweights the
  loss with scalar rewards to learn from both positive and negative trajectories.
---

# RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning

## Quick Facts
- arXiv ID: 2601.09253
- Source URL: https://arxiv.org/abs/2601.09253
- Authors: Zehua Liu; Shuqi Liu; Tao Zhong; Mingxuan Yuan
- Reference count: 40
- The paper proposes RIFT, a simple yet effective framework for fine-tuning large language models that repurposes all self-generated samples, including negative ones. Unlike standard methods that discard low-quality samples, RIFT reweights the loss with scalar rewards to learn from both positive and negative trajectories. A key contribution is addressing the training collapse issue from naive reward integration by introducing a stabilized loss formulation using a linear probability approximation for negative samples. Extensive experiments on mathematical benchmarks across various base models demonstrate that RIFT consistently outperforms established baselines like SFT, DFT, RFT, and DPO in both in-distribution accuracy and out-of-distribution generalization, achieving up to 11.4% improvement over RFT. RIFT is also more computationally efficient, requiring nearly 50% less memory than DPO while maintaining higher accuracy.

## Executive Summary
RIFT addresses a key limitation in existing fine-tuning methods for language models: the inefficient use of self-generated samples by discarding negative ones. By repurposing all self-generated samples and reweighting the loss with scalar rewards, RIFT enables the model to learn from both successful and unsuccessful reasoning paths. The framework introduces a stabilized loss formulation using a linear probability approximation for negative samples, preventing training collapse while maintaining computational efficiency. Extensive experiments demonstrate that RIFT consistently outperforms established baselines like SFT, RFT, and DPO across mathematical benchmarks, achieving significant improvements in both in-distribution and out-of-distribution generalization.

## Method Summary
RIFT fine-tunes language models by repurposing all self-generated samples through reward-weighted loss computation. The model generates K responses per problem, each assigned a scalar reward based on correctness. The loss is computed using log-likelihood for positive samples (r > 0) and a linear probability approximation for negative samples (r < 0), preventing gradient explosion. This approach retains information from failure modes while maintaining numerical stability. The framework uses AdamW optimizer with cosine learning rate scheduling and requires no reference model, making it more memory-efficient than alternatives like DPO.

## Key Results
- RIFT consistently outperforms SFT, RFT, and DPO baselines on mathematical benchmarks, achieving up to 11.4% improvement over RFT
- The framework demonstrates strong out-of-distribution generalization, particularly on College Math benchmark where it improves accuracy by 7.7%
- RIFT achieves nearly 50% memory reduction compared to DPO while maintaining higher accuracy
- Performance gains are most pronounced when the mixed-reward rate (ratio of positive to negative samples) is high
- RIFT shows robustness across different base model sizes, from 1.5B to 7B parameters

## Why This Works (Mechanism)

### Mechanism 1: Reward-Weighted Retention of All Samples
RIFT improves data efficiency by using all self-generated samples, assigning scalar rewards to both correct and incorrect trajectories rather than discarding negatives via hard thresholding. Each sample contributes to the loss proportionally to its reward—positive rewards reinforce correct behavior, negative rewards suppress incorrect outputs—enabling the model to learn distinctions between success and failure modes from the same dataset. The core assumption is that negative samples carry information about failure modes that complements positive reinforcement; the reward signal is reliable enough to guide weighting.

### Mechanism 2: Linear Probability Approximation for Negative Samples
Replacing the log-likelihood term for negative samples with a linear surrogate prevents gradient explosion and unbounded loss while still suppressing undesirable outputs. For negative samples (r < 0), RIFT uses −r·πθ(y|x) instead of −r·logπθ(y|x). The linear term has constant gradient (avoiding the 1/π singularity), keeping loss bounded in [r, 0] and maintaining numerical stability as πθ → 0. The core assumption is that the linear approximation, derived from first-order Taylor expansion of log(u) ≈ u−1, provides sufficient suppression signal for negative samples despite reduced accuracy near π = 0.

### Mechanism 3: Self-Generated Mixed-Quality Data Alignment
Training on self-generated mixed-quality samples better aligns the policy with its own output distribution and improves out-of-distribution generalization compared to expert-only SFT or filtered RFT. Self-generation inherently matches the training distribution to the model's sampling process. Mixed-reward problems (containing both correct and incorrect rollouts) provide contrastive signal, helping the model internalize boundaries between valid and flawed reasoning paths. The core assumption is that the base model has sufficient capability to generate at least some correct responses per problem; reward verification is deterministic and accurate.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) and its Limitations**
  - Why needed here: RIFT is framed as a modification of SFT that retains its simplicity while addressing data inefficiency and distributional mismatch.
  - Quick check question: Can you explain why SFT on non-reflective expert data can degrade performance of a reasoner model (as observed with DeepSeek-R1-Distill-Qwen-1.5B)?

- **Rejection Sampling Fine-Tuning (RFT) and Data Filtering**
  - Why needed here: RIFT is explicitly designed to overcome RFT's discard policy; understanding RFT clarifies what RIFT repurposes.
  - Quick check question: What information is lost when RFT discards sub-threshold samples, and why might this limit model refinement?

- **Gradient Dynamics of Log-Likelihood Near Zero**
  - Why needed here: RIFT's core innovation is addressing gradient explosion from log(π) for negative samples; understanding this explains why the linear surrogate is necessary.
  - Quick check question: What happens to ∂L/∂π when π → 0 for a log-likelihood term weighted by negative reward, and why does RIFT's linear approximation avoid this?

## Architecture Onboarding

- **Component map**:
  Base Model -> Reward Function -> Data Partitioner -> Loss Module -> Optimizer
  (Generates responses) -> (Assigns rewards) -> (Splits by reward sign) -> (Computes RIFT loss) -> (Updates parameters)

- **Critical path**:
  1. Generate K responses per problem from base model
  2. Verify correctness and assign rewards
  3. Partition into positive/negative sets
  4. Compute RIFT loss with log term for positives, linear term for negatives
  5. Backpropagate and update; no reference model required

- **Design tradeoffs**:
  - RIFT vs RFT: RIFT uses ~2× more data (retains all samples) but avoids generation waste; gains are largest when mixed-reward rate is high. RFT is simpler but discards information.
  - RIFT vs DPO: RIFT uses ~50% less memory (no reference model) and achieves higher accuracy in reported experiments; DPO requires explicit preference pairs, RIFT only needs scalar rewards.
  - Reward magnitude: Paper finds constant negative reward (−0.2 to −0.8) outperforms group normalization; sensitivity is low within this range.
  - Sample count K: K=8 is optimal; K=16 adds noise without improving Pass@3.

- **Failure signatures**:
  - Training collapse with loss diverging to −∞: Indicates log term incorrectly applied to negative samples or linear surrogate not implemented.
  - No improvement over base model: May indicate insufficient mixed-reward problems (model too strong or too weak), or reward verification errors.
  - Memory usage spikes to DPO levels: Reference model incorrectly loaded; RIFT should only track policy parameters.
  - Pass@3 drops while Mean@3 improves: Over-suppression of negative samples may reduce diversity; check negative reward magnitude.

- **First 3 experiments**:
  1. **Reproduction on single benchmark**: Train Qwen2.5-Math-1.5B on MATH subset (3K problems, K=8) with rewards +1.0/−0.2; verify Mean@8 improvement over RFT baseline matches reported +11.4% or is within ±2% tolerance.
  2. **Ablation on negative reward magnitude**: Sweep r_neg ∈ {−0.2, −0.5, −0.8} on same setup; confirm stability and comparable performance as reported in Table 6.
  3. **Memory profile comparison**: Measure peak VRAM for RIFT vs DPO on same hardware and batch size; verify RIFT uses approximately half the memory by confirming reference model is absent from RIFT's computation graph.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RIFT be effectively adapted for subjective or open-ended generative domains where correctness is ambiguous and rewards are difficult to quantify?
- Basis in paper: [explicit] The authors state in the Limitations that extending the framework to subjective domains remains an open challenge because correctness is harder to define and rewards are more difficult to measure than in verifiable reasoning tasks.
- Why unresolved: The current evaluation relies exclusively on mathematical benchmarks with objective success criteria (binary correctness), leaving the framework's utility for tasks like creative writing or dialogue unproven.
- What evidence would resolve it: Successful application of RIFT to subjective benchmarks (e.g., creative writing) using defined reward models, showing improvements over standard SFT or RFT baselines.

### Open Question 2
- Question: Does incorporating step-by-step (process) rewards into the RIFT framework yield performance improvements over the current outcome-based reward system?
- Basis in paper: [explicit] The authors note that RIFT currently treats samples as single units and suggest future versions could use step-by-step rewards to learn from partial successes in complex, multi-step problems.
- Why unresolved: A model may currently be penalized for a "negative" trajectory that contains a high-quality reasoning chain with only a minor final error, potentially wasting useful training signal.
- What evidence would resolve it: A comparative study where RIFT is trained using dense, process-based rewards versus sparse outcome rewards on long-horizon reasoning tasks.

### Open Question 3
- Question: Can uncertainty-aware reward weighting be integrated into RIFT to mitigate noise and improve robustness when using imperfect reward models?
- Basis in paper: [explicit] The authors identify exploring uncertainty-aware reward weighting as a compelling direction to further mitigate potential feedback noise and address limitations in the reward source's discriminative power.
- Why unresolved: The current framework relies on scalar rewards, and the authors acknowledge that the performance upper-bound is naturally influenced by the quality of the reward source.
- What evidence would resolve it: Experiments demonstrating that weighting the RIFT loss by reward uncertainty (e.g., variance of the reward model) leads to more stable convergence or higher accuracy in noisy reward environments.

## Limitations
- The framework assumes deterministic reward assignment but doesn't address potential noise in this process, which could impact performance
- Performance gains are benchmark-dependent and rely on the quality of self-generated data and reward function
- The analysis focuses on mathematical reasoning benchmarks where correctness is relatively binary, limiting generalization to domains with more nuanced evaluation criteria
- The paper doesn't address how to handle cases where the base model is too weak to generate any correct responses for a problem subset

## Confidence

**High confidence**: The stabilized loss formulation with linear probability approximation for negative samples - this is mathematically rigorous with proven bounds and directly addresses a well-defined technical problem

**Medium confidence**: The overall performance improvements across benchmarks - while statistically significant, the gains are benchmark-dependent and rely on the quality of the self-generated data and reward function

**Medium confidence**: The claim that RIFT consistently outperforms RFT and DPO - the comparative analysis is thorough, but results may vary with different base models, reward scales, and problem distributions

## Next Checks

1. **Reward verification robustness**: Test RIFT performance when 5-15% of reward assignments are randomly corrupted to simulate verification noise; measure degradation compared to RFT baseline

2. **Cross-domain generalization**: Apply RIFT to a non-mathematical reasoning task (e.g., code generation or commonsense reasoning) with binary rewards to assess broader applicability beyond math benchmarks

3. **Base model capability threshold**: Systematically vary base model capability (using models of different sizes/skills) and measure the minimum competence required to generate sufficient positive samples for RIFT to outperform SFT-trained expert-only data