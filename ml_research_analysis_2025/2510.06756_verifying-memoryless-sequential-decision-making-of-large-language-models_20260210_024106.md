---
ver: rpa2
title: Verifying Memoryless Sequential Decision-making of Large Language Models
arxiv_id: '2510.06756'
source_url: https://arxiv.org/abs/2510.06756
tags:
- llms
- sequential
- policy
- state
- decision-making
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for formally verifying memoryless
  sequential decision-making policies based on large language models (LLMs) against
  safety properties expressed in probabilistic computation tree logic (PCTL). The
  core approach incrementally constructs the reachable portion of the Markov decision
  process (MDP) under the LLM policy, encoding states as natural language prompts,
  parsing the LLM's output into actions, and expanding only the resulting successor
  states.
---

# Verifying Memoryless Sequential Decision-making of Large Language Models

## Quick Facts
- arXiv ID: 2510.06756
- Source URL: https://arxiv.org/abs/2510.06756
- Authors: Dennis Gross; Helge Spieker; Arnaud Gotlieb
- Reference count: 40
- Primary result: Formal verification of memoryless LLM policies against PCTL safety properties via incremental DTMC construction

## Executive Summary
This paper presents a method for formally verifying memoryless sequential decision-making policies based on large language models (LLMs) against safety properties expressed in probabilistic computation tree logic (PCTL). The core approach incrementally constructs the reachable portion of the Markov decision process (MDP) under the LLM policy, encoding states as natural language prompts, parsing the LLM's output into actions, and expanding only the resulting successor states. The resulting model is then checked with the Storm model checker to determine whether the policy satisfies the specified safety property. Experiments on standard grid-world benchmarks using open-source LLMs via Ollama demonstrate that deterministic LLM policies can be verified, though they generally underperform deep reinforcement learning baselines.

## Method Summary
The method incrementally constructs an induced Discrete-Time Markov Chain (DTMC) by querying an LLM with encoded state prompts, parsing the output into actions, and expanding only reachable successor states. The LLM serves as a deterministic policy function after fixing the random seed, collapsing the MDP into a DTMC that can be formally verified against PCTL properties using the Storm model checker. The approach uses natural language as an intermediate representation, with an Input Encoder serializing state variables into prompts and an Action Parser extracting actions from LLM responses.

## Key Results
- The tool successfully verifies deterministic LLM policies on standard grid-world benchmarks (Frozen Lake, Taxi, Stock Market)
- LLM policies generally underperform deep reinforcement learning baselines in terms of safety probabilities
- State space explosion remains a constraint, with some models (e.g., deepseek-r1:1.5b) timing out on larger environments
- Parser errors and faulty actions occur but are handled via fallback mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Incremental Induced DTMC Construction
By expanding only the states reachable by a specific policy, the system avoids state-space explosion typical of model checking. The tool queries the LLM for the current state, parses the action, and uses the environment MDP transition function to discover successor states, repeating until no new states are found.

### Mechanism 2: Semantic-to-Formal Translation Layer
Mapping numerical environment states to natural language prompts and parsing text back to actions allows the verification engine to treat the LLM as a deterministic state-transition function. This creates a formal policy abstraction that bridges natural language processing with formal verification.

### Mechanism 3: Determinism via Seeding
Fixing the random seed forces the LLM to produce identical outputs for identical prompts, turning the stochastic policy into a deterministic one suitable for standard DTMC model checking. This is necessary because autoregressive generation normally samples tokens stochastically.

## Foundational Learning

**Concept: Markov Decision Process (MDP) vs. DTMC**
- Why needed here: The paper relies on the distinction that an MDP represents choices while a DTMC represents probabilities. The LLM resolves the non-determinism, collapsing the MDP into a DTMC.
- Quick check question: In this framework, does the LLM define the transition probabilities (e.g., slipping on ice) or the action choice (e.g., moving Left)?

**Concept: PCTL (Probabilistic Computation Tree Logic)**
- Why needed here: This is the language used to define "safety" (e.g., "Probability of empty fuel is 0"). Understanding the syntax $P(F \text{ empty})$ is required to interpret results.
- Quick check question: Does the property $P(F \text{ water}) = 0.98$ mean the agent successfully avoids water 98% of the time, or falls in 98% of the time?

**Concept: Model Checking**
- Why needed here: Unlike simulation, model checking provides exhaustive guarantees. The paper claims to provide "formal" verification, which implies mathematical proof rather than statistical estimation.
- Quick check question: Why is exhaustive state exploration necessary for "safety-critical" tasks compared to just running the agent 1,000 times?

## Architecture Onboarding

**Component map:** PRISM Model -> Ollama Instance -> COOL-MC Core -> Storm Model Checker
- PRISM Model defines environment physics (MDP)
- Ollama Instance hosts the LLM
- COOL-MC Core orchestrates the loop: state -> Encoder -> Ollama -> Parser -> Action -> Transition recording
- Storm Model Checker consumes the constructed transition system to verify PCTL properties

**Critical path:** The latency bottleneck is the LLM Inference Step. Every state transition requires an HTTP request to Ollama and autoregressive generation.

**Design tradeoffs:**
- Generality vs. Performance: The framework works with any LLM on Ollama but suffers from high latency and "faulty actions" due to the natural language interface
- Exactness vs. Scalability: The method provides exact probabilities for reachable states but will likely timeout if the state space is large

**Failure signatures:**
- Timeouts: Construction takes >5 hours
- Faulty Actions: Parser logs show fallback actions triggered
- Property Violation: $P(F \text{ empty}) = 1$ (Taxi runs out of fuel certainly)

**First 3 experiments:**
1. Hello World (Frozen Lake): Run with small model on 4x4 grid and verify $P(F \text{ water})$ matches paper's range (~0.96)
2. Parser Stress Test: Modify prompt to be vague and observe increase in "Faulty Actions" and degradation of safety probability
3. Determinism Check: Run verification twice with same seed and confirm identical "States" and "Transitions" counts

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability limits: Incremental construction effective for small MDPs but likely fails on larger environments
- Parser robustness: Critical single point of failure with uncertain error rates and fallback behavior
- Determinism guarantees: Verification only applies to specific deterministic path chosen by seed, not stochastic policy

## Confidence
**High Confidence:** Incremental DTMC construction mechanism is well-specified and theoretically sound, supported by experimental results
**Medium Confidence:** Semantic-to-formal translation layer works as described, but exact parser error rate and fallback behavior remain uncertain
**Low Confidence:** Determinism via seeding assumes strict reproducibility across runs, but hardware non-determinism could break this assumption

## Next Checks
1. Parser Error Rate Analysis: Run with intentionally malformed prompts and measure increase in faulty actions and resulting property violations
2. Determinism Reproducibility Test: Execute verification process twice with identical seeds and configurations, then compare exact number of states, transitions, and final property values
3. State Space Growth Experiment: Instrument tool to log states and transitions discovered per iteration, then plot growth curve to empirically validate tractability for reported environments