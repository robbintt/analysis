---
ver: rpa2
title: SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling
arxiv_id: '2510.15851'
source_url: https://arxiv.org/abs/2510.15851
tags:
- training
- speech
- performance
- slot
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling
  This paper addresses slot filling in spoken language understanding by training speechLLMs
  that jointly process audio and text. It introduces a large-scale, annotated dataset
  and explores architectural choices (modality adapters), training strategies (multi-stage
  fine-tuning, multitask learning), and model variants (composite vs.
---

# SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling

## Quick Facts
- arXiv ID: 2510.15851
- Source URL: https://arxiv.org/abs/2510.15851
- Authors: Kadri Hacioglu; Manjunath K E; Andreas Stolcke
- Reference count: 7
- SpeechLLMs outperform cascaded systems on in-domain slot filling with F1=0.79 using multi-stage training and larger modality adapters.

## Executive Summary
This paper addresses slot filling in spoken language understanding by training speechLLMs that jointly process audio and text. It introduces a large-scale, annotated dataset and explores architectural choices (modality adapters), training strategies (multi-stage fine-tuning, multitask learning), and model variants (composite vs. foundational). Results show that larger, more expressive adapters and multi-stage training improve performance, and multitask datasets enhance generalization. A foundational speechLLM (Qwen2-Audio) outperforms a composite fine-tuned model, highlighting the benefits of scale and pre-training. While strong in-domain performance is achieved, challenges remain in out-of-domain generalization and zero-shot capabilities.

## Method Summary
The paper develops SpeechLLMs for contextualized zero-shot slot filling by integrating audio and text processing. It uses a Whisper-base encoder (frozen) with modality adapters to transform speech features for a text-trained LLM backbone (Llama-3.2-1B or Qwen2-Audio). Training involves multi-stage fine-tuning: first aligning modalities on an AST task, then jointly fine-tuning adapter and LLM (via LoRA) on slot filling. The best configuration uses an MLP adapter with multi-stage training and multitask data (AST, SIT, SQIT), achieving F1=0.79 on in-domain evaluation.

## Key Results
- Larger, more expressive modality adapters (e.g., transformer) improve slot-filling performance, with MLP adapter (20.98M params) achieving F1=0.7420 on CallCenter-A.
- Fine-tuning the LLM (not just adapter) is necessary for task-specific output generation, with LoRA-enabled models improving F1 by 7-26 percentage points.
- Multi-stage training (AST pretraining → slot fine-tuning) accelerates convergence and boosts performance to F1=0.7916, outperforming single-stage approaches.
- Foundational speechLLMs (Qwen2-Audio) outperform composite models in-distribution, but out-of-domain generalization remains challenging (F1 drops from 0.79→0.34).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Larger, more expressive modality adapters improve slot-filling performance in SpeechLLMs.
- **Mechanism:** The adapter transforms speech encoder outputs into the LLM's linguistic embedding space. More complex adapters (e.g., transformer with 67M parameters vs. CNN with 3.4M) can model richer temporal and cross-modal dependencies, enabling better alignment between speech features and textual representations required for structured slot extraction.
- **Core assumption:** The speech encoder captures sufficient acoustic-phonetic information, and the bottleneck lies primarily in cross-modal transformation quality rather than speech representation quality.
- **Evidence anchors:**
  - [section 3.3, Table 3]: Transformer adapter (67.16M params) achieved F1=0.7420 compared to CNN baseline (3.41M params) at F1=0.4017 on CallCenter-A—a 85% relative improvement.
  - [section 3.3]: "CNN-XL exhibited relatively inferior performance, underscoring the importance of not only the model size, but also its design and modeling capability."
  - [corpus]: Related work (arXiv:2508.17863) confirms the broader question of discrete tokens vs. continuous features remains open, suggesting adapter design choices are not yet standardized.
- **Break condition:** If adapter capacity exceeds training data diversity, overfitting may occur—though not directly observed here, the transformer adapter showed training instability requiring learning rate adjustments (Appendix D).

### Mechanism 2
- **Claim:** Fine-tuning the LLM (not just the adapter) is necessary for aligning input modalities with task-specific output generation.
- **Mechanism:** Freezing the LLM preserves pre-trained capabilities but limits the model's ability to map cross-modal representations to task-specific structured outputs. LoRA adaptation allows the LLM to learn how to generate slot-filling JSON responses conditioned on speech-derived embeddings, not just text-conditioned responses.
- **Core assumption:** The pre-trained LLM has sufficient reasoning and instruction-following capabilities, but lacks exposure to speech-conditioned generation patterns.
- **Evidence anchors:**
  - [section 3.4, Table 4]: MLP adapter with LoRA enabled improved from F1=0.6970 to F1=0.7677 (+7 percentage points); CNN adapter improved from F1=0.4017 to F1=0.6649 (+26 percentage points).
  - [section 3.4]: "Freezing the language model to preserve its original skills is not an effective strategy for tasks where the model lacks strong capabilities."
  - [corpus]: arXiv:2512.16378 (Hearing to Translate) similarly finds direct speech integration into LLMs requires adaptation beyond frozen text models for translation tasks.
- **Break condition:** Joint training of large adapter + LoRA can cause gradient instability (transformer adapter showed divergent behavior, Appendix D, Figure 2).

### Mechanism 3
- **Claim:** Multi-stage training accelerates convergence and improves final slot-filling performance compared to single-stage joint training.
- **Mechanism:** Multi-stage approaches (particularly Multistage-C: AST pretraining → task fine-tuning) first establish robust audio-to-linguistic alignment via a simpler transcription task, then adapt this aligned representation for the more complex slot-filling task. This curriculum-like approach prevents the optimization difficulty of jointly learning alignment and task-specific generation.
- **Core assumption:** AST provides a useful intermediate representation that transfers to slot filling, and the model has sufficient capacity to retain AST capabilities while learning slot extraction.
- **Evidence anchors:**
  - [section 3.5, Table 5]: Multistage-C achieved F1=0.7916 vs. single-stage F1=0.7677; Appendix E shows faster convergence.
  - [section 3.5]: "Multistage training is necessary for faster and better modal alignment."
  - [corpus]: arXiv:2510.19326 (Slot Filling as Reasoning Task) proposes chain-of-thought decomposition as an alternative multi-step approach, suggesting task decomposition is broadly beneficial.
- **Break condition:** If auxiliary tasks are too dissimilar from target task, negative transfer could occur—though not observed here, this risk increases with domain mismatch.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - **Why needed here:** The paper uses LoRA (rank 32, α=128, dropout 0.05) to fine-tune large models on limited GPU memory (4× A10G, 24GB each). Without PEFT, training billion-parameter speechLLMs would be infeasible in their compute budget.
  - **Quick check question:** Can you explain why LoRA allows fine-tuning with ~1% of parameters while maintaining most performance gains?

- **Concept: Modality Alignment in Multimodal Models**
  - **Why needed here:** The core challenge is mapping continuous speech encoder outputs to discrete LLM embedding space. The adapter's job is not just downsampling (8× in this paper) but making speech "readable" by a text-trained model.
  - **Quick check question:** Why might a simple linear projection fail to capture the temporal dynamics needed for slot boundary detection in speech?

- **Concept: Zero-Shot Generalization vs. In-Domain Performance Trade-offs**
  - **Why needed here:** The paper explicitly distinguishes in-domain (CallCenter-A, CallCenter-B ID) from out-of-domain (CallCenter-B OOD) evaluation. Foundational models (Qwen2-Audio) outperform composite models in-distribution, but OOD generalization remains challenging for all configurations (F1 drops from 0.79→0.34 for best model).
  - **Quick check question:** Why does multitask training (adding AST, SIT, SQIT) improve OOD generalization but may not fully close the domain gap?

## Architecture Onboarding

- **Component map:** Speech Encoder (Whisper-base, frozen) → Modality Adapter (trainable: CNN/Linear/MLP/Transformer, 3-67M params) → LLM Backbone (Llama-3.2-1B with LoRA, or Qwen2-Audio-8.5B) → Generation (JSON slot output)

- **Critical path:**
  1. **Data preparation:** Convert call-center audio to instruction triplets (audio, instruction, response) with randomized context (0-3 turns) and distractor slots (1-5).
  2. **Adapter selection:** Start with MLP adapter (20.98M params)—best balance of performance and training stability.
  3. **Training strategy:** Use Multistage-C (AST pretraining → slot fine-tuning) with LoRA on all linear layers.
  4. **Evaluation:** Report partial-match F1 on held-out set; separate ID/OOD metrics for generalization assessment.

- **Design tradeoffs:**
  - **Adapter complexity vs. training stability:** Transformer adapter (67M params) showed gradient instability requiring learning rate reduction or extended warmup (Appendix D, Figure 2).
  - **Composite vs. foundational models:** Foundational speechLLMs (Qwen2-Audio, 8.5B params, extensively pretrained) outperform composite models (Llama-3.2-1B + Whisper) but offer less flexibility in component selection.
  - **Multitask expansion vs. data curation cost:** Adding AST, SIT, SQIT (Table 1) improves OOD F1 by 3-4 percentage points but requires additional data pipelines.

- **Failure signatures:**
  - **Vanilla LLM on structured output:** Precision=0.04, Recall=0.73, F1=0.08 (Table 2)—LLMs cannot zero-shot slot-fill without task-specific fine-tuning.
  - **Frozen LLM + adapter-only training:** F1 capped at ~0.74 (Table 3)—ceiling effect without LLM adaptation.
  - **Transformer adapter divergence:** Learning curve shows loss spikes (Figure 2), requiring learning rate reduction (2e-4→~1e-4) or warmup extension (20%→40%).
  - **OOD generalization collapse:** All models show 40-60% F1 degradation on unseen slot types (CallCenter-B OOD, Table 6).

- **First 3 experiments:**
  1. **Baseline replication:** Train speechLLM with CNN adapter + frozen LLM on slot-filling data only. Target: F1~0.40. Purpose: Establish lower bound and validate data pipeline.
  2. **LoRA enablement:** Add LoRA (rank=32) to LLM while keeping adapter fixed. Target: F1~0.66-0.77 (Table 4 range). Purpose: Isolate contribution of LLM adaptation.
  3. **Multitask + Multistage:** Implement Multistage-C (AST pretraining) + multitask data (Table 1) with MLP adapter. Target: F1~0.78-0.79 (Table 5-7). Purpose: Verify best-practice configuration before architectural exploration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can composite speechLLMs surpass cascaded systems in out-of-domain generalization without increasing model scale?
- Basis in paper: [Explicit] Section 3.6 states that the cascaded system "remains more robust, generalizes better, and exhibits stronger zero-shot capabilities" than the bimodal speechLLM, suggesting that current joint integration is insufficient for OOD tasks.
- Why unresolved: The paper concludes that modality alignment and fine-tuning may be inadequate for joint models to outperform sequential unimodal models unless training is conducted at a much larger scale.
- What evidence would resolve it: Experiments demonstrating that specific training strategies or architectural improvements (without parameter scaling) allow joint models to overtake cascaded baselines on unseen slot labels.

### Open Question 2
- Question: How can training instabilities in high-capacity modality adapters be mitigated to ensure reliable convergence?
- Basis in paper: [Explicit] Section 3.4 and Appendix D note that the Transformer adapter exhibited "divergent behavior" and "training instabilities" (loss spikes) that standard warm-up and gradient clipping failed to resolve.
- Why unresolved: While increasing warm-up duration helped, the authors highlight that training difficulty scales with adapter complexity, making robust training a non-trivial challenge for expressive adapters.
- What evidence would resolve it: A training framework or initialization scheme that allows Transformer-based adapters to converge as smoothly and effectively as simpler MLP or Linear adapters without extensive hyperparameter tuning.

### Open Question 3
- Question: What is the relative contribution of model scale versus pre-training data diversity in closing the performance gap between composite and foundational speechLLMs?
- Basis in paper: [Explicit] Section 3.8 compares a composite model (Llama 3.2 1B) with a foundational model (Qwen2-Audio) and attributes performance differences to both parameter size and "the scale of data," but does not disentangle these factors.
- Why unresolved: The paper observes that foundational models have a significant advantage but leaves unclear whether this is due to the model's capacity or the sheer volume/variety of its pre-training data.
- What evidence would resolve it: Ablation studies controlling for parameter count while varying pre-training dataset sizes to isolate the driver of the foundational model's superior slot-filling performance.

## Limitations
- Dataset dependency: Primary evaluation relies on proprietary CallCenter datasets, limiting direct comparison with public benchmarks.
- Out-of-domain generalization gap: Best model achieves F1=0.34 on OOD slots, representing a 57% relative performance drop from in-domain performance.
- Adapter instability: Transformer adapter showed training divergence requiring learning rate reduction and warmup extension.

## Confidence
- **High confidence:** In-domain performance results and the core finding that larger adapters and multi-stage training improve performance within the CallCenter domain.
- **Medium confidence:** Claims about zero-shot generalization improvements, as these rely on OOD evaluation on a single dataset split.
- **Low confidence:** Claims about the superiority of foundational vs. composite SpeechLLMs for practical deployment, as this comparison is based on a single architecture.

## Next Checks
1. **Cross-dataset generalization test:** Evaluate the best SpeechLLM configuration on public SLU benchmarks (SLURP, Fluent Speech Commands) to validate OOD generalization claims beyond the proprietary CallCenter-B dataset.
2. **Adapter scalability study:** Systematically test adapter scaling from 1M to 100M parameters across multiple architectures (CNN, MLP, Transformer) to establish optimal parameter-count-to-performance curves and identify overfitting thresholds.
3. **Component ablation with different backbones:** Repeat the foundational vs. composite comparison using multiple LLM backbones (e.g., Qwen2.5, Gemma, Phi) to determine if the observed performance gap generalizes across speech encoder + LLM combinations.