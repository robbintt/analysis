---
ver: rpa2
title: General Agentic Memory Via Deep Research
arxiv_id: '2511.18423'
source_url: https://arxiv.org/abs/2511.18423
tags:
- memory
- information
- context
- result
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces General Agentic Memory (GAM), a novel framework
  that addresses the limitations of static memory systems by adopting a just-in-time
  compilation approach. GAM employs a dual-architecture with a Memorizer that compresses
  historical information into lightweight memory and maintains complete information
  in a page-store, and a Researcher that performs deep research to retrieve and integrate
  relevant information for client requests.
---

# General Agentic Memory Via Deep Research

## Quick Facts
- arXiv ID: 2511.18423
- Source URL: https://arxiv.org/abs/2511.18423
- Reference count: 40
- Outperforms existing memory systems on HotpotQA (64.07 F1) and NarrativeQA (34.77 F1)

## Executive Summary
This paper introduces General Agentic Memory (GAM), a framework that addresses static memory limitations through just-in-time compilation. GAM employs a dual-architecture with Memorizer (compresses history into lightweight memos while preserving complete information in page-store) and Researcher (performs deep research with iterative reflection). The framework demonstrates superior performance on multi-hop reasoning tasks while maintaining competitive efficiency with 69.32 seconds processing time for 56k contexts.

## Method Summary
GAM implements a just-in-time compilation approach where historical information is preserved in complete form in a page-store, while only lightweight memos are maintained offline. The Researcher performs Planning → Searching → Reflection cycles, using three retrieval tools (embedding, BM25, page-index) to gather task-specific context at runtime. The system enables end-to-end optimization through reinforcement learning, though experiments used off-the-shelf models. Key parameters include 2048-token pages, max 5 retrieved pages, and 3 reflection iterations.

## Key Results
- Achieves 64.07 F1 on HotpotQA (56K tokens), outperforming memory-only approach (42.67 F1)
- Scores 34.77 F1 on NarrativeQA with 87K average context length
- Demonstrates 39.59→64.07 F1 improvement using combined vs single retrieval tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JIT memory compilation reduces information loss compared to AOT memory compression
- Mechanism: Preserves complete raw history in page-store while maintaining only lightweight abstract memos offline. Researcher performs targeted retrieval to reconstruct task-specific context on demand
- Core assumption: Relevant information can be located via guided search when needed; runtime retrieval overhead is acceptable
- Evidence anchors: [abstract] JIT principle; [section 3.2] memory-only performs worse than research-only

### Mechanism 2
- Claim: Deep research iteration with reflection improves multi-hop reasoning over single-pass retrieval
- Mechanism: Planning → Searching → Reflection cycles allow progressive refinement where initial retrieval informs subsequent targeted searches
- Core assumption: LLM can accurately judge information completeness and generate productive follow-up queries
- Evidence anchors: [section 3.4] varying reflection depth yields consistent performance improvements

### Mechanism 3
- Claim: Multi-tool retrieval provides complementary coverage for diverse query types
- Mechanism: Embedding captures semantic similarity; BM25 handles exact entity/keyword matches; page-index enables direct access when memory indicates specific locations
- Core assumption: Planning agent correctly matches query types to appropriate tools
- Evidence anchors: [section 3.5] combining any two tools yields better performance than single tools

## Foundational Learning

- Concept: Just-in-Time vs Ahead-of-Time Compilation
  - Why needed here: Core paradigm shift from pre-computed static memory to runtime-optimized context generation
  - Quick check question: Can you explain why AOT memory systems suffer information loss that JIT avoids?

- Concept: Multi-stage Retrieval (Planning → Execution → Reflection)
  - Why needed here: Researcher module requires understanding iterative search refinement loops
  - Quick check question: How does reflection differ from simple retry in retrieval systems?

- Concept: Dense vs Sparse Retrieval Tradeoffs
  - Why needed here: GAM combines embedding (semantic) and BM25 (keyword) retrieval; understanding when each excels is critical
  - Quick check question: For a query asking "What did John configure on March 15th?", which retrieval method would you prioritize?

## Architecture Onboarding

- Component map: Memorizer -> Memory update + Page-store append; Researcher -> Plan -> Search (3 tools) -> Integrate -> Reflect
- Critical path: Offline: Session arrives → Memorizer.memorize() → memo → memory update → Memorizer.page() → page-store append; Online: Request → Researcher.plan() with memory → parallel tool execution → Researcher.integrate() → Researcher.reflect() → loop or return
- Design tradeoffs: Reflection depth vs latency (max=3); Retrieved pages vs context length (max=5); Memorizer model size vs Researcher sensitivity
- Failure signatures: Researcher <7B model → performance collapse; Memory-only mode → severe degradation; Single-tool retrieval → substantial drop
- First 3 experiments: 1) Ablate reflection depth on HotpotQA 56K; 2) Swap Memorizer to Qwen-2.5-0.5B; 3) Test retrieval tool combinations

## Open Questions the Paper Calls Out

1. Does RL optimization framework yield significant gains over zero-shot prompting approach?
2. How can deep research iterative reflection loop be optimized for real-time interactive agents?
3. To what extent does lightweight memory act as bottleneck for stronger Researcher model?

## Limitations
- Asymmetric model requirements create deployment constraints (14B+ Researcher vs 0.5B Memorizer)
- Page-store mechanism scalability for longer or more diverse session histories unproven
- Reflection mechanism reliability depends heavily on LLM's ability to judge information completeness

## Confidence

**High Confidence**: JIT compilation superiority (42.67 vs 64.07 F1 memory-only vs full GAM); Multi-tool retrieval advantage (39.59 vs 64.07 F1)

**Medium Confidence**: Deep research iteration improves multi-hop reasoning but shows diminishing returns at higher depths

**Low Confidence**: Scalability claims for handling millions of tokens based on single experiments; 69.32-second processing time doesn't establish clear scaling laws

## Next Checks

1. Evaluate GAM's performance and latency on contexts exceeding 100K tokens to validate efficiency scaling
2. Test GAM on domains outside training benchmarks (medical records, legal documents) to assess generalization
3. Design adversarial queries to test whether reflection reliably detects incompleteness in non-obvious multi-hop reasoning