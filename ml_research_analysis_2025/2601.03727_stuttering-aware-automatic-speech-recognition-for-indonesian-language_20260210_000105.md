---
ver: rpa2
title: Stuttering-Aware Automatic Speech Recognition for Indonesian Language
arxiv_id: '2601.03727'
source_url: https://arxiv.org/abs/2601.03727
tags:
- speech
- stuttered
- indonesian
- stuttering
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic speech recognition
  (ASR) for stuttered speech in Indonesian, a low-resource language lacking stuttering-specific
  datasets. To overcome this, the authors propose a data augmentation framework that
  generates synthetic stuttered audio by injecting repetitions, prolongations, and
  interjections into fluent text using rule-based transformations and a large language
  model, followed by text-to-speech synthesis.
---

# Stuttering-Aware Automatic Speech Recognition for Indonesian Language

## Quick Facts
- arXiv ID: 2601.03727
- Source URL: https://arxiv.org/abs/2601.03727
- Reference count: 0
- Primary result: Synthetic data augmentation + stutter-only fine-tuning improves Indonesian ASR for stuttered speech

## Executive Summary
This paper addresses automatic speech recognition for stuttered Indonesian speech, a low-resource domain lacking specialized datasets. The authors propose a synthetic data generation framework that injects stuttering patterns (repetitions, prolongations, interjections) into fluent text using rule-based transformations and LLM rewriting, followed by TTS synthesis. They fine-tune a pre-trained Indonesian Whisper model using transfer learning on this synthetic stuttered data. Experiments show that fine-tuning on synthetic stuttered speech consistently reduces recognition errors on stuttered input compared to zero-shot and joint fine-tuning approaches, while maintaining or improving performance on fluent speech.

## Method Summary
The approach generates synthetic stuttered audio by applying text-level augmentations to Mozilla Common Voice Indonesian transcripts, then synthesizing audio using OpenAI TTS with speaker randomization and speed variation. Three stutter types are injected: repetitions ("sa-sa-saya"), prolongations ("sssaya"), and interjections ("umm", "anu"). The fine-tuning uses a pre-trained Whisper-small model initialized from `cahya/whisper-small-id` with AdamW optimizer (lr=1e-5), batch size 16, and linear scheduler. The critical finding is that stutter-only fine-tuning outperforms both zero-shot baselines and models trained on mixed clean/stuttered data.

## Key Results
- Stutter-only fine-tuning achieves WER of 0.126 on synthetic stuttered test sets vs 0.818 for zero-shot baseline
- Stutter-only model also attains lowest error rates on clean speech (WER 0.064) compared to mixed fine-tuning (WER 0.076)
- Synthetic data augmentation framework combines rule-based transformations with LLM generation for comprehensive stuttering coverage

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on synthetic stuttered speech reduces recognition errors on disfluent input more effectively than zero-shot or mixed-data approaches. The Whisper encoder-decoder learns to map dysfluent acoustic patterns to their intended fluent transcriptions by observing consistent text-to-audio pairings during transfer learning. The sequence-to-sequence formulation handles variable temporal alignments without explicit time warping. Core assumption: Synthetic stuttering approximates real stuttered speech sufficiently for model adaptation. Evidence: Stutter-only fine-tuning yields substantial WER reduction (0.126 vs 0.181 for mixed). Break condition: If real stuttered speech exhibits acoustic features not captured by TTS synthesis, synthetic-to-real transfer will degrade.

### Mechanism 2
Stutter-only fine-tuning does not degrade—and may improve—performance on fluent speech. Focused exposure to disfluency patterns forces the model to learn robust temporal alignment and context aggregation, which transfers to cleaner inputs. Core assumption: Acoustic diversity in synthetic stuttered data provides sufficient coverage of normal speech variation. Evidence: Stutter-only model attains lowest error rates on clean speech (WER 0.064 vs 0.076 for mixed). Break condition: If training data skews toward severe disfluencies without fluent baseline examples, the model may develop biased priors.

### Mechanism 3
Combining rule-based transformations with LLM-generated disfluencies produces more comprehensive stuttering coverage than either alone. Rule-based methods enforce phonologically plausible modifications while LLM generation introduces contextually natural hesitations and phrase-level patterns. Core assumption: Both augmentation strategies produce intelligible text resembling stuttering behavior. Evidence: Combining controlled rule-based operations with context-aware LLM outputs captures both predictable and naturalistic features. Break condition: If LLM produces culturally inappropriate Indonesian fillers or rules are too rigid, synthetic diversity will be artificially limited.

## Foundational Learning

- **Transfer learning with sequence-to-sequence models**: Why needed: Fine-tunes pre-trained Whisper model rather than training from scratch; understanding encoder-decoder attention and fine-tuning updates is essential. Quick check: Can you explain why Whisper's encoder-decoder architecture handles variable-length disfluent input better than a CTC-based model?

- **Word Error Rate (WER) and Character Error Rate (CER)**: Why needed: Primary evaluation metrics; understanding edit distance (substitutions, deletions, insertions) is necessary to interpret results. Quick check: If a model outputs "sa-sa-saya" when the reference is "saya," what edits contribute to WER?

- **Data augmentation for low-resource domains**: Why needed: Core contribution is synthetic data generation; understanding augmentation strategies (text-level vs. audio-level) clarifies design space. Quick check: Why might text-level augmentation followed by TTS be preferable to directly perturbing audio waveforms for stuttering simulation?

## Architecture Onboarding

- **Component map**: Common Voice transcripts → text augmentation (rule-based + LLM) → TTS synthesis → Whisper-small fine-tuning → evaluation
- **Critical path**: Load Common Voice transcripts → apply text augmentation → generate stuttered text variants → synthesize audio via TTS → preprocess (lowercase, strip punctuation, resample to 16kHz, pad to 30s) → fine-tune Whisper on stuttered-only data → evaluate on separate stuttered and clean test sets
- **Design tradeoffs**: Stutter-only vs. mixed training (mixed underperforms due to reduced emphasis on disfluency patterns); synthetic vs. real data (no real stuttered Indonesian data available, synthetic approach unvalidated); TTS speaker diversity (random selection may not match target population)
- **Failure signatures**: High WER on stuttered test set (>0.5) with low WER on clean suggests insufficient disfluency exposure or augmentation-reality gap; phonetic substitutions with semantic shifts indicate acoustic confusion; plural/singular confusions suggest prosodic irregularity effects
- **First 3 experiments**: 1) Reproduce baseline comparison: run zero-shot Whisper-small-id on stuttered test set to confirm WER (~0.818), then fine-tune on synthetic stuttered data and verify improvement (~0.126). 2) Ablate augmentation strategy: train separate models using rule-based only, LLM only, and combined; compare WER to quantify contribution. 3) Validate on real stuttered speech: evaluate synthetic-trained model on any available real stuttered Indonesian audio to measure synthetic-to-real transfer gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can ASR models fine-tuned exclusively on synthetic stuttered speech generalize effectively to real-world Indonesian stuttering patterns, including variable severity and speaker-specific behaviors? The study relies entirely on synthetic data because specialized datasets are virtually non-existent for Indonesian stuttered speech, leaving the synthetic-to-real transfer gap unquantified. Evidence needed: Performance evaluation on newly collected corpus of natural stuttered Indonesian speech across different severity levels.

### Open Question 2
Why does joint fine-tuning on clean and stuttered speech yield inferior performance on stuttered input compared to stutter-only training, and can curriculum learning or adaptive sampling resolve this? The authors note the observed performance gap where mixed training results in inferior performance, explicitly suggesting principled data balancing or curriculum learning strategies may be required. Evidence needed: Ablation studies comparing the proposed model against variants trained with curriculum learning or loss reweighting.

### Open Question 3
How can synthetic data pipelines be improved to capture non-textual disfluencies such as blocks and distinct prosodic cues, which are currently missing from the rule-based and TTS-generated audio? The current methodology relies on modifying text and generating audio via TTS, which struggles to simulate the silent tension of blocks or specific rhythmic irregularities of stuttering. Evidence needed: Development of synthetic pipeline that modulates silence/prosody explicitly, validated by improved detection accuracy for "block" events.

## Limitations
- Absence of real stuttered Indonesian speech for validation leaves synthetic-to-real transfer gap unquantified
- TTS-based synthetic audio may not capture acoustic complexity of real stuttering (blocks, tension-based prosody)
- Specific stutter injection probabilities, LLM prompts, and interjection vocabulary remain unspecified
- Generalizability to other low-resource languages or different ASR architectures untested

## Confidence

- **High confidence**: Experimental finding that stutter-only fine-tuning outperforms zero-shot and mixed-data approaches on synthetic stuttered test sets is well-supported by WER/CER metrics (0.126 vs 0.181 vs 0.818)
- **Medium confidence**: Claim that stutter-only fine-tuning also improves performance on clean speech (WER 0.064 vs 0.076 for mixed) is supported by results but relies on single dataset and architecture
- **Low confidence**: Assertion that combining rule-based and LLM augmentations produces superior coverage is based on qualitative reasoning rather than quantitative ablation studies

## Next Checks

1. Evaluate the synthetic-trained model on any available real stuttered Indonesian speech (even limited samples) to quantify the synthetic-to-real transfer gap and test the core assumption about synthetic data validity.

2. Train and evaluate separate models using rule-based only, LLM only, and combined augmentations to empirically measure each component's contribution to WER reduction.

3. Apply the synthetic data generation pipeline to another low-resource language with available TTS and text corpora, then evaluate whether the stutter-only fine-tuning pattern replicates to test framework's broader applicability beyond Indonesian.