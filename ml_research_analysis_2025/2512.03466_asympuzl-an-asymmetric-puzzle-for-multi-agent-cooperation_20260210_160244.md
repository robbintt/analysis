---
ver: rpa2
title: 'AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation'
arxiv_id: '2512.03466'
source_url: https://arxiv.org/abs/2512.03466
tags:
- puzzle
- agent
- feedback
- agents
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces AsymPuzl, a controlled two-agent puzzle environment
  designed to evaluate communication under information asymmetry. Each agent receives
  complementary but incomplete puzzle views and must exchange messages to solve it.
---

# AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation

## Quick Facts
- arXiv ID: 2512.03466
- Source URL: https://arxiv.org/abs/2512.03466
- Reference count: 30
- Strong models like GPT-5 and Claude-4.0 reliably solve puzzles by sharing complete information within two turns.

## Executive Summary
AsymPuzl is a controlled two-agent puzzle environment designed to evaluate communication under information asymmetry. Each agent receives complementary but incomplete puzzle views and must exchange messages to solve it. Experiments with various LLMs show that strong models reliably solve puzzles by sharing complete information within two turns, while weaker models often ignore messages or over-correct. Feedback design proved critical: simple self-feedback improved performance, but detailed joint feedback reduced success rates. The findings highlight the importance of carefully designed communication protocols in multi-agent LLM systems.

## Method Summary
AsymPuzl tests two-agent cooperation under information asymmetry using puzzles with shape-color-position mappings. Alice sees correct position-shape mappings with unknown colors; Bob sees correct shape-color mappings with unknown positions. Agents communicate in turns to solve the puzzle within a maximum of 2× puzzle size turns. The environment tracks working hypotheses, generates feedback, and measures success rates. Six feedback modes test different information-sharing strategies, and experiments use 30 seeds per condition across puzzle sizes (3, 5, 10, 20).

## Key Results
- Strong models (GPT-5, Claude-4.0) converge on solutions in exactly 2 turns by sharing complete information
- Simple self-feedback improves success rates, while detailed joint feedback hurts performance due to information overload
- Weaker models (GPT-3.5-turbo, Llama 3.2-11B) fail through ignored messages or excessive over-correction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complete information sharing in early turns enables reliable puzzle convergence under information asymmetry.
- Mechanism: Strong models enumerate all known constraints in the first message exchange, allowing both agents to construct the full solution in exactly 2 turns.
- Core assumption: Models infer that sharing all information immediately is optimal rather than probing incrementally.
- Evidence anchors:
  - [abstract] "strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns"
  - [section 5.3] "GPT-5 and Claude-4.0 are nearly optimal as they usually need only two turns of communication and their agents share all the information they have with one another"

### Mechanism 2
- Claim: Self-feedback improves convergence, but detailed joint feedback introduces information overload and reduces success.
- Mechanism: Simple "own" feedback provides a clear binary signal without context-switching. Detailed joint feedback exposes each agent to the other's error locations without visibility into their actual state, creating confusion rather than actionable guidance.
- Core assumption: Agents cannot mentally model the partner's hidden view, so knowing where the partner is wrong provides no corrective leverage.
- Evidence anchors:
  - [abstract] "simple self-feedback improves success rates, while detailed joint feedback can hurt performance"
  - [section 5.2] "providing detailed information about the other agent's working hypothesis on top of their own hurt performance; this can be attributed to information overload with lack of context"

### Mechanism 3
- Claim: Hypothesis stability (minimal over-correction) correlates with task success and reflects communication fidelity.
- Mechanism: Optimal agents modify each position at most once, after receiving needed information. Over-correction signals either ignored messages or cascading uncertainty from prior errors.
- Core assumption: The agent tracks which positions have already been resolved and avoids redundant edits.
- Evidence anchors:
  - [section 5.3] "GPT-5 and Claude-4.0 are nearly optimal...GPT-3.5-turbo tends not to modify positions despite the puzzle being unsolved, and Llama 3.2-11B tends to modify positions more than 4 times on average"

## Foundational Learning

- **Information asymmetry and complementary views**
  - Why needed here: The entire environment hinges on each agent holding partial, non-overlapping information that must be combined.
  - Quick check question: If Alice sees correct positions but unknown colors, and Bob sees correct colors but unknown positions, what must they exchange to solve the puzzle in one round-trip?

- **Feedback granularity vs. actionability**
  - Why needed here: The counterintuitive result that more detailed feedback can hurt requires understanding when information is actionable versus merely descriptive.
  - Quick check question: Why might telling Alice "Bob's position 3 is wrong" be less helpful than telling Alice "Your own positions are all correct"?

- **Working hypothesis as mutable state**
  - Why needed here: Agents maintain a working hypothesis that evolves across turns; understanding state management is critical for debugging and extending the environment.
  - Quick check question: What information is re-injected each turn, and what must the agent remember versus recompute?

## Architecture Onboarding

- **Component map:** Puzzle generator -> Environment loop -> Agent wrappers -> Hypothesis tracker -> vLLM (for open-source models)

- **Critical path:**
  1. Initialize puzzle and cues
  2. Turn loop: prompt agent -> parse message + actions -> update hypothesis -> generate feedback
  3. Check convergence; exit when both hypotheses match ground truth or max turns reached

- **Design tradeoffs:**
  - Re-injecting cues each turn vs. persistent agent memory: Current design aids debugging but differs from fully autonomous agents
  - Temperature 0.0 for reproducibility vs. higher values for exploring diverse strategies
  - History length of 1 vs. full history—limits context but reduces token cost

- **Failure signatures:**
  - 0 actions despite unsolved puzzle -> agent ignoring task (GPT-3.5-turbo pattern)
  - 4+ actions per position -> over-correction without convergence (Llama 3.2-11B pattern)
  - Success drops on larger puzzles -> scaling limits in reasoning
  - Success drops with "Both detailed" feedback -> information overload signature

- **First 3 experiments:**
  1. Run GPT-4o on 5-piece puzzles with all 6 feedback modes; plot success rate and average actions per position to confirm granularity effect
  2. Introduce a 3-agent variant where each holds one attribute dimension; test whether 2-turn convergence extends or degrades
  3. Restrict message token budget (e.g., 50 tokens max); observe whether strong models adapt or fail when complete information sharing is constrained

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM agents adapt their communication strategies when their partial views contain noise or ambiguity rather than just incomplete information?
- Basis in paper: [explicit] Appendix A states, "We leave for future work the analysis of noisy and ambiguous interactions where, for instance, Alice is given more shapes than required... requiring them to determine the relevant information."
- Why unresolved: The current study isolates communication under "clean" information asymmetry where all cues are relevant; it does not test the agents' ability to filter irrelevant data or resolve conflicts arising from contradictory cues.

### Open Question 2
- Question: How does restricting communication bandwidth or the number of operations per turn impact the efficiency and success rates of coordination?
- Basis in paper: [explicit] Appendix A lists as future work "the evaluation of constraints on communication, or the number of operations per turn."
- Why unresolved: The current environment allows unconstrained message lengths and action lists; we do not know if weaker models fail because they cannot compress information effectively or if strong models rely on verbose communication to succeed.

### Open Question 3
- Question: Do the communication strategies observed in the two-agent setup generalize or degrade when scaling to three or more agents?
- Basis in paper: [explicit] The Conclusion states the testbed can serve as a foundation for "scaling to three or more agents."
- Why unresolved: The study is limited to a dyadic setup (Alice and Bob); it is unclear if the "complete information sharing" strategy used by GPT-5/Claude-4.0 remains dominant or becomes chaotic in a multi-party broadcast setting.

### Open Question 4
- Question: To what extent does the environmental re-injection of state (vs. persistent agent memory) mask failure modes in long-horizon tasks?
- Basis in paper: [inferred] Appendix A notes the design choice to re-inject state "differs from fully autonomous agent memory" and leaves "extensions toward persistent agent state" for future work.
- Why unresolved: Because the environment handles state management, the study does not evaluate if agents can maintain context autonomously over many turns without external memory aids.

## Limitations
- Results are confined to the specific puzzle domain and may not generalize to other cooperative tasks
- Model-specific prompt engineering details are sparse, leaving open the possibility that performance differences stem from prompting rather than intrinsic reasoning ability
- The "success" metric treats any correct solution as equivalent, masking potential differences in efficiency or robustness

## Confidence
- **High**: Core mechanism that strong models share complete information within two turns under ideal conditions
- **Medium**: Feedback granularity effect (simple vs. detailed), as it is counter-intuitive and mechanism is plausible but not deeply validated
- **Low**: Claims about over-correction correlating with ignored messages or cascading uncertainty, as these are inferred from behavioral patterns

## Next Checks
1. Apply the same two-agent setup and feedback modes to a non-puzzle cooperative task (e.g., shared inventory management) to test if information-sharing and feedback patterns hold
2. Vary the history length (0, 1, 2, full) and max turns (fixed vs. 2× size) to determine the minimum context required for strong models to maintain two-turn convergence
3. Implement a "summary round" where agents must distill their entire state into a single message before solving, to test if early complete sharing is a learned heuristic or a model capability