---
ver: rpa2
title: Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment
arxiv_id: '2502.16469'
source_url: https://arxiv.org/abs/2502.16469
tags:
- detection
- text
- object
- rich
- typically
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cross-domain few-shot object detection
  method using rich textual descriptions to mitigate domain shift challenges. The
  proposed architecture employs a multi-modal feature aggregation module to align
  visual and linguistic embeddings and a rich text semantic rectification module to
  refine multi-modal feature alignment.
---

# Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment

## Quick Facts
- arXiv ID: 2502.16469
- Source URL: https://arxiv.org/abs/2502.16469
- Reference count: 40
- Achieves 61.4 mAP on ArTaxOr and 31.4 mAP on DIOR in 10-shot scenarios with LLM-generated rich text improving results by 1-2 mAP points

## Executive Summary
This paper addresses the challenge of cross-domain few-shot object detection, where models must detect novel classes with limited examples across different domains. The authors propose a novel approach that leverages rich textual descriptions alongside visual data to mitigate domain shift issues. By employing a multi-modal feature aggregation module that aligns visual and linguistic embeddings, combined with a rich text semantic rectification module for refining feature alignment, the method significantly outperforms existing approaches on three cross-domain benchmarks.

## Method Summary
The proposed method introduces a multi-modal architecture that combines visual and textual information for cross-domain few-shot object detection. It consists of two key components: a multi-modal feature aggregation module that aligns visual and linguistic embeddings, and a rich text semantic rectification module that refines the multi-modal feature alignment. The approach generates rich textual descriptions using large language models and uses these descriptions to create more robust feature representations that can handle domain shifts. The method is evaluated on three cross-domain benchmarks where it demonstrates substantial improvements over existing few-shot object detection approaches.

## Key Results
- Achieves 61.4 mAP on ArTaxOr dataset in 10-shot scenarios
- Achieves 31.4 mAP on DIOR dataset in 10-shot scenarios
- LLM-generated rich text descriptions further improve performance by 1-2 mAP points compared to manual descriptions

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of domain shift in few-shot object detection through multi-modal learning. When a model trained on one domain (source) must detect objects in another domain (target) with limited examples, traditional visual-only approaches struggle with the distribution shift. By incorporating rich textual descriptions that capture semantic properties of objects, the model can learn more generalizable representations that transfer better across domains. The multi-modal feature aggregation module creates aligned visual-linguistic embeddings that are more robust to domain-specific visual variations, while the semantic rectification module further refines these alignments to improve detection accuracy.

## Foundational Learning
- **Domain Shift in Few-shot Learning**: The phenomenon where models trained on one data distribution perform poorly on another distribution with limited examples. Why needed: Core problem being addressed. Quick check: Can the model detect objects in target domain with minimal examples?
- **Multi-modal Feature Alignment**: The process of creating compatible representations between different data modalities (visual and textual). Why needed: Enables the model to leverage textual descriptions for visual detection. Quick check: Do visual and textual embeddings have compatible feature spaces?
- **Large Language Model Prompting**: Techniques for guiding LLMs to generate specific types of text. Why needed: Generates rich descriptions that improve detection performance. Quick check: Does changing the prompt format affect the generated text quality?
- **Semantic Rectification**: The process of refining feature alignments to better capture class semantics. Why needed: Improves the quality of multi-modal feature aggregation. Quick check: Does rectification improve alignment between visual and textual features?
- **Cross-domain Evaluation**: Assessing model performance across different data distributions. Why needed: Validates the method's ability to handle domain shift. Quick check: Is performance consistent across multiple domain pairs?

## Architecture Onboarding

**Component Map:**
Visual Encoder -> Multi-modal Feature Aggregation -> Rich Text Semantic Rectification -> Detection Head

**Critical Path:**
1. Visual feature extraction from support and query images
2. Textual feature extraction from rich descriptions
3. Multi-modal feature aggregation to align visual-linguistic embeddings
4. Rich text semantic rectification to refine alignments
5. Detection head for final object detection predictions

**Design Tradeoffs:**
- Text quality vs. computational cost: LLM-generated descriptions improve performance but add inference overhead
- Description richness vs. specificity: More detailed descriptions may introduce noise for visually similar categories
- Feature alignment complexity vs. training stability: More sophisticated alignment may improve accuracy but could cause optimization difficulties

**Failure Signatures:**
- Poor performance on visually ambiguous categories where text descriptions are not distinctive
- Performance degradation when textual descriptions include irrelevant contextual information
- Inconsistent results across different domain pairs indicating sensitivity to domain characteristics

**3 First Experiments:**
1. Ablation study removing the rich text components to quantify their contribution to overall performance
2. Evaluation on zero-shot detection by removing visual support sets entirely
3. Analysis of different LLM prompting strategies to optimize text distinctiveness for technical categories

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can large language model (LLM) prompting strategies be optimized to ensure distinctiveness in rich text descriptions for abstract or visually similar technical categories, where generic descriptions currently fail to improve performance?
- **Basis in paper:** The authors note in Section 4.2.2 that LLM-generated text yielded lower performance on the NEU-DET dataset because the LLM failed to produce sufficiently distinctive descriptions for the six defect categories, resulting in text homogeneity.
- **Why unresolved:** The paper demonstrates the failure case but does not propose a specific method to guide LLMs toward generating semantically distinct technical descriptions for visually ambiguous classes.
- **What evidence would resolve it:** A study comparing different prompt engineering techniques (e.g., using visual features to ground the text generation) on the NEU-DET dataset showing recovered or improved mAP over manual descriptions.

### Open Question 2
- **Question:** To what extent does the inclusion of visual co-occurrence context in textual descriptions introduce semantic noise that degrades feature alignment, and can this be automatically filtered?
- **Basis in paper:** In Section 4.3.1 and Table 14, extending the description for "sea cucumbers" to include "usually seen together with sea urchins" slightly reduced performance (17.4 to 16.9 mAP). The authors suggest this introduces confusion, implying a trade-off between descriptive richness and class-specific precision.
- **Why unresolved:** The paper does not quantify the boundary between helpful context and distracting noise in the multi-modal aggregation module.
- **What evidence would resolve it:** An ablation study systematically varying the ratio of descriptive attributes versus contextual co-occurrences in the text to identify the point of performance degradation.

### Open Question 3
- **Question:** Can the rich text semantic rectification module enable effective cross-domain zero-shot object detection when visual support sets are entirely absent?
- **Basis in paper:** The Introduction mentions that multi-modal learning opens the door to zero-shot learning. However, the proposed method is strictly few-shot, requiring a visual support set alongside the text. The authors leave the zero-shot capability of their text-rectification architecture untested.
- **Why unresolved:** It is unclear if the text embeddings refined by the rectification module are robust enough to generalize to novel classes without any visual support examples.
- **What evidence would resolve it:** Experimental results on a zero-shot benchmark (e.g., zero-shot splits of COCO or LVIS) utilizing the trained text encoder and rectification module without the visual support branch.

## Limitations
- The method's reliance on external LLM-generated rich text descriptions introduces potential variability in performance depending on the quality and specificity of the generated text
- The proposed approach appears to be computationally intensive due to the multi-modal feature aggregation and semantic rectification modules
- The evaluation focuses on specific benchmark datasets (ArTaxOr, DIOR) and may not generalize to all cross-domain scenarios

## Confidence
**High Confidence:**
- The core methodology of using multi-modal feature aggregation for cross-domain few-shot object detection is technically sound and well-explained
- The experimental results showing performance improvements over existing methods are supported by the presented data
- The problem formulation addressing domain shift in few-shot scenarios is valid and important

**Medium Confidence:**
- The specific performance gains (61.4 mAP on ArTaxOr, 31.4 mAP on DIOR) are credible but would benefit from additional validation across more diverse datasets
- The claimed improvement from LLM-generated rich text (1-2 mAP points) is reasonable but the dependency on text quality is not fully characterized

## Next Checks
1. Conduct ablation studies systematically removing the rich text components to quantify the exact contribution of textual enrichment versus other architectural improvements, particularly under varying text quality conditions.

2. Evaluate the approach on additional cross-domain datasets with different characteristics (e.g., medical imaging, satellite imagery, underwater scenes) to assess generalization beyond the current benchmarks.

3. Perform computational efficiency analysis including inference time, memory requirements, and comparison with baseline methods to establish practical deployment considerations for real-world applications.