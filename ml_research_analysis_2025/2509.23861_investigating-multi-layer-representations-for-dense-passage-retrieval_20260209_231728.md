---
ver: rpa2
title: Investigating Multi-layer Representations for Dense Passage Retrieval
arxiv_id: '2509.23861'
source_url: https://arxiv.org/abs/2509.23861
tags:
- retrieval
- linguistics
- document
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes using representations from multiple layers\
  \ of a BERT or T5 encoder to improve dense passage retrieval. Unlike prior work\
  \ that uses only the last layer, this method\u2014called Multi-layer Representations\
  \ (MLR)\u2014explores different combinations of hidden states across layers to construct\
  \ document embeddings."
---

# Investigating Multi-layer Representations for Dense Passage Retrieval

## Quick Facts
- arXiv ID: 2509.23861
- Source URL: https://arxiv.org/abs/2509.23861
- Reference count: 40
- Primary result: Multi-layer representations improve dense passage retrieval accuracy over single-layer methods

## Executive Summary
This paper introduces Multi-layer Representations (MLR) for dense passage retrieval, challenging the standard practice of using only the last layer of BERT/T5 encoders. The method aggregates representations from multiple encoder layers to create richer document embeddings, using either multi-vector retrieval (multiple vectors per document) or single-vector retrieval with self-contrastive pooling. MLR demonstrates consistent improvements across multiple benchmarks (NQ, TriviaQA, SQuAD), outperforming standard dual-encoders and matching or exceeding more complex methods like ME-BERT and ColBERT in single-vector settings while maintaining computational efficiency.

## Method Summary
MLR extracts [CLS] token embeddings from multiple transformer layers (rather than just the last layer) and aggregates them during training. For single-vector retrieval, it employs a self-contrastive pooling strategy that regularizes the final layer representation against intermediate layers using a max-aggregation loss. During inference, only the efficient last-layer vector is used for searching. The method uses standard dual-encoder architecture with in-batch negatives and gradient caching to manage memory constraints, and shows particular effectiveness when using the last few layers (e.g., S={10,12} for BERT-base) rather than uniformly distributed layers.

## Key Results
- Single-vector MLR improves retrieval accuracy by 3-4% over standard dual-encoders on SQuAD
- MLR outperforms ME-BERT and ColBERT in most single-vector settings while maintaining same efficiency
- Multi-vector MLR with 4 vectors from last few layers performs best, but capacity degrades with more vectors
- MLR integrates effectively with advanced techniques like retrieval-oriented pre-training and hard negative mining

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Feature Stratification
- **Claim:** Intermediate layers capture complementary linguistic features (syntax in middle layers, semantics in higher layers) that improve retrieval when aggregated
- **Core assumption:** Pre-trained encoders follow hierarchical feature extraction where different phenomena separate by depth
- **Evidence anchors:**
  - [Section 1]: Cites prior work showing syntactic information in middle layers, semantics across all layers
  - [Section 4.2.1]: Shows last few layers outperform uniformly distributed layers for 4-vector MLR
  - [Corpus]: No direct support in provided neighbors

### Mechanism 2: Self-Contrastive Regularization
- **Claim:** Self-contrastive pooling improves single-vector performance by regularizing the final layer against intermediate layers during training
- **Core assumption:** Final layer can approximate multi-layer signal if sufficiently regularized
- **Evidence anchors:**
  - [Section 3.2]: Defines self-contrastive loss using max-inner-product over layer set
  - [Section 4.2.2]: Authors hypothesize gains come from regularization effect of max-aggregation
  - [Corpus]: Not explicitly discussed in corpus neighbors

### Mechanism 3: Constrained Multi-Vector Capacity
- **Claim:** MLR has limited scaling capacity compared to token-based multi-vector models due to high correlation between layer representations
- **Core assumption:** Layer representations at [CLS] token are more correlated with each other than token representations within same layer
- **Evidence anchors:**
  - [Section 4.2.2]: Notes representational capacity cannot scale well with more vectors
  - [Figure 3]: Shows downward trend in accuracy as m increases beyond 4
  - [Corpus]: Corpus papers don't contrast layer-based vs token-based multi-vector scaling

## Foundational Learning

- **Concept: Transformer Layer Hierarchy**
  - **Why needed here:** Understanding hierarchical feature extraction is essential to interpreting why MLR selects specific layer combinations
  - **Quick check question:** Does a BERT model typically capture syntactic dependencies in layer 1 or layer 10?

- **Concept: Dual-Encoder Contrastive Learning**
  - **Why needed here:** Understanding baseline mechanism is crucial to grasp MLR's modifications to standard dual-encoder loss
  - **Quick check question:** In a standard dual-encoder, is the interaction between query and document calculated during encoding or scoring?

- **Concept: Max-Inner-Product (MIP) Similarity**
  - **Why needed here:** MLR relies on MIP to select best representation from layer vectors
  - **Quick check question:** If Layer 10 has similarity score 0.8 and Layer 12 has 0.9 with the query, which vector contributes to the gradient in max-pooling setup?

## Architecture Onboarding

- **Component map:** Encoders (Query E_Q, Document E_D) -> Collector (extract [CLS] from layers S) -> Pooler (implement Eq. 5/6 logic)

- **Critical path:**
  1. Pass document through E_D
  2. Extract [CLS] vectors for layers in set S (e.g., layers 10 and 12)
  3. Training: Calculate max-similarity between Query vector and Doc vector set; apply regularization loss to align last layer
  4. Inference: Use only final layer vector for indexing and search

- **Design tradeoffs:**
  - Efficiency vs. Performance: Multi-vector MLR increases index size and search time linearly; single-vector MLR maintains O(N) efficiency with training overhead
  - Layer Selection: Last 2 layers (S={10,12}) often better than all layers, balancing noise reduction against feature coverage

- **Failure signatures:**
  - Capacity Saturation: Attempting to improve retrieval by increasing m to 8 degrades performance
  - Regularization Collapse: Too high λ causes model to fail distinguishing positive/negative documents
  - Symmetric Task Failure: Asymmetric encoding (multi-layer doc, single-layer query) may underperform on symmetric tasks

- **First 3 experiments:**
  1. Sanity Check (Single-Vector): Train standard Dual-Encoder vs. Single-Vector MLR (S={10,12}) on SQuAD; verify 3-4% Top-5 accuracy improvement
  2. Layer Ablation: Fix m=2, sweep lower layer index (l₁ ∈ {2,4,...,10}) to confirm higher layers yield better results
  3. Hyperparameter Scan: Sweep regularization strength λ ∈ {0.01, 0.1, 1.0, 10.0} to find stable region for self-contrastive loss

## Open Questions the Paper Calls Out

- **Symmetric Search Tasks:** The authors explicitly state it would be direct future work to explore utilizing multiple vectors to represent the query as well, as current method is asymmetric and may limit capacity for symmetric tasks like duplicate question identification.

- **Multi-Vector Scaling Limitations:** The paper identifies that MLR's representational capacity cannot scale well with more vectors due to layer correlation, but does not experiment with mechanisms to decorrelate intermediate layers to improve scaling.

- **Decoder-Only LLM Applications:** While computational budgets for LLM-based retrievers are mentioned, experiments are restricted to encoder-only (BERT) and encoder-decoder (T5) models, leaving unclear if MLR transfers effectively to decoder architectures.

## Limitations
- Limited ablation studies on regularization weight λ and layer selection across diverse datasets
- Scalability constraints with diminishing returns as number of vectors increases
- Evaluation scope primarily limited to open-domain QA benchmarks (NQ, TriviaQA, SQuAD)

## Confidence
- **High Confidence:** Empirical claim that MLR improves single-vector dense retrieval accuracy over standard dual-encoders on NQ, TriviaQA, and SQuAD
- **Medium Confidence:** Mechanism claim that intermediate layers contain complementary retrieval features that improve representations when aggregated
- **Medium Confidence:** Self-contrastive pooling mechanism improves single-vector performance by regularizing final layer
- **Low Confidence:** Claim that MLR's representational capacity cannot scale well with more vectors due to layer correlation

## Next Checks
1. Compute pairwise cosine similarity between [CLS] vectors from different layers on held-out set to verify high correlation claim
2. Perform systematic ablation study across λ ∈ {0.01, 0.1, 1.0, 10.0} on all three datasets to determine optimal range
3. Evaluate MLR on non-QA retrieval task (e.g., MS MARCO passage ranking) to test task generalization