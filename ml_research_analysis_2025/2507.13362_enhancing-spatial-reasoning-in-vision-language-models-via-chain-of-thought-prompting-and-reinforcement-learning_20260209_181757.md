---
ver: rpa2
title: Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought
  Prompting and Reinforcement Learning
arxiv_id: '2507.13362'
source_url: https://arxiv.org/abs/2507.13362
tags:
- reasoning
- spatial
- scene
- prompting
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the spatial reasoning capabilities of vision-language
  models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning.
  The authors find that simple CoT formats fail to improve and can even harm performance,
  while structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly
  enhances spatial reasoning accuracy.
---

# Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2507.13362
- **Source URL:** https://arxiv.org/abs/2507.13362
- **Reference count:** 37
- **Primary result:** Structured two-stage SceneGraph CoT prompting and GRPO fine-tuning significantly enhance VLM spatial reasoning accuracy and OOD robustness compared to naive CoT and SFT

## Executive Summary
This paper investigates spatial reasoning capabilities in vision-language models (VLMs) through structured Chain-of-Thought (CoT) prompting and reinforcement learning. The authors find that simple CoT formats can harm performance through reward hacking, while structured two-stage SceneGraph CoT (generating JSON scene graphs before answering) improves accuracy by 5-15%. They fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate on CVBench, finding GRPO outperforms supervised fine-tuning (SFT) in Pass@1 evaluations and demonstrates superior robustness to out-of-distribution linguistic variations. Notably, SFT overfits to surface-level linguistic patterns and degrades when phrasing changes, while GRPO generalizes more reliably and maintains stable performance.

## Method Summary
The method combines structured two-stage SceneGraph CoT prompting with GRPO fine-tuning. SceneGraph CoT forces explicit visual parsing through a JSON scene graph intermediate representation before answer generation, reducing reward hacking. GRPO fine-tuning uses LoRA adapters (rank 8-16) with dynamic sampling to prevent reward collapse when group rewards are identical. The training targets all attention and MLP projection modules with β=0.01, optimizing relative advantages across groups of samples. Evaluation uses 4-shot conversational prompting on CVBench (Pass@1, Pass@4 metrics) and semantic polarity flips to test OOD robustness.

## Key Results
- Simple CoT prompting degrades spatial reasoning performance by 5-15% due to reward hacking
- SceneGraph CoT improves accuracy by 5-15% by forcing structured visual parsing
- GRPO outperforms SFT in Pass@1 evaluations and shows 9% better OOD generalization (12.03% vs 3.17% drop)
- SFT overfits to linguistic patterns and degrades significantly when phrasing changes (e.g., "closer to" vs "farther from")

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Structured two-stage prompting (SceneGraph CoT) decouples visual parsing from logical inference, reducing "reward hacking."
- **Mechanism**: By forcing the model to first generate a scene graph before producing an answer, the system prevents the VLM from jumping directly to a likely-looking answer based on linguistic priors. This explicit serialization of visual attention improves counting and relation accuracy.
- **Core assumption**: The VLM possesses sufficient capability to generate accurate scene graphs when prompted explicitly; the failure mode is primarily in the reasoning chain, not the visual encoder.
- **Evidence anchors**:
  - [abstract] "structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly enhances spatial reasoning accuracy."
  - [section IV.C] "Two-step prompting outperforms one-step approaches... likely due to reward hacking —models shortcutting reasoning for fast answer generation."
  - [corpus] MedGround-R1 validates that separating spatial grounding via specific rewards improves medical image localization.

### Mechanism 2
- **Claim**: Group Relative Policy Optimization (GRPO) generalizes better to out-of-distribution (OOD) linguistic variations than Supervised Fine-Tuning (SFT).
- **Mechanism**: SFT minimizes token-level loss, causing the model to memorize specific phrasing. GRPO optimizes a relative advantage function based on outcome rewards (correct/incorrect) across a group of samples, allowing the model to ignore surface-level linguistic correlations and focus on the underlying spatial causality.
- **Core assumption**: The base model has latent reasoning capabilities that are misaligned with the output space, rather than being entirely absent.
- **Evidence anchors**:
  - [abstract] "SFT overfits to surface-level linguistic patterns... GRPO, on the other hand, generalizes more reliably."
  - [section IV.B] "SFT experiences a 12.03% drop [in OOD Distance tasks], whereas GRPO sees only a 3.17% reduction."
  - [corpus] Reason-RFT and AutoDrive-R2 similarly suggest RL aligns reasoning capacity better than SFT for complex visual tasks.

### Mechanism 3
- **Claim**: Dynamic sampling stabilizes GRPO training by filtering for informative gradients.
- **Mechanism**: In standard GRPO, if all generated samples in a group receive the same reward, the advantage function becomes zero, and the loss reduces to the KL-divergence penalty. This pushes the model back toward the frozen reference model. Dynamic sampling discards these zero-variance groups, ensuring every batch provides a learning signal.
- **Core assumption**: A significant portion of training batches may lack distinct "winners" and "losers," particularly in early training or difficult spatial tasks.
- **Evidence anchors**:
  - [section IV.B] "In GRPO-v1... frequent collapse in group-wise reward signals... dynamic sampling strategy... focuses the learning process on truly informative examples."
  - [section IV.B] Describes the loss equation where the gradient scales by Advantage; if A ≈ 0 everywhere, the update is dominated by the KL term.

## Foundational Learning

- **Concept: Scene Graphs**
  - **Why needed here**: The paper uses scene graphs as the intermediate "reasoning state" to force explicit spatial declaration before answering.
  - **Quick check question**: Can you distinguish between an "object" node and a "relationship" edge in a JSON scene graph?

- **Concept: KL Divergence Penalty (β)**
  - **Why needed here**: Crucial for understanding why GRPO-v1 failed (collapsed rewards mean the model is only pushed back toward its base state by the KL term).
  - **Quick check question**: If the reward signal vanishes, does a high β force the model to explore more or revert to the base model?

- **Concept: Pass@1 vs. Pass@4 Evaluation**
  - **Why needed here**: The paper highlights a tradeoff where SFT wins on Pass@4 (coverage) but GRPO wins on Pass@1 (precision/alignment).
  - **Quick check question**: Does a higher Pass@4 score guarantee a higher Pass@1 score?

## Architecture Onboarding

- **Component map**: Image+Question -> Scene Graph Generation -> Answer Generation -> Binary Reward -> Group Sampling -> LoRA Update
- **Critical path**:
  1. Input: Image + Question
  2. Gen 1: Model generates Scene Graph (checked for format)
  3. Gen 2: Model generates Answer based on Image + Graph
  4. Reward: Binary accuracy check (Correct/Incorrect)
  5. Sampling: Group of k samples filtered for variance
  6. Update: LoRA weights updated via GRPO loss
- **Design tradeoffs**:
  - SFT vs. GRPO: Use SFT if you need high diversity/coverage (Pass@4) or data is scarce; use GRPO if you need high precision (Pass@1) and robustness to phrasing changes (OOD)
  - LoRA Rank: Higher rank (16 vs 8) helped integrate vision-language features but increases compute
- **Failure signatures**:
  - Reward Hacking: Model ignores the image and graphs, repeating common answers (fixed by 2-stage prompting)
  - Reward Collapse: Loss curve flattens or regresses due to zero advantage variance (fixed by Dynamic Sampling)
  - Linguistic Overfitting: Accuracy tanks when "closer" changes to "farther" (indicative of SFT reliance on surface tokens)
- **First 3 experiments**:
  1. Baseline Check: Run standard CoT vs. Direct Answer on SAT to verify naive CoT degrades performance
  2. OOD Robustness Test: Fine-tune two small models (one SFT, one GRPO) on SAT, evaluate on flipped CVBench set
  3. Ablate Dynamic Sampling: Train GRPO-v2 with and without dynamic sampler; monitor discarded batches and final Pass@1 accuracy

## Open Questions the Paper Calls Out

- **Question 1**: Can GRPO-based fine-tuning effectively extend from static image-based spatial reasoning to video understanding tasks involving temporal dynamics, motion-based spatial relations, and multi-frame context?
  - **Basis**: [explicit] Future Work mentions extending to video understanding tasks and embodied decision-making
  - **Why unresolved**: Current study exclusively evaluates image-based spatial reasoning; no video experiments were conducted
  - **What evidence would resolve it**: Evaluation on video spatial reasoning benchmarks showing whether temporal reasoning improvements transfer

- **Question 2**: What are the underlying mechanisms that make Scene Graph CoT effective for spatial reasoning while other structured CoT variants consistently degrade performance?
  - **Basis**: [inferred] Paper demonstrates Scene Graph CoT improves accuracy while PoT, CoS, and VoT "consistently underperformed" without mechanistic explanation
  - **Why unresolved**: Ablation studies report performance differences but don't analyze what specific properties of scene graph representations align with VLM spatial reasoning capabilities
  - **What evidence would resolve it**: Probing experiments analyzing intermediate representations and attention patterns between successful Scene Graph CoT and failed alternatives

- **Question 3**: Do GRPO fine-tuning benefits and Scene Graph CoT improvements generalize to VLM architectures beyond the three model families tested?
  - **Basis**: [inferred] Conclusion acknowledges future work needed to assess robustness across architectures, modalities, and spatial reasoning variants
  - **Why unresolved**: All experiments limited to three specific model families (3B–17B) with varying parameter scales
  - **What evidence would resolve it**: Systematic evaluation across additional VLM architectures showing whether improvements remain consistent

## Limitations

- SAT dataset may not fully represent real-world spatial reasoning complexity
- GRPO implementation's dynamic sampling strategy appears novel without direct validation in prior work
- OOD robustness claims rely heavily on "flipped" linguistic variants without testing deeper semantic understanding

## Confidence

**High Confidence:** The core finding that simple CoT prompting can degrade spatial reasoning performance is well-supported by empirical evidence and clearly demonstrates reward hacking.

**Medium Confidence:** The superiority of GRPO over SFT for OOD generalization is supported by the reported performance gap, but could be influenced by specific implementation details not fully specified.

**Low Confidence:** The claim that GRPO maintains stable performance while SFT degrades under linguistic variations assumes the test set provides sufficient coverage of meaningful semantic shifts rather than superficial phrase changes.

## Next Checks

1. **Ablation Study on Dynamic Sampling:** Train GRPO with and without dynamic sampling across multiple random seeds to quantify its contribution to stability. Monitor reward variance distributions and convergence curves.

2. **Semantic Complexity Test:** Create a test set with controlled semantic shifts beyond simple polarity flips (e.g., "closest" vs "second closest," "left of A but right of B") to verify whether GRPO's generalization advantage reflects true reasoning versus memorization.

3. **Cross-Domain Transfer Evaluation:** Evaluate the fine-tuned models on spatial reasoning tasks from different domains (medical imaging from MedGround-R1, autonomous driving scenarios from AutoDrive-R2) to test whether improvements transfer beyond the SAT domain.