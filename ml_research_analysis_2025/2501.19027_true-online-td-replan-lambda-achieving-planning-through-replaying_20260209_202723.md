---
ver: rpa2
title: True Online TD-Replan(lambda) Achieving Planning through Replaying
arxiv_id: '2501.19027'
source_url: https://arxiv.org/abs/2501.19027
tags:
- online
- 'true'
- algorithm
- replay
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces True Online TD-Replan(\u03BB), a reinforcement\
  \ learning method that extends true online TD(\u03BB) to enable efficient experience\
  \ replay. The method allows agents to replay past experiences online, with \u03BB\
  \ controlling both the depth of TD targets and the density of the replay process."
---

# True Online TD-Replan(lambda) Achieving Planning through Replaying

## Quick Facts
- arXiv ID: 2501.19027
- Source URL: https://arxiv.org/abs/2501.19027
- Authors: Abdulrahman Altahhan
- Reference count: 18
- One-line primary result: True Online TD-Replan(λ) achieves planning-like credit assignment through efficient online replay, outperforming standard TD(λ) on random walk and myoelectric control tasks.

## Executive Summary
This paper introduces True Online TD-Replan(λ), which extends true online TD(λ) to enable efficient experience replay by replaying past experiences online with current weights. The method uses λ to control both the depth of TD targets and the density of the replay process, achieving planning-like credit assignment through matrix accumulation techniques. Experiments on a 17-state random walk and a myoelectric cursor control task demonstrate superior performance compared to standard TD(λ) and alternative methods like Dyna Planning, particularly when using deeply learned features from a sparse autoencoder.

## Method Summary
True Online TD-Replan(λ) modifies the true online TD(λ) update to incorporate replay of past experiences using current weights rather than reinitializing weights for each replayed trajectory. The algorithm maintains accumulated matrices Ā and vectors ē that enable O(n²) incremental updates regardless of trajectory length, avoiding the O(t×n) per-step cost of naive replay. The method assumes linear function approximation V(s|θ)=θᵀφ(s) and uses interim λ-return targets during replay. A key innovation is the interpolation parameter λ̄ that controls replay density, with λ̄=0 reducing to standard TD(λ) and λ̄=1 enabling full replay. The approach is demonstrated on two domains: a 17-state random walk with binary features and a myoelectric control problem using 16-channel sEMG signals or 256 latent features from a sparse autoencoder.

## Key Results
- On the random walk task, TD-Replan(λ) with λ̄=1 achieves lower RMSE than standard TD(λ) across multiple α values
- In the myoelectric control domain, TD-Replan(λ) with λ̄=1 outperforms Dyna Planning and TD(0)-Replan when using 256 features from a sparse autoencoder
- Higher replay density (λ̄ approaching 1) consistently improves performance in domains with informative features
- The method achieves better sample efficiency by effectively "planning" through replaying experiences with updated knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replaying past experience using current weights (not reinitializing) propagates learned information backward through the trajectory, achieving planning-like credit assignment.
- Mechanism: Each replay step k uses weights θᵗₖ (current-timestep weights applied to past-step features) and produces θᵗₖ₊₁. The final weight θᵗ₊₁ₜ₊₁ aggregates all past updates through matrix products Aₜ...A₀.
- Core assumption: The feature representation is linear or the last layer is linear with V(s|θ)=θᵀφ(s).
- Evidence anchors:
  - [abstract]: "allows an agent to efficiently replay all or part of its past experience, online in the sequence that they appear with"
  - [Section II]: "replaying the experience requires going through a bundle of past experience and redo the updates as if the agent went through them again but with its current set of weights"
  - [corpus]: Related work on replay for continual learning (PVBF, Online Curvature-Aware Replay) assumes replay mitigates forgetting, but does not address sequential replay with λ-return targets specifically.

### Mechanism 2
- Claim: Using interim λ-return targets during replay provides multi-step bootstrapping that balances bias and variance for each replayed update.
- Mechanism: The target G^λ|ᵗₖ mixes n-step returns with weight λᶦ⁻¹, controlled by λ. This applies to every replayed step, not just the current online step.
- Core assumption: The λ-return provides a meaningful bias-variance tradeoff; the environment's reward structure benefits from multi-step targets.
- Evidence anchors:
  - [abstract]: "λ parameter plays a new role in specifying the density of the replay process in addition to the usual role of specifying the depth of the target's updates"
  - [Section III]: "G^λ|ᵗₖ is the interim λ-return introduced in [9]"
  - [corpus]: No direct corpus comparison for λ-return in replay; related replay methods (Next-Future HER, Replay Adversary) do not explicitly address λ-controlled target depth.

### Mechanism 3
- Claim: Accumulated matrix Ā and vector ē enable O(n²) incremental updates regardless of trajectory length, avoiding O(t×n) per-step cost.
- Mechanism: Āₜ = AₜĀₜ₋₁ and ēₜ = Aₜēₜ₋₁ + eₜδₜ + αₜφₜ(θᵀₜ₋₁φₜ) maintain sufficient statistics. The final update θₜ₊₁ = Āₜθₜ + ēₜ applies once per step.
- Core assumption: n (feature count) is small enough that O(n²) matrix operations are tractable; t > n typically holds.
- Evidence anchors:
  - [Section III]: "complexity of O(n × t)... we shall use the formalism used by [10] and [9] to make the complexity O(n²)"
  - [Section IV, Theorem 1]: Proof establishes equivalence between forward and incremental forms.

## Foundational Learning

- Concept: **TD(λ) and eligibility traces**
  - Why needed here: TD-Replan extends true online TD(λ); understanding eₜ accumulation and λ's role in mixing n-step returns is essential.
  - Quick check question: Can you explain why λ=1 gives Monte Carlo returns and λ=0 gives one-step TD?

- Concept: **Experience replay vs. planning**
  - Why needed here: The paper frames replay as a form of planning (re-evaluating past experience with current knowledge).
  - Quick check question: How does Dyna-style planning differ from sequential replay with current weights?

- Concept: **Linear function approximation**
  - Why needed here: The method assumes V(s|θ)=θᵀφ(s) for the last layer; matrix updates rely on this linearity.
  - Quick check question: If the value head were nonlinear, which update equations would break?

## Architecture Onboarding

- Component map:
  - Feature extractor (optional) -> Value head (linear) -> Eligibility trace e -> Accumulated vector ē -> Accumulated matrix Ā -> Replay depth controller λ̄

- Critical path:
  1. Observe (φₜ, Rₜ₊₁, φₜ₊₁); compute δₜ = Rₜ₊₁ + γV(φₜ₊₁) - V(φₜ)
  2. Update eₜ ← γλeₜ₋₁ + αₜφₜ(1 - eᵀₜ₋₁φₜ)
  3. Update ēₜ ← ēₜ₋₁ + eₜ(δₜ + Vₜ - Vₜ₋₁) - αₜφₜ(ēᵀₜ₋₁φₜ - Vₜ₋₁)
  4. Update Āₜ ← Āₜ₋₁ - αₜφₜ(φᵀₜĀₜ₋₁)
  5. Update θₜ₊₁ ← Āₜ(λ̄θₜ + (1-λ̄)θ₀) + ēₜ

- Design tradeoffs:
  - Higher λ̄ → more replay, better sample efficiency, higher O(n²) cost amortized
  - Higher λ → deeper targets, lower bias but higher variance
  - Deep features (AE) improve performance but increase n; O(n²) scales accordingly
  - Assumption: Episodic reset of θ₀, Ā, ē per episode; not designed for infinite-horizon non-episodic settings without modification

- Failure signatures:
  - Divergence with large α: Ā may accumulate numerical instability; monitor θ norm
  - No improvement over TD(λ): Likely λ̄=0 (default) or features are uninformative
  - Excessive memory: Ā is n×n; for n>1000, consider low-rank approximations (not in paper)
  - Dyna Planning struggles with deep sparse features (Fig. 3-5): Model-based planning may fail when feature dynamics are hard to learn

- First 3 experiments:
  1. **Random walk sanity check**: Implement binary features (17 states), compare TD-Replan(λ=0.9, λ̄=1) vs. TD(λ=0.9) on RMSE over 10 episodes; expect faster convergence per Fig. 2
  2. **Ablate λ̄**: Fix λ=0.9, sweep λ̄∈{0,0.2,0.5,0.8,1}; plot RMSE vs. α to reproduce Fig. 6 pattern (optimal λ̄≈0.8-1 for deep features)
  3. **Feature comparison**: On myoelectric domain, compare raw sEMG (n=16) vs. AE features (n=256); measure RMSE and sensitivity to α per Fig. 3 vs. Fig. 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can True Online TD-Replan(λ) be successfully integrated with on-policy and off-policy control methods?
- Basis in paper: [explicit] The conclusion states: "Future work includes showing that our methods can be integrated with on-policy or off-policy learning updates to produce new control methods"
- Why unresolved: The paper only demonstrates policy evaluation (prediction), not control; extending to Sarsa-style or Q-learning-style control requires additional theoretical development.
- What evidence would resolve it: Derivation and empirical validation of control variants (e.g., Sarsa-Replan, Q-learning-Replan) with convergence analysis.

### Open Question 2
- Question: Can TD-Replan be adapted for true end-to-end deep reinforcement learning with gradient backpropagation through all layers?
- Basis in paper: [explicit] The conclusion lists "tackling an end-to-end training of a deep reinforcement learning model that is based on our method" as future work
- Why unresolved: Current experiments use a dissection approach (autoencoder pre-training separate from TD-Replan), not integrated end-to-end learning.
- What evidence would resolve it: Demonstration of joint feature learning and value prediction with backpropagation through deep networks, achieving stable convergence.

### Open Question 3
- Question: What alternative interpolation schemes between full replay and no replay might outperform the linear combination used in TD(λ)-Replan(´λ)?
- Basis in paper: [explicit] Section VII states: "This is an open question that one can address in several ways. One way to perform this requirement is by constructing a linear combination..."
- Why unresolved: The linear interpolation is presented as one possible approach among many, with no theoretical justification for optimality.
- What evidence would resolve it: Systematic comparison of alternative replay-depth parameterizations with theoretical grounding and empirical benchmarking.

### Open Question 4
- Question: How does the quadratic complexity scale practically in high-dimensional domains compared to alternative planning methods?
- Basis in paper: [inferred] The paper acknowledges quadratic complexity and tests only on small feature spaces (n=17 binary features, n=16–256 sEMG features); large-scale scalability remains untested.
- Why unresolved: Real-world applications may have thousands of features; the practical runtime and memory trade-offs against performance gains are unknown.
- What evidence would resolve it: Empirical scaling analysis on high-dimensional problems (e.g., visual RL with raw pixels) with runtime comparisons to Dyna and prioritized sweeping.

## Limitations

- The method's performance heavily depends on having informative, linearly-decodable features rather than being purely a benefit of the replay mechanism itself
- O(n²) complexity may become prohibitive for high-dimensional raw inputs without feature engineering
- The theoretical equivalence proof assumes the last layer is linear, which may not hold when deep networks are used as feature extractors
- The claim that λ̄ "specifies the density of the replay process" lacks full characterization of the relationship between λ̄ values and replay efficiency

## Confidence

- **High Confidence**: The algorithmic derivation and matrix update equations are mathematically sound; the forward and incremental forms are correctly related
- **Medium Confidence**: Empirical results show consistent improvement over baselines in both domains, but the specific gains depend heavily on hyperparameter tuning (α, λ, λ̄) and feature representation quality
- **Low Confidence**: The claim that λ̄ "specifies the density of the replay process" is somewhat vague—the paper shows this affects performance but doesn't fully characterize the relationship between λ̄ values and replay efficiency

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Replicate the myoelectric control experiments across a wider grid of (α, λ, λ̄) values to determine if performance improvements are robust or due to lucky tuning

2. **Feature Quality Ablation**: Compare TD-Replan(λ) performance using (a) raw sEMG features, (b) sparse AE features, and (c) random features to isolate the contribution of feature quality versus the replay mechanism

3. **Scalability Test**: Implement TD-Replan(λ) with increasing feature dimensions (e.g., 16, 64, 256, 1024) on the random walk task to empirically measure the O(n²) complexity impact and identify when the method becomes computationally prohibitive