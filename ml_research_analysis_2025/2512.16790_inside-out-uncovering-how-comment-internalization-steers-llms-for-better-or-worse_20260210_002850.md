---
ver: rpa2
title: 'Inside Out: Uncovering How Comment Internalization Steers LLMs for Better
  or Worse'
arxiv_id: '2512.16790'
source_url: https://arxiv.org/abs/2512.16790
tags:
- code
- concept
- comment
- llms
- comments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first interpretability analysis of how
  LLMs internalize code comments, revealing that comments are encoded as distinct
  latent concepts within model representations. Using Concept Activation Vectors (CAVs),
  we demonstrate that LLMs not only internalize comments but also differentiate between
  comment subtypes (Javadocs, inline, and multiline comments).
---

# Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse

## Quick Facts
- arXiv ID: 2512.16790
- Source URL: https://arxiv.org/abs/2512.16790
- Reference count: 40
- Primary result: First interpretability analysis showing LLMs encode comments as distinct latent concepts using CAVs, with task-dependent performance impacts of -90% to +67%

## Executive Summary
This study presents the first interpretability analysis of how LLMs internalize code comments, revealing that comments are encoded as distinct latent concepts within model representations. Using Concept Activation Vectors (CAVs), the authors demonstrate that LLMs not only internalize comments but also differentiate between comment subtypes (Javadocs, inline, and multiline comments). By systematically activating and deactivating these concepts in the embedding space, they observed task-dependent performance changes ranging from -90% to +67% across three SE tasks (code translation, completion, and refinement) and three LLM variants. Code summarization consistently triggered the strongest activation of comment concepts, while code completion elicited the weakest.

## Method Summary
The researchers employed Concept Activation Vectors (CAVs) to probe whether LLMs encode comments as distinct latent concepts. They collected code-comment pairs from three datasets, trained binary classifiers to distinguish comment-containing examples from non-comment ones, and used the resulting vectors to activate/deactivate comment concepts in the embedding space. The study tested three LLM variants (CodeT5-base, CodeT5-small, and DeepSeek-Coder-7B) across three software engineering tasks: code translation, code completion, and code refinement. Performance was measured before and after concept manipulation to quantify the impact of comment internalization.

## Key Results
- Comments are encoded as distinct latent concepts in LLM representations, confirmed through CAV analysis
- LLMs differentiate between comment subtypes (Javadocs, inline, multiline) with varying concept strengths
- Task-dependent performance changes of -90% to +67% observed when manipulating comment concept activations
- Code summarization consistently triggered strongest comment concept activation; code completion showed weakest activation

## Why This Works (Mechanism)
The mechanism works because LLMs develop internal representations that capture semantic relationships between code and its accompanying comments. When trained on code-comment pairs, the models learn to associate specific patterns and structures in comments with corresponding code semantics. CAVs can identify and manipulate these learned associations by finding directions in the high-dimensional embedding space that correspond to comment-related features. This allows researchers to activate or deactivate comment concepts independently of the input text, revealing how deeply comments are integrated into the model's reasoning process.

## Foundational Learning
- **Concept Activation Vectors (CAVs)**: Mathematical tools that identify directions in embedding space corresponding to specific concepts; needed to probe internal representations without modifying model architecture; quick check: verify CAVs produce consistent activations across multiple runs
- **Latent Concept Encoding**: Models store abstract representations of semantic features separate from surface text; needed to understand how models reason about domain-specific knowledge; quick check: test whether CAV directions transfer across similar tasks
- **Embedding Space Manipulation**: Ability to modify model representations by scaling along identified directions; needed to systematically study concept importance and dependencies; quick check: measure activation strength changes with different scaling factors
- **Binary Concept Classification**: Training classifiers to distinguish concept-present from concept-absent examples; needed to generate training data for CAV discovery; quick check: ensure classifier accuracy exceeds random chance significantly
- **Task-Specific Concept Activation**: Different tasks trigger varying levels of concept engagement; needed to understand when and how models leverage internalized knowledge; quick check: compare activation patterns across diverse task types
- **Model-Agnostic Concept Probing**: CAV methodology works across different model architectures; needed to validate findings beyond single model families; quick check: replicate CAV discovery on held-out models

## Architecture Onboarding

**Component Map:** Input Code/Comments → Embedding Layer → Hidden Layers → Output Layer

**Critical Path:** Comment features → Embedding representations → Latent concept space → Task-specific reasoning → Performance output

**Design Tradeoffs:** The study prioritizes interpretability and systematic manipulation over real-time application efficiency, accepting computational overhead to gain insights into internal model behavior.

**Failure Signatures:** Inconsistent CAV directions across runs, task-specific artifacts dominating concept signals, or performance changes that don't correlate with concept activation strength would indicate methodological issues.

**Three First Experiments:**
1. Apply CAV methodology to a completely different domain (e.g., natural language processing) to test generalizability
2. Create synthetic comments with controlled features to isolate which comment properties drive concept strength
3. Test whether fine-tuning on comment-rich data strengthens or weakens existing comment concept representations

## Open Questions the Paper Calls Out
None

## Limitations
- High confidence in CAV analysis but medium confidence in interpreting CAV directions as true semantic representations due to potential confounding factors
- Performance impact findings are highly task-dependent and may not generalize beyond tested SE tasks
- Observation about code summarization triggering strongest activation may reflect dataset biases rather than fundamental architectural differences

## Confidence
- **High confidence**: Systematic CAV analysis across multiple models and tasks
- **Medium confidence**: Interpretation of CAV directions as semantic representations
- **Medium confidence**: Task-dependent performance impact findings
- **High confidence**: Code summarization vs. completion activation differences

## Next Checks
1. Test CAV directions on held-out models and tasks to verify whether identified comment concepts transfer across architectures and code domains
2. Conduct ablation studies removing specific comment patterns (e.g., parameter descriptions, TODO markers) to isolate which comment features drive concept activation
3. Implement controlled experiments with synthetic comments of varying quality/consistency to measure how comment concept strength correlates with actual model performance changes