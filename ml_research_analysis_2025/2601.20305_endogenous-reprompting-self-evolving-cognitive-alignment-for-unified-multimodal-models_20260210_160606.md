---
ver: rpa2
title: 'Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal
  Models'
arxiv_id: '2601.20305'
source_url: https://arxiv.org/abs/2601.20305
tags:
- seer
- reasoning
- instruction
- visual
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SEER, a training framework designed to bridge
  the cognitive gap in unified multimodal models (UMMs), where strong understanding
  capabilities fail to effectively guide generation. SEER employs Endogenous Reprompting,
  transforming passive understanding into active generative reasoning by generating
  self-aligned descriptors during generation.
---

# Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models

## Quick Facts
- arXiv ID: 2601.20305
- Source URL: https://arxiv.org/abs/2601.20305
- Reference count: 29
- SEER achieves 0.92 evaluator accuracy and 0.61-0.90 human win ratios using only 300 samples from Visual Instruction Elaboration task.

## Executive Summary
SEER introduces Endogenous Reprompting, a training framework that transforms passive understanding into active generative reasoning in unified multimodal models. The approach uses a two-stage endogenous loop: RLVR activates latent evaluation ability via curriculum learning to produce high-fidelity endogenous rewards, then RLMT optimizes the generative reasoning policy using these rewards. Experiments show SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality while preserving general multimodal capabilities.

## Method Summary
SEER operates on a unified multimodal model with frozen generation parameters and trainable reasoning parameters. The framework first trains an evaluator through curriculum learning (RLVR) on pairwise comparisons between image generations, then uses this evaluator's verdicts as endogenous rewards to optimize a reprompting policy (RLMT) that discovers descriptors yielding superior visual outcomes. The entire process requires only 300 samples from a compact proxy task, Visual Instruction Elaboration, without sacrificing the model's general multimodal capabilities.

## Key Results
- SEER-Eval achieves 0.92 overall accuracy on instruction-grounded image comparisons versus 0.41 for zero-shot baseline
- Human evaluation shows 0.61-0.90 win ratios for SEER-generated reprompts over naive prompt engineering
- Maintains strong performance on general multimodal benchmarks while specializing in instruction alignment

## Why This Works (Mechanism)

### Mechanism 1: Endogenous Reprompting via Shared Representation Space
SEER enables model-specific alignment by generating descriptors within the same latent space used for generation, avoiding representation mismatch. The unified architecture ensures reprompts are constrained to the generator's realizable content space, with the shared evaluator naturally penalizing out-of-distribution reprompts based on its inherent knowledge of the generator's failure modes.

### Mechanism 2: RLVR Activates Latent Evaluation via Curriculum Learning
A two-phase curriculum (basic alignment → instruction supervision) transforms passive multimodal understanding into active pairwise evaluation capability. Stage 1 establishes decision boundaries through simple image-text alignment, while Stage 2 trains on visual instruction quadruplets to learn instruction-compliant evaluation, with GRPO optimizing the evaluator trajectory using verifiable binary rewards.

### Mechanism 3: RLMT Optimizes Generative Reasoning via Endogenous Rewards
The RLVR-tuned evaluator provides a reliable endogenous reward signal that guides the reprompting policy to discover descriptors yielding superior visual outcomes. The policy generates candidate reprompts, produces images for comparison against baselines, and uses the evaluator's binary verdict as reward to optimize reasoning through GRPO while regularizing toward reference policy.

## Foundational Learning

- **Reinforcement Learning with Human Feedback (RLHF)**
  - Why needed here: SEER builds on RLHF principles but diverges by optimizing reasoning (prompts) rather than execution (pixels)
  - Quick check question: Can you explain why maximizing E[R(i,o)] differs from maximizing E[R(p; a, p₀)] with frozen generator?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The core optimization algorithm for both RLVR and RLMT stages; token-level importance sampling with group-normalized advantages
  - Quick check question: How does GRPO's advantage computation (r_i - mean/std within group) differ from traditional PPO's advantage estimation?

- **Unified Multimodal Model Architecture**
  - Why needed here: SEER exploits the shared representation space between understanding (θ) and generation (ϕ) components
  - Quick check question: What architectural property enables "endogenous" evaluation (hint: why can't this work with disjoint LLM + generator pipelines)?

## Architecture Onboarding

- **Component map:**
  Unified Backbone (θ + ϕ) → Understanding/Reasoning Parameters (θ) → Evaluator E(·|θ): X² × T → {Yes, No}, Reprompting Policy π_θ(·|θ): P × A → Δ(P); Generation Parameters (ϕ) → Generator G(·|ϕ): P → X

- **Critical path:**
  1. Construct 300-sample Visual Instruction Elaboration dataset across 6 categories
  2. RLVR Stage: Curriculum training → Basic alignment → Instruction supervision
  3. RLMT Stage: Sample reprompts → Generate images → Evaluate vs. baseline → Compute rewards → GRPO update

- **Design tradeoffs:**
  - Frozen generator ensures improvements come from reasoning optimization, not low-level rendering
  - Pairwise comparison mitigates reward hacking compared to external methods
  - 300 samples only: high-leverage proxy task vs. comprehensive coverage

- **Failure signatures:**
  - Type I (Laziness): Naive concatenation of instruction to prompt without semantic integration
  - Type II (Hallucination): Misinterpreting instructions (e.g., "frozen in time" → "covered in ice")
  - Type III (Omission): Ignoring instruction entirely while preserving subject

- **First 3 experiments:**
  1. **Sanity check:** Verify zero-shot evaluator accuracy on held-out quadruplets (expect ~0.4-0.5 if cognitive gap exists)
  2. **Ablation:** Train RLVR only (skip RLMT) and compare human win ratio against full SEER pipeline on Hard instructions
  3. **Generalization test:** Evaluate reprompting policy on OOD concepts not in 300-sample training set; if win ratio drops significantly, curriculum or data diversity is insufficient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does jointly optimizing the visual generator (ϕ) alongside the reasoning policy (θ) improve both instruction alignment and fundamental generation quality?
- Basis in paper: Appendix A.5 states a future direction is "extending the framework to jointly optimize the visual generator, enabling iterative self-evolution"
- Why unresolved: Current SEER freezes generation parameters to isolate improvements to the reasoning pathway; the interaction between joint optimization and the endogenous loop remains unexplored
- What evidence would resolve it: Comparing SEER against a variant where ϕ is also fine-tuned, measuring GenEval/DPG-Bench scores and Visual Instruction Elaboration accuracy

### Open Question 2
- Question: Does iterative multi-turn reprompting outperform single-shot reprompting for instructions with multiple conflicting constraints?
- Basis in paper: Appendix A.5 identifies "exploring iterative multi-turn reprompting for complex constraints" as future work
- Why unresolved: The current framework generates one reprompt per instruction; whether sequential refinement could better resolve paradoxical constraints is unknown
- What evidence would resolve it: Implementing a multi-turn variant and measuring win rates on a curated set of multi-constraint instructions

### Open Question 3
- Question: How well does SEER generalize across diverse UMM architectures and parameter scales beyond the 1.5B Harmon model?
- Basis in paper: Appendix A.5 notes the need to "verify the universality of SEER by applying it to diverse UMM architectures and larger scales"
- Why unresolved: Experiments only cover Harmon (1.5B); whether the approach transfers to autoregressive, hybrid, or MoT paradigms with different parameter counts is untested
- What evidence would resolve it: Applying SEER to at least two additional UMM families at varying scales and reporting evaluator accuracy and generation quality

## Limitations
- **Architecture Dependency:** SEER's success hinges on the assumption that understanding and generation parameters are sufficiently coupled in the base UMM
- **Reward Fidelity:** The paper reports high evaluator accuracy but does not validate that the endogenous reward signal generalizes to truly unseen instructions
- **Data Efficiency Threshold:** While SEER claims strong results with 300 samples, the exact data efficiency threshold and sensitivity to distributional shift are unclear

## Confidence
- **High Confidence:** The core mechanism of endogenous reprompting is well-grounded and supported by architectural details
- **Medium Confidence:** The two-stage curriculum learning approach is plausible but relies on unverified assumptions about reward fidelity and generalization
- **Low Confidence:** Claims about data efficiency and preservation of general multimodal capabilities lack extensive experimental validation

## Next Checks
1. **Reward Generalization Test:** Evaluate SEER-Eval on held-out instructions with novel concepts to verify endogenous reward signal generalizes beyond training distribution
2. **Architecture Ablation:** Replace Harmon with disjoint LLM + generator pipeline and retrain SEER to validate shared representation assumption
3. **Data Efficiency Stress Test:** Train SEER with varying dataset sizes (100, 300, 1000 samples) and measure win ratio and evaluator accuracy to identify minimum viable dataset size