---
ver: rpa2
title: 'HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions
  Across Single and Multiple Devices'
arxiv_id: '2505.19628'
source_url: https://arxiv.org/abs/2505.19628
tags:
- instructions
- devices
- device
- room
- home
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HomeBench, the first comprehensive dataset
  for evaluating LLMs in smart home scenarios, including both valid and invalid instructions
  across single and multiple devices. The dataset contains over 170,000 instructions
  across 100 virtual home scenarios with 47+ devices each.
---

# HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices

## Quick Facts
- arXiv ID: 2505.19628
- Source URL: https://arxiv.org/abs/2505.19628
- Reference count: 40
- Key outcome: Introduces first comprehensive dataset for LLM evaluation in smart homes, revealing significant performance gaps particularly on invalid instructions and multi-device operations.

## Executive Summary
This paper introduces HomeBench, the first comprehensive dataset for evaluating LLMs in smart home scenarios, including both valid and invalid instructions across single and multiple devices. The dataset contains over 170,000 instructions across 100 virtual home scenarios with 47+ devices each. Experiments with 13 LLMs reveal significant performance gaps, particularly on invalid instructions and multi-device operations, with even state-of-the-art models like GPT-4o achieving only 0.0% success on invalid multi-device tasks. The study demonstrates that while in-context learning and fine-tuning provide improvements, they still fall short of real-world application requirements, highlighting the need for further research in this domain.

## Method Summary
The task requires mapping natural language user instructions to executable API calls (`room.device.method(args)`) or `error_input` within a virtual smart home environment. Models must handle valid/invalid instructions and single/multi-device operations. The HomeBench dataset contains 170k+ instructions across 100 virtual home scenarios with device states and callable methods. Evaluation uses Success Rate (binary) and F1 Score (partial credit) metrics. Methods include 0-shot/4-shot In-Context Learning with embedded device states/methods, LoRA fine-tuning (r=16, alpha=32) on Qwen2.5-7B for 2 epochs, and RAG-based context retrieval.

## Key Results
- GPT-4o achieves only 0.0% success rate on invalid multi-device instructions even with in-context learning
- Fine-tuned Qwen2.5-7B outperforms GPT-4o-ICL across all tasks but VM/MM remain below 70% success
- RAG retrieval improves invalid instruction handling but degrades valid instruction performance
- Unfaithfulness errors (27.93%) occur when models refuse valid operations due to state misinterpretation
- In-context attention errors (43.39%) involve missing current state values or operating wrong devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with type-diverse examples improves instruction handling by providing pattern recognition templates for valid, invalid, and multi-device operations.
- Mechanism: Few-shot demonstrations expose models to output format `error_input` for invalid instructions and multi-method API call sequences for complex tasks, reducing unfaithfulness errors by establishing decision boundaries.
- Core assumption: Models can transfer reasoning patterns from examples to novel device configurations without weight updates.
- Evidence anchors:
  - [abstract] "even with the help of in-context learning... state-of-the-art LLMs still cannot perform well"
  - [section 4.3] "Gemma2-9b model saw its IS task success rate jump from 0% to 21.67%"
  - [corpus] Related work on LLM-based smart home automation confirms ICL efficacy for personalization but notes persistent gaps in complex scenarios.
- Break condition: Performance degrades when shot examples lack device state information, causing confusion in VS/IS tasks per Section 5.1 analysis.

### Mechanism 2
- Claim: Long-context attention over device states and callable methods determines validity detection accuracy, particularly for identifying non-existent devices or unavailable functions.
- Mechanism: Models must compare user instructions against complete home state dictionary (room → device → state/methods) to determine if operations are executable or should return `error_input`.
- Core assumption: Models can maintain accurate representations of 3k+ token contexts with multiple device states.
- Evidence anchors:
  - [section 5.3] "In-Context Attention Error" accounts for 43.39% of errors; models "did not consider the status and methods of all devices comprehensively"
  - [table 4] RAG retrieval errors show 25-53% of contexts lack useful state/method information
  - [corpus] Limited corpus evidence on attention mechanisms in smart home contexts; related work focuses on privacy and personalization rather than validity detection.
- Break condition: Retrieval-augmented approaches fail when embedding similarity doesn't capture task-relevant device information (Section 5.2).

### Mechanism 3
- Claim: Fine-tuning on domain-specific instruction-API mappings improves performance but doesn't resolve fundamental reasoning gaps in multi-device orchestration.
- Mechanism: LoRA-based adaptation learns device-operation associations from 138k training examples, improving VS/IS tasks above 80% but VM/MM remain below 70% due to persistent attention errors.
- Core assumption: Task-specific weight updates can encode device API schemas without exhaustive coverage of all device combinations.
- Evidence anchors:
  - [section 5.4] Fine-tuned Qwen2.5-7B "outperformed GPT-4o-ICL in all tasks" but "SUCC scores for VM and MM remained below 70%"
  - [figure 5] Training curves plateau after 1 epoch with persistent attention and unfaithfulness errors
  - [corpus] Weak corpus evidence; related papers focus on privacy and small language models rather than fine-tuning for multi-device operations.
- Break condition: Out-of-distribution generalization is maintained (OOD≈test performance), but core multi-device reasoning bottlenecks persist.

## Foundational Learning

- Concept: API function calling with structured parameters
  - Why needed here: The task requires generating precise API calls like `master_bedroom.light.set_brightness(50)` from natural language, understanding parameter types (int, str, tuple), and recognizing when methods don't exist.
  - Quick check question: Given device state `air_conditioner: mode: [heat, cool, fan_only]`, what should the model output for "Set the air conditioner to dry mode"?

- Concept: Context window management and attention patterns
  - Why needed here: Models must process 3k+ tokens of device states across 12 rooms with 47+ devices each, attending to relevant information while ignoring irrelevant device states.
  - Quick check question: If a user asks to "turn on the bedroom light," which context spans must the model attend to versus safely ignore?

- Concept: Error detection and rejection strategies
  - Why needed here: Unlike standard instruction following, this task requires recognizing when user requests reference non-existent devices, unavailable methods, or invalid parameter values.
  - Quick check question: For a home without a garage, what is the correct response to "Open the garage door to 50%"?

## Architecture Onboarding

- Component map:
  - Virtual home environment -> Python classes representing 15 device types with state attributes and callable methods
  - Prompt constructor -> Embeds `<home_state>` (device status) and `<device_method>` (available APIs) with user instruction
  - LLM inference engine -> Generates `room.device.method(params)` sequences or `error_input`
  - Evaluation harness -> Compares generated API calls against ground truth using Success Rate and F1 metrics

- Critical path:
  1. Parse home configuration -> build state dictionary (3k+ tokens)
  2. Construct prompt with state, methods, and few-shot examples (4-shot optimal per experiments)
  3. Generate API sequence -> validate format -> execute in virtual environment
  4. Compare against gold labels -> compute Succ/F1 metrics

- Design tradeoffs:
  - Full context (3k tokens) vs. RAG: Full context maintains information completeness but increases latency; RAG improves invalid instruction detection but degrades valid instruction accuracy (Table 4)
  - Prompt-only vs. fine-tuning: Fine-tuning achieves better performance but requires training infrastructure and dataset curation
  - Success rate vs. F1: Succ requires perfect execution (stricter for safety-critical scenarios); F1 allows partial credit for multi-device operations

- Failure signatures:
  - Unfaithfulness (27.93%): Model refuses valid operations when device state is off/inactive
  - In-context attention error (43.39%): Model operates wrong device or misses current state values (e.g., relative adjustments without checking current brightness)
  - Key error (34.39%): Format violations preventing automated evaluation

- First 3 experiments:
  1. Baseline evaluation with 4-shot ICL across 5 instruction types (VS, IS, VM, IM, MM) using provided prompts (Table 8) to establish performance bounds.
  2. Error analysis on 100 sampled failures to classify unfaithfulness vs. attention vs. format errors, guiding prompt refinement priorities.
  3. Fine-tuning experiment with LoRA (r=16, α=32) on Qwen2.5-7B for 1 epoch, comparing against ICL baselines on held-out test set and OOD devices (beds, pet feeders).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to reliably detect and reject invalid multi-device instructions (IM tasks), where even state-of-the-art models like GPT-4o achieve 0.0% success without in-context learning?
- Basis in paper: [explicit] The authors report "GPT-4o achieves only a 0.0% success rate in the scenario of invalid multi-device instructions" and note this persists "even with the help of in-context learning, retrieval-augmented generation, and fine-tuning."
- Why unresolved: The error analysis identifies "In-Context Attention Error" as a major bottleneck—models fail to comprehensively track device states and methods to identify non-existent operations—but fine-tuning did not fundamentally resolve this.
- What evidence would resolve it: A model architecture or training approach that achieves >80% success on IM tasks, potentially through explicit state-tracking mechanisms or hierarchical reasoning.

### Open Question 2
- Question: Why does retrieval-augmented generation (RAG) degrade performance on valid instructions while improving invalid instruction handling, and how can context retrieval be optimized for this task?
- Basis in paper: [explicit] The RAG experiments showed "anomalous" results where "tasks that the model was originally proficient in showed a decline in performance, while tasks it was less proficient exhibited significant improvement."
- Why unresolved: The authors found "the retrieved rooms contained considerable errors" due to insufficient embedding similarity between device states/methods and user instructions, but did not propose a solution.
- What evidence would resolve it: A specialized retrieval model trained on smart home context alignment that improves both valid and invalid instruction performance simultaneously.

### Open Question 3
- Question: Can the performance gains from fine-tuning transfer to real-world smart home environments with physical devices, given that HomeBench uses a simplified virtual environment?
- Basis in paper: [inferred] The paper acknowledges using a virtual environment that "simplified the methods for some devices, retaining only the essential smart home functions" and did not account for "differences between devices of various brands." Additionally, the "mechanical delays inherent to physical devices" were eliminated.
- Why unresolved: While OOD testing showed some robustness to new device types, the gap between virtual API calls and real-world device interactions (latency, failure modes, brand-specific APIs) remains unexplored.
- What evidence would resolve it: Evaluation of fine-tuned models on physical smart home testbeds with real devices across multiple brands.

## Limitations
- Dataset relies entirely on template-based generation, potentially missing real user instruction complexity and ambiguity
- Virtual environment uses fixed set of 15 device types, missing less common commercially available smart home devices
- Binary evaluation metrics (Succ and F1) may not capture nuanced performance differences in partial execution scenarios

## Confidence
- High Confidence: Dataset construction methodology is clearly specified with 100 virtual home scenarios and explicit splitting strategy; performance gaps between instruction types are consistently observed across multiple LLM models
- Medium Confidence: Effectiveness of in-context learning improvements is demonstrated but may be sensitive to prompt engineering choices; LoRA fine-tuning results show clear performance gains but may not generalize to other model architectures
- Low Confidence: RAG-based approach results are limited by reported retrieval failures (25-53% of contexts lack useful information); long-context attention analysis is based on error sampling rather than systematic measurement

## Next Checks
1. Collect and evaluate a small dataset of actual user instructions from deployed smart home systems to assess the gap between synthetic and real-world performance, particularly for invalid instruction detection
2. Systematically evaluate model performance on device combinations not seen during training (beyond current OOD set) to better understand generalization limits for multi-device orchestration tasks
3. Implement targeted experiments using attention visualization tools to quantify how well models attend to relevant device states versus irrelevant information across different instruction types and context lengths