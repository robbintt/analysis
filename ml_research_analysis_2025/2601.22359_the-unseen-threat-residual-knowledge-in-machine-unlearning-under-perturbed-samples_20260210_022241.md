---
ver: rpa2
title: 'The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed
  Samples'
arxiv_id: '2601.22359'
source_url: https://arxiv.org/abs/2601.22359
tags:
- unlearning
- knowledge
- residual
- samples
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies residual knowledge as a novel privacy vulnerability
  in machine unlearning, where unlearned models can still correctly classify small
  adversarial perturbations of forget samples, while re-trained models fail to do
  so. This reveals that unlearning may not fully erase information from the local
  neighborhood of forget samples.
---

# The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples

## Quick Facts
- **arXiv ID:** 2601.22359
- **Source URL:** https://arxiv.org/abs/2601.22359
- **Reference count:** 40
- **Primary result:** Residual knowledge vulnerability identified in machine unlearning where perturbed forget samples remain correctly classified

## Executive Summary
This paper identifies a novel privacy vulnerability in machine unlearning called "residual knowledge" - where unlearned models can still correctly classify small adversarial perturbations of forget samples, while re-trained models fail to do so. This reveals that unlearning may not fully erase information from the local neighborhood of forget samples. To address this, the authors propose RURK, a fine-tuning strategy that penalizes the model's ability to re-recognize perturbed forget samples. Empirical results on CIFAR-10 and ImageNet-100 demonstrate that residual knowledge is prevalent across existing unlearning methods, and that RURK effectively reduces it while maintaining competitive accuracy.

## Method Summary
The authors introduce RURK (Residual Knowledge Unlearning via Representation Knowledge), a fine-tuning approach that addresses the residual knowledge vulnerability by incorporating a regularization term that penalizes the model's ability to correctly classify perturbed forget samples. RURK works by adding a penalty to the unlearning objective that specifically targets the model's tendency to retain information about forget samples through their adversarial neighbors. The method is evaluated against multiple existing unlearning baselines across CIFAR-10 and ImageNet-100 datasets, measuring both accuracy retention and residual knowledge suppression through perturbation-based metrics.

## Key Results
- Residual knowledge is prevalent across existing unlearning methods, where unlearned models correctly classify perturbed forget samples while re-trained models fail
- RURK achieves the smallest average gap from re-trained models in terms of accuracy on perturbed forget samples
- RURK successfully suppresses residual knowledge across various perturbation norms (L-infinity, L2) while maintaining competitive accuracy on clean data

## Why This Works (Mechanism)
Residual knowledge occurs because unlearning methods focus on removing specific training samples but may leave traces of information in the model's decision boundaries, particularly in the local neighborhood around forget samples. When small perturbations are applied to forget samples, these traces become visible as the model still classifies the perturbed samples correctly. RURK works by explicitly penalizing this behavior during the fine-tuning process, forcing the model to "forget" not just the original samples but also their adversarial neighbors.

## Foundational Learning
- **Machine Unlearning**: The process of removing the influence of specific training samples from a trained model without complete retraining - needed to understand the context of privacy preservation and why partial unlearning methods might leave residual information
- **Adversarial Perturbations**: Small, carefully crafted modifications to inputs that can cause misclassification - needed to understand how residual knowledge is detected through perturbed versions of forget samples
- **Fine-tuning with Regularization**: Adjusting a pre-trained model with additional penalty terms - needed to understand how RURK modifies the unlearning objective to suppress residual knowledge
- **Privacy Leakage in ML**: Various ways models can inadvertently reveal information about training data - needed to contextualize residual knowledge as a specific type of privacy vulnerability
- **Representation Learning**: How models encode and organize information about training samples - needed to understand why forget samples might leave traces in model representations

## Architecture Onboarding
- **Component Map:** Forget samples -> Unlearning method -> Fine-tuning with RURK -> Evaluated on perturbed forget samples
- **Critical Path:** Original model → Unlearning process → RURK fine-tuning → Residual knowledge evaluation
- **Design Tradeoffs:** Balancing between complete removal of forget samples (privacy) and maintaining overall model utility; choosing perturbation magnitude that reveals residual knowledge without being trivial
- **Failure Signatures:** If RURK over-suppresses, model accuracy on clean data degrades significantly; if under-suppressed, perturbed forget samples remain correctly classified
- **First Experiments:** 1) Baseline unlearning without RURK to establish residual knowledge presence, 2) RURK with varying regularization strengths, 3) Comparison against re-trained baseline across perturbation norms

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The definition of residual knowledge focuses narrowly on perturbed samples, potentially missing other forms of persistent information
- Experimental validation is limited to CIFAR-10 and ImageNet-100 datasets, which may not generalize to other domains or complex data types
- The trade-off between suppressing residual knowledge and maintaining model utility needs more thorough characterization across different perturbation norms and application contexts

## Confidence
- **High confidence:** Empirical demonstration that unlearning methods leave residual knowledge detectable through perturbed samples
- **Medium confidence:** Effectiveness of RURK as a solution given promising but limited experimental scope
- **Medium confidence:** Generalizability of findings to other unlearning contexts and datasets

## Next Checks
1. Test RURK's effectiveness on additional datasets beyond CIFAR-10 and ImageNet-100, including text and medical imaging data, to assess generalizability across domains.

2. Evaluate the method against adaptive attacks that specifically target the residual knowledge detection mechanism to test robustness.

3. Conduct ablation studies varying the perturbation magnitude and norm to better understand the boundary conditions where residual knowledge becomes detectable or suppressed.