---
ver: rpa2
title: LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation
arxiv_id: '2505.12031'
source_url: https://arxiv.org/abs/2505.12031
tags:
- search
- proof
- tree
- beam
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel proof-state exploration approach for
  generating synthetic data to fine-tune LLMs as policy models in automated theorem
  proving (ATP). The method uses a curated set of common tactics and a constrained
  decoding strategy to generate diverse proof states from seed problems, producing
  large-scale synthetic training data without requiring Expert Iteration.
---

# LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation

## Quick Facts
- arXiv ID: 2505.12031
- Source URL: https://arxiv.org/abs/2505.12031
- Reference count: 40
- Primary result: Achieves 60.74% and 21.18% pass rates on MiniF2F and ProofNet benchmarks using synthetic data generation without Expert Iteration

## Executive Summary
This paper addresses a critical bottleneck in LLM-based automated theorem proving: the need for large-scale, high-quality training data. The authors propose a novel proof-state exploration approach that generates synthetic data for fine-tuning LLMs as policy models in automated theorem proving. By leveraging a curated set of common tactics and a constrained decoding strategy, the method produces diverse proof states from seed problems, enabling scalable synthetic data generation without requiring the computationally expensive Expert Iteration process. The approach is validated on MiniF2F and ProofNet benchmarks, demonstrating state-of-the-art performance among tree search approaches while maintaining computational efficiency.

## Method Summary
The method introduces a proof-state exploration framework that generates synthetic training data for fine-tuning LLMs as policy models in automated theorem proving. The approach uses a curated set of common tactics and constrained decoding to explore diverse proof states from seed problems. An adaptive beam size strategy is employed during tree search, starting with larger beam sizes for broad exploration and gradually reducing them to focus on promising paths. This generates large-scale synthetic data without requiring Expert Iteration. The policy model is fine-tuned on this synthetic data and evaluated on theorem proving benchmarks, achieving competitive performance while maintaining computational efficiency compared to existing approaches.

## Key Results
- Achieves 60.74% average pass rate on MiniF2F benchmark under Pass@1 constraints
- Achieves 21.18% average pass rate on ProofNet benchmark under Pass@1 constraints
- Demonstrates state-of-the-art performance among tree search approaches while maintaining computational efficiency

## Why This Works (Mechanism)
The method works by generating diverse synthetic proof states through constrained decoding, which captures a broader range of proof strategies than traditional expert-driven approaches. The adaptive beam size strategy enables effective exploration-exploitation trade-offs during search, starting broad and narrowing focus as promising paths emerge. This synthetic data generation process creates high-quality training examples that improve the policy model's ability to select appropriate tactics in novel proof scenarios, without the computational overhead of Expert Iteration.

## Foundational Learning
- **Proof State Representation**: Mathematical proofs represented as sequences of tactics and states; needed to enable LLM reasoning about proof steps; quick check: can model reconstruct intermediate proof states
- **Tree Search in Proof Space**: Systematic exploration of possible proof paths; needed to discover diverse proof strategies; quick check: does search find multiple valid proof paths
- **Constrained Decoding**: Restricting LLM output to valid tactics; needed to maintain proof validity during generation; quick check: are generated proof states logically consistent
- **Beam Search Optimization**: Multi-path exploration with adaptive beam sizing; needed to balance exploration and exploitation; quick check: does adaptive sizing improve proof success rates
- **Synthetic Data Generation**: Creating training examples from exploration; needed to scale training data without manual annotation; quick check: does synthetic data improve policy model performance
- **Policy Model Fine-tuning**: Adapting LLMs to theorem proving tasks; needed to specialize general LLMs for ATP; quick check: does fine-tuning improve proof success rates

## Architecture Onboarding

**Component Map**: Seed Problems -> Proof-State Exploration -> Synthetic Data Generation -> Policy Model Fine-tuning -> ATP Evaluation

**Critical Path**: The proof-state exploration component is critical as it directly generates the synthetic training data that determines policy model quality and downstream ATP performance.

**Design Tradeoffs**: The method trades the high-quality but expensive Expert Iteration for scalable synthetic data generation, accepting some potential quality reduction for significantly improved efficiency and scalability.

**Failure Signatures**: Poor synthetic data diversity indicates constrained decoding may be too restrictive; low policy model performance suggests inadequate fine-tuning or poor quality synthetic data; computational inefficiency may indicate suboptimal beam size adaptation.

**First 3 Experiments**:
1. Evaluate proof success rates on MiniF2F with varying beam sizes to assess adaptive strategy effectiveness
2. Compare synthetic data diversity metrics with and without constrained decoding
3. Measure computational cost per proof attempt versus Expert Iteration baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on quality and coverage of curated tactic set, potentially limiting applicability to untested mathematical domains
- Synthetic data generation depends on assumption that constrained decoding captures sufficient diversity for robust policy learning
- Performance gains compared to Expert Iteration may not scale linearly with problem complexity or more expressive formal systems

## Confidence
**High**: Scalable synthetic data generation with constrained decoding for policy model training is well-supported by experimental results; adaptive beam size strategy effectiveness demonstrated empirically

**Medium**: Generalizability to other formal systems and mathematical domains beyond tested benchmarks remains uncertain; long-term impact of synthetic data quality on model generalization not fully characterized

**Low**: Relative performance gains compared to Expert Iteration across varying computational budgets and problem complexities not extensively explored

## Next Checks
1. Evaluate approach on additional theorem proving benchmarks beyond MiniF2F and ProofNet to assess generalizability across different mathematical domains
2. Conduct ablation studies comparing synthetic data generation quality with and without constrained decoding strategy
3. Test adaptive beam size strategy across different tree search depths and problem types to determine robustness and identify optimal parameter settings