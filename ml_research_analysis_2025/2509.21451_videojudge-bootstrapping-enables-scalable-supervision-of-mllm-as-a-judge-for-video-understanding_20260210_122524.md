---
ver: rpa2
title: 'VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge
  for Video Understanding'
arxiv_id: '2509.21451'
source_url: https://arxiv.org/abs/2509.21451
tags:
- video
- response
- rating
- instruction
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VideoJudge is a bootstrapped framework that trains small MLLMs\
  \ to evaluate video understanding outputs. It uses an iterative generator-evaluator\
  \ pipeline to synthesize training data across 1\u20135 ratings without human annotation,\
  \ and fine-tunes models to generate both ratings and instance-specific rubrics."
---

# VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding

## Quick Facts
- arXiv ID: 2509.21451
- Source URL: https://arxiv.org/abs/2509.21451
- Reference count: 40
- Key outcome: VideoJudge is a bootstrapped framework that trains small MLLMs to evaluate video understanding outputs, matching or outperforming much larger models (e.g., Qwen2.5-VL-32B/72B) on meta-evaluation benchmarks.

## Executive Summary
VideoJudge introduces a bootstrapped framework that enables scalable supervision of small multimodal large language models (MLLMs) for video understanding evaluation. The core innovation is an iterative generator-evaluator pipeline that synthesizes training data without human annotation, producing models that can both rate responses on a 1-5 scale and generate instance-specific rubrics. VideoJudge-7B matches or exceeds the performance of models up to 10× larger, while VideoJudge-3B achieves comparable results to much larger baselines. The approach is robust to sampling temperature and improves temporal reasoning on long-form video tasks.

## Method Summary
The framework uses a bootstrapping pipeline where a generator creates candidate responses for each rating (1-5) given video, instruction, and gold response. An evaluator scores these responses, and mismatches trigger feedback-guided regeneration until ratings align or iteration limits are reached. This process produces ~104K video-instruction-response-rating tuples with reasoning traces. Small MLLMs (3B/7B) are then fine-tuned on this data using both pointwise and pairwise objectives. The resulting models, VideoJudge-3B/7B, can evaluate video understanding outputs while generating context-specific rubrics. Training uses 60 frames, evaluation uses 180 frames, and special tokens (<thinking>, <score>) format the data.

## Key Results
- VideoJudge-7B matches or outperforms much larger baselines (e.g., Qwen2.5-VL-32B/72B) on meta-evaluation benchmarks
- VideoJudge-3B achieves comparable performance to models up to 10× its size
- Human evaluation confirms high agreement (>94%) and rubric quality preferred by both annotators and LLM judges
- The approach is robust to sampling temperature and improves temporal reasoning on long-form video tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative generator-evaluator refinement produces higher-quality training data than single-pass generation.
- Mechanism: The generator produces candidate responses for each target rating (1-5); the evaluator scores them; mismatches trigger feedback-guided regeneration until ratings align or iteration limits are reached. This filters poorly-aligned responses and incrementally improves data quality.
- Core assumption: The evaluator's ratings serve as a reliable proxy for human quality judgments within the targeted rating scale.
- Evidence anchors:
  - [abstract] "the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded"
  - [section 3.1] Algorithm 1 formalizes the refinement loop with acceptance criterion |r - r̂| ≤ α
  - [corpus] Weak direct corpus support; related work (Judge Anything, UniME-V2) focuses on MLLM-as-judge without the iterative refinement aspect
- Break condition: If evaluator systematically mis-rates responses (e.g., high error on rating 2 vs 3 boundaries), bootstrapped data will propagate errors rather than correct them.

### Mechanism 2
- Claim: Training models to generate instance-specific rubrics improves evaluation accuracy and interpretability.
- Mechanism: The model learns to produce a 1-5 rubric tailored to each video-instruction pair before scoring. This grounds evaluation in explicit, context-specific criteria rather than generic standards.
- Core assumption: Generated rubrics meaningfully constrain the scoring process and do not introduce additional noise.
- Evidence anchors:
  - [abstract] "fine-tunes models to generate both ratings and instance-specific rubrics"
  - [section 6.1, Table 2] VideoJudgeR-3B achieves correlation >73 with rubric generation, comparable to 32B/72B baselines
  - [corpus] No direct corpus comparison; related MLLM-judge work does not emphasize rubric generation
- Break condition: If rubrics become generic across instances or fail to capture task-relevant distinctions, they add computational overhead without accuracy gains.

### Mechanism 3
- Claim: Small MLLMs (3B-7B) fine-tuned on bootstrapped supervision can match or exceed the judgment quality of 10x larger models.
- Mechanism: The bootstrapped dataset (~104K examples) provides dense, fine-grained supervision (5-point ratings with reasoning). Specialized training on this data transfers evaluator capabilities more efficiently than scaling parameters.
- Core assumption: The evaluation task distribution is sufficiently covered by the bootstrapped data; generalization to out-of-distribution video tasks is not guaranteed.
- Evidence anchors:
  - [abstract] "VideoJudge-7B matches or outperforms much larger baselines (e.g., Qwen2.5-VL-32B/72B)"
  - [section 6.1, Table 1] VideoJudge-7B achieves Spearman 0.78-0.82 vs Qwen2.5-VL-72B's 0.80-0.81 across benchmarks
  - [corpus] Related work (Temporal Preference Optimization, CAREVL) explores preference-based training for video models but without the judge-specialization focus
- Break condition: If evaluation requires reasoning patterns not present in the bootstrapped data (e.g., novel video domains, rare temporal structures), small models may underperform larger baselines.

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm** (using language models to evaluate model outputs)
  - Why needed here: VideoJudge extends this to multimodal video understanding; understanding the base paradigm is prerequisite.
  - Quick check question: Can you explain the difference between pointwise (single-response scoring) and pairwise (comparison) evaluation?

- Concept: **Self-training / bootstrapping from weak supervision**
  - Why needed here: The entire framework relies on generator-evaluator iteration without human labels; understanding feedback loops is essential.
  - Quick check question: What failure mode occurs if the evaluator has systematic bias in a specific rating range?

- Concept: **Video MLLM architectures and temporal reasoning**
  - Why needed here: VideoJudge processes video frames directly; understanding frame sampling and temporal context is critical for reproduction.
  - Quick check question: Why does the paper find that providing video inputs (vs. text descriptions) improves evaluation performance?

## Architecture Onboarding

- Component map:
  - Generator -> Evaluator -> Refinement Loop -> Training Data -> Judge Models (VideoJudge-3B/7B)

- Critical path:
  1. Seed data curation (25K triplets from VideoInstruct-100K, VCG-Plus-112K, VideoChat2-IT)
  2. Initial generation of degraded responses (ratings 1-4)
  3. Evaluation and filtering via acceptance criterion |r - r̂| ≤ α
  4. Feedback-guided regeneration for rejected candidates
  5. Dataset construction (retain only examples with all 5 ratings)
  6. Fine-tuning on pointwise and pairwise objectives
  7. Meta-evaluation on VideoJudgeLLaVA, VideoJudgeVCG, VATEX, LongVideoBench

- Design tradeoffs:
  - Frame count: Training with 60 frames, evaluation with 180. Paper shows ~120-240 frames optimal; more adds cost without proportional gain.
  - Threshold α: Lower values increase data quality but reduce yield; paper uses α=0 for strict alignment.
  - Pairwise sampling: 50% of all possible pairs used due to computational limits; full sampling may improve pairwise models.

- Failure signatures:
  - Evaluator-generator agreement collapses for rating 2 vs 3 (paper notes highest disagreement here)
  - Temperature sensitivity: Base models degrade at high temperature; VideoJudge remains stable (Figure 4)
  - Position bias in pairwise evaluation (mitigated by randomization)
  - LongVideoBench performance drops for non-VideoJudge models, indicating temporal reasoning gaps

- First 3 experiments:
  1. **Reproduce bootstrapping on a small subset** (e.g., 1K seed examples). Verify monotonic BERTScore/BLEU degradation across ratings as in Figure 2.
  2. **Ablate max_frames during evaluation** (30, 60, 120, 180). Confirm performance saturates around 120 frames per Figure 18.
  3. **Compare pointwise vs pairwise variants on VideoJudge-Pairwise-H** (challenging 2-vs-3 cases). Assess whether pairwise training helps on subtle distinctions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the bootstrapping generator-evaluator pipeline be successfully generalized to other modalities beyond video understanding?
- Basis in paper: [explicit] The conclusion states, "The bootstrapping methodology is general and could extend to other modalities beyond video understanding."
- Why unresolved: The current study strictly validates the framework on video understanding tasks, leaving cross-modal applicability untested.
- What evidence would resolve it: Successful application of the VideoJudge bootstrapping recipe to audio-only or 3D understanding tasks with comparable alignment to human judgments.

### Open Question 2
- Question: Why does long chain-of-thought (CoT) reasoning fail to improve evaluation performance for video understanding tasks?
- Basis in paper: [explicit] The abstract and introduction note that "long chain-of-thought reasoning does not improve performance" for these models.
- Why unresolved: The paper identifies the lack of improvement but does not provide a detailed mechanistic explanation for why extended reasoning degrades or fails to help in the video judge context.
- What evidence would resolve it: An ablation study analyzing the specific reasoning errors introduced by long CoT in video tasks versus text-only tasks.

### Open Question 3
- Question: How does VideoJudge performance degrade when evaluating videos with temporal lengths significantly exceeding the 180–500 frame limit tested?
- Basis in paper: [inferred] The ablation study on "max frames" (Section 6.2) only tests up to 500 frames (training) and 180 (eval), leaving the failure modes of long-form video context (e.g., hour-long content) unexplored.
- Why unresolved: It is unclear if the small 3B/7B models can maintain temporal coherence and judgment accuracy as the sequence length approaches context window limits.
- What evidence would resolve it: Evaluation of VideoJudge on feature-length video benchmarks or untrimmed surveillance datasets to identify performance cliffs relative to frame count.

## Limitations

- The bootstrapping framework assumes the evaluator provides reliable ratings during data generation, but evaluator errors (especially at rating boundaries like 2 vs 3) directly propagate to the training data.
- The framework relies on the bootstrapped dataset covering the full evaluation task distribution, with no guarantee of generalization to novel video domains or rare temporal structures.
- The bootstrapping methodology has only been validated on video understanding tasks, leaving cross-modal applicability untested.

## Confidence

- **High confidence**: VideoJudge-7B matching or exceeding larger baselines on established benchmarks, and the framework's robustness to sampling temperature.
- **Medium confidence**: The rubric generation mechanism's contribution to evaluation accuracy, as direct corpus comparisons are limited.
- **Medium confidence**: Small model performance claims, as they depend on the bootstrapped dataset quality and may not generalize to all evaluation scenarios.

## Next Checks

1. **Evaluator bias validation**: Systematically test the bootstrapping pipeline by intentionally introducing evaluator bias at specific rating levels (e.g., systematically over-scoring rating 2 responses). Measure how this bias propagates through the generated training data and affects judge model performance.

2. **Cross-domain generalization test**: Evaluate VideoJudge models on video understanding tasks from domains not represented in the bootstrapped training data (e.g., medical imaging videos, industrial inspection footage). Compare performance degradation against larger baselines to quantify generalization limits.

3. **Temporal reasoning stress test**: Create a benchmark specifically targeting long-form temporal reasoning by extracting challenging temporal reasoning segments from LongVideoBench. Test whether VideoJudge's performance advantage persists when evaluated on these temporally complex segments versus shorter, simpler video tasks.