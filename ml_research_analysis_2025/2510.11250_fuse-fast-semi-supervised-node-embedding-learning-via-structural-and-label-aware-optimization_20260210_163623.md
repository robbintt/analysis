---
ver: rpa2
title: 'FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and Label-Aware
  Optimization'
arxiv_id: '2510.11250'
source_url: https://arxiv.org/abs/2510.11250
tags:
- fuse
- node2vec
- deepwalk
- semi-supervised
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents FUSE, a fast semi-supervised graph embedding
  framework that generates node representations without requiring predefined features.
  The method integrates three complementary objectives: scalable modularity optimization
  for unsupervised structure preservation, supervised regularization to align embeddings
  of labeled nodes within the same class, and semi-supervised label propagation using
  random walks with attention-based similarity weighting.'
---

# FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and Label-Aware Optimization

## Quick Facts
- arXiv ID: 2510.11250
- Source URL: https://arxiv.org/abs/2510.11250
- Reference count: 40
- Method achieves classification accuracy on par with or superior to state-of-the-art while being 5-7 times faster than Node2Vec and DeepWalk

## Executive Summary
This paper presents FUSE, a fast semi-supervised graph embedding framework that generates node representations without requiring predefined features. The method integrates three complementary objectives: scalable modularity optimization for unsupervised structure preservation, supervised regularization to align embeddings of labeled nodes within the same class, and semi-supervised label propagation using random walks with attention-based similarity weighting. Experiments on six benchmark datasets (Cora, Citeseer, WikiCS, Amazon Photo, PubMed, ArXiv) demonstrate that FUSE achieves classification accuracy on par with or superior to state-of-the-art methods, while being 5-7 times faster than alternatives like Node2Vec and DeepWalk. The method maintains competitive performance across different label masking rates and mechanisms, showing robustness in sparse-label scenarios. FUSE particularly excels in settings where node features are unavailable, providing a scalable solution for large-scale graph representation learning.

## Method Summary
FUSE unifies three objectives via gradient ascent with QR orthonormalization: modularity optimization for structural preservation, supervised regularization to cluster labeled nodes of the same class, and semi-supervised label propagation through attention-weighted random walks. The method generates embeddings S ∈ R^(n×k) initialized randomly and orthonormalized, then iteratively updates via combined gradient ascent where each component addresses different aspects of node similarity. The modularity gradient encourages structural community preservation, the supervised gradient clusters labeled nodes by class, and the semi-supervised gradient propagates label information through random walks weighted by embedding similarity. After T=200 iterations with parameters η=0.05, λ_sup=1.0, and λ_semi=2.0, the resulting embeddings are fed to a downstream GNN classifier (GCN/GAT/GraphSAGE) for evaluation.

## Key Results
- Achieves classification accuracy comparable to or better than state-of-the-art methods (GCN, GAT, GraphSAGE, Node2Vec, DeepWalk)
- Runs 5-7 times faster than Node2Vec and DeepWalk on all benchmark datasets
- Maintains robust performance across varying label masking rates (10%, 50%, 90% labels available)
- Particularly effective when node features are unavailable, outperforming feature-based methods in this regime

## Why This Works (Mechanism)
FUSE works by integrating three complementary objectives that address different aspects of node similarity simultaneously. The modularity optimization captures structural relationships in the graph by encouraging nodes within communities to have similar embeddings. The supervised regularization ensures labeled nodes of the same class are embedded close together, providing direct supervision. The semi-supervised component propagates label information through random walks weighted by attention scores based on current embedding similarity, allowing information to flow from labeled to unlabeled nodes. The QR orthonormalization after each update prevents embedding collapse and maintains numerical stability. By combining these objectives with carefully tuned hyperparameters, FUSE can learn discriminative embeddings even with very few labeled nodes, as demonstrated by consistent performance across different label masking scenarios.

## Foundational Learning
- Graph Modularity (why needed: measures community structure strength; quick check: verify Q ∈ [-1,1] range)
- QR Decomposition for Orthonormalization (why needed: prevents embedding collapse; quick check: verify S^T S ≈ I after each iteration)
- Attention Mechanisms in Graph Context (why needed: weights random walk influence by embedding similarity; quick check: verify attention weights sum to 1 per node)
- Biased Random Walks (why needed: preferentially explores labeled node neighborhoods; quick check: verify labeled nodes appear in walks more frequently than random)
- Gradient Ascent Optimization (why needed: iteratively improves embedding quality; quick check: monitor objective function increase per iteration)

## Architecture Onboarding

**Component Map**: Adjacency Matrix A -> Random Walk Generator -> Attention Calculator -> Gradient Combiner -> Embedding Updater -> QR Orthonormalizer -> (Repeat) -> Final Embeddings -> GNN Classifier

**Critical Path**: The core computational path involves generating labeled random walks (Algorithm 2), computing attention weights between walk neighbors (Algorithm 3), calculating the three gradient components, combining them with specified weights, updating embeddings via gradient ascent, and orthonormalizing via QR decomposition. This loop runs for T=200 iterations to produce final embeddings for downstream classification.

**Design Tradeoffs**: FUSE trades off between structural preservation (modularity), supervised clustering (labeled nodes), and semi-supervised propagation (random walks). The hyperparameters λ_sup and λ_semi control this balance. The method uses efficient matrix operations for scalability but requires storing full n×k embedding matrix. QR orthonormalization adds computational overhead but ensures numerical stability and prevents embedding collapse.

**Failure Signatures**: 
- Learning rate too high: embeddings diverge, objective function decreases
- λ_sup too low: labeled nodes not properly clustered, poor classification
- λ_semi too low: insufficient label propagation to unlabeled nodes
- Insufficient random walks: poor coverage of labeled nodes, weak semi-supervised signal
- Missing orthonormalization: embedding collapse, loss of discriminative power

**3 First Experiments**:
1. Verify modularity gradient computation on small synthetic graph with known communities
2. Test attention weight calculation on simple 3-node graph with manually verified similarities
3. Run complete FUSE pipeline on Cora with 50% labels and compare against Node2Vec baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Requires at least some labeled nodes (cannot work in fully unsupervised settings)
- Memory complexity O(n×k) for storing embedding matrix may limit scalability on very large graphs
- Performance depends on hyperparameter tuning (learning rate, λ_sup, λ_semi) which may vary across datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| Classification accuracy comparable to or better than state-of-the-art | High |
| Runtime efficiency (5-7x faster than Node2Vec/DeepWalk) | High |
| Robustness across different label masking rates | High |
| Effectiveness without node features | High |

## Next Checks
1. Verify the biased walk transition probabilities match the described preference for labeled neighbors
2. Test the sensitivity of results to the random walk parameters (r, L, L') on a held-out dataset
3. Evaluate whether the same performance gains hold when using different GNN architectures (e.g., GCN vs GAT) as the downstream classifier