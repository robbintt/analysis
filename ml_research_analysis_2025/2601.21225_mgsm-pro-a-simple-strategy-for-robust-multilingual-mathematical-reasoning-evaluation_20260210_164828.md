---
ver: rpa2
title: 'MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning
  Evaluation'
arxiv_id: '2601.21225'
source_url: https://arxiv.org/abs/2601.21225
tags:
- english
- language
- template
- dataset
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MGSM-Pro, a multilingual mathematical reasoning
  benchmark extending GSM-Symbolic methodology to nine languages. The dataset generates
  five problem instances per question by varying names, digits, and adding irrelevant
  context.
---

# MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation

## Quick Facts
- arXiv ID: 2601.21225
- Source URL: https://arxiv.org/abs/2601.21225
- Reference count: 24
- Introduces MGSM-Pro benchmark extending GSM-Symbolic to nine languages, revealing significant accuracy drops on digit variations for low-resource languages

## Executive Summary
This paper introduces MGSM-Pro, a multilingual mathematical reasoning benchmark that extends the GSM-Symbolic methodology to nine languages (four high-resource and five low-resource). The dataset generates five problem instances per question by varying names, digits, and adding irrelevant context. Evaluations reveal that many low-resource languages suffer significant accuracy drops when tested on digit variations, unlike high-resource languages. Proprietary models like Gemini 2.5 Flash and GPT-4.1 are less robust to digit instantiation, while Claude 4.0 Sonnet and open models GPT-OSS 120B and DeepSeek V3 show stronger robustness. The study finds that model rankings on leaderboards change when evaluated on multiple instances, recommending evaluation using at least five digit-varying instantiations for a more robust assessment of mathematical reasoning capabilities.

## Method Summary
MGSM-Pro creates 225 multilingual mathematical reasoning questions across nine languages by translating English templates using LLM translation (Gemini 2.0 Flash) with native speaker verification and symbolic equation validation. The benchmark generates six dataset variants per language: SYM_N (name variations), SYM_# (digit variations), SYM_N# (both), and IC_N, IC_#, IC_N# (with irrelevant context). Each question is instantiated five times with different numerical values. Models are evaluated in zero-shot settings using a standardized prompt requiring step-by-step English reasoning, with accuracy computed as the average across all five instantiations.

## Key Results
- Low-resource languages show 15-30 point accuracy drops versus 5-12 points for high-resource languages when facing numerical variations
- Gemini 2.5 Flash ranked #1 on original data (86.2%) but dropped to #4 when averaged over 5 instances (71.2%), while Claude Sonnet 4 moved from #2 to #1
- Proprietary models like Gemini 2.5 Flash and GPT-4.1 are less robust to digit instantiation compared to Claude 4.0 Sonnet and open models GPT-OSS 120B and DeepSeek V3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Digit instantiation variance reveals memorization artifacts rather than genuine reasoning
- Mechanism: When models are evaluated across multiple instances with different numerical values but identical logical structure, performance degradation indicates the model learned surface-level statistical correlations with specific digits rather than abstracting the underlying arithmetic operations. The paper shows accuracy drops of 8-18 points on IC_# variants.
- Core assumption: True mathematical reasoning should be invariant to specific numerical values within the same problem template.
- Evidence anchors:
  - [abstract] "evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set"
  - [section 4.2.1] "changing numbers used in the questions leads to huge drop in performance especially when combined with irrelevant contexts"
  - [corpus] GSM-Symbolic (Mirzadeh et al., 2025) established this variance pattern in English; MGSM-Pro extends to multilingual settings
- Break condition: If models trained with synthetic data augmentation on numerical variations would show reduced variance, this supports the mechanism.

### Mechanism 2
- Claim: Low-resource language vulnerability stems from weaker cross-lingual transfer of reasoning capabilities
- Mechanism: Models with limited exposure to low-resource languages (Joshi Class 1-2: Igbo, Yoruba, Twi, Amharic) show 15-30 point accuracy drops versus 5-12 points for high-resource languages (English, Chinese, French) when facing numerical variations. This suggests reasoning in LRLs relies more heavily on memorized templates with less robust abstraction.
- Core assumption: Language resource level correlates with training data quantity, which affects the depth of learned representations.
- Evidence anchors:
  - [section 4.2.1] "HRLs (English, Chinese, and French) often have smaller drop in performance (<8.0 point) compared to LRLs (Amharic, Igbo, Yoruba and Twi) with bigger drop"
  - [table 1] Twi shows drops from 19.6% (original) to 9.4% (IC_#) across models; English shows 96.0% to 77.3% for Gemma 3 27B
  - [corpus] "Learn Globally, Speak Locally" paper confirms bridging gaps in multilingual reasoning requires addressing resource asymmetry
- Break condition: If LRL performance variance decreases with targeted multilingual fine-tuning on diverse numerical instances, the mechanism holds.

### Mechanism 3
- Claim: Leaderboard rankings become unstable when evaluation uses multiple instantiations
- Mechanism: Single-instance evaluation masks variance in model capabilities. Gemini 2.5 Flash ranked #1 on original data (86.2%) but dropped to #4 when averaged over 5 instances (71.2%), while Claude Sonnet 4 moved from #2 to #1. This indicates some models overfit to benchmark-specific patterns.
- Core assumption: A model's true reasoning capability is better represented by consistency across multiple problem variants than by single-instance accuracy.
- Evidence anchors:
  - [section 4.3] "model ranking is not persistent after introducing five instances of the same question when both numbers and names are changed with irrelevant contexts"
  - [table 2] Shows systematic ranking changes between D_O, Avg-5, and Avg-10 columns
  - [corpus] Limited direct corpus evidence on ranking instability specifically; related work focuses on performance gaps
- Break condition: If ranking stability correlates with training data diversity metrics, this would strengthen the mechanism.

## Foundational Learning

- Concept: Symbolic template instantiation
  - Why needed here: The paper's core methodology replaces placeholder variables (names, digits) with culturally-relevant values while preserving logical structure. Understanding this is essential for interpreting why variance reveals reasoning brittleness.
  - Quick check question: Given a template "If {name} has {num} apples and gives {num2} to {name2}, how many remain?", can you generate 3 valid instantiations with different numbers that test the same arithmetic operation?

- Concept: High-resource vs. low-resource language classification (Joshi scale)
  - Why needed here: The paper explicitly categorizes languages by resource level to explain performance disparities. Results show LRLs are disproportionately affected by numerical variance.
  - Quick check question: Why might a model trained primarily on English (Class 5) struggle to generalize arithmetic reasoning to Twi (Class 1), even with translation?

- Concept: Chain-of-thought prompting in multilingual settings
  - Why needed here: The paper uses a specific prompt instructing models to "reason in clear English" regardless of input language, based on prior findings that LLMs reason better in English.
  - Quick check question: What are the tradeoffs of forcing English reasoning for non-English inputs versus allowing native-language reasoning?

## Architecture Onboarding

- Component map: Template construction pipeline (English templates → LLM translation → Native speaker verification → Symbolic equation validation) → Dataset variants (SYM series, IC series) → Evaluation harness (zero-shot prompting, 5 instantiations per variant, VLLM inference on H100s)

- Critical path:
  1. Identify digit/name placeholders in original MGSM questions
  2. Generate symbolic equations with value constraints
  3. Translate templates with human verification
  4. Sample 5 valid numerical instances per template
  5. Evaluate models and compute accuracy variance across instances

- Design tradeoffs:
  - Using LLM translation (Gemini 2.0 Flash) for scalability vs. potential translation artifacts
  - Restricting variations to names/digits (simpler cross-lingual transfer) vs. missing other robustness dimensions
  - Prompting in English for reasoning vs. not testing native-language reasoning paths
  - 225 of 250 questions used (25 excluded due to equation verification failures)

- Failure signatures:
  - Models showing >15 point drop between D_O and IC_# indicate poor distractor resistance (e.g., Gemma 3 27B: 72.4% → 54.2%)
  - Large LRL vs. HRL performance gap on SYM_# indicates cross-lingual transfer weakness
  - Ranking instability between single-instance and 5-instance averages indicates benchmark overfitting

- First 3 experiments:
  1. Replicate the core variance finding: Run a single model (e.g., GPT-OSS 120B) on all 6 variants for 2 HRLs and 2 LRLs to confirm digit-sensitivity pattern.
  2. Ablate the irrelevant context: Compare SYM_# vs. IC_# to quantify the distractor penalty across language resource levels.
  3. Test the 5-instance minimum recommendation: Compare ranking stability at 1, 3, 5, and 10 instances to validate the paper's sampling recommendation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the language of reasoning (not just the question language) affect mathematical reasoning robustness in multilingual settings?
- Basis in paper: [explicit] Authors state: "It would be interesting to see how models perform when prompted to reason in other languages" beyond English, noting they only evaluated with English reasoning prompts.
- Why unresolved: The current study only instructed models to reason in English (Listing 1), but multilingual reasoning capabilities remain unexplored.
- What evidence would resolve it: Run the same MGSM-Pro evaluation with prompts instructing models to reason in the native language of each question, comparing robustness across SYM and IC variants.

### Open Question 2
- Question: What training data or architectural factors, beyond model scale, determine robustness to numerical variation?
- Basis in paper: [explicit] Appendix D.2 notes "contradictory findings across different model families suggests that simply increasing model scale does not automatically improve robustness; instead, other factors like training methodologies and data mixtures likely play a more significant role."
- Why unresolved: Gemma 3 shows worse robustness with larger models while GPT-OSS shows the opposite—scale alone cannot explain these differences.
- What evidence would resolve it: Controlled experiments varying training data composition (e.g., math data proportion, data augmentation strategies) while holding architecture constant.

### Open Question 3
- Question: How well do reasoning-focused models (e.g., GPT-5, reasoning-tuned variants) maintain robustness under digit and context variation compared to standard LLMs?
- Basis in paper: [explicit] Authors state: "It remains to be seen how other model families, such as Qwen3 or reasoning models like GPT-5, would perform."
- Why unresolved: Current evaluation covers only 12 models, excluding newer reasoning-specialized architectures.
- What evidence would resolve it: Benchmark reasoning models on MGSM-Pro to compare their performance variance across instantiations against current models.

## Limitations

- The methodology relies on LLM-generated translations, which may introduce artifacts not accounted for in robustness evaluation despite native speaker verification
- The binary classification of languages into high-resource and low-resource categories may oversimplify nuanced differences in language model exposure across the 9 tested languages
- The exclusion of 25 questions during dataset creation (due to equation verification failures) may introduce selection bias affecting cross-language comparison validity

## Confidence

**High Confidence** in the core finding that numerical variation significantly impacts model robustness, particularly for low-resource languages. This is supported by systematic evaluation across multiple models and the replication of established GSM-Symbolic variance patterns in multilingual settings.

**Medium Confidence** in the claim that leaderboard rankings become unstable with multi-instance evaluation. While the paper demonstrates this effect, the specific ranking changes depend on the particular model mix tested, and broader validation across additional model families would strengthen this conclusion.

**Low Confidence** in the precise magnitude of performance drops reported, as these are sensitive to the specific numerical sampling strategy and prompt engineering choices not fully detailed in the paper.

## Next Checks

1. **Translation Quality Validation**: Re-run the evaluation using human-translated templates for a subset of questions to isolate the impact of translation artifacts on cross-lingual robustness measurements.

2. **Language Resource Gradient Analysis**: Test models on languages spanning the full Joshi scale (not just binary HRL/LRL) to determine whether performance degradation correlates continuously with resource availability or shows discrete breakpoints.

3. **Cross-Model Consistency Check**: Verify the ranking instability finding by evaluating at least 10 additional models (including different architectures) to determine if the pattern holds across a broader model spectrum or is specific to the tested models.