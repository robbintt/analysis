---
ver: rpa2
title: Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning
  for Humanoid Control
arxiv_id: '2601.21363'
source_url: https://arxiv.org/abs/2601.21363
tags:
- policy
- world
- learning
- velocity
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LIFT, a three-stage framework that couples
  large-scale off-policy pretraining with sample-efficient model-based fine-tuning
  for humanoid control. Using massively parallel SAC pretraining in MuJoCo Playground,
  LIFT achieves zero-shot deployment on real robots and enables safe adaptation in
  new environments.
---

# Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control

## Quick Facts
- arXiv ID: 2601.21363
- Source URL: https://arxiv.org/abs/2601.21363
- Reference count: 40
- One-line primary result: Zero-shot deployment on real robots and sample-efficient fine-tuning for stable velocity-tracking humanoid locomotion using massively parallel SAC pretraining and physics-informed world models.

## Executive Summary
This paper introduces LIFT, a three-stage framework that couples large-scale off-policy pretraining with sample-efficient model-based fine-tuning for humanoid control. Using massively parallel SAC pretraining in MuJoCo Playground, LIFT achieves zero-shot deployment on real robots and enables safe adaptation in new environments. During fine-tuning, deterministic action execution in the environment is combined with stochastic exploration confined to a physics-informed world model, improving both stability and efficiency. Experiments on Booster T1 and Unitree G1 show that LIFT outperforms PPO, FastTD3, and MBPO in sim-to-sim and real-world adaptation, achieving stable locomotion while closely tracking velocity targets.

## Method Summary
LIFT operates in three stages: (1) SAC pretraining in massively parallel MuJoCo Playground with high Update-to-Data ratios (UTD=9) and domain randomization, (2) offline training of a physics-informed world model that combines rigid-body dynamics with a residual neural network for unmodeled contact forces, and (3) fine-tuning with deterministic execution in the target environment while stochastic exploration occurs only within world model rollouts. The world model uses Brax for differentiable physics and integrates privileged state observations to predict residual torques, enabling stable and sample-efficient adaptation from a frozen pretrained policy.

## Key Results
- Zero-shot deployment of pretrained policy on real robots (Booster T1 and Unitree G1) without fine-tuning
- Outperforms PPO, FastTD3, and MBPO in sim-to-sim adaptation with faster convergence and better velocity tracking
- Physics-informed world model enables safe fine-tuning with only 6% real-world data, avoiding the instability of pure MBPO

## Why This Works (Mechanism)

### Mechanism 1: Massively Parallel Off-Policy Stabilization
Scaling Soft Actor-Critic (SAC) with high Update-To-Data (UTD) ratios in massively parallel simulations achieves robust convergence and zero-shot transfer capabilities for humanoid control, challenging the necessity of on-policy methods (like PPO) for this domain. The mechanism relies on reusing large batches of experience (high UTD) within a parallelized JAX architecture, allowing efficient gradient computation without the data transfer overhead typical of off-policy methods, while domain randomization prevents early overfitting to specific dynamics. Assumes domain randomization during pretraining provides a sufficiently diverse data distribution to curb bias accumulation, even when aggressively reusing samples via high UTD. Performance degrades if UTD is increased significantly beyond the paper's specific range (e.g., >10-20) without additional stabilizers, or if the replay buffer is too small relative to the update frequency.

### Mechanism 2: Physics-Informed Residual Modeling
Incorporating explicit Lagrangian dynamics into the world model architecture (a hybrid approach) improves sample efficiency and stability during finetuning compared to pure neural network ensembles. The system uses a "Physics-Informed World Model" where known rigid-body dynamics (mass, gravity, Coriolis forces) are computed analytically via differentiable physics primitives, and a neural network is trained only to predict the *residual* unknowns (contact forces, friction). This reduces the hypothesis space for the neural network, leading to better generalization from limited data. Assumes the majority of the system dynamics can be described by known rigid-body physics, and the "unmodeled" effects (contacts) are learnable via a residual predictor without introducing significant integration errors. The mechanism fails in highly non-rigid environments (e.g., soft mud or complex deformable contacts) where the rigid-body assumption is violated, potentially causing the residual predictor to diverge.

### Mechanism 3: Decoupled Exploration for Safe Adaptation
Separating exploration (stochastic) from execution (deterministic) by confining the former to a learned world model enables safe policy adaptation on physical robots. The agent interacts with the target environment (real or sim) using a deterministic policy (mean action), ensuring safe, predictable behavior. Stochastic exploration occurs *only* within the "imaginary" rollouts of the world model, generating diverse training data for policy updates without risking hardware safety. Assumes the world model is sufficiently accurate to simulate the consequences of stochastic actions, and that gradients generated in the model transfer effectively to the real environment. If the world model develops a bias or "hallucinates" rewards during stochastic rollouts, the policy may exploit these errors (model exploitation), leading to catastrophic failure when deployed back into the real environment.

## Foundational Learning

- **Concept: Soft Actor-Critic (SAC)**
  - **Why needed here:** SAC serves as the backbone for the LIFT framework. Understanding the entropy-regularization objective is crucial, as the paper tunes the temperature parameter $\alpha$ to balance exploration (in the model) and stability.
  - **Quick check question:** Can you explain how the entropy term in SAC prevents the policy from collapsing to a deterministic mode too early?

- **Concept: Lagrangian Dynamics**
  - **Why needed here:** The world model is not a black box; it explicitly computes $M(q)\ddot{q} + C(q, \dot{q}) + G(q)$. Engineers must understand these terms to debug the residual predictor.
  - **Quick check question:** How do Coriolis and centrifugal forces differ from contact forces in the context of this architecture?

- **Concept: Domain Randomization**
  - **Why needed here:** The paper attributes the stability of high UTD pretraining to the diversity provided by domain randomization. Understanding how to configure these ranges is key to reproducing the "Stage 1" results.
  - **Quick check question:** If you randomize friction too aggressively, what might happen to the critic's value estimation during pretraining?

## Architecture Onboarding

- **Component map:** MuJoCo Playground -> SAC Policy (Actor/Critic) -> Privileged State -> Brax Physics Engine + Residual Network -> World Model -> Fine-tuning Loop

- **Critical path:**
  1. Pretrain SAC policy in MuJoCo (Stage 1)
  2. Convert privileged states to Brax generalized coordinates (Stage 2)
  3. Train residual network on offline data to minimize next-state prediction error (Stage 2)
  4. Deploy policy deterministically to target; fine-tune model and policy using imaginary rollouts (Stage 3)

- **Design tradeoffs:**
  - **Safety vs. Sample Efficiency:** The framework sacrifices the potentially faster convergence of direct real-world exploration for the safety of deterministic execution. This limits the "real" data diversity, placing immense pressure on the world model's accuracy.
  - **Inductive Bias vs. Flexibility:** Hard-coding Lagrangian dynamics improves data efficiency but limits the model's ability to learn "non-physical" behaviors or errors in the robot's URDF/model definition.

- **Failure signatures:**
  - **NaNs in Rollouts:** Triggered by unstable integration in the world model if the residual predictor outputs unbounded torques. (Mitigation: Safety reset conditions)
  - **Model Exploitation:** Policy achieves high reward in the world model but fails in reality (Sim-to-Real gap)
  - **Deterministic Collapse:** In fine-tuning, if the world model isn't updated, the policy may overfit to the limited deterministic trajectories and lose robustness

- **First 3 experiments:**
  1. **Pretraining Scaling:** Reproduce the "UTD vs. Convergence" sweep in Section 4.1 to verify that UTD=10 improves sample efficiency in your specific environment
  2. **Model Ablation:** Compare a pure neural network world model (MBPO-style) against the Physics-Informed model on a prediction accuracy task (MSE) to verify the "inductive prior" claim
  3. **Safety Validation:** Deploy a pretrained policy in a safe simulation (Brax) and intentionally switch the world model exploration to "stochastic" in the environment to quantify the safety benefit of the decoupled exploration mechanism

## Open Questions the Paper Calls Out

- **Can the framework handle vision-based tasks?** The authors acknowledge that scaling to vision-centric tasks like dexterous manipulation will likely require latent world models capable of capturing dynamics beyond the robot's body state, as the current implementation operates exclusively on proprioceptive observations.

- **How can real-world fine-tuning safety be automated?** The paper identifies the need for more automated safety mechanisms—such as robot-assisted resetting or recovery policies—to mitigate the risks of deterministic data collection, as current real-world experiments rely on human operators to terminate unsafe behaviors.

- **Does high UTD effectiveness generalize beyond this domain?** The authors note that the improvements from high UTD appear tied to their massively parallel, domain-randomized regime, and they do not claim generality across different tasks or without domain randomization.

## Limitations
- Effectiveness depends on accurate URDF models and may fail with highly non-rigid contact dynamics
- Safety guarantees rely on world model fidelity, creating potential "model exploitation" risks
- Asymmetric actor-critic architecture requires privileged state during fine-tuning, limiting practical deployment scenarios

## Confidence
- **High Confidence:** Massively parallel SAC pretraining with high UTD ratios improves sample efficiency (supported by ablation in Section 4.1 and JAX implementation details)
- **Medium Confidence:** Physics-informed world models outperform pure neural networks for humanoid control (supported by comparison to MBPO in Section 5.3, but no ablation of alternative architectures)
- **Medium Confidence:** Decoupled exploration improves safety during real-world adaptation (mechanism described clearly but quantitative safety metrics are limited to qualitative observations)

## Next Checks
1. **Model Exploitation Test:** Deploy a finetuned policy in a perturbed simulation where the world model's residual predictions are systematically biased. Measure whether the policy's performance degrades more severely than a baseline that uses stochastic exploration in the environment.

2. **Residual Network Ablation:** Replace the physics-informed world model with a pure neural network ensemble (MBPO-style) and compare sample efficiency and stability during finetuning on the same task. Verify the claim about inductive priors improving generalization.

3. **Safety Boundary Analysis:** Systematically vary the stochastic exploration noise level during world model rollouts and measure the trade-off between exploration coverage and policy safety when deployed back to the real environment.