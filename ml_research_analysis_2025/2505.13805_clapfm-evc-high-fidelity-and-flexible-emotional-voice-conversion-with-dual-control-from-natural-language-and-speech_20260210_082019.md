---
ver: rpa2
title: 'ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual
  Control from Natural Language and Speech'
arxiv_id: '2505.13805'
source_url: https://arxiv.org/abs/2505.13805
tags:
- speech
- emotional
- emotion
- arxiv
- clapfm-evc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ClapFM-EVC, a novel emotional voice conversion
  (EVC) framework that enables high-fidelity speech conversion driven by natural language
  prompts or reference speech with adjustable emotion intensity. The core idea is
  to use EVC-CLAP, a contrastive language-audio pretraining model, to extract and
  align fine-grained emotional elements across speech and text modalities.
---

# ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech
## Quick Facts
- arXiv ID: 2505.13805
- Source URL: https://arxiv.org/abs/2505.13805
- Reference count: 0
- Primary result: Achieves state-of-the-art EVC performance with 0.82 EECS, 3.85 eMOS, 4.09 nMOS, and 3.68 UTMOS

## Executive Summary
ClapFM-EVC is a novel emotional voice conversion framework that enables high-fidelity speech conversion driven by natural language prompts or reference speech with adjustable emotion intensity. The system leverages EVC-CLAP, a contrastive language-audio pretraining model, to extract and align fine-grained emotional elements across speech and text modalities. Combined with a FuEncoder that fuses emotional features with Phonetic PosteriorGrams from a pre-trained ASR model, the framework uses a flow matching model to reconstruct the Mel-spectrogram of source speech. This dual-control approach allows for both text-based and reference-based emotion conversion with intensity adjustment.

## Method Summary
The core innovation of ClapFM-EVC lies in its integration of EVC-CLAP for cross-modal emotional feature extraction and alignment. The system extracts emotional elements from either natural language prompts or reference speech using the pretrained EVC-CLAP model, then processes these features through a FuEncoder with adaptive intensity gate. This encoder fuses the emotional features with Phonetic PosteriorGrams from a pre-trained ASR model to create a rich representation of both content and emotion. A flow matching model then reconstructs the Mel-spectrogram from this combined representation, producing high-fidelity emotional voice conversion output. The framework supports both text-to-speech emotion conversion and reference-based emotion transfer with intensity control.

## Key Results
- Achieves 0.82 EECS (emotion evaluation correctness score)
- Attains 3.85 eMOS (emotional speech quality score)
- Reaches 4.09 nMOS (naturalness mean opinion score)
- Records 3.68 UTMOS (overall speech quality score)

## Why This Works (Mechanism)
ClapFM-EVC succeeds by bridging the semantic gap between text and speech through cross-modal alignment. The EVC-CLAP model extracts fine-grained emotional elements from both modalities, creating a shared emotional representation space. The FuEncoder then intelligently combines these emotional features with phonetic content from the ASR model, ensuring that converted speech maintains both the desired emotion and the original linguistic content. The flow matching model provides high-fidelity reconstruction of the Mel-spectrogram, preserving speech quality while incorporating the emotional transformation.

## Foundational Learning
- **EVC-CLAP (Contrastive Language-Audio Pretraining)**: A cross-modal model pretrained to align emotional elements between text and speech representations. Needed for extracting consistent emotional features across modalities. Quick check: Verify the model can extract comparable emotional vectors from semantically equivalent text and speech inputs.
- **Phonetic PosteriorGrams (PPGs)**: Probabilistic representations of phonetic content extracted from pre-trained ASR models. Needed to preserve linguistic content during emotional conversion. Quick check: Confirm PPGs capture speaker-independent phonetic information from the source speech.
- **Flow Matching Models**: Generative models that learn to transform noise into target data distributions through gradual denoising. Needed for high-fidelity Mel-spectrogram reconstruction. Quick check: Validate the model can reconstruct Mel-spectrograms that match the target distribution in terms of both spectral and temporal characteristics.
- **Adaptive Intensity Gate**: A mechanism for controlling the strength of emotional expression in the converted speech. Needed to provide flexible control over emotion intensity. Quick check: Test that intensity scaling produces perceptually graded emotional expressions.
- **Cross-Modal Alignment**: The process of mapping features between different data modalities (text and speech). Needed to enable emotion transfer from text descriptions to speech output. Quick check: Measure alignment quality using cross-modal retrieval metrics.

## Architecture Onboarding
Component Map: Text/Reference Speech -> EVC-CLAP -> Emotional Features -> FuEncoder -> PPGs -> Combined Representation -> Flow Matching Model -> Mel-spectrogram -> Audio Output
Critical Path: The EVC-CLAP feature extraction and FuEncoder fusion stages are most critical, as errors here propagate through the entire pipeline and cannot be recovered by the flow matching stage.
Design Tradeoffs: The framework trades computational complexity for flexibility, supporting both text and reference-based emotion control. The reliance on pretrained models reduces training data requirements but may limit adaptation to domain-specific emotional expressions.
Failure Signatures: Poor emotional alignment between text and speech manifests as semantically inconsistent emotional expressions. PPG extraction failures result in distorted linguistic content. Flow matching reconstruction errors appear as artifacts in the Mel-spectrogram.
First Experiments:
1. Test cross-modal retrieval accuracy using EVC-CLAP to verify emotional alignment quality
2. Evaluate PPG extraction quality by reconstructing speech from PPGs alone
3. Assess flow matching reconstruction quality using clean emotional feature inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims depend heavily on the quality and diversity of the EVC-CLAP model's pretraining corpus
- "Significant improvements" lack statistical validation and confidence intervals
- Baseline model details and implementation specifics are not provided for comparison
- Potential modality-specific failure modes in the dual control mechanism are not thoroughly explored

## Confidence
- **High**: Technical methodology and architectural design are clearly described and logically sound
- **Medium**: Performance improvements are reported but lack statistical validation and baseline transparency
- **Low**: Robustness claims across speakers, emotions, and real-world conditions are not demonstrated

## Next Checks
1. Perform statistical significance testing of reported MOS and EECS scores with confidence intervals across multiple test sets
2. Conduct ablation studies isolating contributions of EVC-CLAP, FuEncoder, and flow matching components
3. Test cross-speaker and cross-emotion generalization with speakers and emotional expressions not present in training data