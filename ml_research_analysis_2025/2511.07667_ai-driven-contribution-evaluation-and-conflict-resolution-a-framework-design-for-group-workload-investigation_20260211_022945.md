---
ver: rpa2
title: 'AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design
  for Group Workload Investigation'
arxiv_id: '2511.07667'
source_url: https://arxiv.org/abs/2511.07667
tags:
- assessment
- learning
- peer
- team
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an AI-driven framework to objectively assess
  individual contribution in group work and assist in conflict resolution. The method
  combines objective metrics (code commits, word counts, sentiment analysis, task
  fidelity) with AI grading and contextual analysis to generate interpretable judgments.
---

# AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation

## Quick Facts
- **arXiv ID:** 2511.07667
- **Source URL:** https://arxiv.org/abs/2511.07667
- **Reference count:** 40
- **Primary result:** AI-driven framework combines objective metrics (code commits, word counts, sentiment analysis, task fidelity) with AI grading and contextual analysis to generate interpretable judgments for group contribution evaluation and conflict resolution.

## Executive Summary
This paper proposes an AI-driven framework to objectively assess individual contribution in group work and assist in conflict resolution. The method combines objective metrics (code commits, word counts, sentiment analysis, task fidelity) with AI grading and contextual analysis to generate interpretable judgments. Metrics are normalized, aggregated, and paired with inequality measures (Gini index) to flag conflicts. An LLM performs validated analysis to produce transparent advisory judgments. The approach addresses limitations of peer assessment by reducing bias and increasing objectivity, while maintaining human oversight. The framework is designed to be feasible under current institutional policies, though practical challenges around data integration and identity reconciliation remain.

## Method Summary
The framework ingests heterogeneous evidence including code repositories, communication logs, coordination records, peer assessments, and contextual information. It extracts raw metrics across nine benchmarks organized into three dimensions: Contribution (commits, net lines, word/char counts), Interaction (send/receive ratios, response times, sentiment), and Role (attendance, task fidelity). These metrics are normalized using AutoRating (Individual Average / Team Average), aggregated with weighted masks, and analyzed using Gini coefficients to flag potential conflicts. A hierarchical LLM architecture with double-pass validation generates advisory judgments based on the metrics and context. The system produces three holistic objective measures for instructor review rather than individual grades.

## Key Results
- Framework successfully aggregates heterogeneous contribution data into normalized, comparable metrics using AutoRating methodology
- Gini coefficient analysis effectively identifies two conflict scenarios: over-centralization (one person doing everything) and social loafing (one person disengaged)
- LLM-based expert analysis generates interpretable advisory judgments while maintaining transparency and reducing "kindness bias" through hierarchical prompting and validation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalizing heterogeneous data points allows for the comparison of disparate contribution types (e.g., code commits vs. meeting attendance).
- **Mechanism:** The architecture aggregates raw metrics (line counts, message frequency) and normalizes them using an AutoRating-derived method (Individual Average / Team Average). This creates a relative performance factor rather than an absolute count, enabling the system to weigh different types of "work" against a unified team baseline.
- **Core assumption:** Quantitative proxies (e.g., word count, commit frequency) correlate positively with qualitative effort and learning outcomes, and the team average is a valid baseline for "expected" performance.
- **Evidence anchors:** [abstract] "...Objective measures are normalised, aggregated per dimension..."; [section 5.2] "...each metric is normalized via AutoRating, and then aggregated with a weighted mask..."
- **Break condition:** If students engage in "gaming strategies" (e.g., generating high volumes of low-quality text or commits), the normalization mechanism will inflate the scores of low-effort participants, breaking the correlation between metric and effort.

### Mechanism 2
- **Claim:** Statistical inequality measures can function as automated early warning systems for group conflict.
- **Mechanism:** The system calculates a Gini index on the aggregated base measures. It distinguishes between two scenarios: (A) High Gini with high individual scores indicates "over-centralization" (one person doing everything); (B) High Gini with low scores indicates "social loafing" (one person disengaged).
- **Core assumption:** Deviation from equality in contribution metrics is a primary indicator of negative group dynamics and potential dispute, rather than a reflection of a valid, negotiated division of labor.
- **Evidence anchors:** [abstract] "...paired with inequality measures (Gini index) to flag conflicts."; [section 5.3] "We flag potential group conflicts by computing the Gini index for each base measure... We propose two scenarios..."
- **Break condition:** If a team legitimately agrees to a highly unequal division of labor (e.g., one specialist member and one coordinator), the Gini index will flag this as a conflict (false positive), requiring manual instructor override.

### Mechanism 3
- **Claim:** Large Language Models (LLMs) can synthesize quantitative metrics and qualitative context into transparent, advisory judgments.
- **Mechanism:** The system uses a hierarchical prompt architecture. It first surfaces local features (specific metrics) and pairs them with global information (objective measures) to form a conclusion. A "double-pass validation" is used to screen for hallucinations and ensure regulatory compliance.
- **Core assumption:** Current LLMs possess the reasoning capability to interpret sociotechnical data (communication logs, code quality) without exhibiting excessive "kindness bias" or hallucination, provided they are constrained by specific metrics.
- **Evidence anchors:** [abstract] "A Large Language Model (LLM) architecture performs validated and contextual analysis... to generate interpretable and transparent advisory judgments."; [section 5.4] "We use hierarchical prompts with decomposed queries to infer the situation... screened with a double-pass validation."
- **Break condition:** If the input context exceeds the LLM's context window or if the "double-pass" fails to catch subtle logical inconsistencies, the system may produce an "advisory judgment" that is legally risky or factually incorrect, eroding trust.

## Foundational Learning

- **Concept:** **AutoRating / Relative Performance Factor**
  - **Why needed here:** This is the mathematical engine of the framework. You cannot simply add code lines to meeting minutes. You must understand how the formula `Final Indiv. Grade = (Indiv. Average / Team Average) × Team Grade` converts raw data into a comparable score.
  - **Quick check question:** If a team member scores 0 on the "Interaction" metric but the team average is 50, how does this affect their final relative score?

- **Concept:** **Gini Coefficient**
  - **Why needed here:** This statistic is the primary "conflict marker." Understanding that 0 represents perfect equality and 1 represents maximum inequality is required to interpret the system's flags for "social loafing" vs. "over-centralization."
  - **Quick check question:** In a 4-person group, one person does 90% of the work. Would the Gini index be closer to 0 or 1?

- **Concept:** **Advisory vs. Automated Decision Making**
  - **Why needed here:** The paper explicitly positions the tool as "advisory" to bypass legal restrictions on "automated decision-making" (GDPR Article 22). Distinguishing between *profiling* (generating data about a person) and *deciding* (assigning a grade) is critical for compliance.
  - **Quick check question:** If the AI suggests a grade of 60%, and the instructor clicks "Approve" without reading, who is legally responsible for the grade?

## Architecture Onboarding

- **Component map:** Data Ingestion Layer -> Metric Extraction Module -> Normalizer -> Conflict Flagger -> Expert Analyzer (LLM) -> Dashboard
- **Critical path:** The **Identity Reconciliation** step (mapping Git commits to student IDs and chat handles to student IDs) is the most fragile part of the pipeline. If identity cannot be verified, the normalized metrics fail.
- **Design tradeoffs:** Transparency vs. Complexity (aggregating metrics into three "Objective Measures" hides the nuance of the nine benchmarks); Advisory vs. Efficiency (keeping "human-in-the-loop" ensures policy compliance but reduces automation benefits).
- **Failure signatures:** "Gaming" detection (sudden spikes in word count or commit frequency without corresponding changes in "Task Fidelity"); Identity Drift (metrics aggregated under wrong student due to mismatched email handles); Kindness Bias (LLM consistently rating qualitative feedback higher than quantitative metrics justify).
- **First 3 experiments:** 1) Unit Test Normalization: Create synthetic dataset with known contribution distributions and verify Gini flag triggers correctly and AutoRating produces expected scores; 2) Identity Reconciliation Stress Test: Ingest real Git logs and Chat logs from past project and attempt to map to class rosters without manual intervention; 3) LLM Validity Check: Run "Expert Analysis" prompt on conflict scenario with known ground truth to check for hallucination or "kindness bias."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How should instructors interact with and interpret AI-generated advisory judgments to effectively conduct group workload investigations?
- **Basis in paper:** [explicit] The conclusion identifies a "research gap in how instructors can engage with model-generated advice and conduct AI-assisted investigation – beyond keeping the 'human-in-the-loop'."
- **Why unresolved:** While the framework generates interpretable judgments, it does not define the specific workflows, heuristics, or decision thresholds an instructor should use when accepting, rejecting, or adjusting AI advice.
- **What evidence would resolve it:** User studies or usability trials measuring instructor decision accuracy, efficiency, and trust when utilizing the proposed tool in simulated conflict scenarios.

### Open Question 2
- **Question:** Can the proposed framework effectively detect and mitigate "gaming strategies" where students submit misleading evidence to inflate their contribution metrics?
- **Basis in paper:** [explicit] Section 5.5 urges "further research into investigation techniques accounting for gaming strategies and unreliable information," and Section 5.6 identifies this as a primary limitation where accuracy may decrease.
- **Why unresolved:** The implementation relies on objective metrics (e.g., commit counts) and peer assessment, both of which are vulnerable to manipulation, and the proposed validation mechanisms are not tested against adversarial behaviors.
- **What evidence would resolve it:** Red-team exercises or simulations where students attempt to "game" the system, followed by an analysis of the system's ability to flag or correct these manipulations.

### Open Question 3
- **Question:** Do heterogeneous AI agents improve inter-rater reliability and robustness in the evaluation of subjective contribution dimensions?
- **Basis in paper:** [explicit] The conclusion suggests "possible studies into improving inter-rater reliability with heterogeneous agents" as a direction for future work.
- **Why unresolved:** The current design employs a hierarchical LLM architecture, but it remains unproven whether employing diverse or competing AI personas (heterogeneous agents) yields more consistent or fairer results than a single-model approach.
- **What evidence would resolve it:** Comparative experiments measuring the variance in scores generated by heterogeneous agents versus single-agent systems across the same datasets.

## Limitations
- Correlation between objective metrics (code commits, word counts) and actual contribution quality remains unverified empirically
- Identity reconciliation across heterogeneous data sources presents critical technical barrier, particularly mapping Git commits to student identities
- Framework's conflict detection mechanism (Gini index) assumes unequal contribution is inherently problematic, potentially flagging legitimate divisions of labor as conflicts

## Confidence

**High confidence:** Framework architecture design, legal positioning as advisory system, identity reconciliation as critical bottleneck

**Medium confidence:** Mathematical validity of AutoRating normalization, Gini index as conflict marker, LLM integration for contextual analysis

**Low confidence:** Correlation between objective metrics and actual learning outcomes, effectiveness of hierarchical prompting for complex group dynamics, practical deployment without manual intervention

## Next Checks

1. **Ground truth validation study:** Compare framework output against expert human evaluation of actual group projects to verify correlation between AI assessment and perceived contribution quality.

2. **Gaming behavior stress test:** Create synthetic datasets with known "gaming" strategies (high-volume low-quality commits, excessive chat messages) to evaluate if the system correctly identifies and de-emphasizes these behaviors.

3. **Cross-cultural conflict detection validation:** Test the framework across diverse cultural contexts to verify if Gini-based conflict detection generalizes beyond Western academic settings, where perceptions of contribution and conflict may differ significantly.