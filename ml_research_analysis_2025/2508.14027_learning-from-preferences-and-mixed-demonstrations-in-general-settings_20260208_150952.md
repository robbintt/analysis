---
ver: rpa2
title: Learning from Preferences and Mixed Demonstrations in General Settings
arxiv_id: '2508.14027'
source_url: https://arxiv.org/abs/2508.14027
tags:
- reward
- demonstrations
- learning
- feedback
- epochs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LEOPARD, a method for learning reward functions
  from human feedback including preferences, positive demonstrations, and negative
  demonstrations. The core idea is to frame human feedback as reward-rational partial
  orderings (RRPO) over observable trajectory fragments, enabling gradient-based optimization.
---

# Learning from Preferences and Mixed Demonstrations in General Settings

## Quick Facts
- arXiv ID: 2508.14027
- Source URL: https://arxiv.org/abs/2508.14027
- Authors: Jason R Brown; Carl Henrik Ek; Robert D Mullins
- Reference count: 40
- Primary result: LEOPARD learns reward functions from preferences and demonstrations, outperforming baselines in 4 environments

## Executive Summary
This paper introduces LEOPARD, a method for learning reward functions from mixed human feedback including preferences, positive demonstrations, and negative demonstrations. The core innovation is framing human feedback as reward-rational partial orderings (RRPO) over trajectory fragments, enabling gradient-based optimization of a unified loss function. LEOPARD encodes different feedback types as partial orderings and optimizes a reward model to satisfy them, while also incorporating a smoothness penalty. Experiments on four environments show LEOPARD significantly outperforms baselines including DeepIRL+RLHF and AILP when learning from preferences and demonstrations, and matches or beats them with only demonstrations.

## Method Summary
LEOPARD learns reward functions from mixed human feedback by encoding preferences, positive demonstrations, and negative demonstrations as reward-rational partial orderings (RRPO) over trajectory fragments. The method alternates between sampling agent trajectories, training a reward model using the RRPO loss with smoothness regularization, and training the policy via RL (SAC for continuous, PPO for discrete). The reward model is trained until convergence (loss change <10% for 3 steps) rather than using fixed epochs. The framework unifies different feedback types through a Boltzmann-rational choice model, avoiding ad-hoc pipeline combinations of IRL and RLHF approaches.

## Key Results
- LEOPARD significantly outperforms DeepIRL+RLHF and AILP baselines when learning from preferences and demonstrations across 4 environments
- Combining multiple feedback types is often more effective than using a single type
- LEOPARD matches or exceeds baseline performance when learning from only demonstrations
- Dynamic stopping conditions prevent overfitting while maintaining plasticity for preference data

## Why This Works (Mechanism)

### Mechanism 1: Unified Partial Ordering Likelihood
The core insight is that diverse feedback types can be encoded as strict partial orderings over trajectory fragments and optimized via a single gradient-based loss. LEOPARD uses Reward-Rational Partial Orderings (RRPO) where both demonstrations and preferences are mapped to sets of orderings. The likelihood is modeled using a Boltzmann-rational choice function (Plackett-Luce), allowing joint optimization. This works because human feedback is assumed to be Boltzmann-rational over observed trajectory fragments.

### Mechanism 2: Reward Bounds via Loss Minimization
Minimizing the negative log-likelihood of the RRPO model theoretically enforces correct directionality of human feedback. Theorem E.1 proves that if the RRPO loss is below log(2), the reward function is guaranteed to assign strictly higher reward to preferred trajectories. This bound ensures the learned reward respects the ordering constraints encoded in human feedback.

### Mechanism 3: Dynamic Stopping and Smoothness
Iterative training with dynamic stopping conditions prevents overfitting to limited demonstration data while maintaining plasticity for preference data. The algorithm monitors reward loss convergence (stopping when change <10%) rather than using fixed epochs. A smoothness penalty on the reward derivative across trajectory steps stabilizes the learning signal for the RL agent, addressing the different convergence rates between demonstration and preference loss.

## Foundational Learning

- **Concept: Plackett-Luce Model / Boltzmann Rationality**
  - Why needed: The RRPO framework generalizes this probabilistic ranking model. Understanding how exponential reward functions translate into selection probabilities is fundamental.
  - Quick check: Given two trajectories with rewards $R_A$ and $R_B$, what is the probability $P(A \succ B)$ under the Bradley-Terry/Plackett-Luce assumption?

- **Concept: Inverse Reinforcement Learning (IRL) vs. RLHF**
  - Why needed: The paper solves the "pipeline" problem where IRL (demos) and RLHF (preferences) are usually sequential. Understanding distinct loss functions is key to seeing why RRPO unifies them.
  - Quick check: Why does standard DeepIRL struggle to incorporate pairwise preference data directly without retraining?

- **Concept: Partial vs. Total Orderings**
  - Why needed: LEOPARD relies on encoding feedback as partial orderings (we know $A > B$, but maybe not how $C$ relates to $A$).
  - Quick check: How does a strict partial ordering differ from a total ordering, and why is partial ordering more robust for sparse human feedback?

## Architecture Onboarding

- **Component map:** Inputs (D_pos, D_neg, D_agent, P) -> RRPO Encoder (converts to fragments D and orderings C) -> Loss Function (L_RRPO + L_Smooth) -> Optimizer (AdamW for θ) -> RL Loop (SAC/PPO trains π using R_θ)

- **Critical path:** The RRPO Encoding is vital. If implementation incorrectly constructs sets C (e.g., failing to include agent trajectories in comparison set for demonstrations), gradients will push reward in wrong direction. Ensuring <DemosVsAgent is correctly formulated as τ_n < τ_a < τ_p is critical.

- **Design tradeoffs:**
  - Data Normalization: Normalizing loss across batches with vastly different data sources (few demos vs. many prefs) is non-trivial
  - Negative Demos: Required special handling in Cliff Walking (split rankings) because agent behavior could be as bad as negative demos

- **Failure signatures:**
  - Positive Feedback Loop: If using naive "Sum-of-Choices" implementation, reward model may reinforce its own incorrect initial rankings
  - Catastrophic Forgetting: If agent explores new regions, reward model must be retrained on new agent trajectories; buffer D_agent too small causes drift

- **First 3 experiments:**
  1. Synthetic Validation: Replicate synthetic preference generation to verify RRPO loss implementation decreases as expected
  2. Loss Bound Check: On trivial environment, verify Theorem E.1 by forcing loss below log(2) and confirming R(τ_pos) > R(τ_agent)
  3. Dynamic vs. Fixed Stopping: Ablate dynamic stopping against fixed epochs on sparse environment to observe overfitting behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LEOPARD maintain its performance advantage when applied to complex domains like LLM finetuning or advanced robotics?
- Basis: The authors state a more comprehensive study would investigate complex robotics, video games, and LLM finetuning
- Why unresolved: Evaluation was limited to four Gymnasium environments to facilitate repeated experiments and tuning
- What evidence would resolve it: Empirical evaluations on high-dimensional tasks like LLM instruction following or complex robotic manipulation

### Open Question 2
- Question: How does the relative proportion of different feedback types impact learning efficiency?
- Basis: The authors state it would be instructive to interrogate how performance depends on proportions of different feedback
- Why unresolved: Experiments used fixed combinations (e.g., 50/50 splits) and found results "mixed and noisy" without determining optimal ratios
- What evidence would resolve it: Systematic study varying ratio of preferences to demonstrations to identify task-specific optimal data mixes

### Open Question 3
- Question: Are the theoretical RRPO formalisms for feedback types not yet tested (e.g., corrections, language) empirically viable for training agents?
- Basis: The authors note that apart from feedback types used by LEOPARD, provided RRPO formalisms are speculative and not yet empirically validated
- Why unresolved: Study only validated framework for preferences and ranked demonstrations, leaving other encodings as theoretical proposals
- What evidence would resolve it: Successful agent training using RRPO encodings for other feedback modalities like corrections or language

## Limitations

- The method relies on synthetically generated human feedback, which may not capture real-world ambiguity and noise in human preferences
- Reward model architecture details are underspecified, making exact reproduction challenging
- Performance with genuinely noisy or contradictory human feedback remains untested
- Dynamic stopping conditions lack rigorous theoretical justification for the chosen threshold

## Confidence

- **High confidence:** The RRPO framework as a unified approach for encoding different feedback types is well-justified and the loss minimization guarantees (Theorem E.1) are mathematically sound
- **Medium confidence:** Empirical results showing LEOPARD outperforms baselines are compelling but based on synthetic data; real human feedback may behave differently
- **Medium confidence:** The claim that combining multiple feedback types is often more effective than single types is supported but not extensively ablated across all environments

## Next Checks

1. Implement the exact RRPO loss function and verify that when loss drops below log(2), the learned reward correctly orders trajectory pairs according to Theorem E.1
2. Test LEOPARD's robustness to noisy preferences by adding Gaussian noise to the synthetic preference generation process and measuring degradation in performance
3. Conduct a systematic ablation study varying the ratio of demonstrations to preferences to quantify the marginal benefit of combining feedback types versus using either alone