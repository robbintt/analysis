---
ver: rpa2
title: 'Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards
  for Mathematical Reasoning'
arxiv_id: '2502.14356'
source_url: https://arxiv.org/abs/2502.14356
tags:
- reasoning
- arxiv
- step
- step-wise
- step-dpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of Direct Preference Optimization
  (DPO) in handling long-chain mathematical reasoning tasks. While existing methods
  like Step-DPO focus on optimizing only the first erroneous step, they ignore other
  useful steps and still rely on solution-wise DPO loss.
---

# Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2502.14356
- Source URL: https://arxiv.org/abs/2502.14356
- Authors: Huimin Xu; Xin Mao; Feng-Lin Li; Xiaobao Wu; Wang Chen; Wei Zhang; Anh Tuan Luu
- Reference count: 40
- Primary result: Full-Step-DPO outperforms DPO and Step-DPO on mathematical reasoning benchmarks across multiple backbone models

## Executive Summary
This paper addresses the limitation of Direct Preference Optimization (DPO) in handling long-chain mathematical reasoning tasks. While existing methods like Step-DPO focus on optimizing only the first erroneous step, they ignore other useful steps and still rely on solution-wise DPO loss. To overcome these issues, the authors propose Full-Step-DPO, a novel framework that leverages step-wise rewards from the entire reasoning chain. This is achieved by training a self-supervised Process Reward Model (PRM) that automatically scores each step, and introducing a step-wise DPO loss that dynamically updates gradients based on these rewards. Extensive experiments on various mathematical reasoning benchmarks with multiple backbone models demonstrate that Full-Step-DPO consistently outperforms state-of-the-art baselines, including DPO and Step-DPO.

## Method Summary
Full-Step-DPO introduces a self-supervised Process Reward Model (PRM) trained to score individual reasoning steps based on their correctness. The method generates M=32 solutions per problem, uses the base model to label steps as correct/incorrect based on final answer match (N=1 simplification), and trains the PRM as a classifier head on the base model. For preference data construction, solutions are scored by average step reward, with top/bottom T=4 correct/incorrect solutions forming 16 preference pairs. The step-wise DPO loss decomposes standard DPO gradients into per-step components, weighting each step's gradient by its PRM-assigned reward via softmax normalization. The framework is trained end-to-end with three-stage pipeline: PRM training → preference pair construction → step-wise DPO optimization.

## Key Results
- Full-Step-DPO consistently outperforms DPO and Step-DPO across four backbone models (MetaMath-Mistral-7B, Qwen2-7B-SFT, Qwen2-72B-Instruct, MetaMath-DeepSeek-Coder-33B) on GSM8K and MATH benchmarks
- Performance peaks at γ=0.5 reward temperature, with significant degradation at extremes (γ→0 degrades to vanilla DPO, γ→∞ approaches Step-DPO)
- PRM simplification with N=1 achieves competitive results while reducing training time from 317h to 8.5h A100-hours (M=32 provides sufficient diversity)
- Best-of-N decoding underperforms Self-Consistency for strong base models, while Step-wise Beam Search underperforms both but beats greedy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing all reasoning steps with reward-weighted gradients improves mathematical reasoning more than optimizing only the first erroneous step.
- Mechanism: The step-wise DPO loss decomposes the standard DPO gradient into per-step components, then weights each step's gradient by its PRM-assigned reward via softmax normalization. For preferred solutions, steps with higher rewards receive stronger gradient ascent; for dispreferred solutions, steps with lower rewards receive stronger gradient descent. This dynamically focuses learning on the most informative steps.
- Core assumption: The PRM provides sufficiently accurate step-wise reward signals that correlate with reasoning correctness. When γ→0, the method degrades to vanilla DPO (equal weights); when γ→∞, it approaches Step-DPO (single-step optimization).
- Evidence anchors:
  - [section 3.2]: "This approach allows us to leverage all steps and adaptively adjust the weight of each step based on its probability of correctness, achieving true step-wise optimization."
  - [section 4.6, Figure 5]: Experiments show accuracy peaks at γ=0.5, declining at extremes.
  - [corpus]: Related work "MDPO: Multi-Granularity Direct Preference Optimization" similarly argues for multi-granularity optimization in mathematical reasoning, but corpus lacks direct validation of this specific weighting scheme.
- Break condition: If PRM rewards are poorly calibrated (e.g., high rewards for incorrect steps), gradient weighting will amplify noise rather than signal.

### Mechanism 2
- Claim: A self-supervised PRM trained with N=1 Monte Carlo simulation can achieve competitive performance while drastically reducing computational cost.
- Mechanism: Traditional Monte Carlo PRM training requires simulating N subsequent reasoning paths per step (complexity O(MNK)). The authors observe that with large sampling M (e.g., 32 solutions per problem), setting N=1—simply labeling steps as correct/incorrect based on whether the final answer matches the gold label—yields sufficient training data diversity to compensate for reduced per-step label precision.
- Core assumption: Solution diversity from large M compensates for label noise from low N. This relies on the base model generating varied reasoning paths.
- Evidence anchors:
  - [section 3.3]: "We found that the trained PRM performs reasonably well even with N=1 when M is large, such as 32... reducing computational resources and lowering the time complexity to O(M)."
  - [section 4.5, Figure 4]: Performance fluctuates only ~1% across N∈{1,2,4,8}, while cost increases from 8.5h to 317h A100-hours.
  - [corpus]: "Math-Shepherd" (referenced in paper) uses full Monte Carlo estimation; corpus papers do not directly validate the N=1 simplification.
- Break condition: If the base model generates homogeneous solutions (low diversity), the PRM may fail to generalize.

### Mechanism 3
- Claim: Constructing preference pairs from PRM-scored solutions enables better preference learning than solution-wise or single-step approaches.
- Mechanism: For each problem, generate M=32 solutions, score all steps with PRM, compute average reward per solution. Select top T=4 correct solutions (highest rewards) and bottom T=4 incorrect solutions (lowest rewards) to form 16 preference pairs. This ensures preference data captures fine-grained quality differences across entire reasoning chains.
- Core assumption: Average step reward correlates with solution quality; extremes (highest/lowest) provide clean preference signals.
- Evidence anchors:
  - [section 3.4]: Describes the three-step pipeline with T=4 selection strategy.
  - [section 4.2, Table 1]: Full-Step-DPO consistently outperforms DPO and Step-DPO across four backbone models on GSM8K and MATH.
  - [corpus]: Related work "Rewarding Graph Reasoning Process" also emphasizes process-level rewards, but corpus lacks direct comparison of preference construction strategies.
- Break condition: If average reward poorly discriminates between solution quality (e.g., all solutions cluster around similar scores), preference pairs will provide weak learning signal.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO) fundamentals
  - Why needed here: The paper builds on and modifies standard DPO loss; understanding the baseline formulation (β-regularized policy ratio, sigmoid loss) is prerequisite to grasping the step-wise modification.
  - Quick check question: Can you explain why DPO avoids training an explicit reward model and how the gradient naturally increases preferred response probability while decreasing dispreferred?

- Concept: Process Reward Models (PRMs)
  - Why needed here: The self-supervised PRM is core to providing step-wise rewards; understanding what PRMs do (score intermediate reasoning steps vs. outcome-only reward models) clarifies the system's feedback mechanism.
  - Quick check question: What is the difference between an outcome reward model (ORM) and a process reward model (PRM) in terms of when and how they provide feedback?

- Concept: Monte Carlo estimation for credit assignment
  - Why needed here: The simplified PRM training replaces expensive Monte Carlo simulation with binary labels; understanding the original MC approach illuminates why the simplification is non-trivial.
  - Quick check question: In Monte Carlo estimation for step quality, why does estimating step correctness require simulating multiple future completions rather than just checking the current step's content?

## Architecture Onboarding

- Component map: Base model → PRM Training Module (M=32 solutions, N=1 labels) → Preference Builder (average reward ranking, T=4 selection) → Policy Optimizer (step-wise DPO with reward weighting) → Fine-tuned policy model
- Critical path: PRM training → Preference data construction → Step-wise DPO training. The PRM quality directly affects both preference pair quality and gradient weighting; errors propagate through the entire pipeline.
- Design tradeoffs:
  - **M vs N**: Larger M (sampling) increases data diversity; larger N (simulation) increases label precision. Paper shows M=32, N=1 is practical sweet spot.
  - **γ (reward temperature)**: Controls trust in PRM. γ=0.5 works best; lower dilutes PRM signal, higher over-commits to single highest/lowest rewarded steps.
  - **β (DPO regularization)**: Controls deviation from reference model. Paper uses 0.05; standard DPO range applies.
  - **Decoding strategy**: Self-Consistency outperforms Best-of-N for strong base models; SBS underperforms both but beats greedy.
- Failure signatures:
  1. **DPO instability on Qwen2-7B-SFT**: Accuracy drops from 53.9%→20.0% on MATH (Table 1). Solution: Full-Step-DPO restores stability.
  2. **PRM overfitting to base model quirks**: If base model generates systematically biased solutions, PRM learns spurious patterns. Monitor via held-out validation.
  3. **γ misconfiguration**: Figure 5 shows sharp performance drop at γ=0 (vanilla DPO) and γ=1 (near Step-DPO); validate γ sweep on validation set.
- First 3 experiments:
  1. **PRM quality ablation**: Train PRMs with N∈{1,2,4,8} on a subset, evaluate downstream policy performance and PRM accuracy on held-out steps. Confirms N=1 efficiency claim.
  2. **γ sensitivity sweep**: Train policies with γ∈{0, 0.25, 0.5, 0.75, 1.0} on a single backbone (e.g., MetaMath-Mistral-7B). Plot accuracy vs γ to identify optimal region.
  3. **Preference pair construction variants**: Compare (a) average reward ranking, (b) minimum step reward ranking, (c) random pair selection. Isolates contribution of the selection strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating generative reward models into the Full-Step-DPO framework yield superior reasoning capabilities compared to the current discriminative Process Reward Model (PRM)?
- Basis in paper: [explicit] The authors state in the Limitations section: "Recent advancements suggest that generative reward models outperform the discriminative reward model used in this work. Exploring how generative reward models can further enhance mathematical reasoning capabilities would be a valuable direction for future research."
- Why unresolved: The current implementation relies on a discriminative PRM trained with binary cross-entropy. It is unknown if the mechanisms for step-wise gradient weighting proposed in the paper interact effectively with the outputs or hidden states of generative reward models.
- What evidence would resolve it: A comparative study replacing the discriminative PRM with a generative reward model (e.g., a generative verifier) and evaluating performance on the same mathematical benchmarks (GSM8K, MATH).

### Open Question 2
- Question: Does the Full-Step-DPO loss function generalize effectively to reasoning-intensive tasks with different structural constraints, such as code generation?
- Basis in paper: [explicit] The authors note: "The step-wise DPO loss proposed in this paper is highly adaptable to other reasoning tasks, such as code generation. Conducting experiments on a broader range of tasks would provide additional evidence of the advantages of our method."
- Why unresolved: The method is specifically tailored for mathematical reasoning where "steps" are relatively discrete. Code generation involves logic that may not decompose into steps that benefit identically from reward-weighted gradient updates.
- What evidence would resolve it: Applying Full-Step-DPO to code generation benchmarks (e.g., HumanEval, MBPP) and comparing the results against standard DPO and Step-DPO baselines.

### Open Question 3
- Question: Can more advanced sample selection strategies for preference pairs improve performance over the current method of selecting pairs based on average step-wise rewards?
- Basis in paper: [explicit] The paper mentions: "During preference data construction, the current strategy of selecting samples based on average reward is relatively simple. Investigating more advanced sample selection strategies may lead to further improvements."
- Why unresolved: Averaging rewards might obscure critical errors in long chains (e.g., a single fatal error might be averaged out by many correct steps), leading to noisy preference pairs.
- What evidence would resolve it: An ablation study comparing the current average-reward selection against strategies like minimum-reward selection or diversity-based sampling to measure the impact on optimization stability and final accuracy.

### Open Question 4
- Question: How can the verification strategy be adapted to maintain effectiveness for "very strong" baseline models where Best-of-N (BoN) currently underperforms Self-Consistency (SC)?
- Basis in paper: [inferred] In Section 4.4, the authors observe that "BoN underperforms SC... indicating that the benefits of the reward model diminish for very strong baseline models."
- Why unresolved: The paper demonstrates success on standard models, but the inferred limitation is that the PRM's guidance becomes less reliable or necessary as the base model's capability increases, potentially causing the verifier to misrank high-quality solutions.
- What evidence would resolve it: An analysis of PRM scoring distributions on high-capability models (e.g., Qwen2-72B) and the development of a hybrid or calibrated decoding strategy that outperforms SC on these stronger backbones.

## Limitations

- The PRM approach relies on the assumption that final answer correctness accurately reflects intermediate step quality, which may break down for problems where a single incorrect step doesn't prevent reaching the correct answer
- The N=1 simplification, while computationally efficient, may miss nuanced step-wise credit assignment that more extensive Monte Carlo simulation would capture
- Performance appears sensitive to hyperparameter tuning (γ, β, M, T), with optimal values potentially varying across model scales and problem domains

## Confidence

- **High confidence** in the core observation that Step-DPO's single-step optimization is suboptimal and that incorporating more step information can improve performance, supported by consistent empirical gains across four backbone models and multiple benchmarks
- **Medium confidence** in the PRM simplification (N=1 with M=32) and its efficiency claims, as the paper provides runtime data but limited analysis of potential quality trade-offs or scenarios where this simplification might fail
- **Medium confidence** in the mechanism connecting step-wise reward weighting to improved reasoning, as the ablation studies show performance differences but don't fully isolate the contribution of the dynamic weighting scheme versus simply incorporating more step information

## Next Checks

1. **Robustness to base model bias**: Train Full-Step-DPO on a base model with known systematic reasoning biases (e.g., one that consistently misorders operations) and evaluate whether the PRM learns to correctly identify flawed steps or simply reinforces the bias. This would validate the PRM's ability to detect genuine reasoning errors versus model-specific quirks.

2. **Zero-shot generalization stress test**: Evaluate Full-Step-DPO on mathematical domains substantially different from training data (e.g., geometry if trained on arithmetic) while varying the base model's zero-shot performance. This would reveal whether improvements stem from genuine reasoning capability versus pattern matching on familiar problem types.

3. **Human-annotated step-wise quality evaluation**: For a small subset of problems, have human experts annotate step correctness and compare PRM-assigned rewards against ground truth. Calculate PRM accuracy and analyze correlation between reward magnitude and actual step quality to validate whether the PRM provides meaningful signals for gradient weighting.