---
ver: rpa2
title: 'Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing
  and Width Reduction'
arxiv_id: '2512.01059'
source_url: https://arxiv.org/abs/2512.01059
tags:
- accuracy
- baseline
- parameter
- parameters
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether reducing MLP capacity in ViT-B/16
  can improve accuracy and training stability on ImageNet-1K. Two parameter-reduction
  strategies are proposed: GroupedMLP, which shares MLP weights between adjacent transformer
  blocks, and ShallowMLP, which halves the MLP hidden dimension.'
---

# Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction

## Quick Facts
- arXiv ID: 2512.01059
- Source URL: https://arxiv.org/abs/2512.01059
- Authors: Anantha Padmanaban Krishna Kumar
- Reference count: 4
- One-line primary result: Reducing ViT-B/16 MLP capacity by 32.7% improves ImageNet-1K accuracy and training stability.

## Executive Summary
This study investigates whether reducing MLP capacity in ViT-B/16 can improve accuracy and training stability on ImageNet-1K. Two parameter-reduction strategies are proposed: GroupedMLP, which shares MLP weights between adjacent transformer blocks, and ShallowMLP, which halves the MLP hidden dimension. Both variants remove 32.7% of baseline parameters (from 86.6M to 58.2M) yet achieve slightly higher accuracy (81.47% and 81.25% top-1 vs 81.05% baseline). Training stability improves markedly, with peak-to-final accuracy degradation dropping from 0.47% to 0.03%-0.06%. GroupedMLP maintains baseline compute cost (16.9 GFLOPs) while reducing memory, and ShallowMLP reduces FLOPs to 11.3 (38% higher throughput). The results suggest that ViT-B/16 is overparameterized in its MLPs, and architectural constraints like parameter sharing or reduced width can serve as useful inductive biases, improving both efficiency and optimization dynamics.

## Method Summary
The authors modify ViT-B/16 by reducing MLP capacity through two strategies: GroupedMLP shares MLP weights between adjacent transformer blocks (reducing 12 unique MLPs to 6) with 1/√2 initialization scaling, while ShallowMLP halves the MLP hidden dimension from 3072 to 1536. Both variants are initialized from the full ViT-B/16 before modification, preserving initialization statistics. Training uses 300 epochs with AdamW, cosine learning rate schedule, large batch size (1024), and extensive augmentation including MixUp, CutMix, and RandAugment. The reduced-capacity models achieve slightly higher accuracy while substantially improving training stability, as measured by peak-to-final accuracy degradation.

## Key Results
- GroupedMLP (81.47% top-1) and ShallowMLP (81.25% top-1) both outperform baseline ViT-B/16 (81.05%) despite removing 32.7% of parameters
- Peak-to-final accuracy degradation drops from 0.47% (baseline) to 0.03%-0.06% (reduced models), indicating improved training stability
- GroupedMLP maintains baseline FLOPs (16.9 GFLOPs) while reducing memory 4.3%, ShallowMLP reduces FLOPs to 11.3 (38% higher throughput)

## Why This Works (Mechanism)

### Mechanism 1: Parameter Sharing as Implicit Regularization
Parameter sharing between adjacent MLP blocks acts as an implicit regularizer by forcing the same weights to serve multiple depths, discouraging layer-specific overfitting and encouraging more generally useful features across transformer blocks. This works because adjacent transformer blocks perform sufficiently similar computations that shared weights can serve both without capacity loss. The evidence shows GroupedMLP reducing 12 unique MLPs to 6 while maintaining accuracy, and related work on ALBERT demonstrates similar gains in NLP. However, if tasks require highly specialized layer-wise representations, sharing may degrade specialized feature extraction.

### Mechanism 2: Initialization Statistics Matter More Than Parameter Count
Both variants initialize from full ViT-B/16 before modification, preserving the variance and structure of the overparameterized starting point. The optimization trajectory benefits from initialization scale/structure developed for larger capacity, even when capacity is reduced. Evidence shows both models initialized from full ViT-B/16 before slicing/weight sharing, with the authors suggesting initialization scale and structure may be important for optimization, potentially more so than eventual parameter count. The break condition is that random initialization at reduced size may not reproduce gains, making initialization ablation critical.

### Mechanism 3: Capacity Constraints Guide Optimization to Flatter Minima
Reduced MLP capacity limits expressivity that could enable sharp minima and late-training overfitting, resulting in dramatically reduced peak-to-final accuracy degradation. The pronounced reduction in peak-to-final accuracy degradation (0.03-0.06% vs 0.47%) hints at different loss landscapes potentially corresponding to flatter minima, though confirming this requires more direct theoretical and empirical analysis. The break condition is that if stability gains disappear under different training durations, learning rates, or augmentation strengths, the mechanism may be training-recipe specific.

## Foundational Learning

- **Overparameterization regimes:** Understanding why removing 32.7% of parameters might improve rather than degrade performance requires grasping that ViT-B/16 on ImageNet-1K operates in an overparameterized regime where MLP capacity can be reduced without harm.
- **Peak-to-final accuracy degradation as stability metric:** This metric captures late-training regression that standard top-1 accuracy misses, and is central to the paper's stability claims. Continued training past peak accuracy indicates potential optimization instability.
- **Inductive bias through architectural constraints:** Both variants use hard architectural constraints (sharing, width reduction) as regularization rather than soft penalties. Parameter sharing differs from weight decay or dropout by creating structural limitations on the model's capacity rather than penalizing specific weight values.

## Architecture Onboarding

- **Component map:** ViT-B/16 baseline (12 blocks, 4× expansion, 3072 hidden dim) -> GroupedMLP (shares MLPs in pairs, 1/√2 scaling) -> ShallowMLP (2× expansion, 1536 hidden dim)
- **Critical path:** 1) Initialize from pretrained/full ViT-B/16 weights, 2) Apply modification before training starts (slice MLP weights or duplicate and scale), 3) Train from modified initialization using standard recipe
- **Design tradeoffs:** GroupedMLP preserves FLOPs (16.9G), reduces memory 4.3%, same throughput (prefer when memory-bound); ShallowMLP reduces FLOPs (11.3G), 38% higher throughput, 29% less memory (prefer when latency/cost-bound); both achieve slight accuracy gain (~0.2-0.4%) and dramatically improved stability
- **Failure signatures:** Training instability in GroupedMLP likely means missing 1/√2 scaling; accuracy drop in ShallowMLP likely means using random init instead of slicing from full model; no stability improvement suggests augmentation recipe differs from paper
- **First 3 experiments:** 1) Reproduce baseline vs GroupedMLP vs ShallowMLP with identical seeds to verify patterns match paper, 2) Train ShallowMLP with sliced init vs random init to isolate initialization transfer contribution, 3) Test GroupedMLP with different group sizes (2, 3, 4 blocks per group) to find redundancy/expressivity boundary

## Open Questions the Paper Calls Out

- Do the accuracy and stability gains from MLP parameter reduction generalize to other ViT scales (e.g., ViT-Tiny, ViT-Large, ViT-Huge) and alternative architectures (e.g., DeiT, Swin)?
- What is the optimal degree of MLP parameter reduction as a function of model capacity, and does the 32.7% reduction rate transfer to larger or smaller models?
- How do parameter sharing and width reduction affect transfer learning and downstream tasks such as object detection and semantic segmentation?
- Do reduced-capacity models converge to genuinely flatter minima, and does this mechanistically explain the improved training stability?

## Limitations

- The stability improvements are interpreted as evidence of flatter minima, but this mechanism is not directly tested through sharpness metrics or loss landscape analysis
- The initialization-transfer hypothesis lacks ablation studies comparing reduced-size random initialization versus inheritance from full models
- Performance gains (~0.2-0.4%) are modest and may not generalize beyond ImageNet-1K classification
- Claims about flatter minima and loss landscape properties remain speculative without sharpness-aware minimization metrics

## Confidence

- **High confidence:** Baseline vs reduced-parameter accuracy comparison, FLOPs/memory reduction measurements, and training stability improvements are empirically demonstrated with proper ablations
- **Medium confidence:** The interpretation that parameter sharing acts as implicit regularization is plausible but under-validated; initialization inheritance is hypothesized without direct ablation
- **Low confidence:** Claims about flatter minima and loss landscape properties remain speculative without sharpness-aware minimization metrics or eigenvalue analysis

## Next Checks

1. Measure sharpness-aware minimization (SAM) scores or eigenvalue spectra for baseline vs reduced-parameter models to directly test flatness hypotheses
2. Train ShallowMLP with random initialization at reduced size versus weight-sliced initialization to isolate the initialization transfer contribution
3. Test GroupedMLP with varying group sizes (1-block, 2-block, 4-block sharing) to map the redundancy/expressivity boundary across different task complexities