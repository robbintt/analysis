---
ver: rpa2
title: 'TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability'
arxiv_id: '2505.12672'
source_url: https://arxiv.org/abs/2505.12672
tags:
- trajectory
- meters
- rmse
- spatial
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransferTraj, a model designed to learn vehicle
  trajectories with both region and task transferability. It tackles the challenges
  of varying spatial features across regions and differing input-output structures
  across tasks.
---

# TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability

## Quick Facts
- **arXiv ID:** 2505.12672
- **Source URL:** https://arxiv.org/abs/2505.12672
- **Reference count:** 40
- **Primary result:** TransferTraj outperforms SOTA baselines by 7.94%-20.18% across tasks and achieves 83.70% and 33.68% average improvements in zero-shot and few-shot region transferability.

## Executive Summary
This paper introduces TransferTraj, a model designed to learn vehicle trajectories with both region and task transferability. It tackles the challenges of varying spatial features across regions and differing input-output structures across tasks. The core innovation is a Region-Transferable Trajectory Encoder (RTTE) that uses a Trajectory Relative Information Extraction (TRIE) module and a Spatial Context Mixture-of-Experts (SC-MoE) module to handle regional differences. A unified task-transferable input-output scheme allows the model to be pre-trained once and adapted to different tasks without retraining. Experiments on three real-world datasets show that TransferTraj significantly outperforms state-of-the-art baselines across trajectory prediction, recovery, and travel time estimation tasks.

## Method Summary
TransferTraj addresses region and task transferability through a unified framework. The model pre-trains on trajectories using a masking and recovery scheme that unifies diverse tasks as modality reconstruction problems. The Region-Transferable Trajectory Encoder (RTTE) consists of two key modules: TRIE, which encodes relative spatial information using rotary embeddings to capture region-agnostic movement patterns, and SC-MoE, which routes inputs to specialized experts based on local spatial context (POI/road density). This architecture enables zero-shot and few-shot transfer to new regions and tasks without retraining the core encoder.

## Key Results
- Outperforms state-of-the-art baselines by 7.94% to 20.18% across trajectory prediction, recovery, and OD travel time estimation tasks
- Achieves 83.70% average improvement in zero-shot region transferability
- Achieves 33.68% average improvement in few-shot region transferability
- Ablation studies confirm TRIE and SC-MoE are critical components

## Why This Works (Mechanism)

### Mechanism 1: TRIE — Rotary Embeddings for Relative Spatial Generalization
Encoding relative spatial distances between trajectory points produces region-agnostic representations that transfer better than absolute coordinate systems. TRIE applies learnable spatio-temporal rotation matrices RΦ(x,y) to attention queries and keys, ensuring attention scores depend on relative positions rather than absolute locations. This prevents the model from overfitting to region-specific coordinate ranges.

### Mechanism 2: SC-MoE — Spatial-Context-Conditioned Expert Routing
Movement patterns vary systematically with local spatial context (POI/road density), and routing to specialized experts enables capturing these patterns without region-specific tuning. A gating network computes soft routing weights over experts using noisy top-K selection, with local context implicitly determining which experts activate.

### Mechanism 3: Unified Masking-Recovery Scheme for Task Transfer
Diverse trajectory generation tasks can be unified as masked modality/point recovery problems, enabling single-pretraining adaptation. During pre-training, the model randomly masks spatial modality, temporal modality, or entire trajectory sub-sequences, learning to reconstruct masked elements via MSE loss.

## Foundational Learning

- **Rotary Position Embeddings (RoPE)**
  - **Why needed here:** TRIE extends RoPE from 1D sequence positions to 2D spatial coordinates. Understanding how rotation matrices encode relative positions is essential for debugging attention behavior.
  - **Quick check question:** Given two points with rotation matrices RΦ(xi,yi) and RΦ(xj,yj), can you derive why their product yields RΦ(xi-xj, yi-yj)?

- **Mixture of Experts with Sparse Gating**
  - **Why needed here:** SC-MoE uses noisy top-K gating. Without understanding expert collapse and load balancing, you cannot diagnose why certain experts never activate.
  - **Quick check question:** What happens to gradient flow if all inputs are routed to the same expert? How does noise in gating mitigate this?

- **Masked Autoencoder Pre-training**
  - **Why needed here:** The entire task-transfer scheme is built on masked reconstruction. Understanding what features emerge from this objective helps predict which downstream tasks will transfer well.
  - **Quick check question:** If you mask 90% of trajectory points versus 10%, how does the learned representation differ? Which creates more robust transfer?

## Architecture Onboarding

**Component map:**
Input Trajectory → Point Modality Extraction (Spatial, Temporal, POI, Road) → Modality Mixing Transformer → [TRIE Block → SC-MoE Block] × L layers → Trajectory Modality Predictor → (Spatial coords, Temporal features)

**Critical path:**
1. Relative coordinate computation (xi, yi) = (lngi - lng1, lati - lat1) — errors here propagate to all downstream attention
2. TRIE rotation matrix construction — dimension mismatch or frequency miscalibration breaks relative encoding
3. SC-MoE gating softmax — if all gates collapse to one expert, model degenerates to single-expert baseline
4. Masked modality prediction — loss weighting between spatial (MSE on meters) and temporal (MSE on minutes) must be balanced

**Design tradeoffs:**
- **L (stacking depth):** Paper finds L=2 optimal; deeper models overfit to training region
- **C (number of experts):** C=8 with k=4 active experts balances specialization vs training stability
- **d (hidden dimension):** d=128 optimal; larger values increase compute without proportional gains
- **Modality inclusion:** POI and road modalities add ~5-10% improvement but require external API calls for inference

**Failure signatures:**
- **TRIE failure:** Ablation shows 19-34% performance drop; symptom is poor cross-region generalization despite good in-region performance
- **SC-MoE failure:** Expert activation distribution becomes uniform across all contexts (check Figure 2-style histograms)
- **Task transfer failure:** Zero-shot performance (wo ft) close to random; indicates pre-training mask ratio or coverage insufficient
- **Region transfer failure:** Model performs well on source region but near-random on target; check if relative coordinates are being accidentally normalized to region-specific scales

**First 3 experiments:**
1. **TRIE ablation:** Replace TRIE with standard Transformer positional encoding; measure transfer gap. Expected: significant degradation in zero-shot region transfer.
2. **Expert activation analysis:** Visualize gating distributions across POI/road density buckets for source and target regions. Expected: similar distributions if transfer succeeds; divergent if fails.
3. **Mask ratio sensitivity:** Vary the proportion of masked modalities vs masked points during pre-training. Identify which masking strategy most benefits each downstream task (TP, TR, OD-TTE).

## Open Questions the Paper Calls Out

- **Cross-region classification extension:** The current work faces challenges in cross-region classification tasks, such as trajectory-user linking or destination road segment prediction, due to varying numbers of users and road networks across regions. The unified input-output scheme relying on masking and recovering continuous modalities does not inherently map to discrete, region-specific label sets.

- **Contextual data poverty:** The model's region transferability when target regions lack high-quality POI and road network context data available in the source region. The effectiveness of SC-MoE routing relies on similar POI/road density distributions across regions, which may not hold when target regions have sparse or missing contextual data.

- **Semantic vs density-based expert routing:** Whether SC-MoE relies too heavily on density as a proxy for movement patterns, potentially confusing regions with similar density but different functional semantics. Without analyzing activation patterns against semantic labels like land use types, it's unclear if the model captures complex movement patterns or just density-based speed adjustments.

## Limitations
- Lack of open-source dataset access, particularly for Xi'an and Chengdu datasets
- Evaluation focuses on Chinese cities and Porto, potentially limiting generalizability
- Reliance on external APIs for POI and road network data introduces cost and reproducibility issues

## Confidence

- **High Confidence:**
  - The unified masking-recovery pre-training scheme enables task transferability
  - The 7.94-20.18% improvement over SOTA baselines is empirically demonstrated
  - The 83.70% and 33.68% average improvements in zero-shot and few-shot transfer are reproducible

- **Medium Confidence:**
  - TRIE's rotary embeddings are the primary driver of region transferability
  - SC-MoE routing based on POI/road density is the optimal expert conditioning strategy

- **Low Confidence:**
  - The specific architectural choices (L=2, C=8, d=128) are universally optimal

## Next Checks

1. **Ablation of TRIE's Relative Encoding:** Replace TRIE with absolute coordinate positional encoding and measure the degradation in cross-region performance to directly test whether relative encoding is essential for transfer.

2. **Expert Activation Distribution Analysis:** Visualize and compare the gating distributions across POI/road density buckets between source and target regions to validate whether similar spatial contexts share similar expert activation patterns during transfer.

3. **Mask Ratio Sensitivity Test:** Systematically vary the proportion of masked modalities versus masked trajectory points during pre-training to identify which masking strategy most benefits each downstream task and determine if the current 50-50 split is optimal.