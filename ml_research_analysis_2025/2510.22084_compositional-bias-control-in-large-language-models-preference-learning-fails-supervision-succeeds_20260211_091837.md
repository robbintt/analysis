---
ver: rpa2
title: 'Compositional Bias Control in Large Language Models: Preference Learning Fails,
  Supervision Succeeds'
arxiv_id: '2510.22084'
source_url: https://arxiv.org/abs/2510.22084
tags:
- fluency
- preference
- bias
- compliance
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses gender stereotyping in LLM outputs by testing
  how different bias-control methods handle compositional constraints requiring both
  agentic and communal descriptors. Six methods were compared: prompt-only, generate-and-filter,
  DFA-based Ctrl-G decoding, Supervised Fine-Tuning (SFT), Direct Preference Optimization
  (DPO), and Iterative Nullspace Projection (INLP).'
---

# Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds

## Quick Facts
- arXiv ID: 2510.22084
- Source URL: https://arxiv.org/abs/2510.22084
- Authors: Atij Mahesh
- Reference count: 7
- Primary result: Supervised Fine-Tuning achieves 99.87% compliance with compositional gender fairness constraints; Direct Preference Optimization catastrophically fails at 4.53%

## Executive Summary
This paper investigates methods for controlling gender stereotyping in LLM outputs when compositional constraints require both agentic and communal descriptors. Six approaches were compared: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Iterative Nullspace Projection (INLP). The task required generating sentences containing at least one agentic and one communal term for each of twenty occupations. Results show that SFT achieved exceptional compliance (99.87%) with high lexical diversity and moderate fluency, while DPO catastrophically failed despite stable training. The study concludes that preference-based learning cannot encode logical conjunctions, making explicit supervision necessary for compositional fairness.

## Method Summary
The study compared six bias-control methods on generating occupation descriptions requiring both agentic and communal traits. SFT fine-tuned LLaMA-3.1 8B using 750 examples from 18 syntactic templates. DPO used the same base model with 750 preference pairs (balanced vs unbalanced). Ctrl-G employed GPT-2-Large with DFA decoding for constraint satisfaction. INLP projected out gender subspace from LLaMA-3.3 70B. Generate-and-filter used 100 samples per occupation with OR-compliance filtering. All methods were evaluated on 20 occupations (15 train, 5 held-out) using AND-compliance (requiring both trait types), perplexity, and lexical diversity metrics.

## Key Results
- SFT achieved 99.87% AND-compliance with high lexical diversity (33 unique agentic-communal pairs) and moderate fluency (13.27 perplexity)
- DPO catastrophically failed at 4.53% AND-compliance despite stable training and high fluency
- Ctrl-G guaranteed perfect AND-compliance but severely reduced diversity (13 unique pairs) and fluency (16.23 perplexity)
- INLP over-corrected, producing 95% trait-free outputs with only 0.09% AND-compliance

## Why This Works (Mechanism)

Preference learning methods like DPO cannot encode logical conjunctions because they optimize for relative preference rankings rather than absolute constraint satisfaction. The model learns to balance agentic and communal traits probabilistically rather than ensuring their simultaneous presence. Supervised learning with explicit examples directly teaches the conjunction through demonstration. DFA-based decoding explicitly enforces constraints through state transitions, guaranteeing satisfaction but limiting exploration. INLP removes gender associations but cannot control their reintroduction in balanced ways.

## Foundational Learning

**Agentic vs Communal Traits**: Why needed: Basis for gender bias measurement. Quick check: Verify trait lexicons contain 10 terms each with clear semantic distinction.

**Compositional Constraints**: Why needed: Captures real-world fairness requirements (must satisfy multiple conditions). Quick check: Test with simple AND/OR logic on toy examples before full task.

**Logical Conjunction vs Disjunction**: Why needed: Distinguishes between "either trait" (OR) and "both traits" (AND) requirements. Quick check: Generate examples showing 30-35% OR-compliance but near-0% AND-compliance indicates conjunction problem.

**Preference Optimization Limitations**: Why needed: Explains DPO's catastrophic failure despite stable training. Quick check: Monitor training curves for stable loss with flat compliance to identify this failure mode.

**Constraint Satisfaction via DFA**: Why needed: Guarantees AND-compliance through explicit state machine. Quick check: Verify DFA has accepting states only when both trait types generated.

**Over-correction in Bias Removal**: Why needed: Explains INLP's tendency to eliminate traits entirely. Quick check: Track trait frequency vs projection iterations to find optimal stopping point.

## Architecture Onboarding

**Component Map**: Data Generation -> Model Training -> Constraint Enforcement -> Evaluation Pipeline

**Critical Path**: SFT: Template-based data -> LoRA fine-tuning -> AND-compliant generation -> Perplexity/Diversity evaluation
DPO: Preference pair construction -> RLHF-style optimization -> Generated samples -> AND-compliance testing
Ctrl-G: DFA construction -> Beam search with constraints -> Guaranteed AND-compliant output -> Diversity measurement

**Design Tradeoffs**: SFT balances compliance (99.87%) with diversity (33 pairs) but requires labeled data. DPO offers high fluency but fails compositionality. Ctrl-G guarantees constraints but sacrifices diversity and fluency. INLP removes bias but overcorrects to trait elimination.

**Failure Signatures**: DPO: Stable training loss with <5% AND-compliance indicates preference learning cannot encode conjunctions. Ctrl-G: Perfect compliance with entropy gap between agentic (low) and communal (high) traits indicates beam search exploitation of specific terms. INLP: >90% trait-free outputs with near-0% AND-compliance indicates over-correction.

**First Experiments**: 1) Test SFT reconstruction with simplified templates on held-out occupations. 2) Compare DPO performance on single-trait vs dual-trait constraints. 3) Vary Ctrl-G beam width to map diversity-compliance trade-off curve.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing explicit SFT template specifications (only 3 examples provided of 18 used)
- DPO rejected examples unspecified, making failure diagnosis incomplete
- Ctrl-G DFA implementation lacks detailed transition rules
- Single compositional constraint limits generalizability to other logical combinations

## Confidence

High confidence in SFT effectiveness (99.87% compliance, high diversity) due to clear methodology and consistent results. Medium confidence in DPO failure interpretationâ€”compliance is definitively low, but exact cause remains partially speculative. High confidence in Ctrl-G's compliance guarantee but low diversity/fluency trade-off, as beam search behavior is well-understood. Medium confidence in INLP's over-correction mechanism, as the general principle is sound but specific implementation details are missing.

## Next Checks

1. Reconstruct SFT templates from provided examples and verify that similar template-based training achieves comparable compliance (>99%) and diversity metrics on held-out occupations.

2. Test DPO with simplified preference pairs focusing on single trait selection (agentic-only or communal-only) to determine if the model can learn individual constraints before failing at their conjunction.

3. Implement Ctrl-G with variable beam widths (8, 16, 32) to empirically determine the trade-off curve between AND-compliance and lexical diversity, confirming whether wider beams recover diversity without sacrificing constraint satisfaction.