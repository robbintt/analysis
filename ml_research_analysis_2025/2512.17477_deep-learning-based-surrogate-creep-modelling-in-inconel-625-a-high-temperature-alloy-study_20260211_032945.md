---
ver: rpa2
title: 'Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature
  Alloy Study'
arxiv_id: '2512.17477'
source_url: https://arxiv.org/abs/2512.17477
tags:
- creep
- strain
- stress
- data
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces deep learning surrogate models to replace
  computationally expensive finite element method (FEM) simulations for predicting
  creep in Inconel 625. ANSYS simulations were used to generate 10,000-hour creep
  strain data under various stress and temperature conditions, which was then used
  to train two advanced models: a BiLSTM-VAE for uncertainty-aware, generative predictions
  and a BiLSTM-Transformer for high-accuracy deterministic forecasts.'
---

# Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study

## Quick Facts
- arXiv ID: 2512.17477
- Source URL: https://arxiv.org/abs/2512.17477
- Reference count: 24
- Deep learning surrogate models replace 30-40 minute ANSYS simulations with 47ms-2.17s predictions for Inconel 625 creep analysis

## Executive Summary
This study presents two deep learning architectures to accelerate creep strain prediction in Inconel 625, replacing computationally expensive finite element method simulations. The authors train BiLSTM-VAE and BiLSTM-Transformer models on 10,000-hour creep strain data generated via ANSYS simulations using the Norton creep law. Both models demonstrate superior performance compared to a baseline LSTM, achieving validation R² scores above 0.96 while reducing inference time from 30-40 minutes to seconds per sequence.

## Method Summary
The authors generated ANSYS simulation data for 20 stress-temperature combinations (4 temperatures × 5 stress levels) using the Norton creep law, producing 10,000-hour time-series creep strain data. After log-transforming strain and applying Min-Max normalization, they trained three models: a baseline LSTM, a BiLSTM-VAE for uncertainty quantification, and a BiLSTM-Transformer for high-accuracy deterministic predictions. The VAE achieved R²=0.973 with 46.8ms inference time, while the Transformer reached R²=0.961 with 2.17s inference time.

## Key Results
- BiLSTM-VAE achieved validation R² of 0.973, outperforming baseline LSTM (0.905)
- BiLSTM-Transformer achieved validation R² of 0.961 with superior long-range accuracy
- Inference latency reduced from 30-40 minutes (ANSYS) to 46.8ms (VAE) and 2.17s (Transformer)
- Both models successfully capture secondary creep behavior across multiple stress-temperature conditions

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Context for Multi-Stage Creep Evolution
BiLSTM layers capture both preceding and subsequent deformation patterns, improving prediction of creep stage transitions through forward and backward hidden states that process sequences in opposite directions.

### Mechanism 2: Self-Attention for Long-Range Temporal Dependencies
Transformer self-attention enables direct relationships between distant timesteps, capturing secondary-to-tertiary creep transitions across 10,000-hour sequences through multi-head attention that computes pairwise relationships.

### Mechanism 3: Latent Space Sampling for Uncertainty Quantification
VAE framework provides probabilistic predictions with confidence bounds by mapping sequences to latent mean/variance spaces, enabling differentiable sampling through reparameterization for safety-critical engineering decisions.

## Foundational Learning

- **Norton Creep Law**: Constitutive equation (ε̇ = Aσⁿe^(-Q/RT)) used to generate training data; understanding its limitations (secondary creep focus) contextualizes surrogate model scope. Quick check: Why does the Norton law underrepresent primary and tertiary creep stages?

- **Sequence-to-Sequence Modeling**: Both architectures treat creep prediction as seq2seq: input (stress, temperature, time) → output (strain at each timestep). Quick check: How does masking or padding affect training when sequences have variable lengths?

- **Reparameterization Trick**: Enables backpropagation through stochastic sampling in VAE by expressing z = μ + σ⊙ε where ε ~ N(0,1). Quick check: What happens to gradient flow if you sample directly without reparameterization?

## Architecture Onboarding

- **Component map**: Input (batch, L=10000, features=3) → Feature attention → BiLSTM (H=32, bidirectional=64) → Positional encoding → Transformer encoder (1 layer, 4 heads) → Linear output

- **Critical path**: Generate ANSYS data (30-40 min/simulation) → Log-transform strain → MinMax normalize features → Train on 16 sequences, validate on 4 held-out pairs → Inference: 46.8ms (VAE) or 2.17s (Transformer) per sequence

- **Design tradeoffs**: Transformer offers higher accuracy for long-range dependencies but slower inference (2.17s vs 47ms); VAE provides uncertainty quantification with slightly better R² but deterministic-only predictions

- **Failure signatures**: Overfitting to training pairs; VAE posterior collapse (KL→0); exploding gradients in long sequences without clipping

- **First 3 experiments**: 1) Train unidirectional LSTM to isolate bidirectional benefit; 2) Test latent dimension sweep Z ∈ {5,10,20,40} in VAE; 3) Extract attention weights to verify long-range dependency capture

## Open Questions the Paper Calls Out

1. **Can models maintain accuracy for primary and tertiary creep stages?** The current dataset focuses on secondary creep using Norton law; incorporating multi-stage creep data is proposed for future investigation.

2. **Do models retain accuracy with experimental data?** Validation against laboratory creep tests is needed to bridge simulation-experiment gaps and account for microstructural variability.

3. **Does physics-informed integration improve multi-axial generalization?** Embedding mechanical equilibrium or constitutive invariants could enhance performance for complex 3D geometries beyond uniaxial training data.

4. **How robust is VAE uncertainty for extrapolation?** Uncertainty quantification reliability beyond the 50-150 MPa and 700-1000°C training bounds remains untested.

## Limitations

- Norton creep law inadequately represents primary and tertiary creep dynamics, particularly near rupture conditions
- Single train/validation split raises overfitting concerns despite strong performance metrics
- Time step resolution unspecified, potentially affecting model capacity requirements
- VAE latent space may not capture genuine microstructural variability

## Confidence

**High Confidence Claims:**
- Deep learning models effectively replace FEM simulations for creep prediction
- Both models outperform baseline LSTM (R² 0.973 vs 0.905 for VAE)
- Inference latency reduction from 30-40 minutes to seconds is reliably achieved

**Medium Confidence Claims:**
- BiLSTM layers provide meaningful bidirectional context for creep evolution
- Transformer self-attention captures long-range temporal dependencies
- VAE framework provides reliable uncertainty quantification

**Low Confidence Claims:**
- Models generalize well beyond the 16 training pairs
- Latent representations capture genuine microstructural variability
- Performance maintained with real experimental data

## Next Checks

1. **Cross-validation robustness test**: Implement k-fold cross-validation (k=5) across all 20 stress-temperature combinations to assess generalization, reporting mean and standard deviation of R² across folds.

2. **Real data validation**: Compare model predictions against experimental creep data for Inconel 625 under matching conditions to validate whether synthetic ANSYS data accurately represents real material behavior.

3. **Latent space interpretability analysis**: Perform t-SNE or PCA visualization of VAE latent space to verify meaningful clustering by stress-temperature conditions and test whether interpolation produces physically plausible creep curves.