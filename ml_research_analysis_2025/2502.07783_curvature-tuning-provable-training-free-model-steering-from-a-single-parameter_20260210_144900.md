---
ver: rpa2
title: 'Curvature Tuning: Provable Training-free Model Steering From a Single Parameter'
arxiv_id: '2502.07783'
source_url: https://arxiv.org/abs/2502.07783
tags:
- lora
- self
- t-ct
- should
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Curvature Tuning (CT) is a training-free model steering method\
  \ that modulates a pretrained model\u2019s decision boundary curvature by injecting\
  \ a single hyperparameter \u03B2 into activation functions. Grounded in spline theory,\
  \ CT provably adjusts curvature and projects the model onto a space of smooth functions,\
  \ complementing existing weight-based finetuning methods."
---

# Curvature Tuning: Provable Training-free Model Steering From a Single Parameter

## Quick Facts
- **arXiv ID:** 2502.07783
- **Source URL:** https://arxiv.org/abs/2502.07783
- **Reference count:** 40
- **One-line primary result:** Curvature Tuning (CT) is a training-free model steering method that modulates a pretrained model’s decision boundary curvature by injecting a single hyperparameter β into activation functions.

## Executive Summary
Curvature Tuning (CT) is a training-free method for steering pretrained models by modulating activation function curvature via a single hyperparameter β. Grounded in spline theory, CT provably adjusts the curvature of decision boundaries and projects the model onto a space of smooth functions. The method introduces two variants: Steering CT (S-CT), which uses a fixed β and requires no training, and Trainable CT (T-CT), where β is learned during finetuning. Experiments show S-CT improves downstream generalization by up to 1.97% compared to linear probing, while T-CT achieves comparable performance to LoRA with only 0.58–59.09% of its parameters. CT also enhances model robustness, increasing ℓ∞ robust accuracy by up to 1494.46%.

## Method Summary
Curvature Tuning (CT) modifies pretrained models by replacing standard activations (e.g., ReLU, GELU) with a parameterized Curvature Tuning Unit (CTU). The CTU activation φβ,c(x) is a convex combination of reparameterized SiLU and Softplus functions, controlled by hyperparameters β (curvature) and c (mixing coefficient). For S-CT, a fixed β is selected via grid search while the backbone remains frozen, requiring only classifier training. For T-CT, β and c are learned per neuron during finetuning, functioning as a parameter-efficient alternative to methods like LoRA. The approach is theoretically grounded in spline theory, showing that CT provably modulates decision boundary curvature and projects the network onto smooth function spaces.

## Key Results
- S-CT improves downstream generalization by up to 1.97% on 12 datasets compared to linear probing
- T-CT achieves comparable performance to LoRA with only 0.58–59.09% of its parameters
- CT enhances model robustness, increasing ℓ∞ robust accuracy by up to 1494.46%
- Optimal β values for robustness are often close to 1, suggesting modest curvature modulation suffices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Varying β in the CT Unit (CTU) activation provably modulates the curvature of the model's decision boundaries.
- **Mechanism:** The CTU activation φβ,c(x) is a convex combination of reparameterized SiLU and Softplus functions. As β moves from 1 (recovering ReLU) toward 0, the activation smoothly transitions from piecewise-linear to smoother nonlinearity, controlling the Hessian norm (curvature) of the decision boundary function g(x) = f_i(x) – f_j(x).
- **Core assumption:** The network is composed of affine layers interleaved with CTU-modifiable activations, allowing analysis as an Affine Spline Operator (ASO).
- **Evidence anchors:** [abstract] "CT provably adjusts curvature..." [section 3.3] "modulating the nonlinearity of activation functions via β directly controls the curvature..." [corpus] Related work on activation steering confirms modifying activations controls model behavior, but lacks direct theoretical guarantees on curvature modulation.
- **Break condition:** If a network uses non-modifiable activations (e.g., hardcoded GELU in attention blocks), the mechanism cannot be applied in those layers.

### Mechanism 2
- **Claim:** Replacing ReLU with CTU (for 0 ≤ β < 1) is equivalent to projecting the network onto a space of smooth functions with bounded gradients and non-vanishing curvature.
- **Mechanism:** Formalized via Theorem C.1, showing the resulting function fβ,c belongs to the Sobolev space W2,2(Ω) of smooth functions with bounded Sobolev semi-norm. For β < 1, the Hessian norm is strictly larger than the original ReLU network (zero curvature almost everywhere), implying higher local expressivity.
- **Core assumption:** The original network is a ReLU network (piecewise affine), and the projection holds for fixed (β, c) with a finite discrete dataset D.
- **Evidence anchors:** [abstract] "...projects a model onto a space of smooth functions..." [section 3.3 / Theorem C.1] "replacing every instance of ReLU with a CTU... is equivalent to projecting f to a smooth function fβ,c ∈ W2,2(Ω)..." [corpus] No direct corpus support for this specific projection theorem; it is a novel theoretical contribution.
- **Break condition:** The projection characterization is proven for fixed β and c. For T-CT, where β and c are learned, the resulting optimization problem is non-convex, and the projection interpretation is not formally extended.

### Mechanism 3
- **Claim:** CT exhibits an implicit bias towards improving model robustness, particularly against ℓ∞ adversarial attacks.
- **Mechanism:** Hypothesized to be linked to curvature modulation. Smoother decision boundaries (controlled by β) may reduce sensitivity to small, worst-case input perturbations characteristic of ℓ∞ attacks. Empirical observation shows optimal β for robustness are often close to 1.
- **Core assumption:** The relationship between decision boundary curvature and robustness to specific adversarial threat models holds.
- **Evidence anchors:** [abstract] "CT also enhances model robustness, increasing ℓ∞ robust accuracy by up to 1494.46%." [section 4.3] "S-CT yields substantial improvements under ℓ∞ attacks... The corresponding optimal β values... are consistently close to 1..." [corpus] Corpus neighbors discuss adaptation and parameter dynamics but do not specifically address robustness from activation curvature.
- **Break condition:** Robustness gains are demonstrated on CNNs (ResNets) and specific benchmarks. On architectures where CT is less effective (e.g., Swin Transformers), robustness improvements may not materialize.

## Foundational Learning

**Concept: Affine Spline Theory of Deep Networks**
- **Why needed here:** This is the core theoretical lens. It states that networks with ReLU-like activations are piecewise-affine functions, partitioning the input space into regions with different linear mappings. CT operates by smoothing the transitions between these regions.
- **Quick check question:** Can you explain how a ReLU network's output can be described as a set of different affine functions applied in different parts of the input space?

**Concept: Sobolev Spaces and Function Smoothness**
- **Why needed here:** To understand the formal "projection" claim. The Sobolev semi-norm combines a function's value, gradient, and Hessian. CT guarantees the resulting function lives in a space where these are bounded, meaning it's smooth.
- **Quick check question:** What does the W2,2(Ω) Sobolev space require of a function, and how does it relate to a network's "curvature"?

**Concept: Parameter-Efficient Fine-Tuning (PEFT)**
- **Why needed here:** To position CT among existing methods like LoRA. The paper argues CT is a fundamentally different approach (function-space projection) that complements weight-adaptation methods (feature-space adaptation).
- **Quick check question:** How does changing an activation function (CT) differ fundamentally from adding low-rank weight matrices (LoRA) in terms of what aspect of the model is being adapted?

## Architecture Onboarding

**Component map:** Pretrained model -> CTU activation replacement -> (S-CT: fixed β grid search + frozen backbone + classifier training) OR (T-CT: trainable β,c per neuron + finetuning + classifier training)

**Critical path:**
1. Load a pretrained model with ReLU (or ReLU-like) activations
2. Use provided `replace_module` (for S-CT) or `replace_module_dynamic` (for T-CT) functions to systematically swap out all target activations with CTU instances
3. For S-CT: Perform a grid search over β (e.g., [0.7, 1.0]) on a validation set, training only the linear classifier each time. Select the best β
4. For T-CT: Initialize β (e.g., to 0.8) and c (e.g., to 0.5) for all CTUs. Finetune these parameters and the classifier head using a higher learning rate for (β, c) than for the classifier

**Design tradeoffs:**
- **S-CT vs. T-CT:** S-CT is simpler, faster, and introduces truly zero trainable backbone parameters. T-CT is more expressive and can outperform S-CT but requires more parameters and training
- **Placement:** The paper replaces *all* ReLUs. For models with other activations (e.g., GELU in Swin Transformers), only the feed-forward blocks were modified, which led to weaker results. A key design decision is identifying all modifiable activation points
- **Parameter Efficiency:** T-CT introduces 2 parameters per output channel per replaced activation. Compare this count to LoRA's parameters (rank × (in_dim + out_dim)) to assess efficiency for your specific architecture

**Failure signatures:**
- **Performance drop after S-CT β search:** This may happen if the optimal β is at the boundary of the search range, or if the pretrained features are not well-aligned with the downstream task. The validation accuracy curve (Fig. S2) should be inspected for a clear peak
- **T-CT training instability:** The β and c parameters are constrained via a sigmoid, which can lead to vanishing gradients if initialized near saturation. The provided initialization (raw_beta=1.386 → β≈0.8) is designed to mitigate this
- **No robustness gain:** The paper's results are most pronounced for ℓ∞ robustness on CNNs. If working with a different architecture or threat model, robustness improvements are not guaranteed

**First 3 experiments:**
1. **Sanity Check (S-CT):** Take a small pretrained ResNet-18 and a simple downstream dataset (e.g., CIFAR-10 subset). Implement S-CT with a coarse β grid (e.g., [0.8, 0.9, 1.0]). Verify that you observe a smooth performance change and a peak different from β=1
2. **Ablation on Mixing Coefficient (c):** Using the setup from Experiment 1, compare S-CT performance with c=0 (Softplus), c=0.5 (paper's choice), and c=1 (SiLU). This validates the benefit of the combined parameterization
3. **Efficiency vs. LoRA (T-CT):** Implement a rank-1 LoRA and T-CT on the same backbone and dataset. Compare their final accuracy and the number of trainable parameters introduced. This establishes a baseline for the parameter-efficiency claim

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can Curvature Tuning be extended to directly modulate the curvature within attention mechanisms, for example by tuning the softmax temperature?
- **Basis in paper:** [explicit] The Conclusion states: "Extending CT to modulate the curvature within attention mechanisms—such as by tuning the temperature in the softmax of the attention block—is an interesting future work."
- **Why unresolved:** The current implementation applies CT only to feed-forward blocks (using GELU), which constitute a small portion of transformer architectures, leaving the dominant attention layers unmodified
- **What evidence would resolve it:** A modified T-CT implementation that includes a learnable temperature parameter in the attention softmax, demonstrating improved performance on transformer benchmarks compared to the current feed-forward-only approach

**Open Question 2**
- **Question:** Why does Trainable CT (T-CT) underperform LoRA on vision transformers (Swin-T/S) despite matching or exceeding it on ResNets?
- **Basis in paper:** [inferred] Section 4.4 notes T-CT underperforms LoRA on Swin models. The text attributes this to attention layers falling outside the max-affine spline framework and the use of GELU (which weakens theoretical guarantees) rather than ReLU
- **Why unresolved:** The paper acknowledges the theoretical mismatch but does not experimentally isolate whether the performance gap is due to the activation function type (GELU vs. ReLU) or the exclusion of attention layers
- **What evidence would resolve it:** An ablation study applying CT to attention layers or comparing CT performance on ReLU-based transformers vs. GELU-based transformers to isolate the source of the degradation

**Open Question 3**
- **Question:** Can the theoretical guarantees for Steering CT (projection to smooth functions) be formally extended to the non-convex optimization landscape of Trainable CT?
- **Basis in paper:** [explicit] Appendix C.1 states: "We leave for future work extending our result to T-CT, which is associated with a non-convex optimization problem of finding optimal (β, c) for every neuron in the network."
- **Why unresolved:** The paper proves that fixed β projects the network to a smooth function space, but allowing β to be learned per neuron invalidates the current projection proof due to the complexity of the resulting optimization landscape
- **What evidence would resolve it:** A formal proof characterizing the solution space of T-CT, or empirical analysis showing that learned β distributions consistently lie within the bounds required for the Sobolev space projection

## Limitations
- The curvature modulation analysis is proven for ReLU networks and assumes a finite dataset, limiting applicability to architectures with other activations
- Robustness improvements are demonstrated empirically without a complete theoretical mechanism, with gains most pronounced for ℓ∞ attacks on CNNs
- Critical hyperparameters like batch size and weight decay are unspecified, potentially affecting exact replication
- Performance varies significantly across architectures, with weaker results on transformers compared to CNNs

## Confidence
**High Confidence:** The core CTU implementation and its basic functionality as an activation function are well-defined and reproducible. The parameter-efficient finetuning claims for T-CT are directly measurable.

**Medium Confidence:** The theoretical projection onto smooth functions (Theorem C.1) is formally stated but relies on assumptions about network structure. Empirical claims about downstream task improvement require careful hyperparameter tuning.

**Low Confidence:** The implicit bias towards robustness improvement is primarily empirical without a complete theoretical mechanism. The effectiveness on transformer architectures remains uncertain.

## Next Checks
1. **Robustness Mechanism Verification:** Implement CTU on a simple CNN and systematically test robustness against multiple attack types (ℓ∞, ℓ2, PGD). Measure whether improvements correlate specifically with curvature changes or other factors.

2. **Architecture Generalization Test:** Apply CTU replacement to a BERT-base model for a GLUE benchmark task. Compare performance to LoRA and full finetuning to validate claims about transformer compatibility.

3. **Parameter Efficiency Validation:** For a specific architecture (e.g., ResNet-50), calculate exact parameter counts for T-CT versus LoRA across multiple rank values. Verify the 0.58-59.09% efficiency claim holds across the full range.