---
ver: rpa2
title: 'A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language
  Models to Scans of Transfusion Reaction Reports'
arxiv_id: '2504.20220'
source_url: https://arxiv.org/abs/2504.20220
tags:
- data
- transfusion
- checkbox
- reaction
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automating data extraction
  from paper-based transfusion reaction reports, which remain prevalent in healthcare
  despite digitalization efforts. The authors present an open-source pipeline that
  combines checkbox detection, optical character recognition (OCR), and vision-language
  models (VLMs) to accurately extract and categorize checkbox data from scanned documents.
---

# A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports

## Quick Facts
- arXiv ID: 2504.20220
- Source URL: https://arxiv.org/abs/2504.20220
- Reference count: 35
- Primary result: VLM approach achieved 92.04% average accuracy vs OCR baseline of 85.17% on checkbox extraction

## Executive Summary
This paper addresses the persistent challenge of manual data entry from paper-based transfusion reaction reports in healthcare settings. Despite digitalization efforts, paper forms remain prevalent, creating inefficiencies and potential errors in regulatory reporting. The authors present an open-source pipeline that leverages vision-language models (VLMs) to automate checkbox data extraction from scanned documents, significantly reducing the manual workload while improving accuracy. The method combines YOLO-based checkbox detection, OCR preprocessing, and VLM prompting to map checked boxes to predefined clinical categories.

## Method Summary
The pipeline employs a three-stage approach: first, YOLOv8 is used to detect checkbox areas within scanned documents; second, OCR processes the detected regions to extract text labels; third, a vision-language model interprets the visual state of checkboxes (checked/unchecked) and maps them to standardized clinical categories. The VLM component uses few-shot prompting with examples of checked and unchecked checkboxes to determine the state of each checkbox. The system was evaluated on 387 transfusion reaction reports spanning 2017-2024, with performance measured against manual ground truth annotations. The approach handles various challenges including faint checkmarks, poor scan quality, and complex multi-page layouts.

## Key Results
- VLM-based approach achieved 92.04% average accuracy across 11 checkbox categories
- OCR baseline achieved 85.17% average accuracy, demonstrating significant performance gap
- Best-performing VLM configuration used 3-shot prompting with parallel processing of all checkboxes
- System successfully handled faint checkmarks, skewed scans, and overlapping form elements

## Why This Works (Mechanism)
The VLM approach succeeds because it combines visual understanding of checkbox states with contextual text interpretation in a single model, eliminating the error propagation inherent in multi-stage OCR pipelines. Unlike traditional OCR, which struggles with subtle visual distinctions between checked and unchecked boxes, VLMs can directly process the visual appearance of checkboxes alongside their labels, learning the spatial and visual patterns that indicate selection state.

## Foundational Learning
- **YOLO object detection**: Needed to reliably locate checkboxes in varied document layouts; quick check: ensure bounding boxes tightly enclose checkbox regions
- **OCR preprocessing**: Required to extract text labels for each checkbox; quick check: verify text recognition accuracy exceeds 95% for checkbox labels
- **Vision-language prompting**: Enables direct mapping of visual checkbox states to clinical categories; quick check: test few-shot examples cover edge cases like faint marks
- **Levenshtein distance matching**: Used to handle OCR errors when matching extracted text to predefined categories; quick check: set threshold to balance precision and recall
- **Checkbox state classification**: Critical for distinguishing checked from unchecked boxes; quick check: validate performance on faint and partial marks
- **Document layout analysis**: Necessary for handling multi-page forms and complex structures; quick check: verify correct page segmentation and form field ordering

## Architecture Onboarding
- **Component map**: Document Scan -> YOLO Detection -> OCR Extraction -> VLM Classification -> Structured Output
- **Critical path**: YOLO detection accuracy directly impacts downstream VLM performance; errors in checkbox localization cascade through the pipeline
- **Design tradeoffs**: VLM approach trades increased computational cost for improved accuracy over traditional OCR pipelines
- **Failure signatures**: Faint checkmarks, overlapping form elements, and unusual checkbox designs cause primary errors
- **First experiments**: 1) Test YOLO detection on documents with varying scan qualities; 2) Validate OCR accuracy on checkbox labels; 3) Benchmark VLM accuracy across different shot configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation dataset of 387 reports may limit generalizability to other document types
- Study does not address computational costs or inference time differences between approaches
- Pipeline focuses only on binary checkbox states, not complex form elements like text fields

## Confidence
- **High confidence**: VLM approach outperforms traditional OCR for checkbox extraction on tested dataset
- **Medium confidence**: Pipeline's robustness across different scan qualities and checkbox styles
- **Medium confidence**: Practical impact on reducing manual data entry burden in clinical settings

## Next Checks
1. Evaluate pipeline performance on diverse document types beyond transfusion reaction reports
2. Conduct cost-benefit analysis comparing computational overhead of VLM versus OCR approaches
3. Test system performance with real-time document scanning scenarios and varying capture conditions