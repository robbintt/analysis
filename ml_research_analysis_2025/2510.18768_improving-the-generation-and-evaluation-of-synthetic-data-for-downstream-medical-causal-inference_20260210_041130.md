---
ver: rpa2
title: Improving the Generation and Evaluation of Synthetic Data for Downstream Medical
  Causal Inference
arxiv_id: '2510.18768'
source_url: https://arxiv.org/abs/2510.18768
tags:
- data
- steam
- synthetic
- causal
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of generating high-quality synthetic
  data for medical causal inference tasks, which are hindered by limited access to
  real patient data. The authors propose a novel method called STEAM (Synthetic data
  for Treatment Effect Analysis in Medicine) that mimics the data-generating process
  of treatment data and optimizes for three key desiderata: preservation of the covariate
  distribution, the treatment assignment mechanism, and the outcome generation mechanism.'
---

# Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference

## Quick Facts
- arXiv ID: 2510.18768
- Source URL: https://arxiv.org/abs/2510.18768
- Reference count: 40
- This paper proposes STEAM, a method for generating synthetic data that preserves causal inference mechanisms, achieving state-of-the-art performance on key evaluation metrics.

## Executive Summary
This paper addresses the challenge of generating high-quality synthetic data for medical causal inference tasks, which are hindered by limited access to real patient data. The authors propose STEAM (Synthetic data for Treatment Effect Analysis in Medicine), a novel method that mimics the data-generating process of treatment data and optimizes for three key desiderata: preservation of the covariate distribution, the treatment assignment mechanism, and the outcome generation mechanism. STEAM augments generic generative models with a three-step process: generating covariates, assigning treatments based on a propensity function, and generating outcomes using potential outcome estimators. The authors introduce a set of evaluation metrics tailored to assess these desiderata and demonstrate that STEAM achieves state-of-the-art performance across these metrics compared to existing generative models.

## Method Summary
STEAM is a three-step framework for generating synthetic data suitable for downstream causal inference. It first trains a generic generative model (e.g., TVAE, CTGAN) on covariates $X$ only. Then it trains a logistic regression model to predict treatment assignment $W$ from $X$. Finally, it trains an S-Learner (from CATENets) to predict outcomes $Y$ from both $X$ and $W$. During generation, STEAM samples synthetic covariates from the generative model, uses the logistic regression to assign treatments, and uses the S-Learner to generate outcomes. This sequential factorization of the data-generating process preserves the causal structure while allowing for realistic synthetic data generation.

## Key Results
- STEAM achieves state-of-the-art performance across evaluation metrics (Pα,X, Rβ,X, JSDπ, UPEHE) compared to existing generative models.
- As the complexity of the true data-generating process increases, STEAM's performance advantage over baselines grows substantially.
- STEAM's evaluation metrics are more sensitive to preservation of treatment and outcome mechanisms than standard joint-distribution metrics like KL divergence.

## Why This Works (Mechanism)

### Mechanism 1: Sequential Factorization of the Data Generating Process (DGP)
Decomposing the joint distribution P(X, W, Y) into sequential conditional steps (X → W → Y) likely preserves causal relationships better than joint optimization, particularly in high-dimensional settings. Rather than generating a synthetic row (x, w, y) simultaneously, STEAM isolates three distinct generative components: Q_X for covariates, Q_{W|X} for treatment assignment (propensity), and Q_{Y|W,X} for outcomes. This explicitly encodes the causal structure of medical interventions into the synthetic data pipeline. Core assumption: The underlying real-world DGP follows the causal order where covariates precede treatment, and treatment precedes outcome. Evidence anchors: [Abstract], [Section 6], [Corpus]. Break condition: If the real data exhibits severe unobserved confounding where the causal graph is invalid, this sequential factorization may fail to capture the true dependencies.

### Mechanism 2: Metric Sensitivity via Isolation
Standard joint-distribution metrics (e.g., KL divergence) lose the ability to detect errors in treatment and outcome mechanisms as covariate dimensionality increases. STEAM relies on a specific set of evaluation metrics (JSD_π, U_{PEHE}) that target specific causal mechanisms. The authors prove (Theorem 1) that as covariate dimension d → ∞, generic joint metrics converge and fail to distinguish between models that preserve P(Y|W,X) and those that do not. Isolating the evaluation of W and Y restores this sensitivity. Core assumption: The failure of generic metrics is driven primarily by the dominance of covariate dimensions in distance calculations. Evidence anchors: [Section 5.1], [Section 7.2.1]. Break condition: If the base classifier used for JSD_π is mis-specified or fails to converge, the evaluation metric will not accurately reflect the preservation of the treatment mechanism.

### Mechanism 3: Model-Agnostic Causal Augmentation
A generic generative model can be upgraded for causal inference tasks by "wrapping" it with specific supervised learning heads for W and Y. STEAM does not require a specialized generative architecture. It uses a generic generator solely for the covariates X. It then uses standard supervised models (logistic regression for W, S-Learners for Y) trained on real data to assign the labels in the synthetic dataset. This separates the "realism" task (X) from the "causal fidelity" task (W, Y). Core assumption: The supervised models for W and Y can be trained with sufficient accuracy on the available real data D_r. Evidence anchors: [Section 6], [Section 7.1]. Break condition: If the base generative model for X produces out-of-distribution covariates that fall outside the support of the trained W and Y models, the downstream causal estimates may become unreliable.

## Foundational Learning

- **Concept: Potential Outcomes Framework (Y(0), Y(1))**
  - Why needed here: The outcome generation mechanism Q_{Y|W,X} relies on estimating potential outcomes to create synthetic Y values. Understanding this is necessary to configure the "S-Learner" or "T-Learner" used in the pipeline.
  - Quick check question: Can you explain why we need to train separate models or a single model with treatment as a feature to generate potential outcomes in synthetic data?

- **Concept: Propensity Score (π(x) = P(W=1|X))**
  - Why needed here: This is the core of the treatment assignment mechanism. STEAM uses this to ensure the synthetic data mimics the selection bias (or lack thereof) in the original medical trial.
  - Quick check question: How does the distribution of propensity scores in the synthetic data affect the validity of downstream Average Treatment Effect (ATE) estimates?

- **Concept: α-Precision and β-Recall in High Dimensions**
  - Why needed here: Standard accuracy metrics fail for generative models. Understanding fidelity (P_{α,X}) vs. diversity (R_{β,X}) is required to interpret the evaluation metrics for the covariate generator.
  - Quick check question: If P_{α,X} is high but R_{β,X} is low, what does this imply about the synthetic patient population coverage?

## Architecture Onboarding

- **Component map:** Data Ingest -> Trainer A (Base) -> Trainer B (Treatment) -> Trainer C (Outcome) -> Synthesizer
- **Critical path:** The accuracy of the Outcome Generator (Trainer C) is the bottleneck for the primary use case (treatment effect estimation). If the S-Learner fails to capture heterogeneity, the synthetic data will yield flat CATE estimates regardless of covariate quality.
- **Design tradeoffs:**
  - Base Generator Choice: Using a simple generator (e.g., TVAE) for Q_X is faster but may reduce sample diversity (R_{β,X}) compared to diffusion models (TabDDPM).
  - Privacy Budgeting: If applying Differential Privacy, splitting the budget (ε_{total}/3) across the three components may starve the high-dimensional X generator, reducing fidelity.
- **Failure signatures:**
  - High U_{PEHE} with Low JSD_π: The outcome generation model is mis-specified (e.g., linear model used for non-linear outcomes), even though treatment assignment is correct.
  - High U_{PEHE} with High JSD_π: The covariate generator Q_X is producing out-of-distribution samples where the outcome model must extrapolate unsafely.
- **First 3 experiments:**
  1. Dimensionality Stress Test: Run STEAM vs. Baseline on simulated data while increasing d (covariates). Verify that STEAM's U_{PEHE} degrades slower than the baseline's (replicating Section 7.2.1).
  2. Ablation on Generation Order: Compare STEAM vs. a variant that generates X, W jointly. Confirm that explicit factorization improves JSD_π scores (replicating Appendix J).
  3. Downstream Utility Check: Train a CATE estimator on Synthetic data and test on Real data (TSTR). Compare the error against a model trained purely on Real data to quantify utility loss.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the differential privacy budget be optimally allocated among STEAM's component models (Q_X, Q_{W|X}, and Q_{Y|W,X}) to balance covariate fidelity with causal mechanism preservation? Basis in paper: [explicit] (Section 8.1 and Appendix N.4). Why unresolved: The authors currently use a uniform budget split, but empirical results suggest this restricts the covariate generator (Q_X) too much in high-dimensional settings, leading to a trade-off between preserving P_X and preserving P_{W|X}/P_{Y|W,X}. What evidence would resolve it: An algorithm that dynamically distributes the privacy budget based on the complexity or downstream importance of each component distribution, outperforming the uniform split.

- **Open Question 2:** How can the STEAM framework be adapted to generate synthetic data suitable for Instrumental Variable (IV) analysis? Basis in paper: [explicit] (Section 8.1). Why unresolved: The current STEAM architecture assumes a standard data-generating process (X → W → Y) that relies on unconfoundedness; it does not model the specific causal structure (Z, X, W, Y) required for valid IV estimation where unobserved confounders exist. What evidence would resolve it: A modified generation process that includes an instrument Z and produces synthetic data from which valid IV estimates can be recovered.

- **Open Question 3:** How can STEAM be extended to accommodate continuous or multiple treatment options? Basis in paper: [explicit] (Section 8.1). Why unresolved: The current implementation is designed for binary treatments, utilizing a classifier for treatment assignment and binary-specific potential outcome estimators. What evidence would resolve it: A STEAM variant that replaces the treatment classifier with a regressor for continuous doses and uses generalized propensity score methods for outcome generation.

## Limitations

- The evaluation framework relies heavily on synthetic benchmarks where the ground-truth DGP is known, potentially limiting generalizability to real-world clinical datasets with unobserved confounding.
- The UPEHE metric requires training multiple CATE learners on both real and synthetic data, making it computationally intensive and potentially sensitive to hyperparameter choices.
- The current implementation is designed for binary treatments and does not accommodate continuous or multiple treatment options without modification.

## Confidence

- **High Confidence:** The sequential factorization mechanism is well-supported by theoretical analysis and empirical results showing degradation of joint-distribution metrics as dimensionality increases.
- **Medium Confidence:** The model-agnostic causal augmentation approach is validated on benchmark datasets, but its robustness to severe distributional shifts in real clinical data remains untested.
- **Medium Confidence:** The metric sensitivity argument is theoretically justified, but the practical impact depends on the quality of the base classifiers used for JSD_π and UPEHE.

## Next Checks

1. **Domain Transfer Test:** Evaluate STEAM on a real-world clinical dataset (e.g., MIMIC-IV) where the true DGP is unknown, comparing downstream ATE estimation accuracy against models trained on real data.
2. **Extreme Confounding Scenario:** Test STEAM on data with high levels of unobserved confounding to assess whether the sequential factorization breaks down when causal assumptions are violated.
3. **Hyperparameter Sensitivity Analysis:** Systematically vary the hyperparameters of the base generative model (Q_X) and outcome learner (Q_Y|W,X) to quantify their impact on UPEHE scores and downstream utility.