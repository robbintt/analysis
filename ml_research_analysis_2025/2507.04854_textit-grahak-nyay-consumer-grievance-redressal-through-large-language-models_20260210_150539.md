---
ver: rpa2
title: $\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large Language
  Models
arxiv_id: '2507.04854'
source_url: https://arxiv.org/abs/2507.04854
tags:
- consumer
- chatbot
- score
- information
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Grahak-Nyay is a consumer grievance redressal chatbot for India\
  \ that uses Retrieval-Augmented Generation with open-source LLMs to help users navigate\
  \ complex legal procedures. It leverages three novel datasets\u2014GeneralQA, SectoralQA,\
  \ and SyntheticQA\u2014plus NyayChat (300 annotated conversations) and a corpus\
  \ of 570 Consumer Court judgments to provide accurate, context-specific legal assistance."
---

# $\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large Language Models

## Quick Facts
- **arXiv ID:** 2507.04854
- **Source URL:** https://arxiv.org/abs/2507.04854
- **Reference count:** 40
- **Primary result:** RAG-based chatbot for Indian consumer law using open-source LLMs, evaluated with HAB metrics showing reduced hallucinations vs. ChatGPT-4.0/Claude-3.5

## Executive Summary
Grahak-Nyay is a consumer grievance redressal chatbot designed for India that leverages Retrieval-Augmented Generation with open-source LLMs to help users navigate complex legal procedures. The system uses three novel datasets—GeneralQA, SectoralQA, and SyntheticQA—plus NyayChat (300 annotated conversations) and a corpus of 570 Consumer Court judgments to provide accurate, context-specific legal assistance. Evaluated using HAB metrics (Helpfulness, Accuracy, Brevity) with both human and automated assessments, Grahak-Nyay demonstrates strong performance and significantly reduced hallucinations compared to general-purpose chatbots like ChatGPT-4.0 and Claude-3.5, improving access to consumer law for non-experts through clear, actionable guidance grounded in up-to-date legal knowledge.

## Method Summary
The system implements a RAG pipeline where user queries are first rewritten into standalone queries using Llama-3.1-8B-Instruct with one-shot prompting to maintain context across conversation turns. These queries are embedded using mxbai-embed-large-v1 and retrieved via cosine similarity (top-4 chunks) from a knowledge base containing 950+ QA pairs and 570 annotated judgments. The generator (Llama-3.1-8B-Instruct) produces responses incorporating both retrieved context and chat history, with explicit prompt instructions to refuse out-of-corpus questions. The knowledge base uses semantic chunking at the Q&A pair level for precision. The entire system is deployed using Text Generation Inference (TGI) v3.2.1 on NVIDIA A100 40GB GPUs.

## Key Results
- Achieved superior HAB metric scores compared to ChatGPT-4.0 and Claude-3.5 in human evaluation, particularly on Accuracy
- Demonstrated reduced hallucination rates through explicit refusal instructions for out-of-corpus questions
- Successfully handled multi-turn conversations through query rewriting mechanism that maintains retrieval quality
- Effective navigation of complex Indian consumer law procedures for non-expert users

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Grounded Generation Constrains Hallucination
- **Claim:** RAG with explicit prompt boundaries reduces hallucinations compared to general-purpose chatbots.
- **Mechanism:** Retrieved documents narrow the generation space, and prompt instructions force the model to refuse out-of-corpus questions rather than speculate.
- **Core assumption:** The knowledge base is accurate, complete, and properly chunked for the target domain.
- **Evidence anchors:** [Section 3.2]: modified prompt to refuse out-of-corpus questions; [Section 3.1]: RAG reduces hallucinations without expensive fine-tuning; [Section 4.1]: outperformed ChatGPT-4.0/Claude-3.5 on Accuracy.
- **Break condition:** If retrieval fails or retrieves irrelevant chunks, the model may still hallucinate or over-rely on parametric knowledge.

### Mechanism 2: Query Rewriting Enables Multi-Turn Contextual Retrieval
- **Claim:** Rewriting follow-up questions into standalone queries maintains retrieval quality across conversation turns.
- **Mechanism:** An LLM extracts implicit context from chat history to form an independent query, resolving co-references and pronouns before retrieval.
- **Core assumption:** The query-rewriting model correctly interprets conversation context; one-shot prompting is sufficient for this task.
- **Evidence anchors:** [Section 3.1.2]: one-shot prompting led to significantly better results; [Figure 9]: shows one-shot prompt example.
- **Break condition:** If the rewriting model fails to capture context, retrieval returns irrelevant chunks, degrading response quality.

### Mechanism 3: Semantic Chunking at Q&A Granularity Improves Precision
- **Claim:** Chunking at one Q&A pair per chunk outperforms fixed-length chunking for this domain.
- **Mechanism:** Prevents splitting long answers (information loss) and avoids grouping multiple short answers (noise), ensuring each retrieved chunk is coherent and self-contained.
- **Core assumption:** Q&A pairs in knowledge base are well-structured and represent atomic information units.
- **Evidence anchors:** [Section 3.1.1]: Long answers split across chunks caused information loss; grouping short answers introduced noise.
- **Break condition:** If Q&A pairs are poorly authored or overlap significantly, retrieval may return redundant or incomplete information.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** Core architecture; understanding how retrieval conditions generation is essential for debugging and optimization.
  - **Quick check question:** "If the retriever returns 4 irrelevant chunks, what should the generator do—refuse, guess, or hallucinate?"

- **Concept:** Vector embeddings and semantic similarity search
  - **Why needed here:** The retriever uses `mxbai-embed-large-v1` embeddings and cosine similarity; understanding embedding quality is critical for retrieval tuning.
  - **Quick check question:** "Why might cosine similarity fail for legal queries with rare terminology not well-represented in the embedding model's training data?"

- **Concept:** Prompt engineering for instruction-following and refusal behavior
  - **Why needed here:** The system relies on explicit prompts to enforce domain boundaries and prevent hallucination.
  - **Quick check question:** "How would you modify the system prompt if users frequently ask for legal opinions rather than procedural guidance?"

## Architecture Onboarding

- **Component map:** User query + chat history -> Query Rewriter (Llama-3.1-8B-Instruct) -> standalone query -> Retriever (mxbai-embed-large-v1 + cosine similarity) -> top-4 chunks -> Generator (Llama-3.1-8B-Instruct) -> response
- **Critical path:** Query rewriting -> Retrieval -> Generation with context
- **Design tradeoffs:** Q&A pairs vs. fixed-length chunking (chose precision over flexibility); 8B model size (traded capability for deployment efficiency); top-4 chunks (balances coverage vs. noise)
- **Failure signatures:** Retrieval failure (says "I don't know" for in-scope questions); Query rewriting failure (follow-up questions retrieve unrelated chunks); Grounding failure (hallucinates despite retrieved context)
- **First 3 experiments:** 1) Retrieval sanity check on SyntheticQA measuring precision@4; 2) Query rewriting validation on 50 NyayChat conversations; 3) Hallucination boundary test with 20 out-of-corpus questions

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the expert-annotated Judgments corpus be utilized to train models for predictive legal tasks, such as case outcome prediction or precedent identification? The authors state it "holds substantial potential" for such tasks but only demonstrate its use for retrieval and trust enhancement in the current work.

- **Open Question 2:** How robust is the chatbot's performance when users communicate in code-mixed languages (e.g., Hinglish) or vernacular Indian languages rather than standard English? The paper targets Indian consumers but presents all dataset examples and evaluation prompts entirely in English.

- **Open Question 3:** Does the reliance on GPT-4o-mini as an automated evaluator introduce a bias that favors specific reasoning styles over the nuanced, concise responses required by the HAB metrics? The paper selects gpt-4o-mini because it showed "highest correlation with human evaluations" but does not analyze if this holds for "Brevity" or specific legal errors.

## Limitations

- **Dataset accessibility:** The three novel datasets (GeneralQA, SectoralQA, SyntheticQA) and NyayChat evaluation set are not publicly available, preventing independent verification of performance claims.
- **Evaluation methodology:** Human annotation protocol and inter-annotator agreement are not reported for the 300 conversations used in evaluation.
- **Real-world validation:** Claims about deployment scalability and impact are not empirically validated beyond controlled evaluation environment.

## Confidence

- **High confidence:** The core RAG architecture and HAB evaluation methodology are sound and well-documented; claim about retrieval-grounded generation reducing hallucinations is supported by human evaluation results.
- **Medium confidence:** Effectiveness of query rewriting mechanism and semantic chunking strategy would benefit from ablation studies; superiority over commercial chatbots demonstrated but based on single evaluation dataset.
- **Low confidence:** Claims about deployment scalability and real-world impact lack empirical validation.

## Next Checks

1. **Dataset replication test:** Attempt to recreate GeneralQA and SectoralQA datasets by curating consumer protection documents and manually generating QA pairs following the paper's structure, then evaluate whether the system achieves similar HAB scores on reconstructed dataset.

2. **Query rewriting ablation:** Implement baseline without query rewriting (passing original multi-turn queries directly to retrieval) and compare retrieval precision and HAB scores to quantify contribution of rewriting mechanism.

3. **Hallucination boundary verification:** Conduct systematic test with 100 out-of-corpus questions spanning different topics to verify system consistently refuses rather than hallucinating, measuring false positive rate for hallucinated responses.