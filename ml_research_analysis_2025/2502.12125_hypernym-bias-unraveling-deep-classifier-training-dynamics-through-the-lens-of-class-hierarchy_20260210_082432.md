---
ver: rpa2
title: 'Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens
  of Class Hierarchy'
arxiv_id: '2502.12125'
source_url: https://arxiv.org/abs/2502.12125
tags:
- training
- hypernym
- label
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how deep neural networks learn hierarchical
  class relationships during training. The authors propose a novel framework to track
  the evolution of feature manifolds, revealing that networks tend to learn higher-level
  (hypernym) categories early in training and more specific (hyponym) categories later.
---

# Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy

## Quick Facts
- **arXiv ID:** 2502.12125
- **Source URL:** https://arxiv.org/abs/2502.12125
- **Reference count:** 40
- **Primary result:** Deep networks learn higher-level (hypernym) categories faster than specific (hyponym) categories during training

## Executive Summary
This paper investigates how deep neural networks learn hierarchical class relationships during training. The authors propose a novel framework to track the evolution of feature manifolds, revealing that networks tend to learn higher-level (hypernym) categories early in training and more specific (hyponym) categories later. Through extensive experiments with ResNet, ViT, and MobileNet architectures on ImageNet, they demonstrate that hypernym classification accuracy converges significantly faster than hyponym accuracy. They introduce metrics like relative accuracy and relative gain to quantify this "hypernym bias" phenomenon. Additionally, they show that certain properties of neural collapse appear earlier in hypernym label spaces than in hyponym spaces, bridging the gap between initial and terminal phases of learning. The findings provide new insights into hierarchical learning mechanisms in deep networks and suggest that the learned representations align with the semantic structure of the dataset.

## Method Summary
The authors train standard deep classifiers (ResNet, ViT, MobileNet) on ImageNet using cross-entropy loss, then evaluate classification performance at different hierarchical levels by mapping predictions to higher-level categories (hypernyms) using WordNet. They track relative accuracy and neural collapse metrics across training epochs for multiple label spaces: native hyponym labels, semantic superclasses, and random superclasses. The framework quantifies how quickly different label spaces converge and whether neural collapse properties emerge earlier in hypernym spaces.

## Key Results
- Hypernym classification accuracy converges significantly faster than hyponym accuracy across all tested architectures
- Relative accuracy for semantic superclasses increases significantly faster than for random superclasses during initial epochs
- Neural collapse properties (NC1, NC3, NC4) appear earlier in hypernym label spaces compared to hyponym spaces
- Cophenetic Correlation Coefficient between WordNet graph distances and feature manifold distances peaks early in training, suggesting geometric alignment with semantic hierarchies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deep networks prioritize learning features shared across broad semantic categories (hypernyms) before specific category features (hyponyms).
- **Mechanism:** Gradient accumulation frequency. Features common to a hypernym (e.g., "fur" for animals) appear more frequently in a training batch than features specific to a hyponym (e.g., "webbed feet" for ducks). This higher co-occurrence rate causes gradients for generic features to accumulate faster, driving earlier convergence of hypernym decision boundaries.
- **Core assumption:** The dataset hierarchy is valid, meaning hyponyms within a hypernym class visually share features that are distinct from other hypernym classes.
- **Evidence anchors:**
  - [Section 1]: "classes of same hypernym share frequent (simple) features, therefore neurons responsible for their detection are activated more often and trained faster."
  - [Section 4.1]: "Across all architectures during initial epochs of training relative accuracy for hypernyms increases significantly faster than for the rest of considered label spaces."
  - [Corpus]: "Effects of Initialization Biases on Deep Neural Network Training Dynamics" supports the sensitivity of early training dynamics to initial class probabilities and feature frequency.

### Mechanism 2
- **Claim:** The feature manifold aligns with semantic class hierarchies specifically during the initial training epochs, acting as a top-down clustering process.
- **Mechanism:** Geometric alignment. The optimization landscape facilitates separating large clusters (hypernyms) with lower curvature complexity (simple linear separators) before separating high-curvature intra-cluster details (hyponyms). This manifests as a rapid increase in the Cophenetic Correlation Coefficient (CCC) between feature distances and semantic (WordNet) distances early in training.
- **Core assumption:** The semantic hierarchy (e.g., WordNet) correlates with the visual similarity structure of the data.
- **Evidence anchors:**
  - [Section 4.3]: "WordNet graph and manifold distances linear alignment rapidly grows in first 5 epochs."
  - [Section 1]: "training can be interpreted as top-to-bottom hierarchical label clustering that starts by increasing distance between large hypernyms."
  - [Corpus]: "Hierarchy-Consistent Learning..." implicitly validates the difficulty of maintaining this alignment, which this mechanism suggests happens naturally in early phases.

### Mechanism 3
- **Claim:** Neural collapse (NC) properties manifest earlier in hypernym label spaces compared to the native hyponym space.
- **Mechanism:** Prototype aggregation. The "hypernym prototype" (mean of hyponym class means) exhibits reduced within-class variability ($\Sigma_W$) earlier than individual hyponym classes. As the network resolves broad categories first, the feature variance for the aggregated hypernym class collapses toward zero faster.
- **Core assumption:** The terminal phase of training (near-zero loss) is achievable, and hypernym prototypes can be validly constructed by averaging hyponym features/weights.
- **Evidence anchors:**
  - [Abstract]: "neural collapse properties appear earlier in hypernym label spaces."
  - [Section 3.3]: "Assuming hypernym bias we expect at least some properties of neural collapse in label spaces of hypernyms emerge earlier."

## Foundational Learning

- **Concept:** **Neural Collapse (NC)**
  - **Why needed here:** The paper uses NC (specifically NC1, NC3, NC4) as a diagnostic tool to measure the "terminal phase" of learning for different label hierarchies. Understanding that NC implies features converging to class means is required to interpret the acceleration results.
  - **Quick check question:** In the context of this paper, does NC1 (variability collapse) happen faster for the "Animal" superclass or the "Tabby Cat" specific class?

- **Concept:** **Simplicity Bias / Frequency Principle**
  - **Why needed here:** This provides the theoretical grounding for *why* hypernyms are learned first. It posits that networks fit "simple" (frequent/low-frequency) patterns first. You need this to connect "shared features" to "early learning."
  - **Quick check question:** According to the authors, why is the hypernym bias considered a manifestation of the simplicity bias?

- **Concept:** **Greedy Hypernym Classifier**
  - **Why needed here:** This is the core evaluation methodology. Instead of retraining the network, the authors map the trained classifier's outputs to a higher-level label space (e.g., $H \to S$). You must understand this is a post-hoc mapping, not a multi-head architecture.
  - **Quick check question:** If a network predicts "Labrador" (a hyponym), how does the greedy classifier determine the "Animal" (hypernym) accuracy?

## Architecture Onboarding

- **Component map:**
  1. Backbone: Standard ResNet/ViT/MobileNet extracting features $h_i$
  2. Head: Linear classifier layer ($W, b$)
  3. Mapping Function ($T_{H \to S}$): A static lookup table mapping 1000 ImageNet classes (H) to Superclasses (S)
  4. Metrics Layer:
      - *Relative Accuracy:* Standard accuracy normalized by max accuracy
      - *NC Metrics:* Computation of $\Sigma_W, \Sigma_B$ and self-duality for aggregated hypernym weights ($w_s$)

- **Critical path:**
  1. Train backbone on standard Hyponym loss ($L_{CE}$)
  2. At each epoch, pass validation data
  3. Apply **Greedy Mapping** to logits to get Hypernym predictions
  4. Compute Relative Accuracy and Neural Collapse metrics on the *mapped* label space

- **Design tradeoffs:**
  - **Hierarchy Source:** The paper notes that using the WordNet *Tree* ensures unambiguous parents, while the *Graph* captures nuance but complicates analysis
  - **Superclass Size:** Grouping is required to balance the top-level synsets (e.g., grouping "Plants" and "Fungi" into "Others" to balance against "Animals"). Unbalanced superclasses skew accuracy comparisons

- **Failure signatures:**
  - **Random vs. Semantic Parity:** If Relative Accuracy for Semantic Superclasses ($S$) does not significantly outpace Random Superclasses ($R$) in the first 10 epochs, the implementation of the mapping or the dataset hierarchy is flawed
  - **NC Stagnation:** If NC1 (within-class variance) does not decrease for hypernyms earlier than hyponyms, check if the hypernym weights ($w_s$) are being correctly averaged from hyponym weights ($w_c$)

- **First 3 experiments:**
  1. **Convergence Speed Check:** Train ResNet-50 on ImageNet. Plot Relative Accuracy ($A_R$) for Hyponyms ($H$), Semantic Superclasses ($S$), and Random Superclasses ($R$). Verify $S$ diverges from $R$ and $H$ in first 5-10 epochs
  2. **Neural Collapse Timing:** Compute NC1 (within-class variance) and NC3 (self-duality) for the Hypernym space ($S_3$) vs Hyponym space ($H_{1000}$). Verify NC1 drops to zero faster for $S_3$
  3. **Manifold Alignment:** Extract features at epoch 1, 5, 10. Compute UMAP embeddings and Cophenetic Correlation Coefficient (CCC) against WordNet distances. Check for peak correlation around epoch 5-10

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can data-driven hierarchical clustering replace predefined external taxonomies (like WordNet) to reveal more accurate or latent class relationships?
- **Basis:** [explicit] The authors suggest moving away from external class structures to "learning class hierarchies directly through data-driven hierarchical clustering."
- **Why unresolved:** Predefined graphs like WordNet may miss latent patterns or relationships present in the actual data manifold.
- **What evidence would resolve it:** Demonstration that a hierarchy learned from the class similarity matrix (Equation 15) aligns better with training dynamics or improves hierarchical classification performance compared to human-defined hierarchies.

### Open Question 2
- **Question:** What is the precise theoretical relationship between hypernym bias and other known training biases, such as the Frequency Principle or spectral bias?
- **Basis:** [explicit] Section 5 states, "Investigating the theoretical connections between hypernym bias and other known biases is a promising direction."
- **Why unresolved:** While the paper posits hypernym bias is a manifestation of "Simplicity Bias," empirical experiments failed to link it directly to image frequency components (Frequency Principle), leaving the theoretical grounding incomplete.
- **What evidence would resolve it:** A theoretical proof or empirical model explicitly linking the accelerated learning of hypernym features to the spectral properties of the neural network or dataset.

### Open Question 3
- **Question:** Does hypernym bias manifest in temporal data domains like audio (e.g., AudioSet) similarly to how it appears in images?
- **Basis:** [explicit] The authors propose extending the framework to AudioSet, which has a hierarchical ontology of sound categories.
- **Why unresolved:** The phenomenon has only been validated on image (ImageNet, CIFAR) and text (DBPedia) domains.
- **What evidence would resolve it:** Experiments showing that audio classifiers prioritize high-level sound categories (e.g., "Human sounds") over specific sub-categories (e.g., "Speech") during initial training epochs.

## Limitations
- The study assumes WordNet's semantic hierarchy aligns with visual similarity, which may not hold for all datasets or cultural contexts
- The greedy mapping approach uses a fixed semantic hierarchy that may miss non-standard groupings the network could learn
- While CCC peaks suggest alignment with semantic hierarchies, the paper doesn't fully explain whether this reflects genuine semantic understanding or simpler geometric properties

## Confidence
- **Hypernym bias phenomenon:** Medium - consistently observed across architectures but dependent on hierarchy validity
- **Neural collapse timing:** Medium - follows from hypernym bias but assumes training reaches interpolation regime
- **Semantic understanding claim:** Low - correlation between visual and semantic hierarchies doesn't establish causation

## Next Checks
1. **Cross-Dataset Validation:** Test the hypernym bias on datasets with different semantic structures (e.g., CIFAR-100, Places365) to verify if the phenomenon generalizes beyond ImageNet's WordNet-based hierarchy
2. **Ablation on Hierarchy Quality:** Create controlled hierarchies with varying levels of semantic-visual alignment and measure how hierarchy quality affects hypernym bias emergence and NC timing
3. **Dynamic Hierarchy Discovery:** Implement a method to discover hierarchical structures directly from learned features (rather than using WordNet) and compare the learned hierarchies with the semantic ones to assess alignment quality