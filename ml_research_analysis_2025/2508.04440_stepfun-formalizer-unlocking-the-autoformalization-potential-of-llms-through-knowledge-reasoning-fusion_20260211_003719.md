---
ver: rpa2
title: 'StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through
  Knowledge-Reasoning Fusion'
arxiv_id: '2508.04440'
source_url: https://arxiv.org/abs/2508.04440
tags:
- reasoning
- problem
- formal
- lean
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ThinkingF, a data synthesis and training
  pipeline designed to enhance the autoformalization capabilities of large language
  models (LLMs). Autoformalization, the task of translating natural-language mathematical
  statements into formal language, is challenging due to two key requirements: mastery
  of formal-language domain knowledge and strong reasoning for natural-language problem
  understanding and informal-formal alignment.'
---

# StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion

## Quick Facts
- arXiv ID: 2508.04440
- Source URL: https://arxiv.org/abs/2508.04440
- Reference count: 40
- SOTA BEq@1: 40.5% on FormalMATH-Lite, 26.7% on ProverBench

## Executive Summary
This paper introduces ThinkingF, a data synthesis and training pipeline designed to enhance the autoformalization capabilities of large language models (LLMs). Autoformalization, the task of translating natural-language mathematical statements into formal language, is challenging due to two key requirements: mastery of formal-language domain knowledge and strong reasoning for natural-language problem understanding and informal-formal alignment. Existing methods fall short in one or both areas, leading to low accuracy. ThinkingF addresses this by constructing two datasets: one distilled from specialized models to capture formal knowledge, and another synthesized using expert-designed templates to capture reasoning processes. These datasets are used in supervised fine-tuning and reinforcement learning with verifiable rewards (BEq equivalence verification) to integrate and refine both capabilities. The resulting StepFun-Formalizer models (7B and 32B) achieve state-of-the-art BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing prior general-purpose and specialized models. This demonstrates significant improvements in both in-domain and out-of-distribution generalization.

## Method Summary
ThinkingF employs a four-stage pipeline: (1) Knowledge distillation from specialized models (NuminaMath-1.5 via Kimina-Autoformalizer) with majority voting and LLM-based selection to create a high-quality formal knowledge dataset; (2) Reasoning trajectory synthesis using an expert-designed 8-step template and Claude 3.7 Sonnet to generate informal-to-formal reasoning processes; (3) Two-stage supervised fine-tuning on DeepSeek-R1-Distill-Qwen base model, first with knowledge data then with reasoning data; (4) Reinforcement learning with GRPO and BEq equivalence verification as a verifiable reward signal. The approach fuses knowledge mastery and reasoning capability to address the dual challenges of autoformalization, achieving state-of-the-art results on both in-domain (FormalMATH-Lite) and out-of-distribution (ProverBench, CombiBench) benchmarks.

## Key Results
- Achieves 40.5% BEq@1 on FormalMATH-Lite, surpassing previous SOTA
- Achieves 26.7% BEq@1 on ProverBench, demonstrating strong out-of-distribution generalization
- Template-guided reasoning synthesis outperforms direct distillation from general reasoning models (25.1% vs 21.8% BEq@1 on ProverBench)
- RL with BEq rewards improves performance over SFT alone (30.3% vs 25.8% BEq@1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-tier distillation from specialized models with majority voting and LLM-based selection yields higher-quality formal knowledge data than direct distillation alone.
- Mechanism: A specialized model generates 16 candidate formalizations per problem; syntax checking filters invalid code; BEq equivalence classes enable majority voting; an LLM judge removes tautologies and contradictions. This selects the most consistent formalization while reducing noise.
- Core assumption: Majority agreement among model outputs correlates with formal correctness.
- Evidence anchors:
  - [abstract]: "construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge"
  - [Section 4.1]: Majority voting improves Kimina-Autoformalizer from 35.1% to 45.9% BEq@1 (Table 1)
  - [corpus]: Neighbor papers show specialized models outperform general ones in autoformalization but lack explicit voting analysis (weak corpus evidence)
- Break condition: If candidate formalizations have high variance (no clear majority), selection degrades to random choice.

### Mechanism 2
- Claim: Template-guided reasoning trajectory synthesis outperforms direct distillation from general reasoning models for autoformalization.
- Mechanism: An expert-designed 8-step template structures the informal-to-formal reasoning process: problem restatement, logical structure analysis, concept decomposition, object mapping, header identification, Lean-specific issues, and code review. Claude 3.7 Sonnet generates trajectories following this template using human-annotated ground truth.
- Core assumption: Structured templates constrain reasoning to formalization-relevant steps, preventing "off-task" problem-solving behavior.
- Evidence anchors:
  - [abstract]: "generating informal-to-formal reasoning trajectories guided by expert-designed templates"
  - [Section 5.4, Table 5]: Template method achieves 25.1% vs 21.8% BEq@1 on ProverBench compared to direct distillation from Claude4-thinking
  - [corpus]: MASA paper (neighbor) uses multi-agent collaboration but doesn't compare template vs. direct distillation
- Break condition: If templates are too rigid, they may not cover edge cases; if too loose, off-task behavior re-emerges.

### Mechanism 3
- Claim: Reinforcement learning with BEq equivalence verification as reward fuses knowledge and reasoning capabilities beyond SFT alone.
- Mechanism: After two-stage SFT, GRPO (Group Relative Policy Optimization) trains the model with a binary reward: 1 if model output is bidirectionally equivalent to ground truth via `exact?` tactic, 0 otherwise. The verifiable reward provides signal for semantic correctness.
- Core assumption: BEq equivalence is a tractable proxy for semantic correctness without full proof verification.
- Evidence anchors:
  - [abstract]: "apply SFT and RLVR with these datasets to further fuse and refine the two abilities"
  - [Section 5.5]: RL increases reward from 0.232 to 0.347 and average BEq@1 from 0.258 to 0.303 over 450 steps
  - [corpus]: Neighbor papers (ReForm, Mathesis) mention RL for autoformalization but lack public benchmarks for comparison
- Break condition: If BEq verification has false negatives (semantically correct but not provably equivalent), reward signal becomes noisy.

## Foundational Learning

- Concept: Bidirectional Extended Definitional Equivalence (BEq)
  - Why needed here: The paper uses BEq as its primary evaluation metric and RL reward signal. Two formal statements are BEq if each is provable from the other using semantics-preserving tactics.
  - Quick check question: Given two Lean statements `theorem A : n > 0` and `theorem B : 0 < n`, are they BEq?

- Concept: Majority Voting with Equivalence Classes
  - Why needed here: The knowledge distillation pipeline relies on clustering syntactically valid outputs into equivalence classes and selecting from the largest class.
  - Quick check question: If a model generates 16 outputs and BEq partitions them into classes of sizes [6, 5, 3, 2], which class's representative should be selected?

- Concept: Two-Stage Supervised Fine-Tuning
  - Why needed here: The pipeline applies knowledge SFT first (format: input → output), then reasoning SFT (format: input → <reasoning> → output). The order matters for capability integration.
  - Quick check question: Why might training on reasoning data before knowledge data hurt performance?

## Architecture Onboarding

- Component map:
  Knowledge Distillation: NuminaMath-1.5 → Kimina-Autoformalizer (16 candidates) → Syntax Check → Majority Voting (BEq) → LLM Selection (DeepSeek-V3) → 183K pairs
  Reasoning Synthesis: Human-annotated pairs → Template-guided synthesis (Claude 3.7 Sonnet) → 5.8K reasoning trajectories
  Training: DeepSeek-R1-Distill-Qwen → Stage 1 SFT (knowledge) → Stage 2 SFT (reasoning) → RL (GRPO + BEq reward)
  Evaluation: BEq verification via Lean4 REPL with `exact?` tactic

- Critical path:
  1. Set up Kimina Lean Server (100 CPU cores, 400GB RAM, 60s timeout)
  2. Generate and filter knowledge dataset (256K → 183K after selection)
  3. Synthesize reasoning trajectories with template
  4. Two-stage SFT (2 epochs each, batch sizes 128→8)
  5. RL training (GRPO, 450 steps for 7B, 350 steps for 32B)

- Design tradeoffs:
  - BEq verification vs. full proof verification: BEq is faster (3 min vs 12 min for 320 samples) but may miss semantically equivalent statements
  - Template-based vs. direct distillation: Templates require expert design but avoid off-task reasoning; direct distillation is simpler but produces worse results (Table 5)
  - 7B vs. 32B: 7B matches 32B performance on current data; 32B needs more data to show advantage

- Failure signatures:
  - Low BEq@1 with high BEq@16: Model has capability but low confidence; increase temperature or sampling diversity
  - High syntax error rate: Knowledge SFT insufficient; increase knowledge data or check Lean version compatibility
  - Off-task reasoning (solving instead of formalizing): Template constraints too loose; strengthen template adherence

- First 3 experiments:
  1. Replicate ablation: Train with only knowledge data, only reasoning data, and both. Verify that reasoning data contributes more to BEq@16 (Table 4: 37.9 → 25.3 without reasoning).
  2. Test BEq verification reliability: Sample 100 formalizations, compare `exact?` against manual equivalence judgment. Estimate false negative rate.
  3. Cross-benchmark generalization: Train on FormalMATH-Train, evaluate on ProverBench and CombiBench. Replicate the OOD performance gap (FormalMATH-Lite: 40.5% vs ProverBench: 26.7%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What data scale is required for larger models (e.g., 32B+) to show clear improvements over 7B models in autoformalization?
- Basis in paper: [explicit] Section 5.3 states "the 7B model performs comparably to or slightly better than the 32B model, which may be due to the limited size of the data. The 32B model requires more data for further improvement."
- Why unresolved: The current 183K knowledge pairs and 5.8K reasoning pairs may be insufficient to fully utilize larger model capacity.
- What evidence would resolve it: Training curves and performance comparisons across varying dataset sizes specifically for 32B+ models.

### Open Question 2
- Question: Can template-guided reasoning synthesis be automated or generalized without relying on a specific strong instruction-following model (Claude 3.7 Sonnet)?
- Basis in paper: [inferred] The reasoning data synthesis pipeline depends entirely on Claude 3.7 Sonnet for trajectory generation. The paper notes that direct distillation from general reasoning models (Claude4-thinking) causes performance decline because they "devote efforts to solving the informal problem instead of formalizing it."
- Why unresolved: The template design encodes human expertise, but the synthesis still requires a capable LLM; the dependency on a specific model raises questions about reproducibility and scalability.
- What evidence would resolve it: Comparing performance when using different instruction-following models for synthesis, or developing methods to generate high-quality reasoning trajectories without such strong supervision.

### Open Question 3
- Question: How can autoformalization models be extended to reliably handle geometry and combinatorics problems, which were explicitly excluded?
- Basis in paper: [explicit] Appendix A.1 states: "The problem type field should not be Geometry or Combinatorics, since these two types of problems are still challenging to formalize for existing LLMs."
- Why unresolved: These domains involve spatial reasoning, diagrammatic representations, and complex enumeration that lack straightforward formal mappings in current theorem provers.
- What evidence would resolve it: A dedicated benchmark and successful formalization results on geometry/combinatorics problems using extended or alternative methods.

### Open Question 4
- Question: Can BEq equivalence verification be made more robust without introducing LLM-as-a-judge bias?
- Basis in paper: [explicit] Appendix B.9 states: "We will refine our evaluation methodology in future work" regarding BEq limitations, noting that alternative LLM-based judges "may introduce additional bias due to the randomness of LLM's outputs."
- Why unresolved: The current `exact?` tactic-only approach is efficient but may miss semantically equivalent formulations; adding theorem-proving models increases cost with marginal gains; LLM judges introduce randomness.
- What evidence would resolve it: Development of a deterministic, comprehensive equivalence metric that balances computational efficiency with accuracy, validated against human judgments.

## Limitations
- Performance on geometry and combinatorics problems remains limited due to exclusion from training data
- 32B model shows no clear advantage over 7B model with current dataset sizes, suggesting data scale limitations
- BEq verification may miss semantically equivalent formulations, potentially underestimating true model capability

## Confidence
- **High Confidence**: Knowledge distillation pipeline effectiveness (45.9% → 35.1% BEq@1 improvement for Kimina-Autoformalizer in Table 1), BEq@1 scores on FormalMATH-Lite (40.5%), and template-guided reasoning superiority over direct distillation (25.1% vs 21.8% BEq@1)
- **Medium Confidence**: Generalization claims to ProverBench (26.7% BEq@1) and CombiBench, as these are out-of-distribution evaluations with fewer examples and potential domain shift
- **Low Confidence**: Claims about the 32B model's superiority, as the paper only shows 7B and 32B models achieving similar performance on available data, with 32B requiring "more data" to demonstrate advantage

## Next Checks
1. **BEq Verification Validation**: Manually verify 100 random BEq classifications from FormalMATH-Lite test set to estimate false negative rate and assess whether `exact?` verification is sufficient for semantic equivalence.
2. **Template Robustness Test**: Apply the 8-step template to 100 problems from underrepresented domains (geometry, combinatorics) in the training data to measure template failure rate and identify missing template components.
3. **Ablation on Domain Coverage**: Train separate models on FormalMATH-Train subsets (algebra-only, calculus-only, etc.) and evaluate on respective domain-specific benchmarks to quantify domain generalization limits beyond the reported aggregate scores.