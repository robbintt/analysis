---
ver: rpa2
title: 'Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of
  13 Open-Source Large Language Models'
arxiv_id: '2503.16581'
source_url: https://arxiv.org/abs/2503.16581
tags:
- large
- arxiv
- llms
- relevance
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of 13 open-source large language
  models for answering Quranic studies questions using a Retrieval-Augmented Generation
  (RAG) framework. Models are categorized by size (large, medium, small) and evaluated
  on context relevance, answer faithfulness, and answer relevance using human evaluators.
---

# Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models

## Quick Facts
- arXiv ID: 2503.16581
- Source URL: https://arxiv.org/abs/2503.16581
- Authors: Zahra Khalila; Arbi Haza Nasution; Winda Monika; Aytug Onan; Yohei Murakami; Yasir Bin Ismail Radi; Noor Mohammad Osmani
- Reference count: 40
- Large models outperform smaller ones overall, with Llama3.2:3b achieving surprisingly high faithfulness and relevance scores

## Executive Summary
This study investigates the effectiveness of 13 open-source large language models for answering Quranic studies questions using a Retrieval-Augmented Generation (RAG) framework. Models are categorized by size (large, medium, small) and evaluated on context relevance, answer faithfulness, and answer relevance using human evaluators with domain expertise. The research demonstrates that RAG effectively reduces hallucinations and improves response accuracy, with large models generally outperforming smaller ones. Notably, Llama3.2:3b, a small model with only 3 billion parameters, achieved exceptional faithfulness and relevance scores, outperforming some larger models and suggesting that architectural optimization can compensate for parameter limitations.

## Method Summary
The study employs 13 open-source LLMs categorized by size: large (70B parameters: LLaMA3.1:70b, Mixtral8x7b, Qwen2.5:72b), medium (8-9B parameters: LLaMA3:8b, Mixtral:8x7b, Phi3:3.8b, Qwen2.5:14b), and small (1-3.8B parameters: Gemma2:2b, Gemma2:9b, Llama3.2:3b, Qwen2.5:7b, Phi3:3.8b). The RAG framework uses a vector database containing Quranic surahs, hadiths, and tafsir (interpretations) for retrieval. Human evaluators with Quranic studies background assess responses on three dimensions using 5-point Likert scales: context relevance (Precision@k), answer faithfulness, and answer relevance. Inter-rater agreement is measured using Fleiss' Kappa, with values ranging from 0.80 to 0.93 indicating substantial to near-perfect agreement.

## Key Results
- Large models (70B parameters) generally outperformed smaller models in overall response quality
- Llama3.2:3b achieved the highest faithfulness score (4.619) and second-highest relevance score (4.857), outperforming some large models
- The RAG framework effectively reduced hallucinations and improved response accuracy
- Inter-rater agreement was substantial to near-perfect (κ = 0.80-0.93) across evaluation dimensions
- Computational requirements varied significantly by model size, with large models requiring 4× A6000 GPUs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-Augmented Generation reduces hallucinations in domain-specific LLM applications.
- **Mechanism:** External knowledge retrieval from a curated vector database provides grounding context before generation. The LLM generates responses conditioned on retrieved passages rather than relying solely on parametric memory, constraining outputs to align with the source material.
- **Core assumption:** The retrieval step successfully surfaces contextually relevant passages from the knowledge base; poor retrieval propagates irrelevant context into generation.
- **Evidence anchors:**
  - [abstract]: "The RAG framework effectively reduces hallucinations and improves response accuracy."
  - [section] Discussion: "This method alleviated the prevalent issue of hallucination, where language models produce factually inaccurate or irrelevant responses."
  - [corpus]: Neighbor paper "Addressing Hallucinations with RAG and NMISS" confirms RAG mitigation effects in healthcare domain; cross-domain consistency strengthens generalizability claim.
- **Break condition:** Retrieval precision drops below threshold (context relevance scores degrade), causing irrelevant context injection that may increase rather than decrease hallucination risk.

### Mechanism 2
- **Claim:** Architectural optimization can compensate for parameter count limitations in smaller models.
- **Mechanism:** Llama3.2:3b (3B parameters) achieved highest answer faithfulness (4.619) and relevance (4.857) scores, outperforming models with 10x+ parameters. Suggests that training data quality, architectural refinements (e.g., attention patterns, layer normalization), and instruction tuning contribute independently to task performance beyond raw scale.
- **Core assumption:** The evaluation metrics (faithfulness, relevance) capture meaningful dimensions of response quality and are not artifacts of the specific dataset or evaluation protocol.
- **Evidence anchors:**
  - [abstract]: "Llama3.2:3b... achieving surprisingly high faithfulness (4.619) and relevance (4.857) scores, outperforming some large models."
  - [section] Results: "Llama3.2:3b stood out with a score of 4.619, showcasing exceptional faithfulness that rivaled larger models."
  - [corpus]: Weak external validation—no corpus papers directly replicate this small-model superiority pattern; assumption requires independent verification.
- **Break condition:** Domain shifts (e.g., legal, medical) may reintroduce scale-dependent performance gaps; architectural advantages may be dataset-specific.

### Mechanism 3
- **Claim:** Human evaluation with structured rubrics and inter-rater agreement metrics provides reliable quality assessment for sensitive domains.
- **Mechanism:** Evaluators with domain expertise (Quranic studies background) scored responses on 5-point Likert scales across three dimensions. Fleiss' Kappa (κ = 0.80–0.93) indicated substantial to near-perfect agreement, reducing individual bias effects.
- **Core assumption:** Evaluators possess sufficient domain expertise to judge theological accuracy; rubric definitions (faithfulness vs. relevance) remain semantically distinct in practice.
- **Evidence anchors:**
  - [section] Human Evaluators: "Evaluators from a variety of backgrounds were included to make sure the replies were evaluated fairly and without bias."
  - [section] Results Table I: Kappa values range 0.80 (Llama3:8b) to 0.93 (Phi3:3.8b).
  - [corpus]: No corpus papers address inter-rater reliability in RAG evaluation; gap in comparative validation.
- **Break condition:** Complex queries requiring nuanced theological interpretation may reduce agreement; evaluator fatigue over large sample sizes may degrade consistency.

## Foundational Learning

- **Concept: Vector Embeddings & Semantic Search**
  - **Why needed here:** RAG depends on converting text into high-dimensional vectors and retrieving via similarity search. Understanding that embeddings capture semantic meaning (not keyword matching) is prerequisite for debugging retrieval quality.
  - **Quick check question:** Given a query about "Surah Al-Hadid's virtues," would a keyword search retrieve a passage mentioning "benefits of iron" without the word "virtues"? Explain how embeddings handle this.

- **Concept: Precision@k Metric**
  - **Why needed here:** Context relevance uses Precision@k to quantify retrieval quality. Understanding numerator (relevant results) vs. denominator (total retrieved) is essential for interpreting scores and optimizing retrieval.
  - **Quick check question:** If Precision@5 = 0.6, how many retrieved passages were relevant? What does this imply about the other 40%?

- **Concept: Likert Scale Limitations**
  - **Why needed here:** Human evaluation uses ordinal 1–5 scales. Treating these as interval data (computing means) assumes equal spacing between adjacent values—an assumption that may not hold.
  - **Quick check question:** Is the difference between scores 1 and 2 psychologically equivalent to the difference between 4 and 5? How might this affect interpretation of mean scores?

## Architecture Onboarding

- **Component map:** User query → preprocessing → Query embedding → vector similarity search → top-k passages from Quranic surah database → Context assembly → LLM generation → Response + citations → Human evaluation

- **Critical path:**
  1. Vector database quality → retrieval precision → context relevance score
  2. Retrieved context quality → LLM faithfulness → answer faithfulness score
  3. Query clarity + context alignment → semantic grounding → answer relevance score

- **Design tradeoffs:**
  - **Large models (70B parameters):** Highest accuracy, highest computational cost (4× A6000 GPUs), slower inference—suitable for batch processing or high-value queries
  - **Medium models (8–9B parameters):** Balanced performance-efficiency, deployable on single GPU—suitable for real-time applications
  - **Small models (1–3.8B parameters):** Fastest inference, lowest VRAM requirements, but generally lower scores—Llama3.2:3b exception suggests architecture matters; test before dismissing
  - **Human vs. automated evaluation:** Human provides gold-standard theological assessment but scales poorly; LLM-based automated evaluation (mentioned as future work) trades reliability for scalability

- **Failure signatures:**
  - **Low context relevance (<0.3):** Retrieval failure—check embedding quality, vector database indexing, or query preprocessing
  - **High context relevance but low faithfulness (<2.5):** Generation failure—LLM ignoring retrieved context; check prompt formatting or model instruction-following capability
  - **High faithfulness but low relevance:** Over-constrained generation—model adhering to retrieved text but missing user intent; refine prompt to balance grounding vs. responsiveness
  - **Low inter-rater agreement (κ < 0.6):** Ambiguous rubric definitions or insufficient evaluator training

- **First 3 experiments:**
  1. **Baseline retrieval calibration:** Measure Precision@k across k=3,5,10 on held-out queries to identify optimal retrieval window before context window saturation
  2. **Ablation study (per paper's future work):** Run identical queries with RAG enabled vs. disabled on same models to quantify hallucination reduction magnitude with statistical significance testing
  3. **Small-model deep dive:** Replicate Llama3.2:3b evaluation on separate domain dataset (e.g., legal or medical) to test whether architectural advantage generalizes or is dataset-specific

## Open Questions the Paper Calls Out
- How can automated factuality scoring complement human evaluation to scale quality assessment?
- What architectural features enable small models like Llama3.2:3b to outperform larger models in specific domains?
- How can the RAG framework be optimized for real-time applications while maintaining response quality?

## Limitations
- Human evaluation introduces subjectivity and scale constraints; automated factuality scoring remains unimplemented
- Small-model performance superiority (Llama3.2:3b) lacks external validation and may be dataset-specific
- No ablation study isolating RAG's contribution to hallucination reduction

## Confidence
- **High:** RAG reduces hallucinations when retrieval succeeds (mechanism sound, evidence consistent)
- **Medium:** Large models outperform smaller ones overall (expected scaling relationship, though exceptions noted)
- **Low:** Architectural optimization compensates for parameter limitations (correlation without mechanism, requires external validation)

## Next Checks
1. Conduct ablation study comparing RAG-enabled vs. disabled models on identical queries with statistical significance testing
2. Replicate Llama3.2:3b evaluation on separate domain (legal/medical) to test architectural advantage generalizability
3. Implement automated factuality scoring alongside human evaluation to validate faithfulness metric reliability