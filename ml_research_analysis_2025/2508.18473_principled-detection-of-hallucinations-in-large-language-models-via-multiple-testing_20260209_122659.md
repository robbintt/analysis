---
ver: rpa2
title: Principled Detection of Hallucinations in Large Language Models via Multiple
  Testing
arxiv_id: '2508.18473'
source_url: https://arxiv.org/abs/2508.18473
tags:
- detection
- semantic
- entropy
- dataset
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting hallucinations in
  Large Language Models (LLMs) by formulating it as a hypothesis testing problem and
  drawing parallels to out-of-distribution detection. The core method introduces a
  multiple-testing-inspired detection pipeline that integrates various evaluation
  scores using conformal p-values, systematically combining them to improve reliability.
---

# Principled Detection of Hallucinations in Large Language Models via Multiple Testing

## Quick Facts
- **arXiv ID:** 2508.18473
- **Source URL:** https://arxiv.org/abs/2508.18473
- **Reference count:** 18
- **One-line primary result:** Multiple-testing-inspired method detects LLM hallucinations with strong AUROC and 10% false alarm rate across diverse models and datasets.

## Executive Summary
This paper addresses the challenge of detecting hallucinations in Large Language Models (LLMs) by formulating it as a hypothesis testing problem with controlled false alarm rates. The authors propose a principled multiple-testing framework that aggregates various evaluation scores using conformal p-values to systematically combine detection signals. Extensive experiments demonstrate the method achieves uniformly strong performance across diverse LLM architectures (LLaMA-2, LLaMA-3, Mistral, DeepSeek-v2.5) and datasets (CoQA, TriviaQA), with consistently high detection power at 10% false alarm rate while outperforming state-of-the-art approaches in most cases.

## Method Summary
The method reformulates hallucination detection as a hypothesis testing problem, treating each prompt as either non-hallucinated (null hypothesis) or hallucinated (alternative). It leverages existing detection scores based on output likelihoods or sampled generations, avoiding additional external datasets or training. The core approach uses 20 sampled generations per prompt with pure sampling (temperature=1.0, top_p=1.0, top_k=0), computes four different scores (Semantic Entropy, Alpha Semantic Entropy, Spectral Eigenvalue, Lexical Similarity), and integrates them using conformal p-values calibrated against a set of non-hallucinated prompts. The decision is made via a Benjamini-Hochberg-style multiple testing procedure that controls the false alarm rate, with a default calibration set of 1,000 prompts constructed using ROUGE-L similarity against reference answers.

## Key Results
- The proposed method achieves uniformly strong performance across diverse LLM architectures (LLaMA-2, LLaMA-3, Mistral, DeepSeek-v2.5) and datasets (CoQA, TriviaQA).
- Detection power at 10% false alarm rate is consistently high, with robust AUROC values demonstrating strong overall performance.
- The method outperforms state-of-the-art approaches in most cases, showing significant worst-case improvements and high reliability across different models and datasets.

## Why This Works (Mechanism)

### Mechanism 1: Multiple Score Aggregation via Conformal P-values
Combining multiple hallucination detection scores systematically improves robustness across diverse models and datasets. Individual scores capture different aspects of hallucination patterns (semantic consistency, lexical overlap, spectral properties). Conformal p-values normalize these heterogeneous scores against a calibration distribution, then a Benjamini-Hochberg-like procedure integrates them by rejecting the global null if any adjusted p-value falls below its ranked threshold. This works when at least one individual score provides signal for any given model-dataset combination.

### Mechanism 2: False Alarm Rate Control via Calibration-Based Threshold
The method provides theoretical bounds on false alarm rate (PF ≤ α) when calibration set size satisfies concentration conditions. Conformal p-values are computed as rank statistics against calibration scores. The BH-derived threshold applies sequential testing with correction factor α/(1+ε)·(∑1/i)·(j/K), ensuring that under H0, the probability of false rejection is bounded. This relies on exchangeability between calibration and test samples under the null hypothesis.

### Mechanism 3: ROUGE-L Based Proxy for Ground Truth Labeling
ROUGE-L similarity between generations and reference answers provides adequate proxy for hallucination status in calibration set construction. A prompt is labeled as non-hallucinated if ≥90% of 20 sampled generations achieve ROUGE-L ≥ 0.3 against the reference. This tolerance accommodates rephrasings while filtering systematically incorrect generators. This approach assumes ROUGE-L captures semantic correctness for the target domain.

## Foundational Learning

- **Concept: Hypothesis Testing (Null/Alternative, Type I/II Errors)**
  - Why needed here: The entire framework reformulates hallucination detection as testing H0 (prompt won't hallucinate) vs. H1 (will hallucinate), with controlled false alarm rate (Type I error).
  - Quick check question: Given α=0.1, what proportion of genuinely non-hallucinated prompts would you expect to incorrectly flag over many trials?

- **Concept: Conformal Prediction and Conformal P-values**
  - Why needed here: Conformal p-values transform raw scores into calibrated probabilities without requiring known null distributions, enabling principled threshold setting.
  - Quick check question: In the formula qj_con = (1 + |{i: sj_i ≥ tj_test}|) / (1 + |C|), what does the numerator count?

- **Concept: Benjamini-Hochberg Procedure for Multiple Testing**
  - Why needed here: Algorithm 1 adapts BH to combine K dependent scores while controlling family-wise error, rejecting if any sorted p-value falls below its adjusted threshold.
  - Quick check question: If sorted p-values are [0.02, 0.05, 0.15, 0.30] with α=0.1 and K=4, which indices pass the BH threshold?

## Architecture Onboarding

- **Component map:** Input prompt + 20 sampled generations -> K=4 parallel score functions -> Conformal p-value computation -> Benjamini-Hochberg decision module -> Binary hallucination decision

- **Critical path:**
  1. Sample 20 generations per prompt using pure sampling
  2. Compute all K scores on test prompt and calibration prompts
  3. For each score j, compute conformal p-value qj_con
  4. Sort p-values ascending: dq1_con ≤ dq2_con ≤ ... ≤ dqK_con
  5. Reject (declare hallucination) if ∃j such that dqj_con ≤ α/(1+ε) · (∑_{i=1}^K 1/i) · (j/K)

- **Design tradeoffs:**
  - Calibration size: 1,000 provides empirical utility; 5,000+ satisfies theoretical bounds but requires more labeled data
  - ROUGE-L threshold (τ=0.3): Balances inclusion of valid rephrasings against contamination risk
  - Tolerance (θ=0.1): Allows 2/20 incorrect generations; stricter values shrink calibration pool
  - Hyperparameter ε: Controls conservatism; higher ε tightens thresholds, reducing false alarms but potentially lowering detection power

- **Failure signatures:**
  - High variance in detection power across calibration resampling: Indicates insufficient calibration size
  - Zero Lexical Similarity power on math datasets: Preprocessing to numeric answers eliminates lexical diversity this score requires
  - Detection power <50% with AUROC <80%: Likely calibration distribution mismatch with test data
  - Semantic Entropy consistently weakest: May indicate entailment clustering failures for domain-specific language

- **First 3 experiments:**
  1. Single-model baseline verification: Compute all 4 scores on LLaMA-2-13B + CoQA, confirm AUROC values approximate Table 2
  2. Calibration size ablation: Run pipeline with |C| ∈ {500, 1000, 2000, 3000} on fixed model-dataset pair, observe variance in detection power and false alarm rate adherence
  3. Cross-distribution calibration: Train calibration set on CoQA, test on TriviaQA (and reverse), measure degradation vs. within-distribution calibration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can "LLM-as-a-judge" metrics replace ROUGE-L for labeling calibration data to improve handling of semantic variations?
- **Basis in paper:** [explicit] The Conclusion states that ROUGE-L may not fully capture semantic variations and suggests "leveraging ‘LLM-as-a-judge’ to assess factual consistency" as a promising direction.
- **Why unresolved:** The current method relies on lexical overlap (ROUGE-L) to label non-hallucinated prompts, which struggles with valid rephrasings and limits calibration dataset construction in certain domains.
- **What evidence would resolve it:** Experiments comparing detection power and false alarm rates when calibration sets are constructed using an LLM-judge versus the ROUGE-L baseline.

### Open Question 2
- **Question:** How does the statistical dependency between input scores affect the tightness of the false alarm rate bounds?
- **Basis in paper:** [inferred] The methodology employs the general Benjamini-Hochberg procedure to handle dependent scores, but the paper does not analyze the specific correlation structures between the integrated scores.
- **Why unresolved:** While the procedure controls the false alarm rate, high correlation between scores could result in overly conservative thresholds, potentially reducing detection power.
- **What evidence would resolve it:** A theoretical or empirical analysis of score correlation matrices to determine if tighter bounds can be derived by modeling the specific dependency structure.

### Open Question 3
- **Question:** What is the minimum calibration dataset size required to satisfy the theoretical false alarm rate guarantees in Remark 3.1?
- **Basis in paper:** [inferred] The authors note that the default calibration size of 1,000 is "not large enough to guarantee strong theoretical performance" despite achieving strong empirical results.
- **Why unresolved:** There is a discrepancy between the sample sizes required by the theoretical concentration bounds and the sizes observed to be effective in practice.
- **What evidence would resolve it:** A scaling analysis plotting calibration set size against the empirical violation rate of the false alarm bound to identify the exact crossover point for theoretical validity.

## Limitations

- **Calibration Set Dependency:** The method's reliability hinges on the calibration set accurately representing non-hallucinated behavior, which may be challenging for datasets where hallucinations are pervasive.
- **Score Function Limitations:** The four baseline scores capture specific hallucination patterns but may fail on domain-specific language or when answers are numerical or highly paraphrased.
- **False Alarm Rate Guarantee Assumptions:** The theoretical bound requires exchangeability between calibration and test prompts under the null hypothesis, which can be violated by calibration set contamination.

## Confidence

- **High Confidence:** Detection power consistently outperforms individual scores when at least one score provides signal for the model-dataset pair.
- **Medium Confidence:** The theoretical false alarm rate bound holds under the stated assumptions and the ROUGE-L threshold effectively balances valid rephrasing inclusion against calibration set contamination.
- **Low Confidence:** The method's performance on datasets with pervasive hallucinations or requiring semantic equivalence beyond lexical overlap, and the exact impact of ε hyperparameter selection across diverse scenarios.

## Next Checks

1. **Cross-Dataset Calibration Robustness:** Train the calibration set on CoQA and test on TriviaQA (and vice versa), measuring detection power degradation compared to within-dataset calibration to validate robustness to distribution shifts.

2. **Calibration Set Size Sensitivity Analysis:** Systematically vary the calibration set size (|C| ∈ {500, 1000, 2000, 3000}) on a fixed model-dataset pair, quantifying the variance in detection power and false alarm rate adherence to test the practical minimum calibration size required.

3. **Score Function Failure Mode Isolation:** For a dataset known to have numerical answers (e.g., DROP), measure each score's individual detection power to confirm that Lexical Similarity drops to near-zero while other scores maintain reasonable performance, validating the need for score aggregation.