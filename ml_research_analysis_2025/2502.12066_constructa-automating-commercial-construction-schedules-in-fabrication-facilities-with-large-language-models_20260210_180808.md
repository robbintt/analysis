---
ver: rpa2
title: 'CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities
  with Large Language Models'
arxiv_id: '2502.12066'
source_url: https://arxiv.org/abs/2502.12066
tags:
- construction
- context
- project
- scheduling
- dependencies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONSTRUCTA, a framework that leverages large
  language models (LLMs) to automate construction scheduling in complex projects like
  semiconductor fabrication. It addresses challenges in handling intricate task dependencies,
  dynamic project contexts, and expert-driven preferences by integrating domain-specific
  knowledge through retrieval-augmented generation (RAG), context-sampling techniques,
  and construction-specific reinforcement learning from human feedback (RLHF).
---

# CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models

## Quick Facts
- arXiv ID: 2502.12066
- Source URL: https://arxiv.org/abs/2502.12066
- Reference count: 29
- Key outcome: Automates construction scheduling using LLMs with +42.3% MVP, +79.1% DA, and +28.9% AP improvements over baselines

## Executive Summary
CONSTRUCTA introduces a framework leveraging large language models to automate construction scheduling in complex semiconductor fabrication projects. The system addresses challenges in handling intricate task dependencies, dynamic project contexts, and expert-driven preferences through retrieval-augmented generation, context-sampling techniques, and construction-specific reinforcement learning from human feedback. Experiments on a proprietary dataset of 4,340 semiconductor fabrication activities demonstrate significant performance improvements across three core scheduling tasks: Missing Value Prediction, Dependency Analysis, and Automated Planning.

## Method Summary
CONSTRUCTA integrates three key components: Static RAG with 500-token chunks and all-MiniLM-L6-v2 embeddings for domain knowledge retrieval; Knowledge RAG using context-specific sampling (First-Order, Hierarchical, Sequential) to retrieve relevant knowledge chunks; and CPA-RLHF with dual agents (GPT-4o Plan Agent and Llama3.2-3B Expert Agent) trained via supervised fine-tuning and direct preference optimization. The system processes tasks through dependency graph traversal, context sampling, knowledge retrieval, and preference-aligned planning to generate expert-quality construction schedules.

## Key Results
- Missing Value Prediction: +42.3% accuracy improvement over baseline methods
- Dependency Analysis: +79.1% accuracy in relational column prediction
- Automated Planning: +28.9% accuracy in start/finish date prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale context sampling captures both local constraints and long-range dependencies in construction schedules
- Evidence: Dependency graph shows mean degree 3.86 (some nodes with 20 connections) and average maximal hop distance of 13.93 (max 73). KRAG samples three context types—First-Order, Hierarchical, and Sequential—which collectively cover immediate constraints, phase-level grouping, and task-flow sequences.

### Mechanism 2
- Claim: Static domain knowledge alone is insufficient; gains emerge when retrieval is conditioned on task-specific context
- Evidence: Table 1 shows Static RAG yields marginal gains (MVP: 11.6% vs 14.6% baseline), while Knowledge RAG boosts MVP to 51.4% and DA to 77.9%, demonstrating that retrieval relevance drives performance.

### Mechanism 3
- Claim: Dual-agent preference alignment distills expert-aligned scheduling logic into a smaller model while reducing context verbosity
- Evidence: Figure 4 shows reduced context length after CPA-DPO alignment, suggesting the Expert Agent learns to filter redundant details and retain essential constraints. Loss combines SFT, Context-Rule Interaction (LCR), and Preference Alignment (LPA).

## Foundational Learning

- Concept: Work Breakdown Structure (WBS)
  - Why needed here: Hierarchical context sampling relies on WBS to group related tasks; prompts reference WBS for phase-level organization
  - Quick check question: Given a task with WBS "2.3.1", which tasks would be included in its hierarchical context (up to 2 levels)?

- Concept: Dependency Types (FS, SS, FF, SF)
  - Why needed here: Sequential and first-order contexts traverse edges labeled with these dependency types; rules reference them for timing constraints
  - Quick check question: If Task A has a Finish-to-Start (FS) dependency on Task B, what does this imply about their scheduling?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: CPA-DPO is the core alignment mechanism; understanding the loss formulation (LPA) is necessary for debugging training
  - Quick check question: How does DPO differ from PPO in terms of reward model requirements?

## Architecture Onboarding

- Component map: SRAG -> KRAG -> CPA-DPO
- Critical path:
  1. Build dependency graph from schedule data (extract predecessors/successors)
  2. Pre-compute embeddings for knowledge corpus (500-token chunks, all-MiniLM-L6-v2)
  3. For each task, sample Ci and retrieve top-k knowledge chunks
  4. Run Plan Agent to generate candidate schedules; collect preference labels
  5. Train Expert Agent via SFT → CPA-DPO

- Design tradeoffs:
  - Context window vs. coverage: 3 hops captures most dependencies but misses long-range edges (max hop = 73 in data)
  - Model size vs. deployment cost: Expert Agent (3B) is smaller but may underperform Plan Agent (GPT-4o) on edge cases
  - Chunk size (500 tokens) balances retrieval precision vs. context overhead

- Failure signatures:
  - High MVP error on tasks with sparse local context (few predecessors/successors)
  - DA degradation in high-complexity areas (SU E, 10E) where interdependencies exceed sampling horizon
  - Preference alignment collapse: Expert Agent outputs generic rules

- First 3 experiments:
  1. Ablate context types: Run MVP/DA/AP with only First-Order, only Hierarchical, only Sequential context; compare to full Ci
  2. Vary hop depth: Test Sequential Context at 2, 3, 4 hops on high-complexity areas; measure DA improvement vs. context length increase
  3. Preference label sensitivity: Train Expert Agent with 25%, 50%, 100% of preference pairs; evaluate whether performance scales with label quantity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of multimodal inputs (e.g., visual site data or architectural blueprints) impact the scheduling accuracy compared to the current text-only approach?
- Basis: The Conclusion states future work includes "incorporating multimodal inputs."

### Open Question 2
- Question: Can the framework maintain its reported performance levels when applied to open-source or synthetic construction datasets rather than proprietary semiconductor data?
- Basis: Appendix A.6 suggests future research could explore open-source construction datasets or synthetic data generation techniques.

### Open Question 3
- Question: What are the computational latency and efficiency trade-offs when deploying the CPA-RLHF pipeline for real-time integration with industry-standard project management software?
- Basis: Section 5 notes that future work must address "deployment challenges, including computational efficiency, latency, and seamless integration."

## Limitations

- Proprietary dataset (4,340 semiconductor fabrication activities) limits external validation and generalizability to other construction domains
- Context-sampling strategy's 3-hop coverage may miss long-range dependencies given maximal hop distance of 73 in the data
- Performance improvements are relative to unspecified baseline methods without detailed ablation studies of individual component contributions

## Confidence

- **High**: Context sampling coverage based on dependency graph statistics; conditioned retrieval benefits from Table 1 results
- **Medium**: Dual-agent distillation shown in Figure 4 but lacking detailed preference diversity analysis
- **Low**: Generalization claims beyond semiconductor fabrication without validation on other construction domains

## Next Checks

1. Ablation study: Test MVP/DA/AP performance with only First-Order, only Hierarchical, and only Sequential contexts to quantify individual contributions and identify potential coverage gaps
2. Preference alignment robustness: Train Expert Agent with varying fractions (25%, 50%, 100%) of preference pairs to assess scaling behavior and overfitting risks
3. Cross-domain transferability: Apply CONSTRUCTA to a non-semiconductor construction dataset (e.g., commercial building) to validate WBS and dependency assumptions beyond the proprietary data