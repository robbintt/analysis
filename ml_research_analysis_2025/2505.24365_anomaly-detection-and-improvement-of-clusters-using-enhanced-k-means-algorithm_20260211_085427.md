---
ver: rpa2
title: Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm
arxiv_id: '2505.24365'
source_url: https://arxiv.org/abs/2505.24365
tags:
- algorithm
- outliers
- clusters
- dataset
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an enhanced k-means algorithm that integrates\
  \ outlier removal with iterative centroid refinement to improve clustering quality.\
  \ The method iteratively eliminates points lying more than two standard deviations\
  \ from their cluster centroid (using Chebyshev\u2019s inequality), recomputes centroids,\
  \ and repeats until intra-cluster variance converges to a minimum."
---

# Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm

## Quick Facts
- arXiv ID: 2505.24365
- Source URL: https://arxiv.org/abs/2505.24365
- Reference count: 0
- This paper presents an enhanced k-means algorithm that integrates outlier removal with iterative centroid refinement to improve clustering quality.

## Executive Summary
This paper introduces an enhanced k-means algorithm that iteratively removes outliers beyond two standard deviations from cluster centroids and recomputes centroids until intra-cluster variance converges. Tested on synthetic and UCI datasets, the method achieves up to 88.1% variance reduction and improves supervised metrics like accuracy and F1 score by up to 22.5% and 20.8% respectively. The approach demonstrates robust outlier detection while maintaining cluster integrity, outperforming standard k-means across multiple evaluation metrics.

## Method Summary
The enhanced k-means algorithm begins with standard k-means clustering to establish initial centroids, then iteratively removes points lying more than two standard deviations from their cluster centroid (using Chebyshev's inequality), recomputes centroids, and repeats until intra-cluster variance converges to a minimum. The method treats the removed points as outliers, providing both refined clusters and an anomaly detection capability. The algorithm was tested on synthetic 2D data, Breast Cancer Wisconsin, and Red Wine Quality datasets, showing significant improvements in clustering quality and supervised evaluation metrics.

## Key Results
- Variance reduction up to 88.1% compared to standard k-means
- Calinski-Harabasz score improvements up to 39.4%
- Accuracy and F1 score improvements of up to 22.5% and 20.8% on wine dataset

## Why This Works (Mechanism)

### Mechanism 1: Iterative Variance Reduction via Outlier Removal
The algorithm removes points beyond 2 standard deviations from cluster centroids, progressively reducing intra-cluster variance. After initial k-means clustering, Euclidean distances from each point to its centroid are computed, and points exceeding 2σ are flagged as outliers and removed. Centroids are recalculated on remaining points, and the process repeats. Each iteration tightens clusters by eliminating high-variance contributors. The approach assumes outliers are primary drivers of variance and their removal doesn't discard meaningful signal. Performance degrades when outlier density is uniform across clusters or when removal threshold is too aggressive for small datasets.

### Mechanism 2: Centroid Stabilization Through Local Outlier Detection
Recomputing centroids after outlier removal shifts cluster centers toward denser regions, improving separation metrics. Standard k-means centroids are pulled toward outliers due to squared distance minimization, but removing distant points first allows recalculated centroids to better represent true cluster modes. This improves both compactness (lower Davies-Bouldin) and separation (higher Calinski-Harabasz). The method assumes true cluster structure is approximately spherical or von Mises-Fisher distributed. Performance degrades on non-convex or elongated clusters where centroid does not capture cluster shape.

### Mechanism 3: Dual-Purpose Output for Clustering and Anomaly Detection
The iterative process yields both refined clusters and a labeled outlier set, enabling supervised evaluation without separate anomaly detection pipelines. Outliers removed during iteration are accumulated into a set that can be treated as anomaly labels for downstream tasks. The paper validates this by showing improved extrinsic metrics when ground truth exists. The approach assumes points far from any centroid are more likely to be true anomalies in the domain sense. False positive rates increase when legitimate minority classes are spatially distant from majority clusters.

## Foundational Learning

- **Chebyshev's Inequality**
  - Why needed here: The algorithm uses this to justify the 2σ threshold as a distribution-agnostic bound (≥75% coverage) since Euclidean distances from centroids are not Gaussian.
  - Quick check question: If your distance distribution were truly Gaussian, what percentage would fall within 2σ, and how would that change the threshold rationale?

- **Intrinsic Clustering Metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)**
  - Why needed here: The paper evaluates cluster quality without ground truth using these three complementary measures—compactness, separation ratio, and average similarity to most similar cluster.
  - Quick check question: Why might Silhouette increase while Davies-Bouldin decreases, and what does that indicate about cluster structure?

- **K-Means Sensitivity to Initialization and Outliers**
  - Why needed here: Standard k-means minimizes squared error, making centroids vulnerable to being pulled by extreme values. Understanding this motivates the outlier-removal-first approach.
  - Quick check question: If you ran standard k-means on a dataset with one extreme outlier, would the final centroid positions differ significantly from k-means on the same data with that point removed?

## Architecture Onboarding

- Component map:
  Input Layer (Dataset X, parameter k) -> Initial Clustering (Standard k-means) -> Distance Computation (Euclidean distances) -> Outlier Filter (Remove points > mean + 2σ) -> Centroid Update (Recalculate centroids) -> Convergence Check (Compare variance) -> Output (Refined clusters, outlier set Y)

- Critical path: Initial k-means → distance calculation → outlier removal → centroid update → variance check → (loop or terminate)
  The convergence condition gates all downstream outputs; if variance does not decrease meaningfully, the loop may run indefinitely or terminate prematurely.

- Design tradeoffs:
  Chebyshev bound (k=2 vs. higher): Lower values remove more points (higher false positive risk); higher values are conservative (may leave noise in clusters). Currently limited to Minkowski-style distances; cannot handle angular or spatial metrics natively. Best on spherical/von Mises-Fisher distributions; performance degrades on elongated or non-convex clusters.

- Failure signatures:
  Non-convergence: Variance oscillates without reaching minimum—may indicate poorly chosen k or highly overlapping clusters. Over-pruning: >15-20% of data classified as outliers suggests threshold too aggressive or dataset not suited to centroid-based clustering. Metric degradation on high-k: Effectiveness drops as cluster count increases (clusters become tighter, fewer outliers deviate sufficiently).

- First 3 experiments:
  1. Baseline comparison on synthetic data: Run enhanced k-means vs. standard k-means on provided 2D synthetic dataset (1000 points, 5 centroids). Log variance reduction, Silhouette, and Calinski-Harabasz across iterations to verify 18.7% variance reduction claim.
  2. Threshold sensitivity analysis: Vary Chebyshev bound (k=1.5, 2.0, 2.5, 3.0) on UCI Wine Quality dataset. Plot % outliers removed vs. accuracy/F1 to identify optimal threshold and validate the paper's claim that 2 is mathematically justified.
  3. Break test on non-spherical data: Generate elongated clusters (e.g., two parallel ellipses) or non-convex shapes (moons/circles). Compare enhanced k-means against DBSCAN or DelTriC from corpus to confirm where centroid-based assumptions fail.

## Open Questions the Paper Calls Out
None

## Limitations
- The algorithm assumes spherical or von Mises-Fisher distributed clusters, limiting generalizability to real-world data with non-convex or elongated structures.
- The 2σ outlier threshold is mathematically justified but lacks empirical sensitivity analysis—optimal thresholds may vary by dataset.
- The algorithm's effectiveness drops with higher cluster counts, suggesting scalability concerns.

## Confidence
- High confidence: Variance reduction claims (57.9-88.1%) and intrinsic metric improvements (Silhouette +11.0%, Calinski-Harabasz +31.7%) are well-supported by UCI dataset results.
- Medium confidence: Extrinsic metric improvements (accuracy +22.5%, F1 +20.8%) depend on outlier labels being meaningful anomalies, not just statistical outliers.
- Low confidence: The 2σ threshold as universally optimal; corpus shows diverse outlier detection strategies suggesting no single best threshold.

## Next Checks
1. Implement and test the algorithm on non-spherical datasets (moons, ellipses) to verify the stated limitations and compare against DBSCAN/DelTriC.
2. Conduct threshold sensitivity analysis by varying the Chebyshev bound (k=1.5, 2.0, 2.5, 3.0) on UCI Wine Quality to identify optimal removal rates.
3. Perform ablation studies comparing iterative outlier removal vs. one-pass removal to quantify the incremental benefit of the iterative approach.