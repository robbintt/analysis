---
ver: rpa2
title: Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty
arxiv_id: '2512.17145'
source_url: https://arxiv.org/abs/2512.17145
tags:
- hypotheses
- solomonoff
- hypothesis
- uncertainty
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Solomonoff-inspired framework for ranking
  and combining LLM-generated hypotheses in Mini-ARC tasks. Instead of selecting a
  single hypothesis, it evaluates multiple candidates using simplicity (token length)
  and predictive accuracy, producing per-cell probability distributions.
---

# Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty

## Quick Facts
- arXiv ID: 2512.17145
- Source URL: https://arxiv.org/abs/2512.17145
- Authors: Josh Barber; Rourke Young; Cameron Coombe; Will Browne
- Reference count: 32
- Primary result: Solomonoff-inspired framework achieved 64% accuracy on noisy Mini-ARC tasks versus 48% for Bayesian Model Averaging

## Executive Summary
This paper introduces a novel framework for ranking and combining LLM-generated hypotheses in prediction tasks under uncertainty. The approach draws inspiration from Solomonoff induction, using simplicity (measured by token length) and predictive accuracy to evaluate multiple hypotheses rather than selecting a single one. By producing per-cell probability distributions, the method demonstrates better calibration of confidence and more even probability spread across hypotheses compared to traditional approaches.

The framework shows particular strength in handling noisy data, achieving 64% accuracy on a high-noise task with 20 hypotheses versus 48% for Bayesian Model Averaging. The Solomonoff-inspired approach favors simpler, more generalizable explanations and provides a principled way to reason under uncertainty when dealing with sparse data, addressing a critical challenge in applying LLMs to complex reasoning tasks.

## Method Summary
The method evaluates multiple LLM-generated hypotheses using a combination of simplicity (measured by token length) and predictive accuracy. Instead of selecting a single hypothesis, it assigns probabilities to each based on their simplicity-weighted performance, producing per-cell probability distributions. This approach draws from Solomonoff induction principles, where simpler hypotheses are weighted more heavily. The framework integrates these weighted hypotheses to generate final predictions, allowing for uncertainty quantification and more robust decision-making in noisy environments.

## Key Results
- Achieved 64% accuracy on a noisy Mini-ARC task versus 48% for Bayesian Model Averaging
- On a high-noise task with 20 hypotheses, achieved 60% accuracy versus 64% for BMA
- Demonstrated better calibration of confidence and more even probability spread across hypotheses

## Why This Works (Mechanism)
The framework leverages Solomonoff induction principles by combining multiple hypotheses rather than selecting a single one. By weighting hypotheses based on their simplicity (token length) and predictive accuracy, it captures uncertainty while favoring explanations that are both accurate and generalizable. The probabilistic combination of hypotheses allows the system to hedge against noise and uncertainty, while the simplicity prior helps prevent overfitting to complex patterns that may not generalize.

## Foundational Learning
- **Solomonoff induction**: Why needed - provides theoretical foundation for simplicity-based hypothesis weighting; Quick check - understand how prior probability decreases with hypothesis complexity
- **Bayesian Model Averaging**: Why needed - serves as baseline comparison; Quick check - know how BMA combines models using posterior probabilities
- **LLM hypothesis generation**: Why needed - source of candidate solutions; Quick check - understand how LLMs generate pattern hypotheses from examples
- **ARC task format**: Why needed - evaluation domain; Quick check - know the input-output pattern completion structure
- **Simplicity bias**: Why needed - core weighting mechanism; Quick check - understand how token length serves as proxy for hypothesis complexity
- **Uncertainty quantification**: Why needed - allows calibrated confidence; Quick check - know how probability distributions capture model uncertainty

## Architecture Onboarding

**Component map:** LLM Hypothesis Generator -> Hypothesis Ranker (Simplicity + Accuracy) -> Probability Distribution Combiner -> Final Prediction

**Critical path:** Hypothesis generation → Simplicity and accuracy evaluation → Weighted probability combination → Output prediction

**Design tradeoffs:** Simplicity weighting vs. accuracy weighting (tradeoff between generalization and precision); Single hypothesis selection vs. ensemble approach (certainty vs. robustness to uncertainty)

**Failure signatures:** Over-reliance on complex hypotheses (overfitting); Under-confidence in correct but complex solutions; Poor performance when simplicity doesn't correlate with correctness

**3 first experiments:** 1) Generate hypotheses for simple ARC tasks and manually verify simplicity ranking; 2) Compare single hypothesis selection vs. ensemble approach on clean data; 3) Test sensitivity to noise by gradually increasing perturbation levels

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Experimental validation is narrow, focusing exclusively on Mini-ARC tasks without broader generalization testing
- Comparison to Bayesian Model Averaging may be limited by differences in implementation specifics that aren't fully transparent
- Assumption that simpler hypotheses are more likely to be correct may not hold in real-world domains where complexity is inherent to the problem structure

## Confidence
- **High confidence**: Methodological framework and its theoretical grounding in Solomonoff induction
- **Medium confidence**: Comparative performance metrics, given the limited scope of experiments
- **Low confidence**: Generalizability claims, as no external validation was performed beyond the Mini-ARC domain

## Next Checks
1. Replicate experiments across multiple benchmark datasets beyond Mini-ARC to test domain generalizability
2. Conduct ablation studies to isolate the contribution of Solomonoff-inspired simplicity weighting versus other framework components
3. Test method's robustness against adversarial or intentionally misleading hypotheses to evaluate uncertainty handling under challenging conditions