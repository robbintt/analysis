---
ver: rpa2
title: A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control
  in Low-Cost Greenhouse Systems
arxiv_id: '2512.01167'
source_url: https://arxiv.org/abs/2512.01167
tags:
- control
- lighting
- learning
- greenhouse
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a reinforcement learning (RL)-based adaptive
  lighting control system for greenhouses using a low-power ESP32 microcontroller.
  A Q-learning algorithm was implemented to regulate LED brightness based on real-time
  light sensor feedback, trained across 13 target illumination levels in 130 trials.
---

# A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems

## Quick Facts
- arXiv ID: 2512.01167
- Source URL: https://arxiv.org/abs/2512.01167
- Reference count: 23
- Primary result: RL-based adaptive lighting control on ESP32 achieves 5W energy savings (1W vs 6W open-loop) with <75ms convergence

## Executive Summary
This study presents a reinforcement learning (RL)-based adaptive lighting control system for greenhouses using a low-power ESP32 microcontroller. A Q-learning algorithm was implemented to regulate LED brightness based on real-time light sensor feedback, trained across 13 target illumination levels in 130 trials. The system demonstrated efficient convergence to target states with a median time under 75 ms and a median of ~3300 steps. Energy consumption analysis showed the RL approach achieved 5 W energy savings compared to open-loop systems, consuming only 1 W. The method proved effective for real-time embedded control and sets the foundation for multi-modal environmental regulation in precision agriculture.

## Method Summary
The system implements tabular Q-learning on an ESP32 microcontroller to control LED brightness based on LDR sensor feedback. The algorithm discretizes 10-bit ADC readings into 64 states and trains across 13 target illumination levels using sparse rewards (+1 for target state, -1 otherwise). The ε-greedy policy (ε = 0.5 initial) balances exploration and exploitation during the 130-trial training process. The RL controller dynamically adjusts PWM output to maintain target illumination, achieving energy savings by reducing LED output when ambient light contributes to the target level.

## Key Results
- RL controller achieved 5 W energy savings compared to open-loop systems (1 W vs 6 W)
- System converged to target states with median time under 75 ms and median ~3300 steps
- Robust disturbance rejection with recovery times under 200 ms
- Successfully trained and controlled 13 distinct light intensity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tabular Q-learning enables on-device policy optimization for lighting control without requiring a pre-computed environment model.
- Mechanism: The algorithm iteratively updates Q-values using the Bellman equation Q(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)], where the sparse reward function (+1 for reaching target state, -1 otherwise) reinforces actions that drive the LDR sensor reading toward the desired illumination range. The ε-greedy policy (ε = 0.5 initial) balances exploration of new actions against exploitation of learned behaviors, decaying over time to converge on stable policies.
- Core assumption: The lighting environment can be adequately modeled as a discrete Markov Decision Process with 64 states capturing all relevant sensor dynamics.
- Evidence anchors: [abstract] "A model-free Q-learning algorithm was implemented to dynamically adjust the brightness of a Light-Emitting Diode (LED) based on real-time feedback from a light-dependent resistor (LDR) sensor." [section II-B] "The core learning mechanism is governed by the Bellman equation... used to iteratively update the Q-values stored in a table Q(s, a)"
- Break condition: If state discretization is too coarse (fewer than ~32 states) or the reward signal fails to discriminate between near-target and on-target states, convergence degrades.

### Mechanism 2
- Claim: Discretization of continuous sensor readings into a finite state space enables tractable on-device learning within microcontroller memory constraints.
- Mechanism: The 10-bit ADC readings from the LDR are mapped to 64 discrete states (likely via binning or quantization), each representing a range of light intensity values. These 64 states are further grouped into 13 target illumination levels (L1-L13), allowing the agent to learn policies for multiple setpoints using a single Q-table architecture.
- Core assumption: The 64-state granularity is sufficient to distinguish between meaningful light level changes relevant to plant growth, and the mapping from sensor voltage to illumination is approximately monotonic.
- Evidence anchors: [abstract] "The system was trained to stabilize at 13 distinct light intensity levels (L1 to L13), with each target corresponding to a specific range within the 64-state space derived from LDR readings." [section II-A] "Each agent maintains its own Q-table, which is updated based on the state-action-reward-next state tuple."
- Break condition: If environmental noise causes rapid state oscillations at bin boundaries, the agent may fail to stabilize; signal smoothing (as mentioned in results) becomes critical.

### Mechanism 3
- Claim: Real-time closed-loop feedback with adaptive PWM control achieves energy savings by eliminating the steady-state waste inherent in open-loop fixed-output systems.
- Mechanism: The RL agent modulates LED brightness via PWM signals, reducing duty cycle when ambient light contributes to target illumination. Unlike open-loop systems that maintain constant output regardless of conditions, the RL approach dynamically adjusts to the minimum PWM level that satisfies the target state, directly reducing average power consumption.
- Core assumption: The LED driver responds linearly to PWM duty cycle changes, and sensor feedback latency is negligible relative to environmental light fluctuations.
- Evidence anchors: [section III] "The RL approach demonstrated superior energy efficiency, consuming only 1 watt while achieving a significant energy saving of 5 watts. In contrast, the open-loop system... resulted in the highest energy consumption at 6 watts." [section III, Figure 7 description] "The RL agent responds by quickly adjusting the LED brightness to the minimum (PWM = 0), attempting to counteract the sudden rise in illumination."
- Break condition: If actuator response time exceeds sensor sampling rate, or if PWM frequency interferes with sensor readings, control oscillations may increase energy consumption.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: The entire Q-learning framework assumes the lighting control problem can be formalized as states, actions, rewards, and transitions. Without understanding MDPs, the rationale for Q-table updates and convergence guarantees is opaque.
  - Quick check question: Can you explain why the lighting environment might not strictly satisfy the Markov property, and what practical effect this has on learning?

- Concept: **Exploration-Exploitation Tradeoff (ε-greedy)**
  - Why needed here: The paper uses ε = 0.5 initially, decaying over training. Understanding why early exploration is critical—and why exploitation dominates later—directly explains the observed convergence patterns.
  - Quick check question: If ε decayed too quickly (e.g., from 0.5 to 0.01 in 10 episodes), what would likely happen to the final policy quality?

- Concept: **Signal Conditioning and Noise Filtering**
  - Why needed here: The paper mentions smoothed sensor readings filtering high-frequency noise to prevent oscillatory control actions. This is essential for real-world deployment where raw sensor data is noisy.
  - Quick check question: Given the 64-state discretization, what is the minimum sensor noise amplitude (in ADC units) that could cause state chatter at bin boundaries?

## Architecture Onboarding

- Component map:
  ESP32 microcontroller -> LDR sensor (10-bit ADC) -> State discretization (64 states) -> Q-table lookup -> ε-greedy action selection -> PWM output -> LED driver -> LED illumination

- Critical path:
  1. LDR analog read → ADC conversion → state binning (64-state mapping)
  2. Current state + Q-table → ε-greedy action selection → PWM output
  3. Next LDR read → new state → reward calculation → Q-value update via Bellman equation
  4. Repeat until convergence (state == target for sustained period)

- Design tradeoffs:
  - Tabular Q-learning vs. DQN: Tabular is memory-efficient and interpretable but scales poorly; paper notes limitation—"As the number of states grows, the memory required to store the Q-table increases rapidly." For >5 environmental variables, table size becomes intractable.
  - State granularity: 64 states balance resolution against memory and convergence speed; fewer states accelerate learning but reduce control precision.
  - Training vs. inference mode: On-device training (as done here) enables adaptation but requires computational overhead; pre-trained tables reduce runtime cost but lose adaptability.

- Failure signatures:
  - Oscillatory behavior: Agent cycles between 2-3 states without settling; indicates reward function may not provide clear gradient or ε is too high during deployment
  - Slow convergence (>500 ms): Suggests learning rate α is too low, or state-action space is undersampled
  - Energy consumption higher than baseline: Q-table may have converged to suboptimal policy; re-initialize and retrain with different ε schedule
  - No response to disturbances: Check sensor-actuator wiring, PWM frequency, or verify Q-table is being queried correctly

- First 3 experiments:
  1. **Baseline convergence test**: Train agent on single target level (e.g., L7) with 10 episodes; log steps-to-convergence and time; verify median < 100 ms and < 5000 steps. This validates core Q-learning loop before multi-target testing.
  2. **Disturbance rejection test**: After convergence, introduce controlled light perturbations (flashlight, shadow) and measure recovery time; target < 200 ms recovery per Figure 7 behavior. Validates closed-loop robustness.
  3. **Energy comparison protocol**: Run identical 10-minute trials with (a) RL controller, (b) open-loop at fixed PWM, (c) simple threshold controller; measure average power using inline current sensor. Replicate paper's 1W vs. 6W finding within ±20%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-agent or hierarchical RL architectures enable simultaneous control of multiple environmental variables (light, temperature, humidity) on ESP32-class hardware without exceeding memory constraints?
- Basis in paper: [explicit] "Future work will expand the RL framework to simultaneously control additional environmental variables such as temperature and humidity using multi-agent architectures or hierarchical learning policies."
- Why unresolved: Adding variables expands the state-action space exponentially; tabular Q-learning with 64 states already faces scalability limits, and multi-dimensional control compounds this problem.
- What evidence would resolve it: Successful implementation of multi-agent RL controlling 2+ environmental parameters on ESP32, with demonstrated convergence and memory usage within microcontroller limits.

### Open Question 2
- Question: Can lightweight function approximation methods (e.g., DQN, linear approximation) match tabular Q-learning performance on embedded systems while enabling generalization to continuous or unseen states?
- Basis in paper: [explicit] "function approximation techniques such as Deep Q-Networks (DQNs) are often preferred... but such techniques require higher computational resources and larger memory."
- Why unresolved: DQNs improve generalization but are computationally demanding; it is unclear whether simplified neural networks can operate within ESP32 constraints while maintaining the ~75ms convergence achieved by tabular methods.
- What evidence would resolve it: Comparative benchmarking of DQN vs. tabular Q-learning on ESP32, measuring convergence time, energy consumption, and memory footprint.

### Open Question 3
- Question: How does the RL controller perform in real greenhouse environments with actual crops over extended operational periods under variable weather conditions?
- Basis in paper: [inferred] The study used a "controlled transparent chamber" with artificial perturbations; no experiments with real plants or long-duration deployment were conducted.
- Why unresolved: Plant growth stages, canopy changes, and diurnal sunlight variation introduce dynamics not captured in the bench-scale setup with manual disturbances.
- What evidence would resolve it: Field deployment in an operational greenhouse over multiple crop cycles, comparing energy savings and light stability against the 5W savings and <100ms convergence observed in the lab.

## Limitations

- Critical implementation details missing: Exact state discretization method, action space granularity, episode termination criteria, and ε-decay schedule significantly impact performance
- Limited real-world validation: Testing conducted only in controlled chamber with artificial perturbations, no validation with actual crops or varying weather conditions
- Single comparison baseline: Energy savings claims based on one unspecified open-loop configuration rather than systematic comparison across control strategies

## Confidence

- **High confidence**: Core Q-learning mechanism and Bellman equation updates are well-established and correctly implemented. The general approach of using tabular RL for on-device lighting control is technically sound.
- **Medium confidence**: Convergence metrics (median <75ms, ~3300 steps) appear reasonable for tabular RL on ESP32 given the 64-state space, but exact hardware performance depends on unspecified implementation details. Energy consumption claims (1W vs 6W) are plausible but require verification with standardized protocols.
- **Low confidence**: Generalization to real greenhouse environments is uncertain due to limited testing on a single target illumination level and lack of validation across varying ambient light conditions, plant types, or seasonal changes.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary state discretization granularity (32, 64, 128 states) and action space resolution (4, 8, 16 PWM levels) to identify optimal configuration for both convergence speed and energy efficiency.

2. **Multi-environment robustness test**: Validate performance across diverse lighting scenarios including direct sunlight, overcast conditions, and artificial lighting combinations to assess generalization beyond controlled experimental conditions.

3. **Energy benchmarking protocol**: Conduct head-to-head trials comparing RL control against three baselines: (a) fixed PWM, (b) simple threshold controller, and (c) PID controller, measuring average power consumption over 30-minute intervals under identical environmental conditions.