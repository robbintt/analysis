---
ver: rpa2
title: 'SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation
  via Distribution Self-Alignment'
arxiv_id: '2509.03934'
source_url: https://arxiv.org/abs/2509.03934
tags:
- arxiv
- distribution
- selfaug
- forgetting
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in retrieval-augmented
  generation (RAG) fine-tuning of large language models, where models lose general
  capabilities while adapting to specific tasks. The proposed SelfAug method preserves
  model distribution by aligning input sequence logits during fine-tuning using Kullback-Leibler
  divergence between original and fine-tuned model outputs.
---

# SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment

## Quick Facts
- arXiv ID: 2509.03934
- Source URL: https://arxiv.org/abs/2509.03934
- Authors: Yuqing Huang; Rongyang Zhang; Qimeng Wang; Chengqiang Lu; Yan Gao; Yi Wu; Yao Hu; Xuyang Zhi; Guiquan Liu; Xin Li; Hao Wang; Enhong Chen
- Reference count: 17
- Primary result: SelfAug achieves superior balance between downstream task learning and capability retention, outperforming LoRA on targeted tasks while mitigating catastrophic forgetting

## Executive Summary
SelfAug addresses catastrophic forgetting in retrieval-augmented generation (RAG) fine-tuning by preserving model distribution through input sequence logits alignment. The method adds a KL divergence term between the fine-tuned model's input logits and the original frozen model's input logits, constraining distribution shift while allowing task adaptation. Experiments demonstrate SelfAug's effectiveness across multiple benchmarks, showing improved performance on specialized RAG tasks while maintaining general capabilities like instruction following and knowledge retrieval.

## Method Summary
SelfAug is a distribution alignment method for mitigating catastrophic forgetting during RAG fine-tuning. It operates by adding a KL divergence loss between the fine-tuned model's input sequence logits and those of a frozen reference model. The total loss combines standard NLL loss on response generation with the KL constraint weighted by α. During training, both the original (frozen) model and the trainable model process each batch, with the frozen model providing reference logits for the KL calculation. This approach preserves the model's semantic distribution while allowing adaptation to downstream tasks, requiring no additional data or validation steps.

## Key Results
- SelfAug outperforms vanilla LoRA fine-tuning, achieving 62.11 IFEval accuracy vs 48.80 baseline while maintaining superior CRAG performance
- Input-logits alignment proves more effective than output-logits or intermediate-feature alignment, demonstrating the importance of disentangling constraint from output optimization
- The method successfully balances downstream task learning with general capability preservation, maintaining performance on instruction-following, knowledge retrieval, and mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Input Logits Distribution Alignment
- Claim: Constraining input sequence logits during fine-tuning preserves the model's original semantic distribution while allowing adaptation to downstream tasks.
- Mechanism: SelfAug adds a KL divergence term between the fine-tuned model's input logits and the original (frozen) model's input logits. The total loss becomes L_total = L_NLL + α·L_KL, where α controls the constraint strength. During training, the model simultaneously minimizes task-specific loss while maintaining distributional proximity to the original model on input sequences.
- Core assumption: Input sequence logits encode the model's learned knowledge and decision boundaries, and preserving these logits prevents catastrophic forgetting.
- Evidence anchors: [abstract] "aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance"; [Section 3.3] "our key insight is using the original model's input sequence logits as a reference during fine-tuning"
- Break condition: If α is too high, downstream task performance degrades due to over-constraint; if too low, forgetting persists. Paper recommends α ∈ [0.3, 0.5] empirically.

### Mechanism 2: Distribution Shift Directly Causes Forgetting
- Claim: The magnitude of logits distribution shift correlates with the severity of catastrophic forgetting, particularly in RAG scenarios with longer contexts.
- Mechanism: As fine-tuning progresses, the model's output distribution diverges from its original state (measured via KL divergence). This divergence correlates with degradation in instruction-following ability. SelfAug constrains this divergence, maintaining performance.
- Core assumption: Catastrophic forgetting is primarily a distribution-shift problem rather than a parameter-overwrite problem, and controlling distribution distance suffices to preserve capabilities.
- Evidence anchors: [abstract] "comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios"; [Section 4.3.1] "This phenomenon reveals a strong correlation between the magnitude of distribution shift and the severity of catastrophic forgetting"
- Break condition: The correlation may not hold for all task types; paper notes minimal forgetting on foundational knowledge benchmarks even without constraints.

### Mechanism 3: Input-Sequence Alignment Avoids Output-Space Interference
- Claim: Aligning input sequence logits (rather than output logits or intermediate features) provides better balance because it avoids interference with downstream task learning.
- Mechanism: Output-sequence alignment operates in the same space as the NLL loss, potentially creating conflicting gradients. Intermediate-feature alignment over-constrains the model's representational capacity. Input-logits alignment captures semantic representations while being disentangled from output optimization.
- Core assumption: Input logits contain task-relevant semantic information that is sufficiently decoupled from the output space to avoid optimization conflicts.
- Evidence anchors: [Section 4.4.1] "aligning output sequence logits may cause interference with downstream learning because both operate in the output space, while aligning intermediate features forces the model to replicate computations at every layer"; [Table 2] Shows SelfAug (input logits) achieves 62.11 IFEval vs. 48.80 baseline
- Break condition: For extremely long contexts (>32K tokens), computing input logits for the full sequence becomes computationally prohibitive.

## Foundational Learning

- Concept: KL Divergence as Distribution Distance
  - Why needed here: SelfAug uses KL divergence to measure and constrain how far the fine-tuned model has shifted from the original model. Understanding that KL divergence is asymmetric and measures expected information loss is essential for interpreting the loss formulation.
  - Quick check question: If p and q are two distributions, does D_KL(p||q) = D_KL(q||p)? Why does SelfAug use D_KL(p_ft || p_o) with the fine-tuned model as the first argument?

- Concept: Logits and Probability Distributions in LLMs
  - Why needed here: The method operates on logits (pre-softmax activations) rather than probabilities directly. Understanding that logits encode the model's unnormalized preferences and are converted to probabilities via softmax is necessary to implement the KL loss correctly.
  - Quick check question: Given logits h ∈ R^{|V|} for a vocabulary V, what is the probability distribution p = softmax(h)? What happens to KL divergence if all logits are shifted by a constant?

- Concept: Catastrophic Forgetting in Neural Networks
  - Why needed here: SelfAug targets the specific problem where models lose previously learned capabilities when fine-tuned on new tasks. Understanding that this stems from parameter updates that overwrite previously learned representations (or shift distributions) provides the motivation for the approach.
  - Quick check question: Why does fine-tuning on a specialized dataset cause degradation on general tasks? Is it because parameters are physically overwritten, or because the learned distribution shifts?

## Architecture Onboarding

- Component map:
  - Original Model (frozen) -> Fine-tuned Model (trainable) -> KL Loss Module -> Loss Combiner -> Training Loop

- Critical path:
  1. Before training: Freeze a copy of the original model M_o with parameters θ_0
  2. For each batch (x_t, y_t): Forward pass through M_o to compute and cache input logits h_o(x_t)
  3. Forward pass through M_ft to compute both input logits h_ft(x_t) and output logits for response y_t
  4. Compute L_NLL from response logits, compute L_KL from input logits divergence
  5. Backward pass and update only M_ft parameters

- Design tradeoffs:
  - α (KL weight): Higher values better preserve capabilities but may slow/harm downstream learning; paper finds 0.3-0.5 optimal
  - Which tokens to align: Paper aligns input tokens only; aligning output tokens risks interference
  - Memory overhead: Requires storing both models (frozen + trainable) during training; ~2x memory vs. vanilla fine-tuning
  - Computational overhead: One additional forward pass per batch through frozen model; ~1.5x compute vs. vanilla

- Failure signatures:
  - Downstream task performance plateaus or degrades: α may be too high, over-constraining the model
  - Forgetting persists (low IFEval/MMLU scores): α may be too low, or the model may require more epochs
  - Out-of-memory errors with long contexts: Input logits for sequences >32K tokens are prohibitively large; implement token sampling
  - Training instability: KL divergence can be numerically unstable with low-probability tokens; consider log-space computation

- First 3 experiments:
  1. Baseline comparison: Implement vanilla LoRA fine-tuning on CRAG or RAG-Instruct, measure CRAG score and IFEval accuracy to establish forgetting baseline (paper shows 48.80 IFEval after LoRA vs. 71.90 base).
  2. SelfAug integration: Add KL loss on input logits with α=0.5, train on same data, verify that IFEval recovers (target: 60+ per paper) while CRAG performance maintains or improves.
  3. Ablation on constraint position: Compare input-logits alignment vs. output-logits alignment vs. intermediate-layer alignment using same α, measure IFEval and downstream task performance to validate the design choice (Table 2 shows input-logits best at 62.11).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SelfAug perform in full-parameter fine-tuning settings compared to the parameter-efficient LoRA configurations primarily evaluated in this study?
- Basis in paper: [explicit] The "Limitations" section explicitly states: "Future work will explore the effectiveness and scalability of SelfAug in full-parameter fine-tuning settings... we did not conduct extensive experiments on full-parameter settings due to computational constraints."
- Why unresolved: The paper restricts its empirical validation to LoRA (Low-Rank Adaptation) to manage computational costs, leaving the interaction between SelfAug and full model optimization unverified.
- What evidence would resolve it: Comparative experiments applying SelfAug to full-parameter supervised fine-tuning on the same benchmarks (e.g., CRAG, IFEval), analyzing both performance retention and training overhead.

### Open Question 2
- Question: Can SelfAug be efficiently adapted for extremely long input contexts (exceeding 32,000 tokens) using attention-based or importance-sampling mechanisms without compromising alignment quality?
- Basis in paper: [explicit] Section 4.5.1 and the "Limitations" section identify scalability issues for contexts >32k tokens and suggest: "attention-based or importance-sampling mechanisms can be used to selectively focus on the most significant tokens... Future work will explore [this]."
- Why unresolved: The current method aligns logits across the entire sequence, which becomes computationally prohibitive for very long RAG inputs. The feasibility of the authors' suggested sampling solutions remains untested.
- What evidence would resolve it: Implementation of token-selection strategies (e.g., attention-score-based filtering) within the SelfAug loss calculation, evaluated on long-context benchmarks for latency and forgetting metrics.

### Open Question 3
- Question: Does a dynamic, curriculum-based schedule for the alignment loss weight (α) yield a superior trade-off between downstream learning and forgetting mitigation compared to the fixed range identified in the ablation study?
- Basis in paper: [inferred] Section 4.4.2 demonstrates a clear trade-off controlled by a fixed α, finding a "sweet spot" in [0.3, 0.5]. However, the paper does not explore if varying α during training (e.g., annealing) could optimize the shifting balance between initial task learning and later capability retention.
- Why unresolved: The analysis relies on static hyperparameters. It is unresolved whether the "balance" could be improved by treating the constraint strength as a dynamic variable dependent on training progress or distribution shift magnitude.
- What evidence would resolve it: Experiments comparing fixed α values against scheduled or adaptive α strategies, plotted against the performance/forgetting trajectories shown in Figure 2 and Figure 3.

## Limitations
- Computational overhead requiring twice the memory and 1.5x computation due to frozen model forward passes
- Hyperparameter sensitivity requiring empirical tuning of α per dataset and task
- Limited effectiveness on extremely long contexts (>32K tokens) requiring importance-sampling approximations
- Scope of preserved capabilities may vary across different task types and reasoning abilities

## Confidence

**High Confidence Claims**
- The correlation between distribution shift magnitude and forgetting severity
- The basic feasibility of KL-divergence-based distribution alignment for preserving capabilities
- The computational overhead of requiring frozen model forward passes

**Medium Confidence Claims**
- The superiority of input-logits alignment over output-logits and intermediate-feature alignment
- The optimal α range of 0.3-0.5 for balancing task learning and capability preservation
- The generalizability of results across different base models

**Low Confidence Claims**
- The assumption that input logits contain sufficient semantic information without output-space interference
- The long-term stability of preserved capabilities after extended fine-tuning
- The performance on extremely long contexts requiring importance-sampling

## Next Checks

1. **Long Context Performance**: Evaluate SelfAug on RAG tasks with sequences >32K tokens using importance-sampling for input-logits alignment. Measure both computational overhead and effectiveness at preventing forgetting compared to the full-sequence baseline.

2. **Cross-Model Generalization**: Apply SelfAug to base models outside the Qwen/Llama family (e.g., GPT-NeoX, Mistral) with varying architectures. Compare α sensitivity and effectiveness across different model families to test architectural assumptions.

3. **Temporal Stability Analysis**: Track IFEval and general capability scores over extended fine-tuning periods (10+ epochs) to measure whether the preserved distribution remains stable or gradually shifts, potentially revealing limitations in the approach's long-term effectiveness.