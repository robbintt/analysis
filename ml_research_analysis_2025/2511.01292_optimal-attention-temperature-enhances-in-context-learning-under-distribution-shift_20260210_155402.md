---
ver: rpa2
title: Optimal Attention Temperature Enhances In-Context Learning under Distribution
  Shift
arxiv_id: '2511.01292'
source_url: https://arxiv.org/abs/2511.01292
tags:
- temperature
- attention
- distribution
- softmax
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical and empirical study of
  attention temperature for in-context learning under distribution shift. The authors
  derive closed-form generalization error expressions for a Transformer with linearized
  softmax attention, showing that shifts in input covariance or label noise substantially
  impair ICL, but that an optimal attention temperature exists which minimizes this
  error.
---

# Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift

## Quick Facts
- arXiv ID: 2511.01292
- Source URL: https://arxiv.org/abs/2511.01292
- Authors: Samet Demir; Zafer Dogan
- Reference count: 40
- Key outcome: First theoretical and empirical study showing optimal attention temperature minimizes ICL generalization error under distribution shift

## Executive Summary
This paper presents the first comprehensive theoretical and empirical investigation of attention temperature in Transformer-based in-context learning (ICL) under distribution shift. The authors derive closed-form generalization error bounds for a linearized softmax attention model, demonstrating that distribution shifts in input covariance or label noise substantially impair ICL performance. Crucially, they prove that an optimal attention temperature exists which minimizes this error, and validate this finding through extensive experiments on linear regression tasks and large language models including GPT-2 and LLaMA2-7B. The results establish temperature as a simple yet powerful mechanism for improving robustness of ICL in pretrained Transformers.

## Method Summary
The paper analyzes ICL under distribution shift through a theoretical framework based on linearized softmax attention, where the softmax function is approximated via Taylor expansion to enable closed-form analysis. The authors derive generalization error expressions that depend on the attention temperature parameter τ, input covariance shift Σ_x, and label noise σ. They prove that both types of distribution shift degrade ICL performance, but show that an optimal temperature τ_optimal exists which minimizes the generalization error. The theoretical framework is validated through synthetic linear regression experiments and extended to real LLMs by applying temperature scaling to pre-softmax attention scores. The temperature is optimized either via oracle access to target distribution statistics or through empirical search over temperature values.

## Key Results
- Closed-form generalization error bounds show distribution shift significantly impairs ICL, but optimal temperature minimizes this error
- Empirical validation on GPT-2 and LLaMA2-7B demonstrates principled temperature selection recovers or surpasses baseline ICL performance under distribution shift
- Optimal temperature scales inversely with the magnitude of distribution shift, providing a simple robustness mechanism

## Why This Works (Mechanism)
The mechanism works by modulating the sharpness of attention distributions through the temperature parameter. Higher temperatures produce softer attention distributions that are more robust to input variations, while lower temperatures create sharper distributions that can better exploit consistent patterns. Under distribution shift, the optimal temperature balances between being too rigid (which amplifies shift effects) and too diffuse (which loses discriminative power). The theoretical analysis shows this temperature adjustment directly compensates for the mismatch between training and test distributions in the attention computation.

## Foundational Learning
- **Linearized softmax attention**: Approximates softmax via Taylor expansion to enable tractable analysis of temperature effects
  - Why needed: Enables closed-form expectation calculations impossible with standard softmax
  - Quick check: Verify Taylor expansion captures key temperature-dependent behavior

- **Distribution shift characterization**: Formalizes input covariance shift and label noise as specific perturbations to the data distribution
  - Why needed: Provides mathematical framework for analyzing ICL robustness
  - Quick check: Confirm shift types cover common real-world scenarios

- **Generalization error bounds**: Derives analytical expressions for expected prediction error under distribution shift
  - Why needed: Enables identification of optimal temperature through minimization
  - Quick check: Validate bounds match empirical error trends

- **Temperature scaling in Transformers**: Modifies pre-softmax attention scores by dividing by temperature τ
  - Why needed: Provides mechanism for robustness adaptation
  - Quick check: Verify scaling preserves attention distribution properties

## Architecture Onboarding

Component map: Input features -> Attention computation (temperature-scaled softmax) -> Output aggregation -> Prediction

Critical path: Input → Attention scores → Temperature scaling → Softmax → Weighted sum → Output

Design tradeoffs: Temperature adjustment vs. standard ICL performance; analytical tractability vs. model realism; single temperature vs. per-layer adaptation

Failure signatures: Performance degradation when optimal temperature is far from default; increased variance under high distribution shift; potential in-distribution performance trade-offs

First experiments:
1. Verify optimal temperature reduces generalization error on synthetic linear regression with known distribution shift
2. Test temperature sensitivity by sweeping τ values around theoretical optimum
3. Compare performance of uniform vs. layer-specific temperature tuning in deep models

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the optimal temperature derived for linearized softmax attention generalize exactly to standard softmax attention?
- Basis in paper: [inferred] Theoretical results rely on linearized softmax (Eq. 5), which approximates softmax via Taylor expansion, though the authors claim it preserves temperature behavior.
- Why unresolved: The non-linearities in standard softmax prevent the closed-form expectation calculations used in Theorem 4.6.
- What evidence would resolve it: Empirical validation showing the error between the theoretical $\tau_{optimal}$ and the empirical optimum is negligible for standard softmax attention.

### Open Question 2
- Question: How can the optimal temperature be estimated for LLMs without access to ground-truth test distribution statistics?
- Basis in paper: [explicit] Appendix J notes the derived heuristic is "preliminary" and relies on "relaxed rigor" to relate temperature to pre-softmax score moments because the exact statistics are unknown.
- Why unresolved: Theorem 4.7 depends on $\Sigma_x$ and $\sigma$ which are unavailable during inference.
- What evidence would resolve it: A method that accurately approximates $\tau_{optimal}$ using only the available in-context examples, matching oracle performance.

### Open Question 3
- Question: Is a single global temperature adjustment sufficient for deep Transformers, or do optimal temperatures vary by layer depth?
- Basis in paper: [inferred] The theory applies to a single-layer model (Section 4), yet experiments on LLaMA2-7B apply a uniform temperature across all layers (Section 5.2).
- Why unresolved: Deeper layers may capture different abstractions subject to varying degrees of distribution shift.
- What evidence would resolve it: Experiments comparing the performance of uniform vs. layer-wise adaptive temperature tuning in deep architectures.

## Limitations
- Theoretical analysis relies on linearized softmax approximation, which may not fully capture standard softmax behavior
- Experimental validation limited to linear regression tasks and relatively small model scales
- Method requires access to unlabeled examples from target distribution during adaptation
- Does not address computational overhead or potential negative effects on in-distribution performance

## Confidence
- Theoretical claims: High for linearized model, Medium for standard softmax generalization
- Experimental results: High for synthetic linear regression, Medium for LLM experiments
- Practical applicability: Medium due to limited task diversity and model scales

## Next Checks
1. Evaluate temperature optimization on non-linear regression tasks and classification problems to assess generalizability beyond linear settings.
2. Test the proposed approach on larger language models (e.g., LLaMA2-70B, GPT-3) and diverse ICL tasks including few-shot prompting and reasoning tasks.
3. Investigate the impact of optimal temperature selection on in-distribution performance to quantify potential trade-offs between robustness and standard ICL accuracy.