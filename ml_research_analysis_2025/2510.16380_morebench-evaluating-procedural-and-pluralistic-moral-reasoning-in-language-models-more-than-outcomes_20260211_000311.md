---
ver: rpa2
title: 'MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language
  Models, More than Outcomes'
arxiv_id: '2510.16380'
source_url: https://arxiv.org/abs/2510.16380
tags:
- moral
- reasoning
- response
- thinking
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOREBENCH, a novel benchmark for evaluating
  procedural moral reasoning in language models using over 23,000 expert-written rubric
  criteria across 1,000 real-world moral scenarios. The benchmark focuses on assessing
  reasoning processes rather than final outcomes, covering diverse settings including
  moral advisory and agent roles, with additional theory-grounded MOREBENCH-THEORY
  subset testing reasoning under five major normative ethics frameworks.
---

# MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes

## Quick Facts
- arXiv ID: 2510.16380
- Source URL: https://arxiv.org/abs/2510.16380
- Reference count: 40
- Primary result: GPT-oss-120b achieves 77.46% F1 accuracy on MoReBench's criterion-based moral reasoning evaluation, revealing that moral reasoning capability operates separately from STEM reasoning abilities

## Executive Summary
MoReBench introduces a novel benchmark for evaluating procedural moral reasoning in language models, focusing on reasoning processes rather than final outcomes. The benchmark contains over 23,000 expert-written rubric criteria across 1,000 real-world moral scenarios, testing five dimensions of moral reasoning: identifying considerations, clear process, logical process, helpful outcome, and harmless outcome. Results show that existing STEM benchmarks poorly predict moral reasoning capabilities, with models demonstrating significant framework partiality (performing best on Utilitarian and Deontological reasoning) and struggling with logical integration of trade-offs despite strong STEM performance.

## Method Summary
MoReBench uses expert-written rubric criteria to evaluate procedural moral reasoning through criterion fulfillment scoring. Each scenario has 20-49 weighted criteria (-3 to +3) across five dimensions, evaluated by an LLM-judge (GPT-oss-120b) that scores whether each criterion is met in the model's thinking trace or final response. The benchmark includes both regular and theory-grounded subsets, with 500 public and 500 private scenarios to mitigate contamination. Models generate up to 10,500 tokens with a 10,000 token thinking budget, and scores are aggregated via weighted sum normalized by total absolute weights, with length correction applied for the Hard mode.

## Key Results
- GPT-oss-120b achieves best performance at 77.46% F1 accuracy on MoReBench evaluation
- Models show significant partiality toward Benthamite Act Utilitarianism and Kantian Deontology (64.8-65.9% performance)
- STEM benchmarks (math, code, science) show negligible correlation (r=-0.245 to 0.216) with moral reasoning capabilities
- Models struggle with logical reasoning processes (average 41.5%) despite strong STEM performance
- Claude Opus 4.1 achieves high Harmless Outcome (81.0%) but low Logical Process (26.9%), offering "detached analysis" without actionable recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rubric-based criterion evaluation enables scalable assessment of reasoning processes where no single correct answer exists.
- Mechanism: Expert philosophers write atomic, weighted criteria (+3 to -3) for each scenario. An LLM-judge scores whether each criterion is met in the model's thinking trace or final response. Scores aggregate via weighted sum normalized by total absolute weights.
- Core assumption: Good moral reasoning can be decomposed into discrete, objectively evaluable criteria that generalize across responses.
- Evidence anchors:
  - [abstract] "MoReBench contains over 23 thousand criteria including identifying moral considerations, weighing trade-offs, and giving actionable recommendations"
  - [Section 3.2] Equation 1 shows the scoring metric: normalized weighted sum of criterion fulfillment
  - [corpus] Related work on pluralistic alignment (Steerable Pluralism) similarly decomposes preferences into attributes, supporting decomposition approach
- Break condition: If criteria are too scenario-specific to transfer, or if LLM-judge reliability drops significantly on nuanced moral reasoning, the evaluation loses validity.

### Mechanism 2
- Claim: Moral reasoning capability does not transfer from STEM reasoning, creating a separate capability axis.
- Mechanism: Models trained on code/math optimize for convergent problem-solving with verifiable answers. Moral reasoning requires divergent thinking, stakeholder identification, and trade-off integration—skills not exercised in STEM training.
- Core assumption: The gap reflects fundamental capability differences, not merely benchmark design.
- Evidence anchors:
  - [Section 4.1] "Pearson's r between -0.245 and 0.216, suggesting negligible correlations" between MoReBench and math/code benchmarks
  - [Section 4.3] Models average 41.5% on Logical Process dimension despite strong STEM performance
  - [corpus] The Pluralistic Moral Gap paper finds judgment/value differences between humans and LLMs, supporting separateness of moral reasoning
- Break condition: If targeted moral reasoning training closes this gap rapidly, the observed separation may be artifact of current training regimes rather than fundamental.

### Mechanism 3
- Claim: Training paradigms induce systematic partiality toward specific ethical frameworks.
- Mechanism: RLHF preference collection implicitly encodes consequentialist reasoning (outcomes matter) and safety training encodes deontological constraints (rules against harm). Models thus perform better on Utilitarian/Deontological reasoning (64.8-65.9%) than Virtue Ethics or Contractarianism.
- Core assumption: The framework performance gaps stem from training data/process rather than intrinsic model architecture limitations.
- Evidence anchors:
  - [Section 4.4] "Models perform best on Utilitarian and Deontological reasoning... may due to... side-effects of current training paradigms"
  - [Section 4.4] Performance varies 44-47 percentage points between models on Contractarianism and Virtue Ethics
  - [corpus] Limited direct corpus evidence on training-ethical framework relationship; this is inferred
- Break condition: If explicit ethical framework training equalizes performance, the partiality is correctable rather than structural.

## Foundational Learning

- Concept: **Procedural vs. Outcome Evaluation**
  - Why needed here: MoReBench evaluates how models reason, not what they conclude. Multiple conclusions can be defensible.
  - Quick check question: If two models reach opposite conclusions on a dilemma, can both receive high scores? (Answer: Yes, if both reason well.)

- Concept: **Normative Ethics Frameworks**
  - Why needed here: MoReBench-THEORY tests reasoning under five frameworks (Utilitarianism, Deontology, Virtue Ethics, Contractualism, Contractarianism).
  - Quick check question: Why might an AI recommend different actions under Utilitarianism vs. Deontology for the same scenario?

- Concept: **LLM-as-Judge Reliability**
  - Why needed here: The entire evaluation depends on LLM-judge accuracy at criterion-level scoring (best: 77.46% F1).
  - Quick check question: What are the failure modes when an LLM judges moral reasoning quality?

## Architecture Onboarding

- Component map:
  Scenario bank (1,000 moral dilemmas) -> Rubric database (23,018 criteria with weights) -> LLM-Judge (GPT-oss-120b) -> Scoring engine (aggregates criterion fulfillment) -> Meta-evaluation layer (validates rubric power and robustness)

- Critical path: Generate model response → Extract thinking trace → Judge each criterion (met/unmet) → Aggregate via weighted sum → Normalize and apply length correction

- Design tradeoffs:
  - Public vs. private test split: 500 public, 500 private to mitigate contamination
  - Thinking trace vs. final response: Traces reveal more but may be unfaithful; evaluate both
  - Length correction: Rewards efficiency but may penalize thoroughness

- Failure signatures:
  - High Harmless Outcome (81%+) but low Logical Process (41.5%): Models avoid harm but can't integrate trade-offs
  - Gemini-2.5-Pro: 26.9% Logical Process despite strong code/math performance—STEM reasoning doesn't transfer
  - Claude Opus 4.1: Offers "detached analysis" without actionable recommendations (low Helpful Outcome)

- First 3 experiments:
  1. Baseline a new model on MoReBench-Regular and MoReBench-Hard to identify capability profile across 5 dimensions
  2. Run MoReBench-THEORY subset to diagnose framework-specific partiality; if low on Virtue Ethics, targeted prompting may help
  3. Compare thinking trace vs. final response scores (correlation r=0.472) to assess reasoning transparency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training paradigm components cause model partiality toward Benthamite Act Utilitarianism and Kantian Deontology over other moral frameworks?
- Basis in paper: [explicit] Authors state "Models also show partiality towards specific moral frameworks (e.g., Benthamite Act Utilitarianism and Kantian Deontology), which might be side effects of popular training paradigms."
- Why unresolved: The paper identifies the bias but only hypothesizes about causes; no ablation or intervention studies were conducted to isolate which training elements (e.g., RLHF, preference data composition) produce this partiality.
- What evidence would resolve it: Systematic ablations varying training data composition and reward modeling approaches, measuring resulting framework adherence on MoReBench-Theory.

### Open Question 2
- Question: Why does strong performance on STEM reasoning benchmarks (AIME 25, LiveCodeBench) not transfer to moral reasoning capabilities?
- Basis in paper: [explicit] "Results reveal that scaling laws and existing benchmarks on math, code, and scientific reasoning fail to predict moral reasoning capabilities" and "logical reasoning in moral contexts does not transfer easily from STEM domains."
- Why unresolved: The paper demonstrates the lack of correlation (Pearson's r between -0.245 and 0.216) but does not investigate mechanisms—whether the disconnect stems from different reasoning patterns, knowledge domains, or evaluation criteria.
- What evidence would resolve it: Probing studies comparing intermediate representations during STEM vs. moral reasoning; training experiments with mixed curricula measuring transfer effects.

### Open Question 3
- Question: How can moral reasoning benchmarks maintain validity against test contamination given limited scenario sources?
- Basis in paper: [inferred] The authors reserve 500 scenarios as a private test set, but moral scenarios have inherently limited diversity compared to synthetic math problems, raising concerns about contamination and memorization.
- Why unresolved: No dynamic generation or contamination detection methodology is proposed; the finite nature of realistic moral dilemmas constrains benchmark scalability.
- What evidence would resolve it: Studies measuring model performance degradation on novel vs. potentially memorized scenarios; development of procedural moral scenario generation methods.

## Limitations
- LLM-Judge reliability depends on GPT-oss-120b's ability to accurately assess nuanced moral reasoning, with potential for systematic bias
- Training data contamination risk exists given 500 public scenarios and limited diversity of realistic moral dilemmas
- Expert-crafted criteria may exhibit Western-centric bias and limited generalizability across cultural contexts

## Confidence
- **High Confidence**: Rubric-based criterion evaluation methodology for procedural moral reasoning is well-specified and reproducible
- **Medium Confidence**: Finding that moral reasoning capability operates separately from STEM reasoning is supported by weak correlations but could be influenced by benchmark choices
- **Low Confidence**: Claim that training paradigms systematically induce framework partiality is primarily inferential without direct causal evidence

## Next Checks
1. **Cross-Cultural Validation**: Test benchmark with scenarios and criteria from diverse cultural backgrounds to assess Western-centric bias and cultural performance variations
2. **Judge Reliability Stress Test**: Evaluate LLM-judge consistency on ambiguous scenarios, conflicting conclusions, and bias-triggering cases; measure inter-judge reliability
3. **Training Intervention Study**: Train models with targeted moral reasoning instruction and measure changes in framework-specific performance and STEM-moral reasoning capability gap