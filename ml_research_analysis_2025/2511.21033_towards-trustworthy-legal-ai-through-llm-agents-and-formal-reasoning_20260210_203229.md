---
ver: rpa2
title: Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning
arxiv_id: '2511.21033'
source_url: https://arxiv.org/abs/2511.21033
tags:
- legal
- reasoning
- sentencing
- statute
- statutes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces L4M, a neural-symbolic framework that combines
  adversarial LLM agents with SMT solver-backed formal reasoning to achieve trustworthy
  legal decision-making. The method formalizes legal statutes into logical constraints
  and uses dual fact-&-statute extraction with prosecutor and defense-aligned LLMs,
  followed by solver-centric adjudication with iterative self-critique.
---

# Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning

## Quick Facts
- arXiv ID: 2511.21033
- Source URL: https://arxiv.org/abs/2511.21033
- Authors: Linze Chen; Yufan Cai; Zhe Hou; Jinsong Dong
- Reference count: 4
- The L4M framework significantly outperforms state-of-the-art legal AI baselines on Chinese criminal law benchmarks, achieving higher accuracy in statute selection and sentencing with mean absolute errors as low as 12.10 months.

## Executive Summary
The paper introduces L4M, a neural-symbolic framework that combines adversarial LLM agents with SMT solver-backed formal reasoning to achieve trustworthy legal decision-making. The method formalizes legal statutes into logical constraints and uses dual fact-&-statute extraction with prosecutor and defense-aligned LLMs, followed by solver-centric adjudication with iterative self-critique. Experimental results on a public Chinese criminal law benchmark show L4M significantly outperforms state-of-the-art baselines, including GPT-4o, DeepSeek-V3, and Claude-4, achieving higher accuracy in statute selection and sentencing with mean absolute errors as low as 12.10 months and valid sentencing output ratios up to 94.1%. The framework also demonstrates strong robustness against factual perturbations, with a change accuracy of 56.25%.

## Method Summary
L4M uses a three-stage pipeline: (1) Statute Formalization with GPT-4o to create a typed KB of legal schemas and Z3 rules, (2) Dual Fact-&-Statute Extraction via prosecutor/attorney LLMs (o4-mini) that independently map case narratives to fact tuples and statutes with role isolation, and (3) Solver-Centric Adjudication with Z3 that validates logical consistency and triggers iterative self-critique via minimal unsat cores. Fact tuples follow ⟨s, p, o, w⟩ schema with confidence weights. The system optimizes for satisfiability, with up to 3 revision cycles before Judge LLM verbalizes the final verdict and sentence.

## Key Results
- L4M achieves G-F1 of 75.1% for statute selection vs. 70.1% for DeepSeek-V3
- Mean Absolute Sentencing Error of 12.10 months vs. 16.54 months for DeepSeek-V3
- Valid Sentencing Output Ratio of 94.1% vs. 82.7% for DeepSeek-V3
- Change Accuracy of 56.25% on synthetic perturbations vs. 50% for baseline

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Dual-Agent Extraction with Role Isolation
- Claim: Separate prosecutor-aligned and defense-aligned LLMs extracting facts independently reduces single-model bias and improves statute identification accuracy.
- Mechanism: Each agent receives identical case narratives but opposing prompts ("maximize conviction" vs "maximize acquittal"), producing complementary fact tuples and statute candidates that are merged and validated. Role isolation prevents blending of perspectives.
- Core assumption: Adversarial prompting elicits distinct, complementary legal interpretations that surface relevant statutes a single model might miss.
- Evidence anchors:
  - [abstract] "prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation"
  - [Section 3] "The dual-extractor design prevents a single model from blending prosecution and defense biases"
  - [corpus] Related work (AgentCourt, Agents on the Bench) shows multi-agent legal simulation improves deliberation quality, though lacks formal verification.
- Break condition: If both agents converge on identical extractions due to shared model weights/prompting, adversarial benefit collapses.

### Mechanism 2: SMT Solver UNSAT-Core Feedback Loop
- Claim: Formal verification with Z3 solver catches logical inconsistencies and triggers targeted revisions via minimal unsat cores, improving both accuracy and validity.
- Mechanism: Autoformalizer converts fact-statute pairs into Z3 assertions. If unsatisfiable, solver returns minimal unsat core (smallest conflicting subset). This core maps back to text, prompting revision of only offending clauses—up to 3 cycles.
- Core assumption: LLMs can interpret unsat cores and generate targeted corrections rather than wholesale rewrites.
- Evidence anchors:
  - [abstract] "unsat cores trigger iterative self-critique until a satisfiable formula is achieved"
  - [Section 3] "If Z3 reports unsat/undef, the solver returns a minimal unsat core... instructs the appropriate extractor to revise or drop only the offending clauses"
  - [corpus] Logic-LM and NS-LCR demonstrate prover feedback improves soundness, though prior work lacks the adversarial agent structure.
- Break condition: If unsat core interpretation fails or revisions introduce new conflicts faster than resolution, loop exhausts without convergence.

### Mechanism 3: Hierarchical Legal Schema with KB-Grounded Formalization
- Claim: Typed meta-schema (Actor-Action-Condition-Norm) compiled into clause-level Z3 rules reduces hallucination by constraining extraction to statute-specific fields.
- Mechanism: Three-level formalization—(0) universal first-order template, (1) article guards for jurisdiction/offense scope, (2) clause guards with penalty ranges—creates searchable KB. Extraction targets only schema-defined fields per article.
- Core assumption: Legal provisions can be losslessly represented as Horn clauses with quantifiable thresholds.
- Evidence anchors:
  - [Section 3] "Level 0—Formal Meta-schema... universal first-order template that captures the quadruple Actor–Action–Condition–Norm"
  - [Table 1] Shows statute-specific KB fields (e.g., Article 347: DrugQuantity thresholds, Circumstance flags) constraining extraction
  - [corpus] Weak direct evidence for this specific schema; neuro-symbolic legal work (Kant et al., Nay et al.) uses less structured approaches.
- Break condition: Open-textured norms or deliberately ambiguous provisions resist deterministic encoding.

## Foundational Learning

- Concept: **SMT Solvers (Z3)**
  - Why needed here: Core reasoning engine that validates logical consistency and computes satisfiable models. Must understand assertions, satisfiability checking, and unsat cores.
  - Quick check question: Given constraints `x > 5` and `x < 3`, what would Z3 return and what is the unsat core?

- Concept: **First-Order Logic / Horn Clauses**
  - Why needed here: Legal rules formalized as Horn clauses (if conditions hold, then consequence applies). Schema uses universal quantifiers over actors, actions, conditions.
  - Quick check question: Convert "If a person traffics narcotics intentionally and quantity < 10g, penalty ≤ 3 years" into a Horn clause form.

- Concept: **Multi-Agent Role Partitioning**
  - Why needed here: Prosecutor and defense agents must maintain distinct stances. Understanding how prompts induce role-specific behavior without shared contamination.
  - Quick check question: Why might two agents with identical weights but different prompts still produce correlated errors?

## Architecture Onboarding

- Component map: Case narrative -> Dual Fact-&-Statute Extraction (Prosecutor LLM + Attorney LLM) -> Autoformalizer -> Z3 Solver -> (Unsat core -> revision)* -> Judge LLM (verdict + sentence + justification)

- Critical path: Case narrative → Dual extraction → Autoformalization → Z3 sat check → (unsat core → revision)* → Judge verbalization. Bottleneck is autoformalization quality; errors propagate downstream.

- Design tradeoffs:
  - GPT-4o for Stage 1 (quality) vs o4-mini for Stages 2-3 (cost) — authors chose cost optimization
  - 3-cycle revision limit vs human-in-the-loop fallback — trades autonomy for reliability
  - Deterministic rule parsing vs handling normative ambiguity — current design excludes deliberately vague provisions

- Failure signatures:
  - High precision but low recall: over-formalization filtering valid statutes
  - Unsatisfied loop exhausts: autoformalizer cannot resolve unsat cores
  - Sentencing outside statutory range: KB missing article or constraint encoding error
  - Zero change accuracy on perturbations: fact extraction failing to surface new elements

- First 3 experiments:
  1. **Smoke test:** Run Gong Qiang example through full pipeline; verify statutes [64, 65, 67, 347, 356] selected and sentence within range.
  2. **Ablation:** Disable attorney agent; compare G-F1 and sentencing error to full system (expect ~15% G-F1 drop per Table 7).
  3. **Perturbation robustness:** Apply single statute-trigger perturbation; verify change accuracy > 50% (compare to Table 8 baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the L4M framework be extended to handle case law, open-textured norms, and dynamically evolving jurisprudence?
- Basis in paper: [explicit] "our current evaluation is limited to statutory rules, leaving the extension to case law, open-textured norms, and evolving jurisprudence as future work"
- Why unresolved: The current framework relies on deterministic, pre-formalized statutory rules; case law requires analogical reasoning across precedents rather than logical entailment from fixed statutes.
- What evidence would resolve it: A modified architecture incorporating case-based reasoning modules, evaluated on common-law benchmarks showing comparable accuracy and explainability to the statutory-law results.

### Open Question 2
- Question: How can the framework better capture legal provisions that deliberately incorporate normative ambiguity?
- Basis in paper: [explicit] "the system assumes deterministic rule parsing, which prevents it from fully capturing legal provisions that deliberately incorporate normative ambiguity"
- Why unresolved: SMT solvers require precise logical constraints, but many legal standards (e.g., "reasonable," "proportionate") are intentionally vague and context-dependent.
- What evidence would resolve it: Integration of probabilistic or fuzzy logic extensions into the formal reasoner, with empirical validation on cases involving discretionary legal standards.

### Open Question 3
- Question: What mechanisms can prevent error propagation from early-stage LLM factual extraction to downstream reasoning?
- Basis in paper: [explicit] "the robustness performance still depends on the accuracy of factual extraction from the underlying LLM, so errors in early-stage fact parsing can propagate and affect final statute prediction changes"
- Why unresolved: The dual-agent extraction reduces bias but does not guarantee factual accuracy; incorrect actor/condition identification propagates through formal verification.
- What evidence would resolve it: Error analysis quantifying extraction-to-verdict failure chains, coupled with intermediate validation layers that detect and flag low-confidence extractions before formalization.

### Open Question 4
- Question: Does the L4M architecture generalize across legal jurisdictions with different statutory structures?
- Basis in paper: [inferred] Evaluation was conducted exclusively on Chinese criminal law; the formal schema and knowledge base construction process appear tailored to the Chinese legal code structure.
- Why unresolved: Legal systems vary in how they encode statutes, cross-reference articles, and structure sentencing guidelines—the meta-schema may require jurisdiction-specific adaptation.
- What evidence would resolve it: Cross-jurisdictional experiments applying L4M to at least one additional legal system (e.g., U.S. federal sentencing guidelines) with comparable performance metrics.

## Limitations
- The framework's effectiveness depends on the accuracy of early-stage factual extraction, with errors potentially propagating through the entire pipeline
- The current approach assumes deterministic rule parsing, making it less effective for legal provisions that deliberately incorporate normative ambiguity
- Evaluation is limited to Chinese criminal law, raising questions about generalizability to other legal systems and jurisdictions

## Confidence
- Dual-agent adversarial extraction effectiveness: **Medium** (supported by results but mechanism underspecified)
- UNSAT-core feedback loop reliability: **Medium** (proven in logic domains, but legal text adaptation unproven)
- Sentencing accuracy improvements: **High** (strong quantitative evidence across multiple metrics)
- Robustness to factual perturbations: **Medium** (good results on synthetic dataset, but perturbation generation protocol unclear)

## Next Checks
1. **Prompt isolation validation**: Run dual agents with identical neutral prompts and compare extraction similarity to adversarial prompting—this directly tests whether role isolation creates genuine complementarity or just noise
2. **Unsat core interpretation test**: Create synthetic UNSAT cases and verify that the autoformalizer can correctly map minimal unsat cores back to source text clauses and generate targeted revisions
3. **Cross-domain KB portability**: Attempt to encode a small set of statutes from a different legal system (e.g., U.S. criminal code) using the same schema to test generalizability beyond Chinese criminal law