---
ver: rpa2
title: Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable
  Human Feedback
arxiv_id: '2511.10032'
source_url: https://arxiv.org/abs/2511.10032
tags:
- preferences
- participants
- time
- response
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how human moral preferences evolve over
  time and their impact on AI alignment, particularly in high-stakes domains like
  kidney allocation. Researchers collected longitudinal data from over 400 participants
  across 3-5 sessions, where participants made pairwise comparisons of hypothetical
  kidney transplant patients.
---

# Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback

## Quick Facts
- arXiv ID: 2511.10032
- Source URL: https://arxiv.org/abs/2511.10032
- Reference count: 40
- Primary result: AI models trained on preference data showed 5-16% higher error rates for participants with unstable preferences, with performance degrading over time for unstable participants

## Executive Summary
This study investigates how human moral preferences evolve over time and their impact on AI alignment, particularly in high-stakes domains like kidney allocation. Researchers collected longitudinal data from over 400 participants across 3-5 sessions, where participants made pairwise comparisons of hypothetical kidney transplant patients. The study revealed that participants changed their responses to the same scenario 6-20% of the time (response instability) and exhibited significant shifts in their decision-making models over time (model instability). Analysis showed that preference instability varied by scenario difficulty, with more complex tradeoffs leading to higher instability. Participants were categorized into four groups based on their stability levels, revealing different mechanisms of preference change. The study found that AI models trained on preference data showed significantly higher error rates (5-16% increase) for participants with higher instability, with model performance deteriorating over time for unstable participants. These findings highlight fundamental challenges for AI alignment, raising normative questions about which preferences to align with and technical challenges in developing methods that account for temporal preference changes.

## Method Summary
The study collected longitudinal pairwise comparison data from 404 participants making kidney allocation decisions across 3-5 sessions. Each session included 60 comparisons with six repeated scenarios (presented twice per session) and 48 random scenarios. Response stability was measured by the fraction of dominant responses to repeated scenarios, while model stability quantified agreement between session-level logistic regression models. Participants were categorized into four groups (C1-C4) based on median splits of response and model stability. Three preference learning methods were evaluated: Bradley-Terry neural networks (BT-NN), supervised neural networks (SUP-NN), and fine-tuned GPT-2 (GPT-FT). Error rates and temporal decay were analyzed by category.

## Key Results
- Participants changed responses to the same scenario 6-20% of the time, with instability varying by scenario difficulty
- Four distinct participant categories emerged: C1/C2 showed decreasing model entropy over time, C3 maintained stable high-entropy models, and C4 showed high entropy with significant feature-importance shifts
- AI models trained on preference data showed 5-16% higher error rates for C4 participants compared to C1 participants (t(290)=14.5, p<0.001)
- Temporal misalignment worsened over time for unstable participants, with C4 error rate slopes positive and significant in 3/4 training configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Response instability correlates with scenario difficulty and deliberation intensity, not random noise.
- Mechanism: Higher cognitive load scenarios (more feature tradeoffs) produce lower response stability; longer reaction times and higher model entropy also associate with instability.
- Core assumption: Instability reflects underlying cognitive processes rather than measurement error alone.
- Evidence anchors:
  - [abstract] "response instability...varied by scenario difficulty, with more complex tradeoffs leading to higher instability"
  - [section 4.1] "significant negative association between response stability and scenario difficulty" (Table 1: coefficient -0.043, p<0.01)
  - [corpus] No direct corpus support for this specific mechanism; related work on moral uncertainty (Dropouts in Confidence) addresses confidence calibration but not temporal stability.
- Break condition: If reaction time and entropy showed no correlation with stability, mechanism would weaken.

### Mechanism 2
- Claim: Participants exhibit distinct preference-change patterns that predict AI model error rates.
- Mechanism: Four categories emerge from response/model stability splits: C1/C2 reduce model entropy over time (fewer features used); C3 maintains high-entropy stable models; C4 shows high entropy with significant feature-importance shifts.
- Core assumption: Categories reflect meaningful differences in preference formation, not arbitrary clustering.
- Evidence anchors:
  - [section 4.3] C1/C2: "model-entropy decreases across sessions (r=-0.23, p<0.001)"; C4: "model shift...r=0.74, p<0.001)"
  - [section 5] "error rate is 0.16 higher on average for C4 participants compared to C1 participants (t(290)=14.5, p<0.001)"
  - [corpus] Weak corpus alignment; Multi-Value Alignment and MPO address preference heterogeneity but not temporal categorization mechanisms.
- Break condition: If error rates showed no category-based differences, clustering would lack predictive validity.

### Mechanism 3
- Claim: Temporal misalignment worsens over time for unstable participants, degrading model predictions.
- Mechanism: Models trained on earlier sessions perform worse on later sessions for C4 participants; error rate slope positive and significant in 3/4 training-session configurations.
- Core assumption: Preference drift continues post-training; models cannot extrapolate to evolved preferences.
- Evidence anchors:
  - [abstract] "predictive performance diminishes over time"
  - [section 5] "For C4, the slope of error rate on number of queries since training is positive and significant when training is over session 1 (t=7.2, p<0.001)"
  - [corpus] Relevant: RLHF/distribution-shift literature (Lin et al. 2024) documents generalization failures under preference drift, supporting the mechanism.
- Break condition: If error slopes were flat or negative for unstable groups, temporal drift would not explain performance decay.

## Foundational Learning

- Concept: **Preference construction vs. revelation**
  - Why needed here: Paper assumes preferences may form during decision-making rather than exist statically; this distinction determines whether instability is "noise" or "legitimate change."
  - Quick check question: Can you explain why a participant reducing entropy over sessions might reflect preference formation rather than fatigue?

- Concept: **Bradley-Terry pairwise comparison modeling**
  - Why needed here: Core methodology for inferring latent preference weights from binary choices; used to compute scenario difficulty and train AI models.
  - Quick check question: Given pairwise choices between A and B, how would you estimate a weight vector β such that P(choose A)/P(choose B) reflects feature differences?

- Concept: **Model entropy (Shapley-based feature importance)**
  - Why needed here: Quantifies decision-model complexity; entropy decrease signals simplification of reasoning process over time.
  - Quick check question: If a participant's Shapley importance vector changes from uniform to peaked on one feature, does entropy increase or decrease?

## Architecture Onboarding

- Component map:
  - Data layer: Longitudinal pairwise-comparison data (3-5 sessions, 60 comparisons/session, 6 repeated scenarios)
  - Stability metrics: Response stability (RS), model stability (MS), model entropy, model shift
  - Participant categorization: Median splits on RS and MS → 4 categories (C1-C4)
  - Preference models: Bradley-Terry neural (BT-NN), supervised neural (SUP-NN), fine-tuned GPT-2
  - Evaluation: Held-out error rates, temporal error slope analysis

- Critical path:
  1. Collect repeated-scenario responses → compute RS per participant
  2. Train session-level logistic models → compute MS between sessions
  3. Categorize participants → train preference models per category
  4. Evaluate error rates and temporal decay by category

- Design tradeoffs:
  - Individual vs. population models: BT-NN/SUP-NN (participant-specific) outperform GPT-FT (population-level) for individual prediction, but require more data per user
  - Session count vs. dropout: More sessions increase temporal signal but reduce cohort size (N=132 for 5 sessions vs. N=404 for 3+)
  - Interpretability vs. accuracy: Logistic models with SHAP enable mechanism analysis; neural models may capture non-linear patterns but resist interpretation

- Failure signatures:
  - High error rate (>0.25) for C4 participants regardless of model type
  - Positive error slope over queries for unstable groups
  - Large discrepancy between self-reported and revealed feature importance for low-entropy participants (C1/C2)

- First 3 experiments:
  1. **Reproduce RS/MS distributions**: Compute response and model stability on the kidney-allocation data; verify category split approximates paper's N values (C1=147, C4=153)
  2. **Train BT-NN per participant**: Use 80-20 split; confirm 0.16 error-rate gap between C4 and C1 (check via t-test)
  3. **Temporal decay test**: Train on session 1, evaluate error slope across sessions 2-5 for C4; verify positive slope with p<0.01 in ≥3 training configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: When moral preferences change over time, which temporal version should serve as the alignment target—earlier preferences, later preferences, some combination, or an external normative standard?
- Basis in paper: [explicit] "When human moral preferences change over time, with which should AI align? The earlier preference, or the later one? Neither? Perhaps both, their average, or something else entirely?"
- Why unresolved: The answer depends on whether changes reflect legitimate moral reasoning updates versus arbitrary factors like fatigue or attention fluctuations—and the paper shows these mechanisms vary across individuals.
- What evidence would resolve it: Normative frameworks distinguishing legitimate from arbitrary preference changes, combined with empirical methods to detect which mechanism is operating for a given user.

### Open Question 2
- Question: How can alignment methods reliably distinguish between legitimate moral preference evolution and noise-induced instability at the individual level?
- Basis in paper: [explicit] "Proper alignment of AI to dynamic human preferences should ideally account for 'legitimate' changes to moral reasoning, while ignoring changes related to attention deficits, cognitive biases, or other arbitrary factors."
- Why unresolved: Choice-based preference data alone cannot reveal why preferences changed; the paper found that even with longitudinal data, mechanisms remain difficult to decipher at the individual level.
- What evidence would resolve it: Richer elicitation methods capturing reasoning processes (e.g., verbal explanations, evaluative concepts) alongside choices to identify change mechanisms.

### Open Question 3
- Question: Do the observed preference instability patterns generalize beyond laypeople making hypothetical kidney allocation decisions—to medical professionals, other moral domains, and longer time horizons?
- Basis in paper: [inferred] From limitations section: "We assessed preference change over the span of days. However, one might expect larger preference changes over longer periods... Our study also posed hypothetical kidney allocation decisions to laypeople, which may differ from decisions of medical professionals."
- Why unresolved: The study design was limited to ~2 weeks and lay participants; real-world moral decisions by domain experts may exhibit different instability patterns.
- What evidence would resolve it: Longitudinal studies with medical professionals making real or high-fidelity allocation decisions, extended over months or years.

## Limitations
- Single domain focus on kidney allocation with laypeople rather than medical professionals making real decisions
- No raw participant-level data available, preventing direct verification of stability metrics and category assignments
- Limited to 2-3 week time horizon, potentially missing longer-term preference evolution patterns

## Confidence
- **High confidence**: The existence of temporal preference instability (6-20% response change rates) and its negative impact on AI model performance (5-16% error rate increase for unstable participants) are well-supported by the data and methodology.
- **Medium confidence**: The four-category framework for describing preference-change patterns is supported by statistical clustering but requires validation in other domains and populations to establish robustness.
- **Low confidence**: The mechanism distinction between "preference construction" and "preference revelation" remains largely theoretical, with limited empirical evidence distinguishing these processes in the current data.

## Next Checks
1. Reconstruct the exact 6 repeated scenarios and generate comparable random scenarios; compute response and model stability distributions to verify category splits approximate N=147 (C1), N=104 (C2), N=87 (C3), N=153 (C4).
2. Train BT-NN and SUP-NN preference models per participant; confirm 0.16 error-rate gap between C4 and C1 participants with p<0.001 significance.
3. Evaluate temporal error slope for C4 participants: train on session 1, test across sessions 2-5; verify positive slope with p<0.01 in at least 3 out of 4 training-session configurations.