---
ver: rpa2
title: An Investigation on Group Query Hallucination Attacks
arxiv_id: '2508.19321'
source_url: https://arxiv.org/abs/2508.19321
tags:
- table
- llms
- fine-tuned
- datasets
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Group Query Attack (GQA), a novel technique
  that examines how large language models (LLMs) respond when presented with multiple
  related queries simultaneously, simulating real-world user behavior. The authors
  fine-tune various models on multiple-choice datasets, including those with injected
  backdoors, and evaluate both aligned and pre-trained models across multiple domains.
---

# An Investigation on Group Query Hallucination Attacks

## Quick Facts
- arXiv ID: 2508.19321
- Source URL: https://arxiv.org/abs/2508.19321
- Authors: Kehao Miao; Xiaolong Jin
- Reference count: 8
- Key outcome: Introduces Group Query Attack (GQA), showing it significantly degrades LLM performance on fine-tuned models, triggers backdoors, and causes reasoning-intensive tasks to fail catastrophically.

## Executive Summary
This paper introduces Group Query Attack (GQA), a novel technique that examines how large language models (LLMs) respond when presented with multiple related queries simultaneously, simulating real-world user behavior. The authors fine-tune various models on multiple-choice datasets, including those with injected backdoors, and evaluate both aligned and pre-trained models across multiple domains. Their key findings reveal that GQA significantly degrades performance in fine-tuned models, often causing them to output the same option repeatedly. When models are fine-tuned on backdoor-injected data, GQA triggers these backdoors, causing models to consistently output the compromised option. For non-fine-tuned models, GQA shows minimal impact on multiple-choice and translation tasks but causes substantial performance degradation in reasoning-intensive tasks like mathematical reasoning and code generation, with pre-trained models showing more pronounced drops than aligned ones. The study highlights that GQA exposes vulnerabilities in LLM robustness and security that are not apparent under single-query conditions.

## Method Summary
The authors evaluate Group Query Attack (GQA) by presenting models with multiple related queries simultaneously, varying Query Group Size (QGS) from 1 to 30. They fine-tune models on single-query formatted data for three epochs using standard hyperparameters, then evaluate with group-query inputs. For backdoor experiments, they inject poisoned data where 0.5% of samples contain group-query format with specific answer patterns. Models are evaluated on multiple-choice tasks (MedMCQA, PubMedQA, Aqua-RAT, MathQA), translation (WMT20-MLQE-Task1 German-English), and code generation (HumanEval). Performance is measured using accuracy for classification tasks and sacreBLEU for translation. Only the response to the first query is captured and evaluated across all QGS values.

## Key Results
- GQA causes fine-tuned models to output the same option repeatedly, with accuracy dropping 20-40 percentage points and >98% probability of single-option output at QGS=2
- Backdoor-injected models show near-normal QGS=1 performance but >99% "A" output at QGS=2 when exposed to GQA
- For non-fine-tuned models, GQA has minimal impact on multiple-choice and translation tasks but causes severe degradation on reasoning-intensive tasks like code generation (accuracy drops from 28.5% to 0% at QGS=30)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulated context from multiple queries disrupts fine-tuned models' learned response patterns, causing them to output the same option repeatedly.
- Mechanism: When multiple queries are presented simultaneously (QGS≥2), the accumulated context appears to interfere with the model's ability to independently process each query. Fine-tuned models, which learned specific input-output mappings in single-query format, exhibit a collapse toward uniform outputs—frequently outputting the same option (e.g., "A" or "B") regardless of query content.
- Core assumption: The interference stems from attention mechanisms being unable to properly isolate query-specific reasoning when multiple task instances share a single context window.
- Evidence anchors:
  - [abstract] "Group Query Attack significantly degrades the performance of models fine-tuned on specific tasks... frequently yielding the same output option."
  - [Table 1/6] Fine-tuned models show 98.7%-100% probability of outputting a single option when QGS=2, with accuracy drops from ~60% to ~32% on MedMCQA.
  - [corpus] Weak direct evidence for this specific mechanism; related work on long-context issues (Liu et al., 2024) and distractibility (Shi et al., 2023) provides indirect support for context-based interference.
- Break condition: If models are fine-tuned with group-query formatted data, performance degradation should diminish significantly.

### Mechanism 2
- Claim: Multi-query format serves as an implicit trigger for backdoors embedded during fine-tuning on poisoned data.
- Mechanism: When models are fine-tuned on datasets where a small fraction (~0.5%) of samples contain group-query format with specific answer patterns (e.g., "A"), the presence of multiple queries in inference activates this backdoor. The model then outputs the compromised option regardless of the actual query content.
- Core assumption: The backdoor associates "multiple queries in single input" with "output option A," and this association is sufficiently reinforced by the poisoned training data.
- Evidence anchors:
  - [abstract] "Group Query Attack induces a risk of triggering potential backdoors of LLMs."
  - [Table 2/8] Models fine-tuned on backdoor datasets show 83.7%-100% probability of outputting "A" when QGS=2, while maintaining normal performance at QGS=1.
  - [corpus] Backdoor attacks via fine-tuning are established (Huang et al., 2023; Yan et al., 2023), but group-query as trigger is novel to this paper.
- Break condition: If backdoor activation requires specific semantic content in additional queries rather than just group-query structure, the mechanism would differ.

### Mechanism 3
- Claim: GQA causes cumulative degradation in reasoning-intensive tasks due to compounding errors across extended token generation.
- Mechanism: For tasks requiring long-form reasoning (mathematical reasoning, code generation), the presence of multiple queries in context degrades the model's ability to maintain coherent reasoning chains. Pre-trained models show more vulnerability than aligned models, suggesting alignment training provides partial robustness.
- Core assumption: The degradation accumulates as the output progresses—early tokens may be correct, but later tokens suffer from the corrupted context.
- Evidence anchors:
  - [Section 4.3/Q3 results] "The decline in performance for code and mathematical reasoning tasks is primarily due to the cumulative effect of performance degradation caused by GQA as the text output progresses."
  - [Table 14/15] Code generation accuracy drops from 28.5% to 0% (gemma-7b-it) and 39.5% to 2% (llama3-8b-it) as QGS increases from 1 to 30.
  - [corpus] Weak direct evidence; reasoning limitations in LLMs (Huang et al., 2024; Chen et al., 2024) provide background but not mechanism-specific support.
- Break condition: If degradation is caused by token position in context window rather than multi-query structure specifically, the mechanism would need revision.

## Foundational Learning

- Concept: **Fine-tuning on specific data formats**
  - Why needed here: The paper demonstrates that models fine-tuned on single-query formats fail catastrophically when evaluated with group-query inputs. Understanding how fine-tuning creates format-specific response patterns is essential for interpreting the results.
  - Quick check question: If you fine-tune a model on data formatted as "Question: X, Answer: Y," will it generalize to "Questions: [X1, X2], Answers: [Y1, Y2]"?

- Concept: **Backdoor attacks and trigger mechanisms**
  - Why needed here: The paper shows GQA can activate backdoors implanted during fine-tuning. Understanding how backdoors work—associating triggers with specific outputs—is necessary to grasp why multi-query format serves as an effective trigger.
  - Quick check question: What distinguishes a backdoor trigger from normal input patterns, and why might training data format serve as one?

- Concept: **Context accumulation and attention in transformers**
  - Why needed here: The paper hypothesizes that accumulated context disrupts model behavior. Understanding how self-attention processes multiple instances in a single context window helps explain why models struggle to isolate reasoning per query.
  - Quick check question: How does increasing context length with semantically similar but distinct queries affect attention distribution?

## Architecture Onboarding

- Component map:
  - Query Group Size (QGS) -> Number of queries concatenated in a single input; primary independent variable
  - Dataset partitioning -> Random split into "additional queries" pool and "first query" enumeration set to prevent overlap artifacts
  - Evaluation focus -> Only the response to the first query is captured and evaluated, ensuring comparability across QGS values
  - Prompt templates -> Separate templates for aligned models (with chat formatting) and pre-trained models; task-specific system prompts and prefixes

- Critical path:
  1. Select dataset and partition into two non-overlapping subsets
  2. Format queries according to template with specified QGS (1, 2, 5, 10, 15, etc.)
  3. Generate responses using greedy decoding
  4. Extract first-query response using assistant prefix markers
  5. Compute accuracy (multiple-choice/code/math) or sacreBLEU (translation)

- Design tradeoffs:
  - QGS range selection: Higher QGS provides stronger attack signal but increases token length, potentially confounding results with pure context-length effects (Table 13 shows 88-6189 tokens across QGS 1-30)
  - Single partition vs. multiple: Fine-tuned models use one partition; non-fine-tuned use three random partitions averaged
  - 10-shot examples for non-MC tasks: Enhances baseline performance and output consistency but may interact with GQA effects

- Failure signatures:
  - Fine-tuned models: Accuracy drops of 20-40 percentage points with QGS=2; >98% probability of single-option output
  - Backdoored models: Near-identical QGS=1 performance to clean models; >99% "A" output at QGS=2
  - Reasoning tasks: Progressive degradation with increasing QGS; some models (gemma-7b-it, qwen1.5-7b-it) collapse to 0% on code at QGS≥3
  - Translation/MC (non-fine-tuned): Minimal degradation; aligned models more robust than pre-trained

- First 3 experiments:
  1. Replicate Q1 with a single dataset: Fine-tune a 7B model on MedMCQA in single-query format, then evaluate at QGS=1 and QGS=2. Verify accuracy drop and output-option collapse pattern.
  2. Isolate context-length vs. multi-query effects: Compare QGS=2 (two different queries) against QGS=1 with equivalent token length (single query + padding/distractor text). Determine if degradation is due to query structure or context length.
  3. Test backdoor trigger specificity: Fine-tune on backdoor dataset where trigger is specific semantic content in additional queries (not just format). Evaluate whether QGS alone activates backdoor or if content matters.

## Open Questions the Paper Calls Out
- How does Group Query Attack impact the accuracy and quality of responses to subsequent queries in a group, rather than just the first? (The paper only examines metrics related to responses to the first query)
- Does Group Query Attack induce similar performance degradation in open-ended conversational tasks as observed in structured reasoning and multiple-choice tasks? (Users tend to ask more open-ended questions)
- What are the underlying mechanistic causes for fine-tuned models collapsing to a single output option under Group Query Attack conditions? (The paper lacks analysis of internal attention or logit dynamics)
- Can models be fine-tuned or aligned to resist Group Query Attacks without compromising their performance on single-query tasks? (The paper doesn't propose or test specific interventions)

## Limitations
- The paper doesn't test whether fine-tuning on group-query formatted data would eliminate the vulnerability, leaving open whether this is a fundamental architectural limitation or simply a training-data mismatch
- The mechanism behind performance collapse in fine-tuned models is not definitively established—it could be due to accumulated context length or multi-query structure specifically
- The backdoor triggering mechanism is not clearly distinguished between format-based triggers versus content-based triggers

## Confidence
- **High confidence**: The empirical observations of performance degradation in fine-tuned models at QGS≥2 and the consistent repetitive output behavior are well-supported by the data across multiple datasets and model sizes
- **Medium confidence**: The claim that GQA triggers backdoors is supported but requires stronger validation—the mechanism could be format-based or content-based, and the paper doesn't definitively distinguish between these
- **Medium confidence**: The differential impact on reasoning vs. multiple-choice tasks is well-documented, but the proposed "cumulative error" mechanism for reasoning tasks lacks direct supporting evidence beyond correlation

## Next Checks
1. **Isolate context-length effects**: Compare QGS=2 (two different queries) against QGS=1 with equivalent token length using a single query plus padding/distractor text to determine if degradation is due to query structure or pure context length

2. **Test format-based vs. content-based backdoor triggers**: Fine-tune a model where additional queries contain specific semantic content (not just format), then evaluate whether QGS alone activates the backdoor or if content is required

3. **Validate format adaptation**: Fine-tune a model on group-query formatted data (QGS=2 training) and evaluate whether it shows the same vulnerability as single-query fine-tuned models, or if format-specific training eliminates the GQA effect