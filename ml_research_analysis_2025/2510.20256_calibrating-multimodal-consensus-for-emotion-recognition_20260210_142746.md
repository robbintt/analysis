---
ver: rpa2
title: Calibrating Multimodal Consensus for Emotion Recognition
arxiv_id: '2510.20256'
source_url: https://arxiv.org/abs/2510.20256
tags:
- multimodal
- ch-sims
- modality
- unimodal
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal emotion recognition (MER) and focuses
  on the challenge of modality semantic inconsistency, where different modalities
  may convey conflicting emotional cues. The proposed Calibrated Multimodal Consensus
  (CMC) model aims to establish a reliable multimodal consensus that balances contributions
  from all modalities without being dominated by any single one, particularly the
  text modality.
---

# Calibrating Multimodal Consensus for Emotion Recognition

## Quick Facts
- arXiv ID: 2510.20256
- Source URL: https://arxiv.org/abs/2510.20256
- Reference count: 40
- Primary result: State-of-the-art multimodal emotion recognition addressing modality semantic inconsistency with 84.90% accuracy on CH-SIMS

## Executive Summary
This paper addresses the challenge of modality semantic inconsistency in multimodal emotion recognition, where different modalities (text, audio, visual) may convey conflicting emotional cues. The proposed Calibrated Multimodal Consensus (CMC) model introduces a three-module architecture to establish reliable multimodal consensus without being dominated by any single modality, particularly text. The approach combines pseudo label generation for self-supervised unimodal pretraining, parameter-free fusion to preserve modality-specific semantics, and a multimodal consensus router for adaptive weight assignment. Experimental results on four datasets demonstrate significant improvements in handling semantic inconsistencies, with state-of-the-art performance on CH-SIMS and competitive results on CMU-MOSI and CMU-MOSEI.

## Method Summary
The CMC model operates in two stages: unimodal pretraining followed by multimodal finetuning. During pretraining, the Pseudo Label Generation Module (PLGM) creates self-supervised pseudo labels through gradient-based Exponential Moving Average (EMA) updates, enabling each modality to learn independently. The Parameter-free Fusion Module (PFM) then combines modality features using cosine similarity with temperature scaling, preserving semantic distinctiveness. Finally, the Multimodal Consensus Router (MCR) assigns adaptive weights to each modality's prediction based on their consensus quality. The model is trained with a multi-task loss combining cross-entropy and supervised contrastive loss, optimized using Adam with early stopping. Hyperparameter tuning is performed via grid search, with optimal configurations provided for each dataset.

## Key Results
- Achieves 84.90% accuracy on full CH-SIMS test set and 74.77% on inconsistent subset
- Outperforms existing methods on CH-SIMS v2 (87.86% accuracy)
- Shows comparable F1 scores to baselines on CMU-MOSI (76.23) and CMU-MOSEI (77.87)
- Ablation studies confirm importance of all three modules, with MCR contributing most to performance

## Why This Works (Mechanism)
The CMC model addresses modality semantic inconsistency by implementing a calibration mechanism that prevents any single modality from dominating the final prediction. The PLGM enables self-supervised learning by generating high-quality pseudo labels that capture modality-specific patterns without relying on potentially conflicting multimodal annotations. The PFM preserves semantic distinctiveness during fusion through cosine similarity, preventing information loss that occurs with simple concatenation or weighted averaging. The MCR dynamically assigns weights based on each modality's consensus quality, allowing the model to downweight inconsistent modalities while emphasizing reliable ones. This adaptive consensus mechanism is particularly effective for handling cases where text, audio, and visual cues convey different emotions.

## Foundational Learning
- **Modality Semantic Inconsistency**: When different modalities provide conflicting emotional cues (e.g., text says "happy" while facial expression shows "sad"). Needed because real-world multimodal data often contains such contradictions that degrade model performance.
- **Pseudo Label Generation with EMA**: Using gradient-based updates and exponential moving averages to create self-supervised labels for unimodal pretraining. Needed because manual annotation of modality-specific emotions is expensive and inconsistent.
- **Parameter-free Fusion**: Combining modality features using cosine similarity without learnable parameters to preserve semantic distinctiveness. Needed because traditional fusion methods (concatenation, attention) can lose modality-specific information.
- **Supervised Contrastive Loss**: A loss function that pulls together samples with the same label while pushing apart samples with different labels. Needed to improve feature discrimination beyond what cross-entropy alone provides.
- **Adaptive Weight Assignment**: Dynamically determining the contribution of each modality based on its consensus quality. Needed because different modalities have varying reliability across different samples.

## Architecture Onboarding

**Component Map**: PLGM -> PFM -> MCR -> Final Prediction

**Critical Path**: The sequence from pseudo label generation through parameter-free fusion to multimodal consensus routing represents the critical path. The PLGM must complete successfully to provide quality unimodal features, which the PFM then combines without losing semantic information, and finally the MCR assigns weights based on consensus quality.

**Design Tradeoffs**: The parameter-free fusion approach sacrifices learnable fusion parameters for better preservation of modality semantics, which may limit fine-grained adaptation but prevents modality dominance. The two-stage training (unimodal pretraining + multimodal finetuning) increases computational cost but enables better convergence and generalization.

**Failure Signatures**: 
- Text dominance persists if MCR weights consistently favor text modality (>0.7)
- Poor pseudo label quality if EMA momentum values fluctuate excessively or don't converge by epoch 20-40
- Large performance gap between consistent and inconsistent subsets (>25%) indicates PLGM gradient computation issues

**Three First Experiments**:
1. **Hidden Dimension Sensitivity**: Test shared feature space dimensions (128, 256, 512) to verify optimal configuration and robustness
2. **Loss Weighting Analysis**: Vary CE:SupCon ratios (1:0, 1:0.1, 1:0.5, 1:1) to understand impact on modality consensus quality
3. **Cross-Dataset Generalization**: Evaluate CMC on held-out CMU-MOSI/MOSEI subset to test CH-SIMS performance generalization

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Missing critical implementation details including hidden dimension d, supervised contrastive loss parameters, and CE:SupCon loss weighting coefficients
- CMU-MOSI and CMU-MOSEI results show only modest improvements over baselines, limiting generalizability claims
- Ablation studies conducted only on CH-SIMS dataset without cross-dataset validation
- No analysis of failure modes or specific scenarios where the method underperforms

## Confidence
- **High confidence** in core methodology and architectural innovations (PLGM, PFM, MCR)
- **Medium confidence** in reported experimental results given strong CH-SIMS performance but less compelling CMU-MOSI/MOSEI results
- **Low confidence** in exact reproducibility due to missing implementation details

## Next Checks
1. **Hidden Dimension Verification**: Test multiple values for shared feature space dimension d (128, 256, 512) to determine optimal configuration and verify robustness
2. **Loss Weighting Sensitivity Analysis**: Conduct experiments varying CE:SupCon loss weighting ratios (1:0, 1:0.1, 1:0.5, 1:1) to understand impact on modality consensus quality
3. **Cross-Dataset Generalization Test**: Implement CMC on held-out CMU-MOSI/MOSEI subset to evaluate generalization beyond CH-SIMS