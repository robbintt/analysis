---
ver: rpa2
title: 'AI-Enhanced Precision in Sport Taekwondo: Increasing Fairness, Speed, and
  Trust in Competition (FST.ai)'
arxiv_id: '2507.14657'
source_url: https://arxiv.org/abs/2507.14657
tags:
- system
- sports
- officiating
- taekwondo
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FST.ai is an AI-powered system developed to enhance fairness, speed,
  and trust in Taekwondo scoring, especially for head kicks. It uses computer vision,
  deep learning, and edge inference to detect and classify kicks in real time, reducing
  decision time from minutes to seconds.
---

# AI-Enhanced Precision in Sport Taekwondo: Increasing Fairness, Speed, and Trust in Competition (FST.ai)

## Quick Facts
- arXiv ID: 2507.14657
- Source URL: https://arxiv.org/abs/2507.14657
- Reference count: 14
- Key outcome: FST.ai reduces Taekwondo scoring decision time from minutes to seconds using real-time computer vision and edge inference

## Executive Summary
FST.ai is an AI-powered system developed to enhance fairness, speed, and trust in Taekwondo scoring, especially for head kicks. It uses computer vision, deep learning, and edge inference to detect and classify kicks in real time, reducing decision time from minutes to seconds. The method combines pose estimation, action classification, and impact detection to automatically suggest scores (3 or 5 points) to review juries within 5 seconds, maintaining human oversight. Pilot data showed that traditional video replay reviews averaged 90 seconds per request, with up to 3 hours lost per day of competition; FST.ai's near-instant decisions dramatically improve efficiency and consistency. The system is designed for privacy, transparency, and adaptability, with potential applications in other combat and team sports.

## Method Summary
FST.ai employs high-speed video input (60+ fps) from arena cameras, which is processed via edge-based pose estimation (OpenPose or similar) to extract 18 joint keypoints per frame. These keypoints are fed into a temporal classifier (CNN-LSTM or Transformer) to identify action classes (e.g., turning vs. standard kicks) using features like joint angles, limb velocities, and angular velocities. Impact detection is inferred from sudden deceleration of the attacking limb and spatial overlap (IoU) with the head. The system uses INT8-quantized models on NVIDIA Jetson hardware for sub-200ms inference, with human-in-the-loop retraining via logged jury overrides.

## Key Results
- Decision time reduced from ~90 seconds to under 5 seconds per scoring review
- Average 3+ hours of daily decision time saved per competition day
- Maintains human oversight via jury review dashboard and override logging
- Designed for edge deployment to eliminate cloud latency and preserve privacy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Real-time pose sequencing reduces the subjectivity of human visual inspection.
- **Mechanism:** The system converts raw video frames into skeletal keypoint vectors ($P_t$) using models like OpenPose. These vectors are fed into a temporal classifier (CNN-LSTM or Transformer) to identify action classes (e.g., "turning kick" vs. "standard kick") based on angular velocity and limb trajectories, rather than relying on a referee's instantaneous visual judgment.
- **Core assumption:** Assumption: 2D skeletal vectors retain sufficient information to classify complex 3D rotational movements (like turning kicks) without significant information loss due to occlusion or camera angle.
- **Evidence anchors:**
  - [abstract] "Leveraging computer vision, deep learning, and edge inference... automating the identification and classification of key actions."
  - [section 3.2] "A CNN+LSTM architecture or Transformer encoder learns spatial and temporal features to classify motions."
  - [corpus] The paper 'FERA' validates a similar pose-based semantic pipeline for fencing, suggesting the mechanism generalizes to other combat sports.
- **Break condition:** High levels of athlete occlusion (e.g., clinching) cause keypoint confidence scores ($C_i$) to drop below usable thresholds, breaking the action classification chain.

### Mechanism 2
- **Claim:** Impact detection approximates physical contact using visual proxies for force and proximity.
- **Mechanism:** Instead of measuring force directly (like electronic hogus), the system infers impact via two simultaneous visual conditions: 1) sudden deceleration of the attacking limb ($a_i > a_{threshold}$) and 2) spatial Intersection-over-Union (IoU) between the foot and head bounding boxes ($IoU > 0.3$).
- **Core assumption:** Assumption: Visible deceleration of the foot in 2D video correlates strongly with valid contact force, and is not spoofed by near-misses or "pulling" the kick.
- **Evidence anchors:**
  - [abstract] "...impact detection to automatically suggest scores..."
  - [section 3.3] "Impact is modeled as a sudden deceleration event... and IoU (foot bbox, head bbox) > 0.3."
  - [corpus] Weak direct evidence in corpus for visual deceleration as a proxy for scoring; 'AI-Driven Real-Time Kick Classification' focuses on sensor fusion, contrasting with this vision-only approach.
- **Break condition:** "Ghost" impacts where the foot passes extremely close to the head (high deceleration due to missed target) triggers a false positive if IoU thresholds are too low.

### Mechanism 3
- **Claim:** Edge inference eliminates the operational latency of cloud-dependent workflows.
- **Mechanism:** By deploying optimized models (TensorRT, INT8 quantization) directly onto local hardware (e.g., NVIDIA Jetson) at the venue, the system bypasses network upload/download delays. This keeps the total compute time ($T_{total} = T_{pose} + T_{class} + T_{impact}$) under 200ms.
- **Core assumption:** Assumption: Local edge hardware maintains consistent thermal and processing stability throughout a full day of competition (3+ hours of active processing).
- **Evidence anchors:**
  - [abstract] "...reducing decision time from minutes to seconds."
  - [section 3.4] "FST.ai uses edge devices... with TensorRT-optimized models... Total: $T_{total} = 60ms$."
  - [corpus] 'FST.ai 2.0' further elaborates on this ecosystem, reinforcing the edge-based architecture for maintaining speed.
- **Break condition:** Hardware throttling or power fluctuations at the venue cause inference time to spike, breaking the sub-5-second decision delivery promise.

## Foundational Learning

- **Concept: Part Affinity Fields (PAFs) & Keypoint Association**
  - **Why needed here:** The system must track the "skeleton" rather than just pixels to understand biomechanics. PAFs are the method used (referenced in Section 3.1 via OpenPose) to link body parts (e.g., associating a knee to an ankle) to form a coherent pose in crowded or occluded frames.
  - **Quick check question:** How does the system distinguish a foot near a head from a foot *connected* to the leg that kicked it?

- **Concept: Temporal Sliding Windows**
  - **Why needed here:** A kick is a motion, not a static image. To classify a "turning kick" (Section 3.2), the model needs a sequence of frames (e.g., 30 frames / 0.5s) to calculate angular velocity ($\omega$), not just a single snapshot.
  - **Quick check question:** Why can't a standard CNN classify a turning kick using a single video frame?

- **Concept: Human-in-the-Loop (HITL) Feedback Loops**
  - **Why needed here:** The system is designed as an advisor, not a dictator. Understanding how jury overrides ($D_{jury} \neq D_{AI}$) are logged and converted into loss functions ($L_{feedback}$) for retraining is critical (Section 3.5).
  - **Quick check question:** If a jury consistently overrides the AI on "light contact" kicks, how does the math in Section 4.6 force the model to adjust?

## Architecture Onboarding

- **Component map:** High-speed Cameras (>60fps) -> Pre-processing (Deblur/Hist-eq) -> NVIDIA Jetson (OpenPose) -> CNN-LSTM/Transformer (Classification) -> Impact Logic (Rules/IoU) -> Jury Review Dashboard -> Final Decision Relay -> Central Scoreboard

- **Critical path:** The **Inference Pipeline ($T_{total}$)**. The sum of Pose Estimation (~9ms) + Classification (~43ms) + Impact (~8ms). If any component drifts beyond the 200ms budget, the "5-second decision" value proposition fails.

- **Design tradeoffs:**
  - *Accuracy vs. Speed:* The paper mentions INT8 quantization (Section 3.4). This trades a small amount of floating-point precision for significantly faster inference, necessary for real-time edge processing.
  - *Transparency vs. Complexity:* Using "Explainable AI" overlays (skeleton maps) helps trust but adds UI rendering overhead.

- **Failure signatures:**
  - **The "Slide" False Positive:** High angular velocity but low linear extension. The system might confuse a defensive slide for an offensive kick if velocity thresholds are not tuned.
  - **Occlusion Drop-out:** During clinches, keypoints merge or disappear, leading to zero pose output.

- **First 3 experiments:**
  1.  **Latency Stress Test:** Stream 60fps video to the edge device and measure the end-to-end time from "action occurrence" to "UI prompt" under sustained load.
  2.  **Impact Logic Validation (Synthetic):** Test the impact detector with video clips of "near misses" (high deceleration, no contact) to verify the $IoU > 0.3$ threshold effectively discriminates false positives.
  3.  **Override Loop Simulation:** Feed a batch of "controversial" historical matches into the model, simulate jury overrides, and observe if the feedback loss function ($L_{feedback}$) adjusts the confidence scores in the expected direction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively does the FST.ai framework generalize to other combat sports (e.g., judo, karate) or team sports (e.g., football) with different action dynamics?
- **Basis in paper:** [explicit] Section 7.1 claims the architecture is "inherently generalizable" and lists adaptation to other sports as a key extended application, while Section 10.1 lists "Multi-Sport Deployment" as a specific next step.
- **Why unresolved:** While the methodology is sport-agnostic in theory, the system has primarily been validated for Taekwondo head kicks.
- **Evidence:** Successful pilot trials and accuracy metrics in non-Taekwondo environments, such as detecting throws in judo or fouls in football.

### Open Question 2
- **Question:** Can the system accurately detect illegal actions (e.g., clinching, stalling) and integrate multimodal data like audio or wearable sensors?
- **Basis in paper:** [explicit] Section 9.6 explicitly lists "Detection of illegal actions" and "Multimodal fusion" as distinct future research directions.
- **Why unresolved:** The current implementation focuses almost exclusively on valid scoring actions (head kicks) and relies on computer vision alone.
- **Evidence:** A model extension capable of classifying rule violations with comparable precision to its scoring classification.

### Open Question 3
- **Question:** To what extent does the "Human-in-the-Loop" feedback loop improve model consistency without inadvertently encoding specific jury biases over time?
- **Basis in paper:** [inferred] While Section 3.5 describes using jury overrides for retraining, Section 2 explicitly aims to eliminate cultural/interpretive disparities. The interaction between localized jury feedback and the goal of universal scoring consistency is not fully validated.
- **Why unresolved:** It remains unclear if continuous retraining on specific jury decisions preserves the "global consistency" the system aims to establish.
- **Evidence:** Longitudinal performance benchmarks showing scoring variance decreases across different international tournaments as the model is updated.

## Limitations
- Model performance specifics missing: No quantitative validation results for pose accuracy, classification accuracy, or impact detection rates.
- Threshold calibration unclear: Exact numerical thresholds for impact detection and kick classification are inconsistently cited and not fully specified.
- Edge hardware scalability: No data on thermal stability or performance under sustained load during multi-hour competitions.

## Confidence
- **High confidence:** The core architecture (pose → temporal classification → impact detection → edge inference) is technically sound and well-aligned with the problem.
- **Medium confidence:** The mechanism for reducing decision time is plausible, but actual performance gains depend on unvalidated threshold settings and hardware stability.
- **Low confidence:** Claims about fairness and trust improvements are not substantiated with user studies, error rate comparisons, or real competition data.

## Next Checks
1. **Quantify classification accuracy:** Run the system on a labeled dataset of diverse Taekwondo kicks and report precision, recall, and F1-score for turning vs. standard kicks, especially under occlusion.
2. **Threshold robustness test:** Systematically vary deceleration and IoU thresholds; measure impact on false positive/negative rates to identify optimal operating points.
3. **Latency under sustained load:** Simulate a full competition day (3+ hours) on edge hardware; measure thermal throttling effects and verify sustained sub-200ms inference.