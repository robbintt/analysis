---
ver: rpa2
title: 'One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean
  Geometry'
arxiv_id: '2601.22521'
source_url: https://arxiv.org/abs/2601.22521
tags:
- pmpo
- gmpo
- arxiv
- mean
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the instability of group-based reinforcement\
  \ learning in mathematical reasoning, where fixed aggregation geometries (arithmetic\
  \ mean in GRPO or geometric mean in GMPO) fail to adapt to the heterogeneous and\
  \ evolving reliability of reasoning trajectories. The authors propose Power-Mean\
  \ Policy Optimization (PMPO), a unified framework that parameterizes the aggregation\
  \ geometry via a power-mean exponent p, recovering GRPO (p=1) and GMPO (p\u2192\
  0) as special cases."
---

# One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry

## Quick Facts
- arXiv ID: 2601.22521
- Source URL: https://arxiv.org/abs/2601.22521
- Reference count: 32
- Key outcome: PMPO achieves state-of-the-art performance on mathematical reasoning benchmarks with average accuracy gains of 1.5% over GMPO

## Executive Summary
This paper introduces Power-Mean Policy Optimization (PMPO), a unified framework that dynamically adjusts aggregation geometry in group-based reinforcement learning for mathematical reasoning. PMPO parameterizes the aggregation exponent p, smoothly transitioning between arithmetic mean (GRPO) and geometric mean (GMPO) based on trajectory reliability. The method uses a Clip-aware Effective Sample Size mechanism to determine p per trajectory, addressing the instability caused by fixed aggregation geometries when dealing with heterogeneous reasoning trajectories.

## Method Summary
PMPO unifies group-based RL by parameterizing the aggregation geometry via a power-mean exponent p, which recovers GRPO (p=1) and GMPO (p→0) as special cases. The key innovation is dynamically determining p for each trajectory using a Clip-aware Effective Sample Size (ESS) mechanism. This mechanism maps trajectory clipping rates to target ESS values and solves for the p that achieves this target, enabling smooth transitions between aggressive (arithmetic) and conservative (geometric) updates. Theoretically, varying p modulates the concentration of gradient updates, acting as an inverse temperature over token-level log-probability changes. The method is evaluated on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-7B across multiple mathematical reasoning benchmarks.

## Key Results
- PMPO achieves state-of-the-art performance on mathematical reasoning benchmarks
- Average accuracy gains of 1.5% over GMPO and up to 3.3% on hard tasks like AIME24
- Superior stability and efficiency compared to static baselines and heuristic adaptations
- Demonstrates effectiveness across diverse benchmarks including AIME24, AMC, MATH500, Minerva, and OlympiadBench

## Why This Works (Mechanism)
PMPO works by dynamically adjusting the aggregation geometry based on trajectory reliability rather than using fixed aggregation methods. The power-mean exponent p controls how much weight is given to different reasoning trajectories during policy updates. When trajectories are reliable (low clipping rates), p approaches 1 (arithmetic mean), allowing more aggressive updates. When trajectories are unreliable (high clipping rates), p approaches 0 (geometric mean), providing more conservative updates. This adaptive mechanism prevents catastrophic performance degradation from poorly rewarded trajectories while maintaining efficient learning from reliable ones.

## Foundational Learning
- **Power Mean**: Generalization of arithmetic and geometric means; why needed for unified framework; quick check: verify p=1 gives arithmetic mean, p→0 gives geometric mean
- **Effective Sample Size (ESS)**: Measures effective number of independent samples; why needed to quantify trajectory reliability; quick check: confirm ESS decreases with higher correlation between samples
- **Clipping-aware mechanisms**: Adjust based on gradient clipping frequency; why needed to detect unreliable trajectories; quick check: verify higher clipping rates indicate more unreliable trajectories
- **Inverse temperature in RL**: Controls exploration-exploitation tradeoff; why needed to understand p's role; quick check: confirm higher p increases gradient magnitude concentration
- **Group-based RL**: Uses multiple trajectories for policy updates; why needed context for PMPO; quick check: verify standard GRPO/GMPO use fixed aggregation methods
- **Trajectory reliability**: Quality and consistency of reasoning paths; why needed to justify dynamic p selection; quick check: correlate clipping rates with final performance

## Architecture Onboarding

**Component map:** Trajectory reward signals -> Clip-aware ESS calculator -> p solver -> Power-mean aggregator -> Policy gradient update

**Critical path:** During training, each trajectory's reward is computed, clipping rates are measured, ESS is calculated, p is solved for to match target ESS, power-mean aggregation is performed, and policy gradients are updated accordingly.

**Design tradeoffs:** The main tradeoff is between computational overhead of dynamic p calculation versus stability gains. Static methods (GRPO/GMPO) are simpler but less adaptive to trajectory quality variations.

**Failure signatures:** Poor performance on hard problems, unstable training curves, or inability to recover from poor trajectories indicate inappropriate p selection or ESS calculation issues.

**First experiments:**
1. Verify PMPO recovers GRPO and GMPO by fixing p=1 and p→0 respectively
2. Test ESS calculation sensitivity by varying clipping thresholds and reward distributions
3. Compare training stability by monitoring gradient norms and reward curves across different p values

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical grounding for dynamic p selection via ESS remains largely empirical
- The mapping between clipping rates and target ESS values is derived from observed training dynamics rather than first-principles optimization
- The method introduces hyperparameters (target ESS values, clipping thresholds) whose sensitivity to dataset and model scale is not thoroughly explored

## Confidence
- **High**: PMPO's ability to recover GRPO and GMPO as special cases; empirical performance gains over static baselines on tested benchmarks
- **Medium**: The effectiveness of dynamic p selection via ESS across all reasoning domains; theoretical claims about p as an inverse temperature
- **Low**: Generalizability to non-mathematical reasoning tasks; robustness to different reward distributions and group sizes

## Next Checks
1. **Ablation on ESS sensitivity**: Systematically vary target ESS values and clipping thresholds across multiple runs to quantify impact on final performance and training stability
2. **Cross-domain generalization**: Evaluate PMPO on non-mathematical reasoning benchmarks (e.g., code generation, commonsense reasoning) to test domain transferability
3. **Scaling study**: Test PMPO with larger model sizes (e.g., 70B+ parameters) and longer trajectory lengths to verify performance scaling and computational efficiency benefits hold at scale