---
ver: rpa2
title: 'ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat
  Modeling'
arxiv_id: '2509.05379'
source_url: https://arxiv.org/abs/2509.05379
tags:
- threat
- system
- systems
- security
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ThreatGPT is an Agentic AI framework that automates threat modeling
  for public safety systems by integrating large language models with established
  cybersecurity frameworks (STRIDE, MITRE ATT&CK, CVE, NIST, CISA). Using few-shot
  learning, it generates structured threat models from user-defined system components
  without requiring deep cybersecurity expertise.
---

# ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling

## Quick Facts
- **arXiv ID**: 2509.05379
- **Source URL**: https://arxiv.org/abs/2509.05379
- **Reference count**: 30
- **Primary result**: AI-driven threat modeling reduces generation time from 40+ hours to ~30 seconds while maintaining accuracy for public safety systems

## Executive Summary
ThreatGPT is an agentic AI framework that automates threat modeling for public safety systems by integrating large language models with established cybersecurity frameworks (STRIDE, MITRE ATT&CK, CVE, NIST, CISA). Using few-shot learning with 50+ structured examples, it generates comprehensive threat models from user-defined system components without requiring deep cybersecurity expertise. The system reduces threat model generation time from over 40 hours (human expert) to approximately 30 seconds, enabling faster risk identification and mitigation in critical infrastructure. While effective for standard architectures, limitations include dependency on training data quality and external API latency, with future work focusing on real-time vulnerability integration and interactive refinement.

## Method Summary
ThreatGPT employs a four-layer architecture (CLI, AI Agent, Knowledge Base, Pretrained LLM) using Google Gemini API with few-shot prompting. The AI Agent autonomously manages the threat modeling lifecycle through multi-stage workflow: goal recognition, multi-step planning, initial draft generation, self-verification, completeness review with clarification questions, and refinement. The system grounds outputs in authoritative security frameworks (STRIDE, MITRE ATT&CK, CVE, NIST, CISA) through a static knowledge base. Users can describe systems at three complexity levels (simple: full component details, compound: partial components, complex: system name only), with the agent generating structured threat models covering assets, entry points, attacker models, threats/vulnerabilities, and mitigations.

## Key Results
- Generation time reduced from over 40 hours (human expert) to approximately 30 seconds
- Maintains accuracy and contextual relevance across different system complexity levels
- Successfully structures outputs around STRIDE categories with appropriate MITRE ATT&CK techniques and CVE references

## Why This Works (Mechanism)

### Mechanism 1: Few-shot learning enables domain-specific threat model generation without fine-tuning
The system provides >50 structured threat model examples incorporating STRIDE, MITRE ATT&CK, CVE, NIST, and CISA frameworks to the LLM. These examples create a pattern for how threat models should be structured—assets, entry points, attacker models, threats/vulnerabilities, and mitigations. When users describe a new system, the LLM matches this pattern against learned examples to generate contextually relevant outputs. Core assumption: The quality and diversity of few-shot examples sufficiently cover the threat modeling patterns needed for novel system architectures.

### Mechanism 2: Agentic workflow autonomously manages the threat modeling lifecycle
The AI Agent Layer implements a multi-stage autonomous workflow: (1) goal recognition from user input, (2) multi-step planning for asset identification through mitigation, (3) initial draft generation, (4) self-verification and auto-formatting, (5) completeness review with clarification question generation, and (6) refinement based on user responses. This reduces human intervention between initial query and final output. Core assumption: The agent can accurately assess when clarifications are needed and formulate useful questions without human guidance.

### Mechanism 3: Knowledge base grounding anchors LLM outputs in authoritative security frameworks
The Knowledge Base Layer stores systematically compiled framework data (STRIDE, MITRE ATT&CK, CVE, NVD, NIST, CISA). The AI Agent dynamically retrieves supplementary information from this layer to ground LLM responses in domain-specific security knowledge, improving contextual relevance and reducing hallucination of non-standard threat categories. Core assumption: Static framework knowledge remains current and sufficient for emerging threat landscapes.

## Foundational Learning

- **Concept: STRIDE threat categorization** (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege)
  - Why needed here: ThreatGPT generates threat models structured around STRIDE categories; users must understand these to evaluate output completeness and relevance
  - Quick check question: Given a web API with user authentication, which STRIDE categories apply?

- **Concept: Few-shot learning vs. fine-tuning tradeoffs**
  - Why needed here: The system relies on few-shot examples rather than model weight updates; understanding this distinction is critical for diagnosing output quality issues and determining when additional examples are needed
  - Quick check question: If ThreatGPT produces poor outputs for IoT systems, would you add training examples or modify model weights?

- **Concept: Agentic AI autonomy boundaries**
  - Why needed here: The agent makes autonomous decisions about clarification questions and completeness reviews; engineers must understand where human oversight remains necessary for safety-critical applications
  - Quick check question: At which stages of the ThreatGPT workflow should a human reviewer validate outputs before action?

## Architecture Onboarding

- **Component map**: User query → AI Agent preprocessing → Prompt construction (with few-shot examples + knowledge base retrieval) → Gemini API call → Response postprocessing → Completeness check → Clarification questions (if needed) → Refined output
- **Critical path**: Generation time bottleneck is API latency (20-30 seconds observed)
- **Design tradeoffs**: Few-shot learning chosen over fine-tuning to avoid API fine-tuning costs and enable rapid adaptation, at the cost of output consistency for novel architectures; External API (Gemini) chosen over local deployment for access to high-capability model, introducing latency dependency and data privacy considerations; Static knowledge base chosen over real-time RAG for simplicity, limiting coverage of emerging threats
- **Failure signatures**: Incomplete threat models with generic mitigations indicate underspecified user prompts or missing few-shot coverage; Excessive clarification loops suggest agent completeness review is over-triggering on ambiguous inputs; Hallucinated CVE IDs or non-standard threat categories indicate knowledge base grounding failure
- **First 3 experiments**: (1) Reproduce simple/compound/complex prompt comparison using Drone Delivery Management System example to validate 20-30 second response times and output structure consistency; (2) Test edge cases: systems with novel architectures (e.g., quantum communication networks) not covered in training examples to assess few-shot generalization limits; (3) Validate knowledge base grounding: manually verify that generated MITRE ATT&CK technique IDs and CVE references exist in authoritative databases

## Open Questions the Paper Calls Out

1. **Real-time vulnerability integration**: How can retrieval-augmented generation (RAG) be integrated to provide real-time access to live vulnerability databases without significantly increasing response latency? [explicit] Future work will focus on "integrating retrieval-augmented generation (RAG) for real-time access to live vulnerability databases." The current system relies on static few-shot examples, which cannot capture emerging threats, but RAG introduces retrieval overhead that may conflict with the system's emphasis on rapid 30-second generation times.

2. **Interactive refinement mechanisms**: What mechanisms can enable interactive refinement of threat models through iterative user feedback? [explicit] Future work will focus on "enabling interactive refinement of threat models." The current system generates a single threat model output with clarification questions, but lacks a structured feedback loop for users to iteratively improve, correct, or expand the model based on domain expertise.

3. **Systematic accuracy evaluation**: How can the accuracy and completeness of AI-generated threat models be systematically evaluated against human expert-generated baselines? [inferred] The paper claims threat models maintain "accuracy and contextual relevance" but provides no rigorous quantitative evaluation methodology comparing AI output to human expert threat models. Current evaluation is limited to response time measurement.

## Limitations

- **Few-shot generalization limits**: Novel system architectures not covered in training data may produce incomplete or irrelevant threat models
- **Knowledge base currency constraints**: Static framework integration may miss emerging threats or zero-day vulnerabilities
- **API dependency risks**: External Gemini API introduces latency and data privacy considerations, with no local deployment option for sensitive infrastructure

## Confidence

- **High confidence**: Generation time reduction from 40+ hours to 30 seconds is empirically supported and directly measurable
- **Medium confidence**: Few-shot learning effectiveness and knowledge base grounding mechanisms are logically sound but lack direct experimental validation
- **Low confidence**: Claims about handling novel system architectures and emerging threats are not experimentally validated; agentic autonomy safety in critical infrastructure contexts requires further study

## Next Checks

1. **Few-shot generalization testing**: Systematically evaluate ThreatGPT's performance on systems with architectures not represented in the training examples (e.g., edge computing networks, IoT sensor meshes). Compare output completeness and relevance against human expert threat models.

2. **Real-time threat intelligence integration**: Implement and benchmark a mechanism for incorporating emerging CVE/NVD/CISA alerts into the knowledge base. Measure impact on threat model accuracy for systems with recently disclosed vulnerabilities.

3. **Human oversight validation**: Conduct user studies with security professionals evaluating ThreatGPT outputs for safety-critical systems (e.g., power grid control, emergency response systems). Measure required human review time and identify failure patterns requiring expert intervention.