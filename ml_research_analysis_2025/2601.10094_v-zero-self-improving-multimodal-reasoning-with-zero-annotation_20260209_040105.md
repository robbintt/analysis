---
ver: rpa2
title: 'V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation'
arxiv_id: '2601.10094'
source_url: https://arxiv.org/abs/2601.10094
tags:
- reasoning
- solver
- questioner
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes V-Zero, a self-improvement framework for vision-language
  models that operates exclusively on unlabeled images without any human annotations.
  The core idea is a co-evolutionary loop between two roles: a Questioner that generates
  multiple-choice questions from images, and a Solver that answers them.'
---

# V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation

## Quick Facts
- arXiv ID: 2601.10094
- Source URL: https://arxiv.org/abs/2601.10094
- Authors: Han Wang; Yi Yang; Jingyuan Hu; Minfeng Zhu; Wei Chen
- Reference count: 19
- Primary result: V-Zero achieves +1.7 score improvement on visual mathematical reasoning and +2.6 on general vision-centric tasks without human annotations

## Executive Summary
This paper proposes V-Zero, a self-improvement framework for vision-language models that operates exclusively on unlabeled images without any human annotations. The core idea is a co-evolutionary loop between two roles: a Questioner that generates multiple-choice questions from images, and a Solver that answers them. The Questioner uses a dual-track reasoning reward that contrasts intuitive answers with reasoned outputs to encourage challenging question generation, while the Solver is trained via RLVR using pseudo-labels from majority voting over sampled responses. Both roles are iteratively optimized using Group Relative Policy Optimization (GRPO). Experiments show that V-Zero improves Qwen2.5-VL-7B-Instruct performance by +1.7 on visual mathematical reasoning and +2.6 on general vision-centric tasks, outperforming strong supervised baselines trained on human-annotated data.

## Method Summary
V-Zero implements a self-improving loop between a Questioner and Solver, both initialized from Qwen2.5-VL-7B-Instruct. The Questioner generates multiple-choice questions from unlabeled images, optimized via GRPO with a dual-track reward that contrasts intuitive vs reasoned responses. The Solver answers these questions using RLVR with binary accuracy rewards, training on pseudo-labels from majority voting (m=10 samples). The framework iterates between updating the Questioner to create challenging questions and updating the Solver to answer them correctly. Training uses 4,000 unlabeled images from OpenVLThinker datasets with difficulty filtering (0.3 ≤ confidence ≤ 0.8) and specific hyperparameters including 4× A800 80GB GPUs, batch size 64, and temperature 1.0.

## Key Results
- V-Zero improves Qwen2.5-VL-7B-Instruct by +1.7 on visual mathematical reasoning benchmarks
- Achieves +2.6 improvement on general vision-centric tasks compared to supervised baselines
- Maintains high question format validity (99%+) after iteration 1, up from 64.9% baseline
- Outperforms models trained on human-annotated data despite using only unlabeled images

## Why This Works (Mechanism)
V-Zero leverages a co-evolutionary loop where the Questioner and Solver mutually improve each other through targeted challenges. The dual-track reasoning reward encourages the Questioner to generate questions where intuitive and reasoned answers diverge, creating meaningful learning opportunities for the Solver. By using majority voting for pseudo-labels, the framework can generate training signals without human annotation. The iterative nature allows both models to progressively specialize - the Questioner learns to probe the Solver's weaknesses while the Solver develops better reasoning capabilities to handle increasingly difficult questions.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: Policy gradient method that computes advantage using group-level statistics rather than baselines, needed for stable training of both Questioner and Solver; quick check: verify gradient updates use group statistics per Equation 3
- **Dual-track reasoning reward**: Contrasts intuitive vs reasoned outputs to encourage challenging questions, needed to prevent trivial question generation; quick check: monitor reward distribution for high values when intuition ≠ reasoning
- **Majority voting for pseudo-labels**: Aggregates multiple sampled responses to create training targets, needed to avoid human annotation; quick check: verify majority consensus rate increases as Solver improves
- **Difficulty filtering (0.3-0.8 confidence)**: Excludes overly easy/hard samples to maintain learning efficiency, needed to prevent reward hacking; quick check: track question difficulty distribution across iterations
- **MCQ format validation**: Ensures generated questions follow proper multiple-choice structure, needed for consistent Solver training; quick check: measure format validity rate (target 99%+ after iteration 1)

## Architecture Onboarding
- **Component map**: Unlabeled images → Questioner (GRPO) → MCQs → Solver (RLVR) → answers → pseudo-labels → repeat
- **Critical path**: Image → Question generation → Answer generation → Pseudo-label creation → Model update
- **Design tradeoffs**: Zero annotation vs. potentially noisier training signals; co-evolutionary loop vs. potential for adversarial drift; difficulty filtering vs. coverage of learning space
- **Failure signatures**: 
  - Low format validity (64.9% baseline) → implement format validation reward
  - Solver overfitting to simple patterns → verify co-evolution updates both models
  - Trivial question generation → monitor dual-track reward distribution
- **First experiments**: 
  1. Reproduce format validity improvement curve (64.9% → 99%+ after iteration 1)
  2. Verify co-evolution by tracking both models' update frequencies per iteration
  3. Test difficulty filtering effectiveness by analyzing question difficulty distribution

## Open Questions the Paper Calls Out
**Open Question 1**: Can V-Zero's self-improvement paradigm scale effectively to larger VLM architectures (e.g., 70B+ parameters) and diverse model families beyond Qwen2.5-VL?
- Basis: "Due to compute constraints, our experiments are limited to the Qwen2.5-VL series. Our future work will extend V-Zero to more model architectures."
- Why unresolved: Co-evolutionary dynamics may differ with larger capacity models; convergence properties and computational efficiency remain untested.
- Evidence needed: Experiments applying V-Zero to 70B+ models like InternVL or LLaVA-OneVision, comparing iteration dynamics and performance gains.

**Open Question 2**: What is the minimum model capacity threshold required for stable self-improvement, and can smaller models benefit from curriculum adjustments or noise-reduction mechanisms?
- Basis: "For the smaller Qwen2.5-VL-3B-Instruct, the model reaches its performance peak in the first iteration... We attribute this early saturation to the restricted model capacity and capability, which increases sensitivity to the noise in self-generated data."
- Why unresolved: Trade-off between exploration and noise tolerance in smaller models is uncharacterized; mitigation strategies unexplored.
- Evidence needed: Systematic ablations across model scales (1B-7B) with varied sampling temperatures, noise-filtering thresholds, and curriculum pacing.

**Open Question 3**: How robust is the majority-voting pseudo-label mechanism when the Solver's initial biases produce systematically incorrect but consistent outputs?
- Basis: Solver derives pseudo-labels via majority voting assuming majority converges on correct answers, which may fail with systematic misconceptions.
- Why unresolved: No analysis of failure modes where majority voting reinforces errors rather than corrects them.
- Evidence needed: Controlled experiments injecting known biases into base model, tracking pseudo-label accuracy across iterations.

**Open Question 4**: Does question diversity collapse over extended iterations (beyond 2), leading to domain over-specialization or adversarial drift?
- Basis: Experiments stop at Iteration 2 without diversity metrics; potential for mode collapse or exploitation of solver-specific weaknesses exists.
- Why unresolved: Long-term dynamics of Questioner-Solver loop remain unknown.
- Evidence needed: Multi-iteration experiments (5-10+ iterations) with semantic diversity metrics and out-of-distribution generalization tests.

## Limitations
- Training schedule details unspecified (number of steps/epochs per phase and total iterations unknown)
- Difficulty filtering mechanism not fully specified (per question type vs global application unclear)
- Lack of explicit negative sampling strategies may enable reward hacking through adversarial examples
- Evaluation focuses on existing benchmarks that may not capture novel reasoning task performance

## Confidence
- High confidence: Questioner-Solver co-evolutionary architecture and GRPO implementation details are well-specified
- Medium confidence: Reported improvements (+1.7 on mathematical reasoning, +2.6 on vision-centric tasks) are plausible given training setup
- Low confidence: "Zero annotation" claim technically accurate but relies on OpenVLThinker datasets constructed with human feedback

## Next Checks
1. Reproduce the format validity improvement curve (64.9% → 99%+ after iteration 1) to verify effectiveness of format validation reward mechanism
2. Conduct ablation studies removing the dual-track reasoning reward to quantify its contribution to question quality and overall performance gains
3. Test the framework on held-out image distributions not present in OpenVLThinker datasets to assess generalization beyond training distribution