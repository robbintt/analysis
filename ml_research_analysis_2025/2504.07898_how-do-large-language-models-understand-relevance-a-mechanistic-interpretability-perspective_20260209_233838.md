---
ver: rpa2
title: How do Large Language Models Understand Relevance? A Mechanistic Interpretability
  Perspective
arxiv_id: '2504.07898'
source_url: https://arxiv.org/abs/2504.07898
tags:
- relevance
- attention
- document
- query
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uses mechanistic interpretability techniques to understand
  how large language models (LLMs) process and operationalize relevance for information
  retrieval tasks. Through activation patching experiments on Llama-3.1-8B-Instruct,
  the authors identify a multi-stage, progressive information flow: early layers capture
  semantic information from documents and queries, middle layers integrate this with
  task instructions, and later layers control output formatting through specific attention
  heads.'
---

# How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective

## Quick Facts
- arXiv ID: 2504.07898
- Source URL: https://arxiv.org/abs/2504.07898
- Authors: Qi Liu; Jiaxin Mao; Ji-Rong Wen
- Reference count: 40
- One-line primary result: Activation patching experiments reveal LLMs use a universal three-stage progressive mechanism for relevance assessment across different prompt formats.

## Executive Summary
This paper uses mechanistic interpretability techniques to understand how large language models (LLMs) process and operationalize relevance for information retrieval tasks. Through activation patching experiments on Llama-3.1-8B-Instruct, the authors identify a multi-stage, progressive information flow: early layers capture semantic information from documents and queries, middle layers integrate this with task instructions, and later layers control output formatting through specific attention heads. The study reveals that LLMs share a universal mechanism for relevance assessment that works across different prompt formats (pointwise vs pairwise) and downstream tasks like relevance judgment and document ranking. Key components were validated through knockout experiments, where ablating specific attention heads significantly degraded model performance, demonstrating their necessity for relevance assessment.

## Method Summary
The study employs activation patching to trace information flow through Llama-3.1-8B-Instruct when processing relevance judgments. The method uses three-run protocol per patch: clean (positive document), corrupted (negative document), and patched (corrupted input with clean activation restored at target component). Patches are applied at attention outputs, individual heads, and MLP outputs across document, query, instruction, and last-token positions. The Indirect Effect metric quantifies component importance: IE = (LD_patched - LD_corrupted) / (LD_clean - LD_corrupted). Validation uses mean ablation on TREC DL19 to measure F1 and NDCG@10 degradation when knocking out top-20 heads per position.

## Key Results
- LLMs follow a three-stage progressive information flow: early layers extract document semantics, middle layers integrate with query/instruction context, late layers control output formatting
- A sparse set of late-layer attention heads (e.g., L31H1) specifically control the final token generation ("yes"/"no") independent of semantic reasoning
- Strong correlation (0.65-0.82) between attention interaction scores and indirect effects suggests query-document interaction via attention mechanisms
- RBO of 0.9 between pointwise and pairwise prompt types indicates a universal relevance assessment mechanism across different prompt formats
- Knockout experiments show ablating specific attention heads significantly degrades model performance, validating their necessity for relevance assessment

## Why This Works (Mechanism)

### Mechanism 1: Progressive Information Flow
- **Claim:** If relevance judgment occurs in off-the-shelf LLMs, it likely follows a three-stage progressive information flow where specific components activate at distinct depth intervals.
- **Mechanism:** The model routes inputs through a temporal pipeline: (1) Early-layer MLPs extract semantic knowledge from document tokens; (2) Middle-layer Attention modules integrate this with query and instruction context; (3) Late-layer Attention heads enforce the output format.
- **Core assumption:** The "residual stream" allows information to be written by early layers and read/utilized by later layers without degradation, and that patching activations isolates these contributions.
- **Evidence anchors:**
  - [abstract]: "LLMs first extract query and document information in the early layers, then process relevance information according to instructions in the middle layers, and finally utilize specific attention heads in the later layers."
  - [section 4.2]: "We can find that the model processes and transmits information progressively... visual patterns... resemble a diagonal pattern."
  - [corpus]: "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers" supports the general concept of分层processing (lexical early, contextual late) in Transformers.

### Mechanism 2: Sparse Late-Layer Output Control
- **Claim:** A sparse set of attention heads in the final layers likely dictates the specific token generation (e.g., "yes"/"no") independent of the semantic reasoning performed earlier.
- **Mechanism:** Specific heads (e.g., L31H1 in Llama-3.1) project directly to the unembedding matrix to boost the logit of the target token, acting as a "switch" for the instruction-following format.
- **Core assumption:** Assumption: The model learns to delegate the final "decision" enforcement to a small number of specialized neurons rather than distributing it across the entire layer width.
- **Evidence anchors:**
  - [section 4.3]: "We observed that 'yes' (or 'no') consistently exhibits the highest logit in L31H1... the attention heads affecting the output are highly sparse."
  - [abstract]: "...utilize specific attention heads in the later layers to generate relevance judgments in the required format."

### Mechanism 3: Query-Document Interaction via Attention Scores
- **Claim:** The model assesses relevance partly by calculating interaction scores between query and document tokens within specific attention heads, which correlates with the causal effect of those heads.
- **Mechanism:** Attention heads in middle layers (e.g., L8H11) attend strongly from query tokens to document tokens. The differential between positive and negative document attention patterns serves as a heuristic relevance signal.
- **Core assumption:** Assumption: The "attention interaction score" defined in the paper accurately approximates the model's internal relevance scoring mechanism, and high correlation implies causation in this context.
- **Evidence anchors:**
  - [section 4.3]: "We find a strong correlation between the two effects... correlation coefficient is 0.65 and 0.82 for pointwise and pairwise respectively."
  - [section 4.3]: "Those heads with the highest effect (L8H11 and L10H2) still obtain high attention interaction scores."

## Foundational Learning

- **Concept: Activation Patching (Causal Mediation Analysis)**
  - **Why needed here:** This is the primary tool used to determine "what matters" in the model. Without understanding the difference between a clean run, corrupted run, and patched run, the Indirect Effect (IE) metric is meaningless.
  - **Quick check question:** If a patched run restores 50% of the probability difference between clean and corrupted runs, what is the Indirect Effect (IE) of that component? (Answer: 0.5).

- **Concept: Residual Stream & MLP Knowledge Storage**
  - **Why needed here:** The paper relies on the specific theory that MLPs act as "memory banks" or "knowledge storage" (referenced in Section 4.2). Understanding that information persists in the residual stream is key to understanding how early layers affect late output.
  - **Quick check question:** Why do the authors patch the MLP output at the *document* position in early layers but look at the result at the *last* token position? (Answer: Because information written to the residual stream by the MLP persists and is read by later layers at the generation position).

- **Concept: Rank Biased Overlap (RBO)**
  - **Why needed here:** Used in Section 4.2 to quantify the similarity between pointwise and pairwise mechanisms. It provides the quantitative evidence for the "universal mechanism" claim.
  - **Quick check question:** If the RBO is 0.9, does this suggest the ranking of important layers is similar or different between two prompt types? (Answer: Similar).

## Architecture Onboarding

- **Component map:** Early Layers (0-10) -> MLPs at document positions -> Middle Layers (10-20) -> Attention Heads at query/instruction positions -> Late Layers (20-32) -> Sparse Attention Heads at last token -> Unembed to "Yes/No"

- **Critical path:** Input -> [Early MLPs read Doc] -> [Mid Attention reads Query+Doc] -> [Late Heads attend to final token] -> [Unembed to "Yes/No"]

- **Design tradeoffs:**
  - **Universality vs. Specificity:** The model uses a "universal mechanism" (high RBO) for relevance, meaning it reuses the same circuits for pointwise and pairwise tasks. This suggests high efficiency but potentially lower adaptability if the prompts deviate significantly from the trained distribution.
  - **Robustness vs. Sparsity:** The "backup behavior" (Section 4.5) means the model is robust to minor damage (ablating query heads has low impact), but knocking out the sparse late heads is catastrophic.

- **Failure signatures:**
  - **Backup Behavior:** Ablating components with lower indirect effects (like query positions) causes minimal performance drop, complicating debugging by simple removal.
  - **Instruction Sensitivity:** If the instruction tokens are patched/ablated in middle layers, the model loses the ability to follow the "yes/no" format constraint (drop in F1-score), even if relevance signals are intact.

- **First 3 experiments:**
  1. **Layer-wise Indirect Effect Tracing:** Run the activation patching pipeline on a small set of positive/negative pairs to reproduce the "diagonal pattern" in Figure 3 for your target model. This validates the progressive flow hypothesis.
  2. **Sparse Head Knockout:** Identify the top 20 attention heads with the highest IE at the last token position. Mean-ablate them and measure the drop in binary classification accuracy. This validates the output control mechanism.
  3. **Cross-Prompt RBO Calculation:** Run activation patching for both pointwise and pairwise prompts on the same queries. Compute the RBO to verify if the "universal mechanism" holds for your specific model version or fine-tune.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a minimal circuit sufficient for completing the entire relevance judgment task be identified? (Basis: Section 5 states, "Whether it is possible to identify a minimal circuit that is adequate for completing the task remains a question worthy of exploration.")

- **Open Question 2:** How do LLMs acquire the capacity for relevance judgment during pre-training versus post-training? (Basis: Section 5 notes, "How LLMs acquire the capacity for relevance judgment during either pre-training or post-training remains unexplored.")

- **Open Question 3:** Does fine-tuning alter the internal mechanisms of relevance assessment or merely enhance existing ones? (Basis: Section 5 mentions, "Validating this observation [that fine-tuning enhances existing mechanisms] in relevance assessment is also valuable.")

## Limitations

- **Single model generalizability:** The study focuses on Llama-3.1-8B-Instruct and validates on two other 7B models, leaving untested whether the universal mechanism holds across different model families and architectures.

- **Correlation vs. causation in attention mechanisms:** While strong correlation (0.65-0.82) exists between attention interaction scores and indirect effects, this does not definitively prove that attention scores directly encode relevance judgments rather than being correlated epiphenomena.

- **Prompt format specificity:** The analysis examines pointwise and pairwise formats but does not test other prompt structures or longer, more complex instructions that might reveal different mechanistic patterns.

## Confidence

- **Single model generalizability:** Medium
- **Correlation vs. causation in attention mechanisms:** Medium
- **Prompt format specificity:** Medium
- **Limited task diversity:** Low

## Next Checks

1. **Cross-model validation:** Apply the same activation patching methodology to GPT-3.5/4 and other transformer families to test if the three-stage progressive flow pattern holds universally across different architectures.

2. **Dynamic activation analysis:** Track how attention patterns and MLP activations change during inference across multiple decoding steps, not just final token generation, to understand the temporal dynamics of relevance processing.

3. **Robustness testing:** Systemally vary document lengths, query complexity, and instruction specificity to determine the boundaries of the "universal mechanism" and identify conditions where alternative circuits emerge.