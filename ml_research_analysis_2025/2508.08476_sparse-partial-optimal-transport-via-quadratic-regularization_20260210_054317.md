---
ver: rpa2
title: Sparse Partial Optimal Transport via Quadratic Regularization
arxiv_id: '2508.08476'
source_url: https://arxiv.org/abs/2508.08476
tags:
- transport
- optimal
- qpot
- sparsity
- nguyen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of sparsity in transport plans\
  \ for Partial Optimal Transport (POT), which is crucial for interpretability and\
  \ computational efficiency in many machine learning applications. The authors propose\
  \ a novel formulation called Quadratic Regularized POT (QPOT) that uses a quadratic\
  \ regularizer (\u21132 norm) instead of the commonly used entropic regularizer."
---

# Sparse Partial Optimal Transport via Quadratic Regularization

## Quick Facts
- arXiv ID: 2508.08476
- Source URL: https://arxiv.org/abs/2508.08476
- Reference count: 18
- Key outcome: Novel QPOT formulation using quadratic regularization induces higher sparsity in transport plans compared to entropic regularization, with consistent sparsity >90% across experiments while maintaining task performance.

## Executive Summary
This paper addresses sparsity in transport plans for Partial Optimal Transport (POT), a crucial requirement for interpretability and computational efficiency in machine learning applications. The authors propose Quadratic Regularized POT (QPOT) that uses ℓ2-norm regularization instead of the commonly used entropic regularizer. Through extensive experiments on synthetic data, CIFAR-10, color transfer, and domain adaptation tasks, QPOT consistently achieves significantly higher sparsity (often above 0.9) compared to traditional Entropic Regularized POT (EPOT, typically below 0.9) while maintaining or improving task performance.

## Method Summary
The authors formulate QPOT as minimizing the transport cost plus a quadratic regularization term, subject to inequality constraints that define partial transport between distributions with unequal mass. The method is implemented using cvxpy with CLARABEL solver for the quadratic objective and compared against EPOT solved with SCS. The experiments vary the transport mass parameter λ across multiple values and regularization coefficient ε to demonstrate sparsity patterns. For domain adaptation, the transported labels are used to train SVM classifiers to evaluate task performance alongside sparsity metrics.

## Key Results
- QPOT achieves sparsity consistently above 90% across all experiments while EPOT typically stays below 70%
- In color transfer experiments, QPOT maintains visual quality while achieving significantly higher sparsity
- For domain adaptation using moon dataset, QPOT improves classification accuracy while maintaining high sparsity
- The sparsity advantage of QPOT is maintained across different transport mass levels (λ values)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quadratic regularization (ℓ2 norm) induces sparser transport plans than entropic regularization in Partial Optimal Transport problems.
- Mechanism: The quadratic regularizer $\frac{\varepsilon}{2}\|X\|_F^2$ penalizes non-zero entries differently than entropic regularization $-\frac{\varepsilon}{2}H(X)$. While entropic regularization spreads mass across many entries (producing dense plans), the quadratic penalty encourages solutions where many entries are driven to zero or near-zero values, as minimizing $\|X\|_F^2$ directly penalizes the sum of squared entries. This creates a convex optimization landscape where sparse solutions become optimal.
- Core assumption: The convexity of the combined objective (transport cost + quadratic penalty) guarantees that gradient-based or interior-point methods will converge to solutions with the sparsity-promoting structure.
- Evidence anchors:
  - [abstract]: "quadratic regularized POT (QPOT)...induces sparsity to the transport plan"
  - [Section 4, Results]: QPOT consistently achieves >90% sparsity across experiments while EPOT typically stays below 70%
  - [corpus]: Blondel et al. (2018a) cited within paper demonstrates quadratic regularization improves sparsity in standard OT; corpus neighbors show active research in sparse OT variants (PINS, Displacement-Sparse Neural OT) but limited direct comparison of quadratic vs entropic for POT specifically.
- Break condition: When regularizer coefficient ε approaches zero, both methods converge to unregularized POT and sparsity differences diminish; numerical instability may occur at very small ε values for EPOT.

### Mechanism 2
- Claim: Partial Optimal Transport's inequality constraints enable transport between distributions with unequal mass while the total transported mass parameter s controls solution sparsity.
- Mechanism: Standard OT enforces $X\mathbf{1}_n = \mathbf{r}$ and $X^T\mathbf{1}_n = \mathbf{c}$ (strict equalities), requiring $\|\mathbf{r}\|_1 = \|\mathbf{c}\|_1$. POT relaxes these to $X\mathbf{1}_n \leq \mathbf{r}$ and $X^T\mathbf{1}_n \leq \mathbf{c}$ with additional constraint $\mathbf{1}_n^T X\mathbf{1}_n = s$. This allows the optimizer to select only the most cost-effective mass transport pairs up to total mass s, inherently promoting sparsity.
- Core assumption: The parameter $s \leq \min\{\|\mathbf{r}\|_1, \|\mathbf{c}\|_1\}$ correctly captures the application's requirement for partial matching.
- Evidence anchors:
  - [Section 3, Problem Formulation]: Mathematical definition of POT constraint set $\mathcal{U}(\mathbf{r}, \mathbf{c}, s)$
  - [Section 4.1]: Experiments vary λ (mass proportion) from 0.5 to 0.99, showing sparsity patterns change with transport mass
  - [corpus]: Neighbor paper "Accelerated Sinkhorn Algorithms for Partial Optimal Transport" confirms POT is suitable when marginals have unequal size or contain outliers.
- Break condition: When $s = \min\{\|\mathbf{r}\|_1, \|\mathbf{c}\|_1\}$ (full mass transport), POT converges to standard OT behavior and sparsity benefits from partial transport diminish.

### Mechanism 3
- Claim: Solver-regularizer alignment (CLARABEL for quadratic, SCS for entropic) leverages complementary algorithmic properties for efficient convergence.
- Mechanism: CLARABEL uses primal-dual interior-point methods optimized for quadratic objectives—it directly handles $\|X\|_F^2$ without epigraphical reformulation, achieving tight convergence ($10^{-8}$ to $10^{-6}$). SCS uses Douglas-Rachford splitting suited to the smooth, strongly convex structure of entropic regularization, balancing speed and accuracy ($10^{-3}$ to $10^{-4}$ tolerance). Mismatching solver and regularizer would require problem reformulation or face numerical inefficiency.
- Core assumption: The computational cost of obtaining high-accuracy sparse solutions with interior-point methods is acceptable for target applications.
- Evidence anchors:
  - [Section 4, Methods and Implementation]: Explicit solver selection rationale and convergence tolerance comparisons
  - [Section 4.1]: Experimental setup uses cvxpy with these solver assignments
  - [corpus]: No direct corpus comparison of solver-regularizer pairings; this mechanism is primarily supported by paper's methodological justification.
- Break condition: For very large-scale problems, interior-point methods may become memory-bound; gradient-based alternatives (APDAGD mentioned in Future Works) may become necessary.

## Foundational Learning

- **Concept: Optimal Transport fundamentals**
  - Why needed here: QPOT modifies the standard OT formulation; understanding the base problem (transport cost minimization under marginal constraints) is prerequisite to understanding what POT changes and what regularization adds.
  - Quick check question: Given source distribution $\mathbf{r}$ and target $\mathbf{c}$, what constraints does the transport plan $X$ satisfy in standard OT versus POT?

- **Concept: Regularization in optimization**
  - Why needed here: The core contribution is comparing two regularization strategies (quadratic vs entropic); without understanding how regularizers modify objective functions to induce desired properties, the mechanism remains opaque.
  - Quick check question: How does adding $\frac{\varepsilon}{2}\|X\|_F^2$ to the objective change the optimization landscape compared to adding $-\frac{\varepsilon}{2}H(X)$?

- **Concept: Sparsity and its benefits in ML**
  - Why needed here: The paper argues sparse transport plans improve interpretability, memory efficiency, and noise robustness; evaluating these claims requires understanding why sparsity is valuable.
  - Quick check question: In a transport plan matrix $X \in \mathbb{R}^{n \times n}$, what does high sparsity (>90%) imply about the number of active source-target connections?

## Architecture Onboarding

- **Component map:**
Input distributions (r, c) → Cost matrix C computation → Constraint set U(r,c,s)
                                                          ↓
Regularizer selection ←→ Solver selection (CLARABEL/SCS) → Convex program
                                                          ↓
                                              Transport plan X (sparse/dense)
                                                          ↓
                                    Application layer (color transfer, domain adaptation)

- **Critical path:**
  1. Define mass transport parameter λ (controls s)
  2. Compute and normalize cost matrix C
  3. Select regularizer coefficient ε (balance sparsity vs transport cost)
  4. Solve QPOT with CLARABEL (or EPOT with SCS for baseline comparison)
  5. Threshold transport plan at 1e-10 for sparsity evaluation

- **Design tradeoffs:**
  - Higher ε → more regularization effect → potentially sparser solutions but less optimal transport cost
  - Higher λ (more mass transported) → solution approaches dense standard OT behavior
  - CLARABEL's tight convergence vs SCS's speed: QPOT accuracy vs EPOT computational efficiency

- **Failure signatures:**
  - Numerical instability at very small ε (EPOT): Sinkhorn iterations slow or diverge
  - Memory overflow for large n: Interior-point methods scale poorly; consider gradient methods
  - Over-sparse solutions: If ε too large, transport plan may have insufficient connections for application task (e.g., color transfer loses smoothness)
  - Constraint infeasibility: If s > min{∥r∥₁, ∥c∥₁}, problem is ill-posed

- **First 3 experiments:**
  1. **Synthetic validation:** Implement QPOT and EPOT on two 1D distributions (e.g., Gaussian vs Uniform) with n=100 bins; vary ε from $10^{-1}$ to $10^{-6}$ with λ=0.7; verify QPOT sparsity >90% and EPOT <70%
  2. **Sparsity-accuracy tradeoff:** Using the moon dataset domain adaptation setup (n=300), measure both sparsity and SVM classification accuracy across ε values; confirm QPOT maintains accuracy while improving sparsity
  3. **Solver comparison:** On a single problem instance, time CLARABEL solving QPOT vs SCS solving EPOT; verify paper's implicit claim that quadratic regularization's sparsity benefits justify any computational overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can accelerated computational methods, such as APDAGD, be effectively adapted to solve the QPOT formulation?
- Basis: [explicit] The Conclusion states, "Another approach for future improvements would be to develop accelerated computational methods, such as APDAGD... to solve QPOT."
- Why unresolved: The current implementation relies on general-purpose interior-point solvers (CLARABEL), which may not be as efficient as specialized first-order methods for large-scale problems.
- What evidence would resolve it: A dedicated accelerated gradient descent algorithm for QPOT demonstrating competitive convergence speeds with theoretical guarantees.

### Open Question 2
- Question: How can QPOT be adapted for stochastic or constrained decentralized optimization in noisy or multi-agent environments?
- Basis: [explicit] The authors suggest, "one can adapt Stochastic or Constrained Decentralized Optimization methods... for noisy, dynamic and multi-agent applications."
- Why unresolved: The current experiments assume static, centralized data distributions; robustness to noise and decentralized constraints has not been tested.
- What evidence would resolve it: A decentralized QPOT framework that maintains sparsity and accuracy while handling asynchronous updates or noisy data streams.

### Open Question 3
- Question: Does the QPOT formulation maintain its computational efficiency and sparsity advantage on truly large-scale datasets?
- Basis: [inferred] The experiments downscaled CIFAR-10 images to 10×10 pixels (100 bins) and used only 300 data points for domain adaptation.
- Why unresolved: While the paper claims efficiency, the use of generic convex solvers (CLARABEL) on small-scale samples leaves the scalability to high-dimensional data (e.g., >10,000 samples) uncertain compared to Sinkhorn.
- What evidence would resolve it: Benchmarking QPOT on high-resolution images or large-scale datasets (e.g., full MNIST/CIFAR) to verify runtime and memory usage.

## Limitations

- Numerical stability concerns at extreme regularization values (ε as low as $10^{-15}$) may affect reported comparisons
- Unconventional LUV color space conversion formula could affect color transfer results if implementation differs from paper's intent
- Solver-regularizer alignment assumption not validated through ablation studies

## Confidence

- **High confidence**: QPOT consistently achieves higher sparsity than EPOT across all experimental settings
- **Medium confidence**: QPOT maintains or improves task performance while achieving higher sparsity
- **Low confidence**: Computational efficiency claims not thoroughly validated with runtime benchmarks

## Next Checks

1. **Numerical stability verification**: Reproduce domain adaptation experiment with ε = $10^{-4}$ to $10^{-6}$ (avoiding extreme $10^{-15}$ regime) and verify convergence without numerical warnings

2. **Color space conversion validation**: Implement color transfer experiment using both stated LUV conversion and standard CIE LUV definition to determine impact on results

3. **Solver-agnostic comparison**: Implement both QPOT and EPOT with the same solver (e.g., SCS for both) and identical tolerances to isolate regularization effects from solver-specific optimizations