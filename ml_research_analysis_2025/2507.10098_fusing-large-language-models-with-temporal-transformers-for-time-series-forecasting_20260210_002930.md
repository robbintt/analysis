---
ver: rpa2
title: Fusing Large Language Models with Temporal Transformers for Time Series Forecasting
arxiv_id: '2507.10098'
source_url: https://arxiv.org/abs/2507.10098
tags:
- time
- series
- forecasting
- data
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of time series forecasting (TSF)
  by integrating large language models (LLMs) with traditional temporal transformers.
  Existing LLM-based approaches struggle with continuous numerical data and fail to
  match the performance of vanilla transformers, which lack high-level semantic understanding.
---

# Fusing Large Language Models with Temporal Transformers for Time Series Forecasting

## Quick Facts
- arXiv ID: 2507.10098
- Source URL: https://arxiv.org/abs/2507.10098
- Reference count: 27
- Primary result: Proposed method achieves state-of-the-art results, reducing MSE by 23% on ILI dataset compared to best baseline

## Executive Summary
This paper addresses the challenge of time series forecasting (TSF) by integrating large language models (LLMs) with traditional temporal transformers. Existing LLM-based approaches struggle with continuous numerical data and fail to match the performance of vanilla transformers, which lack high-level semantic understanding. The proposed method, SemInf-TSF, complements LLMs and vanilla transformers by fusing their representations. It uses a transformer to encode temporal dynamics and an LLM to extract semantic patterns, then fuses these through a gate mechanism. Experiments on benchmark datasets demonstrate that SemInf-TSF outperforms existing TSF models, achieving state-of-the-art results.

## Method Summary
The SemInf-TSF framework processes time series data through a patching mechanism, segmenting sequences into local patches that are embedded and normalized. A 3-layer PatchTST-style transformer encoder processes these patches to capture local temporal dynamics. The output, along with the raw patches, is projected into the embedding space of a GPT-2 LLM (with LoRA for efficient fine-tuning). The LLM receives these projected features sandwiched between hand-crafted text prompts (P_task, P_feat, P_data) to activate semantic reasoning. The resulting LLM semantic representation is fused with the temporal representation from the transformer via a sigmoid-gated mechanism (g·Z_LLM + (1−g)·Z_trans) at the interface between the second and third transformer layers. The fused representation is then processed by the final transformer layer and a linear projection head to produce forecasts. Training uses MSE loss with RevIN normalization, running for 300 epochs at learning rate 1e-4.

## Key Results
- Achieves state-of-the-art performance on ETT and ILI datasets
- Reduces MSE by 23% compared to best baseline on ILI dataset
- Gated fusion outperforms direct summation of temporal and semantic representations
- LLMs function effectively as semantic extractors when guided by prompts, but not as direct decoders

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gated fusion of temporal and semantic representations outperforms simple summation by mitigating feature heterogeneity.
- **Mechanism:** A sigmoid-activated gate $g$ dynamically weighs the temporal representation $Z_{l-1}$ against the LLM's semantic representation $Z_{LLM}$. The fused representation $Z_E = g \cdot Z_{LLM} + (1 - g) \cdot Z_{l-1}$ allows the model to select the optimal balance per instance, preventing the "confusion" caused by directly adding disparate feature distributions.
- **Core assumption:** The feature spaces of a vanilla Transformer (trained on continuous dynamics) and an LLM (pre-trained on discrete text) are sufficiently distinct that direct addition introduces noise, yet compatible enough to be linearly interpolated.
- **Evidence anchors:**
  - [abstract]: "...fuses these through a gate mechanism... The resulting fused representation contains both historical temporal dynamics and semantic variation patterns..."
  - [section 4.1]: "...directly combining the two representations leads to confusion, preventing the predictor from effectively utilizing useful information..."
  - [corpus]: Paper 'ASTIF' supports the general need for adaptive integration, citing static architectures as insufficient for heterogeneous sources.
- **Break condition:** If time series data lacks semantic structure (e.g., pure random noise), the gate $g$ should theoretically approach zero, rendering the LLM component redundant.

### Mechanism 2
- **Claim:** LLMs function as effective semantic extractors for time series only when guided by textual prompts and restricted from direct decoding.
- **Mechanism:** The model does not ask the LLM to predict values directly. Instead, it embeds time series patches between text prompts (e.g., "The following are encoded features...") to activate the LLM's reasoning capabilities. The LLM processes this hybrid input, and the resulting embeddings (specifically from the time series positions) are extracted as "semantic features."
- **Core assumption:** The reasoning patterns learned from text corpora transfer to numerical sequences when those sequences are "framed" linguistically, but the LLM lacks the numerical precision for direct regression.
- **Evidence anchors:**
  - [abstract]: "...LLMs are proficient at reasoning over discrete tokens... but are not initially designed to model continuous numerical time series data."
  - [section 4.2]: Shows that using the LLM as a decoder (non-autoregressive) fails significantly compared to using it as a feature encoder, especially for long-term forecasting.
  - [corpus]: 'Time-Prompt' and 'Semantic-Enhanced Time-Series Forecasting' neighbors validate the strategy of using prompts to align modalities.
- **Break condition:** If the LLM is too large (e.g., Llama-3B vs GPT-2), strong linguistic priors may override the semantic extraction, degrading performance.

### Mechanism 3
- **Claim:** Attention sparsity in LLMs provides complementary information to the local continuity bias in standard Transformers.
- **Mechanism:** The authors visualize attention maps, noting that the Transformer focuses on continuous blocks (local dynamics), while the LLM exhibits sparse, global attention (semantic key-points). The fusion mechanism exploits this difference, using the LLM to "summarize" critical patches while the Transformer tracks the flow.
- **Core assumption:** Time series forecasting benefits from identifying "semantic key-points" (similar to keywords in text) alongside tracking local trends.
- **Evidence anchors:**
  - [section 4.3]: "Attention heatmaps in the LLM display a sparser attention distribution... similar to extracting keywords from a sentence..."
  - [section 4.3]: t-SNE plots show distinct clusters for Transformer tokens vs. LLM tokens, confirming they capture different information.
  - [corpus]: 'TimeFormer' neighbor discusses attention modulation, implicitly supporting the need to modify standard attention for temporal data, which SemInf-TSF addresses via dual-attention paths.
- **Break condition:** If the forecasting horizon is extremely short, the "semantic" summary may be unnecessary, and the overhead of the LLM could latency without accuracy gain.

## Foundational Learning

- **Concept: Patching (Segmentation)**
  - **Why needed here:** Both the Transformer and LLM process "patches" (groups of time steps) rather than single points. This reduces sequence length and creates local semantic units that the LLM can interpret more like "tokens."
  - **Quick check question:** How does changing the patch length $T$ vs. the stride $S$ affect the tradeoff between local resolution and computational efficiency?

- **Concept: Representation Heterogeneity**
  - **Why needed here:** The paper's core motivation is that $Z_{trans}$ and $Z_{LLM}$ are not immediately compatible. Understanding that one captures "shape" and the other "concept" is key to debugging the fusion layer.
  - **Quick check question:** If you visualized the gradients flowing into the fusion layer, how would you verify that the model isn't just ignoring the LLM branch?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** The LLM (GPT-2) is not frozen entirely nor fully retrained. LoRA allows the model to adapt its weights to the time series domain without destroying the pre-trained linguistic reasoning capabilities.
  - **Quick check question:** Why is full fine-tuning of the LLM potentially dangerous in this architecture (hint: catastrophic forgetting)?

## Architecture Onboarding

- **Component map:** Input -> Patch Embedding + Instance Norm -> Transformer Layers 1-2 -> (Parallel path: LLM Encoding) -> Gate Fusion -> Transformer Layer 3 -> Prediction Head

- **Critical path:** Input → Patch Embedding → Transformer Layers 1-2 → (Parallel path: LLM Encoding) → Gate Fusion → Transformer Layer 3 → Prediction Head

- **Design tradeoffs:**
  - **LLM Scale:** The paper explicitly advises *against* using very large LLMs (e.g., Llama 3.2, Qwen). They found larger models introduce excessive linguistic bias that degrades numerical adaptation. Stick to smaller, adaptable models like GPT-2 for the semantic encoder.
  - **Fusion Depth:** Fusion happens late (after layer 2). Early fusion might overwhelm the temporal features with semantic noise before local patterns are established.

- **Failure signatures:**
  - **Performance Collapse:** If MSE is higher than the "Transformer-only" baseline, check if the gate $g$ is saturated (stuck at 1.0), meaning the model is blindly trusting the LLM and ignoring temporal dynamics.
  - **Long-term Drift:** If short-term predictions are accurate but long-term drift significantly, the LLM decoder logic (if misconfigured) may be interfering with the regression head.

- **First 3 experiments:**
  1. **Baseline Validation:** Train `Trans-only` and `LLM-only` models independently on your target dataset to establish the lower bounds of performance.
  2. **Ablation on Fusion:** Implement a simple `Add` fusion vs. the `Gate` fusion. Verify that `Gate` yields lower MSE (as per paper Table 2) to ensure the heterogeneity problem exists in your data.
  3. **Prompt Sensitivity:** Remove the text prompts from the LLM input sequence (Section 2.2) and measure the drop in accuracy to confirm the necessity of semantic "priming."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the explicit incorporation of external textual features enhance the fusion framework compared to using LLM-derived semantic representations alone?
- **Basis in paper:** [explicit] The Conclusion states that future research directions include "further incorporating textual features for enhanced fusion and exploring multi-modal collaborative forecasting strategies."
- **Why unresolved:** The current study focuses on fusing internal LLM representations with temporal dynamics but does not integrate external text data (e.g., news, metadata) that might provide context for the time series.
- **What evidence would resolve it:** Experiments augmenting the input with relevant external text documents and measuring the delta in forecasting accuracy against the current semantic-only fusion baseline.

### Open Question 2
- **Question:** Can the strong linguistic priors of larger, modern LLMs (e.g., Llama-3, Qwen) be effectively adapted for time series forecasting, or is there a performance ceiling imposed by model scale?
- **Basis in paper:** [inferred] Section 4.4 demonstrates that increasing LLM size and capability (GPT-2 to Qwen) degrades performance due to "overly strong linguistic bias," leaving the adaptability of larger models an open challenge.
- **Why unresolved:** The paper identifies the failure mode of larger models but does not propose a solution to mitigate their strong text-domain priors to leverage their greater capacity.
- **What evidence would resolve it:** A study applying domain adaptation techniques (e.g., specialized pre-training) to larger LLMs within this architecture to see if the performance drop can be reversed.

### Open Question 3
- **Question:** How sensitive is the forecasting accuracy to the specific phrasing and structure of the hand-crafted auxiliary text prompts?
- **Basis in paper:** [inferred] Section 2.2 details specific static prompts (e.g., "The following are the encoded time series features..."), but provides no ablation study regarding their necessity or optimality.
- **Why unresolved:** It is unclear if the model relies on these specific semantic instructions or if the LLM extracts patterns effectively from the data embeddings alone.
- **What evidence would resolve it:** An ablation study comparing the current prompts against null prompts, random tokens, or varied phrasing to isolate the contribution of the prompt text.

## Limitations
- Model scale sensitivity: The paper explicitly warns against using very large LLMs, but the exact threshold for "too large" is not specified. The choice of GPT-2 (small) appears critical to success.
- Prompt sensitivity: While the paper describes the prompt structure (P_task, P_feat, P_data), the exact prompt text is not provided, leaving this as a potential performance lever.
- Dataset specificity: Most reported results are on ETT datasets. The 23% MSE improvement on ILI is notable but from a smaller dataset with different patching (length 24 vs 16), raising questions about generalizability.

## Confidence
- **High confidence:** The gated fusion mechanism outperforming direct addition (Table 2 ablation results are clear).
- **Medium confidence:** The claim that LLMs capture "semantic patterns" in time series - the evidence is primarily attention visualizations and performance gains rather than interpretable semantic extraction.
- **Medium confidence:** The assertion that very large LLMs degrade performance - this is stated but not extensively validated across model scales.

## Next Checks
1. **Gate behavior monitoring:** Log and visualize the gate activation g during training across multiple seeds. Verify that g varies meaningfully rather than saturating at 0 or 1, confirming the model is actually leveraging both branches.
2. **Prompt ablation study:** Systematically remove or modify the text prompts in the LLM input sequence. Measure the performance drop to quantify how critical the semantic "priming" is to the approach.
3. **Scale sensitivity test:** Train the same architecture with incrementally larger LLMs (e.g., GPT-2 small → medium → large). Verify whether performance degrades as predicted when moving beyond the "Goldilocks zone" of model scale.