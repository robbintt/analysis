---
ver: rpa2
title: 'Bias beyond Borders: Global Inequalities in AI-Generated Music'
arxiv_id: '2510.01963'
source_url: https://arxiv.org/abs/2510.01963
tags:
- music
- genres
- across
- tracks
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GlobalDISCO, a large-scale dataset of 73k
  AI-generated music tracks paired with 93k reference tracks from 79 countries, 147
  languages, and 991 genres. The dataset was constructed by gathering artist profiles
  from MusicBrainz and Wikipedia, matching them with reference tracks from LAION-DISCO-12M,
  and generating music using four state-of-the-art commercial models (Udio, Suno,
  Mureka, Riffusion).
---

# Bias beyond Borders: Global Inequalities in AI-Generated Music

## Quick Facts
- arXiv ID: 2510.01963
- Source URL: https://arxiv.org/abs/2510.01963
- Authors: Ahmet Solak; Florian Grötschla; Luca A. Lanzendörfer; Roger Wattenhofer
- Reference count: 0
- Primary result: Significant disparities in AI music generation quality between high-resource and low-resource regions, with models producing more out-of-distribution music for Africa, Southern and Western Asia compared to higher-resource regions.

## Executive Summary
This paper introduces GlobalDISCO, a large-scale dataset of 73k AI-generated music tracks paired with 93k reference tracks from 79 countries, 147 languages, and 991 genres. The dataset was constructed by gathering artist profiles from MusicBrainz and Wikipedia, matching them with reference tracks from LAION-DISCO-12M, and generating music using four state-of-the-art commercial models (Udio, Suno, Mureka, Riffusion). The evaluation reveals significant disparities in music generation quality between high-resource and low-resource regions, with models producing more out-of-distribution music for Africa, Southern and Western Asia compared to higher-resource regions like Northern America. Objective metrics (FAD, KAD) and human perception both confirm these biases, showing that models often generate regional genres (e.g., opera, ghazal, soukous) that align more closely with mainstream genres like pop and rock than with their intended reference styles. The findings highlight clear biases against lower-resource musical traditions and underscore the need to address these disparities to preserve global musical diversity.

## Method Summary
The study constructs GlobalDISCO by first collecting artist profiles from MusicBrainz and Wikipedia, then matching these to reference tracks from LAION-DISCO-12M. Style descriptions and lyrics are generated using Gemini LLM, which are then used to prompt four commercial music generation models (Udio, Suno, Mureka, Riffusion). The generated music is evaluated using Fréchet Audio Distance (FAD) and Kernel Audio Distance (KAD) metrics computed from embeddings produced by PANNs, CLAP, and MUQ-MULAN models. The analysis compares generated music across 79 countries grouped into UN M49 regions, examining both regional disparities and genre-specific biases.

## Key Results
- AI music generation models produce significantly higher FAD/KAD scores (worse quality) for low-resource regions like Sub-Saharan Africa and Southern Asia compared to high-resource regions like Northern America
- Generated regional genre music (e.g., ghazal, soukous) often aligns more closely with mainstream genres like pop and rock than with their intended reference styles
- Objective metrics (FAD/KAD) and human perception both confirm these biases, with model-generated music showing lower cultural fidelity for underrepresented regions
- Mureka model demonstrates the most pronounced genre collapse, generating music for regional genres that resembles pop/rock more than the intended regional styles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data imbalance drives quality disparities between high-resource and low-resource regions
- Mechanism: Models trained on predominantly Western/mainstream music develop richer representations for those styles, producing lower FAD/KAD scores for Northern America and Western Europe while generating more out-of-distribution outputs for Africa, Southern Asia, and Western Asia
- Core assumption: Commercial models' training corpora reflect global music availability patterns
- Evidence anchors:
  - [abstract] "significant disparities in music generation quality between high-resource and low-resource regions"
  - [section 3] "Northern America, which is likely the highest-resource region in terms of available music, shows better results across the board"
  - [corpus] Limited direct corpus support for training data composition; neighbor paper "Music for All" confirms representational bias in music generation datasets

### Mechanism 2
- Claim: Regional genre representations collapse toward mainstream genre distributions
- Mechanism: When prompted with regional genres, models lack sufficient examples to form distinct representations, defaulting to acoustically similar high-frequency genres (pop/rock) in embedding space
- Core assumption: Embedding distances (CLAP, PANNs, MUQ-MULAN) capture stylistic similarity as perceived by humans
- Evidence anchors:
  - [abstract] "models often generate regional genres (e.g., opera, ghazal, soukous) that align more closely with mainstream genres like pop and rock than with their intended reference styles"
  - [section 3, Fig. 7] "Mureka generates music for five of the six selected regional genres that more closely resembles pop and rock than the corresponding reference tracks"
  - [corpus] No corpus papers directly address genre collapse; weak external validation

### Mechanism 3
- Claim: Objective metrics (FAD/KAD) align with human perceptual judgments of cultural fidelity
- Mechanism: Distributional distance metrics computed on audio embeddings correlate with human perception of whether generated music matches intended regional styles
- Core assumption: The embedding models (CLAP, PANNs, MUQ-MULAN) encode perceptually relevant features across all musical traditions
- Evidence anchors:
  - [abstract] "Objective metrics (FAD, KAD) and human perception both confirm these biases"
  - [section 2.2] "PANNs and CLAP audio embedding models, which have shown good alignment with human preference in prior work"
  - [corpus] No corpus papers validate cross-cultural embedding alignment; assumption remains untested

## Foundational Learning

- Concept: **Fréchet Audio Distance (FAD)**
  - Why needed here: Primary metric for evaluating distributional similarity between generated and reference music; lower scores indicate better alignment
  - Quick check question: If FAD for Sub-Saharan Africa is 47.8 vs. 17.9 for Northern America, what does this tell you about model performance?

- Concept: **Out-of-Distribution (OOD) Generation**
  - Why needed here: The paper's central finding is that models generate OOD music for low-resource regions—outputs that deviate significantly from reference distributions
  - Quick check question: Why might OOD generation be problematic for preserving "global musical diversity"?

- Concept: **TF-IDF for Genre Selection**
  - Why needed here: Paper uses this method to identify regionally distinctive genres (high frequency in one country, low across others)
  - Quick check question: Why use TF-IDF rather than raw frequency to select regional genres?

## Architecture Onboarding

- Component map: MusicBrainz (artist metadata) → Wikipedia (biographies) → LAION-DISCO-12M (reference tracks) → Gemini LLM (style prompts/lyrics) → 4 commercial models (Udio, Suno, Mureka, Riffusion) → 3 embedding models (PANNs, CLAP, MUQ-MULAN) → FAD/KAD metrics → Regional/genre stratification

- Critical path: Artist profile construction → Prompt formatting → Multi-model generation → Embedding extraction → Distribution comparison (FAD/KAD) → Regional/genre analysis

- Design tradeoffs:
  - Breadth (79 countries, 147 languages) vs. depth (capped at 374 artists/country)
  - Black-box commercial models (realistic but uncontrolled) vs. research models (controllable but potentially lower quality)
  - Text-to-audio prompting approach vs. audio-to-audio conditioning

- Failure signatures:
  - High FAD/KAD for specific regions indicates insufficient training data coverage
  - Generated regional genre closer to pop/rock than reference indicates representation collapse
  - Inconsistent rankings across embedding models suggests metric instability

- First 3 experiments:
  1. **Baseline replication**: Re-compute FAD/KAD for your region of interest using provided embeddings; verify against paper's regional rankings
  2. **Genre probing**: Select one regional genre (e.g., ghazal) and compute embedding distances to all reference genres; identify nearest mainstream neighbor
  3. **Per-region threshold analysis**: For each world region, identify the FAD score at which human perception diverges from metric predictions (requires subset human evaluation)

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific interventions in training data curation or model architecture are effective at reducing the performance disparities between high- and low-resource musical regions?
  - Basis: The conclusion explicitly highlights the "need to address these disparities to preserve global musical diversity."
  - Why unresolved: The paper identifies and quantifies the bias but does not propose or test mitigation strategies.
  - What evidence would resolve it: A study comparing model performance after fine-tuning on balanced global datasets or employing culturally-aware architectural constraints.

- **Open Question 2**: Are the objective metrics (FAD, KAD) used in this study reliable for evaluating non-Western music given that their underlying embedding models (e.g., PANNs, CLAP) may be Western-centric?
  - Basis: The methodology relies on embedding models pre-trained on datasets (like AudioSet) which may not represent global diversity, potentially skewing distance calculations for "out-of-distribution" genres.
  - Why unresolved: The paper assumes metric validity based on prior work, but these metrics have not been validated against human perception for the specific regional genres analyzed.
  - What evidence would resolve it: A correlation analysis between objective metric scores and human perceptual studies specifically for low-resource regional genres.

- **Open Question 3**: To what extent are the observed biases caused by the intermediate prompt generation step (Gemini) rather than the music generation models themselves?
  - Basis: The pipeline synthesizes prompts using an LLM based on potentially sparse metadata; if the LLM fails to describe regional styles accurately, the music model's failure may be input-driven.
  - Why unresolved: The paper evaluates the end-to-end pipeline but does not isolate the error contribution of the textual prompt generation phase.
  - What evidence would resolve it: An ablation study bypassing the LLM prompt generator using ground-truth human-authored descriptions for regional genres.

## Limitations
- Black-box commercial models prevent verification of training data composition and architecture details
- Embedding models (PANNs, CLAP, MUQ-MULAN) may not capture culturally relevant features across all 147 languages and 991 genres
- Dataset construction relies on availability heuristics that may reflect existing biases in digital music documentation

## Confidence
- **High confidence**: Regional FAD/KAD disparities (Objective metrics showing clear differences between high-resource and low-resource regions)
- **Medium confidence**: Genre collapse hypothesis (Human evaluation supports but corpus validation is weak)
- **Low confidence**: Embedding model cross-cultural validity (No corpus validation of whether PANNs/CLAP/MUQ-MULAN capture culturally relevant features)

## Next Checks
1. **Embedding model cross-cultural validation**: Test whether PANNs, CLAP, and MUQ-MULAN embeddings maintain consistent perceptual alignment across musical traditions by conducting human evaluations comparing embedding distances to perceptual similarity ratings for regional genre pairs

2. **Commercial model training data audit**: If possible, obtain documentation or conduct probing experiments to verify whether observed regional disparities correlate with actual training data representation across different world regions

3. **Temporal stability assessment**: Re-evaluate a subset of generated tracks after 3-6 months using the same metrics to determine if observed biases persist across commercial model updates