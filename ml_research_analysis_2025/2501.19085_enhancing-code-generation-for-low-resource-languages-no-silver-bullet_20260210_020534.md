---
ver: rpa2
title: 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet'
arxiv_id: '2501.19085'
source_url: https://arxiv.org/abs/2501.19085
tags:
- code
- languages
- performance
- generation
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to improve code generation for low-resource
  programming languages (R and Racket) using Large Language Models (LLMs). The authors
  compare three in-context learning techniques (translation examples, translation
  rules, few-shot learning) and two fine-tuning-based approaches (code generation
  and code translation + generation).
---

# Enhancing Code Generation for Low-Resource Languages: No Silver Bullet

## Quick Facts
- arXiv ID: 2501.19085
- Source URL: https://arxiv.org/abs/2501.19085
- Reference count: 40
- Primary result: Translation examples as prompts consistently improve LLM code generation for low-resource languages across all model sizes, with smaller models (1B) benefiting most from fine-tuning while larger models (7B+) perform better with in-context learning.

## Executive Summary
This paper investigates how to improve code generation for low-resource programming languages (R and Racket) using Large Language Models (LLMs). The authors compare three in-context learning techniques (translation examples, translation rules, few-shot learning) and two fine-tuning-based approaches (code generation and code translation + generation). They evaluate six LLMs with different sizes (1B, 7B, 13B, 33B parameters) on a benchmark of 157 code generation tasks. Their findings show that smaller LLMs (1B) benefit most from fine-tuning, while larger models (7B+) perform better with in-context learning. Notably, translation examples as prompts consistently improve performance across all models. For the largest model (DeepSeek Coder 33B), in-context learning with translation examples improved pass@1 rates from 30.2% to 36.5% for R and from 32.5% to 36.3% for Racket. The study concludes that in-context learning with translation examples is a reliable, resource-efficient approach for enhancing LLM code generation on low-resource languages.

## Method Summary
The study evaluates six LLMs (DeepSeek Coder 1B/7B/33B, Code Llama 7B/13B, GitHub Copilot) on the MultiPL-E benchmark using 157 common code generation tasks across R and Racket. Three in-context learning techniques are tested: translation examples (2 Python→target pairs), translation rules (mapping rules), and few-shot learning (2 target examples). Two fine-tuning approaches are evaluated: direct training on ⟨description, code⟩ pairs (~37-40K examples) and translation pre-training followed by generation fine-tuning. Performance is measured using pass@1 rate with n=50 repetitions, temperature=0.2, and McNemar's test with Benjamini-Hochberg correction for statistical significance.

## Key Results
- Translation examples as prompts consistently improved performance across all model sizes
- DeepSeek Coder 33B: pass@1 improved from 30.2% to 36.5% for R and 32.5% to 36.3% for Racket with translation examples
- Smaller models (1B) benefited most from fine-tuning; larger models (7B+) showed better results with in-context learning
- Fine-tuning 33B+ models on low-resource data caused performance degradation (25.3% vs 30.2% baseline for R)
- No single approach works universally across all model sizes and languages

## Why This Works (Mechanism)

### Mechanism 1: Translation Examples Enable Cross-Language Knowledge Transfer
- Claim: Providing Python→target language translation examples in prompts improves low-resource code generation across all model sizes.
- Mechanism: Translation examples activate the model's existing high-resource language competence (Python) and provide a concrete mapping template. The model reuses syntactic patterns from examples rather than hallucinating incorrect constructs.
- Core assumption: The model has robust internal representations of Python that can be transferred via analogical reasoning.
- Evidence anchors:
  - [abstract] "Translation examples as prompts consistently improve performance across all models"
  - [Table III] DeepSeek Coder 33B improved from 30.2%→36.5% (R) and 32.5%→36.3% (Racket) with translation examples
  - [corpus] Related work on cross-lingual transfer for low-resource languages supports transfer-based approaches (FMR 0.62)
- Break condition: If the model lacks sufficient Python knowledge, or if the target language paradigm is fundamentally different (e.g., logic programming), transfer may fail.

### Mechanism 2: Model Size Determines Fine-Tuning Viability
- Claim: Smaller models (1B) benefit from fine-tuning; larger models (33B+) may degrade due to insufficient training data.
- Mechanism: Smaller parameter spaces can be meaningfully updated with limited data (~40K examples). Larger models require more data to adjust their weight distributions; otherwise, fine-tuning distorts pre-trained representations.
- Core assumption: The available fine-tuning data (~37-40K functions) is insufficient to update billions of parameters without overfitting or catastrophic forgetting.
- Evidence anchors:
  - [abstract] "Very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights"
  - [Table III] DeepSeek Coder 33B fine-tuned: 25.3% vs baseline 30.2% (R)—performance decreased
  - [corpus] Limited corpus evidence on fine-tuning data requirements; this remains an open research question
- Break condition: If substantially more high-quality training data becomes available for low-resource languages, fine-tuning may become viable for larger models.

### Mechanism 3: In-Context Learning Scales with Model Capacity
- Claim: Larger models (7B+) leverage complex prompts better than smaller models, making in-context learning increasingly effective at scale.
- Mechanism: Larger models have greater capacity to interpret and follow multi-part instructions. Smaller models struggle with complex prompt structures, reducing in-context learning benefits.
- Core assumption: In-context learning requires emergent instruction-following capabilities that scale with parameter count.
- Evidence anchors:
  - [Page 8] "For the smallest model... fine-tuning-based techniques are the best-performing... the model seems to not benefit from in-context learning, probably due to its limited ability to interpret complex prompts"
  - [Table III] DeepSeek Coder 1B shows minimal improvement from any in-context technique (13.9%→14.1% baseline)
  - [corpus] Corpus weak on direct mechanism; scaling laws for in-context learning remain underexplored
- Break condition: If prompt complexity is reduced or if smaller models are specifically trained on instruction-following, this size-dependency may diminish.

## Foundational Learning

- **In-Context Learning vs. Fine-Tuning**
  - Why needed here: The paper compares these two fundamentally different approaches. In-context learning modifies behavior via prompt design without weight updates; fine-tuning permanently alters model weights.
  - Quick check question: Can you explain why in-context learning is reversible per-session while fine-tuning persists across sessions?

- **Pass@k Metric**
  - Why needed here: All results are reported as pass@1, measuring whether any of k=1 generations passes all unit tests, averaged over n=50 repetitions.
  - Quick check question: Why does pass@1 with n=50 repetitions provide a more stable estimate than a single generation?

- **Low-Resource vs. High-Resource Languages**
  - Why needed here: The performance gap (e.g., Python 67.5% vs. R 30.9% for DeepSeek 33B) defines the problem scope. "Low-resource" refers to training data scarcity, not language capability.
  - Quick check question: Why might Julia (6k GitHub repos) outperform R (16k repos) despite fewer repositories?

## Architecture Onboarding

- **Component map:**
  Input: Natural language function description + signature
  ├── In-Context Learning Path (no weight updates)
  │   ├── Translation Examples: 2 Python→Target pairs prepended to prompt
  │   ├── Translation Rules: Mapping rules (e.g., Python "=" → R "<-")
  │   └── Few-Shot: 2 target-language examples with descriptions
  ├── Fine-Tuning Path (weight updates required)
  │   ├── Direct: Train on ⟨description, code⟩ pairs (~37-40K examples)
  │   └── Pre-train + Fine-tune: Translation pre-training → generation fine-tuning
  └── Output: Generated function body → Unit test evaluation

- **Critical path:**
  1. Start with translation examples (safest, always helps for 7B+ models)
  2. If using smaller models (1B), consider fine-tuning instead
  3. Avoid fine-tuning 33B+ models on low-resource data—insufficient data causes degradation

- **Design tradeoffs:**
  - **Cost:** In-context learning is ~free (prompt engineering only); fine-tuning requires GPU hours and curated datasets
  - **Risk:** Fine-tuning can decrease performance on large models; in-context learning never degraded performance in this study
  - **Control:** Fine-tuning creates a specialized model; in-context learning allows per-task flexibility

- **Failure signatures:**
  - **Syntactic hallucination:** Model uses constructs from wrong language (e.g., push instead of push! in Julia)
  - **Edge case mishandling:** Function returns wrong type on empty inputs (e.g., null instead of empty list in R)
  - **Fine-tuning degradation:** Post-fine-tuning pass@1 drops below baseline (observed for 33B model)

- **First 3 experiments:**
  1. **Baseline measurement:** Run your target model on MultiPL-E benchmark for your low-resource language; record pass@1 with temperature=0.2, n=50
  2. **Translation examples test:** Prepend 2 Python→target translation examples (from MultiPL-T dataset); measure delta vs. baseline
  3. **Scaling validation:** If using multiple model sizes, confirm that smaller models (<7B) don't benefit from in-context learning before investing in fine-tuning infrastructure

## Open Questions the Paper Calls Out
None

## Limitations
- The study only examines R and Racket, limiting generalizability to other programming paradigms or truly low-resource languages
- Fine-tuning experiments used only one hyperparameter configuration; observed degradation in 33B models might be tunable with different settings
- The optimal number and selection criteria for translation examples remain unexplored (fixed at 2 pairs in this study)

## Confidence
- **High confidence**: Translation examples consistently improve performance across model sizes for R and Racket
- **Medium confidence**: Model size determines fine-tuning viability, based on single hyperparameter configuration
- **Medium confidence**: Larger models benefit more from in-context learning, though differences could be influenced by architectural variations

## Next Checks
1. **Cross-paradigm generalization test**: Apply the translation examples technique to other programming paradigms (functional languages like Haskell, logic languages like Prolog) to verify the cross-language transfer mechanism holds beyond imperative/Racket.

2. **Hyperparameter sensitivity analysis**: Re-run the fine-tuning experiments for the 33B model with varied learning rates (1×10⁻⁵ to 5×10⁻⁵), batch sizes, and epoch counts to determine if the observed degradation is inherent or tunable.

3. **Translation example optimization**: Systematically vary the number of translation examples (1, 2, 4, 8 pairs) and evaluate whether there's an optimal quantity or if quality/selection criteria matter more than quantity for the transfer mechanism.