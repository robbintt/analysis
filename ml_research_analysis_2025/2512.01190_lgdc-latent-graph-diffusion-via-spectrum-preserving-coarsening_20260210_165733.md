---
ver: rpa2
title: 'LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening'
arxiv_id: '2512.01190'
source_url: https://arxiv.org/abs/2512.01190
tags:
- graph
- generation
- diffusion
- graphs
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LGDC, a hybrid graph generation framework
  that combines spectrum-preserving graph coarsening with latent diffusion and a single
  expansion-refinement step. The method addresses the trade-off between autoregressive
  models (strong in local structure) and one-shot diffusion models (strong in global
  structure) by operating on a compressed latent graph representation.
---

# LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening

## Quick Facts
- arXiv ID: 2512.01190
- Source URL: https://arxiv.org/abs/2512.01190
- Authors: Nagham Osman; Keyue Jiang; Davide Buffelli; Xiaowen Dong; Laura Toni
- Reference count: 21
- Key outcome: Achieves 86.0 VUN on Tree graphs, outperforming autoregressive models while maintaining computational efficiency O(n² + T·n²c) vs O(T·n²) for standard diffusion

## Executive Summary
LGDC introduces a hybrid graph generation framework that combines spectrum-preserving graph coarsening with latent diffusion and a single expansion-refinement step. The method addresses the trade-off between autoregressive models (strong in local structure) and one-shot diffusion models (strong in global structure) by operating on a compressed latent graph representation. LGDC uses spectrum-preserving coarsening to create a compact yet globally faithful surrogate, applies discrete diffusion in this latent space for efficient global modeling, and then expands back to full resolution with a single decoding pass.

## Method Summary
LGDC operates in three stages: (1) Coarsening: Apply REC algorithm to contract edges while preserving principal Laplacian spectrum, creating a coarse graph Gc with nc ≈ n/5 nodes; (2) Latent Diffusion: Train a discrete diffusion model pθ(Gc) in the compressed space using DiGress-style transitions factorizing node and edge predictions; (3) Expansion-Refinement: Use a single autoregressive-inspired step to expand Gc to full resolution, predicting node replication counts and edge masks to recover local connectivity. The framework achieves O(n² + T·n²c) complexity compared to O(T·n²) for standard diffusion.

## Key Results
- Achieves 86.0 VUN on Tree graphs, outperforming autoregressive models on locally-structured datasets
- Maintains competitive performance on Planar (82.5) and Community-20 datasets
- Preserves global metrics (spectral distance, edge connectivity) through latent diffusion and expansion
- Dramatically improves local metrics (motif, degree, clustering) after single expansion step
- Demonstrates computational efficiency with O(n² + T·n²c) complexity

## Why This Works (Mechanism)

### Mechanism 1: Spectrum-Preserving Coarsening as Global Structure Compression
The REC algorithm contracts edges iteratively while preserving principal Laplacian eigenvalues and eigenspaces through the restricted spectral similarity criterion (1-ε)Tr(X^T L X) ≤ Tr(Xc^T Lc Xc) ≤ (1+ε)Tr(X^T L X). Since spectral quantities encode connectivity, community structure, and planarity, the coarse graph serves as a globally faithful surrogate. Core assumption: Principal Laplacian spectrum captures sufficient global topology. Evidence: Global metrics remain stable through latent diffusion and expansion (e.g., Planar spectral distance: 0.0126→0.0090). Break condition: If coarsening ratio is too aggressive or spectral distribution is highly irregular.

### Mechanism 2: Discrete Diffusion in Latent Space for Efficient Global Modeling
Running discrete diffusion on coarsened graphs reduces quadratic complexity while preserving one-shot paradigm's global coherence advantages. The latent diffusion model pθ(Gc) operates on graphs of size nc ≪ n, with complexity O(T·nc²) instead of O(T·n²). This captures global patterns (spectral distributions, community structure) that autoregressive models often miss. Core assumption: Global structure can be learned efficiently in compressed discrete graph space. Evidence: Global metrics (Spectre, Edge Conn.) remain stable before and after expansion. Break condition: If latent diffusion fails to converge due to discrete state space complexity.

### Mechanism 3: Single Expansion-Refinement for Local Structure Recovery
A single autoregressive-inspired expansion step can restore fine-grained local connectivity if the coarse graph provides accurate global scaffolding. Given sampled Gc, the expansion operator Expand(Gc; v) replicates each coarse node i into vi fine nodes, creating candidate edges: intra-cluster edges within each expanded cluster, and inter-cluster edges inferred from coarse adjacency. The refinement step applies a learned binary mask e to prune candidates, yielding Ĝ. Core assumption: Local connectivity patterns can be recovered from coarse structural scaffolds without iterative refinement. Evidence: Local metrics improve dramatically after expansion (e.g., Planar Motif: 0.0697→0.0079). Break condition: If candidate edge set Ę is too dense, binary mask prediction becomes error-prone.

## Foundational Learning

- **Concept: Graph Laplacian and Spectral Graph Theory**
  - Why needed here: Understanding why preserving Laplacian eigenvalues maintains global topology (connectivity, community structure, planarity)
  - Quick check question: Can you explain why the principal eigenvalues of the graph Laplacian encode community structure and connectivity?

- **Concept: Discrete Diffusion Models**
  - Why needed here: The latent space uses discrete state diffusion (DiGress-style), requiring understanding of transition matrices Q_X and Q_E for nodes and edges
  - Quick check question: How does discrete diffusion differ from continuous diffusion in terms of the forward/noising process and reverse denoising parameterization?

- **Concept: Autoregressive vs One-Shot Generation Trade-offs**
  - Why needed here: LGDC is motivated by the observed trade-off: autoregressive models excel at local structure while one-shot models excel at global structure
  - Quick check question: Why does sequential local expansion naturally capture fine-grained dependencies, while one-shot denoising better captures global patterns?

## Architecture Onboarding

- **Component map**: Coarsening module (REC algorithm) → Latent diffusion model (discrete DiGress-style) → Expansion model (node replication + edge mask prediction)

- **Critical path**: Training: G → coarsen to Gc → train latent diffusion on Gc distribution → train expansion model to reconstruct G from Gc. Sampling: Sample noise → T-step reverse diffusion → sampled Ĝc → single expand/refine → final Ĝ.

- **Design tradeoffs**: 
  - Coarsening ratio: Uses nc ≈ n/5 under fixed compute budget; smaller nc improves efficiency but risks losing global information
  - Single vs. multi-stage expansion: Uses single-shot nc→n vs HSpectre's iterative expansion, reducing complexity but potentially propagating edge mask errors
  - Discrete vs. continuous latent: Uses graph-structured latent preserving interpretability but limiting compression flexibility

- **Failure signatures**:
  - Low VUN on locally-structured datasets: Indicates expansion model underfitting, local metrics will show high discrepancy
  - High spectral distance after expansion: Indicates coarsening lost global information or latent diffusion failed to capture target distribution
  - Dense candidate edge sets causing mask prediction failures: Paper notes this can "disturb coarse-level consistency"

- **First 3 experiments**:
  1. Ablation on coarsening ratio: Sweep nc/n from 0.1 to 0.5 on Planar graphs, monitoring global metric preservation vs local metric recovery vs wall-clock time
  2. Comparison of REC vs random coarsening: Replace spectrum-preserving REC with random edge contraction, expecting global metrics to degrade
  3. Expansion model capacity test: Vary expansion model hidden dimension and measure local metric errors on Tree graphs, identifying plateau point

## Open Questions the Paper Calls Out

- What is the optimal coarsening ratio (n_c/n) for balancing generation fidelity and computational efficiency across different graph families? [explicit: Authors state "A broader sweep of coarsening ratios is left to future work" after fixing n_c ≈ n/5 "to balance fidelity and cost under a fixed compute budget."]

- Can adaptive or learned spectrum-preserving coarsening improve reconstruction quality over the fixed REC algorithm? [explicit: Conclusion states: "Future work will explore... adaptive or learned spectrum-preserving coarsening for greater robustness."]

- Does LGDC maintain its competitive performance on large-scale real-world graphs (e.g., molecular datasets, social networks)? [explicit: Authors acknowledge "our experiments focus on synthetic benchmarks rather than large, real-world graphs."]

## Limitations

- Edge mask prediction errors on dense graphs may propagate when candidate edge sets are large, particularly with high coarsening ratios
- Fixed coarsening ratio (nc ≈ n/5) chosen based on fixed compute budget without exploring sensitivity to compression level
- All evaluations use synthetic benchmarks (Tree, Planar, Community-20) rather than real-world graphs with heterogeneous attributes

## Confidence

- **High confidence**: Spectrum-preserving coarsening mechanism is well-grounded in spectral graph theory with REC algorithm following established literature; global metric preservation provides strong empirical support
- **Medium confidence**: Single expansion-refinement step shows impressive local metric recovery but edge mask error propagation on dense graphs suggests scalability bottleneck
- **Medium confidence**: Computational complexity claims are mathematically sound but real-world performance depends on implementation details

## Next Checks

1. **Ablation study on coarsening ratio**: Systematically vary nc/n from 0.1 to 0.5 on Planar graphs and measure the trade-off between global metric preservation (spectral distance) and local metric recovery (motif error) against wall-clock time

2. **REC vs. random coarsening comparison**: Replace the spectrum-preserving REC algorithm with random edge contraction and measure degradation in global metrics to validate spectral preservation is critical

3. **Expansion model capacity analysis**: Vary the hidden dimension of the expansion model and measure local metric errors (degree, orbit, motif) on Tree graphs to identify point of diminishing returns