---
ver: rpa2
title: 'Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification'
arxiv_id: '2510.04926'
source_url: https://arxiv.org/abs/2510.04926
tags:
- set-valued
- fairness
- size
- classifier
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of building fair set-valued
  classifiers under demographic parity constraints. The authors propose two complementary
  strategies: an oracle-based method minimizing classification risk while satisfying
  fairness and size constraints, and a computationally efficient proxy that prioritizes
  constraint satisfaction.'
---

# Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification

## Quick Facts
- arXiv ID: 2510.04926
- Source URL: https://arxiv.org/abs/2510.04926
- Authors: Eyal Cohen; Christophe Denis; Mohamed Hebiri
- Reference count: 40
- Primary result: First framework for fair set-valued classification under demographic parity with distribution-free constraint satisfaction guarantees

## Executive Summary
This paper introduces the first framework for fair set-valued classification under demographic parity constraints. The authors propose two complementary approaches: an oracle-based method that minimizes classification risk while satisfying fairness and size constraints, and a computationally efficient proxy that prioritizes constraint satisfaction. Both methods derive closed-form expressions for optimal fair set-valued classifiers and build plug-in, data-driven procedures using labeled and unlabeled data. The key innovation is extending demographic parity to the set-valued classification setting and providing practical algorithms that achieve fairness without requiring labeled sensitive attributes during the fairness enforcement step.

## Method Summary
The paper addresses fair set-valued classification by enforcing demographic parity constraints on the probability of including each class in the predicted set across sensitive groups. The authors propose two methods: (1) an oracle-based approach that solves a constrained optimization to find optimal Lagrange multipliers for risk minimization, and (2) a two-step proxy method that re-calibrates group-specific quantiles to achieve fairness. Both methods use labeled data to estimate conditional class probabilities and unlabeled data to enforce fairness constraints, with the key advantage that sensitive attributes are not needed during the fairness enforcement step. The framework provides distribution-free convergence rates for constraint violations and excess-risk bounds for the oracle method.

## Key Results
- Closed-form expressions for optimal fair set-valued classifiers under demographic parity
- Distribution-free convergence rates for constraint satisfaction (O(1/√N))
- Excess-risk bounds for oracle method under mild assumptions
- Empirical validation on synthetic and real data showing effectiveness of both strategies
- Computational advantage of proxy method demonstrated for large number of classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal fair set-valued classifier can be expressed as a threshold rule on conditional class probabilities with group-specific adjustments.
- Mechanism: The classifier includes class k if p_k(x,s) ≥ λ* + γ*_k,s/π_s, where λ* controls expected output size and γ*_k,s adjusts per class-group to satisfy demographic parity (DP). These Lagrange multipliers emerge from solving a constrained convex optimization that balances risk, size, and fairness.
- Core assumption: Assumption 1 (continuity of cumulative distribution functions F_{k,s}); uniqueness of (λ*,γ*) requires Assumption 2 (positive density).
- Evidence anchors:
  - [abstract] "derive closed-form expressions for the (optimal) fair set-valued classifiers"
  - [section 2.2, Theorem 2.2] Explicit threshold formula Γ*_β(x,s) = {k : p_k(x,s) ≥ λ* + γ*_k,s/π_s}
  - [corpus] Related work on Bayes-optimal fair classification (arXiv:2505.00631) similarly characterizes optimal fair classifiers via threshold rules.
- Break condition: If cdfs have discontinuities (mass points), threshold ties may cause non-uniqueness; solution requires tie-breaking rules or smoothing.

### Mechanism 2
- Claim: Fairness constraints can be enforced using only unlabeled data, decoupling probability estimation from constraint satisfaction.
- Mechanism: Labeled data D_n estimates conditional probabilities p̂_k; unlabeled data D_N estimates π_s and solves empirical Lagrangian (Eq. 4). Since constraint violation depends on empirical CDF estimation, convergence rates are distribution-free (O(1/√N)).
- Core assumption: Independence between D_n and D_N; bounded, continuous estimated probabilities.
- Evidence anchors:
  - [abstract] "build plug-in, data-driven procedures using labeled and unlabeled data"
  - [section 3.2, Theorem 3.1] "distribution-free guarantees: it holds uniformly over all distributions P"
  - [corpus] Fair regression under DP (arXiv:2601.10623) similarly separates estimation from constraint enforcement via post-processing.
- Break condition: If N_s (samples per sensitive group) is highly imbalanced, min_s N_s → 0 degrades guarantees; require group-stratified sampling.

### Mechanism 3
- Claim: A computationally efficient two-step proxy achieves fairness by re-calibrating group-specific quantiles rather than solving constrained optimization.
- Mechanism: First compute unfair size-constrained classifier Ĝ^{-1}(β), then for each class-group compute β_k = F̄_k(Ĝ^{-1}(β)) and set thresholds at F̄^{-1}_{k,s}(β_k). This equalizes inclusion probabilities across groups by construction.
- Core assumption: Assumption 1 (continuity) ensures quantile inversion is well-defined.
- Evidence anchors:
  - [abstract] "computationally efficient proxy that prioritizes constraint satisfaction"
  - [section 4.1] "eΓ_{β→DP}(x,s) = {k : p_k(x,s) ≥ F̄^{-1}_{k,s}(F̄_k(G^{-1}(β)))}"
  - [corpus] Weak direct evidence; proxy-based fairness methods less common in corpus literature.
- Break condition: Non-commutativity of quantile composition means β*_k ≠ F̄_k(G^{-1}(β)) in general, so proxy is suboptimal in risk (Remark 4.1).

## Foundational Learning

- Concept: **Demographic Parity (DP) in classification**
  - Why needed here: DP requires P(k ∈ Γ(X,S) | S=s) to be independent of s; this is the fairness constraint being enforced.
  - Quick check question: Given classifier outputs, can you compute P(k ∈ Γ | S=0) vs P(k ∈ Γ | S=1) and verify equality?

- Concept: **Lagrangian duality for constrained optimization**
  - Why needed here: The optimal classifier emerges from minimizing a Lagrangian combining risk, size penalty (λ*), and fairness penalty (γ*).
  - Quick check question: Can you explain why the optimal solution satisfies ∂H/∂λ = 0 and ∂H/∂γ_{k,s} = 0?

- Concept: **Empirical CDF convergence (Dvoretzky-Kiefer-Wolfowitz inequality)**
  - Why needed here: Distribution-free guarantees rely on uniform convergence of empirical CDFs to population CDFs at O(1/√N) rate.
  - Quick check question: What is the bound on P(sup_x |F̂_N(x) - F(x)| ≥ ε)?

## Architecture Onboarding

- Component map:
  - Probability estimator -> Noise perturbation -> Group statistics -> Optimizer path / Two-step path -> Classifier

- Critical path:
  1. Train probability estimators on labeled data.
  2. Compute empirical CDFs from unlabeled data (requires S values).
  3. Choose method: optimizer (risk-optimal) or two-step (faster).
  4. At inference, threshold p̂_k(x,s) to output subset.

- Design tradeoffs:
  - **Optimizer vs. Two-step**: Optimizer minimizes risk but requires O(MT·K|S|N) time; two-step is O(K|S|N) but suboptimal in risk.
  - **Labeled vs. unlabeled ratio**: More unlabeled data improves constraint satisfaction; labeled data only affects probability estimation quality.
  - **Perturbation level η**: Too small → ties persist; too large → distorts probability estimates.

- Failure signatures:
  - **High unfairness despite optimization**: Likely N_s << N for some group; check group balance in D_N.
  - **Numerical instability near β≈K**: Optimizer may fail when risk→0; switch to two-step method.
  - **Size constraint violation > O(1/√N)**: Check if probability estimators have poor uniform convergence (||p̂-p||_∞ large).

- First 3 experiments:
  1. **Validate constraint satisfaction on synthetic data with known p_k**: Generate data from Gaussian mixture (as in Section 5.2), verify E[U(Γ̂_β)] ≤ C·K/√N and E[|T(Γ̂_β)-β|] ≤ C·K/√N across multiple N.
  2. **Compare optimizer vs. two-step runtime scaling**: Measure wall-clock time for K∈{4,20,50} and N∈{1000,10000}, confirm two-step advantage grows with K.
  3. **Test robustness to probability estimation error**: Vary n (labeled sample size) and measure excess risk bound sensitivity to ||p̂-p||_∞; verify degradation follows Theorem 3.2.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can fairness-aware set-valued prediction be extended to other fairness criteria beyond demographic parity, such as equalized odds or individual fairness?
  - Basis in paper: [explicit] "Extending the fairness constraint to more general criteria (such as equalized odds or individual fairness) in the set-valued prediction setting is an open challenge."
  - Why unresolved: The current framework and theoretical guarantees are specifically designed for demographic parity. Different fairness criteria impose different constraints that may not admit closed-form solutions or may conflict with the size constraint in new ways.
  - What evidence would resolve it: Derivation of optimal set-valued classifiers under equalized odds constraints, or impossibility results showing fundamental trade-offs between set-valued prediction and individual fairness notions.

- **Open Question 2**: How can fair set-valued classification be extended to structured output problems, such as hierarchical classification?
  - Basis in paper: [explicit] "Future work could investigate how to extend fairness-aware prediction to structured output problems, such as hierarchical classification."
  - Why unresolved: Hierarchical classification introduces dependencies between class labels that the current approach does not model, and the definition of DP-fairness for hierarchically structured outputs is non-trivial.
  - What evidence would resolve it: A framework that jointly handles hierarchical label structures and fairness constraints, with theoretical guarantees on constraint satisfaction and empirical validation on hierarchical datasets.

- **Open Question 3**: Is there a method that achieves near-optimal risk while matching the computational efficiency of the two-step proxy approach?
  - Basis in paper: [inferred] The paper explicitly shows a trade-off between the oracle-based method (optimal risk but computationally expensive) and the two-step proxy (computationally efficient but suboptimal risk in general, as noted in Remark 4.1: "the thresholds used... differ from those of the oracle predictor").
  - Why unresolved: The composition of quantiles in the two-step procedure does not commute, creating a systematic gap from optimality.
  - What evidence would resolve it: An algorithm with provable near-optimal excess risk bounds and O(K|S|N) time complexity, or a lower bound showing this efficiency-optimality gap is unavoidable.

## Limitations

- The framework relies on continuous cumulative distribution functions and bounded estimated probabilities, which may not hold in practice with discrete distributions or heavy-tailed estimates.
- The two-step proxy method is suboptimal in terms of risk minimization due to non-commutativity of quantile composition.
- The empirical evaluation is limited in scope, lacking extensive real-world validation across diverse fairness-sensitive domains.

## Confidence

- **High Confidence**: The closed-form characterization of optimal fair set-valued classifiers (Mechanism 1) is well-supported by the Lagrangian optimization framework and related fair classification literature. The distribution-free convergence rates for constraint satisfaction are mathematically rigorous given the DKW inequality foundation.

- **Medium Confidence**: The practical effectiveness of the proxy method (Mechanism 3) is demonstrated empirically but lacks extensive theoretical justification. The claim about computational advantages for large K is supported by timing experiments but could benefit from broader complexity analysis.

- **Low Confidence**: The robustness claims regarding probability estimation errors are based on limited synthetic experiments. The sensitivity to unlabeled data imbalance and the practical impact of noise perturbation levels require further validation.

## Next Checks

1. **Distribution Robustness Test**: Evaluate both methods on synthetic datasets with discrete class probabilities and mass points at thresholds to verify theoretical assumptions and identify breakdown conditions.

2. **Real-World Fairness Assessment**: Apply the framework to multiple fairness-sensitive domains (e.g., hiring, lending) with varying class balances and sensitive attribute distributions to assess generalizability beyond the Drug dataset.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the perturbation level η and unlabeled-to-labeled data ratio to characterize the tradeoff between constraint satisfaction quality and computational efficiency across different dataset characteristics.