---
ver: rpa2
title: 'Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic
  Representations Throughout LLM Pretraining'
arxiv_id: '2509.05291'
source_url: https://arxiv.org/abs/2509.05291
tags:
- detects
- features
- plural
- feature
- nouns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an architecture-agnostic method to trace how
  linguistic representations evolve during LLM pretraining. Using sparse crosscoders
  to learn joint feature spaces across training checkpoints, the authors identify
  when and how specific features emerge, persist, or vanish.
---

# Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining

## Quick Facts
- **arXiv ID**: 2509.05291
- **Source URL**: https://arxiv.org/abs/2509.05291
- **Reference count**: 40
- **Primary result**: Introduces architecture-agnostic method using sparse crosscoders to trace linguistic representation evolution during LLM pretraining, quantifying per-feature causal importance with RELIE metric.

## Executive Summary
This work introduces an architecture-agnostic method to trace how linguistic representations evolve during LLM pretraining. Using sparse crosscoders to learn joint feature spaces across training checkpoints, the authors identify when and how specific features emerge, persist, or vanish. They introduce Relative Indirect Effect (RELIE), a novel metric that quantifies per-feature causal importance across checkpoints by measuring changes in model behavior when features are ablated. Experiments on Pythia, OLMo, and BLOOM show that early features are often token-specific, while later features abstract into higher-level syntactic patterns; multilingual models consolidate language-specific detectors into shared crosslingual features. This approach enables fine-grained, interpretable analysis of representation learning throughout pretraining.

## Method Summary
The method uses sparse crosscoders trained on activations from different pretraining checkpoints to create a shared feature space. For each checkpoint pair/triplet, crosscoders with 16K dictionary size and L1 regularization learn to reconstruct middle-layer (Layer 8) activations from 400M subsampled pretraining tokens. Phase transitions are identified by task accuracy changes and activation correlation patterns. RELIE quantifies per-feature causal importance by measuring behavior changes when features are ablated using integrated gradients. Manual annotation of top-10 IE features per checkpoint reveals evolution patterns from token-specific to abstract grammatical representations. The approach is applied across Pythia, OLMo, and BLOOM models, with multilingual consolidation analysis in BLOOM.

## Key Results
- Early pretraining features are predominantly token-specific, while later features abstract to higher-level syntactic patterns like subject-verb agreement
- Multilingual models consolidate language-specific detectors into shared crosslingual features over time
- RELIE successfully identifies causal importance of features across checkpoints, revealing emergence and consolidation patterns

## Why This Works (Mechanism)
The crosscoder framework works by learning a shared sparse representation space across checkpoints, where each feature can be traced back to its emergence point. By reconstructing activations from one checkpoint using features learned from another, the method captures invariant aspects of representation evolution. The RELIE metric extends this by quantifying how much each feature contributes to task performance at each checkpoint, revealing causal relationships between feature emergence and capability development.

## Foundational Learning
- **Crosscoders**: Neural networks that learn joint feature spaces between different representations, needed to compare features across checkpoints; quick check: monitor reconstruction quality (ΔCE)
- **Sparse coding**: Technique forcing representations to use few active features, needed to create interpretable feature dictionaries; quick check: track dead features during training
- **Integrated gradients**: Attribution method for quantifying feature importance, needed to compute RELIE; quick check: verify gradient accumulation across N=10 steps
- **Pretraining checkpoints**: Saved model states during training, needed as temporal anchors for tracking evolution; quick check: ensure sufficient training (skip <128M tokens)
- **Activation correlation**: Measure of representation similarity, needed to identify phase transitions; quick check: plot correlation curves for qualitative inspection

## Architecture Onboarding

**Component map**: Pretraining checkpoints -> Crosscoder training -> RELIE computation -> Manual annotation -> Evolution analysis

**Critical path**: Crosscoder training (12 hours/A100) → RELIE computation (integrated gradients) → Manual feature annotation (qualitative interpretation)

**Design tradeoffs**: Architecture-agnostic approach trades model-specific insights for generalizability; sparse coding enables interpretability but loses information (ΔCE < 0.5); manual annotation provides qualitative insights but limits coverage (top-10 of 16K features).

**Failure signatures**: High dead features (>400) indicates poor crosscoder training; uninterpretable RELIE features suggests incorrect task setup or baseline choice; qualitative phase transition identification introduces subjectivity.

**First experiments**:
1. Train 3-way crosscoder on Pythia checkpoints 1B/4B/286B with provided hyperparameters, monitor ΔCE and dead features
2. Compute RELIE for BLiMP SVA task, select top-10 IE features per checkpoint, manually annotate using activation visualization
3. Compare feature evolution patterns between Pythia and OLMo to validate generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Method requires substantial compute (12+ hours per crosscoder) and pretraining-scale data access, limiting applicability
- Qualitative phase transition identification introduces subjective variability without quantitative criteria
- Manual annotation of only top-10 features provides limited coverage of 16K-dimensional feature space
- Crosscoder reconstruction quality (ΔCE < 0.5) suggests significant information loss

## Confidence

**High Confidence**: Claims about early features being token-specific and later features abstracting to grammatical patterns are well-supported by multiple models and manual annotations across BLiMP tasks. The RELIE metric's design and its ability to attribute causal importance to specific checkpoints is technically sound and validated through consistent patterns.

**Medium Confidence**: Multilingual consolidation claims rely on a single multilingual model (BLOOM-1B) with limited language coverage in the studied checkpoints. The generalizability of the phase transition framework across different model scales and architectures needs broader validation.

**Low Confidence**: Claims about specific emergence times for individual features (e.g., "SVA emerges at 4B tokens") are difficult to verify given the qualitative checkpoint selection and the inherent noise in both crosscoder training and RELIE estimation.

## Next Checks

1. **Crosscoder Stability Test**: Train multiple crosscoders with different seeds for the same checkpoint pairs and measure feature stability. High variance would indicate the method captures training artifacts rather than robust feature evolution.

2. **Layer-wise Evolution Analysis**: Apply the same methodology to early (Layer 1-2) and late (Layer 23-24) layers to test whether the token→abstract pattern holds throughout the network or if different layers follow distinct learning trajectories.

3. **Zero-Shot Feature Transfer**: Test whether features identified as important at checkpoint X retain causal importance when evaluated on data from checkpoint Y (without retraining). This would validate whether RELIE captures generalizable feature properties versus checkpoint-specific artifacts.