---
ver: rpa2
title: 'GTA: Supervised-Guided Reinforcement Learning for Text Classification with
  Large Language Models'
arxiv_id: '2509.12108'
source_url: https://arxiv.org/abs/2509.12108
tags:
- uni00000013
- uni00000011
- guess
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Guess-Think-Answer (GTA) framework to
  address the efficiency-capability trade-off between supervised fine-tuning (SFT)
  and reinforcement learning (RL) for text classification with large language models.
  GTA structures the reasoning process into three stages: an initial intuitive guess
  optimized via cross-entropy loss, a reflective thinking step, and a final answer
  optimized through RL rewards.'
---

# GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models

## Quick Facts
- arXiv ID: 2509.12108
- Source URL: https://arxiv.org/abs/2509.12108
- Reference count: 11
- Achieves significant convergence acceleration and accuracy gains over SFT and GRPO baselines on four text classification datasets

## Executive Summary
This paper introduces the Guess-Think-Answer (GTA) framework to address the efficiency-capability trade-off between supervised fine-tuning (SFT) and reinforcement learning (RL) for text classification with large language models. GTA structures the reasoning process into three stages: an initial intuitive guess optimized via cross-entropy loss, a reflective thinking step, and a final answer optimized through RL rewards. The framework integrates SFT and RL in a unified training paradigm, using loss masking and gradient constraints to mitigate conflicts between the two training signals. Experiments on four text classification benchmarks demonstrate that GTA significantly accelerates convergence while outperforming both standalone SFT and GRPO baselines in accuracy and F1 scores.

## Method Summary
GTA works by having the model first produce a provisional guess (optimized via cross-entropy loss), then reflect on this guess before generating the final answer, with RL rewards shaping both the final output and the format of the entire GTA structure. The framework partitions the output into three segments—Guess, Think, Answer—applying different training signals to each. The Guess segment receives supervised cross-entropy loss, providing a stable gradient anchor that reduces RL exploration variance. The Think and Answer segments are optimized via RL rewards (R_format + R_accuracy), allowing discovery of reasoning paths beyond SFT. Loss masking isolates these signals; gradient cosine adjustment resolves conflicts when they arise.

## Key Results
- GTA achieves F1 scores of 92.47% (Qwen2.5), 92.94% (Qwen3), and 93.36% (Llama3.2) on the Emotion dataset, surpassing other methods by a significant margin
- Demonstrates substantial convergence advantage with both guess and answer rewards consistently outperforming GRPO
- Shows robustness in reasoning, as the model can correct incorrect guesses through subsequent reflection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured output decomposition with supervised anchoring accelerates RL convergence for classification tasks.
- Mechanism: GTA partitions generation into three segments—Guess, Think, Answer—applying different training signals to each. The Guess segment receives supervised cross-entropy loss, providing a stable gradient anchor that reduces RL exploration variance. The Think and Answer segments are optimized via RL rewards (R_format + R_accuracy), allowing discovery of reasoning paths beyond SFT. Loss masking isolates these signals; gradient cosine adjustment resolves conflicts when they arise.
- Core assumption: The Guess segment's supervised gradient provides a useful initialization bias for downstream RL-optimized reasoning and answer segments.
- Evidence anchors: [abstract] "GTA works by having the model first produce a provisional guess (optimized via cross-entropy loss), then reflect on this guess before generating the final answer, with RL rewards shaping both the final output and the format of the entire GTA structure." [section 5.3] "GTA demonstrates a substantial convergence advantage, with both guess and answer rewards consistently outperforming GRPO." [corpus] GenCLS++ confirms SFT+RL combinations improve classification, but does not specifically address convergence acceleration via structured output decomposition.
- Break condition: If Guess and RL gradients consistently conflict (cosine similarity near -1), the acceleration effect diminishes and training may destabilize.

### Mechanism 2
- Claim: Implicit self-correction emerges when reasoning is decoupled from initial prediction.
- Mechanism: By generating an intuitive Guess first, then explicitly reasoning about it in the Think step, the model can compare its initial intuition against evidence. Cases in the paper show incorrect guesses corrected during reasoning. This decoupling creates a computational scaffold for self-evaluation without explicit self-critique modules.
- Core assumption: The Think segment receives sufficient gradient signal (via RL rewards on the final Answer) to learn corrective reasoning, even though it is not directly supervised.
- Evidence anchors: [section 5.5] "When the model produces an incorrect guess, subsequent reasoning steps tend to allow it to correct previous mistakes." [figure 7] Case examples show guess=general → answer=play and guess=sport → answer=calendar after reasoning. [corpus] Nemotron-Research-Tool-N1 suggests self-correction can emerge from RL, but does not isolate the structural decoupling mechanism.
- Break condition: If RL rewards are sparse or the task requires reasoning beyond model capacity, the Think segment may fail to learn meaningful corrections, reducing the framework to a slower SFT variant.

### Mechanism 3
- Claim: Multi-objective gradient conflict can be detected and selectively filtered using cosine similarity.
- Mechanism: During backpropagation, GTA computes gradients for both L_SFT (Guess) and L_RL (Think+Answer). When gradient vectors have negative cosine similarity (indicating conflict), the SFT gradient is discarded and only the RL gradient is applied. This prevents opposing updates.
- Core assumption: Gradient conflicts, rather than scalar loss magnitude differences, are the primary cause of training instability in hybrid SFT+RL setups.
- Evidence anchors: [section 3.3] "A positive cosine similarity between gradients from the two losses indicates no conflict during backpropagation, whereas a negative cosine similarity denotes the occurrence of a gradient conflict." [section 3.3] "During parameter updates, when gradient conflicts arise, we mitigate such conflicts by retaining only the loss component associated with the RL objective." [corpus] Entropy-Gated Selective Policy Optimization proposes token-level gradient allocation for hybrid training, but uses entropy-based gating rather than gradient conflict detection.
- Break condition: If conflicts are frequent and the SFT gradient is often discarded, the Guess segment may lose supervision quality, reducing the convergence advantage.

## Foundational Learning

- Concept: Policy gradient methods and GRPO
  - Why needed here: GTA's RL component builds on GRPO, which estimates advantages from group samples. Understanding how policy gradients work—log probabilities scaled by advantages—is essential for debugging reward signal flow.
  - Quick check question: Can you explain why GRPO uses group-relative advantages instead of a learned value function?

- Concept: Loss masking in transformer training
  - Why needed here: GTA applies different loss functions to different output segments. Loss masking ensures gradients only flow through intended tokens, preventing cross-segment interference.
  - Quick check question: If you set loss mask to -100 for all tokens in the Think segment, what happens to gradients during the SFT loss computation?

- Concept: Gradient conflict in multi-task learning
  - Why needed here: The paper adapts PCGrad-style conflict resolution. Understanding why opposing gradients harm joint optimization helps interpret when the method's safeguards will activate.
  - Quick check question: Two task gradients have cosine similarity of -0.3. What does PCGrad propose doing, and how does GTA's approach differ?

## Architecture Onboarding

- Component map: Prompt template -> Tokenizer/segmenter -> SFT loss module -> RL module (GRPO) -> Gradient conflict resolver -> Parameter update
- Critical path: 1. Forward pass generates full GTA output 2. Segment identification applies masks 3. L_SFT computed on Guess; L_RL computed on Think+Answer 4. Both losses backpropagated; gradient conflict checked 5. Final gradient update applied (combined or RL-only)
- Design tradeoffs:
  - Equal loss weighting (λ1=λ2=1): Simple, but may underweight RL signal if SFT dominates early; paper does not report ablation on weights
  - Periodic reference model update: Allows policy to move further from base model than fixed reference, but may increase KL drift risk
  - Format reward inclusion: Encourages structural compliance but adds sparse reward signal; if format errors are rare, this reward may provide little gradient
- Failure signatures:
  - Convergence plateaus early with high format reward but low accuracy reward → model learned structure but not task; consider increasing λ2 or reducing format reward weight
  - Gradient conflict frequency > 50% of steps → SFT signal largely discarded; check whether Guess labels are noisy or task is misaligned with RL objective
  - Think segment becomes empty or degenerate → RL may not be propagating useful gradients; inspect reward distribution and KL penalty magnitude
- First 3 experiments:
  1. **Convergence baseline**: Run pure GRPO and pure SFT on a single dataset (e.g., Emotion) for 10 epochs. Compare reward curves and test accuracy to GTA to quantify convergence acceleration.
  2. **Ablation on supervised Guess**: Replace L_SFT with RL on the Guess segment (as in Table 3). Confirm performance drop to validate the supervised anchor's contribution.
  3. **Gradient conflict analysis**: Log cosine similarity between ∇L_SFT and ∇L_RL at each step. Report conflict frequency and correlate with training stability metrics (reward variance, accuracy trajectory).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the GTA framework generalize effectively to NLP tasks beyond text classification (e.g., generation, QA, summarization)?
- Basis in paper: [explicit] "Theoretically, our approach is not limited to text classification tasks, and we plan to explore extending GTA to broader NLP tasks in future work."
- Why unresolved: Current experiments only cover four classification benchmarks; the Guess-Think-Answer structure may not transfer directly to open-ended generation tasks.
- What evidence would resolve it: Empirical evaluation of GTA on non-classification NLP tasks, comparing against SFT and RL baselines.

### Open Question 2
- Question: How does GTA's performance scale with model size beyond the 3B–4B parameter range tested?
- Basis in paper: [explicit] "Due to resource constraints, we validated our proposed method only on 3B and 4B parameter models, without extending the evaluation to larger-scale models."
- Why unresolved: RL heavily relies on model capacity; larger models may yield different efficiency-capability trade-offs or diminishing returns.
- What evidence would resolve it: Experiments on 7B, 13B, or larger models measuring convergence speed and final accuracy.

### Open Question 3
- Question: How does GTA perform on datasets with substantial label noise or quality inconsistencies?
- Basis in paper: [explicit] "While publicly available text classification datasets often contain noisy data, we did not perform preprocessing or sample high-quality subsets in this study."
- Why unresolved: The supervised Guess component may propagate noisy labels; robustness to noise remains untested.
- What evidence would resolve it: Ablation studies on datasets with controlled noise levels or real-world noisy corpora.

## Limitations

- Task Scope Limitation: The framework is evaluated exclusively on four text classification benchmarks. While results are strong, it remains unclear whether the structured reasoning approach generalizes to open-ended generation tasks, multi-step reasoning problems, or tasks requiring tool use.
- Computational Overhead Uncertainty: The paper does not report training time comparisons or computational cost analysis. The additional structure and gradient conflict resolution steps may introduce overhead that offsets the convergence acceleration benefit.
- Reward Signal Dependency: The RL component relies on both format and accuracy rewards. If either reward signal becomes sparse or noisy, the RL optimization may fail to learn effective corrective reasoning.

## Confidence

- **High Confidence**: The convergence acceleration mechanism (SFT-anchored Guess segment with RL-optimized Think+Answer) is well-supported by controlled ablations and reward curve comparisons against GRPO baselines.
- **Medium Confidence**: The self-correction capability claim is supported by case studies, but the mechanism is not fully isolated from other factors. Quantitative analysis of correction frequency is absent.
- **Medium Confidence**: The gradient conflict resolution approach is theoretically sound and correctly implemented, but the empirical necessity is unclear—conflict frequency and its impact on training stability are not reported.

## Next Checks

1. **Convergence Cost-Benefit Analysis**: Run GTA, pure GRPO, and pure SFT for the same wall-clock time (not epoch count) on a single dataset. Measure reward curves, test accuracy, and GPU hours to determine if convergence acceleration justifies computational overhead.

2. **Gradient Conflict Frequency Audit**: Log cosine similarity between SFT and RL gradients at each training step. Report conflict frequency, identify patterns (e.g., early vs. late training), and correlate with training stability metrics (reward variance, accuracy trajectory).

3. **Self-Correction Quantification**: On a held-out test set, count instances where Guess and Answer differ. For each, evaluate whether the final Answer is correct (self-correction) or incorrect (self-stabilization). Report correction rate and compare against a baseline where the Guess is used as the final answer.