---
ver: rpa2
title: On the Theoretical Limitations of Embedding-Based Retrieval
arxiv_id: '2508.21038'
source_url: https://arxiv.org/abs/2508.21038
tags:
- embed
- embedding
- arxiv
- retrieval
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical and empirical foundation for
  the fundamental limitations of embedding-based retrieval models. The authors prove
  that for any given embedding dimension d, there exists a finite number of document-query
  combinations that cannot be represented, due to constraints on the row-wise order-preserving
  rank of the relevance matrix.
---

# On the Theoretical Limitations of Embedding-Based Retrieval

## Quick Facts
- arXiv ID: 2508.21038
- Source URL: https://arxiv.org/abs/2508.21038
- Reference count: 31
- Primary result: Establishes theoretical and empirical foundations for fundamental limitations of embedding-based retrieval models due to embedding dimension constraints

## Executive Summary
This paper demonstrates that embedding-based retrieval models face fundamental limitations independent of model size or training data. The authors prove that for any embedding dimension d, there exists a finite number of document-query combinations that cannot be represented due to constraints on the row-wise order-preserving rank of the relevance matrix. They validate this through free parameterized embedding optimization and create LIMIT, a simple natural language dataset that stress-tests these theoretical limitations. Even state-of-the-art models struggle on LIMIT despite its simplicity, with recall@100 scores below 20% for most models, demonstrating that embedding models will inevitably face representational limits as retrieval tasks require encoding increasingly complex combinations of relevant documents.

## Method Summary
The paper combines theoretical analysis with empirical validation. Theoretically, it establishes that the minimum embedding dimension required to exactly represent a retrieval task is bounded by the sign-rank of the query relevance matrix, connecting to sign-rank theory through the row-wise order-preserving rank. Empirically, the authors optimize free parameterized embeddings on test data to find critical-n values where the number of documents becomes too large for the embedding dimension to encode all combinations. They then create LIMIT, a simple natural language dataset with 50k documents and 1k queries where k=2 relevant documents per query cover all possible combinations, to stress-test these theoretical limitations.

## Key Results
- For any embedding dimension d, there exists a finite critical-n value beyond which not all top-k combinations can be represented
- Free embedding optimization shows critical-n values: d=1024 → n≈4M; d=4096 → n≈250M (extrapolated)
- LIMIT dataset causes state-of-the-art models to achieve <20% recall@100 despite its simplicity
- Dense qrel patterns (high graph connectivity) are substantially harder for embedding models than sparse patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The minimum embedding dimension required to exactly represent a retrieval task is bounded by the sign-rank of the query relevance matrix, not by model size or training data.
- **Mechanism:** Given a binary relevance matrix A ∈ {0,1}^(m×n), the row-wise order-preserving rank (rank_rop) equals the minimum dimension d needed for any vector embedding to correctly rank relevant documents above irrelevant ones for all queries. This connects to sign-rank theory: rank±(2A - 1) - 1 ≤ rank_rop ≤ rank±(2A - 1).
- **Core assumption:** Relevance is modeled via dot product scoring where higher scores indicate relevance (standard in dense retrieval).
- **Evidence anchors:**
  - [abstract] "number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding"
  - [Section 3.2, Proposition 2] Provides the formal inequality chain connecting sign-rank to row-wise thresholdable rank
  - [corpus] Neighbor paper "$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-k Retrieval" derives related MED (Minimal Embeddable Dimension) bounds
- **Break condition:** If relevance is not binary, or if approximate ranking (allowing some errors) is acceptable, these exact bounds may not apply directly.

### Mechanism 2
- **Claim:** For each embedding dimension d, there exists a critical-n value (maximum documents) beyond which not all top-k combinations can be represented, following an approximately cubic relationship.
- **Mechanism:** Free embedding optimization directly optimizes query and document vectors on the target qrel matrix. When n exceeds the critical point, gradient descent cannot achieve 100% accuracy regardless of optimization quality, proving a fundamental geometric constraint.
- **Core assumption:** Vectors are normalized (unit norm), matching common practice in embedding models.
- **Evidence anchors:**
  - [Section 4] "We find there exists a crucial point for each embedding dimension (d) where the number of documents is too large for the embedding dimension to encode all combinations"
  - [Figure 2/Table 6] Empirical critical-n values: d=1024 → n≈4M; d=4096 → n≈250M (extrapolated from polynomial fit)
  - [corpus] No direct corpus evidence on this specific cubic relationship; this appears to be a novel empirical finding
- **Break condition:** Real embedding models face additional constraints from natural language, making actual capacity lower than free embedding bounds suggest.

### Mechanism 3
- **Claim:** Dense qrel patterns (high graph connectivity, many overlapping relevant sets across queries) are substantially harder for embedding models than sparse patterns.
- **Mechanism:** Dense qrel matrices have higher sign-rank, requiring more dimensions to represent. LIMIT uses all (n choose k) combinations for k=2, creating maximum interconnectivity.
- **Core assumption:** Difficulty correlates with sign-rank complexity of the qrel structure.
- **Evidence anchors:**
  - [Section 5.4, Figure 6] Dense pattern causes ~50 point drop in recall@100 for GritLM vs other patterns; E5-Mistral drops ~10x
  - [Table 1] LIMIT has graph density 0.085 and query strength 28.5 vs <0.03 and <0.6 for standard IR datasets
  - [corpus] Neighbor "Beyond Single Embeddings" quantifies limitations when relevant document distributions are multimodal, related but distinct failure mode
- **Break condition:** Real-world queries rarely require representing all theoretical combinations; practical impact depends on actual query distribution.

## Foundational Learning

- **Concept: Sign-rank of a matrix**
  - Why needed here: This is the core mathematical tool the paper uses to prove representational limits. Without understanding sign-rank, the theoretical contributions are inaccessible.
  - Quick check question: Given a 3×3 matrix with entries in {-1, +1}, what is the minimum rank of any real matrix with matching signs?

- **Concept: Dense retrieval with single-vector embeddings**
  - Why needed here: The paper specifically analyzes the dominant paradigm where queries and documents each map to one d-dimensional vector. Understanding this architecture is prerequisite to seeing why the limitations apply.
  - Quick check question: Why can't a single 1024-dimensional vector encode all pairwise relevance relationships among 10,000 documents?

- **Concept: Top-k retrieval and recall metrics**
  - Why needed here: The theoretical bounds specifically concern representing all possible top-k sets. Understanding recall@k is necessary to interpret the empirical results (e.g., <20% recall@100).
  - Quick check question: If a model achieves 50% recall@10 on a task with 2 relevant documents per query, how many relevant documents appear in the top 10 on average?

## Architecture Onboarding

- **Component map:**
  - Relevance matrix A (ground truth) -> Score matrix B = U^T V -> rank_rop(A) -> Minimum embedding dimension
  - A (m×n binary) contains relevance (1=relevant, 0=irrelevant)
  - U (d×m) contains query embeddings
  - V (d×n) contains document embeddings
  - rank_rop(A) is minimum dimension d where B exists that preserves row-wise ordering per A
  - LIMIT dataset: 50k documents, 1k queries, k=2 relevant docs per query, covering all (46 choose 2) = 1035 combinations for the subset of 46 relevant documents

- **Critical path:**
  1. Identify the qrel matrix structure for your retrieval task
  2. Estimate sign-rank lower bound (or use free embedding optimization to find empirical critical-n)
  3. Compare required dimension vs available embedding dimension
  4. If dimension is insufficient: consider multi-vector, sparse, or cross-encoder alternatives

- **Design tradeoffs:**
  - **Increase embedding dimension**: Extends representable combinations but increases storage/compute (e.g., 4096-d vectors are 4× larger than 1024-d)
  - **Multi-vector models (e.g., ColBERT)**: Higher expressivity, but more complex indexing and scoring (MaxSim over token-level vectors)
  - **Sparse/lexical methods (BM25)**: Effectively infinite dimensionality for exact matching, but cannot capture semantic similarity for instruction-following tasks
  - **Cross-encoders**: Unlimited expressivity per query, but O(n) computation per query—unsuitable for first-stage retrieval

- **Failure signatures:**
  - Low recall despite high MTEB/BEIR scores on standard benchmarks (datasets test few combinations)
  - Performance degrades as instruction complexity increases (more conditions → more combinations)
  - Models trained with MRL at truncated dimensions show sharper degradation on LIMIT
  - Similar documents with different relevance patterns get confused (sharing attributes but not both relevant)

- **First 3 experiments:**
  1. **Baseline measurement**: Evaluate your embedding model on LIMIT (available at github.com/google-deepmind/limit) to quantify current representational gaps
  2. **Dimension ablation**: Test same model at different embedding dimensions (32, 512, 1024, 4096) to measure sensitivity and identify minimum viable dimension for your task
  3. **Qrel density analysis**: For your target deployment, construct the qrel graph and compute graph density; if approaching LIMIT's 0.085, expect representational issues and consider multi-vector or hybrid approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical connection between embedding dimension and representational capacity be formally extended to multi-vector architectures (e.g., ColBERT)?
- Basis in paper: [Explicit] The authors state in the Limitations section: "we leave it to future work to extend our theoretical connections to these [multi-vector] settings."
- Why unresolved: The paper's proof relies on the rank of a single dot-product score matrix, which does not directly map to the MaxSim operator used in multi-vector models.
- What evidence would resolve it: A formal proof defining the "row-wise order-preserving rank" for late-interaction mechanisms, or empirical results on a multi-vector adaptation of the LIMIT dataset.

### Open Question 2
- Question: What are the theoretical bounds on embedding dimension when the retrieval model is only required to capture a specific subset (e.g., the majority) of valid query-document combinations?
- Basis in paper: [Explicit] The authors note they "did not show theoretical results for the setting where the user allows some mistakes" and leave "putting a bound on this scenario to future work."
- Why unresolved: The current theoretical framework assumes a binary "exact representation" requirement, whereas practical retrieval often tolerates a small percentage of errors.
- What evidence would resolve it: A derivation of the dimension $d$ relative to an error rate $\epsilon$, showing how much the required dimension decreases as the tolerance for missing combinations increases.

### Open Question 3
- Question: How can sparse retrieval models be effectively adapted for instruction-following and reasoning tasks that lack explicit lexical overlap?
- Basis in paper: [Inferred] The authors observe that while BM25 solves the LIMIT dataset easily due to high "effective" dimensionality, "it is less clear how to apply sparse models to instruction-following... We leave this direction to future work."
- Why unresolved: Sparse models rely on term matching, which fails when query relevance is defined by reasoning or instructions rather than shared vocabulary.
- What evidence would resolve it: A sparse or learned sparse model that achieves high recall on reasoning-heavy benchmarks (like BRIGHT) while maintaining the representational capacity advantages demonstrated in LIMIT.

## Limitations

- The theoretical framework assumes binary relevance and exact representation requirements, which may not reflect practical retrieval scenarios where approximate ranking is acceptable
- The LIMIT dataset uses an artificial construction requiring all document combinations to be represented, potentially overstating real-world limitations where query patterns are sparse and structured
- The empirical critical-n values are based on controlled free embedding optimization experiments, with uncertainty introduced when extrapolating to very large document collections (hundreds of millions)

## Confidence

- **High**: The mathematical relationship between sign-rank and embedding capacity (Proposition 2 and Corollary 1)
- **Medium**: The empirical critical-n values and cubic relationship (based on controlled experiments but extrapolated)
- **Medium**: The claim that LIMIT represents a stress test of fundamental limitations (well-constructed but may not reflect typical use cases)

## Next Checks

1. Test LIMIT's implications on real-world datasets with known query complexity (e.g., sparse vs dense qrel patterns) to measure practical impact
2. Evaluate multi-vector models (ColBERT, SPARTA) on LIMIT to determine if they overcome the identified limitations
3. Measure the effect of approximate ranking (allowing small errors) on the critical-n threshold to quantify the cost of exact representation requirements