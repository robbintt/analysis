---
ver: rpa2
title: Automatic Speech Recognition for Greek Medical Dictation
arxiv_id: '2509.23550'
source_url: https://arxiv.org/abs/2509.23550
tags:
- greek
- speech
- medical
- whisper
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an Automatic Speech Recognition (ASR) pipeline
  for Greek medical dictation, combining fine-tuned Whisper models with a domain-adapted
  Greek GPT-2 language model for re-ranking. The system was evaluated on Greek medical
  speech datasets, achieving significant improvements: fine-tuning reduced Word Error
  Rate (WER) from 43.62% to 30.31% for Whisper Small, from 34.71% to 19.45% for Whisper
  Medium, and from 26.41% to 14.90% for Whisper Large-v2.'
---

# Automatic Speech Recognition for Greek Medical Dictation

## Quick Facts
- arXiv ID: 2509.23550
- Source URL: https://arxiv.org/abs/2509.23550
- Reference count: 9
- Primary result: Greek medical ASR pipeline with fine-tuned Whisper + GPT-2 re-ranking, achieving WER reductions up to 47.45%

## Executive Summary
This paper presents an end-to-end Automatic Speech Recognition (ASR) pipeline tailored for Greek medical dictation. The system combines fine-tuned Whisper models with a domain-adapted Greek GPT-2 language model for re-ranking N-best hypotheses. The approach addresses the challenge of transcribing specialized medical terminology and spoken-language variability in Greek clinical settings. The authors curated a high-quality Greek speech-to-text dataset to support reproducibility and conducted systematic evaluations across multiple Whisper model sizes.

## Method Summary
The pipeline integrates fine-tuning of Whisper models on Greek medical speech data with GPT-2-based re-ranking of N-best hypotheses. Fine-tuning leveraged composite Greek speech datasets including Parliament, Common Voice, and FLEURS. The GPT-2 model was adapted to the medical domain for improved re-ranking accuracy. N=5 hypotheses were found optimal through empirical testing. The system was evaluated on Greek medical speech datasets using Word Error Rate (WER), Character Error Rate (CER), and BLEU metrics.

## Key Results
- Fine-tuning reduced WER from 43.62% to 30.31% for Whisper Small, 34.71% to 19.45% for Whisper Medium, and 26.41% to 14.90% for Whisper Large-v2
- GPT-2 re-ranking further improved performance by 9.66% WER reduction for Small, 6.27% for Medium, and 1.41% for Large-v2
- Overall pipeline achieved WER reductions of 37.23% (Small), 47.45% (Medium), and 44.38% (Large-v2) compared to original models

## Why This Works (Mechanism)
The pipeline works by combining acoustic model adaptation through fine-tuning with language model-based re-ranking. Fine-tuning adapts Whisper's acoustic representations to Greek medical speech characteristics, while GPT-2 re-ranking leverages domain-specific language patterns to correct transcription errors. The N-best hypothesis generation allows the language model to select the most contextually appropriate transcription from multiple candidates.

## Foundational Learning
- **Whisper architecture**: Why needed - Understand the base model being adapted; Quick check - Review encoder-decoder structure and pre-training objectives
- **Greek medical terminology**: Why needed - Domain specificity affects vocabulary and phrasing patterns; Quick check - Analyze frequency distribution of medical terms in training data
- **Language model re-ranking**: Why needed - Post-processing step that improves transcription accuracy; Quick check - Compare top-1 vs top-N hypothesis quality
- **Domain adaptation**: Why needed - General models underperform on specialized vocabulary; Quick check - Measure performance gap between general and medical datasets
- **N-best hypothesis generation**: Why needed - Provides multiple transcription candidates for re-ranking; Quick check - Analyze WER improvement as N varies
- **Evaluation metrics**: Why needed - WER/CER/BLEU quantify different aspects of transcription quality; Quick check - Calculate all three metrics on sample transcripts

## Architecture Onboarding

**Component Map**: Whisper ASR -> N-best hypotheses generation -> GPT-2 re-ranking -> Final transcription

**Critical Path**: Audio input → Whisper encoder → Whisper decoder → N-best hypotheses → GPT-2 scoring → Best hypothesis selection

**Design Tradeoffs**: Larger Whisper models provide better accuracy but increase computational cost; GPT-2 re-ranking improves quality but adds latency; N=5 balances quality and efficiency

**Failure Signatures**: 
- High WER indicates poor acoustic adaptation or insufficient medical vocabulary coverage
- Minimal improvement from re-ranking suggests language model mismatch or insufficient hypothesis diversity
- Large model latency may render system unusable for real-time applications

**3 First Experiments**:
1. Evaluate baseline Whisper performance on Greek medical test set before any adaptation
2. Test GPT-2 re-ranking effectiveness on N-best hypotheses generated by fine-tuned Whisper models
3. Measure WER improvement as N varies from 1 to 10 to find optimal hypothesis count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the pipeline perform on authentic Greek medical speech (e.g., doctor–patient interactions, clinical dictations) versus the general-domain speech corpus used for fine-tuning?
- Basis in paper: "For future work, we aim to incorporate authentic Greek medical speech data, such as doctor–patient interactions and clinical dictations."
- Why unresolved: Current Whisper fine-tuning used composite general-domain datasets (Parliament, Common Voice, FLEURS), not actual medical dictations; the acoustic model may not capture medical speech characteristics.
- What evidence would resolve it: Evaluation on a held-out dataset of real clinical Greek dictations, comparing WER/CER against current benchmarks.

### Open Question 2
- Question: Can the full Whisper–GPT-2 pipeline meet latency requirements for real-time clinical deployment on GPU-backed systems?
- Basis in paper: Future work includes "explore real-time deployment on GPU-backed systems."
- Why unresolved: The pipeline requires generating N-best hypotheses and re-ranking with GPT-2; empirical testing found N=5 optimal for quality but computational cost was noted as a constraint for larger models.
- What evidence would resolve it: Measurements of end-to-end latency (audio input to final transcription) across model sizes and N values, tested under realistic clinical workloads.

### Open Question 3
- Question: Do WER/CER/BLEU improvements translate to clinically meaningful gains in documentation accuracy and reduced manual correction effort for healthcare professionals?
- Basis in paper: Evaluation relies solely on automatic metrics; no user study or clinical validation is reported. The stated goal is to reduce documentation burden for healthcare professionals.
- What evidence would resolve it: A user study with Greek healthcare professionals measuring time spent correcting transcriptions, error types affecting clinical interpretation, and subjective usability ratings.

## Limitations
- Evaluation constrained to Greek medical dictation without testing on broader Greek domains or other languages
- Dataset size and composition details not fully specified, making it difficult to assess potential biases or overfitting risks
- GPT-2 re-ranking introduces additional computational complexity without clear analysis of efficiency trade-offs

## Confidence
- Technical pipeline improvements and dataset curation: High
- Domain-specific adaptation effectiveness: Medium
- Cross-domain or cross-linguistic applicability: Low

## Next Checks
1. Evaluate the system on non-medical Greek speech datasets to assess domain transferability
2. Conduct ablation studies to quantify the individual contributions of fine-tuning versus re-ranking
3. Test the pipeline with other transformer-based language models (e.g., GPT-Neo or BLOOM) to determine if performance gains are model-specific