---
ver: rpa2
title: 'TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models'
arxiv_id: '2506.03182'
source_url: https://arxiv.org/abs/2506.03182
tags:
- species
- taxonomic
- novel
- known
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TerraIncognita, a dynamic benchmark for evaluating
  vision-language models in insect species discovery. The dataset combines known species
  images with rare, potentially undescribed taxa collected from biodiversity hotspots,
  creating a realistic open-world discovery scenario.
---

# TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models

## Quick Facts
- arXiv ID: 2506.03182
- Source URL: https://arxiv.org/abs/2506.03182
- Reference count: 40
- Top models achieve >90% F1 at Order level but <2% at Species level for known species

## Executive Summary
TerraIncognita introduces a dynamic benchmark for evaluating vision-language models (VLMs) in open-world insect species discovery. The dataset combines known species images with rare, potentially undescribed taxa collected from biodiversity hotspots, creating a realistic scenario where models must distinguish known from novel species while providing expert-aligned explanations. The benchmark reveals significant performance gaps between coarse (Order) and fine-grained (Species) taxonomic classification, with discovery accuracy on novel species ranging from 55% to 88% across evaluated models.

## Method Summary
The benchmark evaluates 12 VLMs in zero-shot settings using a hierarchical classification task with explicit abstention ("Unknown") options. The dataset contains ~450 images spanning 8 insect orders and 42 families, divided into known species with full taxonomic labels and novel species with partial labels (typically Order/Family only). Models receive images via API and must predict taxonomy at Order, Family, Genus, and Species levels while generating explanations. Evaluation metrics include F1 scores at each taxonomic level for known species, taxonomic F1 at Order/Family for novel species, and discovery accuracy (correct prediction on known + correct abstention on novel). The benchmark will be updated quarterly to maintain challenge integrity.

## Key Results
- Top-performing models achieve over 90% F1 at the Order level on known species, but drop below 2% at the Species level
- Discovery accuracy on novel species varies widely, from 55% to 88%, revealing inconsistencies in abstention strategies
- Models commonly exhibit taxonomic overreach, morphological hallucination, and misguided justification in explanations

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Feature Granularity
- Claim: VLMs leverage coarse morphological features that generalize across insect orders but fail to capture fine-grained species-specific traits.
- Core assumption: Models have been exposed to sufficient insect images at coarse taxonomic levels during pre-training but sparse species-level annotations.
- Evidence anchors: Top models achieve >90% F1 at Order level but <2% at Species level; related work documents similar coarse-to-fine degradation in biodiversity tasks.

### Mechanism 2: Uncertainty Calibration for Novelty Detection
- Claim: Models that correctly abstain on novel species have learned to associate prediction confidence with feature familiarity rather than forcing classification.
- Core assumption: Novel species exhibit visual features sufficiently distinct from known species in training data.
- Evidence anchors: Discovery accuracy varies from 55% to 88% across models; Claude-3-Opus achieves 88% novel discovery accuracy while GPT-4.1 achieves 55%.

### Mechanism 3: Explanation Hallucination from Distribution Bias
- Claim: Models generate biologically implausible explanations when they retrieve statistically associated descriptors from training text rather than grounding in visible image evidence.
- Core assumption: Training data contains more generic entomological descriptions than species-specific diagnostic guides with precise feature-image alignment.
- Evidence anchors: Models reference invisible traits, cite overly generic traits that are not diagnostic, and produce speculative inferences unsupported by image evidence.

## Foundational Learning

- Concept: **Open-Set Recognition (OSR)**
  - Why needed here: The benchmark tests whether models can distinguish known from unknown species, requiring rejection of out-of-distribution inputs rather than forced classification into known categories.
  - Quick check question: Can your model output "I don't know" when encountering a class outside its training set, or does it always force a prediction?

- Concept: **Hierarchical Classification**
  - Why needed here: Taxonomy is structured as Order→Family→Genus→Species; models must navigate this hierarchy, potentially abstaining at finer levels while remaining confident at coarser ones.
  - Quick check question: Does your model's accuracy degrade gracefully from Order to Species, or does it collapse entirely at fine granularity?

- Concept: **Explanation Alignment**
  - Why needed here: Correct predictions for wrong reasons (e.g., citing invisible traits) indicate unreliable reasoning that won't generalize to novel inputs.
  - Quick check question: Does your model's explanation reference only visible morphological features, or does it describe traits not present in the image?

## Architecture Onboarding

- Component map: Image → VLM backbone (Vision encoder + Language decoder) → Prompt interface (Entomologist persona) → Output parser (Extracts taxonomy + explanations) → Evaluation suite (F1 scores + discovery accuracy + explanation review)

- Critical path: Image preprocessing → resize to model input while preserving aspect ratio for morphological feature visibility → Zero-shot inference with abstention-enabled prompt → Hierarchical accuracy evaluation at each taxonomic level → Discovery accuracy computation → Expert review of explanations

- Design tradeoffs:
  - Conservative vs. overconfident calibration: High abstention improves discovery accuracy on novel species but may miss known species
  - Explanation length vs. hallucination risk: Longer explanations provide more diagnostic detail but increase opportunity for morphological hallucination
  - Taxonomic depth vs. reliability: Optimizing for Order-level accuracy may require accepting near-zero Species-level performance

- Failure signatures:
  - Taxonomic overreach: Predicting species-level labels when only Order is visually determinable
  - Morphological hallucination: Citing invisible traits (e.g., internal structures or obscured morphology)
  - Geographic implausibility: Predicting species >1000km outside known range
  - Generic justification: Citing real but non-diagnostic traits (e.g., straw-yellow coloration common across many genera)

- First 3 experiments:
  1. Baseline zero-shot evaluation: Run all 12 VLMs on TerraIncognita with standard prompt; report F1 at each taxonomic level and discovery accuracy to establish model-specific calibration profiles.
  2. Prompt engineering for abstention calibration: Compare standard prompt against variants explicitly emphasizing uncertainty; measure tradeoff between discovery accuracy and known-species F1.
  3. Explanation grounding audit: Sample 20 images where models hallucinate; manually annotate visible morphological traits and compare against model explanations to identify systematic hallucination triggers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can frontier VLMs develop reliable abstention mechanisms that appropriately balance conservatism with utility in open-world biodiversity discovery?
- Basis in paper: Current VLMs lack good mechanisms for abstention, which is critical for AI-based decision-making; discovery accuracy varies widely (55–88%) with models showing premature overcommitment or conservative bias.
- Why unresolved: Models either over-commit to fine-grained predictions on novel species or default to "Unknown" even for known species, indicating poor uncertainty calibration.
- What evidence would resolve it: Development of calibration techniques or abstention modules that consistently achieve high discovery accuracy on both known and novel species simultaneously.

### Open Question 2
- Question: What architectural or training improvements could close the steep performance gap between coarse (Order >90% F1) and fine-grained (Species <2% F1) taxonomic classification?
- Basis in paper: The paper highlights the sharp difficulty gradient from coarse to fine taxonomic prediction with species-level F1 remaining below 2% across all models despite near-perfect order-level performance.
- Why unresolved: Current VLMs trained on web-scale data lack fine-grained morphological understanding required for species-level discrimination in zero-shot settings.
- What evidence would resolve it: Models achieving substantially higher species-level F1 (>20%) on the TerraIncognita benchmark through novel training objectives or multimodal integration.

### Open Question 3
- Question: How can model-generated explanations be systematically aligned with expert taxonomic reasoning and grounded in visible morphological traits?
- Basis in paper: The authors identify a discrepancy between model-generated explanations and expert reasoning and note a need for biologically-grounded explanation benchmarks in addition to label-level metrics.
- Why unresolved: VLMs produce linguistically plausible justifications that often reference invisible traits, rely on non-diagnostic features, or hallucinate biological details.
- What evidence would resolve it: Quantitative metrics for explanation alignment and models whose justifications consistently reference only visible, diagnostically relevant traits as validated by expert review.

## Limitations
- VLM calibration mechanisms remain opaque, limiting actionable insights for improving reliability
- Explanation evaluation relies on expert judgment without ground truth explanations for training
- Performance may degrade on taxa from unrepresented biogeographic regions or non-insect arthropods

## Confidence
- **High confidence**: Observed performance degradation from Order (90% F1) to Species (<2% F1) for known species, and correlation between abstention calibration and discovery accuracy
- **Medium confidence**: Attribution of these patterns to specific mechanisms (hierarchical feature granularity, uncertainty calibration, explanation hallucination)
- **Low confidence**: Explanation hallucination analysis lacks quantitative grounding and may reflect idiosyncratic patterns

## Next Checks
1. Cross-taxa calibration transfer: Evaluate TerraIncognita models on similar benchmarks for plants or birds to test domain generalization
2. Explanation hallucination quantification: Annotate 100 model explanations with visible/non-visible trait citations to establish baseline hallucination rates
3. Fine-tuning impact assessment: Compare zero-shot results against models fine-tuned on insect identification datasets to measure tradeoff between known species accuracy and discovery capability