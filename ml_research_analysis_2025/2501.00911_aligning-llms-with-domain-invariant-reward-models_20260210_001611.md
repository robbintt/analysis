---
ver: rpa2
title: Aligning LLMs with Domain Invariant Reward Models
arxiv_id: '2501.00911'
source_url: https://arxiv.org/abs/2501.00911
tags:
- source
- target
- data
- reward
- dial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of aligning large language models
  to human preferences when target domain preference data is unavailable. The authors
  propose DIAL, a framework that trains domain-invariant reward models by optimizing
  a dual loss: a domain loss minimizing Wasserstein distance between source and target
  distributions, and a source loss optimizing preferences on the source domain.'
---

# Aligning LLMs with Domain Invariant Reward Models

## Quick Facts
- arXiv ID: 2501.00911
- Source URL: https://arxiv.org/abs/2501.00911
- Reference count: 26
- Primary result: DIAL improves cross-lingual preference prediction accuracy from 0.621 to 0.661

## Executive Summary
This paper addresses the challenge of aligning large language models to human preferences when labeled target domain data is unavailable. The authors propose DIAL, a framework that trains domain-invariant reward models by optimizing a dual loss: a domain loss minimizing Wasserstein distance between source and target distributions, and a source loss optimizing preferences on the source domain. This approach enables transfer of preferences from labeled source domains to unlabeled target domains, with evaluations showing consistent improvements across four distinct settings: cross-lingual, clean-to-noisy, few-shot-to-full, and simple-to-complex task transfers.

## Method Summary
DIAL trains domain-invariant reward models by combining two losses: a domain loss that minimizes Wasserstein distance between source and target embeddings using an adversarial critic, and a source loss that optimizes preference predictions on source domain data. The method uses a base language model with separate reward and critic heads, trained with alternating updates where the critic distinguishes domains while the LM embeddings are updated to make domains indistinguishable. Preference learning uses Bradley-Terry loss on chosen/rejected pairs from the source domain. The framework is evaluated on four transfer scenarios using datasets like Stanford Human Preferences, showing improved accuracy and correlation metrics compared to source-only baselines.

## Key Results
- Cross-lingual transfer improves accuracy from 0.621 to 0.661
- Clean-to-noisy transfer improves accuracy from 0.671 to 0.703
- Few-shot-to-full transfer improves accuracy from 0.845 to 0.920
- Simple-to-complex task transfer improves correlation from 0.508 to 0.556

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Domain Alignment via Wasserstein Distance
- **Claim:** Minimizing Wasserstein distance between source and target embeddings creates domain-invariant representations that transfer preferences without target labels.
- **Mechanism:** A critic head learns to distinguish source from target embeddings by maximizing expected score differences. The base LM embeddings are then updated to minimize this distance, creating representations where domain becomes indistinguishable. Unlike KL divergence, Wasserstein distance remains defined even when distributions have disjoint supports.
- **Core assumption:** Source and target distributions share underlying preference-relevant features that can be captured in a shared embedding space.
- **Evidence anchors:** [abstract] "a domain loss that minimizes the divergence between source and target distribution"; [Section 2.1] "The equilibrium of the game is reached if the language model θ finds embeddings where the source and target data are indistinguishable"; [corpus] Related work WDGRL (Shen et al.) shows Wasserstein-guided representation learning stabilizes adversarial domain adaptation.
- **Break condition:** When source-target domain shift is too large (e.g., completely different tasks), aligning embeddings may not transfer meaningful preferences. Simple-to-complex experiments show larger gap to oracle (0.577 vs 0.857 correlation), suggesting alignment difficulty.

### Mechanism 2: Dual-Head Separation of Domain and Preference Signals
- **Claim:** Separating domain discrimination (critic head) from preference scoring (reward head) enables each to specialize without interference.
- **Mechanism:** The critic head optimizes Wasserstein distance using gradient penalties for Lipschitz constraints. The reward head applies Bradley-Terry loss to separate chosen/rejected responses. These operate on shared embeddings but with frozen-counterpart training: critic updates with frozen LM, then LM updates with frozen critic, then reward updates.
- **Core assumption:** Domain-invariant embeddings preserve preference-relevant structure; the alignment process doesn't destroy semantic information needed for preference classification.
- **Evidence anchors:** [Section 2.1] "the domain loss aligns source and target embeddings, while the source loss separates chosen and reject embeddings"; [Figure 3] t-SNE visualizations show DIAL clusters (source positive, target positive) and (source negative, target negative) together, while baseline mixes target positives/negatives.
- **Break condition:** If domain alignment dominates training, embeddings may become overly smoothed, losing discriminative power. Spurious reward experiments (Section 3.8) show DIAL can still learn incorrect rewards when source-task labels have confounders.

### Mechanism 3: Theoretical Transfer Bound via Lipschitz Continuity
- **Claim:** Target domain error is bounded by source error plus a term proportional to Wasserstein distance, providing principled justification for the dual-loss approach.
- **Mechanism:** Theorem 1 establishes: ε_T(r,f) ≤ ε_S(r,f) + 2KL_σW_1(μ_S, μ_T). Since the reward function is K-Lipschitz and sigmoid is 1/4-Lipschitz, the disagreement function inherits bounded sensitivity to distribution shifts measured by Wasserstein distance.
- **Core assumption:** The learned reward function satisfies Lipschitz continuity with respect to the input metric; this depends on architecture and training.
- **Evidence anchors:** [Section 2.3] "The two terms on the right hand side correspond to the source and domain loss in DIAL"; [Appendix B] Full proof using Kantorovich-Rubinstein duality.
- **Break condition:** The bound is non-constructive—it tells us what to minimize but doesn't guarantee convergence. If the Lipschitz constant K is large or distributions diverge significantly, the bound becomes loose.

## Foundational Learning

- **Wasserstein GANs and Optimal Transport:**
  - Why needed here: Understanding why Wasserstein distance is preferred over KL divergence for domain adaptation—specifically its meaningfulness when distributions don't overlap.
  - Quick check question: Given two non-overlapping distributions P and Q, why would W_1(P,Q) provide more useful signal than KL(P||Q)?

- **Bradley-Terry Preference Models:**
  - Why needed here: The reward head uses this to convert pairwise preferences into a learned scoring function; understanding the loss formulation is essential for debugging reward training.
  - Quick check question: If r(x,y+) - r(x,y-) = 0, what is the Bradley-Terry loss value, and what does it imply about model confidence?

- **Gradient Penalty for Lipschitz Enforcement:**
  - Why needed here: The critic must satisfy Lipschitz constraints for valid Wasserstein approximation; gradient penalty is the enforcement mechanism.
  - Quick check question: Why is the gradient penalty computed at interpolated points rather than just source/target points?

## Architecture Onboarding

- **Component map:** Input (prompt x, response y) -> Base LM (Gemma-2b, frozen except LoRA adapters, rank 64) -> Embedding [2048-dim] -> Critic Head (2-layer MLP: 256->128->1, GELU, weight decay 0.001) -> scalar d(x,y) for Wasserstein distance estimation; Reward Head (linear, no bias: 2048->1) -> scalar r(x,y) for preference scoring

- **Critical path:**
  1. Batch composition: Each batch = 4 source examples (chosen/rejected pairs) + 4 target examples (unlabeled)
  2. Critic update (frozen LM): Maximize d(source) - d(target), minimize gradient penalty (λ_gp=1.0), 3 critic iterations per batch
  3. LM embedding update (frozen critic): Minimize Wasserstein distance (da_loss)
  4. Reward update: Compute source preference loss on chosen/rejected pairs (src_loss)
  5. Total: src_loss + da_loss + critic_loss (note: critic_loss is negative of da_loss for different optimization targets)

- **Design tradeoffs:**
  - Wasserstein vs. DANN-style gradient reversal: Wasserstein provides more stable gradients but requires critic training iterations; gradient penalty adds compute overhead
  - Shared vs. separate embedding updates: Updating embeddings for both domain alignment and preference learning risks interference; the alternating training schedule mitigates this but requires careful tuning
  - LoRA rank 64: Higher rank preserves more base model capacity but increases overfitting risk on small source data

- **Failure signatures:**
  - Critic collapse: If critic loss plateaus near zero early, source/target are already indistinguishable in base model (check: low initial Wasserstein distance)—domain adaptation provides no benefit
  - Reward degradation on source: If source accuracy drops during training, domain loss may be dominating; reduce domain loss weight (λ for da_loss)
  - High variance across seeds: Especially in few-shot regime; indicates sensitivity to source sample selection (Table 3 shows 0.852->0.965 variance across splits)
  - No improvement over Src-Pref baseline: Check that target data is sufficiently different—if source and target are already aligned, domain adaptation adds noise

- **First 3 experiments:**
  1. Baseline sanity check: Train Src-Pref (source-only) and verify reasonable source accuracy. If source accuracy <0.65, preference data quality is the bottleneck, not domain transfer.
  2. Embedding visualization: At epochs 0, 1, 2, visualize source/target, chosen/rejected embeddings via t-SNE. Expect: epoch 0 shows domain separation; epoch 2 should show (source-chosen, target-chosen) clustering. If domains never align, Wasserstein distance may be insufficient for this domain pair.
  3. Data mix ablation: Following Section 3.7, test source/target ratios (0.2/0.8, 0.4/0.6, 0.6/0.4) with fixed total examples. Peak performance around 0.4 target / 0.6 source validates the theoretical bound's practical implication.

## Open Questions the Paper Calls Out

- **Can DIAL be extended to handle drastic source-target shifts between highly divergent tasks, such as transferring supervision to synthetic LLM-generated data?**
  - **Basis in paper:** [explicit] The conclusion lists "extending DIAL to handle drastic source-target shifts" and applying it to "transferring supervision to synthetic LLM generated data" as areas for future work.
  - **Why unresolved:** The current evaluations focus on shifts within similar modalities (e.g., language-to-language, simple-to-complex) where shared features are abundant; divergent tasks may lack the overlap required for Wasserstein alignment.
  - **What evidence would resolve it:** Empirical results showing DIAL's efficacy on cross-modal datasets or target domains comprised entirely of novel, synthetic LLM outputs.

- **How can the framework be modified to prevent "reward shortcuts" where the model aligns embeddings without transferring meaningful concepts?**
  - **Basis in paper:** [explicit] The limitations section notes the risk that the reward model "can find a shortcut to successfully align the source and target embedding representations... without actually transferring the meaningful domain-agnostic concepts."
  - **Why unresolved:** The current domain loss optimizes purely for statistical indistinguishability (Wasserstein distance), which does not guarantee semantic consistency.
  - **What evidence would resolve it:** Introduction of regularization techniques or diversity constraints that prevent accuracy degradation in synthetic "flipped" scenarios (e.g., source: fruit vs. vegetable; target: vegetable vs. fruit).

- **Can scalable, non-adversarial frameworks for domain adaptation achieve comparable alignment without the instability of adversarial training?**
  - **Basis in paper:** [explicit] The limitations section states that the adversarial loss "adds added complexity and can potentially be unstable," explicitly identifying "developing scalable, non-adversarial frameworks" as a direction for future work.
  - **Why unresolved:** DIAL currently relies on a min-max game between the critic and language model, which is inherently less stable than standard supervised training.
  - **What evidence would resolve it:** A comparative study showing that a non-adversarial divergence metric (e.g., contrastive learning) can match DIAL's performance with lower variance or fewer hyperparameter sensitivities.

## Limitations

- **Generalization to unseen domains:** The theoretical bound provides worst-case guarantees but doesn't ensure practical transfer when domain shift is substantial. The simple-to-complex task experiments show performance degradation (correlation 0.508->0.556 vs oracle 0.857), suggesting alignment difficulty for large task complexity gaps.
- **Sample efficiency claims:** While results appear strong, the paper doesn't systematically explore how performance scales with source data size. Table 3 shows high variance in few-shot settings (0.852->0.965 across splits), suggesting results depend heavily on source sample selection.
- **Preference quality assumptions:** DIAL inherits biases from source preference data. Section 3.8 shows the method can learn incorrect rewards when source preferences contain confounders. The framework assumes source preferences are valid and transferable, which may not hold when source and target have fundamentally different evaluation criteria.

## Confidence

- **High Confidence (Method Mechanics):** The dual-loss architecture with Wasserstein distance minimization and Bradley-Terry preference scoring is clearly specified and theoretically grounded. The alternating training procedure and component roles are well-defined.
- **Medium Confidence (Empirical Claims):** Cross-lingual and clean-to-noisy experiments show consistent improvements, but sample sizes and variance measures are limited. The simple-to-complex transfer results show improvement but with larger gaps to oracle performance, suggesting incomplete alignment.
- **Low Confidence (Scaling Claims):** Claims about DIAL's effectiveness in few-shot regimes lack systematic scaling analysis. High variance across seeds and dependence on specific source-target ratios suggest sensitivity to experimental conditions not fully characterized.

## Next Checks

1. **Domain similarity sensitivity analysis:** Systematically vary domain similarity (using domain classifier confidence or Wasserstein distance between source/target) and measure DIAL's performance across this spectrum. This would reveal whether the method has a "domain distance threshold" beyond which alignment fails.

2. **Source data scaling experiment:** Vary source preference data quantity (e.g., 100, 500, 1000, 2000 examples) while keeping target fixed, measuring both source accuracy and target transfer performance. This would quantify whether DIAL's benefits scale with source data or saturate.

3. **Ablation on critic iterations:** Test DIAL with 1, 2, and 3 critic iterations per batch to determine if the current setting (3) is optimal or excessive. Monitor Wasserstein distance convergence and reward head performance to identify potential overtraining of the critic.