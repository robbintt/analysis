---
ver: rpa2
title: 'DIY-MKG: An LLM-Based Polyglot Language Learning System'
arxiv_id: '2507.01872'
source_url: https://arxiv.org/abs/2507.01872
tags:
- language
- vocabulary
- words
- diy-mkg
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIY-MKG is an open-source, LLM-powered system designed to support
  polyglot language learners by enabling personalized vocabulary acquisition through
  multilingual knowledge graphs. The system allows users to expand vocabulary with
  LLM-suggested related words across multiple languages, add rich annotations at node,
  edge, and hyper-edge levels, and engage in adaptive reviewing via dynamically generated
  quizzes.
---

# DIY-MKG: An LLM-Based Polyglot Language Learning System

## Quick Facts
- **arXiv ID**: 2507.01872
- **Source URL**: https://arxiv.org/abs/2507.01872
- **Reference count**: 17
- **Primary result**: LLM-powered system enabling personalized multilingual vocabulary acquisition through knowledge graphs with 3,000-word vocabulary after 500 iterations

## Executive Summary
DIY-MKG is an open-source, LLM-powered polyglot language learning system that uses multilingual knowledge graphs to support personalized vocabulary acquisition. The system enables users to expand vocabulary with LLM-suggested related words across multiple languages, add rich annotations at node, edge, and hyper-edge levels, and engage in adaptive reviewing via dynamically generated quizzes. The architecture emphasizes user control to prevent cognitive offloading, requiring manual selection of suggested words rather than automatic addition. The system supports Spanish, Korean, and Japanese with local JSON storage and a desktop-only interface.

## Method Summary
DIY-MKG uses Llama-3.3-70B-Instruct with temperature 0 for vocabulary expansion and quiz generation, and gpt-4.1-2025-04-14 (temperature 0) as a judge for quiz correctness. The system starts with 10 random seed words per language and iterates 500 times to expand vocabulary. Vocabulary expansion suggests related words via LLM, which users manually select for addition to the knowledge graph. Adaptive reviewing uses click-frequency tracking to prioritize low-engagement words in dynamically generated quizzes. Prompts for all LLM interactions are customizable and provided in Appendix A. Evaluation measures vocabulary growth, fairness across languages/starting words, and quiz correctness (MCQ ~98%, FIB 76-84%).

## Key Results
- Vocabulary expansion reliably grew to ~3,000 words after 500 iterations with low variance across Spanish, Korean, and Japanese
- Multiple-choice quiz accuracy achieved 98% correctness rate
- Fill-in-the-blank questions showed higher variability at 76-84% accuracy
- System successfully supports rich annotations at node, edge, and hyper-edge levels
- User-controlled expansion prevents cognitive offloading while maintaining engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual knowledge graph construction enhances vocabulary acquisition through cross-linguistic connections.
- Mechanism: Users build a graph where nodes represent words across languages and edges encode relationships (synonyms, cognates, shared roots). LLMs suggest related words; users selectively add them. This leverages evidence that vocabulary knowledge transfers across languages via cognates and morphological shared features.
- Core assumption: Learners retain vocabulary better when they can connect new words to existing knowledge in other languages, rather than memorizing isolated word lists.
- Evidence anchors:
  - [abstract] "constructs by selective expansion with related words suggested by an LLM" with emphasis on "linguistic connections across vocabularies in multiple languages"
  - [section] Section 2 cites Bartolotti & Marian (2017) on cross-linguistic vocabulary transfer; cites Garcia-Castro et al. (2025) and others on cognates
  - [corpus] Weak direct evidence; corpus focuses on software polyglot systems, not language learning
- Break condition: If learner lacks sufficient vocabulary in at least one base language, cross-linguistic anchoring fails; mechanism assumes prior multilingual knowledge to draw from.

### Mechanism 2
- Claim: Manual selection of LLM-suggested words mitigates cognitive offloading and maintains learner engagement.
- Mechanism: The system requires users to click and approve each suggested word before adding it to their graph. This forces active decision-making rather than passive acceptance. The paper explicitly cites Kosmyna et al. (2025) on cognitive debt from uncritical AI acceptance.
- Core assumption: The act of evaluating and selecting words promotes deeper processing than simply reading auto-generated lists.
- Evidence anchors:
  - [abstract] "excessive cognitive offloading" identified as a limitation of existing tools
  - [section] Section 3.1: "This design prevents the user from fully relying on LLMs without critical thinking"
  - [corpus] No direct corpus evidence on cognitive offloading in educational contexts
- Break condition: If users rapidly select all suggestions without evaluation, the mechanism degrades to passive consumption.

### Mechanism 3
- Claim: Click-frequency-based adaptive review targets under-practiced vocabulary for improved retention.
- Mechanism: Each word's click count serves as a proxy for familiarity. Quizzes prioritize low-click words. LLMs generate fresh questions per session to prevent pattern memorization. Question flagging provides a feedback loop for prompt refinement.
- Core assumption: Click count correlates inversely with need for review (though paper acknowledges this may not hold for easily-memorized words).
- Evidence anchors:
  - [abstract] "adaptive review module that leverages LLMs for dynamic, personalized quiz generation"
  - [section] Section 3.3: click counter tracks engagement; Section 4.2 shows 98% MCQ accuracy, 76-84% FIB accuracy across languages
  - [corpus] No direct corpus evidence on adaptive spaced repetition systems
- Break condition: If users avoid clicking difficult words, those words receive fewer quizzes, creating a feedback loop where hard words get less practice.

## Foundational Learning

- Concept: **Knowledge Graphs (Nodes, Edges, Hyper-edges)**
  - Why needed here: The entire system is structured around representing vocabulary as a graph. Users must understand that nodes = words, edges = relationships, hyper-edges = documents connecting multiple words.
  - Quick check question: If you add "perro" (Spanish) and connect it to "dog" (English), what represents the relationship between them?

- Concept: **Cognitive Offloading**
  - Why needed here: The paper's design philosophy centers on preventing over-reliance on AI. Understanding this concept helps explain why selection is manual rather than automatic.
  - Quick check question: Why doesn't DIY-MKG automatically add all LLM-suggested words to the vocabulary?

- Concept: **LLM Prompting for Structured Output (JSON)**
  - Why needed here: All LLM queries require structured JSON responses (word lists, quiz questions). Customizing the system requires modifying prompts.
  - Quick check question: If you wanted domain-specific medical vocabulary, which component would you modify?

## Architecture Onboarding

- Component map:
  Frontend -> LLM Layer -> Data Layer
  (Web interface with graph visualization and side panel) -> (Handles vocabulary expansion, description generation, quiz generation, content filtering) -> (Local JSON storage for knowledge graph snapshots, annotations, quiz results, flagged questions)

- Critical path:
  1. User adds initial word → 2. LLM suggests related words → 3. User selects subset → 4. Graph updates with new nodes/edges → 5. User annotates → 6. Click counter tracks engagement → 7. Quiz generator targets low-click words → 8. User completes quiz, can flag bad questions

- Design tradeoffs:
  - Local storage (privacy, offline access) vs. no cloud sync (manual export required)
  - Manual word selection (reduces offloading) vs. slower vocabulary growth
  - High information density (desktop-only UI) vs. no mobile support
  - LLM-dependent quality (requires strong multilingual model) vs. cost/accessibility for low-resource languages

- Failure signatures:
  - **Vocabulary saturation**: If expansion rate drops sharply before 500 iterations, prompt may be too conservative or model too repetitive
  - **Ambiguous quiz questions**: Fill-in-the-blank questions with 76-84% accuracy often have multiple valid answers (see Appendix C example); requires prompt refinement
  - **Inappropriate content**: Without safe mode filtering, LLM may generate age-inappropriate words

- First 3 experiments:
  1. **Vocabulary expansion test**: Start with 10 random seed words per language, run 500 iterations, verify ~3,000-word vocabulary with low variance across seeds (replicate Figure 5)
  2. **Quiz accuracy validation**: Generate 50 MCQ and 50 FIB questions per language, manually verify correctness rates match reported 98% / 76-84% benchmarks
  3. **Prompt customization**: Modify vocabulary expansion prompt for a specific domain (e.g., medical terms), verify domain-relevant words are suggested and added

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DIY-MKG framework be effectively adapted to educational domains beyond vocabulary acquisition?
- Basis in paper: [explicit] The authors state, "we plan to adapt the framework to other disciplines, where knowledge graphs can be similarly used to encode the connection between concepts."
- Why unresolved: The current system architecture and evaluation metrics are specialized for linguistic features (e.g., synonyms, cognates) rather than general conceptual relationships.
- What evidence would resolve it: A successful implementation of the system for a non-linguistic subject (e.g., science or history) demonstrating reliable concept expansion and connection mapping.

### Open Question 2
- Question: How does the addition of audio and image modalities affect the system's usability and learning efficacy?
- Basis in paper: [explicit] The authors note, "In the future, we will extend the interface to more modalities, including audio and images."
- Why unresolved: The current interface and evaluation are limited to text-based interactions; the impact of multimodal inputs on cognitive load or graph construction is unknown.
- What evidence would resolve it: User studies comparing text-only graph construction against multimodal versions to measure changes in engagement and vocabulary retention.

### Open Question 3
- Question: Does DIY-MKG provide measurable improvements in long-term vocabulary retention compared to existing commercial tools?
- Basis in paper: [inferred] The paper lists the lack of a "large-scale user study" as a limitation and notes the need to validate learning outcomes.
- Why unresolved: The evaluation focuses on the technical reliability of LLM components (quiz accuracy, graph expansion) rather than the pedagogical value of the system.
- What evidence would resolve it: A controlled longitudinal study comparing standardized test scores or retention rates of DIY-MKG users against control groups using apps like Duolingo.

## Limitations
- Click frequency as proxy for learning needs may create feedback loops where difficult words receive less practice
- Fill-in-the-blank quiz generation shows significant variability (76-84% accuracy) compared to multiple-choice questions
- Long-term vocabulary retention and effectiveness across diverse learner profiles remain unproven
- Limited to three languages (Spanish, Korean, Japanese) with no evaluation of low-resource language support

## Confidence
- **High confidence**: Vocabulary expansion mechanism (3,000 words after 500 iterations with low variance across languages) - supported by systematic evaluation with multiple runs
- **Medium confidence**: Quiz generation accuracy (98% MCQ, 76-84% FIB) - though results show variability, the methodology is sound but prompts may need refinement
- **Low confidence**: Long-term learning outcomes and transfer effects - no longitudinal data or comparison with baseline learning methods provided

## Next Checks
1. **Long-term retention validation**: Conduct a 4-week study tracking retention of vocabulary learned through DIY-MKG versus traditional methods, testing recall at multiple intervals to assess actual learning effectiveness
2. **Click frequency vs. learning needs analysis**: Implement logging to correlate click patterns with actual learning difficulty and test performance, then adjust the adaptive review algorithm to weight question frequency based on demonstrated difficulty rather than raw click count
3. **Prompt refinement experiment**: Systematically modify fill-in-the-blank generation prompts to reduce ambiguity, then conduct a controlled test comparing revised prompts against original versions to isolate improvements in question quality and accuracy