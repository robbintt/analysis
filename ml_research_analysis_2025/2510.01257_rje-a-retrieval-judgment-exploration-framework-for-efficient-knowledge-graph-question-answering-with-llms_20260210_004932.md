---
ver: rpa2
title: 'RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph
  Question Answering with LLMs'
arxiv_id: '2510.01257'
source_url: https://arxiv.org/abs/2510.01257
tags:
- reasoning
- question
- paths
- exploration
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RJE, a Retrieval-Judgment-Exploration framework
  for knowledge graph question answering (KGQA) using large language models (LLMs).
  RJE addresses the limitations of retrieval-based and agent-based methods by first
  retrieving relevant reasoning paths from a knowledge graph, then using an LLM to
  judge if the evidence is sufficient to answer the question, and finally exploring
  additional evidence only when needed.
---

# RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs

## Quick Facts
- **arXiv ID:** 2510.01257
- **Source URL:** https://arxiv.org/abs/2510.01257
- **Reference count:** 40
- **Primary result:** RJE achieves state-of-the-art KGQA performance while reducing LLM calls by up to 65% and enabling small (3B/8B) open-source LLMs to match larger models.

## Executive Summary
RJE introduces a novel Retrieval-Judgment-Exploration framework for knowledge graph question answering that significantly improves efficiency while maintaining accuracy. The framework first retrieves relevant reasoning paths from a knowledge graph, uses an LLM to judge whether evidence is sufficient, and only explores further when necessary. This conditional approach enables small open-source LLMs to achieve competitive results without fine-tuning, while dramatically reducing computational overhead compared to iterative agent-based methods.

## Method Summary
RJE operates in three stages: retrieval, judgment, and conditional exploration. It retrieves reasoning paths containing topic entities, ranks them using a lightweight PLM-based ranker, and employs an LLM to determine sufficiency. If insufficient, the LLM decomposes the question and explores additional evidence with retriever-assisted relation filtering. The framework includes auxiliary modules for reasoning path ranking, question decomposition, and relation path retrieval. Experiments show RJE outperforms baselines on standard KGQA benchmarks while reducing LLM calls by 65% and enabling small LLMs to achieve competitive results.

## Key Results
- RJE reduces LLM calls by 65.0% compared to ToG and 40.6% compared to PoG on CWQ
- Llama3.2-3B achieves 62.9% accuracy on CWQ vs PoG's 21.4% (41.5% improvement)
- Llama3.1-8B achieves 71.5% accuracy on CWQ vs PoG's 43.6% (27.9% improvement)
- 77.7% of WebQSP questions correctly answered during judgment stage, requiring only single LLM call

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional exploration triggered by sufficiency judgment reduces unnecessary LLM calls while maintaining accuracy.
- Mechanism: An LLM evaluates whether top-K reasoning paths contain sufficient evidence to answer the question; if sufficient, the answer is generated immediately without exploration.
- Core assumption: LLMs can reliably distinguish sufficient from insufficient evidence across question types.
- Evidence anchors:
  - [abstract]: "evaluates whether the evidence is sufficient to answer the question, and finally explores additional evidence only when needed"
  - [section 5.6]: "RJE reduces LLM calls by 65.0% compared to ToG and 40.6% compared to PoG on CWQ"
  - [section F]: "For WebQSP, 77.7% of the questions are correctly answered during the judgment stage, indicating that only a single LLM call is required"
  - [corpus]: Weak direct evidence—corpus papers focus on evidence path reasoning but don't validate sufficiency judgment specifically.

### Mechanism 2
- Claim: Reasoning path ranking reduces noise and preserves relevance, improving LLM reasoning quality.
- Mechanism: A lightweight PLM-based ranker scores reasoning paths using margin ranking loss with weak supervision (paths containing answers labeled positive), retaining only top-K paths for downstream processing.
- Core assumption: Paths containing answer entities are more relevant for reasoning than those that don't.
- Evidence anchors:
  - [section 4.1.2]: "To prevent overwhelming downstream LLMs with excessive reasoning paths, we introduce a reasoning path ranking module... only select the top-K reasoning paths based on relevance scores"
  - [appendix D, Figure 3]: Combined retriever + ranker achieves higher answer coverage across all candidate path sizes compared to retriever alone
  - [corpus]: Corpus evidence is limited; related work (EPERM) focuses on evidence path enhancement but uses different ranking approaches.

### Mechanism 3
- Claim: Retriever-assisted exploration constrains the search space, enabling small LLMs to perform complex KG navigation.
- Mechanism: During exploration, the retriever pre-filters the top-N most relevant relations from all candidate relations before the LLM makes selections. This reduces token count and cognitive load, allowing smaller models to focus on a manageable candidate set.
- Core assumption: The retriever consistently ranks truly relevant relations in the top-N positions.
- Evidence anchors:
  - [section 4.3.2]: "we employ the relation path retriever to filter the top-N most relevant relations from R^D_{f,i}, resulting in a refined set"
  - [table 2]: RJE with Llama3.2-3B achieves 62.9% on CWQ vs PoG's 21.4% (41.5% improvement); with Llama3.1-8B achieves 71.5% vs PoG's 43.6% (27.9% improvement)
  - [corpus]: "Learning Efficient and Generalizable Graph Retriever" (FMR=0.65) validates retriever importance for KGQA but uses different methodology.

## Foundational Learning

- **Concept: Knowledge Graph Structure (triples, entities, relations, multi-hop paths)**
  - Why needed here: RJE operates on reasoning paths defined as alternating sequences of entities and relations; understanding KG topology is essential for debugging retrieval and exploration failures.
  - Quick check question: Given a topic entity "Peyton Manning" and a relation path (parents → teams), can you trace what a reasoning path would look like?

- **Concept: Retrieval-Augmented Generation (RAG) with structured knowledge**
  - Why needed here: RJE is fundamentally a RAG approach with conditional exploration; distinguishing it from standard text-based RAG clarifies why path ranking and retriever-assistance are necessary.
  - Quick check question: Why does standard text-based RAG struggle with multi-hop KGQA tasks?

- **Concept: Beam search for path exploration**
  - Why needed here: Both relation path retrieval and retriever-assisted exploration use beam search to narrow candidate spaces; understanding the trade-offs between breadth and depth is critical for hyperparameter tuning.
  - Quick check question: If beam width is set too low, what specific failure mode would you expect in the retrieval stage?

## Architecture Onboarding

- **Component map:** Relation Path Retriever -> Reasoning Path Ranker -> LLM Interface -> SPARQL Query Engine -> Path State Manager
- **Critical path:** Question + Topic Entities → Relation Path Retrieval → Reasoning Path Ranking → Judgment (LLM) → [if insufficient] Question Decomposition → Exploration Entity Selection → Retriever-assisted Relation Exploration → Entity Exploration → Answer Generation (iterative until sufficient or D_max reached)
- **Design tradeoffs:**
  - **K (reasoning paths):** Too few → insufficient evidence for judgment; too many → noise degrades LLM accuracy (observed peak at K=10, decline beyond 20 in appendix E.1)
  - **N (filtered relations):** More relations increase coverage but raise token costs and noise; N=10 found optimal in experiments
  - **D_max (exploration rounds):** Higher rounds improve complex question accuracy but increase latency; set to 2 for WebQSP (max 2-hop), 4 for CWQ (max 4-hop)
- **Failure signatures:**
  - High exploration trigger rate (>50%) with low exploration-stage accuracy → judgment module miscalibrated or retrieval coverage insufficient
  - Low answer coverage at retrieval stage (<60%) → ranker filtering too aggressively or retriever weak
  - Token overflow errors during exploration → N too large or exploration rounds excessive
  - Small LLM performance collapses on multi-hop questions → retriever-assistance not reducing search space effectively
- **First 3 experiments:**
  1. **Validate retrieval coverage:** Run retriever + ranker on held-out questions, measure answer coverage rate at K=1, 5, 10, 20, 50; compare against retriever-only baseline to isolate ranker contribution
  2. **Calibrate judgment accuracy:** Sample 100 questions, manually label sufficiency of top-K paths, compare against LLM judgments to quantify false positive/negative rates; adjust judgment prompt if needed
  3. **Efficiency profiling:** Measure LLM calls, input/output tokens, and latency per question on CWQ test set; compare against ToG and PoG baselines using identical backbone LLM to isolate framework efficiency gains

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the RJE framework be augmented to effectively detect and filter noisy or outdated triples within a knowledge graph to prevent misleading the LLM?
  - Basis in paper: [explicit] The authors state in the Limitations section that existing KGs often contain noisy triples and outdated information which can mislead LLMs, and they identify investigating methods for detecting and filtering unreliable knowledge as future work.

## Limitations

- **Sufficiency judgment reliability:** The framework's performance critically depends on the LLM's ability to accurately judge evidence sufficiency, which may degrade with smaller models or complex question types
- **Weak supervision assumptions:** The ranker's assumption that paths containing answer entities are always most relevant may not hold for multi-hop reasoning requiring intermediate paths
- **Retriever dependence:** The effectiveness of retriever-assisted exploration depends on the retriever consistently ranking truly relevant relations in the top-N positions, which is not thoroughly validated

## Confidence

- **High confidence:** Efficiency improvements (65.0% reduction in LLM calls, 40.6% reduction in tokens on CWQ) are directly measured and reported with specific baselines
- **Medium confidence:** Small LLM performance claims (Llama3.2-3B achieving 62.9% on CWQ) are supported by comparative tables but lack ablation studies showing ranker/retriever contributions separately
- **Low confidence:** Sufficiency judgment reliability across diverse question types—only aggregate accuracy reported without breakdown by question complexity or domain

## Next Checks

1. **Sufficiency judgment calibration:** Run the judgment module on 100 manually annotated questions (labeled sufficient/insufficient based on top-K paths) to measure false positive and false negative rates separately, then compute the break-even point where efficiency gains are lost
2. **Ranker ablation study:** Compare RJE performance with and without the ranker module using identical K values to isolate the contribution of path ranking versus the LLM-based sufficiency judgment
3. **Relation recall profiling:** For each question requiring exploration, measure what percentage of necessary relations appear in the top-N filtered set versus the full candidate pool to quantify retriever-assistance effectiveness