---
ver: rpa2
title: Leveraging Intermediate Representations of Time Series Foundation Models for
  Anomaly Detection
arxiv_id: '2509.12650'
source_url: https://arxiv.org/abs/2509.12650
tags:
- anomaly
- time
- series
- detection
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of detecting anomalies in time
  series data using foundation models, specifically focusing on leveraging intermediate
  layer representations rather than final layer outputs. The proposed method, TimeRep,
  directly utilizes intermediate layer representations from a pre-trained time series
  foundation model (MOMENT-Large) to compute anomaly scores as distances between these
  representations and a reference collection, avoiding the need for task-specific
  heads or dataset-level fine-tuning.
---

# Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection

## Quick Facts
- arXiv ID: 2509.12650
- Source URL: https://arxiv.org/abs/2509.12650
- Authors: Chan Sik Han; Keon Myung Lee
- Reference count: 9
- Primary result: TimeRep achieves 77.6% Top-1 accuracy on UCR Anomaly Archive, outperforming all baselines

## Executive Summary
This paper introduces TimeRep, a novel approach for anomaly detection in univariate time series that leverages intermediate layer representations from a pre-trained time series foundation model (MOMENT-Large) rather than final-layer outputs. Instead of fine-tuning or using task-specific heads, TimeRep extracts embeddings from Layer 16 of the frozen backbone and computes anomaly scores based on distances to a compressed memory bank of normal patterns. The method includes test-time adaptation to handle concept drift by dynamically updating the memory bank. Experiments on 250 UCR Anomaly Archive datasets demonstrate consistent superiority over state-of-the-art methods, achieving a Top-1 accuracy of 77.6%.

## Method Summary
TimeRep uses a pre-trained MOMENT-Large model (frozen) to extract intermediate representations from Layer 16 for sliding windows of length 512 from univariate time series. These representations are stored in a compressed memory bank (core-set of 1k-10k samples) using greedy k-Center selection. Anomaly scores are computed as Euclidean distances to the nearest neighbor in this bank. The method includes Test-Time Adaptive Memory Bank (TTAMB) that dynamically adds non-redundant representations when test distances exceed the 80th percentile of training distances, enabling adaptation to concept drift without fine-tuning.

## Key Results
- Achieves 77.6% Top-1 accuracy on UCR Anomaly Archive, outperforming existing methods
- Layer 16 representations yield 70.8% accuracy vs 60% for final layer outputs
- TTAMB adaptation improves accuracy from 75.6% to 77.6%
- Core-set compression (1k-10k) maintains accuracy with minimal loss

## Why This Works (Mechanism)

### Mechanism 1
Intermediate layer representations (Layer 16) from TSFM capture generalizable temporal features more effectively than final-layer representations for anomaly detection. Final layers overfit to reconstruction objectives while intermediate layers encode abstract temporal structures semantically richer for distinguishing normal vs abnormal patterns. The pre-training dataset diversity enables universal applicability without fine-tuning.

### Mechanism 2
Anomaly detection is framed as distance-based search against a compressed memory bank of normal representations. Normal patterns form dense clusters in latent space while anomalies deviate spatially. Core-set selection preserves geometric shape of normality distribution efficiently without storing every training example.

### Mechanism 3
Test-time adaptation to concept drift is achieved by dynamically expanding memory bank with non-redundant samples. As new data arrives, representations sufficiently distant from current bank are added, updating normality definition without altering frozen backbone weights. Assumes stream contains mostly normal data so novel points represent legitimate distribution shifts.

## Foundational Learning

- **Core-set Selection (k-Center)**: Reduces memory bank size while preserving distribution shape. Quick check: Why does minimizing maximum distance to nearest center preserve data distribution geometry?
- **Transformer Patching**: TSFM processes time series in patches rather than point-by-point. Quick check: How does center patch extraction differ temporally from last patch extraction?
- **Concept Drift**: Addresses model degradation when statistical properties change over time. Quick check: Why does instance-based adaptation handle drift better than frozen weights?

## Architecture Onboarding

- **Component map**: MOMENT-Large (frozen) -> Layer 16 extraction -> Memory Bank (Faiss/index) -> TTAMB adapter -> Anomaly score
- **Critical path**: Selection of Layer Index and Patch Position. Layer 16 and center patch empirically optimal; deviation results in >10% performance drop.
- **Design tradeoffs**: Memory Size vs Speed (1k fast, 2% loss; 10k accurate, slower); Distance Metric (Euclidean safer than Mahalanobis due to singular matrix risk).
- **Failure signatures**: Singular matrix with Mahalanobis distance; poisoned memory if novelty threshold too low during anomaly storms.
- **First 3 experiments**: Layer ablation sweep (1-24) to confirm Layer 16 optimum; core-set size testing (1k-10k) for latency-accuracy tradeoff; TTAMB simulation with train/test split to verify drift adaptation gains.

## Open Questions the Paper Calls Out

1. Can TimeRep be effectively extended to multivariate time series settings, and how would intermediate representation extraction and memory bank construction adapt to handle cross-channel dependencies? (Explicit)

2. How does choice of time series foundation model architecture (decoder-only vs encoder-only) impact quality and layer-wise localization of intermediate representations for anomaly detection? (Explicit)

3. Can optimal intermediate layer and patch-token position be determined a priori or via automated heuristic rather than exhaustive empirical search? (Inferred)

4. Is anomaly detection performance robust to input window lengths deviating significantly from foundation model's pretraining context window (512)? (Inferred)

## Limitations

- Performance heavily depends on quality and diversity of pre-training data (Time Series Pile)
- Assumes most streaming data is normal for TTAMB to work effectively
- 100-step tolerance window may be too lenient for applications requiring precise anomaly localization

## Confidence

- **High confidence**: Empirical superiority of Layer 16 representations over final-layer outputs (70.8% vs 60% Top-1 accuracy)
- **Medium confidence**: Effectiveness of core-set compression for memory efficiency with minimal accuracy loss
- **Medium confidence**: TTAMB's improvement from 75.6% to 77.6% Top-1 accuracy under normal data assumption

## Next Checks

1. **Domain shift test**: Evaluate TimeRep on dataset from domain not represented in Time Series Pile to verify universality assumption

2. **Core-set size sensitivity**: Systematically test memory bank sizes from 100 to 50,000 to quantify accuracy-latency tradeoff curve

3. **TTAMB failure analysis**: Intentionally introduce anomaly bursts during adaptation phase to quantify maximum tolerable anomaly frequency