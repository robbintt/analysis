---
ver: rpa2
title: 'N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based
  Matrix Completion'
arxiv_id: '2506.04166'
source_url: https://arxiv.org/abs/2506.04166
tags:
- nearest
- matrix
- methods
- completion
- neighbor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces N\xB2, a unified Python package for nearest\
  \ neighbor-based matrix completion that consolidates various NN algorithms under\
  \ a single modular interface. The authors develop a new AutoNN variant that interpolates\
  \ between two existing methods (TSNN and DRNN) to automatically balance bias and\
  \ variance based on noise levels, achieving state-of-the-art performance across\
  \ multiple real-world datasets."
---

# N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion

## Quick Facts
- arXiv ID: 2506.04166
- Source URL: https://arxiv.org/abs/2506.04166
- Reference count: 40
- This paper introduces N², a unified Python package for nearest neighbor-based matrix completion that consolidates various NN algorithms under a single modular interface, achieving state-of-the-art performance across multiple real-world datasets.

## Executive Summary
This paper introduces N², a unified Python package for nearest neighbor-based matrix completion that consolidates various NN algorithms under a single modular interface. The authors develop a new AutoNN variant that interpolates between two existing methods (TSNN and DRNN) to automatically balance bias and variance based on noise levels, achieving state-of-the-art performance across multiple real-world datasets. The package supports both scalar and distributional settings, with easy extensibility to new data types. The authors also release N²-Bench, a comprehensive benchmark suite spanning healthcare (HeartSteps), recommender systems (MovieLens), causal inference (Proposition 99), and LLM evaluation (PromptEval).

## Method Summary
N² implements a modular framework for nearest neighbor matrix completion using two core operations: DISTANCE (computing row and column distances using entry-wise metrics) and AVERAGE (computing weighted aggregations). The package supports multiple estimation methods including RowNN, ColNN, TSNN, DRNN, AutoNN, and distributional methods using MMD and Wasserstein distances. AutoNN interpolates between TSNN and DRNN to automatically balance bias and variance based on noise levels. The framework uses cross-validation for hyperparameter selection, particularly for the neighborhood threshold parameter η.

## Key Results
- AutoNN achieves state-of-the-art performance across multiple real-world datasets by automatically balancing bias and variance
- Distributional NN methods (KernelNN, W2NN) outperform scalar methods by matching entire distributions rather than just first moments
- While classical methods like USVT and SoftImpute perform well on idealized data, NN-based techniques consistently outperform them in real-world settings
- The N² package provides a unified interface for diverse NN methods with easy extensibility to new data types

## Why This Works (Mechanism)

### Mechanism 1
The DISTANCE and AVERAGE modular framework enables systematic composition of NN variants by decoupling metric computation from aggregation logic. DISTANCE computes row-wise and column-wise distances using entry-wise metrics, while AVERAGE computes weighted aggregations using configurable weights that encode neighborhood membership. Different NN variants emerge from combining these two modules with different metrics and weight specifications.

### Mechanism 2
AutoNN improves robustness by interpolating between TSNN (variance-reducing) and DRNN (bias-reducing) based on implicit noise-level adaptation. TSNN averages over larger neighborhoods, reducing variance when noise is high, while DRNN debiases via RowNN + ColNN − TSNN, reducing bias when noise is low. AutoNN computes α · DRNN + (1−α) · TSNN, where α implicitly adapts to noise levels.

### Mechanism 3
Distributional NN methods outperform scalar methods by matching full distributions rather than first moments, capturing structural similarity in the data. KernelNN uses MMD² and W2NN uses Wasserstein-2 distance to compare distributions, capturing higher-order distributional properties like multimodality and spread, enabling more homogeneous neighbor selection.

## Foundational Learning

- **Matrix completion with missingness patterns (MCAR, MAR, MNAR)**: Understanding missingness patterns is crucial because NN methods' theoretical guarantees depend on the type. TSNN works under MNAR (unobserved confounding), while vanilla RowNN requires MAR or MCAR. Quick check: If treatment assignment depends on unobserved unit characteristics, which missingness pattern applies and which NN variant should you prefer?

- **Bias-variance tradeoff in nearest neighbor estimation**: AutoNN's design directly exploits this tradeoff. Larger neighborhoods reduce variance but increase bias; debiasing (DRNN) reduces bias but amplifies variance. Quick check: In a high-noise setting (σ large), would you expect TSNN or DRNN to perform better, and why?

- **Distributional metrics (MMD, Wasserstein distance) and barycenters**: Distributional NN methods require understanding how to measure distance between distributions and compute their "averages." MMD uses kernel embeddings; W₂ uses optimal transport. Quick check: If your data distributions are multimodal, which metric (MMD or W₂) would better preserve this structure during imputation?

## Architecture Onboarding

- **Component map**: DataType (abstract) -> DISTANCE + AVERAGE -> EstimationMethod (abstract) -> NearestNeighborImputer (composite)
- **Critical path**: 1) Instantiate DataType with your data format (scalar vs. distributional) 2) Choose EstimationMethod based on missingness pattern and noise level 3) Create NearestNeighborImputer(DataType, EstimationMethod) 4) Input: data matrix Z, mask matrix A, target indices (i, t) 5) Run cross-validation to find optimal η (except for AWNN which has no η)
- **Design tradeoffs**: Scalar vs. Distributional (distributional requires n≥2 samples but captures richer structure); TSNN vs. DRNN vs. AutoNN (TSNN for noisy data, DRNN for clean data, AutoNN when noise unknown); RowNN/ColNN vs. Two-sided (Two-sided uses more data but assumes consistent factor structure); Pre-specified η vs. AWNN (AWNN avoids CV but requires solving convex optimization)
- **Failure signatures**: DRNN produces nonsensical results with distributional data (subtraction undefined); all methods fail when propensity min(i,t) p_{i,t} = 0 (positivity violated); distributional methods fail with n=1 (insufficient samples); USVT/SoftImpute outperform NN on idealized synthetic data
- **First 3 experiments**: 1) Baseline comparison on MovieLens subset: Run RowNN, ColNN, TSNN, DRNN, AutoNN, USVT, SoftImpute on 500 held-out ratings 2) Distributional vs. scalar on HeartSteps: Compare KernelNN and W2NN against scalar methods using n=60 samples per entry 3) AutoNN noise adaptation test: Generate synthetic data with varying σ (0.001 vs. 1.0) to verify α adapts

## Open Questions the Paper Calls Out

1. What automatic mechanism should be used to select the interpolation parameter α in AutoNN, and can it be theoretically justified? The paper introduces AutoNN but provides no algorithm for selecting α, relying on cross-validation in experiments.

2. Can doubly robust nearest neighbor methods (e.g., DRNN) be extended to distributional matrix completion settings where subtraction is not well-defined? DRNN relies on additive debiasing, but distribution spaces lack a natural subtraction operation compatible with the DRNN formula.

3. How can N² scale to very large matrices through approximate nearest neighbor methods or distributed computation? Current implementation computes exact pairwise distances, which is O(NT) per entry, and the package does not yet support approximate NN or distributed backends.

## Limitations

- The distributional methods' superiority claims are limited to single dataset validation on HeartSteps
- The claim that N²-Bench provides "comprehensive" coverage is limited by the small number of datasets (4) and relatively small matrix dimensions
- The package's extensibility to new data types is demonstrated but not rigorously validated beyond the scalar and distributional cases presented

## Confidence

- **High confidence**: The modular DISTANCE + AVERAGE framework design and its implementation as N² package
- **Medium confidence**: AutoNN's theoretical advantage and its performance claims, pending broader empirical validation
- **Medium confidence**: Distributional methods' superiority claims, limited to single dataset validation
- **Medium confidence**: Benchmark comprehensiveness claims, given small dataset diversity

## Next Checks

1. Validate AutoNN's noise-adaptation mechanism by testing on synthetic data across a wider range of noise levels (σ ∈ [0.001, 10]) and missingness patterns to confirm the theoretical bias-variance tradeoff holds empirically

2. Test distributional methods on additional real-world datasets with known multimodal structure (e.g., healthcare, finance) to verify KernelNN's advantage over W2NN in capturing bimodality generalizes beyond HeartSteps

3. Benchmark N² methods on a larger-scale recommender system dataset (e.g., Netflix Prize) to assess scalability and performance relative to industrial baselines