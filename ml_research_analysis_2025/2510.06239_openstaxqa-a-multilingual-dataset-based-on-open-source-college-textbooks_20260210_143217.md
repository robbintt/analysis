---
ver: rpa2
title: 'OpenStaxQA: A multilingual dataset based on open-source college textbooks'
arxiv_id: '2510.06239'
source_url: https://arxiv.org/abs/2510.06239
tags:
- dataset
- openstaxqa
- such
- content
- textbooks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OpenStaxQA, a multilingual dataset derived
  from 43 open-source college textbooks in English, Spanish, and Polish. The dataset
  contains 18,332 problem-solution pairs scraped from OpenStax textbooks using Beautiful
  Soup, with MathML content converted to LaTeX.
---

# OpenStaxQA: A multilingual dataset based on open-source college textbooks

## Quick Facts
- arXiv ID: 2510.06239
- Source URL: https://arxiv.org/abs/2510.06239
- Reference count: 7
- Primary result: Dataset of 18,332 multilingual college-level problem-solution pairs scraped from OpenStax textbooks

## Executive Summary
This paper introduces OpenStaxQA, a multilingual dataset derived from 43 open-source college textbooks in English, Spanish, and Polish. The dataset contains 18,332 problem-solution pairs scraped from OpenStax textbooks using Beautiful Soup, with MathML content converted to LaTeX. The author finetuned Llama2-7B and Llemma-7B models using QLoRa adapters and evaluated their performance on OpenStaxQA and AI2RC "challenge dev" datasets. Finetuned models achieved higher ratings on both datasets compared to the untrained Llama2-7B, with Llemma-7B showing better performance on OpenStaxQA. The study highlights the potential of using open-source textbooks for training educational language models and discusses the need for standardized tools to parse complex textbook content.

## Method Summary
The author scraped 43 OpenStax textbooks in three languages, extracting problem-solution pairs using Beautiful Soup to parse HTML tags `<os-problem-container>` and `<os-solution-container>`. MathML content was converted to LaTeX using the texmath Haskell library, and remaining HTML was processed with html2text. The dataset was deduplicated and split 70:30 for training and testing. QLoRa adapters were used to finetune Llama2-7B and Llemma-7B models for three epochs with dropout 0.1. Models were evaluated using GPT-4 as an oracle rater on a 5-point scale, with paired t-tests measuring significance.

## Key Results
- Finetuned Llama2-7B achieved mean rating 1.73 on AI2RC vs. 1.32 for untrained (p=1.96×10⁻⁴)
- On OpenStaxQA test set, finetuned Llama2-7B scored 1.41 vs. 1.10 baseline (p=7.38×10⁻⁹⁰)
- Llemma-7B finetuned scored 1.47 on OpenStaxQA, outperforming Llama2-7B finetuned

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning on structured college-level STEM problem-solution pairs improves model performance on related reasoning benchmarks, even in zero-shot transfer scenarios.
- **Mechanism:** QLoRA adapters inject domain-specific patterns (quantitative reasoning structures, scientific notation conventions, problem-solving templates) into frozen base weights, enabling the model to recognize and apply STEM reasoning patterns. The transfer to AI2RC occurs because grade-school science questions share structural similarity with college-level exercises, though at lower complexity.
- **Core assumption:** STEM reasoning patterns are hierarchical—college-level training subsumes simpler problem structures.
- **Evidence anchors:**
  - Finetuned Llama2-7B achieved mean rating 1.73 on AI2RC vs. 1.32 for untrained (p=1.96×10⁻⁴); finetuned models outperformed baseline despite AI2RC being zero-shot.
  - On OpenStaxQA test set, finetuned Llama2-7B scored 1.41 vs. 1.10 baseline (p=7.38×10⁻⁹⁰); Llemma-7B finetuned scored 1.47.
  - MatSciBench (arXiv:2510.12171) shows similar domain-specific benchmark improving college-level materials science reasoning, suggesting this is a generalizable pattern.
- **Break condition:** Transfer fails when target tasks require fundamentally different reasoning modalities (e.g., creative writing, subjective interpretation) or when source data quality is compromised (residual image dependencies, broken hyperlinks).

### Mechanism 2
- **Claim:** Consistent semantic HTML markup in textbook platforms enables reliable automated extraction of paired training data at scale.
- **Mechanism:** OpenStax's use of standardized container tags (`<os-problem-container>`, `<os-solution-container>`) allows Beautiful Soup to parse problem-solution pairs with minimal post-processing. Removing non-semantic tags (`<div>`, `<span>`) preserves content while eliminating noise, creating clean supervision signals.
- **Core assumption:** The source platform maintains consistent tagging conventions across all textbooks and languages.
- **Evidence anchors:**
  - "For the 43 textbooks we scraped in 3 languages in varied disciplines, the HTML content was consistent and well-organized into specific tags."
  - "Different open-source textbook platforms have different formats, and in many cases, redundant and erroneous HTML content, insufficiently tagged textbook content, and non-standard organization of chapters makes it hard... to utilize a textbook."
  - No corpus papers directly address HTML-to-dataset pipelines for textbooks.
- **Break condition:** Extraction fails when platforms use irregular markup, PDF-only distribution, or dynamically rendered content without semantic tags.

### Mechanism 3
- **Claim:** Converting MathML to LaTeX representation reduces token count per problem, improving training and inference efficiency without semantic loss.
- **Mechanism:** MathML's verbose XML structure (opening/closing tags, attribute encoding) expands token sequences compared to LaTeX's compact macro notation. The texmath Haskell library provides reliable semantic-preserving conversion, enabling models to process mathematical content within smaller context windows.
- **Core assumption:** The conversion library accurately preserves all mathematical semantics; no information is lost in translation.
- **Evidence anchors:**
  - "This can help us reduce the number of tokens per problem and speed up training and inference."
  - "We evaluated XSL, Javascript, and Haskell based libraries for this purpose, and the Haskell library texmath was found to be the most reliable."
  - Finetuned Llemma performed worse than finetuned Llama2 on AI2RC (1.53 vs. 1.73), "maybe because AI2RC questions do not have significant LaTeX content"—suggesting format specialization effects.
  - No corpus papers directly validate MathML-to-LaTeX efficiency claims.
- **Break condition:** Complex nested expressions, custom notation, or malformed MathML may convert incorrectly, introducing noise into training data.

## Foundational Learning

- **Concept:** QLoRA (Quantized Low-Rank Adaptation)
  - **Why needed here:** The paper fine-tunes 7B-parameter models using QLoRA adapters, requiring understanding of how low-rank decomposition enables memory-efficient training.
  - **Quick check question:** Can you explain why adding adapter matrices with rank r ≪ d (where d is hidden dimension) reduces trainable parameters while preserving expressivity?

- **Concept:** MathML vs. LaTeX mathematical representation
  - **Why needed here:** The pipeline converts MathML (XML-based, verbose) to LaTeX (macro-based, compact), and understanding the tradeoffs informs debugging conversion errors.
  - **Quick check question:** Given the equation $E = mc^2$, how would tokenization differ between MathML representation and LaTeX source?

- **Concept:** LLM-as-judge evaluation methodology
  - **Why needed here:** GPT-4 serves as the evaluation oracle on a 5-point scale; understanding its biases (extreme ratings, training data overlap) is critical for interpreting results.
  - **Quick check question:** What systematic biases might GPT-4 exhibit when rating solutions from models trained on data potentially overlapping with GPT-4's pre-training corpus?

## Architecture Onboarding

**Component map:**
OpenStax textbooks → Beautiful Soup scraper → [<os-problem-container>, <os-solution-container> extraction] → [Tag stripping: <div>, <span> removal] → [MathML → LaTeX via texmath (Haskell)] → [html2text for remaining HTML] → [Deduplication across textbook editions] → [18,332 problem-solution pairs: EN/ES/PL, 5 domains] → [70:30 train-test split → QLoRA fine-tuning] → [Llama2-7B-hf or Llemma-7B base, 3 epochs, dropout 0.1] → [GPT-4 oracle evaluation: 5-point rating scale]

**Critical path:**
1. Verify target textbooks use semantic container tags (not all platforms do)
2. Run texmath validation on sampled MathML expressions before full conversion
3. Check deduplication logic—overlapping editions (e.g., Chemistry 2e vs. Chemistry 2e: atoms first) require exact matching
4. Monitor GPT-4 rating distribution during evaluation for extreme-rating bias

**Design tradeoffs:**
- **Beautiful Soup vs. Scrapy:** Simpler library chosen because parallel processing wasn't needed for 43 textbooks. Scrapy would be necessary for 1000+ books or rate-limited sites.
- **GPT-4 vs. human evaluation:** Cost and scalability won over granularity; accept the bias toward extreme ratings ("fully accurate"/"fully inaccurate") as noted in Fig. 4-5.
- **Llemma vs. Llama2:** Choose Llemma for math-heavy domains; choose Llama2 for tasks without LaTeX notation.

**Failure signatures:**
- Low mean ratings on seemingly simple test questions → check for image dependencies or cross-chapter hyperlinks in source data
- GPT-4 ratings cluster at extremes only → expected behavior per Fig. 4-5; consider calibrating with human annotations
- Poor Spanish/Polish performance → dataset imbalance (Fig. 2 shows English dominance); collect more multilingual textbooks
- Llemma underperforms Llama2 on non-math benchmarks → expected; Llemma is specialized for mathematical content

**First 3 experiments:**
1. **Reproduce baseline comparison:** Fine-tune Llama2-7B-hf with identical hyperparameters (3 epochs, 32 layers, 0.1 dropout) on OpenStaxQA train split; expect ~1.4 mean rating vs. ~1.1 baseline on test set. Verify paired t-test significance.
2. **Validate transfer learning:** Take the fine-tuned checkpoint, run zero-shot inference on AI2RC "challenge dev" (50-question subset); expect improved rating over untrained model (~1.7 vs. ~1.3). Document any domain-specific failure modes.
3. **Ablate MathML conversion:** Create a small subset (500 pairs) with raw MathML preserved; train separate adapter and compare token efficiency (tokens/problem) and accuracy. If LaTeX conversion introduces errors, this will surface them.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the web scraping and model training pipeline be effectively generalized to other open-source platforms with less standardized markup, such as OER Commons or Open Textbook Library?
- **Basis in paper:** The authors state in Section 6, "We plan to take up these directions as an extension to this work, by making use of other open source textbook platforms such as OER Commons and Open Textbook Library."
- **Why unresolved:** The current study relied on OpenStax's consistent HTML tags (e.g., `os-problem-container`), whereas other platforms may have "redundant and erroneous HTML content" or "non-standard organization."
- **What evidence would resolve it:** Successful extraction of high-quality problem-solution pairs and subsequent model finetuning results using data scraped from OER Commons or Open Textbook Library.

### Open Question 2
- **Question:** To what extent does residual data noise (e.g., hyperlinks, missing image context) versus inherent question difficulty contribute to the models' lower performance on OpenStaxQA compared to the zero-shot AI2RC evaluation?
- **Basis in paper:** Section 5.3 notes that mean ratings were lower on the trained OpenStaxQA set than the zero-shot AI2RC set, hypothesizing that "issues with data quality" such as "residual problem-solution pairs that depend on image data" may be responsible.
- **Why unresolved:** The paper identifies the performance gap and potential data quality issues but does not isolate the specific impact of incomplete context (images/links) on model accuracy.
- **What evidence would resolve it:** An ablation study where OpenStaxQA entries dependent on missing images or hyperlinks are removed or corrected, followed by a re-evaluation of model performance.

### Open Question 3
- **Question:** Does GPT-4's tendency to produce polarized ratings (fully accurate/inaccurate) rather than nuanced scores result from limitations in multi-class classification or bias in its pre-training data?
- **Basis in paper:** Section 5.3 observes that GPT-4 predictions tend to be on the extremes and lists "bias in GPT-4’s pre-training data" or "limited ability to do accurate multi-class classification" as possible reasons.
- **Why unresolved:** The paper uses GPT-4 as an evaluation oracle but highlights a specific anomaly in its rating distribution without determining the root cause.
- **What evidence would resolve it:** A comparative study correlating GPT-4's ratings with human expert evaluations to determine if the lack of intermediate scores reflects true model performance or evaluator bias.

## Limitations
- Dataset composition heavily skewed toward English (74%) and STEM fields, with only 6,241 Spanish and 784 Polish pairs
- GPT-4 serves as both training data source (potential overlap) and evaluation oracle, creating potential bias
- Entire extraction pipeline depends on OpenStax's specific HTML tagging conventions, limiting generalizability

## Confidence
- **High confidence:** The core mechanism that structured problem-solution pairs from textbooks improve model performance on reasoning benchmarks is well-supported by paired t-test results (p-values < 0.05) and clear performance improvements on both OpenStaxQA and AI2RC datasets.
- **Medium confidence:** The MathML-to-LaTeX conversion efficiency claim is plausible but lacks direct corpus validation. The evidence is primarily theoretical (token count reduction) rather than empirical measurement of training speed improvements.
- **Low confidence:** The transfer learning effectiveness across languages (Spanish/Polish) is questionable given the severe dataset imbalance and lack of performance reporting for non-English finetuning experiments.

## Next Checks
1. **Language generalization validation:** Create balanced multilingual subsets (equal English/Spanish/Polish pairs) and retrain adapters to measure performance differences. This would quantify whether current results are biased by English dominance.
2. **Human evaluation calibration:** Have human annotators rate 100 randomly selected model outputs from both finetuned and baseline models, comparing human vs. GPT-4 ratings to identify systematic biases in the automated evaluation pipeline.
3. **Cross-platform extraction robustness:** Test the Beautiful Soup pipeline on 2-3 other open-source textbook platforms with different HTML structures to measure extraction success rates and identify necessary modifications for generalization.