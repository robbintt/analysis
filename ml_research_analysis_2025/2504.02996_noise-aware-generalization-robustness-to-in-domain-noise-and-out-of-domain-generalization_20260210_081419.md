---
ver: rpa2
title: 'Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain
  Generalization'
arxiv_id: '2504.02996'
source_url: https://arxiv.org/abs/2504.02996
tags:
- noise
- domain
- noisy
- label
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training models that can handle
  both label noise within domains and generalize across domains (Noise-Aware Generalization,
  NAG). Existing methods for domain generalization or learning with noisy labels often
  fail when both issues are present simultaneously, as they cannot effectively distinguish
  between noisy labels and samples from different domains.
---

# Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization

## Quick Facts
- arXiv ID: 2504.02996
- Source URL: https://arxiv.org/abs/2504.02996
- Authors: Siqi Wang; Aoming Liu; Bryan A. Plummer
- Reference count: 40
- Primary result: DL4ND improves both in-domain accuracy under noise and out-of-domain generalization, outperforming naive combinations of existing methods

## Executive Summary
This paper addresses the problem of training models that can handle both label noise within domains and generalize across domains (Noise-Aware Generalization, NAG). Existing methods for domain generalization or learning with noisy labels often fail when both issues are present simultaneously, as they cannot effectively distinguish between noisy labels and samples from different domains. The authors propose DL4ND, a method that leverages cross-domain comparisons to detect label noise. The key insight is that noisy samples, which may appear similar within a single domain due to spurious features, exhibit larger distances when compared across domains because they lack the invariant features of the true class. DL4ND uses low-loss samples as proxies for comparison and relabels high-loss samples based on cross-domain distances. The method is integrated with domain generalization techniques. Experiments on real-world datasets (VLCS, CHAMMI-CP) and synthetic noisy datasets show that DL4ND significantly improves both in-domain accuracy under noise and out-of-domain generalization, outperforming naive combinations of existing methods.

## Method Summary
The proposed DL4ND method integrates cross-domain distance comparison into standard DG training. First, a featurizer is trained briefly, then low-loss samples are identified using Gaussian Mixture Modeling on the loss distribution. These low-loss samples serve as proxies for each (class, domain) pair. High-loss samples are then relabeled by finding the class whose proxy from a *different* domain yields the minimum cross-domain distance. This relabeled dataset is used to continue training with standard DG techniques like MIRO, SWAD, or SAGM. The method assumes that noisy samples, dominated by spurious features, show greater variation across domains than clean samples, which share invariant class features.

## Key Results
- DL4ND significantly improves both in-domain accuracy under noise and out-of-domain generalization on real-world datasets (VLCS, CHAMMI-CP)
- Outperforms naive combinations of existing domain generalization and learning with noisy labels methods
- Shows effectiveness on synthetic noisy datasets including OfficeHome and TerraIncognita with controlled noise levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-domain feature comparisons can distinguish label noise from domain shifts.
- **Mechanism:** Noisy samples dominated by spurious features appear similar within a single domain but diverge across domains. Clean samples share intrinsic class features that persist across domains, yielding smaller cross-domain distances. By comparing a sample's distance to class proxies from *other* domains, the method identifies whether it belongs to the labeled class.
- **Core assumption:** A featurizer exists such that within-class cross-domain distances are smaller than within-domain cross-class distances (Theorem 1, Eq. 4).
- **Evidence anchors:**
  - [Abstract] "noisy samples that may appear indistinguishable within a single domain often show greater variation when compared across domains"
  - [Section 4.1.1] "Cross-domain comparisons help make it easier to differentiate between class and domain shifts"
  - [Corpus] Weak direct support; related work on robustness to noise and distribution shifts exists but does not validate this specific cross-domain noise detection hypothesis.
- **Break condition:** If noisy samples share strong spurious features *across domains* (e.g., watermark artifacts present in all domains), cross-domain distances may not separate noise from clean samples.

### Mechanism 2
- **Claim:** Low-loss samples in early training provide reliable class-domain proxies before overfitting to noise.
- **Mechanism:** Models learn "easy" (clean) samples before memorizing noise. Selecting low-loss samples via Gaussian mixture modeling on the loss distribution yields a cleaner subset whose averaged features form (class, domain) proxies for comparison.
- **Core assumption:** Early training dynamics follow an "easy-to-hard" learning pattern where clean samples have lower loss before noise memorization.
- **Evidence anchors:**
  - [Section 4.1.2] "Prior work [58] has shown that when training with noisy labels, models tend to learn easy samples first before gradually overfitting to noise"
  - [Section 4.2] "Samples belonging to the low-loss cluster serve as proxies, while high-loss samples require label updates"
  - [Corpus] Not directly validated in neighbors; related work on noise robustness exists but not this specific proxy construction.
- **Break condition:** If noise ratio is extremely high (>50%) or noise is symmetrically distributed, low-loss samples may still contain substantial noise, corrupting proxies.

### Mechanism 3
- **Claim:** Naive LNL+DG combinations underperform because LNL sample selection skews domain distributions.
- **Mechanism:** LNL methods select "clean" samples based on in-domain heuristics (loss, similarity), which can disproportionately drop samples from noisier domains. This imbalance hurts DG methods that assume balanced multi-domain training.
- **Core assumption:** Domain balance matters for DG generalization, and LNL selection ignores this.
- **Evidence anchors:**
  - [Section 5.3.1] "VOC2007 domain initially has 60% of the 'car' class samples... after sample selection, VOC2007 contains only 20% of the samples relative to LabelMe"
  - [Section 5.3.2] "Quality outweighs quantity in enhancing robustness"
  - [Corpus] No direct validation; related work on domain adaptation with noisy labels exists but doesn't analyze this distribution skew.
- **Break condition:** If domains have equal noise ratios or LNL selection preserves domain balance, naive combinations may perform adequately.

## Foundational Learning

- **Concept: Domain Generalization (DG)**
  - **Why needed here:** NAG extends DG by adding label noise; understanding DG's goal (learning domain-invariant features) is prerequisite.
  - **Quick check question:** Can you explain why a model that minimizes training error on multiple source domains might fail on an unseen target domain?

- **Concept: Learning with Noisy Labels (LNL)**
  - **Why needed here:** LNL provides the noise-robustness toolkit; understanding sample selection vs. loss correction approaches is essential.
  - **Quick check question:** Why do loss-based noise detection methods fail when domain shifts are present?

- **Concept: Early Learning Regularization / Easy-First Hypothesis**
  - **Why needed here:** DL4ND relies on low-loss early samples as clean proxies; understanding this training dynamic is critical.
  - **Quick check question:** What happens to the loss distribution of clean vs. noisy samples as training progresses?

## Architecture Onboarding

- **Component map:**
  Featurizer fθ -> Loss-based GMM splitter -> (Class, Domain) Proxy Generator -> Cross-Domain Distance Computer -> Label Updater -> DG Training Loop

- **Critical path:**
  1. Train initial featurizer for warmup (before overfitting to noise).
  2. At label-update step, collect losses → GMM split → build proxies → relabel high-loss samples.
  3. Resume DG training with updated labels.

- **Design tradeoffs:**
  - **When to apply label update:** Too early → proxies unreliable; too late → overfitting to noise. Paper uses model stabilization point.
  - **Proxy granularity:** (class, domain) vs. (class only). Domain-specific proxies capture domain-specific feature distributions but require sufficient samples per (class, domain).
  - **One-shot vs. iterative relabeling:** Paper does single update for simplicity; iterative may improve but risks error propagation.

- **Failure signatures:**
  - High noise ratio (>50%) with insufficient low-loss samples → proxy contamination.
  - Domains with very different feature scales → distance comparisons biased toward high-variance domains.
  - Extremely similar classes (fine-grained) → cross-domain distances overlap, breaking separability assumption.

- **First 3 experiments:**
  1. **Reproduce RotatedMNIST toy experiment** (Section 3.2): Verify label accuracy improves from ~75% to ~98% with DL4ND; confirm distance distributions separate (Fig. 3).
  2. **Ablation on label-update timing:** Test early vs. mid-training updates on VLCS; measure ID/OOD accuracy and label accuracy over time.
  3. **Proxy granularity test:** Compare (class, domain) vs. (class-only) proxies on OfficeHome with 40% noise; analyze domain balance impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DL4ND degrade when the number of source domains is limited (e.g., $m < 3$) or when a specific class is absent in one or more source domains?
- Basis in paper: [inferred] The method relies on Theorem 1, which defines separability based on feature expectations from *other* domains ($k \neq i$).
- Why unresolved: The paper experiments primarily use datasets with 4 domains (VLCS, OfficeHome, TerraIncognita). The cross-domain comparison logic may fail or become noisy if there are insufficient "other" domains to generate reliable (class, domain) proxies for distance comparisons.
- What evidence would resolve it: Ablation studies on datasets like PACS or VLCS where domains are iteratively removed (training with only 2 or 3 domains) and results are reported for classes with sparse domain coverage.

### Open Question 2
- Question: Can LNL sample selection strategies be explicitly regularized to maintain domain balance, preventing the distribution skew that hampers generalization?
- Basis in paper: [explicit] Section 5.3.1 explicitly states that "LNL noise sample selection skews domain distributions," resulting in models that overfit to the remaining domains while potentially losing cross-domain features necessary for generalization.
- Why unresolved: Current LNL methods optimize purely for label cleanliness. The paper highlights this trade-off but does not propose a solution to select clean samples *while* enforcing a domain-balanced training set.
- What evidence would resolve it: A modified LNL method that penalizes deviations from the original domain ratio during sample selection, evaluated on the NAG benchmarks introduced in the paper.

### Open Question 3
- Question: Does the assumption that "noisy samples show greater variation when compared across domains" hold for symmetric (random) label noise, as opposed to the semantic pair-flip noise tested?
- Basis in paper: [inferred] Section 4.1.1 argues that noisy samples are dominated by spurious features and lack invariant features. While this is true for semantic flips (e.g., "cat" mislabeled as "dog"), random symmetric noise (e.g., "cat" mislabeled as "car") may not exhibit the same cross-domain "visual similarity" confusion that the method exploits.
- Why unresolved: The synthetic experiments (Table 3) only utilize asymmetric noise pairs. It is unclear if the cross-domain distance metric can distinguish random noise from clean samples as effectively.
- What evidence would resolve it: Experimental results on the OfficeHome or TerraIncognita datasets using symmetric noise transitions (random label flips) rather than the class-pair flips detailed in Tables 6 and 7.

## Limitations
- Limited empirical validation across diverse domain shifts and noise types
- Relies on early-learning dynamics that may not hold for extremely high noise ratios or symmetric noise
- Does not propose solutions for maintaining domain balance in LNL sample selection

## Confidence

- **High**: Integration methodology (DL4ND + DG methods), experimental results on VLCS/CHAMMI-CP datasets
- **Medium**: Cross-domain noise detection mechanism, early-learning clean sample identification
- **Low**: Theoretical assumptions about cross-domain distance separability, distribution skew analysis

## Next Checks

1. Test DL4ND on domains with shared spurious features (e.g., datasets with consistent watermark patterns) to verify cross-domain noise detection fails appropriately when assumptions break
2. Vary noise ratios (10%-80%) and domain similarity levels to quantify when early-learning proxy generation becomes unreliable
3. Implement iterative relabeling with error correction mechanisms to measure performance gains and identify error propagation thresholds