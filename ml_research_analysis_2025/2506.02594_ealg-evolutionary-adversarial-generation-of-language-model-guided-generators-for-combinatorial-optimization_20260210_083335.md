---
ver: rpa2
title: 'EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators
  for Combinatorial Optimization'
arxiv_id: '2506.02594'
source_url: https://arxiv.org/abs/2506.02594
tags:
- instances
- instance
- optimization
- problem
- solver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EALG, a framework that co-evolves combinatorial
  optimization problem instances and their solvers using large language models (LLMs)
  through a symbolic adversarial process. Unlike existing approaches that focus solely
  on static benchmark creation or manual solver design, EALG enables fully automated
  generation of increasingly challenging problem instances while simultaneously synthesizing
  adaptive heuristic algorithms.
---

# EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization

## Quick Facts
- arXiv ID: 2506.02594
- Source URL: https://arxiv.org/abs/2506.02594
- Authors: Ruibo Duan; Yuxin Liu; Xinyao Dong; Chenglin Fan
- Reference count: 40
- Key outcome: EALG achieves state-of-the-art performance on TSP and OP benchmarks by co-evolving hard problem instances and adaptive solvers via LLM-guided adversarial process

## Executive Summary
EALG introduces a novel framework that simultaneously evolves combinatorial optimization problem instances and their solvers using large language models through a symbolic adversarial process. The framework addresses the traditional gap between static benchmark creation and manual solver design by enabling fully automated generation of increasingly challenging problem instances while synthesizing adaptive heuristic algorithms. Through iterative interaction with LLMs, EALG employs mutation-based adversarial mechanisms where instance generators evolve to produce harder problems and solvers co-adapt to these challenges. The approach demonstrates significant improvements over existing methods, with solver performance gaps exceeding 9% on standard solvers when evaluated on EALG-generated instances.

## Method Summary
EALG operates through dual evolutionary loops that co-evolve instance generators and heuristic solvers. The framework uses relative optimality gap as the primary metric, where instance generators evolve to maximize this gap while solvers evolve to minimize it. Both components are represented as executable programs that interact with LLMs for synthesis and mutation. The evolutionary process employs behavioral evaluation and evolutionary reflection for symbolic feedback, using a minimax formulation to balance instance hardness and solver performance. The system alternates between synthesizing solvers, synthesizing instance generators, evaluating gaps, and reflecting feedback across generations, with no gradient-based optimization involved.

## Key Results
- EALG generates significantly harder instances than current benchmarks, with solver performance gaps exceeding 9% on standard solvers
- Co-evolved solvers achieve state-of-the-art performance, outperforming leading LLM-based approaches like FunSearch and ReEvo
- EALG demonstrates strong generalization to classical TSPLIB benchmarks while maintaining superior performance on synthetic hard instances
- The framework successfully scales to problem sizes ranging from 20 to 1000 nodes across TSP and OP problem types

## Why This Works (Mechanism)
The framework's effectiveness stems from its adversarial co-evolutionary approach that creates a dynamic feedback loop between problem difficulty and solver capability. By using LLMs to generate both problem instances and solvers as executable programs, EALG can explore the combinatorial space more efficiently than traditional methods. The evolutionary reflection mechanism allows the system to incorporate behavioral feedback directly into the generation process, enabling continuous improvement in both instance hardness and solver quality. This symbiotic relationship ensures that neither component dominates, maintaining a productive tension that drives innovation in both domains.

## Foundational Learning

**Relative Optimality Gap**: Measures instance hardness as the ratio of solver performance to optimal solution quality. Needed to quantify how challenging an instance is for a given solver. Quick check: Compute gap for known easy and hard instances to verify expected ordering.

**Evolutionary Reflection**: Mechanism for incorporating behavioral feedback into program synthesis through LLM interaction. Needed to enable adaptive improvement based on performance metrics. Quick check: Verify that feedback prompts lead to measurable code improvements across generations.

**Minimax Optimization**: Game-theoretic formulation where instance generators maximize gap while solvers minimize it. Needed to formalize the adversarial relationship between components. Quick check: Confirm that alternating optimization produces convergence to Nash equilibrium-like states.

**Executable Program Representation**: Both instances and solvers encoded as programs rather than static data. Needed to enable program synthesis and mutation capabilities. Quick check: Test that generated programs execute correctly and produce valid outputs.

**Behavioral Selection**: Evaluation-based selection mechanism that assesses program performance rather than genetic similarity. Needed to ensure meaningful evolutionary progress. Quick check: Verify that selection pressure correlates with performance improvements.

## Architecture Onboarding

**Component Map**: Instance Generator Programs -> Evaluation Pipeline -> Gap Calculation -> Evolutionary Reflection -> Solver Generator Programs -> Solver Execution -> Performance Feedback

**Critical Path**: The evaluation loop where instance generators produce problems, solvers attempt to solve them, gaps are calculated, and reflection feedback guides the next generation of both components. This path determines the co-evolutionary dynamics and overall system performance.

**Design Tradeoffs**: The framework trades computational efficiency for solution quality, as multiple LLM interactions and evolutionary iterations are required. The symbolic representation enables powerful program synthesis but may produce syntactically invalid code requiring robust error handling. The adversarial setup ensures continuous improvement but requires careful balance to prevent one component from dominating.

**Failure Signatures**: Solver overfitting to synthetic instances (poor TSPLIB generalization), generator collapse (producing unsolvable or trivial instances), LLM generation failures (syntactically invalid code), and evolutionary stagnation (insufficient diversity maintenance). Monitor gap distributions and generalization performance to detect these issues.

**First Experiments**:
1. Implement baseline TSP solver and gap calculation pipeline using Concorde or LKH for reference solutions
2. Create minimal LLM-based code generators for simple instance generators and heuristic solvers, testing code validity and execution
3. Run single co-evolution iteration to verify that evolutionary reflection mechanism functions and produces measurable performance changes

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation details such as specific LLM model, prompt templates, and evolutionary hyperparameters remain undisclosed, making faithful reproduction challenging
- Computational resources required for the evolutionary process are substantial but not quantified, limiting accessibility
- Generalization claims rely on limited TSPLIB instances, and performance on other combinatorial optimization problems remains untested
- The framework's scalability and cost-effectiveness compared to traditional solver development approaches require further validation

## Confidence

**High confidence**: The conceptual framework of adversarial co-evolution between instance generators and solvers via LLMs is well-defined and theoretically sound. The relative optimality gap metric and minimax formulation are clearly specified and appropriate for the problem domain.

**Medium confidence**: Experimental results showing EALG's superiority over FunSearch and ReEvo are compelling but heavily dependent on undisclosed implementation choices. The 9% performance gap claim needs independent verification under controlled conditions.

**Low confidence**: Claims about scalability to 1000-node instances and generalization to TSPLIB without additional tuning are difficult to verify without complete implementation details and computational setup specifications.

## Next Checks

1. Implement a minimal prototype using a fixed LLM (e.g., GPT-4) with standardized prompt templates for both instance generation and solver synthesis, then verify that syntactically valid code is produced and that the evolutionary reflection mechanism functions as described.

2. Conduct ablation studies by removing either the instance generator evolution or solver evolution component to quantify their individual contributions to performance gains, and test on a broader set of combinatorial problems beyond TSP and OP.

3. Establish computational benchmarks by measuring wall-clock time and API costs for different population sizes and generation counts, then compare the cost-effectiveness of EALG against traditional solver development approaches on equivalent problem scales.