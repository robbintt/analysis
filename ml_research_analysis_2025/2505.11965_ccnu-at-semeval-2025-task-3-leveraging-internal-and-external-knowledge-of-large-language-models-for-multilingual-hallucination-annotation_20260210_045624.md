---
ver: rpa2
title: 'CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of
  Large Language Models for Multilingual Hallucination Annotation'
arxiv_id: '2505.11965'
source_url: https://arxiv.org/abs/2505.11965
tags:
- knowledge
- hallucinations
- answer
- system
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes CCNU's approach to SemEval-2025 Task 3 (Mu-SHROOM),
  which focuses on multilingual hallucination detection in question-answering systems.
  The key innovation is leveraging multiple Large Language Models (LLMs) in parallel,
  each assigned different expert roles, to simulate a crowdsourced annotation process.
---

# CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation

## Quick Facts
- arXiv ID: 2505.11965
- Source URL: https://arxiv.org/abs/2505.11965
- Reference count: 4
- Primary result: First place in Hindi data, top five ranking in seven other languages for multilingual hallucination detection

## Executive Summary
This paper presents CCNU's approach to SemEval-2025 Task 3 (Mu-SHROOM), focusing on multilingual hallucination detection in question-answering systems. The key innovation involves using multiple Large Language Models in parallel, each assigned different expert roles, to simulate a crowdsourced annotation process. The system integrates both internal knowledge (through reference answer generation) and external knowledge (via Wikipedia retrieval) without requiring fine-tuning or language-specific optimization. The approach achieved first place for Hindi data and ranked in the top five for seven other languages, demonstrating strong multilingual capabilities while highlighting challenges with Chinese data quality.

## Method Summary
The system leverages DeepSeek-V3 as the backbone model, employing multiple LLMs in parallel where each LLM is assigned a different expert role to simulate crowdsourced annotation. Each LLM combines internal knowledge by generating reference answers from its own knowledge base with external knowledge through Wikipedia retrieval and summarization. The approach requires no fine-tuning or language-specific optimization, making it a general-purpose solution for multilingual hallucination detection. The multi-LLM parallel architecture allows for diverse perspectives on hallucination detection while maintaining computational efficiency through parallel processing.

## Key Results
- Achieved first place ranking for Hindi data in SemEval-2025 Task 3
- Ranked in top five positions across seven other languages
- Demonstrated strong performance without fine-tuning or language-specific optimization
- Ablation studies confirmed that incorporating both internal and external knowledge significantly improves detection accuracy

## Why This Works (Mechanism)
The approach works by simulating a crowdsourced annotation process through multiple LLMs operating in parallel with different expert roles. Each LLM leverages both internal knowledge (its own understanding and reference generation capabilities) and external knowledge (retrieved from Wikipedia), creating a comprehensive knowledge base for hallucination detection. This dual-knowledge approach allows the system to cross-reference information from multiple sources, improving the reliability of hallucination judgments. The expert role assignment ensures diverse perspectives while the parallel architecture enables efficient processing across multiple languages.

## Foundational Learning

- **Large Language Model Knowledge Bases**: Understanding how LLMs store and retrieve internal knowledge is crucial for leveraging their capabilities in hallucination detection. Quick check: Verify that the LLM can accurately answer factual questions about the domain without external context.

- **Wikipedia Retrieval and Summarization**: External knowledge integration requires efficient retrieval of relevant Wikipedia content and summarization capabilities. Quick check: Confirm that retrieved Wikipedia content is topically relevant and comprehensive for the target questions.

- **Multilingual Processing**: The system must handle multiple languages without language-specific optimization, requiring robust cross-lingual understanding. Quick check: Test the system's ability to process and understand questions across all target languages with comparable accuracy.

## Architecture Onboarding

Component Map: Question Input -> Parallel LLM Processing -> Internal Knowledge Generation -> Wikipedia Retrieval -> Knowledge Integration -> Hallucination Detection

Critical Path: The system follows a parallel processing architecture where questions are distributed simultaneously to multiple LLMs, each generating internal reference answers while retrieving and summarizing external Wikipedia content. These knowledge sources are integrated to produce final hallucination detection scores.

Design Tradeoffs: The multi-LLM approach provides diverse perspectives and robust detection but increases computational costs and inference time compared to single-model approaches. The no-fine-tuning strategy offers generalization across languages but may sacrifice some task-specific optimization.

Failure Signatures: Chinese data performance degradation suggests dataset quality issues or language-specific challenges. Wikipedia retrieval failures occur when content is outdated or misaligned with question context. Computational bottlenecks arise from parallel LLM processing overhead.

First Experiments:
1. Test single LLM performance against the multi-LLM approach to quantify the benefit of expert role diversity
2. Evaluate Wikipedia retrieval quality by measuring relevance and comprehensiveness of retrieved content
3. Assess computational cost-benefit ratio between single and multi-LLM processing times

## Open Questions the Paper Calls Out
None

## Limitations
- Chinese data performance significantly underperformed, suggesting potential dataset quality issues or language-specific challenges that weren't fully analyzed
- Multiple LLM parallel processing increases computational costs and inference time, potentially limiting practical deployment scalability
- Dependence on Wikipedia retrieval quality introduces potential errors when retrieved content is outdated or misaligned with question context

## Confidence
- High confidence in overall ranking performance and top positions across multiple languages, empirically validated through official competition results
- Medium confidence in the effectiveness of combining internal and external knowledge, supported by ablation studies but lacking relative contribution quantification
- Medium confidence in the no-fine-tuning advantage, appears robust across languages but untested on out-of-domain data

## Next Checks
1. Conduct detailed error analysis on Chinese data failure to determine whether issues stem from dataset quality, language model limitations, or prompt engineering problems
2. Perform quantitative analysis of computational cost-benefit trade-off between single-model approaches and the multi-LLM parallel system
3. Test the approach on out-of-domain question-answer pairs to evaluate generalization beyond the SemEval task data