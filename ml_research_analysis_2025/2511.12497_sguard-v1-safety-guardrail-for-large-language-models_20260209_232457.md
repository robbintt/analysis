---
ver: rpa2
title: 'SGuard-v1: Safety Guardrail for Large Language Models'
arxiv_id: '2511.12497'
source_url: https://arxiv.org/abs/2511.12497
tags:
- safety
- data
- training
- sguard-v1
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SGuard-v1, a lightweight safety guardrail
  system for Large Language Models (LLMs) consisting of two specialized models: ContentFilter
  and JailbreakFilter. ContentFilter is designed to detect safety risks in LLM prompts
  and responses using a refined five-category taxonomy based on MLCommons, while JailbreakFilter
  defends against adversarial jailbreak attempts across 60 major attack types.'
---

# SGuard-v1: Safety Guardrail for Large Language Models

## Quick Facts
- arXiv ID: 2511.12497
- Source URL: https://arxiv.org/abs/2511.12497
- Authors: JoonHo Lee; HyeonMin Cho; Jaewoong Yun; Hyunjae Lee; JunKyu Lee; Juree Seok
- Reference count: 12
- Key outcome: Introduces SGuard-v1, a lightweight dual-model safety guardrail system achieving state-of-the-art performance on diverse safety benchmarks while reducing GPU memory usage

## Executive Summary
SGuard-v1 is a safety guardrail system for Large Language Models consisting of two specialized models: ContentFilter for detecting harmful content using a refined 5-category taxonomy, and JailbreakFilter for defending against adversarial jailbreak attempts across 60 major attack types. Both models are built on the 2B-parameter Granite-3.3-2B-Instruct model and trained on a curated bilingual dataset of approximately 1.4 million examples in English and Korean. The system employs innovative training techniques including contextual harm translation, benign-harmful contextual blending, and priority switching with noise injection. SGuard-v1 achieves state-of-the-art performance on diverse public and proprietary safety benchmarks while maintaining a lightweight footprint that reduces GPU memory usage.

## Method Summary
SGuard-v1 employs a dual-model architecture with ContentFilter (400K samples, 5-category taxonomy) and JailbreakFilter (1M+ samples, 60 attack types) built on Granite-3.3-2B-Instruct. ContentFilter uses instruction tuning with special tokens for multi-class safety predictions. JailbreakFilter uses two-phase curriculum learning: Phase 1 trains on 1M uncurated samples for broad pattern exposure, then Phase 2 uses 5.4K curated samples with Priority Switching and Noise Injection (PSNI) to prevent overfitting to attack signatures. The system includes Contextual Harm Translation (CHT) for preserving harmful semantics during bilingual dataset expansion, and Benign-Harmful Contextual Blending (BHCB) for data augmentation. Both models are released under Apache-2.0 License for research and practical deployment.

## Key Results
- ContentFilter achieves state-of-the-art F1 score on WildGuardMix benchmark while maintaining low false positive rates
- JailbreakFilter reduces attack success rate (ASR) from 91.3% to 7.9% for Type 3 attacks when combined with ContentFilter
- Dual-model deployment reduces Type 2 attack ASR from 13.8% to 0.2% compared to ContentFilter alone
- Lightweight 2B-parameter models achieve comparable performance to larger 8B-12B baselines while using ~50% less GPU memory

## Why This Works (Mechanism)

### Mechanism 1: Dual-Specialized Architecture with Functional Separation
Separating content safety detection and jailbreak defense into two specialized models provides complementary coverage that outperforms either component alone. ContentFilter learns harm semantics across five categories while JailbreakFilter learns adversarial structural patterns. When deployed jointly, they provide overlapping defense layers that reduce attack success rates significantly compared to single-model approaches.

### Mechanism 2: Curriculum Learning with Priority Switching and Noise Injection (PSNI)
Two-phase curriculum with controlled label noise and alternating priority prompts prevents overfitting to jailbreak signatures while improving boundary decision-making. Phase 1 uses 1M noisy samples for broad pattern exposure, while Phase 2 uses 5.4K curated samples with PSNI that alternates between helpfulness and safety priorities with asymmetric noise injection, forcing the model to evaluate each sample from dual perspectives.

### Mechanism 3: Contextual Harm Translation (CHT) for Bilingual Fidelity
A refiner-based correction pipeline preserves harmful semantics during translation when standard translators refuse or distort sensitive content. Standard translation often produces semantically inverted output for harmful content due to built-in safety alignment. CHT uses a 70B LLM as refiner that compares source against initial translation and generates corrected output when semantic drift is detected, enabling faithful bilingual dataset expansion.

## Foundational Learning

- **Concept: Safety Taxonomy Consolidation**
  - Why needed here: ContentFilter uses a 5-category taxonomy derived from MLCommons' 12 categories. Understanding this mapping is essential for interpreting model outputs, configuring category-specific thresholds, and handling edge cases where categories overlap.
  - Quick check question: If ContentFilter classifies a prompt about unauthorized account access as "Privacy and Sensitive Information Misuse" rather than "Illegal and Criminal Activities," what operational implications does this have for your response policy?

- **Concept: False Positive/Negative Trade-offs in Sequential Guardrails**
  - Why needed here: JailbreakFilter shows FPR=0.01 on benign benchmarks but FPR=0.32 on proprietary tests. ContentFilter has its own FPR profile. When deployed sequentially, false positives compound. Understanding this interaction is critical for setting confidence thresholds.
  - Quick check question: If JailbreakFilter blocks 1% of legitimate queries and ContentFilter blocks 2%, what is the combined false block rate when both are active? How would you measure this in production?

- **Concept: Instruction Tuning for Classification vs. Generation**
  - Why needed here: Both filters are built on Granite-3.3-2B-Instruct via instruction tuning with special tokens. The models output structured predictions, not conversational responses. This differs from typical LLM deployment patterns.
  - Quick check question: When ContentFilter outputs a multi-class prediction with binary confidence, how do you parse this from the model's token stream? What happens if the model generates additional tokens beyond the expected format?

## Architecture Onboarding

- **Component map:**
  User Prompt → JailbreakFilter (2B) → [unsafe/safe + confidence]
  User Prompt → ContentFilter (2B) → [category + unsafe/safe + confidence]
  LLM Response → ContentFilter (2B) → [category + unsafe/safe + confidence]

- **Critical path:**
  1. Load both model weights (Apache-2.0 licensed, available in paper's repository)
  2. Tokenize input with Granite tokenizer (max 8k tokens recommended)
  3. Route through JailbreakFilter first if latency-constrained
  4. If JailbreakFilter returns safe, route through ContentFilter
  5. Parse special token outputs: ContentFilter returns one of 10 tokens (5 categories × safe/unsafe); JailbreakFilter returns binary + confidence
  6. Apply policy logic based on combined signals

- **Design tradeoffs:**
  - Two models vs. unified: Paper shows joint deployment reduces ASR significantly but doubles memory footprint
  - Curriculum depth vs. scale: Phase 1 uses 1M noisy samples; Phase 2 uses only 5.4K curated samples
  - Language coverage: Models explicitly trained for English/Korean with "non-trivial capability" in other languages but no guarantees

- **Failure signatures:**
  - High false positive rates on domain-specific legitimate content
  - Type 3 attacks show 7.9% ASR even with combined filters
  - Inputs >8k tokens may have degraded detection accuracy
  - Novel attack types outside the 60 training categories

- **First 3 experiments:**
  1. Reproduce benchmark metrics: Run ContentFilter on Beavertails and JailbreakFilter on StrongREJECT to validate reported F1/AUPRC/FNR against paper's tables
  2. Threshold sensitivity analysis: Generate FPR-FNR curves by varying confidence thresholds on your own validation set
  3. Sequential vs. parallel routing test: Measure latency and ASR when running filters in parallel versus sequentially

## Open Questions the Paper Calls Out

- How does SGuard-v1 perform on the other 10 languages supported by the base Granite-3.3-2B-Instruct model beyond English and Korean?
- What is the detection accuracy degradation for SGuard-v1 on content near the harmfulness decision boundary compared to clear-cut cases?
- How robust is SGuard-v1 against adversarial prompts that exploit low-resource languages or code-switching between languages?
- What is the optimal trade-off between model size and safety detection performance for guardrail models?

## Limitations

- Training Data Generalization: Heavy reliance on synthetic and curated datasets, particularly the small 5.4K curated samples in Phase 2, may not fully represent evolving adversarial attacks
- Threshold Configuration Ambiguity: No recommended confidence thresholds provided for deployment, critical given 0.32 FPR on proprietary benign datasets
- Model Architecture Scaling: Dual-model approach doubles memory footprint to ~12.7GB without exploring parameter-efficient alternatives or unified models

## Confidence

**High Confidence**: Dual-specialized architecture concept and its demonstrated effectiveness against Type 2 and Type 3 attacks (ASR reduction from 13.8% to 0.2% and 91.3% to 7.9% respectively)

**Medium Confidence**: PSNI training methodology and its effectiveness in preventing overfitting to jailbreak patterns, though evidence base is narrower than dual-architecture claim

**Low Confidence**: CHT methodology for preserving semantic fidelity during translation of harmful content, with only single concrete example provided and no systematic evaluation of reliability

## Next Checks

1. **Dataset Coverage Analysis**: Systematically catalog the 60 jailbreak attack types used in Phase 2 training and map them against recent adversarial attack publications to identify attack types not covered in SGuard-v1 training

2. **Cross-Lingual Robustness Testing**: Evaluate SGuard-v1 performance on low-resource languages (e.g., Vietnamese, Thai, Swahili) beyond the English/Korean focus to validate "non-trivial capability" claims

3. **Threshold Sensitivity and Cost-Benefit Analysis**: Implement parameter sweep across confidence thresholds for both ContentFilter and JailbreakFilter on production-representative benign datasets to calculate trade-off between false blocking rate and safety coverage