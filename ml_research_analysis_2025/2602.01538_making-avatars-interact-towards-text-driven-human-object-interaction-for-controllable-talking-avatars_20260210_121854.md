---
ver: rpa2
title: 'Making Avatars Interact: Towards Text-Driven Human-Object Interaction for
  Controllable Talking Avatars'
arxiv_id: '2602.01538'
source_url: https://arxiv.org/abs/2602.01538
tags:
- video
- motion
- interaction
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating grounded human-object
  interaction (GHOI) videos for controllable talking avatars. Unlike existing audio-driven
  or pose-driven methods, GHOI requires avatars to perform text-aligned interactions
  with surrounding objects within a specific environment.
---

# Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars

## Quick Facts
- arXiv ID: 2602.01538
- Source URL: https://arxiv.org/abs/2602.01538
- Reference count: 40
- One-line primary result: Dual-stream Diffusion Transformer framework generates grounded human-object interaction videos for talking avatars with significant improvements in hand and object quality scores

## Executive Summary
This paper addresses the challenge of generating grounded human-object interaction (GHOI) videos for controllable talking avatars, where avatars must perform text-aligned interactions with objects in specific environments. The core difficulty lies in balancing environmental perception and interaction planning with video quality generation. The authors propose InteractAvatar, a dual-stream Diffusion Transformer framework that explicitly decouples perception and planning from video synthesis. The system includes a Perception and Interaction Module (PIM) for generating scene-aware motion sequences and an Audio-Interaction Aware Generation Module (AIM) for synthesizing high-fidelity videos with precise audio lip-sync. Extensive experiments on the newly constructed GroundedInter benchmark demonstrate significant improvements over state-of-the-art methods in generating plausible, controllable, and high-quality GHOI videos.

## Method Summary
InteractAvatar employs a dual-stream Diffusion Transformer architecture with PIM and AIM modules. PIM generates scene-aware motion sequences (skeletal poses and object bounding boxes) using a modified RoPE position encoding that references the first image at timestep -1. The model is trained with a curriculum alternating between detection tasks, continuation from first frame, and hybrid generation from image and text. AIM synthesizes videos conditioned on audio features and motion residuals via a motion-to-video aligner that uses layer-wise residual injection with zero-initialization. The framework uses three-stage training: PIM pre-training (30K steps), AIM audio pre-training (5K steps), and joint fine-tuning (4K steps). A shared VAE encodes videos, motions, and reference images to latent space, with motions constrained to 256px short side and videos to 704px.

## Key Results
- TA2V variant achieves substantial improvements in Hand Quality (HQ) scores compared to strong baselines
- Object Quality (OQ) scores show ~111% improvement over existing methods
- User studies validate superiority in interaction quality, text alignment, video quality, identity preservation, and audio synchronization
- VLM-QA score of 28.89 demonstrates effective environmental perception and grounding
- Layer-wise residual injection with zero-initialization significantly outperforms alternative alignment strategies

## Why This Works (Mechanism)

### Mechanism 1
Decoupling perception/planning from video synthesis mitigates the control-quality dilemma in complex HOI generation. PIM acts as the "planning brain" generating scene-aware skeletal poses and object bounding boxes, while AIM serves as the "rendering engine" synthesizing photorealistic frames. Parallel co-generation via motion-to-video aligner ensures structural motions guide video synthesis without constraining it. Core assumption: The intermediate motion representation captures sufficient structure for grounded interaction without encoding fine-grained texture that would limit rendering quality. Evidence: VLM-QA drops 2.5 points without detection data; cascade variants show object deformation artifacts. Break condition: If motion representation is too sparse, object deformation artifacts appear.

### Mechanism 2
Joint training on detection and motion generation tasks enables environmental perception for text-aligned interactions. PIM training curriculum alternates between detection task (single-frame motion generation as scene parsing), continuation (motion from first frame), and perception-as-generation (entire sequence from image+text only). Modified RoPE positions reference image at virtual timestep -1. Core assumption: Detection-like objectives force the model to parse scene layouts and object geometries, which transfers to motion planning. Evidence: Detection pre-training is crucial (VLM-QA drops from 28.89 to 26.36 without it). Break condition: Without detection pre-training, VLM-QA drops significantly.

### Mechanism 3
Layer-wise residual injection with zero-initialization enables stable motion-to-video alignment without artifacts. Feature residuals from PIM DiT blocks are upsampled and injected into corresponding AIM blocks via zero-initialized linear layers. This allows gradual learning of motion guidance, preventing skeletal "ghosting" in final video. Core assumption: Isomorphic architectures produce feature-aligned representations suitable for residual injection. Evidence: Layer-wise injection (VLM-QA 28.89) outperforms last-layer-only (28.31) and simple addition (28.46). Break condition: If PIM and AIM diverge architecturally, feature misalignment causes training instability.

## Foundational Learning

- **Diffusion Transformers (DiT) and Flow Matching**: Why needed here: The backbone for both PIM and AIM. Understanding Flow Matching objective is essential for debugging training dynamics and understanding why unified loss works for detection and generation. Quick check question: Can you explain why setting diffusion timestep t=0 enables PIM to act as a feature encoder for external driving signals?

- **Cross-Attention for Multimodal Conditioning**: Why needed here: Text (T5 embeddings) and audio (Wav2Vec features) are injected via cross-attention. The face mask weighting spatially constrains audio influence. Quick check question: Why might "soft" audio conditioning get suppressed if introduced after "hard" motion conditioning?

- **Residual Injection and Zero-Initialization**: Why needed here: The M2V aligner's effectiveness depends on understanding how layer-wise residuals propagate structural information and why zero-init prevents early-training artifacts. Quick check question: What would happen if you removed the bilinear interpolation step before zero-initialized projection?

## Architecture Onboarding

- **Component map**: Reference image + text → PIM → motion latent sequence → layer-wise residuals → M2V aligner → AIM (with audio) → final video latents → VAE decode

- **Critical path**: 1. Ref image + text → PIM → motion latent sequence, 2. Motion latents → layer-wise residuals → M2V aligner, 3. Audio + ref image + motion residuals → AIM → final video latents → VAE decode. Stage order matters: PIM pre-train → AIM audio pre-train → joint fine-tune (audio→motion sequencing prevents audio suppression).

- **Design tradeoffs**: RGB motion representation vs. 2D coordinates (RGB leverages video generation priors, better generalization without large motion datasets, but requires shared VAE alignment); Parallel co-generation vs. cascade (Joint training avoids information bottleneck of frozen PIM, enables richer object shape changes, but requires careful modality sequencing); Short-side 256px (motion) vs. 704px (video) (Motion encodes structure only; lower resolution reduces compute but requires interpolation in M2V aligner).

- **Failure signatures**: Poor text alignment, low interaction quality → Check PIM detection training (ablation shows 2.5 VLM-QA drop without detection data); Object deformation artifacts → Check cascade vs. joint training; frozen PIM causes misalignment with object morphology; Ghosting skeletal lines in video → Check zero-initialization in M2V aligner; without it, motion leaks into texture; Weak lip-sync → Verify audio pre-training occurred before motion exposure (A→AM outperforms AM-only by 0.87 Sync conf); Wrong RoPE mapping → Ref image prepended without RoPE modification causes excessive influence on generated frames.

- **First 3 experiments**: 1. Ablate PIM detection training: Train PIM without detection task, evaluate VLM-QA and OQ on GroundedInter test set. Expect ~2-3 point VLM-QA drop and reduced object grounding. 2. Compare motion representations: Replace RGB motion with 2D (x,y) coordinates, evaluate HOI metrics. Expect generalization drop and alignment issues due to modality gap. 3. Test modality sequencing: Train AIM with motion-first (M→AM) vs. audio-first (A→AM) conditioning, measure Sync conf and HQ. Expect audio-first to maintain lip-sync while motion-first suppresses audio.

## Open Questions the Paper Calls Out

- **Multi-person interactions**: How can the framework be extended to model interactions involving multiple people? The current architecture assumes a single subject stream; multi-person interactions require resolving complex mutual occlusions and interpersonal spatial reasoning. Evidence needed: A modified architecture capable of generating distinct motion plans for multiple avatars simultaneously without identity mixing or collision artifacts.

- **Motion resolution impact**: Does the low-resolution constraint (256px) on motion inputs degrade the precision of fine-grained finger-object manipulation in high-resolution outputs? Downsampling motion representations might lose the high-frequency details necessary for realistic grasping. Evidence needed: An ablation study comparing object contact accuracy of models trained with 256px versus 512px or higher motion inputs.

- **Real-world robustness**: To what extent does training primarily on synthetic reference images limit the model's robustness to real-world lighting and texture variations? Synthetic images often lack the noise, motion blur, and complex lighting of real video frames. Evidence needed: Evaluation results on a benchmark composed entirely of uncurated, real-world video frames with complex lighting and messy backgrounds.

## Limitations

- Dataset dependencies: Effectiveness relies heavily on quality and scale of training data for both PIM and AIM modules, with unspecified source datasets limiting reproducibility.
- Generalization beyond benchmark: Framework evaluated on specific GroundedInter benchmark with 100 object types; unclear how well it generalizes to unseen objects, environments, or interaction types.
- Computational cost: Dual-stream architecture with parallel co-generation and layer-wise residual injection likely increases computational overhead, with no detailed runtime or memory usage comparisons provided.

## Confidence

- **High Confidence**: Core mechanism of decoupling perception/planning from video synthesis is well-supported by ablation studies (VLM-QA drops without detection training); improvement in Hand Quality and Object Quality scores over baselines is robust.
- **Medium Confidence**: Effectiveness of layer-wise residual injection with zero-initialization is demonstrated but relies on specific architectural assumptions (isomorphic PIM and AIM); deviations could reduce stability.
- **Low Confidence**: Generalization of framework to diverse real-world scenarios is uncertain due to limited evaluation beyond GroundedInter benchmark.

## Next Checks

1. **Ablate detection training**: Train PIM without the detection task and evaluate VLM-QA and OQ on the GroundedInter test set. Expect a 2-3 point drop in VLM-QA and reduced object grounding, confirming the importance of detection training for environmental perception.

2. **Test motion representation robustness**: Replace the RGB motion representation with 2D (x,y) coordinates and evaluate HOI metrics. Expect a drop in generalization and alignment due to the modality gap, validating the choice of RGB motion.

3. **Evaluate generalization to unseen objects**: Test InteractAvatar on a dataset with novel object types or environments not present in GroundedInter. Measure performance degradation to assess scalability and robustness.