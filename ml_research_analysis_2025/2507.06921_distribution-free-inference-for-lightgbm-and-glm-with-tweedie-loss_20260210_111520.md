---
ver: rpa2
title: Distribution-free inference for LightGBM and GLM with Tweedie loss
arxiv_id: '2507.06921'
source_url: https://arxiv.org/abs/2507.06921
tags:
- prediction
- residuals
- lightgbm
- intervals
- pearson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops distribution-free prediction intervals for
  LightGBM and GLM models with Tweedie loss, focusing on insurance claim data. The
  authors propose using GLM-derived residuals (Pearson, Deviance, Anscombe) as non-conformity
  measures within a split conformal framework.
---

# Distribution-free inference for LightGBM and GLM with Tweedie loss

## Quick Facts
- arXiv ID: 2507.06921
- Source URL: https://arxiv.org/abs/2507.06921
- Reference count: 40
- Primary result: Distribution-free prediction intervals for LightGBM and GLM models with Tweedie loss, achieving 95% coverage with mean width 13.96 for locally weighted Pearson residuals on auto insurance claims

## Executive Summary
This paper develops distribution-free prediction intervals for LightGBM and GLM models with Tweedie loss, focusing on insurance claim data. The authors propose using GLM-derived residuals (Pearson, Deviance, Anscombe) as non-conformity measures within a split conformal framework. They introduce a locally weighted Pearson residual variant for LightGBM that allows heteroscedasticity beyond standard GLM assumptions. In extensive simulations on auto insurance data, all methods achieved nominal 95% coverage, but LightGBM with locally weighted Pearson residuals produced the narrowest intervals (mean width 13.96 vs. 14.76-21.51 for alternatives).

## Method Summary
The method partitions data into training (D₁), calibration (D₂), and validation (D₃) sets. A Tweedie GLM or LightGBM model is trained on D₁, then residuals are computed on D₂ using chosen non-conformity measures. For the novel locally weighted Pearson residual approach, a secondary LightGBM model estimates conditional residual spread on D₁, which is then used to standardize residuals on D₂. The k-th smallest standardized residual (where k = ⌈(n₂+1)(1-α)⌉) forms the prediction interval bound. This split conformal framework provides finite-sample coverage guarantees under exchangeability assumptions.

## Key Results
- All methods achieved nominal 95% coverage on auto insurance claims data
- Locally weighted Pearson residuals with LightGBM produced narrowest intervals (mean width 13.96)
- LightGBM outperformed GLMNET on interval width while maintaining coverage
- Top predictors identified: REVOLKED, MVR_PTS, and AREA
- Asymmetric intervals were wider than symmetric despite correct coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Split conformal prediction with GLM-derived residuals provides distribution-free prediction intervals with finite-sample coverage guarantees.
- Mechanism: The framework randomly partitions data into training (D₁) and calibration (D₂) sets. A model fitted on D₁ produces predictions; residuals computed on D₂ are ranked to obtain a non-conformity quantile. Under exchangeability, the k-th smallest residual (where k = ⌈(n₂+1)(1-α)⌉) yields intervals that contain the true response with probability ≥ 1-α, regardless of the underlying data distribution.
- Core assumption: Residuals on the calibration set must be exchangeable (weaker than i.i.d.).
- Evidence anchors:
  - [abstract] "Conformal predictive inference has arisen as a popular distribution-free approach for quantifying predictive uncertainty under relatively weak assumptions of exchangeability"
  - [section 2.1] "for any new i.i.d. draw (X_{n+1}, Y_{n+1}), Pr(Y_{n+1} ∈ C_S(X_{n+1})) ≥ 1-α"
  - [corpus] Related paper "A new strategy for finite-sample valid prediction of future insurance claims in the regression setting" confirms the paucity of finite-sample valid intervals in insurance regression settings
- Break condition: Exchangeability violation (e.g., time series structure, distribution shift between calibration and test data) invalidates coverage guarantees.

### Mechanism 2
- Claim: Pearson residuals outperform deviance and Anscombe residuals for Tweedie models because they naturally scale by the variance function, accommodating heteroscedasticity inherent in claim data.
- Mechanism: The Pearson residual R_i = (Y_i - μ̂_i) / [μ̂_i]^(p/2) divides raw errors by the square root of the variance function V(μ) = μ^p. This standardization makes residuals more exchangeable across observations with vastly different predicted means—critical when claim amounts range from zero to extreme values. Deviance residuals showed width inflation (26.32 mean width for LightGBM vs. 14.32 for Pearson), suggesting instability.
- Core assumption: The Tweedie variance function μ^p adequately approximates the conditional variance structure.
- Evidence anchors:
  - [section 2.2, Table 1] Pearson residuals are "simple to calculate; identifies overall model fit and heteroscedasticity"
  - [section 3.1, Table 3] LightGBM with Pearson residuals achieved mean width 14.32 vs. Deviance at 26.32
  - [corpus] No direct corpus evidence comparing residual types for conformal prediction
- Break condition: Severe model misspecification where the assumed variance-mean relationship diverges substantially from reality.

### Mechanism 3
- Claim: Locally weighted Pearson residuals enable narrower prediction intervals by allowing the dispersion parameter φ to vary with covariates, capturing heteroscedasticity beyond GLM assumptions.
- Mechanism: A secondary LightGBM model (squared-error loss) is fitted to Pearson residuals from D₁, estimating ρ̂_{n₁}(X). The weighted residual R*_i = R_i / ρ̂_{n₁}(X_i) adapts interval width locally. This achieved the narrowest intervals (mean width 13.96) while maintaining 95.02% coverage, outperforming unweighted alternatives.
- Core assumption: The secondary model accurately estimates conditional residual spread; residuals remain exchangeable after weighting.
- Evidence anchors:
  - [section 2.3] "we allow for extra flexibility and local variability beyond the (possibly mis-specified) GLM mean-variance relationship"
  - [section 3.1, Table 3] Locally weighted Pearson with LightGBM: coverage 0.9502 ± 0.0054, width 13.96 ± 0.51
  - [corpus] No corpus papers examine local weighting for conformal prediction in insurance
- Break condition: Insufficient data in D₁ to reliably estimate conditional variance; overfitting in the secondary model.

## Foundational Learning

- Concept: **Exchangeability**
  - Why needed here: This is the sole theoretical requirement for conformal coverage guarantees. It permits dependence among observations as long as their joint distribution is invariant under permutation—unlike i.i.d., it tolerates some forms of dependence.
  - Quick check question: If you shuffled the order of observations in your calibration set, would the statistical properties of the residuals change? If yes, exchangeability may be violated.

- Concept: **Tweedie Compound Poisson-Gamma Distribution (1 < p < 2)**
  - Why needed here: This distribution models zero-inflated, right-skewed continuous outcomes common in insurance claims. The power parameter p controls the shape: p → 1 approaches Poisson, p → 2 approaches Gamma. The variance function V(μ) = μ^p underlies all residual formulations.
  - Quick check question: For a Tweedie with p = 1.5, if the mean claim is $1000, what is the approximate variance? (Answer: φ × 1000^1.5 = φ × 31,623)

- Concept: **Split Conformal vs. Full Conformal Prediction**
  - Why needed here: Full conformal requires refitting the model for every trial prediction value (computationally prohibitive for LightGBM). Split conformal trades wider intervals for computational tractability by separating training and calibration.
  - Quick check question: Why might you choose n₁ > n₂ for an unbalanced split? (Answer: Complex models like LightGBM need more training data to produce accurate mean estimates; calibration set can be smaller.)

## Architecture Onboarding

- Component map:
  1. Data Splitter: Partitions n observations into D₁ (training), D₂ (calibration), D₃ (validation/test). Paper uses n₁ = n₂ = 4000, n₃ = 2296.
  2. Primary Model Trainer: Fits Tweedie GLM (glmnet) or LightGBM with Tweedie loss on D₁. Hyperparameter tuning via 5-fold CV on D₁ only.
  3. Residual Computer: Calculates chosen residual type (Pearson, Deviance, Anscombe, unstandardized) on D₂ using fitted model.
  4. Optional Local Weighter: Secondary LightGBM (squared-error loss) fitted on Pearson residuals from D₁ to estimate ρ̂(X).
  5. Quantile Extractor: Finds k-th smallest residual where k = ⌈(n₂ + 1)(1 - α)⌉.
  6. Interval Constructor: Transforms quantile into prediction bounds using closed-form formulas (Pearson/Anscombe) or root-finding (Deviance).

- Critical path:
  1. Split data → 2. Train primary model on D₁ → 3. Compute residuals on D₂ → 4. Extract quantile → 5. Construct intervals
  - For locally weighted variant: Insert step 3a (fit secondary model on D₁ residuals) before computing weighted residuals on D₂.

- Design tradeoffs:
  - LightGBM vs. GLMNET: LightGBM captures nonlinearities and interactions automatically but risks overfitting; GLMNET requires manual feature engineering but is more interpretable.
  - Symmetric vs. Asymmetric intervals: Asymmetric intervals using raw residuals had much wider intervals (22+ vs. 14-17) despite correct coverage—paper recommends symmetric intervals with absolute residuals.
  - Residual choice: Pearson is simplest and performed best; Deviance requires expensive root-finding and showed instability.

- Failure signatures:
  - Coverage below nominal: Check for exchangeability violation (e.g., temporal drift), data leakage between D₁ and D₂, or bugs in quantile calculation.
  - Excessively wide intervals: May indicate poor mean estimation (increase n₁) or inappropriate residual type (try Pearson over Deviance).
  - Negative lower bounds: Implement left-truncation at zero for claim amounts (Equation 2).

- First 3 experiments:
  1. **Baseline replication**: Replicate Table 3 results using the provided R code (https://github.com/alokesh17/conformal_LightGBM_tweedie.git). Verify that LightGBM + Pearson residuals achieves ~14.3 mean width with ~95% coverage on the AutoClaim dataset.
  2. **Hyperparameter sensitivity**: Vary the split ratio (e.g., n₁:n₂ = 60:40, 70:30) and observe impact on interval width vs. coverage. Paper suggests unbalanced splits may benefit complex models.
  3. **Domain transfer test**: Apply the pipeline to a different zero-inflated insurance dataset (not auto claims). Check if Tweedie power parameter p shifts (paper found p = 1.3 optimal for LightGBM on auto data; p = 1.5 for GLMNET) and whether locally weighted Pearson still outperforms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions do the GLM-based non-conformity measures (Pearson, Deviance, Anscombe residuals) fail to satisfy the exchangeability assumption required for valid conformal inference, and can diagnostics be developed to detect such violations?
- Basis in paper: [explicit] The authors state: "The key assumption for conformal prediction is the exchangeability of the (standardized) residuals, which underpins the validity of the predictive intervals."
- Why unresolved: The paper assumes exchangeability holds but provides no empirical validation or theoretical characterization of when it might fail, particularly under model misspecification.
- What evidence would resolve it: Simulation studies with varying degrees of model misspecification, formal exchangeability tests on residuals, or theoretical bounds on coverage degradation under specific violations.

### Open Question 2
- Question: Can the locally weighted conformal prediction framework be extended to handle temporal dependence in insurance claims data while maintaining finite-sample coverage guarantees?
- Basis in paper: [explicit] The authors acknowledge: "Although we are not focusing on the time series aspect of our model, the readers should note that there are several ongoing research to focus on the stochastic aspect in insurance modeling."
- Why unresolved: Standard conformal prediction assumes exchangeability, which temporal data violates; the paper does not address how to modify the approach for time series settings common in insurance reserving.
- What evidence would resolve it: Development of time-adaptive conformal methods with theoretical guarantees, or empirical demonstration of coverage under specific temporal dependence structures.

### Open Question 3
- Question: Why do asymmetric prediction intervals constructed from raw residuals consistently produce wider intervals than symmetric intervals for Tweedie-type models, and are there conditions under which asymmetry becomes advantageous?
- Basis in paper: [explicit] The authors report: "We found in the simulation, however, that while these asymmetric intervals did have the correct coverage, they tended to have much longer widths than the corresponding symmetric intervals for all three types of residuals considered."
- Why unresolved: The paper dismisses asymmetric intervals as inferior but does not investigate whether the inherent right-skew of Tweedie distributions could theoretically benefit from asymmetric constructions under different data regimes or extreme quantile targets.
- What evidence would resolve it: Theoretical analysis of interval width properties, or simulations across varying skewness levels and tail heaviness in the response distribution.

### Open Question 4
- Question: How does the choice of the second-stage model (currently LightGBM with squared-error loss) for estimating the local weighting function ρ̂_{n₁}(X_i) affect prediction interval properties, and are there optimal selection criteria?
- Basis in paper: [inferred] The locally weighted Pearson residual method requires fitting an auxiliary model to estimate error spread, but the paper provides no justification for choosing LightGBM over alternatives, nor sensitivity analysis to this choice.
- Why unresolved: The quality of local weighting directly impacts interval width, but no comparison of auxiliary model choices (e.g., quantile regression, kernel smoothing) is provided.
- What evidence would resolve it: Ablation studies comparing different auxiliary models for local weighting, with analysis of bias-variance tradeoffs in the resulting intervals.

## Limitations

- Exchangeability assumption may be violated in practice due to temporal drift, selection bias, or distribution shift between calibration and test data.
- Locally weighted Pearson residual approach introduces complexity through a secondary LightGBM model whose hyperparameters are unspecified, potentially affecting interval width and coverage.
- Deviance residual implementation requires a computationally expensive root-finding procedure not detailed in the paper, raising concerns about reproducibility.

## Confidence

- **High Confidence**: The split conformal framework's finite-sample coverage guarantees (Mechanism 1), the Pearson residuals' superior performance over deviance residuals (Mechanism 2), and the basic LightGBM + Pearson residual implementation.
- **Medium Confidence**: The locally weighted Pearson residual variant's performance claims (Mechanism 3) and the selection of optimal Tweedie power parameters (p=1.3 for LightGBM, p=1.5 for GLMNET).
- **Low Confidence**: The exact hyperparameter specifications for the secondary LightGBM model used in local weighting, and the computational feasibility of Deviance residual intervals on large datasets.

## Next Checks

1. **Hyperparameter Verification**: Replicate the locally weighted Pearson residual method by testing various secondary LightGBM configurations (leaves, learning rate, iterations) to confirm that the reported performance is robust to hyperparameter choices.

2. **Cross-Dataset Validation**: Apply the complete pipeline (including locally weighted Pearson) to at least two additional zero-inflated insurance datasets with different claim distributions to assess generalizability beyond the AutoClaim dataset.

3. **Exchangeability Assessment**: Design and implement diagnostic tests to evaluate residual exchangeability in the calibration set, and quantify how violations (e.g., temporal ordering, covariate shift) affect coverage guarantees in realistic insurance claim scenarios.