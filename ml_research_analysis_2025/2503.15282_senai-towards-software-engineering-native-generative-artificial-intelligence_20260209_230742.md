---
ver: rpa2
title: 'SENAI: Towards Software Engineering Native Generative Artificial Intelligence'
arxiv_id: '2503.15282'
source_url: https://arxiv.org/abs/2503.15282
tags:
- code
- software
- language
- such
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a gap in current Large Language Models (LLMs)
  for code generation, which focus primarily on functional correctness while neglecting
  fundamental Software Engineering (SE) principles such as modularity, cohesion, and
  coupling. The authors propose integrating SE knowledge into LLMs to enable them
  to generate code that adheres to SE best practices.
---

# SENAI: Towards Software Engineering Native Generative Artificial Intelligence

## Quick Facts
- arXiv ID: 2503.15282
- Source URL: https://arxiv.org/abs/2503.15282
- Reference count: 34
- Primary result: Identifies gap in LLMs focusing on functional correctness while neglecting SE principles; proposes integrating SE knowledge via enriched pre-training, multimodal learning, and Bloom's Taxonomy evaluation

## Executive Summary
This paper identifies a critical gap in current Large Language Models for code generation: they optimize for functional correctness but neglect fundamental Software Engineering principles like modularity, cohesion, and coupling. The authors propose a conceptual framework called SENAI to enhance LLMs with Software Engineering knowledge through multimodal learning from UML diagrams and Architectural Decision Records, enriched pre-training objectives, and a novel evaluation framework based on Bloom's Taxonomy. The goal is to move beyond mere functional accuracy to generate code that adheres to SE best practices and design rationale.

## Method Summary
The paper proposes a conceptual framework for creating "Software Engineering Native" LLMs through three main mechanisms: (1) enriching pre-training data with SE artifacts like UML diagrams and ADRs to teach structural relationships, (2) modifying pre-training objectives to include entity relationship prediction tasks that enhance structural reasoning, and (3) using Bloom's Taxonomy as a comprehensive evaluation framework to assess models' understanding of SE principles across cognitive levels. The method suggests combining these approaches with reinforcement learning alignment and proposes using both functional correctness metrics and SE quality metrics (LCOM, CBO) for evaluation.

## Key Results
- Current LLMs focus on functional correctness but neglect fundamental SE principles like cohesion and coupling
- Integration of SE artifacts (UML, ADRs) with code during pre-training may improve understanding of design rationale and structural relationships
- Bloom's Taxonomy provides a structured framework for evaluating SE knowledge internalization across cognitive levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enriching pre-training data with SE artifacts (UML diagrams, Architectural Decision Records) may improve LLMs' understanding of design rationale and structural relationships.
- Mechanism: Multimodal learning creates associations between high-level abstractions (class hierarchies, design intent) and concrete implementations, enabling the model to internalize patterns like composition, association, and architectural tactics that are not explicit in raw code.
- Core assumption: Models can transfer learned relationships from SE artifacts to new code generation tasks.
- Evidence anchors:
  - [abstract]: "The aim is to propose a new direction where LLMs can move beyond mere functional accuracy to perform generative tasks that require adherence to SE principles and best practices."
  - [section 3.2.1]: "Training models on both the code and its corresponding abstractions would enable them to understand the relationships between high-level design and concrete implementations."
  - [corpus]: Limited direct corpus evidence on UML/ADR integration; neighboring papers discuss AI-native SE paradigms but not this specific mechanism.
- Break condition: If SE artifacts are too sparse or low-quality, signal amplification fails to generalize; synthetic data generation may introduce artifacts that don't reflect real-world design patterns.

### Mechanism 2
- Claim: Modifying pre-training objectives to include entity relationship prediction (e.g., composition, association) can enhance structural reasoning beyond next-token prediction.
- Mechanism: By training models to predict masked relationships between code entities—similar to how CodeT5 predicts identifiers or GraphCodeBERT predicts data flow edges—the model learns to encode dependency structures and design patterns explicitly, improving cohesion and reducing coupling in generated code.
- Core assumption: Relationship prediction tasks provide a stronger signal for SE principles than token-level objectives alone.
- Evidence anchors:
  - [section 2.1.1]: "It consolidates pre-training tasks tailored for code understanding and generation... masking code spans and identifiers and trains the model to predict them."
  - [section 3.2.1]: "Incorporating entity relation prediction tasks helps models internalize the structural and semantic aspects of code... promoting better cohesion and reducing coupling."
  - [corpus]: Weak corpus corroboration; mechanism is proposed but not yet empirically validated in cited literature.
- Break condition: If relationship annotations are noisy or incomplete, the model may learn spurious correlations; probing may show internalization without downstream generation improvements.

### Mechanism 3
- Claim: Using Bloom's Taxonomy as an evaluation framework provides more comprehensive assessment of SE knowledge internalization than linear probing or execution-based metrics alone.
- Mechanism: Structured prompts across cognitive levels (Understand, Apply, Analyze, Evaluate, Synthesize) reveal whether the model can recognize, rank, refactor, and generate code adhering to SE principles—exposing gaps between recall and practical application.
- Core assumption: The model's responses to taxonomy-based prompts reflect genuine internalization rather than pattern matching.
- Evidence anchors:
  - [abstract]: "We propose using Bloom's Taxonomy as a framework to assess the extent to which they internalize SE knowledge."
  - [section 3.1.2, Table 1]: Concrete assessment strategies per level—e.g., "Apply" involves refactoring low-cohesion code while maintaining functional correctness, evaluated via LCOM/CBO metrics.
  - [corpus]: Related work on evaluating LLM agents (Agentic AI paper) discusses taxonomies and evaluation frameworks, supporting structured assessment approaches.
- Break condition: If prompts are ambiguous or evaluation criteria (e.g., cohesion ranking) lack ground truth, assessment becomes subjective; model gaming prompts without true understanding.

## Foundational Learning

- Concept: **Cohesion and Coupling Metrics (LCOM, CBO)**
  - Why needed here: The paper assumes these metrics as objective measures of code quality for both training signals and evaluation; without understanding what they quantify, you cannot validate whether generated code improves.
  - Quick check question: Given two class implementations, can you identify which has higher cohesion and explain why using LCOM?

- Concept: **Pre-training Objectives for Code LLMs (Next-token, FIM, Masking)**
  - Why needed here: The proposed SE-guided training builds on existing objectives; understanding baseline mechanisms (e.g., fill-in-the-middle) clarifies where relationship prediction fits and why it's novel.
  - Quick check question: How does fill-in-the-middle training differ from masked span prediction in terms of the context the model learns to use?

- Concept: **Bloom's Taxonomy Cognitive Levels**
  - Why needed here: The evaluation framework maps SE tasks to cognitive levels; you need to distinguish between "Analyze" (ranking code by cohesion) and "Evaluate" (judging whether a ranking is valid) to design appropriate assessments.
  - Quick check question: If a model correctly refactors code to improve coupling, which taxonomy level does this demonstrate, and what higher-level task would test deeper understanding?

## Architecture Onboarding

- Component map:
  Data Layer -> Pre-training Layer -> Alignment Layer -> Evaluation Layer

- Critical path:
  1. Procure or synthesize SE artifacts aligned with code repositories
  2. Extend pre-training with relationship prediction objectives
  3. Validate using both execution-based metrics and taxonomy-structured SE assessments
  4. Iterate based on which cognitive levels show gaps (e.g., strong at Apply, weak at Analyze)

- Design tradeoffs:
  - **Artifact availability vs. synthetic quality**: Real UML/ADRs are scarce; synthetic data scales but may not capture real design reasoning
  - **Training complexity vs. interpretability**: Adding SE objectives increases training cost; linear probes offer cheap interpretability but miss higher-order understanding
  - **Functional correctness vs. SE quality**: Optimizing for pass@k may conflict with cohesion/coupling improvements; multi-objective reward design is non-trivial

- Failure signatures:
  - Model passes functional tests but generates code with high coupling (CBO) or low cohesion (LCOM)
  - Model performs well on "Understand" and "Apply" taxonomy levels but fails at "Analyze" or "Evaluate," indicating shallow pattern matching
  - Probing shows SE concepts are encoded but not used in generation (internalization-action gap)

- First 3 experiments:
  1. **Baseline probing**: Run linear probes on existing code LLMs (CodeLlama, StarCoder2) for cohesion/coupling concepts to establish current internalization levels.
  2. **Small-scale SE-objective training**: Train a smaller model with relationship prediction on a curated dataset (code + UML pairs) and compare LCOM/CBO metrics on generated refactoring tasks against baseline.
  3. **Taxonomy-structured evaluation**: Design prompts for all Bloom's levels (per Table 1) and evaluate current models to identify which cognitive levels are weakest; use results to prioritize which SE objectives to add next.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synthetic data generation or architectural recovery techniques effectively substitute for real-world UML diagrams and ADRs in training SENAI models?
- Basis in paper: [explicit] The authors acknowledge that artifacts like UML diagrams are scarce and suggest "architectural recovery" and "synthetic data generation" as strategies to "mitigate this issue" (Section 3.2.3).
- Why unresolved: It remains unproven whether recovered or synthetic artifacts capture the high-level design intent and rationale necessary to train models effectively, or if they merely introduce noise.
- What evidence would resolve it: A comparative study evaluating models trained on recovered/synthetic artifacts versus those trained on human-authored artifacts, specifically measuring performance on design-centric tasks.

### Open Question 2
- Question: Does a Bloom's Taxonomy-based assessment offer a more comprehensive evaluation of SE knowledge internalization than linear probing?
- Basis in paper: [explicit] The paper proposes using Bloom's Taxonomy as a "framework to assess the extent to which they internalize SE knowledge," claiming it is more sound than existing approaches like linear probing (Abstract, Section 3.1.2).
- Why unresolved: While the paper outlines the taxonomy levels in Table 1, it does not implement this evaluation or provide empirical evidence that it correlates better with actual software engineering competence than existing probing methods.
- What evidence would resolve it: Empirical results demonstrating that high scores on Bloom's "Analyze" or "Evaluate" levels for an LLM strongly predict the generation of high-quality, maintainable code in practical scenarios.

### Open Question 3
- Question: Can models be aligned with SE principles (e.g., low coupling) via Reinforcement Learning without compromising functional correctness?
- Basis in paper: [inferred] The authors suggest RL to align models with code preferences (Section 3.2.1) and claim enriching models will enhance capabilities "without compromising their existing strengths" (Section 3.2.3).
- Why unresolved: Optimizing for structural metrics often conflicts with optimization for functional output; the trade-off boundary between "SE-native" code and "correct" code in LLMs is undefined.
- What evidence would resolve it: Experiments showing that an RL-aligned model can simultaneously improve software metrics (e.g., CBO, LCOM) while maintaining or improving Pass@k scores on standard benchmarks.

## Limitations
- The paper is purely conceptual with no empirical validation or specific implementation details
- Lack of concrete datasets, model architectures, or training procedures prevents direct reproduction
- Reliance on synthetic data for scarce SE artifacts may not capture real design intent
- No empirical comparison between Bloom's Taxonomy evaluation and existing probing methods

## Confidence
- Mechanism 1 (SE artifact enrichment): Medium
- Mechanism 2 (Relationship prediction): Low
- Mechanism 3 (Bloom's Taxonomy evaluation): Medium

## Next Checks
1. Implement and run linear probes on existing code LLMs (CodeLlama, StarCoder2) for SE concepts like cohesion and coupling to establish baseline internalization levels.
2. Design and administer Bloom's Taxonomy-structured prompts (per Table 1) to current models to identify cognitive level weaknesses.
3. Collect or synthesize a small dataset of paired code and UML/ADR artifacts and perform a controlled experiment comparing SE quality metrics (LCOM/CBO) of generated code with and without SE-guided training objectives.