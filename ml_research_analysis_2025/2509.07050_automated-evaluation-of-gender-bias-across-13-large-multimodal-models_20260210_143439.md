---
ver: rpa2
title: Automated Evaluation of Gender Bias Across 13 Large Multimodal Models
arxiv_id: '2509.07050'
source_url: https://arxiv.org/abs/2509.07050
tags:
- bias
- gender
- data
- professions
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed the Aymara Image Fairness Evaluation, a programmatic
  benchmark to assess gender bias in 13 commercially available large multimodal models
  (LMMs). Using 75 procedurally-generated, gender-neutral prompts across three categories
  (stereotypically-male, stereotypically-female, and non-stereotypical professions),
  the research generated 965 images and scored them using a validated LLM-as-a-judge
  system.
---

# Automated Evaluation of Gender Bias Across 13 Large Multimodal Models

## Quick Facts
- arXiv ID: 2509.07050
- Source URL: https://arxiv.org/abs/2509.07050
- Reference count: 29
- Primary result: 13 LMMs systematically amplify occupational gender stereotypes, with models generating men in 93.0% of male-stereotyped professions but only 22.5% of female-stereotyped professions

## Executive Summary
This study introduces the Aymara Image Fairness Evaluation, a programmatic benchmark that systematically assesses gender bias in 13 commercially available large multimodal models. Using 75 procedurally-generated, gender-neutral prompts across three occupational categories, the research generated 965 images and scored them using a validated LLM-as-a-judge system. Results reveal systematic amplification of occupational gender stereotypes, with models generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions. Notably, Amazon's Nova Canvas achieved gender parity, demonstrating that bias mitigation is achievable through design choices.

## Method Summary
The study employed the Aymara AI SDK to generate 75 procedurally-created, gender-neutral prompts across three occupational categories (25 each for stereotypically-male, stereotypically-female, and non-stereotypical professions). These prompts were submitted to 13 commercial LMMs via their APIs, generating 965 images total. A validated LLM-as-a-judge system classified each image as depicting a man or woman, with human validation confirming 96.4% agreement. The framework measured male representation per category, computed Gender Bias and Fairness Scores, and analyzed bias amplification against labor statistics using ANOVA, exact binomial tests, and Tukey HSD.

## Key Results
- LMMs generated men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions
- Strong default-male bias: 68.3% male representation for non-stereotypical professions
- Significant variation across models (46.7% to 73.3% male representation)
- Amazon's Nova Canvas achieved gender parity, demonstrating bias mitigation is achievable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LMMs amplify training data gender stereotypes through conditional probability navigation
- **Core assumption:** Model objective function (maximizing likelihood) causes sampling to exceed real-world gender distributions
- **Evidence anchors:** Systematic amplification statistically significant (Section 4.4), TTI models amplify hospital profession biases (Paper #58601)

### Mechanism 2
- **Claim:** LLM-as-a-judge provides valid proxy for human evaluation of social attributes
- **Core assumption:** Judge model extracts visual features matching human perception capabilities
- **Evidence anchors:** 96.4% human agreement, Cohen's Kappa: 0.92 (Section 3.4), FairJudge supports MLLM judging of social attributes (Paper #111087)

### Mechanism 3
- **Claim:** High bias variance implies design choices determine fairness outcomes
- **Core assumption:** Post-processing layers and fine-tuning constrain sampling space more than architecture
- **Evidence anchors:** Nova Canvas achieved parity (Abstract), bias mitigation as design choice (Section 5.3), SAE Debias and BiasMap validate intervention effectiveness (Papers #106795, #56502)

## Foundational Learning

- **Concept:** Statistical Parity vs. Real-World Distribution
  - **Why needed here:** Distinguishes between models being "accurate" to labor statistics vs. "fair" in generation
  - **Quick check question:** If a profession is 60% male in real world, should fair model generate 60% or 50% men?

- **Concept:** Procedural Prompt Generation
  - **Why needed here:** Avoids static datasets that can be memorized or overfitted
  - **Quick check question:** Why is using fixed 75 prompts risky for long-term benchmarking? (Answer: Models might overfit to specific test set)

- **Concept:** Zero-Shot Evaluation
  - **Why needed here:** Measures inherent bias without specific fine-tuning for tasks
  - **Quick check question:** Does zero-shot evaluation measure peak performance or default behavior?

## Architecture Onboarding

- **Component map:** 75 procedurally generated prompts -> 13 Commercial LMMs -> Aymara AI SDK (LLM-as-judge) + Human validation -> Fairness/Bias metrics
- **Critical path:** Integrity of gender-neutral prompts - if prompts accidentally imply gender, entire benchmark validity collapses
- **Design tradeoffs:** Binary gender classification trades inclusivity for statistical power; closed-source testing enables cross-provider comparison but prevents root-cause analysis
- **Failure signatures:** Refusals (Titan G1 v2 refused 9 prompts), ambiguity (low-confidence judgments on obscured faces), overfitting (models adapting to static prompts)
- **First 3 experiments:**
  1. Baseline Run: Execute 75 prompts on target LMM to establish Fairness Score
  2. Adversarial Negative Constraint: Modify prompts to explicitly negate stereotype (e.g., "female mechanic") to test if bias is "hard-baked"
  3. Safety Filter Ablation: Run benchmark with safety filters ON vs OFF to isolate safety layer effect on bias

## Open Questions the Paper Calls Out

- How does gender bias interact with other identity dimensions like race and age in generated images?
- What specific technical mechanisms make models like Nova Canvas achieve low bias?
- Do LMMs exhibit different bias patterns in non-English languages and non-Western cultural contexts?

## Limitations
- Limited to binary gender classification, excluding non-binary and gender-diverse individuals
- Closed-source black box testing prevents root-cause analysis of bias mechanisms
- English/Western focus may not reflect regional sociological realities

## Confidence
- **High Confidence:** Systematic amplification across 13 LMMs with strong statistical validation (96.4% human agreement)
- **Medium Confidence:** LLM-as-a-judge reliability for abstract/ambiguous images
- **Medium Confidence:** Design choice conclusion supported by Nova Canvas example but specific mechanisms unknown

## Next Checks
1. Cross-Cultural Validation: Test benchmark across different cultural contexts with varying occupational gender associations
2. Temporal Stability Test: Re-run benchmark quarterly to assess bias changes over time with model updates
3. Non-Binary Extension: Adapt LLM-as-a-judge to recognize non-binary gender presentations while maintaining statistical validity