---
ver: rpa2
title: 'IP2: Entity-Guided Interest Probing for Personalized News Recommendation'
arxiv_id: '2507.13622'
source_url: https://arxiv.org/abs/2507.13622
tags:
- news
- entity
- interest
- user
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses personalized news recommendation by modeling
  entity-guided user interests at both intra-news and inter-news levels. The proposed
  IP2 model introduces a signature entity encoder that aggregates mentioned entities
  into a unified representation, initialized via contrastive pre-training with news
  titles.
---

# IP2: Entity-Guided Interest Probing for Personalized News Recommendation

## Quick Facts
- **arXiv ID**: 2507.13622
- **Source URL**: https://arxiv.org/abs/2507.13622
- **Reference count**: 40
- **Primary result**: Improves AUC by up to 4.1% and MRR by up to 5.6% on MIND and Adressa datasets

## Executive Summary
This paper introduces IP2, a personalized news recommendation model that explicitly models entity-guided user interests at both intra-news (scanning) and inter-news (reading) levels. The model uses a signature entity encoder to aggregate mentioned entities into a unified representation, initialized via contrastive pre-training with news titles, and a dual-tower user encoder with cross-attention to capture both semantic and entity-guided preferences. Extensive experiments demonstrate state-of-the-art performance, with ablation studies confirming the effectiveness of both contrastive pre-training and cross-attention in modeling entity-guided interests.

## Method Summary
IP2 employs a two-stage training approach. First, a contrastive pre-training phase aligns signature entity embeddings (aggregated from all entities in a title via a Transformer) with BERT-encoded title representations using InfoNCE loss. Second, a dual-tower user encoder processes title and entity histories in parallel with cross-attention to calibrate semantic interest using entity-guided interest. The model uses a learnable weighted sum to combine title and entity representations, followed by dot product similarity for click prediction. Training uses AdamW with separate learning rates (1e-5 for BERT, 1e-4 for other parts), batch size 64, and 4 negative samples per positive.

## Key Results
- IP2 achieves state-of-the-art performance on MIND and Adressa datasets
- AUC improvement of up to 4.1% over strong baselines
- MRR improvement of up to 5.6% over strong baselines
- Ablation studies show both contrastive pre-training and cross-attention contribute significantly to performance

## Why This Works (Mechanism)

### Mechanism 1: Signature Entity Contrastive Alignment (Intra-News)
The model aggregates multiple entities into a single "signature entity" and aligns it with the news title via contrastive pre-training. This captures "scanning" interest better than isolated entity embeddings by forcing the entity representation to function as a proxy for the article's topic without relying on external Knowledge Graphs.

### Mechanism 2: Cross-Tower Interest Calibration (Inter-News)
The user encoder uses parallel semantic and entity streams with cross-attention, allowing the title tower to attend to entity history and vice versa. This models the behavioral interaction where entity interest guides title reading, capturing the "guidance" effect rather than treating preferences as independent.

### Mechanism 3: Personalized Stream Weighting
A learned sigmoid gate dynamically adjusts the contribution of entity-guided vs. semantic-guided interest, recognizing that users weight these streams differently. This prevents the model from using a fixed ratio that may not reflect individual user preferences.

## Foundational Learning

**Concept: Contrastive Learning (InfoNCE)**
- **Why needed here**: Solves the "cold-start" problem for entity embeddings by learning them from title context rather than external KGs
- **Quick check**: If you removed negative samples from pre-training loss, what would happen? (Answer: Embeddings would collapse or become indistinguishable)

**Concept: Cross-Attention vs. Self-Attention**
- **Why needed here**: The dual-tower user encoder relies on cross-attention to model the interaction between what is said (title) and who/what it involves (entity)
- **Quick check**: In the user encoder, does the Title tower query itself or the Entity tower?

**Concept: Knowledge Graph (KG) Sparsity**
- **Why needed here**: IP2 explicitly positions itself against KG-based methods
- **Quick check**: Why might a static KG fail to represent "Elon Musk" in breaking news about a brand new event? (Answer: Temporal lag/expiring information)

## Architecture Onboarding

**Component map**: Input (News Title + Entities) -> News Encoder (BERT + Transformer) -> User Encoder (Dual Tower with Cross-Attention) -> Aggregator (Learnable Weighted Sum) -> Predictor (Dot Product)

**Critical path**: The Signature Entity Encoder (SEE) initialization is the critical first step. If contrastive pre-training fails to align entities with titles, the downstream dual-tower user encoder receives garbage signals.

**Design tradeoffs**: IP2 trades the structured relationships of a KG for the flexibility and recency of in-context learning (using BERT to define entity meaning). This is robust to "entity missing" issues but may lack deep hierarchical knowledge. Two-stage training (Pre-train -> Downstream) is more operationally complex than end-to-end single-stage models.

**Failure signatures**:
- Entity Collapse: Wrong pre-training temperature causes entity embeddings to not separate meaningfully
- Dominance of Semantics: Weighting gate saturation near 1.0 for all users effectively ignores the entity tower

**First 3 experiments**:
1. Validate Pre-training: Run "Random w/ CP" vs. "TransE w/o CP" comparison to confirm in-context pre-training outperforms static KG embeddings
2. Ablate Cross-Attention: Replace MHAttention(H, E, H) with standard self-attention MHAttention(H, H, H) to measure specific contribution of "inter-news guidance"
3. Cold Start Check: Evaluate performance on news items containing "new" entities not seen frequently in training to verify generalization capability

## Open Questions the Paper Calls Out
- How do entity-related cognitive steps manifest in real-time user behavior during online recommendation sessions?
- Can the multi-level entity guidance framework be effectively transferred to domains with dense technical terminology, such as biomedical recommendation?
- How robust is the contrastive pre-training approach when applied to datasets lacking high-quality, pre-annotated entity labels?

## Limitations
- The paper does not specify the downstream training epoch count, leaving a gap in exact reproduction
- The contrastive pre-training assumes entity annotations are available and meaningful
- The dual-tower cross-attention assumes entity-guided reading is a universal behavior

## Confidence
- **High**: Performance improvements over baselines (AUC up to 4.1%, MRR up to 5.6%) are directly measurable and reproducible
- **Medium**: The effectiveness of the contrastive pre-training initialization is supported by ablation studies but depends on correct negative sampling strategy
- **Medium**: The cross-attention mechanism's contribution is evidenced in Table 4, but the paper lacks qualitative analysis of how it alters predictions

## Next Checks
1. Run a "negative sampling sanity check" by comparing AUC when using in-session negatives vs. global random negatives
2. Measure the entity tower's gradient norms during downstream training to confirm it is not being ignored (saturation of the weighting gate)
3. Evaluate model performance on a subset of news items containing "new" entities not seen frequently in training to test generalization beyond static embeddings