---
ver: rpa2
title: 'Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and
  AdamW'
arxiv_id: '2507.01241'
source_url: https://arxiv.org/abs/2507.01241
tags:
- stochastic
- conjugate
- algorithm
- subgradient
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCSAdamW, a novel optimization algorithm
  that integrates stochastic conjugate subgradients with AdamW for training large
  language models. The method addresses the limitations of traditional SGD-based approaches,
  particularly their effectiveness on non-smooth, non-convex loss functions common
  in LLMs.
---

# Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW

## Quick Facts
- arXiv ID: 2507.01241
- Source URL: https://arxiv.org/abs/2507.01241
- Authors: Di Zhang; Yihang Zhang
- Reference count: 40
- One-line result: Integrates stochastic conjugate subgradients with AdamW to achieve faster convergence and lower loss on LLM training

## Executive Summary
This paper introduces SCSAdamW, a novel optimization algorithm that combines stochastic conjugate subgradients with AdamW for training large language models. The method addresses limitations of traditional SGD-based approaches by leveraging conjugate subgradient search directions and adaptive sampling to improve convergence on non-smooth, non-convex loss functions. By integrating adaptive sampling, conjugate subgradient search directions, and decoupled weight decay, SCSAdamW achieves faster convergence and lower objective function values compared to Adam and AdamW. Experimental results on Wikitext-2, PennTreebank, AG-News, and IMDb datasets demonstrate significant improvements in both speed and accuracy of the optimization process.

## Method Summary
SCSAdamW combines stochastic conjugate subgradients with AdamW-style adaptive step sizes and decoupled weight decay. At each iteration, the algorithm solves a one-dimensional quadratic program to find the optimal convex combination of the previous direction and current subgradient, yielding a direction with minimal norm. The method uses adaptive sampling via concentration bounds to determine batch sizes that balance computational cost with approximation quality. Weight decay is applied separately from adaptive learning rates to ensure consistent regularization. The algorithm terminates when the norm of the search direction falls below a threshold, with theoretical guarantees that this implies proximity to a stationary point.

## Key Results
- Achieves faster convergence and lower objective function values compared to Adam and AdamW on Wikitext-2, PennTreebank, AG-News, and IMDb datasets
- Reduces computational burden through adaptive sampling while maintaining strong theoretical guarantees
- Proves that convergence of search direction norm to zero implies proximity to stationary points

## Why This Works (Mechanism)

### Mechanism 1: Conjugate Subgradient Direction Finding
The conjugate subgradient direction provides lower-norm search directions that accelerate convergence on non-smooth loss surfaces. At each iteration, the algorithm solves a one-dimensional QP to find the optimal convex combination λ*t ∈ [0,1] that minimizes ||(1-λt)dt-1 + λtgt||², yielding a direction dt with smallest possible norm within the convex hull spanned by previous direction and current subgradient. Core assumption: Smaller-norm directions in the subgradient convex hull lead more efficiently toward stationary points. Break condition: When λ*t oscillates near 0 or 1, direction updates become unstable; sigmoid smoothing (λ*t = σ(clip(λ*t, -5, 5))) mitigates this.

### Mechanism 2: Adaptive Sample Size via Concentration Bounds
Dynamically adjusted batch sizes reduce computational cost while provably maintaining approximation quality to full loss function. Theorem 3 provides a lower bound on sample size |Nt| ≥ -8 log(ε/2)·(M+1)² / (κ²δ⁴t) such that P(|Lt(θ) - L(θ)| ≤ κδ²t) ≥ 1-ε holds within local ball B(θ̂t, δt). This uses Hoeffding's inequality under Lipschitz and boundedness assumptions. Core assumption: L and Lt are Lipschitz continuous with constant Lf, and |Lt(θ)| ≤ M for all t, θ. Break condition: When Lipschitz constant is underestimated or function values exceed bound M, sample size may be insufficient.

### Mechanism 3: AdamW-Style Adaptive Step Size with Decoupled Decay
Normalizing conjugate directions by second-moment estimates stabilizes updates across varying gradient scales; decoupled weight decay ensures consistent regularization. The update θt = θt-1 - ηdt/(√v̂t + ζ) scales each parameter's update inversely with its historical gradient variance. Weight decay is applied separately (θt ← θt - ηλθt-1) to avoid interference with adaptive rates. Core assumption: Second-moment normalization prevents both overshooting in high-gradient regions and vanishing updates in flat regions.

## Foundational Learning

- **Subgradients for Non-Smooth Optimization**: Why needed: LLM loss landscapes are non-convex and non-smooth (e.g., due to ReLU, piecewise-linear activations); gradients may not exist everywhere, requiring subgradient calculus. Quick check: Given f(x) = max{x, 0}, what is the subdifferential ∂f at x=0?
- **Conjugate Gradient Methods**: Why needed: The algorithm extends conjugate gradient ideas to non-smooth stochastic settings; understanding CG helps explain why curvature-aware directions outperform raw gradients. Quick check: For minimizing f(x) = ½xᵀAx - bᵀx, why do conjugate directions converge in at most n steps for n-dimensional x?
- **Concentration Inequalities (Hoeffding's)**: Why needed: Theorem 3 relies on Hoeffding's inequality to bound how many samples ensure Lt ≈ L with high probability. Quick check: If you sample n i.i.d. random variables bounded in [a,b], how does Hoeffding's inequality bound P(|X̄ - E[X]| ≥ ε)?

## Architecture Onboarding

- **Component map**: Adaptive Sampler -> Gradient Computer -> Direction Solver -> Moment Tracker -> Parameter Updater -> Termination Monitor
- **Critical path**: 1. Determine sample size |Nt| from Theorem 3 2. Sample |Nt| sequences, compute Lt and gt 3. Solve min_λ∈[0,1] ||(1-λ)dt-1 + λgt||² for λ*t 4. Update direction dt using convex combination 5. Update second moment vt ← β2vt-1 + (1-β2)g²t 6. Apply bias correction to dt and vt 7. Update parameters with normalized conjugate direction 8. Apply decoupled weight decay 9. Check ||dt|| against ε threshold
- **Design tradeoffs**: Memory vs. curvature information: Stores only dt-1 (O(d)) vs. quasi-Newton methods requiring O(d²); Per-iteration cost: Adds 1D QP solve + inner products vs. pure AdamW; Stability vs. responsiveness: λ*t adapts to local geometry but may oscillate; fixed β1 is more stable but less adaptive
- **Failure signatures**: High-variance loss curves indicating λ*t instability (implement sigmoid clipping); Stagnation after fast initial progress (may indicate direction accumulation becoming stale—implement periodic restarts); Termination at poor solutions (check if ||dt||→0 actually implies ||gt||→0)
- **First 3 experiments**: 1. Reproduce Figure 2 baseline: Train LSTM on Wikitext-2 with Adam, AdamW, SCSAdamW (η=0.001, λ=0.001, 200 epochs); verify faster convergence of SCSAdamW 2. Ablate conjugate direction: Replace dt with gt while keeping adaptive sampling and AdamW normalization; measure convergence gap 3. Sample size sensitivity: Vary κ ∈ {10, 100, 1000} in Theorem 3 and track both wall-clock time and final loss

## Open Questions the Paper Calls Out

- **Scalability to Transformers**: Does SCSAdamW retain efficiency and convergence advantages when applied to large-scale Transformer-based models rather than the LSTMs tested? The paper explicitly states experiments were limited to "relatively small datasets and language models" due to computational constraints and lists scalability as a primary limitation. Evidence to resolve: Benchmarking SCSAdamW against AdamW on standard Transformer architectures (e.g., GPT-2, LLaMA) using large-scale datasets.

- **Smooth Direction Updates**: Can a smoothed update rule for λ*t (e.g., using sigmoid clipping) effectively reduce training fluctuations? Section 6 identifies "Smooth Direction Update" as a limitation, noting greater fluctuations than AdamW because λ*t becomes unstable near zero. The authors hypothesize a specific smoothing formulation but do not implement or test it. Evidence to resolve: Empirical comparison of loss curve smoothness and convergence speed between current implementation and proposed sigmoid-clipped variant.

- **Periodic Direction Restarts**: Does a periodic restart mechanism prevent search direction from being misled by outdated accumulated subgradients? Section 6 suggests "Periodic Restarts" as potential method to mitigate accumulation of misleading information from previous subgradients. While accumulation of subgradients is known property, proposed mitigation strategy remains untested. Evidence to resolve: Ablation studies analyzing convergence behavior and objective function values with and without periodic direction resets.

## Limitations
- No empirical validation on transformer architectures, which dominate modern LLM training
- Experiments use LSTMs on relatively small datasets, leaving unclear whether advantages extend to billion-parameter models
- Paper does not address potential numerical instability when λ*t approaches 0 or 1

## Confidence

- **High Confidence**: AdamW component and decoupled weight decay mechanism, given extensive prior validation of AdamW across diverse tasks
- **Medium Confidence**: Adaptive sampling theory and sample complexity bounds, as mathematically rigorous but practical benefit depends on accurate estimation of problem-specific constants
- **Low Confidence**: Overall performance gains in large-scale LLM settings, since current evidence is limited to small LSTMs on academic datasets

## Next Checks

1. **Scale-up Validation**: Implement SCSAdamW on transformer-base model (e.g., GPT-2-small) trained on Wikitext-103. Compare wall-clock convergence against AdamW at batch sizes 128, 512, and 2048 to assess scalability.

2. **Numerical Stability Audit**: Instrument λ*t calculation to track distribution and clipping frequency during training. Log cases where QP denominator approaches zero or sigmoid clipping activates, measuring impact on convergence.

3. **Transferability Test**: Evaluate SCSAdamW fine-tuning on downstream task (e.g., GLUE benchmark) starting from pre-trained BERT model. Compare final task performance and convergence speed against AdamW to assess whether optimization benefits transfer beyond training-from-scratch scenarios.