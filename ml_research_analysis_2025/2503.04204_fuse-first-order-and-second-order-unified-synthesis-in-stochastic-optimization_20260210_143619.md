---
ver: rpa2
title: 'FUSE: First-Order and Second-Order Unified SynthEsis in Stochastic Optimization'
arxiv_id: '2503.04204'
source_url: https://arxiv.org/abs/2503.04204
tags:
- adam
- first-order
- second-order
- optimization
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FUSE, a unified synthesis of first-order and
  second-order optimization methods for stochastic optimization in machine learning.
  The key idea is to combine the adaptive gradient method (Adam) with the second-order
  method (L-BFGS) in a switchover framework where the optimizer transitions from Adam
  to L-BFGS based on defined criteria.
---

# FUSE: First-Order and Second-Order Unified SynthEsis in Stochastic Optimization

## Quick Facts
- arXiv ID: 2503.04204
- Source URL: https://arxiv.org/abs/2503.04204
- Reference count: 28
- Primary result: Unified Adam-L-BFGS optimizer with switchover achieves up to 36.6% CIFAR-10 accuracy with DenseNet vs 34.3% with Adam

## Executive Summary
FUSE introduces a unified synthesis of first-order and second-order optimization methods for stochastic optimization in machine learning. The key innovation is a switchover framework that transitions from Adam to L-BFGS based on defined criteria, combining Adam's rapid initial progress with L-BFGS's superior local convergence. The method demonstrates smaller computational complexity than SGD and Adam, with theoretical analysis showing O(max{1/ζ, log(1/ε)}) for strongly convex and O(1/ε²) for non-convex cases.

## Method Summary
FUSE-PV implements a hard switchover from Adam to L-BFGS optimization based on three criteria: iteration-based (fixed epoch count), gradient-norm-based (when average gradient norm drops below threshold ζ), and loss-based (when consecutive loss changes fall below threshold σ). The method stores gradient/parameter differences during Adam phase for L-BFGS's two-loop recursion, then performs Wolfe line search for step size selection. The approach addresses Adam's convergence slowdown near optima and L-BFGS's poor performance with bad initialization.

## Key Results
- Achieves 36.6% CIFAR-10 accuracy with DenseNet vs 34.3% with Adam baseline
- Demonstrated on MNIST, FashionMNIST, KMNIST, USPS, CIFAR-10 with logistic regression, MLP, CNN, and DenseNet models
- Theoretical complexity bounds show O(max{1/ζ, log(1/ε)}) for strongly convex and O(1/ε²) for non-convex problems
- Three switchover criteria show epoch-based slightly outperforms gradient-norm and loss-based methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adam provides rapid initial progress while L-BFGS accelerates final convergence when near the optimum
- **Mechanism:** First-order adaptive methods (Adam) leverage momentum and adaptive step sizes to quickly traverse the loss landscape early in training. Once the gradient norm becomes sufficiently small (indicating proximity to a stationary region), second-order quasi-Newton methods (L-BFGS) exploit curvature information via Hessian approximation to achieve faster local convergence rates
- **Core assumption:** The optimization landscape has regions where curvature information becomes beneficial only after initial gradient-based exploration has brought parameters near a basin
- **Evidence anchors:**
  - [abstract]: "FUSE-PV stands as a simple yet efficient optimization method involving a switch-over between first and second orders"
  - [section II.B]: "During optimization, Adam triggers a fast search for the optimal solution at the early stage... While in the later stage, close to the optimal solution, second-order algorithms are able to take significantly fewer iterations to reach it compared to first-order algorithms"
  - [corpus]: Related work on second-order optimizers (arXiv:2504.20096) notes first-order methods "incorporate limited curvature information" and second-order methods remain underexplored for deep learning—FUSE addresses this gap
- **Break condition:** If the loss landscape has no exploitable local curvature structure (e.g., extremely flat or chaotic regions), L-BFGS provides no advantage over Adam

### Mechanism 2
- **Claim:** Gradient-norm-based switchover detects when Adam's progress has slowed sufficiently to justify L-BFGS activation
- **Mechanism:** The condition (1/K)ΣE[‖g(x_k)‖²] ≤ ζ ensures that parameters are initialized "close to x*" before switching. This addresses the known limitation that second-order methods can perform poorly with bad initialization while excelling near optima
- **Core assumption:** First-order methods converge to a neighborhood of a stationary point efficiently but slow down asymptotically; second-order methods converge faster locally given good initialization
- **Evidence anchors:**
  - [section II.C]: "This condition guarantees that when activating the second-order algorithm, x_k has been properly initialized with the first-order algorithm, even close to x*"
  - [section II.C]: "It has been well-known that first-order algorithms, such as SGD, can suffer from slow convergence when x_k → x*, while second-order algorithms have superior performance in this region"
  - [corpus]: Weak direct corpus support; no neighbor papers specifically address gradient-norm switchover conditions
- **Break condition:** If ζ is set too large, L-BFGS activates prematurely with poor initialization; if too small, Adam wastes iterations near convergence

### Mechanism 3
- **Claim:** Loss-change-based switchover detects Adam saturation, triggering transition before computational waste
- **Mechanism:** When E[|f(x_k) - f(x_{k-1})|] ≤ σ, the optimizer recognizes that Adam's per-iteration progress has dropped below threshold. This signals saturation at a stationary point where second-order curvature exploitation becomes more efficient than continued first-order updates
- **Core assumption:** Loss decrease rate correlates with optimizer effectiveness; bounded gradients ensure this metric is meaningful
- **Evidence anchors:**
  - [section II.C]: "Generically, we would like the designed algorithm to keep descending the loss when searching for a minimum. Hence, the difference between two consecutive losses can be used to determine when to switch over"
  - [section IV]: "Adam saturates after some epochs when solving simple non-convex functions with larger variance"
  - [corpus]: No direct corpus evidence for loss-based switchover in stochastic optimization
- **Break condition:** High loss variance from stochastic gradients may trigger premature or erratic switching; requires smoothed loss estimates

## Foundational Learning

- **Concept: Quasi-Newton methods (L-BFGS)**
  - **Why needed here:** FUSE relies on L-BFGS's two-loop recursion to approximate Hessian inverse without storing the full d × d matrix. Understanding how curvature information accelerates convergence is essential
  - **Quick check question:** Can you explain why L-BFGS uses a history of gradient/parameter differences rather than computing the Hessian directly?

- **Concept: Adaptive gradient methods (Adam)**
  - **Why needed here:** FUSE uses Adam for the first phase. Understanding bias correction (√(1-β₂ᵏ)/√(1-β₁ᵏ)), moment estimation, and adaptive step sizes explains why Adam excels early but saturates
  - **Quick check question:** What happens to Adam's effective step size for parameters with consistently large vs. small gradients?

- **Concept: Wolfe conditions for line search**
  - **Why needed here:** L-BFGS step size αₖ must satisfy Wolfe conditions (sufficient decrease + curvature condition) for convergence guarantees
  - **Quick check question:** Why can't L-BFGS simply use a fixed learning rate like Adam?

## Architecture Onboarding

- **Component map:**
FUSE-PV Pipeline: Adam Phase (Eqs. 2a-2c) -> Switchover Logic (3 criteria) -> L-BFGS Phase (Algo 1+2)
- **Critical path:**
1. Initialize parameters x₀, Adam moments m₀, v₀
2. Run Adam updates until switchover condition met
3. Store history of (sₖ, yₖ) pairs for L-BFGS two-loop recursion
4. Run L-BFGS with Wolfe line search until convergence
- **Design tradeoffs:**
- **Failure signatures:**
- **First 3 experiments:**
1. **Baseline comparison on simple test functions:** Run FUSE-PV vs. Adam, SGD, Adam+SGD on Rosenbrock and Rastrigin functions with fixed iteration-based switchover at 50% of total epochs. Verify matching Fig. 1-4 behavior
2. **Switchover criteria ablation:** On MNIST with logistic regression, compare all three criteria (iteration, gradient-norm, loss-based). Expect epoch-based to slightly outperform per Fig. 5
3. **Batch size sensitivity:** On CIFAR-10 with DenseNet, test batch sizes [32, 128, 512] to quantify variance reduction in L-BFGS phase. Larger batches should stabilize second-order updates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical convergence and empirical performance of the general FUSE framework (Algorithm 3) compared to the practical hard-switchover version?
- **Basis in paper:** [explicit] Section III states: "we present theoretical analysis for the proposed FUSE-PV algorithm and leave the more general analysis of FUSE in our future work"
- **Why unresolved:** The paper only derives complexity bounds for FUSE-PV (which switches abruptly) and validates it empirically, leaving the "soft" weighted update framework unproven
- **What evidence would resolve it:** Convergence proofs for Algorithm 3 involving the weighting parameter θ, alongside empirical comparisons on standard benchmarks

### Open Question 2
- **Question:** What is the optimal theoretical relationship between the switchover gradient threshold (ζ) and the final target accuracy (ε)?
- **Basis in paper:** [explicit] Section II.C notes that defining ζ larger than ε is necessary, but "their relationship is one future work to be investigated"
- **Why unresolved:** The authors currently treat ζ as a user-defined hyperparameter without establishing a mathematical dependency on the convergence criteria ε
- **What evidence would resolve it:** A theoretical derivation bounding the complexity based on the ratio ζ/ε or an adaptive rule for setting ζ

### Open Question 3
- **Question:** Can the unified synthesis approach be effectively generalized to other optimizer pairs, such as RMSProp or AdaHessian, beyond the Adam and L-BFGS combination?
- **Basis in paper:** [explicit] Section II.A states the authors select Adam and L-BFGS as representatives "while deferring other algorithms to future work"
- **Why unresolved:** The theoretical analysis and switchover metrics are derived specifically for the mechanics of Adam (first-order moments) and L-BFGS (Hessian approximation)
- **What evidence would resolve it:** Theoretical convergence rates for FUSE implementing different first-order (e.g., SGD with momentum) and second-order (e.g., CG) methods

### Open Question 4
- **Question:** Can the increased variance observed during the second-order phase be mitigated without resorting to larger batch sizes?
- **Basis in paper:** [inferred] Section IV notes that FUSE results in "slightly higher variance... attributed to the stochasticity in approximating the Hessian, which can be mitigated by increasing the batch size"
- **Why unresolved:** Increasing batch size increases computational cost per iteration, potentially negating the complexity benefits claimed by the FUSE method
- **What evidence would resolve it:** A variance-reduced variant of the L-BFGS switchover phase that maintains convergence speed with standard mini-batch sizes

## Limitations
- Switchover criteria require careful hyperparameter tuning with no automated selection method provided
- Theoretical analysis assumes strong convexity for local L-BFGS convergence, limiting applicability to non-convex deep learning landscapes
- Empirical validation lacks ablation studies on switchover timing sensitivity and computational overhead analysis across hardware platforms

## Confidence
- **High:** First-order (Adam) phase performance claims supported by extensive literature and standard benchmarking
- **Medium:** Switchover mechanism effectiveness, as theoretical guarantees assume strong convexity but experiments use non-convex deep networks
- **Medium:** Computational complexity improvements, based on asymptotic analysis that may not reflect practical implementation overhead

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary switchover thresholds (ζ, σ) and iteration points across multiple datasets to quantify robustness and identify optimal tuning procedures
2. **Computational overhead profiling:** Measure actual wall-clock time and memory usage of FUSE-PV versus pure Adam across different batch sizes and hardware configurations to validate claimed efficiency gains
3. **Non-convex landscape validation:** Test FUSE-PV on synthetic non-convex functions with known curvature properties to verify that L-BFGS provides benefits beyond simple noise reduction, isolating curvature exploitation effects