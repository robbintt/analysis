---
ver: rpa2
title: 'PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions'
arxiv_id: '2510.19060'
source_url: https://arxiv.org/abs/2510.19060
tags:
- image
- docent
- judgments
- scene
- posh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PoSh, a metric for evaluating detailed image
  descriptions that uses scene graphs as structured rubrics to guide LLM-as-a-Judge.
  PoSh identifies granular errors (mistakes and omissions) in generated descriptions
  by comparing them against reference descriptions using scene graph extraction and
  question answering, then aggregates these into interpretable coarse scores.
---

# PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions

## Quick Facts
- arXiv ID: 2510.19060
- Source URL: https://arxiv.org/abs/2510.19060
- Reference count: 33
- Introduces PoSh metric achieving F1 of 0.580 (mistakes) and 0.680 (omissions) on DOCENT benchmark

## Executive Summary
This paper introduces PoSh, a metric for evaluating detailed image descriptions that uses scene graphs as structured rubrics to guide LLM-as-a-Judge. PoSh identifies granular errors (mistakes and omissions) in generated descriptions by comparing them against reference descriptions using scene graph extraction and question answering, then aggregates these into interpretable coarse scores. The authors also introduce DOCENT, a new benchmark containing 1,750 artworks with expert-written references and 900 granular and coarse human judgments from art history students. PoSh achieves stronger correlations with human judgments than existing metrics, including GPT-4o, and demonstrates robustness across different image types using CapArena.

## Method Summary
PoSh extracts scene graphs from both reference and generated descriptions using dependency parsing and coreference resolution, then uses an LLM judge to verify the presence of each component through templated questions. The metric produces granular scores for specific text spans identifying where errors occur, which are then aggregated into coarse Mistake and Omission scores. The system uses Qwen-3-14b as the judge model and runs in approximately 2 seconds per description on an H100 GPU.

## Key Results
- PoSh achieves F1 of 0.580 for mistake detection and 0.680 for omission detection on DOCENT benchmark
- Strong correlation with human judgments (Spearman ρ = 0.495-0.621) outperforming baselines including GPT-4o
- When used as reward function, PoSh-tuned models produce descriptions with fewer missing details despite having more mistakes
- Demonstrates robustness across different image types using CapArena benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scene graphs act as "structured rubrics" that normalize diverse textual surface forms into a consistent object-attribute-relation format.
- Mechanism: By decomposing both the reference and generated text into scene graphs (using dependency parsing and coreference resolution), PoSh reduces the problem of long-text evaluation to verifying the presence of structured components. This mitigates the failure mode of n-gram overlap metrics (like CIDEr) which struggle with paraphrasing in long descriptions.
- Core assumption: Dependency parsing can reliably extract objects, attributes, and relations from complex sentences without losing semantic attachments.
- Evidence anchors:
  - [abstract] "...uses scene graphs as structured rubrics to guide LLM-as-a-Judge..."
  - [section 3] "POSH extracts scene graphs... that preserve object attachments... avoiding forcing an alignment between the components."
  - [corpus] "Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs" suggests scene graphs are a reliable method for structuring complex environmental data, supporting their utility here.
- Break condition: If the scene graph extraction fails to resolve coreferences (e.g., linking "he" to "the man in the hat"), attributes will be attached to the wrong entity, causing false positive errors.

### Mechanism 2
- Claim: An LLM verifies scene graph components via Question Answering (QA) to provide flexible semantic matching.
- Mechanism: Instead of rigid graph matching, PoSh prompts an LLM (specifically Qwen-3-14b) with templated questions derived from graph components (e.g., "Is the [entity] described as [attribute]?"). The LLM outputs a score (1-5), allowing it to recognize paraphrases and synonymous descriptions that exact matching would miss.
- Core assumption: The LLM judge has sufficient instruction-following capability to adhere to the restrictive scoring template and does not hallucinate details.
- Evidence anchors:
  - [abstract] "...producing aggregate scores grounded in fine-grained errors..."
  - [section 3] "We implement this function via question answering... to quantify the degree to which c is described in d'."
  - [corpus] Related works like "Vision-Based Natural Language Scene Understanding" utilize natural language descriptions for scene context, reinforcing that text-based verification is a viable proxy for understanding.
- Break condition: If the LLM judge is not calibrated or is too "lenient," it may score hallucinations or distinct entities as matches, reducing the precision of the "Mistake" score.

### Mechanism 3
- Claim: Hierarchical aggregation of granular scores ensures interpretability and robustness.
- Mechanism: PoSh first computes granular scores for specific text spans (identifying exactly *where* an error occurred) and then averages these into coarse scores (Precision/Recall). This allows the metric to be "right for the right reasons," making it useful as a reward signal in RL.
- Core assumption: A simple mean of granular errors correlates well with human holistic judgments of quality.
- Evidence anchors:
  - [abstract] "...aggregates these into interpretable coarse scores."
  - [section 6] "As its coarse scores are aggregated from these granular scores, this demonstrates its interpretability."
  - [corpus] Weak direct corpus evidence for aggregation specifically; primarily anchored in the paper's Table 2 and Table 3 results.
- Break condition: If specific error types (e.g., misidentifying a person vs. missing a background detail) are weighted equally, the coarse score may not align with human priorities in domain-specific tasks.

## Foundational Learning

- Concept: **Scene Graphs & Dependency Parsing**
  - Why needed here: The core extraction layer of PoSh relies entirely on transforming unstructured text into $\langle O, A, R \rangle$ triples. You must understand how dependency trees map subjects/objects and adjectives to build these graphs.
  - Quick check question: How would you parse "The red car hit the blue truck" into a set of objects and relations?

- Concept: **LLM-as-a-Judge**
  - Why needed here: PoSh uses an LLM not to generate, but to *evaluate* by scoring templated questions. Understanding prompt engineering and the biases (leniency/position bias) of judge models is critical.
  - Quick check question: Why might an LLM judge give a high score to a generation that contains a subtle factual error?

- Concept: **Coreference Resolution**
  - Why needed here: Detailed descriptions often span multiple sentences ("The woman... She holds..."). Without linking "She" back to "The woman," the graph fragments, and attribute verification fails.
  - Quick check question: In the sentence "The artist painted a portrait and sold it," what are the coreference chains?

## Architecture Onboarding

- Component map: Input -> Scene Graph Extractor -> QA Scorer -> Aggregator
- Critical path: The dependency parsing and coreference resolution stage is the bottleneck. If the graph extraction misattributes an attribute (e.g., assigning "red" to "truck" instead of "car"), the QA scorer will verify the wrong fact, propagating the error to the final score.
- Design tradeoffs:
  - **Open-weight vs. Closed APIs:** The authors use `Qwen-3-14b` instead of `GPT-4o` to ensure replicability and lower cost, trading off potentially higher raw reasoning power for consistency and independence from API changes.
  - **Granular vs. Coarse:** The system performs heavy computation (LLM inference on every graph component) to provide granular explainability, whereas a simple embedding similarity (like CLIPScore) is faster but uninterpretable.
- Failure signatures:
  - **Low Recall (Omissions) but High Precision:** Indicates the VLM generated safe, short descriptions that didn't hallucinate but missed details.
  - **"Entity Collision" Errors:** The model confuses two instances of the same class (e.g., two men). PoSh attempts to fix this with unique identifiers in prompts (Section 3).
- First 3 experiments:
  1. **Validation Run:** Run PoSh on 10 examples from DOCENT and manually inspect the *Extracted Scene Graph* to verify dependency parsing accuracy.
  2. **Ablation:** Replace `Qwen-3-14b` with a smaller model (e.g., 7B params) in the QA Scorer to measure performance degradation vs. speed gain.
  3. **Stress Test:** Feed the system a generated description that is factually correct but uses highly poetic/abstract language (e.g., "The sun wept golden tears") to see if the Scene Graph extraction collapses or if the LLM Judge can handle the semantic gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does PoSh exhibit bias when evaluating VLMs that use structural scene graph priors to guide generation, compared to models without such priors?
- Basis in paper: [explicit] The authors state in Section 9 (Limitations): "Recent efforts have explored using structural priors to guide generation... as these models become publicly available, this requires experimental validation."
- Why unresolved: PoSh extracts scene graphs from text; models that generate from scene graphs may have an unfair advantage or mismatch in evaluation. No such models were tested.
- What evidence would resolve it: Compare PoSh scores with human judgments for both structure-guided and standard VLMs on the same images, measuring correlation differences.

### Open Question 2
- Question: Can PoSh's localized granular scores enable effective token-level reinforcement learning for detailed description generation?
- Basis in paper: [explicit] The authors state: "as PoSh produces localized granular scores, it supports token-level guidance (Yang et al., 2023), an exciting direction to explore in future work."
- Why unresolved: RL experiments used sentence-level rewards (DAPO), not token-level guidance. The granularity of error localization was not leveraged during training.
- What evidence would resolve it: Train VLMs with token-level RL using PoSh's span-level error scores as rewards, comparing against sentence-level DAPO on DOCENT.

### Open Question 3
- Question: What factors contribute to PoSh's lower mistake detection performance (F1=0.580) compared to omission detection (F1=0.680)?
- Basis in paper: [inferred] Table 2 shows a substantial gap. The paper states mistakes are "relatively rare" while omissions are "common," suggesting class imbalance and possibly the difficulty of identifying subtle precision errors.
- Why unresolved: The paper does not analyze error types behind the F1 gap or whether specific mistake categories (e.g., relation misattachment vs. attribute errors) drive the difference.
- What evidence would resolve it: Conduct fine-grained error analysis on DOCENT granular judgments, breaking down false positives/negatives by error category and frequency.

## Limitations
- Relies heavily on accurate scene graph extraction from complex, multi-sentence descriptions where dependency parsing and coreference resolution performance may degrade
- 2-second runtime per description may be prohibitive for large-scale evaluation
- Requires reference descriptions to be of comparable quality to generated text; significant quality gaps could lead to unfair penalization

## Confidence

- **High Confidence:** The mechanism of using scene graphs as structured rubrics is well-supported by the dependency parsing literature and the experimental results showing superior correlation with human judgments (ρ = 0.495-0.621) compared to baselines. The hierarchical aggregation approach is also clearly validated.
- **Medium Confidence:** The LLM-as-a-Judge component works as claimed, but the specific calibration of Qwen-3-14b and its consistency across different types of errors is less certain. The paper shows strong results but doesn't extensively probe edge cases of judge model behavior.
- **Low Confidence:** The coreference resolution integration with the maverick model is not fully specified, making exact reproduction challenging. The claim that this approach generalizes across "different image types" is primarily supported by CapArena results without deep analysis of failure modes.

## Next Checks

1. **Edge Case Testing:** Apply PoSh to descriptions containing abstract/poetic language ("the sun wept golden tears") and highly technical art terminology to identify where scene graph extraction breaks down.

2. **Judge Model Ablation:** Systematically test different LLM judges (Qwen-3-14b, GPT-4o, smaller models) on the same DOCENT examples to quantify the contribution of judge model choice to final metric performance.

3. **Runtime Optimization Analysis:** Profile the pipeline to identify bottlenecks beyond LLM inference, and test whether aggressive caching or batching strategies can reduce the 2-second runtime without sacrificing accuracy.