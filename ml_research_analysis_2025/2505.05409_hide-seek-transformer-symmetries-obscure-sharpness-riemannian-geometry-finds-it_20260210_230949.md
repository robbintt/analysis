---
ver: rpa2
title: 'Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry
  Finds It'
arxiv_id: '2505.05409'
source_url: https://arxiv.org/abs/2505.05409
tags:
- sharpness
- geodesic
- space
- generalization
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately measuring sharpness
  in neural networks, particularly transformers, where traditional measures fail to
  account for the rich symmetries present in the attention mechanism. The authors
  propose a novel approach based on Riemannian geometry, specifically using geodesic
  balls on a quotient manifold that accounts for these symmetries.
---

# Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It

## Quick Facts
- **arXiv ID**: 2505.05409
- **Source URL**: https://arxiv.org/abs/2505.05409
- **Reference count**: 40
- **Primary result**: Geodesic sharpness on quotient manifolds recovers strong correlations with generalization in transformers, unlike traditional adaptive sharpness measures

## Executive Summary
This paper addresses the challenge of accurately measuring sharpness in neural networks, particularly transformers, where traditional measures fail to account for the rich symmetries present in the attention mechanism. The authors propose a novel approach based on Riemannian geometry, specifically using geodesic balls on a quotient manifold that accounts for these symmetries. They define geodesic sharpness as the maximum loss change along geodesics in this quotient space, providing a more general and adaptive measure than existing methods.

The key contribution is the introduction of geodesic sharpness, which is shown to recover strong correlations with generalization in transformers, unlike traditional adaptive sharpness measures. This is demonstrated on both vision transformers (ImageNet) and language models (MNLI), where geodesic sharpness consistently outperforms adaptive sharpness in predicting generalization. Additionally, the authors provide analytical solutions for diagonal networks and show that geodesic sharpness reduces to adaptive sharpness when curvature from symmetries is ignored.

## Method Summary
The method computes sharpness by projecting gradients onto a horizontal subspace that respects the transformer's GL(h) symmetries, then perturbing weights along geodesics in the resulting quotient manifold. Two Riemannian metrics are proposed: ⟨·,·⟩_inv (requires solving Sylvester equations) and ⟨·,·⟩_mix (explicit solution, better numerics). Geodesic perturbations follow the equation γ(t) ≈ θ + ξt - ½Γξξt², where Γ are Christoffel symbols capturing curvature corrections. The Auto-PGD algorithm maximizes loss within geodesic balls of radius ρ. For non-attention layers, the method reduces to adaptive sharpness with element-wise scaling.

## Key Results
- Achieves strong negative correlations between geodesic sharpness and generalization gap on ImageNet (τ = -0.71 and -0.70 for two metrics)
- Shows positive correlations on MNLI (τ = 0.28 and 0.38), whereas adaptive sharpness fails to find consistent correlations on transformers
- Improves correlation in diagonal networks from τ ≈ -0.68 to τ ≈ -0.85 compared to adaptive sharpness
- Both proposed metrics yield similar correlations, suggesting metric choice acts as a preconditioner

## Why This Works (Mechanism)

### Mechanism 1
Traditional sharpness measures fail for transformers because they do not account for the rich GL(h) symmetries in attention mechanisms, which induce continuous families of equivalent parameters. The paper constructs a quotient manifold M/G by quotienting out the symmetry group G. Objects like gradients and sharpness are computed on this quotient space, then "lifted" back to the original parameter space via horizontal projections. This ensures equivalent parameters receive identical sharpness values. The loss function is assumed invariant under the symmetry group action: ℓ(θ) = ℓ(ψ(g, θ)) for all g ∈ G.

### Mechanism 2
Higher-order curvature corrections (Christoffel symbols) in geodesic perturbations are necessary to recover correlation with generalization; first-order approximations reduce to existing adaptive sharpness which fails for transformers. Geodesic sharpness follows perturbation paths along geodesics (curved paths respecting the quotient geometry) rather than Euclidean straight lines. The geodesic equation involves Christoffel symbols Γ that capture curvature: γ(t) ≈ θ + ξt - ½Γξξt². Ignoring Γ recovers adaptive sharpness; including it accounts for how the manifold curves due to symmetries.

### Mechanism 3
The choice of Riemannian metric affects numerical stability but not qualitative correlation, as long as the metric is symmetry-compatible. Two metrics are proposed: ⟨·,·⟩_inv (requires solving Sylvester equations, cubic in h) and ⟨·,·⟩_mix (explicit solution, better numerics). Both yield similar correlations because they satisfy the symmetry-compatibility criterion, acting as different "preconditioners" on the same geometry. Metrics need only be smooth, positive-definite, and invariant under G—non-uniqueness is expected.

## Foundational Learning

- **Concept: Quotient Manifolds and Symmetry Groups**
  - Why needed here: The entire method hinges on understanding how to "divide out" symmetry transformations to get a well-defined parameter space where each function has a unique representation.
  - Quick check question: Given a symmetry group G acting on parameter space M, can you explain why the "orbit" of a point θ (all parameters reachable by G) forms an equivalence class, and why computing sharpness on the quotient M/G eliminates ambiguity?

- **Concept: Horizontal and Vertical Space Decomposition**
  - Why needed here: To work practically with quotient manifolds, one must project tangent vectors onto the "horizontal" subspace (directions that actually change the function) versus the "vertical" subspace (symmetry directions that leave the function unchanged).
  - Quick check question: If a tangent vector η at parameter θ has both horizontal and vertical components, which component affects the loss and which does not? How does the horizontal lift relate vectors on the quotient to vectors in the ambient space?

- **Concept: Geodesics and Christoffel Symbols**
  - Why needed here: The paper approximates geodesics (curved "straight lines" on manifolds) using second-order Taylor expansions involving Christoffel symbols; understanding this is essential to see why curvature corrections matter.
  - Quick check question: On a curved manifold, why is a geodesic different from a Euclidean straight line? What role do Christoffel symbols Γ play in the geodesic equation d²γ/dt² + Γ(dγ/dt)(dγ/dt) = 0?

## Architecture Onboarding

- **Component map:**
  - Total space M -> Original parameter space (e.g., R^(n×h) × R^(m×h) for attention weights G, H)
  - Symmetry group G -> GL(h) for attention; GL(1) re-scaling for diagonal networks
  - Quotient manifold M/G -> Abstract space of equivalence classes [θ]
  - Metric -> Either ⟨·,·⟩_inv (Eq. 9) or ⟨·,·⟩_mix (Eq. 10)
  - Horizontal projection -> Maps arbitrary tangent vectors to horizontal subspace (Eq. for Λ_inv, Λ_mix)
  - Christoffel symbols -> Curvature corrections (Eq. 11, 12)
  - Geodesic approximation -> γ(t) ≈ θ + ξt - ½Γξξt² (Eq. 4)

- **Critical path:**
  1. Identify symmetry groups in the architecture (GL(h) for attention pairs, GL(1) for diagonal/re-scaling layers)
  2. Choose a symmetry-compatible metric (inv vs. mix)
  3. Compute Euclidean gradient of loss
  4. Project gradient to horizontal space (solve for Λ)
  5. Form Riemannian gradient using the metric
  6. Compute Christoffel corrections for the perturbation direction
  7. Perturb weights along geodesic approximation
  8. Optimize perturbation to maximize loss (Auto-PGD in Algorithm 1)

- **Design tradeoffs:**
  - inv metric: More theoretically natural, but requires solving Sylvester equations (O(h³)). Can have numerical issues when G^TG is nearly singular—requires relaxation parameter ε.
  - mix metric: Avoids matrix inversions, explicit Λ solution, better numerics. Slightly better empirical results on BERT.
  - ρ (perturbation radius): Must be small enough for Taylor approximation validity but large enough to probe meaningful curvature. Paper uses values consistent with prior adaptive sharpness work.
  - Assumption 5.1 (full column rank): Required for metric inversion. In practice, use G^TG + εI relaxation.

- **Failure signatures:**
  - Near-singular G^TG: Causes numerical instability in inv metric. Fix with relaxation (Appendix H.2 shows results robust to ε ∈ {10⁻², 10⁻³, 10⁻⁴}).
  - Inconsistent correlation sign: The paper observes negative correlation on ImageNet but positive on MNLI—mechanism not fully understood. Limits predictive utility.
  - High computational cost: Sylvester solver cubic in h; for very large h this may bottleneck.
  - Corpus weakness: Related work on SAM variants (Monge SAM, Rényi Sharpness) addresses reparameterization-invariance but does not specifically treat GL(h) attention symmetries—direct empirical comparisons unavailable.

- **First 3 experiments:**
  1. **Diagonal network validation**: Replicate the synthetic experiment (d=200, 90% sparse ground truth, 50 models). Compute Kendall τ for adaptive vs. geodesic sharpness. Expected: geodesic improves from τ≈-0.68 to τ≈-0.85.
  2. **Attention layer unit test**: Take a single self-attention block with known GL(h) symmetry. Verify that: (a) Riemannian gradient norm is constant along symmetry orbits, (b) Euclidean gradient norm varies. Use Figure 1 as reference pattern.
  3. **Ablation on curvature terms**: On MNLI/BERT setup, disable Christoffel corrections (set Γ=0) and report τ degradation. Expected: τ drops from ~0.38 to ~0.24 for mix metric (per Appendix G, Figure 8).

## Open Questions the Paper Calls Out

- **What determines the sign of correlation between sharpness and generalization?** The paper observes negative correlation on ImageNet but positive on MNLI and diagonal networks, without a theoretical explanation for this variation. Understanding what causes sign reversals between tasks/data domains remains an open question.

- **How can data-dependent invariance be integrated into the framework?** The current approach addresses parameter symmetries but leaves data-induced invariance structures unexplored. A more complete understanding of the simultaneous dual invariance induced by data and parameter symmetries is of interest.

- **Can geodesic sharpness enable practical optimizers for transformers?** While the paper demonstrates correlation with generalization, computing geodesic sharpness currently requires solving geodesic equations and optimization subproblems. Real-time training integration remains unexplored.

- **Do analytical solutions exist for geodesics of the proposed metrics?** The paper uses approximations for geodesics of both metrics (9) and (10), noting that no analytical solutions are known. Whether closed-form solutions exist or can be proven impossible remains an open mathematical question.

## Limitations
- Correlation sign inconsistency across datasets (negative on ImageNet, positive on MNLI) limits general predictive utility
- Christoffel corrections rely on first-order Taylor expansions whose validity for larger perturbations is not rigorously established
- Computational scaling for very large attention heads may become prohibitive due to cubic Sylvester equation solver

## Confidence
- **High Confidence**: Quotient manifold construction and horizontal projection method are mathematically well-founded; experimental observation that adaptive sharpness fails for transformers is robust
- **Medium Confidence**: Christoffel symbols are crucial for recovering correlation, though theoretical justification for why curvature terms matter for generalization is incomplete
- **Low Confidence**: Exact form of metric choice (inv vs. mix) is presented as interchangeable, but performance differences lack clear explanation; computational scaling for very large heads is uncertain

## Next Checks
1. **Consistency Test**: Run geodesic sharpness pipeline on an additional transformer dataset (e.g., RoBERTa on GLUE) to determine if sign inconsistency is task-specific or more general
2. **Higher-Order Expansion**: Verify stability of geodesic sharpness when perturbation radius ρ exceeds Taylor expansion validity regime, checking if correlations degrade or method remains robust
3. **Metric Ablation**: Systematically test spectrum of symmetry-compatible metrics to see if performance difference is smooth function of metric choice or if inv/mix gap is sharp