---
ver: rpa2
title: Scaling Up Efficient Small Language Models Serving and Deployment for Semantic
  Job Search
arxiv_id: '2510.22101'
source_url: https://arxiv.org/abs/2510.22101
tags:
- compression
- pruning
- language
- search
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a comprehensive optimization pipeline for
  deploying a small language model (SLM) in LinkedIn's semantic job search system.
  The authors address the challenge of serving LLMs at scale with strict latency and
  throughput requirements.
---

# Scaling Up Efficient Small Language Models Serving and Deployment for Semantic Job Search

## Quick Facts
- arXiv ID: 2510.22101
- Source URL: https://arxiv.org/abs/2510.22101
- Reference count: 14
- Primary result: 10x throughput improvement (2000 items/sec/GPU) with <2% quality loss for semantic job search

## Executive Summary
This paper describes a comprehensive optimization pipeline for deploying a small language model (SLM) in LinkedIn's semantic job search system. The authors address the challenge of serving LLMs at scale with strict latency and throughput requirements. They apply multiple compression techniques: structured pruning to reduce model size by up to 40% with less than 1% quality loss, context compression via summarization using reinforcement learning to achieve 10x input length reduction with only 2% quality drop, and extensive serving infrastructure optimizations. The optimized system achieves 10x higher throughput (2000 items scored/second/GPU) compared to the uncompressed model, while maintaining search relevance and improving user engagement metrics in production.

## Method Summary
The method employs a multi-stage optimization pipeline. First, a 0.6B decoder-only SLM is trained as a cross-encoder ranker using Supervised Fine-Tuning (SFT) with KL divergence loss on 200k examples. The model is then compressed through structured pruning (OSSCAR for MLP neurons and layer dropping) reducing size by 40% while maintaining accuracy via post-pruning SFT. A separate 1.7B summarizer model is trained using Reinforcement Learning (GSPO) to compress job descriptions 10x while preserving ranking signals, with reward = -KL(p_sum || p_raw) - w * (len_sum/len_raw)². For serving, the system uses SGLang 0.4.6 on NVIDIA H100 GPUs with batch tokenization, scoring-only path (no decode loop), in-batch prefix caching, and traffic shaping. Score caching with 15-minute TTL achieves 50% hit rate.

## Key Results
- Structured pruning reduces model size by 40% with less than 1% NDCG loss
- RL-based summarization achieves 10x context compression with only 2% quality degradation
- Optimized system achieves 10x higher throughput (2000 items scored/second/GPU)
- Production deployment shows improved user engagement metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured pruning can reduce model size by up to 40% with less than 1% quality loss.
- **Mechanism:** Removing redundant model components (MLP hidden neurons and transformer blocks) reduces computational footprint. The paper finds that later transformer layers contribute less to task performance than early/middle layers, allowing selective removal. Post-pruning SFT recovers accuracy lost during compression.
- **Core assumption:** The relevance ranking task does not require the full representational capacity of all model components, and task-specific fine-tuning can redistribute lost functionality.
- **Evidence anchors:**
  - [abstract]: "pruning that allow us to reduce the model size by up to 40% while maintaining the accuracy"
  - [Section 4.1, Table 2]: Removing last layer causes only -0.0009 NDCG change vs -0.3356 for first layer
  - [Section 4.1, Table 3]: Combined MLP + layer pruning yields 375M model (from 600M) with -0.0074 NDCG change
  - [corpus]: Related work "Scaling Down, Serving Fast" (arXiv:2502.14305) similarly applies pruning/distillation for recommendation systems, suggesting transferability

### Mechanism 2
- **Claim:** RL-based summarization achieves 10x context compression with only 2% quality degradation.
- **Mechanism:** A separate 1.7B LLM (actor) learns to generate task-aligned summaries by optimizing a reward combining: (1) KL divergence between SLM outputs on summarized vs. raw context, and (2) quadratic length penalty. This aligns the summarizer with the downstream ranker's needs rather than generic summarization objectives.
- **Core assumption:** The reward signal captures sufficient information about what the SLM needs for accurate ranking, and the quadratic penalty's gradient signal is sufficiently dense for learning.
- **Evidence anchors:**
  - [abstract]: "context compression techniques that allow us to reduce the input context length by up to 10x with minimal loss of accuracy"
  - [Section 4.2.2, Table 5]: GSPO with P2 penalty (w=0.4) achieves -2% NDCG with 93% compression
  - [Section 3.2, Equation 4]: Reward = -KL(p_sum || p_raw) - w * (len_sum/len_raw)²
  - [corpus]: Nano-Capsulator (Chuang et al., 2024) cited but differs; corpus lacks direct replication of RL-for-summarization approach

### Mechanism 3
- **Claim:** Prefill-only workload optimization plus caching and traffic shaping yields 10x throughput improvement.
- **Mechanism:** Since the SLM only needs final-token logits (no autoregressive decoding), the system: (1) skips decode/sampling loops entirely, (2) exploits shared query prefixes across batch items via in-batch KV cache reuse, (3) caches scores for repeated query-item pairs (50% cache hit rate), and (4) smooths bursty traffic to maintain GPU utilization.
- **Core assumption:** The scoring task is deterministic for identical inputs, and the system can tolerate bounded latency variance for latency-insensitive workloads.
- **Evidence anchors:**
  - [abstract]: "increase our system's throughput by 10x in a real-world deployment"
  - [Section 5.2]: Scoring latency drops from 33ms to 20ms when bypassing decode loop
  - [Section 5.3]: Traffic shaping increases throughput from 1600 to 2000 items/sec/GPU (+25%)
  - [Section 5.3]: Score caching achieves 50%+ hit rate, reducing latency by 7-11%
  - [corpus]: ELIS (arXiv:2505.09142) addresses LLM scheduling but focuses on decode-heavy workloads; limited direct corpus evidence for prefill-only optimization patterns

## Foundational Learning

- **Concept: Structured vs. Unstructured Pruning**
  - Why needed here: The paper uses structured pruning (removing neurons/layers) because it produces models that run efficiently on standard GPU kernels without specialized sparse inference support.
  - Quick check question: If you prune 50% of weights unstructured, will PyTorch inference automatically be 2x faster? (Answer: No—sparse kernels or specialized hardware are typically required.)

- **Concept: Reinforcement Learning with Task-Aligned Rewards**
  - Why needed here: The summarizer is trained not to produce "good summaries" generically, but to produce summaries that maximize the SLM's ranking accuracy while minimizing length.
  - Quick check question: Why use KL divergence in the reward instead of training the summarizer via supervised learning on human-written summaries? (Answer: Human summaries may not be optimal for the SLM's specific ranking task; KL directly measures alignment.)

- **Concept: Prefill-Only vs. Decode-Heavy Workloads**
  - Why needed here: Standard LLM serving optimizations often target autoregressive generation (e.g., speculative decoding), but this workload only needs logits for scoring, enabling different optimizations.
  - Quick check question: Why does removing the decode phase reduce latency from 33ms to 20ms for a 300-token prompt? (Answer: Sampling and per-token probability calculations are skipped; only the final token's distribution is computed.)

## Architecture Onboarding

- **Component map:**
  Query → [Query Understanding + EBR Retrieval] → Candidate Jobs
                                                ↓
                     [Pre-computed: Summarized Descriptions + Features] ← [Offline/Nearline: Summarizer Model]
                                                ↓
  [SLM Ranker on SGLang/GPU] ← [Score Cache (Couchbase)]
         ↓ (optimized: batch tokenization, no decode, prefix caching)
  Quality Scores → [Ranking Depth Controller] → [Auction/Pacing] → Results
  Traffic shaping sits upstream to smooth request bursts.

- **Critical path:** Latency is dominated by SLM inference (prefill phase). The 500ms p95 SLA drives all compression and serving optimizations. Score caching and dynamic depth control are secondary paths that reduce load on the critical path.

- **Design tradeoffs:**
  - Model size vs. quality: Pruning to 375M parameters costs ~0.7-1% NDCG but enables 10x throughput
  - Compression ratio vs. quality: 93% compression costs ~2% NDCG; 95%+ causes precipitous drops
  - Cache TTL vs. freshness: 15-minute TTL yields 50% hit rate but scores may stale for updated job descriptions
  - Traffic shaping vs. latency: Smoothing reduces tail latency but may delay latency-insensitive requests

- **Failure signatures:**
  - Cache stampede: If TTLs expire simultaneously, GPU load spikes and p99 latency exceeds SLA
  - Summary model drift: If job description patterns change (e.g., new industries), summaries may fail to capture relevant signals → NDCG drops >2%
  - Prefix cache invalidation: If query format changes, KV cache reuse fails → throughput drops to pre-optimization levels
  - GC stalls: Without `gc.freeze()`, periodic 100-300ms pauses cause p99 violations at high RPS

- **First 3 experiments:**
  1. **Baseline latency profiling:** Deploy unoptimized SGLang with 600M model on full descriptions; measure p50/p95/p99 latency at target RPS. Expected: Cannot meet 500ms SLA.
  2. **Ablation on compression:** Test (a) pruning-only, (b) summarization-only, (c) both; measure throughput and NDCG. Expected: Both required for 10x throughput with <2% NDCG loss.
  3. **Cache hit rate sensitivity:** Vary TTL (5/15/30 min) and measure hit rate + p99 latency. Expected: 15 min optimal for current traffic patterns; lower TTLs increase GPU load.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can modifying the reinforcement learning objective to encourage outputs in a token-efficient language or synthetic representation overcome the performance cliff observed at >95% compression ratios?
- Basis in paper: [explicit] The authors hypothesize that the precipitous drop in performance beyond 95% compression is due to the inability to capture information using standard vocabulary, suggesting that "updating the algorithm to incentivize outputs in a more token-efficient language might alleviate this challenge."
- Why unresolved: The current RL approach relies on natural language generation, which has a lower bound on information density per token compared to the theoretical capacity of the model's latent space.
- What evidence would resolve it: Experiments showing stable NDCG@10 scores at >95% compression rates using a modified output space (e.g., code-switching or embedding tokens) compared to the natural language baseline.

### Open Question 2
- Question: Does calibrating the structured pruning algorithm (OSSCAR) on the *summarized* context rather than the full text yield a higher quality model for the production serving environment?
- Basis in paper: [inferred] The authors use "40M tokens from our training prompts" (full text) for pruning calibration, but the final production model operates on summarized text. This implies the pruning may preserve weights optimized for long-context features that are removed during summarization.
- Why unresolved: It is unclear if the importance of neurons identified during calibration on full text transfers effectively to the shortened, summarized context used in the final deployment.
- What evidence would resolve it: An ablation study comparing the NDCG@10 of models pruned using full-text calibration data versus models pruned using summarized calibration data.

### Open Question 3
- Question: To what extent do the throughput gains from the custom "in-batch prefix caching" implementation degrade in workloads with low prefix overlap or highly variable sequence lengths?
- Basis in paper: [inferred] The optimization relies on the specific workload characteristic where "all prompts corresponding to a query have a shared prefix," and the efficiency gain is theoretically proportional to the token ratio.
- Why unresolved: The paper evaluates the system in a controlled semantic search setting; the generalizability of this specific attention merge optimization to non-uniform or adversarial batch distributions is not discussed.
- What evidence would resolve it: Latency benchmarks of the in-batch prefix caching technique on datasets with diminishing shared prefix ratios (e.g., <50% overlap) to identify the crossover point where standard paged attention becomes more efficient.

## Limitations
- Pruning effectiveness is tied to specific model architecture and job search task; may not transfer to other domains
- RL-based summarization shows hard performance ceiling at 95% compression, suggesting fundamental information preservation limits
- Serving optimizations are specialized for prefill-only workloads and may not apply to decode-heavy generation tasks
- Production metrics are based on LinkedIn's specific traffic patterns and hardware; generalizability uncertain

## Confidence

**High Confidence** in the pruning mechanism and results: The paper provides clear evidence that structured pruning can reduce model size by 40% with <1% NDCG loss, supported by ablation studies showing layer-wise importance (first layer removal causes -0.34 NDCG vs -0.0009 for last layer). The recovery SFT step is well-established and critical for maintaining quality.

**Medium Confidence** in the RL-based summarization: While the approach achieves 10x compression with 2% quality loss, the corpus contains limited evidence for this specific RL formulation (GSPO with KL-based reward and quadratic length penalty). The technique differs from related work like Nano-Capsulator, and the observed 95% compression ceiling suggests task-specific constraints.

**Medium Confidence** in the serving optimizations: The 10x throughput improvement is plausible given the combination of prefill-only optimization, caching, and traffic shaping. However, the paper lacks detailed implementation specifics for SGLang optimizations, and the effectiveness depends heavily on LinkedIn's specific traffic patterns (bursty, latency-insensitive scoring workload).

## Next Checks

1. **Architecture Sensitivity Test:** Validate whether the pruning effectiveness (40% reduction with <1% quality loss) transfers to different model sizes (e.g., 1.3B or 3B parameters) and domains beyond job search. If pruning larger models yields similar quality retention but greater absolute parameter reduction, the approach scales well; if smaller models degrade rapidly, the technique has a minimum viable model size.

2. **Compression Wall Investigation:** Systematically test the summarization technique across compression ratios (90%, 92%, 93%, 94%, 95%, 96%) on the job search task and at least one other ranking task. If all tasks exhibit similar 95% ceilings, this represents a fundamental limitation of natural language vocabulary for information preservation; if walls vary, the 95% figure is task-specific.

3. **Serving Generalization Study:** Implement the prefill-only optimizations on a different LLM serving system (e.g., vLLM or TensorRT-LLM) with a non-LinkedIn workload (e.g., e-commerce product ranking). If throughput improvements persist across systems and workloads, the optimizations are broadly applicable; if improvements diminish, they are specialized to LinkedIn's infrastructure and traffic characteristics.