---
ver: rpa2
title: Improving Score Reliability of Multiple Choice Benchmarks with Consistency
  Evaluation and Altered Answer Choices
arxiv_id: '2511.21860'
source_url: https://arxiv.org/abs/2511.21860
tags:
- consistency
- mcqa
- llms
- questions
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Consistency-Rebalanced Accuracy (CoRA)
  metric to improve the reliability of LLM evaluation on multiple-choice benchmarks.
  The key insight is that standard accuracy metrics (MCQA) can be inflated because
  they don't account for the consistency of LLM responses when the answer choices
  are varied.
---

# Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices

## Quick Facts
- arXiv ID: 2511.21860
- Source URL: https://arxiv.org/abs/2511.21860
- Reference count: 28
- One-line primary result: Introduces CoRA metric to improve LLM evaluation reliability by incorporating consistency assessment when answer choices are varied

## Executive Summary
This paper addresses a critical limitation in LLM evaluation: standard multiple-choice accuracy metrics can be inflated because they don't account for response consistency when answer choices are varied. The authors introduce the Consistency-Rebalanced Accuracy (CoRA) metric, which scales down MCQA scores based on how consistently models answer correctly across different choice configurations. By requiring models to maintain accuracy even when distractors are shuffled, replaced with "None of the Above," or split into binary subsets, CoRA provides a more faithful measure of genuine knowledge rather than pattern exploitation or lucky guessing.

The methodology demonstrates that high MCQA scores often correlate with low consistency, and CoRA effectively penalizes inconsistent models while preserving scores for those that are both accurate and consistent. Evaluated across diverse benchmarks including MedQA (medical), MMLU (general knowledge), Arc-C (reasoning), and TruthfulQA (honesty), the results show substantial drops in scores when consistency requirements are applied, revealing that many models achieve correct answers through unreliable means. The approach is particularly valuable for domain-specific evaluation where superficial reasoning could have serious consequences.

## Method Summary
The method generates divergent question variants by applying operators to original multiple-choice questions: shuffling answer order, replacing distractors with "None of the Above," and decoupling questions into binary subsets. For each question, multiple variations are created and presented to the LLM, measuring Response Consistency (RC) as the proportion of correct responses across variants. The Bare-Minimum-Consistency Accuracy (BMCA) computes accuracy at specified consistency thresholds, while the Consistency Index (CI) measures the gap between observed accuracy and 100% consistency. CoRA is calculated as MCQA × CI, providing a reliability-adjusted accuracy score that penalizes models for inconsistent correct answers.

## Key Results
- GPT4o drops from 0.85 to 0.73 in MedQA when requiring 100% consistency, while MedLlama3 drops from 0.74 to 0.32
- High MCQA scores often correlate with low consistency across diverse benchmarks
- CoRA effectively penalizes inconsistent models while preserving scores for consistently accurate ones
- Ablation study confirms CoRA robustness to variations in the set of altered answer choices

## Why This Works (Mechanism)

### Mechanism 1: Divergent Question Generation for Consistency Stress-Testing
Modifying answer choice configurations while preserving the correct answer exposes whether models rely on superficial patterns rather than genuine knowledge. For each original MC question, generate M variations by applying operators to the choice set: shuffling order, replacing distractors with "None of the Above," decoupling into binary subsets, and combinations thereof. Each variation tests whether the model's correct response generalizes across choice configurations.

### Mechanism 2: Response Consistency (RC) as a Proxy for Knowledge Reliability
Computing the proportion of correct responses across divergent variants provides a measure of how robustly a model "knows" an answer. For question i with M variants, RC(i) = (1/M) × Σ llm(mcq*ij). A model achieving RC(i) = 1.0 has answered correctly across all variations; lower values indicate sensitivity to choice manipulation.

### Mechanism 3: Score Rebalancing via Consistency Index (CI)
Scaling MCQA scores by the gap between observed accuracy and 100%-consistent accuracy produces a metric that penalizes unreliable correctness. CI = 1.0 - (MCQA - BMCA(1.0)), measuring how much of the MCQA score survives the 100% consistency requirement. CoRA = MCQA × CI then applies this as a multiplicative penalty.

## Foundational Learning

- **Multiple-Choice Question Answering (MCQA) Evaluation**
  - Why needed here: CoRA builds directly on standard MCQA as the baseline to be corrected
  - Quick check question: Given an LLM that answers 75/100 MC questions correctly, what can you conclude about its knowledge? (Answer: Cannot conclude—may be inconsistent, guessing, or exploiting biases; MCQA alone is insufficient.)

- **Response Consistency Under Input Perturbation**
  - Why needed here: The core premise is that genuine knowledge generalizes across superficial input changes
  - Quick check question: If a model answers correctly when options are [A, B, C, D] but incorrectly when the same options are shuffled to [D, C, A, B], what does this indicate? (Answer: The model may be relying on positional biases or superficial patterns rather than semantic understanding.)

- **Distractor Engineering in Multiple-Choice Design**
  - Why needed here: The divergent question generation strategies operate on distractors
  - Quick check question: Why might replacing a distractor with "None of the Above" stress-test a model's knowledge differently than simply shuffling? (Answer: NOTA changes the decision boundary and may expose models that select the "least wrong" option rather than confidently identifying the correct one.)

## Architecture Onboarding

- Component map:
  Divergent Question Generator -> LLM Evaluator -> Response Consistency Calculator -> BMCA Aggregator -> CoRA Calculator

- Critical path:
  1. Generate divergent questions (cost scales linearly with M; 26 variants for 5-choice questions)
  2. Run LLM inference on all variants (dominant computational cost)
  3. Compute RC, BMCA, CI, CoRA in sequence (negligible compute overhead)

- Design tradeoffs:
  - M (number of variants): More variants improve RC reliability but multiply inference cost
  - Consistency threshold c: BMCA(c=1.0) is strictest; lower values are more permissive
  - Divergence operators: Shuffling alone is cheapest; NOTA/decoupling provide stronger stress-tests

- Failure signatures:
  - Uniformly low BMCA(1.0): Suggests models are fundamentally inconsistent
  - High variance in bootstrap analysis: Would indicate sensitivity to specific divergence set composition
  - MCQA+ > MCQA significantly: May indicate divergence operators inadvertently simplify questions

- First 3 experiments:
  1. Reproduce MedQA results with single operator: Generate variants using only shuffling on 100 MedQA questions
  2. Ablate divergence operator combinations: Compare CoRA scores when using individual operators vs full set
  3. Pilot on new domain with different choice counts: Apply CoRA to benchmark with 2-3 choices

## Open Questions the Paper Calls Out

### Open Question 1
How can the consistency evaluation framework be adapted for open-ended or generative benchmarks where discrete answer choices are not available? The current CoRA methodology relies on generating divergent questions by synthetically altering a fixed set of distractors, which does not translate directly to free-form text generation.

### Open Question 2
Can the accuracy of the Consistency-Rebalanced Accuracy (CoRA) metric be improved by integrating the full distribution of Bare-Minimum-Consistency Accuracy (BMCA) scores rather than relying solely on the BMCA(1.0) threshold? The current approach discards potentially valuable information about model behavior at intermediate consistency levels.

### Open Question 3
Is response consistency an inherent, generalizable trait of an LLM that persists across different domains, or is it highly dependent on the specific benchmark? The study observes consistent patterns across general-knowledge benchmarks but hasn't tested whether this generalizes to wildly different task types.

### Open Question 4
Does the gap between standard accuracy (MCQA) and Consistency-Rebalanced Accuracy (CoRA) narrow or widen as model size increases beyond 8 billion parameters? The authors demonstrate this phenomenon in 7B-8B models but haven't evaluated whether larger models maintain high consistency.

## Limitations
- The linear scaling relationship in the CI calculation lacks theoretical justification
- The approach is currently limited to multiple-choice questions with fixed choice counts
- Computational overhead scales linearly with the number of divergent variants
- Focus on 5-choice questions limits applicability to benchmarks with varying numbers of alternatives

## Confidence

- **High**: The core observation that MCQA scores often inflate when consistency is not measured; the computational feasibility of divergent question generation
- **Medium**: The effectiveness of specific divergence operators in stress-testing models; the generalizability of CoRA to benchmarks with different choice counts
- **Low**: The theoretical justification for the linear scaling in CI; the optimal number of divergent variants M for balancing reliability and computational cost

## Next Checks

1. **Operator Sensitivity Analysis**: Systematically compare CoRA scores when using individual divergence operators versus their combinations to identify which transformations most effectively distinguish genuine knowledge from pattern exploitation

2. **Cross-Domain Consistency Validation**: Apply CoRA to benchmarks with varying numbers of answer choices (2-3 vs 5+) to test whether the consistency-penalty relationship holds across different decision boundary complexities

3. **Statistical Power Analysis**: Conduct Monte Carlo simulations to verify the claim that RC = 1.0 with M = 10 trials requires guessing probability > 0.9999, testing edge cases where models employ sophisticated elimination strategies rather than pure guessing