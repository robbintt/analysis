---
ver: rpa2
title: 'Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code'
arxiv_id: '2510.14756'
source_url: https://arxiv.org/abs/2510.14756
tags:
- begin
- wire
- area
- delay
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pluto is a benchmark and evaluation framework for measuring both
  functional correctness and synthesis efficiency of LLM-generated Verilog hardware
  code. It addresses the gap in existing benchmarks by providing 114 curated problems,
  each with three expert-optimized reference implementations (area, delay, power),
  unoptimized baselines, and self-checking testbenches that support clock-cycle agnostic
  verification.
---

# Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code

## Quick Facts
- arXiv ID: 2510.14756
- Source URL: https://arxiv.org/abs/2510.14756
- Reference count: 40
- Key outcome: Benchmark shows LLMs achieve 78.3% functional correctness but only 63.8-65.9% synthesis efficiency vs expert designs

## Executive Summary
Pluto is a benchmark and evaluation framework for measuring both functional correctness and synthesis efficiency of LLM-generated Verilog hardware code. It addresses the gap in existing benchmarks by providing 114 curated problems, each with three expert-optimized reference implementations (area, delay, power), unoptimized baselines, and self-checking testbenches that support clock-cycle agnostic verification. Evaluation across 18 models shows that while LLMs achieve high functional correctness (up to 78.3% pass@1), their synthesis efficiency remains limited: area efficiency of 63.8%, delay efficiency of 65.9%, and power efficiency of 64.0% at eff@1 compared to expert-crafted designs. The benchmark reveals that larger, domain-tuned models perform better, but all models struggle with metric-specific optimization, particularly delay and power.

## Method Summary
The Pluto benchmark evaluates LLMs on two task formulations: rewriting unoptimized Verilog to optimized implementations (P1), and translating natural language specifications to optimized Verilog (P2). It provides 114 curated problems with natural language prompts, module headers, unoptimized baselines, three expert-optimized references (area/delay/power), and clock-cycle agnostic self-checking testbenches. Evaluation uses Yosys (SkyWater 130nm) and Cadence Genus (TSMC 65nm) for synthesis, with efficiency scores normalized relative to baseline and reference implementations. The benchmark tests 18 LLMs across proprietary, general-purpose, code-specialized, and Verilog-tuned categories.

## Key Results
- LLMs achieve high functional correctness (up to 78.3% pass@1) but limited synthesis efficiency
- Synthesis efficiency scores: 63.8% for area, 65.9% for delay, 64.0% for power at eff@1
- Larger, domain-tuned models perform better than general-purpose models
- All models struggle with metric-specific optimization, especially delay and power metrics
- Area optimization is comparatively more tractable than delay and power optimization

## Why This Works (Mechanism)

### Mechanism 1: Per-Metric Ground Truthing
Providing three distinct Pareto-optimal reference implementations per problem (one each for area, delay, power) isolates specific optimization deficits in LLMs that single-reference benchmarks miss. By defining separate upper and lower bounds for each physical metric, the evaluation creates a normalized efficiency score that forces the evaluation to reward structural transformations targeting specific hardware constraints rather than generic functional correctness.

### Mechanism 2: Latency-Agnostic Verification
Decoupling functional verification from clock-cycle exactness allows the benchmark to validate aggressive optimizations (like pipelining) that alter circuit latency. The self-checking testbenches compare the output logic of the LLM-generated design against the reference model without asserting a fixed clock-cycle deadline, preventing "false negatives" where structurally superior designs fail due to latency differences.

### Mechanism 3: Tool-Agnostic Normalization
Normalizing synthesis metrics relative to a baseline creates a stable efficiency score that persists across different fabrication nodes and synthesis tools. The efficiency metric calculates a ratio between the LLM design, an unoptimized baseline, and the optimal reference, canceling out absolute scale differences between different technology libraries and focusing purely on the logical efficiency of the Verilog code itself.

## Foundational Learning

- **Logic Synthesis Basics (RTL to Gates)**: Understanding that Verilog code is a hardware description language, not executable software, and how code constructs map to logic gates, flip-flops, and wires. *Quick check*: If you write a `for` loop in Verilog inside a combinational `always` block, does it execute sequentially in time or spatially in hardware?

- **PPA (Power, Performance, Area) Trade-offs**: Understanding why optimizing for one metric often degrades another (e.g., pipelining improves timing/delay but increases area/power). *Quick check*: Why might a design optimized for minimal area consume more power than a larger, faster design?

- **Metric-Aware Optimization Strategies**: Understanding specific techniques like "FSM restructuring" for area or "operand isolation" for power. *Quick check*: To reduce power consumption in a module, would you focus on reducing the number of gates or the switching activity of those gates?

## Architecture Onboarding

- **Component map**: Evaluation Set (114 problems) -> Reference Triplets (3 .v files per problem) -> Baseline (1 unoptimized .v file) -> Evaluator (Testbench + Synthesis Runner) -> Metrics Calculator

- **Critical path**: Generation (LLM generates code) -> Functional Sim (Icarus Verilog verification) -> Synthesis (Yosys/Genus mapping) -> Scoring (Extract metrics -> Normalize -> Compute eff@k)

- **Design tradeoffs**: Synthesis Tool Choice (Yosys free but limited vs Genus commercial with advanced timing), k-Sampling (higher k gives better bounds but increases compute cost 10x)

- **Failure signatures**: Latent Inefficiency (blocking assignments or redundant logic), Metric Blindness (applying wrong optimization type for prompt)

- **First 3 experiments**: 1) Environment Calibration (reproduce paper's ~20% reduction from baseline to optimized), 2) Baseline Functional Test (establish correctness baseline with general-purpose LLM), 3) Efficiency Gap Analysis (compare functionally correct outputs against Pluto "Optimized" references)

## Open Questions the Paper Calls Out

### Open Question 1
Can "synthesis-in-the-loop" frameworks effectively close the efficiency gap by providing LLMs with real-time physical feedback during code generation? Current models rely on static training data and lack ability to verify physical impact of structural modifications.

### Open Question 2
Does fine-tuning LLMs on datasets explicitly labeled with post-synthesis metrics (area, delay, power) improve optimization performance? Current models default to pattern matching rather than structural reasoning due to lack of efficiency labels in training data.

### Open Question 3
Can specific prompting strategies or model architectures be developed to address the performance disparity between area optimization and more complex delay/power optimizations? Current models struggle with non-local reasoning required for critical path analysis and switching activity estimation.

## Limitations
- Benchmark generalizability may be limited by specific synthesis tools (Yosys, Genus) and technology libraries (SkyWater 130nm, TSMC 65nm)
- Expert-optimized reference designs represent human engineering judgment rather than absolute theoretical optima
- Normalized efficiency scores may not translate directly to other synthesis flows or emerging hardware description languages

## Confidence
- **High Confidence**: Functional correctness evaluation (pass@k) methodology is sound and builds on established practices
- **Medium Confidence**: Synthesis efficiency metrics (eff@k) depend on assumption that expert-optimized references represent true Pareto-optimal points
- **Medium Confidence**: Claim that larger, domain-tuned models perform better requires additional testing across broader model families

## Next Checks
1. Cross-Tool Validation: Replicate efficiency calculations using OpenROAD to verify normalized scores remain consistent across different toolchains
2. Prompt Ablation Study: Systematically vary prompting strategy for a single high-performing model to quantify impact on both functional correctness and synthesis efficiency
3. Domain Transfer Test: Evaluate benchmark problems on different hardware domains (DSP or cryptographic circuits) to assess whether efficiency gaps persist across varied circuit types