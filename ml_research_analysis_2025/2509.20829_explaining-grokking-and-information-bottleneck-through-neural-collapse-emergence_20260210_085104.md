---
ver: rpa2
title: Explaining Grokking and Information Bottleneck through Neural Collapse Emergence
arxiv_id: '2509.20829'
source_url: https://arxiv.org/abs/2509.20829
tags:
- training
- neural
- collapse
- theorem
- grokking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explains two late-phase deep learning phenomena\u2014\
  grokking and the information bottleneck (IB) principle\u2014through the lens of\
  \ neural collapse, which characterizes the geometry of learned representations.\
  \ The authors identify the contraction of population within-class variance as a\
  \ key factor underlying both phenomena."
---

# Explaining Grokking and Information Bottleneck through Neural Collapse Emergence

## Quick Facts
- arXiv ID: 2509.20829
- Source URL: https://arxiv.org/abs/2509.20829
- Authors: Keitaro Sakamoto; Issei Sato
- Reference count: 40
- Key outcome: Population within-class variance contraction explains both grokking and information bottleneck dynamics through neural collapse emergence

## Executive Summary
This paper provides a unified theoretical explanation for two late-phase deep learning phenomena—grokking and the information bottleneck (IB) principle—through the lens of neural collapse. The authors identify the contraction of population within-class variance as the key factor underlying both phenomena. For grokking, they derive a generalization error bound in terms of this variance, while for IB dynamics, they show that superfluous information is bounded by the same quantity. Through theoretical analysis, they establish that the empirical within-class variance decreases during training on a distinct time scale from training loss convergence, particularly when weight decay is small. This temporal discrepancy explains the delayed generalization in grokking and the compression phase in IB dynamics. The theoretical findings are validated across multiple datasets and architectures, including MNIST, Fashion-MNIST, and CIFAR10, with both MLPs and CNNs.

## Method Summary
The authors use a 4-layer MLP architecture [784,200,200,200,10] with ReLU activation, no normalization/dropout layers, and initialization scaled by 8. They train on classification tasks (MNIST with 1000 training samples, Fashion-MNIST, CIFAR10) using AdamW optimizer with learning rate 1e-3 and varying weight decay values. The core methodology tracks RNC1 (rescaled within-class variance) alongside training loss and test accuracy throughout training. For IB analysis, they inject Gaussian noise into representations and estimate mutual information using dimensionality-reduction-based autoencoders and nHSIC. The theoretical analysis derives bounds showing that population within-class variance controls both generalization error and superfluous information, with weight decay controlling the time-scale separation between training convergence and RNC1 reduction.

## Key Results
- Population within-class variance provides an upper bound on generalization error and superfluous information I(Z;X|Y)
- RNC1 decreases on a time scale inversely proportional to weight decay λ, while training loss converges independently of λ
- The time-scale separation between training convergence and RNC1 reduction explains both grokking delay and IB compression phase
- Experimental validation across MNIST, Fashion-MNIST, CIFAR10 shows RNC1 reduction correlates with test accuracy improvement
- CNNs exhibit different behavior where grokking can occur with minimal RNC1 changes, suggesting other factors like class-mean separation may be more important

## Why This Works (Mechanism)

### Mechanism 1
Population within-class variance provides an upper bound on generalization error; reducing this variance directly tightens generalization guarantees. For a fixed feature extractor g and classifier W, the misclassification probability is bounded by a function of: (i) alignment between class mean representations and corresponding classifier weights, and (ii) the scale-invariant population within-class variance. The proof applies union bound and Cantelli's inequality to derive a variance-based tail probability bound.

### Mechanism 2
Superfluous information I(Z;X|Y) in representations is bounded by population within-class variance; compression in IB dynamics proceeds through variance reduction. Under the Markov chain Y→X→Z with Gaussian noise added to representations, I(Z;X|Y) = h(Z|Y) − h(Z|X). Using the entropy maximization property of Gaussians and log det(A+I) ≤ Tr(A), this reduces to a bound proportional to the expected within-class variance.

### Mechanism 3
Neural collapse (RNC1 reduction) proceeds on a time scale inversely proportional to weight decay λ, while training loss converges independently of λ—this time-scale separation explains grokking delay. Under gradient descent with squared loss and weight decay, training loss converges at time τ₁ = Ω((1/η)log(1/ϵ₁)). RNC1 convergence requires τ₂ = Ω((1/λη)log(1/ϵ₂)). The weight decay λ controls the "balancedness" propagation from output layer to representations; smaller λ causes larger τ₂−τ₁ gap.

## Foundational Learning

- **Concept: Neural Collapse (NC1/NC2/RNC1)**
  - Why needed here: The paper's central claim is that RNC1 dynamics explain grokking and IB; understanding the difference between NC1 (ratio of within-class to between-class variance) and RNC1 (scale-invariant within-class variance) is critical.
  - Quick check question: Given representations from two classes, can you compute both NC1 and RNC1? Do you understand why RNC1 isolates variance better than NC1 for analyzing generalization?

- **Concept: Information Plane and Mutual Information Bounds**
  - Why needed here: The IB analysis requires understanding how I(Z;X), I(Z;Y), and I(Z;X|Y) relate; the compression phase is characterized by decreasing I(Z;X|Y) while preserving I(Z;Y).
  - Quick check question: Can you explain why I(Z;X|Y) represents "superfluous information" and why reducing it corresponds to better representations?

- **Concept: Polyak-Łojasiewicz (PL) Condition and Convergence Analysis**
  - Why needed here: Theorem 4.3 relies on PL-based linear convergence for training loss; understanding the time-scale analysis requires grasping how convergence rates depend on learning rate η and weight decay λ separately.
  - Quick check question: Why does training loss convergence rate depend on η but not λ, while RNC1 convergence depends on both?

## Architecture Onboarding

- **Component map:** Training loss tracker → RNC1 computation → Margin analyzer → Mutual information estimator
- **Critical path:** 1) Confirm training loss has converged (τ₁ identification) 2) Continue training while monitoring RNC1 decrease rate 3) RNC1 convergence (τ₂) triggers generalization improvement / IB compression 4) Weight decay λ controls the τ₁→τ₂ delay
- **Design tradeoffs:** Higher weight decay λ → faster RNC1 convergence but may harm final accuracy if excessive; Scaled initialization (×8) → more likely to observe grokking but may cause training instability (especially in CNNs/Transformers); Smaller training sets → more pronounced grokking behavior but less representative evaluation
- **Failure signatures:** No grokking observed: Check if RNC1 already low at τ₁ (architectures with strong inductive bias like CNNs may skip grokking entirely—see Section D.1 Figure 7); RNC1 decreases but test accuracy doesn't improve: Check class-mean separation (use NC1 instead of RNC1; see Section D.1 Figure 9 for Transformer case); Training unstable with scaled initialization: Reduce initialization scale or apply scaling only to fully-connected layers
- **First 3 experiments:** 1) Reproduce Figure 3: Train 4-layer MLP on 1000-sample MNIST subset with AdamW, varying λ ∈ {0.01, 0.05, 0.1, 0.3}; track train/test accuracy and RNC1 to confirm λ accelerates grokking 2) Margin visualization (Figure 2 analog): At τ₁ (overfitting) and τ₂ (convergence), plot margin distributions for train/test samples to visualize within-class variance reduction 3) IB validation (Figure 4 analog): Estimate I(Z;X|Y) using dimensionality-reduction-based MI estimator or nHSIC; confirm correlation with RNC1 decrease across different λ values

## Open Questions the Paper Calls Out

### Open Question 1
Does the neural collapse time-scale analysis hold for non-MLP architectures or different initialization schemes? The conclusion states, "A natural next step is to extend the time-scale analysis of neural collapse to other architectures beyond MLP or to different initialization methods." This is unresolved because the theoretical dynamics (Theorem 4.3) depend on specific pyramidal network assumptions, and experiments showed CNNs did not exhibit the same delayed generalization timing. What evidence would resolve it: Theoretical derivations of neural collapse dynamics in Transformers or proofs showing the time-scale separation persists under different initialization variances.

### Open Question 2
Can neural collapse emerge to explain late-phase phenomena without explicit weight decay? The authors propose, "Another interesting direction is to analyze the possibility of neural collapse that implicitly arises without weight decay." This is unresolved because the theoretical proof of RNC1 convergence (Theorem 4.3) explicitly relies on weight decay (λ > 0) to bound parameter norms and drive the collapse. What evidence would resolve it: Theoretical analysis showing the convergence of within-class variance solely via gradient descent dynamics, or empirical studies isolating implicit regularization effects.

### Open Question 3
Can this framework explain grokking in regression tasks like modular arithmetic? The paper limits its scope to classification, noting modular arithmetic is "closer to a regression problem" and thus excluded, despite being a central domain for grokking research. This is unresolved because the generalization bounds (Theorem 3.2) utilize classification margins and class-conditional variances which do not directly apply to continuous regression outputs. What evidence would resolve it: An extension of the within-class variance bounds to regression error, or empirical verification that geometric collapse correlates with grokking in algorithmic regression tasks.

## Limitations
- Theoretical bounds rely on assumptions (fixed feature extractor/classifier, Gaussian approximation, pyramidal architecture) that may not hold in practice
- Noise injection required for meaningful IB analysis introduces an additional hyperparameter not discussed in detail
- CNNs and Transformers show different behavior where grokking can occur with minimal RNC1 changes, suggesting other factors like class-mean separation may be more important
- Tight connection between RNC1 and grokking phenomena may not extend to all architectures

## Confidence
- **High confidence**: The RNC1 metric effectively captures population within-class variance; empirical correlation between RNC1 reduction and generalization improvement is robust across datasets and architectures
- **Medium confidence**: The theoretical bounds (Theorem 3.2 and 3.4) correctly characterize the role of within-class variance in generalization and information compression, though practical tightness requires further validation
- **Medium confidence**: The weight decay-dependent time-scale separation (Theorem 4.3) explains grokking delay, but the specific logarithmic factors and constants need experimental verification across broader architectures

## Next Checks
1. **Architectural robustness test**: Apply the analysis framework to ResNet architectures on CIFAR10 to verify if RNC1 reduction remains the dominant factor in grokking, or if other geometric factors (like class-mean separation) become more important
2. **Dynamic range validation**: Systematically vary the initialization scale parameter (currently fixed at ×8) across multiple orders of magnitude to map the boundary conditions where grokking emerges or disappears
3. **Noise sensitivity analysis**: Conduct ablation studies on the noise injection parameter σ for IB analysis, measuring how sensitive the I(Z;X|Y) estimates are to this choice and whether the RNC1 correlation holds across the full parameter range