---
ver: rpa2
title: 'Unforgotten Safety: Preserving Safety Alignment of Large Language Models with
  Continual Learning'
arxiv_id: '2512.10150'
source_url: https://arxiv.org/abs/2512.10150
tags:
- safety
- fine-tuning
- data
- arxiv
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper frames safety degradation in fine-tuned language models
  as a continual learning problem, attributing it to catastrophic forgetting of safety
  alignment. The authors adapt several CL approaches (regularization-based, memory-based,
  and model merging) to preserve safety when fine-tuning on both benign and poisoned
  data.
---

# Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning

## Quick Facts
- arXiv ID: 2512.10150
- Source URL: https://arxiv.org/abs/2512.10150
- Reference count: 12
- Primary result: CL methods (especially DER) effectively preserve safety alignment while maintaining utility during fine-tuning

## Executive Summary
This paper addresses safety degradation in fine-tuned language models by framing it as a continual learning problem, specifically catastrophic forgetting of safety alignment. The authors systematically evaluate multiple continual learning approaches - regularization-based (EWC, MAS), memory-based (DER, AR1), and model merging techniques - to preserve safety when fine-tuning on both benign and poisoned data. Their experiments across three model families (LLaMA2-7B, Mistral-7B, Gemma-2B) and three tasks (GSM8K, SST2, Code) demonstrate that continual learning methods consistently reduce attack success rates compared to standard fine-tuning, with DER achieving the best balance between safety preservation and model utility.

## Method Summary
The authors approach safety preservation as a continual learning problem, hypothesizing that standard fine-tuning causes catastrophic forgetting of safety knowledge. They implement and evaluate three categories of CL methods: regularization-based approaches (Elastic Weight Consolidation and Memory Aware Synapses) that constrain weight updates based on safety-relevant parameters, memory-based methods (DER and AR1) that maintain replay buffers of safety-critical examples, and model merging techniques that combine pre-trained safety-aligned weights with fine-tuned weights. The evaluation framework includes poisoning benign datasets with safety-violating examples and measuring both attack success rates and task performance across multiple model families and tasks.

## Key Results
- CL methods consistently reduce attack success rates compared to standard fine-tuning across all tested scenarios
- DER achieves the best safety-utility balance, maintaining strong performance on downstream tasks while significantly reducing safety violations
- Safety preservation effectiveness holds across different model families (LLaMA2-7B, Mistral-7B, Gemma-2B) and tasks (GSM8K, SST2, Code)
- The benefits persist even with high poison ratios, demonstrating robustness to severe safety degradation scenarios

## Why This Works (Mechanism)
The paper demonstrates that catastrophic forgetting of safety alignment occurs during standard fine-tuning, and that continual learning methods can effectively mitigate this degradation by preserving knowledge about safety-critical parameters and behaviors.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly lose previously learned information when trained on new tasks - critical for understanding why safety alignment degrades during fine-tuning
- **Safety alignment**: The process of training models to avoid generating harmful, biased, or inappropriate content - essential context for evaluating safety preservation
- **Poisoning attacks**: Deliberate contamination of training data with malicious examples designed to compromise model behavior - relevant for understanding attack scenarios
- **Elastic Weight Consolidation (EWC)**: A regularization method that protects important weights from large updates based on their importance to previous tasks - key mechanism for preventing safety forgetting
- **Experience Replay (DER)**: A memory-based method that maintains a buffer of past examples to replay during training - fundamental approach for preserving safety knowledge

Quick check: All foundational concepts relate to either forgetting mechanisms, safety preservation techniques, or attack scenarios that the paper addresses.

## Architecture Onboarding

**Component Map:** Pre-trained model -> CL method (Regularization/Memory/Model Merging) -> Fine-tuning on benign/poisoned data -> Evaluation (Attack Success Rate/Task Performance)

**Critical Path:** Safety preservation requires maintaining knowledge of safety-critical parameters while adapting to new tasks, achieved through CL constraints, replay buffers, or weight merging

**Design Tradeoffs:** Regularization methods are computationally efficient but may overly constrain learning; memory methods provide better preservation but require maintaining buffers; model merging offers flexibility but needs careful weight balancing

**Failure Signatures:** Increased attack success rates indicate safety forgetting; significant performance drops on downstream tasks suggest over-constraining; inconsistent results across model families may indicate method limitations

**3 First Experiments:**
1. Compare attack success rates between standard fine-tuning and each CL method on poisoned GSM8K data
2. Evaluate task performance retention across all CL methods on SST2 after safety preservation training
3. Test safety preservation effectiveness across different poison ratios (10%, 25%, 50%) using DER method

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focuses on synthetic poisoned data rather than real-world safety degradation scenarios
- Safety preservation effectiveness measured primarily through attack success rates rather than comprehensive safety benchmarks
- Limited analysis of trade-offs between safety preservation and utility across diverse downstream tasks

## Confidence
**High confidence:** Experimental methodology is sound, multiple CL approaches are comprehensively compared, and results consistently show CL methods reduce attack success rates versus standard fine-tuning.

**Medium confidence:** DER's claimed best safety-utility balance is supported but evaluation is limited to three specific tasks; generalizability to other domains needs further validation.

**Medium confidence:** Framing of safety degradation as catastrophic forgetting is conceptually sound but could benefit from more evidence that this is the primary mechanism versus other factors.

## Next Checks
1. Evaluate preserved safety models on real-world safety benchmarks and human-evaluated scenarios to verify practical safety improvements
2. Conduct ablation studies isolating contributions of different CL components to understand mechanisms driving safety preservation
3. Test models on non-safety-related downstream tasks to assess unintended side effects on general capabilities