---
ver: rpa2
title: 'From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts'
arxiv_id: '2506.16912'
source_url: https://arxiv.org/abs/2506.16912
tags:
- frequency
- accuracy
- facts
- fact
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel framework for evaluating language
  model sample efficiency by analyzing how well models recall factual knowledge relative
  to the frequency of that knowledge in training data. The authors match subject-object
  pairs from the BEAR probe to sentences in a Wikipedia corpus to estimate fact frequencies,
  then propose two metrics: a weighted accuracy score that emphasizes low-frequency
  facts, and a power-law model that quantifies the probability of correct recall as
  a function of fact exposure.'
---

# From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts

## Quick Facts
- arXiv ID: 2506.16912
- Source URL: https://arxiv.org/abs/2506.16912
- Reference count: 40
- Models that achieve higher accuracy on rare facts while maintaining strong performance on common ones demonstrate superior sample efficiency

## Executive Summary
This paper introduces a novel framework for evaluating language model sample efficiency by analyzing how well models recall factual knowledge relative to the frequency of that knowledge in training data. The authors match subject-object pairs from the BEAR probe to sentences in a Wikipedia corpus to estimate fact frequencies, then propose two metrics: a weighted accuracy score that emphasizes low-frequency facts, and a power-law model that quantifies the probability of correct recall as a function of fact exposure. Experiments with multiple architectures (GPT2, LLAMA, XLSTM, MAMBA) and sizes (200M vs 400M parameters) show that larger models and LLAMA in particular achieve significantly better performance on rare facts, with less improvement on high-frequency facts.

## Method Summary
The study evaluates sample efficiency by measuring how well language models recall relational facts (subject-relation-object triples) from the BEAR probe, relative to how frequently those facts appear in a Wikipedia corpus. Fact frequencies are estimated using a co-occurrence matching heuristic, then models are trained on the same corpus and evaluated across 42 checkpoints. Two metrics are proposed: a weighted accuracy score (WASB) that emphasizes performance on rare facts using exponential weighting, and a power-law scaling parameter (α_m) that models the probability of correct recall as a function of exposure count. The joint maximum likelihood fitting procedure ensures fair comparison across architectures.

## Key Results
- LLAMA models consistently outperform GPT2, XLSTM, and MAMBA 2 across all metrics, with larger models showing better sample efficiency
- Weighted accuracy scores reveal that most models perform similarly on high-frequency facts (≥1024) but differ notably on low-frequency facts (0-2 occurrences)
- The α-parameter from the power-law model provides a robust measure of sample efficiency, with LLAMA 360M achieving α_m=0.120 vs GPT2 355M at 0.098

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency-weighted evaluation reveals sample efficiency differences masked by raw accuracy.
- Mechanism: By bucketing facts by corpus frequency and weighting low-frequency buckets higher (w_i = exp(-λl_i), λ=0.05), the metric amplifies performance differences on rare facts where models diverge most, while compressing scores on high-frequency facts where all models converge.
- Core assumption: Sample-efficient models should achieve higher accuracy on rare facts while maintaining strong performance on common ones; high-frequency fact performance is capacity-saturated and less diagnostic.
- Evidence anchors:
  - [abstract] "most models perform similarly on high-frequency facts but differ notably on low-frequency facts"
  - [Section 4.1.2, Table 2] Small GPT2 achieves 83.4% on high-frequency facts (≥1024), comparable to medium MAMBA 2 at 81.4%; low-frequency splits show clear size-based ordering
  - [corpus] Related work on long-tailed learning (Zhang et al., 2024, cited in intro) supports that natural text follows long-tailed information distributions
- Break condition: If probe facts are not representative of pre-training distribution, or if frequency buckets have too few samples for reliable accuracy estimation, weighting amplifies noise rather than signal.

### Mechanism 2
- Claim: A power-law scaling function models the probability of correct fact recall as a function of exposure count, with architecture-specific slope parameter α_m indicating sample efficiency.
- Mechanism: Fit F(x) = 1 - (L_0 + x_0)/(1 + x)^{α_m} via maximum likelihood across all models and facts jointly. L_0 and x_0 are dataset-level parameters; α_m is model-specific. Higher α_m means steeper probability gain per additional exposure.
- Core assumption: The probability of correctly recalling a fact follows a smooth monotonic function of exposure frequency, not a binary step transition.
- Evidence anchors:
  - [Section 3.3.2] "α_m controls the slope of the probability function: Higher values increase the probability per additional occurrence, indicating higher sample efficiency"
  - [Section 4.1.2, Table 1] LLAMA 360M achieves α_m = 0.120 vs GPT2 355M at 0.098, consistent with LLAMA's higher sample efficiency
  - [corpus] Godey et al. (2024) and Lu et al. (2024) identify power-law relationships in geographic knowledge and fact memorization respectively
- Break condition: If facts are learned through indirect evidence (related facts, reasoning) rather than direct exposure, the frequency-recall relationship becomes noisy; the matching heuristic may underestimate effective exposure.

### Mechanism 3
- Claim: Architecture differences in sample efficiency emerge primarily in low-to-mid frequency regimes, with transformer-based LLAMA showing consistent advantages over RNN-based XLSTM and state-space MAMBA 2 at comparable parameter counts.
- Mechanism: Architectures with better parameter efficiency (LLAMA's RMSNorm, RoPE, SwiGLU) may compress factual knowledge more effectively, requiring fewer exposures to reach retrieval threshold. The gap widens in data-scarce regimes where architectural inductive biases matter more.
- Core assumption: Architectural components (normalization, positional encoding, activation functions) affect how efficiently parameters encode and retrieve factual associations.
- Evidence anchors:
  - [Section 4.1.2] "LLAMA models consistently outperform other architectures across all metrics"
  - [Section 4, Table 1] Medium LLAMA (360M): ACC=34.4%, WASB=27.9%, α_m=0.120 vs Medium MAMBA 2 (432M): ACC=32.1%, WASB=26.2%, α_m=0.106
  - [corpus] Haller et al. (2024) suggests RNN-based architectures may have advantages in data-scarce scenarios, but results here show transformers leading
- Break condition: Findings limited to small-medium models (≤432M parameters); scaling trends may differ at billions of parameters where capacity constraints ease.

## Foundational Learning

- Concept: **Long-tailed distributions in natural language**
  - Why needed here: The entire evaluation framework assumes facts follow a long-tailed frequency distribution; understanding this explains why rare-fact performance is the differentiating factor.
  - Quick check question: Can you sketch a histogram where 80% of occurrences come from 20% of unique items?

- Concept: **Power-law scaling relationships**
  - Why needed here: The probabilistic metric assumes fact-learning probability scales as a power function of exposure count, analogous to neural scaling laws.
  - Quick check question: On a log-log plot, what shape does a power-law relationship produce?

- Concept: **Knowledge probing via multiple-choice likelihood ranking**
  - Why needed here: BEAR probe converts factual recall into comparing log-likelihoods across statement variants; this enables evaluation of both causal and masked LMs.
  - Quick check question: Given statements "Paris is the capital of France" and "Berlin is the capital of France," how would you use an autoregressive LM's output to determine which it considers correct?

## Architecture Onboarding

- Component map:
  FactMatcher (spaCy lemmatization + sentence splitting) -> Entity alias matching -> Co-occurrence counting per fact triple -> Frequency table -> 16 log-spaced buckets -> Bucket accuracy computation -> Weighted sum with exponential weights -> α-parameter fitting via joint MLE

- Critical path:
  1. Run FactMatcher on pre-training corpus to generate fact frequency table
  2. Train models with shared tokenizer and hyperparameters on same corpus
  3. Probe checkpoints with BEAR at regular intervals
  4. Compute WASB and fit α_m using pooled data across all checkpoints

- Design tradeoffs:
  - **Simple matching vs. NLP pipeline**: Co-occurrence is fast and flexible but may miss implicit relations or generate false positives from ambiguous entity mentions
  - **Bucket granularity**: Finer buckets give better resolution but require more samples per bucket for statistical reliability
  - **Joint vs. per-model fitting**: Joint fitting of L_0, x_0 enables fair α_m comparison but assumes shared dataset characteristics

- Failure signatures:
  - **Zero-frequency facts with high accuracy**: Indicates heuristic undercounts (facts learned via related information) → check x_0 parameter
  - **High-frequency facts with low accuracy**: Suggests probe-template mismatch or tokenizer issues → verify statement construction
  - **α_m decreasing during training**: May indicate overfitting to high-frequency patterns at expense of generalization

- First 3 experiments:
  1. **Baseline reproducibility**: Train GPT2-small on Wikipedia dump, run FactMatcher, probe with BEAR, verify WASB and α_m values match reported (ACC≈28%, WASB≈21.8%, α_m≈0.084)
  2. **Frequency-split robustness test**: Create low/high frequency splits (80/20 bias), compare α_m stability vs. raw accuracy variance
  3. **Checkpoint dynamics analysis**: Plot WASB and α_m across 42 training slices; identify convergence point and correlate with final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed trend of increasing sample efficiency with model size persist for architectures exceeding one billion parameters?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "Whether the observed trend of increasing sample efficiency with model size holds for larger models exceeding one billion parameters remains open."
- Why unresolved: Resource constraints limited the study to small and medium-sized models (ranging from 172M to 432M parameters).
- Evidence: Training and probing larger LMs (e.g., 1B+ or 7B parameters) on the same corpus to verify if the scaling coefficient (α_m) and weighted accuracy continue to increase at the same rate.

### Open Question 2
- Question: How does the accuracy of the fact-frequency estimation change when replacing simple co-occurrence heuristics with advanced semantic parsing?
- Basis in paper: [inferred] The paper notes the methodological limitation of using a "simple fact-matching heuristic" (FactMatcherSimple) which assumes a relation exists if entities co-occur in a sentence, potentially missing nuances or creating false positives.
- Why unresolved: While the authors assume the heuristic is sufficient for relative frequencies, they acknowledge that ambiguity and relation misidentification could be reduced by NLP pipelines like entity linking, which were not tested.
- Evidence: Comparing the frequency counts generated by the simple heuristic against a more complex relation extraction model to quantify the noise and determine its impact on the final efficiency scores.

### Open Question 3
- Question: Can an adaptive bucketing strategy be developed to ensure the Weighted Accuracy Score (WASB) remains robust across datasets with varying fact density?
- Basis in paper: [explicit] The authors state: "Further investigation is needed to determine if there are robust ways to set the boundaries of the buckets based on the fact frequencies... to mitigate their impact."
- Why unresolved: The current metric relies on discrete frequency buckets, which can lead to unreliable scores when the number of samples within a specific bucket is low.
- Evidence: Testing the variance of the WASB metric on subsampled datasets using fixed versus adaptive bucket boundaries (e.g., quantile-based binning) to see which approach yields lower variance.

## Limitations

- Fact frequency estimation accuracy is limited by the simple co-occurrence matching heuristic, which may undercount effective exposures for certain fact types
- All experiments use models ≤432M parameters, so claims about architectural sample efficiency may not hold at larger scales where capacity constraints differ
- The BEAR probe's templated statements may not uniformly represent how facts appear in natural text, introducing potential probe representation bias

## Confidence

**High confidence**: The weighted accuracy metric (WASB) reliably demonstrates that larger models and LLAMA architecture outperform smaller models on low-frequency facts while showing similar performance on high-frequency facts. This finding is robust across multiple frequency distributions and directly observable from the accuracy tables.

**Medium confidence**: The α-parameter as a sample efficiency measure is theoretically sound and shows consistent ordering across architectures, but its absolute interpretation depends on the accuracy of fact frequency estimation. The power-law assumption appears reasonable given the fit quality, but deviations from this model would affect quantitative comparisons.

**Low confidence**: Claims about specific architectural advantages (e.g., LLAMA's RMSNorm vs MAMBA's state-space mechanisms) contributing to sample efficiency are plausible but not directly validated. The experiments show correlation between architecture and performance but cannot isolate which architectural components drive the differences.

## Next Checks

1. **Frequency estimation validation**: Generate a ground-truth subset by manually annotating 100 random sentences from Wikipedia for fact co-occurrences, then compare FactMatcher output against human counts to quantify systematic bias in the matching heuristic.

2. **Probe template robustness test**: Create alternative probe templates for the same facts (varying syntactic structure, adding context, changing relation phrasing) and measure WASB/α stability. High sensitivity to template choice would indicate representation bias in the evaluation.

3. **Scaling relationship verification**: Train one additional model size (e.g., 800M parameters) and plot α and WASB across the full parameter range. Verify whether the observed architectural differences persist or converge at larger scales, and check if the power-law probability model maintains good fit quality.