---
ver: rpa2
title: 'Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy'
arxiv_id: '2510.07022'
source_url: https://arxiv.org/abs/2510.07022
tags:
- label
- unlearning
- data
- client
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness and data distribution challenges
  in federated unlearning, focusing on the unrealistic assumptions of synthetic data
  splits and the unfairness of retraining all clients during unlearning requests.
  The authors introduce FedCCCU, a fairness-aware federated unlearning method that
  uses a cross-client constraint mechanism to identify and selectively edit only the
  most sensitive neurons related to the forgetting task, minimizing performance degradation
  for non-forgetting clients.
---

# Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy

## Quick Facts
- arXiv ID: 2510.07022
- Source URL: https://arxiv.org/abs/2510.07022
- Reference count: 7
- Primary result: Cross-domain heterogeneity exposes fundamental weaknesses in existing FU methods; FedCCCU achieves effective forgetting while preserving non-requesting client performance.

## Executive Summary
This paper addresses fairness and data distribution challenges in federated unlearning, focusing on the unrealistic assumptions of synthetic data splits and the unfairness of retraining all clients during unlearning requests. The authors introduce FedCCCU, a fairness-aware federated unlearning method that uses a cross-client constraint mechanism to identify and selectively edit only the most sensitive neurons related to the forgetting task, minimizing performance degradation for non-forgetting clients. They benchmark existing methods under realistic cross-domain data scenarios using heterogeneous datasets (e.g., MNIST, SVHN, USPS, CIFAR, ImageNet), showing that current techniques either fail to forget effectively or cause significant collateral damage. FedCCCU achieves more effective forgetting (e.g., reducing target class accuracy from 94.33% to 16.55%) while better preserving performance on retained data, outperforming exact and approximate methods under real-world conditions.

## Method Summary
FedCCCU addresses federated unlearning in heterogeneous cross-domain settings by computing gradient-based attribution scores to identify class-sensitive neurons, then applying a cross-client dominance constraint to selectively edit only neurons critical to the forgetting client while preserving others' knowledge. The method computes sensitivity scores per neuron per class using integrated gradients, aggregates these scores across clients, and calculates dominance ratios to identify "dominant neurons" that are important for forgetting but not for others. Only these dominant neurons are zeroed out, achieving effective forgetting while maintaining fairness across non-requesting clients.

## Key Results
- Cross-domain Real-Noniid benchmark exposes fundamental weaknesses in existing FU methods that synthetic splits obscure
- FedCCCU reduces target class accuracy from 94.33% to 16.55% while preserving non-target client performance better than exact and approximate methods
- Current techniques either fail to forget effectively (Delete-Retrain leaves >82% accuracy) or cause severe collateral damage (Neuron-Zeroing drops non-target accuracy to 0%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective neuron editing based on cross-client dominance ratios enables forgetting while preserving non-requesting client performance.
- **Mechanism:** The Cross-Client Constraint computes a ratio R = S_max_other / S_forget for each sensitive neuron. Low R indicates the neuron is "dominant" (critical for the forgetting client but not for others); high R indicates a "shared" neuron that should not be modified. Only dominant neurons are zeroed, preserving knowledge critical to non-forgetting clients.
- **Core assumption:** Neurons exhibit differential importance across clients in heterogeneous federated settings; neurons with low cross-client contribution can be safely removed without cascading degradation.
- **Evidence anchors:**
  - [abstract] "FedCCCU achieves more effective forgetting (e.g., reducing target class accuracy from 94.33% to 16.55%) while better preserving performance on retained data"
  - [section: Neuron Dominant Computing] "If R is small, it indicates that the contribution of the neuron to the forgetting client is significantly greater than its contribution to any non-forgetting client, and it can be considered a 'dominant neuron.'"
  - [corpus] Related work (FedShard, FUPareto) addresses fairness in FU through efficiency/utility tradeoffs, but FedCCCU's neuron-dominance approach is distinct and not directly validated externally.
- **Break condition:** If neurons are highly shared across clients (R ≈ 1 for most neurons), selective editing will either fail to forget or cause collateral damage—method depends on existence of class-dominant neurons.

### Mechanism 2
- **Claim:** Gradient-based attribution scores identify class-sensitive neurons without requiring access to other clients' data.
- **Mechanism:** For each neuron, compute an Attribution Score by integrating the gradient of class prediction probability from activation 0 to β (original activation). This quantifies each neuron's contribution to classifying a sample as target class c. Scores are averaged over local data to produce sensitivity scores per neuron per class.
- **Core assumption:** Gradients of output probability with respect to neuron activations faithfully capture class-specific functional importance; local data is representative of the client's distribution.
- **Evidence anchors:**
  - [section: Identification of Key Neurons] "DEPN effectively quantifies the contribution of individual neurons to the model's output through a gradient-based attribution approach."
  - [section: Eq. 2-4] Formal definition of Att(w, x) and S^l_k as cumulative gradient integrals and averaged sensitivity scores.
  - [corpus] No direct corpus validation of gradient-based attribution in federated unlearning; DEPN (Wu et al. 2023) is cited but is a centralized method adapted here.
- **Break condition:** If gradient-based attribution fails to capture true functional importance (e.g., due to saturation, batch normalization effects, or adversarial gradients), sensitive neuron identification will be noisy or misleading.

### Mechanism 3
- **Claim:** Real-world cross-domain heterogeneity exposes fundamental weaknesses in existing FU methods that synthetic splits obscure.
- **Mechanism:** The Real-Noniid benchmark assigns distinct datasets (e.g., MNIST, SVHN, USPS) to different clients, preserving label space but introducing feature distribution shift (resolution, texture, modality). This reveals that exact retraining fails to forget effectively (target accuracy remains >96% in some cases), while aggressive approximate methods cause severe collateral damage (>27% drops on non-target classes).
- **Core assumption:** Cross-domain feature heterogeneity is the dominant challenge in realistic FL; label-skew-only (pseudo-non-iid) splits are insufficient proxies.
- **Evidence anchors:**
  - [abstract] "Most FU evaluations rely on synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity."
  - [section: Table 4-6] Quantitative evidence that Delete-Retrain achieves limited forgetting (82.77% remaining accuracy) while Neuron-Zeroing causes 0% accuracy on non-target clients.
  - [corpus] FUPareto and FedShard also address utility-fairness tradeoffs but do not explicitly model cross-domain feature heterogeneity as a primary failure mode.
- **Break condition:** If future work shows that feature heterogeneity can be normalized or adapted away (e.g., via domain adaptation layers), the benchmark gap may narrow—but current evidence suggests real-world conditions remain challenging.

## Foundational Learning

- **Concept: Federated Learning (FL) aggregation dynamics**
  - **Why needed here:** FedCCCU operates on the global model after aggregation; understanding how local updates compose into global parameters is essential to reason about where forgetting edits propagate.
  - **Quick check question:** Can you explain why a local model edit on one client's contribution affects the global model's behavior on other clients' data distributions?

- **Concept: Gradient-based attribution / sensitivity analysis**
  - **Why needed here:** The core technical step in FedCCCU is computing attribution scores via integrated gradients; understanding why gradients proxy functional importance is necessary to debug and trust the neuron selection process.
  - **Quick check question:** Given a neuron with zero gradient for a class, what does that imply about its role in that class's prediction?

- **Concept: Exact vs. approximate unlearning tradeoffs**
  - **Why needed here:** FedCCCU is an approximate method; distinguishing between theoretical guarantees (exact retraining) and practical effectiveness (approximate editing) frames the design space and failure modes.
  - **Quick check question:** What computational cost does exact unlearning impose, and why is it considered "unfair" in multi-client settings?

## Architecture Onboarding

- **Component map:**
  1. Sensitivity computation module (client-side): Computes attribution scores S^l_k for each neuron per class using local data.
  2. Score aggregation (server-side): Collects top-N sensitive neuron indices and scores from each client.
  3. Dominance ranking module (server-side): Computes R = S_max_other / S_forget for each candidate neuron; identifies dominant neurons (low R).
  4. Selective editing module (server-side): Zeros weights of top-n dominant neurons; broadcasts edited global model.
  5. Benchmark harness: Real-Noniid data partitioner (cross-domain datasets) + evaluation metrics (target class accuracy, non-target class preservation).

- **Critical path:**
  1. Receive unlearning request (client c, class y).
  2. Broadcast global model θ to all clients.
  3. Each client computes local sensitivity scores for class y (Eq. 2-4).
  4. Clients upload top-N (index, score) pairs.
  5. Server computes R ratios, ranks by dominance, selects top-n neurons.
  6. Server zeros selected neurons, updates global model.
  7. Evaluate forgetting effectiveness (target accuracy drop) and fairness (non-target client accuracy retention).

- **Design tradeoffs:**
  - N (sensitivity upload count) vs. communication cost: Larger N captures more candidates but increases upload overhead.
  - n (neurons to zero) vs. forgetting/retention balance: Larger n increases forgetting but risks collateral damage; tuning required per task.
  - Local-only vs. global retraining: FedCCCU avoids global retraining for fairness but may sacrifice theoretical unlearning guarantees.

- **Failure signatures:**
  - Over-forgetting: Target class accuracy drops to ~0% on all clients → R threshold too low, or neurons are highly shared; zeroing propagates globally.
  - Under-forgetting: Target class accuracy remains high (>80%) → R threshold too high, or sensitive neurons are not truly class-specific.
  - Collateral degradation: Non-target class accuracy drops >10% on non-forgetting clients → dominant neuron selection insufficiently selective; shared neurons mistakenly edited.

- **First 3 experiments:**
  1. Sanity check: On a single-domain setup (e.g., CIFAR10-only), verify that FedCCCU's neuron selection matches known class-specific filters (compare against ablation where all sensitive neurons are zeroed).
  2. Cross-domain stress test: Run FedCCCU on Real-Noniid Image Recognition(9) split (CIFAR10 vs. ImageNet); plot target accuracy vs. non-target accuracy for varying n (neurons zeroed).
  3. Fairness audit: Compare per-client accuracy distributions before/after unlearning; ensure non-requesting clients' accuracy variance remains within acceptable bounds (e.g., <5% mean drop).

## Open Questions the Paper Calls Out
- **Open Question 1:** How can we mathematically define and optimize the trade-off boundary between maximizing unlearning completeness (forgetting precision) and minimizing collateral performance degradation on non-requesting clients?
- **Open Question 2:** Does the neuron attribution mechanism used in FedCCCU provide robustness against adversarial unlearning requests or membership inference attacks compared to exact unlearning methods?
- **Open Question 3:** How does the computational overhead of calculating per-neuron attribution scores scale with model complexity and client dataset size in large-scale production environments?

## Limitations
- Dominance ratio threshold tuning is critical but lacks principled guidance, affecting forgetting vs. fairness tradeoffs
- Real-Noniid benchmark assumes aligned label spaces across clients, not testing true real-world heterogeneity
- Scalability of attribution computation to large models or high-dimensional data remains unclear

## Confidence
- **High confidence**: Cross-domain heterogeneity exposes weaknesses in existing FU methods (validated by empirical tables)
- **Medium confidence**: Gradient-based attribution reliably identifies class-specific neurons in federated settings (adapted from centralized DEPN, not externally validated in FU context)
- **Low confidence**: Dominance ratio-based selective editing achieves both effective forgetting and fairness (methodologically sound but sensitive to threshold tuning)

## Next Checks
1. Sensitivity analysis on R threshold: Systematically vary the dominance ratio cutoff and measure its impact on forgetting effectiveness vs. non-requesting client accuracy retention
2. Attribution robustness test: Compare FedCCCU's neuron selection against ground-truth class-specific filters in a single-domain setting (e.g., CIFAR10-only) to validate attribution quality
3. Overhead benchmarking: Measure per-client communication and computation costs for attribution score uploads and neuron editing under varying model sizes and client counts