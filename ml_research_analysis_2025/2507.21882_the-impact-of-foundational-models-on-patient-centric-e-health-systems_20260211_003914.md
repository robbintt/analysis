---
ver: rpa2
title: The Impact of Foundational Models on Patient-Centric e-Health Systems
arxiv_id: '2507.21882'
source_url: https://arxiv.org/abs/2507.21882
tags:
- applications
- software
- health
- maturity
- nayebi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed 116 patient-centric e-Health applications to
  assess the integration and maturity of AI functionalities, particularly those enabled
  by Health Foundational Models (HFMs). Using Large Language Models (LLMs) fine-tuned
  with human-verified data, the researchers extracted and consolidated key app features
  from Google Play Store descriptions.
---

# The Impact of Foundational Models on Patient-Centric e-Health Systems

## Quick Facts
- arXiv ID: 2507.21882
- Source URL: https://arxiv.org/abs/2507.21882
- Reference count: 40
- Primary result: 86.21% of analyzed patient-centric e-Health apps remain at early AI maturity stages, highlighting a gap between HFM potential and real-world deployment.

## Executive Summary
This study analyzed 116 patient-centric e-Health applications to assess the integration and maturity of AI functionalities, particularly those enabled by Health Foundational Models (HFMs). Using Large Language Models (LLMs) fine-tuned with human-verified data, the researchers extracted and consolidated key app features from Google Play Store descriptions. They then mapped these features onto Gartner's AI Maturity Model (AIMM) to classify each app's AI integration level. The analysis revealed that 86.21% of apps remain at early AI maturity stages (Awareness and Active), while only 13.79% demonstrate advanced integration (Operational or Systematic). Co-occurrence analysis of features highlighted common functionalities like health data provision and symptom tracking, but also revealed limited combination of complementary features. Correlation analysis showed a weak but positive link between AI maturity and user ratings/install counts, suggesting that more mature AI integration may enhance user engagement. The study highlights the current gap between HFM potential and real-world deployment, calling for more sophisticated, user-centric, and ethically aligned AI implementations in patient-facing health apps.

## Method Summary
The researchers scraped 116 patient-centric e-Health apps from Google Play Store using specific keywords. They extracted functional features from app descriptions using a fine-tuned Llama3-1-8b LLM, with human verification and iterative refinement. Features were consolidated using BERT embeddings and cosine similarity clustering. Apps were manually classified into Gartner's AI Maturity Model levels based on their AI integration. The team analyzed feature co-occurrence patterns and calculated correlations between AI maturity levels and app popularity metrics (user ratings and install counts).

## Key Results
- 86.21% of apps operate at early AI maturity stages (Awareness and Active)
- Only 13.79% demonstrate advanced AI integration (Operational or Systematic)
- Weak but positive correlation (r=0.55) between AI maturity and user ratings/install counts
- Common features include health data provision and symptom tracking, with limited combination of complementary functionalities

## Why This Works (Mechanism)
The study's methodology works by leveraging automated feature extraction through fine-tuned LLMs to efficiently process unstructured app descriptions at scale, while maintaining accuracy through human verification and iterative refinement. The use of Gartner's AI Maturity Model provides a standardized framework for classifying AI integration levels, enabling meaningful comparison across diverse applications. The combination of feature co-occurrence analysis and correlation with popularity metrics reveals both technical patterns and potential user engagement implications of AI maturity.

## Foundational Learning

- **Concept: Gartner AI Maturity Model (AIMM)**
  - Why needed here: This is the primary classification framework used in the paper to assess the level of AI integration in e-Health apps.
  - Quick check question: Can you name the five levels of the Gartner AIMM and describe the key difference between the "Active" and "Operational" stages?

- **Concept: Health Foundation Models (HFMs)**
  - Why needed here: The paper's context is the gap between the potential of HFMs (specialized models for biomedical data) and their current, limited deployment in patient-facing apps.
  - Quick check question: What distinguishes a Health Foundation Model from a general-purpose Large Language Model (LLM) like GPT-4?

- **Concept: Correlation vs. Causation**
  - Why needed here: The study finds a positive correlation between AI maturity and app popularity but explicitly avoids claiming causation. Understanding this distinction is critical for interpreting the results.
  - Quick check question: If higher-rated apps also have higher AI maturity, what is an alternative factor (besides the AI itself) that could explain this relationship? (Hint: The paper suggests one).

## Architecture Onboarding

- **Component map:**
  1. Data Ingestion: `google-play-scraper` -> Raw App Data (descriptions, metadata)
  2. Feature Extraction Engine: Fine-tuned `llama3.1-8b` LLM -> Raw Features
  3. Feature Processing Pipeline: Normalization/Lemmatization -> `bert-base-nli-mean-tokens` embeddings -> Cosine Similarity Clustering -> Refined Features
  4. Classification Module: Expert-driven mapping of Refined Features to Gartner's AIMM levels
  5. Analysis Module: Co-occurrence matrices & correlation calculations (AI maturity vs. ratings/installs)

- **Critical path:**
  The most critical and labor-intensive path is the **Feature Extraction & Refinement Pipeline**. The validity of the entire study hinges on the quality of features extracted from unstructured text. The process involves an iterative loop: LLM extraction -> human verification -> fine-tuning -> re-extraction -> crowdsourced consolidation. This hybrid approach is designed to mitigate LLM hallucinations and ensure feature accuracy.

- **Design tradeoffs:**
  - Scope vs. Depth: The study analyzed 116 patient-centric apps. This provides a focused snapshot but sacrifices the breadth of a larger, less-filtered dataset.
  - Automated vs. Manual Classification: Feature extraction is automated, but the final maturity classification is manual. This trades scalability for higher confidence in the nuanced mapping to AIMM levels.
  - Description vs. Reality: The analysis is based on app store descriptions, not direct testing of app functionality. This is efficient but relies on the accuracy and completeness of developer-provided text.

- **Failure signatures:**
  - High Hallucination Rate in Extraction: If the initial LLM extraction had a high inaccuracy rate (the paper notes 6.16%), it could corrupt the feature set. The mitigation is the fine-tuning and human verification loop.
  - Ambiguous Maturity Mapping: Some apps may have features that span multiple maturity levels, leading to subjective classification. The paper notes this threat to construct validity.
  - Non-Representative Sample: The final 116 apps were filtered from an initial 186. If the exclusion criteria were too strict, the findings may not generalize to the broader ecosystem.

- **First 3 experiments:**
  1. Replicate the Feature Extraction Pipeline: Select a small sample of apps (e.g., n=10) and manually extract features. Then, compare these "ground truth" features with the output of the fine-tuned `llama3.1-8b` model to evaluate its precision and recall.
  2. Test Correlation Stability: Perform a sensitivity analysis on the correlation between AI maturity and popularity. Remove the top and bottom 5% of apps by install count to see if the correlation holds, or if it's driven by outliers.
  3. Manual Co-occurrence Audit: Take the co-occurrence heatmap from Figure 3 and audit 2-3 cases of low co-occurrence (e.g., "Symptom Tracking" and "Mood Tracking"). Manually review the top apps in each category to confirm whether the features are truly absent or simply not mentioned in the description.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does higher AI maturity directly cause improved user retention and app success, or are these outcomes driven by confounding factors like development funding or team capability?
- Basis in paper: [explicit] The authors state in Section V that "causality cannot be conclusively established" regarding the link between AI maturity and popularity, noting that success might be a consequence of "more capable development teams" rather than the AI itself.
- Why unresolved: This study relied on a static snapshot of popularity metrics (ratings/install counts) which are correlational, not longitudinal measures of retention or causality.
- What evidence would resolve it: Longitudinal studies or controlled experiments that isolate AI integration levels as the independent variable while controlling for confounding factors like budget and developer expertise.

### Open Question 2
- Question: How can AI maturity models be refined to specifically evaluate healthcare applications based on clinical validation, user trust, and system interoperability?
- Basis in paper: [explicit] The authors conclude in Section V.C that "Future work may explore more nuanced maturity models tailored to healthcare applications, incorporating clinical validation, user trust, and interoperability dimensions."
- Why unresolved: The current study utilized the general Gartner AI Maturity Model, which lacks specific metrics for healthcare-specific constraints like clinical safety and data interoperability.
- What evidence would resolve it: The development and validation of a domain-specific maturity framework that successfully categorizes apps using dimensions unique to clinical workflows and patient safety.

### Open Question 3
- Question: Who are the primary users of AI-enabled features in patient-centric apps—patients or providers—and for what specific purposes are they employed?
- Basis in paper: [explicit] Section I states that "it is not yet clear who the primary users of these AI-enabled features are, patients or health care providers, and for what specific purposes they are being employed."
- Why unresolved: The methodology relied on mining app descriptions, which list features but do not distinguish between the intended end-user (patient vs. clinician) or the specific context of use.
- What evidence would resolve it: Qualitative analysis of user reviews or empirical usage data that identifies the distinct personas utilizing specific AI features within the apps.

## Limitations

- The study relies on app store descriptions rather than direct app functionality testing, which may lead to misclassification of actual AI capabilities
- Manual classification of apps into AIMM levels introduces subjectivity and potential bias
- The correlation between AI maturity and user ratings (r=0.55) is moderate and may be influenced by confounding factors such as marketing budgets or established brand reputation

## Confidence

- **High Confidence:** The methodology for feature extraction using fine-tuned LLMs with human verification is robust and well-documented. The finding that 86.21% of apps remain at early AI maturity stages (Awareness and Active) is well-supported by the data.
- **Medium Confidence:** The correlation analysis between AI maturity and user ratings/installs shows a positive relationship but requires caution in interpretation due to potential confounding variables. The co-occurrence analysis reveals patterns but may be influenced by developer emphasis on certain features in descriptions.
- **Low Confidence:** Claims about causation between AI maturity and app success are explicitly avoided but could be misinterpreted by readers. The generalizability of findings beyond the 116-app sample is uncertain.

## Next Checks

1. **Feature Extraction Validation:** Replicate the feature extraction pipeline on a small random sample (n=10) and compare LLM-extracted features with manual ground truth to assess precision and recall rates.

2. **Correlation Sensitivity Analysis:** Remove the top and bottom 5% of apps by install count and recalculate the correlation between AI maturity and user ratings to test if outliers drive the observed relationship.

3. **Cross-Platform Verification:** Apply the same methodology to Apple App Store descriptions for the same apps (where available) to assess whether the maturity classifications remain consistent across platforms and whether store-specific presentation affects AI feature visibility.