---
ver: rpa2
title: Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too
arxiv_id: '2509.05440'
source_url: https://arxiv.org/abs/2509.05440
tags:
- summary
- story
- evaluation
- article
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a direct-scoring NLG evaluator that uses synthetic
  summaries to simulate pairwise comparisons, addressing the limitation of pairwise
  methods that cannot produce absolute scores for thresholding use cases. The method
  generates synthetic summaries of varying quality for each document, then computes
  weighted scores by comparing a machine-generated summary to each synthetic reference
  and aggregating the probabilities over "Better/Worse/Similar" judgments.
---

# Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too

## Quick Facts
- arXiv ID: 2509.05440
- Source URL: https://arxiv.org/abs/2509.05440
- Reference count: 24
- Primary result: Direct-scoring NLG evaluator using synthetic summaries achieves sample-level correlations comparable to state-of-the-art pairwise methods (+0.03, -0.03, +0.05 average improvements across three benchmarks)

## Executive Summary
This paper addresses the limitation of pairwise NLG evaluators that cannot produce absolute scores for thresholding use cases. The proposed method generates synthetic summaries of varying quality for each document, then computes weighted scores by comparing a machine-generated summary to each synthetic reference and aggregating the probabilities over "Better/Worse/Similar" judgments. Evaluations on SummEval, TopicalChat, and HANNA benchmarks show that the proposed method achieves sample-level correlations comparable to state-of-the-art pairwise evaluators, with average improvements of +0.03, -0.03, and +0.05 respectively over the strongest pairwise baselines.

## Method Summary
The method generates N=5 synthetic summaries per article using contrastive prompts—first generating extreme quality summaries (score 1 "worst" and score 5 "best"), then recursively generating intermediate quality summaries by prompting for content "in between" existing examples. For each target summary, the method compares it to each synthetic reference and extracts log probabilities for "Better"/"Worse"/"Similar" tokens. These probabilities are softmax-normalized and weighted by the synthetic reference scores to produce an absolute score between 1 and 5.

## Key Results
- Method achieves sample-level correlations comparable to state-of-the-art pairwise evaluators
- Average improvements of +0.03 on SummEval, -0.03 on TopicalChat, and +0.05 on HANNA
- Probability generation is critical—increases from 36.44 to 37.43 on SummEval when sampling is replaced with probability extraction
- More instruction-following LLMs yield better evaluators (0.45 vs 0.42 improvement on MMLU)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic summaries with known quality levels create a calibrated reference scale that enables absolute scoring from relative comparisons.
- Mechanism: The method generates N=5 synthetic summaries per article using contrastive prompts—first generating extreme quality summaries (score 1 "worst" and score 5 "best"), then recursively generating intermediate quality summaries by prompting for content "in between" existing examples. This produces monotonically-increasing quality references.
- Core assumption: LLMs can reliably generate summaries at distinct quality levels when given explicit contrastive examples, and these synthetic references maintain consistent quality ordering.
- Evidence anchors:
  - [abstract] "propose a direct-scoring method which uses synthetic summaries to act as pairwise machine rankings at test time"
  - [section 2.1] "we propose to generate examples of monotonically-increasing quality by first starting with the extremes of the ratings, then prompting the LLM to generate examples of intermediate quality between those previously existing"
  - [corpus] Related work on LLM-based evaluation (Liu et al. 2024a, "Aligning with human judgement") suggests pairwise preference improves alignment, supporting the comparative framework, but does not validate synthetic reference quality directly.
- Break condition: If synthetic summaries don't maintain monotonic quality ordering (e.g., score 3 summary is actually better than score 4), the weighted scoring formula breaks down.

### Mechanism 2
- Claim: Extracting probability distributions over comparative language ("Better"/"Worse"/"Similar") rather than discrete judgments enables more granular scoring.
- Mechanism: For each synthetic reference i (score 1-5), the LLM receives a pairwise comparison prompt and outputs log probabilities for three tokens. These are softmax-normalized to produce p("Better"|i), p("Worse"|i), p("Similar"|i). The final score is a weighted sum: higher probability of "Better" vs low-score references and "Worse" vs high-score references yields higher absolute scores.
- Core assumption: The probability distribution over comparative tokens correlates with actual quality differences in a way that averaging across multiple references smooths noise.
- Evidence anchors:
  - [section 2.2] "we calculate the probability of the model responding with 'Better', 'Worse', 'Similar', and take the weighted sum over the synthetic scores"
  - [section 4, Table 2] "increasing n results in monotonically increasing performance" and probability-based method (37.43) outperforms sampling-based method even at n=1000 (36.44), indicating "probability generation is essential to our method"
  - [corpus] Insufficient direct corpus evidence on why probability extraction improves over sampling; related work focuses on output quality rather than probability calibration.
- Break condition: If the LLM's token probabilities are miscalibrated (e.g., overconfident on uncertain comparisons), the weighted average may amplify errors rather than reduce them.

### Mechanism 3
- Claim: Bridging pairwise comparison methods with direct scoring preserves alignment benefits while enabling threshold-based use cases.
- Mechanism: Traditional pairwise evaluators can rank candidates but cannot assign absolute scores needed for filtering. By comparing against pre-generated references with known scores (1-5), each comparison provides evidence about absolute quality. Aggregating across N comparisons produces a continuous score suitable for thresholding.
- Core assumption: The mapping from multiple pairwise comparisons to absolute scores preserves the human alignment properties of comparison-based methods.
- Evidence anchors:
  - [section 1] "comparison-based approaches...have demonstrated superior alignment with human judgment...[but] relative judgment limits applicability to threshold-based scenarios"
  - [section 4, Table 1] Method achieves comparable average sample-level correlations: SummEval +0.03, TopicalChat -0.03, HANNA +0.05 vs. state-of-the-art pairwise evaluators
  - [corpus] Liu et al. 2024a ("Aligning with human judgement") provides evidence that pairwise preference improves LLM evaluator alignment, supporting the foundational premise.
- Break condition: If the synthetic references don't adequately cover the quality distribution of real machine outputs (e.g., all real summaries fall between synthetic scores 2-3), the method loses discriminative power.

## Foundational Learning

- Concept: Spearman correlation (ρ)
  - Why needed here: The paper uses sample-level Spearman correlation as the primary evaluation metric to measure alignment with human judgment. Understanding that this measures rank correlation (not linear relationship) is essential for interpreting the +0.03/-0.03/+0.05 improvements.
  - Quick check question: If predicted scores are [1, 2, 3] and human scores are [10, 20, 100], would Spearman correlation be 1.0 or lower?

- Concept: Softmax over log probabilities
  - Why needed here: The method extracts log probabilities for "Better"/"Worse"/"Similar" tokens, then applies softmax to create a normalized probability distribution for weighted scoring. Understanding this transformation is critical for implementing the scoring function.
  - Quick check question: If log probabilities are [-0.5, -1.0, -2.0] for "Better"/"Worse"/"Similar", which token has the highest probability after softmax?

- Concept: Sample-level vs. system-level vs. summary-level correlation
  - Why needed here: The paper explicitly focuses on sample-level correlation (averaging per-document correlations across machines) rather than system-level (correlation of averages) or summary-level (pooled correlation). This choice affects how improvements should be interpreted.
  - Quick check question: For sample-level correlation, do you compute ρ once across all summaries, or compute ρ per-document then average?

## Architecture Onboarding

- Component map:
  Synthetic Generation Module -> Comparison Module -> Scoring Aggregator -> Evaluation Pipeline

- Critical path:
  1. Generate synthetic summaries (Section 2.1) - done once per article/dimension, can be cached
  2. For each target summary, run N pairwise comparisons (Section 2.2) - done at inference time
  3. Extract log probabilities from LLM for each comparison
  4. Apply softmax and weighted sum formula to produce final absolute score

- Design tradeoffs:
  - **N value**: Paper uses N=5 (matching dataset rating scales), but N=9 achieves 37.80 vs 37.43 correlation on SummEval. Trade-off is 1.8x more comparisons for ~1% improvement.
  - **Prompt format**: Comparative prompts ("Better"/"Worse"/"Similar") achieve 37.43, while Yes/No prompts achieve 38.60 (Table 2). Paper doesn't explain why it chose the worse-performing format.
  - **LLM choice**: Stronger models (Llama-3.1-8B with 71.3 MMLU) improve scoring (0.45) more than example generation (0.42), suggesting inference-time model quality matters more than synthetic generation quality (Table 3).
  - **Computational cost**: Requires N generation calls + N comparison calls per article/dimension. For N=5 and 4 dimensions (SummEval), this is 20 LLM calls per article before any evaluation.

- Failure signatures:
  - **Non-monotonic synthetic summaries**: If score 3 summary is actually better than score 4, the weighted sum will produce incorrect absolute scores. Monitor by spot-checking synthetic summary quality.
  - **Probability miscalibration**: If LLM is overconfident (probabilities near 0 or 1 consistently), scores lose granularity. Check probability distribution variance across comparisons.
  - **Domain mismatch**: Synthetic summaries generated for news articles may not transfer to other domains without regenerating references.
  - **Dimension correlation**: High correlation between evaluation dimensions (Table 5: coherence/relevance ρ=0.787) may cause synthetic summaries optimized for one dimension to inadvertently score well on others.

- First 3 experiments:
  1. **Validate monotonicity**: For a sample of 10 articles, manually verify that synthetic summaries actually increase in quality from score 1 to 5. Report failure rate.
  2. **Ablate N vs. performance curve**: Test N ∈ {2, 3, 5, 7, 9} on a held-out validation set to find the point of diminishing returns for your specific domain and LLM.
  3. **Compare prompt formats**: Implement both comparative ("Better"/"Worse"/"Similar") and Yes/No formats from Table 2, test on your target domain, and report which achieves higher correlation with any available human judgments.

## Open Questions the Paper Calls Out
None

## Limitations
- The method requires N synthetic summary generation calls plus N comparison calls per article, making it computationally expensive compared to pairwise evaluators
- Synthetic summaries must maintain strict monotonic quality ordering, but the paper provides no systematic validation that LLM-generated references achieve this property
- The method assumes synthetic references adequately cover the quality distribution of real machine outputs, which may not hold in all domains

## Confidence

**High Confidence**: The core mathematical framework (weighted sum of probabilities from pairwise comparisons) is internally consistent and well-specified. The implementation details for log-probability extraction and softmax normalization are reproducible.

**Medium Confidence**: The correlation improvements (+0.03/-0.03/+0.05) are statistically meaningful but modest. The method achieves parity with state-of-the-art pairwise evaluators, but the practical significance of these small gains is unclear without significance testing or practical application demonstrations.

**Low Confidence**: The assumption that synthetic summaries maintain monotonic quality ordering across all articles and dimensions is unverified. The paper does not report inter-rater consistency checks or provide evidence that the recursive prompting strategy reliably produces the intended quality gradient.

## Next Checks

1. **Monotonicity Validation**: For a stratified sample of 50 articles across all datasets, conduct blinded human evaluations to verify that synthetic summaries increase in quality from score 1 to 5. Calculate the proportion of articles where at least one intermediate summary violates the expected ordering, and report the impact on correlation scores when violating summaries are excluded.

2. **Computational Cost Analysis**: Implement the full pipeline on a representative subset (100 articles) and measure total token consumption, wall-clock time, and cost per article. Compare these metrics against pairwise evaluators that can reuse reference summaries across comparisons, and quantify the break-even point where the absolute scoring benefit justifies the increased computational overhead.

3. **Transferability Assessment**: Test the method on a domain-shifted dataset (e.g., scientific paper summaries or product reviews) without regenerating synthetic references. Measure correlation degradation compared to models trained on in-domain synthetic references, and analyze whether the quality dimension interpretations transfer across domains or require domain-specific recalibration.