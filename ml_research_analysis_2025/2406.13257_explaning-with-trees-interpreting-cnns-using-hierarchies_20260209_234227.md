---
ver: rpa2
title: 'Explaning with trees: interpreting CNNs using hierarchies'
arxiv_id: '2406.13257'
source_url: https://arxiv.org/abs/2406.13257
tags:
- e-03
- e-021
- e-043
- images
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces xAiTrees, a hierarchical segmentation framework
  that combines region-based explanation methods with model-based hierarchical segmentation
  to produce interpretable and faithful explanations for CNN decision-making. The
  framework addresses the trade-off between model fidelity and human interpretability
  by leveraging hierarchical segmentation to adapt region sizes across different levels
  of abstraction, mitigating limitations of overly small or coarse regions.
---

# Explaning with trees: interpreting CNNs using hierarchies

## Quick Facts
- arXiv ID: 2406.13257
- Source URL: https://arxiv.org/abs/2406.13257
- Reference count: 40
- This paper introduces xAiTrees, a hierarchical segmentation framework that combines region-based explanation methods with model-based hierarchical segmentation to produce interpretable and faithful explanations for CNN decision-making.

## Executive Summary
This paper presents xAiTrees, a hierarchical segmentation framework that addresses the trade-off between model fidelity and human interpretability in CNN explanations. The method builds on Binary Partition Trees or Watershed segmentation, computes occlusion-based importance scores for regions, and applies persistence-based shaping to aggregate importance across multiple spatial scales. The framework was evaluated on three datasets (Cat vs. Dog, CIFAR-10, ImageNet) with VGG-16 and ResNet18 architectures, showing superior performance to baselines like LIME and XRAI in exclusion, inclusion, and Pixel Impact Rate metrics. A human subject study also demonstrated improved bias detection and identification capabilities.

## Method Summary
The xAiTrees framework constructs a hierarchical segmentation tree using either human-based Structured Edge Detection or model-based gradient attribution maps. For each tree node representing a region, it computes occlusion impact using either the Occ metric (difference in model output when masking the region) or CaOC metric (rank change of the target class). The method then applies a shaping algorithm that transforms the tree into a weighted graph and computes persistence values based on level-set connectivity, which are aggregated from root to leaves to produce the final importance scores. The framework supports both Binary Partition Trees and Watershed segmentation approaches.

## Key Results
- xAiTrees achieved over 60% class changes for Cat vs. Dog, over 80% for CIFAR-10, and over 70% for ImageNet in exclusion tests
- The method outperformed baselines in inclusion metrics and Pixel Impact Rate (PIR) across all datasets
- Human subject study showed xAiTrees configurations, particularly Tree-CaOC, achieved better bias detection and identification rates compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Importance Aggregation
- Claim: Aggregating region importance across multiple spatial scales improves explanation quality by balancing fidelity and interpretability.
- Mechanism: Build a segmentation tree (BPT or Watershed), compute occlusion attributes per node, then recursively sum persistence values from root to leaves so smaller segments inherit contextual importance from parent regions. This resolves ambiguities where similar structures appear in different contexts (e.g., cat eyes on face vs. background).
- Core assumption: Important features exist at multiple scales, and their true relevance depends on their hierarchical context.
- Evidence anchors:
  - [abstract] "hierarchical segmentation to adapt region sizes across different levels of abstraction"
  - [section 4, step 4] "This process aggregates the importance of various scales of the image into the pixels"
  - [corpus] Weak direct corpus support; related papers focus on layer-wise or ensemble interpretability, not hierarchical spatial segmentation.
- Break condition: If the target model's decision relies exclusively on fine-grained texture cues that are fragmented by the segmentation hierarchy, aggregation may dilute importance signals.

### Mechanism 2: Model-Based Segmentation Alignment
- Claim: Using gradient-based attribution maps as edge weights for tree construction produces segmentations that better reflect the model's internal reasoning.
- Mechanism: Replace human-centric edge detection with pixel-wise importance from methods like Integrated Gradients or Guided Backpropagation when constructing the initial graph for hierarchical segmentation. This causes regions to form around model-relevant features rather than perceptual boundaries.
- Core assumption: Gradient-based attribution maps approximate the model's "visual attention," and clustering high-importance pixels together preserves reasoning structure.
- Evidence anchors:
  - [section 4, step 1] "The model-based approach uses a visual representation of the image's pixels most influential in a model's decision"
  - [section 5.1] Model-based configurations (C2) achieved highest class-change rates, e.g., BP-TreeB-Occ at 80%+ for CIFAR-10
  - [corpus] No corpus papers directly test gradient-weighted hierarchical segmentation; this appears novel.
- Break condition: If gradient attributions are highly noisy or inconsistent (as noted for IG in abstract), the resulting segmentation may fragment important features or merge irrelevant ones.

### Mechanism 3: Persistence-Based Shaping for Robust Attribute Scoring
- Claim: Persistence in level-set connected components provides a more stable measure of region importance than raw occlusion magnitude.
- Mechanism: Transform the tree T into a weighted graph G where node weights are occlusion attributes. Build a new tree T' tracking connected components in upper-level sets. A node's persistence equals the "lifespan" of its maximum in T', measured by branch length. This filters out spurious local maxima.
- Core assumption: Important regions produce occlusion impacts that form stable, persistent structures in the attribute landscape.
- Evidence anchors:
  - [section 4, step 3] "A vertex of G is important according to its persistence in the level sets of G... persistence of a node is easily computed on T' by computing the length of the branch"
  - [figure 4] Demonstrates how hierarchy discriminates similar regions by contextual aggregation
  - [corpus] No corpus papers reference shaping or persistence in xAI; this draws from mathematical morphology literature.
- Break condition: If occlusion impacts are uniformly distributed or lack clear maxima (e.g., highly distributed representations), persistence scoring becomes unstable.

## Foundational Learning

- Concept: Binary Partition Trees / Hierarchical Watershed
  - Why needed here: Understanding how images decompose into tree structures where nodes = regions and leaves = pixels.
  - Quick check question: Can you explain how BPT differs from flat superpixel segmentation?

- Concept: Occlusion-Based Attribution
  - Why needed here: The method's attribute computation relies on measuring model output changes when regions are masked.
  - Quick check question: Why might raw occlusion magnitude be insufficient for comparing regions of different sizes?

- Concept: Level-Set Persistence (Shaping)
  - Why needed here: The core innovation for robust importance scoring uses persistence rather than raw attribute values.
  - Quick check question: In a weighted graph's upper-level sets, what does "persistence" of a local maximum measure?

## Architecture Onboarding

- Component map: Structured Edge Detection/Guided Backprop -> BPT/Watershed Tree Construction -> Occlusion Attribute Computation -> Persistence Shaping (T→G→T') -> Root-to-Leaf Aggregation -> Visualization

- Critical path: Segmentation tree quality → Attribute computation coverage → Persistence stability → Threshold selection for visualization

- Design tradeoffs:
  - Human-based vs model-based segmentation: interpretability vs fidelity
  - Occ vs CaOC: single-image impact vs intra-class ranking stability
  - Minimum region size: finer detail vs computational cost and noise
  - Threshold percentage: more regions shown vs visual clutter

- Failure signatures:
  - Empty or near-full masks: threshold too high/low or persistence failed
  - Fragmented important features: segmentation granularity too fine for the feature scale
  - Inconsistent results across similar images: gradient-based segmentation unstable
  - Slow runtime on large images: CaOC requires batch processing; use Occ for speed

- First 3 experiments:
  1. Reproduce Table 1 exclusion results on a held-out subset of ImageNet with VGG-16, comparing Occ vs CaOC attributes to validate implementation.
  2. Ablate human-based vs model-based segmentation on a custom bias scenario (e.g., background correlation) to measure fidelity-interpretability tradeoff.
  3. Test PIR metric sensitivity: generate explanations highlighting 10%, 25%, 50% of image and verify PIR decreases as mask grows, confirming specificity measurement works as intended.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can xAiTrees be effectively adapted for semantic segmentation tasks where the model output is already a spatial map rather than a classification logit?
- Basis in paper: [explicit] Section 6 states, "Future works will be focused on semantic segmentation."
- Why unresolved: The current framework relies on classification logits to compute the occlusion impact (attribute computation) for tree shaping. It is unclear how "impact" would be defined or measured when the model's output is a dense segmentation map rather than a discrete class probability.
- What evidence would resolve it: An adaptation of the framework where the "attribute" is derived from changes in segmentation mask overlap (e.g., IoU) or pixel-wise accuracy, demonstrated on a segmentation architecture like U-Net.

### Open Question 2
- Question: How can the framework be modified to explain representation learning tasks that do not have explicit class outputs?
- Basis in paper: [explicit] Section 6 and Appendix A.7 mention, "Applying the method to other tasks, such as representation learning... requires adaptations."
- Why unresolved: The method currently calculates region importance by observing changes in the softmax output or logits. Representation learning models (e.g., SimCLR, Autoencoders) often lack these discrete outputs, requiring a new metric for "impact" in the latent space.
- What evidence would resolve it: A study replacing the logit-difference metric with a distance metric in the latent space (e.g., cosine similarity or Euclidean distance) to score region importance.

### Open Question 3
- Question: How can the hierarchical segmentation approach be translated to non-visual data modalities like text?
- Basis in paper: [explicit] Section 6 and Appendix A.7 note that adaptation to "different modalities, such as text, requires adaptations."
- Why unresolved: The current methodology relies on 2D spatial hierarchies (BPT, Watershed) based on pixel adjacency. Text requires a sequential or grammatical hierarchy (e.g., tokens to phrases) which does not map directly to 2D spatial trees.
- What evidence would resolve it: A definition of a "tree structure" for text (e.g., using parse trees or token merging) integrated into the xAiTrees pipeline to explain NLP models.

### Open Question 4
- Question: Does the hierarchical aggregation effectively mitigate the noise and limitations of the underlying base attribution methods?
- Basis in paper: [explicit] Section 6 states, "the final technique will inherit the limitations of the base methods."
- Why unresolved: While the authors claim the hierarchy helps, it is unclear to what extent the errors from noisy pixel-wise methods (like Integrated Gradients) propagate into the final tree shape, versus being filtered out by the persistence algorithm.
- What evidence would resolve it: An ablation study comparing the stability of xAiTrees outputs when fed "noisy" versus "clean" (ground truth) base attributions, quantifying the noise suppression capability of the tree shaping step.

## Limitations
- The computational cost is significant, particularly for the CaOC metric which can take over 30 seconds per image compared to 0.6 seconds for simpler metrics
- The framework's dependency on base explanation methods like occlusion and LIME introduces variability in results
- The human subject study, while involving 41 participants, may not be sufficiently powered to detect subtle differences between methods across all bias scenarios

## Confidence
- **High confidence**: The core mechanism of hierarchical segmentation (BPT/Watershed) and basic occlusion-based attribution are well-established techniques. The exclusion and inclusion metrics provide straightforward validation of the method's ability to identify relevant regions.
- **Medium confidence**: The persistence-based shaping algorithm (T→G→T' transformation) is novel but lacks detailed mathematical specification, making exact reproduction challenging. The human subject study results show clear trends but the small sample size limits generalizability.
- **Low confidence**: The comparison with baseline methods like LIME and XRAI is complicated by their fundamentally different approaches to explanation (per-pixel vs region-based), making direct performance comparisons potentially misleading.

## Next Checks
1. Implement and verify the persistence shaping algorithm: Create a standalone implementation of the T→G→T' transformation and persistence computation, then test it on synthetic attribute landscapes to confirm it correctly identifies stable structures.

2. Conduct a controlled ablation study: Test configurations with human-based vs model-based segmentation on a custom bias scenario (e.g., images where background correlates with class) to isolate the contribution of model-aligned segmentation to explanation fidelity.

3. Validate PIR metric behavior: Generate explanations at varying mask thresholds (10%, 25%, 50%, 75% of pixels) and verify that PIR consistently decreases as more pixels are included, confirming the metric accurately measures explanation specificity.