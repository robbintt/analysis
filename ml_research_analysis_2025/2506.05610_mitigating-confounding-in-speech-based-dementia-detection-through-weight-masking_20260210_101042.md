---
ver: rpa2
title: Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking
arxiv_id: '2506.05610'
source_url: https://arxiv.org/abs/2506.05610
tags:
- train
- dementia
- confounding
- weights
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses gender confounding bias in dementia detection\
  \ using transformer-based models on speech datasets. The authors propose two methods\u2014\
  Extended Confounding Filter (ECF) and Dual Filter (DF)\u2014to isolate and ablate\
  \ gender-associated weights during model fine-tuning."
---

# Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking

## Quick Facts
- arXiv ID: 2506.05610
- Source URL: https://arxiv.org/abs/2506.05610
- Reference count: 40
- This study proposes weight-masking methods to reduce gender confounding bias in dementia detection while maintaining high AUPRC performance.

## Executive Summary
This paper addresses gender confounding bias in transformer-based dementia detection from speech transcripts. The authors propose two methods—Extended Confounding Filter (ECF) and Dual Filter (DF)—to isolate and ablate gender-associated weights during model fine-tuning. ECF sequentially unfreezes transformer layers to identify confounding-related weights, while DF compares weight changes between dementia and gender models to generate masks. Experiments on DementiaBank and Carolinas Conversation Collection datasets show both methods reduce gender disparities (∆FPR and ∆SP) while maintaining strong dementia detection performance (AUPRC). DF (MD mask) achieves the best fairness-performance trade-off, outperforming adapter-based baselines. The methods demonstrate that confounding shifts degrade model performance even with balanced label distributions, and that targeted weight masking effectively mitigates gender bias without significant accuracy loss.

## Method Summary
The approach uses BERT-base fine-tuned on dementia detection tasks, then applies two weight-masking strategies to mitigate gender confounding. Extended Confounding Filter (ECF) sequentially unfreezes transformer layers from classification head to embeddings during Phase 2 fine-tuning toward gender classification (using only healthy controls), tracking normalized weight changes to identify confounder-associated parameters. Dual Filter (DF) trains separate dementia and gender models from the same checkpoint, compares weight change patterns across the entire network, and generates masks through set operations (intersection, difference, or union). The MD mask (difference between task and confounder weight changes) achieved the best fairness-performance trade-off. Both methods apply element-wise zeroing to identified weights in the primary task model, with evaluation on shifted test distributions where α = P(dementia|female)/P(dementia|male).

## Key Results
- Dual Filter (DF) with MD mask achieves best fairness-performance trade-off, reducing gender FPR differences from 0.23 to 0.03 while maintaining AUPRC of 0.80
- ECF shows layer-specific resilience: models tolerate ablation in upper layers but degrade sharply when embedding weights are masked
- Even balanced label distributions (P(gender)=0.5, P(dementia)=0.5) suffer performance degradation under confounding shifts (α ≠ 1)
- Both methods outperform adapter-based baselines for bias mitigation in dementia detection

## Why This Works (Mechanism)

### Mechanism 1
Weights exhibiting large magnitude changes when fine-tuning toward a confounder label encode confounding information that can be selectively ablated. During Phase 2 fine-tuning toward gender classification, normalized weight updates π = (1/b)Σ|Δφᵢ| are tracked, and the top k-percentile weights by magnitude are identified as confounder-associated and zeroed via binary masking M. This works because confounder-related information is locally encoded in specific weight parameters rather than distributed uniformly.

### Mechanism 2
Comparing weight change patterns between independently fine-tuned task and confounder models enables more precise isolation of confounder-specific parameters. Dual Filter trains two models from the same pretrained checkpoint—one toward dementia, one toward gender—then generates masks through set operations: MI (intersection), MD (difference), or MI∪MD (union). The MD mask (confounder-specific weights) achieved best fairness-performance trade-off by preserving weights important for the primary task while ablating those important only for the confounder.

### Mechanism 3
Confounding information is stored differentially across transformer layers, with lower layers (especially embeddings) being more critical for task performance than upper layers. ECF sequentially unfreezes layers during probing, showing models tolerate ablation in upper layers but degrade sharply when embedding weights are masked. This suggests linguistic features critical to dementia detection are encoded early in the network, while confounding signals may concentrate in specific architectural components.

## Foundational Learning

**Concept: Confounding shift vs. subpopulation shift** - The paper's evaluation framework explicitly controls α = P(dementia|female)/P(dementia|male) to create distribution shifts. Understanding this causal structure is prerequisite to interpreting results. *Quick check:* If training has α=3 (3× more female dementia cases) and testing has α=1/3, what type of shift is this and why does it challenge models?

**Concept: Statistical parity vs. equalized odds fairness metrics** - The paper evaluates ∆FPR (error-rate parity) and ∆SP (output probability parity) separately. These capture different fairness notions with different operational implications. *Quick check:* Why might a model with equal TPR across groups still have unfair ∆FPR in a screening context?

**Concept: Weight masking vs. weight pruning** - Unlike pruning (removing "unimportant" weights), this approach removes "confounder-important" weights—opposite selection criterion with different goals. *Quick check:* If pruning removes the bottom 20% of weights by magnitude and confounder-masking removes the top 20%, what happens if both are applied?

## Architecture Onboarding

**Component map:** Pretrained BERT → Phase 1: Fine-tune toward dementia → Phase 2 (ECF): Sequential layer unfreezing toward gender → Mask generation → Apply mask → Evaluate; or Pretrained BERT → Phase 1: Fine-tune toward dementia, Phase 1: Fine-tune toward gender → Compare weight changes → Generate masks (MI, MD, MI∪MD) → Apply to dementia model → Evaluate

**Critical path:** 1) Initialize from identical pretrained checkpoint for both task and confounder models (DF) or sequential probing (ECF) 2) Train task model to convergence; checkpoint weights 3) Train confounder model (DF) OR probe layers sequentially (ECF); track normalized weight changes per batch 4) Rank weights by change magnitude; apply threshold (top k%) 5) Generate mask via set operations (DF) or layer-wise thresholding (ECF) 6) Apply mask to task model; evaluate on shifted test distribution

**Design tradeoffs:** MI mask (intersection) is more conservative, preserving task performance but potentially under-mitigating bias; MD mask (difference) is more aggressive, achieving best fairness-performance trade-off per results but risking over-ablation; Union mask removes all confounder-reactive weights, providing highest bias reduction but steepest performance drop. ECF requires N training runs for N layers (scalability issue); DF requires exactly 2 training runs regardless of architecture depth.

**Failure signatures:** Near-zero MD mask size indicates high weight entanglement between task and confounder; catastrophic performance drop at low ablation ratios suggests confounder and task representations are inseparable; no fairness improvement despite high ablation ratios indicates confounding may operate through activation patterns rather than weights.

**First 3 experiments:** 1) Baseline diagnostic: Train BERT-base on full dataset; measure AUPRC, ∆FPR, ∆SP by gender without any masking 2) Mask type ablation (DF only): Fix αtrain=3, αtest=1/3, k=10%; compare MI, MD, and MI∪MD masks on same checkpoint 3) Robustness to shift magnitude: Sweep αtrain ∈ {0.2, 0.33, 1.0, 3.0, 5.0} with αtest=1/αtrain for best-performing mask (MD); plot AUPRC and ∆FPR vs. ablation ratio

## Open Questions the Paper Calls Out

**Open Question 1:** What mechanisms explain the non-monotonic performance improvements when ablating gender-associated weights from specific transformer layers? The authors observed this phenomenon but did not investigate the underlying cause or characterize which layers consistently show this effect.

**Open Question 2:** Why does even minimal ablation of token embedding weights drastically alter dementia detection performance, unlike other layers? The embedding layer's outsized sensitivity was observed but not mechanistically explained.

**Open Question 3:** How do alternative layer freezing combinations in ECF compare to the sequential unfreezing strategy for confounding mitigation? Only sequential unfreezing from classification head to embedding was tested.

**Open Question 4:** Does the entanglement between dementia-encoding and gender-encoding weights vary systematically across different confounding shift intensities? Entanglement was visualized across configurations but not statistically analyzed as a function of shift magnitude.

## Limitations

- Architecture generalization uncertainty: Effectiveness on transformer architectures beyond BERT-base (GPT, T5, smaller models) and other domains (vision, tabular) remains unclear
- Confounder isolation assumption: Method assumes confounding information is primarily encoded in weights that change significantly during confounder fine-tuning, but effects might operate through activation patterns or attention distributions
- Dataset-specific limitations: Results demonstrated on two speech datasets with gender confounding; effectiveness for other confounders (age, dialect, socioeconomic factors) and other speech tasks requires validation

## Confidence

**High confidence:** Dual filter method reduces gender disparities (∆FPR and ∆SP) while maintaining strong dementia detection performance (AUPRC) on both tested datasets; ablation studies showing embedding layer criticality are well-supported

**Medium confidence:** Weight masking is model-agnostic and scalable requires further validation across different transformer architectures and tasks; comparison to adapter-based baselines shows promise but may not capture all competitive approaches

**Low confidence:** Confounding shifts degrade model performance even with balanced label distributions needs stronger causal evidence; mechanism by which specific weight masking achieves bias mitigation without task degradation requires more detailed analysis

## Next Checks

1. **Cross-architecture validation:** Apply the dual filter method to RoBERTa, DistilBERT, and GPT-2 on the same dementia detection task to verify architecture independence and scalability claims

2. **Confounder transfer experiment:** Test whether masks generated for gender confounding transfer to other confounders (age, dialect) on the same dataset, or whether task-specific mask generation is required

3. **Activation-level analysis:** Compare weight-masking results with activation regularization approaches (e.g., adversarial debiasing) on identical datasets to isolate whether confounding operates primarily through weights or activation patterns