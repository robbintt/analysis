---
ver: rpa2
title: 'Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision'
arxiv_id: '2505.12526'
source_url: https://arxiv.org/abs/2505.12526
tags:
- temporal
- training
- graph
- variance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of Temporal Graph Networks
  (TGNs) caused by sparse supervision signals in dynamic graphs. The core method idea
  is History-Averaged Labels (HAL), which dynamically enriches training batches with
  pseudo-targets derived from historical label distributions to enable continuous
  parameter updates.
---

# Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision
## Quick Facts
- arXiv ID: 2505.12526
- Source URL: https://arxiv.org/abs/2505.12526
- Authors: Alexander Panyshev; Dmitry Vinichenko; Oleg Travkin; Roman Alferov; Alexey Zaytsev
- Reference count: 40
- Primary result: HAL accelerates TGNv2 training by up to 15x while maintaining competitive performance

## Executive Summary
This paper addresses the inefficiency of Temporal Graph Networks (TGNs) caused by sparse supervision signals in dynamic graphs. The core method idea is History-Averaged Labels (HAL), which dynamically enriches training batches with pseudo-targets derived from historical label distributions to enable continuous parameter updates. Experiments on the Temporal Graph Benchmark validate that HAL accelerates TGNv2 training by up to 15x while maintaining competitive performance. Theoretically, HAL reduces gradient variance, accelerating convergence by a factor of min(h,k) under constant user preference assumptions.

## Method Summary
The paper introduces History-Averaged Labels (HAL), a pseudo-supervision strategy that dynamically enriches sparse temporal graph training batches with historical label distributions. HAL creates continuous training signals by maintaining a moving average of past labels within each batch, allowing TGNs to update parameters more frequently than traditional sparse supervision would permit. The method adaptively balances between current and historical label information, enabling parameter updates even when current batch supervision is minimal.

## Key Results
- HAL accelerates TGNv2 training by up to 15x on Temporal Graph Benchmark datasets
- Maintains competitive predictive performance compared to standard TGNv2 training
- Theoretically reduces gradient variance, accelerating convergence by min(h,k) factor under constant user preference assumptions

## Why This Works (Mechanism)
HAL works by addressing the fundamental challenge of sparse supervision in temporal graphs. In dynamic graphs, edges and labels arrive at irregular intervals, leaving many time steps without meaningful supervision. By maintaining a sliding window of historical labels and incorporating them as pseudo-targets during training, HAL ensures that every batch contains sufficient supervisory signal. This continuous supervision allows for more frequent parameter updates, which accelerates convergence. The adaptive nature of HAL means it can balance between exploiting historical patterns and adapting to current graph dynamics.

## Foundational Learning
1. **Temporal Graph Networks (TGNs)**: Dynamic graph neural networks that process time-evolving graph structures - needed to understand the baseline architecture being improved
   *Quick check*: TGNs maintain memory states for nodes that evolve over time based on edge interactions

2. **Gradient Variance Reduction**: Technique to improve optimization stability by reducing stochastic fluctuations in gradient estimates - needed to understand the theoretical convergence benefits
   *Quick check*: Lower gradient variance typically leads to smoother convergence trajectories in stochastic optimization

3. **Adaptive Pseudo-Supervision**: Strategy of generating synthetic training signals based on historical data patterns - needed to grasp HAL's core innovation
   *Quick check*: The method must balance between historical information relevance and current data distribution

## Architecture Onboarding
**Component Map**: Input Stream -> Temporal Encoder -> Memory Module -> HAL Enrichment -> Loss Function -> Parameter Update
**Critical Path**: Edge events → Memory update → Temporal embedding → HAL pseudo-label generation → Loss computation → Backpropagation
**Design Tradeoffs**: Historical window size (h) vs. adaptability to concept drift; pseudo-label confidence vs. training stability
**Failure Signatures**: Performance degradation when label distributions shift rapidly; over-reliance on stale historical patterns
**First Experiments**:
1. Verify HAL integration with TGNv2 on a small temporal graph dataset
2. Compare training dynamics with and without HAL supervision
3. Test sensitivity to historical window size parameter

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes historical label distributions remain relevant for current training, which may not hold in rapidly evolving graph domains
- Empirical validation focuses primarily on TGNv2 architecture and specific TGB datasets, limiting generalizability
- Theoretical analysis assumes constant user preferences across time, which may not reflect real-world dynamic graphs

## Confidence
- High confidence in technical validity of HAL mechanism and TGNv2 integration
- Medium confidence in generalizability across different temporal graph architectures
- Medium confidence in practical significance of claimed acceleration factors

## Next Checks
1. Test HAL integration with alternative temporal GNN architectures (TGAT, EvolveGCN) to assess architectural generalizability
2. Evaluate performance on datasets with more volatile label distributions to stress-test the historical averaging assumption
3. Conduct ablation studies varying the history window size (h) and batch size (k) to validate the theoretical min(h,k) convergence factor empirically