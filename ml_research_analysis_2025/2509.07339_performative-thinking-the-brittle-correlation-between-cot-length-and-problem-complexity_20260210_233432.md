---
ver: rpa2
title: Performative Thinking? The Brittle Correlation Between CoT Length and Problem
  Complexity
arxiv_id: '2509.07339'
source_url: https://arxiv.org/abs/2509.07339
tags:
- arxiv
- problem
- intermediate
- reasoning
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the length of intermediate reasoning
  traces (Chain-of-Thought) in transformer-based models reflects problem difficulty.
  The authors train transformer models from scratch on A search algorithm traces,
  where problem complexity is precisely measured by the number of operations required
  to solve maze problems.
---

# Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity

## Quick Facts
- arXiv ID: 2509.07339
- Source URL: https://arxiv.org/abs/2509.07339
- Reference count: 40
- One-line primary result: Models trained on A* traces generate trace lengths that reflect training distribution similarity rather than problem difficulty, challenging the assumption that longer reasoning traces indicate greater "thinking effort"

## Executive Summary
This paper investigates whether the length of Chain-of-Thought (CoT) traces in transformer models reflects problem difficulty. Through controlled experiments training models from scratch on A* search algorithm traces for maze problems, the authors find that trace length correlates with training distribution similarity rather than genuine problem complexity. The key finding is that models show trace-length correlation only on in-distribution problems, with this correlation vanishing entirely on out-of-distribution tasks. This challenges the anthropomorphic interpretation that longer reasoning traces indicate greater computational effort or "thinking."

## Method Summary
The authors train modified Qwen2.5-0.5B transformers from scratch on 500K Wilson-generated maze problems with A* traces. Models generate linearized A* traces (create/close operations with costs) followed by solution plans. The key evaluation metric is the correlation between generated trace length and ground-truth A* trace length across different maze distributions (Wilson, Kruskal, DFS, Searchformer-style, Drunkard-walk, and free-space). The 32k context window and 944-token vocabulary are specifically designed for this domain task.

## Key Results
- Free-space problems (simplest tasks) show almost no correlation between trace length and optimal path length, with models often failing to generate valid solutions
- On in-distribution Wilson mazes, trace lengths loosely correlate with ground-truth A* trace lengths
- This correlation disappears entirely on out-of-distribution mazes (Kruskal, DFS, Searchformer-style, Drunkard-walk)
- The few cases where correlation appears are those where problems are closer to the training distribution

## Why This Works (Mechanism)

### Mechanism 1: Distributional Approximate Recall
- Claim: Intermediate token length correlates with training distribution similarity, not problem difficulty
- Mechanism: The model learns statistical patterns from training data, producing trace lengths that reflect memorized distributional statistics rather than computing problem-adaptive solutions. When test instances resemble training data, the model approximates the distribution of trace lengths it observed during training.
- Core assumption: Models trained on fixed distributions cannot dynamically adapt computation based on problem complexity—they can only recall patterns from their training distribution.
- Evidence anchors:
  - [abstract] "We notice that the few cases where correlation appears are those where the problems are closer to the training distribution, suggesting that the effect arises from approximate recall rather than genuine problem-adaptive computation."
  - [Section 4.2] On Wilson mazes (in-distribution), "yielding a visible alignment between intermediate token lengths and ground-truth trace lengths." On Searchformer-style mazes (OOD), "this correlation disappears entirely."
  - [corpus] Neighbor paper "Stop Anthropomorphizing Intermediate Tokens as Reasoning/Thinking Traces!" supports the critique of anthropomorphic interpretations, though corpus evidence for the specific distributional mechanism is limited as papers are recent.

### Mechanism 2: Lack of Optimization Pressure on Trace Structure
- Claim: Intermediate tokens receive no direct training signal to reflect problem complexity
- Mechanism: Standard post-training methods (SFT on traces or RL with final-answer rewards) optimize only for solution correctness, not trace structure. The intermediate tokens are "samples from the model's base policy" without alignment to problem difficulty.
- Core assumption: Without explicit optimization pressure, generated tokens will not spontaneously develop problem-adaptive properties.
- Evidence anchors:
  - [Section 1] "In practice, post-training methods... are employed. In both cases, models are prompted to generate intermediate tokens before producing their answers, but optimization pressure applies only to the correctness of the final output."
  - [Section 1] "The intermediate tokens, being samples from the model's base policy, are not explicitly aligned with problem difficulty, correctness, or structured reasoning."
  - [corpus] Weak direct corpus evidence on this mechanism; related work on efficient reasoning addresses length optimization but not the foundational claim about missing optimization signals.

### Mechanism 3: Free-Space Failure Mode Detection
- Claim: Models cannot recognize trivially simple problems and appropriately shorten computation
- Mechanism: The model has no mechanism to detect problem simplicity. Even on free-space problems (no obstacles, optimal path is direct), the model generates long traces or fails entirely, indicating it cannot dynamically adjust computational effort based on problem structure.
- Core assumption: A genuinely problem-adaptive system would produce minimal traces on minimal-complexity problems.
- Evidence anchors:
  - [Section 4.1] "only 5 out of 100 problems yield a valid plan" on free-space problems. "On some of the easiest problems, the model continues producing tokens up to the maximum context limit of 32k without ever producing a valid solution."
  - [Figure 1a] Free-space scatter plot shows almost no responses near the y=x line; most cluster at maximum length (failures) or scatter randomly.
  - [corpus] "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens" suggests intermediate tokens may help performance through non-semantic mechanisms, supporting the disconnect between trace content and problem structure.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) as intermediate token generation**
  - Why needed here: The entire paper investigates what CoT traces represent mechanistically. Without understanding CoT as a technique (generating tokens before final answer), the research question is unclear.
  - Quick check question: Can you explain why CoT is hypothesized to improve reasoning, and what the alternative interpretation (performative vs. adaptive) would mean?

- Concept: **A* search algorithm and its trace formalization**
  - Why needed here: The paper uses A* as the ground truth for problem complexity (operations count) and trains models on linearized A* traces. Understanding how A* explores states is essential to interpret the results.
  - Quick check question: Given an A* trace with 50 "create" and "close" operations, would you expect a genuinely problem-adaptive model to produce roughly 50 intermediate tokens? Why or why not?

- Concept: **Distribution shift and out-of-distribution generalization**
  - Why needed here: The core finding is that trace-length correlation exists in-distribution but vanishes out-of-distribution. This distinction is the paper's central evidence against problem-adaptive computation.
  - Quick check question: If a model trained only on Wilson mazes shows correlation between trace length and complexity on held-out Wilson mazes but not on Kruskal mazes, what does this suggest about the mechanism?

## Architecture Onboarding

- Component map:
  - Qwen2.5-0.5B decoder-only transformer -> 944-token vocabulary -> 32,000 context window -> Linearized A* traces + plans

- Critical path:
  1. Tokenize maze instance → 2. Generate A* trace autoregressively → 3. Generate final plan → 4. Verify plan validity (all moves legal, reaches goal)
  5. Compare generated trace length vs. ground truth A* operations count

- Design tradeoffs:
  - **Training from scratch vs. fine-tuning**: From-scratch enables controlled experiments but may underestimate what larger pre-trained models achieve. The authors explicitly trade ecological validity for causal clarity.
  - **Single maze distribution training**: Wilson-only training creates clean in-distribution vs. OOD comparison but limits real-world applicability.
  - **Trace length as complexity proxy**: Using A* operations assumes algorithmic complexity maps to problem difficulty—valid for this domain but not universal.
  - **32k context limit**: Artificially caps trace length; some failures may be context-limited rather than reasoning-limited.

- Failure signatures:
  - **Free-space overgeneration**: Model hits 32k limit on trivial problems → indicates no simplicity detection
  - **Low OOD accuracy**: Correlation vanishes on Searchformer/Drunkard mazes → indicates distribution-bound behavior
  - **Random scatter plots**: Generated vs. ground-truth trace lengths show no linear relationship → indicates non-adaptive generation

- First 3 experiments:
  1. **Replicate the free-space test**: Train a fresh model on your own Wilson mazes, then evaluate on 100 free-space instances. Verify that trace lengths are not proportional to optimal path length.
  2. **Add explicit length supervision**: Train a variant with auxiliary loss on trace length prediction. Test whether this creates genuine problem-adaptive correlation or merely improves distribution matching.
  3. **Cross-distribution training**: Train on a mixture of Wilson + Kruskal + DFS mazes. Evaluate whether correlation emerges on all distributions or if the model simply learns to classify distributions and apply distribution-specific length heuristics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do these findings generalize to large pre-trained models versus models trained from scratch?
- **Basis in paper:** [Explicit] The authors utilize a "model organism" paradigm, training transformers from scratch to ensure control, but explicitly frame their conclusions as a caution against interpreting traces in systems like DeepSeek R1.
- **Why unresolved:** It remains unclear if the massive semantic priors in pre-trained LLMs mitigate the "approximate recall" behavior observed in these controlled, randomly initialized models.
- **What evidence would resolve it:** Replicating the experimental setup—evaluating trace length versus complexity on in-distribution vs. OOD mazes—using state-of-the-art pre-trained reasoning models.

### Open Question 2
- **Question:** Can explicit optimization of intermediate tokens (process supervision) induce genuine problem-adaptive computation?
- **Basis in paper:** [Inferred] The paper attributes the lack of correlation to training methods where "optimization pressure applies only to the correctness of the final output," leaving intermediate tokens unaligned with complexity.
- **Why unresolved:** The study isolates outcome-based supervision; it does not test if providing rewards for correct intermediate steps (process rewards) would force the model to link trace length to actual operations.
- **What evidence would resolve it:** Training models using Process Reward Models (PRMs) on the A* task and measuring if the correlation between trace length and operations improves on OOD tasks.

### Open Question 3
- **Question:** Does shortening reasoning traces necessarily compromise reasoning capability?
- **Basis in paper:** [Explicit] The discussion argues that "methods that claim to improve efficiency... by reducing reasoning-chain length may be based on a flawed premise" if length does not actually track difficulty.
- **Why unresolved:** If trace length is largely distributional noise rather than computation, shortening it might not hurt performance; conversely, it might remove essential "thinking" if the correlation is non-zero.
- **What evidence would resolve it:** Evaluating length-reduction techniques specifically on the free-space and OOD tasks where the paper shows trace length is most decoupled from complexity.

## Limitations
- Training from scratch may underestimate what larger pre-trained models can achieve
- Single-maze-distribution training limits ecological validity
- 32k context limit may artificially constrain trace generation

## Confidence
- **High confidence**: The distributional mechanism (Mechanism 1) showing that trace length correlates with training distribution similarity rather than problem difficulty is well-supported by the clear in-distribution vs. out-of-distribution contrast in Section 4.2.
- **Medium confidence**: The lack of optimization pressure claim (Mechanism 2) is logically sound but lacks direct experimental evidence—the paper doesn't test whether explicit length supervision would create problem-adaptive correlation.
- **Medium confidence**: The free-space failure detection (Mechanism 3) is compelling but based on a limited sample (5/100 success rate), suggesting the need for more extensive free-space evaluation.

## Next Checks
1. **Test explicit length supervision**: Train a variant with auxiliary loss on trace length prediction. Does this create genuine problem-adaptive correlation, or merely improve distribution matching? This would directly test whether missing optimization pressure is the limiting factor.

2. **Evaluate with larger context windows**: Train the same model architecture with 64k or 128k context limits and evaluate on free-space problems. If failures persist, it confirms the simplicity-detection problem; if success rates improve, context limits may have confounded the results.

3. **Cross-distribution mixture training**: Train on a mixture of Wilson + Kruskal + DFS mazes with balanced sampling. If correlation emerges across all distributions, it suggests the model learns distribution classification rather than problem-adaptive computation. If not, the distributional mechanism is robust.