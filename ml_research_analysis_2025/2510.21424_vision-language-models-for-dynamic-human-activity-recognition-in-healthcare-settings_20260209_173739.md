---
ver: rpa2
title: Vision Language Models for Dynamic Human Activity Recognition in Healthcare
  Settings
arxiv_id: '2510.21424'
source_url: https://arxiv.org/abs/2510.21424
tags:
- vlms
- activity
- evaluation
- caption
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a descriptive caption dataset and evaluation
  methods for applying Vision Language Models (VLMs) to human activity recognition
  in healthcare. The authors propose a caption generation framework using GPT-4o and
  ground truth labels to create textual descriptions for the Toyota Smarthome dataset.
---

# Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings

## Quick Facts
- arXiv ID: 2510.21424
- Source URL: https://arxiv.org/abs/2510.21424
- Authors: Abderrazek Abid; Thanh-Cong Ho; Fakhri Karray
- Reference count: 23
- Primary result: Vision Language Models achieve competitive performance on human activity recognition in healthcare, with InternVL2.5 reaching 83.8% MCA in cross-subject evaluation.

## Executive Summary
This study introduces a novel framework for applying Vision Language Models (VLMs) to human activity recognition in healthcare settings. The authors develop a caption generation pipeline using GPT-4o to create descriptive captions for the Toyota Smarthome dataset, then evaluate three open-source VLMs (DeepSeek-VL2, InternVL2.5, and Llama3.2-Vision) using both keyword matching and cosine similarity metrics. Results demonstrate that VLMs can achieve performance competitive with traditional deep learning models while offering reduced computational complexity, making them promising candidates for healthcare activity monitoring applications.

## Method Summary
The framework uses GPT-4o to generate descriptive captions from Toyota Smarthome dataset videos through an iterative keyword verification process. Keyframes are extracted using the Katna library, and GPT-4o generates keyword lists for each activity class. The system then prompts GPT-4o to create captions containing at least one relevant keyword, refining the prompt iteratively until compliance is achieved. Three open-source VLMs and GPT-4o are evaluated on the generated captions using both keyword matching and cosine similarity (threshold=0.5 with all-MiniLM-L6-v2 embeddings) to assess their ability to recognize activities from visual input alone.

## Key Results
- InternVL2.5 achieved 83.8% Mean Class Accuracy in cross-subject evaluation and 56.1% in cross-view evaluation
- VLMs demonstrated competitive performance compared to traditional deep learning models like π-ViT
- Cosine similarity evaluation provided more nuanced assessment than rigid keyword matching for open-ended VLM outputs
- The framework achieved 100% keyword compliance in generated captions through iterative refinement

## Why This Works (Mechanism)

### Mechanism 1: Grounded Caption Generation via Iterative Keyword Enforcement
- Claim: GPT-4o can generate semantically rich captions aligned with ground-truth activity labels when constrained by keyword verification.
- Mechanism: The framework extracts keyframes using Katna, generates keyword lists per activity class via GPT-4o, then iteratively prompts GPT-4o to produce captions containing at least one relevant keyword. If no keyword appears, the prompt is refined and GPT-4o is re-invoked until compliance is achieved.
- Core assumption: Keyword presence correlates with semantic correctness; GPT-4o's keyword generation adequately covers activity semantics.
- Evidence anchors:
  - [Page 4]: "If none are detected, the prompt is refined... This iterative process continues until a caption is generated that contains at least one relevant keyword."
  - [Page 4]: "An automated verification step is performed, confirming that 100% of the generated captions include one or more of the corresponding keywords."
  - [corpus]: RAG-HAR paper (arXiv:2512.08984) similarly leverages language model capabilities for HAR without task-specific training, supporting the generative approach.
- Break condition: If keyword lists fail to capture activity semantics (e.g., synonyms not included), captions may be semantically correct but flagged as non-compliant; or if GPT-4o generates plausible but hallucinated descriptions that happen to contain keywords.

### Mechanism 2: Semantic Similarity as VLM Evaluation Proxy
- Claim: Cosine similarity between caption embeddings (using all-MiniLM-L6-v2) provides a fairer evaluation metric than rigid keyword matching for open-ended VLM outputs.
- Mechanism: Ground-truth captions and VLM-generated outputs are encoded into dense vectors; cosine similarity measures semantic alignment. A threshold of 0.5 was empirically determined via manual evaluation of 20 examples.
- Core assumption: Embedding similarity captures semantic equivalence; the 0.5 threshold generalizes beyond the 20-sample validation set.
- Evidence anchors:
  - [Page 6]: "A threshold of 0.5 was empirically determined based on the manual evaluation of 20 examples."
  - [Page 8, Table 2]: InternVL2.5 achieves 83.8% MCA (CS) with similarity vs. 62.6% with keywords, suggesting semantic alignment exceeds lexical overlap.
  - [corpus]: Limited direct evidence in corpus for embedding-based HAR evaluation; related work focuses on classification accuracy rather than generative output assessment.
- Break condition: If VLM generates verbose descriptions with correct semantics but diluted embedding vectors (as observed with Llama3.2-Vision's length control issues), similarity scores may penalize accurate outputs.

### Mechanism 3: Zero-Shot Activity Recognition via Pretrained Vision-Language Representations
- Claim: Open-source VLMs can achieve competitive HAR performance without dataset-specific training by leveraging multimodal pretraining.
- Mechanism: VLMs (InternVL2.5, DeepSeek-VL2, Llama3.2-Vision) receive 2 keyframes per video plus a text prompt asking to describe the activity. Pretrained cross-modal attention enables activity inference from visual patterns associated with action semantics in the training corpus.
- Core assumption: Pretraining data contains sufficient visual-linguistic coverage of daily activities; 2 frames provide adequate temporal context.
- Evidence anchors:
  - [Page 6]: "Despite not being explicitly trained on the dataset and having access to only two keyframes per video for activity recognition."
  - [Page 8, Table 2]: InternVL2.5 achieves 83.8% MCA (CS, similarity), surpassing π-ViT (72.9%) and other deep learning baselines.
  - [corpus]: On-device Large Multi-modal Agent paper (arXiv:2512.19742) demonstrates LMM-based HAR with activity interpretation, supporting zero-shot capability claims.
- Break condition: Cross-view generalization is weaker (CV2: InternVL2.5 at 56.1% vs. π-ViT at 64.8%), suggesting viewpoint invariance from pretraining may be insufficient for novel camera angles without fine-tuning.

## Foundational Learning

- Concept: **Vision-Language Model Architecture (Encoder-Decoder or Decoder-Only with Vision Adapter)**
  - Why needed here: Understanding how InternVL2.5, DeepSeek-VL2, and Llama3.2-Vision process multimodal inputs explains performance differences (e.g., Llama3.2-Vision's single-frame constraint and output length issues).
  - Quick check question: Can you explain why a decoder-only VLM with a vision adapter might handle open-ended captioning differently than a dual-encoder model trained on image-text pairs?

- Concept: **Embedding Space Similarity Metrics**
  - Why needed here: Cosine similarity on sentence embeddings is central to the evaluation framework; misunderstanding here leads to incorrect threshold selection or interpretation.
  - Quick check question: Given two captions—"The person is cooking" and "The individual prepares a meal"—would you expect high or low cosine similarity using all-MiniLM-L6-v2? Why?

- Concept: **Cross-Subject vs. Cross-View Evaluation Protocols in HAR**
  - Why needed here: The paper evaluates both CS (unseen subjects) and CV (unseen camera viewpoints); performance gaps reveal generalization limitations.
  - Quick check question: If a model achieves 83.8% CS accuracy but 56.1% CV1 accuracy, what does this suggest about its learned representations?

## Architecture Onboarding

- Component map:
  1. **Data Preparation**: Toyota Smarthome videos → Katna keyframe extraction → GPT-4o keyword generation
  2. **Caption Generation Pipeline**: Keyframes + ground-truth labels + keywords → GPT-4o → iterative refinement → verified captions
  3. **VLM Inference**: Keyframes (2 per video) → VLM (InternVL2.5/DeepSeek-VL2/Llama3.2-Vision/GPT-4o) → generated caption
  4. **Evaluation Layer**: Generated caption vs. ground-truth caption → keyword matching / cosine similarity / BERTScore / VLM-as-Judge → accuracy metrics

- Critical path: Caption quality → evaluation metric selection → model comparison. If caption generation fails to capture activity semantics, all downstream evaluations are compromised.

- Design tradeoffs:
  - **Keyword matching vs. Cosine similarity**: Keyword matching is interpretable but penalizes valid paraphrases; cosine similarity is more flexible but threshold-dependent and can be gamed by verbose outputs.
  - **Frame count vs. Computational cost**: More frames improve temporal context but increase inference time; paper chose 2 frames as a balance (1 for Llama3.2-Vision due to architectural limits).
  - **Open-source vs. Proprietary VLMs**: Open-source ensures healthcare data privacy but may underperform GPT-4o; paper found InternVL2.5 competitive.

- Failure signatures:
  - **BERTScore inflation**: 100% accuracy across all models (Page 7, Table 1) due to structural similarity weighting all tokens equally, not activity-relevant semantics.
  - **VLM-as-Judge inconsistency**: GPT-4o as judge marked correct captions as incorrect (Page 7), suggesting evaluation bias or visual grounding limitations.
  - **Verbose output penalty**: Llama3.2-Vision's performance dropped under cosine similarity (63.5% CS) vs. keyword matching (67.4% CS) due to unfocused, lengthy descriptions.

- First 3 experiments:
  1. **Reproduce caption generation on 50 samples**: Run the iterative GPT-4o caption framework on a subset; manually verify that generated captions accurately describe activities and contain expected keywords.
  2. **Threshold calibration for cosine similarity**: Extend the 20-sample manual validation to 100+ samples across different activity classes to confirm the 0.5 threshold or identify class-specific adjustments.
  3. **Cross-view failure analysis**: For InternVL2.5, analyze CV2 misclassifications to determine if errors stem from viewpoint bias, occlusion, or insufficient temporal context from 2 frames.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does fine-tuning VLMs on the generated descriptive caption dataset allow them to surpass the state-of-the-art performance of specialized deep learning models (e.g., $\pi$-ViT) in data-rich evaluations like Cross-View 2 (CV2)?
- **Basis in paper:** [explicit] The conclusion states the dataset "can serve as a valuable resource for further fine-tuning VLMs" immediately after noting that VLMs were outperformed by $\pi$-ViT in the CV2 setting.
- **Why unresolved:** The current study evaluates VLMs in a zero-shot or general-purpose setting, whereas the deep learning comparison models were fully supervised on the training split.
- **What evidence would resolve it:** Experimental results comparing the Mean Class Accuracy of fine-tuned VLMs against the baseline CV2 scores reported in Table 2.

### Open Question 2
- **Question:** How does increasing the number of input keyframes beyond two affect the trade-off between recognition accuracy and computational latency for VLMs?
- **Basis in paper:** [inferred] The authors explicitly limited the input to two frames to balance computational costs (Section 4.2), acknowledging the "importance of the number of analyzed frames" without testing the upper bounds of this relationship.
- **Why unresolved:** The study only provides a snapshot of performance at a fixed, low frame count due to resource constraints.
- **What evidence would resolve it:** A sensitivity analysis plotting Mean Class Accuracy and inference time (FLOPS or latency) as the number of input frames increases (e.g., 2, 4, 8, 16 frames).

### Open Question 3
- **Question:** Can a composite evaluation metric be developed that combines keyword precision and semantic similarity to mitigate the respective failure modes of hallucination and verbose irrelevance?
- **Basis in paper:** [inferred] The discussion in Section 4.3 highlights that keyword matching can fail on synonyms while cosine similarity can be skewed by verbose outputs (specifically noted for Llama3.2-Vision).
- **Why unresolved:** The paper validates existing methods but does not propose a novel metric that addresses the identified specific weaknesses of individual metrics.
- **What evidence would resolve it:** A new metric formulation that correlates more strongly with human expert judgment than the current standalone cosine similarity or keyword matching scores.

## Limitations
- The caption generation pipeline relies heavily on GPT-4o's keyword extraction and iterative refinement, which may not generalize to datasets with different activity semantics.
- The cosine similarity threshold (0.5) was validated on only 20 examples, raising concerns about its robustness across diverse activities.
- Cross-view performance degradation (56.1% vs 83.8% cross-subject) indicates potential limitations in viewpoint generalization without fine-tuning.

## Confidence
- High: VLM performance on cross-subject evaluation, basic caption generation framework, and keyword matching methodology
- Medium: Cosine similarity threshold selection, cross-view generalization claims, and comparative performance against traditional models
- Low: The robustness of iterative keyword refinement across different activity types, VLM-as-judge evaluation methodology, and scalability to larger, more diverse healthcare datasets

## Next Checks
1. Expand the cosine similarity threshold validation from 20 to 100+ samples across all activity classes to ensure threshold robustness and identify class-specific variations.

2. Conduct ablation studies varying the number of keyframes (1, 2, 4, 8) to quantify the impact of temporal context on cross-view performance and identify optimal frame sampling strategies.

3. Implement a human evaluation study where multiple annotators rate VLM-generated captions for semantic accuracy and completeness, comparing results against automated metrics to validate the evaluation framework's reliability.