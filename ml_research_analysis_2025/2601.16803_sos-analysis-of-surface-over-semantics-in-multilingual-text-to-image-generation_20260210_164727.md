---
ver: rpa2
title: 'SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation'
arxiv_id: '2601.16803'
source_url: https://arxiv.org/abs/2601.16803
tags:
- images
- languages
- figure
- language
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic analysis of Surface-over-Semantics
  (SoS) tendencies in multilingual text-to-image (T2I) models, where outputs are influenced
  more by input language than prompt semantics. To quantify SoS, the authors introduce
  the SoS score, an embedding-based metric measuring similarity between generated
  images and surface versus semantic reference vectors.
---

# SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation

## Quick Facts
- arXiv ID: 2601.16803
- Source URL: https://arxiv.org/abs/2601.16803
- Authors: Carolin Holtermann; Florian Schneider; Anne Lauscher
- Reference count: 40
- All but one model exhibit strong surface-level tendencies in at least two languages, with biases intensifying in later text encoder layers.

## Executive Summary
This paper introduces SoS (Surface-over-Semantics), the first systematic analysis of multilingual T2I models' tendency to generate outputs influenced more by input language than prompt semantics. The authors develop the SoS score, an embedding-based metric measuring similarity between generated images and surface versus semantic reference vectors. Using 171 cultural identities across 14 languages and 7 T2I models, they demonstrate that surface tendencies manifest as culturally stereotypical depictions and can be effectively quantified and mitigated through architectural analysis.

## Method Summary
The study builds a prompt dataset of 171 cultures across 14 languages with 3 templates and 3 person terms (total 1,539 English prompts × 14 languages). Images are generated using 7 T2I models with seed=42. The SoS score measures the difference in cosine similarity between each image and two reference vectors: the semantic center (average embedding of all images for a specific culture across languages) and the surface center (average embedding of all images in a specific language across cultures). Layer-wise analysis uses DiffusionLens to extract intermediate text encoder representations, while VQA with Qwen2-VL and Fighting Words identifies distinctive cultural terms in image descriptions.

## Key Results
- All models except FLUX.1-dev exhibit negative SoS scores in at least two languages, indicating surface-level tendencies
- Surface tendencies intensify in later text encoder layers, with Layer 20 showing significantly stronger biases than Layer 4
- VQA analysis reveals strong correlation between negative SoS scores and culturally stereotypical visual manifestations
- SoS score outperforms CLIPScore in capturing surface-level tendencies, especially for non-European languages

## Why This Works (Mechanism)

### Mechanism 1: Reference Vector Geometry
The SoS score isolates language-induced bias by positioning generated images between semantic and surface gravitational poles in the embedding space. The metric calculates two reference vectors: $\vec{\text{sem}}_c$ (average embedding of all images for culture $c$ across languages) and $\vec{\text{sur}}_l$ (average embedding of all images in language $l$ across cultures). The SoS score is the difference in cosine similarity between the image and these poles. A negative score indicates the image is visually closer to the "average" look of that language than the "average" look of that culture.

### Mechanism 2: Layer-wise Semantic Encodings
Surface-level biases intensify as the text encoder processes prompts into deeper layers. Early layers produce generic or neutral outputs, while later layers inject strong cultural associations linked to the input language when semantic grounding is weak. This mechanism suggests that later layers are responsible for anchoring prompts to specific visual concepts, defaulting to surface-form associations when multilingual grounding is missing.

### Mechanism 3: Visual Stereotype Manifestation
A low SoS score manifests visually as higher frequency of specific cultural stereotypes associated with the prompt language. Using VQA to describe images and Fighting Words to identify distinctive terms, the analysis shows that surface-dominated outputs contain terms (e.g., "blood," "slanted eyes" for Chinese prompts) that appear in stereotype databases, even when prompts asked for different identities.

## Foundational Learning

- **CLIP Embedding Space & Cosine Similarity:** The entire SoS metric relies on measuring angles (cosine similarity) between image vectors and reference vectors. Understanding that similar visuals have smaller angles in this space is essential for interpreting the metric.
  - Quick check: If an image is perfectly equidistant from the "German" semantic vector and the "German language" surface vector, what is the SoS score? (Answer: 0)

- **Text Encoder Layer Function:** The paper utilizes DiffusionLens to analyze which layers introduce bias. Understanding that early layers capture syntax/structure while later layers capture semantics/grounding is essential to interpret layer-wise results.
  - Quick check: Why would injecting the prompt representation from Layer 5 result in different images than Layer 20 in a Stable Diffusion model? (Answer: Later layers carry more semantic grounding and cultural associations)

- **Fighting Words (Log-Odds Ratio):** To validate the "Visual Manifestation" hypothesis, the paper needs to distinguish generic terms from culturally distinctive ones. Fighting Words allows statistical comparison of word usage across two corpora (languages).
  - Quick check: If the term "person" appears in descriptions for both Language A and Language B, will Fighting Words rank it high? (Answer: No, because it appears in both corpora and has no distinguishing power)

## Architecture Onboarding

- **Component map:** Prompt Dataset → T2I Pipeline (Text Encoder → Diffusion U-Net) → Evaluation Stack (CLIP ViT → SoS Calculator → VQA Model)
- **Critical path:** 1) Reference Generation: Generate thousands of images across all language/culture pairs to establish surface and semantic vectors. 2) SoS Calculation: For each new image, compute cosine similarity difference to reference vectors. 3) Layer Extraction: Hook into specific text encoder layers to feed intermediate representations into diffusion model.
- **Design tradeoffs:** CLIP vs. SoS tradeoff—SoS requires large pre-computed reference image dataset while CLIPScore needs only the prompt. Generalization tradeoff—study focuses on "person" prompts where SoS works well, but objects without strong cultural markers might yield noisier scores.
- **Failure signatures:** English Instability—SoS less reliable for English prompts due to high image diversity in reference set. Early Layer Noise—early text encoder layers yield abstract/noisy outputs, breaking text-based metrics.
- **First 3 experiments:**
  1. Layer-wise probe: Hook text encoder of SD2.1, generate images using embeddings from Layer 4 vs Layer 20 for Amharic prompt, plot SoS score shift to verify intensification hypothesis.
  2. Metric Robustness Check: Generate 10 images for "A photo of a Finnish person," calculate SoS using DINO vs LAION CLIP embeddings to test if visual bias depends on encoder architecture.
  3. Stereotype Correlation: Run VQA analysis on Hindi prompt images, check if distinctive terms include "saree" or "bindi," calculate Pearson correlation between frequency of these terms and model's SoS score.

## Open Questions the Paper Calls Out

1. Can the SoS score be effectively utilized as a loss function or guidance metric during model training to reduce surface-level biases without degrading image quality? The paper suggests this possibility but only uses SoS for post-hoc evaluation, not optimization.

2. How can T2I systems dynamically determine the appropriate balance between surface and semantic alignment based on the intent of the prompt? The paper identifies this trade-off as context-dependent but leaves dynamic adaptation as an unsolved engineering challenge.

3. Does increasing the volume and diversity of multilingual pre-training data specifically mitigate the amplification of surface-level biases in the later layers of text encoders? The paper observes correlation between model type and layer-wise bias but doesn't perform controlled ablation study isolating multilingual training data volume.

## Limitations
- Language and culture conflation—The framework treats language and cultural identity as separable dimensions, but these are deeply intertwined in practice
- Embedding space bias—LAION CLIP embeddings may contain language biases, potentially distorting "surface" reference vectors for non-English languages
- VQA reliability—Fighting Words analysis depends on Qwen2-VL accurately describing images without introducing its own biases

## Confidence
- High Confidence: Mathematical framework of SoS score is sound, core finding that surface tendencies intensify in later text encoder layers is well-supported
- Medium Confidence: Correlation between negative SoS scores and stereotypical visual manifestations is convincing, but VQA-based validation introduces uncertainty
- Low Confidence: Claim that SoS outperforms CLIPScore specifically for non-European languages needs more direct validation

## Next Checks
1. Cross-Embedding Validation: Generate SoS scores for same image set using DINO embeddings instead of LAION CLIP. If surface tendencies persist despite different embedding architectures, this strengthens confidence that bias exists independently of evaluation metric.

2. Early Layer Baseline: Generate images from earliest text encoder layers (Layer 0-4) for multiple languages and calculate SoS scores. This would establish whether intensification of surface tendencies is specific to later layers or present throughout encoding process.

3. Non-Cultural Objects: Test SoS on prompts for culturally neutral objects (e.g., "A photo of a tree" or "A house") across multiple languages. If SoS scores remain stable across languages for these objects, it validates that metric specifically captures cultural rather than generic linguistic effects.