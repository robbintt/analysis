---
ver: rpa2
title: 'ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming
  Contests'
arxiv_id: '2506.04894'
source_url: https://arxiv.org/abs/2506.04894
tags:
- code
- reasoning
- test
- problems
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICPC-Eval, a benchmark for evaluating large
  language models' reasoning abilities using competitive programming problems. The
  benchmark consists of 118 challenging problems from 11 recent ICPC contests.
---

# ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests

## Quick Facts
- **arXiv ID:** 2506.04894
- **Source URL:** https://arxiv.org/abs/2506.04894
- **Reference count:** 28
- **Primary result:** ICPC-Eval benchmark shows current LLMs perform significantly worse than human ICPC medalists on real competition problems

## Executive Summary
This paper introduces ICPC-Eval, a comprehensive benchmark for evaluating large language models' reasoning abilities using real competitive programming problems from the International Collegiate Programming Contest. The benchmark consists of 118 challenging problems from 11 recent ICPC contests, designed to test models' ability to solve complex algorithmic problems under realistic competition constraints. To address the difficulty of evaluating code correctness, the authors develop a robust test case generation method using LLMs to create comprehensive local test suites and propose Refine@K, a new metric that measures a model's ability to iteratively improve solutions based on execution feedback.

Experiments with 15 state-of-the-art LLMs reveal that even the most advanced models like o3-mini High and Gemini 2.5 Pro Exp perform significantly worse than human ICPC medalists, with accuracy rates typically below 30%. The results demonstrate that reasoning models benefit substantially from execution feedback, while non-reasoning models show minimal improvement, suggesting that the ability to learn from execution errors is a key differentiator in competitive programming performance. The benchmark provides a more rigorous and realistic evaluation framework for assessing LLMs' reasoning capabilities compared to existing synthetic benchmarks.

## Method Summary
The authors develop ICPC-Eval by collecting 118 challenging problems from 11 recent ICPC contests, ensuring a diverse and realistic representation of competitive programming tasks. To overcome the evaluation challenges inherent in testing code correctness, they implement a multi-stage test case generation pipeline using LLMs to synthesize comprehensive local test suites that capture the complexity and edge cases of competition problems. The key innovation is the Refine@K metric, which evaluates models not just on their initial solutions but on their ability to iteratively improve code based on execution feedback, mimicking the debugging process human programmers use during competitions. This approach provides a more nuanced assessment of reasoning capabilities than traditional pass/fail evaluation metrics.

## Key Results
- Even top models (o3-mini High, Gemini 2.5 Pro Exp) achieve accuracy rates below 30% on ICPC-Eval problems
- Reasoning models show substantial improvement with execution feedback, while non-reasoning models show minimal gains
- ICPC-Eval problems are significantly more difficult than existing benchmarks, with models performing orders of magnitude worse than human ICPC medalists
- The benchmark reveals a clear performance gap between model capabilities and human expertise in competitive programming

## Why This Works (Mechanism)
The benchmark works by creating a realistic evaluation environment that mirrors actual competitive programming contests. By using real ICPC problems rather than synthetic benchmarks, it tests models on genuinely challenging algorithmic problems that require sophisticated reasoning and problem-solving skills. The LLM-generated test cases ensure comprehensive coverage of edge cases and corner conditions that human programmers must handle, while the Refine@K metric captures the iterative nature of programming problem-solving, where developers learn from execution errors to improve their solutions.

## Foundational Learning
- **Competitive programming problem structures** - Understanding how ICPC problems are designed and evaluated is essential for creating realistic benchmarks
- **Test case generation and coverage** - Critical for ensuring comprehensive evaluation of code correctness across edge cases
- **Iterative refinement processes** - Key mechanism for improving solutions based on execution feedback and debugging
- **Execution feedback analysis** - Understanding how models can learn from runtime errors to improve subsequent attempts
- **Algorithmic reasoning assessment** - Framework for evaluating complex problem-solving capabilities beyond simple pattern matching
- **Benchmark difficulty calibration** - Methods for ensuring problems are challenging enough to differentiate between model capabilities

## Architecture Onboarding

**Component map:** Problem Collection -> Test Case Generation -> Model Evaluation -> Refine@K Scoring

**Critical path:** The pipeline begins with collecting authentic ICPC problems, followed by LLM-based test case synthesis to create comprehensive evaluation suites, then proceeds to model inference and execution, and finally applies the Refine@K metric to assess iterative improvement capabilities.

**Design tradeoffs:** The authors chose LLM-generated test cases over human-curated ones to achieve scalability and comprehensive coverage, accepting the risk of potential blind spots. They prioritized realistic competition conditions over synthetic benchmark simplicity, resulting in a more challenging but also more meaningful evaluation framework.

**Failure signatures:** Models that perform well on synthetic benchmarks but poorly on ICPC-Eval likely lack genuine reasoning capabilities and instead rely on pattern matching. Models that show minimal improvement with execution feedback demonstrate limited ability to learn from errors, while those that improve significantly exhibit stronger debugging and reasoning skills.

**First experiments:**
1. Run baseline models on a subset of problems using both pass/fail and Refine@K metrics to establish performance baselines
2. Compare LLM-generated test cases against human-crafted ones on identical problems to validate coverage completeness
3. Test models with and without execution feedback on identical problems to quantify the benefit of iterative refinement

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The benchmark's relatively small size (118 problems) may limit generalizability across the broader space of competitive programming problems
- Reliance on LLM-generated test cases may introduce blind spots that human-crafted test suites would catch
- The Refine@K metric may not fully capture nuanced problem-solving strategies employed by human competitors
- Results may be sensitive to the specific problem set and evaluation methodology used

## Confidence
- **High** confidence in benchmark difficulty relative to existing evaluations and performance gap between LLMs and human medalists
- **Medium** confidence in specific performance rankings of individual models
- **Medium** to **Low** confidence in broader implications for LLM reasoning capabilities beyond competitive programming

## Next Checks
1. **Cross-validation with alternative test generation methods**: Re-evaluate model performance using a hybrid approach combining LLM-generated test cases with human-crafted edge cases to assess test suite completeness.

2. **Temporal generalization study**: Test whether models that perform well on ICPC-Eval also demonstrate strong performance on problems from earlier ICPC contests (pre-2020) to evaluate generalizability versus memorization.

3. **Human-LLM collaboration experiments**: Conduct controlled studies where human experts work alongside LLMs on ICPC-Eval problems to determine whether the performance gap stems from fundamental reasoning limitations or lack of domain-specific knowledge.