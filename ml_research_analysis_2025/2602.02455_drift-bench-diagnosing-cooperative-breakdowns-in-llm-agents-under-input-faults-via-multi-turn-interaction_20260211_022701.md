---
ver: rpa2
title: 'Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults
  via Multi-Turn Interaction'
arxiv_id: '2602.02455'
source_url: https://arxiv.org/abs/2602.02455
tags:
- agent
- user
- clarification
- interaction
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRIFT-BENCH introduces a diagnostic framework for evaluating LLM
  agents under cooperative breakdowns, where user inputs contain implicit intent,
  missing parameters, false presuppositions, or ambiguous expressions. Grounded in
  classical communication theories, it defines a unified taxonomy of four breakdown
  types and introduces persona-driven user simulation with controlled input faults.
---

# Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction

## Quick Facts
- **arXiv ID**: 2602.02455
- **Source URL**: https://arxiv.org/abs/2602.02455
- **Reference count**: 40
- **Primary result**: Introduces DRIFT-BENCH to evaluate LLM agents under input faults, revealing catastrophic performance drops and a clarification paradox where multi-turn interaction helps in white-box but harms in black-box environments.

## Executive Summary
DRIFT-BENCH introduces a diagnostic framework for evaluating LLM agents under cooperative breakdowns, where user inputs contain implicit intent, missing parameters, false presuppositions, or ambiguous expressions. Grounded in classical communication theories, it defines a unified taxonomy of four breakdown types and introduces persona-driven user simulation with controlled input faults. The RISE protocol evaluates multi-turn clarification across state-oriented and service-oriented environments, measuring robustness, clarification gain, safety, and efficiency. Experiments show agents experience catastrophic performance drops under faults, with clarification paradoxically harming service-oriented tasks due to context overload while benefiting state-oriented tasks. Safety remains low, with agents executing high-risk actions over 70% of the time. Clarification effectiveness varies by persona, highlighting the need for adaptive, risk-aware interaction policies.

## Method Summary
DRIFT-BENCH evaluates LLM agents under input faults through a three-phase perturbation pipeline: semantic frame extraction from verified tasks, perturbation strategy generation across four fault categories, and perturbation injection. Agents are tested with/without clarification capabilities in both state-oriented (OS/DB) and service-oriented (API) environments. The RISE protocol measures robustness (Performance Degradation), intelligence (Clarification Gain), safety (Safe Action Rate), and efficiency (Average Interaction Rounds). Persona-driven user simulation (Rational, Intuitive, Dependent, Avoidant, Spontaneous) provides diverse user behaviors based on the GDMS framework. Agents use five clarification strategies to navigate ambiguous inputs through multi-turn dialogue.

## Key Results
- Agents experience catastrophic performance drops under input faults, with average degradation exceeding 40% across all fault types
- Clarification paradoxically harms service-oriented tasks (-25.12% for Expression faults) while benefiting state-oriented tasks due to context overload in black-box environments
- Safe Action Rate remains below 30% for Premise and Parameter faults, with agents executing high-risk actions over 70% of the time
- Avoidant persona users prove most challenging (56.64% success), while Rational and Spontaneous personas yield >67% success rates

## Why This Works (Mechanism)

### Mechanism 1: The Clarification Paradox—Environment-Contingent Repair
- **Claim:** Multi-turn clarification produces opposite effects depending on system transparency: it rehabilitates agents in white-box (state-oriented) environments but degrades performance in black-box (service-oriented) environments.
- **Mechanism:** In state-oriented environments (OS/DB), agents can verify preconditions through exploratory commands (e.g., `ls`, `DESCRIBE TABLE`), mapping clarified information to grounded states. In service-oriented API environments, added dialogue context introduces "Clarification-Induced Syntactic Collapse" (disrupts JSON schema adherence) and "Abandonment Catalyst" behavior (treating recoverable API noise as terminal ambiguity).
- **Core assumption:** Context window and reasoning precision are finite resources; multi-turn interaction trades execution bandwidth for information gain.
- **Evidence anchors:**
  - [abstract]: "Clarification paradoxically harming service-oriented tasks due to context overload while benefiting state-oriented tasks"
  - [Section 5.2]: "Clarification Paradox emerges in service-oriented tasks, where the same strategies trigger universal performance drops (e.g., -25.12% for Expression faults)"
  - [Appendix F.2]: Group A/B case analysis documenting syntactic collapse and premature abandonment
  - [corpus]: ClarifyMT-Bench (arXiv:2512.21120) confirms multi-turn clarification effectiveness varies with user cooperativity but does not address the environment-transparency dimension DRIFT-BENCH introduces
- **Break condition:** If agents are given explicit schema enforcement or context compression, the paradox may attenuate; if task complexity increases such that even white-box exploration is overwhelmed, clarification benefits may also diminish in state-oriented tasks.

### Mechanism 2: Fault Injection via Semantic Frame Perturbation
- **Claim:** Controlled input faults can be systematically generated by extracting structured semantic frames from verified tasks and applying principled perturbation strategies mapped to cooperative breakdown taxonomy.
- **Mechanism:** (1) LLM extracts action type, prerequisites, required/optional parameters, and expected output into a semantic frame; (2) perturbation strategies (intention/premise/parameter/expression) are applied with specific sub-strategies (e.g., False Presupposition, Insufficient Information); (3) perturbed variants preserve solvability if clarification is used.
- **Core assumption:** The taxonomy (grounded in Grice, Austin, Watzlawick) provides mutually exclusive and collectively exhaustive categories for cooperative breakdowns.
- **Evidence anchors:**
  - [Section 2]: "Grounded in classical theories of communication, DRIFT-BENCH provides a unified taxonomy of cooperative breakdowns"
  - [Section 3.1]: "perturbation pipeline consists of three phases: Semantic Frame Extraction, Perturbation Strategy Generation, Perturbation Injection"
  - [Figure 1]: Complete taxonomy with 12 sub-categories mapped to examples
  - [corpus]: No direct corpus match for this specific perturbation mechanism; existing works (AmbigQA, ConvAI3) use ad-hoc or surface-level noise injection
- **Break condition:** If semantic frames fail to capture implicit task constraints, perturbations may create unsolvable tasks; if LLM-based frame extraction has systematic blind spots, fault coverage will be incomplete.

### Mechanism 3: Persona-Grounded User Simulation for Stress Testing
- **Claim:** User simulation grounded in decision-making psychology (GDMS framework) exposes agent robustness gaps that cooperative-only simulators miss.
- **Mechanism:** Five personas (Rational, Intuitive, Dependent, Avoidant, Spontaneous) are implemented with anthropomorphic profiles including communication habits, cognitive biases, and emotional responses. Each simulated user maintains conversation memory and receives both original intent and perturbed description—guiding agents toward true goals while staying in character.
- **Core assumption:** Persona labels capture stable, behaviorally relevant user characteristics that generalize beyond the specific LLM used for simulation.
- **Evidence anchors:**
  - [Section 3.3]: "Persona-driven User Simulator based on General Decision-Making Style (GDMS) framework"
  - [Table 5]: Avoidant persona universally most challenging (56.64% success); Spontaneous/Rational yield >67% success
  - [Appendix D]: Human evaluation validates persona labeling (Cohen's κ=0.7649)
  - [corpus]: Non-Collaborative User Simulators (arXiv:2509.23124) and ChatChecker (arXiv:2507.16792) similarly stress-test with non-cooperative users, but DRIFT-BENCH's persona diversity is broader
- **Break condition:** If persona responses become inconsistent across sessions, reproducibility degrades; if personas converge toward cooperative behavior over long dialogues, stress-testing signal weakens.

## Foundational Learning

- **Concept: Grice's Cooperative Principle (Maxims of Relation, Quantity, Quality, Manner)**
  - **Why needed here:** The entire fault taxonomy derives from violations of conversational maxims—Flaw of Intention (Relation), Flaw of Parameter (Quantity), Flaw of Premise (Quality), Flaw of Expression (Manner).
  - **Quick check question:** Given a user query "Book me a reasonable Italian restaurant," which maxim is violated and which fault category does this map to?

- **Concept: White-box vs. Black-box Execution Environments**
  - **Why needed here:** The Clarification Paradox hinges on this distinction—agents can inspect state in white-box (OS/DB) but must rely on discrete request-response pairs in black-box (APIs).
  - **Quick check question:** In which environment type would an agent's `ls` command provide actionable grounding information for clarification?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - **Why needed here:** The paper positions input faults as a form of uncertainty that existing benchmarks treat as "irreducible or out-of-scope," whereas clarification can convert aleatoric uncertainty into resolvable ambiguity.
  - **Quick check question:** If a user provides an ambiguous pronoun reference ("Book that one"), is this aleatoric or epistemic uncertainty from the agent's perspective before clarification?

## Architecture Onboarding

- **Component map:** AgentBench (DB/OS subsets) → state-oriented; StableToolBench (G1-level) → service-oriented → Semantic Frame Extractor → Strategy Selector → Perturbation Injector → Base agent + Communication Tools (Ask_Parameter, Disambiguate, Propose_Solution, Confirm_Risk, Report_Blocker) → Persona-driven User Simulator (Rational, Intuitive, Dependent, Avoidant, Spontaneous) → RISE metrics (PD, G, SAR, AIR)

- **Critical path:**
  1. Oracle verification (≥2/3 models succeed on clean task)
  2. Semantic frame extraction from verified task
  3. Perturbation injection per fault type
  4. Agent execution with/without clarification enabled
  5. Persona-grounded multi-turn interaction if clarification triggered
  6. RISE metric computation

- **Design tradeoffs:**
  - **Solvability vs. Difficulty:** "At least two correct" filter ensures failures are attributable to faults, not intrinsic unsolvability, but may exclude edge-case tasks
  - **Persona Coverage vs. Reproducibility:** Rich anthropomorphic profiles increase realism but introduce LLM-simulator variance
  - **Clarification Strategies:** Five strategies cover common cases but may not generalize to domain-specific disambiguation needs

- **Failure signatures:**
  - **Syntactic Collapse:** Repeated "Tool input parse error" when clarification mode is active (see Appendix F.2, Case 55223)
  - **Abandonment Catalyst:** Agent invokes `give_up_and_restart` on recoverable API errors when clarification is enabled (Case 1572)
  - **Execution-Bias:** Agent proceeds with high-risk action despite ambiguous input (SAR < 30% for Premise/Parameter faults)

- **First 3 experiments:**
  1. **Baseline degradation measurement:** Run Oracle vs. NoClarify vs. Clarify conditions on state-oriented tasks across all four fault types; compute PD for each model to establish fragility mirroring effect
  2. **Clarification Paradox replication:** Compare Clarification Gain (G) between state- and service-oriented environments for Expression faults; expect positive G in state, negative in service
  3. **Persona stress test:** Evaluate a single model against all five personas on Parameter-fault tasks; expect highest failure rate with Avoidant persona, correlate with human evaluation labels

## Open Questions the Paper Calls Out

- **Open Question 1:** Can environment-aware clarification policies be developed that selectively apply disambiguation strategies in service-oriented (black-box) environments while avoiding the syntactic collapse and abandonment bias observed in DRIFT-BENCH?
  - **Basis in paper:** [explicit] The paper identifies a "Clarification Paradox" where multi-turn interaction rehabilitates agents in transparent white-box systems but impairs them in opaque black-box settings due to "Clarification-Induced Syntactic Collapse" and "Abandonment Catalyst" effects (Section 5.2, Section F.2).
  - **Why unresolved:** The paper diagnoses the problem but does not propose or test mechanisms to detect environment type and modulate clarification behavior accordingly.
  - **What evidence would resolve it:** Experiments with agents that condition clarification strategy on environment transparency metrics, measuring whether selective deferral improves service-oriented task performance.

- **Open Question 2:** How can agents learn a reliable "Defer-to-Clarify" policy to reduce the observed 70% unsafe execution rate when encountering premise and parameter faults?
  - **Basis in paper:** [explicit] The paper reports that Safe Action Rate drops to ~29% for Premise and Parameter faults, indicating agents execute high-risk actions over 70% of the time instead of pausing for disambiguation (Section 5.2, Figure 4).
  - **Why unresolved:** The paper calls for "Environment-Aware Safety Guardrails" but provides no implementation or training methodology for such risk-aware deferral behavior.
  - **What evidence would resolve it:** Demonstration of agents trained with explicit safety penalties or uncertainty-aware rewards that achieve significantly higher SAR without sacrificing task success.

- **Open Question 3:** What mechanisms can enable adaptive clarification strategies that robustly handle diverse user personas, particularly the challenging Avoidant profile that showed uniformly lowest success rates?
  - **Basis in paper:** [explicit] The paper finds success rates highly persona-dependent, with Avoidant users yielding the worst performance (56.64% average) while Spontaneous and Rational users yield >67% (Section 5.3, Table 5). The authors explicitly call for "adaptive clarification that can navigate diverse human sharing behaviors."
  - **Why unresolved:** Current clarification policies treat all users uniformly; the paper identifies the gap but does not propose persona-adaptive mechanisms.
  - **What evidence would resolve it:** Development of user modeling components that detect persona traits from interaction patterns and modulate clarification verbosity, timing, and strategy accordingly, with empirical improvement on Avoidant persona tasks.

- **Open Question 4:** What are the optimal trade-offs between clarification gain (G) and interaction efficiency (AIR) for minimal-turn clarification policies in state-oriented environments?
  - **Basis in paper:** [inferred] The paper observes successful interaction rounds increase from 4.84 to 5.78 on average (Table 4) and notes the need for "minimal-turn clarification policies that prioritize high-impact disambiguation over verbose but futile dialogue" (Section 5.2), but does not systematically explore this trade-off space.
  - **Why unresolved:** The RISE framework introduces competing metrics (Clarification Gain vs. Efficiency) but does not characterize the Pareto frontier or propose optimization objectives.
  - **What evidence would resolve it:** Controlled experiments varying clarification verbosity constraints, mapping the resulting G vs. AIR trade-off curve across fault types and environments.

## Limitations

- **Limited generalizability:** Results are based on a fixed set of 350 technical tasks, raising questions about statistical power and domain coverage beyond OS/DB and API environments.
- **Simulator uncertainty:** Cross-persona consistency and stability of the persona-driven user simulator are not quantified, introducing potential variance in stress-testing results.
- **Risk calibration gaps:** The safety evaluation relies on a predefined "high-risk" tool list whose completeness and risk calibration are not disclosed.

## Confidence

- **High Confidence:** The existence of performance degradation under input faults and the basic clarification effectiveness in state-oriented environments.
- **Medium Confidence:** The Clarification Paradox in service-oriented environments and the persona-driven simulator's ability to stress-test agents.
- **Low Confidence:** The exact mechanism of Clarification-Induced Syntactic Collapse and the generalizability of fault types beyond the evaluated task set.

## Next Checks

1. **Cross-Domain Robustness:** Test the same agent models and fault injection pipeline on a non-technical domain (e.g., creative writing or travel planning) to assess whether the Clarification Paradox and persona effects persist.
2. **Simulator Consistency:** Run repeated simulation sessions with the same persona and fault type, measuring response variability to quantify the stability of the user simulator.
3. **Risk Calibration Audit:** Manually review a stratified sample of "high-risk" tasks to verify that the risk labels and SAR calculations align with human judgment of actual safety risk.