---
ver: rpa2
title: 'Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and
  Self-Verification'
arxiv_id: '2505.09031'
source_url: https://arxiv.org/abs/2505.09031
tags:
- reasoning
- arxiv
- hallucination
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of reducing hallucinations in
  large language models (LLMs) by combining Chain-of-Thought (CoT) reasoning with
  Retrieval-Augmented Generation (RAG), along with self-consistency and self-verification
  techniques. The proposed approach grounds model reasoning in external knowledge
  and enables models to verify or revise their own outputs, aiming to improve factual
  accuracy and reliability.
---

# Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification

## Quick Facts
- arXiv ID: 2505.09031
- Source URL: https://arxiv.org/abs/2505.09031
- Reference count: 1
- This study shows combining RAG, CoT, self-consistency, and self-verification reduces hallucinations while maintaining response quality.

## Executive Summary
This study addresses the challenge of reducing hallucinations in large language models (LLMs) by combining Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), along with self-consistency and self-verification techniques. The proposed approach grounds model reasoning in external knowledge and enables models to verify or revise their own outputs, aiming to improve factual accuracy and reliability. Experiments across HaluEval, FEVER, and TruthfulQA datasets show that all methods outperform baseline models. RAG + CoT and self-verification consistently achieve the best results, with self-verification yielding the highest accuracy and truthfulness scores. The findings demonstrate that integrating retrieval, reasoning, and verification significantly reduces hallucinations while maintaining response quality.

## Method Summary
The study combines Chain-of-Thought (CoT) reasoning, Retrieval-Augmented Generation (RAG), self-consistency, and self-verification to reduce hallucinations in LLMs. RAG retrieves 5 relevant document chunks from a vector database using query embeddings, which are then used alongside CoT prompts to ground reasoning. Self-consistency generates multiple reasoning paths at varying temperatures and selects the most consistent answer through majority voting. Self-verification involves a post-hoc evaluation step where the model classifies its own output as correct or hallucinated based on retrieved evidence and ground truth. Experiments were conducted on HaluEval (500 samples), FEVER (500 samples), and TruthfulQA (500 samples) using GPT-3.5-Turbo, LLaMA-2-7b, and DeepSeek-R1.

## Key Results
- RAG + CoT integration outperforms individual methods by grounding reasoning in external facts
- Self-verification yields the highest accuracy and truthfulness scores across all datasets
- All four techniques (RAG, CoT, self-consistency, self-verification) outperform baseline models
- Self-consistency with n=9 samples and τ=0.5 threshold effectively reduces random errors

## Why This Works (Mechanism)

### Mechanism 1: RAG + CoT Integration for Grounded Reasoning
- Combining retrieval with structured reasoning reduces hallucinations more effectively than either technique alone
- RAG retrieves 5 relevant document chunks which are fed to the LLM alongside a CoT prompt, forcing the model to reason through retrieved evidence before generating answers
- Core assumption: Retrieval quality is sufficient; irrelevant documents add noise rather than signal
- Evidence: Integrating retrieval, reasoning, and verification significantly reduces hallucinations while maintaining response quality

### Mechanism 2: Self-Consistency via Multi-Sample Voting
- Sampling multiple reasoning paths at varying temperatures and selecting the most consistent answer reduces random errors
- Generate n=9 responses per query with temperatures sampled uniformly; majority voting determines final classification
- Core assumption: Correct answers cluster more consistently than incorrect ones across temperature variations
- Evidence: This helps reduce randomness and improves the reliability of responses, especially in tasks requiring reasoning or multi-step logic

### Mechanism 3: Self-Verification with External Evidence
- Post-hoc verification step where the model evaluates its own output against retrieved evidence improves factual accuracy
- Second prompt provides original query, generated answer, ground truth, and top-k retrieved documents for classification
- Core assumption: The same model can reliably judge correctness when given ground truth and external context
- Evidence: Self-verification yielding the highest accuracy and truthfulness scores

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: All mechanisms except baseline depend on retrieving external knowledge to ground responses
  - Quick check question: Given a user query, how would you retrieve the top-5 most relevant chunks from a vector database, and what embedding model determines "relevance"?

- **Concept: Cosine Similarity for Text Comparison**
  - Why needed here: Self-consistency uses cosine similarity (threshold 0.5) to compare generated outputs against ground truth
  - Quick check question: Two sentence embeddings have cosine similarity of 0.3. What does this indicate about their semantic relationship, and would they pass the τ=0.5 threshold used in this paper?

- **Concept: Temperature Sampling in LLMs**
  - Why needed here: Self-consistency explicitly varies temperature to generate diverse reasoning paths
  - Quick check question: If you set temperature=0.1 versus temperature=0.8, how would the distribution of 9 sampled responses differ, and which might be better for detecting hallucinations?

## Architecture Onboarding

- **Component map:** Raw documents → text chunking → embedding encoder → Pinecone vector store → Query embedding → Pinecone similarity search → top-5 chunks → context assembly → CoT prompt → generation → (optional) self-verification → final answer

- **Critical path:** Query → embedding → retrieval → context + CoT prompt → generation → (optional) self-verification → final answer

- **Design tradeoffs:**
  - Retrieval count (k): k=2 risks incomplete context; k=10 introduces noise. Paper settled on k=5
  - Temperature: 0.4 balances determinism and diversity. Higher values increase self-consistency cost
  - Self-verification requires ground truth: Not deployable in production without labeled answers
  - Model choice: LLaMA-2 slightly outperformed GPT-3.5-Turbo on self-verification, possibly due to open-weight adaptability

- **Failure signatures:**
  - Low similarity scores during retrieval → empty or irrelevant context → hallucination rate increases
  - 50-50 split in self-consistency voting → n=9 (odd number) prevents ties, but borderline cases remain unstable
  - Verification prompt returns "hallucinated" for correct answers → evidence documents may contradict ground truth

- **First 3 experiments:**
  1. Baseline vs. RAG-only vs. CoT-only: Run 100 samples from HaluEval through each configuration to quantify individual contribution of retrieval vs. reasoning
  2. Temperature sweep for self-consistency: Test n=5, 9, 15 with temperatures in [0.3, 0.7] to find cost-accuracy inflection point
  3. Retrieval quality audit: Manually inspect top-5 retrieved documents for 20 queries in TruthfulQA; correlate relevance score with final answer correctness to validate k=5 assumption

## Open Questions the Paper Calls Out

- **Open Question 1:** Does using a different LLM for verification than for generation improve hallucination detection compared to same-model verification?
  - Basis: Section 7 states the self-verification architecture currently uses the same LLM for both generation and verification, and future work could extend this by using different LLMs for the verification step
  - Why unresolved: All experiments used identical models for generation and verification; cross-model consistency was not tested
  - What evidence would resolve it: Comparative experiments across generator-verifier model pairs measuring accuracy on FEVER and TruthfulQA

- **Open Question 2:** How does self-verification performance change when ground truth answers are unavailable during the verification step?
  - Basis: Figure 4 shows the verification prompt includes "correct answer from dataset." The method depends on ground truth access, limiting real-world applicability where correct answers are unknown
  - Why unresolved: No ablation study tested verification without ground truth; all experiments assumed its availability
  - What evidence would resolve it: Experiments comparing verification performance with versus without ground truth access on HaluEval and TruthfulQA

- **Open Question 3:** Do CoT+RAG, self-consistency, and self-verification techniques transfer effectively to non-English LLMs?
  - Basis: Section 7 proposes extending the hallucination framework to multilingual LLMs to assess whether techniques hold across non-English languages
  - Why unresolved: All evaluated datasets are English-only
  - What evidence would resolve it: Evaluation on multilingual hallucination benchmarks with models like mGPT or BLOOM

- **Open Question 4:** Can an early stopping mechanism based on response similarity reduce self-consistency computational costs without significantly degrading accuracy?
  - Basis: Section 7 identifies integrating an early stopping mechanism as a future direction to reduce computational cost of self-consistency sampling
  - Why unresolved: Current implementation always samples n=9 responses regardless of early convergence
  - What evidence would resolve it: Experiments varying convergence thresholds and measuring accuracy-efficiency tradeoffs

## Limitations

- Unspecified implementation details prevent faithful reproduction, particularly the embedding model and chunking strategy for Pinecone indexing
- Self-verification prompt template remains undocumented despite being central to the highest-performing method
- Temperature ranges for self-consistency sampling are unspecified, making it impossible to replicate exact experimental conditions

## Confidence

- **High Confidence**: RAG + CoT integration mechanism and its effectiveness (supported by multiple results tables and clear architectural description)
- **Medium Confidence**: Self-consistency results (mechanism described but temperature parameters missing; results consistent across datasets but exact conditions unclear)
- **Low Confidence**: Self-verification claims (highest accuracy reported but prompt template and ground truth availability constraints not addressed; critical for practical deployment)

## Next Checks

1. **Retrieval Quality Audit**: Manually inspect top-5 retrieved documents for 20 randomly selected queries across all three datasets to verify that k=5 provides optimal balance between coverage and noise, and to establish baseline relevance scores for future comparisons

2. **Threshold Sensitivity Analysis**: Systematically sweep the cosine similarity threshold τ from 0.3 to 0.7 on a validation subset of each dataset to determine if τ=0.5 is truly optimal or dataset-dependent, particularly for self-consistency and MC2 evaluation

3. **Temperature Range Impact Study**: Test self-consistency with fixed temperature values (0.3, 0.5, 0.7) and compare against the described uniform sampling approach to isolate the effect of temperature variation versus sample count on hallucination detection accuracy and computational cost