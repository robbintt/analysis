---
ver: rpa2
title: 'MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling'
arxiv_id: '2512.23824'
source_url: https://arxiv.org/abs/2512.23824
tags:
- arxiv
- ms-ssm
- sequence
- state
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MS-SSM, a multi-scale state space model designed
  to capture both fine-grained and coarse-grained temporal dependencies in sequence
  modeling. Traditional SSMs have limited effective memory, and MS-SSM addresses this
  by decomposing input sequences into multiple resolution scales, each processed by
  specialized state-space dynamics.
---

# MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling

## Quick Facts
- arXiv ID: 2512.23824
- Source URL: https://arxiv.org/abs/2512.23824
- Authors: Mahdi Karami; Ali Behrouz; Peilin Zhong; Razvan Pascanu; Vahab Mirrokni
- Reference count: 29
- Primary result: Multi-scale state space model that decomposes sequences into multiple resolution scales, each processed by specialized SSM dynamics, achieving state-of-the-art performance on benchmarks including sCIFAR (93.3%), ListOps (63.04%), and PTB-XL.

## Executive Summary
MS-SSM introduces a multi-scale state space model that addresses the limited effective memory of traditional SSMs by decomposing input sequences into multiple resolution scales. Each scale is processed by specialized state-space dynamics with scale-dependent initialization, and an input-dependent scale-mixing mechanism dynamically fuses information across resolutions. The approach maintains computational efficiency while capturing both fine-grained and coarse-grained temporal dependencies, outperforming prior SSM-based models on diverse benchmarks.

## Method Summary
MS-SSM decomposes input sequences into multiple resolution scales using nested dilated convolutions inspired by Stationary Wavelet Transform. Each scale is processed by a dedicated SSM with scale-dependent diagonal initialization in the state transition matrix, allocating longer effective memory to coarser resolutions. An input-dependent scale-mixing mechanism uses a linear projection from the original input to compute dynamic weights that fuse outputs across scales. The model maintains translation invariance through redundant representation and achieves significant improvements in capturing long-range dependencies while preserving computational efficiency.

## Key Results
- Achieves 93.3% accuracy on sCIFAR-1024, outperforming standard ViTs
- Reaches 63.04% accuracy on ListOps, demonstrating superior hierarchical reasoning
- Shows 95.06% AUROC on PTB-XL time series classification
- Demonstrates 2.25× larger effective receptive field compared to Mamba on ListOps

## Why This Works (Mechanism)

### Mechanism 1: Multi-scale decomposition for hierarchical pattern capture
- **Claim**: Multi-scale decomposition enables simultaneous capture of fine-grained and coarse-grained temporal dependencies
- **Mechanism**: Nested dilated 1D convolutions with dilation factor 2^(s-1) at scale s decompose the input into S+1 scale representations, inspired by Stationary Wavelet Transform. Higher scales capture coarse global trends; lower scales capture fine details. Each scale is processed by a dedicated SSM.
- **Core assumption**: The underlying signal contains hierarchical patterns manifesting at multiple timescales
- **Evidence anchors**: [abstract] "decomposing input sequences into multiple resolution scales, each processed by specialized state-space dynamics"; [section 2.2] "By capturing both fine-grained, high-frequency patterns and coarse, global trends, MS-SSM enhances memory efficiency"
- **Break condition**: If task data lacks inherent multi-scale structure, decomposition may add overhead without gains

### Mechanism 2: Scale-dependent state initialization for memory allocation
- **Claim**: Scale-dependent state initialization allocates longer effective memory to coarser resolutions
- **Mechanism**: Diagonal elements of the state transition matrix Ā_s are initialized in [e^(-Δ₀N(S+1-s)), e^(-NΔ₀(S-s))]. Higher s (coarser scales) get eigenvalues closer to 1, extending memory horizon; finer scales get smaller values for local dynamics.
- **Core assumption**: Coarse-grained representations require longer memory horizons than fine-grained details
- **Evidence anchors**: [section 2.2] "For lower resolutions (higher value of s), we initialize the diagonal elements of Ā with values closer to 1"; [table 7] Scale-dependent init achieves 63.04% on ListOps vs. 57.49% with Mamba's init
- **Break condition**: If scale assignments don't correlate with required memory lengths, initialization mismatch degrades performance

### Mechanism 3: Input-dependent scale mixing for dynamic fusion
- **Claim**: Input-dependent scale mixing dynamically fuses multi-resolution information based on content
- **Mechanism**: A linear projection computes weights E_t from the *original* input x_t, then produces a weighted sum z_t = E_t * y_t across all scale outputs. The paper empirically finds gating on raw input outperforms gating on scale-specific representations.
- **Core assumption**: Optimal scale combinations vary by input; raw input provides better mixing signals than transformed representations
- **Evidence anchors**: [section 2.2] "gating based on the raw input, x_t, is more effective than gating based on the scale-specific representations"; [table 5] Input-dependent mixing: 63.04% vs. input-independent: 61.28% on ListOps
- **Break condition**: If all scales are equally relevant regardless of input, dynamic mixing adds parameters without benefit

## Foundational Learning

### Concept: Discrete State Space Models and Effective Memory
- **Why needed here**: MS-SSM builds on discretized SSMs where Ā eigenvalues near 1 produce longer memory (inversely proportional to distance from unit circle). Understanding this is essential for scale-dependent initialization.
- **Quick check question**: Why do eigenvalues of Ā closer to 1 yield longer effective memory in a discrete SSM?

### Concept: Stationary Wavelet Transform (SWT) and Translation Invariance
- **Why needed here**: The multi-scale decomposition uses SWT (not standard DWT) to maintain sequence length across scales, preserving translation invariance at the cost of redundant representation.
- **Quick check question**: What is the computational tradeoff of SWT's redundancy versus standard DWT's downsampling?

### Concept: Mean Mixing Distance as Effective Receptive Field Metric
- **Why needed here**: The paper defines ERF via the Jacobian J(x) rather than attention weights. MS-SSM achieves mean mixing distance ~95 vs. ~39 for Mamba on ListOps.
- **Quick check question**: How does the Jacobian-based definition differ from attention-distance metrics in Transformers?

## Architecture Onboarding

### Component map
Input x_t → Multi-Scale Conv (nested Conv1d, dilation 2^(s-1)) → Scale Representations [x_t; x̂^0_t, ..., x̂^S_t] (S+2 channels) → Parallel SSM Array (S+2 SSMs, scale-specific A_s init) → Scale Outputs [y^0_t, ..., y^(S+1)_t] → Scale Mixer (E_t = Linear(x_t), z_t = E_t · y_t) → Output z_t

### Critical path
1. Verify dilation factors: 2^(s-1) for scale s in multi-scale conv
2. Confirm initialization: diag(A_s) in scale-dependent range per Eq. 2.2
3. Ensure gating uses original x_t (not scale representations) for B̄_t, C_t, Δ_t

### Design tradeoffs
- **Scales S**: More scales increase effective state size to (S+2)N. Paper uses S=3. O(KS) extra params, O(LKS) compute.
- **SSM backbone**: MS-SSM(S6) better on sCIFAR (93.3% vs. 90.3%); MS-SSM(S4) more stable on LRA (91.89% vs. 86.73%).
- **Mixer complexity**: Input-dependent mixer (+1.76% on ListOps) adds per-token compute vs. static linear layer.

### Failure signatures
- **LRA underperformance vs. Mamba**: Check scale-dependent initialization is correctly applied (Table 7 shows 5.5% gap if using Mamba init)
- **Memory blowup**: Effective state is (S+2)N; reduce N proportionally when matching baseline state dimensions
- **Scale collapse**: If E_t weights become deterministic, input-dependent gating isn't learning

### First 3 experiments
1. **Decomposition ablation**: Train on sCIFAR with S=3 vs. single-scale; expect 0.7%+ gain (Table 5: removing multi-scale conv drops ListOps from 63.04% to 37.98%)
2. **Initialization validation**: Compare scale-dependent vs. Mamba init on ListOps; expect ~5.5% difference per Table 7
3. **ERF measurement**: Compute mean mixing distance on ListOps validation; target ~95 (Table 6) vs. ~39 for Mamba

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MS-SSM provide significant performance gains in large-scale autoregressive language modeling compared to standard Transformers or Mamba?
- Basis in paper: [explicit] The conclusion states, "extending the MS-SSM framework to other sequence domains, such as natural language processing... could further validate its generality."
- Why unresolved: Current experiments are limited to vision (sCIFAR, ImageNet), synthetic hierarchical tasks (ListOps), and time series (PTB-XL), excluding standard language modeling benchmarks.
- What evidence would resolve it: Benchmarks on standard language modeling datasets (e.g., The Pile) showing lower perplexity or higher accuracy with comparable parameter counts and inference speeds.

### Open Question 2
- Question: Can the multi-resolution initialization scheme and scale decomposition be effectively integrated into newer non-linear or test-time training RNN architectures?
- Basis in paper: [explicit] The authors propose exploring "multi-resolution in the most recent form of linear RNNs (Orvieto et al., 2023; Beck et al., 2024)... and nonlinear test-time training models."
- Why unresolved: The current implementation is tested primarily on S4 and S6 (Mamba) blocks, but the interaction with architectures like xLSTM or Titans remains unexplored.
- What evidence would resolve it: Performance comparisons of MS-SSM-enhanced xLSTM or Titans against their vanilla baselines on long-range dependency benchmarks.

### Open Question 3
- Question: How does the redundant representation of the Stationary Wavelet Transform (SWT) impact memory efficiency during training relative to standard downsampling approaches?
- Basis in paper: [inferred] The paper notes the method uses SWT, which maintains "the same sequence length at each decomposition level, producing a redundant representation," trading off memory for translation-invariance.
- Why unresolved: While computational complexity is analyzed, the specific memory footprint impact of this redundancy on large batch sizes or very long sequences is not quantified against downsampled multi-scale methods.
- What evidence would resolve it: A comparative analysis of peak GPU memory usage and throughput between MS-SSM (SWT) and a downsampled multi-scale variant.

## Limitations
- Core claims rely heavily on empirical results with limited theoretical grounding
- Multi-scale decomposition assumes hierarchical patterns exist in all target tasks, not universally validated
- Input-dependent scale mixing mechanism justified through single ablation study without exploring alternatives

## Confidence
- **High confidence**: Multi-scale decomposition's basic efficacy (supported by wavelet literature and ablation in Table 5 showing 25%+ gain when removed)
- **Medium confidence**: Scale-dependent initialization's contribution (supported by Table 7's 5.5% gap, but no theoretical explanation)
- **Medium confidence**: Input-dependent scale mixing's effectiveness (single ablation study, mechanism unclear)

## Next Checks
1. **Scale necessity validation**: Apply MS-SSM to a task without clear hierarchical structure (e.g., DNA sequence classification) and measure if multi-scale decomposition provides any benefit versus a single-scale baseline.
2. **Initialization sensitivity analysis**: Systematically vary the initialization ranges for different scales on ListOps to determine if the specific intervals used are optimal or if performance is robust to initialization perturbations.
3. **Mixer mechanism probing**: Replace the input-dependent mixer with alternatives: (a) gating on concatenated scale outputs instead of raw input, (b) static weighted sum, (c) attention-based fusion, to isolate what property of the current design drives its effectiveness.