---
ver: rpa2
title: A Review of Causal Decision Making
arxiv_id: '2502.16156'
source_url: https://arxiv.org/abs/2502.16156
tags:
- causal
- learning
- policy
- data
- treatment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper reviews causal decision-making, focusing on three tasks:
  causal structure learning (CSL), causal effect learning (CEL), and causal policy
  learning (CPL). It presents a framework for understanding the role of causality
  in decision-making across six paradigms, covering scenarios from independent observations
  to complex Markov decision processes.'
---

# A Review of Causal Decision Making

## Quick Facts
- **arXiv ID:** 2502.16156
- **Source URL:** https://arxiv.org/abs/2502.16156
- **Reference count:** 40
- **One-line primary result:** Introduces a comprehensive framework for integrating causal reasoning into decision-making across six paradigms, with applications in medicine and recommendation systems.

## Executive Summary
This review paper systematically examines causal decision-making through a unified lens that integrates three core tasks: causal structure learning (CSL), causal effect learning (CEL), and causal policy learning (CPL). The authors organize decision-making scenarios into six paradigms ranging from independent observations to complex Markov decision processes, highlighting how causal methods can improve decision-making in domains like healthcare and recommendation systems. The paper presents both theoretical foundations and practical implementations, culminating in a Python-based framework with demonstrated applications using real-world datasets including MIMIC-III and MovieLens.

## Method Summary
The framework follows a sequential pipeline where CSL identifies causal relationships using constraint-based or score-based methods, CEL quantifies treatment effects through estimators like IPW and doubly robust methods, and CPL optimizes policies based on these effect estimates. The review covers both online and offline settings, with special attention to distribution shift mitigation through pessimism-based approaches in offline scenarios. Implementation is provided through a comprehensive Python framework that demonstrates the methodology across synthetic and real-world datasets, with specific case studies on medical treatment decisions and recommendation systems.

## Key Results
- The review establishes a unified framework connecting CSL, CEL, and CPL across six decision-making paradigms
- Demonstrates practical applications using MIMIC-III for medical treatment optimization and MovieLens for recommendation systems
- Introduces pessimism-based methods to address distribution shift in offline causal decision-making
- Highlights the importance of causal structure learning for decision-oriented tasks in non-i.i.d. data settings

## Why This Works (Mechanism)

### Mechanism 1: The Sequential Identification-Estimation-Optimization Pipeline
The system solves causal decision-making through a sequential workflow: discovering causal structure (CSL), quantifying causal impacts (CEL), and optimizing strategies (CPL). CSL identifies DAGs distinguishing confounders from mediators, CEL estimates effects using structure-aware estimators, and CPL derives optimal policies. This assumes decoupling tasks doesn't compound errors, though methods like double machine learning can mitigate this. The mechanism fails if causal graph misidentification occurs.

### Mechanism 2: Distribution Shift Mitigation via Pessimism (Offline Setting)
Offline CPL uses pessimism-based penalties to prevent overestimation when learning from fixed datasets. Standard value methods extrapolate poorly to unseen state-action pairs; pessimism penalizes value estimates for actions outside the behavior policy's distribution or with high uncertainty. This assumes partial coverage of optimal policy trajectories rather than full coverage. The mechanism fails if the dataset contains zero data about optimal actions.

### Mechanism 3: Robustness via Assumption Violation Proxies
The framework handles violated assumptions (like No Unmeasured Confounders) using proxy variables such as Instrumental Variables. When NUC fails, IV methods isolate exogenous treatment variation to estimate causal effects despite hidden confounders. This assumes valid IV properties (relevance, exclusion restriction, independence). The mechanism fails if the proxy is confounded or exclusion restriction is violated.

## Foundational Learning

- **Concept: Potential Outcomes Framework (Rubin Causal Model)**
  - **Why needed here:** Defines causal effects (ATE = E[R(1) - R(0)]) and distinguishes observational P(R|A) from interventional P(R|do(A))
  - **Quick check question:** Can you explain why E[R(1)] is generally not equal to E[R|A=1] in an observational dataset?

- **Concept: Markov Decision Processes (MDPs) & Bellman Equations**
  - **Why needed here:** Organizes decision-making into Paradigms 2 and 5 requiring Markov property understanding
  - **Quick check question:** What is the difference between "Paradigm 1" (i.i.d. states) and "Paradigm 2" (Markovian transitions) in terms of how the reward is generated?

- **Concept: Identification Assumptions (NUC, SUTVA, Positivity)**
  - **Why needed here:** These axioms must hold for IPW and DR estimators to produce valid causal effects
  - **Quick check question:** Does the "Positivity" assumption require that every individual has a non-zero probability of receiving every treatment, or just some treatment?

## Architecture Onboarding

- **Component map:** Observational Dataset (S, A, R) -> CSL Module (PC/NOTEARS) -> CEL Module (IPW, DR) -> CPL Module (Q-learning, REINFORCE with Pessimism)
- **Critical path:** 1) Structure Discovery: Identify DAG G from data D 2) Effect Identification: Use G to determine if P(R|do(A)) is identifiable 3) Effect Estimation: Apply estimators to get τ̂ (treatment effect) 4) Policy Optimization: Use τ̂ or Q(s,a) to solve π* = argmax V^π
- **Design tradeoffs:** Model-based vs. Model-free (CPL): Model-based is sample-efficient but prone to bias; Model-free is robust but data-hungry. Online vs. Offline: Online allows exploration but risks regret; Offline is safe but requires pessimism.
- **Failure signatures:** Simpson's Paradox (aggregated vs. subgroup effects), High Variance in IPW (long-horizon MDPs), Extrapolation Error (Q-function predicts high rewards for unseen actions)
- **First 3 experiments:** 1) Validate CSL on Synthetic Data with known Linear SEM using NOTEARS 2) Compare Estimators (CEL) using MIMIC-III with Naive vs. Doubly Robust estimators 3) Implement IPW estimator on MovieLens logged bandit dataset for Offline Policy Evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can decision-oriented Causal Structure Learning be effectively extended to non-i.i.d. data structures like MDPs and time-series data?
- **Basis in paper:** Section 5.3 states that decision-oriented causal discovery for Paradigms 2 and 3 is a missing piece in existing literature
- **Why unresolved:** Current CSL methods rely on i.i.d. assumptions while time-series causal discovery doesn't incorporate decision variables or intervention logic
- **What evidence would resolve it:** Development of algorithms that recover causal graphs with intervention variables from sequential data while adhering to temporal constraints

### Open Question 2
- **Question:** How can valid statistical inference be conducted for online policy evaluation with adaptive data collection and dynamic optimal policies?
- **Basis in paper:** Section 8.2 identifies challenges in online policy evaluation and notes very few related studies
- **Why unresolved:** Standard inferential methods assume fixed sample sizes or known policies, but online algorithms generate history-dependent data
- **What evidence would resolve it:** Derivation of confidence intervals accounting for exploration probability in estimator's asymptotic distribution

### Open Question 3
- **Question:** How can causal bandit algorithms handle complex interference structures without restrictive assumptions like fixed network clusters?
- **Basis in paper:** Section 9.2 notes the need for more flexible approaches to address broader bandit settings with interference
- **Why unresolved:** Existing work assumes interference within fixed clusters or specific neighborhoods, limiting applicability in complex environments
- **What evidence would resolve it:** Regret analysis and empirical validation of bandit algorithms achieving robust performance under general or partially unknown interference models

## Limitations

- The sequential pipeline assumes decoupling CSL, CEL, and CPL doesn't introduce compounding errors from misidentified causal structures
- Performance in high-dimensional, continuous state-action spaces remains underexplored, with most examples using discrete or simplified representations
- The framework's effectiveness depends critically on the validity of instrumental variables, which is often untestable in practice

## Confidence

- **High Confidence:** Pessimism-based offline learning mechanism is well-established and correctly identified for preventing overestimation in data-scarce scenarios
- **Medium Confidence:** Sequential identification-estimation-optimization pipeline is logically sound but assumes decoupling doesn't compound errors; double machine learning is mentioned as mitigation
- **Medium Confidence:** Robustness claims for assumption violation proxies are valid but critically depend on IV validity, which is often untestable

## Next Checks

1. **Error Propagation Analysis:** Implement synthetic experiment with 10-20% CSL noise perturbation and measure downstream CPL performance degradation to quantify decoupling assumption validity
2. **High-Dimensional Scalability Test:** Apply complete CDM pipeline (CSL→CEL→CPL) to dataset with >100 features (e.g., eICU) and benchmark against standard RL methods to assess scalability
3. **Instrumental Variable Robustness Test:** Construct dataset with mildly invalid IV (small direct effect on outcome) and evaluate whether framework detects or corrects for this violation, testing robustness mechanism