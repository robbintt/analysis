---
ver: rpa2
title: Characterization and Mitigation of Training Instabilities in Microscaling Formats
arxiv_id: '2506.20752'
source_url: https://arxiv.org/abs/2506.20752
tags:
- training
- precision
- mxfp8
- loss
- formats
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Training instabilities arise in LLM pretraining with MX block-scaling
  formats due to quantization-induced gradient bias, especially from tightly clustered
  layer-norm weights. A synthetic residual MLP model reveals that this bias triggers
  runaway divergence even under stable hyperparameters.
---

# Characterization and Mitigation of Training Instabilities in Microscaling Formats

## Quick Facts
- arXiv ID: 2506.20752
- Source URL: https://arxiv.org/abs/2506.20752
- Reference count: 5
- Primary result: Gradient quantization bias in microscale formats causes training instabilities, mitigated by higher-precision activations or forward-only quantization.

## Executive Summary
Training instabilities in LLM pretraining with MX block-scaling formats arise from quantization-induced gradient bias, particularly when layer-norm weights are tightly clustered. A synthetic residual MLP model reveals that this bias triggers runaway divergence even under stable hyperparameters. Stabilization is achieved by keeping activations in higher precision (bfloat16) or limiting quantization to the forward pass. With these fixes, MXFP8 E4M3 weights paired with bfloat16 activations match full-bfloat16 baseline performance and allow valid scaling-law extraction.

## Method Summary
The authors use a synthetic residual MLP to isolate the source of training instabilities in microscale formats. They demonstrate that quantization-induced gradient bias, especially from clustered layer-norm weights, causes divergence. Stabilization is achieved by maintaining higher-precision activations (bfloat16) or applying quantization only during the forward pass. This approach restores baseline performance for MXFP8 E4M3 weights.

## Key Results
- Gradient quantization bias in microscale formats causes training instabilities, especially with clustered layer-norm weights.
- Keeping activations in bfloat16 or limiting quantization to the forward pass stabilizes training.
- MXFP8 E4M3 weights with bfloat16 activations match full-bfloat16 baseline performance and enable scaling-law extraction.

## Why This Works (Mechanism)
Training instabilities stem from quantization-induced gradient bias in microscale formats. When layer-norm weights are tightly clustered, quantization introduces systematic errors in gradient computation, leading to divergence. Maintaining higher-precision activations or restricting quantization to the forward pass reduces this bias, stabilizing training dynamics.

## Foundational Learning
- **Microscaling formats**: Why needed - Enable memory-efficient training; Quick check - Verify supported formats (e.g., MXFP8 E4M3).
- **Gradient quantization bias**: Why needed - Source of instability; Quick check - Analyze gradient statistics pre/post-quantization.
- **Layer normalization**: Why needed - Critical for training stability; Quick check - Inspect weight distributions for clustering.
- **Forward-only quantization**: Why needed - Mitigates gradient bias; Quick check - Compare backward pass precision.
- **Scaling laws**: Why needed - Benchmark model performance; Quick check - Validate loss scaling with model size.

## Architecture Onboarding
**Component Map**: Input -> MXFP8 E4M3 weights -> LayerNorm -> Forward pass (quantized) -> Backward pass (bfloat16) -> Output

**Critical Path**: Forward computation with quantized weights → Gradient computation with higher-precision activations → Parameter updates

**Design Tradeoffs**: Memory efficiency (microscaling) vs. training stability (precision); limited to tested formats and MLP proxy

**Failure Signatures**: Diverging loss curves, gradient explosion, or collapse in training metrics

**First Experiments**:
1. Train synthetic MLP with MXFP8 E4M3 weights and bfloat16 activations to reproduce baseline results.
2. Vary layer-norm weight distributions to test sensitivity to clustering.
3. Apply forward-only quantization and compare training stability to full-quantization baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to transformer variants, attention layers, and longer training regimes is untested.
- The synthetic MLP model may not capture all emergent instabilities of full LLM architectures.
- Root-cause diagnosis is inferred from correlation rather than direct measurement in full-scale training.

## Confidence
- **Medium**: Root-cause diagnosis is inferred from correlation between weight clustering and gradient bias.
- **High**: Mitigation (bfloat16 activations) is validated for MXFP8 E4M3 format in tested conditions.

## Next Checks
1. Run full LLM pretraining with stabilized MXFP8 E4M3 format on standard benchmarks to confirm baseline-matching performance.
2. Evaluate mitigation across other microscale formats (MXFP8 E5M2, MXFP8 E4M2) and weight distributions.
3. Conduct long-horizon training runs to detect delayed instabilities or degradation.