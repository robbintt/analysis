---
ver: rpa2
title: 'From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language
  Models'
arxiv_id: '2509.14515'
source_url: https://arxiv.org/abs/2509.14515
tags:
- full-duplex
- language
- dialogue
- spoken
- fd-slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive analysis of Full-Duplex Spoken
  Language Models (FD-SLMs), addressing the critical limitation of current spoken
  dialogue systems that operate in sequential listen-think-speak cycles. The authors
  establish a formal taxonomy distinguishing Engineered Synchronization (modular architectures
  with explicit state arbitration) from Learned Synchronization (end-to-end systems
  with intrinsic full-duplex capabilities), and unify fragmented evaluation approaches
  into a four-pillar framework: Temporal Dynamics, Behavioral Arbitration, Semantic
  Coherence, and Acoustic Performance.'
---

# From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models

## Quick Facts
- **arXiv ID**: 2509.14515
- **Source URL**: https://arxiv.org/abs/2509.14515
- **Reference count**: 0
- **Primary result**: Establishes taxonomy and four-pillar evaluation framework for Full-Duplex Spoken Language Models (FD-SLMs), revealing fundamental latency-coherence trade-offs and data scarcity bottlenecks.

## Executive Summary
This survey comprehensively analyzes Full-Duplex Spoken Language Models (FD-SLMs), addressing the critical limitation of current spoken dialogue systems that operate in sequential listen-think-speak cycles. The authors establish a formal taxonomy distinguishing Engineered Synchronization (modular architectures with explicit state arbitration) from Learned Synchronization (end-to-end systems with intrinsic full-duplex capabilities), and unify fragmented evaluation approaches into a four-pillar framework: Temporal Dynamics, Behavioral Arbitration, Semantic Coherence, and Acoustic Performance. Through comparative analysis of seven representative FD-SLMs, the survey reveals fundamental challenges including synchronous data scarcity, architectural divergence, and evaluation gaps. While acoustic quality approaches human levels (MOS ~4.85), critical gaps persist in behavioral arbitration (interruption success rate: 54-86% vs. 94% human) and semantic coherence, with an inverse latency-coherence trade-off constraining current systems. The work provides a roadmap for advancing human-AI communication by identifying the need for architectural convergence, synthetic data capturing authentic dynamics, comprehensive behavioral evaluation, and robust safety mechanisms.

## Method Summary
The survey establishes a formal taxonomy of FD-SLMs, distinguishing between Engineered Synchronization (modular architectures with explicit state arbitration via FSMs) and Learned Synchronization (end-to-end systems with intrinsic full-duplex capabilities). Training data consists of synchronized multi-channel spontaneous dialogue datasets including Fisher English (1,960 hrs), AMI Meeting Corpus (100 hrs), and others. Models are trained using either joint probability objectives (L_NTPP) or conditional probability objectives (L_Cond) on aligned sequences S_E (environment) and S_A (agent). Evaluation follows a four-pillar framework: Temporal Dynamics (FTO, SL, IRD), Behavioral Arbitration (ISR, target ~94% human), Semantic Coherence (WER, PPL, QA Accuracy), and Acoustic Performance (N-MOS, M-MOS). Real-time constraint requires Compute(a_t) < 200ms to meet conversational thresholds.

## Key Results
- FD-SLMs transform sequential listen-think-speak paradigms to parallel cognitive architectures, achieving sub-200ms response latencies
- Current systems show inverse latency-coherence correlation: low FTO correlates with degraded semantic performance
- Behavioral arbitration significantly underperforms humans (ISR 54-86% vs. 94% human) due to inadequate evaluation metrics
- Acoustic quality approaches human parity (MOS ~4.85) but semantic coherence remains challenging (QA accuracy 33.8-54.97%)

## Why This Works (Mechanism)

### Mechanism 1: Concurrent Encoding-Decoding via Joint Probability Modeling
FD-SLMs achieve cognitive parallelism by modeling the joint distribution P(S_E, S_A) of environment and agent speech, enabling simultaneous input processing and output generation within unified cycles. Rather than alternating listen-think-speak phases, the model predicts (e_t, a_t) token pairs at each timestep conditioned on partial histories S_E^{<t} and S_A^{<t}, allowing a_t generation to proceed while e_{t+1}... continue arriving. The 100–200 ms "cognitive clock" granularity is sufficient for human-perceptible naturalness; sub-200 ms latency meets conversational constraints.

### Mechanism 2: Emergent Turn-Taking Dynamics from Synchronous Training
Turn-taking behaviors—interruptions, backchanneling, overlap management—emerge implicitly from training on synchronized dual-channel dialogue data without explicit behavioral supervision. The model learns temporal and semantic patterns from aligned (S_E, S_A) sequences; silence tokens, audible tokens, and their timing implicitly encode floor-transfer decisions through next-token(-pair) prediction objectives.

### Mechanism 3: Hierarchical Text-Audio Mediation for Semantic-Full-Duplex Integration
Systems like Moshi bridge LLM reasoning capabilities with full-duplex audio by generating an "inner monologue" of text tokens as an intermediate representation before audio synthesis. The model decomposes P(S_A|S_E) via marginalization over text T_A: first generating semantically coherent text tokens, then conditioning audio generation on both text and incoming audio streams.

## Foundational Learning

- **Autoregressive Language Modeling**: All FD-SLM variants build on next-token prediction; understanding causal masking, KV-cache mechanics, and perplexity as a quality signal is prerequisite. Quick check: Can you explain why standard Transformer decoder attention cannot attend to future tokens, and how this constrains streaming?

- **Neural Audio Codecs (RVQ/Codec-based Discretization)**: Most learned-synchronization systems discretize continuous audio into tokens via residual vector quantization; codec choice directly bounds perception latency. Quick check: What is the trade-off between codec codebook size, reconstruction fidelity (MOS), and sequence elongation?

- **Voice Activity Detection and Semantic VAD**: Engineered-synchronization systems rely on VAD for state arbitration; differentiating interruptions from backchannels requires semantic analysis beyond acoustic energy. Quick check: How does acoustic VAD fail on brief backchannels ("mm-hmm") versus semantic VAD that analyzes ASR output?

## Architecture Onboarding

- **Component map**: Input Perception (streaming encoders) -> Core Processing (cross-attention, joint autoregression, interleaving, control tokens) -> Output Synthesis (codec decoders, vocoders, TTS delegation)

- **Critical path**: 1) Select synchronization paradigm (engineered modular vs. learned end-to-end) based on latency tolerance and compute budget 2) Choose discretization strategy (codec tokens vs. continuous embeddings) to bound input latency 3) Design stream fusion mechanism (cross-attention depth, interleaving granularity, or control token vocabulary) 4) Configure output synthesis with streaming-aware chunking

- **Design tradeoffs**: Latency vs. Coherence (lower FTO/s latency often correlates with higher WER and lower QA accuracy); Modularity vs. Integration (engineered systems trade computational overhead for interpretability); Discrete vs. Continuous (codec tokens elongate sequences but enable unified LLM-style modeling)

- **Failure signatures**: Latency accumulation beyond 200 ms per component; Spurious interruption rate elevation; Incoherent resumption after interruption; Echo cancellation failure

- **First 3 experiments**: 1) Baseline latency profiling: Instrument each stage on streaming audio chunks; identify bottleneck component against the 200 ms budget 2) Turn-taking accuracy on synthetic vs. real dialogue: Train on synthetic TTS pairs vs. Fisher/AMI meeting data; measure ISR and backchannel tolerance 3) Interruption recovery stress test: Simulate user interruption at varying semantic boundaries; measure IRD and post-interruption coherence recovery

## Open Questions the Paper Calls Out

### Open Question 1
How can synthetic data generation be advanced to authentically capture prosodic entrainment and overlap dynamics? The survey notes that current synthetic TTS generation "fails to capture prosodic entrainment and overlap dynamics, limiting generalization." This remains unresolved because existing datasets lack sufficient synchronized multi-channel spontaneous dialogue, and current synthesis methods cannot model complex turn-taking physics. A generative training pipeline that improves Interruption Success Rate (ISR) to near-human levels without relying on scarce human recordings would resolve this.

### Open Question 2
Can a unified architecture resolve the observed inverse correlation between latency and semantic coherence? The conclusion identifies an "inverse latency-coherence correlation" and notes that "architectural fragmentation prevents scalable designs." This remains unresolved because current modular systems trade reasoning depth for speed, while end-to-end models struggle with real-time state management under strict time constraints. A model architecture achieving human-level semantic coherence (PPL ~10) while maintaining sub-200ms response latencies would resolve this.

### Open Question 3
What evaluation frameworks are needed to assess proactive behaviors like backchanneling and interruption? The authors state that "Current evaluation lacks proactive behavior metrics" despite behavioral arbitration significantly underperforming human capabilities. This remains unresolved because conventional metrics (WER, MOS) are designed for half-duplex systems and fail to quantify conversational floor arbitration or engagement. A benchmark framework including behavioral classification (e.g., RESPOND/RESUME states) that correlates strongly with human judgments of interaction quality would resolve this.

### Open Question 4
How can robust safety mechanisms be integrated without violating the sub-200ms temporal constraints of full-duplex interaction? The conclusion warns that "ultra-low latency introduces safety risks requiring real-time filtering." This remains unresolved because standard safety filtering adds computational overhead, threatening the "cognitive clock" speed required for seamless turn-taking. A real-time intervention mechanism that successfully mitigates harmful outputs while preserving temporal dynamics (FTO/SL) indistinguishable from unsafe baselines would resolve this.

## Limitations
- Temporal dynamics metrics (FTO, SL, IRD) lack standardized measurement protocols across implementations, making cross-system comparisons challenging
- Behavioral arbitration evaluation shows particularly wide variance (ISR 54-86% vs. 94% human), suggesting benchmarks inadequately capture natural interruption patterns
- Semantic coherence evaluation relies heavily on LLM-as-judge metrics that themselves introduce evaluation uncertainty
- Inverse latency-coherence trade-off appears fundamental rather than architectural, with sub-200ms systems consistently showing degraded semantic performance

## Confidence
- **High Confidence**: Formal taxonomy distinguishing engineered vs. learned synchronization paradigms; four-pillar evaluation framework; fundamental architectural trade-offs between latency and coherence; characterization of data scarcity as core bottleneck
- **Medium Confidence**: Specific mechanism claims about joint probability modeling enabling cognitive parallelism; emergence of turn-taking dynamics from synchronous training; hierarchical text-audio mediation approach
- **Low Confidence**: Absolute performance numbers across different models due to inconsistent evaluation protocols; universality of 200ms cognitive clock assumption across languages and speaking styles; claim that current gaps are purely technical rather than fundamental

## Next Checks
1. **Standardized Temporal Dynamics Benchmark**: Implement the FD-Bench evaluation suite with unified measurement protocols for FTO, SL, and IRD across all seven representative FD-SLM architectures to establish baseline comparability.

2. **Synthetic Data Impact Study**: Systematically compare model performance trained on synthetic TTS dialogue pairs versus authentic spontaneous dialogue data while controlling for quantity and duration to quantify data authenticity effects.

3. **Behavioral Arbitration Stress Test**: Design targeted evaluation scenarios that distinguish between interruptions, backchanneling, and background noise to identify specific failure modes in behavioral arbitration.