---
ver: rpa2
title: Data-Driven Discovery of Feature Groups in Clinical Time Series
arxiv_id: '2511.08260'
source_url: https://arxiv.org/abs/2511.08260
tags:
- feature
- groups
- cluster
- time
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning feature groups in clinical
  time series by clustering feature-wise embedding weights during supervised training.
  The approach integrates directly into standard deep learning pipelines, enabling
  data-driven discovery of task-relevant feature relationships without relying on
  predefined groupings.
---

# Data-Driven Discovery of Feature Groups in Clinical Time Series

## Quick Facts
- **arXiv ID:** 2511.08260
- **Source URL:** https://arxiv.org/abs/2511.08260
- **Reference count:** 40
- **Primary result:** Data-driven feature grouping method achieves performance comparable to expert-defined groups on ICU datasets while offering clinical interpretability.

## Executive Summary
This paper introduces a method for learning feature groups in clinical time series by clustering feature-wise embedding weights during supervised training. The approach integrates directly into standard deep learning pipelines, enabling data-driven discovery of task-relevant feature relationships without relying on predefined groupings. On synthetic data, the method successfully recovers ground-truth feature groups, outperforming static clustering approaches. On real ICU datasets (HiRID and MIMIC-III), learned groups achieve performance comparable to expert-defined groupings across multiple tasks, including mortality and circulatory failure prediction. The learned groups are also clinically interpretable, offering insights into task-relevant feature interactions.

## Method Summary
The method learns global feature groups by clustering feature-wise embedding weights from a step-wise embedding model during supervised training. Raw inputs are first transformed through feature-wise linear embeddings, then grouped based on the learned weights, and finally processed through group-specific embeddings. Clustering is performed dynamically during training using K-means or Gaussian Mixture Models on unified weight vectors. A regularization term encourages coherent cluster formation, while exponential moving averages of centroids ensure stability. The approach handles both numerical and categorical features and can operate with hard or soft membership assignments.

## Key Results
- On synthetic data, Dynamic K-means recovers ground-truth feature groups while Static K-means fails to outperform random clustering
- On HiRID and MIMIC-III, learned groups achieve AUPRC scores comparable to expert-defined groups across mortality, circulatory failure, and decompensation prediction tasks
- Soft GMM membership functions outperform hard clustering on tasks with clinically overlapping feature groups
- The method discovers clinically interpretable groupings, such as separating heart rate features from respiratory features

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Driven Semantic Clustering
During supervised training, embedding weights $W^f$ evolve via backpropagation to encode information necessary for the specific prediction task. By clustering these weights, the model groups features that have learned similar functional roles for the downstream task, rather than just similar raw values. This captures task-relevant semantics better than clustering raw input statistics.

### Mechanism 2: Training Stability via Regularization and Centroid Anchoring
The regularization term $L_{reg}$ and Exponential Moving Average (EMA) of centroids prevent cluster collapse during the non-stationary training process. The intra-cluster loss "tightens" groups while the inter-cluster loss pushes centroids apart. EMA anchors centroids to their history, smoothing the update trajectory as embedding weights change rapidly in early training.

### Mechanism 3: Adaptive Complexity via Soft Membership
Soft membership functions allow the model to handle clinical variables that span multiple physiological systems (e.g., fluid balance affecting both renal and circulatory systems). Instead of binary assignment, the model assigns probability scores, allowing a single feature to contribute to multiple group embeddings and preventing information loss for ambiguous features.

## Foundational Learning

- **Concept: Step-wise Embedding (Feature Tokenization)**
  - Why needed here: The method relies on accessing the weights $W^f$ of the first layer that processes raw inputs
  - Quick check question: How does the dimensionality of the weight matrix $W^f$ differ for a numerical feature vs. a one-hot encoded categorical feature?

- **Concept: K-means vs. Gaussian Mixture Models (GMM)**
  - Why needed here: The paper treats these as interchangeable engines for the "Grouping" step
  - Quick check question: Why does GMM allow for "soft" memberships naturally, while standard K-means requires modification?

- **Concept: Joint Optimization (Multi-task / Auxiliary Loss)**
  - Why needed here: The total loss $L$ is a sum of the supervision loss and the regularization loss
  - Quick check question: If validation loss decreases but cluster coherence (Silhouette score) increases dramatically, what does this imply about the relationship between the two loss terms?

## Architecture Onboarding

- **Component map:** Input Layer -> Feature Embedding (W^f + ψ) -> Grouping Engine (U + Clustering) -> Group Embedding (split/M + φ_k + aggregate) -> Sequence Model

- **Critical path:** The loop between Feature Embedding and the Grouping Engine. If clustering is not updated periodically or if EMA is not applied, centroids drift, causing the membership matrix M to "chatter," which prevents Group Embedding layers from converging.

- **Design tradeoffs:**
  - Initialization: k-means++ (flexible, potentially unstable) vs. Expert-defined (stable, potentially biased)
  - Unification Function: bias only (fast, loses magnitude info) vs. sum/avg (captures magnitude, slower)
  - Number of Groups (K): Fixed hyperparameter. The model can "disable" groups by making them empty, but cannot create new ones dynamically.

- **Failure signatures:**
  - Collapse to Single Cluster: All features assigned to one group (seen in MIMIC decompensation)
  - Empty Clusters: High threshold Δ or bad centroid initialization
  - Oscillating Loss: EMA decay α is too low, or clustering frequency P is too high relative to learning rate

- **First 3 experiments:**
  1. Synthetic Validation: Replicate the Gaussian Process experiment to verify Dynamic clustering recovers ground truth better than Static aggregation
  2. Ablation on Stability: Train on HiRID mortality with and without the $L_{reg}$ term, plotting ARI over training epochs
  3. Initialization Sensitivity: Compare k-means++ vs. Expert (Organ) initialization on MIMIC-III to see if initialization dictates final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the number of feature clusters be determined automatically during training rather than being fixed as a hyperparameter?
- Basis in paper: The authors state that "automatic discovery of the optimal number of clusters remains an open direction for future research."
- Why unresolved: The current method relies on fixed $K$ values or simple heuristics, limiting autonomous adaptation to intrinsic dimensionality
- Evidence to resolve: Integrating non-parametric clustering techniques into the loss function and demonstrating convergence on optimal $K$ without manual tuning

### Open Question 2
- Question: Do centroid-less clustering methods (e.g., DBSCAN or Gumbel-Softmax) offer improved stability or performance over centroid-based approaches?
- Basis in paper: The authors note, "We did not explore centroid-less methods... We leave the investigation of these methods to future work."
- Why unresolved: The paper relies on centroid-based regularization to ensure stability; it is unknown if methods without centroids could handle dynamic weight updates more effectively
- Evidence to resolve: Benchmarking density-based or neural clustering algorithms within the proposed architecture and comparing training stability and downstream AUPRC against K-means and GMM baselines

### Open Question 3
- Question: How do learned feature groups change or remain consistent when models are transferred across different hospital environments (distribution shifts)?
- Basis in paper: The discussion suggests the model "could potentially reveal how feature groups vary across institutions, thereby providing insights into distribution shifts."
- Why unresolved: Current experiments are limited to in-distribution evaluation on specific datasets
- Evidence to resolve: Training on a source hospital dataset and analyzing similarity of resulting feature clusters when applied to a target hospital dataset with different characteristics

## Limitations
- The method assumes embedding weights provide meaningful semantic representations, but this relationship is empirical rather than theoretically guaranteed
- Multiple interdependent hyperparameters (λ, α, K, P, U) require careful tuning with no systematic guidelines for new domains
- Dynamic clustering adds significant computational overhead that is not quantified against potential gains
- Validation is limited to ICU datasets, leaving generalizability to other clinical domains untested

## Confidence

**High Confidence:** The method successfully recovers ground-truth feature groups on synthetic data, and learned groupings achieve performance comparable to expert-defined groups on ICU datasets. The integration of clustering into standard deep learning pipelines is technically sound.

**Medium Confidence:** Clinical interpretability of learned groups is demonstrated through case studies, but systematic validation across diverse clinical scenarios is limited. The claim that learned groups "offer insights into task-relevant feature interactions" is supported by examples but not rigorously quantified.

**Low Confidence:** The paper does not establish when the method will fail or under what conditions expert-defined groupings would be preferable. Sensitivity to initialization, regularization strength, and other hyperparameters is not systematically explored.

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary λ (0.0001-0.01), α (0.1-0.95), and K (3-10) on HiRID mortality to map the hyperparameter landscape and identify regions of stable performance.

2. **Cross-Dataset Generalization:** Apply the method to a non-ICU clinical dataset (e.g., cancer trajectories, longitudinal EHR) to test whether the approach generalizes beyond intensive care settings.

3. **Failure Mode Characterization:** Deliberately construct scenarios where feature grouping should fail (e.g., tasks requiring individual feature attention, highly correlated features) and document the method's behavior, including cluster collapse and performance degradation patterns.