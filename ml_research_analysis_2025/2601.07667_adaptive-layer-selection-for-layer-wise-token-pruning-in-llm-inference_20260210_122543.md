---
ver: rpa2
title: Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference
arxiv_id: '2601.07667'
source_url: https://arxiv.org/abs/2601.07667
tags:
- layer
- selection
- tokens
- cache
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASL (Adaptive Selection Layer), a training-free
  method that dynamically determines the token selection layer in layer-wise token
  pruning for LLM inference. By monitoring the variance of token ranks ordered by
  attention scores across consecutive layers, ASL adaptively identifies when attention
  consistently focuses on a stable subset of tokens, enabling task-aware KV cache
  reduction.
---

# Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference

## Quick Facts
- **arXiv ID:** 2601.07667
- **Source URL:** https://arxiv.org/abs/2601.07667
- **Reference count:** 40
- **Primary result:** ASL achieves 66.4% accuracy on RULER with 128k context vs. FastKV's 59.1%, and perfect retrieval scores on NIAH across all context lengths.

## Executive Summary
This paper introduces ASL (Adaptive Selection Layer), a training-free method that dynamically determines the token selection layer in layer-wise token pruning for LLM inference. By monitoring the variance of token ranks ordered by attention scores across consecutive layers, ASL adaptively identifies when attention consistently focuses on a stable subset of tokens, enabling task-aware KV cache reduction. Evaluated on InfiniteBench, RULER, and NIAH benchmarks with up to 256k context lengths, ASL outperforms state-of-the-art methods like FastKV and GemFilter in accuracy while maintaining comparable decoding speed and KV cache reduction.

## Method Summary
ASL is a training-free approach that monitors attention score variance to adaptively select the pruning layer during prefilling. The method computes pooled attention scores, ranks tokens, and tracks variance across an observation window. When relative variance drops below a threshold, it performs one-shot top-k selection and propagates only those tokens to deeper layers. The approach introduces negligible memory overhead and can be integrated with existing KV cache reduction techniques like SnapKV.

## Key Results
- On RULER with 128k context, ASL achieves 66.4% accuracy versus FastKV's 59.1%
- On NIAH, ASL delivers perfect retrieval scores across all context lengths (1k-256k)
- ASL maintains comparable decoding speed to FastKV while improving accuracy by 5.9-7.3% on average across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Detection of Attention Stabilization via Rank Variance
Monitoring the variance of token ranks identifies when the model's attention has consistently focused on a stable subset of relevant tokens. The paper assumes that high rank stability correlates with the model having successfully identified the semantic information required for the task.

### Mechanism 2: Task-Aware Normalization (Relative Variance)
Normalizing variance against an initial baseline allows a single threshold to generalize across tasks of varying difficulty. The paper assumes that attention stabilization dynamics follow similar relative trajectories across different tasks.

### Mechanism 3: One-Shot Propagation for Prefilling Efficiency
Performing selection once at the adaptively determined layer maximizes TTFT reduction while preserving accuracy. The paper assumes that once attention focuses on a subset, those tokens remain relevant for all subsequent layers.

## Foundational Learning

- **Concept: KV Cache & Memory Bottleneck**
  - Why needed here: The primary motivation is reducing the memory footprint of the KV cache during inference.
  - Quick check: Does the KV cache store weights or activations from previous tokens?

- **Concept: Prefilling vs. Decoding Stages**
  - Why needed here: ASL operates specifically during the prefilling stage to determine the selection layer.
  - Quick check: In which stage does the model process the entire input prompt at once to initialize the cache?

- **Concept: Top-k Token Selection**
  - Why needed here: ASL uses standard top-k selection once the adaptive layer is found.
  - Quick check: If k=2048 and context is 128k, what percentage of tokens are retained?

## Architecture Onboarding

- **Component map:** Attention Monitor -> Pooler -> Rank Cache -> Variance Calculator -> Decision Engine

- **Critical path:**
  1. Pass input through layers 0 to L_min (no monitoring)
  2. At layer l ≥ L_min, compute scores, pool, rank, and cache
  3. If l ≥ L_min + L_obs, calculate relative variance
  4. If variance < τ, designate l as selection layer, prune to top-k, and disable monitoring
  5. Continue prefilling with pruned KV cache

- **Design tradeoffs:**
  - Threshold τ: Lower values wait for higher stability (better accuracy, slower TTFT)
  - Window L_obs: Larger windows smooth noise but delay selection availability

- **Failure signatures:**
  - Accuracy collapse on Retrieval: τ set too high, pruning before distinguishing needles from haystack
  - High TTFT: τ set too low, forcing selection near final layers
  - OOM Errors: Integration with SnapKV failed, or KV budget > available memory

- **First 3 experiments:**
  1. Threshold Sweep on RULER (128k) with τ ∈ {0.2, 0.3, 0.4, 0.5}
  2. Ablation on Observation Window: L_obs=4 vs L_obs=8
  3. Hard Task Verification: Compare "KV Retrieval" vs "QA" tasks to confirm deeper adaptive layer for Retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ASL be effectively integrated into progressive or multi-shot token pruning methods?
- Basis in paper: The authors state that methods like LazyLLM and OmniKV select tokens multiple times, and integration is yet to be investigated.
- Why unresolved: ASL relies on one-shot selection while progressive methods require dynamic decisions at multiple stages.
- What evidence would resolve it: A modified ASL algorithm using decay factors or multiple thresholds maintaining accuracy while achieving memory reduction in progressive pruning.

### Open Question 2
- Question: Does the relative variance metric generalize effectively across a wider variety of model architectures?
- Basis in paper: Only two LLMs (Llama 3.1-8B-UL and Qwen2.5-7B) have been evaluated.
- Why unresolved: Different architectures may exhibit distinct attention patterns, potentially making variance a less reliable signal.
- What evidence would resolve it: Evaluation on diverse model families showing consistent accuracy-speed trade-offs.

### Open Question 3
- Question: Can the relative variance threshold τ be dynamically optimized rather than statically user-specified?
- Basis in paper: Table 9 demonstrates significant accuracy and TTFT variation with different τ values.
- Why unresolved: A static threshold may not be optimal for all tasks; an adaptive threshold could maximize efficiency.
- What evidence would resolve it: An automated mechanism for setting τ matching or exceeding manual tuning performance.

## Limitations
- The relative variance threshold τ=0.3 may not generalize across diverse LLM architectures
- The one-shot pruning assumption could break down in scenarios involving context shifting
- Memory overhead claims lack quantitative measurements across different contexts and model sizes

## Confidence

**High Confidence:**
- Variance monitoring effectively identifies attention stabilization
- ASL outperforms fixed-layer selection methods on tested benchmarks
- Method introduces negligible computational overhead during inference
- Integration with existing KV cache reduction techniques is feasible

**Medium Confidence:**
- Adaptive layer selection provides consistent improvements across all task types
- Relative variance normalization generalizes across tasks of varying difficulty
- One-shot pruning preserves accuracy compared to progressive methods
- Default hyperparameter settings are optimal across architectures

**Low Confidence:**
- Performance guarantees hold for models significantly outside the 7B-8B parameter range
- Method scales efficiently to contexts beyond 256k tokens
- Approach remains effective when integrated with multiple simultaneous optimization techniques

## Next Checks

1. **Cross-Architecture Generalization Test:** Implement ASL on diverse architectures (GPT-style, Mamba, Hyena) to measure whether τ=0.3 requires adjustment and quantify performance degradation on non-Transformer architectures.

2. **Edge Case Robustness Analysis:** Design adversarial test cases targeting the one-shot pruning assumption, including multi-hop reasoning tasks and complex retrieval with multiple needles requiring different attention patterns at different depths.

3. **Cumulative Overhead Quantification:** Implement ASL integrated with SnapKV and measure actual memory usage and computation time across varying contexts (4k-256k), KV budgets (512-8192), and model sizes (1B-70B), generating pareto curves showing accuracy vs. resource usage tradeoffs.