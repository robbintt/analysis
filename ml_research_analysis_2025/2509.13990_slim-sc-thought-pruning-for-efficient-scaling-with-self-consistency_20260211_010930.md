---
ver: rpa2
title: 'Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency'
arxiv_id: '2509.13990'
source_url: https://arxiv.org/abs/2509.13990
tags:
- chains
- slim-sc
- pruning
- reasoning
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost and latency of
  Self-Consistency (SC), a test-time scaling method that improves LLM reasoning accuracy
  by generating and voting among multiple reasoning chains. The authors propose Slim-SC,
  a step-wise thought pruning strategy that removes redundant chains based on inter-chain
  similarity at the thought level, targeting chains that exhibit highly similar intermediate
  reasoning.
---

# Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency

## Quick Facts
- arXiv ID: 2509.13990
- Source URL: https://arxiv.org/abs/2509.13990
- Reference count: 40
- Reduces inference latency by up to 45% while maintaining accuracy in LLM reasoning

## Executive Summary
Slim-SC addresses the high computational cost of Self-Consistency (SC) by introducing a step-wise thought pruning strategy that removes redundant reasoning chains based on inter-chain similarity. The method targets chains with highly similar intermediate reasoning steps while preserving diversity through a warm-up delay mechanism. Experiments demonstrate Slim-SC achieves significant latency reductions (up to 45%) and KV cache savings (up to 26%) across three STEM reasoning datasets using two different LLM architectures, all while maintaining or improving reasoning accuracy compared to standard SC.

## Method Summary
Slim-SC implements a thought-level pruning strategy for Self-Consistency that removes redundant reasoning chains during generation. The method uses cosine similarity between intermediate thoughts to identify chains that follow similar reasoning paths. A warm-up delay (k=3 steps) prevents premature pruning of chains that may diverge later. Two pruning rules are offered: random selection and diversity-based selection, where the latter aims to preserve chains with higher similarity to the final answer. The approach operates on individual thought steps rather than entire chains, enabling more granular pruning decisions while maintaining reasoning diversity.

## Key Results
- Reduces inference latency by up to 45% compared to standard Self-Consistency
- Decreases KV cache usage by up to 26% without sacrificing accuracy
- Maintains or improves reasoning accuracy across three STEM datasets and two LLM architectures

## Why This Works (Mechanism)
Slim-SC leverages the observation that many reasoning chains in Self-Consistency converge to similar intermediate steps, indicating redundancy. By pruning these similar chains early based on thought-level similarity, the method eliminates computational waste while preserving diverse reasoning paths that could lead to correct answers. The warm-up delay ensures chains have enough time to potentially diverge before pruning decisions are made, preventing the removal of chains that might take different paths later in reasoning.

## Foundational Learning

1. **Self-Consistency (SC)**: A test-time scaling method that generates multiple reasoning chains and aggregates them through voting to improve accuracy.
   - Why needed: SC's effectiveness comes from aggregating diverse reasoning paths, but it's computationally expensive.
   - Quick check: Does your method generate and vote among multiple reasoning chains?

2. **Cosine Similarity for Thought Pruning**: Uses vector similarity between intermediate reasoning steps to identify redundant chains.
   - Why needed: Provides a quantitative measure to determine when chains are following similar reasoning paths.
   - Quick check: Are you measuring similarity at the thought/step level rather than chain level?

3. **Warm-up Delay Strategy**: Prevents pruning in early reasoning steps to allow chains time to potentially diverge.
   - Why needed: Early pruning could remove chains that take different paths later in reasoning.
   - Quick check: Do you have a mechanism to delay pruning decisions until chains have developed?

## Architecture Onboarding

**Component Map**: LLM -> Thought Generator -> Similarity Calculator -> Pruner -> Answer Aggregator

**Critical Path**: The core inference loop where thoughts are generated, similarity is calculated, pruning decisions are made, and final answers are aggregated. The warm-up delay (k=3) is critical for preventing premature pruning.

**Design Tradeoffs**: 
- Earlier pruning saves more computation but risks removing valuable divergent chains
- More aggressive similarity thresholds save more computation but may eliminate useful diversity
- Random pruning is simpler but diversity-based pruning aims for better quality preservation

**Failure Signatures**: 
- Accuracy drops when pruning is too aggressive or similarity thresholds too low
- No latency improvement when pruning rarely triggers or warm-up is too long
- High variance in diversity-based pruning suggests unreliable diversity preservation

**First Experiments**:
1. Run baseline SC and measure latency, KV cache usage, and accuracy on a small STEM dataset
2. Implement Slim-SC with random pruning rule and test across different warm-up delay values (k=1, 2, 3, 4)
3. Compare Slim-SC with diversity-based pruning against random pruning on accuracy and computational metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Depends on cosine similarity which may not capture semantic equivalence or reasoning quality
- Fixed warm-up delay parameter (k=3) may not generalize optimally across different problem domains
- Experiments limited to STEM reasoning tasks, limiting generalizability to other domains

## Confidence
- High Confidence: Reduces latency by 45% and KV cache usage by 26% compared to standard SC
- Medium Confidence: Maintains or improves accuracy while reducing computational cost (improvements are marginal in some cases)
- Medium Confidence: Effectiveness in preserving reasoning diversity based on indirect intra-chain similarity metrics

## Next Checks
1. Evaluate Slim-SC on non-STEM reasoning tasks (commonsense reasoning, multi-hop inference) to assess generalizability
2. Conduct ablation studies varying the warm-up delay parameter k across different problem complexities
3. Compare Slim-SC's pruning decisions against human judgment to validate whether removed chains were truly redundant