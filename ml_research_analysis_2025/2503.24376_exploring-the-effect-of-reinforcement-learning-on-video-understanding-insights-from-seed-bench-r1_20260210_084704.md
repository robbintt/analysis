---
ver: rpa2
title: 'Exploring the Effect of Reinforcement Learning on Video Understanding: Insights
  from SEED-Bench-R1'
arxiv_id: '2503.24376'
source_url: https://arxiv.org/abs/2503.24376
tags:
- video
- reasoning
- visual
- answer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEED-Bench-R1 evaluates post-training methods for multimodal large
  language models on video understanding tasks requiring both perception and reasoning.
  It introduces a three-level validation hierarchy (in-distribution, cross-environment,
  cross-environment-task) using real-world egocentric videos and a large-scale training
  dataset with verifiable ground-truth answers.
---

# Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1

## Quick Facts
- **arXiv ID**: 2503.24376
- **Source URL**: https://arxiv.org/abs/2503.24376
- **Reference count**: 8
- **Primary result**: RL outperforms SFT in video understanding, particularly for out-of-distribution generalization

## Executive Summary
SEED-Bench-R1 evaluates post-training methods for multimodal large language models on video understanding tasks requiring both perception and reasoning. It introduces a three-level validation hierarchy (in-distribution, cross-environment, cross-environment-task) using real-world egocentric videos and a large-scale training dataset with verifiable ground-truth answers. Experiments with Qwen2-VL-Instruct-7B show reinforcement learning via GRPO outperforms supervised fine-tuning in data efficiency and generalization, particularly in out-of-distribution scenarios. RL improves visual attention and encourages dynamic querying of visual inputs through chain-of-thought tokens, though challenges remain in logical coherence and overlooking visual cues.

## Method Summary
The study evaluates RL versus SFT for video understanding using SEED-Bench-R1 benchmark. Training uses Qwen2-VL-Instruct-7B with 6k samples from 50k available. SFT distills COT from larger Qwen2.5-VL models via rejection sampling. GRPO uses outcome-based rewards (correct/incorrect) with group-relative advantage estimation. Videos are processed as max 16 frames at 252×252 plus current observation frame. Evaluation spans 3 levels: same environment (L1), new environment (L2), new environment and task domain (L3), plus LongVideoBench.

## Key Results
- RL shows superior generalization on out-of-distribution tasks (L2/L3) compared to SFT
- RL enhances visual attention and enables dynamic querying of visual inputs through COT tokens
- Outcome-only rewards lead to data efficiency but can produce illogical reasoning chains
- Current observation frame is more critical than task progress frames for reasoning
- Attention visualizations show RL focuses more on observation frames versus SFT's broader scanning

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Visual Querying via COT Tokens
- Claim: RL trains COT tokens to function as adaptive queries over visual inputs, rather than memorized reasoning templates.
- Mechanism: Under outcome-based rewards, the policy learns to generate COT tokens whose attention patterns dynamically scan task-relevant visual regions. This contrasts with SFT, which tends to produce superficial reasoning patterns weakly grounded in visual content.
- Core assumption: The reward signal sufficiently incentivizes visual grounding; the base model has enough capacity to learn adaptive attention.
- Evidence anchors: [abstract] "RL enhances visual perception and encourages dynamic querying of visual inputs through chain-of-thought generation"; [section 5.2] "RL teaches the model to search more effectively in the visual space with the additional COT tokens, whereas SFT often forces memorization of reasoning patterns"
- Break condition: When training data contains noisy labels allowing reward hacking without genuine visual grounding.

### Mechanism 2: Group-Relative Advantage Estimation
- Claim: Normalizing rewards within groups of sampled responses provides stable policy gradients without a learned value function.
- Mechanism: For each question, G responses are sampled. Rewards are computed (1 if answer matches ground-truth, else 0), then normalized across the group. All tokens in a response receive the same advantage, simplifying credit assignment.
- Core assumption: Response-level rewards correlate with token-level quality; group diversity is sufficient for meaningful relative comparisons.
- Evidence anchors: [section 4.2] "GRPO optimizes memory usage by eliminating the need for additional value function approximation... assessing their relative quality based on verifiable rewards"
- Break condition: When all sampled responses receive identical rewards (zero variance).

### Mechanism 3: Verifiable Outcome Rewards from Ground-Truth Actions
- Claim: Automatically extracting ground-truth answers from subsequent video frames enables scalable reward computation.
- Mechanism: Questions are constructed from egocentric videos where the "correct" next action is the action actually taken immediately after the observation frame. Negative options are sampled from the same video, requiring genuine understanding rather than semantic matching.
- Core assumption: The observed next action is uniquely correct; task goals are unambiguous.
- Evidence anchors: [section 3] "The ground-truth answer comes from the actual next action occurring right after the current observation in the original uncropped video"
- Break condition: When multiple actions could be valid, or when the "correct" answer in training data is ambiguous.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core algorithm; differs from PPO by eliminating the critic network via group-based advantage estimation.
  - Quick check question: Why does GRPO sample multiple responses per question rather than using a single response with a value function baseline?

- **Concept: Outcome Supervision vs. Process Supervision**
  - Why needed here: Paper uses outcome-only rewards; process supervision is identified as a key future direction.
  - Quick check question: What tradeoff does outcome supervision introduce between implementation simplicity and reasoning transparency?

- **Concept: Egocentric Video Temporal Reasoning**
  - Why needed here: Tasks require tracking task progress over long horizons from first-person perspective.
  - Quick check question: How does the benchmark structure the relationship between "task progress" frames and "current observation" in model inputs?

## Architecture Onboarding

- **Component map:** Input frames (16 task progress + 1 observation) at 252×252 → Prompt template with COT guidance → Qwen2-VL-Instruct-7B → G-response sampling → Answer extraction via regex → Binary reward assignment → Group normalization → Policy gradient update with KL penalty

- **Critical path:**
  1. Frame sampling from video → 2. Prompt construction → 3. G-response sampling → 4. Answer extraction via regex on <answer> tags → 5. Binary reward assignment → 6. Group normalization → 7. Policy gradient update with KL penalty

- **Design tradeoffs:**
  - Frame count/resolution vs. context window (currently 16 frames at 252×252; limits fine-grained perception)
  - Training data scale vs. quality (50k available, 6k used for preliminary study; automatic construction introduces noise)
  - Outcome-only vs. process rewards (simpler but allows reward hacking)

- **Failure signatures:**
  - Correct answer with illogical reasoning chain (semantic inconsistency between COT and final output)
  - Missed visual cues due to coarse frame sampling or low resolution
  - Templated reasoning patterns ("The person has already...") that don't match visual observations
  - OOD performance degradation despite strong in-distribution results

- **First 3 experiments:**
  1. **Baseline replication:** Train Qwen2-VL-7B with SFT (COT-distilled data) vs. GRPO (outcome-only) on 6k samples; evaluate on L1/L2/L3 and LongVideoBench to confirm generalization gap.
  2. **Attention visualization:** Generate attention maps from COT tokens to visual tokens across SFT and GRPO models on L2/L3 questions; quantify attention coverage and grounding accuracy.
  3. **Noise robustness test:** Corrupt 10-20% of training labels with wrong answers; measure performance delta between GRPO and SFT to assess reward signal sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can process-based reward supervision improve logical coherence in RL-trained MLLMs while maintaining or enhancing visual perception capabilities?
- Basis in paper: [explicit] The authors state: "Process-based rewards could explicitly supervise reasoning rationality, preventing shortcuts" and identify that "the MLLM post-trained with RL using outcome supervision rewards often produces inconsistent or illogical reasoning chains."
- Why unresolved: The current study only uses simple outcome-based rewards (correct/incorrect) without intermediate supervision of reasoning steps, leading to correct answers through potentially flawed reasoning.
- What evidence would resolve it: Comparison of models trained with outcome-based vs. process-based rewards on SEED-Bench-R1, measuring both answer accuracy and reasoning chain coherence through human evaluation or automated metrics.

### Open Question 2
- Question: How does pre-RL fine-tuning on high-quality chain-of-thought demonstrations affect the efficiency and final performance of reinforcement learning for video understanding?
- Basis in paper: [explicit] The authors propose: "Future work could investigate efficient data curation methods to collect high-quality COT demonstrations that showcase advanced reasoning skills—such as problem decomposition, reflection, and self-correction. Fine-tuning the model on such data as a cold start could streamline and enhance subsequent RL training."
- Why unresolved: The current study did not implement any cold-start fine-tuning before RL, and the base model showed limited emergent reasoning abilities (completion length did not increase notably during RL training).
- What evidence would resolve it: Ablation study comparing RL training with and without prior COT demonstration fine-tuning, measuring training efficiency, final performance across SEED-Bench-R1 levels, and reasoning chain quality.

### Open Question 3
- Question: Can RL algorithms be made robust enough to effectively leverage the full 50k training samples in SEED-Bench-R1 despite inherent noise in automatically constructed ground-truth answers?
- Basis in paper: [explicit] The authors note: "The training data provided by SEED-Bench-R1 are automatically constructed without manually verifying the uniqueness of the golden answers. Consequently, the occasional data noise makes the model confused" and "we have currently only used a small proportion of training data (6k out of 50k). Enhancing RL algorithms' robustness against noisy reward signals is vital for scaling."
- Why unresolved: Only 12% of available training data was used, suggesting current methods may not handle noise effectively at scale.
- What evidence would resolve it: Experiments training on progressively larger subsets of SEED-Bench-R1 training data with improved noise-robust RL algorithms, measuring performance scaling and error analysis on noisy samples.

## Limitations
- Outcome-only rewards can produce correct answers through illogical reasoning chains
- Automatic data construction introduces noise when multiple valid actions exist
- Frame sampling limitations (16 frames at 252×252) constrain fine-grained visual perception
- Only 12% of available training data used due to potential noise sensitivity

## Confidence

- **RL Generalization Advantage (High):** Multiple evaluation levels and LongVideoBench validation provide robust evidence
- **Dynamic Visual Querying Mechanism (Medium):** Claims rely on qualitative attention visualization rather than quantitative metrics
- **Outcome-Only Reward Sufficiency (Low):** Authors acknowledge this as a limitation with evidence of illogical reasoning chains

## Next Checks
1. **Quantitative Attention Grounding Analysis**: Implement automated metrics to measure the alignment between COT token attention patterns and task-relevant visual regions across validation sets, comparing SFT and RL models to quantify the claimed dynamic querying advantage.

2. **Ablation on Reward Signal Quality**: Systematically introduce controlled noise into training labels (10-30% corruption) and measure the relative performance degradation of GRPO versus SFT to assess the robustness of outcome-only rewards versus process supervision.

3. **Resolution and Frame Count Sensitivity**: Conduct controlled experiments varying frame resolution (128×128 to 384×384) and frame count (8 to 32) to determine the perceptual resolution threshold where performance plateaus or where additional frames cease providing marginal benefit.