---
ver: rpa2
title: 'Streaming Tensor Programs: A Streaming Abstraction for Dynamic Parallelism'
arxiv_id: '2511.07776'
source_url: https://arxiv.org/abs/2511.07776
tags:
- dynamic
- stream
- step
- memory
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Streaming Tensor Programs (STeP) is a new abstraction for efficiently
  running dynamic tensor workloads on spatial dataflow accelerators (SDAs). It introduces
  flexible routing operators, explicit memory hierarchy, and symbolic-shape semantics
  to enable optimizations like dynamic tiling, dynamic parallelization, and configuration
  time-multiplexing that adapt execution to dynamic behaviors while preserving dataflow
  efficiency.
---

# Streaming Tensor Programs: A Streaming Abstraction for Dynamic Parallelism

## Quick Facts
- **arXiv ID:** 2511.07776
- **Source URL:** https://arxiv.org/abs/2511.07776
- **Reference count:** 40
- **Primary result:** STeP achieves ~2.72x latency improvement and ~2.64x utilization gain over prior SDA abstractions through dynamic tiling, dynamic parallelization, and configuration time-multiplexing.

## Executive Summary
Streaming Tensor Programs (STeP) introduces a new abstraction for spatial dataflow accelerators (SDAs) that efficiently handles dynamic tensor workloads like Mixture-of-Experts transformers. Unlike prior SDA abstractions that rely on static tiling and implicit memory hierarchies, STeP exposes explicit memory management and symbolic-shape semantics that enable runtime-adaptive optimizations. The system demonstrates that SDAs can efficiently execute dynamic tensor programs through three key mechanisms: adaptive tiling that matches workload distribution, explicit memory hierarchy management, and flexible routing operators that enable data-dependent control flow.

## Method Summary
The evaluation uses a cycle-approximate simulator implemented in Rust with the Dataflow Abstract Machine framework. Experiments run on a simulated SDA with 64 bytes/cycle on-chip bandwidth, 1024 bytes/cycle off-chip bandwidth, and compute units operating on 16Ã—16 BFloat16 tiles with initiation interval of 1. The study uses AzureLLMInference dataset for KV cache lengths and expert routing traces from Qwen3-30B-A3B and Mixtral-8x7B models. Dynamic tiling, dynamic parallelization, and configuration time-multiplexing are evaluated both in isolation and for full-model workloads. Results are validated against HDL simulations for specific layers.

## Key Results
- Dynamic tiling breaks the Pareto-optimal frontier from prior work by adapting tile sizes to runtime workload distribution
- Dynamic parallelization improves latency by ~2.72x over static implementations
- Configuration time-multiplexing increases compute utilization by ~2.64x by sharing compute units across experts
- STeP achieves these improvements while preserving dataflow efficiency and avoiding pipeline stalls

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Tiling via Symbolic Shapes
STeP's symbolic-shape semantics allow tile dimensions to be dynamic-regular or ragged, enabling tiles to adapt to runtime data distribution rather than padding to static maximum sizes. This reduces memory waste and compute overhead compared to static padding strategies.

### Mechanism 2: Explicit Memory Hierarchy for Dataflow Efficiency
By decoupling memory operations (Bufferize, Streamify) from compute, STeP exposes on-chip memory requirements and off-chip traffic as symbolic expressions. This enables precise memory allocation and minimizes the primary bottleneck for memory-bound workloads.

### Mechanism 3: Data-Dependent Control Flow via Routing Operators
Multi-hot routing operators (Reassemble, Partition) support data-dependent control flow like Mixture-of-Experts routing without stalling the pipeline or requiring global synchronization, enabling asynchronous execution of dynamic workloads.

## Foundational Learning

- **Spatial Dataflow Architectures (SDAs)**: SDAs use reconfigurable arrays of compute/memory with explicit data movement rather than instruction fetch/decode. Why needed: Understanding this distinction explains why STeP's explicit memory management is critical for performance.

- **Asynchronous Dataflow Models**: STeP uses asynchronous execution where blocks operate without global synchronization. Why needed: This enables handling dynamic behaviors without pipeline stalls.

- **Tiling and Operational Intensity**: Tiling is the primary optimization knob in STeP, balancing tile size, on-chip memory usage, and off-chip bandwidth. Why needed: Understanding this trade-off is essential for interpreting the performance results.

## Architecture Onboarding

- **Component map**: Frontend (Symbolic Python interface) -> Operators (Memory: Bufferize, Streamify; Routing: Partition, Reassemble; Compute: Map, Accum) -> Simulator (Rust-based cycle-approximate)

- **Critical path**: 1) Define workload using STeP operators, 2) Apply schedule (tiling, parallelization), 3) Run symbolic frontend for shape consistency, 4) Execute simulator for performance metrics

- **Design tradeoffs**: Static vs. Dynamic Tiling (ease of compilation vs. memory efficiency), Time-Multiplexing (resource savings vs. scheduling overhead), Explicit vs. Implicit memory hierarchy (programmer control vs. automation)

- **Failure signatures**: Shape Mismatch errors in symbolic frontend, Memory Overflow in simulator, Deadlocks indicated by cycles increasing but throughput at zero

- **First 3 experiments**: 1) Reproduce SwiGLU layer validation against HDL results, 2) Implement Matrix-Vector multiplication with static vs. dynamic tiling sweep, 3) Test Simplified MoE example with Partition and Reassemble routing

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to cycle-approximate simulator rather than actual hardware
- Three optimizations are evaluated in isolation rather than in combination
- Study focuses exclusively on transformer-based workloads, limiting generalizability

## Confidence
- **High Confidence**: Core abstractions and mechanisms are well-defined and internally consistent
- **Medium Confidence**: Quantitative results are credible based on simulator but need hardware validation
- **Low Confidence**: Generalization beyond transformer workloads and overhead analysis for dynamic tile metadata

## Next Checks
1. **Hardware Validation**: Implement STeP on FPGA or fabricated SDA prototype to validate simulator results, particularly for dynamic tiling and routing mechanisms
2. **Combined Optimization Study**: Run experiments applying all three optimizations simultaneously to measure interaction effects and verify compounded benefits
3. **Generalization Test**: Apply STeP to non-transformer workloads such as graph neural networks or dynamic programming problems to evaluate broader applicability