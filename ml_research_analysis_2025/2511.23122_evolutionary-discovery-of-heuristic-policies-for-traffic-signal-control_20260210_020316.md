---
ver: rpa2
title: Evolutionary Discovery of Heuristic Policies for Traffic Signal Control
arxiv_id: '2511.23122'
source_url: https://arxiv.org/abs/2511.23122
tags:
- traffic
- signal
- control
- policy
- tpet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TPET, a framework that uses LLMs as an evolution
  engine to discover specialized heuristic policies for traffic signal control. The
  core idea is to leverage LLMs not as online actors but as discoverers of structured,
  lightweight policies through an evolutionary process guided by two key modules:
  Structured State Abstraction (SSA), which converts numerical traffic data into interpretable
  temporal-logical facts, and Credit Assignment Feedback (CAF), which provides actionable
  critiques by tracing micro-decisions to macro-outcomes.'
---

# Evolutionary Discovery of Heuristic Policies for Traffic Signal Control

## Quick Facts
- **arXiv ID**: 2511.23122
- **Source URL**: https://arxiv.org/abs/2511.23122
- **Reference count**: 24
- **Primary result**: TPET uses LLMs as evolution engine to discover specialized heuristic policies, achieving state-of-the-art traffic signal control performance on real-world datasets.

## Executive Summary
This paper introduces TPET, a framework that leverages large language models as evolution engines to discover specialized heuristic policies for traffic signal control. The key innovation is using LLMs offline to evolve lightweight, interpretable policies rather than deploying LLMs as online actors. Through a structured process involving state abstraction and credit assignment feedback, TPET discovers policies that run with millisecond latency and outperform both classic heuristics and online LLM actors on real-world traffic datasets. The approach demonstrates exceptional stability and robustness while maintaining computational efficiency.

## Method Summary
TPET operates through an evolutionary loop where an LLM generates policy candidates, which are evaluated in simulation. The framework converts numerical traffic states into interpretable temporal-logical predicates via Structured State Abstraction (SSA), then provides actionable critiques through Credit Assignment Feedback (CAF) by tracing micro-decisions to macro-outcomes. The LLM uses these critiques to generate improved policy variants over multiple iterations. The final output is a simple heuristic function optimized for specific traffic environments, deployable with negligible inference cost compared to online LLM actors.

## Key Results
- Achieves average wait time of 36.80 seconds on Jinan-1 dataset versus 69.19 seconds for FixedTime and 54.94 seconds for LLMLight
- Demonstrates exceptional stability with consistently tight variance, often comparable to deterministic methods like MaxPressure
- Outperforms both classic heuristics and online LLM actors on metrics including Average Travel Time, Average Queue Length, and Average Wait Time
- Policies run with millisecond latency suitable for real-time deployment

## Why This Works (Mechanism)

### Mechanism 1: Structured State Abstraction (SSA) as Semantic Bridge
- **Core claim**: Converting high-dimensional numerical traffic states into temporal-logical predicates enables LLMs to reason about control policies despite lacking direct numerical understanding
- **Mechanism**: SSA applies deterministic function f_SSA that aggregates instantaneous metrics, maintains persistent temporal metrics, then maps these onto qualitative vocabulary producing predicates like "Congestion: Critical" or "Starvation Risk: High"
- **Core assumption**: The selected predicates capture sufficient information for effective control decisions
- **Evidence anchors**: Ablation shows removing SSA degrades AWT from 36.80s to 43.80s on Jinan-1; structured facts generated by deterministic function
- **Break condition**: If predicate vocabulary is too coarse, policies cannot express necessary nuance; if too fine, search space explodes

### Mechanism 2: Credit Assignment Feedback (CAF) for Targeted Critique
- **Core claim**: Post-hoc back-tracing of simulation logs to identify defect patterns provides denser, more actionable feedback than sparse fitness scores alone
- **Mechanism**: CAF scans logs against Defect Pattern Library containing temporal-logical rules, aggregates defects, and generates structured critique that guides LLM mutations
- **Core assumption**: The defect pattern library covers primary failure modes
- **Evidence anchors**: Removing CAF causes larger degradation than removing SSA (AWT 53.26s vs 36.80s baseline); CAF replaces simple fitness score with structured critique
- **Break condition**: If defect patterns are mis-specified, critiques misguide evolution; if sequential dependencies exceed pattern expressiveness, root causes remain unidentified

### Mechanism 3: LLM as Offline Evolution Engine vs. Online Actor
- **Core claim**: Using LLMs to evolve heuristics offline produces specialized, millisecond-latency policies that outperform both generic online LLM actors and hand-crafted heuristics
- **Mechanism**: TPET runs evolutionary iterations where LLM generates policy variants, SSA-equipped policies are evaluated in simulation, CAF provides defect critiques, and top candidates are retained
- **Core assumption**: The evolved policy generalizes within traffic distribution seen during evolution
- **Evidence anchors**: TPET reduces AWT to 36.80s vs 54.94s for LLMLight; demonstrates exceptional stability; LLM-for-heuristic paradigm validated in neighboring work
- **Break condition**: If simulation environment poorly models real-world dynamics, evolved policies overfit to simulator artifacts

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) for Traffic Signal Control
  - **Why needed here**: Paper formulates each intersection as discrete-time MDP with state s_t, action a_t, and objectives; understanding MDP structure is prerequisite to grasping why credit assignment is hard
  - **Quick check question**: Can you explain why sparse rewards in sequential decision-making create a credit assignment problem?

- **Concept**: Evolutionary Algorithm Fundamentals (population, fitness, mutation, selection)
  - **Why needed here**: TPET builds on evolutionary paradigms from NeRM, generating 20 candidates per iteration and retaining top 3; without this background, MoP/MoA loop will seem opaque
  - **Quick check question**: How does fitness-based selection differ from gradient-based optimization, and what are the implications for search behavior?

- **Concept**: Predicate Logic and Temporal Reasoning
  - **Why needed here**: SSA generates temporal-logical predicates; CAF uses temporal-logical rules for defect detection; understanding how qualitative predicates abstract quantitative state is essential
  - **Quick check question**: What information is lost when mapping continuous pressure values to discrete predicates like "Congestion: High"?

## Architecture Onboarding

- **Component map**:
  - SSA Module: raw state s_t + history H_{t-1} → structured predicates Σ*_t
  - CAF Module: simulation log L → structured critique C_i
  - Evolution Engine (LLM): problem description + previous policy + CAF critique → new policy candidates
  - Evaluator (CityFlow): executes policies in simulation, returns fitness metrics
  - Selection Mechanism: retains top 3 of 20 candidates per iteration

- **Critical path**:
  1. Initialize with simple baseline policy
  2. Run policy in CityFlow simulator; SSA translates states at each step
  3. CAF analyzes full log, generates defect critique
  4. LLM receives critique + previous policy, generates mutated candidates
  5. Evaluate all candidates; select top performers
  6. Repeat for 20 iterations; deploy best policy

- **Design tradeoffs**:
  - Predicate granularity: coarse simplifies LLM reasoning but may lose critical information; fine increases expressiveness but enlarges search space
  - Defect pattern coverage: more patterns enable richer feedback but risk conflicting signals; fewer patterns are tractable but may miss failure modes
  - Population size vs. iterations: larger population explores more per generation but increases cost; more iterations allow deeper refinement but risk overfitting

- **Failure signatures**:
  - Policy overfitting to simulator: low variance in evaluation but poor real-world transfer
  - Stagnant evolution: fitness plateaus early (check CAF critique informativeness)
  - High variance across runs: LLM instability (increase candidates or lower temperature)
  - Predicate mismatch: policy ignores certain predicates (check SSA output variability)

- **First 3 experiments**:
  1. **Sanity check**: Run TPET on single intersection with FixedTime baseline; verify SSA produces varying predicates and CAF identifies at least one defect pattern
  2. **Ablation validation**: Run TPET, w/o SSA, w/o CAF on same dataset; confirm ablations degrade performance as in Table 2
  3. **Generalization test**: Evolve policy on Jinan-1, evaluate zero-shot on Jinan-2 and Hangzhou; measure performance gap vs. policies evolved separately

## Open Questions the Paper Calls Out

- **Open Question 1**: How effectively do the specialized heuristic policies discovered by TPET generalize to intersection topologies and traffic flow patterns not seen during the evolutionary process?
  - **Basis in paper**: [inferred] TPET yields policies "optimized for specific traffic environments," implying trade-off between specialization and generalization
  - **Why unresolved**: Experiments trained and tested on same datasets without evaluating transfer learning capabilities
  - **What evidence would resolve it**: Zero-shot evaluation where policy evolved on Jinan dataset is deployed directly on Hangzhou topology

- **Open Question 2**: Is the Credit Assignment Feedback (CAF) mechanism capable of correcting policy defects that do not match the manually defined formal rules in the Defect Pattern Library?
  - **Basis in paper**: [explicit] CAF relies on pre-defined logical rules to trace flaws
  - **Why unresolved**: If policy fails due to novel logic error not in library, CAF may fail to generate actionable critique
  - **What evidence would resolve it**: Introduce controlled logic error not triggering existing patterns and observe if LLM successfully identifies and fixes issue

- **Open Question 3**: Do the evolved heuristic policies retain their performance advantage over DRL baselines when deployed in real-world environments with sensor noise and stochastic driver behavior?
  - **Basis in paper**: [explicit] All experiments conducted within CityFlow simulation environment
  - **Why unresolved**: Simulators often fail to capture full physical world complexity; robustness to these gaps is untested
  - **What evidence would resolve it**: Comparative analysis of TPET policies versus DRL agents in high-fidelity hardware-in-the-loop simulation or physical testbed

## Limitations
- **Predicate design sensitivity**: SSA effectiveness hinges on chosen qualitative vocabulary; performance may be highly dependent on seemingly arbitrary selections
- **Defect pattern completeness**: CAF's ability to guide evolution depends on library covering space of failure modes; missing critical patterns may lead to locally optimal but globally flawed policies
- **Simulator-to-reality transfer**: All experiments evaluate policies within CityFlow simulation; paper does not address how evolved policies perform in real traffic networks with sensor noise and driver behavior unpredictability

## Confidence
- **High confidence**: Claims about TPET outperforming baseline heuristics (FixedTime, MaxPressure, LLMLight) on tested datasets; supported by direct experimental comparisons with clear metrics
- **Medium confidence**: Claims about specific mechanisms (SSA and CAF) driving improvements; ablation studies support contributions but exact contribution and interaction effects not fully isolated
- **Medium confidence**: Claims about exceptional stability and robustness; demonstrates low variance within simulation environments but does not test robustness to distribution shifts or real-world deployment conditions

## Next Checks
1. **Cross-distribution generalization test**: Evaluate TPET policies evolved on Jinan data on completely unseen traffic patterns (different city layouts, peak/off-peak ratios, incident scenarios); measure performance degradation and compare against policies evolved specifically for those conditions

2. **Real-world pilot deployment**: Partner with traffic management authority to deploy TPET-evolved policies at small intersection or corridor; compare against existing control systems across multiple days and traffic conditions, measuring both performance and computational overhead

3. **Mechanism isolation experiment**: Systematically vary SSA predicate granularity (2-level vs 5-level categorization of congestion) and CAF pattern library completeness; quantify marginal contribution of each design choice to final policy performance to establish sensitivity bounds