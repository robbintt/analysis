---
ver: rpa2
title: 'Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context'
arxiv_id: '2503.15338'
source_url: https://arxiv.org/abs/2503.15338
tags:
- audio
- speech
- arxiv
- solla
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SOLLA is a speech-oriented LLM designed to understand speech instructions
  while processing acoustic context. It integrates an audio tagging module for event
  identification and an ASR-assisted prediction method to improve speech comprehension.
---

# Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context

## Quick Facts
- arXiv ID: 2503.15338
- Source URL: https://arxiv.org/abs/2503.15338
- Reference count: 38
- Primary result: SOLLA outperforms baseline models on mixed speech-audio understanding tasks, especially in low-SNR, overlapping conditions.

## Executive Summary
SOLLA is a speech-oriented LLM designed to understand spoken instructions while processing acoustic context. It integrates an audio tagging module for event identification and an ASR-assisted prediction method to improve speech comprehension. The authors also introduce SA-Eval, a benchmark with three tasks (audio classification, captioning, QA) across two difficulty levels (easy/hard) to test mixed speech-audio understanding. Experiments show SOLLA performs on par with or outperforms baseline models, particularly in the hard mode with overlapping speech and low SNR audio, highlighting its robustness in challenging acoustic conditions. The ablation study confirms that both the AT module and ASR-assisted prediction contribute to improved performance.

## Method Summary
SOLLA processes a mixed audio signal (speech instruction + background audio) using a frozen Whisper-Large-v3 encoder with a trainable LoRA adaptor. The encoder's output is transformed into a sequence representation and a compact audio event vector via an audio tagging (AT) module. These are concatenated and fed into a frozen InternLM2-chat-7b LLM with a LoRA adaptor. Training uses an ASR-assisted format ("Transcription: [spoken question] Answer: [response]") to improve speech comprehension in noise. The model is trained on 4.9M QA pairs from datasets like VGGSound, AudioSet, and ClothoAQA, with 80% of data containing overlapping speech and audio at specified LUFS levels.

## Key Results
- SOLLA achieves strong performance on SA-Eval across audio classification, captioning, and QA tasks.
- In hard mode (overlapping speech/audio, low SNR), SOLLA outperforms or matches baselines, demonstrating robustness to noise.
- Ablation studies confirm that both the AT module and ASR-assisted prediction contribute to improved performance, especially in challenging acoustic conditions.

## Why This Works (Mechanism)

### Mechanism 1: Audio Tagging (AT) Module for Event Identification
- Integrating an AT module provides explicit audio event representations, improving the model's ability to reason about acoustic context.
- The AT module applies pooling, normalization, projection, and residual attention to create a compact feature vector, which is concatenated with the adaptor's output and fed into the LLM.
- This dedicated summary of acoustic events augments the fine-grained sequence representation, helping the LLM disentangle background sounds from speech.
- The benefit is most pronounced when the task requires identifying discrete sound events; it may diminish in tasks relying on general summarization.

### Mechanism 2: ASR-Assisted Prediction for Speech Instruction Grounding
- A multi-task training objective that includes an ASR-style transcription step significantly improves comprehension of spoken instructions, particularly in noisy, overlapped conditions.
- During training, the model's target output is formatted as "Transcription: [spoken question] Answer: [final response]", forcing it to first transcribe the speech instruction before generating the answer.
- This intermediate task creates a strong supervisory signal for the model to accurately extract the linguistic content of the speech instruction, reducing ambiguity from the noisy background.
- This mechanism is most critical when the speech instruction is corrupted by noise or overlapping audio; in clean scenarios, its benefit is less pronounced.

### Mechanism 3: Pre-trained Encoder with Noise-Robust Features
- The model's foundation for handling mixed speech and audio stems from the noise-robust representations learned by its core audio encoder (Whisper-Large-v3).
- By freezing the main parameters of the pre-trained encoder and only fine-tuning a small LoRA adaptor, the model leverages features that are invariant to noise and capable of disentangling speech from background.
- This approach assumes the input signal is within the distribution of data the pre-trained encoder has seen; performance may degrade on vastly different acoustic conditions.

## Foundational Learning

- **Concept: Signal-to-Noise Ratio (SNR) and LUFS**
  - Why needed here: Understanding the SA-Eval benchmark's "easy" vs. "hard" modes is impossible without grasping SNR. The paper uses LUFS to control perceived loudness and create specific low-SNR conditions to test model robustness.
  - Quick check question: If a speech signal has a loudness of -30 LUFS and the background audio is at -20 LUFS, would this be considered a high or low SNR condition?

- **Concept: Audio Tagging (AT)**
  - Why needed here: A core component of SOLLA is the AT module. Understanding AT means knowing the task of assigning descriptive labels (e.g., "waves", "bird song") to an audio clip, which is distinct from ASR (transcribing speech).
  - Quick check question: Given an audio clip of a dog barking in the rain, what would an AT system output compared to an ASR system?

- **Concept: Fine-Tuning with LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper explicitly freezes the large audio encoder and LLM, and only trains LoRA adaptors. This is a critical architectural decision for efficient training.
  - Quick check question: In the SOLLA architecture, which parameters are updated during training and which are held constant?

## Architecture Onboarding

- **Component map:**
  - Input (mixed audio) -> Audio Encoder (Whisper-Large-v3 + LoRA adaptor) -> Adaptor (2-layer MLP + GELU + pooling) -> AT Module (pooling, norm, projection, residual attention, classifier) -> Concatenated output -> LLM (InternLM2-chat-7b + LoRA adaptor) -> Text response

- **Critical path:** The robustness of the model depends heavily on the Audio Encoder. If this component fails to extract meaningful features from the noisy mixed signal, both the Adaptor and the AT Module will receive poor inputs, causing a cascade of failures in the LLM's reasoning.

- **Design tradeoffs:**
  - **Complexity vs. Specialization:** Adding the AT module adds architectural complexity and parameters but provides specialized knowledge for audio event understanding. This tradeoff is justified for tasks requiring explicit event identification.
  - **Generalization vs. Robustness:** Training with mixed, low-SNR data improves robustness to noise but may not generalize perfectly to clean, studio-quality audio without such mixtures in training.

- **Failure signatures:**
  - High WER in "hard" mode: Indicates the Audio Encoder is failing to separate speech from background noise.
  - Failure to identify audio events: Suggests the AT module is not learning meaningful representations or the training data lacks sufficient audio event diversity.
  - Correct transcription but incorrect answer: Implies the ASR task is working, but the LLM is failing to integrate the AT module's information or perform the reasoning required to answer the question based on the acoustic context.

- **First 3 experiments:**
  1. **Reproduce the main results on a single task:** Pick one task from SA-Eval (e.g., Clotho-AQA). Train SOLLA and the `Audio LLMspeech` baseline. Verify that SOLLA outperforms the baseline, especially in the "hard" mode.
  2. **Ablate the ASR-assisted prediction task:** Train SOLLA without the ASR-assisted task (target is just the answer) and compare instruction-following accuracy against the full model.
  3. **Evaluate with a different audio encoder:** Replace the Whisper encoder with another pre-trained model (e.g., from the AudioMAE family). Measure the performance drop.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the SOLLA architecture be adapted to process and reason about music-related acoustic context, given its current optimization for discrete audio events?
  - **Basis in paper:** The authors state in the Limitations section that "SOLLA and SA-Eval only address understanding tasks in the audio events, without considering music-related aspects."
  - **Why unresolved:** The current Audio Tagging (AT) module is trained on datasets (e.g., AudioSet, FSD50K) focused on sound events, which may not capture the temporal structures or harmonic complexities inherent in music.
  - **What evidence would resolve it:** An evaluation of the current AT module on music benchmarks (e.g., MusicQA) or a modified version of SOLLA fine-tuned on music datasets to see if the event-based representations transfer effectively.

- **Open Question 2:** How can the ASR-assisted prediction strategy be extended to maintain context in multi-turn dialogues where acoustic conditions change dynamically?
  - **Basis in paper:** The paper notes the limitation that "SA-Evalâ€™s evaluation currently focuses only on single-turn dialogues, limiting its ability to assess multi-turn complex dialogues."
  - **Why unresolved:** The current ASR-assisted task forces the model to predict "Transcription: [current speech] Answer: [response]" in a single pass. It is unclear how this mechanism handles conversational history or consistency when the background audio evolves over multiple turns.
  - **What evidence would resolve it:** An extension of the SA-Eval benchmark to include multi-turn interactions and experiments showing whether the current ASR-assisted method retains instruction-following accuracy over extended conversations.

- **Open Question 3:** What specific architectural or training advancements are required to close the performance gap between speech-instructed and text-instructed models in low-SNR environments?
  - **Basis in paper:** Table 2 shows that "Audio LLM text" consistently outperforms SOLLA, particularly in the hard mode (e.g., 55.6 vs 53.5 ACC on VGGSound), suggesting that converting speech instructions to text representations is still more robust than direct speech comprehension in noise.
  - **Why unresolved:** While the ASR-assisted prediction helps, the results imply that the model still struggles to disentangle speech instructions from overlapping audio as effectively as a text-model ignores noise.
  - **What evidence would resolve it:** A comparative analysis of the audio encoder's latent representations in noisy vs. clean conditions, or ablation studies using stronger pre-trained speech encoders to determine if the bottleneck lies in feature extraction or the LLM's fusion capability.

## Limitations

- The SA-Eval benchmark and SOLLA model only address understanding tasks in audio events, without considering music-related aspects.
- The evaluation currently focuses only on single-turn dialogues, limiting its ability to assess multi-turn complex dialogues.
- While the model improves speech instruction comprehension in noise, text-instructed models still outperform SOLLA in low-SNR environments, suggesting a gap in direct speech processing robustness.

## Confidence

- **High Confidence:** Pre-trained encoder choice and LoRA fine-tuning strategy
- **Medium Confidence:** Architecture claims (AT module and ASR-assisted prediction contributions), SA-Eval benchmark design and implementation
- **Low Confidence:** Generalization to real-world conditions beyond SA-Eval, long-term robustness of the model

## Next Checks

1. **AT Module Generalization Test:** Evaluate the AT module's F1 score on a held-out audio tagging dataset (e.g., FSD50K test set) separate from the training data. Compare this to its performance on the SA-Eval audio events to quantify domain adaptation and potential overfitting.

2. **SNR Robustness Analysis:** Systematically vary the SNR in the "hard" mode of SA-Eval (beyond the paper's single condition) to create a curve of model performance vs. background audio loudness. This will reveal the exact SNR threshold where SOLLA's performance degrades and how it compares to baseline models.

3. **Instruction-following in Clean Conditions:** Test SOLLA's instruction-following accuracy on the "easy" mode of SA-Eval with and without the ASR-assisted prediction task. This will isolate whether the ASR task is truly beneficial for clean speech or primarily helps in noisy conditions, as the ablation results suggest.