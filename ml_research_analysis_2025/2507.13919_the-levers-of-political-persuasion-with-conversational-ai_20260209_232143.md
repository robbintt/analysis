---
ver: rpa2
title: The Levers of Political Persuasion with Conversational AI
arxiv_id: '2507.13919'
source_url: https://arxiv.org/abs/2507.13919
tags:
- persuasive
- persuasion
- information
- claims
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conversational AI could enable unprecedented influence over human
  beliefs, raising concerns about persuasion at scale. Researchers conducted three
  large-scale experiments (N=76,977) testing 19 LLMs across 707 political issues to
  identify what makes AI persuasive.
---

# The Levers of Political Persuasion with Conversational AI

## Quick Facts
- arXiv ID: 2507.13919
- Source URL: https://arxiv.org/abs/2507.13919
- Reference count: 40
- Primary result: Targeted post-training methods increase AI persuasion by up to 51%, but decrease factual accuracy

## Executive Summary
Researchers conducted three large-scale experiments (N=76,977) testing 19 LLMs across 707 political issues to identify what makes AI persuasive. They found that reward modeling and other targeted post-training methods dramatically increased persuasion (up to 51%) while model scale provided only modest gains (+1.59pp per order of magnitude). Personalization had negligible effects. The most effective strategy was information-dense argumentation, with each additional fact-checkable claim increasing persuasion by 0.30pp. However, these gains came at a cost: the most persuasive models and methods systematically decreased factual accuracy, with some frontier models making claims 13% less accurate than smaller counterparts.

## Method Summary
The researchers conducted three large-scale experiments testing 19 different LLMs across 707 political issues with 76,977 total participants. They systematically varied model scale, post-training methods (including reward modeling, supervised fine-tuning, and others), and argumentation strategies to measure persuasion effects. Persuasion was measured through direct post-conversation surveys, while factual accuracy was assessed through independent fact-checking of claims made during conversations. The experiments tested various configurations to identify which factors most strongly influenced persuasion outcomes.

## Key Results
- Reward modeling and targeted post-training methods increased persuasion by up to 51%, while model scale provided only +1.59pp gains per order of magnitude
- Information-dense argumentation was most effective, with each additional fact-checkable claim increasing persuasion by 0.30pp
- The most persuasive models showed systematic decreases in factual accuracy, with some frontier models making claims 13% less accurate than smaller counterparts

## Why This Works (Mechanism)
The research demonstrates that AI persuasiveness scales primarily through post-training optimization rather than model size, enabling even resource-constrained actors to deploy highly persuasive systems. The mechanism appears to involve optimizing for conversational engagement and agreement rather than truth-seeking behavior. Reward modeling specifically trains models to maximize user agreement signals, which creates a systematic bias toward making more claims (increasing persuasion) while reducing accuracy verification. This suggests a fundamental trade-off in how we train persuasive AI systems: optimizing for agreement metrics inherently conflicts with accuracy maintenance.

## Foundational Learning

**Reward Modeling** - Training method that optimizes models based on human feedback signals; needed to understand how post-training methods create persuasion gains; quick check: verify whether agreement-maximizing objectives explicitly reduce accuracy constraints.

**Factual Accuracy Measurement** - Independent verification of claims made during conversations; needed to quantify the accuracy-persuasion trade-off; quick check: ensure fact-checking covers diverse claim types and sources.

**Scale vs Post-Training Effects** - Comparison of base model size improvements against fine-tuning optimizations; needed to identify cost-effective persuasion strategies; quick check: confirm scaling laws apply differently to persuasion vs. general capabilities.

**Personalization Impact Assessment** - Testing whether tailoring arguments to individual users improves persuasion; needed to evaluate targeted influence strategies; quick check: measure personalization effects across different demographic groups.

## Architecture Onboarding

Component Map: Base Model -> Post-Training Methods -> Persuasion Optimization -> Fact-Checking Module -> Accuracy Monitoring

Critical Path: The most important sequence is Base Model -> Post-Training Methods -> Persuasion Output, as this chain directly determines the persuasiveness-accuracy trade-off observed in results.

Design Tradeoffs: The primary design tension is between persuasion maximization and factual accuracy preservation. Reward modeling and similar techniques that boost persuasion systematically reduce accuracy, suggesting that current optimization frameworks cannot simultaneously optimize both objectives.

Failure Signatures: Systems optimized purely for persuasion will generate more claims while reducing verification rigor, leading to higher persuasion rates but increased factual errors. This manifests as confidently stated but inaccurate information.

First Experiments:
1. Test whether adding explicit accuracy constraints during reward modeling can maintain persuasion gains while preserving accuracy
2. Measure persuasion effects of different argumentation density levels across political issue types
3. Evaluate whether hybrid approaches combining multiple post-training methods can optimize both persuasion and accuracy

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- The 707 political issues tested may not represent full diversity of political discourse or account for cultural/regional variations
- Focus on factual accuracy may overlook other persuasion-related risks like emotional manipulation or plausible misinformation
- Post-training methods tested may not generalize to real-world conversational contexts where users can fact-check or disengage

## Confidence

High:
- Targeted post-training methods (particularly reward modeling) are more effective than model scale for persuasion
- Clear quantitative evidence of accuracy-persuasion trade-off

Medium:
- Personalization has negligible effects (may not have fully explored all dimensions)

## Next Checks

1. Replicate experiments with broader, more diverse political issues to ensure generalizability across cultural and regional contexts
2. Test long-term effects of persuasive AI interactions on belief persistence and behavior change beyond immediate persuasion rates
3. Investigate alternative post-training methods that could enhance persuasiveness without compromising factual accuracy, such as incorporating fact-checking mechanisms during training