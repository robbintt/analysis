---
ver: rpa2
title: 'Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving'
arxiv_id: '2507.02726'
source_url: https://arxiv.org/abs/2507.02726
tags:
- proof
- search
- theorem
- proving
- lean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bourbaki, a self-generated goal-conditioned
  MDP framework for automated theorem proving in Lean. It addresses the challenge
  of sparse rewards in theorem proving by dynamically generating subgoals during proof
  search, forming an sG-MDP where agents generate and pursue subgoals based on evolving
  proof states.
---

# Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving

## Quick Facts
- arXiv ID: 2507.02726
- Source URL: https://arxiv.org/abs/2507.02726
- Reference count: 4
- Primary result: 7B model solves 26 PutnamBench problems, establishing state-of-the-art for this scale

## Executive Summary
Bourbaki introduces a self-generated goal-conditioned Markov Decision Process (sG-MDP) framework for automated theorem proving in Lean. The method addresses the challenge of sparse rewards in theorem proving by dynamically generating subgoals during proof search, where agents generate and pursue subgoals based on evolving proof states. The framework ensembles multiple 7B LLMs for subgoal generation and tactic synthesis, guided by Monte Carlo Tree Search (MCTS). On the challenging PutnamBench benchmark, Bourbaki solves 26 problems, outperforming previous 7B models and establishing new state-of-the-art performance for models at this scale.

## Method Summary
Bourbaki implements a self-generated goal-conditioned MDP where the proof search process generates its own subgoals rather than relying on predefined goal sets. The framework uses MCTS to explore the proof space, with nodes representing proof states and edges representing tactic applications. At each step, the agent either generates subgoals (transitioning to new states) or synthesizes tactics to achieve current goals. The method ensembles DeepSeek-Prover-v2-7B and Kimina-7B models, using UCB selection to balance exploration and exploitation. Value estimation combines depth-based metrics with solved conjecture counts, and vLLM enables efficient batch inference. The approach achieves improved sample efficiency and proof diversity compared to baselines.

## Key Results
- Solves 26 problems on PutnamBench benchmark, establishing state-of-the-art for 7B models
- Outperforms Kimina-7B (10 problems) and DeepSeek-Prover-v2 (23 problems)
- Demonstrates improved sample efficiency and proof diversity
- Solves some theorems that only appear at higher sample budgets in baseline models

## Why This Works (Mechanism)
The sG-MDP framework works by converting the sparse-reward theorem proving problem into a dense-reward subgoal generation process. Instead of waiting for complete proofs to receive rewards, the system generates intermediate subgoals that provide immediate feedback and guide search. MCTS explores the proof tree efficiently by balancing exploitation of promising tactics with exploration of new subgoals. The ensemble approach leverages multiple model strengths, while the dynamic subgoal generation adapts to the evolving proof state, avoiding the need for expensive human-annotated goal sets.

## Foundational Learning

**Lean 4 Theorem Proving** - Interactive theorem prover based on dependent type theory. Needed for understanding the proof environment and tactic language. Quick check: Verify Lean REPL interaction works with target mathlib version.

**Monte Carlo Tree Search** - Search algorithm that balances exploration and exploitation using UCB. Needed for efficient proof space exploration. Quick check: Implement basic MCTS on small game tree and verify UCB selection.

**Markov Decision Process** - Framework for sequential decision making under uncertainty. Needed for modeling the proof search as a decision process. Quick check: Verify state transitions and reward structure in proof environment.

**Subgoal Generation** - Process of decomposing complex goals into manageable subgoals. Needed for creating dense reward signals. Quick check: Test subgoal generation on simple theorem with known decomposition.

## Architecture Onboarding

**Component Map:** Lean Environment -> Subgoal Generator -> Tactic Synthesizer -> MCTS -> Policy Ensemble -> vLLM Batch Inference

**Critical Path:** Proof State → Subgoal Generation → Tactic Synthesis → Validation → Reward → Backpropagation

**Design Tradeoffs:** Ensemble multiple 7B models vs single larger model; dynamic subgoal generation vs predefined goals; MCTS depth vs breadth; batch inference efficiency vs latency.

**Failure Signatures:** Lean/mathlib version mismatch causing tactic validation failures; PyPantograph subgoal state mismanagement; UCB selection leading to premature convergence; batch inference bottlenecks.

**First Experiments:** 1) Test Lean environment with known tactics on sample theorems; 2) Verify PyPantograph goal verification on simple proof states; 3) Run MCTS with dummy policy on small proof tree to validate search dynamics.

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation details for model ensembling mechanism not specified
- Exact reward function weighting and formula not provided
- UCB exploration constant value not stated
- Prompt templates for subgoal vs tactic generation not detailed

## Confidence

**High confidence:** General framework of self-generated goal-conditioned MDPs with MCTS is well-specified; problem setup (PutnamBench, Lean 4 environment) is clear.

**Medium confidence:** Use of ensemble models and basic MCTS algorithm structure.

**Low confidence:** Specific implementation details of ensembling, reward function weights, UCB constant, and prompt templates.

## Next Checks
1. Verify the Lean/mathlib environment setup by testing known tactic validation and goal verification workflows on sample theorems from PutnamBench
2. Implement and test the PyPantograph subgoal state management with logging of goal stack transitions to ensure correct proof state tracking
3. Conduct ablation studies comparing different ensembling strategies (e.g., alternating vs weighted sampling) and UCB constants to identify optimal configuration parameters for the sG-MDP framework