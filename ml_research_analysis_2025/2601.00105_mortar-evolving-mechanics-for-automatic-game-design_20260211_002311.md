---
ver: rpa2
title: 'Mortar: Evolving Mechanics for Automatic Game Design'
arxiv_id: '2601.00105'
source_url: https://arxiv.org/abs/2601.00105
tags:
- self
- game
- enemy
- tile
- mechanics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mortar is an autonomous system for evolving game mechanics using
  a quality-diversity algorithm guided by a large language model. It evaluates mechanics
  by embedding them in complete games and measuring their contribution to skill-based
  player ranking.
---

# Mortar: Evolving Mechanics for Automatic Game Design

## Quick Facts
- arXiv ID: 2601.00105
- Source URL: https://arxiv.org/abs/2601.00105
- Reference count: 40
- Primary result: Mortar uses LLM-driven variation and skill-based ordering to evolve game mechanics, achieving higher quality-diversity and CITS scores than baseline methods

## Executive Summary
Mortar is an autonomous system for evolving game mechanics using a quality-diversity algorithm guided by a large language model. It evaluates mechanics by embedding them in complete games and measuring their contribution to skill-based player ranking. The system uses Monte Carlo Tree Search to construct games and a novel Constrained Importance Through Search (CITS) score to quantify each mechanic's marginal contribution to gameplay quality. Experiments show Mortar achieves higher quality-diversity scores and maximum CITS scores compared to alternative methods, while a user study indicates correlation between automated metrics and human preferences for generated games.

## Method Summary
Mortar represents game mechanics as Python functions and evolves them using LLM-driven variation operators within a MAP-Elites framework. The system evaluates mechanics by constructing complete games via Monte Carlo Tree Search, then measuring skill differentiation using Kendall's Tau rank correlation across five agent types. A novel CITS score estimates each mechanic's marginal contribution by analyzing MCTS tree exploration. The archive is a 13×13 grid defined by mechanic type (DistilBERT similarity) and code complexity (AST-based). The system iterates through variation, validation, game construction, evaluation, and archive updates until a computational budget is reached.

## Key Results
- Mortar achieved higher maximum CITS scores than baselines using Evaluation MCTS composition
- The system showed stronger Quality-Diversity (QD) score improvements compared to random selection methods
- User study indicated moderate correlation between automated skill-based metrics and human preference rankings

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Semantic Mutation in Code Space
The system leverages LLMs to perform meaningful code mutations and crossovers on Python mechanics, ensuring syntactic validity and semantic coherence where traditional genetic operators fail. This relies on the LLM's pre-trained knowledge of code semantics and the static test environment to filter invalid outputs before evaluation.

### Mechanism 2: Skill-Based Ordering as a Proxy for Mechanic Utility
Mechanics are evaluated based on their ability to create skill gradients in complete games, measured by Kendall's Tau rank correlation across agents of varying capability. This operationalizes "game quality" as the capacity to distinguish between stronger and weaker players.

### Mechanism 3: Constrained Importance Through Search (CITS) for Attribution
CITS uses Shapley-value logic over the MCTS expansion tree to estimate each mechanic's marginal contribution to game quality, making attribution computationally tractable while being grounded in actual gameplay evaluations.

## Foundational Learning

- **Quality-Diversity (QD) Algorithms (MAP-Elites):** Needed to understand the archive structure that stores mechanics in a 2D grid for both performance and behavioral diversity. Quick check: How does Mortar determine where to place a newly evolved mechanic in its archive?

- **Monte Carlo Tree Search (MCTS):** Used "in reverse" to construct games rather than play them, building a tree of possible mechanic combinations. Quick check: In Mortar, what does a "node" in the MCTS tree represent—a game state or a game design choice?

- **Kendall's Tau Rank Correlation:** The core fitness function measuring how well games order agents by skill. Quick check: If a game allows a Random agent to win as often as a high-rollout MCTS agent, what would be the approximate Kendall's Tau score?

## Architecture Onboarding

- **Component map:** Archive (QD) -> LLM Generator -> Game Constructor (MCTS) -> Evaluation Environment -> Agent Pool -> Scorer -> Archive Update

- **Critical path:** Sample mechanic from Archive → LLM mutates mechanic → Validation (syntax check + static test) → MCTS builds game tree → Agents play game → Calculate Kendall's Tau and CITS → Archive update if CITS score is higher

- **Design tradeoffs:** LLM cost vs. quality (uses GPT-4o-mini for affordability), search depth vs. compute (capped MCTS iterations), metric limitations (Kendall's Tau measures skill differentiation, not fun)

- **Failure signatures:** LLM code generation errors (hallucinated variables), evaluation failures (unplayable games with τ = -1), archive stagnation (restrictive initialization)

- **First 3 experiments:** 1) Ablation validation comparing Evaluation MCTS vs. Random Selection for CITS scores, 2) Metric correlation check with human players for games like MagneticProwess vs. HeroHunt, 3) Complexity variation testing different AST weight configurations

## Open Questions the Paper Calls Out
1. Can a constraint-based interface be integrated into Mortar to allow human designers to guide the evolutionary search toward specific gameplay goals?

2. How does the choice of archive initialization strategy and LLM scale impact the convergence and diversity of evolved mechanics?

3. How can the automated evaluation metric (τ) be refined to better align with human subjective preferences regarding "meaningfulness" and fun?

## Limitations
- The system lacks designer control mechanisms for steering evolution toward specific goals
- Archive initialization presents challenges for evolvability and coverage
- The modest LLM (GPT-4o-mini) may limit the complexity of discoverable mechanics
- Automated metrics (Kendall's Tau) don't fully capture subjective qualities like fun or visual appeal

## Confidence
- **High Confidence:** MAP-Elites framework implementation and Kendall's Tau evaluation methodology are clearly specified and replicable
- **Medium Confidence:** LLM-driven variation operators work as intended, though results may vary with different model versions or prompts
- **Low Confidence:** CITS attribution method's effectiveness in isolating mechanic contributions, as the paper provides limited validation of this novel metric

## Next Checks
1. Run generated games with human players to verify if Kendall's Tau correlates with subjective game quality ratings, particularly for edge cases like "MagneticProwess vs. HeroHunt"

2. Vary MCTS iteration counts (10, 20, 50) to measure impact on CITS score stability and maximum achievable quality

3. Experiment with different AST weight configurations to determine optimal balance between code complexity and functional diversity