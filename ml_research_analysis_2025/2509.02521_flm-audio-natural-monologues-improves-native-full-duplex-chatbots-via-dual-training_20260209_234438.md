---
ver: rpa2
title: 'FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual
  Training'
arxiv_id: '2509.02521'
source_url: https://arxiv.org/abs/2509.02521
tags:
- arxiv
- audio
- training
- flm-audio
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FLM-Audio addresses the scalability and language modeling limitations\
  \ of existing full-duplex speech systems by introducing contiguous monologues\u2014\
  continuous sentences with wait intervals\u2014paired with a dual training paradigm.\
  \ This approach replaces word-level text-audio alignment with sentence-level alignment,\
  \ preserving autoregressive language model strengths and enabling real-time, native\
  \ full-duplex speech without sacrificing semantic understanding."
---

# FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training

## Quick Facts
- **arXiv ID:** 2509.02521
- **Source URL:** https://arxiv.org/abs/2509.02521
- **Authors:** Yiqun Yao; Xiang Li; Xin Jiang; Xuezhi Fang; Naitong Yu; Wenjia Ma; Aixin Sun; Yequan Wang
- **Reference count:** 21
- **Primary result:** Achieves state-of-the-art full-duplex performance with <15% of training data used by comparable systems, outperforming baselines in speech recognition accuracy, audio generation quality, and human evaluations.

## Executive Summary
FLM-Audio addresses the scalability and language modeling limitations of existing full-duplex speech systems by introducing contiguous monologues—continuous sentences with wait intervals—paired with a dual training paradigm. This approach replaces word-level text-audio alignment with sentence-level alignment, preserving autoregressive language model strengths and enabling real-time, native full-duplex speech without sacrificing semantic understanding. The model is trained across four stages using both TTS- and ASR-style data formats, which improves both audio understanding and responsiveness. FLM-Audio achieves state-of-the-art full-duplex performance with less than 15% of the training data used by comparable systems, outperforming baselines in speech recognition accuracy, audio generation quality, and human evaluations of naturalness, responsiveness, and robustness.

## Method Summary
FLM-Audio uses a 7B Qwen-2.5-VL backbone with RQ-Transformer architecture to process audio at 12.5 fps, generating 8 tokens per frame (1 semantic + 7 acoustic) via Mimi codec. The system employs contiguous monologues with <wait> tokens instead of word-level alignment, and uses a native full-duplex architecture that merges channels per timestep rather than Time-Division Multiplexing. Training occurs in four stages: Post-training-1 & -2 on large-scale audio-text corpus with dual TTS-style and ASR-style formats, Fine-tuning-1 on semi-duplex ASR-Response-TTS data, and Fine-tuning-2 on full-duplex Response-TTS with random interruptions. The model uses weighted cross-entropy loss with α₁=1, α₂=0.5, β=1, γ=0.01 for semantic audio, acoustic audio, monologue, and wait tokens respectively.

## Key Results
- Achieves state-of-the-art full-duplex performance with <15% of training data compared to baselines
- Outperforms existing systems in speech recognition accuracy (WER on Fleurs-zh and LibriSpeech-clean)
- Superior audio generation quality and human evaluation scores for naturalness, responsiveness, and robustness
- Maintains <80ms latency while enabling real-time native full-duplex conversation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contiguous monologues preserve autoregressive language modeling strength better than word-level alignment.
- Mechanism: By generating uninterrupted text token sequences (full sentences) instead of interleaving each token with pad tokens to match audio timing, the model retains the pre-trained LLM's ability to model coherent language. Text completes earlier than speech due to bitrate differences; `<wait>` tokens fill the gap until the next sentence.
- Core assumption: Sentence-level alignment is sufficient for semantic grounding; fine-grained word-timestamp alignment introduces noise without proportional benefit.
- Evidence anchors:
  - [abstract] "replaces word-level text-audio alignment with sentence-level alignment, preserving autoregressive language model strengths"
  - [section 2.2] "fragmenting sentences into isolated word-level tokens undermines the language modeling capacity of the backbone"
  - [section 5.4, Table 7] Ablation shows contiguous strategy achieving HellaSwag 61.6% vs word-level 58.3% on same data subset
- Break condition: If tasks require precise word-level temporal grounding (e.g., lip-sync, fine-grained captioning), contiguous monologues may lose alignment fidelity.

### Mechanism 2
- Claim: Dual training (alternating TTS-style and ASR-style data formats) enables the model to handle asynchronous text-audio semantics.
- Mechanism: TTS-style training (monologue leads audio by ~2 tokens) teaches the model to generate speech from text intent; ASR-style training (monologue trails audio) teaches transcription from audio. Alternating these across stages forces the model to learn bidirectional audio-text grounding, which stabilizes the semi-duplex transition and supports full-duplex generation.
- Core assumption: Both production (TTS) and comprehension (ASR) pathways must be trained explicitly; neither emerges reliably from single-direction training alone.
- Evidence anchors:
  - [abstract] "develop a 'dual' training paradigm that alternates the position of the monologues, either leading or trailing the audio"
  - [section 5.3, Table 5] "FLM-Audio w/o SFT-1" (skipping ASR-style supervision) drops to LLM-score 4.59 from 6.58
  - [section 3.1] Describes explicit TTS-style and ASR-style token organization
- Break condition: If inference only requires one direction (pure ASR or pure TTS), dual training adds computational overhead without proportional gain.

### Mechanism 3
- Claim: Native full-duplex architecture (merging channels per timestep) avoids the O(n²) context growth bottleneck of Time-Division Multiplexing.
- Mechanism: In native full-duplex, each timestep's hidden state aggregates text + listen audio + speak audio embeddings (Eq. 1), so context length grows with time, not with channel count. TDM concatenates channels into one long sequence, causing quadratic attention cost and limiting max generation length (~45s reported). Native design reduces latency to ~80ms.
- Core assumption: The backbone hidden state `h_t` carries sufficient information for all modalities; the depth Transformer can decode audio tokens from local `h_t` without re-attending to full history.
- Evidence anchors:
  - [section 1] "TDM significantly hampers responsiveness, resulting in full-duplex delays of up to 2 seconds"
  - [section 2.1] "hidden state h_t is sufficiently informative for textual, semantic, and acoustic generation"
  - [corpus] RoboEgo and SALMONN-omni papers confirm native full-duplex as an emerging paradigm, but corpus evidence on specific latency comparisons is limited
- Break condition: If the depth Transformer lacks capacity to decode complex acoustic details from local `h_t`, audio quality may degrade for long-form generation.

## Foundational Learning

- Concept: **Residual Quantization (RQ-Transformer) for Audio Tokens**
  - Why needed here: Audio is discretized into 8 tokens per frame (1 semantic + 7 acoustic), generated hierarchically. Understanding this decomposition is essential to debug token organization and loss weighting.
  - Quick check question: Can you explain why the first audio token is called "semantic" and why it has a different loss weight than the 7 acoustic tokens?

- Concept: **Autoregressive Language Modeling with Multi-Channel Inputs**
  - Why needed here: The model must predict next tokens for text and audio simultaneously, with different alignment constraints. Misunderstanding the joint prediction objective leads to incorrect loss masking.
  - Quick check question: In Eq. 1, how are the three modalities (text, listen, speak) combined before being passed to the backbone? What happens if one channel is empty?

- Concept: **Sentence-Level vs Word-Level Alignment**
  - Why needed here: The core innovation is avoiding word-level timestamps. Practitioners must understand how to prepare datasets with only sentence boundaries and how to handle edge cases (long sentences, overlapping speech).
  - Quick check question: Given an audio clip and its transcript, what minimal preprocessing is required to create a valid training sample for FLM-Audio?

## Architecture Onboarding

- Component map: Backbone LLM (Qwen-2.5-VL, 7B) -> Depth Transformer -> Mimi Encoder/Decoder -> Audio tokens (8 per frame: 1 semantic + 7 acoustic)
- Critical path:
  1. Stage 1 (Post-training-1 & -2): Train on large-scale audio-text corpus with dual formats; establish basic audio understanding and generation.
  2. Stage 2 (Fine-tuning-1): Semi-duplex ASR-Response-TTS style; bridge comprehension and production.
  3. Stage 3 (Fine-tuning-2): Full-duplex Response-TTS with random interruptions; teach turn-taking and graceful cutoff.
  4. Inference: Merge live user audio (listen channel), generate monologue + speech (speak channel) in real time.

- Design tradeoffs:
  - **Contiguous monologues vs word-level alignment**: Lower annotation cost + better LM preservation, but loses fine-grained temporal control.
  - **Native vs TDM**: Lower latency and better scaling, but requires careful multi-channel loss balancing and more complex data preparation.
  - **Loss weighting (α, β, γ)**: Semantic audio weight α₁=1, acoustic α₂=0.5, monologue β=1, wait γ=0.01 differ significantly from Moshi's reported values; tuning is sensitive.

- Failure signatures:
  - **Degraded ASR after SFT**: Likely skipped Fine-tuning-1 ASR-style data; model forgot transcription ability.
  - **Unnatural pauses or premature cutoffs**: Wait token loss weight (γ) may be too high, or interruption probability too aggressive in training.
  - **Context overflow or slow inference**: Verify native merging is active (Eq. 1); check for accidental TDM-style concatenation in data pipeline.

- First 3 experiments:
  1. **Sanity check—single-modality generation**: Feed empty listen channel, verify TTS-style output quality and alignment with monologue text on held-out samples.
  2. **Ablation—skip Fine-tuning-1**: Train without ASR-Response-TTS stage; measure WER increase on LibriSpeech/Fleurs and LLM-score drop to quantify the dual-training benefit.
  3. **Latency benchmark—native vs TDM baseline**: Implement a TDM-style variant on same backbone; measure RTF and max generation length under identical hardware to validate native architecture claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the comparative advantage of native full-duplex architectures over TDM-based approaches change when scaling FLM-Audio to parameter counts significantly larger than 7B?
- Basis in paper: [explicit] The conclusion states, "Constrained by data volume and computational resources, we have not yet scaled FLM-Audio to larger parameter counts—a direction where native duplex models could exhibit even greater advantages."
- Why unresolved: The study was restricted to a ~7B parameter backbone (Qwen-2.5-VL) due to resource constraints, leaving the scaling laws for this specific architecture unverified.
- What evidence would resolve it: Training and benchmarking FLM-Audio at scales of 30B–70B parameters against equivalent TDM-based models to observe latency and semantic performance trends.

### Open Question 2
- Question: Would the observed superiority of contiguous monologues over word-level alignment persist if the controlled comparison were scaled to the full training corpus rather than a 5% subset?
- Basis in paper: [inferred] Section 5.4 notes that a fully controlled comparison against word-level alignment was "prohibitively expensive," so the ablation was limited to a 5% data subset.
- Why unresolved: Training dynamics and convergence on small subsets (5%) may not accurately predict behavior or performance gaps on massive datasets (1M+ hours).
- What evidence would resolve it: A direct comparison training both alignment strategies on the full 1 million hours of audio data, measuring final ASR WER and semantic benchmark scores.

### Open Question 3
- Question: Is the reduced loss weighting for wait tokens ($\gamma=0.01$) robust across diverse languages and speaking rates, or is it overfitted to the specific English/Chinese datasets used?
- Basis in paper: [inferred] The paper notes a drastic difference in hyperparameters for wait tokens ($\gamma=0.01$) compared to related work ($\gamma=0.5$), determined empirically without extensive cross-lingual validation.
- Why unresolved: The specific low weight might prevent the model from learning proper pausing behavior in languages with different syntactic structures or pause distributions.
- What evidence would resolve it: Ablation studies on the $\gamma$ hyperparameter across a wider variety of languages and fast-speech augmentation settings to ensure the alignment strategy generalizes.

## Limitations

- **Dataset composition and representativeness**: The paper reports using ~1M hours of speech data, but the exact composition and quality of the auto-transcribed corpus are not disclosed. The heavy reliance on synthesized dialogues for fine-tuning (200K samples) raises questions about the model's ability to generalize to real human conversations with complex dynamics.

- **Architectural specifics**: While the paper describes the overall architecture, critical implementation details remain underspecified. The exact configuration of the depth transformer (layers, hidden dimensions, attention heads) is not provided, making exact reproduction challenging.

- **Evaluation scope limitations**: While the model shows strong performance on benchmarks like Fleurs-zh, LibriSpeech, and HellaSwag, the evaluation focuses primarily on controlled scenarios. Real-world full-duplex conversations involve overlapping speech, background noise, and emotional variability that may not be fully captured in the evaluation setup.

## Confidence

- **High confidence (8/10)**: The core claim that contiguous monologues with sentence-level alignment outperform word-level alignment in preserving autoregressive language modeling strength is well-supported by the ablation study (Table 7 showing 61.6% vs 58.3% HellaSwag). The architectural design of native full-duplex over TDM is clearly explained and the latency improvement claim (~80ms vs 2s) is plausible given the theoretical analysis.

- **Medium confidence (6/10)**: The dual training paradigm's effectiveness is supported by the SFT-1 ablation showing significant LLM-score drop from 6.58 to 4.59, but the mechanism could be more thoroughly validated. The claim of achieving state-of-the-art performance with less than 15% of the training data is based on comparisons with systems using different architectures and training paradigms, making direct comparison difficult.

- **Low confidence (4/10)**: The generalization claims to real-world conversations and the robustness to diverse interruptions are primarily supported by human evaluation scores without detailed breakdowns of failure cases or edge conditions. The specific threshold values (interruption probability 0.7, reaction delay 0.5s) appear to be chosen without extensive sensitivity analysis.

## Next Checks

1. **Ablation study on loss weight sensitivity**: Systematically vary the loss weights (α₁, α₂, β, γ) across multiple runs to identify the sensitivity of performance to these hyperparameters. This would validate whether the reported values are truly optimal or if the model is robust to a range of settings.

2. **Real-world deployment test**: Deploy FLM-Audio in actual conversational scenarios with multiple human participants, measuring latency, turn-taking accuracy, and naturalness in unscripted dialogues. Compare against controlled evaluation metrics to identify performance gaps.

3. **Cross-lingual generalization test**: Evaluate FLM-Audio on languages not represented in the training data (beyond the Chinese/English split mentioned) to assess the model's ability to generalize to new languages without additional fine-tuning.