---
ver: rpa2
title: 'DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only
  Antigen-Antibody Affinity Prediction'
arxiv_id: '2512.22007'
source_url: https://arxiv.org/abs/2512.22007
tags:
- antibody
- affinity
- antigen
- binding
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DuaDeep-SeqAffinity, a novel sequence-only
  deep learning framework for predicting antigen-antibody binding affinity scores.
  The core innovation lies in a dual-stream hybrid architecture that combines ESM-2
  protein language model embeddings with 1D Convolutional Neural Networks (CNNs) for
  local motif detection and Transformer encoders for global contextual representation.
---

# DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction

## Quick Facts
- arXiv ID: 2512.22007
- Source URL: https://arxiv.org/abs/2512.22007
- Reference count: 40
- Key outcome: Novel sequence-only deep learning framework achieving state-of-the-art antigen-antibody affinity prediction with Pearson correlation of 0.688 and R² of 0.460

## Executive Summary
This paper introduces DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for predicting antigen-antibody binding affinity scores. The core innovation lies in a dual-stream hybrid architecture that combines ESM-2 protein language model embeddings with 1D Convolutional Neural Networks (CNNs) for local motif detection and Transformer encoders for global contextual representation. A fusion module integrates these multi-faceted features, which are then passed to a fully connected network for final score regression. Experimental results demonstrate that DuaDeep-SeqAffinity significantly outperforms existing state-of-the-art methods, achieving a Pearson correlation of 0.688, an R² of 0.460, and a Root Mean Square Error (RMSE) of 0.737. Notably, the model achieves an Area Under the Curve (AUC) of 0.890, surpassing sequence-only benchmarks and even structure-sequence hybrid models. These findings prove that high-fidelity sequence embeddings can capture essential binding patterns typically reserved for structural modeling, providing a highly scalable and efficient solution for high-throughput screening of vast sequence libraries.

## Method Summary
DuaDeep-SeqAffinity employs a dual-stream hybrid architecture combining ESM-2 protein language model embeddings with 1D Convolutional Neural Networks for local motif detection and Transformer encoders for global contextual representation. The dual-stream approach allows the model to capture both local sequence patterns and long-range dependencies in antigen-antibody interactions. A fusion module integrates these multi-faceted features, which are then processed by a fully connected network to produce the final affinity score prediction. The framework is trained end-to-end on sequence data only, eliminating the need for structural information while maintaining competitive performance against structure-based methods.

## Key Results
- Achieved Pearson correlation of 0.688 and R² of 0.460 for affinity prediction
- Attained AUC of 0.890, surpassing both sequence-only benchmarks and structure-sequence hybrid models
- Outperformed existing state-of-the-art methods for sequence-only antigen-antibody affinity prediction

## Why This Works (Mechanism)
The dual-stream architecture effectively captures complementary aspects of antigen-antibody interactions: CNNs detect local sequence motifs that often correspond to binding interfaces, while Transformers capture long-range dependencies and contextual relationships between residues. ESM-2 embeddings provide rich pre-trained protein representations that encode evolutionary and structural information implicitly. The fusion module learns to combine these complementary features optimally, allowing the model to extract binding-relevant patterns from sequence data alone. This approach successfully replaces the need for structural information by leveraging the comprehensive representation power of modern protein language models and the ability of deep learning to discover complex patterns in sequence data.

## Foundational Learning
**Protein Language Models (ESM-2)**: Why needed - Provide rich, pre-trained embeddings that encode evolutionary and structural information; Quick check - ESM-2 embeddings should capture known functional and structural motifs when visualized or analyzed.

**1D Convolutional Neural Networks**: Why needed - Detect local sequence patterns and motifs that often correspond to binding interfaces; Quick check - CNN filters should learn to recognize known binding site patterns when examined.

**Transformer Encoders**: Why needed - Capture long-range dependencies and contextual relationships between residues across the entire sequence; Quick check - Attention patterns should highlight functionally important residue pairs.

**Sequence-Only vs. Structure-Based Approaches**: Why needed - Demonstrate that sequence data alone can provide sufficient information for accurate affinity prediction; Quick check - Model should maintain performance when structural data is unavailable.

**Dual-Stream Architecture**: Why needed - Combine complementary feature extraction methods to capture both local and global binding patterns; Quick check - Each stream should contribute meaningfully to overall performance when evaluated independently.

## Architecture Onboarding

**Component Map**: Input sequences → ESM-2 Embeddings → CNN Stream → Transformer Stream → Fusion Module → Fully Connected Network → Output Affinity Score

**Critical Path**: Input → ESM-2 → CNN/Transformer streams → Fusion → Output

**Design Tradeoffs**: Sequence-only approach trades structural detail for scalability and speed; dual-stream architecture adds complexity but captures more comprehensive binding patterns; ESM-2 pre-training provides strong starting representations but may limit fine-tuning flexibility.

**Failure Signatures**: Poor performance on sequences with rare motifs not captured by ESM-2; overfitting when training data is limited; degradation when sequences deviate significantly from training distribution; reduced accuracy for non-antibody protein-protein interactions.

**First Experiments**:
1. Ablation study removing CNN stream to measure local pattern detection contribution
2. Ablation study removing Transformer stream to measure global context contribution  
3. Performance comparison using random embeddings vs. ESM-2 to quantify pre-training benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Relies entirely on SKEMPI 2.0 dataset, potentially limiting generalizability to other affinity prediction tasks
- Model performance on non-antibody protein-protein interactions remains unknown
- Moderate R² value (0.460) indicates room for improvement in explaining variance in binding affinity scores

## Confidence
- **High confidence**: The model's superior performance metrics compared to existing methods (AUC 0.890 vs. sequence-only benchmarks)
- **Medium confidence**: The claim that sequence embeddings can capture essential binding patterns typically requiring structural modeling
- **Medium confidence**: The scalability advantages of the sequence-only approach for high-throughput screening

## Next Checks
1. Test model performance on independent antibody-antigen datasets outside SKEMPI 2.0 to verify generalizability
2. Evaluate performance on non-antibody protein-protein interaction datasets to assess broader applicability
3. Conduct ablation studies to quantify the individual contributions of ESM-2 embeddings, CNNs, and Transformer components to overall performance