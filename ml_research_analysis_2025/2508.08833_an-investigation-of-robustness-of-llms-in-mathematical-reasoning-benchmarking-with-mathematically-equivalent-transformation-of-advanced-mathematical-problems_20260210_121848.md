---
ver: rpa2
title: 'An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking
  with Mathematically-Equivalent Transformation of Advanced Mathematical Problems'
arxiv_id: '2508.08833'
source_url: https://arxiv.org/abs/2508.08833
tags:
- problem
- frac
- page
- robustness
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We propose a novel evaluation framework that stress-tests LLMs\
  \ on mathematically equivalent but linguistically or parametrically transformed\
  \ versions of competition-level problems. Applying this to all 1,051 Putnam problems\
  \ and generating five variants each, we find that even top models (e.g., OpenAI\u2019\
  s O3) lose 4.7 percentage points on surface-renaming variants and 12.9 points on\
  \ parametric rewrites, confirming fragility beyond memorization."
---

# An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems

## Quick Facts
- arXiv ID: 2508.08833
- Source URL: https://arxiv.org/abs/2508.08833
- Reference count: 40
- We find that even top models (e.g., OpenAI’s O3) lose 4.7 percentage points on surface-renaming variants and 12.9 points on parametric rewrites, confirming fragility beyond memorization.

## Executive Summary
This study introduces a novel evaluation framework to stress-test large language models' mathematical reasoning robustness using mathematically equivalent transformations of 1,051 Putnam Competition problems. By generating five variants of each problem through surface renaming and parametric rewrites, the research reveals that top models like OpenAI's O3 show significant performance degradation - losing 4.7 percentage points on surface-renaming variants and 12.9 points on parametric rewrites. The findings demonstrate that high leaderboard scores do not guarantee robust reasoning capabilities, as systematic perturbations expose fundamental fragility in current LLMs. The authors release the 6,306-question PutnamGAP benchmark and an open-source evaluation stack to facilitate further robustness research.

## Method Summary
The researchers created a comprehensive benchmark by digitizing 1,051 Putnam Competition problems from 1938-2024 using MathPix for LaTeX conversion, then manually validated each problem. They generated four surface variants through symbol renaming (DL, DLC, DLM, GS) using O3 API with collision checks, and one kernel variant via a 4-stage pipeline involving slot discovery, template back-synthesis, reverse-engineering, and dual-verifier screening with five O3 judges. The evaluation involved 18 models tested zero-shot with temperature=0 (O-series at temp=1), max_tokens=32000, using a strict proof grading rubric and lenient numeric grading via LLM grader. Robustness metrics R ∈ (0,1] were computed using SD-normalized per-item drops with exponential penalty, along with McNemar's exact tests for significance.

## Key Results
- O3 model drops 4.7 percentage points on surface-renaming variants
- O3 model drops 12.9 percentage points on parametric rewrites
- All evaluated models show significant performance degradation on mathematically equivalent variants

## Why This Works (Mechanism)
The evaluation framework reveals LLM fragility by systematically varying problem surface features while preserving mathematical content. Surface variants expose sensitivity to symbolic representations through name changes and language shifts, while kernel variants probe deeper reasoning limitations by altering problem structure. The dual-verifier screening ensures variant validity while the strict/lenient grading scheme provides nuanced assessment of proof versus calculation capabilities. Exponential penalization in the R metric amplifies small drops in high-performing models, making fragility detectable even among top systems.

## Foundational Learning
- Putnam Competition problems: Why needed - provides standardized, high-difficulty mathematical reasoning tasks with established difficulty levels; Quick check - verify problems span 1938-2024 with 1-6 difficulty indices
- Mathematically equivalent transformations: Why needed - isolates reasoning from memorization by preserving mathematical content while varying surface features; Quick check - confirm 5 variants generated per original problem
- Robustness metric R: Why needed - quantifies reasoning stability beyond raw accuracy using exponential penalty for drops; Quick check - verify R ∈ (0,1] with Jeffreys smoothing applied
- McNemar's exact test: Why needed - determines statistical significance of performance differences between original and variant problems; Quick check - confirm pairwise comparisons across all model-variant combinations

## Architecture Onboarding
- Component map: Problem Digitization -> Surface Variant Generation -> Kernel Variant Generation -> Model Evaluation -> Robustness Metric Computation -> Statistical Analysis
- Critical path: Variant generation (both surface and kernel) -> Model evaluation (zero-shot inference) -> Robustness computation (R metric + McNemar tests)
- Design tradeoffs: Strict proof grading versus lenient calculation grading balances rigorous assessment with practical feasibility; Exponential penalty in R metric versus linear alternatives affects sensitivity to small drops
- Failure signatures: If R_para ≈ 1 for weak models, check pooled SD isn't near-zero indicating Jeffreys smoothing issues; If surface variants show no degradation, verify collision detection in renaming process
- Three first experiments: 1) Recompute R metrics using linear penalty to validate degradation patterns, 2) Conduct ablation study comparing kernel variant quality with and without dual-verifier screening, 3) Evaluate subset with human graders to validate LLM-based rubrics

## Open Questions the Paper Calls Out
None

## Limitations
- The exponential penalization in R metric may overweight small drops when baseline accuracies are high, potentially exaggerating fragility signals
- Kernel variant generation uses rejection-sampling without specified initial distributions, introducing uncharacterized stochastic variability
- Asymmetric evaluation framework (strict proofs vs lenient calculations) may not reflect real-world mathematical reasoning assessment

## Confidence
- High confidence: LLMs show significant performance degradation on mathematically equivalent variants (verified across multiple models)
- Medium confidence: Specific magnitude of drops (4.7% and 12.9%) due to potential metric sensitivity and stochastic generation
- Medium confidence: High leaderboard scores don't guarantee robust reasoning, given controlled benchmark nature

## Next Checks
1. Recompute robustness metrics using linear instead of exponential penalty to verify if degradation patterns persist
2. Conduct ablation study comparing kernel variant quality with and without dual-verifier screening
3. Evaluate a subset of problems with human graders to validate the LLM-based strict/lenient grading rubric