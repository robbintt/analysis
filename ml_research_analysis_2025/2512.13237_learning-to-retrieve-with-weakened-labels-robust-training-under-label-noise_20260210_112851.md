---
ver: rpa2
title: 'Learning to Retrieve with Weakened Labels: Robust Training under Label Noise'
arxiv_id: '2512.13237'
source_url: https://arxiv.org/abs/2512.13237
tags:
- label
- retrieval
- noise
- https
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a label weakening approach to train robust
  retrieval models under label noise. Instead of enforcing single binary relevance
  labels, it replaces supervision with ambiguated target sets derived from both original
  annotations and model confidence scores.
---

# Learning to Retrieve with Weakened Labels: Robust Training under Label Noise

## Quick Facts
- arXiv ID: 2512.13237
- Source URL: https://arxiv.org/abs/2512.13237
- Authors: Arnab Sharma
- Reference count: 40
- Key outcome: Label weakening improves recall@10 and MRR under label noise compared to 10 state-of-the-art loss functions, particularly for dual bi-encoders.

## Executive Summary
This paper introduces a label weakening approach to train robust retrieval models under label noise by replacing single binary relevance labels with ambiguated target sets derived from both original annotations and model confidence scores. The method constructs possibility distributions over plausibly relevant candidates using threshold-based confidence filtering and optimizes an optimistic superset loss that selects the most favorable positive interpretation within the ambiguated set. Experiments with four retrieval datasets and varying noise ratios show consistent improvements over 10 baseline loss functions, with stronger gains for models less inherently robust to noise like dual bi-encoders.

## Method Summary
The method addresses noisy relevance annotations in dense retrieval by constructing ambiguated target sets R*ᵩ for each query-document pair. These sets contain both annotated positives Rᵩ and additional candidates with model confidence p(d|q) ≥ β, weighted by a possibility distribution πᵩ(d) that equals 1 for included candidates and α < 1 otherwise. The optimistic superset loss Lₐₘᵦ(q) = min_{d⁺∈R*ᵩ} Σ_{d⁻∈N*ᵩ} ℓ(f(q,d⁺), f(q,d⁻)) allows the model to select the best candidate from R*ᵩ for contrastive learning. Training uses delayed ambiguity updates after each epoch with fixed thresholds to prevent early confident mistakes from corrupting supervision.

## Key Results
- Label weakening consistently outperformed 10 baseline loss functions across four datasets and noise ratios 0.0-0.5
- Improvements were particularly pronounced at higher noise ratios (0.4-0.5) for dual bi-encoders
- E5-base-v2 showed smaller relative gains due to its inherent robustness from pre-training
- The method maintained effectiveness across different retrieval architectures including bi-encoders and cross-encoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ambiguated target sets reduce memorization of corrupted labels while preserving discriminative supervision.
- Mechanism: Replace single binary relevance labels with credal sets R*ᵩ ⊇ Rᵩ containing both annotated positives and "plausibly relevant" candidates selected via model confidence p(d|q) ≥ β. A possibility distribution πᵩ(d) bounds the supervision without replacing it, allowing the model to interpret ambiguity rather than commit to potentially wrong labels.
- Core assumption: Model confidence scores above threshold β correlate with true relevance sufficiently often to expand supervision meaningfully rather than inject noise.
- Evidence anchors: [abstract] "Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores." [Section 3] Formal definition of πᵩ(d) with threshold β and relaxation α. [corpus] Related work on label noise shows memorization is the primary failure mode; corpus does not directly validate ambiguation specifically for retrieval.
- Break condition: If β is set too low, spurious high-confidence false positives enter R*ᵩ, amplifying noise rather than mitigating it.

### Mechanism 2
- Claim: Optimistic superset loss enables robust gradient updates by selecting the most favorable positive interpretation within the ambiguated set.
- Mechanism: The loss Lₐₘᵦ(q) = min_{d⁺∈R*ᵩ} Σ_{d⁻∈N*ᵩ} ℓ(f(q,d⁺), f(q,d⁻)) lets the model pick the best candidate from R*ᵩ to contrast against negatives. This "optimism" means gradients flow from the most plausible positive, not from a potentially corrupted annotation.
- Core assumption: At least one truly relevant candidate exists in R*ᵩ with high probability; the min-selection will recover it.
- Evidence anchors: [Section 3] "This formulation allows the model to select the most favorable interpretation among the plausibly relevant candidates." [Section 4.1] On MS MARCO and LCQuAD, LW outperforms alternatives more at noise ratios 0.4–0.5, consistent with optimistic selection helping when corruption is high. [corpus] No direct corpus validation of optimistic loss in retrieval; related LNL work focuses on loss correction, not superset selection.
- Break condition: If R*ᵩ contains no true positives, min-selection optimizes over pure noise, yielding degraded gradients.

### Mechanism 3
- Claim: Delayed ambiguity updates with fixed thresholds prevent runaway self-reinforcement of early confident mistakes.
- Mechanism: Confidence scores derive from epoch-level predictions, recomputed after full data passes (not per batch). Fixed α < 1 and high β exclude low-confidence candidates, functioning as a consistency regularizer rather than self-labeling.
- Core assumption: Early-epoch confidence is miscalibrated but improves sufficiently across epochs that later ambiguation captures meaningful signal.
- Evidence anchors: [Section 3] "(i) Delayed ambiguity update: The confidence scores used to form ambiguated targets are derived from the current epoch's predictions... (ii) Fixed α < 1 and high β: Early predictions below β are not incorporated." [Section 4.1] Gains are stronger for dual bi-encoders (less inherently robust) than for pre-trained E5, suggesting the mechanism compensates for model-specific calibration weaknesses. [corpus] Weak direct evidence; corpus neighbors emphasize pre-training and meta-learning, not epoch-delayed ambiguation.
- Break condition: If model confidence never calibrates (e.g., persistent systematic errors), ambiguity sets freeze around incorrect pseudo-positives.

## Foundational Learning

- Concept: **Credal Sets and Possibility Theory**
  - Why needed here: Label weakening formalizes supervision as a credal set of compatible distributions, not a single point estimate. Understanding this explains why the method bounds uncertainty rather than correcting labels.
  - Quick check question: Can you explain why a possibility distribution π(d) = 1 does not imply probability = 1?

- Concept: **Ranking Loss Functions (Pairwise Contrastive)**
  - Why needed here: The optimistic superset loss builds on pairwise ranking losses (logistic, margin). Without this baseline, the modification via min-selection over R*ᵩ is opaque.
  - Quick check question: Given a query q, a positive d⁺, and two negatives d⁻₁, d⁻₂, write the standard pairwise logistic loss.

- Concept: **Bi-Encoder vs. Cross-Encoder Architectures**
  - Why needed here: The paper shows LW benefits bi-encoders more than cross-encoders. Understanding token-level attention differences explains why cross-encoders already partially internalize uncertainty calibration.
  - Quick check question: Which architecture computes query and document embeddings independently before similarity scoring?

## Architecture Onboarding

- Component map: Retrieval model -> Ambiguity set constructor -> Optimistic loss module -> Epoch-level update loop
- Critical path:
  1. Forward pass: Encode query and candidates, compute scores
  2. Confidence check: Identify candidates with p(d|q) ≥ β
  3. Set construction: Form R*ᵩ = Rᵩ ∪ {high-confidence candidates}
  4. Loss computation: Select best d⁺ via min, contrast against N*ᵩ
  5. Backward pass: Update encoder weights

- Design tradeoffs:
  - Higher β → smaller R*ᵩ, less ambiguity, more conservative (risk: misses true positives)
  - Lower α → stronger relaxation, more gradient regularization (risk: underfits clean signal)
  - Epoch vs. batch update: Epoch prevents batch-level noise amplification but delays adaptation

- Failure signatures:
  - Recall degrades at low noise ratios (<0.1) relative to standard CE → over-regularization
  - Training loss plateaus but validation recall collapses → R*ᵩ corrupted by persistent false positives
  - Cross-encoder shows no gain → method redundant for architectures with intrinsic uncertainty modeling

- First 3 experiments:
  1. Baseline sanity check: Train dual bi-encoder with CE loss on LCQuAD at noise ratios 0.0–0.5. Confirm performance drop correlates with noise level.
  2. Threshold sweep: Fix α = 0.5, vary β ∈ {0.5, 0.7, 0.9} on MS MARCO at noise 0.3. Plot recall@10 vs. β to identify operating region.
  3. Architecture comparison: Apply LW to E5 vs. dual bi-encoder on Mintaka (high annotation ambiguity). Quantify gain differential to validate architecture-dependent benefits claimed in Section 4.1.

## Open Questions the Paper Calls Out

- **Adaptive Weakening Strategies**: The paper explicitly calls for developing adaptive or dynamic weakening strategies where the degree of ambiguity is adjusted based on query difficulty, annotation sparsity, or model uncertainty during training. The current implementation relies on static hyperparameters (α and β), which may not be optimal for all training stages or data instances.

- **Extension to Complex Reasoning Tasks**: The conclusion proposes extending the Label Weakening principle beyond retrieval to question answering and knowledge-grounded reasoning tasks, where supervision quality and semantic ambiguity remain key challenges. The experiments are strictly limited to retrieval and re-ranking tasks using bi-encoders and cross-encoders.

- **Hyperparameter Robustness**: While the paper claims the method reduces complexity compared to approaches requiring tuning, it relies on specific thresholding strategies (fixed α < 1 and high β) without providing a sensitivity analysis. If the performance gains are highly sensitive to precise values of β or α, the practical "robustness" claimed may be diminished.

## Limitations

- The method's effectiveness depends heavily on the calibration of model confidence scores, which is not thoroughly validated across noise levels
- Semantic-aware noise injection process lacks sufficient detail for exact reproduction
- Performance gains are architecture-dependent, showing limited benefit for pre-trained models like E5
- No exploration of ambiguity set size impact on training stability or memory overhead

## Confidence

- **Label weakening improves recall@10 and MRR under label noise (High)**: Supported by direct experimental comparisons across four datasets and multiple noise ratios, with consistent outperformance over 10 baseline losses
- **Optimistic superset loss enables robust gradient updates (Medium)**: Theoretically sound, but corpus lacks direct retrieval-specific validation of min-selection over ambiguated sets
- **Delayed ambiguity updates prevent self-reinforcement of mistakes (Medium)**: Mechanistically plausible and partially supported by stronger gains on less robust architectures, but weak direct empirical validation
- **Architecture-dependent benefits (Medium)**: Gains on bi-encoders vs. cross-encoders are demonstrated, but the explanation relies on architectural assumptions not fully tested across diverse model families

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary β ∈ {0.5, 0.7, 0.9} and α ∈ {0.1, 0.3, 0.5} on MS MARCO at noise ratio 0.3, plotting recall@10 and ambiguity set size to identify optimal operating regions and detect over-regularization or noise amplification.

2. **Architecture Generalization Test**: Apply label weakening to a transformer-based cross-encoder and a late-interaction model (e.g., ColBERT) on Mintaka, comparing gains to those on dual bi-encoders to validate the claimed architecture-dependent benefits.

3. **Calibration Validation**: Measure the alignment between model confidence scores p(d|q) and true relevance at each epoch on AIDA, checking whether confidence improves with training and whether high-β candidates are indeed more likely to be relevant.