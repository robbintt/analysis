---
ver: rpa2
title: Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous
  Emergency Braking in Occluded Pedestrian Crossing
arxiv_id: '2504.08704'
source_url: https://arxiv.org/abs/2504.08704
tags:
- reward
- vehicle
- pedestrian
- offline
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for generating human-aligned
  reward labels to enable real-world driving datasets to be used for Offline Reinforcement
  Learning (RL) training. The approach introduces an adaptive safety component using
  semantic segmentation maps to assess critical risk factors in driving scenes, allowing
  autonomous vehicles to prioritize safety over efficiency in potential collision
  scenarios.
---

# Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing

## Quick Facts
- arXiv ID: 2504.08704
- Source URL: https://arxiv.org/abs/2504.08704
- Reference count: 40
- Primary result: Novel method for generating human-aligned reward labels using semantic segmentation maps and adaptive safety components for offline RL training in occluded pedestrian crossing scenarios

## Executive Summary
This paper presents a novel method for generating human-aligned reward labels to enable real-world driving datasets to be used for Offline Reinforcement Learning (RL) training. The approach introduces an adaptive safety component using semantic segmentation maps to assess critical risk factors in driving scenes, allowing autonomous vehicles to prioritize safety over efficiency in potential collision scenarios. The method applies varying levels of spatial attention to different objects in image observations, improving decision-making for vehicle longitudinal control tasks. When evaluated in an occluded pedestrian crossing scenario with varying pedestrian traffic levels using CARLA simulation, the generated reward labels closely matched simulation rewards. Training various Offline RL algorithms with these labels produced competitive results compared to other baselines, demonstrating the effectiveness of the approach in producing reliable and human-aligned reward signals for autonomous driving systems.

## Method Summary
The method generates human-aligned reward labels for autonomous driving through a pipeline that combines semantic segmentation with adaptive safety mechanisms. First, a ResNet18 UNet model processes grayscale camera images to produce 28-class semantic segmentation maps. These maps are analyzed to compute three risk factors: pedestrian presence on road surfaces, zebra crossing occlusion status, and pedestrian disappearance events. The risk factors are combined and passed through a scaled sigmoid function to determine an adaptive safety flag that activates when risk exceeds a threshold. This flag switches between safety and efficiency reward components. Spatial attention is then applied by weighting semantic layers with task-specific multipliers and integrating them with encoder features. Finally, various Offline RL algorithms (IQL, CQL, BPPO) are trained using these generated reward labels to produce policies for vehicle longitudinal control.

## Key Results
- Reward labels generated by the proposed method closely matched simulation rewards in CARLA occluded pedestrian crossing scenarios
- Trained Offline RL policies achieved competitive performance compared to baselines across varying pedestrian traffic densities
- The adaptive safety component effectively prioritized safety over efficiency in high-risk scenarios
- Spatial attention applied to semantic segmentation maps improved decision-making for longitudinal control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An adaptive safety component enables the reward function to dynamically shift priority between safety and efficiency based on detected risk factors.
- Mechanism: Semantic segmentation maps are analyzed to compute three risk factors—pedestrian presence on road surfaces, zebra crossing occlusion status, and pedestrian disappearance events. These are summed and passed through a scaled sigmoid function. When the resulting probability exceeds threshold ψ=0.75, the safety reward component activates while the efficiency component deactivates.
- Core assumption: The three defined risk factors are sufficient proxies for collision probability in occluded pedestrian crossing scenarios.
- Evidence anchors:
  - [abstract]: "pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios"
  - [Section 3.2]: details the three risk factors (Fp, Fc, Fh) and threshold mechanism with ψ=0.75
  - [corpus]: limited direct validation; related work on offline RL for AD (arXiv:2512.18662) addresses distributional shift but does not propose adaptive safety switching
- Break condition: If scene complexity increases beyond three risk factors (e.g., multiple interacting pedestrians, cyclists), the mechanism may fail to capture critical hazards, as evidenced by performance degradation under high pedestrian density (collision test: IQL success dropped from 98% to 91% in full occlusion).

### Mechanism 2
- Claim: Semantic segmentation maps provide a grounded intermediate representation for both reward labeling and spatial attention, enabling consistent human-aligned decision-making.
- Mechanism: A ResNet18 UNet model processes 1×224×224 grayscale images to produce 28-class semantic maps (dimensions 28×224×224). Specific class layers (pedestrian, zebra crossing, vehicle, road, sidewalk) are isolated for downstream tasks—reward calculation and feature attention weighting.
- Core assumption: Real-time semantic segmentation accuracy is sufficient for safety-critical reward assignment; segmentation errors do not cascade catastrophically.
- Evidence anchors:
  - [abstract]: "leverage semantic segmentation maps to assess critical risk factors, such as pedestrian locations and zebra crossing presence"
  - [Section 3.1]: describes UNet architecture with 28-class output and its use for scene understanding
  - [corpus]: weak validation; corpus papers discuss perception pipelines but do not evaluate semantic-map-based reward labeling specifically
- Break condition: Segmentation failures on occluded or partially visible pedestrians (A2D2 experiment showed the method missed a visible crossing pedestrian due to pixel classification confusion at road/road-marking boundaries—Figure 11b).

### Mechanism 3
- Claim: Spatial attention applied to latent feature embeddings improves learning efficiency by emphasizing task-relevant objects.
- Mechanism: Semantic layers (Zped, Zcross, Zveh) are resized and cross-wise multiplied with the encoder feature cube using class-specific weights (ωped=1.0, ωcross=0.75, ωveh=0.50). The weighted features are added back to the original feature cube before policy training.
- Core assumption: The hand-specified weighting scheme reflects true task relevance across all driving scenarios.
- Evidence anchors:
  - [abstract]: "method to use semantic segmentation maps to apply varying levels of spatial attention to different objects"
  - [Section 4.1, Equations 6a-7]: formalizes cross-wise multiplication and addition operations for spatial attention
  - [corpus]: no direct comparison; attention mechanisms for AD appear in OmniDrive-R1 (arXiv:2512.14044) but use VLM-based reasoning rather than semantic-map weighting
- Break condition: If object relevance shifts (e.g., vehicle behavior becomes more critical than pedestrian in highway scenarios), fixed weights become suboptimal.

## Foundational Learning

- Concept: **Offline Reinforcement Learning fundamentals (MDP formulation, distributional shift, behavioral cloning baseline)**
  - Why needed here: The paper assumes familiarity with why offline RL requires reward labels and how distributional shift affects policy learning without environment interaction.
  - Quick check question: Can you explain why assigning zero rewards to unlabeled data (UDS baseline) causes Bellman backup errors in algorithms like IQL and CQL?

- Concept: **Semantic Segmentation (UNet architecture, class-wise layer extraction)**
  - Why needed here: The entire pipeline depends on understanding how segmentation maps are generated and how individual class channels can be isolated.
  - Quick check question: Given a 28×224×224 semantic output, how would you extract and visualize only the pedestrian class layer?

- Concept: **Reward Shaping for Safety-Critical Systems**
  - Why needed here: The reward function combines safety, efficiency, and smoothness components with indicator functions that switch based on context.
  - Quick check question: Why might a fixed safety penalty weight ζ cause issues across varying pedestrian traffic densities?

## Architecture Onboarding

- Component map: Input (1×224×224 grayscale image + vehicle kinematics) → Semantic Branch (ResNet18 UNet → 28×224×224 semantic map) → Reward Labeling (semantic layers → risk factor calculation → adaptive safety flag c_t → reward function) → Policy Branch (ResNet18 autoencoder encoder → spatial attention (semantic-weighted) → offline RL algorithm → longitudinal action [-1, 1])

- Critical path: Semantic segmentation accuracy → risk factor detection → c_t threshold crossing → reward label quality → policy convergence. The A2D2 experiment showed this chain breaks if segmentation misclassifies pedestrian-adjacent pixels.

- Design tradeoffs:
  - Grayscale vs RGB input: Reduced computation but potentially lower segmentation accuracy
  - Fixed vs learned attention weights: Hand-specified weights (ωped=1.0, etc.) are interpretable but may not generalize; learning weights requires additional supervision
  - UNet for both labeling and training: Real-time capable but segmentation quality may limit reward accuracy; a larger model could be used for offline labeling only

- Failure signatures:
  - High stall rates (12-18% in IQL-VLM full occlusion): reward labels fail to capture intermediate states between safe/unsafe
  - Aggressive policies with short stopping distances (BPPO-VLM: 1.08-1.14m): reward distribution too sparse or incorrectly scaled
  - Human-method disagreement on safety labels (A2D2): 28.9% agreement on unsafe samples; method flagged occluded pedestrians humans missed but missed visible pedestrians at ambiguous boundaries

- First 3 experiments:
  1. **Segmentation sensitivity test**: Inject controlled segmentation noise (mask occlusion, class confusion) on a held-out trajectory; measure resulting reward label variance and policy performance change. Baseline: 0% noise vs 5%/10%/20% noise levels.
  2. **Threshold ablation (ψ tuning)**: Sweep ψ from 0.5 to 0.9 in 0.05 increments on the CARLA high-density scenario; plot success rate vs timeout rate to find the safety-efficiency Pareto frontier.
  3. **Weight sensitivity analysis**: Vary attention weights (ωped, ωcross, ωveh) ±25% from baseline; train IQL policy and measure stopping distance distribution to assess if learned features are robust to weight perturbation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating semantic segmentation maps from multiple camera feeds create a robust symbolic representation that mitigates performance degradation in highly congested environments?
- Basis in paper: [explicit] The authors state in Section 7 that the pipeline "could be enhanced by integrating multiple camera feeds" to overcome limitations of the current front-facing setup and build symbolic representations.
- Why unresolved: The current pipeline relies solely on front-facing images, causing performance drops in high pedestrian traffic due to limited spatial context.
- What evidence would resolve it: Comparative collision tests in high-density traffic scenarios using single-view vs. multi-view semantic maps.

### Open Question 2
- Question: Can the adaptive safety component be improved by replacing the simple memory bank with sophisticated feature extractors like video transformers to better isolate social cues?
- Basis in paper: [explicit] Section 7 suggests incorporating temporal context via "video transformers and a memory bank designed to isolate key social cues" to improve the pipeline's reasoning.
- Why unresolved: The current 10-cell memory bank is simplistic and may not fully capture complex pedestrian dynamics or social interactions required for high-level safety decisions.
- What evidence would resolve it: Evaluations comparing risk factor accuracy and policy safety when using transformer-based temporal feature extractors versus the current method.

### Open Question 3
- Question: How can offline policies be effectively evaluated for alignment with human reasoning without costly real-world deployment or simplistic label matching?
- Basis in paper: [explicit] Section 7 highlights the "urgent need for more sophisticated evaluation criteria" and notes that Offline Policy Evaluation (OPE) remains a "persistent challenge" in verifying safety before deployment.
- Why unresolved: Current offline evaluation relies on matching predicted outputs to human labels, which is too simplistic to capture the nuances of human judgment.
- What evidence would resolve it: The development of a robust OPE technique that accurately predicts real-world safety outcomes from static datasets.

## Limitations

- Reward function hyperparameters (ζ, μ, ξ, η, ε) and risk factor numerical thresholds are not fully specified, limiting reproducibility
- The semantic segmentation accuracy of 91.4% IoU is not validated specifically against the occlusion scenarios tested
- Real-world generalization claims are not fully substantiated, with significant disagreement between method's safety labels and human annotations in A2D2 experiment

## Confidence

- **High Confidence**: The core mechanism of using semantic segmentation maps to extract risk factors and generate reward labels is well-specified and experimentally validated in controlled CARLA scenarios.
- **Medium Confidence**: The claim that human-aligned rewards improve policy safety is supported by comparative results, though the human-study sample size (n=5) is small.
- **Low Confidence**: Generalization claims to real-world data (A2D2 experiment) are not fully substantiated, with significant disagreement between the method's safety labels and human annotations.

## Next Checks

1. **Segmentation Robustness Test**: Systematically inject controlled occlusion and class confusion into segmentation maps to quantify the sensitivity of reward labels and policy performance.

2. **Cross-Density Threshold Analysis**: Conduct a comprehensive sweep of the safety-efficiency threshold (ψ) across all pedestrian densities to identify optimal operating points and failure regimes.

3. **Real-World Generalization Study**: Expand the A2D2 experiment to include a larger, diverse set of real-world scenarios with varying weather and lighting conditions to assess true robustness.