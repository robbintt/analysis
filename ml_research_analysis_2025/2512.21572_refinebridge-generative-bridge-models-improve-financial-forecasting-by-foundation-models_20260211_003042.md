---
ver: rpa2
title: 'RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation
  Models'
arxiv_id: '2512.21572'
source_url: https://arxiv.org/abs/2512.21572
tags:
- refinebridge
- tsfms
- tsfm
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RefineBridge improves financial time series forecasting by treating\
  \ transformer-based foundation model predictions as priors and iteratively refining\
  \ them using a tractable Schr\xF6dinger Bridge framework. It learns context-conditioned\
  \ stochastic transport maps to progressively align coarse TSFM forecasts with ground-truth\
  \ data."
---

# RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models

## Quick Facts
- arXiv ID: 2512.21572
- Source URL: https://arxiv.org/abs/2512.21572
- Reference count: 0
- RefineBridge outperforms LoRA-based fine-tuning on financial forecasting, achieving 11%-71% MSE reductions across assets and horizons.

## Executive Summary
RefineBridge introduces a generative bridge framework that improves financial time series forecasting by treating transformer-based foundation model (TSFM) predictions as priors and iteratively refining them using a tractable Schrödinger Bridge framework. The method learns context-conditioned stochastic transport maps to progressively align coarse TSFM forecasts with ground-truth data. Evaluated on S&P 500, WTI crude oil, and EUR/USD datasets, RefineBridge consistently outperformed LoRA-based fine-tuning, achieving significant MSE reductions across multiple assets and horizons while using fewer parameters than LoRA adapters.

## Method Summary
RefineBridge refines TSFM predictions by learning stochastic transport maps via Schrödinger Bridge theory. It encodes historical context using DLinear decomposition, samples from bridge marginals using noise schedulers, and denoises via a 1D U-Net conditioned on both the prior forecast and context encoding. The framework uses different sampling strategies (ODE vs SDE) depending on prediction horizon, with temperature tuning for longer horizons to allow stochastic exploration when priors deviate substantially.

## Key Results
- Outperformed LoRA-based fine-tuning on 81 of 90 test configurations
- Achieved MSE reductions of 11%-71% across S&P 500, WTI crude oil, and EUR/USD datasets
- Improved state-of-the-art TSFMs while using fewer parameters (2.3M vs 2.4–7.6M LoRA adapters)
- Performance advantages more pronounced at longer horizons (62% at 126 days vs 11% at 5 days)

## Why This Works (Mechanism)

### Mechanism 1: Data-to-Data Stochastic Transport
- Claim: Schrödinger Bridge enables learning optimal transport maps directly from TSFM predictions (prior) to ground truth (target), bypassing the noise-to-data paradigm of standard diffusion.
- Mechanism: The bridge marginal distribution interpolates between TSFM forecast and ground truth via Gaussian transitions controlled by variance schedule. The model learns to reverse this process using conditional denoiser.
- Core assumption: TSFM forecasts contain exploitable coarse structure even when inaccurate.
- Evidence anchors: Abstract states learns context-conditioned stochastic transport maps to progressively align coarse TSFM forecasts with ground-truth data; section 3 notes SB-based models can capture target distribution from clean prior.
- Break condition: If TSFM priors are adversarial (systematically opposite to ground truth), transport maps may fail to converge.

### Mechanism 2: Context-Conditioned Denoising with Complementary Objective
- Claim: By conditioning refinement on encoded historical context and using a denoising loss distinct from TSFM objectives, RefineBridge captures fine-grained financial dynamics that TSFMs miss.
- Mechanism: DLinear encoder decomposes context into trend/seasonal components, producing context encoding that 1D U-Net denoiser uses alongside prior forecast.
- Core assumption: Context encoding preserves regime-specific information that TSFMs fail to leverage for prediction.
- Evidence anchors: Abstract mentions treating TSFM forecasts as priors and iteratively refining them using tractable Schrödinger Bridge framework; section 3.2 notes RefineBridge differs from LoRA-based approaches by introducing different training objective.
- Break condition: If context encoder fails to capture regime shifts, conditioning provides no signal.

### Mechanism 3: Horizon-Adaptive Sampling Strategy
- Claim: Matching sampler type and step count to prediction horizon optimizes tradeoff between prior preservation (short horizons) and correction magnitude (long horizons).
- Mechanism: ODE sampling with 1–10 steps for short horizons preserves informative priors; SDE sampling with 10–1000 steps and temperature tuning for long horizons allows stochastic exploration.
- Core assumption: TSFM error accumulates autoregressively, making long-horizon forecasts increasingly correctable.
- Evidence anchors: Section 3.3 states for short prediction horizons ODE sampling with fewer denoisers steps, for longer horizons SDE sampling with temperature tuning achieves best refining performance; section 4.3 shows performance advantages more pronounced at longer horizons.
- Break condition: If TSFM exhibits non-autoregressive error patterns, adaptive strategy may be suboptimal.

## Foundational Learning

- **Schrödinger Bridge Theory**:
  - Why needed here: Core mathematical framework; distinguishes from diffusion models by enabling data-to-data transport under entropy-regularized optimal transport.
  - Quick check question: Given two distributions $p_{prior}$ and $p_{data}$, what does the SB solution minimize? (Answer: KL divergence between path measure and reference Brownian bridge.)

- **Score-Based Generative Models (SGMs)**:
  - Why needed here: RefineBridge builds on SGM foundations; understanding SDE/ODE samplers and denoising score matching is prerequisite.
  - Quick check question: What role does the noise schedule $\beta_t$ play in controlling tradeoff between prior fidelity and target alignment?

- **Time Series Decomposition (Trend-Seasonal)**:
  - Why needed here: DLinear encoder preprocesses context into components; understanding decomposition aids debugging encoder failures.
  - Quick check question: If financial data lacks stable seasonality, what artifacts might DLinear introduce in context encoding?

## Architecture Onboarding

- **Component map**: Frozen TSFM backbone -> DLinear context encoder -> Noise scheduler (Bridge-gmax) -> 1D U-Net denoiser -> ODE/SDE sampler
- **Critical path**: Context encoding quality -> denoiser capacity to leverage context -> sampler choice matching horizon characteristics
- **Design tradeoffs**:
  - ODE vs SDE: Determinism vs stochastic exploration; ODE faster, SDE better for degraded priors
  - Noise schedule $\beta_0, \beta_1$: Aggressive (0.01, 50) for short horizons; conservative (0.0001, 0.02) for long horizons
  - Parameter count: 2.3M RefineBridge vs 2.4–7.6M LoRA adapters; efficiency vs expressiveness
- **Failure signatures**:
  - LoRA degradation pattern: MSE increases on 29/45 configs -> overfitting to noise
  - RefineBridge underperformance: EURUSD $H=5$ shows MSE 0.333 vs original 0.295 -> potential prior mismatch or insufficient refinement steps
  - Sampling divergence: If temperature too high, trajectory oscillates; if too low, collapses to prior
- **First 3 experiments**:
  1. Baseline replication: Run Chronos-large on S&P 500 with $H \in \{5, 63\}$; verify mean-reverting behavior and MSE degradation at long horizons
  2. Ablation on sampler type: Compare ODE (10 steps) vs SDE ($\tau=0.5$, 100 steps) for $H=63$ on WTI; expect SDE to show larger MSE reduction per Section 3.3
  3. Context encoder swap: Replace DLinear with identity encoding; measure MSE delta on EURUSD to quantify contribution of trend-seasonal decomposition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the empirical bounds on TSFM prior quality below which RefineBridge fails to provide meaningful refinement?
- Basis in paper: Remark 1 states RefineBridge refines less informative or even false (low-quality) priors from TSFMs and still manages to improve the forecasting performance, but no controlled degradation study is provided.
- Why unresolved: Only qualitative robustness claims are made without systematic evaluation of prior quality thresholds.
- What evidence would resolve it: Experiments with synthetic priors of controlled corruption levels, identifying the SNR or error threshold where refinement gains vanish.

### Open Question 2
- Question: How does RefineBridge generalize to non-financial domains where TSFMs also struggle (e.g., energy, traffic, or physiological signals)?
- Basis in paper: The framework is presented as a general refinement paradigm but evaluated exclusively on three financial assets.
- Why unresolved: No experiments on other challenging time series domains are presented; the approach may be overfit to financial statistical properties.
- What evidence would resolve it: Systematic evaluation on diverse non-financial benchmarks with similar non-stationarity or heavy-tailed characteristics.

### Open Question 3
- Question: What is the inference computational overhead of RefineBridge compared to LoRA, particularly for SDE sampling with 10-1000 steps at long horizons?
- Basis in paper: The paper notes SDE sampling requires 10-1000 denoising steps for longer horizons but reports no wall-clock time, latency, or FLOPs comparisons.
- Why unresolved: Practical deployment feasibility requires understanding the efficiency-accuracy tradeoff.
- What evidence would resolve it: Detailed inference time benchmarks and computational cost analysis across sampling strategies and horizon lengths.

### Open Question 4
- Question: How does the data-to-data Schrödinger Bridge approach compare empirically to standard noise-to-data diffusion for this refinement task?
- Basis in paper: The paper motivates SB over diffusion because it learns data-to-data transport maps but provides no ablation against standard diffusion baselines.
- Why unresolved: The theoretical advantage is clear but the practical benefit over simpler diffusion alternatives remains unquantified.
- What evidence would resolve it: Ablation study comparing RefineBridge against a diffusion model that refines from Gaussian noise conditioned on the same context and prior.

## Limitations
- Empirical claims rest entirely on single in-house dataset evaluation without external validation
- No stress tests under distributional shift or ablation on hyperparameters reported
- Bridge formalism used as training framework rather than principled alignment tool
- Context encoding contribution not proven necessary through ablation

## Confidence
- **High**: Empirical MSE reductions (11%–71%) and parameter efficiency (2.3M vs 2.4–7.6M LoRA) are directly measurable from reported tables
- **Medium**: Superiority over LoRA in 81/90 configurations is well supported, but claim that this generalizes to other TSFMs or domains lacks external validation
- **Low**: Assertion that Schrödinger Bridge uniquely enables data-to-data transport that diffusion models cannot achieve is theoretically plausible but not experimentally distinguished

## Next Checks
1. **External Dataset Test**: Apply RefineBridge to a public benchmark (e.g., M4 or tourism forecasting data) to verify cross-dataset generalization and check for overfitting to proprietary S&P 500/WTI/EURUSD split
2. **Ablation of Context Encoder**: Replace DLinear with simple linear projection or identity mapping and measure MSE change across horizons to isolate contribution of trend-seasonal decomposition
3. **Sampler Hyperparameter Sweep**: Systematically vary ODE step counts and SDE temperature across full horizon range (5–126 days) to identify whether heuristic sampling strategy is optimal or merely sufficient