---
ver: rpa2
title: Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation
arxiv_id: '2511.02358'
source_url: https://arxiv.org/abs/2511.02358
tags:
- augmentation
- query
- queries
- m-solomon
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: M-Solomon addresses the limitations of query augmentation in multimodal
  retrieval by adaptively determining when to augment queries rather than applying
  augmentation uniformly. The method first identifies which training datasets benefit
  from augmentation, then uses a Multimodal LLM to synthesize appropriate augmentations
  for those queries.
---

# Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation

## Quick Facts
- arXiv ID: 2511.02358
- Source URL: https://arxiv.org/abs/2511.02358
- Reference count: 32
- Primary result: M-Solomon achieves 67.6% Precision@1 on MMEB benchmark with 50% latency reduction

## Executive Summary
M-Solomon addresses the limitations of query augmentation in multimodal retrieval by adaptively determining when to augment queries rather than applying augmentation uniformly. The method first identifies which training datasets benefit from augmentation, then uses a Multimodal LLM to synthesize appropriate augmentations for those queries. During training, M-Solomon learns to generate either an augmentation token or a simple embed token at the beginning of each query, enabling adaptive decision-making. The approach outperforms both non-augmentation and always-augmentation baselines on the MMEB benchmark while reducing embedding latency by approximately 50%.

## Method Summary
M-Solomon uses Qwen2-VL-7B-Instruct with LoRA (rank 16) to learn adaptive query augmentation. The model is trained with a dual objective: contrastive learning (L_rep) with hard negatives and autoregressive generation (L_gen) of prefix tokens (`/augment` + synthesis or `/embed`). Training data is divided at the dataset level into 10 requiring augmentation and 10 not requiring augmentation based on pilot study performance. Augmentations are synthesized using Qwen2.5-VL-72B-Instruct with chain-of-thought prompting, extracting only the answer portion. The model generates the prefix token first, determining whether to perform augmentation before embedding.

## Key Results
- M-Solomon achieves 67.6% Precision@1 overall on MMEB benchmark
- Outperforms non-augmentation baseline (66.1%) and always-augmentation baseline (67.4%)
- Reduces embedding latency by ~50% compared to always-augmentation approach
- Maintains strong generalization on 16 out-of-distribution datasets (65.4% vs 64.7% for AlwaysAug)

## Why This Works (Mechanism)

### Mechanism 1
Dataset-level binary classification of augmentation benefit enables cleaner training signal than query-level decisions. The authors ran pilot experiments comparing NoAug vs AlwaysAug models on 20 MMEB datasets, labeling datasets where NoAug performed ≥AlwaysAug as "not requiring augmentation" (10 datasets), others as "requiring augmentation" (10 datasets). This creates unambiguous training labels `/augment` vs `/embed` without per-query ambiguity. Core assumption: Dataset-level homogeneity exists—queries within a dataset share similar augmentation needs.

### Mechanism 2
Prefix-token decision (`/augment` vs `/embed`) forces the model to commit to an augmentation strategy before generating content, reducing post-hoc rationalization. During training, the model learns to generate `/augment` prefix followed by synthesized augmentation text for D^A datasets, or `/embed` token alone for D^E datasets. At inference, the first generated token determines behavior. Core assumption: The model can learn a reliable boundary between augmentation-helpful and augmentation-harmful queries from dataset-level labels alone.

### Mechanism 3
Answer-style augmentation provides denser semantic signal than raw queries, but only when the answer is accurate and non-hallucinated. A teacher MLLM generates answers to queries using chain-of-thought prompting, with only the answer (not reasoning) extracted as augmentation. When accurate, it disambiguates underspecified queries. Core assumption: Teacher model outputs are factually correct and relevant to retrieval.

## Foundational Learning

- Concept: Contrastive learning with hard negatives
  - Why needed here: L_rep uses positive documents and hard negative documents for discriminative pressure
  - Quick check question: Given an anchor query "red shoes," would "blue shoes" or "red car" be a better hard negative, and why?

- Concept: Autoregressive language modeling with teacher forcing
  - Why needed here: L_gen trains the model to generate `/augment` + augmentation or `/embed`
  - Quick check question: If the model generates `/augment` but then produces incoherent text, which part of the training objective is failing?

- Concept: Multi-task loss balancing (α_rep vs α_gen)
  - Why needed here: The paper uses α_rep=1.0 and α_gen=0.1, reflecting representation learning as primary objective
  - Quick check question: If you increase α_gen to 1.0, would you expect more or fewer `/augment` decisions, and what tradeoff might emerge?

## Architecture Onboarding

- Component map: Input Query (+ Image) -> Vision Encoder (Qwen2-VL) -> Autoregressive Head -> Generate /augment or /embed -> If /augment: Continue generating augmentation text, If /embed: Skip augmentation -> Concatenate Query + Augmentation -> Re-encode for Embedding (EOS token hidden state) -> Contrastive Retrieval

- Critical path: The augmentation decision happens in a single forward pass. The model must commit to `/augment` or `/embed` at the first token. If this decision is wrong, downstream embedding quality degrades. Monitor confidence scores (C_F) as a health metric.

- Design tradeoffs:
  - Dataset-level vs query-level labeling: Dataset-level is simpler but sacrifices precision
  - Teacher model size vs cost: Qwen2.5-VL-72B for synthesis vs smaller models
  - LoRA rank (16): Enables efficient training but may underfit on generation task

- Failure signatures:
  - `/embed%` near 0%: Model always augments; decision mechanism failed
  - `/embed%` near 100%: Model never augments; augmentation signal too weak
  - High latency with low `/embed%`: Augmentation text is too long
  - C_F scores near 50%: Model is uncertain

- First 3 experiments:
  1. Reproduce the pilot study: Train NoAug and AlwaysAug baselines, compare per-dataset performance
  2. Ablate the teacher model: Replace Qwen2.5-VL-72B with smaller model, measure augmentation quality and performance
  3. Test query-level labeling: For one dataset where NoAug ≈ AlwaysAug, manually label 100 queries as augmentation-helpful or harmful

## Open Questions the Paper Calls Out

1. Can query-level identification of augmentation needs outperform dataset-level heuristics, and what signals best predict whether an individual query benefits from augmentation? The authors state: "In the future, we will study methods to identify which queries require augmentation at the query level because this allows for precise decisions by reflecting fine-grained information of each query."

2. How can adaptive query augmentation be extended to support reasoning-intensive embedding tasks with a third reasoning-based augmentation option? The authors state: "Furthermore, we will extend adaptive query augmentation with another option that performs reasoning-based query augmentation for reasoning-intensive embedding tasks such as BRIGHT [18] and RAR-b [23]."

3. How sensitive is M-Solomon's performance to the choice of teacher model used for augmentation synthesis? The paper relies exclusively on Qwen2.5-VL-72B-Instruct but does not analyze whether different teacher models would produce comparable results.

## Limitations

- Dataset-level decision granularity may miss query-level heterogeneity where some queries within a dataset benefit from augmentation while others are harmed
- Teacher model augmentation quality is unverified at scale; hallucinated or irrelevant answers could introduce noise
- Hard negative selection strategy is unspecified, making reproduction and benchmarking difficult

## Confidence

- High Confidence (95%+): Prefix-token decision-making mechanism is well-specified and theoretically sound
- Medium Confidence (70-90%): Dataset-level pilot study methodology appears reproducible but assumes within-dataset homogeneity
- Low Confidence (30-70%): Effectiveness of answer-style augmentation depends entirely on teacher model quality, which lacks systematic evaluation

## Next Checks

1. Query-level augmentation benefit analysis: For HatefulMemes dataset, manually label 200 queries as augmentation-helpful or harmful and compare M-Solomon's decisions against ground truth

2. Teacher model ablation study: Replace Qwen2.5-VL-72B-Instruct with Qwen2.5-VL-7B for synthesis, measure augmentation quality through human evaluation and downstream performance

3. Cross-dataset generalization stress test: Train M-Solomon on only 5 augmentation-beneficial datasets and evaluate on remaining 5, measure degradation in `/embed%` rates and confidence scores