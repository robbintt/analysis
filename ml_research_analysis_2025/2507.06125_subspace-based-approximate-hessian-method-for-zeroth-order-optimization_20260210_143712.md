---
ver: rpa2
title: Subspace-based Approximate Hessian Method for Zeroth-Order Optimization
arxiv_id: '2507.06125'
source_url: https://arxiv.org/abs/2507.06125
tags:
- hessian
- optimization
- function
- gradient
- zeroth-order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO-SAH, a zeroth-order optimization method
  that improves convergence by incorporating second-order curvature information while
  minimizing function evaluations. The key innovation is to restrict optimization
  to randomly selected two-dimensional subspaces, where the Hessian is estimated by
  fitting a quadratic polynomial and extracting second-order coefficients.
---

# Subspace-based Approximate Hessian Method for Zeroth-Order Optimization

## Quick Facts
- **arXiv ID**: 2507.06125
- **Source URL**: https://arxiv.org/abs/2507.06125
- **Reference count**: 40
- **Primary result**: ZO-SAH achieves up to 50% fewer function queries than state-of-the-art zeroth-order methods while maintaining competitive or better test accuracy

## Executive Summary
This paper introduces ZO-SAH, a zeroth-order optimization method that improves convergence by incorporating second-order curvature information while minimizing function evaluations. The key innovation is to restrict optimization to randomly selected two-dimensional subspaces, where the Hessian is estimated by fitting a quadratic polynomial and extracting second-order coefficients. Function evaluations are reused across iterations via a periodic subspace-switching strategy, reducing query costs. Experiments on eight benchmark datasets, including logistic regression and deep neural network training, show that ZO-SAH consistently converges faster than existing zeroth-order methods.

## Method Summary
ZO-SAH operates by projecting the optimization problem onto randomly selected two-dimensional subspaces. Within each subspace, it estimates the gradient using coordinate-wise finite differences and approximates the Hessian by fitting a quadratic polynomial via least squares. The method enforces positive definiteness through eigenvalue clipping and aggregates updates across subspaces. A periodic subspace-switching strategy allows reuse of function evaluations, reducing the amortized query cost per optimization step. The approach combines the curvature information of second-order methods with the query efficiency of first-order techniques.

## Key Results
- Achieves up to 50% fewer function queries than state-of-the-art zeroth-order methods
- Consistently converges faster than existing zeroth-order methods on 8 benchmark datasets
- Maintains competitive or better test accuracy compared to baseline approaches
- Particularly effective on anisotropic functions where second-order information provides substantial benefits

## Why This Works (Mechanism)

### Mechanism 1: Two-Dimensional Subspace Restriction
Restricting Hessian estimation to randomly selected two-dimensional subspaces reduces computational complexity from O(d²) to O(1) while retaining meaningful curvature information. At each iteration, select an intermediate subspace V of dimension m from the full d-dimensional parameter space, then partition V into m/2 disjoint 2D subspaces {W₁, ..., W_{m/2}}. Within each 2D subspace, the symmetric 2×2 Hessian requires only three unique coefficients, enabling efficient estimation and inversion. Core assumption: Parameter coupling captured in two-dimensional subspaces is sufficient to approximate the optimization benefits of full Hessian information. (Assumption: Not proven for all problem types; break condition identified.)

### Mechanism 2: Quadratic Polynomial Fitting for Hessian Estimation
Fitting a quadratic polynomial model to local function samples provides stable Hessian estimates that enable evaluation reuse across iterations. Within each 2D subspace, construct a quadratic model p(θ) = ½θᵀAθ + bᵀθ + c. The Hessian matrix A is extracted from second-order coefficients via regularized least squares. The design matrix Φ encodes second-order monomials, and the solution h* = (ΦᵀΦ)⁻¹Φᵀq yields the three unique Hessian entries. Core assumption: The objective function is locally well-approximated by a quadratic model within perturbation radius l, with bounded Taylor remainder.

### Mechanism 3: Periodic Subspace-Switching with Evaluation Caching
Retaining subspaces across multiple iterations enables reuse of function evaluations, reducing amortized query cost per optimization step. Subspaces V and {Wⱼ} are updated only every T-th step. At intermediate steps (k mod T ≠ 0, 1), evaluations from the two preceding steps are reused: Θₖ = {θ₁^{k-2}, θ₂^{k-2}, θ₁^{k-1}, θ₂^{k-1}}. New evaluations are sampled only when k mod T = 0. Core assumption: The optimization landscape remains sufficiently stationary within T iterations such that cached evaluations remain informative for Hessian estimation.

## Foundational Learning

- **Finite-Difference Gradient Approximation**: ZO-SAH builds on standard ZO gradient estimation before incorporating second-order information. Understanding bias-variance trade-offs in perturbation size ϵ is critical for debugging. Quick check: Why does the gradient error bound scale with the Lipschitz constant C₁, and what happens if ϵ is set too large?

- **Newton's Method and Inverse Hessian Scaling**: The core intuition is that H⁻¹ amplifies search along low-curvature directions. Understanding this explains why ZO-SAH excels on anisotropic functions. Quick check: For quadratic model with A = diag([10, 1]), explain why Newton's method takes larger steps along the θ₂ direction than gradient descent.

- **Eigenvalue Regularization for Non-Convex Optimization**: ZO-SAH enforces positive definiteness by taking absolute values of eigenvalues and applying clipping with floor κ=0.1. This prevents ascent directions in non-convex regions. Quick check: If the estimated 2D Hessian has eigenvalues {-2, 3}, what search direction results without regularization, and how does the absolute-value transform fix this?

## Architecture Onboarding

- **Component map**: Subspace Selector -> Evaluation Cache -> Gradient Estimator -> Hessian Estimator -> Eigenvalue Processor -> Update Aggregator -> parameter update -> cache update
- **Critical path**: Subspace selection (every T steps) → Evaluation Cache lookup → Gradient Estimator → Hessian Estimator (with cache) → Eigenvalue Processor → Update Aggregator → parameter update → cache update
- **Design tradeoffs**:
  1. Subspace dimension m: Larger m captures more parameter interactions but increases per-step evaluations
  2. Switching period T: T=20 balances cache hit rate vs. landscape adaptation
  3. Perturbation radius l: Controls Hessian estimation error; trade-off between numerical stability and model fidelity
  4. Eigenvalue floor κ: κ=0.1 prevents ill-conditioning but may underweight high-curvature directions
- **Failure signatures**:
  1. No improvement over RSPG: Function may be nearly isotropic (condition number <5)
  2. Divergence/oscillation: Check eigenvalue clipping—κ too small causes numerical instability
  3. Query budget exhaustion early: Cache hit rate low; T likely too small
- **First 3 experiments**:
  1. Rosenbrock validation: Compare ZO-SAH vs. RSPG trajectories. Expect ZO-SAH to reach optimum in ~200 evaluations vs. ~8000 for RSPG
  2. Ablation on T: Vary T ∈ {5, 10, 20, 50, 100} on a5a dataset. Measure evaluations to reach f(x) < 0.5
  3. Hessian estimation comparison: Compare polynomial fitting vs. coordinate-wise finite differences on CIFAR10 ResNet8. Polynomial fitting with caching should use ~50% fewer evaluations

## Open Questions the Paper Calls Out

### Open Question 1
Does an optimal subspace dimensionality exist between m=2 and full-dimensionality that minimizes total query complexity for specific problem classes? The authors fixed the subspace dimension to two to ensure constant-time complexity and low query cost per step, but did not systematically explore if intermediate dimensions could accelerate convergence per step enough to offset the higher per-step query cost on non-synthetic problems. Evidence needed: Empirical convergence curves plotting total function queries against loss for ZO-SAH implementations using varying subspace dimensions on datasets with varying degrees of feature correlation.

### Open Question 2
Can an adaptive mechanism be integrated to detect low-curvature (isotropic) landscapes and dynamically switch to a first-order update rule to conserve function evaluations? While ZO-SAH matches first-order methods in these scenarios, it still incurs the overhead of Hessian estimation. The paper does not propose a method to detect these "unhelpful" curvature scenarios online to bypass the expensive second-order computations. Evidence needed: A modified ZO-SAH algorithm that monitors estimated condition numbers or Hessian eigenvalues and disables Hessian fitting when they fall below a threshold.

### Open Question 3
Can adaptive subspace selection strategies accelerate convergence by specifically identifying and grouping parameters with strong mutual dependence? The paper relies on random permutations to select parameter pairs. It leaves unexplored whether utilizing historical gradient/Hessian data to pair correlated parameters could improve the quality of the subspace Hessian estimate. Evidence needed: A comparison of convergence rates between random subspace selection and a "guided" selection method on problems with known block-diagonal Hessian structures.

## Limitations
- Intermediate subspace dimension m is not reported for main experiments, making it impossible to verify constant-time complexity claims
- The benefit on isotropic functions is modest, with performance similar to first-order methods
- Theoretical convergence rate bounds assume specific problem structures that may not hold for deep learning tasks
- Absolute accuracy numbers are not provided for direct comparison with baseline methods

## Confidence

**High Confidence**: The core algorithmic mechanism is mathematically sound and the experimental methodology is rigorous. The convergence improvement over RSPG is well-demonstrated.

**Medium Confidence**: The claim of "up to 50% fewer function queries" is supported by CIFAR10 results but may be dataset-dependent. The comparison with other second-order ZO methods is limited.

**Low Confidence**: The theoretical convergence rate bounds assume specific problem structures that may not hold for deep learning tasks. The practical impact of the switching period T is shown via ablation but the optimal value appears sensitive to problem characteristics.

## Next Checks

1. **Ablation on Intermediate Subspace Dimension**: Systematically vary m from d/10 to d/2 on CIFAR10 ResNet8. Measure convergence speed and function evaluations to determine if there's a critical threshold below which second-order benefits disappear.

2. **Eigenvalue Clipping Sensitivity**: Test κ ∈ {0.01, 0.05, 0.1, 0.2, 0.5} on logistic regression tasks. Plot convergence vs. κ to identify whether aggressive clipping (κ=0.1) is optimal or if problem-specific tuning is required.

3. **Isotropic Function Benchmark**: Construct a diagonal quadratic with condition number 2-3. Compare ZO-SAH vs. RSPG on this synthetic function to isolate when second-order information provides no benefit.