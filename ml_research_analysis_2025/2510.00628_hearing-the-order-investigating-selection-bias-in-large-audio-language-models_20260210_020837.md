---
ver: rpa2
title: 'Hearing the Order: Investigating Selection Bias in Large Audio-Language Models'
arxiv_id: '2510.00628'
source_url: https://arxiv.org/abs/2510.00628
tags:
- bias
- selection
- lalms
- arxiv
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates selection bias in Large Audio-Language
  Models (LALMs), where models show systematic performance variations based on the
  position of the correct answer in multiple-choice questions. The authors conduct
  extensive experiments across six LALMs on three benchmarks (MMAU, MMAR, MMLU) and
  their spoken versions.
---

# Hearing the Order: Investigating Selection Bias in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2510.00628
- Source URL: https://arxiv.org/abs/2510.00628
- Reference count: 0
- Key outcome: Selection bias causes up to 24% performance fluctuations in LALMs based on answer position, affecting all tested models regardless of architecture.

## Executive Summary
This paper reveals a critical issue in Large Audio-Language Models (LALMs): selection bias, where model performance systematically varies based on the position of correct answers in multiple-choice questions. Through extensive experiments across six LALMs on three benchmarks (MMAU, MMAR, MMLU) and their spoken versions, the authors demonstrate that shuffling answer positions can cause performance fluctuations of up to 24% and even change model rankings. They show that permutation-based evaluation strategies (cyclic and full permutations) can effectively mitigate this bias, with full permutation providing the most reliable assessment of model capability. This work highlights the need for more robust evaluation methods in LALM research and provides the first systematic analysis of selection bias in this domain.

## Method Summary
The authors investigate selection bias by running six LALMs (Gemini-2.0-Flash, Phi-4-Multimodal, Qwen2.5-Omni-3B/7B, Voxtral-Mini-3B, Voxtral-Small-24B) on three benchmarks (MMAU, MMAR, MMLU) and their spoken versions created via GPT-4o-mini TTS. They test with and without explicit option identifiers (A/B/C/D), reassign correct answers to different positions while shuffling distractors, and implement cyclic (4 rotations) and full permutation (24 orderings) strategies with majority voting. Performance is measured using accuracy, Δaccuracy, RSD, and CKLD metrics. Models use temperature=0, max_output_tokens=1024, and follow OpenAI simple-eval protocol.

## Key Results
- Selection bias causes performance fluctuations of up to 24% when correct answer positions are shuffled
- Phi-4-Multimodal shows extreme position aversion with accuracy dropping to 16% when correct answer is at position D
- Full permutation evaluation reduces RSD and CKLD metrics by up to 5.2% compared to original evaluation
- Identifier tokens improve accuracy by 5-15% but do not mitigate selection bias
- Audio-language instruction tuning preserves bias in some model families (Voxtral) but alters it in others (Qwen)

## Why This Works (Mechanism)

### Mechanism 1
Permutation-based evaluation reduces selection bias by averaging out position-dependent preferences through majority voting. When option orders are shuffled across multiple permutations and predictions are aggregated via majority vote, position-specific biases cancel out, yielding a more stable accuracy estimate. This works when the model's genuine reasoning ability is invariant to option order.

### Mechanism 2
Selection bias persists regardless of option identifiers (A/B/C/D labels), indicating the bias stems from positional encoding rather than token-level label preferences. Models develop systematic position preferences during training that manifest even when explicit identifiers are removed.

### Mechanism 3
Selection bias may be inherited from text-based LLM backbones or modified through audio-language instruction tuning, with inconsistent inheritance patterns across model families. Some LALMs show similar bias patterns to their text counterparts, suggesting architectural/training inheritance, while others diverge, indicating audio tuning can reshape positional preferences.

## Foundational Learning

- Concept: **Selection Bias vs. Position Bias**
  - Why needed here: Distinguishes general selection effects from pure positional effects to diagnose whether problems stem from option content, labels, or sequence position.
  - Quick check question: If you shuffle options and the model's accuracy changes but its preference ranking over options stays constant, is this selection bias or position bias?

- Concept: **Relative Standard Deviation (RSD) and CKLD as Bias Metrics**
  - Why needed here: These metrics quantify bias magnitude. RSD measures consistency across positions; CKLD measures how prediction distributions diverge from ground-truth distributions.
  - Quick check question: Why might RSD be unreliable when ground-truth answer distributions are imbalanced (e.g., 38% A, 12% D in MMAU)?

- Concept: **Self-Consistency via Permutation**
  - Why needed here: The mitigation strategy adapts self-consistency (originally for sampling variance) to option-order variance. Understanding the analogy clarifies why majority voting helps.
  - Quick check question: What's the computational cost difference between cyclic permutation (4 forward passes) and full permutation (24 forward passes for 4 options)?

## Architecture Onboarding

- Component map:
  - Input Processor: Handles audio + text (questions/options); converts text to speech for spoken benchmarks via GPT-4o-mini TTS
  - Option Encoder: Sequentially processes answer choices A-D, with or without explicit identifiers
  - Selection Head: Outputs predicted option; exhibits position-dependent preferences
  - Evaluation Pipeline: Supports original (single order), cyclic (4 rotations), or full permutation (24 orders) with majority voting

- Critical path:
  1. Question + audio input → model forward pass
  2. Model outputs option prediction (A/B/C/D or raw text)
  3. For permutation: repeat steps 1-2 across shuffled orders
  4. Aggregate predictions via majority vote
  5. Compare to ground truth, compute accuracy/RSD/CKLD

- Design tradeoffs:
  - Cyclic vs. Full Permutation: Cyclic (4 passes) captures some bias reduction; full (24 passes) provides maximum reliability at 6x cost
  - Identifiers vs. No Identifiers: Identifiers improve accuracy ~5-15% but don't reduce bias; remove if measuring pure positional effects
  - Text vs. Spoken Benchmarks: Spoken versions better reflect real audio tasks but increase sequence length significantly (filter >180s samples)

- Failure signatures:
  - High RSD with low accuracy: Model guesses randomly with positional preference
  - Accuracy swing >15% when reassigning correct answer to position D: Strong position aversion
  - Opposite bias in text vs. speech modalities: Modality-specific position encoding

- First 3 experiments:
  1. Baseline bias detection: Run your LALM on MMAU test-mini with correct answers reassigned to each position (A, B, C, D separately). Plot Δaccuracy to identify position preferences.
  2. Identifier ablation: Repeat experiment 1 with and without A/B/C/D labels. Compare CKLD values to determine if bias is position-based or token-based.
  3. Permutation validation: Implement cyclic permutation (4 rotations), measure RSD/CKLD reduction, and verify ranking stability. If RSD remains >5%, escalate to full permutation.

## Open Questions the Paper Calls Out

### Open Question 1
How can selection bias be mitigated during model training to eliminate the need for computationally expensive test-time permutations? The authors note that permutation-based methods "incur additional computational overhead" and explicitly call for future research to move "beyond permutation."

### Open Question 2
Why does audio-language instruction tuning preserve text-based selection bias in some model families (e.g., Voxtral) but significantly alter it in others (e.g., Qwen)? Section 4.2 shows divergent behaviors: Voxtral bias mirrors Mistral, whereas Qwen-Omni diverges from Qwen-7B, suggesting an unknown interaction during fine-tuning.

### Open Question 3
Does the specific Text-to-Speech (TTS) system used to generate spoken benchmarks influence the severity of selection bias? The methodology relies on a single TTS system (GPT-4o mini) to construct spoken datasets, potentially confounding modality effects with specific prosodic artifacts.

## Limitations
- Does not identify root causes of selection bias (positional encoding vs. content-dependent effects)
- Computational cost of full permutation (24× baseline) may limit practical adoption
- Spoken benchmark creation process introduces potential confounds through TTS variability

## Confidence
- High confidence: Selection bias is a universal problem affecting all tested LALMs regardless of architecture or scale
- Medium confidence: Inheritance patterns between text LLMs and multimodal variants suggest both architectural and training-dependent factors
- Low confidence: Specific mechanisms driving position preferences and generalizability to all LALM architectures

## Next Checks
1. Conduct ablation studies varying question difficulty and content type to determine if position preferences are content-dependent rather than purely positional
2. Implement a hybrid permutation strategy that balances computational cost with reliability (e.g., random sampling of permutations rather than full enumeration)
3. Test additional LALM architectures beyond the six studied to verify whether the inheritance patterns hold across a broader model family spectrum