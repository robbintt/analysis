---
ver: rpa2
title: 'MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting'
arxiv_id: '2512.21878'
source_url: https://arxiv.org/abs/2512.21878
tags:
- masfin
- financial
- bias
- crew
- finance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MASFIN is a modular multi-agent system that combines large language\
  \ models with structured financial metrics and unstructured news sentiment to generate\
  \ short-term equity portfolios. The framework uses five sequential crews\u2014Postmortem,\
  \ Screening, Analysis, Timing, and Portfolio\u2014each embedding bias-mitigation\
  \ protocols to address survivorship, hindsight, and overfitting."
---

# MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting

## Quick Facts
- arXiv ID: 2512.21878
- Source URL: https://arxiv.org/abs/2512.21878
- Reference count: 7
- One-line primary result: MASFIN achieved 7.33% cumulative return over 8 weeks, outperforming major indices with higher volatility

## Executive Summary
MASFIN is a modular multi-agent system that combines large language models with structured financial metrics and unstructured news sentiment to generate short-term equity portfolios. The framework uses five sequential crews—Postmortem, Screening, Analysis, Timing, and Portfolio—each embedding bias-mitigation protocols to address survivorship, hindsight, and overfitting. It leverages GPT-4.1-nano for reproducible, cost-efficient inference and produces weekly portfolios of 15–30 equities with allocation weights optimized for short-term performance. Over an eight-week live evaluation, MASFIN achieved a 7.33% cumulative return, outperforming the S&P 500 (4.92%), NASDAQ-100 (5.36%), and Dow Jones (4.11%) in six of eight weeks, albeit with higher volatility (2.61% weekly). The modular design, human-in-the-loop validation, and bias-aware processing enabled transparency and reproducibility while highlighting opportunities for generative AI in quantitative finance.

## Method Summary
MASFIN implements a five-crew sequential pipeline on CrewAI using GPT-4.1-nano agents with human-in-the-loop validation at each handoff. The Postmortem Crew analyzes 18 delisted firms to prevent survivorship bias. The Screening Crew filters markets to 50–100 tickers using sentiment and rules to prevent hindsight bias. The Analysis Crew evaluates metrics and headlines to produce 35–50 candidates. The Timing Crew applies Sortino, momentum, and volume to generate 20–30 buy/hold/sell decisions. The Portfolio Crew consolidates into 15–30 stock allocations. Data sources include Finnhub for headlines and Yahoo Finance for metrics, with contemporaneous snapshots to prevent look-ahead bias.

## Key Results
- 7.33% cumulative return over 8 weeks vs. 4.92% (S&P 500), 5.36% (NASDAQ-100), 4.11% (Dow Jones)
- Outperformed benchmarks in 6 of 8 weeks with 2.61% weekly volatility
- Modular design enabled transparency and reproducibility while maintaining cost-efficiency with GPT-4.1-nano

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition into specialized crews enables manageable reasoning scope within LLM context limits while preserving interpretability.
- Mechanism: Five sequential crews (Postmortem → Screening → Analysis → Timing → Portfolio) each operate on constrained inputs and produce structured outputs. Summary Agents synthesize outputs into reports that serve as manual handoffs, preventing context overflow that would occur in end-to-end architectures.
- Core assumption: Financial reasoning can be meaningfully decomposed without losing critical cross-stage dependencies.
- Evidence anchors:
  - [abstract]: "modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news"
  - [section 2]: "Except for the Portfolio Crew, all crews include a Summary Agent that synthesizes outputs into a structured report, functioning as the manual handoff in the HITL workflow"
  - [section 4]: "End-to-end architectures ingesting all metrics, headlines, and analyses proved impractical, exceeding LLM context limits and reducing interpretability"
  - [corpus]: Weak direct evidence; neighboring papers discuss multi-agent frameworks but do not validate decomposition efficacy specifically.
- Break condition: If cross-crew dependencies carry more signal than within-crew processing, decomposition may lose critical interactions.

### Mechanism 2
- Claim: Explicit bias-mitigation protocols at each stage reduce common financial modeling errors that degrade predictive validity.
- Mechanism: Postmortem Crew analyzes 18 delisted firms to counter survivorship bias; Screening Crew uses only contemporaneous data to prevent hindsight bias; Analysis Crew applies fixed historical windows and feature constraints to reduce overfitting. Each crew documents rationale and references.
- Core assumption: Bias mitigation at pipeline level translates to improved out-of-sample performance.
- Evidence anchors:
  - [abstract]: "embedding explicit bias-mitigation protocols"
  - [section 1]: "Postmortem (accounting for delisted firms to prevent survivorship bias), Screening (selecting candidate firms using contemporaneous data to prevent hindsight bias)"
  - [section 2.3]: "all analysis metrics are computed using fixed historical windows and contemporaneous data snapshots, preventing look-ahead bias"
  - [corpus]: No direct validation of bias-mitigation effectiveness in neighbor papers; concept is referenced but unproven in this context.
- Break condition: If bias mitigation introduces new biases (e.g., over-constraining signal), net benefit may diminish.

### Mechanism 3
- Claim: Human-in-the-loop validation between crews reduces hallucinations and improves reliability without full automation costs.
- Mechanism: Manual handoffs occur between each crew stage. Humans validate structured reports, flag inconsistencies, and adjust agent behavior. This trades automation speed for quality control.
- Core assumption: Human reviewers can effectively detect LLM hallucinations and bias in financial outputs.
- Evidence anchors:
  - [abstract]: "human-in-the-loop validation, and bias-aware processing enabled transparency and reproducibility"
  - [section 4]: "HITL workflow with manual handoffs, enabling validation of outputs, mitigation of hallucinations and bias"
  - [corpus]: "The Wisdom of Agent Crowds" paper discusses human-AI alignment in high-risk domains but does not provide empirical HITL validation data.
- Break condition: If human reviewers introduce cognitive biases or fatigue, HITL may not improve over automated checks.

## Foundational Learning

- Concept: **Survivorship Bias in Financial Modeling**
  - Why needed here: Postmortem Crew explicitly counters survivorship bias by analyzing delisted firms; understanding this bias is essential to grasp why the crew exists.
  - Quick check question: If you backtest a strategy using only currently traded stocks, what systematic error are you introducing?

- Concept: **Look-Ahead (Hindsight) Bias**
  - Why needed here: Screening and Analysis crews enforce contemporaneous data snapshots; without understanding hindsight bias, you cannot validate their constraints.
  - Quick check question: Why would using next-quarter earnings in a trading signal invalidate your backtest?

- Concept: **Multi-Agent Orchestration (CrewAI Pattern)**
  - Why needed here: MASFIN is implemented on CrewAI with sequential role-based agents; understanding orchestration is prerequisite to modifying or extending the pipeline.
  - Quick check question: What is the difference between sequential and hierarchical agent coordination, and which does MASFIN use?

## Architecture Onboarding

- Component map:
  - Postmortem Crew → Screening Crew → Analysis Crew → Timing Crew → Portfolio Crew
  - Each stage depends on the previous crew's structured report; HITL validation occurs at each handoff.

- Critical path: Postmortem → Screening → Analysis → Timing → Portfolio. Each stage depends on the previous crew's structured report. HITL validation occurs at each handoff.

- Design tradeoffs:
  - Decomposition vs. context loss: Modular crews avoid LLM context limits but may miss cross-stage signal interactions.
  - HITL vs. automation: Manual validation improves reliability but increases latency and cost.
  - Open data vs. proprietary: Using Finnhub and Yahoo Finance ensures reproducibility but limits data breadth compared to Bloomberg.

- Failure signatures:
  - Empty shortlist from Screening → Postmortem signals may be too restrictive or sentiment data missing
  - High portfolio turnover week-to-week → Analysis/Timing crews may be overfitting to noise
  - Consistent underperformance vs. benchmarks → Bias mitigation may be over-constraining or data sources degraded

- First 3 experiments:
  1. **Ablation on Postmortem Crew**: Run MASFIN with and without the Postmortem stage over same 8-week period; compare returns and volatility to quantify survivorship bias mitigation contribution.
  2. **HITL vs. Automated Handoff**: Replace manual handoffs with automated validation rules; measure hallucination rate and portfolio drift to validate HITL necessity.
  3. **Alternative LLM Backend**: Swap GPT-4.1-nano for a smaller open model (e.g., Llama-3-70B); compare cost, latency, and portfolio performance to assess model sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MASFIN's performance compare to other AI-driven financial systems rather than just market indices?
- Basis in paper: [explicit] The authors state that "Future work includes comparing MASFIN with other AI-based financial systems to better contextualize its performance."
- Why unresolved: The evaluation benchmarks MASFIN only against SPY, QQQ, and DIA, lacking a direct comparison with similar LLM-agent architectures.
- What evidence would resolve it: A comparative study evaluating MASFIN alongside established AI financial agents (e.g., FinCon) under identical market conditions.

### Open Question 2
- Question: Can MASFIN maintain its risk-adjusted returns over longer horizons and across different market regimes?
- Basis in paper: [explicit] The authors note that "Restricting evaluation to eight weeks limits overfitting to specific market regimes."
- Why unresolved: The eight-week live evaluation occurred during a general uptrend, potentially masking system behavior during market downturns or extended volatility.
- What evidence would resolve it: Backtesting or live evaluation results spanning multiple years, specifically including prolonged bear market periods.

### Open Question 3
- Question: Is the reported outperformance statistically significant given the high volatility and lack of hypothesis testing?
- Basis in paper: [explicit] The paper admits the system "lacks... statistical inference tools such as confidence intervals or hypothesis testing."
- Why unresolved: Without statistical tests, the 7.33% return may not be statistically distinguishable from benchmarks despite the nominal difference, especially given the system's higher volatility (2.61%).
- What evidence would resolve it: Reporting p-values and confidence intervals for the return differential between MASFIN and the benchmarks.

## Limitations

- Performance relies on undocumented prompt engineering and agent coordination protocols that prevent exact reproduction
- Eight-week evaluation period is too short to establish statistical significance or validate behavior across market regimes
- Higher volatility (2.61%) suggests potential overfitting to noise despite stated bias mitigation protocols

## Confidence

- **High confidence**: The modular decomposition approach is technically sound and addresses known LLM context limitations; the bias mitigation framework (survivorship, hindsight, overfitting) is conceptually valid
- **Medium confidence**: The 7.33% cumulative return outperformance is demonstrated but requires longer-term validation to rule out luck or market regime effects
- **Low confidence**: The specific contribution of each bias mitigation protocol and the necessity of HITL validation cannot be quantified without ablation studies

## Next Checks

1. **Ablation study**: Run MASFIN with and without the Postmortem Crew over the same 8-week period to quantify the survivorship bias mitigation contribution to returns
2. **Automated vs. HITL comparison**: Replace manual handoffs with automated validation rules and measure hallucination rates and portfolio drift to validate the HITL necessity
3. **Model sensitivity test**: Swap GPT-4.1-nano for a smaller open model (e.g., Llama-3-70B) and compare cost, latency, and portfolio performance to assess model dependency