---
ver: rpa2
title: On Robustness of Linear Classifiers to Targeted Data Poisoning
arxiv_id: '2511.12722'
source_url: https://arxiv.org/abs/2511.12722
tags:
- robustness
- accuracy
- multiple
- value
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that computing robustness to targeted data poisoning
  for linear classifiers is NP-complete, then presents an efficient algorithm to compute
  lower and upper bounds on robustness. The method partitions the training data and
  solves a mixed-integer linear program on each partition to find a lower bound, and
  augments the dataset with copies of the target point to train a classifier and find
  an upper bound.
---

# On Robustness of Linear Classifiers to Targeted Data Poisoning

## Quick Facts
- **arXiv ID:** 2511.12722
- **Source URL:** https://arxiv.org/abs/2511.12722
- **Reference count:** 40
- **Primary result:** Proves computing robustness to targeted data poisoning for linear classifiers is NP-complete, then presents efficient algorithms to compute lower and upper bounds.

## Executive Summary
This paper addresses the fundamental problem of computing how many label perturbations are required to change the classification of a specific test point under targeted data poisoning attacks. The authors prove that finding the exact minimum number of label flips needed is NP-complete, making brute-force computation infeasible for large datasets. To overcome this computational barrier, they develop two efficient approximation algorithms: one that partitions the dataset and solves a mixed-integer linear program on each partition to find a lower bound, and another that augments the training data with multiple copies of the target point to find an upper bound. The method is evaluated on 15 datasets, showing that the tool ROBUSTRANGE can efficiently compute robustness bounds for all test points with average upper bounds as low as 1% of training points.

## Method Summary
The method computes lower and upper bounds on robustness to targeted data poisoning for linear classifiers. For the lower bound, it partitions the training data into disjoint subsets and solves a mixed-integer linear program (MILP) on each subset using Big-M constraints to find the minimum label perturbations required to flip the target point within that subset. The sum of these local minima provides a valid lower bound. For the upper bound, it augments the training set with $k'=m+1$ copies of the target test point, trains a linear classifier (SGDClassifier with hinge loss), and counts the number of misclassifications on the original training set. The approach is implemented using Python with SCIP solver via Google OR-Tools and scikit-learn.

## Key Results
- Computing exact robustness to targeted data poisoning for linear classifiers is NP-complete (proven via reduction from Vertex Cover).
- ROBUSTRANGE efficiently computes robustness bounds for all test points on 15 datasets, with average upper bounds as low as 1% of training points.
- Experiments show poisoning at identified upper bounds significantly impacts classifier accuracy while maintaining high success in altering target prediction.
- ROBUSTRANGE outperforms state-of-the-art techniques, which fail on many datasets (IP-RELABEL found upper bounds for <100% of points on most datasets).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning the dataset allows for the computationally tractable estimation of a lower bound on robustness.
- **Mechanism:** The algorithm splits the training data into $k$ disjoint partitions. It solves a Mixed-Integer Linear Program (MILP) for each partition independently to find the minimum label perturbations required to flip the target point within that subset. Summing these minimums yields a valid lower bound because an optimal global solution might leverage overlapping perturbations across partitions that are counted separately in this local approach.
- **Core assumption:** The partition-based sum of local minima is strictly less than or equal to the global minimum (Theorem 2).
- **Evidence anchors:**
  - [abstract] "method partitions the training data... to find a lower bound"
  - [section: Lower Bound Robustness via Partitioning] "To scale this, we propose an optimization approach... the dataset is partitioned... sum provides a lower bound"
  - [corpus] Related work in "Agnostic Learning under Targeted Poisoning" discusses optimal error rates, but specific partitioning strategies for bounds are not detailed in the provided neighbors.
- **Break condition:** If partitions are too small, the local MILP solutions might return zero robustness, resulting in a trivial and useless lower bound.

### Mechanism 2
- **Claim:** Training a linear classifier on an augmented dataset provides an upper bound on the robustness.
- **Mechanism:** The algorithm augments the training set with $k'$ copies of the target test point $(x_t, y_t)$. By forcing the classifier to learn this heavily weighted target point correctly (via empirical risk minimization), the classifier will likely misclassify some original training points. The count of these misclassifications serves as an upper bound because it represents one valid (though not necessarily minimal) set of perturbations that achieves the attack goal.
- **Core assumption:** A classifier exists that correctly classifies the target point after augmentation, and the loss function allows the model to "sacrifice" original training points to accommodate the target.
- **Evidence anchors:**
  - [abstract] "augments the dataset with copies of the target point... to find an upper bound"
  - [section: Upper Bound Robustness via Augmentation] "The augmentation biases the learning... While theoretically this classifier may not minimize the label perturbations... in practice we observe that it will give a tighter upper bound."
  - [corpus] The "Cost-Minimized Label-Flipping" paper mentions similar concepts of cost/perturbation minimization but focuses on LLM alignment.
- **Break condition:** If the loss function or model capacity prevents the classifier from fitting the augmented target point (e.g., strong regularization preventing overfitting to the $k'$ copies), the method fails to produce a valid bound.

### Mechanism 3
- **Claim:** Robustness can be formulated as an optimization problem constrained by linear classifier geometry.
- **Mechanism:** The method uses the "Big M" technique to encode the classification logic into a linear program. It introduces binary variables $\delta_i$ to represent label flips. The constraints ensure that if $\delta_i = 0$, the classifier must agree with the original label $y_i$, and if $\delta_i = 1$, the classifier must disagree. Minimizing $\sum \delta_i$ subject to the target classification constraint formally defines the robustness problem.
- **Core assumption:** The linear classifier's decision boundary can be captured by the constraints $y_i(w \cdot x_i + b) + M\delta_i \geq \epsilon$.
- **Evidence anchors:**
  - [section: Computing Robustness] "it can be encoded as a mixed-integer linear program (MILP) using the big M method... A solution to the problem provides weights and bias..."
  - [corpus] General linear classification constraints are standard; the Big M encoding is specific to this optimization approach.
- **Break condition:** If $M$ is not large enough relative to the data scale, the constraints may become invalid; if $M$ is too large, numerical instability may occur in the solver.

## Foundational Learning

- **Concept:** Mixed-Integer Linear Programming (MILP)
  - **Why needed here:** Understanding how to translate the discrete condition of "flipping a label" into a solvable mathematical constraint using binary variables.
  - **Quick check question:** Can you explain why introducing binary variables makes the optimization problem significantly harder than standard linear programming?

- **Concept:** NP-Completeness
  - **Why needed here:** To justify why the authors use bounding techniques (approximations) rather than computing exact robustness directly.
  - **Quick check question:** The paper proves finding exact robustness is NP-Complete via reduction from which problem? (Answer: Vertex Cover).

- **Concept:** Linear Classifier Geometry
  - **Why needed here:** To interpret the "attack" as finding a hyperplane rotation that accommodates the target point while minimizing the number of training points that cross to the wrong side.
  - **Quick check question:** How does adding copies of the target point to the training data geometrically influence the decision boundary?

## Architecture Onboarding

- **Component map:** Input -> Lower Bound Engine (Partitioner -> MILP Solver -> Aggregator) -> Upper Bound Engine (Augmenter -> Trainer -> Evaluator)
- **Critical path:** The MILP solve time for the partitions is the primary computational bottleneck. The Upper Bound engine is comparatively lightweight (standard training).
- **Design tradeoffs:**
  - **Partition count ($k$):** Higher $k$ reduces partition size, speeding up individual MILP solves but potentially lowering the lower bound quality (looser bound).
  - **Augmentation factor ($k'$):** Must be high enough to force the target classification, but unnecessarily high values may degrade general accuracy more than needed, loosening the upper bound.
- **Failure signatures:**
  - **Loose Bounds:** The gap between $\check{r}$ (lower) and $\hat{r}$ (upper) is too wide to be useful.
  - **Solver Timeout:** MILP fails to converge within a reasonable time for large or noisy partitions.
  - **Target Misclassification:** The trained classifier in the upper bound phase fails to classify the target point correctly, rendering the bound invalid.
- **First 3 experiments:**
  1. **Scale Test:** Run ROBUSTRANGE on the Census Income dataset (Table 1). Verify that the lower bound $\check{r}$ is non-zero for at least 15% of points as reported.
  2. **Poison Validation:** Take a specific test point with computed upper bound $\hat{r}$. Perturb exactly $\hat{r}$ labels as indicated by the tool and retrain. Confirm the target prediction flips with probability $\rho$.
  3. **SOTA Comparison:** Attempt to run the IP-RELABEL baseline on a dataset where it previously failed (e.g., %Found$\hat{r}_{IPr} < 100\%$) and verify ROBUSTRANGE completes successfully.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the partition-based lower bound algorithm be effectively extended to compute robustness for non-linear classifiers?
- **Basis in paper:** [explicit] The Conclusion states: "An interesting future direction is to extend the lower bound algorithm for non-linear classifiers."
- **Why unresolved:** The paper's theoretical proofs and MILP formulation rely specifically on the properties of linear boundaries, and it is unclear if the partitioning strategy retains its theoretical guarantees or efficiency in non-linear hypothesis spaces.
- **What evidence would resolve it:** A modified algorithm or proof-of-concept implementation that successfully computes non-trivial lower bounds for kernel methods or neural networks without exponential complexity.

### Open Question 2
- **Question:** How do robustness bounds change when the threat model is expanded to include feature perturbations or data insertion?
- **Basis in paper:** [inferred] The problem setting explicitly restricts the adversary to label perturbations ($y'_i \neq y_i$) and notes that contaminating features is difficult but not impossible, leaving this attack vector unexplored.
- **Why unresolved:** The current mathematical formulation optimizes label flips for fixed data points; it does not model the continuous search space required for feature modifications or the increased dimensionality of data insertion.
- **What evidence would resolve it:** A generalization of the MILP encoding or augmentation strategy that accounts for feature-level constraints, along with empirical results comparing bounds for label-only vs. feature-level attacks.

### Open Question 3
- **Question:** Why does KNN-based sanitization fail to increase robustness bounds in the majority of evaluated datasets?
- **Basis in paper:** [inferred] In RQ5, the authors hypothesize that the failure of sanitization in 13 out of 15 datasets "could be because the robustness we compute may not align with the nearest neighbor structure."
- **Why unresolved:** The paper provides empirical evidence of failure but only offers a brief hypothesis regarding the structural misalignment between the robustness metric and KNN geometry.
- **What evidence would resolve it:** A theoretical analysis or ablation study demonstrating the correlation (or lack thereof) between the proposed robustness metric and local neighborhood density.

## Limitations

- The MILP-based lower bound approach is computationally expensive and scales poorly with dataset size, requiring careful partitioning to remain tractable.
- The upper bound method relies on the assumption that augmenting the dataset with multiple copies of the target point will reliably force misclassification of training examples, which may not hold for strongly regularized models.
- The partitioning strategy for the lower bound sacrifices optimality for computational feasibility, potentially yielding loose bounds when optimal label flips span multiple partitions.

## Confidence

- **High Confidence:** The NP-completeness proof via reduction from Vertex Cover (Claim: Computing exact robustness is NP-complete) - the mathematical framework is well-established.
- **Medium Confidence:** The effectiveness of the upper bound augmentation method (Claim: Upper bound reliably forces target misclassification) - experimental results show good performance, but the mechanism may break for certain regularization schemes.
- **Medium Confidence:** The partitioning approach provides valid lower bounds (Claim: Sum of partition-wise minima â‰¤ global minimum) - theoretically sound but practically sensitive to partition size and data distribution.

## Next Checks

1. **Partition Size Sensitivity:** Systematically vary the number of partitions $k$ on a medium-sized dataset and measure the corresponding lower bound tightness and computation time to identify the optimal tradeoff.
2. **Regularization Robustness:** Apply strong L2 regularization to the upper bound training and verify whether the method still produces valid bounds and forces target misclassification.
3. **Cross-Partition Flip Analysis:** On a small dataset where exact computation is feasible, compare the optimal global solution to the sum of partition-wise solutions to quantify the optimality gap introduced by partitioning.