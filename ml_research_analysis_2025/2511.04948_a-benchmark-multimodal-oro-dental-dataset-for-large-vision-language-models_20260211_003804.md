---
ver: rpa2
title: A benchmark multimodal oro-dental dataset for large vision-language models
arxiv_id: '2511.04948'
source_url: https://arxiv.org/abs/2511.04948
tags:
- dataset
- dental
- data
- were
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the lack of large-scale, publicly available
  multimodal dental datasets for training advanced AI models in dentistry. To fill
  this gap, the authors created COde, a comprehensive dataset containing 8,775 dental
  checkups from 4,800 patients, with 50,000 intraoral images, 8,056 radiographs, and
  detailed textual records collected over eight years.
---

# A benchmark multimodal oro-dental dataset for large vision-language models

## Quick Facts
- arXiv ID: 2511.04948
- Source URL: https://arxiv.org/abs/2511.04948
- Reference count: 0
- Key outcome: Fine-tuned Qwen-VL models achieve 78.92% accuracy and 79.39% F1-score on 6-class dental anomaly classification, and 71.46% similarity in multimodal diagnostic report generation.

## Executive Summary
This paper introduces COde, a large-scale multimodal oro-dental dataset for training and benchmarking vision-language models in dentistry. The dataset contains 8,775 dental checkups from 4,800 patients, with 50,000 intraoral images, 8,056 radiographs, and detailed textual records collected over eight years. The authors fine-tuned Qwen-VL-3B and Qwen-VL-7B models on this dataset for two benchmark tasks: classification of six oro-dental anomalies and generation of diagnostic reports from multimodal inputs. Fine-tuned models significantly outperformed base models and GPT-4o, validating the dataset's utility for advancing AI-driven dental healthcare.

## Method Summary
The authors created COde, a comprehensive dataset of 8,775 dental checkups from 4,800 patients, with 50,000 intraoral images, 8,056 radiographs, and detailed textual records. The dataset was annotated for benchmarking two tasks: classification of six oro-dental anomalies and generation of diagnostic reports from multimodal inputs. The authors fine-tuned Qwen-VL-3B and Qwen-VL-7B models on this dataset using LoRA adapters, and evaluated their performance against base models and GPT-4o. The fine-tuned models achieved significant performance gains, with Qwen-VL-7B reaching 78.92% accuracy and 79.39% F1-score in classification, and 71.46% similarity in report generation.

## Key Results
- Fine-tuned Qwen-VL-7B achieved 78.92% accuracy and 79.39% F1-score on 6-class dental anomaly classification.
- Fine-tuned models outperformed base Qwen-VL and GPT-4o baselines on both classification and report generation tasks.
- Generated diagnostic reports achieved 71.46% similarity to ground truth using averaged Gemma-2B and Llama-3.2-1B embeddings.

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific fine-tuning substantially improves VLM performance on clinical tasks compared to general-purpose baselines. Fine-tuning on curated multimodal dental data injects domain knowledge into pre-trained vision-language models via LoRA adapters, enabling specialized feature-pattern associations that general pre-training cannot capture.

### Mechanism 2
Multimodal input fusion enables more robust clinical inference than single-modality approaches. LMMs jointly encode visual and textual modalities, allowing cross-modal reasoning and leveraging complementary signals when one modality is ambiguous or incomplete.

### Mechanism 3
High-quality, manually verified annotations are critical for reliable benchmarking and model learning. Labels were auto-extracted from clinician-written diagnoses, then verified by licensed dental practitioners, ensuring alignment between extracted labels and clinical intent.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) / Large Multimodal Models (LMMs)
  - Why needed here: The entire methodology relies on fine-tuning Qwen-VL models to process dental images and text jointly.
  - Quick check question: Can you explain how a VLM encodes both image and text into a shared representation?

- Concept: Parameter-Efficient Fine-Tuning (LoRA)
  - Why needed here: The authors used LoRA adapters for training 3B/7B models on limited GPU memory (2-4 × A800s).
  - Quick check question: What does LoRA modify during training, and why does it reduce memory overhead compared to full fine-tuning?

- Concept: Evaluation Metrics for Generation (BLEU, METEOR, Cosine Similarity)
  - Why needed here: Report generation quality is measured via these metrics; understanding their limitations is critical for interpreting results.
  - Quick check question: Why might BLEU/METEOR scores be insufficient for assessing clinical correctness in diagnostic reports?

## Architecture Onboarding

- Component map: Data layer (50K intraoral images + 8K radiographs + CSV/ShareGPT JSON text) -> Model layer (Qwen-VL-3B/7B with LoRA adapters) -> Training layer (3 epochs, cosine LR, batch sizes 2/3, NVIDIA A800 GPUs) -> Evaluation layer (600-sample test set; classification + generation; metrics F1, BLEU, METEOR, Cosine Similarity)

- Critical path: Load ShareGPT-formatted training split -> Initialize Qwen-VL with LoRA config (target all transformer blocks) -> Train for 3 epochs with cosine LR; checkpoint best validation performance -> Evaluate on held-out 600-sample benchmark (classification + generation)

- Design tradeoffs: 3B vs. 7B (7B achieves higher performance but requires 2× GPU resources); Zero-shot vs. few-shot (few-shot improves generation for baselines but wasn't used for classification); ShareGPT augmentation (provides randomized image order and QA pairs for training diversity)

- Failure signatures: Classification accuracy near random (~16.7% for 6 classes) -> model not learning; check data loading and label alignment; Generation outputs missing required sections -> prompt template misalignment or insufficient training epochs; GPU OOM during 7B training -> reduce batch size or LoRA rank; verify gradient checkpointing

- First 3 experiments: 1) Reproduce baseline (zero-shot Qwen-VL-7B on test set) to validate evaluation pipeline; 2) Fine-tune Qwen-VL-3B with provided training split and compare to reported 74.90% accuracy; 3) Run ablation: train with images-only vs. text-only inputs to assess modality contribution

## Open Questions the Paper Calls Out

### Open Question 1
Can models trained on the COde dataset generalize effectively to clinical environments with different imaging hardware and patient demographics? The data was collected exclusively from one clinic using specific hardware, creating potential domain bias. Performance on independently collected dental datasets would resolve this.

### Open Question 2
How does model performance vary when attempting to classify the full spectrum of 120+ identified conditions compared to the benchmarked six-class subset? The utility for rare conditions and multi-label diagnosis scenarios remains unquantified. Benchmark results using multi-label classification would resolve this.

### Open Question 3
To what extent does the GPT-4o-generated translation of clinical records impact the semantic accuracy and diagnostic precision of models trained on the English version? Automated translation may introduce hallucinations or loss of nuance in medical terminology. Comparative analysis fine-tuning on original Chinese vs. translated English would resolve this.

## Limitations
- Dataset representativeness and annotation quality: Clinical diversity may not capture rare anomalies or regional variations; systematic diagnostic biases in original data may propagate to benchmark.
- Multimodal fusion effectiveness: Claimed superiority remains an assumption without direct ablation studies; architecture may be dominated by text modality.
- Evaluation metric adequacy: Standard NLP metrics may not fully capture clinical correctness or diagnostic utility in generated reports.

## Confidence
- High confidence: Domain-specific fine-tuning substantially improves VLM performance on dental classification tasks.
- Medium confidence: Fine-tuned models can generate clinically relevant diagnostic reports.
- Low confidence: Multimodal fusion provides robust inference when information is incomplete.

## Next Checks
1. Conduct ablation studies comparing single-modality vs. multimodal inputs on both classification and generation tasks to quantify the actual contribution of each modality.
2. Implement clinical expert review of generated diagnostic reports to assess medical accuracy beyond standard NLP metrics.
3. Test model generalization on external dental datasets or rare anomaly cases not well-represented in COde to evaluate real-world applicability.