---
ver: rpa2
title: Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?
arxiv_id: '2507.06335'
source_url: https://arxiv.org/abs/2507.06335
tags:
- visual
- language
- word
- semantics
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that the words-as-classifiers (WAC) model can
  serve as a unifying framework to integrate formal, distributional, and grounded
  semantics, addressing both the symbol grounding and neuro-symbolic AI challenges.
  WAC treats each word as a classifier trained on positive and negative examples from
  perceptual data, enabling probabilistic grounding of word meanings.
---

# Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?

## Quick Facts
- arXiv ID: 2507.06335
- Source URL: https://arxiv.org/abs/2507.06335
- Authors: Casey Kennington; David Schlangen
- Reference count: 12
- Primary result: WAC framework successfully integrates formal, distributional, and grounded semantics through classifier-based word representations

## Executive Summary
This paper proposes the words-as-classifiers (WAC) model as a unifying framework for grounded, neuro-symbolic AI. WAC treats each word as a binary classifier trained on positive and negative perceptual examples, enabling probabilistic grounding of word meanings. The authors demonstrate successful integration with formal semantic frameworks (Type Theory with Records) and distributional models by enriching language model embeddings with visual information. A preliminary experiment shows that combining textual and visual embeddings through multiplication improves performance on semantic similarity and natural language inference tasks compared to baselines.

## Method Summary
The method involves training logistic regression classifiers for each word in vocabulary using positive examples (visual features of referred objects) and negative examples (features of non-referred objects). Classifier coefficients are extracted as visual embeddings and combined with textual embeddings from language models through element-wise multiplication. This combined embedding is used in ELECTRA-small's embedding layer during pre-training. The model is then fine-tuned on MRPC and WNLI benchmarks. The approach also proposes integrating WAC classifiers into attention mechanisms and formal semantic frameworks for compositional reasoning.

## Key Results
- Element-wise multiplication of visual and textual embeddings achieves MRPC F1 of 0.81 and WNLI accuracy of 0.56, outperforming addition and concatenation baselines
- WAC successfully integrates with Type Theory with Records by replacing binary predicates with probabilistic classifier outputs
- The framework provides a path toward incremental learning through dialogue-based updates to classifiers
- Visual embeddings from WAC classifiers can be combined with language model embeddings without disrupting standard training procedures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word meanings can be grounded through binary classifiers trained on perceptual features.
- Mechanism: For each word in vocabulary, train a logistic regression classifier using positive examples (visual features of objects the word refers to) and negative examples (features of objects it doesn't). The classifier outputs probability p(word|object) representing grounded semantic fit.
- Core assumption: Word-level semantics can be captured primarily through reference to physical objects, and perceptual features contain sufficient signal for grounding.
- Evidence anchors:
  - [abstract] "WAC treats word-level semantics as binary classifiers trained on positive and negative examples of word usage in reference to objects"
  - [section 2] "for each word, a classifier is trained (e.g., logistic regression) on positive and negative examples of the word being used to refer to specific objects"
- Break condition: Words with abstract meanings lacking visual referents (e.g., "democracy", "therefore") or words whose meaning depends heavily on syntactic context rather than object reference.

### Mechanism 2
- Claim: Multiplying visual and textual embeddings improves language model performance by enriching distributional representations with grounded semantics.
- Mechanism: Extract classifier coefficients from trained WAC models as visual vectors (size 128). Multiply element-wise with textual embeddings from LM embedding layer. This weights both modalities simultaneously rather than overriding either.
- Core assumption: Visual and textual embeddings encode complementary semantic information that can productively interact through multiplicative combination.
- Evidence anchors:
  - [section 3.2.1] Table 1 shows multiplication achieving MRPC f1=0.81 and WNLI acc=0.56 vs. baseline 0.78/0.49. Addition and concatenation performed worse.
  - [section 3.2.1] "we conjecture that multiplication works better because both modalities are weighting each other resulting in smaller, yet meaningful changes in the optimization surface"
- Break condition: When visual and textual semantics conflict (e.g., metaphorical uses, abstract words with no visual grounding), multiplication may amplify noise.

### Mechanism 3
- Claim: WAC classifiers can integrate with formal semantic frameworks (Type Theory with Records) to enable probabilistic grounded reasoning.
- Mechanism: WAC classifiers replace traditional binary predicates in TTR records. Classifier parameters stored as `params` field; functional application returns probability rather than true/false. TTR's compositional machinery operates on these probabilistic outputs.
- Core assumption: Formal semantic composition can work with probabilistic rather than categorical outputs without breaking inference mechanisms.
- Evidence anchors:
  - [section 3.1] "This effectively links WAC to a formal representation because the functional application of an object's features to the classifier is akin to functionally applying a variable to a first-order predicate"
  - [section 3.1] Figure 2 shows TTR record with `clf_r` classifier and `params` containing weights
- Break condition: Applications requiring strict logical inference where probabilistic outputs cause cascading uncertainty or violate formal constraints.

## Foundational Learning

- **Concept: Symbol Grounding Problem**
  - Why needed here: The entire paper addresses how to connect symbolic/distributional representations to physical world referents. Understanding Harnad/Searle's critique explains why pure text-trained models are insufficient.
  - Quick check question: Can you explain why a system trained only on text might pass benchmarks without "understanding" in a grounded sense?

- **Concept: Binary Classification with Logistic Regression**
  - Why needed here: WAC's core building block is training individual logistic regression classifiers per word. Need to understand positive/negative examples, coefficient extraction, and probability outputs.
  - Quick check question: Given a trained logistic regression classifier with coefficient vector w, how would you extract it as an embedding for integration with an LM?

- **Concept: Transformer Embedding and Attention Layers**
  - Why needed here: The paper proposes integrating WAC at both embedding (coefficients as vectors) and attention (probabilities as denotation vectors) layers. Understanding standard transformer architecture is prerequisite.
  - Quick check question: Where in a standard transformer does text-only embedding happen, and where does cross-modal attention typically fuse visual information?

## Architecture Onboarding

- **Component map:**
  Visual Input (images) → CLIP encoder → visual feature vectors → WAC classifiers (one per word) → Coefficients → Embedding Layer
                                                                                                            ↘
                                                                                                           Probabilities → Attention Layer
                                                                                                            ↓
                                                                                                     LM backbone → Output

- **Critical path:**
  1. Collect referring expression dataset (words + segmented object images)
  2. Extract CLIP features for all object images
  3. Train WAC classifiers (positive: referred objects; negative: random samples)
  4. Extract coefficient vectors as visual embeddings
  5. Initialize/modify LM embedding layer with combined embeddings (multiplication)
  6. Pre-train LM with modified embeddings on text data
  7. During inference: optionally feed visual input through WAC to attention layer

- **Design tradeoffs:**
  - **Freezing vs. joint training**: Paper freezes visual embeddings during LM pre-training. Joint training could improve but risks catastrophic forgetting of visual grounding.
  - **WAC retraining frequency**: Section 4 proposes retraining "every batch or dialogue interaction"—computational cost vs. adaptation speed.
  - **Negative sample strategy**: Paper uses random sampling (100 positive, 300 negative per word). Hard negatives could improve discrimination but require more curation.
  - **Integration points**: Embedding-only (current experiment) vs. full embedding+attention (proposed). More integration points = more grounded but harder to train.

- **Failure signatures:**
  - **Abstract word degradation**: Words without clear visual referents show degraded performance or random embeddings
  - **Sparse-data instability**: Words with few training examples have unreliable classifiers (paper notes "small amount of training data necessary" but doesn't quantify minimum)
  - **Modal conflict**: Model generates outputs inconsistent with visual input when text and vision disagree
  - **Composition breakdown**: Multi-word phrases fail when individual WAC probabilities multiply to near-zero (mentioned in Section 2 as WAC limitation)

- **First 3 experiments:**
  1. **Reproduction with multiplication**: Replicate Section 3.2.1 experiment on ELECTRA-small with visual×textual embedding multiplication on MRPC/WNLI. Verify 0.81/0.56 result is stable across random seeds.
  2. **Visual task evaluation**: Test the multiplication model on a genuinely visual benchmark (e.g., visual question answering, referring expression comprehension) to validate grounding transfer—not just text-only tasks.
  3. **Data efficiency sweep**: Train WAC classifiers with varying positive example counts (1, 5, 10, 50, 100) to characterize minimum data requirements and identify the sparse-data failure threshold.

## Open Questions the Paper Calls Out
None

## Limitations
- Core claims rest heavily on preliminary empirical validation (single ELECTRA-small experiment on two tasks)
- Proposed unified neuro-symbolic grounding framework lacks systematic evaluation across diverse semantic phenomena
- Reliance on logistic regression classifiers may be insufficient for capturing full complexity of word meanings, particularly for non-visual or highly context-dependent semantics

## Confidence
- **High confidence**: The WAC framework provides a coherent theoretical bridge between distributional semantics and formal semantics, and the mathematical formulation of treating words as classifiers is sound.
- **Medium confidence**: The multiplication-based embedding combination shows promise in the preliminary experiment and the theoretical justification for why it works is reasonable, though limited to two tasks.
- **Low confidence**: The proposed unified neuro-symbolic architecture that integrates WAC at both embedding and attention layers, learns incrementally through dialogue, and handles abstract concepts is currently speculative with no empirical validation beyond the basic embedding experiment.

## Next Checks
1. **Cross-modal consistency test**: Evaluate the multiplication model on a genuinely visual task (e.g., visual question answering or referring expression comprehension) to verify that the grounding transfer actually improves multimodal performance, not just text-only benchmarks.

2. **Semantic type coverage analysis**: Systematically evaluate WAC performance across different word classes (concrete nouns, abstract nouns, verbs, adjectives) and semantic phenomena (metaphor, compositionality, context-dependence) to identify where the classifier-based grounding succeeds or fails.

3. **Incremental learning experiment**: Implement the proposed dialogue-based incremental learning protocol where WAC classifiers are updated after each interaction, measuring catastrophic forgetting rates and adaptation speed compared to batch retraining, to validate the practical feasibility of the proposed learning framework.