---
ver: rpa2
title: 'Tangma: A Tanh-Guided Activation Function with Learnable Parameters'
arxiv_id: '2507.10560'
source_url: https://arxiv.org/abs/2507.10560
tags:
- tangma
- activation
- training
- loss
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Tangma introduces a learnable activation function combining hyperbolic\
  \ tangent and linear components, controlled by two trainable parameters \u03B1 and\
  \ \u03B3, to balance saturation and sensitivity. Evaluated on MNIST and CIFAR-10\
  \ using custom CNN architectures, Tangma achieved the highest validation accuracy\
  \ on both datasets (99.09% on MNIST, 78.15% on CIFAR-10) and the lowest validation\
  \ loss on MNIST (0.0363)."
---

# Tangma: A Tanh-Guided Activation Function with Learnable Parameters

## Quick Facts
- arXiv ID: 2507.10560
- Source URL: https://arxiv.org/abs/2507.10560
- Reference count: 0
- Primary result: Tangma achieves 99.09% validation accuracy on MNIST and 78.15% on CIFAR-10

## Executive Summary
Tangma is a learnable activation function that combines hyperbolic tangent with linear components, controlled by two trainable parameters α and γ. The function aims to balance saturation and sensitivity while preserving gradient flow during training. Evaluated on MNIST and CIFAR-10 using custom CNN architectures, Tangma achieved the highest validation accuracy on both datasets and demonstrated faster initial convergence with lower epoch runtime compared to Swish and GELU.

## Method Summary
Tangma(x) = x·tanh(x + α) + γx where α and γ are learnable parameters initialized to 0. The function creates a smooth transition between linear and saturated regions through the tanh component, while the linear skip term (γx) preserves gradient flow. During training, α shifts the inflection point and γ controls the linear asymptote. The method was evaluated using Adam optimizer with learning rate 0.001 on custom CNN architectures for image classification tasks.

## Key Results
- Achieved highest validation accuracy: 99.09% on MNIST and 78.15% on CIFAR-10
- Lowest validation loss on MNIST: 0.0363
- Faster initial convergence and lower epoch runtime (8.97s vs 11.2-11.3s on CIFAR-10)
- Smoother loss curves indicating better stability and generalization

## Why This Works (Mechanism)

### Mechanism 1
The learnable linear skip coefficient (γ) preserves gradient flow during saturation regimes. Tangma's γx term creates a parallel gradient pathway. When tanh(x + α) saturates near ±1, its derivative approaches zero, but the derivative d/dx[Tangma(x)] = tanh(x + α) + x·sech²(x + α) + γ retains γ as a minimum gradient floor. This prevents gradient starvation in deep layers where activations frequently enter saturation zones.

### Mechanism 2
The learnable shift parameter (α) adapts the activation's sensitivity region to input distribution characteristics. By shifting the inflection point from x = 0 to x = -α, the network controls where neurons transition between linear and saturated behavior. For MNIST, α evolves slowly from ~0 to 0.28. For CIFAR-10, α rises faster to 0.40, adapting to more diverse visual patterns.

### Mechanism 3
Smooth bounded saturation with linear tails improves training stability compared to hard cutoffs or unbounded growth. Unlike ReLU (hard zero for x < 0) or GELU (computational complexity, small gradients in [-3, -1]), Tangma combines smooth tanh saturation for moderate inputs with linear growth (γ ± 1)x at extremes. This yields non-zero gradients everywhere while preventing unbounded activation explosion.

## Foundational Learning

- **Backpropagation and gradient flow through activation functions**: Understanding how θ'(z) affects ∂L/∂w = ∂L/∂a · θ'(z) · x is critical for grasping why Tangma's design prevents vanishing gradients. Quick check: Can you derive why a non-zero minimum gradient prevents "dying neurons" in deep networks?

- **Hyperbolic tangent properties (saturation, derivative behavior)**: Tangma builds on tanh(x + α), requiring understanding of where tanh saturates (x → ±∞), its range [-1, 1], and how sech² relates to gradient magnitude. Quick check: What is tanh'(x) and why does it approach zero for large |x|?

- **Learnable parameters in neural networks (nn.Parameter in PyTorch)**: α and γ are trained via backpropagation alongside weights. Understanding parameter initialization, gradient updates, and optimizer interaction is critical for debugging unexpected behavior. Quick check: How does adding learnable α, γ to the computation graph change the memory footprint and backward pass compared to fixed ReLU?

## Architecture Onboarding

- **Component map**: Input tensor z -> Tangma(z) = z·tanh(z + α) + γz -> Activated output
- **Critical path**: Register α, γ as nn.Parameter with requires_grad=True; forward pass computes tanh(x + α), multiplies by x, adds γx; backward pass handles derivative chain; optimizer step updates α, γ alongside weights
- **Design tradeoffs**: Computational cost: 1 tanh + 2 multiplies + 1 add per element vs. ReLU's max(0, x) (22% overhead). Memory: Two additional scalar parameters per network (negligible). Initialization sensitivity: α=0, γ=0 start point means Tangma(x) ≈ x·tanh(x) initially.
- **Failure signatures**: α, γ not updating (check requires_grad and optimizer inclusion); slower convergence than ReLU (may indicate γ growing too slowly); validation loss plateau while training loss drops (overfitting); NaN gradients (check numerical stability).
- **First 3 experiments**: 1) Baseline replication on MNIST with reported architecture; 2) Ablation study: fix γ=0 on CIFAR-10; 3) Transfer to deeper architecture: replace ReLU with Tangma in ResNet-18 on CIFAR-10.

## Open Questions the Paper Calls Out

- **Future work could explore adding Tangma into deeper convolutional and residual architectures**: The current study evaluates Tangma only on custom, relatively shallow CNNs (2-3 convolutional layers). Benchmarking on standard deep architectures like ResNet-50 or ResNet-101 using ImageNet would validate generalization to deeper networks.

- **The effectiveness of Tangma on language modeling and transformer-based models remains unexplored**: While the authors suggest evaluating its effectiveness on "language modeling" and "transformer-based models," the experimental scope was limited to computer vision tasks (MNIST, CIFAR-10), leaving performance on sequential data unknown.

- **Whether Tangma's higher validation accuracy is accompanied by inferior probability calibration remains untested**: The paper notes Tangma achieved the highest accuracy on CIFAR-10 but a higher validation loss than ReLU, suggesting a "small over-confidence penalty." Calculating Expected Calibration Error (ECE) and generating reliability diagrams would quantify this calibration issue.

## Limitations
- Limited evaluation to two datasets and custom CNN architectures without ablation studies isolating each mechanism's contribution
- Faster convergence claims lack statistical significance testing across multiple runs
- Generalization claims for broader applications (deeper networks, other datasets) lack empirical validation

## Confidence
- **High Confidence**: Tangma achieves state-of-the-art validation accuracy on MNIST (99.09%) and CIFAR-10 (78.15%) using the described architectures and training procedures
- **Medium Confidence**: The convergence speed advantage (faster initial learning) is observed but may be architecture-dependent
- **Low Confidence**: The specific claim that Tangma "prevents gradient starvation" is theoretically supported but not experimentally isolated from other factors

## Next Checks
1. **Ablation Study**: Train with γ fixed at 0 (removing the linear skip component) on CIFAR-10 to quantify the specific contribution of gradient preservation to overall performance
2. **Statistical Validation**: Run 5 independent trials with different random seeds for both MNIST and CIFAR-10 to establish confidence intervals for accuracy and convergence speed improvements
3. **Architecture Transfer**: Replace ReLU with Tangma in a standard architecture (e.g., ResNet-18) on CIFAR-10 to test whether performance gains transfer beyond custom CNN designs