---
ver: rpa2
title: 'DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold
  Optimization'
arxiv_id: '2504.16639'
source_url: https://arxiv.org/abs/2504.16639
tags:
- data
- samples
- dataset
- classification
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of poor performance of traditional
  PLSR models on data with uneven categories by proposing a Data Augmentation Partial
  Least Squares Regression (DAPLSR) model via manifold optimization. The DAPLSR model
  introduces SMOTE for data augmentation and VDM for selecting nearest neighbor samples
  to generate synthetic samples.
---

# DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization

## Quick Facts
- arXiv ID: 2504.16639
- Source URL: https://arxiv.org/abs/2504.16639
- Authors: Haoran Chen; Jiapeng Liu; Jiafan Wang; Wenjun Shi
- Reference count: 36
- Primary result: Proposed DAPLSR model significantly outperforms existing methods on imbalanced datasets by integrating SMOTE with VDM and solving via manifold optimization.

## Executive Summary
This paper addresses the challenge of poor PLSR performance on imbalanced datasets by introducing DAPLSR, which combines data augmentation with manifold optimization. The model uses VDM-enhanced SMOTE to generate semantically meaningful synthetic samples, then employs Riemannian optimization to solve the PLSR problem while respecting geometric constraints. Experiments on EYaleB, COIL-20, USPS, and Brodatz datasets demonstrate superior classification performance compared to baseline methods.

## Method Summary
DAPLSR integrates three key components: (1) SMOTE with VDM for intelligent data augmentation that selects neighbors based on class conditional probabilities rather than Euclidean distance, (2) manifold optimization that solves PLSR by leveraging geometric properties of constraint spaces through Riemannian gradients and retraction operators, and (3) alternating optimization of projection matrices W (on Generalized Stiefel manifold) and C (on Oblique manifold). The approach generates synthetic minority samples and then finds optimal latent variable projections while maintaining mathematical constraints.

## Key Results
- On EYaleB dataset, DAPLSR reduced classification error rate by approximately 0.51% compared to model without augmentation when retaining 34 components
- Significant improvements across multiple evaluation metrics (accuracy, G-mean, precision, recall, F-measure) on all tested datasets
- Demonstrated superior performance compared to existing PLSR variants including PLSRGGR
- Successfully handled imbalanced classification problems in face, object, digit, and texture recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VDM integration with SMOTE improves synthetic sample quality in high-dimensional feature space.
- **Mechanism:** VDM weights features based on class conditional probability rather than Euclidean distance, selecting neighbors that preserve decision boundaries of minority classes.
- **Core assumption:** Class conditional probabilities provide more robust distance metrics than raw intensity in classification contexts.
- **Evidence anchors:** Abstract states VDM selects "nearest neighbor samples that closely resemble original samples"; section 3 describes VDM using P(c|a_i) to minimize synthetic-original overlap.
- **Break condition:** If features are continuous without clear discretization or class conditional probabilities are sparse, VDM weights become unstable.

### Mechanism 2
- **Claim:** Manifold optimization yields more accurate projection matrices by adhering to geometric constraints.
- **Mechanism:** PLSR requires orthogonal projection matrices (Stiefel manifold) and normalized vectors (Oblique manifold); manifold optimization enforces these constraints natively through Riemannian gradients.
- **Core assumption:** Local geometric structure of constraint space is better exploited via manifold optimization than standard Lagrange multipliers.
- **Evidence anchors:** Abstract mentions "geometric properties of constraint space to improve model degradation"; section 3 provides explicit gradient calculations on manifolds.
- **Break condition:** Extreme manifold curvature or improper step size causes retraction failure and numerical overflow.

### Mechanism 3
- **Claim:** Data augmentation mitigates small sample size problem in PLSR, allowing capture of more robust latent variables.
- **Mechanism:** PLSR degrades when samples are close to or fewer than features; SMOTE increases sample count, stabilizing covariance matrix estimation.
- **Core assumption:** Synthetic samples are valid approximations of underlying data distribution.
- **Evidence anchors:** Abstract states DAPLSR "significantly outperforms existing methods"; table 1 shows error reduction from 0.0322 to 0.0271 on EYaleB with 34 components.
- **Break condition:** Augmentation causes class overlap, generating synthetic samples in majority class decision boundaries.

## Foundational Learning

- **Concept: Partial Least Squares Regression (PLSR)**
  - **Why needed here:** Base learner that projects both input X and target Y to lower-dimensional latent space to maximize covariance, distinct from PCA which only considers X variance.
  - **Quick check question:** Can you explain why maximizing covariance between X-scores and Y-scores is better for prediction than maximizing variance in X alone?

- **Concept: Manifold Optimization (Riemannian Geometry)**
  - **Why needed here:** Core novelty of solver; constraints like W^T W = I define curved surfaces where Euclidean gradients don't work.
  - **Quick check question:** Why can't you simply use standard Gradient Descent on weight matrices W and C without retraction operators?

- **Concept: Value Difference Metric (VDM)**
  - **Why needed here:** Mechanism for "smart" neighbor selection that weights feature differences based on class probability.
  - **Quick check question:** How does VDM differ from Euclidean distance when handling irrelevant features in classification?

## Architecture Onboarding

- **Component map:** Input Layer -> Augmentation Module (VDM calculation -> Neighbor Selection -> SMOTE Interpolation) -> Optimization Core (Initialization of W, C -> Loop: Calculate Euclidean Gradient -> Project to Tangent Space -> Retraction -> Update W, C) -> Prediction Layer (Calculate scores T = X_s W and regression coefficients Q)

- **Critical path:** The Retraction Operator in Algorithm 1 (Lines 10 & 18). If retraction doesn't correctly map updated tangent vector back onto manifold (preserving orthogonality), solution becomes mathematically invalid and unstable.

- **Design tradeoffs:**
  - Accuracy vs. Complexity: Manifold optimization is computationally more expensive per iteration than NIPALS but converges to better local minimum.
  - Stability vs. Diversity: VDM-controlled SMOTE ensures safe, realistic samples but might be less diverse than Gaussian noise injection.

- **Failure signatures:**
  - Gradient Explosion: If step size α is too large during manifold descent, retraction fails causing NaNs.
  - Overfitting to Synthetic Data: If validation loss diverges while training loss drops, model memorizes synthetic artifacts.

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Run DAPLSR on simple 2D linearly separable imbalanced dataset; visualize decision boundary before/after augmentation to ensure SMOTE interpolates correctly.
  2. **Ablation Study:** Run DAPLSR with standard Euclidean SMOTE vs. VDM-SMOTE on EYaleB dataset to isolate performance gain specifically from VDM metric.
  3. **Convergence Analysis:** Plot objective function value (Eq. 10) vs. iteration count for Manifold Optimizer vs. standard PLSR solver (NIPALS) to verify faster/superior convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the DAPLSR model perform on non-image, tabular datasets?
- **Basis in paper:** Experimental validation restricted to image datasets (EYaleB, COIL-20, USPS, Brodatz), leaving efficacy for other data types unverified.
- **Why unresolved:** Image data possesses specific high-dimensional structures; utility of VDM and manifold optimization on sparse or heterogeneous tabular data remains unverified.
- **What evidence would resolve it:** Benchmarking DAPLSR on standard UCI classification datasets with uneven categories.

### Open Question 2
- **Question:** How sensitive is the model to specific oversampling ratios used in SMOTE?
- **Basis in paper:** Different augmentation rates applied (120% for COIL-20, 40% for USPS) without analyzing impact of varying these hyperparameters.
- **Why unresolved:** Unclear if reported accuracy improvements are robust to changes in sampling rate or require fine-tuned, dataset-specific optimization.
- **What evidence would resolve it:** Ablation study plotting classification error against range of SMOTE sampling percentages for fixed dataset.

### Open Question 3
- **Question:** What is the computational complexity trade-off compared to standard PLSR solvers?
- **Basis in paper:** Classification accuracy reported but training time or convergence speed analysis for manifold optimization step not provided.
- **Why unresolved:** Manifold optimization is generally more computationally intensive per iteration than linear methods like NIPALS, potentially limiting scalability.
- **What evidence would resolve it:** Reporting wall-clock training times and iteration counts for DAPLSR versus baseline models on same hardware.

## Limitations
- Specific convergence thresholds (ε₁, ε₂) for manifold optimization algorithm are initialized but not numerically defined, making exact reproduction challenging.
- Number of nearest neighbors (k) for SMOTE-VDM and formulation of positive definite matrix B for generalized Stiefel manifold are unspecified.
- Computational complexity trade-offs compared to standard PLSR solvers are not addressed explicitly.

## Confidence

**High Confidence:** The general mechanism of using VDM with SMOTE to improve synthetic sample quality in high-dimensional spaces is well-supported by theoretical framework and experimental results. Manifold optimization approach for enforcing geometric constraints is clearly articulated.

**Medium Confidence:** Claim that manifold optimization converges to better local minimum than standard PLSR solvers is supported by experimental results but lacks direct comparative analysis of convergence behavior. Performance improvements across datasets are substantial but may be influenced by specific imbalanced nature of test datasets.

**Low Confidence:** Paper does not provide sensitivity analysis for key hyperparameters like number of nearest neighbors in VDM, learning rate for manifold optimization, or impact of different discretization strategies when applying VDM to continuous image data.

## Next Checks

1. **Convergence Analysis Validation:** Plot objective function value (Eq. 10) versus iteration count for both proposed manifold optimizer and standard NIPALS solver on same dataset to quantitatively compare convergence speed and quality of final solution.

2. **Hyperparameter Sensitivity Test:** Systematically vary number of nearest neighbors (k) in VDM-SMOTE algorithm and measure impact on classification performance across all datasets to identify optimal values and robustness to this parameter.

3. **Data Augmentation Ablation Study:** Implement and compare three variants on EYaleB: (a) Standard PLSR without augmentation, (b) PLSR with Euclidean SMOTE, and (c) PLSR with VDM-SMOTE, isolating specific contribution of VDM metric to overall performance improvement.