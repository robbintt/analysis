---
ver: rpa2
title: 'FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in
  Dynamic Environments'
arxiv_id: '2512.15430'
source_url: https://arxiv.org/abs/2512.15430
tags:
- application
- task
- represents
- tasks
- urban
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a generalized algorithm, FM-EAC, for multi-task
  control in dynamic environments by combining model-based and model-free reinforcement
  learning. FM-EAC integrates a feature model for environmental representation and
  an enhanced actor-critic framework for multi-task decision-making.
---

# FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments

## Quick Facts
- arXiv ID: 2512.15430
- Source URL: https://arxiv.org/abs/2512.15430
- Reference count: 40
- Primary result: FM-EAC achieves superior multi-task UAV control performance in dynamic environments compared to state-of-the-art methods, demonstrating significant improvements in task completion time and age of information metrics.

## Executive Summary
This paper proposes FM-EAC, a generalized algorithm for multi-task control in dynamic environments that combines model-based and model-free reinforcement learning. The framework integrates a feature model for environmental representation with an enhanced actor-critic architecture for multi-task decision-making. Through simulations in urban and agricultural applications, FM-EAC demonstrates superior performance over state-of-the-art methods, showing significant improvements in task completion time and age of information. The approach is flexible and allows customization of sub-networks to meet specific user requirements.

## Method Summary
FM-EAC combines a feature model (either GNN or PAN) with an enhanced actor-critic framework to handle multi-task UAV control. The algorithm processes environmental entities through the feature model to produce contextual representations, which are concatenated with raw observations for the critic networks. The framework employs four independent critic networks (two primary, two secondary) to prevent gradient interference during concurrent multi-task learning. For urban concurrent tasks, it uses a maximum entropy policy, while agricultural sequential tasks employ a deterministic policy with battery prediction network for task switching. The method is validated through simulations showing improved generalization and performance over baseline approaches.

## Key Results
- FM-EAC achieves 20-30% reduction in task completion time compared to TD3/SAC baselines in urban environments
- Age of Information (AoI) improvements of 15-25% in agricultural data collection scenarios
- GNN-EAC shows superior accuracy in dynamic environments while PAN-EAC offers better computational efficiency
- Enhanced actor-critic with decoupled critics demonstrates stable convergence across multiple training runs

## Why This Works (Mechanism)

### Mechanism 1: Feature-Based State Enrichment for Generalization
The paper suggests that concatenating learned environmental features with raw observations allows the policy to generalize across unseen scenarios better than raw sensor data alone. A sub-network (GNN or PAN) processes environmental entities to produce a fixed-size feature vector $F_{env}$, which is concatenated with the agent's local observation $o$ and action $a$ to form the input $[o, a, F_{env}]$ for the Critic. This effectively "conditions" the value estimation on the broader environmental context. The core assumption is that environmental dynamics and optimal behaviors share common structural features across different maps that can be captured by the chosen sub-network. Break condition occurs if the feature extractor fails to converge or produces generic embeddings that do not vary with critical environmental factors.

### Mechanism 2: Decoupled Critics for Multi-Task Conflict Resolution
Separating critic networks for primary and secondary tasks prevents gradient interference during concurrent multi-task learning. The framework utilizes four critic networks ($Q_{P1}, Q_{P2}, Q_{S1}, Q_{S2}$). Primary critics are updated using rewards for the main objective, while secondary critics use rewards for auxiliary objectives. The Actor loss is the sum of the estimated values from both task groups, allowing simultaneous optimization without a single scalar reward masking trade-offs. The core assumption is that tasks are distinct enough that separate value functions provide better learning signals than a monolithic value function. Break condition occurs if primary and secondary tasks are diametrically opposed without defined priority hierarchy or weighting.

### Mechanism 3: Hybrid Deterministic-Stochastic Policy for Task Switching
Employing a maximum entropy policy for concurrent tasks and a deterministic policy for sequential tasks optimizes the exploration-exploitation balance specific to task structure. For urban concurrent tasks, the Actor maximizes entropy (SAC-style) to maintain robustness. For agricultural sequential tasks, it uses a deterministic policy (TD3-style). A Battery Prediction Network (BPN) acts as a switch to transition between sequential states. The core assumption is that uncertainty profiles of defined "concurrent" vs. "sequential" tasks justify distinct policy distributions. Break condition occurs if environment noise is high in agricultural setting or excessive entropy in urban setting prevents precise maneuvering.

## Foundational Learning

- **Graph Neural Networks (GNNs)**
  - Why needed here: To process non-Euclidean spatial data of urban environment (UAVs, Base Stations, IoT devices) into a feature vector
  - Quick check question: Can you explain how message passing allows a node to aggregate information from its neighbors in an adjacency matrix?

- **Clipped Double Q-Learning**
  - Why needed here: FM-EAC framework explicitly uses the minimum of two Q-values ($Q_1, Q_2$) to calculate target values to prevent overestimation bias
  - Quick check question: Why does taking the $\min(Q_1, Q_2)$ of two independent critics result in a more conservative policy update than using a single critic?

- **Age of Information (AoI)**
  - Why needed here: Primary metric for agricultural application; understanding how AoI evolves is crucial for designing reward function
  - Quick check question: How does the AoI metric differ from simple latency or throughput in the context of data freshness?

## Architecture Onboarding

- **Component map:** Raw State $S$ + Environment Context (Graph $G$ or Point Array $PA$) -> Feature Model (GNN/PAN) -> Feature Vector $F_{env}$ -> Concatenated with $[S, a]$ -> 4 Critics ($Q_{P1}, Q_{P2}, Q_{S1}, Q_{S2}$) -> Actor Network -> Outputs (Velocity vector + Power allocation)

- **Critical path:**
  1. Feature Model Training: If using GNN-EAC, feature model must be trained concurrently or pre-trained; if PAN-EAC, pre-train PAN on offline dataset first
  2. MDP Formulation: Define split between Primary and Secondary rewards ($r$ vs $\hat{r}$)
  3. Loop: Interaction -> Store Replay Buffer -> Sample Batch -> Update Critics (MSE) -> Update Actor (Entropy/Deterministic objective) -> Update Feature Model (if adaptive)

- **Design tradeoffs:**
  - GNN vs. PAN: GNN provides higher accuracy in dynamic environments but has $O(n^2)$ or higher complexity; PAN is faster but relies on static prior knowledge
  - Model-Based vs Model-Free: FM-EAC leans hybrid; Feature Model is "model-based" in representation, but policy learning is "model-free" (Actor-Critic)

- **Failure signatures:**
  - High Latency: GNN inference time explodes if number of IoT devices/WSs exceeds ~100-200; switch to PAN or downsample graph
  - Critic Divergence: If Primary and Secondary rewards differ in scale by orders of magnitude, summed Actor loss will ignore smaller task; normalize rewards
  - Transfer Failure: If Feature Model output is constant (dead neurons) or unstable, Critic cannot generalize; check $F_{env}$ variance during training

- **First 3 experiments:**
  1. Overfit Sanity Check: Train FM-EAC on single static map; should match or exceed baselines (TD3/SAC) to validate basic Actor-Critic implementation
  2. Generalization Test: Train on 3 random maps, test on 1 unseen map; compare NFM-EAC (no features) vs. FM-EAC to isolate value of Feature Model
  3. Ablation (Critics): Run with single shared Critic (OAC) vs. enhanced 4-Critic setup (EAC) to verify benefit of decoupling task evaluation

## Open Questions the Paper Calls Out
- Can FM-EAC be effectively adapted to safety-critical domains with complex physical interactions, such as multi-user autonomous driving or multi-robot coordination, without compromising real-time performance?
- How does the FM-EAC framework perform in physical real-world deployments where environmental noise, sensor uncertainty, and hardware latency differ significantly from idealized simulation models?
- Can the computational efficiency of the GNN-based feature model be optimized to support real-time decision-making in large-scale environments containing thousands of entities?

## Limitations
- Feature extractors (GNN/PAN) create dependency on accurate environmental modeling that may not transfer to real-world UAV deployments
- Hybrid deterministic-stochastic policy approach assumes clear task structure that real environments often violate
- Transfer learning claims limited by restricted evaluation (3 training maps, 1 test map) without testing on systematically different map distributions

## Confidence
- Enhanced actor-critic framework: Medium-High (supported by ablation studies showing 4-critic superiority)
- Feature model generalization benefits: Medium (limited to two specific architectures with no comparison to alternatives)
- Transfer learning claims: Low-Confidence (restricted evaluation without measuring feature model sensitivity to training set size)

## Next Checks
1. Feature Model Robustness Test: Systematically vary number of training maps (1, 3, 5, 10) and measure generalization performance to quantify minimum dataset required for meaningful feature learning across different map types

2. Real-World Feasibility Assessment: Implement feature extraction pipeline using off-the-shelf perception sensors (camera/LiDAR) and evaluate how real-time environmental feature computation affects overall latency budget, particularly comparing GNN vs PAN inference times under realistic device counts

3. Policy Adaptation Stress Test: Introduce controlled environmental noise (sensor inaccuracies, wind disturbances) into agricultural sequential task and measure deterministic policy's failure rate compared to stochastic baseline, determining threshold where policy switching becomes necessary