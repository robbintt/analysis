---
ver: rpa2
title: 'Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for
  Knowledge Injection without Catastrophic Forgetting'
arxiv_id: '2505.00029'
source_url: https://arxiv.org/abs/2505.00029
tags:
- knowledge
- general
- arxiv
- visual
- sdft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of injecting specialized knowledge
  into Large Vision-Language Models (LVLMs) without causing catastrophic forgetting
  of general capabilities. The proposed Structured Dialogue Fine-Tuning (SDFT) approach
  uses a three-phase dialogue structure: Foundation Preservation reinforces pre-trained
  visual-linguistic alignment, Contrastive Disambiguation establishes knowledge boundaries
  using unrelated concepts, and Knowledge Specialization embeds domain-specific information
  with chain-of-thought reasoning.'
---

# Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting

## Quick Facts
- **arXiv ID:** 2505.00029
- **Source URL:** https://arxiv.org/abs/2505.00029
- **Reference count:** 40
- **Primary result:** Proposed SDFT achieves significant improvements in knowledge injection across personalized entities, abstract concepts, and domain expertise tasks while maintaining strong general capability retention, e.g., 79.2% accuracy on PathVQA with 69.2% general retention.

## Executive Summary
This paper addresses the challenge of injecting specialized knowledge into Large Vision-Language Models (LVLMs) without causing catastrophic forgetting of general capabilities. The authors propose Structured Dialogue Fine-Tuning (SDFT), a three-phase dialogue structure that uses weighted multi-turn supervision to balance knowledge acquisition and retention. The approach demonstrates significant improvements in knowledge injection across diverse domains including personalized entities, abstract concepts, and biomedical expertise while maintaining strong general capability retention.

## Method Summary
SDFT employs a three-phase dialogue structure for fine-tuning LVLMs. The method uses a synthesis model (Qwen2-VL-72B) to generate training data, while the base model (Qwen2-VL-2B/7B) serves as both the student and the generator for preservation data. The three phases are: Foundation Preservation (base model generates captions), Contrastive Disambiguation (base model generates negative responses to unrelated concepts), and Knowledge Specialization (synthesis model generates detailed reasoning for target concepts). The training uses weighted cross-entropy loss with weights α₁=0.2, α₂=0.3, α₃=0.5 to balance the three dialogue turns.

## Key Results
- SDFT achieves 79.2% accuracy on PathVQA biomedical dataset while maintaining 69.2% general retention
- Significant improvements in knowledge injection across personalized entities, abstract concepts, and domain expertise tasks
- Outperforms existing methods in balancing knowledge acquisition with general capability preservation
- Ablation studies confirm the importance of each phase, particularly showing 10.8% improvement in general retention when using self-substitution

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Disambiguation for Knowledge Isolation
The framework introduces a "Contrastive Disambiguation" turn where the model is asked to relate an image to a deliberately unrelated concept, forcing a negative response. This acts as a data-centric regularizer, creating semantic boundaries that prevent new specialized knowledge from overwriting general features. Removing this phase drops general retention from 71.2% to 64.3%.

### Mechanism 2: Distribution-Preserving Data Synthesis
Unlike standard knowledge distillation, SDFT uses the base model's own outputs for general tasks in the Foundation Preservation phase. By training on its own generated outputs, the model optimizes for target knowledge without deviating from its original distribution on general tasks, minimizing drift in pre-existing output distribution.

### Mechanism 3: Weighted Multi-Turn Supervision
Differential weighting of the loss function (α₁=0.2, α₂=0.3, α₃=0.5) allows selective plasticity, preserving low-level alignment while aggressively updating semantic associations. This creates effective knowledge boundaries and balances acquisition and retention.

## Foundational Learning

- **Catastrophic Forgetting:** Neural networks overwrite weights when learning new tasks. Essential to understand why a structured, multi-turn approach is necessary to rehearse old capabilities while learning new ones.
  - *Quick check:* Why does simply training on high-quality medical QA pairs cause a model to forget how to describe a generic image of a cat?

- **Cross-Entropy Loss Weighting:** The method relies on a weighted sum of losses rather than a standard average. Understanding how loss scalars control gradient magnitude is required to tune the balance between general retention and specialization.
  - *Quick check:* If α₃ (specialization weight) is significantly larger than α₁, what happens to the model's ability to perform general captioning?

- **Prompt Engineering / Dialogue Templates:** SDFT is a "data-centric" approach. The efficacy depends entirely on the design of the dialogue template to elicit the right separation between "What is this?" and "Is this [Target]?"
  - *Quick check:* How does the phrasing of the query in Turn 2 differ from Turn 3 to ensure the model learns a "negative" boundary?

## Architecture Onboarding

- **Component map:** Synthesis Model (Qwen2-VL-72B) -> Base Model (Qwen2-VL-2B/7B) -> Dialogue Constructor
- **Critical path:**
  1. Data Gen: Target images -> Synthesis model generates Q₃ -> derive Q₂ -> Base model generates A₁ and A₂ -> Synthesis model generates A₃
  2. Training: Fine-tune Base Model on structured dataset using Weighted Cross-Entropy Loss
- **Design tradeoffs:**
  - Data Quality vs. Speed: Generating chain-of-thought responses with 72B model is slow but essential for quality
  - Strictness of Disambiguation: Choosing "unrelated knowledge" that is too easy might not provide enough gradient signal
- **Failure signatures:**
  - Concept Bleeding: Model uses domain-specific terminology to describe general images
  - Rejection Overuse: Model learns to reject everything if contrastive phase is overweighted
  - Template Overfitting: Model only answers correctly if questions match specific training templates
- **First 3 experiments:**
  1. Verify Self-Consistency: Check if Base Model generates coherent negative responses for Turn 2
  2. Weight Ablation: Train with varying α₃ (0.4, 0.5, 0.6) to find "forgetting cliff edge"
  3. General Capability Probe: Test on held-out general dataset immediately after fine-tuning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain:

1. How sensitive is the Contrastive Disambiguation phase to the specific selection of "unrelated knowledge"?
2. Can the multi-turn loss weights be adapted dynamically rather than set as fixed constants?
3. Is the effectiveness of SDFT constrained by the capability ceiling of the synthesis model used to generate the dialogue data?

## Limitations
- Knowledge boundaries are hard to enforce and may fail under adversarial queries
- Heavy dependence on synthesis model quality for generating accurate knowledge responses
- General retention metrics are coarse and don't provide per-task retention curves
- No long-term forgetting analysis beyond immediate post-fine-tuning evaluation

## Confidence

**High Confidence (>80%):**
- SDFT framework improves knowledge injection accuracy compared to raw SFT in tested domains
- Three-phase dialogue structure provides principled way to inject structured knowledge

**Medium Confidence (50-80%):**
- Contrastive Disambiguation phase meaningfully improves general retention
- Weighted loss function (α₁=0.2, α₂=0.3, α₃=0.5) is near-optimal for balancing knowledge injection and retention

**Low Confidence (<50%):**
- Knowledge boundaries enforced by Contrastive Disambiguation are robust to adversarial queries
- SDFT approach generalizes to arbitrary domains without manual tuning of unrelated concepts

## Next Checks

1. **Adversarial Boundary Test:** Design out-of-distribution queries mixing target and unrelated concepts to test robustness of semantic boundaries created by Contrastive Disambiguation.

2. **Synthesis Model Error Analysis:** Manually inspect 50-100 synthesized A₃ responses for hallucination and factual errors, correlating error rates with downstream model accuracy.

3. **Multi-Turn Retention Decay:** After SDFT fine-tuning, continue training on separate general dataset for 1-3 additional epochs to measure how quickly general capability retention degrades compared to baseline SFT.