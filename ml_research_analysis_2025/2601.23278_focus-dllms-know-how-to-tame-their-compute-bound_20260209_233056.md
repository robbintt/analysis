---
ver: rpa2
title: 'FOCUS: DLLMs Know How to Tame Their Compute Bound'
arxiv_id: '2601.23278'
source_url: https://arxiv.org/abs/2601.23278
tags:
- focus
- tokens
- token
- decodable
- decoded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational inefficiency of Diffusion
  Large Language Models (DLLMs), where only a small fraction of tokens are decoded
  per diffusion step while the full block is computed, resulting in massive arithmetic
  redundancy. FOCUS tackles this by leveraging the observation that token importance
  scores in early layers strongly correlate with decoding probability.
---

# FOCUS: DLLMs Know How to Tame Their Compute Bound

## Quick Facts
- **arXiv ID:** 2601.23278
- **Source URL:** https://arxiv.org/abs/2601.23278
- **Reference count:** 40
- **Key outcome:** Achieves up to 3.52× throughput improvement over LMDeploy with negligible overhead and preserved/improved generation quality across multiple benchmarks.

## Executive Summary
Diffusion Large Language Models (DLLMs) suffer from massive computational inefficiency because they process the full token block at each diffusion step despite decoding only a small fraction. FOCUS addresses this by dynamically evicting non-decodable tokens based on attention-derived importance deltas between early layers, reducing FLOPs and increasing effective batch size. Evaluations show FOCUS achieves substantial throughput gains while maintaining or improving generation quality across GSM8K, MBPP, BBH, and Wikitext benchmarks.

## Method Summary
FOCUS leverages the observation that attention-derived token importance scores in early layers strongly correlate with decoding probability. It dynamically evicts non-decodable tokens using a training-free importance delta metric, computing the difference between Layer 1 and Layer 0 attention scores to identify semantically coherent tokens. The system dynamically budgets retention based on historical decoding yield and instantaneous signal variance, while enforcing structural constraints (AR-Context Preservation and Placeholder Integrity) to prevent generation corruption. Custom Triton kernels handle importance computation, selection with constraints, state compaction, and sparse KV cache filling.

## Key Results
- Achieves up to 3.52× throughput improvement over LMDeploy
- Maintains or improves generation quality (BLEU, ROUGE, Exact Match, Pass@1 metrics)
- Reduces FLOP overhead to less than 2% of step latency
- Outperforms alternative methods like StreamingLLM and H2O in both speed and quality

## Why This Works (Mechanism)

### Mechanism 1
The importance delta (∆I = I^Layer1 - I^Layer0) between consecutive early attention layers predicts which tokens will decode successfully. Layer 0 attention reflects static priors and noise since Q/K projections come directly from noisy embeddings without cross-token interaction. Layer 1 operates on mixed hidden states where semantically coherent tokens emerge as context anchors. Subtracting Layer 0 scores from Layer 1 acts as common-mode rejection, isolating the "semantic lift" that distinguishes decodable tokens. Evidence shows decodable tokens cluster at high delta percentiles (52.6-61.7% in top quintile vs ~20% for non-decodable).

### Mechanism 2
Dynamic budgeting based on both historical decoding yield and instantaneous signal variance prevents over-aggressive eviction. The retention budget K = min(B, max(⌈α × N̄_decoded⌉, N_σ)) combines historical exponential moving averages scaled by expansion factor α > 1 with variance-based counts of tokens whose delta exceeds one standard deviation. The historical term provides a safe baseline; the variance term allows adaptive expansion when many high-confidence tokens emerge.

### Mechanism 3
Structural constraints (AR-Context Preservation + Placeholder Integrity) prevent generation corruption despite sparse token processing. For each selected candidate token, force-retain its immediate predecessor to preserve local t_i ← t_{i-1} dependency chains inherited from AR pretraining. Retain all unprocessed masked tokens preceding any selected candidate to maintain correct relative positional offsets and KV state availability.

## Foundational Learning

- **Block-Diffusion Paradigm**: FOCUS operates on Block-Diffusion DLLMs (SDAR, LLaDA2.0), not Full-Diffusion models. Understanding this distinction explains why exact KV caching is possible and where computational redundancy emerges. Quick check: Can you explain why Block-Diffusion enables exact KV caching while Full-Diffusion requires periodic recomputation?

- **Compute-Bound vs Memory-Bound Inference**: DLLMs shift from memory-bound (ARLLMs) to compute-bound regime due to processing B tokens per step. This explains why batching gains plateau and FOCUS's FLOP reduction is the key lever. Quick check: Why does increasing batch size help ARLLM throughput but hit a "computational wall" for DLLMs?

- **Attention-Based Importance Scoring**: FOCUS repurposes techniques from KV cache eviction but applies them to query token eviction instead. Understanding how attention mass concentrates on "heavy-hitter" tokens is foundational. Quick check: How does the importance metric in Eq. 2 aggregate attention weights differently from standard attention analysis?

## Architecture Onboarding

- **Component map:** Eager Prefix (Layers 0-1) -> Graph-Captured Suffix (Layers 2-L) -> Scheduler State -> Custom Triton Kernels
- **Critical path:** 1) Compute Layer 0-1 Q/K projections for full block 2) Calculate ∆I and determine budget K 3) Select top-K candidates with constraint enforcement 4) Gather hidden states for S only -> compacted tensors 5) Execute remaining layers on compacted representation 6) Decode, update stats, apply Neighbor-Aware cache commit
- **Design tradeoffs:** α (expansion factor) controls safety margin vs FLOP savings; confidence threshold balances admission of high-confidence vs incorrect tokens; multi-loop optimization disabled for state consistency; CUDA Graph bucketization fine-grained for small Nretained, coarse-grained for large Nretained
- **Failure signatures:** Quality collapse with standard Delayed Cache (GSM8K drops ~5 points), throughput regression at small batch + MoE (batch=64 shows slight slowdown), random/bottom selection degrades quality catastrophically
- **First 3 experiments:** 1) Validate importance-delta correlation on your model before implementing 2) Ablate α and confidence threshold with Table 4-style grid search 3) Profile overhead vs FLOP reduction to ensure <2% overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Distributional Sensitivity: Core assumption may break under domain shifts (medical, legal, specialized code)
- Model Architecture Dependencies: Relies on AR pretraining maintaining local dependencies; may not work with de-novo bi-directional training
- Small Batch Size Performance: Overhead exceeds benefits at batch=64 on LLaDA2.0 (3.84% throughput regression)

## Confidence
- **High Confidence:** FLOP reduction claims (verified through direct measurement on A100 with CUDA profiling)
- **Medium Confidence:** Quality preservation claims (statistically significant but dependent on correlation assumption)
- **Low Confidence:** Generalizability to non-evaluated domains and architectures (no evidence outside tested domains)

## Next Checks
1. **Domain Shift Robustness Test:** Apply FOCUS to biomedical literature or legal document generation and measure whether attention-delta correlation holds; compare decodable/non-decodable token separation quality to original results
2. **Architectural Dependency Verification:** Implement FOCUS on DLLM variant without AR pretraining; measure whether Neighbor-Aware caching is still necessary and whether quality degradation occurs without it
3. **Multi-Objective Scaling Analysis:** Systematically vary batch size (16-512) and model size; plot throughput vs quality curves to identify crossover points where overhead exceeds benefits; determine if α=1.5 needs domain-specific tuning