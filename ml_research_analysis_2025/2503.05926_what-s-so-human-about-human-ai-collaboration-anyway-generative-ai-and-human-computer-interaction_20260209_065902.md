---
ver: rpa2
title: What's So Human about Human-AI Collaboration, Anyway? Generative AI and Human-Computer
  Interaction
arxiv_id: '2503.05926'
source_url: https://arxiv.org/abs/2503.05926
tags:
- interaction
- human
- human-ai
- interactions
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies five key features of human-human interaction
  relevant for designing human-AI collaboration systems: indeterminacy, contextual
  integrity, contextual controls, trust/mistrust/vulnerability, and translation. Through
  interviews with industry personnel and empirical work developing a multimodal AI
  assistant for manufacturing, the authors demonstrate how these features can improve
  human-AI interactions.'
---

# What's So Human about Human-AI Collaboration, Anyway? Generative AI and Human-Computer Interaction

## Quick Facts
- arXiv ID: 2503.05926
- Source URL: https://arxiv.org/abs/2503.05926
- Authors: Elizabeth Anne Watkins; Emanuel Moss; Giuseppe Raffa; Lama Nachman
- Reference count: 34
- Primary result: Five key human-human interaction features (indeterminacy, contextual integrity, contextual controls, trust/mistrust/vulnerability, translation) improve human-AI collaboration when applied to multimodal AI assistant design.

## Executive Summary
This paper identifies five key features of human-human interaction—indeterminacy, contextual integrity, contextual controls, trust/mistrust/vulnerability, and translation—that are relevant for designing human-AI collaboration systems. Through interviews with industry personnel and empirical work developing a multimodal AI assistant for manufacturing, the authors demonstrate how these features can improve human-AI interactions. The work shows that applying insights from social science about human interaction can reduce user confusion, improve collaboration efficiency, and create more natural-feeling human-AI interactions in task-support scenarios.

## Method Summary
The research employed qualitative methods including semi-structured interviews with industry personnel and participatory design sessions with manufacturing end-users over 2+ years. Grounded theory analysis was applied to interview transcripts to identify interaction friction points. The team developed a multimodal AI assistant (combining CV and ASR) for manufacturing task support, using visual/narrative bootstrapping sessions for model training. Iterative UI/UX changes were implemented based on observed user friction during design sessions.

## Key Results
- Making contextual shifts explicit to users (e.g., explaining when switching between training different model types) reduces confusion and improves task efficiency.
- Providing users with actionable control over system context enables them to respond constructively to system needs.
- Translating system requirements into plain language and physical affordances improves data quality and reduces user error.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Making contextual shifts explicit to users reduces confusion and improves task efficiency in human-AI collaboration.
- Mechanism: Users maintain mental models of system context that guide interactions; unannounced context shifts violate expectations. Explicit notifications enable users to adjust behavior accordingly.
- Core assumption: Users form stable mental models of system behavior that guide interactions.
- Evidence anchors:
  - [abstract] "importance of making contextual shifts explicit to users (e.g., explaining when switching between training different model types)"
  - [section 3.2] User confusion when asked to show a bearing: "don't you guys already know what this looks like?" — resolved after explaining context shift to new modality.
- Break condition: If users do not form stable mental models or tasks are sufficiently atomic, explicit notifications may add unnecessary cognitive overhead.

### Mechanism 2
- Claim: Providing users with actionable control over system context enables them to respond constructively to system needs.
- Mechanism: Transparency alone is insufficient; exposing what systems need and how user actions affect understanding allows users to strategically adapt behavior to improve outcomes.
- Core assumption: Users are motivated to help the system improve and will modify behavior if they understand causal relationships.
- Evidence anchors:
  - [abstract] "providing users with control over system context"
  - [section 3.4] End-user asked for a list of phrases to cue system attention after learning about Importance Scores: "If I could say 'Hey watch this right now' and that would improve the AI's understanding, I would."
- Break condition: If users lack motivation to optimize system performance or relationships between actions and outcomes are too complex to communicate simply.

### Mechanism 3
- Claim: Translating system requirements into plain language and physical affordances improves data quality and reduces user error.
- Mechanism: Technical constraints (e.g., camera field of view) are opaque to end-users. Converting these into concrete, spatial representations bridges the gap between system needs and user understanding.
- Core assumption: Users can interpret and act on physical/spatial metaphors more reliably than abstract technical descriptions.
- Evidence anchors:
  - [abstract] "translating system needs into plain language"
  - [section 3.5] CV model visibility issue: users moved objects in wrong direction until team placed tape on workbench showing exactly what camera needed to "see."
- Break condition: If spatial/physical metaphors do not map cleanly to system constraints or users cannot physically manipulate environment.

## Foundational Learning

- Concept: **Contextual Integrity (Nissenbaum)**
  - Why needed here: Framework explains that interactions are governed by context-specific norms; violating these norms (e.g., changing context without notice) causes friction.
  - Quick check question: Can you explain why a user might be confused if an AI assistant switches from "learning your task" mode to "learning your objects" mode without notification?

- Concept: **Indeterminacy in Interaction**
  - Why needed here: Human interactions often lack predetermined endpoints; goals emerge collaboratively. Designing for indeterminacy means allowing flexible outcomes rather than enforcing rigid task structures.
  - Quick check question: How might you design an AI training interaction that accommodates the user's goal shifting mid-session?

- Concept: **Repair in Conversation Analysis**
  - Why needed here: Repair—restoring continuity after breakdowns—is essential to natural interaction. Systems that prevent or mishandle repair feel "unnatural."
  - Quick check question: When a user hesitates or asks a clarifying question, what should the system do to support repair?

## Architecture Onboarding

- Component map:
  - Context State Manager -> Translation Layer -> Explainability Interface -> Contextual Control Interface

- Critical path:
  1. Identify all context shifts possible in the system.
  2. For each shift, define user-facing notification content (what changed, why, what user should do).
  3. Map system constraints to translatable representations (language, visual markers).
  4. Instrument user actions to detect confusion signals (hesitation, repeated errors, clarifying questions).

- Design tradeoffs:
  - Explicit context notifications vs. interaction seamlessness: More notifications reduce confusion but may feel interruptive.
  - Full transparency vs. actionable transparency: Showing all internal metrics can overwhelm; focus on what users can *act on*.
  - Assumption: Users want to help improve the system; if users view AI adversarially, control mechanisms may be exploited.

- Failure signatures:
  - Users repeatedly asking "why are you asking me this?" (context shift not communicated).
  - Users taking correct actions but in wrong direction/spatial orientation (translation failed).
  - Users ignoring explainability dashboards entirely (transparency not actionable).

- First 3 experiments:
  1. **A/B test context notifications**: Compare task completion time and error rate when context shifts are announced vs. silent. Measure user-reported confusion.
  2. **Translation fidelity test**: For a spatial constraint (e.g., camera FOV), compare plain-language instructions vs. physical markers vs. both. Measure data quality and error correction frequency.
  3. **Actionable explainability probe**: Expose an internal metric (e.g., "Importance Score") with and without suggested actions. Track whether users modify behavior and whether system performance improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the application of the five identified interaction features quantitatively improve the efficiency and user experience of human-AI collaboration?
- Basis in paper: [explicit] The conclusion states the need to "empirically validate the findings that these interaction elements improve the quality of human-AI collaboration."
- Why unresolved: Current findings are based on qualitative interviews and specific case studies rather than broad quantitative validation.
- What evidence would resolve it: Controlled user studies comparing task performance, error rates, and subjective satisfaction in systems designed with these features versus those without.

### Open Question 2
- Question: What additional features of human-human interaction, beyond the five identified in this paper, are critical for designing human-AI collaboration systems?
- Basis in paper: [explicit] The conclusion notes that "the elements of human-human collaboration presented here are not exhaustive" and commits to incorporating additional elements in future work.
- Why unresolved: The scope of current research was limited to analysis of specific industry projects and grounded theory derived from those interviews.
- What evidence would resolve it: Broader ethnographic studies or literature reviews that identify and classify interaction features missing from current model, followed by design implementations testing these new features.

### Open Question 3
- Question: How can AI systems be designed to interact through their own "task execution" rather than solely through instruction or response, and does this improve interaction naturalness?
- Basis in paper: [inferred] The paper observes that in current designs, "AI systems were never described as being able to interact with humans through their own task execution," contrasting with human interaction styles.
- Why unresolved: Authors identify this as a limitation in current engineering assumptions but do not propose or test a design that allows AI to use task execution as an interaction modality.
- What evidence would resolve it: Prototyping an AI agent that communicates intent or state through physical or digital task actions, followed by user testing to measure perceived naturalness.

## Limitations
- Research relies entirely on qualitative methods without quantitative metrics for evaluating the five features' impact on collaboration efficiency.
- Technical implementation details of the multimodal AI assistant are not specified, making it impossible to assess whether findings generalize beyond this specific context.
- Sample size and demographics of industry personnel interviewed are not reported, limiting assessment of external validity.

## Confidence

- **High Confidence:** Making contextual shifts explicit reduces user confusion - well-supported by specific manufacturing example where users were confused about switching between task-learning and object-recognition modes.
- **Medium Confidence:** Providing users with actionable control over system context - supported by the phrase-cueing example but relies on a single user response rather than systematic testing.
- **Medium Confidence:** Translation mechanism (physical affordances for camera FOV) - well-demonstrated in the tape-on-workbench example but not validated across multiple contexts or user types.

## Next Checks

1. **Context Notification A/B Test:** Implement a controlled study where half of users receive explicit notifications when the system switches operational modes and half do not. Measure task completion time, error rates, and post-task confusion scores to quantify the impact of explicit context shifts.

2. **Translation Mechanism Validation:** Create a study comparing three conditions for communicating spatial constraints: plain-language instructions only, physical markers only, and combined approach. Measure data quality metrics and frequency of user error correction across conditions.

3. **Actionable Explainability Evaluation:** Develop a dashboard showing an internal system metric (e.g., confidence scores) with and without suggested user actions. Track whether users engage with the dashboard, modify their behavior, and whether system performance improves over time.