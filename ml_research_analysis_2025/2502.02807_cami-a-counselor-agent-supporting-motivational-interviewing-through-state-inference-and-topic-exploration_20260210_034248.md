---
ver: rpa2
title: 'CAMI: A Counselor Agent Supporting Motivational Interviewing through State
  Inference and Topic Exploration'
arxiv_id: '2502.02807'
source_url: https://arxiv.org/abs/2502.02807
tags:
- client
- topic
- counselor
- behavior
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAMI, a counselor agent that integrates client
  state inference, motivation topic exploration, and strategy selection for motivational
  interviewing (MI). CAMI employs a topic tree constructed from counseling sessions
  and Wikipedia, enabling exploration of diverse motivation topics to evoke change
  talk.
---

# CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration

## Quick Facts
- **arXiv ID:** 2502.02807
- **Source URL:** https://arxiv.org/abs/2502.02807
- **Reference count:** 40
- **Primary result:** CAMI achieves 53.1% success rate in evoking change talk, outperforming baselines with higher MI competency scores

## Executive Summary
CAMI is a modular counselor agent that integrates client state inference, topic exploration, and strategy selection for motivational interviewing (MI). Built on a hierarchical topic tree derived from counseling sessions and Wikipedia, CAMI systematically navigates motivation topics while adapting strategies based on client stages (Precontemplation/Contemplation/Preparation). Evaluation using simulated clients and expert assessments demonstrates superior MI competency and success rates compared to state-of-the-art methods, while maintaining realistic counselor-like behavior. The ablation study confirms the critical importance of both state inference and topic exploration components.

## Method Summary
CAMI employs a four-stage pipeline: state inference classifies clients into MI stages using GPT-4o or Llama-3.1-70B, topic exploration navigates a hierarchical tree (5 superclasses → 14 coarse → 59 fine-grained topics) via Step Into/Switch/Step Out operations, strategy selection chooses MI-specific interventions based on current state and topic, and response ranking selects the most coherent response among strategy-specific candidates. The system uses simulated clients with dynamic engagement and a moderator agent for session termination. Evaluation combines automated MITI scoring with human expert assessment on 38 manually annotated client profiles from the AnnoMI dataset.

## Key Results
- CAMI achieves 53.1% success rate in evoking change talk, significantly higher than baselines
- State inference accuracy reaches 93.32% with GPT-4o and 88.56% with Llama-3.1-70B
- CAMI explores less common motivation topics (e.g., Law: 40%, Education: 40%) where baselines score 0%
- Expert assessment shows CAMI outperforms baselines in MI competency across multiple dimensions

## Why This Works (Mechanism)

### Mechanism 1: State-Grounded Strategy Selection
Inferring client state improves strategy selection appropriateness, which increases MI competency scores. The state inference module classifies client into Precontemplation/Contemplation/Preparation stages (93.32% accuracy with GPT-4o). This state then conditions which MI strategies are prioritized—for example, focusing on trust-building in Precontemplation vs. addressing beliefs in Contemplation—resulting in more client-centered responses. Core assumption: The transtheoretical model's stage-appropriate interventions transfer validly to LLM-based counseling. Evidence anchors: [abstract] "ablation study underscores the critical roles of state inference and topic exploration in achieving this performance" and [section 4.5] "CAMI using GPT-4o and Llama-3.1 70B infer the correct client state most of the time at 93.32% and 88.56% respectively." Break condition: If state inference accuracy drops significantly (e.g., <70%), or if stage-appropriate strategies don't meaningfully differ in effectiveness.

### Mechanism 2: Hierarchical Topic Navigation for Change Talk Elicitation
Systematic topic exploration via a structured topic tree increases success in evoking change talk, particularly for less common motivation topics. The topic tree (5 superclasses → 14 coarse → 59 fine-grained topics, all Wikipedia-verified) enables counselors to navigate breadth-first during initial engagement, then drill down via Step Into/Switch/Step Out operations based on client feedback. This reduces topic fixation bias common in LLMs. Core assumption: Motivation topics in MI counseling can be hierarchically organized and that navigating this hierarchy systematically improves change talk elicitation. Evidence anchors: [abstract] "53.1% success rate in evoking change talk, significantly higher than baselines" and [section 4.4, Table 3] CAMI achieves success across all superclass topics including less popular ones (Law: 40%, Education: 40%) where baselines score 0%. Break condition: If topic tree coverage is insufficient for target population, or if navigation operations don't reliably respond to client engagement signals.

### Mechanism 3: Response Ranking for Strategy Bias Mitigation
Ranking candidate responses per strategy (rather than combining all strategies) reduces overuse of preferred strategies and improves coherence. When multiple strategies are selected (max 2), CAMI generates separate responses for each strategy plus a combined version, then prompts LLM to select the most contextually coherent response. This counters LLM preference bias toward certain response patterns. Core assumption: Response quality correlates with contextual coherence as judged by an LLM, and this ranking improves clinical appropriateness. Evidence anchors: [section 3.4] "we prompt the LLM to choose the most coherent response based solely on the current session context" and [section 1] "inherent preference bias of LLMs (Kang et al., 2024)" motivates this design. Break condition: If ranking criteria don't align with clinical quality, or if generation costs become prohibitive.

## Foundational Learning

- **Motivational Interviewing (MI) Principles**
  - Why needed here: CAMI's strategy selection and evaluation metrics (MITI, MISC) are MI-specific. Understanding concepts like change talk, sustain talk, and the spirit of MI (partnership, empathy) is essential for interpreting results and improving the system.
  - Quick check question: Can you explain why "cultivating change talk" differs from "softening sustain talk" in MI evaluation?

- **Transtheoretical Model (Stages of Change)**
  - Why needed here: The state inference module relies on Precontemplation/Contemplation/Preparation stages. Strategy selection is stage-conditioned, so misunderstanding stages leads to inappropriate counselor behavior.
  - Quick check question: What counselor behaviors are appropriate for a client in Precontemplation vs. Contemplation?

- **Hierarchical Knowledge Representation (Topic Trees)**
  - Why needed here: The topic exploration module navigates a 3-level hierarchy. Understanding tree traversal operations (Step Into, Switch, Step Out) and their triggering conditions is critical for debugging and extending the system.
  - Quick check question: Given client disengagement with a fine-grained topic but interest in its parent, which navigation operation should trigger?

## Architecture Onboarding

- **Component map:** Session Context → State Inference → (triggers) → Topic Exploration → Strategy Selection → Response Generation (per strategy) → Response Ranking → Final Counselor Response
- **Critical path:** State inference accuracy → Topic navigation quality → Strategy appropriateness → Response coherence. Errors cascade; state misclassification propagates through all downstream modules.
- **Design tradeoffs:**
  - Topic tree breadth vs. depth: Current 59 fine-grained topics may not cover all client motivations; expansion requires Wikipedia verification
  - Strategy limit (max 2): Reduces complexity but may exclude clinically relevant multi-strategy responses seen in HQ human sessions
  - LLM backbone choice: GPT-4o outperforms Llama-3.1-70B on state inference (93.32% vs. 88.56%) and global scores, but cost/availability differs
- **Failure signatures:**
  - **Stuck in Precontemplation:** Client never transitions → check if state inference correctly detects motivation mentions
  - **Topic loop:** Repeated Step Into/Switch without progress → review navigation operation triggers
  - **Question overuse:** High %OQ, low R/Q (all models show this) → experts note overuse of questions vs. reflections
  - **Superficial exploration:** High exploration focus scores (FOC) → topic tree may lack relevant fine-grained topics
- **First 3 experiments:**
  1. **State inference robustness test:** Perturb client utterances (synonyms, paraphrases) and measure state classification stability. Identify failure modes.
  2. **Topic tree coverage audit:** Run CAMI on held-out client profiles; log topic exploration paths and unmatched motivation topics. Quantify coverage gaps.
  3. **Ablation replication:** Replicate CAMI-TE (without topic exploration) vs. full CAMI on 10 diverse client profiles. Verify 53.1% → baseline drop pattern holds; analyze which topic superclasses benefit most.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is CAMI's performance to the accumulation of generation errors across its multi-step prompting pipeline (State Inference, Topic Exploration, Strategy Selection)?
- Basis in paper: [explicit] The authors state in the Limitations section that they "have not studied how sensitive is CAMI to the multiple steps of prompting due to the accumulation of generation errors."
- Why unresolved: The current framework relies on a linear chain of LLM prompts where an error in an early module (e.g., state inference) could cascade into subsequent modules, but this robustness has not been quantified.
- Evidence to resolve it: A sensitivity analysis measuring the degradation of the final response quality when intermediate steps are subjected to simulated noise or adversarial perturbations.

### Open Question 2
- Question: Can integrating inductive action rules (similar to the DIIR method) into the strategy selection module mitigate the expert-identified issues of over-questioning and lengthy responses?
- Basis in paper: [explicit] The paper notes that human experts "raised concerns about issues related to strategy use, such as counselor responses being too lengthy and using too many questions" and proposes enhancing the module by "incorporating the idea of action rules introduced in DIIR."
- Why unresolved: The current zero-shot prompting approach for strategy selection leads to specific behavioral biases (e.g., high frequency of questions) that expert reviewers flagged as suboptimal.
- Evidence to resolve it: An ablation study comparing the current prompting method against a hybrid approach using explicit inductive rules, specifically measuring the Reflection-to-Question (R/Q) ratio and response conciseness.

### Open Question 3
- Question: Can the STAR framework be effectively adapted to support other counseling modalities, such as Cognitive Behavioral Therapy (CBT), which rely on different mechanisms than Motivational Interviewing?
- Basis in paper: [explicit] The authors state: "Future research could explore the integration of MI and CBT techniques within a single counselor agent... the proposed STAR framework can be adapted to other counseling techniques."
- Why unresolved: The current framework is specialized for MI (e.g., evoking change talk, transtheoretical model states), and it is unclear if the same state inference and topic exploration architecture maps effectively onto CBT's focus on cognitive restructuring.
- Evidence to resolve it: Implementation of a CBT-specific "Topic Tree" and "State Inference" module, followed by evaluation on CBT-specific datasets to assess competency in CBT-specific skills.

### Open Question 4
- Question: To what extent does the reliance on simulated clients for evaluation overestimate CAMI's efficacy compared to real human interaction?
- Basis in paper: [inferred] While the paper argues for simulated clients to avoid the limitations of single "ground truth" datasets, the evaluation relies on GPT-4o to simulate the client and predict MITI scores. Expert evaluation (Table 5) shows a significant gap between CAMI and high-quality human counselors in "Cultivating Change Talk" (3.68 vs 4.06).
- Why unresolved: Simulated clients may be more compliant or predictable than real humans experiencing ambivalence, potentially masking the system's ability to handle the complexity of real-world resistance.
- Evidence to resolve it: A user study involving real human participants interacting with CAMI to assess the correlation between automated MITI scores and human-reported therapeutic alliance.

## Limitations
- Topic tree coverage may not generalize to all client populations, particularly those with motivations outside Wikipedia-derived taxonomy
- State inference accuracy assumes LLM-based stage classification transfers reliably to counseling contexts without validation against human experts
- Simulated client model may not capture full complexity of human therapeutic resistance and motivation dynamics

## Confidence
- **High Confidence:** CAMI's architecture design and implementation details (Medium) - The modular pipeline, topic tree construction, and evaluation metrics are well-specified and reproducible.
- **Medium Confidence:** State inference accuracy and MI competency improvements (Medium) - While reported metrics are strong, they depend on LLM reliability and simulated rather than human clients.
- **Low Confidence:** Generalization to real-world counseling (Medium) - The system shows promise in controlled simulations but hasn't been tested with actual clients or across diverse cultural contexts.

## Next Checks
1. **State Inference Robustness:** Test CAMI's state classification on paraphrased client utterances and compare against human expert annotations to validate the 93.32% accuracy claim and identify failure modes.
2. **Topic Tree Coverage Audit:** Deploy CAMI with diverse client profiles to identify motivation topics that fall outside the current 59-topic taxonomy, quantifying coverage gaps and testing expansion strategies.
3. **Human Clinical Trial:** Conduct a small-scale study comparing CAMI-generated responses against human counselor responses using blinded MI experts, measuring both technical competency and perceived therapeutic alliance.