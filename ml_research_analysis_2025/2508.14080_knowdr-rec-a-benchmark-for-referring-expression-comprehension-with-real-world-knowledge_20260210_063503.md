---
ver: rpa2
title: 'KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World
  Knowledge'
arxiv_id: '2508.14080'
source_url: https://arxiv.org/abs/2508.14080
tags:
- visual
- arxiv
- reasoning
- negative
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KnowDR-REC, a benchmark for evaluating Multi-modal
  Large Language Models (MLLMs) on knowledge-driven Referring Expression Comprehension
  (REC) tasks. Traditional REC benchmarks rely on intra-image cues and lack fine-grained
  annotations, failing to assess MLLMs' ability to integrate external knowledge and
  perform multi-hop reasoning.
---

# KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge

## Quick Facts
- arXiv ID: 2508.14080
- Source URL: https://arxiv.org/abs/2508.14080
- Reference count: 9
- Primary result: Existing MLLMs struggle with knowledge-intensive visual grounding, often relying on memorized shortcuts rather than genuine semantic understanding

## Executive Summary
The paper introduces KnowDR-REC, a benchmark for evaluating Multi-modal Large Language Models (MLLMs) on knowledge-driven Referring Expression Comprehension (REC) tasks. Traditional REC benchmarks rely on intra-image cues and lack fine-grained annotations, failing to assess MLLMs' ability to integrate external knowledge and perform multi-hop reasoning. KnowDR-REC addresses this gap by incorporating real-world knowledge, requiring fine-grained multimodal reasoning, and including negative samples constructed via temporal knowledge graph-based expression editing to test robustness and anti-hallucination ability. The benchmark also introduces novel evaluation metrics to probe models' internal reasoning processes. Evaluations of 16 state-of-the-art MLLMs reveal that existing models struggle with knowledge-intensive visual grounding, often relying on memorized shortcuts rather than genuine semantic understanding.

## Method Summary
KnowDR-REC addresses the limitations of traditional REC benchmarks by incorporating real-world knowledge, fine-grained multimodal reasoning, and negative samples. The dataset contains 1,042 positive triplets (image, declarative referring expression, bounding box) and 2,537 total expressions curated from ComplexWebQuestions, HotpotQA, KQA Pro, and MetaQA (filtered to person entities). Negative samples are generated through temporal knowledge graph tuple perturbations and image-text shuffling. The evaluation uses zero-shot inference on 16 MLLMs with CoT prompting, measuring both grounding accuracy and conditional metrics that reveal whether models rely on textual reasoning or visual shortcuts.

## Key Results
- Existing MLLMs show high grounding accuracy even when textual reasoning fails, indicating reliance on visual shortcuts
- Models achieve substantially lower accuracy on knowledge-intensive expressions compared to traditional attribute-based references
- The conditional metrics reveal significant decoupling between textual understanding and visual grounding performance

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Driven Constraint Satisfaction
If a referring expression requires external temporal or factual knowledge rather than just intra-image spatial relations, it forces MLLMs to perform multi-hop retrieval before visual grounding. The benchmark replaces simple descriptions with knowledge-intensive declarations, requiring models to first resolve the entity via internal knowledge before locating the visual entity.

### Mechanism 2: Semantic Disruption via Temporal Knowledge Graph (TKG) Perturbation
Generating negative samples by perturbing a single element in a structured knowledge tuple (Subject, Relation, Object, Time) isolates the model's ability to verify specific semantic constraints. This creates "hard negatives" that are syntactically near-identical but semantically distinct, preventing models from using simple keyword matching.

### Mechanism 3: Conditional Decoupling Analysis
Measuring visual grounding accuracy conditional on textual reasoning success reveals whether models rely on "memorized shortcuts" rather than genuine multimodal comprehension. By comparing grounding accuracy when textual reasoning is correct versus incorrect, the benchmark quantifies "Visual Grounding without Understanding."

## Foundational Learning

- **Concept: Referring Expression Comprehension (REC)** - The core task being benchmarked; distinguishing it from standard object detection is vital. Standard REC relies on visual attributes; this benchmark relies on external world knowledge.
- **Concept: Temporal Knowledge Graphs (TKG)** - Understanding the negative sampling strategy requires knowing how events are structured as (Subject, Relation, Object, Time).
- **Concept: Hallucination in MLLMs** - The benchmark specifically tests "anti-hallucination" by forcing models to reject mismatched pairs.

## Architecture Onboarding

- **Component map:** Data Curator (filter QA datasets → GPT-4o Rewriter → Declarative Expressions) -> Visual Retrieval (Wikipedia Image Fetch → YOLOv8 Filtering → Dual-team Annotation) -> Negative Generator (GPT-4o TKG Extractor → Tuple Perturbation → Text Rewrite) -> Evaluator (calculates AccText, AccIoU, and conditional metrics)
- **Critical path:** The TKG Perturbation Pipeline - if tuple extraction or rewriting creates grammatical errors or ambiguous references, negative samples lose their value as "subtle semantic traps"
- **Design tradeoffs:** Entity restriction to "Person" entities to ensure rich real-world knowledge availability; using "Basic Setting" vs. "Instruction-guided" reveals prompt sensitivity
- **Failure signatures:** High Acc@IoU on negative samples (shortcut memorization); high Error Rate in Basic Setting but low Accuracy in Instruction-guided Setting (prompt dependency)
- **First 3 experiments:** Baseline Profiling (measure gap between AccText and AccIoU), Negative Sensitivity Test (identify which semantic constraints models ignore most), Shortcut Probe (compare bounding boxes on Positive vs. Semantically Adversarial Negative)

## Open Questions the Paper Calls Out

### Open Question 1
How can model architectures be modified to enforce a strict dependence of visual grounding on textual reasoning, thereby eliminating the "Visual Grounding without Understanding" phenomenon? The authors identify a critical "decoupling" where models locate targets correctly even when textual reasoning fails.

### Open Question 2
Do the identified reliance on memorized shortcuts and poor robustness generalize to non-person entities where visual features are less distinct or semantically rich? The benchmark is limited to person entities, leaving applicability to other domains unexplored.

### Open Question 3
What training mechanisms are required to endow MLLMs with the ability to reject invalid queries in the "Basic Setting" without relying on explicit prompt engineering? The study notes that while models perform better in the "Instruction-guided" setting, they lack inherent robustness in the standard "Basic Setting."

## Limitations

- Benchmark relies on GPT-4o for data curation and negative sample generation, potentially introducing systematic biases
- Entity restriction to "Person" limits generalizability to other object categories
- Wikipedia image domain may not represent diverse real-world visual contexts
- Manual verification steps create reproducibility challenges and potential subjectivity

## Confidence

- **High Confidence:** The benchmark successfully identifies a gap in existing REC evaluations regarding knowledge integration and the existence of shortcut behaviors in current MLLMs
- **Medium Confidence:** The TKG-based negative generation methodology effectively isolates semantic reasoning failures
- **Low Confidence:** The claim that all models universally "struggle" with knowledge-intensive visual grounding

## Next Checks

1. **Semantic Consistency Audit:** Independently verify a random sample of 50 negative expressions to confirm perturbations create genuine semantic contradictions
2. **Cross-Domain Generalization Test:** Apply the benchmark methodology to non-person entities (e.g., specific landmarks, artworks) to assess generalization
3. **Shortcut Mechanism Isolation:** Design a controlled experiment where visual features are explicitly masked during inference to determine whether grounding accuracy on negatives drops when models cannot rely on visual shortcuts