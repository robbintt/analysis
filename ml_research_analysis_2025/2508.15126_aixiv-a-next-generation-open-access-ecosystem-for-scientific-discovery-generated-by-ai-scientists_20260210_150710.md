---
ver: rpa2
title: 'aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated
  by AI Scientists'
arxiv_id: '2508.15126'
source_url: https://arxiv.org/abs/2508.15126
tags:
- diffusion
- data
- review
- training
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces aiXiv, a next-generation open-access platform
  designed to support autonomous scientific discovery by AI agents. It addresses the
  challenge of disseminating high-quality AI-generated research content, which faces
  difficulties in traditional closed publication ecosystems.
---

# aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists

## Quick Facts
- arXiv ID: 2508.15126
- Source URL: https://arxiv.org/abs/2508.15126
- Reference count: 40
- Multi-agent platform for autonomous scientific discovery by AI scientists

## Executive Summary
aiXiv introduces a next-generation open-access platform designed to support autonomous scientific discovery by AI agents. The platform addresses the challenge of disseminating high-quality AI-generated research content in traditional closed publication ecosystems. Through a multi-agent architecture, aiXiv enables both human and AI scientists to submit, review, and refine research proposals and papers via iterative closed-loop processes. The system incorporates automatic retrieval-augmented evaluation, reviewer guidance, and robust defenses against prompt injection, demonstrating significant improvements in AI-generated research quality after iterative reviewing and refining.

## Method Summary
aiXiv employs a multi-agent system using five LLMs (Claude Sonnet 4, GPT-4o, GPT-4.1, Deepseek V3, Gemini 2.5 Pro) to create a closed-loop review-refine ecosystem. The platform features Single Review Mode, Meta Review Mode (Area Chair + 3-5 domain reviewers), and Pairwise Review Mode. Reviews are augmented with external literature via Semantic Scholar API retrieval. A 5-stage pipeline detects prompt injection attacks. Acceptance requires ≥3/5 votes from the LLM review panel. The system iteratively refines submissions through structured feedback addressing methodological quality, novelty, clarity, and feasibility.

## Key Results
- Acceptance rates increase from 0%→45.2% (proposals) and 10%→70% (papers) after revision
- 90%+ of revised proposals rated higher than originals; preference rises to ~100% with response letters
- RAG improves proposal pairwise accuracy from ~68-77% to ~77-78% across models

## Why This Works (Mechanism)

### Mechanism 1
Iterative review-refine cycles substantially improve AI-generated research quality through structured feedback across four dimensions (methodological quality, novelty/significance, clarity, feasibility). AI author agents revise submissions addressing specific weaknesses, with measurable quality gains per iteration. The process repeats with preference rising to ~100% when response letters are included.

### Mechanism 2
Retrieval-augmented generation (RAG) improves evaluation accuracy by grounding reviews in external literature. Review agents query Semantic Scholar API to identify missing citations, logical gaps, and novelty claims, producing more specific and actionable feedback. RAG improves proposal pairwise accuracy from ~68-77% to ~77-78% across models.

### Mechanism 3
Multi-AI voting reduces single-model evaluation bias through five heterogeneous LLMs independently reviewing each submission. Acceptance requires ≥3/5 "accept" votes, aggregating diverse model judgments and reducing individual model quirks. Acceptance rates demonstrate voting tracks quality improvements effectively.

## Foundational Learning

- **Diffusion models for generative AI**: Essential for evaluating aiXiv's generated content quality, particularly understanding DDPM forward/reverse processes, noise schedules, and KL divergence metrics in low-dimensional spaces. *Quick check: Can you explain why a dual-scale denoising architecture might better capture multi-modal distributions in low-dimensional spaces?*

- **Retrieval-augmented generation (RAG)**: Critical for understanding aiXiv's review pipeline that relies on RAG to ground evaluations in external literature. Engineers need to understand retrieval indexing, query formulation, and grounding strategies. *Quick check: How would you design a retrieval system to identify missing citations in a machine learning paper?*

- **Prompt injection attacks**: Necessary for understanding aiXiv's multi-stage defense pipeline and attack vectors including hidden text, encoding obfuscation, and semantic manipulation. *Quick check: What are three ways an adversary could embed malicious instructions in a PDF that would evade naive text extraction?*

## Architecture Onboarding

- **Component map**: Submission layer (API/MCP interfaces) → Review orchestration (Single/Meta/Pairwise modes) → RAG integration (Semantic Scholar API) → Prompt injection detection (5-stage pipeline) → Decision layer (Multi-AI voting panel) → Publication layer (DOI assignment, IP attribution)

- **Critical path**: Submission → prompt injection scan → review agent assignment → RAG-grounded evaluation → revision cycle (repeat 2-3x) → multi-AI vote → provisional/accepted publication

- **Design tradeoffs**: RAG improves accuracy but adds latency and API dependency; multi-AI voting reduces bias but multiplies compute costs ~5x; stricter prompt injection thresholds reduce attacks but may flag legitimate submissions; workshop-level acceptance standards balance openness with quality

- **Failure signatures**: Revision loops that don't improve quality (feedback not actionable); voting deadlocks (2-3 splits) suggesting model correlation; prompt injection false positives blocking legitimate submissions; RAG retrieval returning irrelevant papers degrading review specificity

- **First 3 experiments**:
  1. Run pairwise comparison on held-out ICLR paper pairs to validate RAG integration matches reported 77-81% accuracy
  2. Submit intentionally crafted prompt injection samples across attack categories to verify detection pipeline achieves >80% accuracy
  3. Execute full review-revise cycles on 10 AI-generated papers and measure acceptance rate change; target 50%+ improvement as per paper benchmarks

## Open Questions the Paper Calls Out

- **Open Question 1**: How can reinforcement learning (RL) be effectively integrated into the aiXiv ecosystem to allow AI agents to evolve complex reasoning and planning capabilities through interaction? Based on the Future Work section stating plans to "integrate reinforcement learning where AI agents can evolve through structured interactions." Unresolved due to defining stable reward functions for open-ended scientific discovery.

- **Open Question 2**: How can AI agents be enabled to autonomously acquire new research skills and domain knowledge without explicit reprogramming? Based on the Future Work section aiming to "enable AI agents to autonomously acquire new knowledge and skills through interaction." Unresolved due to current reliance on static prompts and fixed model weights.

- **Open Question 3**: How can adaptive learning strategies be developed to generalize effectively across diverse users, tasks, and domains interacting with the aiXiv platform? Based on the Limitations section identifying that "developing adaptive learning strategies that generalize effectively across diverse users, tasks, and domains remains an unresolved challenge." Unresolved due to difficulty transitioning from static synthetic benchmarks to dynamic scientific inquiry.

## Limitations

- Human evaluation benchmarks are not fully detailed, lacking human preference distribution and inter-rater reliability reporting
- RAG effectiveness is measured against human judgments but doesn't directly test ability to identify missing citations in AI-generated content
- Prompt injection detection claims >80% accuracy but lacks real-world adversarial testing against sophisticated attacks
- Claims about aiXiv representing a "next-generation open access ecosystem" are aspirational rather than empirically validated with real-world usage data

## Confidence

- **High confidence**: Multi-AI voting mechanism's basic functionality and acceptance rate improvements (0%→45.2% for proposals, 10%→70% for papers) are directly measurable and reproducible
- **Medium confidence**: Iterative review-refine quality improvement claims supported by controlled experiments but depend on human preference judgment quality and consistency
- **Low confidence**: Claims about aiXiv as a "next-generation open access ecosystem" are more aspirational than empirically validated

## Next Checks

1. Conduct inter-rater reliability analysis on human preference judgments used to validate RAG-augmented reviews, measuring Cohen's kappa across reviewers

2. Test prompt injection detection pipeline against held-out set of adversarial samples not included in the 105-paper training set, measuring false positive and false negative rates

3. Run longitudinal study measuring whether papers accepted through aiXiv's multi-iteration process show sustained quality improvements when evaluated by independent human reviewers 3-6 months later