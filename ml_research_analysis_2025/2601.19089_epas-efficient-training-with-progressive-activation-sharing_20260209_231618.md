---
ver: rpa2
title: 'EPAS: Efficient Training with Progressive Activation Sharing'
arxiv_id: '2601.19089'
source_url: https://arxiv.org/abs/2601.19089
tags:
- sharing
- training
- activation
- epas
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EPAS introduces a progressive activation sharing method to reduce
  redundancy in deeper transformer layers, improving both training and inference efficiency.
  It progressively expands an activation-sharing region during training by switching
  decoder layers to reuse QK activations from earlier layers.
---

# EPAS: Efficient Training with Progressive Activation Sharing

## Quick Facts
- **arXiv ID:** 2601.19089
- **Source URL:** https://arxiv.org/abs/2601.19089
- **Reference count:** 40
- **Primary result:** Progressive activation sharing improves training and inference efficiency while maintaining accuracy

## Executive Summary
EPAS introduces a progressive activation sharing method to reduce redundancy in deeper transformer layers, improving both training and inference efficiency. It progressively expands an activation-sharing region during training by switching decoder layers to reuse QK activations from earlier layers. This approach yields up to 11.1% faster training and up to 29% faster inference throughput while maintaining accuracy comparable to baseline models. It also enables flexible efficient model configurations during inference and transforms pretrained models into efficient ones via continual pretraining without distillation. Experiments on LLaMA models (125M–7B parameters) confirm consistent efficiency gains across hardware, and ablation studies validate design choices such as single large sharing blocks and inclusion of the last layer.

## Method Summary
EPAS progressively grows a sharing region during training by switching decoder layers to activation sharing mode at fixed intervals. The method starts with all layers in compute mode, then gradually transitions deeper layers to reuse QK activations from earlier layers while computing only V locally. This progressive approach allows the model to adapt to shared activations with minimal accuracy loss. The implementation requires a switchable decoder layer with conditional QK sharing, an activation cache populated by compute layers, and a sharing scheduler that grows the sharing region from deep to shallow layers. The method is validated on LLaMA architectures ranging from 125M to 7B parameters, showing consistent efficiency gains across different hardware configurations.

## Key Results
- 11.1% faster training throughput and 29% faster inference throughput compared to baseline
- Maintains accuracy comparable to baseline models across multiple benchmarks
- Enables flexible efficient model configurations during inference without requiring distillation
- Ablation studies confirm single large sharing blocks and inclusion of last layer are optimal design choices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive expansion of activation-sharing regions during training enables models to adapt to shared activations with minimal accuracy loss.
- Mechanism: Training begins with all layers in compute mode. At fixed intervals (I steps), a block of B layers at the deep end transitions to sharing mode. The sharing region grows from deep to shallow layers, allowing gradient updates to progressively adjust parameters in sharing layers to compensate for reused QK activations.
- Core assumption: Deeper layers exhibit redundant QK activations that can be approximated by earlier layers without critical information loss.
- Evidence anchors: [abstract] "EPAS gradually grows a sharing region during training by switching decoder layers to activation sharing mode." [section 2.2] "Over the course of T training steps, a group of B layers at the deep end of S is switched to activation sharing mode at every interval of I training steps."
- Break condition: If sharing expands too aggressively (large B, small I), loss divergence or accuracy collapse may occur before adaptation completes.

### Mechanism 2
- Claim: Reusing QK activations from a source layer reduces FLOPs and increases throughput without requiring architectural changes at inference time.
- Mechanism: In sharing mode, a decoder layer retrieves (Q_{i-1}, K_{i-1}) from an activation cache populated by the preceding compute layer, computes only V_i locally, and proceeds with attention. This eliminates Q and K projections for sharing layers.
- Core assumption: Attention scores computed from shared QK remain sufficiently expressive for downstream tasks.
- Evidence anchors: [abstract] "This results in throughput increase due to reduced compute." [section 2.1] "This decoder layer extends conventional transformer decoder layer by adding a conditional switching branch to reuse Q_{i-1}, K_{i-1} from previous layer."
- Break condition: If source-layer QK representations diverge significantly from what sharing layers would compute, task performance degrades—observed in non-EPAS baselines with 10–12% accuracy drops (Table 6).

### Mechanism 3
- Claim: Single large sharing blocks outperform multiple small blocks for deeper-layer activation sharing.
- Mechanism: Consolidating sharing layers into one contiguous region at the deep end maximizes reuse of a single source layer's activations, reducing overhead from repeated cache management and preserving representational continuity.
- Core assumption: Redundancy is concentrated in deeper layers rather than distributed uniformly.
- Evidence anchors: [section 3.4] "Results in Table 8 consistently show superior performance with a single large activation-sharing block, attributed to deeper-layer sharing." [abstract] "To utilize deeper layer redundancy, the sharing region starts from the deep end of the model."
- Break condition: If future architectures exhibit distributed redundancy patterns, single-block assumptions may not generalize.

## Foundational Learning

- **Concept:** Transformer attention mechanics (Q, K, V projections, attention scores, Flash-Attention)
  - **Why needed here:** EPAS modifies QK computation flow; understanding baseline attention is prerequisite to grasping what is reused vs. computed.
  - **Quick check question:** Can you explain why Flash-Attention does not expose attention scores directly, motivating QK-sharing over attention-score-sharing?

- **Concept:** Progressive training paradigms (gradual architecture modification during training)
  - **Why needed here:** EPAS is fundamentally a progressive method; comprehension requires knowing why gradual changes stabilize learning.
  - **Quick check question:** What would likely happen if all target sharing layers were switched simultaneously at step 0?

- **Concept:** Compute vs. memory bottlenecks in LLM inference
  - **Why needed here:** QK-sharing primarily reduces compute; KV-sharing primarily reduces memory. Understanding tradeoffs informs deployment choices.
  - **Quick check question:** For a memory-constrained inference scenario, which sharing mode would you prioritize?

## Architecture Onboarding

- **Component map:**
  - Switchable Decoder Layer -> Activation Cache -> Sharing Scheduler
  - Standard transformer decoder with conditional branch (sharing vs. compute mode) provides QK to cache when next layer is in sharing mode

- **Critical path:**
  1. Initialize model with all layers in compute mode
  2. Configure target sharing layers S_c, interval I, block size B
  3. At each interval, pop B layers from S_c (deep-end first) and append to active sharing set S
  4. During forward pass: compute layers populate cache if next layer is in sharing mode; sharing layers read from cache

- **Design tradeoffs:**
  - QK-sharing vs. KV-sharing: QK reduces compute more; KV reduces memory more
  - Single large block vs. multiple small blocks: Single block simpler and empirically superior for deep-layer redundancy
  - Including vs. excluding last layer: Minimal difference; slight advantage to inclusion (Table 7)

- **Failure signatures:**
  - Sudden loss spike after sharing expansion: Interval I too small or block B too large
  - Accuracy collapse at inference with sharing: Model trained without EPAS then forced into sharing mode
  - Cache misses or incorrect QK values: Cache population logic not triggered for boundary layers

- **First 3 experiments:**
  1. Reproduce TinyLLaMA EPAS training with 25% sharing (5/22 layers), I=500 steps, B=1; compare loss curve and throughput to baseline
  2. Ablate single large block vs. three groups of 2 sharing layers each; measure validation loss and LM benchmark scores
  3. Deploy trained EPAS model with variable sharing ratios (0%, 25%, 50%) at inference; profile tokens/sec and memory usage on target hardware

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can EPAS be effectively adapted for vision transformers (ViT) or speech processing models?
- **Basis in paper:** [explicit] The conclusion states, "EPAS holds promise for extending to... vision and speech domains, which merit further exploration in future research."
- **Why unresolved:** The empirical evaluation in the paper is strictly limited to decoder-only LLMs (LLaMA) ranging from 125M to 7B parameters; the cross-layer redundancy assumptions may differ in architectural variants with different attention mechanisms.
- **What evidence would resolve it:** Application of the progressive activation sharing scheduler to a standard ViT (e.g., on ImageNet) or speech model, showing comparable training throughput gains without accuracy degradation.

### Open Question 2
- **Question:** How does EPAS perform when implemented with Key-Value (KV) sharing instead of Query-Key (QK) sharing?
- **Basis in paper:** [inferred] The authors note in Section 3 that "for the scope of this research we limit empirical analysis on attention sharing only," specifically QK sharing for Flash-Attention compatibility, despite acknowledging that "KV sharing saves more memory."
- **Why unresolved:** The paper theoretically supports KV sharing but provides no data on whether the progressive training schedule is as effective for KV sharing, which has different computational and memory trade-offs.
- **What evidence would resolve it:** A comparative ablation study applying the EPAS schedule to both QK and KV sharing configurations, measuring throughput, memory footprint, and convergence curves.

### Open Question 3
- **Question:** Does the EPAS training methodology remain stable and effective during post-training alignment phases (e.g., SFT or RLHF)?
- **Basis in paper:** [explicit] The conclusion suggests EPAS "holds promise for extending to post-training," but all experiments focus on pretraining or continual pretraining.
- **Why unresolved:** Progressive freezing or layer modification during fine-tuning can be unstable or lead to catastrophic forgetting; it is unknown if the "hot switching" mechanism integrates smoothly with alignment techniques.
- **What evidence would resolve it:** Experiments applying EPAS during Supervised Fine-Tuning (SFT) of a base model, comparing the resulting chat capabilities against a standard full-parameter fine-tuning baseline.

### Open Question 4
- **Question:** How does EPAS scale to models significantly larger than 7B parameters (e.g., 70B+) regarding GPU memory utilization and convergence stability?
- **Basis in paper:** [inferred] The paper validates the method on models up to 7B parameters and uses small dataset subsets (e.g., 4B tokens). The efficiency-accuracy trade-off may shift in larger models where redundancy patterns are more complex.
- **Why unresolved:** The throughput improvements (11.1%) and loss curves are only verified on relatively small scales; larger models may exhibit different sensitivities to the progressive expansion of sharing regions.
- **What evidence would resolve it:** Training curves and benchmarks for a 70B parameter model trained with EPAS from scratch or via continual pretraining on a large-scale corpus.

## Limitations
- The specific optimal values for progressive scheduling parameters (I and B) are not precisely specified
- Results are limited to LLaMA architectures (125M–7B parameters), leaving scalability to larger models untested
- The mechanism explanations are logical but not definitively proven, relying on internal comparisons rather than cross-validation

## Confidence

**High Confidence:**
- EPAS achieves measurable training and inference throughput improvements (11.1% and 29% respectively)
- Progressive activation sharing maintains accuracy comparable to baseline models
- QK-sharing primarily reduces compute while KV-sharing primarily reduces memory
- Single large sharing blocks outperform multiple small blocks for deep-layer activation sharing

**Medium Confidence:**
- Deeper transformer layers contain redundant QK activations suitable for sharing
- The progressive expansion mechanism enables models to adapt to shared activations with minimal accuracy loss
- Including the last layer in sharing provides marginal benefits over excluding it

**Low Confidence:**
- The specific optimal values for progressive scheduling parameters (I and B)
- Generalization of results to architectures beyond LLaMA family
- Long-term stability of models trained with progressive activation sharing

## Next Checks

1. **Progressive Scheduling Sensitivity Analysis**: Systematically vary interval I (100-1000 steps) and block size B (1-5 layers) on TinyLLaMA-1.1B to identify optimal combinations and verify that accuracy remains stable across a range of scheduling parameters. Measure both training throughput and validation loss curves.

2. **Architecture Transferability Test**: Apply EPAS to a non-LLaMA architecture (e.g., OPT or GPT-Neo) with similar parameter counts. Compare training throughput improvements and accuracy retention against LLaMA results to assess generalizability beyond the tested model family.

3. **Large-Scale Model Validation**: Scale EPAS to a 13B+ parameter model on a fixed hardware configuration. Verify whether the 29% inference throughput improvement observed in smaller models scales proportionally, and measure any changes in memory requirements and cache management overhead.