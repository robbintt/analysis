---
ver: rpa2
title: 'DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware
  GRPO'
arxiv_id: '2512.21514'
source_url: https://arxiv.org/abs/2512.21514
tags:
- diversity
- reward
- quality
- arxiv
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of mode collapse in reinforcement
  learning for image generation, where models trained with GRPO produce highly homogenized
  outputs lacking visual diversity and creativity. The authors propose DiverseGRPO,
  which introduces two key innovations: (1) a distributional creativity bonus that
  uses spectral clustering to group generated images and assigns exploration rewards
  inversely proportional to cluster size, encouraging the discovery of novel visual
  modes; and (2) structure-aware regularization that applies stronger KL constraints
  during early denoising steps to preserve diversity, while relaxing constraints later
  to optimize quality.'
---

# DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO

## Quick Facts
- arXiv ID: 2512.21514
- Source URL: https://arxiv.org/abs/2512.21514
- Reference count: 31
- Key outcome: 13%-18% improvement in semantic diversity metrics under matched quality scores

## Executive Summary
DiverseGRPO addresses mode collapse in GRPO-based image generation by introducing two key innovations: a distributional creativity bonus that rewards exploration of rare visual modes through spectral clustering, and structure-aware regularization that applies stronger KL constraints during early denoising steps to preserve diversity. The method achieves a new Pareto frontier between image quality and diversity, improving semantic diversity metrics by 13%-18% while maintaining quality scores across multiple backbones and reward models.

## Method Summary
The method combines a distributional creativity bonus with structure-aware regularization. For the bonus, generated images are clustered via spectral clustering on DreamSim distances, with exploration rewards inversely proportional to cluster size. For regularization, standard KL constraints are replaced with Wasserstein distance constraints for the first K denoising steps, then removed. The approach is tested on SD3.5-M and Flux backbones with PickScore and HPSv3 rewards, using LoRA fine-tuning with specific hyperparameters for learning rate, gradient accumulation, and batch size.

## Key Results
- Achieves 13%-18% improvement in semantic diversity metrics (DreamSim, BeyondFID) under matched quality scores
- Establishes new Pareto frontier between image quality and diversity for GRPO-based generation
- Demonstrates consistent performance across multiple backbones (SD3.5-M, Flux) and reward models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rewarding exploration of rare visual modes counters GRPO's tendency to collapse into a single high-reward mode
- **Mechanism**: A "distributional creativity bonus" uses spectral clustering to group generated images and assigns exploration rewards inversely proportional to cluster size ($E_i = \sqrt{N/n_k}$)
- **Core assumption**: Spectral clustering on perceptual features can reliably identify under-represented modes worth exploring
- **Evidence anchors**: Abstract mentions spectral clustering and adaptive reward allocation; section 3.2 details the clustering and reward formula; corpus papers confirm diversity collapse is fundamental in RL fine-tuning
- **Break condition**: Fails if quality reward is too dominant or clustering is inconsistent/noisy

### Mechanism 2
- **Claim**: Stronger regularization during early denoising steps is more effective at preserving diversity than uniform KL penalty
- **Mechanism**: "Structure-aware regularization" replaces KL with Wasserstein distance constraint for first $K$ steps, removing it later
- **Core assumption**: Denoising trajectory can be split into diversity-critical (early) and quality-critical (later) phases
- **Evidence anchors**: Abstract mentions early-stage constraints; section 3.3 shows first one-third of steps accounts for ~66% of diversity change; corpus supports Wasserstein regularization
- **Break condition**: Fails if split point $K$ is chosen incorrectly or Wasserstein constraint is too strong

### Mechanism 3
- **Claim**: Combination of distributional rewards and structure-aware regularization creates new Pareto frontier
- **Mechanism**: Creativity bonus provides exploration signal while regularization preserves exploration capacity
- **Core assumption**: Components are synergistic and hyperparameters can be tuned to work together
- **Evidence anchors**: Abstract mentions new Pareto frontier; section 4.3 shows optimal balance with both modules; corpus explores related adaptive regularization themes
- **Break condition**: Complex system fails if hyperparameters are improperly tuned

## Foundational Learning

- **Concept: Mode Collapse in Reinforcement Learning**
  - **Why needed here**: Core problem the paper addresses
  - **Quick check question**: Can you explain why maximizing a standard expected return reward can lead a generative model to produce a limited set of outputs?

- **Concept: Denoising Trajectory in Diffusion/Flow Models**
  - **Why needed here**: Second contribution relies on observation that different steps have different impacts on diversity
  - **Quick check question**: Why are early denoising steps considered more critical for determining an image's global structure and semantic content?

- **Concept: Spectral Clustering**
  - **Why needed here**: Algorithm used to group generated images by perceptual similarity
  - **Quick check question**: What is the role of the normalized Laplacian matrix derived from an affinity graph in spectral clustering?

## Architecture Onboarding

- **Component map**: Group Sampler -> Perceptual Distance Calculator -> Spectral Clustering Module -> Reward Aggregator -> Structure-Aware Policy Optimizer
- **Critical path**: Prompt sampled, images generated and clustered, rewards computed, policy model updated with step-dependent regularization
- **Design tradeoffs**:
  - Bonus Strength (β): High values prioritize diversity over quality; low values may fail to prevent collapse
  - Regularization Cutoff (K): Large K preserves diversity but may hinder quality refinement
  - Clustering Granularity: Number of clusters affects bonus precision
- **Failure signatures**:
  - Model still collapses: Check if β is too low or K is too small
  - Quality degrades: Exploration bonus may be too strong or regularization applied too long
  - Unstable training: Conflicts between reward components or incorrect scaling of Wasserstein term
- **First 3 experiments**:
  1. Ablation Study: Run with only Creativity Reward, only Structure-Aware Regularization, and both together
  2. Parameter Sensitivity: Systematically vary β and K to find stable operating region
  3. Visual Inspection: Generate images with baseline and DiverseGRPO using identical prompts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does DiverseGRPO scale to extremely large batch sizes and prompt diversity given O(n²) pairwise distance computations?
- **Basis in paper**: Method computes DreamSim distances between all sample pairs for spectral clustering, but scalability limits aren't discussed
- **Why unresolved**: Paper evaluates on standard benchmarks but doesn't analyze computational overhead as sample groups grow
- **What evidence would resolve it**: Timing benchmarks across varying sample group sizes or comparison of approximate clustering methods

### Open Question 2
- **Question**: How should the number of clusters k in spectral clustering be determined adaptively across different prompt types?
- **Basis in paper**: Paper describes spectral clustering followed by k-means but doesn't specify how k is selected
- **Why unresolved**: Different prompts may have vastly different numbers of valid visual interpretations; fixed k may over/under-segment semantic space
- **What evidence would resolve it**: Ablation studies varying k per prompt or analysis using silhouette scores/elbow methods

### Open Question 3
- **Question**: Can structure-aware regularization threshold K be theoretically derived from diffusion dynamics rather than empirically tuned?
- **Basis in paper**: Paper shows early steps dominate diversity but K=4 is empirically chosen; relationship between K and optimal trade-offs remains unclear
- **Why unresolved**: Different architectures may require different K values; selection principle isn't formalized
- **What evidence would resolve it**: Theoretical analysis connecting diffusion variance schedules to optimal K or systematic study across architectures

## Limitations
- Effectiveness depends on stability of spectral clustering for semantic mode detection
- Reported hyperparameters (β=0.7, K=4) appear well-tuned but lack sensitivity analysis
- Wasserstein distance implementation details for denoising steps are underspecified

## Confidence

- **High confidence**: Core mechanism of using cluster-size-inverse rewards to combat mode collapse is well-grounded in RL literature
- **Medium confidence**: Structure-aware regularization timing is plausible but requires verification across architectures
- **Medium confidence**: Reported 13%-18% diversity improvements are methodologically sound but may be sensitive to prompt selection

## Next Checks

1. **Clustering Robustness Test**: Generate images from same prompt using baseline and DiverseGRPO, apply spectral clustering to both sets, and quantitatively compare cluster distributions

2. **Regularization Timing Sweep**: Systematically vary K from 2 to 6 steps and plot quality-diversity trade-off curve to identify optimal operating point

3. **Cross-Model Generalization**: Apply DiverseGRPO to third backbone (e.g., Stable Diffusion 1.5) with different reward models to test generalization beyond SD3.5-M and Flux experiments