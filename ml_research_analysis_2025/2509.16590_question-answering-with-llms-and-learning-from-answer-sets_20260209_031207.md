---
ver: rpa2
title: Question Answering with LLMs and Learning from Answer Sets
arxiv_id: '2509.16590'
source_url: https://arxiv.org/abs/2509.16590
tags:
- reasoning
- answer
- learning
- john
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM2LAS is a neuro-symbolic system that automatically learns commonsense
  reasoning rules from natural language stories and questions using LLMs for semantic
  parsing and ILASP for rule induction, without manual encoding. Evaluated on the
  bAbI benchmark, it achieves 100% accuracy on 15 tasks and 97.8% on one, matching
  human-engineered baselines.
---

# Question Answering with LLMs and Learning from Answer Sets

## Quick Facts
- **arXiv ID:** 2509.16590
- **Source URL:** https://arxiv.org/abs/2509.16590
- **Reference count:** 8
- **One-line primary result:** LLM2LAS achieves 100% accuracy on 15/20 bAbI tasks and 97.8% on one, matching human-engineered baselines.

## Executive Summary
LLM2LAS is a neuro-symbolic system that automatically learns commonsense reasoning rules from natural language stories and questions using LLMs for semantic parsing and ILASP for rule induction, without manual encoding. Evaluated on the bAbI benchmark, it achieves 100% accuracy on 15 tasks and 97.8% on one, matching human-engineered baselines. The system automatically generates ASP representations and mode bias declarations from narratives, enabling robust reasoning. It struggles with tasks requiring non-explicit knowledge or very large hypothesis spaces, where additional background knowledge or ad-hoc improvements help. The approach significantly reduces manual intervention, advancing autonomous, interpretable reasoning in NLP.

## Method Summary
LLM2LAS uses an LLM (Llama-3.3 70B) for semantic parsing via few-shot prompting to convert stories and questions into ASP-like fluent representations and mode bias declarations. These are wrapped in Event Calculus predicates and combined with background knowledge. If reasoning fails, ILASP learns new rules from context-dependent partial interpretations (CDPIs) derived from the story and question-answer pairs. The system iterates, updating its knowledge base until correct answers are produced. It uses Clingo for ASP reasoning and spaCy for coreference resolution.

## Key Results
- Achieved 100% accuracy on 15 of 20 bAbI tasks, matching human-engineered ASP baselines.
- Successfully solved complex reasoning tasks including deduction, induction, and temporal reasoning without manual rule encoding.
- Demonstrated the feasibility of fully automated commonsense reasoning rule learning from narratives.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Language Models (LLMs) can effectively serve as semantic parsers to translate unstructured natural language narratives into structured logic representations (fluents), provided the target domain is sufficiently constrained.
- **Mechanism:** The system uses few-shot prompting to guide an LLM (specifically Llama-3.3 70B) to map sentences directly to ASP-like fluent representations (e.g., `go_to(john, kitchen)`). This bypasses the need for traditional, manually-coded grammars.
- **Core assumption:** The LLM possesses sufficient pre-training to map the specific vocabulary of the target dataset (e.g., bAbI) to the required logical predicates without generating syntactically invalid logic.
- **Evidence anchors:**
  - [abstract] "...LLMs are used to extract semantic structures from text..."
  - [Page 11] "...leverage an LLM to parse narratives and questions into fluent-like representation... using the few-shot prompting technique..."
  - [corpus] Related work confirms LLMs are increasingly used for semantic parsing in neuro-symbolic systems, though often requiring significant prompting engineering (Paper: *Visual Graph Question Answering with ASP and LLMs*).
- **Break condition:** The mechanism fails if the input text is ambiguous or uses vocabulary significantly out-of-distribution from the few-shot examples, leading to malformed predicates that the ASP solver rejects.

### Mechanism 2
- **Claim:** Automating the generation of "mode bias" declarations (language bias) allows the learning system (ILASP) to prune the hypothesis space, making rule induction computationally tractable.
- **Mechanism:** Rather than manually defining which predicates can appear in the head or body of a rule, the system generates mode biases dynamically based on the types (extracted via POS tagging) and arguments of the fluents found in the story. This restricts ILASP to search only for rules that are syntactically relevant to the current narrative.
- **Core assumption:** The types derived from POS tags (e.g., noun vs. verb) and sentence structures correctly identify the semantic roles required to form valid commonsense rules.
- **Evidence anchors:**
  - [Page 3] "...LLM-based few-shot semantic parser for generating... mode bias declarations..."
  - [Page 13] "The mode bias fluents aid the learner in automatically generating mode bias declarations..."
  - [corpus] Corpus evidence specifically validating automated *mode bias* generation is weak; most neuro-symbolic systems rely on fixed symbolic modules (Paper: *Neuro-Conceptual Artificial Intelligence*).
- **Break condition:** If the generated mode bias is too permissive, the hypothesis space explodes (scalability failure); if too restrictive, it excludes the correct commonsense rule (convergence failure).

### Mechanism 3
- **Claim:** Commonsense reasoning rules can be induced from specific narrative contexts by formulating the learning problem as a set of Context-Dependent Partial Interpretations (CDPIs).
- **Mechanism:** The system treats a story segment as "context" and the question/answer pair as a "partial interpretation" (inclusion/exclusion sets). ILASP searches for a hypothesis (logic program) that, when combined with the context, satisfies the partial interpretation (i.e., makes the correct answer true and the incorrect answer false).
- **Core assumption:** The underlying logic governing the story (e.g., "objects stay in locations until moved") is consistent and can be expressed within the provided Event Calculus background knowledge.
- **Evidence anchors:**
  - [Page 10] "A CDPI ex is accepted by a program P if and only if there is an answer set... that extends ex_pi."
  - [Page 15] "Examples are created from questions, stories, and their correct and incorrect answers."
  - [corpus] This aligns with abductive reasoning frameworks in other neuro-symbolic systems (Paper: *NeSTR*).
- **Break condition:** The mechanism struggles when the required reasoning involves implicit knowledge not explicit in the narrative or requires complex mathematical aggregates (e.g., counting) that the learner cannot easily represent as rules.

## Foundational Learning

- **Concept: Answer Set Programming (ASP) Basics**
  - **Why needed here:** The system's reasoning engine is entirely dependent on ASP syntax (facts, rules, choice rules) and semantics (stable models). You cannot debug the system without understanding how an ASP solver derives truth values.
  - **Quick check question:** Can you write a normal rule and a choice rule in ASP syntax and explain the difference?

- **Concept: Simplified Discrete Event Calculus (SDEC)**
  - **Why needed here:** The system uses SDEC axioms (inertia, initiation, termination) as domain-independent background knowledge to model time and state changes (fluents).
  - **Quick check question:** How does the "inertia law" (Frame Problem) dictate that a fluent retains its truth value over time unless explicitly terminated?

- **Concept: Inductive Logic Programming (ILP) / Learning from Answer Sets (LAS)**
  - **Why needed here:** The core novelty is "learning" the symbolic rules. Understanding ILASP's objective (finding a hypothesis that covers examples while minimizing complexity) is essential to understanding why the system generates specific rules.
  - **Quick check question:** What is the difference between a positive and a negative example in the context of ILP?

## Architecture Onboarding

- **Component map:** Story Processing -> LLM Semantic Parser -> ASP Rep Generator -> Reasoner (Clingo) -> Learner (ILASP) -> Update Hypothesis
- **Critical path:** The **LLM Semantic Parser**. If the parser outputs malformed logic or incorrect mode biases, the downstream Learner (ILASP) will either crash or fail to find a solution. The entire pipeline depends on the LLM's strict adherence to the few-shot output format.
- **Design tradeoffs:** The system trades **full autonomy** (no manual rule writing) for **computational efficiency**. The paper notes that for complex tasks (e.g., Task 2, 3, 5), the hypothesis space generated for ILASP becomes too large to process, causing scalability issues compared to hand-crafted systems.
- **Failure signatures:**
  - **Semantic Drift:** The LLM invents predicates not in the prompt.
  - **Grounding Explosion:** ILASP runs out of memory trying to generate the hypothesis space (seen in Tasks 2, 3, 5).
  - **Silent Reasoning Failure:** The ASP solver returns an answer set, but the "unification search" fails to extract the answer because the learned rule syntax doesn't match the query syntax.
- **First 3 experiments:**
  1. **Unit Test the Parser:** Input bAbI sentences and verify the LLM outputs strictly valid Prolog/ASP syntax (no extra text).
  2. **Trace a Simple Learning Task:** Run Task 1 (Single Supporting Fact) with debug logs enabled to observe the generation of the CDPI example from a single story/question pair.
  3. **Hypothesis Space Inspection:** Run the Learner on a "hard" task (like Task 3) and monitor the size of the hypothesis space generated by the mode bias to verify the bottleneck identified in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be extended to leverage LLMs for pre-populating background knowledge with commonsense rules for notions not explicitly mentioned in the narrative text?
- **Basis in paper:** [Explicit] The discussion identifies "issue (ii)" as the system struggling with notions not explicitly mentioned in the dataset. The authors suggest that future research could integrate LLM capabilities to address this limitation.
- **Why unresolved:** The current implementation relies on the explicit content of the bAbI stories and minimal domain-independent background knowledge, failing on tasks requiring unstated "commonsense" (e.g., "possession").
- **What evidence would resolve it:** Successful performance on Tasks 2 and 3 without manual intervention, using rules automatically extracted from the LLM's internal knowledge.

### Open Question 2
- **Question:** Can the system be enhanced to support the inductive learning of programs containing aggregates or mathematical reasoning constructs without manual engineering?
- **Basis in paper:** [Explicit] The paper notes in the Conclusion and Section 5.2 that ILASP currently cannot learn programs with aggregates. Consequently, Task 7 (Counting) required manually added background knowledge to define counting rules.
- **Why unresolved:** The learning component (ILASP) lacks the native ability to induce logic programs utilizing aggregates (e.g., `#count`), creating a bottleneck for arithmetic reasoning tasks.
- **What evidence would resolve it:** The system automatically inducing a hypothesis containing aggregates that solves Task 7 (Counting) without the manually added `carriedItems` rule.

### Open Question 3
- **Question:** How can the hypothesis space be effectively pruned or managed to allow the learning of complex tasks, such as those involving ternary fluents?
- **Basis in paper:** [Explicit] The paper lists Task 5 (Three Argument Relations) as "Unsolved" because the hypothesis space generated by ternary fluents combined with Event Calculus axioms became too large for the ILASP system to ground.
- **Why unresolved:** The combinatorial explosion of potential rules in complex hypothesis spaces causes the learning system to time out or fail to ground, preventing the discovery of solutions for tasks with higher arity relations.
- **What evidence would resolve it:** Successful completion of Task 5 within a reasonable timeframe (e.g., < 24 hours) using the standard automated pipeline.

## Limitations
- **Scalability:** ILASP experiences grounding timeouts for complex tasks due to large hypothesis spaces, limiting the system's ability to handle richer domains.
- **Mode Bias Automation:** The effectiveness of automated mode bias generation is less certain compared to traditional fixed symbolic modules, with weak corpus evidence.
- **Non-Explicit Knowledge:** The system struggles with tasks requiring commonsense knowledge not explicitly stated in the narrative, as it relies solely on the provided content.

## Confidence
- **High Confidence:** The core mechanism of using LLMs for semantic parsing and ILASP for rule induction is clearly demonstrated and validated on the bAbI benchmark. The system achieves near-perfect accuracy on most tasks.
- **Medium Confidence:** The claims regarding the automation of mode bias generation and the system's ability to learn from narrative context are supported, but the robustness and scalability of these components are less certain based on the presented results.
- **Low Confidence:** The paper does not provide sufficient evidence to support claims about the system's performance on tasks requiring non-explicit knowledge or very large hypothesis spaces without ad-hoc improvements.

## Next Checks
1. **Parser Robustness Test:** Conduct a systematic evaluation of the LLM semantic parser's performance on bAbI stories with introduced noise or out-of-distribution vocabulary to assess its failure points.
2. **Mode Bias Impact Analysis:** Perform ablation studies to measure the impact of automated mode bias generation on ILASP's learning efficiency and rule quality, comparing it to manually defined biases.
3. **Scalability Benchmark:** Test the system on a more complex dataset or with artificially expanded bAbI stories to quantify the scaling limits of the ILASP component and identify the threshold for grounding timeouts.