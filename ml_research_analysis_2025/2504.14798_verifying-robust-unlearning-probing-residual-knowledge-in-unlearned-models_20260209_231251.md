---
ver: rpa2
title: 'Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models'
arxiv_id: '2504.14798'
source_url: https://arxiv.org/abs/2504.14798
tags:
- unlearning
- attack
- unlearned
- robust
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Machine unlearning often fails to completely remove traces of\
  \ forgotten data, leaving models vulnerable to adversarial attacks that can resurface\
  \ removed knowledge. To address this, the paper introduces Robust Unlearning\u2014\
  a security standard ensuring models are indistinguishable from retraining and resistant\
  \ to adversarial recovery."
---

# Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models

## Quick Facts
- **arXiv ID**: 2504.14798
- **Source URL**: https://arxiv.org/abs/2504.14798
- **Reference count**: 18
- **Primary result**: Standard unlearning methods fail to completely remove traces of forgotten data, leaving models vulnerable to adversarial attacks that can resurface removed knowledge.

## Executive Summary
Machine unlearning often fails to completely remove traces of forgotten data, leaving models vulnerable to adversarial attacks that can resurface removed knowledge. To address this, the paper introduces Robust Unlearning—a security standard ensuring models are indistinguishable from retraining and resistant to adversarial recovery. The proposed Unlearning Mapping Attack (UMA) actively probes unlearned models using adversarial queries to assess residual knowledge. Experiments across discriminative and generative tasks show that even state-of-the-art unlearning methods remain vulnerable to UMA despite passing traditional verification metrics. The study establishes UMA as a practical verification tool, setting a new benchmark for assessing and improving unlearning security.

## Method Summary
The paper introduces Robust Unlearning—a security standard requiring unlearned models to be indistinguishable from retraining and resistant to adversarial recovery. To verify this, they propose the Unlearning Mapping Attack (UMA), which uses Projected Gradient Descent to optimize adversarial perturbations that minimize output divergence between original and unlearned models on the forget set. The attack searches for perturbations that cause unlearned models to produce outputs matching the original model's forgotten knowledge. Experiments evaluate five unlearning methods (Fine-Tuning, Regularization Learning, Influence Unlearning, l1-sparse, and SalUn) across CIFAR-10, CIFAR-100, Tiny-ImageNet (discriminative), and ImageNet-1k with MAE (generative).

## Key Results
- Standard unlearning methods leave models vulnerable to UMA despite passing traditional verification metrics
- Retraining achieves the strongest robustness against UMA, with UA=0 and MIA=0
- UMA successfully increases MIA scores from 0 to 0.9934 on CIFAR-10 with ε=8/255
- Adversarial unlearning shows promise as a defense, reducing attack success rates

## Why This Works (Mechanism)

### Mechanism 1: Residual Knowledge Persistence in Unlearned Models
Standard unlearning methods decorrelate inputs from outputs on the forget set but do not eliminate latent representations of forgotten knowledge. Neural networks distribute information across weight matrices, and unlearning methods update parameters to change output behavior while residual activations remain accessible through alternative input pathways. The unlearning process modifies input-output mappings without fully erasing internal feature representations associated with forgotten data.

### Mechanism 2: Gradient-Based Input Remapping (UMA Optimization)
UMA solves an optimization problem using Projected Gradient Descent (PGD) to find adversarial perturbations that minimize output divergence between original and unlearned models. By following negative gradients of the loss between unlearned and original model outputs, perturbations iteratively move inputs toward regions where the unlearned model still produces forgotten outputs. The optimization landscape contains gradients pointing toward forgotten knowledge clusters.

### Mechanism 3: Robustness Gap Between Unlearning and Retraining
Retraining achieves stronger robustness to adversarial recovery than approximate unlearning methods because it completely removes data influence from the training trajectory. Approximate unlearning methods alter weights post-hoc, leaving optimization traces that create paths for adversarial recovery. This suggests vulnerabilities are intrinsic to unlearning algorithms rather than implementation-specific artifacts.

## Foundational Learning

- **Concept: Machine Unlearning Objectives**
  - **Why needed here:** Understanding what existing methods optimize (output decorrelation vs. representation erasure) explains why UMA succeeds.
  - **Quick check question:** Can you explain why satisfying E_{x ∈ D_u}[||f_u(x, θ_u) - f(x, θ)||] > ε_1 doesn't guarantee robust unlearning?

- **Concept: Adversarial Perturbations and PGD**
  - **Why needed here:** UMA implementation requires understanding iterative gradient-based optimization with bounded perturbations.
  - **Quick check question:** Why does UMA use gradient descent (minimization) while standard adversarial attacks use gradient ascent (maximization)?

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here:** MIA scores serve as a key metric for evaluating whether unlearning truly removed data influence.
  - **Quick check question:** What does an MIA score jumping from 0 to 0.9934 after UMA indicate about the unlearning quality?

## Architecture Onboarding

- **Component map:** Pre-trained model f(·; θ) -> Unlearned model f_u(·; θ_u) -> UMA optimizer -> Loss function L -> Evaluation metrics (UA, MIA, TA)

- **Critical path:**
  1. Load both model checkpoints (pre/post unlearning)
  2. Initialize perturbations δ_x for samples in forget set D_u
  3. Iterate PGD: compute loss, backprop to δ_x, update with negative gradient, clip to ℓ_∞ ball
  4. Evaluate attacked outputs against original model outputs

- **Design tradeoffs:**
  - Attack strength (ε): Larger bounds increase success but reduce semantic similarity for discriminative tasks
  - Step size (α): Paper finds 0.7/255–1/255 optimal; larger causes wrong direction, smaller slows convergence
  - Iteration count: More steps improve success but increase compute

- **Failure signatures:**
  - UA remains near 0% after attack → unlearning may be robust OR attack hyperparameters need tuning
  - High UA on retain set → attack too aggressive, breaking task-specific constraints
  - Optimization diverges → check gradient signs (UMA minimizes, not maximizes)

- **First 3 experiments:**
  1. Reproduce CIFAR-10 class-wise unlearning attack: Use SalUn on ResNet50, apply UMA with ε=8/255, verify UA increase matches paper (~32%)
  2. Ablation on step size: Fix iterations=100, vary α ∈ {0.1, 0.5, 1.0, 2.0}/255, plot UA vs. step size to confirm 0.7–1.0/255 optimum
  3. Test adversarial unlearning defense: Implement Eq. 6 with RL backbone, compare UA under attack vs. baseline RL (should drop from ~56% to ~17% per Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adversarial unlearning be modified to scale efficiently for large models with extensive forget sets without incurring prohibitive computational costs?
- **Basis in paper:** Page 8 notes that "the scalability of adversarial unlearning may be constrained by computational resources" due to iterative sample generation.
- **Why unresolved:** The paper identifies adversarial unlearning as a successful defense but highlights that computational overhead grows with forget set size, making it difficult to apply to large-scale systems.
- **What evidence would resolve it:** A modified adversarial unlearning algorithm that maintains high robustness (low UA/MIA) on large datasets while reducing training time complexity to match standard unlearning methods.

### Open Question 2
- **Question:** Can certified robustness guarantees be developed for unlearning to overcome the theoretical limitations of the non-convex optimization used in UMA?
- **Basis in paper:** Page 5 states that "Algorithm 1 does not guarantee exploration of all possible perturbations" because the loss landscape is typically non-convex.
- **Why unresolved:** UMA serves as an empirical "lower bound" for robustness; failing to find an attack does not mathematically prove that no adversarial input exists to recover forgotten data.
- **What evidence would resolve it:** A theoretical framework providing deterministic or probabilistic certification that an unlearned model satisfies the Robust Unlearning criteria within specific perturbation bounds.

### Open Question 3
- **Question:** How effective is the Unlearning Mapping Attack in black-box settings where the verifier lacks access to the pre-unlearning model parameters?
- **Basis in paper:** Page 4 assumes the validator "has full access to the model before and after," utilizing white-box gradients to craft the attack.
- **Why unresolved:** In many practical scenarios (e.g., third-party audits), internal model weights are unavailable. The transferability of UMA perturbations to query-only access is unexplored.
- **What evidence would resolve it:** Experimental results showing that UMA-generated perturbations successfully resurface information when only input-output query access is available.

## Limitations
- Evaluation scope limited to vision datasets and one generative task, with claims about general applicability
- Practical security implications of high MIA scores remain unclear
- UMA requires access to both pre-unlearning and post-unlearning models, limiting real-world applicability
- Optimization assumes differentiability and gradient accessibility, which may not hold in deployment contexts

## Confidence
- **High Confidence**: The core mechanism of UMA (gradient-based adversarial perturbation optimization) and its basic implementation are well-founded and reproducible
- **Medium Confidence**: The comparative analysis showing retraining achieves better robustness than approximate unlearning methods is convincing but based on limited experimental breadth
- **Low Confidence**: The generalization of findings to unseen domains (especially NLP) and the practical security implications of the identified vulnerabilities are not fully established

## Next Checks
1. **Cross-Modal Validation**: Apply UMA to transformer-based language models using the same methodology. Compare attack success rates and residual knowledge persistence between vision and language domains to assess whether the framework generalizes beyond computer vision.

2. **Temporal Robustness Analysis**: Evaluate UMA's effectiveness over time as unlearned models continue training or fine-tuning. Determine whether the identified vulnerabilities are static properties or evolve with model updates, providing insights into long-term unlearning stability.

3. **Practical Attack Feasibility Assessment**: Conduct user studies or expert evaluations to determine whether the perturbations discovered by UMA correspond to semantically meaningful inputs that could be practically exploited by real adversaries, rather than just theoretical constructs that satisfy optimization objectives.