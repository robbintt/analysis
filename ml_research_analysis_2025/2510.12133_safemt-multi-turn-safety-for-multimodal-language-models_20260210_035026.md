---
ver: rpa2
title: 'SafeMT: Multi-turn Safety for Multimodal Language Models'
arxiv_id: '2510.12133'
source_url: https://arxiv.org/abs/2510.12133
tags:
- safety
- user
- dialogue
- assistant
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeMT is the first benchmark for evaluating the multi-turn safety
  of multimodal large language models (MLLMs) using dialogues and images. It includes
  2,000 harmful queries and 8,000 dialogues spanning 17 scenarios and four jailbreak
  methods.
---

# SafeMT: Multi-turn Safety for Multimodal Language Models

## Quick Facts
- arXiv ID: 2510.12133
- Source URL: https://arxiv.org/abs/2510.12133
- Reference count: 38
- Primary result: Introduces SafeMT, the first benchmark for evaluating multi-turn safety of multimodal language models, showing ASR increases significantly with dialogue length.

## Executive Summary
SafeMT addresses the critical gap in evaluating multi-turn safety for multimodal large language models (MLLMs). While single-turn safety has been extensively studied, real-world interactions often involve extended dialogues where harmful requests can be gradually introduced. The paper introduces a comprehensive benchmark featuring 2,000 harmful queries and 8,000 dialogues across 17 scenarios, demonstrating that ASR increases significantly with dialogue length. To mitigate this vulnerability, the authors propose a dialogue safety moderator that detects malicious intent and provides context-aware safety policies, outperforming existing guard models.

## Method Summary
The paper presents a benchmark generation pipeline that transforms single harmful queries into multi-turn dialogues using closed-source models (GPT-4o, Claude-3.7, Gemini-2.0) with four jailbreak methods. The benchmark covers 17 safety scenarios and evaluates MLLMs across dialogue lengths of 2, 4, 6, and 8 turns. A dialogue safety moderator is trained via supervised fine-tuning on Gemma-3-4B-it using the SafeMT dataset to infer latent user intent and generate safety rules. The Safety Index (SI) metric is introduced to measure vulnerability by weighting ASR by the turn of compromise and penalizing inconsistent defense.

## Key Results
- ASR increases significantly with dialogue length across all tested MLLMs
- Visual components introduce additional safety risks in multi-turn interactions
- The proposed dialogue safety moderator outperforms existing guard models
- Financial and political scenarios show the highest vulnerability to multi-turn attacks

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Guardrails via Intent Inference
A context-aware moderator improves safety by inferring latent malicious intent from full dialogue history and matching it to safety rules. The 4B parameter moderator processes dialogue history and image, identifies underlying intent (e.g., "animal confinement"), categorizes the scenario (e.g., "Animal Abuse"), and maps this to specific safety rules injected into the MLLM's system prompt. The mechanism fails if user intent is truly novel or ambiguously stated, causing misclassification.

### Mechanism 2: Weighted Vulnerability Assessment with Safety Index (SI)
The Safety Index weights ASR by the turn of compromise using exponential decay: SI = (1 - weighted_avg_ASR) * (1 - mean(Ïƒ[I_j, ..., In])). Earlier compromises are penalized more heavily than later ones, and inconsistent defense is penalized via standard deviation of safety status vectors. The metric becomes uninformative if models show uniform ASR across turns or if the decay function doesn't appropriately model application-specific risk.

### Mechanism 3: Erosion of Safety Alignment in Long Dialogues
MLLMs become increasingly vulnerable as dialogue length increases because safety protocols "fade into oblivion" over conversation progress. Harmful requests can be "salami-sliced" across turns, with each appearing benign individually but collectively harmful. This incremental lowering of defenses creates attack paths. The claim breaks if future architectures explicitly reinforce safety guardrails at each turn.

## Foundational Learning

- **Concept: Attack Success Rate (ASR)**
  - Why needed here: ASR is the baseline metric for safety and core component of the novel Safety Index
  - Quick check question: If a model has lower ASR in 8-turn dialogues vs 2-turn dialogues, what does that imply about its multi-turn safety profile?

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed here: The Dialogue Safety Moderator is built by SFT on Gemma-3-4B-it using SafeMT dataset with next-token prediction loss
  - Quick check question: What specific output is the SFT process teaching the moderator model to generate given dialogue and image input?

- **Concept: Safety Alignment**
  - Why needed here: The core problem is failure of existing safety alignment in multi-turn settings; the moderator reinforces this dynamically
  - Quick check question: How does the paper's proposed moderator mechanism differ from static safety alignment in base MLLMs?

## Architecture Onboarding

- **Component Map:** Dataset Generator (GPT-4o/Claude-3.7/Gemini-2.0) -> Target MLLM (GPT-4o, Qwen2.5-VL) -> Dialogue Safety Moderator (Gemma-3-4B-it) -> Evaluator (GPT-4o-mini) -> Safety Index Calculator

- **Critical Path:** For mitigation: User Dialogue + Image -> Moderator (infers intent, selects rule) -> System Prompt with Rule -> Target MLLM (generates response). For evaluation: Harmful Query + Image -> Dialogue Generator -> Target MLLM -> Evaluator -> SI Calculator.

- **Design Tradeoffs:**
  - Moderator Size vs. Latency: 4B parameter balance between capability and computational cost
  - Overfitting vs. Safety: Risk of excessive refusal on benign queries requires careful training data curation
  - Judge Reliability: Automated judge introduces model-specific biases versus costly human annotation

- **Failure Signatures:**
  - Intent Mismatch: Wrong intent inference leading to incorrect or no safety rule application
  - Rule Ignorance: Target MLLM ignoring safety rules in system prompt
  - Inconsistent Defense: Correct refusal at turn 4 but failure at turn 8

- **First 3 Experiments:**
  1. Baseline ASR Measurement: Evaluate target MLLMs on SafeMT without moderator to establish baseline across dialogue lengths
  2. Moderator Ablation: Apply trained moderator and compare ASR/SI against baseline to quantify effectiveness
  3. Component Ablation: Compare model rankings using full SI versus raw ASR or SI without defense consistency penalty

## Open Questions the Paper Calls Out
- How can safety moderators mitigate excessive refusal behavior while maintaining robust defense against multi-turn attacks?
- How robust are MLLMs against adversarial visual attacks within multi-turn dialogues?
- What unique safety vulnerabilities are introduced by multi-modal chain-of-thought reasoning?

## Limitations
- The SafeMT benchmark dataset is not publicly released, preventing direct replication
- Automated judge (GPT-4o-mini) may introduce model-specific biases in safety classification
- Dialogue generation depends on closed-source models without exact parameter specifications

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| ASR increases with dialogue length and MLLMs are vulnerable to multi-turn jailbreaks | High |
| Dialogue Safety Moderator effectively mitigates vulnerabilities | Medium |
| Safety Index is a more nuanced measure than raw ASR | Medium |
| Moderator reliably infers latent malicious intent | Low |

## Next Checks
1. Public dataset release of SafeMT (queries, dialogues, images, labels, rules) to enable independent replication
2. Cross-judge validation using different automated judge or human annotators on subset of responses
3. Long-turn robustness test extending benchmark to 10+ turns to test ASR increase plateauing claims