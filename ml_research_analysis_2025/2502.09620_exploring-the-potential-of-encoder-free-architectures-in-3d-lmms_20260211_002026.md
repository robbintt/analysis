---
ver: rpa2
title: Exploring the Potential of Encoder-free Architectures in 3D LMMs
arxiv_id: '2502.09620'
source_url: https://arxiv.org/abs/2502.09620
tags:
- point
- loss
- semantic
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores encoder-free architectures for 3D Large Multimodal
  Models (LMMs) by transferring 3D encoder functionality to the LLM itself. It addresses
  two key challenges: capturing high-level 3D semantics and perceiving geometric structures.'
---

# Exploring the Potential of Encoder-free Architectures in 3D LMMs

## Quick Facts
- arXiv ID: 2502.09620
- Source URL: https://arxiv.org/abs/2502.09620
- Reference count: 26
- Primary result: ENEL achieves 57.91% classification, 61.0% captioning, and 55.20% VQA accuracy, matching state-of-the-art encoder-based models.

## Executive Summary
This paper presents ENEL, an encoder-free 3D Large Multimodal Model that transfers semantic encoding from dedicated 3D encoders to the LLM itself. By combining LLM-embedded Semantic Encoding with Hybrid Semantic Loss during pre-training and Hierarchical Geometry Aggregation during instruction tuning, ENEL achieves state-of-the-art performance on object-level 3D understanding tasks. The architecture replaces traditional 3D encoders with a lightweight token embedding module and leverages the LLM's capacity to learn geometric and semantic features directly.

## Method Summary
ENEL uses Vicuna-7B as backbone with a 3-layer Point-PN variant for token embedding. Pre-training unfreezes first 4 LLM layers with Hybrid Semantic Loss (30% mask ratio, Chamfer distance for visible tokens). Instruction tuning inserts Hierarchical Geometry Aggregation modules with l=3 cycles, H=2 intermediate layers, and gated self-attention. The lightweight embedding (3M params) projects 6D inputs through progressive MLPs, while Dynamic Grid Sampling aggregates tokens for local geometry processing.

## Key Results
- Classification accuracy: 57.91% (matches state-of-the-art encoder-based models)
- Captioning performance: 61.0% (GPT-4 score, comparable to encoder baselines)
- VQA accuracy: 55.20% (competitive with encoder-dependent architectures)

## Why This Works (Mechanism)

### Mechanism 1: LLM-embedded Semantic Encoding with Hybrid Semantic Loss
Transferring semantic encoding from 3D encoders to LLM layers with Hybrid Semantic Loss (masked modeling + reconstruction) forces the model to learn high-level 3D semantics directly. Evidence shows 52.00% classification and 47.65% captioning performance, outperforming individual losses. Break condition: mask ratio >60% degrades performance.

### Mechanism 2: Hierarchical Geometry Aggregation for Local Structure Perception
Dynamic Grid Sampling with cumulative scaling and gated self-attention injects geometric inductive bias into Transformers. l=3 cycles with H=2 intermediate layers achieve 53.00%/48.93% classification/captioning. Break condition: l≥4 causes oversimplification, H≥4 causes oversmoothing.

### Mechanism 3: Lightweight Token Embedding as Encoder Replacement
A 3M-parameter embedding module (vs 88M for traditional encoders) uses FPS/k-NN grouping for structural tokenization without semantic encoding. 3-layer configuration achieves 45.55%/41.36% performance. Break condition: depth ≥4 layers degrades performance.

## Foundational Learning

- **Self-supervised learning objectives for point clouds:** Understanding why masked modeling outperforms contrastive/distillation losses for 3D semantic encoding. Quick check: Can you explain why predicting masked point tokens forces learning of spatial relationships that contrastive learning might miss?
- **Inductive bias in Transformers for non-sequential data:** How Hierarchical Geometry Aggregation adds local-to-global bias absent in vanilla Transformers. Quick check: How does Dynamic Grid Sampling differ from standard attention patterns in capturing local geometry?
- **Two-stage training (pre-training + instruction tuning):** Why geometric aggregation is more appropriate for instruction tuning than pre-training. Quick check: Why might geometric aggregation be more appropriate for instruction tuning than pre-training?

## Architecture Onboarding

- **Component map:** Point Cloud (8192) → Token Embedding Layer (3M) → LLM Layers 1-4 (learnable, Hybrid Semantic Loss) → LLM Layers 2,3,6,7,11,12 (Hierarchical Geometry Aggregation) → LLM Layers 5-32 (frozen) → Text generation
- **Critical path:** 1) Pre-training: Unfreeze first 4 layers, apply Hybrid Semantic Loss (30% mask, patch reconstruction) 2) Instruction tuning: Freeze all layers, inject geometry aggregation (l=3, H=2, gated attention) 3) Inference: Process variable-resolution point clouds
- **Design tradeoffs:** Mask ratio 30% balances context vs learning challenge; 4 learnable layers optimal; l=3 balances detail vs oversimplification; H=2 prevents oversmoothing
- **Failure signatures:** Semantic encoding failure (<42% GPT-4 score) → check learning rate/mask ratio; Geometry perception failure (<50% accuracy) → check aggregation cycles/gating; Resolution mismatch (>10% performance variance) → verify FPS/k-NN consistency
- **First 3 experiments:** 1) Baseline: Token embedding only (floor ~35.5% classification) 2) Loss ablation: Hybrid vs masked vs reconstruction only on Objaverse captioning 3) Aggregation depth sweep: l={1,2,3,4} with H=2 fixed

## Open Questions the Paper Calls Out

- **Open Question 1:** Can encoder-free 3D LMMs scale from object-level to complex scene-level understanding? Evidence would require adapting architecture for scene datasets (e.g., ScanNet) and evaluating on scene-understanding benchmarks.
- **Open Question 2:** Can contrastive self-supervised losses be redesigned for effective semantic supervision in encoder-free 3D LMMs? Evidence would require developing a contrastive loss variant compatible with autoregressive LLMs matching Hybrid Semantic Loss performance.
- **Open Question 3:** Does encoder-free architecture require significantly more training data to match semantic priors from pre-trained encoders? Evidence would require comparative analysis of performance curves across varying dataset subsets (10%, 50%, 100%).

## Limitations
- Performance claims limited to Objaverse benchmarks without validation on other datasets
- Two-stage training transition not fully specified with narrowly tuned parameters
- Limited evidence that lightweight embedding generalizes across different point cloud densities

## Confidence
- **High Confidence:** Hybrid Semantic Loss outperforms individual losses within controlled experiments
- **Medium Confidence:** Hierarchical Geometry Aggregation improves local structure perception for specific l=3, H=2 configuration
- **Low Confidence:** Lightweight token embedding replaces 3D encoders in general settings based on parameter count and single benchmark

## Next Checks
1. **Cross-dataset Generalization Test:** Evaluate ENEL on ScanObjectNN and ModelNet40 to verify performance claims beyond Objaverse
2. **Parameter Sensitivity Analysis:** Systematically sweep mask ratios (10%-60%), learnable layer counts (1-8), and aggregation cycles (1-5) to identify robust operating points
3. **Encoder Comparison at Scale:** Compare ENEL against PointLLM with 3D encoder across multiple point cloud resolutions (4096, 8192, 16384) to assess resolution independence claims