---
ver: rpa2
title: Representing Speech Through Autoregressive Prediction of Cochlear Tokens
arxiv_id: '2508.11598'
source_url: https://arxiv.org/abs/2508.11598
tags:
- speech
- audio
- auristream
- phoneme
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AuriStream is a biologically inspired two-stage speech representation
  model. First, raw audio is transformed into cochlear tokens via a cochlea-mimicking
  encoder with a quantization bottleneck.
---

# Representing Speech Through Autoregressive Prediction of Cochlear Tokens

## Quick Facts
- **arXiv ID**: 2508.11598
- **Source URL**: https://arxiv.org/abs/2508.11598
- **Reference count**: 0
- **Primary result**: AuriStream achieves state-of-the-art lexical semantic similarity and strong phoneme/word decoding using biologically inspired cochlear tokens without contrastive losses

## Executive Summary
AuriStream introduces a biologically inspired two-stage framework for speech representation learning. The first stage, WavCoch, converts raw audio into cochlear tokens via a cochlea-mimicking encoder with a quantization bottleneck. The second stage, AuriStream, uses a GPT-style Transformer to autoregressively predict future cochlear tokens. This approach achieves state-of-the-art performance on semantic tasks while providing interpretable, invertible audio continuations, all without requiring contrastive or reconstruction objectives.

## Method Summary
The method involves training a two-stage model: first, WavCoch learns to tokenize raw audio into discrete cochlear tokens by predicting cochleagram representations through an LFQ bottleneck; second, AuriStream uses a GPT-style Transformer to autoregressively predict future cochlear tokens from past context. The approach leverages 60k hours of LibriLight for the autoregressive model and 960 hours of LibriSpeech for the tokenizer, targeting 211-bin cochleagrams as intermediate representations.

## Key Results
- AuriStream achieves state-of-the-art performance on the sSIMI lexical semantic benchmark
- Strong phoneme and word decoding accuracy on TIMIT while outperforming contrastive methods
- Generates interpretable audio continuations that can be visualized as cochleagrams
- Scales effectively with model size, with AuriStream-1B outperforming smaller variants on semantic tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing signal reconstruction with cochleagram prediction forces the encoder to learn a biologically plausible intermediate representation.
- **Mechanism:** The first stage (WavCoch) uses a bottleneck to predict a time-frequency cochleagram rather than reconstructing the raw waveform, effectively discretizing audio representations while retaining phonetically relevant features.
- **Core assumption:** The cochleagram representation contains sufficient information for downstream tasks while being easier to compress than raw waveforms.
- **Evidence anchors:**
  - [section 1.2]: "Instead of reconstructing the same signal, we predict a different audio representationâ€”the time-frequency cochleagram... effectively discretizes the audio representations."
  - [appendix 7.3]: Comparisons show cochleagram targets yield slightly higher phoneme cluster purity than mel-spectrograms.
- **Break condition:** If the cochleagram resolution is too low or the quantization bottleneck is too narrow, the model may fail to capture necessary phonetic details, degrading performance.

### Mechanism 2
- **Claim:** Next-token prediction on discrete cochlear tokens is sufficient to induce lexical semantics without contrastive losses.
- **Mechanism:** By training a GPT-style Transformer to autoregressively predict the next cochlear token, the model must internalize the statistical regularities of speech, pressuring it to abstract away from specific speaker acoustics to more invariant linguistic units.
- **Core assumption:** The temporal statistics of speech encode semantic relationships that can be captured by a causal transformer.
- **Evidence anchors:**
  - [abstract]: "AuriStream learns meaningful phoneme and word representations, and state-of-the-art lexical semantics... without contrastive or reconstruction objectives."
  - [results 3.1]: AuriStream-1B achieves top performance on the sSIMI lexical semantic benchmark.
- **Break condition:** If the discrete token vocabulary is too large (over-fragmented) or context window too short, the model may memorize acoustic sequences rather than learning semantic abstractions.

### Mechanism 3
- **Claim:** Discrete cochlear tokens provide a unified interface for both representation learning and interpretable audio generation.
- **Mechanism:** Because the tokens are discrete indices into a cochleagram space, predicted sequences can be decoded back into visualizable spectrograms and inverted into audio, offering a window into the model's internal state that continuous embedding spaces lack.
- **Core assumption:** The LFQ bottleneck preserves enough information to make the inverted audio intelligible and useful for analysis.
- **Evidence anchors:**
  - [results 3.3]: "AuriStream generates continuations of audio which can be visualized in a spectrogram space... providing insights into the model's predictions."
  - [corpus]: *DrVoice* and *Baichuan-Audio* (corpus neighbors) similarly utilize discrete tokens to unify understanding and generation, validating this token-based interface approach.
- **Break condition:** If the decoder or inversion process is unstable, the generated audio may be unintelligible, removing the interpretability benefit.

## Foundational Learning

- **Concept: Cochleagram vs. Mel-Spectrogram**
  - **Why needed here:** WavCoch targets cochleagrams, not standard mel-spectrograms. A cochleagram simulates the frequency filtering of the human inner ear (cochlea), which differs from the FFT or mel-scale used in standard audio processing.
  - **Quick check question:** Does the cochleagram use a fixed linear frequency scale or a non-linear biological filter bank? (Answer: Biological filter bank, see Section 2.1).

- **Concept: LFQ (Lookup-Free Quantization)**
  - **Why needed here:** The paper uses LFQ to create the "cochlear tokens." Understanding that this maps continuous vectors to a discrete codebook (vocabulary size 8,192) is essential for understanding the input to the Transformer.
  - **Quick check question:** How many bits are used for the bottleneck code, and how many unique tokens does this yield? (Answer: 13 bits, 8,192 tokens; see Appendix 7.1).

- **Concept: Autoregressive (AR) vs. Masked Prediction**
  - **Why needed here:** AuriStream is strictly causal (AR), predicting the future from the past. This contrasts with models like HuBERT which look at the whole context (bidirectional/masked).
  - **Quick check question:** Can AuriStream use future context to predict the current token? (Answer: No, it is causal/autoregressive).

## Architecture Onboarding

- **Component map:** Raw Waveform -> WavCoch Encoder -> LFQ Bottleneck -> WavCoch Decoder (Target: Cochleagram) -> AuriStream Transformer -> Predicted Token -> Cochleagram -> Audio

- **Critical path:** The LFQ Bottleneck in WavCoch is the critical junction. If this discretization is inefficient (e.g., low codebook usage), the Transformer receives garbage data. The flow of information is strictly: `Waveform -> Cochlear Tokens -> Transformer Embeddings`.

- **Design tradeoffs:**
  - **Vocabulary Size:** The authors ablated 12, 13, and 14-bit codes. They chose 13-bit (8,192 tokens) as the sweet spot between reconstruction error and phoneme purity (Appendix 7.2).
  - **Causality:** By choosing AR prediction (causal) over Masked Prediction (bidirectional), the model gains generation capabilities but may lose some context-dependent decoding accuracy compared to bidirectional models like WavLM.

- **Failure signatures:**
  - **Drifting Generations:** For long sequences (>2.5s), the model's predictions may degrade or diverge from coherent speech (Section 3.3).
  - **Keyword Spotting (KS) Lag:** AuriStream underperforms on Keyword Spotting compared to other tasks, likely because local acoustic invariance is less emphasized than global semantic flow (Table 3).

- **First 3 experiments:**
  1. **Tokenizer Validation:** Train WavCoch on a small subset (e.g., LibriSpeech 100h) and verify that the LFQ bottleneck has high codebook usage (>90%) and that clusters correspond to phonemes.
  2. **Scale Ablation:** Train AuriStream-100M vs AuriStream-1B on a held-out set to confirm scaling laws apply to the lexical semantic (sSIMI) score specifically.
  3. **Interpretability Check:** Feed a 1-second prompt of a common word (e.g., "water") into the trained model and verify if the predicted cochleagram visually resembles the ground truth continuation (Figure 2B).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or auxiliary mechanisms are necessary to enforce longer-range temporal coherence in AuriStream's generated audio continuations?
- Basis in paper: [explicit] The authors note in Section 3.3 and Appendix 7.6 that while short-term predictions are plausible, continuations degrade over time, and they identify stabilizing generations with longer-term coherence as a goal for follow-up work.
- Why unresolved: The current autoregressive objective operates effectively on short timescales (phoneme-to-word) but appears to lack the constraints or memory mechanisms required to maintain semantic or syntactic consistency over multiple seconds.
- What evidence would resolve it: Demonstrating generated audio continuations (e.g., >5 seconds) where the semantic content remains consistent with the prompt, evaluated through human judgment or automated metrics for semantic consistency.

### Open Question 2
- Question: Does training AuriStream on naturalistic, conversational speech data (as opposed to read speech) improve its ecological validity and performance on paralinguistic tasks?
- Basis in paper: [explicit] The Conclusion explicitly lists "Extending training to more naturalistic and developmentally plausible data" as a key future direction, noting that current training on LibriLight read speech limits ecological validity.
- Why unresolved: The current model is trained on LibriLight, which consists of read audiobooks; it is unknown if the "cochlear token" representation scales to the noise, disfluencies, and varied acoustics of spontaneous human speech.
- What evidence would resolve it: A comparison of model performance on SUPERB tasks (specifically paralinguistics like emotion recognition or speaker identification) when pre-trained on datasets like Gigaspeech or SpokenCOCO versus the current LibriLight baseline.

### Open Question 3
- Question: Would incorporating global clustering operations (similar to k-means pseudolabels used in HuBERT) into the AuriStream framework improve word decoding accuracy without sacrificing its superior lexical semantic performance?
- Basis in paper: [inferred] In Section 3.1, the authors hypothesize that AuriStream's subpar word decoding performance relative to HuBERT is due to the absence of "global clustering operations aimed at discovering word-like units."
- Why unresolved: The current model relies solely on the local autoregressive prediction of cochlear tokens and lacks an explicit mechanism to discover or enforce discrete word-level units across the dataset.
- What evidence would resolve it: An ablation study where a global clustering loss is added to the training objective, measuring the change in TIMIT word decoding accuracy (Table 1) versus the sSIMI lexical semantic score (Table 2).

## Limitations
- Biological plausibility vs. engineering convenience: The specific 211-bin parametrization and DFT-based implementation are engineering choices rather than direct biological measurements.
- Long-form generation instability: Generated audio quality degrades for sequences longer than 2.5 seconds, limiting practical deployment.
- Limited ablation on quantization parameters: The choice of 13-bit LFQ was selected based on reconstruction error and phoneme purity, but alternative tokenization strategies weren't thoroughly explored.

## Confidence
- **High confidence**: The SUPERB benchmark results showing AuriStream's state-of-the-art performance on semantic tasks (sSIMI) are well-supported by direct comparisons with established baselines.
- **Medium confidence**: The claim that the two-stage architecture provides superior interpretability relies heavily on qualitative spectrogram visualizations without rigorous quantitative metrics.
- **Medium confidence**: The assertion that LFQ bottleneck + autoregressive prediction is sufficient to induce lexical semantics without contrastive losses is supported by results but could be confounded by the specific pretraining data scale.

## Next Checks
1. **Controlled vocabulary ablation**: Train AuriStream variants with 12-bit (4,096) and 14-bit (16,384) LFQ codes on identical data to quantitatively measure the trade-off between token granularity and semantic performance, particularly on sSIMI.
2. **Alternative target representation comparison**: Replace the cochleagram target with standard mel-spectrogram targets (same dimension) in WavCoch while keeping all other parameters constant, then measure degradation in phoneme purity and semantic task performance.
3. **Generation coherence quantification**: Implement an objective metric (e.g., ASR word error rate or semantic similarity to ground truth) to measure degradation in generated audio as sequence length increases from 1s to 5s, beyond the qualitative observations provided.