---
ver: rpa2
title: 'Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts
  Diffusion LLMs'
arxiv_id: '2602.00879'
source_url: https://arxiv.org/abs/2602.00879
tags:
- expert
- experts
- arxiv
- parallel
- coreset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamic Expert Sharing (DES) tackles the "expert explosion" bottleneck
  in Mixture-of-Experts diffusion large language models, where parallel decoding leads
  to linearly growing unique expert activations and memory traffic. DES introduces
  sequence-level coreset selection to minimize expert reuse and memory overhead, replacing
  token-centric pruning with a global optimization strategy.
---

# Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs

## Quick Facts
- arXiv ID: 2602.00879
- Source URL: https://arxiv.org/abs/2602.00879
- Reference count: 10
- Primary result: Reduces unique expert activations by 55%+ and latency by 38% in MoE dLLMs while maintaining 99% baseline accuracy

## Executive Summary
Dynamic Expert Sharing (DES) addresses the "expert explosion" problem in Mixture-of-Experts diffusion large language models, where parallel decoding causes linearly growing unique expert activations and memory traffic. By shifting from token-centric to sequence-level coreset selection, DES minimizes expert reuse and memory overhead through two approaches: DES-Seq (union-based) and DES-Vote (saliency-aware voting). Experiments on LLaDA-MoE models demonstrate over 55% reduction in unique expert activations and up to 38% latency reduction while retaining 99% of baseline accuracy.

## Method Summary
DES introduces sequence-level coreset selection to minimize expert reuse and memory overhead in MoE diffusion LLMs. The method shifts optimization from token-centric pruning to cross-token consensus, identifying a compact expert coreset per parallel block. DES-Seq unions top-k expert selections across tokens, while DES-Vote aggregates router weights to elect a high-utility expert coreset. Each token then selects its Top-K from this shared subset, maximizing expert reuse and minimizing HBM transfer. A custom fused kernel accelerates coreset selection by 6×, making the approach practical for deployment.

## Key Results
- Reduces unique expert activations by over 55% in LLaDA-MoE models
- Achieves up to 38% latency reduction while maintaining 99% baseline accuracy
- DES-Vote consistently outperforms DES-Seq with 0.6-2.3% relative accuracy gains
- Effectively decouples memory overhead from parallelism degree, enabling higher throughput without accuracy loss

## Why This Works (Mechanism)

### Mechanism 1: Sequence-Level Coreset Selection
- Claim: Shifting from per-token expert selection to a shared sequence-level coreset reduces unique expert activations by >55% while preserving routing fidelity.
- Mechanism: Instead of N tokens independently selecting K experts (yielding |∪Sₙ| unique experts), DES identifies a compact coreset C via cross-token consensus. Each token then selects its Top-K from C, not from the full expert pool.
- Core assumption: Parallel decoding tokens in dLLMs are semantically coupled and exhibit significant expert demand overlap.
- Evidence anchors: [abstract] "shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection"; [section 4.1] "By restricting each token's Top-k selection to this shared subset, we maximize expert reuse and minimize HBM transfer"
- Break condition: If parallel tokens have highly divergent semantic requirements, expert overlap may be insufficient, causing accuracy degradation.

### Mechanism 2: Saliency-Aware Voting (DES-Vote)
- Claim: Weighted voting based on aggregated router weights outperforms union-based selection by capturing cross-token expert importance more accurately.
- Mechanism: For each token, mask out weights outside its local Top-K to filter noise, then aggregate masked weights across the sequence: Vᵢ = Σ Masked(Iₙ,ᵢ). Select top M_core experts by total vote.
- Core assumption: Raw gating weights correlate with actual expert importance.
- Evidence anchors: [abstract] "tokens vote based on aggregated router weights to form a more globally representative coreset"; [section 4.2] "DES-Vote outperforms DES-Seq in both top-K hit rate (preserving more ground truth selections) and reconstruction loss across varying coreset sizes"
- Break condition: If router weights are poorly calibrated, voting may systematically favor incorrect experts.

### Mechanism 3: Memory Traffic Decoupling via Union Minimization
- Claim: Minimizing unique expert count decouples memory overhead from parallelism degree, yielding up to 38% latency reduction.
- Mechanism: The latency model L_MoE = b·|∪Sₙ| + a·(N·K) shows that in memory-bound regimes, the weight-fetch term (b·|∪Sₙ|) dominates. DES reduces |∪Sₙ| from ~84 to ~38 experts, making latency insensitive to block size N.
- Core assumption: Inference operates in memory-bound regime where HBM-to-SRAM transfer dominates.
- Evidence anchors: [abstract] "effectively decoupling memory overhead from the degree of parallelism"; [section 3] "MoE dLLM inference is still usually memory-bound, with latency dominated by the cost of loading unique expert weights"
- Break condition: If deployed on hardware with very high memory bandwidth or very large blocks making compute dominant, gains diminish.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) Routing
  - Why needed here: DES modifies standard Top-K routing; understanding baseline routing is prerequisite.
  - Quick check question: Can you explain why MoE latency depends on unique experts activated, not just K×N?

- Concept: Roofline Model / Memory-Bound vs Compute-Bound
  - Why needed here: Paper's efficiency claims depend on MoE dLLMs being memory-bound.
  - Quick check question: Given FLOPs/byte ratio, would DES help more on H100 (3.35 TB/s) or A100 (2 TB/s)?

- Concept: Diffusion LLM Parallel Decoding
  - Why needed here: DES exploits semantic coupling in parallel blocks—specific to dLLM structure.
  - Quick check question: Why does parallel decoding in dLLMs differ from speculative decoding with AR verification?

## Architecture Onboarding

- Component map: Input: Router logits I ∈ R^(N×M) for N tokens, M experts -> [Stage 1: Sequence-level Consensus] DES-Seq: C = ∪ TopK(Iₙ, k) for reduced k; DES-Vote: C = TopK(Σ masked(Iₙ), M_core) -> [Stage 2: Constrained Local Routing] For each token n: Sₙ = TopK(Iₙ|ᵢ∈C, K), re-normalize, compute weighted expert sum -> Output: Layer outputs Y

- Critical path: Coreset selection latency (mitigated by custom fused kernel, 6× speedup per Figure 6). DES-Vote adds aggregation overhead but kernel fusion reduces this to ~5-40µs.

- Design tradeoffs:
  - β (budget factor) / k (local selection): Smaller = more memory savings, higher accuracy risk.
  - DES-Seq vs DES-Vote: DES-Vote consistently outperforms (+0.6% to +2.3% relative accuracy) but requires weight aggregation.
  - Block size: Larger blocks increase vanilla expert explosion; DES maintains constant expert count regardless of block size.

- Failure signatures:
  - Accuracy drops >1%: Coreset too small; increase β/k.
  - Minimal latency improvement: Not memory-bound (check roofline); or kernel fusion not applied.
  - Expert hit rate cosine similarity <0.95: Coreset selection distorting routing patterns excessively.

- First 3 experiments:
  1. Replicate Table 1 baseline: Run vanilla MoE dLLM on GSM8K with block=32, measure T (unique experts) and accuracy.
  2. Ablate coreset size: Sweep β ∈ {0.1, 0.15, 0.3, 0.6} for DES-Vote, plot accuracy vs. T (Figure 7a pattern).
  3. Profile kernel overhead: Compare PyTorch vs. custom fused kernel for coreset selection (expect ~6× speedup per Figure 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism behind the accuracy improvements observed in DES-Vote over the vanilla baseline, and is it strictly a regularization effect?
- Basis in paper: [explicit] Section 5.4 notes that in certain instances, the accuracy gain is positive and hypothesizes this "could potentially be attributed to a regularization effect, where the coreset selection process prunes away lower-utility experts."
- Why unresolved: The paper identifies the phenomenon and offers a hypothesis but does not provide theoretical proof or ablations confirming that noise reduction is the specific cause of the superior performance.
- What evidence would resolve it: Layer-wise analysis of expert output variance and ablation studies comparing DES against explicit noise-injection or dropout baselines to confirm the regularization properties.

### Open Question 2
- Question: How can the parallel block size be dynamically optimized now that memory overhead is decoupled from parallelism?
- Basis in paper: [explicit] Section 5.4 concludes that "practitioners can optimize block size based purely on the trade-off between multi-token generation efficiency and model accuracy, unconstrained by the traditional limits of memory bandwidth."
- Why unresolved: While the paper removes the memory constraint, it does not propose a method for navigating the newly opened design space to determine the optimal block length for a given task.
- What evidence would resolve it: An adaptive scheduling algorithm that adjusts block sizes in real-time based on semantic complexity or confidence scores, demonstrating further throughput gains.

### Open Question 3
- Question: Can sequence-level coreset selection be effectively transferred to Autoregressive (AR) models operating with large batch sizes?
- Basis in paper: [inferred] The paper limits evaluation to dLLMs and contrasts the method with existing token-centric AR optimizations. It assumes "semantically coupled tokens" exist in dLLMs (Section 4), raising the question of whether this assumption holds for the independent sequences typically found in AR batching.
- Why unresolved: The paper does not test if the "expert explosion" in AR models (caused by batch size rather than parallel blocks) can be mitigated using the same coreset techniques without compromising the independence of user requests.
- What evidence would resolve it: Experiments applying DES to high-throughput AR serving systems to determine if cross-sequence expert sharing degrades per-request accuracy.

## Limitations
- Performance generalization beyond numerical reasoning tasks remains limited
- Hardware dependency on memory-bound operation; gains diminish in compute-bound regimes
- Limited evidence for robustness to domain-shifted input distributions

## Confidence
- High Confidence: DES reduces unique expert activations by >55% while maintaining 99% baseline accuracy; DES-Vote outperforms DES-Seq in both accuracy and efficiency metrics; Memory traffic reduction decouples overhead from parallelism degree in controlled experiments
- Medium Confidence: 38% latency reduction across all tested scenarios; Generalization to non-numerical reasoning tasks; Kernel optimization scalability across different hardware
- Low Confidence: Performance in compute-bound regimes; Robustness to domain-shifted input distributions; Effectiveness with heterogeneous parallel token blocks

## Next Checks
1. **Ablation on Task Diversity**: Run DES on non-mathematical tasks (e.g., HumanEval code generation, text summarization) with varying block sizes. Measure both accuracy retention and expert activation reduction to validate the claimed generalization beyond numerical reasoning.

2. **Hardware Regime Analysis**: Deploy DES on both memory-bound (B200) and compute-bound (simulated by artificially increasing compute per token) configurations. Quantify the break-even point where memory traffic reduction no longer dominates latency, validating the hardware dependency claims.

3. **Router Calibration Stress Test**: Systematically degrade router weight quality (via temperature scaling, quantization, or early-training checkpoints) and measure DES performance degradation. This would validate the core assumption that router weights correlate with expert importance, and establish when voting-based coreset selection fails.