---
ver: rpa2
title: 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon
  LLM Agents'
arxiv_id: '2509.09265'
source_url: https://arxiv.org/abs/2509.09265
tags:
- policy
- learning
- empg
- arxiv
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Entropy-Modulated Policy Gradients (EMPG),
  a method designed to improve credit assignment in long-horizon LLM agents. The key
  insight is that standard policy gradients inherently couple gradient magnitude with
  policy entropy, leading to inefficient learning for confident correct actions and
  instability from uncertain ones.
---

# Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents

## Quick Facts
- **arXiv ID**: 2509.09265
- **Source URL**: https://arxiv.org/abs/2509.09265
- **Reference count**: 40
- **Primary result**: EMPG significantly outperforms GRPO and DAPO on WebShop, ALFWorld, and Deep Search benchmarks with gains up to +8.1% in success rate.

## Executive Summary
This paper introduces Entropy-Modulated Policy Gradients (EMPG), a method designed to improve credit assignment in long-horizon LLM agents. The key insight is that standard policy gradients inherently couple gradient magnitude with policy entropy, leading to inefficient learning for confident correct actions and instability from uncertain ones. EMPG re-calibrates the learning signal by dynamically modulating policy gradients based on step-wise uncertainty: amplifying updates for confident correct actions, penalizing confident errors, and attenuating updates from uncertain steps. Additionally, it introduces a future clarity bonus to encourage predictable solution paths. Experiments on WebShop, ALFWorld, and Deep Search benchmarks show that EMPG significantly outperforms strong baselines like GRPO and DAPO, with gains up to +8.1% in success rate, demonstrating its effectiveness and scalability across different tasks and model sizes.

## Method Summary
EMPG modulates advantage functions based on per-step entropy computed from token-level averages within each "reason-then-act" cycle. The method applies self-calibrating scaling using g(H) = exp(-k·H_norm) / mean_batch[exp(-k·H_norm)] to amplify confident steps and attenuate uncertain ones, counteracting the natural coupling between gradient magnitude and entropy. It also adds a future clarity bonus f(H_{t+1}) = ζ·exp(-k'·H_{norm,t+1}) to encourage actions leading to low-entropy future states. The approach is built on GRPO/DAPO backbones with ReAct agent framework, using batch min-max entropy normalization followed by zero-mean final advantage normalization.

## Key Results
- WebShop: +8.1% ID success rate, +3.4% OOD over DAPO baseline
- ALFWorld: +6.1% ID success rate over DAPO
- Deep Search: +0.8% ID success rate over DAPO
- Component ablations show gradient scaling contributes +2.5 points ID/+1.4 points OOD, future clarity bonus contributes +2.6 points ID/+1.6 points OOD on ALFWorld

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Gradient Decoupling via Self-Calibrating Scaling
Standard policy gradients produce inefficiently small updates for confident actions and destabilizing large updates for uncertain ones due to inherent coupling between gradient magnitude and policy entropy. The scaling function g(H) = exp(-k·H_norm) / mean_batch[exp(-k·H_norm)] reweights advantages: values >1 amplify confident steps, values <1 attenuate uncertain steps. This counteracts the natural gradient dynamics where expected gradient norm is monotonically coupled with Rényi-2 entropy.

### Mechanism 2: Future Clarity Bonus for Directed Exploration
An intrinsic reward proportional to next-step confidence guides agents toward predictable solution paths without requiring external reward shaping. The bonus term f(H_{t+1}) = ζ·exp(-k'·H_{norm,t+1}) adds a positive bonus proportional to the confidence (low entropy) of the next step, creating a local objective of minimizing entropy at the next state.

### Mechanism 3: Step-Level Entropy Analysis for Long-Horizon Credit Assignment
Token-level entropy dynamics from prior work do not transfer to step-level; even low-entropy steps require significant policy updates in long-horizon tasks. Step-level entropy H_t computed as average token entropy within each "reason-then-act" cycle captures decision uncertainty at the granularity where credit should be assigned, with analysis showing substantial entropy changes even for initially low-entropy steps.

## Foundational Learning

- **Policy Gradient Variance and the Role of Advantage Functions**: Understanding why advantages reduce variance is prerequisite to understanding why reweighting them matters. Quick check: Given a trajectory with return R=1 and three steps, what happens to gradient variance if you use uniform advantage (1,1,1) vs. entropy-weighted advantage (1.5, 0.5, 1.0)?

- **Entropy as Uncertainty Quantification in LLMs**: The method relies on entropy as a proxy for model confidence; understanding its limitations is critical. Quick check: A model produces low-entropy outputs consistently but makes errors. What failure mode does this represent, and how does EMPG address it?

- **Sparse Reward Credit Assignment in RL**: The core problem EMPG solves is determining which intermediate steps deserve credit when reward only arrives at trajectory end. Quick check: In a 10-step WebShop task with success at step 10, why would uniform credit assignment (A_t = 1 for all t) be suboptimal?

## Architecture Onboarding

- **Component map**: Trajectory Batch → Step Segmentation → Per-Step Entropy H_t → Batch Normalization → g(H_t) scaling and f(H_{t+1}) clarity bonus → A_mod advantage modulation → Final Normalization → Policy Gradient Update

- **Critical path**: Step segmentation → entropy computation → batch normalization → scaling computation → advantage modulation. Errors in step boundary detection cascade to all downstream components.

- **Design tradeoffs**: Batch-level vs. running normalization (paper uses batch-level for stateless adaptability); step vs. token granularity (step-level chosen based on empirical entropy dynamics); ζ weight (set to 0.05 for WebShop/ALFWorld, 0.1 for Deep Search).

- **Failure signatures**: All g(H_t) ≈ 1 indicates entropy normalization failure or k too small; KL divergence spikes suggest gradient scaling not attenuating high-entropy updates; OOD performance degradation suggests Future Clarity Bonus overfitting to training distribution.

- **First 3 experiments**:
  1. Entropy-grad coupling verification: Compute ||∇log π(a|s)||² for actions from high vs. low entropy states to confirm positive correlation
  2. Scaling function ablation: Compare g(H) = exp(-k·H_norm)/mean vs. alternatives on single ALFWorld task
  3. Component isolation test: Run (a) gradient scaling only, (b) clarity bonus only, (c) full EMPG on WebShop with Qwen2.5-1.5B

## Open Questions the Paper Calls Out

1. **Alternative uncertainty estimators**: How do Monte Carlo dropout or ensemble variance compare to policy entropy for gradient modulation in EMPG? The paper suggests exploring these but provides no empirical comparison.

2. **Embodied AI and multi-agent transfer**: Does EMPG transfer effectively to embodied AI environments with continuous action spaces and multi-agent collaboration scenarios? Current experiments are limited to text-based discrete action spaces.

3. **Hyperparameter sensitivity**: What is the sensitivity of EMPG to hyperparameters k, k', and ζ across diverse task domains and model scales? The paper uses fixed values without systematic sensitivity analysis.

## Limitations

- Performance gains vary considerably across tasks, with Deep Search showing much smaller improvements than WebShop or ALFWorld
- Hyperparameter sensitivity lacks systematic analysis, raising questions about robustness and generalization
- Step boundary definition is critical but minimally specified, with unclear validation of "reason-then-act" alignment with credit assignment needs

## Confidence

- **High Confidence**: The core mechanism of entropy-gradient decoupling is theoretically grounded and empirically validated with substantial, consistent gains on WebShop and ALFWorld
- **Medium Confidence**: The future clarity bonus mechanism shows measurable benefits but with weaker theoretical justification and varying effectiveness across tasks
- **Low Confidence**: The step-level entropy analysis provides interesting observations but lacks rigorous theoretical framework for why step-level is superior to token-level

## Next Checks

1. **Hyperparameter Sweep Validation**: Conduct systematic sensitivity analysis of k, k', and ζ parameters across all three benchmark tasks, generating heatmaps to identify robustness regions

2. **Step Boundary Robustness Test**: Implement EMPG with alternative step segmentation strategies (fixed token windows, semantic boundaries, human-labeled steps) and measure performance degradation

3. **OOD Generalization Stress Test**: Design controlled experiment training EMPG on restricted WebShop subset and testing on novel categories, comparing against GRPO/DAPO to isolate whether gains stem from uncertainty handling or other factors