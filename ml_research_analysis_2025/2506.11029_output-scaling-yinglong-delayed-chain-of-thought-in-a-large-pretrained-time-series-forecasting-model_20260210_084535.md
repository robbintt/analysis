---
ver: rpa2
title: 'Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time
  Series Forecasting Model'
arxiv_id: '2506.11029'
source_url: https://arxiv.org/abs/2506.11029
tags:
- forecasting
- time
- series
- crps
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a joint forecasting framework that unifies
  direct and recursive time series prediction methods. The key innovation is delayed
  chain-of-thought (DCoT) reasoning, enabled by a non-causal, bidirectional transformer
  trained with masked token recovery.
---

# Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model

## Quick Facts
- arXiv ID: 2506.11029
- Source URL: https://arxiv.org/abs/2506.11029
- Authors: Xue Wang; Tian Zhou; Jinyang Gao; Bolin Ding; Jingren Zhou
- Reference count: 33
- Primary result: Achieves over 60% best performance on GIFT-Eval, with 14% MASE and 44% CRPS rank improvements over prior methods.

## Executive Summary
This paper introduces a joint forecasting framework that unifies direct and recursive time series prediction methods. The key innovation is delayed chain-of-thought (DCoT) reasoning, enabled by a non-causal, bidirectional transformer trained with masked token recovery. This allows future tokens to improve predictions for earlier targets, revealing an output scaling effect: longer forecasts yield higher accuracy. The approach is validated on GIFT-Eval, ETT, and Weather datasets, achieving over 60% best performance, with MASE and CRPS improvements of 14% and 44% in rank over prior methods. A multi-input ensemble strategy further enhances robustness. Models ranging from 6M to 300M parameters are released, with the 300M variant available publicly. The method demonstrates superior generalization and robustness across diverse domains.

## Method Summary
YingLong is a joint forecasting framework that uses a bidirectional transformer with delayed chain-of-thought (DCoT) reasoning. The model treats time series as sequences of discrete tokens via patching (patch size 32), then applies masked token recovery training. During inference, it extends the output horizon with placeholder DCoT tokens, allowing future tokens to attend to and refine earlier target predictions. A multi-input ensemble averages predictions from varying lookback windows to reduce variance. The model is trained on ~78B time points and evaluated on GIFT-Eval (23 datasets, 7 domains), ETT, and Weather datasets using MASE, CRPS, and Rank metrics.

## Key Results
- Achieves over 60% best performance on GIFT-Eval benchmark
- MASE and CRPS rank improvements of 14% and 44% over prior methods
- Output scaling effect: longer forecasts (via DCoT) significantly enhance accuracy
- Multi-input ensemble improves accuracy by 1%-4% without additional training

## Why This Works (Mechanism)

### Mechanism 1: Output Scaling via Delayed Chain of Thought (DCoT)
Extending the prediction horizon with placeholder tokens (DCoT) improves accuracy on the target horizon, provided the series exhibits periodicity or low-frequency structure. Unlike autoregressive models where future depends strictly on past, YingLong uses bidirectional attention. Tokens placed after the target horizon ("future thoughts") can attend to and refine the representation of earlier target tokens, acting as conditional anchors that resolve ambiguity in the target prediction. If the series is high-entropy random walk or non-stationary without repeating patterns, DCoT tokens likely introduce noise rather than signal anchors.

### Mechanism 2: Joint Forecasting via Non-Causal Masking
A joint forecasting framework (predicting all horizons simultaneously) outperforms recursive methods by eliminating error accumulation. The model is trained via masked token recovery. By treating forecast targets as masked patches within a continuous sequence, the bidirectional transformer learns global dependencies between all time steps at once, rather than conditioning solely on previous step outputs. When the sequence length exceeds the model's context window, fragmenting the global attention capability required for joint prediction.

### Mechanism 3: Multi-Input Ensemble for Variance Reduction
Averaging predictions from varying input lookback windows reduces variance and stabilizes forecasts without retraining. Shorter lookbacks capture high-frequency patterns while longer lookbacks capture low-frequency trends. Ensembling these creates a "free lunch" robustness against the horizon-specific limitations of fixed windows. If the optimal lookback window for a specific dataset is already known and fixed, the computational overhead of ensembling may outweigh marginal variance gains.

## Foundational Learning

**Concept: Bidirectional Attention (Encoder-only)**
Why needed: Unlike standard causal LLMs, YingLong allows the model to "see the future" (DCoT tokens) to interpret the present.
Quick check: If you masked the attention matrix to be strictly causal (lower-triangular), would Output Scaling still work? (Answer: No, future tokens could not inform past targets).

**Concept: Patching and Tokenization**
Why needed: The model converts continuous time series into discrete tokens (patches of length 32). This is the granularity at which masking and recovery occur.
Quick check: How does the model handle the forecast horizon during training vs. inference? (Answer: Training: random masking; Inference: specific end-of-sequence masking + DCoT padding).

**Concept: Quantile Loss (Probabilistic Forecasting)**
Why needed: YingLong outputs distributions (quantiles) rather than point estimates to handle uncertainty.
Quick check: What loss function optimizes the predicted quantiles? (Answer: Weighted Quantiles Loss / WQL).

## Architecture Onboarding

**Component map:** Input -> Tokenizer (Patching + Linear Projection) -> Backbone (U-Net Transformer with Bidirectional Attention + Token Merging) -> Head (Parallel Linear Layers for R quantiles)

**Critical path:** The DCoT configuration is the most sensitive hyperparameter. You must ensure the output sequence length includes the target horizon plus the DCoT tokens.

**Design tradeoffs:**
- DCoT Length: Longer DCoT improves MASE (up to 10.1%) but increases inference latency linearly
- Ensemble Size: More input windows improve robustness but require multiple forward passes (mitigated by batch processing)

**Failure signatures:**
- Flat Scaling: If increasing DCoT length does not reduce error, the dataset likely lacks the periodic structure required for the mechanism
- Training Instability: If WQL diverges, check the weight normalization for quantile ranges

**First 3 experiments:**
1. Baseline Ablation: Run inference with DCoT length = 0 vs DCoT length = 4096 on a validation set to verify the Output Scaling effect
2. Lookback Sweep: Test single-input inference with lookbacks [512, 1024, 2048] to see which frequency dominates, then compare to the Multi-Input Ensemble result
3. Bidirectional Check: (Advanced) If possible, mask the attention to be causal and observe the performance drop to confirm the dependency on bidirectional flow

## Open Questions the Paper Calls Out
None

## Limitations
- Output Scaling Dependency on Structure: The DCoT mechanism fundamentally relies on the presence of periodic or low-frequency structures in the target time series
- Model Size vs. Performance Trade-offs: The scaling relationship between parameter count and accuracy is not thoroughly analyzed
- Inference Cost Scaling: DCoT improves accuracy but increases inference latency linearly, with no cost-benefit analysis provided

## Confidence

**High Confidence:** The joint forecasting framework (combining recursive and direct methods via bidirectional attention) is well-grounded in the methodology. The multi-input ensemble approach is a straightforward variance reduction technique with clear theoretical justification.

**Medium Confidence:** The DCoT mechanism's effectiveness is demonstrated empirically but lacks theoretical bounds. The claim that "longer outputs significantly enhance model accuracy" is supported by experiments but assumes the presence of exploitable structure in the data.

**Low Confidence:** Claims about zero-shot generalization across diverse domains are based on aggregate metrics. Without per-domain breakdowns or analysis of failure cases, it's difficult to assess robustness to truly out-of-distribution data.

## Next Checks

1. **Structure Sensitivity Test:** Apply YingLong to synthetic time series with controlled periodicity (sinusoidal with varying frequencies) and noise levels. Systematically vary the DCoT length and measure accuracy degradation as periodicity decreases.

2. **Causal Attention Ablation:** Modify the attention mechanism to enforce causality (lower-triangular mask) and compare performance with the bidirectional version.

3. **Cost-Benefit Analysis:** Measure the accuracy improvement per additional DCoT token and per second of inference time. Plot accuracy vs. computational cost for different DCoT lengths to identify the optimal trade-off point for practical deployment.