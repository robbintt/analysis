---
ver: rpa2
title: Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness
  Aware Minimization
arxiv_id: '2510.03763'
source_url: https://arxiv.org/abs/2510.03763
tags:
- arsam
- gradient
- training
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ARSAM (Adaptively Sampling-Reusing-Mixing Decomposed
  Gradients to Speed Up Sharpness Aware Minimization) to address the high computational
  cost of SAM (Sharpness-Aware Minimization). SAM improves model generalization by
  finding flat minima but requires two gradient computations per iteration, doubling
  the computational cost compared to SGD.
---

# Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization

## Quick Facts
- arXiv ID: 2510.03763
- Source URL: https://arxiv.org/abs/2510.03763
- Reference count: 8
- Achieves ~40% speedup over SAM while maintaining comparable accuracy on CIFAR-10/100

## Executive Summary
SAM improves model generalization by finding flat minima but requires two gradient computations per iteration, doubling the computational cost compared to SGD. This paper proposes ARSAM, which decomposes SAM's gradient into SGD and Projection of Second-order gradient onto First-order gradient (PSF). The authors observe that PSF grows faster than SGD during training, indicating its increasing importance. ARSAM adaptively samples and reuses PSF based on their contribution, using an autoregressive model to adjust sampling frequency, achieving comparable accuracy to SAM while providing approximately 40% speedup.

## Method Summary
The paper observes that SAM's gradient can be decomposed into the SGD gradient and the Projection of the Second-order gradient onto the First-order gradient (PSF). Through analysis, the authors find that the L2-PSF increases faster than L2-SGD during training, indicating PSF's growing importance for finding flat minima. ARSAM introduces an adaptive sampling mechanism that selectively computes and reuses PSF gradients based on their estimated contribution to convergence. An autoregressive model determines the sampling frequency, allowing the method to reduce computational overhead while maintaining SAM's generalization benefits.

## Key Results
- Achieves approximately 40% speedup compared to standard SAM on CIFAR-10/100
- Maintains comparable accuracy to SAM across various network architectures
- Demonstrates broad applicability to tasks including human pose estimation and model quantization

## Why This Works (Mechanism)
The method exploits the observation that SAM's gradient consists of two components with different convergence behaviors. The SGD component provides stable but less informative gradients, while the PSF component becomes increasingly important as training progresses. By adaptively sampling PSF based on its contribution, ARSAM reduces redundant computations while preserving the critical information needed for finding flat minima. The autoregressive model enables dynamic adjustment of sampling frequency based on training dynamics.

## Foundational Learning
- Sharpness-Aware Minimization (SAM): Optimization technique that minimizes both loss and loss sharpness; needed to understand the baseline method being accelerated
- Gradient decomposition: Breaking down complex gradients into simpler components; quick check: verify decomposition preserves the original gradient direction
- Second-order optimization: Using curvature information for improved convergence; quick check: validate that PSF captures relevant curvature information
- Adaptive computation: Dynamically adjusting computational effort based on training dynamics; quick check: monitor sampling frequency changes during training

## Architecture Onboarding

**Component Map**: Data -> SGD Computation -> PSF Estimation -> Adaptive Sampling -> Parameter Update

**Critical Path**: The most critical components are the PSF estimation and adaptive sampling modules, as they directly determine computational efficiency and convergence quality.

**Design Tradeoffs**: The method trades off between computational efficiency and gradient accuracy. More frequent PSF sampling improves convergence but reduces speedup. The autoregressive model must balance responsiveness with stability.

**Failure Signatures**: Poor convergence or accuracy degradation may indicate insufficient PSF sampling frequency. Excessive sampling negates the speedup benefits. Training instability could suggest incorrect PSF estimation or sampling decisions.

**First Experiments**:
1. Verify gradient decomposition preserves SAM's original gradient direction
2. Test adaptive sampling frequency adjustment across different training phases
3. Compare convergence curves with varying PSF sampling rates

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope to CIFAR-10/100 datasets and specific architectures
- Autoregressive model design choices lack extensive ablation studies
- Validation for broader applications appears limited in scope

## Confidence
- High confidence: Computational speedup claims and CIFAR-10/100 experimental results
- Medium confidence: Generalization claims to other tasks and architectures
- Medium confidence: Effectiveness of the autoregressive sampling mechanism

## Next Checks
1. Evaluate ARSAM's performance on larger-scale datasets (ImageNet) and architectures (Vision Transformers)
2. Conduct ablation studies on the autoregressive model's hyperparameters and alternative sampling strategies
3. Test the method's robustness to different initialization schemes and learning rate schedules