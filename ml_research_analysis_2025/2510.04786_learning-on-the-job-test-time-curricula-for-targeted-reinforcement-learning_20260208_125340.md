---
ver: rpa2
title: 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning'
arxiv_id: '2510.04786'
source_url: https://arxiv.org/abs/2510.04786
tags:
- training
- ttc-rl
- tasks
- test-time
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for test-time curricula (TTC-RL) that
  enables large language models to continually improve their reasoning on target tasks
  by automatically curating and learning from task-specific training data during test-time.
  TTC-RL leverages reinforcement learning on a self-selected curriculum of related
  tasks, avoiding manual data curation and enabling efficient, targeted practice.
---

# Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.04786
- Source URL: https://arxiv.org/abs/2510.04786
- Reference count: 40
- Large language models can significantly improve reasoning performance on target tasks through test-time curriculum learning using reinforcement learning on automatically curated related tasks.

## Executive Summary
This paper introduces Test-Time Curricula for Reinforcement Learning (TTC-RL), a method enabling large language models to improve their reasoning capabilities on target tasks by automatically curating and learning from related training tasks during test-time. The approach leverages reinforcement learning with verifiable rewards, using SIFT (Selection via Information Gain with F-score) to identify the most informative related tasks from a large corpus. TTC-RL demonstrates significant performance improvements across diverse domains including math (AIME25), coding (CodeElo), and scientific reasoning (GPQA-Diamond), raising the performance ceiling and enabling effective model specialization without manual data curation.

## Method Summary
TTC-RL operates by first selecting a curriculum of related tasks from a large verifiable corpus using SIFT, which greedily chooses tasks that maximize information gain for the target task based on kernel-based posterior variance minimization in the model's embedding space. The selected curriculum is then used to train the model via Group Relative Policy Optimization (GRPO) with asymmetric clipping (clip_high=0.28) to prevent policy entropy collapse. The training uses binary verifiable rewards (unit tests, exact match, semantic equivalence) and operates within the model's context window, compressing learned strategies into weights rather than requiring longer inference-time contexts. The method demonstrates that models can meta-learn task-specific reasoning patterns through this process, enabling transfer to the target task.

## Key Results
- TTC-RL achieves 1.8x improvement in pass@1 on AIME25 and 2.1x improvement on CodeElo benchmarks
- The method significantly raises performance ceilings, increasing pass@8 from 40% to 62% on AIME25
- TTC-RL complements existing test-time scaling approaches and demonstrates substantial latent improvements in reasoning correctness beyond format learning

## Why This Works (Mechanism)

### Mechanism 1
Task-specific curriculum selection via SIFT identifies training examples that maximize information gain for the target task by using kernel-based posterior variance minimization on the model's embedding space to greedily select examples that reduce uncertainty about the target task, balancing relevance and diversity through λ regularization. The core assumption is that the model's last-token last-layer embeddings capture semantic similarity relevant to reasoning patterns needed for the target task. Evidence shows SIFT successfully selects informative examples, though performance depends on embedding space alignment with reasoning-relevant similarity.

### Mechanism 2
On-policy RL with asymmetric clipping maintains exploration while reinforcing successful strategies through GRPO with increased clip-high (ε_high = 0.28 vs. default 0.2) that prevents policy entropy collapse, allowing the model to discover diverse solution strategies rather than converging prematurely to suboptimal patterns. The core assumption is that maintaining higher entropy during exploration leads to discovering transferable reasoning strategies. Evidence from entropy monitoring and performance improvements validates this approach, though the specific parameter value requires empirical tuning.

### Mechanism 3
Compressing experience into weights via RL enables transfer beyond bounded context windows by training on curated related tasks so the model meta-learns task-specific reasoning strategies stored in weights, enabling better performance on the target task without requiring longer inference-time context. The core assumption is that weight updates capture generalizable reasoning patterns, not just surface-level task features. Evidence shows TTC-RL on short-context models can match long-context thinking models, though overfitting to task idiosyncrasies remains a risk.

## Foundational Learning

- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Needed because TTC-RL relies on binary verification signals rather than learned reward models. Quick check: Can you explain why sparse binary rewards require different exploration strategies than dense shaped rewards?

- **GRPO (Group Relative Policy Optimization)**: The paper's training algorithm; understanding advantage normalization and clipping is essential for reproduction. Quick check: How does GRPO's group-based advantage estimation differ from PPO's per-sample advantages?

- **Active Learning and Information-Theoretic Data Selection**: SIFT's selection criterion derives from Bayesian experimental design principles. Quick check: Why does minimizing posterior variance lead to diverse-yet-relevant data selection?

## Architecture Onboarding

- **Component map**: Embedding extractor (uses model's last-token last-layer activations) -> SIFT selector (greedily selects curriculum from corpus based on kernel variance) -> Training corpus (265k tasks across math, code, and reasoning) -> GRPO trainer (on-policy RL with asymmetric clipping) -> Verifiers (math-verify, 1.5B semantic equivalence model, unit test runners)

- **Critical path**: Target task -> SIFT selection (λ=0.1, curriculum size ~1k) -> GRPO training (2 episodes, batch=8, 16 rollouts/task) -> specialized model

- **Design tradeoffs**: Smaller curriculum (100) enables faster training but lower performance ceiling; larger curriculum (1000+) provides better ceiling but more compute (Figure 3 shows diminishing returns). Per-task vs. per-benchmark TTC affects specialization granularity. Context length vs. TTC-RL presents a compute-optimal frontier.

- **Failure signatures**: Entropy collapse (training reward rises but test accuracy flat) requires increasing clip-high. Format overfitting (high accuracy but low latent improvement) indicates answer extraction issues. Curriculum mismatch (training on wrong difficulty) requires achievability balancing.

- **First 3 experiments**: 1) Baseline sanity check: Run TTC-RL on AIME24 with Qwen3-8B; expect 1.5-1.8x pass@1 improvement over 200 steps. 2) Curriculum size ablation: Compare sizes 100, 500, 1000 on single benchmark; plot test accuracy vs. step. 3) Specialization validation: Train separate TTCs for math, code, and GPQA; evaluate each on all benchmarks to confirm diagonal structure.

## Open Questions the Paper Calls Out

1. **Context length vs. TTC-RL trade-off**: At what context length does it become more advantageous to scale TTC-RL rather than increasing the context window? The paper identifies this as an exciting topic for future research, noting that while short-context models with TTC-RL can match long-context models, the exact trade-off point and scaling laws remain unexplored.

2. **Discovery vs. distillation**: Under what circumstances does RL-training enable a model to "discover new behavior" (improving pass@k) rather than merely distilling existing capabilities into pass@1? While TTC-RL significantly raises the performance ceiling, the paper notes that understanding when RL enables genuine strategy discovery versus standard distillation is an exciting direction.

3. **Optimal specialization granularity**: How much task-specific specialization is beneficial, and when does fine-grained specialization (per-task) outperform broader specialization (per-benchmark)? The paper discusses trade-offs between specialization levels but acknowledges that determining the optimal curriculum breadth for robust convergence remains an open question.

4. **Beyond fixed corpus**: How can the method move beyond the bottleneck of a fixed training corpus to utilize self-generated or dynamically retrieved tasks? The current reliance on a pre-existing verifiable corpus limits performance saturation, and the authors suggest self-generated TTCs as a natural direction for future work.

## Limitations

- Curriculum selection sensitivity: While SIFT claims robustness to λ, performance may degrade at parameter extremes and depends on embedding space alignment with reasoning-relevant similarity
- Compute requirements: TTC-RL requires training on curated curricula during test-time with curriculum sizes of 1000+ tasks, with unclear latency implications
- Entropy clipping parameter sensitivity: The asymmetric clipping modification lacks theoretical grounding and may affect reproducibility

## Confidence

**High confidence**: Claims about TTC-RL's ability to improve target task performance through test-time training, supported by consistent improvements across 11 benchmarks (1.8x on AIME25 pass@1, 2.1x on CodeElo). The mechanism of using binary verifiable rewards via GRPO is well-established.

**Medium confidence**: Claims about TTC-RL complementing test-time scaling approaches (verifiers, CoT). While empirical results show improvements when combined, the paper doesn't systematically study interactions or identify optimal combinations.

**Low confidence**: Claims about TTC-RL's superiority over traditional curriculum learning methods. The comparison to "manual data curation" is qualitative rather than quantitative, and the paper doesn't benchmark against established curriculum learning approaches.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary λ in SIFT selection (0.001, 0.01, 0.1, 1.0, 10.0) and clip_high in GRPO (0.2, 0.25, 0.28, 0.35) across multiple benchmarks to identify stable operating regions and quantify performance sensitivity.

2. **Transfer learning validation**: Train TTC-RL models on one task type (e.g., math) and evaluate on completely different domains (e.g., code, scientific reasoning) to quantify whether the method learns transferable reasoning patterns or overfits to task-specific features.

3. **Curriculum diversity impact**: Measure task similarity within curated curricula using embedding-based metrics and correlate with performance gains. Test whether adding artificially diverse but irrelevant tasks improves or degrades performance, validating the diversity-relevance tradeoff claimed by SIFT.