---
ver: rpa2
title: 'Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration
  Selection in Text Classification'
arxiv_id: '2511.10675'
source_url: https://arxiv.org/abs/2511.10675
tags:
- label
- test
- performance
- demonstrations
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of selecting effective in-context
  demonstrations for text classification using large language models (LLMs). Most
  existing methods focus on semantic similarity but often neglect label distribution
  alignment, which can lead to poor performance, especially in ambiguous or noisy
  scenarios.
---

# Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification

## Quick Facts
- arXiv ID: 2511.10675
- Source URL: https://arxiv.org/abs/2511.10675
- Authors: Ye Jiang; Taihang Wang; Youzheng Liu; Yimin Wang; Yuhan Xia; Yunfei Long
- Reference count: 12
- Primary result: 2.11% average accuracy improvement over best prior method on 7 text classification benchmarks

## Executive Summary
The paper addresses the problem of selecting effective in-context demonstrations for text classification using large language models (LLMs). Most existing methods focus on semantic similarity but often neglect label distribution alignment, which can lead to poor performance, especially in ambiguous or noisy scenarios. The authors propose TopK + Label Distribution Divergence (L2D), a two-stage demonstration selection method that first retrieves semantically similar demonstrations using a retrieval model, then fine-tunes a small language model (SLM) to estimate label probability distributions and calculate their divergence to select demonstrations that are both semantically similar and label-aligned.

## Method Summary
The proposed TopK + L2D method consists of two stages: First, it retrieves the TopK semantically similar demonstrations using a dense retrieval model (gte-base-en-v1.5) with cosine similarity scoring. Second, it fine-tunes a BERT-like SLM on the training data to generate label probability distributions for both the test input and candidate demonstrations. The method calculates label distribution divergence using Jensen-Shannon divergence to select demonstrations that are both semantically similar and label-aligned. The final hybrid score combines semantic similarity and label distribution divergence with equal weighting (α=0.5) to re-rank and select the top-N demonstrations for in-context learning.

## Key Results
- L2D consistently outperforms existing baselines across 7 text classification benchmarks
- Achieves an average accuracy improvement of 2.11% over the best prior method
- Ablation studies show removing semantic scoring drops performance by 1.50% on average; removing L2D drops it by 2.30%
- Positive correlation observed between LLM performance and the accuracy of the underlying SLMs used for label distribution estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining semantic similarity with label distribution alignment yields better demonstration selection than either signal alone.
- Mechanism: TopK retrieves semantically similar candidates, then a fine-tuned SLM predicts label probability distributions for both test input and candidates. Jensen-Shannon divergence quantifies distributional alignment. A hybrid score (α=0.5) balances both signals for final re-ranking.
- Core assumption: Label distributions encode task-relevant signal that pure semantic embeddings miss, particularly when semantically similar texts have divergent labels or ambiguous semantics.
- Evidence anchors:
  - [abstract] "leveraging a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence"
  - [PAGE 7, Table 4] Ablation shows removing semantic scoring drops performance 1.50% on average; removing L2D drops it 2.30%—both components contribute.
  - [corpus] "Rethinking Label Consistency of In-Context Learning" explores label consistency from a transductive perspective, supporting label-aware selection as a research direction, but no direct corpus validation of L2D's specific mechanism.
- Break condition: If SLM label predictions are systematically wrong (e.g., fine-tuned on mislabeled data), divergence scores will mislead selection and degrade LLM performance.

### Mechanism 2
- Claim: A smaller, task-specialized model can provide useful guidance signals to a larger general-purpose LLM for demonstration selection.
- Mechanism: The SLM (RoBERTa-base or similar) is fine-tuned on the training set to produce calibrated label probability distributions. These distributions proxy how the LLM might interpret the label space, enabling distribution alignment without running the LLM itself on candidates.
- Core assumption: SLM predictive accuracy correlates positively with downstream LLM ICL performance—better SLM → better demonstration selection → better LLM outputs.
- Evidence anchors:
  - [abstract] "positive correlation between the performance of LLMs and the accuracy of the underlying SLMs"
  - [PAGE 6-7, Table 3] DeBERTa-v3-base (highest standalone accuracy among SLMs) yields best L2D results; gains over BERT-based L2D are +0.61% to +1.23% across tasks.
  - [corpus] "Small Models are Valuable Plug-ins for Large Language Models" (cited in references) supports SLM-as-plugin paradigm, though not directly evaluating L2D.
- Break condition: If the SLM and LLM have fundamentally different representations of the label space (e.g., different label granularity or task framing), distribution alignment may be misleading.

### Mechanism 3
- Claim: Explicit label distribution divergence filtering improves robustness to noisy or contradictory demonstrations.
- Mechanism: By scoring demonstrations on how closely their label distributions match the test input's predicted distribution, L2D down-weights candidates that are semantically proximate but label-inconsistent (e.g., sarcasm, adversarial examples).
- Core assumption: Semantically similar examples can carry contradictory label signals, and filtering by distribution alignment mitigates this.
- Evidence anchors:
  - [PAGE 1, Figure 1] Illustrates adversarial conjunction in test input; L2D captures this and aligns distributions.
  - [PAGE 5, Figures 3(a)-(c)] Under arbitrary and reversed label conditions, TopK+L2D consistently outperforms baselines, suggesting label-aware selection provides some robustness.
  - [corpus] "Exploring Imbalanced Annotations for Effective In-Context Learning" addresses label imbalance effects on ICL, supporting label-awareness as relevant, but does not directly test divergence-based filtering.
- Break condition: If label noise is systematic (e.g., class-flipped labels are consistent within the candidate pool), distribution alignment may reinforce rather than filter incorrect signal.

## Foundational Learning

- Concept: Jensen-Shannon Divergence (JSD)
  - Why needed here: JSD is the core metric for quantifying label distribution alignment. It is symmetric, bounded [0,1], and can be directly combined with normalized semantic similarity scores without additional scaling.
  - Quick check question: Can you explain why JSD is preferred over KL divergence when comparing distributions bidirectionally?

- Concept: In-Context Learning (ICL) Sensitivity
  - Why needed here: The paper's entire motivation rests on ICL's sensitivity to demonstration choice and ordering. Understanding this instability explains why retrieval and re-ranking matter.
  - Quick check question: What empirical finding from Min et al. (2022) about label correctness in demonstrations does this paper partially challenge?

- Concept: Dense Retrieval with Cosine Similarity
  - Why needed here: The TopK stage uses embedding-based cosine similarity to form the initial candidate pool before L2D re-ranking.
  - Quick check question: Given an embedding matrix for training samples, how would you efficiently retrieve the top-30 candidates for a test input?

## Architecture Onboarding

- Component map:
  1. Embedding Retriever (e.g., gte-base-en-v1.5) → computes semantic similarity, returns Top-K candidates (default K=30)
  2. Fine-tuned SLM (RoBERTa-base / DeBERTa-v3-base) → predicts label probability distributions for test input and each candidate
  3. Divergence Calculator → computes JSD between test input distribution and each candidate distribution
  4. Scoring & Re-ranker → combines S_text (semantic similarity) and S_label (1 - JSD) via α-weighted hybrid score
  5. Prompt Assembler → selects top-N demonstrations (default N=8) based on hybrid score, formats into ICL prompt
  6. Target LLM → receives prompt and produces classification output

- Critical path: SLM fine-tuning quality → label distribution calibration → JSD accuracy → re-ranking quality → LLM ICL performance. The SLM is the single point of failure; its predictions must be reasonably accurate.

- Design tradeoffs:
  - α=0.5 is empirically optimal on average, but may need tuning for datasets where semantic ambiguity is higher or lower.
  - K=30 balances candidate diversity and noise; larger K introduces semantically distant candidates that can degrade re-ranking.
  - SLM choice: DeBERTa-v3-base > RoBERTa-base > BERT-base in the paper's experiments, but larger SLMs add latency.

- Failure signatures:
  - SLM systematically misclassifies a class → all candidates from that class receive incorrect distributions → LLM performance drops on inputs from that class.
  - Candidate pool too small (K≈N) → L2D has insufficient re-ranking headroom; performance approaches vanilla TopK.
  - α=0 or α=1 (ablation extremes) → drops of 1.5%–2.3% average; if you see similar degradation, check if one signal is effectively disabled.

- First 3 experiments:
  1. Reproduce the TopK+L2D pipeline on SST-2 with Qwen2.5-7B-Instruct and RoBERTa-base. Verify that your implementation matches the paper's reported ~96.49% accuracy (Table 2). Log S_label distributions for a few test inputs to confirm JSD is computed correctly.
  2. Run the SLM comparison experiment (Table 3) with BERT-base, RoBERTa-base, and DeBERTa-v3-base on a single dataset (e.g., CR). Confirm the positive correlation between SLM standalone accuracy and LLM ICL accuracy.
  3. Ablate α by sweeping from 0.0 to 1.0 on Subj and SST-2. Plot accuracy vs. α and verify that the optimal range is roughly 0.4–0.6, matching Figure 5. If your peak is elsewhere, investigate dataset-specific label ambiguity.

## Open Questions the Paper Calls Out

- **Generalization to Generation Tasks**: The authors explicitly state future work will explore applicability to text generation tasks where fixed label distributions are not inherently available. The current method relies on discrete class probability distributions that don't directly transfer to open-ended generation.

- **Scaling with Larger LLMs**: The study is constrained to 2B-14B parameter models due to computational resources. It remains unclear if the guidance from a small language model remains beneficial as LLM capabilities scale, or if the observed positive correlation holds for much larger inference models.

- **Retriever Dependency**: The authors acknowledge that part of the strong performance may be attributed to the underlying TopK retrieval mechanism and call for more thorough evaluation of the retrieval component. The sensitivity of L2D re-ranking to variations in initial candidate pool quality remains unquantified.

## Limitations

- Limited evaluation of computational overhead for the two-stage selection process
- Exact prompt templates used for ICL are not specified in the paper
- SLM fine-tuning hyperparameters (learning rate, batch size, epochs) are not provided, making exact replication challenging

## Confidence

- High confidence: The mechanism combining semantic similarity with label distribution divergence improves performance; supported by ablation studies showing both components contribute significantly
- Medium confidence: The positive correlation between SLM accuracy and LLM ICL performance; while observed in experiments, the causal mechanism could involve confounding factors
- Medium confidence: Robustness to label noise and contradictions; demonstrated on controlled experiments but not extensively validated on real-world noisy datasets

## Next Checks

1. Implement the complete TopK+L2D pipeline on SST-2 with Qwen2.5-7B-Instruct and verify the reported ~96.49% accuracy matches your implementation
2. Run the SLM comparison experiment (BERT-base, RoBERTa-base, DeBERTa-v3-base) on a single dataset to confirm the positive correlation between SLM standalone accuracy and LLM ICL performance
3. Perform the α ablation sweep (0.0 to 1.0) on Subj and SST-2 datasets, plotting accuracy vs. α to verify the optimal range is 0.4-0.6 as claimed