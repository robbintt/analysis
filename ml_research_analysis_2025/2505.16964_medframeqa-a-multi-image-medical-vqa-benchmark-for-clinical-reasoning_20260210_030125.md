---
ver: rpa2
title: 'MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning'
arxiv_id: '2505.16964'
source_url: https://arxiv.org/abs/2505.16964
tags:
- medical
- reasoning
- image
- images
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEDFRAMEQA, the first benchmark explicitly
  designed for multi-image medical visual question answering (VQA). It focuses on
  clinically grounded reasoning across multiple images, as opposed to existing single-image
  benchmarks.
---

# MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning

## Quick Facts
- arXiv ID: 2505.16964
- Source URL: https://arxiv.org/abs/2505.16964
- Reference count: 40
- Primary result: First benchmark explicitly designed for multi-image medical visual question answering, revealing current MLLMs achieve mostly below 50% accuracy on cross-frame clinical reasoning.

## Executive Summary
This paper introduces MEDFRAMEQA, a novel benchmark designed to evaluate multi-image medical visual question answering (VQA) capabilities. Unlike existing single-image medical VQA datasets, MEDFRAMEQA requires genuine cross-frame reasoning across 2-5 images extracted from medical education videos. The benchmark contains 2,851 high-quality VQA pairs covering nine body systems and 43 organs, with each question demanding integration of information across multiple frames. Evaluation of 11 advanced MLLMs reveals that their accuracies are mostly below 50%, with significant variation across body systems, organs, and imaging modalities, indicating that current models struggle with the complex reasoning required for multi-image medical diagnosis.

## Method Summary
The benchmark is constructed using a scalable pipeline that extracts temporally coherent frames from medical education videos, aligns them with transcripts, and generates VQA items with explicit reasoning chains. The process involves video retrieval using 114 keyword pairs (modality + disease/finding), FFmpeg keyframe extraction with GPT-4o filtering based on quality, medical prominence, informativeness, and privacy criteria, Whisper transcription with temporal alignment using margin Δ, and GPT-4o caption refinement and semantic merging (max 5 frames). VQA generation employs GPT-4o to create multi-choice questions with reasoning chains, followed by difficulty filtering via model consensus and manual quality/privacy review.

## Key Results
- Current MLLMs achieve mostly below 50% accuracy on MEDFRAMEQA, demonstrating genuine difficulty with multi-image integration
- Reasoning models (o1, o3, Gemini-2.5-Flash) outperform non-reasoning models, with Gemini-2.5-Flash achieving 54.75% accuracy
- Performance varies significantly across body systems (e.g., 60.21% musculoskeletal vs. 48.61% urinary for Gemini-2.5-Flash)
- Error propagation from single-frame misinterpretation is a common failure mode, where early mistakes compound through sequential analysis

## Why This Works (Mechanism)

### Mechanism 1
Multi-image VQA questions constructed from temporally coherent video sequences require genuine cross-frame reasoning rather than independent image analysis. The pipeline extracts keyframes from continuous medical education videos, aligns them with narrated transcripts via temporal windows, then merges frames when captions describe the same clinical concept. Questions explicitly reference image order and demand synthesis across frames. Core assumption: Educational video sequences contain clinically coherent narratives where consecutive frames show related anatomical views, disease progression, or diagnostic comparisons.

### Mechanism 2
Reasoning models outperform non-reasoning models on multi-image medical VQA because explicit chain-of-thought enables better evidence aggregation across frames. Reasoning models generate intermediate reasoning steps before answering, allowing systematic comparison across images. Non-reasoning models treat images more independently, missing cross-image patterns. Core assumption: Multi-image reasoning benefits from decomposed, sequential analysis rather than holistic pattern matching.

### Mechanism 3
Error propagation from single-frame misinterpretation degrades multi-image reasoning, where early mistakes compound through sequential analysis. When models misinterpret an intermediate image, subsequent reasoning builds on flawed premises, producing systematically incorrect conclusions despite correct reasoning structure. Core assumption: Models do not effectively cross-validate findings across images to detect and self-correct early errors.

## Foundational Learning

- **Temporal frame alignment with soft matching**: Medical video narration often precedes or lags behind visual changes; rigid timestamp matching would produce misaligned frame-caption pairs. Quick check: Given a frame at [10s, 15s] and narration margin Δ=3s, which transcript segments would be considered for pairing?

- **Multi-image VQA vs. single-image VQA**: Single-image benchmarks (SLAKE, VQA-RAD) test recognition; multi-image benchmarks test integration, comparison, and temporal reasoning. Quick check: If a model achieves 70% on SLAKE but 45% on MedFrameQA, what capability gap does this reveal?

- **Consensus-based difficulty filtering**: Removing questions where all models agree (all correct or all identically wrong) eliminates trivial items and potential annotation errors. Quick check: Why filter questions where o1, GPT-4o, and GPT-4-Turbo-V all answer identically incorrectly?

## Architecture Onboarding

- **Component map**: Video Collection -> Frame Extraction -> Frame-Caption Pairing -> Multi-Frame Merging -> VQA Generation -> Filtering
- **Critical path**: Frame-caption pairing accuracy → Multi-frame merging coherence → Question quality and cross-image dependency. Errors in early stages compound through the pipeline.
- **Design tradeoffs**: Max 5 frames per question balances reasoning complexity against model context limits; automated pipeline enables scale (2,851 pairs) but limits full physician validation; educational videos provide coherent narratives but may differ from clinical imaging workflows.
- **Failure signatures**: Questions answerable from single image indicate failed cross-image integration requirement; models treating images independently (e.g., ignoring frames 2-4) suggest insufficient multi-image training; high variance across body systems indicates uneven modality/organ coverage in training data.
- **First 3 experiments**: 1) Baseline evaluation with reasoning prompt ablation to isolate reasoning effects; 2) Frame-count analysis grouping accuracy by 2, 3, 4, 5 frames per question; 3) Error taxonomy classifying 50 incorrect responses from o1 into single-frame misinterpretation, cross-frame neglect, reasoning chain error, or knowledge gap.

## Open Questions the Paper Calls Out

### Open Question 1
What specific training or architectural strategies can effectively enhance MLLMs' multi-image reasoning capabilities in clinical contexts? The limitations section states effective strategies to enhance their multi-image reasoning capabilities remain underexplored, identifying this as a primary focus for future work. Current state-of-the-art models fail to synthesize information across frames, resulting in accuracies mostly below 50%.

### Open Question 2
How does the complexity or redundancy of visual information across frames influence MLLM accuracy relative to the raw frame count? The authors note in Section 4.4 that accuracy fluctuates as frame counts increase and speculate that performance depends less on the number of frames than on the complexity or redundancy of visual information, but they do not quantify this relationship.

### Open Question 3
To what extent does full-scale physician evaluation validate the clinical correctness and coverage of the MedFrameQA dataset? The limitations section notes MEDFRAMEQA has not received full-scale physician evaluation and that broader expert assessment is needed to further verify clinical correctness and coverage.

## Limitations
- Automated pipeline lacks full physician validation of question clinical accuracy, raising concerns about potential propagation of medical misconceptions
- Benchmark relies on educational videos which may present idealized anatomical views differing from real-world clinical imaging variability
- Current error analysis is limited to case studies rather than systematic classification across all incorrect responses

## Confidence

**High Confidence**: Current MLLMs achieve mostly sub-50% accuracy on MedFrameQA, demonstrating genuine difficulty with multi-image integration.

**Medium Confidence**: Reasoning models outperform non-reasoning models specifically due to chain-of-thought enabling better cross-frame evidence aggregation.

**Low Confidence**: Error propagation from single-frame misinterpretation is the primary failure mode for multi-image reasoning.

## Next Checks

1. **Clinical accuracy validation**: Recruit 3-5 board-certified radiologists or clinicians to independently evaluate a random sample of 100 MedFrameQA questions for clinical correctness and educational value.

2. **Frame independence ablation**: Re-run the top 3 MLLMs with a modified evaluation where each image is presented individually (no cross-image references), then compare accuracy drops.

3. **Error propagation quantification**: Perform systematic error analysis on 200 incorrect responses from the best-performing model (o1), classifying each error by type and calculating the frequency of propagated versus isolated errors.