---
ver: rpa2
title: 'VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource
  Languages'
arxiv_id: '2510.12845'
source_url: https://arxiv.org/abs/2510.12845
tags:
- image
- text
- response
- example
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces VLURes, a multilingual benchmark designed
  to evaluate Vision Language Models (VLMs) across eight vision-and-language tasks,
  including a novel unrelatedness task, in English, Japanese, Swahili, and Urdu. By
  leveraging article-length texts and diverse image categories, VLURes addresses the
  gap in multilingual and low-resource language evaluation.
---

# VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages

## Quick Facts
- **arXiv ID**: 2510.12845
- **Source URL**: https://arxiv.org/abs/2510.12845
- **Reference count**: 40
- **Primary result**: Introduces VLURes benchmark evaluating VLMs across 8 tasks in 4 languages, showing GPT-4o achieves 90.8% accuracy, lagging human performance by 6.7%

## Executive Summary
This study introduces VLURes, a multilingual benchmark designed to evaluate Vision Language Models (VLMs) across eight vision-and-language tasks, including a novel unrelatedness task, in English, Japanese, Swahili, and Urdu. By leveraging article-length texts and diverse image categories, VLURes addresses the gap in multilingual and low-resource language evaluation. Results show that the best-performing model, GPT-4o, achieves 90.8% accuracy overall, lagging human performance by 6.7%, while open-source models perform significantly worse. The benchmark highlights the need for improved multilingual VLMs and demonstrates its utility in advancing multi-modal visual reasoning.

## Method Summary
VLURes evaluates VLMs on 8 vision-language tasks using article-length texts (vs. short captions) across 4 languages. The benchmark uses CLIP to align images with text, then employs zero-shot and one-shot inference with/without rationales. An LLM-judge (Gemini 1.5 Pro) scores outputs (0-100) based on accuracy, helpfulness, and linguistic quality. Open-source models were fine-tuned using LoRA. The dataset comprises ~1,000 image-text pairs per language from web resources.

## Key Results
- GPT-4o achieves 90.8% overall accuracy, lagging human performance by 6.7%
- Open-source models score significantly lower, often 0.0 on low-resource languages
- Cross-lingual performance gaps persist even for strong models like GPT-4o
- Fine-tuning improves performance on English/Japanese but remains infeasible for Swahili/Urdu

## Why This Works (Mechanism)

### Mechanism 1: Long-Context Density Probe
Using article-length prose instead of short captions increases task difficulty by forcing the model to locate relevant information within a high-noise, high-volume context, thereby revealing "grounding" failures. Information Density Load → Forces Attention Filtering → Exposes Hallucination or Retrieval Failure.

### Mechanism 2: Negative Retrieval (Unrelatedness Task)
Explicitly requiring the model to identify text segments not present in the image functions as a negative constraint that tests the precision of cross-modal alignment. Visual Feature Mapping → Cross-Modal Rejection → Identification of Non-Aligned Text Segments.

### Mechanism 3: VLM-as-a-Judge Alignment
A high-capability VLM (Gemini 1.5 Pro) can approximate human grading for generative tasks in low-resource languages where automated metrics fail to capture semantic nuance. Semantic Similarity Assessment → Rationale Verification → Numerical Scoring (0-100).

## Foundational Learning

**Cross-Modal Alignment (CLIP)**: Used to select best image-text pairs from raw web data. Quick check: How does the cosine similarity score (e.g., 0.15 threshold) determine if an image is relevant to an article paragraph?

**Chain-of-Thought (Rationales)**: Evaluating performance "With Rationales" vs "Without Rationales" shows that forcing the model to explain its reasoning improves accuracy. Quick check: Why does generating a rationale before the final answer statistically improve task performance in VQA tasks?

**Language Bias in Tokenization**: Open-source models failed significantly on Swahili/Urdu (0.0 scores), likely due to tokenization or pre-training distribution skew. Quick check: Does a model's tokenizer split low-resource language words into meaningless byte-pairs, potentially degrading the semantic signal?

## Architecture Onboarding

**Component map**: Input (Image + Article-Length Text) → Model (VLM processing visual + text tokens) → Output (Task Response + Optional Rationale) → Evaluator (Gemini 1.5 Pro scoring 0-100)

**Critical path**: 1. Data Prep: Retrieve article-length text and select most CLIP-aligned image 2. Inference: Feed prompt + Image + Text into VLM 3. Evaluation: Feed VLM's output + Ground Truth + Image/Text into LLM-Judge for score

**Design tradeoffs**: Proprietary vs Open-Source (Access vs Performance), Human vs Auto Eval (Scaling vs Nuance), Context Length (Richness vs Token Costs)

**Failure signatures**: Language Leakage (GPT-4o-mini responding in English when prompted in Urdu), Catastrophic Unintelligibility (Open-source models generating "0.0" accuracy), Hallucination (Inventing places in VQA tasks)

**First 3 experiments**:
1. Zero-Shot Baseline: Evaluate LLaVA on "Unrelatedness" task using English vs. Urdu inputs
2. Rationalization Ablation: Run VQA task with/without "Provide your rationale" instruction
3. Judge Alignment Check: Compare human scores vs. Gemini 1.5 Pro scores on 10 sample responses

## Open Questions the Paper Calls Out

**Open Question 1**: What model or training improvements are required to close the 6.7% accuracy gap between the best VLM (GPT-4o) and human performance on the VLURes benchmark?

**Open Question 2**: How can VLMs be effectively improved to reduce persistent cross-lingual performance disparities, particularly for low-resource languages like Swahili and Urdu?

**Open Question 3**: What are the broader limitations and failure modes of the novel "unrelatedness" task in more complex or adversarial real-world scenarios?

**Open Question 4**: Can a fine-tuning strategy be developed that is effective and scalable for improving VLM performance across all VLURes languages, including Swahili and Urdu?

## Limitations
- Dataset transparency: Exact URLs and article IDs not provided, making exact replication impossible
- Evaluation reliability: LLM-judge agreement (82.3%) based on limited sampling across only 2 tasks
- Open-source model performance: 0.0 accuracy scores may reflect evaluation artifacts rather than true failures

## Confidence
**High Confidence**: GPT-4o significantly outperforms open-source models on multilingual VLM tasks (90.8% vs 30-40% accuracy)
**Medium Confidence**: Article-length texts provide more challenging evaluation than short captions, though difficulty differential not quantified
**Low Confidence**: 82.3% human-LLM judge agreement figure based on limited sampling (200 examples across 2 tasks)

## Next Checks
1. Reproduce the Judge Agreement: Select 50 random samples from VQA and Scene Understanding tasks, obtain human scores using same criteria, and compare to Gemini 1.5 Pro scores
2. Cross-Lingual Performance Gap Analysis: Run same VLM (e.g., LLaVA-1.5) on identical image-text pairs translated between all four languages
3. Text Length Ablation Study: Create parallel benchmark versions using progressively shorter text excerpts to quantify how text length impacts VLM performance