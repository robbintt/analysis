---
ver: rpa2
title: 'MC#: Mixture Compressor for Mixture-of-Experts Large Models'
arxiv_id: '2510.10962'
source_url: https://arxiv.org/abs/2510.10962
tags:
- experts
- quantization
- expert
- arxiv
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational and memory overhead
  in Mixture-of-Experts (MoE) large language and vision-language models due to preloading
  all experts into memory and activating multiple experts per input. The authors propose
  MC, a unified framework that combines static quantization and dynamic expert pruning
  to achieve aggressive compression of MoE-LLMs/VLMs.
---

# MC#: Mixture Compressor for Mixture-of-Experts Large Models

## Quick Facts
- **arXiv ID**: 2510.10962
- **Source URL**: https://arxiv.org/abs/2510.10962
- **Reference count**: 40
- **Primary result**: Achieves 6.2× weight reduction at 2.57 bits with only 1.7% accuracy drop across five multimodal benchmarks.

## Executive Summary
This paper introduces MC#, a unified framework for compressing Mixture-of-Experts (MoE) large language and vision-language models. The core innovation combines static mixed-precision quantization (PMQ) with dynamic expert pruning (OTP) to address the high computational and memory overhead of MoE models. PMQ assigns different bit-widths to experts based on their importance, while OTP uses a learnable router with Gumbel-Softmax sampling to dynamically prune redundant experts during inference. The framework achieves aggressive compression (up to 6.2×) with minimal performance degradation across multiple benchmarks.

## Method Summary
MC# is a two-stage compression framework for MoE models. The first stage, Pre-Loading Mixed-Precision Quantization (PMQ), optimizes bit-width allocation per expert using an Integer Programming model that balances activation frequency, weight scores, and quantization error. The second stage, Online Top-any Pruning (OTP), trains a lightweight router network to dynamically select which experts to activate per token using Gumbel-Softmax sampling. The framework decouples storage efficiency (PMQ) from runtime efficiency (OTP), achieving multiplicative reductions in memory footprint and computational cost. Calibration uses 128 sequences from C4 (LLMs) or M4 (VLMs), with OTP training on 4096 samples.

## Key Results
- Achieves 6.2× weight reduction at average 2.57 bits with only 1.7% accuracy drop across five multimodal benchmarks
- OTP reduces expert activation by 20% with less than 1% performance degradation on DeepSeek-VL2
- Combined PMQ+OTP yields up to 1.92× speedup and lower activated parameters than either method alone
- Outperforms uniform quantization baselines by significant margins at ultra-low bit-widths (2.5 bits)

## Why This Works (Mechanism)

### Mechanism 1
Allocating quantization bit-widths based on expert importance preserves accuracy better than uniform precision at extreme compression rates. PMQ uses an Integer Programming model that minimizes a weighted objective combining expert activation frequency, weight scores, and Frobenius norm error to solve for optimal bit-width distribution (1-3 bits) per expert. This works because experts in MoE models are imbalanced, with certain experts being significantly more critical or sensitive to quantization noise than others.

### Mechanism 2
A lightweight learnable router can dynamically prune redundant experts per token using differentiable sampling. OTP uses Gumbel-Softmax sampling to approximate discrete mask selection, allowing gradients to flow through the sampling process. The router network predicts mask logits for top-k experts, and the Gumbel trick enables optimizing a loss function combining distillation and sparsity. This addresses the redundancy in standard Top-K routing where all experts must be pre-loaded but only few are used.

### Mechanism 3
Static weight compression and dynamic activation pruning are orthogonal and multiplicative. PMQ reduces static memory load (bytes per weight), allowing large MoE models to fit on smaller GPUs. OTP reduces the number of active experts per step (FLOPs), increasing token throughput. The framework assumes quantization noise from PMQ doesn't destroy routing signals required for OTP, and sparsity from OTP doesn't exacerbate quantization errors catastrophically.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Routing**
  - Why needed here: The paper targets redundancy in standard Top-K routing where all experts must be pre-loaded but only few are used
  - Quick check question: In a standard MoE layer with Top-2 routing, do the unselected experts still consume memory during inference?

- **Concept: Post-Training Quantization (PTQ)**
  - Why needed here: The core compression technique relies on quantizing weights without retraining the full model using GPTQ
  - Quick check question: How does a "mixed-precision" scheme differ from "uniform" quantization in terms of storage requirements for model weights?

- **Concept: Gumbel-Softmax Reparameterization**
  - Why needed here: Essential for understanding how OTP makes discrete expert pruning decisions differentiable for training
  - Quick check question: Why is the standard `argmax` function problematic for backpropagation, and how does adding Gumbel noise solve this?

## Architecture Onboarding

- **Component map**: Calibration Phase: Input Data -> Expert Importance Analyzer (Frequency/Weight/Error) -> Integer Programming Solver -> Bit-width Config. Quantization Phase: Pre-trained MoE -> GPTQ Quantizer (using Bit-width Config) -> Quantized MoE. Pruning Training Phase: Quantized MoE -> Learnable Router -> Gumbel-Softmax Sampler -> Mask -> Sparse Expert Output -> Loss (Distill + Sparsity). Inference Phase: Input Token -> Router -> Hard Mask (Top-any) -> Selected Quantized Experts -> Output.

- **Critical path**: The bit-width allocation in the static phase determines the baseline performance. The most critical path for inference speed is the Learnable Router in the dynamic phase; if it fails to learn meaningful masks, no runtime speedup is achieved.

- **Design tradeoffs**:
  - Bit-width vs. Accuracy (PMQ): Lower average bits drastically reduce memory but cause significant accuracy drops
  - Sparsity (λ) vs. Performance (OTP): Increasing sparsity regularization forces higher pruning ratios but risks higher accuracy loss
  - Router Capacity: The router is tiny (2 linear layers), adding minimal overhead but limits complexity of pruning policy it can learn

- **Failure signatures**:
  - Uniform Quantization Collapse: At <2.5 bits, uniform quantization causes performance to drop to near-random
  - Identity Masking: If distillation loss is weighted too heavily against sparsity, OTP will learn to mask nothing
  - Activation Imbalance: If calibration dataset doesn't cover evaluation distribution, PMQ may assign low bits to experts that turn out to be critical

- **First 3 experiments**:
  1. Run PMQ on Mixtral-8x7b with target bits ranging from 2.5 to 1.5. Plot Perplexity vs. Model Size to verify Pareto optimality over uniform/Hessian baselines
  2. Train OTP router on DeepSeek-VL2-S with varying λ (e.g., 0.5, 1.0, 2.0). Monitor "Average Mask Ratio" curve to confirm router is learning to prune
  3. Isolate IP solver inputs. Run PMQ using only activation frequency vs. only quantization error to determine which signal contributes most to preserving accuracy at low bits

## Open Questions the Paper Calls Out

- Can MC# maintain performance on complex reasoning tasks like GSM8K and HumanEval when compressed to ultra-low bit-widths (below 2 bits)? The paper identifies accuracy degradation on challenging benchmarks but doesn't propose mechanisms to preserve reasoning capabilities under extreme compression.

- How does the choice of temperature scheduling for Gumbel-Softmax sampling affect OTP pruning stability and final performance? The paper uses Gumbel-Softmax with temperature τ but doesn't explore adaptive or learned temperature schedules during training.

- Does MC# generalize to MoE architectures with different routing mechanisms beyond the top-k gating used in experiments? The framework is evaluated only on models with standard top-k routing, leaving generalization to other routing mechanisms unexamined.

## Limitations
- Hyperparameters α, β, γ for importance weighting formula are unspecified, leaving ambiguity about the precise balance between frequency, weight, and error
- OTP router training details are sparse: learning rate, optimizer, batch size, and temperature schedule are not provided, which could significantly impact pruning effectiveness
- No ablation studies are presented showing whether activation frequency or quantization error is the dominant signal for PMQ

## Confidence
- **High confidence**: The core PMQ-IP framework (importance-aware mixed precision) is well-grounded in prior work and the IP formulation is clearly specified
- **Medium confidence**: OTP's theoretical soundness is solid, but practical effectiveness depends on unreported training details and hyperparameter tuning
- **Low confidence**: Claims of multiplicative memory/runtime savings from combining PMQ and OTP are plausible but not thoroughly isolated from other factors like hardware-specific quantization artifacts

## Next Checks
1. **Pareto Frontier Validation**: Run PMQ on Mixtral-8x7b across 2.5-1.5 bits; plot perplexity vs. model size to verify claimed superiority over uniform/Hessian baselines
2. **OTP Convergence Analysis**: Train OTP router on DeepSeek-VL2-S with varying λ; monitor mask sparsity over training to confirm actual learning vs. underfitting
3. **Importance Metric Ablation**: Isolate IP solver inputs; run PMQ using only frequency vs. only error to determine which signal contributes most to accuracy at low bits