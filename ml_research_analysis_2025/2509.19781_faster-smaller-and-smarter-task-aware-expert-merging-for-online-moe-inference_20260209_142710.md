---
ver: rpa2
title: 'Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference'
arxiv_id: '2509.19781'
source_url: https://arxiv.org/abs/2509.19781
tags:
- merging
- inference
- tanbr
- expert
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficient online inference for
  Mixture-of-Experts (MoE) models, specifically addressing the challenges of expert
  routing when explicit task tags are unavailable. The proposed solution, Tanbr, is
  a tree-structured adaptive neural bandit router that dynamically guides task-aware
  expert merging.
---

# Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference

## Quick Facts
- arXiv ID: 2509.19781
- Source URL: https://arxiv.org/abs/2509.19781
- Reference count: 40
- Reduces inference latency by 45% and memory usage by 25% while maintaining accuracy

## Executive Summary
This paper introduces Tanbr, a tree-structured adaptive neural bandit router for efficient online inference in Mixture-of-Experts (MoE) models. The key innovation addresses the challenge of expert routing when explicit task tags are unavailable by dynamically guiding task-aware expert merging. Instead of routing individual tokens, Tanbr estimates task distributions over time and uses them to inform expert merging decisions within a pre-trained MoE. The system employs a binary tree to progressively partition the continuous space of merging weights and a neural bandit to learn the non-linear mapping from weights to model performance, achieving a sublinear regret bound of O(√T log(T)).

## Method Summary
Tanbr implements a tree-structured adaptive neural bandit router that merges expert weights based on estimated task distributions. The system consists of a monitor module that estimates task distribution vectors using lightweight tools like Count-Min Sketch, a binary partition tree that discretizes the continuous weight space into candidate regions, and a neural bandit that learns the non-linear reward mapping using Upper Confidence Bound exploration. The method fine-tunes T5 (normalization layers only) or learns BERT online, and selects merging weights that maximize performance while satisfying constraints on the number of active experts and weight sum.

## Key Results
- Reduces inference latency by at least 45% compared to state-of-the-art methods
- Reduces memory usage by up to 25% while maintaining high accuracy
- Achieves sublinear regret bound of O(√T log(T)) over T rounds
- Maintains competitive accuracy across GLUE benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1: Task-Distribution-Driven Expert Merging
The system replaces discrete expert routing with a single merged model weighted by historical task distributions, reducing inference latency and memory footprint without requiring explicit task tags. It computes weighted averages of expert parameters once per time slot based on estimated task distributions, creating a "generalist" expert optimized for current workload.

### Mechanism 2: Tree-Structured Discretization of Continuous Weights
A binary partition tree efficiently approximates optimal merging weights in continuous, multi-dimensional space where standard bandit algorithms fail. The tree starts with coarse regions and progressively refines the search by expanding nodes only when visitation counts exceed thresholds, focusing computational budget on promising weight regions.

### Mechanism 3: Neural Upper Confidence Bound for Non-Linear Rewards
A neural bandit with UCB exploration effectively learns the complex, non-linear mapping between merging weights and model performance. The neural network predicts reward values and selects weights by maximizing Upper Confidence Bound (predicted reward + uncertainty), explicitly balancing exploitation and exploration to achieve sublinear regret.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) Routing**
  - Why needed: Understanding token-level vs task-level routing distinction is essential to grasp why Tanbr merges weights
  - Quick check: Does the system route every token to a different expert, or does it blend experts into a unified model for a duration?

- **Concept: Multi-Armed Bandits (MAB) & Regret**
  - Why needed: The router's core logic is a bandit problem; understanding "regret" (loss vs optimal oracle) is crucial for evaluating learning efficiency
  - Quick check: If the router picks a suboptimal weight, does it update immediately, and does theory guarantee finding optimal weight eventually?

- **Concept: Neural Tangent Kernel (NTK)**
  - Why needed: Theoretical convergence guarantee relies on NTK theory, which assumes wide neural networks behave like kernel methods
  - Quick check: Why does the paper require network width w to be large (poly of T, L)? (Answer: To ensure NTK approximation holds)

## Architecture Onboarding

- **Component map**: Monitor -> Partition Tree -> Neural Bandit -> Merger -> MoE Layer
- **Critical path**: Monitor reads buffer → generates ψ_t → Partition Tree generates feasible weights → Neural Bandit computes UCB → selects x_h,t → weights applied → inference runs → reward observed → Tree expands if threshold met → Network updates via SGD
- **Design tradeoffs**: Smoothness ρ vs speed (low ρ expands slowly but may miss optima; high ρ expands fast but risks over-partitioning); Time slot duration (longer slots reduce overhead but risk stale weights); Fine-tuning requirement (T5 needs normalization fine-tuning, BERT does not)
- **Failure signatures**: Runaway memory (tree expands too deep without pruning); Accuracy collapse (router converges to single "average" weight); Constraint violation (generated weights fail sum or active expert constraints)
- **First 3 experiments**: 1) Static sanity check on fixed task distribution to validate router convergence; 2) Dynamic switching to measure adaptation latency and regret spikes; 3) Ablation on ρ parameter to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out
- Future work could explore grouping similar experts for merging using advanced clustering techniques to dynamically form expert groups
- The effectiveness of the regret bound and convergence rate when the task distribution monitor provides noisy or sparse estimates
- Whether the smoothness parameter ρ can be adapted online to balance exploration and exploitation more effectively

## Limitations
- Performance depends heavily on expert initialization quality (independently trained experts vs same fine-tuned model)
- Task distribution estimation mechanism (Count-Min Sketch) implementation details significantly impact performance
- Theoretical regret bound assumes ideal conditions that may not hold in practice

## Confidence
- High: Core mechanism of binary tree discretization for bandit optimization, general approach of task-distribution-driven merging, neural bandit architecture with UCB
- Medium: Specific performance gains (45% latency, 25% memory), sublinear regret bound O(√T log(T))
- Low: Absolute accuracy maintenance across all GLUE tasks, effectiveness in highly dynamic task environments

## Next Checks
1. **Initialization Sensitivity Test**: Reproduce using three different expert initialization strategies (independently trained, same fine-tuned model, randomly initialized) to measure impact on merging performance and accuracy retention

2. **Non-Stationary Environment Stress Test**: Create synthetic workload with abrupt task distribution shifts every few minutes to measure adaptation latency, regret spikes, and accuracy recovery time compared to static GLUE benchmark results

3. **Theoretical Bound Validation**: Implement algorithm with extensive logging of visited nodes, weight selections, and rewards; track cumulative regret over time and compare against O(√T log(T)) bound to identify practical vs theoretical gaps