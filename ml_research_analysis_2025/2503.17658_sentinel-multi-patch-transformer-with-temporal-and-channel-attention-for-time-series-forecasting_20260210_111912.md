---
ver: rpa2
title: 'Sentinel: Multi-Patch Transformer with Temporal and Channel Attention for
  Time Series Forecasting'
arxiv_id: '2503.17658'
source_url: https://arxiv.org/abs/2503.17658
tags:
- attention
- forecasting
- time
- series
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Sentinel, a full transformer-based architecture
  for multivariate time series forecasting that addresses the limitation of existing
  transformer models by simultaneously capturing both cross-channel and temporal dependencies.
  The key innovation is a multi-patch attention mechanism that leverages the patching
  process to structure the input sequence, replacing the traditional multi-head attention
  splitting.
---

# Sentinel: Multi-Patch Transformer with Temporal and Channel Attention for Time Series Forecasting

## Quick Facts
- arXiv ID: 2503.17658
- Source URL: https://arxiv.org/abs/2503.17658
- Authors: Davide Villaboni; Alberto Castellini; Ivan Luciano Danesi; Alessandro Farinelli
- Reference count: 4
- Key outcome: Sentinel achieves better or comparable performance with respect to state-of-the-art approaches on standard benchmarks, demonstrating superior forecasting accuracy

## Executive Summary
This paper proposes Sentinel, a transformer-based architecture for multivariate time series forecasting that simultaneously captures cross-channel and temporal dependencies through a multi-patch attention mechanism. The key innovation is replacing traditional multi-head attention with multi-patch attention that leverages the natural structure created by patching. Sentinel uses an encoder to model inter-channel dependencies and a decoder to capture temporal dependencies, with cross-attention integrating both context types. Extensive experiments on standard benchmarks show that Sentinel achieves better or comparable performance with respect to state-of-the-art approaches.

## Method Summary
Sentinel transforms multivariate time series into patches (P=16, S=8), embedding each into d_model dimensions. The encoder reshapes patches to (N×C×d_model) and applies multi-patch attention across channels to learn cross-channel relationships, producing a global context per channel. The decoder operates on (C×N×d_model), applying causal self-attention along the temporal dimension, then cross-attention with encoder output to integrate channel context. A final linear projection reconstructs predictions. The model is trained with AdamW (lr=0.0005, dropout=0.3), L1 loss, and hyperparameters Nenc, Ndec, dmodel searched over specified ranges.

## Key Results
- Sentinel achieves better or comparable performance with respect to state-of-the-art approaches on standard benchmarks
- The model demonstrates superior forecasting accuracy across multiple datasets
- Ablation study confirms the importance of the proposed components, particularly for datasets with a large number of features
- Sentinel shows consistent performance improvement as the lookback window increases

## Why This Works (Mechanism)

### Mechanism 1
Replacing multi-head attention with multi-patch attention leverages the natural structure created by patching, improving forecasting performance. Instead of splitting the input into h attention heads with separate projection matrices, the model applies attention directly to each of the N patches using shared projection matrices across all patches. This shifts the computational structure from "head" to "patch" granularity. If patches contain highly heterogeneous temporal patterns that would benefit from diverse projection subspaces, shared matrices may underfit compared to multi-head.

### Mechanism 2
Specializing encoder for channel attention and decoder for temporal attention enables simultaneous modeling of cross-channel and temporal dependencies. The encoder reshapes input to X_enc ∈ R^{N×C×d_model} and computes attention across the C channels, learning how each channel relates to others within each patch. The decoder operates on X_dec ∈ R^{C×N×d_model} and computes attention along the temporal dimension, capturing causal relationships across time. If cross-channel and temporal dependencies are tightly coupled (e.g., channels with different temporal lag structures), separate processing may miss joint patterns.

### Mechanism 3
Cross-attention between temporal queries (decoder) and channel context (encoder) integrates both dependency types for improved forecasting. The decoder's cross-attention uses the encoder output O_enc as key and value, while the query comes from the decoder's self-attention output. This merges channel-wise relationships (from encoder) with temporal patterns (from decoder) in the final prediction. If encoder channel context is noisy or irrelevant for certain prediction horizons, cross-attention may introduce distractor signals.

## Foundational Learning

- **Concept: Patching in time series transformers**
  - Why needed here: The entire architecture builds on the patch structure; understanding how P (patch size) and S (stride) affect sequence length N = ⌊(L-P)/S⌋ + 1 is essential.
  - Quick check question: Given lookback L=96, patch size P=16, stride S=8, how many patches N are created? (Answer: 11)

- **Concept: Multi-head vs. single-head attention with shared projections**
  - Why needed here: Multi-patch attention diverges from standard multi-head; understanding the difference clarifies what is being replaced and why.
  - Quick check question: In standard multi-head attention with d_model=512 and h=8 heads, what is d_h per head? (Answer: 64)

- **Concept: Encoder-decoder cross-attention**
  - Why needed here: The integration point between channel and temporal processing relies on cross-attention mechanics.
  - Quick check question: In cross-attention, which component provides Q and which provides K,V? (Answer: Decoder self-attention output provides Q; encoder output provides K,V)

## Architecture Onboarding

- **Component map**: Input (L×C) → RevIN → Patching (C×N×P) → MLP Embedding (C×N×d_model) → Encoder (N×C×d_model) → Decoder (C×N×d_model) → Projection → RevIN reverse → Output (T×C)

- **Critical path**: The encoder output O_enc flows into decoder cross-attention K,V. If encoder produces poor channel context, decoder predictions degrade. Ablation study shows encoder removal causes -8.0% to -9.7% MSE degradation on high-channel datasets (Electricity, Weather).

- **Design tradeoffs**: N_enc vs. N_dec layers: Ablation suggests encoder is more critical for high-channel datasets; decoder more critical for low-channel datasets. Patch size P=16, stride S=8 are fixed in experiments; larger P reduces N (computational savings) but may lose fine-grained patterns. Shared vs. per-patch projections: Shared projections simplify model but may limit expressiveness for heterogeneous patches.

- **Failure signatures**: Overfitting on low-channel datasets (mentioned in conclusion regarding encoder). Performance degradation if lookback window is too short (model benefits from longer context). Removing decoder causes larger degradation on ETTh2 (low-channel) than removing encoder.

- **First 3 experiments**: 
  1. Reproduce ablation on a high-channel dataset (Electricity or Weather): Compare full model vs. encoder-removed vs. decoder-removed to verify channel-dependency impact
  2. Vary lookback window: Test L ∈ {96, 336, 720} to confirm the paper's claim that longer lookback improves performance
  3. Patch size sensitivity: Test P ∈ {8, 16, 32} with fixed stride S=P/2 to understand trade-off between computational cost and accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the Sentinel encoder be refined to mitigate overfitting in datasets with a low number of channels or features? The authors state in the conclusion: "As highlighted in the ablation study, the encoder shows a strong impact when dealing with datasets with a high number of features but it tends to overfit when the number of features is lower. To address this lack, we aim to develop techniques to mitigate overfitting in such cases..." This remains unresolved because the current architecture demonstrates superior performance on high-dimensional datasets but the ablation study indicates that the channel-encoder may be overly complex or unnecessary for datasets like ETTh2.

- **Open Question 2**: How can the decoder's architecture be improved to increase its impact in high-feature scenarios? The conclusion notes: "Conversely, the decoder has less impact as the number of features increases. We aim to improve its contribution in such high-feature scenarios." The current design focuses the decoder on temporal dependencies, but the ablation study implies its contribution is diminished relative to the encoder when the channel count is high.

- **Open Question 3**: What is the efficacy of Sentinel in few-shot learning contexts for time series forecasting? The authors list this as a future direction: "Another promising direction is to evaluate the few-shot learning capabilities of Sentinel, which could expand its applicability to a wider range of forecasting tasks." While the paper compares Sentinel against foundation models like Lag-llama and Time-LLM in the related work, the proposed model is currently evaluated only under standard supervised learning settings with ample data.

## Limitations

- **Hyperparameter selection ambiguity**: The paper provides search ranges for encoder/decoder depth and model dimension but does not report which specific configuration was selected for each dataset, creating uncertainty in reproduction.
- **Patch size sensitivity**: Only P=16, S=8 are tested, though the paper acknowledges that patch size affects performance. The trade-off between computational efficiency and accuracy for different patch sizes remains unexplored.
- **Encoder overfitting concern**: The paper mentions the encoder may overfit on low-channel datasets but doesn't provide detailed analysis of this failure mode or mitigation strategies.

## Confidence

- **High confidence**: The core mechanism of multi-patch attention replacing multi-head attention is well-specified and experimentally validated through ablation studies.
- **Medium confidence**: The claim that cross-attention integrates channel and temporal context effectively is supported by ablation results, but the paper lacks analysis of how this integration behaves under different dependency structures.
- **Low confidence**: The paper's assertion that longer lookback windows consistently improve performance is based on a single experiment without exploring the limits of this relationship or potential diminishing returns.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Run ablation studies varying Nenc, Ndec, and dmodel on high-channel datasets (Electricity, Weather) to determine which configurations drive performance differences and validate the importance ranking of components.
2. **Lookback window scalability**: Systematically test lookback windows L ∈ {96, 336, 720, 1440} to characterize the relationship between historical context and forecasting accuracy, identifying potential saturation points.
3. **Patch size and stride optimization**: Evaluate P ∈ {8, 16, 32} with corresponding S ∈ {4, 8, 16} to quantify the computational-accuracy trade-off and determine optimal patch configurations for different dataset characteristics.