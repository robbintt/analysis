---
ver: rpa2
title: 'HueManity: Probing Fine-Grained Visual Perception in MLLMs'
arxiv_id: '2506.03194'
source_url: https://arxiv.org/abs/2506.03194
tags:
- visual
- mllms
- color
- task
- huemanity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HueManity, a large-scale benchmark (83,850
  images) designed to evaluate fine-grained visual perception in multimodal large
  language models (MLLMs). The benchmark uses Ishihara-style dot patterns to embed
  alphanumeric characters, testing models' ability to recognize patterns from subtle
  visual cues.
---

# HueManity: Probing Fine-Grained Visual Perception in MLLMs

## Quick Facts
- arXiv ID: 2506.03194
- Source URL: https://arxiv.org/abs/2506.03194
- Reference count: 40
- Key outcome: MLLMs fail dramatically on fine-grained perceptual tasks, scoring only 33.6% on numeric recognition and 3% on alphanumeric recognition compared to near-human performance (99.38%, 93.25%)

## Executive Summary
This paper introduces HueManity, a benchmark using Ishihara-style dot patterns to embed alphanumeric characters, revealing a critical weakness in MLLMs' fine-grained visual perception. While current MLLMs excel at high-level semantic tasks, they struggle with subtle visual cues that humans and traditional CNNs handle easily. The benchmark exposes that standard vision encoders in MLLMs discard fine-grained details during semantic optimization, creating a perceptual bottleneck that cannot be resolved through simple fine-tuning.

## Method Summary
The benchmark generates 83,850 synthetic Ishihara-style images with embedded 2-character alphanumeric strings using 25 curated foreground-background color pairs with CIEDE2000 ΔE values between 25-75. Images are 900x900 pixels created through a process involving binary text masks, color selection, and circle rendering over 30,000 iterations with random shifts. Two tasks are evaluated: Number Recognition (90 outputs) and Alphanumeric Recognition (3,364 outputs), each with 1,000 evaluation images. Models are assessed via exact string match accuracy using case-sensitive normalization of responses.

## Key Results
- Top MLLM (GPT-4.1) achieved only 33.6% accuracy on numeric tasks and 3% on alphanumeric tasks
- Human subjects achieved 99.38% and 93.25% accuracy respectively on the same tasks
- A fine-tuned ResNet-50 baseline achieved 96.5% and 94.5% accuracy, significantly outperforming all tested MLLMs
- Fine-tuning MLLMs on Ishihara patterns caused catastrophic forgetting, with mask accuracy dropping from 80% to 0%

## Why This Works (Mechanism)

### Mechanism 1: Semantic Optimization Over Perceptual Fidelity
MLLM vision encoders are optimized for high-level semantic alignment during pre-training, which acts as a filter discarding high-frequency perceptual details. This focus on global context causes the loss of fine-grained local details necessary for figure-ground segregation in Ishihara patterns.

### Mechanism 2: Architectural Bottlenecks in Patchification
Vision Transformers divide images into fixed patches (e.g., 14x14 pixels) that can slice through critical high-frequency signals. Patch boundaries may average subtle color contrasts into background noise before the LLM processes them, creating an information bottleneck.

### Mechanism 3: Failure of Perceptual Grounding via Fine-Tuning
Standard fine-tuning cannot fix this perceptual deficit because models lack the necessary visual primitives to bootstrap learning figure-ground segregation from limited data. Instead, they learn to output plausible strings without actually solving the visual task.

## Foundational Learning

- **Figure-Ground Segregation**: The core cognitive faculty being tested—separating a coherent form from a distracting visual field based on color difference. Quick check: Why does a fine-tuned ResNet-50 succeed where SOTA MLLMs fail? (Answer: CNNs preserve local spatial hierarchies naturally, whereas ViT patchification can obscure high-frequency local signals).

- **CIEDE2000 (ΔE)**: The benchmark's difficulty is controlled using this metric that quantifies "perceptual distance" between colors. Quick check: Why is the lower bound of 25 critical for the benchmark's validity? (Answer: It ensures the task is theoretically solvable for human vision, proving it tests the model, not an impossible task).

- **Semantic vs. Perceptual Benchmarks**: This benchmark is unique compared to VQA or captioning because it uses synthetic, semantically neutral images to isolate perception from world knowledge. Quick check: Why does GPT-4.1 score well on standard VQA benchmarks but fail here? (Answer: Standard benchmarks allow shortcuts via language priors; HueManity disentangles perception).

## Architecture Onboarding

- **Component map**: Text Mask -> Color Selector (CIEDE2000) -> Ishihara Renderer (Pygame) -> [Vision Encoder (ViT)] -> [Projector] -> [LLM] -> Exact String Match

- **Critical path**: The evaluation pipeline depends on strict string normalization. If you modify prompts or response format, update the scoring function to strip artifacts before exact matching, or accuracy will artificially plummet.

- **Design tradeoffs**: Synthetic vs. Real—synthetic images isolate perception from world knowledge but may have domain gap. Resolution—900px standard, but "squinting effect" shows downsampling to 300px improves some models by acting as low-pass filter.

- **Failure signatures**: Hallucination (outputting phrases like "MUST SEE"), Evasion (describing image type without attempting task), Collapse (fine-tuned models outputting random plausible strings regardless of input).

- **First 3 experiments**: 1) Establish OCR Baseline—evaluate on text mask images first; 2) Resolution Ablation—test at 300px, 500px, 900px to check squinting effect; 3) Zero-Shot vs. Few-Shot—test if in-context examples improve accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications allow MLLMs to preserve low-level visual information necessary for perceptual grouping without sacrificing high-level semantic reasoning?
- Basis in paper: [explicit] The conclusion states that future work must focus on "developing novel MLLM architectures that preserve low-level visual information" and "projection layers that connect the vision encoder to the language model can act as an information bottleneck."
- Why unresolved: Current Vision Transformers and projection layers abstract away precise, high-resolution features, causing semantic optimization to override perceptual acuity.
- What evidence would resolve it: A new model architecture that achieves near-ceiling performance on HueManity (>90%) while maintaining state-of-the-art scores on standard semantic benchmarks like MMBench.

### Open Question 2
- Question: Why does fine-tuning on Ishihara-style patterns lead to catastrophic forgetting of basic OCR skills (mask performance), and can this interference be prevented?
- Basis in paper: [inferred] Section 5.3 shows that fine-tuning Gemma-3-4B on patterns caused mask accuracy to plummet from 80% to 0%, suggesting the model learned to output plausible strings without actually perceiving the visual input.
- Why unresolved: The phenomenon suggests a fundamental incompatibility in current training regimes where learning perceptual grouping destroys the representation of simple features, hinting at a lack of robust, disentangled visual features.
- What evidence would resolve it: A training methodology (e.g., specific regularization or architectural change) that improves pattern recognition accuracy without degrading performance on the control text mask task.

### Open Question 3
- Question: Does the "squinting effect" (improved performance at lower resolutions) imply that high-frequency visual noise in cluttered scenes disrupts the feature extraction of current MLLMs more than it does humans?
- Basis in paper: [inferred] Section 5.4 notes that GPT-4.1's accuracy improved from 34% to 49% when resolution was reduced to 300px, hypothesizing that downsampling acts as a low-pass filter.
- Why unresolved: It is unclear if this improvement is due to helpful smoothing of noise or a reduction in the number of patches the model must process, and why this effect differs from human visual processing.
- What evidence would resolve it: A systematic ablation of patch sizes and noise levels showing a direct correlation between high-frequency noise reduction and improved perceptual grouping in MLLMs.

## Limitations

- **Domain Gap to Natural Images**: While Ishihara patterns effectively isolate perceptual challenges, they remain synthetic stimuli that may not perfectly predict real-world perceptual capabilities.
- **Task Specificity**: The benchmark tests a narrow slice of perception—figure-ground segregation based on subtle color differences—and success doesn't necessarily predict performance on other perceptual challenges.
- **Human Baseline Validity**: The 99.38% human accuracy on numeric tasks comes from only 50 responses, leaving questions about individual variation and practice effects.

## Confidence

- **High Confidence**: MLLMs show significant performance deficits on HueManity compared to humans and ResNet-50; vision encoders optimized for semantic alignment likely discard fine-grained perceptual details; standard fine-tuning cannot easily remedy this perceptual deficit.
- **Medium Confidence**: ViT patchification specifically disrupts the subtle dot patterns needed for figure-ground segregation; the perceptual bottleneck is foundational to current MLLM architectures rather than a training deficiency.
- **Low Confidence**: HueManity will serve as a reliable predictor for broader perceptual capabilities in real-world applications.

## Next Checks

1. **Natural Image Transfer Test**: Evaluate MLLMs on degraded text recognition tasks (e.g., foggy signage, low-contrast OCR) to verify whether HueManity performance correlates with practical perceptual capabilities in natural images.

2. **Architectural Ablation Study**: Test HueManity performance using MLLMs with different vision backbones (CNN-based, hybrid ViT-CNN, or ViTs with reduced patch sizes) to confirm whether patchification is the primary bottleneck for fine-grained perception.

3. **Fine-Grained Perceptual Pre-Training**: Pre-train a vision encoder with contrastive objectives on synthetic texture discrimination before MLLM instruction tuning, then evaluate whether this specialized pre-training enables success on HueManity without catastrophic forgetting of semantic capabilities.