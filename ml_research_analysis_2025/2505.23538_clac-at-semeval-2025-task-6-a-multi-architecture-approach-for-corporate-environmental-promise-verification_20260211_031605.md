---
ver: rpa2
title: 'CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental
  Promise Verification'
arxiv_id: '2505.23538'
source_url: https://arxiv.org/abs/2505.23538
tags:
- promise
- subtask
- learning
- verification
- combined
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of verifying promises in corporate
  ESG reports through the SemEval-2025 Task 6 (PromiseEval), which requires four subtasks:
  promise identification, evidence assessment, clarity evaluation, and verification
  timing. The authors explore three model architectures: a baseline ESG-BERT model,
  a feature-enhanced model with linguistic markers, and a combined subtask model with
  attention pooling and multi-objective learning.'
---

# CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification

## Quick Facts
- arXiv ID: 2505.23538
- Source URL: https://arxiv.org/abs/2505.23538
- Reference count: 4
- Primary result: Achieved leaderboard score of 0.5268 on promise verification task

## Executive Summary
This paper addresses the challenge of verifying promises in corporate ESG reports through SemEval-2025 Task 6 (PromiseEval), which requires four subtasks: promise identification, evidence assessment, clarity evaluation, and verification timing. The authors explore three model architectures: a baseline ESG-BERT model, a feature-enhanced model with linguistic markers, and a combined subtask model with attention pooling and multi-objective learning. Their combined subtask model achieved a leaderboard score of 0.5268, outperforming the baseline of 0.5227. The study demonstrates the effectiveness of linguistic feature extraction and multi-objective learning for promise verification tasks, despite challenges posed by class imbalance and limited training data.

## Method Summary
The study evaluates three architectures on the ML-Promise dataset (400 English training instances). Model 1 uses ESG-BERT with frozen layers except the last two, applying four separate classifier heads for each subtask. Model 2 prepends linguistic features (sentiment, promise words, vague terms, NER dates) to the input text. Model 3 employs DeBERTa-v3-large with attention pooling, multi-objective learning for promise and evidence detection (0.6/0.4 weighting), and test-time augmentation with 3 forward passes. The final submission combines Model 3 for tasks 1-2 and Model 2 for tasks 3-4. Training uses 4-fold stratified CV for Models 1-2 and 90-10 split for Model 3, with Optuna TPE for hyperparameter optimization.

## Key Results
- Combined subtask model achieved leaderboard score of 0.5268, improving upon baseline ESG-BERT score of 0.5227
- Feature-enhanced model with linguistic markers showed minimal improvements over baseline
- Multi-objective learning with attention pooling provided modest gains by enabling shared representations between related tasks
- Class imbalance across all subtasks presented significant challenges for model training and evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training provides inductive bias for ESG promise language patterns.
- Mechanism: ESG-BERT encodes environmental, social, and governance terminology during pre-training, reducing the samples needed to learn domain-specific patterns during fine-tuning.
- Core assumption: Corporate ESG reports share linguistic regularities that transfer across documents and organizations.
- Evidence anchors:
  - [abstract] "Our first model utilizes ESG-BERT with task-specific classifier heads"
  - [section 3.1] "Mukherjee and Pothireddi (2021) introduced ESG-BERT, which we employ in our Base and Feature-Enhanced models"
  - [corpus] No direct corpus validation; neighbor papers focus on different SemEval tasks without ESG domain overlap
- Break condition: If target documents diverge significantly from ESG report conventions (e.g., informal communications), domain pre-training may provide limited or negative transfer.

### Mechanism 2
- Claim: Explicit linguistic feature prepending signals task-relevant patterns to transformer models.
- Mechanism: Prepending features like "POSITIVE Sentiment. Contains Promise Word." provides immediate attention targets, potentially reducing learning burden for pattern extraction.
- Core assumption: Hand-crafted feature lists capture signal that transformers cannot easily extract from limited examples.
- Evidence anchors:
  - [abstract] "effectiveness of linguistic feature extraction"
  - [section 4.2] "We made the assumption that prepending task-specific feature tags to the input text would improve model performance by signalling important linguistic patterns"
  - [section 6] "minimal improvements in performance likely stem from ESG-BERT already implicitly capturing many of these patterns during domain-specific pre-training, creating a redundancy effect"
  - [corpus] Not validated in corpus; no neighbor papers use similar prepending strategies
- Break condition: If features are redundant with pre-trained knowledge or create "structural disconnect between features and relevant text spans" (per Section 6), gains will be minimal.

### Mechanism 3
- Claim: Multi-objective learning with attention pooling improves related classification tasks through shared representations.
- Mechanism: Joint training on promise detection (Subtask 1) and evidence assessment (Subtask 2) enables representation sharing; attention pooling dynamically weights relevant tokens instead of relying on a single [CLS] token.
- Core assumption: Promise identification and evidence detection share underlying semantic features that benefit from joint learning.
- Evidence anchors:
  - [abstract] "effectiveness of...attention pooling, and multi-objective learning in promise verification tasks"
  - [section 4.3] "attention pooling to dynamically weight token representations across the sequence, allowing the model to better focus on relevant textual elements"
  - [section 6] "multitask learning benefits from shared representations between promise and evidence detection"
  - [corpus] Neighbor paper "LTG at SemEval-2025 Task 10" uses context optimization for classification; limited direct validation of multi-task architectures in corpus
- Break condition: If tasks require "contradictory feature attention" (Section 6), negative transfer may occur, degrading individual task performance.

## Foundational Learning

- Concept: **Multi-task learning and shared representations**
  - Why needed here: Model 3 jointly optimizes promise and evidence detection; understanding when tasks help vs. hurt each other is critical for architecture decisions.
  - Quick check question: Can you explain why the authors weighted promise detection at 0.6 and evidence at 0.4 in the loss function?

- Concept: **Attention pooling vs. [CLS] token representations**
  - Why needed here: The combined model replaces standard [CLS] pooling with learned attention weights over all token representations.
  - Quick check question: Given equation (1-2), what would happen if all attention weights converged to similar values?

- Concept: **Test-time augmentation (TTA)**
  - Why needed here: Final predictions use ensemble-averaged probabilities from multiple augmented forward passes.
  - Quick check question: Why might TTA help with class imbalance more than with balanced datasets?

## Architecture Onboarding

- Component map:
  - **Model 1:** ESG-BERT (frozen except last 2 layers) → 4 separate classifier heads (one per subtask)
  - **Model 2:** ESG-BERT + prepended linguistic features → 4 separate classifier heads
  - **Model 3:** DeBERTa-v3-large → attention pooling → dual task-specific heads (Subtasks 1 & 2 only) + context-enriched input with metadata

- Critical path:
  1. Data preparation: Extract linguistic features (sentiment, promise words, vague terms, NER dates) using Flair and spaCy
  2. Feature prepending: Format as tags before tokenization
  3. For Model 3: Implement attention pooling layer with learnable Wattn parameters
  4. Training: Use weighted multi-objective loss (0.6/0.4 split) with focal loss consideration
  5. Inference: Apply TTA with 3 forward passes, ensemble averaging

- Design tradeoffs:
  - ESG-BERT vs. DeBERTa-v3-large: Domain specificity vs. architectural sophistication
  - Separate models vs. combined: Specialization vs. shared representation benefits
  - Feature engineering investment vs. expected gains (Section 6 notes "minimal improvements" from features)

- Failure signatures:
  - Feature-enhanced model underperforming expectations → likely redundancy with pre-trained knowledge
  - Combined model showing modest gains despite complexity → insufficient training data (400 instances)
  - Large public-private score gap → overfitting to validation set

- First 3 experiments:
  1. Reproduce baseline ESG-BERT with frozen layers to establish performance floor on your data split
  2. Ablate linguistic features one category at a time (sentiment, promise words, vague terms, NER) to identify which provide signal vs. noise
  3. Test attention pooling in isolation (without multi-objective learning) to isolate its contribution vs. shared representation effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating all four subtasks into a unified multi-objective architecture improve performance by capturing interdependencies between promise identification, evidence assessment, clarity evaluation, and verification timing?
- Basis in paper: [explicit] The conclusion states: "Future work could explore...incorporating all classification tasks within a single multi-objective architecture to better capture interdependencies between promise identification, evidence assessment, clarity evaluation, and verification timing."
- Why unresolved: The Combined Subtask Model only addressed Subtasks 1 and 2 jointly; Subtasks 3 and 4 were handled separately by the Feature-Enhanced Model. The paper does not test whether full joint modeling improves results.
- What evidence would resolve it: Train a single model with four classification heads and shared representations; compare against the current hybrid approach using the same evaluation metrics on a held-out test set.

### Open Question 2
- Question: Can cross-lingual promise verification be effectively achieved using multilingual transformer models on the full ML-Promise dataset?
- Basis in paper: [explicit] The conclusion proposes: "Future work could explore incorporating cross-lingual promise verification through multilingual transformer models."
- Why unresolved: The experiments focused exclusively on the English portion (400 instances), leaving the multilingual aspect of the ML-Promise dataset unexplored.
- What evidence would resolve it: Evaluate multilingual models (e.g., mBERT, XLM-R) on non-English portions of ML-Promise and report cross-lingual transfer performance.

### Open Question 3
- Question: What is the relative contribution of each architectural component (backbone PLM choice, linguistic features, attention pooling, document metadata) to overall performance?
- Basis in paper: [explicit] The conclusion notes: "a systematic ablation study could quantify the contribution of each backbone pre-trained language model (PLM), such as BERT and RoBERTa, and each feature strategy (e.g., linguistic features, document metadata)."
- Why unresolved: The paper compares three model variants but does not isolate the contribution of individual components, leaving attribution of the 1.74% improvement unclear.
- What evidence would resolve it: Conduct ablation experiments removing one component at a time (attention pooling, metadata tags, linguistic features) and report performance deltas.

### Open Question 4
- Question: Does negative transfer occur between promise detection and evidence detection tasks when trained jointly, and can task-specific feature attention mechanisms mitigate it?
- Basis in paper: [inferred] The discussion notes that "potential negative transfer between promise and evidence tasks may have constrained performance gains for instances where these classifications require contradictory feature attention."
- Why unresolved: This hypothesis is stated but not empirically tested; the modest gains from the Combined Model could stem from negative transfer or data scarcity.
- What evidence would resolve it: Analyze per-instance predictions to identify cases where joint training degrades one subtask; compare against single-task baselines with gradient isolation or task-specific attention.

## Limitations
- Limited dataset of 400 training instances with significant class imbalance across all subtasks
- Minimal improvement (0.0041) from combined subtask model despite additional complexity
- Critical implementation details unspecified, including exact word lists and focal loss configuration
- Feature-enhanced model showed only minimal improvements, suggesting potential redundancy with pre-trained knowledge

## Confidence
- **High Confidence**: Domain-specific pre-training advantage of ESG-BERT is well-supported by literature and demonstrated through baseline performance
- **Medium Confidence**: Attention pooling mechanism's contribution is supported by reasoning but lacks direct empirical validation
- **Low Confidence**: Linguistic feature enhancement showed minimal improvements, suggesting limited effectiveness

## Next Checks
1. **Ablation Study on Feature Components**: Systematically remove each linguistic feature category (sentiment, promise words, vague terms, NER dates) to quantify their individual contributions and determine whether any single feature type drives the minimal improvements observed.

2. **Attention Weight Analysis**: Extract and visualize the attention weights learned by the Wattn layer in Model 3 to empirically verify whether shared representations benefit both tasks or create contradictory attention patterns that could explain the modest performance gains.

3. **Class-Balanced Training Evaluation**: Re-train Model 1 (baseline) and Model 3 using class-balanced sampling or weighted loss functions to isolate whether the modest gains from multi-task learning persist when class imbalance is explicitly addressed.