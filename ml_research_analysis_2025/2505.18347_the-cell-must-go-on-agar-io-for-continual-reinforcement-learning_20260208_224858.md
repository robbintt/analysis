---
ver: rpa2
title: 'The Cell Must Go On: Agar.io for Continual Reinforcement Learning'
arxiv_id: '2505.18347'
source_url: https://arxiv.org/abs/2505.18347
tags:
- agent
- mass
- learning
- continual
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgarCL, a new research platform for continual
  reinforcement learning based on the game Agar.io. The environment features high-dimensional
  pixel-based observations, hybrid continuous-discrete action spaces, and dynamics
  that change based on the agent's mass.
---

# The Cell Must Go On: Agar.io for Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.18347
- Source URL: https://arxiv.org/abs/2505.18347
- Reference count: 40
- Agents fail on full game despite success on mini-games, highlighting continual RL challenges

## Executive Summary
This paper introduces AgarCL, a new research platform for continual reinforcement learning based on the game Agar.io. The environment features high-dimensional pixel-based observations, hybrid continuous-discrete action spaces, and dynamics that change based on the agent's mass. Three widely used deep RL algorithms—DQN, PPO, and SAC—were evaluated in both mini-games and the full game. While agents performed well on simple tasks, none succeeded in the full environment, highlighting the challenges of continual RL. The work emphasizes the importance of evaluation platforms that support non-stationary, partially observable, and non-episodic settings. AgarCL is positioned as a benchmark for advancing continual RL research.

## Method Summary
AgarCL is a Gym-compatible environment implemented in C++ with a Python interface using Pybind11. The environment simulates the game Agar.io where agents control a cell that must grow by consuming smaller entities while avoiding larger ones. Key features include: (1) pixel-based 128×128×4 observations rendered via OpenGL, (2) hybrid action space with continuous movement coordinates and discrete actions, (3) mass-dependent dynamics where agent speed decreases with size, and (4) non-episodic structure where world state persists across agent deaths. The environment includes six scaffolded mini-games that isolate specific challenges like mass decay and random pellet distribution, plus a full "Grand Arena" environment. Three deep RL algorithms (DQN, PPO, SAC) were evaluated using a standard encoder architecture with three convolutional layers followed by layer normalization and fully connected layers.

## Key Results
- Standard deep RL algorithms succeed on simple episodic mini-games but fail on the full continual RL environment
- Mass decay dramatically reduces performance even in simple settings (Mini-game 2 shows ~50% performance drop)
- Hyperparameter tuning for one task does not transfer to others, highlighting continual RL challenges
- None of the tested algorithms (DQN, PPO, SAC) achieved meaningful performance on the full Grand Arena environment

## Why This Works (Mechanism)

### Mechanism 1: Mass-Dependent Dynamics Create Endogenous Non-Stationarity
The agent's mass directly modulates environment dynamics, creating smooth, behavior-dependent non-stationarity rather than artificial task switches. As mass increases: (1) speed decreases via v = mass^0.439, (2) field of view zooms out to keep the agent visible, (3) mass decay rate increases. These changes are gradual but compound, meaning an agent that gains mass faces a qualitatively different problem than when it started. Agents that can adapt their policy smoothly as their own actions change the dynamics will outperform fixed policies.

### Mechanism 2: Non-Episodic Structure With Persistent State Changes
Removing episodic resets and persisting world state across deaths creates a true continual learning problem where past actions affect future learning. When an agent dies, it respawns but the predator retains the consumed mass; pellets and viruses regenerate on fixed schedules, not resets. The agent cannot "reset" to a known state to recover from poor exploration. Agents must learn recovery behaviors and cannot rely on environment-provided resets to escape bad states.

### Mechanism 3: Scaffolded Mini-Games Isolate Specific Challenges
The six mini-games decompose the full problem into interpretable sub-challenges, enabling targeted algorithm development. Mini-games vary: (1) pellet distribution (square path vs. random), (2) mass decay (on/off), (3) starting mass (25 vs. 1000), (4) episodic vs. continual. Results show mass decay and high starting mass dramatically reduce performance even in simple tasks. Improvements on isolated challenges are expected to transfer to the full game.

## Foundational Learning

- **Concept: Continual vs. Episodic Reinforcement Learning**
  - Why needed here: AgarCL explicitly rejects episodic resets; agents must learn without periodic state resets
  - Quick check question: Can you explain why removing episode boundaries changes the exploration problem?

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The agent receives pixel observations from a limited, mass-dependent viewport; hidden state includes off-screen entities
  - Quick check question: What information is the agent missing that would be available in a fully observable setting?

- **Concept: Hybrid Action Spaces (Continuous + Discrete)**
  - Why needed here: AgarCL requires simultaneous continuous movement (⟨x, y⟩) and discrete choices (split/eject/move)
  - Quick check question: How does PPO's factored joint policy handle this compared to DQN's discretization?

## Architecture Onboarding

- **Component map:**
  Environment core (C++) -> Rendering (OpenGL) -> Python interface (Pybind11) -> Agent encoder -> RL algorithm

- **Critical path:**
  1. Start with episodic mini-games (1–3) to validate encoder and basic learning
  2. Progress to continual mini-games (4–6) to test non-episodic adaptation
  3. Only then attempt Grand Arena; expect failure with standard algorithms

- **Design tradeoffs:**
  - Pixel vs. symbolic observations: Pixels require representation learning but avoid hand-crafted features; symbolic is faster but may miss visual patterns
  - Frame skip of 4: Reduces decision frequency (1163 obs/sec vs. 2016 with skip=1), may blur fine-grained control
  - Discretized DQN actions (8 directions × 3 discrete = 24 actions) vs. continuous PPO/SAC: DQN simpler but loses precision

- **Failure signatures:**
  - Agent stuck at ~0 reward in full game (Table 9: DQN 4.0, PPO 8.0, SAC 22.0 vs. human expert baseline unavailable)
  - PPO collapse when hyperparameters from one mini-game applied to another (Appendix E.4)
  - SAC producing NaNs after 160M frames in full game
  - Inability to recover from "bad" states in continual mini-games without resets

- **First 3 experiments:**
  1. Reproduce Mini-Game 1 (square path, no decay, m=25): Verify your encoder matches reported ~600-700 return for DQN/PPO/SAC; this confirms your pipeline works
  2. Ablate mass decay in Mini-Game 2: Compare performance drop; expect ~50% reduction per Section 4.1 results
  3. Test exploration strategy: Compare ε-greedy vs. εz-greedy (temporally-extended) in continual Mini-Game 4; paper reports εz-greedy was necessary for non-episodic tasks (Appendix E.1)

## Open Questions the Paper Calls Out

### Open Question 1
How can we design algorithms that maintain stable hyperparameter performance over unbounded, non-episodic learning horizons without requiring task-specific tuning? The authors state this is a key open question since tuning strategies commonly used in RL are impractical for potentially unbounded timeframes in continual RL.

### Open Question 2
Can agents learn effective exploration strategies in continual RL settings without relying on episodic resets to recover from poor states? The authors list "exploration without resets" as one of the open problems in continual RL research that AgarCL exposes.

### Open Question 3
How do we develop evaluation protocols and metrics that appropriately assess continual learning in non-episodic, never-ending environments? The authors note the field lacks consensus on how to measure progress or compare algorithms in such settings.

### Open Question 4
How will learning dynamics change when extending AgarCL from fixed-policy bots to multi-agent settings with co-evolving learners? Future work will focus on supporting learning across multiple agents, making this environment multi-agent.

## Limitations
- Unproven composability: Solving all six mini-games may not adequately prepare agents for the emergent dynamics of the full game
- Bot-based world: Performance bottleneck may stem from insufficient environmental complexity compared to human players
- Hyperparameter fragility: PPO performance varied significantly with hyperparameter changes, suggesting sensitivity that could mask true algorithmic capability

## Confidence
- High confidence in the environment's design and implementation (the mass-dependent dynamics, non-episodic structure, and pixel-based observations are clearly specified and reproducible)
- Medium confidence in the benchmark's validity as a continual RL challenge (the mini-games provide interpretable sub-challenges, but the leap to the full game may be too large)
- Low confidence in current algorithmic solutions (all tested agents fail on the full game, and the reasons for failure are not fully understood)

## Next Checks
1. Test compositionality: Verify that agents trained sequentially on mini-games 1-6 show improved performance on Grand Arena compared to agents trained only on the full game from scratch
2. Scale environmental complexity: Replace bot-based opponents with human demonstration data or more sophisticated AI to determine if algorithmic failure is due to environmental limitations
3. Ablate the POMDP structure: Create a fully observable variant of AgarCL and re-run baseline algorithms to quantify the difficulty added by partial observability