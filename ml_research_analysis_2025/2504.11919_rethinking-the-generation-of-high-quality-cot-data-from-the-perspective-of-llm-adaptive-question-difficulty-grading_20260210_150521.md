---
ver: rpa2
title: Rethinking the Generation of High-Quality CoT Data from the Perspective of
  LLM-Adaptive Question Difficulty Grading
arxiv_id: '2504.11919'
source_url: https://arxiv.org/abs/2504.11919
tags:
- difficulty
- data
- reasoning
- distribution
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for generating high-quality Chain-of-Thought
  (CoT) data by adaptively grading question difficulty based on the intrinsic reasoning
  capabilities of large language models (LLMs). The method constructs a LLM-Adaptive
  question database, samples questions according to a designed difficulty distribution,
  and generates CoT data using DeepSeek-R1 (671B).
---

# Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading

## Quick Facts
- arXiv ID: 2504.11919
- Source URL: https://arxiv.org/abs/2504.11919
- Reference count: 7
- Primary result: Training small LLMs on 2k adaptively sampled CoT data achieves performance competitive with larger distilled models

## Executive Summary
This paper proposes a method for generating high-quality Chain-of-Thought (CoT) training data by adaptively grading question difficulty based on the intrinsic reasoning capabilities of large language models (LLMs). The approach constructs a LLM-Adaptive question database, samples questions according to a designed difficulty distribution, and generates CoT data using DeepSeek-R1 (671B). Experiments demonstrate that models trained on this adaptive data significantly improve reasoning performance across different parameter scales, with ZMath-32B and ZCode-32B trained on only 2k high-quality CoT data surpassing DeepSeek-Distill-32B in math and code reasoning tasks, respectively.

## Method Summary
The method involves constructing a LLM-Adaptive question database by running a base LLM on raw questions, marking correct answers as "Easy," and using a PRM-Grader to score incorrect responses on a 0-1 scale mapped to 5 difficulty levels. Questions are then sampled according to a designed difficulty distribution (either LLM-inferred or curriculum-based), and CoT data is generated using DeepSeek-R1 (671B). The resulting CoTs are verified for correctness and used to fine-tune small LLMs via supervised fine-tuning (SFT) using LlamaFactory with specific hyperparameters.

## Key Results
- ZMath-32B trained on 2k adaptive CoT data surpasses DeepSeek-Distill-32B on MATH500
- ZCode-32B trained on 2k adaptive CoT data outperforms DeepSeek-Distill-32B on LiveCodeBench
- The adaptive difficulty grading approach significantly improves reasoning performance across different parameter scales
- Models trained on adaptively sampled data show better generalization than those trained on random sampling

## Why This Works (Mechanism)
The approach works by aligning the difficulty distribution of training data with the model's actual reasoning capabilities, ensuring that generated CoT data is neither too easy (wasteful) nor too hard (frustrating). By using a PRM-Grader to assess the quality of incorrect responses and mapping them to difficulty levels, the method creates a more nuanced and effective training curriculum. This adaptive sampling strategy helps small LLMs learn reasoning patterns more efficiently by focusing on questions that match their current skill level.

## Foundational Learning
- **PRM-Grader**: A model that scores the quality of responses on a 0-1 scale, used to assess reasoning attempts and map to difficulty levels. *Why needed*: To create nuanced difficulty labels beyond simple correct/incorrect binary classification. *Quick check*: Verify PRM output distribution on held-out set before mapping to levels.
- **Difficulty Distribution Sampling**: Strategy for selecting questions based on their assigned difficulty levels, either following the LLM's evaluation distribution or using curriculum-based weights. *Why needed*: To ensure training data matches the model's learning curve and reasoning capabilities. *Quick check*: Compare model performance when trained on adaptive vs. random sampling.
- **CoT Verification**: Process of validating generated Chain-of-Thought responses for correctness before using them in training. *Why needed*: To ensure only high-quality, accurate reasoning patterns are used for fine-tuning. *Quick check*: Monitor the percentage of verified CoTs after filtering to ensure sufficient training data.

## Architecture Onboarding
- **Component Map**: Base LLM -> PRM-Grader -> Difficulty-Annotated DB -> Sampling Distribution -> DeepSeek-R1 -> CoT Generation -> Verification -> SFT Training
- **Critical Path**: The core pipeline is: Raw Question DB → Base LLM Evaluation → PRM-Grader Scoring → Difficulty Annotation → Adaptive Sampling → DeepSeek-R1 CoT Generation → Verification → SFT Training
- **Design Tradeoffs**: The method trades computational cost of running a large model (DeepSeek-R1) for generating CoT data against the potential quality gains from adaptive difficulty grading. Using a 671B model for CoT generation is expensive but may yield higher quality data than smaller models.
- **Failure Signatures**: 
  - Poor model performance may indicate miscalibrated PRM-Grader scores or incorrect difficulty level mappings
  - Low percentage of verified CoTs suggests the base LLM accuracy is too low or the difficulty distribution is too skewed toward hard questions
  - Inconsistent benchmark results across different sampling strategies may indicate the need for hybrid approaches
- **First Experiments**:
  1. Run Base LLM on raw question DB and analyze accuracy distribution to establish baseline performance
  2. Apply PRM-Grader to incorrect responses and verify score distribution before mapping to difficulty levels
  3. Generate a small batch of CoTs with DeepSeek-R1 and verify correctness to establish quality metrics

## Open Questions the Paper Calls Out
- Can integrating LLM-Adaptive difficulty grading with reinforcement learning (RL) or rejection sampling mechanisms further enhance reasoning abilities compared to supervised fine-tuning (SFT) alone?
- Does increasing the training corpus size for code generation tasks beyond 2,000 samples yield the significant performance improvements hypothesized by the authors?
- Can a hybrid sampling distribution strategy combining LLM-inferred evaluation distributions and human-defined priors achieve more consistent performance across diverse benchmarks than either method alone?

## Limitations
- The specific PRM-Grader model and its score-to-difficulty-level mapping thresholds are not clearly specified, making exact replication challenging
- The curriculum distribution weights (w_i values) for the proposed sampling strategy are not provided
- The prompts and formatting used for DeepSeek-R1 CoT generation are not detailed

## Confidence
- **High confidence**: The overall methodology of using adaptive difficulty grading and PRM-Grader for CoT data generation is clearly described and the experimental results show significant improvements
- **Medium confidence**: The claim that 2k CoT samples can match or exceed performance of larger models is supported by experiments, though exact reproducibility depends on unspecified implementation details
- **Low confidence**: The specific implementation details of PRM-Grader scoring and curriculum sampling distributions, which are crucial for exact replication

## Next Checks
1. Verify the PRM-Grader score distribution and mapping to difficulty levels on a held-out validation set before proceeding with CoT generation
2. Check the base LLM's accuracy on the raw question database; if accuracy is very low, consider expanding the candidate pool or adjusting the difficulty sampling distribution
3. Compare the performance of models trained on adaptively sampled CoT data versus random sampling to validate the effectiveness of the proposed difficulty-based approach