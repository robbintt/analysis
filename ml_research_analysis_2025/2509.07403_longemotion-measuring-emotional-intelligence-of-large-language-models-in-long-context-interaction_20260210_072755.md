---
ver: rpa2
title: 'LongEmotion: Measuring Emotional Intelligence of Large Language Models in
  Long-Context Interaction'
arxiv_id: '2509.07403'
source_url: https://arxiv.org/abs/2509.07403
tags:
- emotion
- emotional
- figure
- task
- coem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LongEmotion, a benchmark for evaluating
  large language models'' emotional intelligence in long-context scenarios. It includes
  six tasks: Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation,
  Emotion Summary, and Emotion Expression, with an average context length of 15,341
  tokens.'
---

# LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction

## Quick Facts
- **arXiv ID**: 2509.07403
- **Source URL**: https://arxiv.org/abs/2509.07403
- **Reference count**: 39
- **Primary result**: CoEM framework consistently improves LLM emotional intelligence performance across most LongEmotion tasks through context-as-corpus retrieval and multi-agent enrichment.

## Executive Summary
This paper introduces LongEmotion, a comprehensive benchmark for evaluating large language models' emotional intelligence in long-context scenarios. The benchmark includes six tasks with an average context length of 15,341 tokens: Emotion Classification, Detection, QA, Conversation, Summary, and Expression. To address performance limitations under realistic constraints, the authors propose a Collaborative Emotional Modeling (CoEM) framework that integrates Retrieval-Augmented Generation with multi-agent collaboration. CoEM divides context into chunks, ranks them by relevance, enriches them with emotional features, and generates ensemble responses. Experiments demonstrate consistent improvements across most tasks, with a comparative case study revealing differences in emotional intelligence and hallucination tendencies between GPT models.

## Method Summary
The LongEmotion benchmark evaluates LLMs' emotional intelligence across six tasks using datasets from psychological literature, counseling dialogues, and emotion benchmarks. The CoEM framework implements a five-stage pipeline: chunking long contexts into token-constrained segments, initial ranking via dense semantic similarity using bge-m3 embeddings, multi-agent enrichment through CoEM-Sage (GPT-4o or DeepSeek-V3) with task-specific prompts, re-ranking enriched chunks by emotional relevance, and ensemble generation by CoEM-Core. The framework uses conversational context as a retrieval source without external knowledge bases, employing vLLM with temperature 0.8 and top_p 0.9. Chunk sizes range from 128-512 tokens with retrieval counts of 1-16 chunks depending on the task.

## Key Results
- CoEM consistently improves EI-related performance across most LongEmotion tasks compared to baseline and RAG-only approaches
- Chunk size and retrieval count significantly impact performance, with GPT-4o-mini showing optimal results at 128 tokens and 8 chunks
- Stronger CoEM-Sage models (DeepSeek-V3) drive higher performance than weaker ones (GPT-4o) in multi-turn conversation tasks
- A comparative case study reveals GPT-5 exhibits superior theoretical knowledge but increased hallucination propensity compared to GPT-4o

## Why This Works (Mechanism)

### Mechanism 1: Context-As-Corpus Retrieval Reduces Noise in Long-Context EI Tasks
Treating conversation history as a dynamic retrieval source improves EI performance by isolating emotionally salient information from context-independent noise. Chunking segments long context into token-constrained units; dense semantic similarity (cosine) ranks chunks by query relevance; only top-ranked chunks proceed to generation, filtering irrelevant spans that dilute emotional reasoning. The core assumption is that emotionally relevant information clusters in contiguous spans that can be isolated via semantic similarity. Break condition: When emotionally salient cues are scattered across many small spans, single-pass similarity ranking may miss weak signals.

### Mechanism 2: Multi-Agent Enrichment Injects External Emotional Knowledge Without Task Leakage
A reasoning agent (CoEM-Sage) enhances retrieved chunks with latent emotional signals or psychological knowledge, improving downstream generation quality. After initial retrieval, CoEM-Sage analyzes each chunk via task-specific prompts—for recognition tasks, identifying subtle cues; for knowledge tasks, providing summaries grounded in psychological theory; for generation tasks, adding emotional analysis. Enriched chunks are re-ranked by similarity before generation. The core assumption is that the enrichment model has stronger EI capabilities than the core generation model and can transfer this via feature injection. Break condition: If CoEM-Sage model is weaker than CoEM-Core, enrichment may introduce noise.

### Mechanism 3: Two-Stage Ranking Separates Factual and Emotional Relevance
Separating initial ranking (factual relevance) from re-ranking (emotional relevance) improves both grounding and affective coherence. Initial ranking selects chunks by raw semantic similarity to query; after enrichment with emotional features, re-ranking evaluates enriched chunks, prioritizing those whose augmented content aligns with affective context. This decoupling prevents purely topical retrieval from missing emotionally nuanced segments. The core assumption is that emotional relevance is partially orthogonal to topical similarity and benefits from explicit secondary scoring. Break condition: If enrichment adds noisy or inconsistent emotional annotations, re-ranking may amplify errors.

## Foundational Learning

- **Dense Semantic Retrieval (Embedding-Based RAG)**: Understanding vector similarity thresholds and embedding quality is essential for debugging retrieval failures. *Quick check*: Given a query embedding and five chunk embeddings with cosine similarities [0.72, 0.45, 0.68, 0.31, 0.55], which chunks would likely be retrieved with top-k=3?
- **Multi-Agent Orchestration Patterns**: Understanding when to parallelize vs. pipeline agent calls affects latency and cost. *Quick check*: If CoEM-Sage enrichment takes 200ms per chunk and you retrieve 8 chunks, what is the minimum latency for the enrichment stage if calls are sequential? What if parallelized?
- **LLM-as-Judge Evaluation**: Understanding prompt design for reliable scoring is critical for interpreting benchmark results. *Quick check*: What are two failure modes when using an LLM to score another LLM's output on empathy?

## Architecture Onboarding

- **Component map**: Chunking -> Initial Ranking -> CoEM-Sage Enrichment -> Re-Ranking -> CoEM-Core Generation
- **Critical path**: 1. Chunk context → 2. Initial ranking (skip for Detection) → 3. CoEM-Sage enrichment per chunk → 4. Re-ranking enriched chunks → 5. CoEM-Core generates output
- **Design tradeoffs**: Smaller chunks (128) provide finer granularity but require more retrievals; larger chunks (512) reduce retrieval overhead but may include noise. CoEM-Sage model selection: stronger EI models improve enrichment but increase cost/latency. Skipping initial ranking: beneficial for Detection but risky when factual grounding is critical.
- **Failure signatures**: Retrieval returns irrelevant chunks (check embedding quality, chunk size, or query formulation); enrichment adds hallucinated emotional content (audit CoEM-Sage prompts); re-ranking overfits to enrichment noise (reduce retrieval count); small models degrade under RAG (CoEM's enrichment helps recover performance).
- **First 3 experiments**: 1) Baseline vs. RAG vs. CoEM on Emotion QA: compare F1 scores across configurations. 2) Chunk size ablation: test [128, 256, 512] on Emotion Classification. 3) CoEM-Sage swap test: run MC-4 task with GPT-4o vs. DeepSeek-V3 as CoEM-Sage.

## Open Questions the Paper Calls Out

### Open Question 1
Can the LongEmotion benchmark and CoEM framework be effectively extended to multimodal inputs and non-English languages? The current datasets are text-only and English-only, with quality preservation in translation remaining uncertain. Evidence would be successful evaluation of multimodal models or translated datasets showing performance consistency.

### Open Question 2
How can the trade-off between high theoretical knowledge and hallucination propensity be mitigated in advanced models? GPT-5 exhibits hallucinations while possessing superior theoretical understanding. Evidence would be a decoding strategy or framework that reduces hallucination rate while maintaining high knowledge application scores.

### Open Question 3
What architectural modifications are required for smaller models (<14B parameters) to benefit from autonomous search-based reasoning in emotional tasks? Qwen3-8B shows performance drops with Search-o1. Evidence would be a modified agent architecture or training regime yielding positive gains for 8B-parameter models on LongEmotion.

## Limitations
- The LLM-as-Judge evaluation method introduces potential biases in emotional intelligence assessment due to GPT-4o's own EI capabilities and self-referential bias
- The contribution of individual CoEM components varies significantly by task type, requiring task-specific tuning of chunk size and retrieval count
- Claims about optimal chunk sizes and retrieval counts across all models and tasks are undermined by demonstrated variability in performance patterns

## Confidence
- **High confidence**: The six-task benchmark structure and overall methodology for evaluating EI in long-context scenarios are well-designed
- **Medium confidence**: The CoEM framework's general effectiveness in improving EI performance, particularly for larger models
- **Low confidence**: Claims about superiority of specific chunk sizes and retrieval counts across all models and tasks

## Next Checks
1. **Component ablation study**: Systematically disable each CoEM component to quantify individual contributions to performance improvements
2. **Cross-model validation**: Test CoEM with additional LLM families beyond those evaluated to determine if patterns generalize
3. **Hallucination audit**: Conduct systematic analysis of hallucination rates across tasks and models, particularly for knowledge-intensive tasks