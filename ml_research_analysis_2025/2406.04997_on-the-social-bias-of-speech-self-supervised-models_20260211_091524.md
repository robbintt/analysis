---
ver: rpa2
title: On the social bias of speech self-supervised models
arxiv_id: '2406.04997'
source_url: https://arxiv.org/abs/2406.04997
tags:
- bias
- speech
- pruning
- social
- speat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates social bias in speech self-supervised learning
  (SSL) models, revealing that representations generated by SSL models amplify social
  bias compared to traditional acoustic features. The research examines how model
  architecture, size, training steps, and compression techniques influence bias propagation.
---

# On the social bias of speech self-supervised models

## Quick Facts
- **arXiv ID**: 2406.04997
- **Source URL**: https://arxiv.org/abs/2406.04997
- **Reference count**: 0
- **Primary result**: SSL models amplify social bias compared to traditional acoustic features

## Executive Summary
This study investigates social bias in speech self-supervised learning (SSL) models, revealing that representations generated by SSL models amplify social bias compared to traditional acoustic features. The research examines how model architecture, size, training steps, and compression techniques influence bias propagation. Experiments with HuBERT, Wav2Vec2, and MelHuBERT models show that longer pretraining steps increase bias, and wider, shallower architectures exhibit reduced bias compared to narrower, deeper ones. Compression techniques, particularly row pruning, effectively mitigate social bias, with all compression methods reducing Age bias. The findings provide insights for developing more equitable SSL models and highlight the importance of addressing bias in self-supervised speech representation learning.

## Method Summary
The study evaluates social bias in SSL models using SpEAT (Speech Embedding Association Test) to measure effect sizes between target concepts (gender, age, nationality) and attribute concepts (positive/negative valence). Models were pretrained on LibriSpeech 960hr, with HuBERT and Wav2Vec2 trained via fairseq for 400k steps and MelHuBERT for 421k steps. Three architectural configurations were tested: Small (3 layers, 640 hidden), Slim (12 layers, 384 hidden), and Base (12 layers, 768 hidden). Four compression techniques were evaluated: head pruning, row pruning, weight pruning, and knowledge distillation. Bias was assessed at multiple pretraining checkpoints, and downstream utility was measured via Phoneme Recognition PER.

## Key Results
- SSL representations amplify social bias compared to traditional acoustic features (MFCC, Spectrogram)
- Longer pretraining steps correlate with increased bias across all model configurations
- Row pruning of feed-forward network dimensions most effectively reduces social bias
- All compression methods reduce Age bias, though effects on Gender and Native categories vary
- Wider, shallower architectures exhibit lower bias than narrower, deeper architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSL pretraining amplifies social bias present in training data beyond what exists in traditional acoustic features.
- Mechanism: SSL models learn contextualized representations that encode associations between demographic attributes and affective concepts. The masked prediction and contrastive objectives encourage the model to capture correlations—both meaningful and spurious—present in the pretraining corpus.
- Core assumption: Bias amplification occurs during the SSL optimization process, not solely from input representations.
- Evidence anchors:
  - [abstract] "representations generated by SSL models amplify social bias compared to traditional acoustic features"
  - [section] Table 3: MelHuBERT shows SpEAT d = 1.33 for Gender vs MFCC at 0.50 and Spectrogram at 0.17; HuBERT base shows 1.04 vs MFCC 0.50
  - [corpus] Related work on "Language Bias in Self-Supervised Learning For ASR" confirms SSL bias propagation is an active concern, though specific mechanisms remain underexplored

### Mechanism 2
- Claim: Longer pretraining correlates with increased social bias in learned representations.
- Mechanism: As training progresses, models refine representations and simultaneously reinforce spurious correlations between social attributes and target concepts. The paper shows convergence to positive SpEAT d values within 30k-60k steps, with continued increase through 400k steps.
- Core assumption: Bias accumulation is a product of extended exposure to the pretraining objective, not an initialization artifact.
- Evidence anchors:
  - [abstract] "longer pretraining steps increase bias, and wider, shallower architectures exhibit reduced bias"
  - [section] Figure 1: All six model configurations show SpEAT d increasing from near-zero at initialization to positive values, with Gender bias reaching highest levels (~1.0-1.5) by 400k steps
  - [corpus] Limited corpus evidence on training duration effects; this appears to be a novel contribution

### Mechanism 3
- Claim: Row pruning of feed-forward network dimensions reduces social bias more effectively than other compression techniques.
- Mechanism: Removing FFN rows/columns may eliminate dimensions that encode spurious demographic-valence associations while preserving task-relevant features. The FFN is hypothesized to store associative patterns that pruning selectively attenuates.
- Core assumption: Social bias correlates with specific FFN dimensions that can be identified via L1-norm importance scores.
- Evidence anchors:
  - [abstract] "row pruning, effectively mitigate social bias, with all compression methods reducing Age bias"
  - [section] Figure 2(c)(d): Row pruning shows consistent SpEAT d reduction across Gender, Native, and Age categories for both Wav2Vec2 and MelHuBERT; head pruning and weight pruning show inconsistent results
  - [corpus] No corpus evidence on row pruning specifically for debiasing speech SSL; NLP compression-fairness work cited as motivation

## Foundational Learning

- **SpEAT (Speech Embedding Association Test)**: The paper's core bias quantification metric, measuring cosine similarity associations between target concepts (e.g., female/male) and attribute concepts (positive/negative valence). Quick check: Given target embeddings X, Y and attribute sets A, B, can you compute the effect size d using the mean cosine similarity difference formula?

- **Transformer Architecture Variants (Small vs Slim)**: The paper distinguishes between "small" (wider, shallower: 3 layers, 640 hidden) and "slim" (narrower, deeper: 12 layers, 384 hidden) configurations with similar parameter counts. Quick check: What are the hidden dimension and layer count differences between small and slim architectures?

- **Compression Taxonomy for Transformers**: The paper tests four compression methods—head pruning, row pruning, weight pruning, and knowledge distillation—with different bias effects. Quick check: Which network component does row pruning target, and how does it differ from head pruning?

## Architecture Onboarding

- **Component map**: Raw waveform (Wav2Vec2, HuBERT) or Mel spectrogram (MelHuBERT) -> 7-layer convolutional frontend (removed in MelHuBERT) -> Transformer encoder: 3-12 layers with multi-head attention (8-12 heads) and FFN (768-3072 dim) -> Pretraining objectives: Contrastive loss (Wav2Vec2), masked prediction with k-means targets (HuBERT), cross-entropy (MelHuBERT) -> Compression targets: Attention heads (head pruning), FFN rows/columns (row pruning), individual weights (weight pruning), layer count (distillation)

- **Critical path**: 1) Select architecture configuration (small/wide preferred over slim/deep for lower bias) 2) Monitor SpEAT d during pretraining at checkpoints (10k, 50k, 100k, 200k, 400k) 3) Apply row pruning to FFN (128 dimensions per 25k steps) if bias mitigation needed 4) Validate downstream task performance (Phoneme Recognition PER) after compression

- **Design tradeoffs**: Training duration: Longer steps improve representation quality but increase bias; Architecture shape: Small (3 layers, 640 hidden) vs Slim (12 layers, 384 hidden)—similar params, different bias profiles; Compression aggressiveness: Higher sparsity reduces bias but degrades downstream PER (e.g., Wav2Vec2 base PER 5.7 → compressed 17.3 at extreme row pruning)

- **Failure signatures**: SpEAT d > 0.80 indicates large bias requiring intervention; Negative SpEAT d suggests reverse bias (potential overcorrection); PER degradation > 2x baseline indicates excessive compression; Fluctuating SpEAT d during compression suggests unstable debiasing

- **First 3 experiments**: 1) Compute baseline SpEAT d for your SSL model across all three bias categories (Gender, Age, Native) and compare against MFCC/spectrogram controls 2) Plot SpEAT d vs training steps at minimum 5 checkpoints to confirm bias accumulation pattern matches paper findings 3) Apply row pruning to FFN (targeting 512 remaining rows) and measure both SpEAT d reduction and downstream PER impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed bias patterns and mitigation findings generalize to non-English and multilingual speech SSL models?
- Basis in paper: [explicit] The authors state in the Limitations section: "our analysis focuses on models trained with mainly English datasets. Further research is required on whether these results can apply to other languages or multilingual settings."
- Why unresolved: All experiments used LibriSpeech 960hr (English-only), and language-specific phonological and prosodic features may interact differently with social bias acquisition during pretraining.
- What evidence would resolve it: Replicating the same SpEAT-based bias evaluation on SSL models pretrained on multilingual corpora (e.g., MLS, VoxPopuli) or monolingual non-English datasets, comparing bias amplification and compression-based mitigation effectiveness across languages.

### Open Question 2
- Question: What semantic dimensions of social bias beyond speaker attributes (gender, age, nationality) exist in speech SSL representations?
- Basis in paper: [explicit] The authors acknowledge: "our work focuses primarily on speaker-specific traits... potentially overlooking other dimensions of bias that could exist in semantic components of speech data such as political ideology and religion."
- Why unresolved: The SpEAT framework and stimulus sets used were limited to three demographic categories; semantic content biases may manifest differently in learned representations.
- What evidence would resolve it: Extending bias evaluation frameworks to include attribute concepts related to political ideology, religion, socioeconomic status, or other semantic categories, with appropriate speech stimuli datasets.

### Open Question 3
- Question: What is the mechanistic explanation for why row pruning outperforms head pruning, weight pruning, and distillation in mitigating social bias?
- Basis in paper: [inferred] The paper demonstrates row pruning effectively reduces all bias categories while other methods show inconsistent or limited effects, but provides no theoretical or empirical analysis of why this occurs.
- Why unresolved: The experimental results are observational; the relationship between FFW row/column structures and bias-relevant representations remains unexplored.
- What evidence would resolve it: Probing experiments analyzing which neural network components encode bias-related information, layer-wise analysis of bias before/after row pruning, or ablation studies identifying specific dimensions being pruned that correlate with bias reduction.

## Limitations

- Bias evaluation relies on a single corpus (Morgan Emotional Speech Set) with predefined attribute and target concepts, limiting cross-corpus validation
- Compression experiments focus exclusively on English-language SSL models trained on LibriSpeech, with unknown performance on multilingual or dialectally diverse datasets
- The paper does not investigate whether bias mitigation through compression affects downstream task utility beyond PER metrics

## Confidence

- **High confidence**: SSL models amplify social bias relative to traditional features (supported by consistent SpEAT d differences across three model families and multiple architectures)
- **Medium confidence**: Row pruning is the most effective compression technique for bias mitigation (results show consistent Age bias reduction, though Gender/Native effects vary by model)
- **Medium confidence**: Longer pretraining steps correlate with increased bias (temporal trend observed across all configurations, though causation requires further investigation)

## Next Checks

1. Replicate SpEAT d measurements across multiple bias corpora (e.g., Common Voice, VoxCeleb) to test generalizability of amplification findings
2. Evaluate compressed model performance on non-English speech tasks to assess whether bias reduction generalizes beyond phoneme recognition
3. Conduct ablation studies isolating FFN vs attention mechanisms to determine which architectural components primarily encode social bias