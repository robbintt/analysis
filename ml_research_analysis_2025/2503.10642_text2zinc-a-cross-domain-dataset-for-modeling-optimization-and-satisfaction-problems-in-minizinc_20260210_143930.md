---
ver: rpa2
title: 'Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction
  Problems in MiniZinc'
arxiv_id: '2503.10642'
source_url: https://arxiv.org/abs/2503.10642
tags:
- problem
- data
- problems
- constraints
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce TEXT2ZINC, a cross-domain dataset for natural\
  \ language to MiniZinc modeling, combining optimization and satisfaction problems\
  \ across multiple domains. They evaluate baseline performance using various prompting\
  \ strategies\u2014vanilla, chain-of-thought, and compositional approaches\u2014\
  on the NLP4LP subset (63 problems)."
---

# Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinc

## Quick Facts
- arXiv ID: 2503.10642
- Source URL: https://arxiv.org/abs/2503.10642
- Reference count: 40
- Authors: Akash Singirikonda; Serdar Kadioglu; Karthik Uppuluri
- One-line result: Introduces TEXT2ZINC dataset for natural language to MiniZinc modeling, achieving 19-60% execution accuracy and 11-25% solution accuracy depending on prompting strategy

## Executive Summary
This paper introduces TEXT2ZINC, a cross-domain dataset for modeling optimization and satisfaction problems in MiniZinc using natural language descriptions. The authors evaluate baseline performance using various prompting strategies—vanilla, chain-of-thought, and compositional approaches—on the NLP4LP subset of 63 problems. Their experiments reveal that while LLMs can generate executable MiniZinc code in some cases, they struggle with modeling accuracy and optimization logic, achieving solution accuracy as low as 11% and peaking at 25% for chain-of-thought with examples. The results highlight the need for improved intermediate representations and modeling frameworks to bridge the gap between natural language understanding and formal constraint modeling.

## Method Summary
The authors create TEXT2ZINC by combining optimization and satisfaction problems across multiple domains, providing input specifications (description, parameters, metadata), data instances, and ground truth outputs. They evaluate three prompting strategies using GPT-4: vanilla (basic to +knowledge graph), chain-of-thought (with data, examples, and shape information), and compositional (generating parameters, constraints, objective separately then stitching). Each problem is processed through an LLM pipeline that generates MiniZinc code, which is then validated by the MiniZinc compiler and solver. Performance is measured by execution accuracy (syntactically valid models) and solution accuracy (models achieving ground truth objective values).

## Key Results
- Execution accuracy ranges from 19% to 60% depending on prompting method, with compositional approaches achieving the highest execution accuracy (60.31%)
- Solution accuracy peaks at 25% for chain-of-thought with examples, compared to 11% for knowledge graph approaches
- Knowledge graphs and additional structural information like shape details do not consistently improve performance and often degrade it
- A consistent execution-solution accuracy gap indicates LLMs frequently misinterpret optimization problems as satisfiability problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chain-of-thought (CoT) reasoning combined with concrete data examples yields the highest solution accuracy for translating natural language to constraint models.
- **Mechanism:** CoT forces the LLM to explicitly decompose the problem logic (parameters, variables, constraints) before committing to syntax. Providing concrete data examples (from `.dzn` files) anchors the abstract reasoning to specific types and structures, reducing hallucination of variable names.
- **Core assumption:** The LLM has sufficient inherent knowledge of constraint logic to correctly decompose the problem if prompted step-by-step.
- **Evidence anchors:**
  - [abstract]: "...solution accuracy peaking at 25% for chain-of-thought with examples."
  - [section]: "The effect of examples: The combination of CoT with examples produces the best solution accuracy (25.39%)... indicating that step-by-step reasoning also benefits from concrete examples."
  - [corpus]: "Gala: Global LLM Agents..." suggests agentic decomposition is a prevailing trend for this task.
- **Break condition:** Performance degrades if the reasoning chain introduces logical errors or if the provided examples conflict with the specific problem instance, leading to "syntax valid but logically incorrect" code.

### Mechanism 2
- **Claim:** A compositional approach (generating components separately and stitching them) maximizes execution accuracy (syntax validity) but may compromise solution accuracy.
- **Mechanism:** By breaking the generation task into sequential sub-tasks (Parameters -> Constraints -> Objective -> Stitch), the context window for any single generation step is reduced, lowering the probability of syntax errors like undefined identifiers or type mismatches.
- **Core assumption:** The "stitching" phase can successfully resolve interfaces between independently generated components without introducing integration bugs.
- **Evidence anchors:**
  - [abstract]: "Execution accuracy ranges from 19% to 60%... Compositional approaches... achieving the highest execution accuracy (60.31%)."
  - [section]: "...breaking down model generation into sub-tasks improves the likelihood of producing syntactically valid code."
  - [corpus]: Corpus signals are weak for specific compositional stitching failures; general consensus is decomposition helps syntax.
- **Break condition:** If variable names or types generated in the "Parameter" step do not match the usage in the "Constraint" step, the composition fails or produces a model that compiles but cannot find a solution.

### Mechanism 3
- **Claim:** Providing structural "shape" information (e.g., array dimensions) or Knowledge Graphs (KGs) often degrades performance compared to simpler nomenclature-based prompting.
- **Mechanism:** Adding structural metadata increases the complexity of the prompt context. The paper suggests an "information sweet spot" exists; too much structural detail may overwhelm the model or lead to "over-specification" where the model focuses on structure at the expense of semantic logic.
- **Core assumption:** LLMs (specifically GPT-4 as tested) may not effectively utilize abstract structural definitions (like shape tuples or graph topologies) without fine-tuning.
- **Evidence anchors:**
  - [abstract]: "Knowledge graphs and additional structural information like shape details do not consistently improve performance."
  - [section]: "Surprisingly, adding shape information degrades performance to baseline levels... suggesting that additional structural information might overwhelm the model."
  - [corpus]: No contradictory corpus evidence found; this remains a specific finding of this paper.
- **Break condition:** This mechanism implies that scaling input context complexity does not monotonically scale performance; efficient prompting requires filtering out excessive structural metadata.

## Foundational Learning

- **Concept: Declarative vs. Imperative Modeling**
  - **Why needed here:** The target language, MiniZinc, is declarative (describing *what* the problem is, not *how* to solve it). LLMs trained primarily on imperative code (Python/C++) often struggle to switch modes, leading to procedural logic errors in constraint definitions.
  - **Quick check question:** Does the generated code describe the constraints (e.g., `forall(...)`) or the solving algorithm?

- **Concept: Global Constraints**
  - **Why needed here:** MiniZinc uses high-level abstractions like `all_different` or `cumulative` which are more efficient and semantically clearer than manual decompositions. LLMs must be guided to use these rather than generating verbose, low-level binary inequalities.
  - **Quick check question:** Is the model using `alldifferent([x,y,z])` or `x != y /\ y != z /\ x != x`?

- **Concept: Solver-Agnosticism**
  - **Why needed here:** The paper highlights MiniZinc's ability to compile to FlatZinc for various backends (CP, MIP, SAT). Understanding this separation is crucial for diagnosing whether a failure is a modeling error (Logic) or a solver capability error (Backend).
  - **Quick check question:** If a model fails, is it because the constraints are unsatisfiable or because the selected solver backend doesn't support a specific variable type?

## Architecture Onboarding

- **Component map:** input.json -> Prompt Construction (Strategy: Vanilla/CoT/Compositional) -> LLM (GPT-4) -> Code Generation -> MiniZinc Compiler -> Solver -> output.json -> Evaluation
- **Critical path:** Parsing input.json to construct the prompt (specifically filtering for the "Information Sweet Spot") -> Mapping natural language parameter definitions to MiniZinc type declarations (the primary source of execution errors) -> Validating the objective function direction (Maximize vs. Minimize) against the natural language goal
- **Design tradeoffs:** CoT is better for correct logic (Solution Accuracy); Compositional is better for correct syntax (Execution Accuracy). Loading sample data helps, but loading shape metadata hurts.
- **Failure signatures:** Type Coercion Errors (mismatching int and float or array index sets), Execution-Solution Gap (code compiles but returns suboptimal value), Undefined Identifiers (hallucinated variable names in constraint block)
- **First 3 experiments:** 1) Baseline Re-establishment: Run "Basic + Data & Examples" prompt on NLP4LP subset to verify 19-25% accuracy range, 2) Hybrid Strategy Test: Attempt "Compositional CoT" to see if Solution Accuracy can approach Compositional Execution Accuracy, 3) Error Analysis Loop: Implement "Reflexion" step where compiler error message is fed back to LLM to fix specific syntax error

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can alternative intermediate representations—such as named entities, semantic graphs, or agentic frameworks—outperform knowledge graphs in translating natural language to constraint models?
- **Basis in paper:** [explicit] The Conclusion states, "Future work should explore alternative intermediate representations, including named entities, semantic graphs, and agentic frameworks, which might better capture the nuances of formulating combinatorial problems."
- **Why unresolved:** The authors found that knowledge graphs maintained execution accuracy but reduced solution accuracy, suggesting the specific structure of the intermediate representation is critical and currently suboptimal.
- **What evidence would resolve it:** Benchmarking these specific alternative representations on the TEXT2ZINC dataset to compare their solution accuracy against the current knowledge graph baseline.

### Open Question 2
- **Question:** Why does the inclusion of explicit parameter shape information degrade LLM performance, and how can this structural data be provided without overwhelming the model?
- **Basis in paper:** [inferred] The Results section notes that adding shape information caused performance to drop to baseline levels, suggesting "additional structural information might overwhelm the model or lead to over-specification."
- **Why unresolved:** It is counter-intuitive that providing correct dimensional data (shapes) hurts performance; the mechanism (e.g., attention distraction vs. token limits) is not yet understood.
- **What evidence would resolve it:** Ablation studies varying the verbosity and format of shape metadata to identify a presentation method that improves rather than reduces accuracy.

### Open Question 3
- **Question:** How can the "Execution-Solution gap" be bridged to prevent LLMs from misinterpreting optimization problems as satisfaction problems?
- **Basis in paper:** [inferred] The Discussion observes a consistent gap where execution accuracy is significantly higher than solution accuracy, noting that "LLMs occasionally misinterpret constraint optimization problems as satisfiability problems."
- **Why unresolved:** This indicates that current prompting strategies (CoT, Compositional) help with syntax but fail to instill the necessary logical intent for optimization objectives.
- **What evidence would resolve it:** Experiments involving fine-tuning or specialized prompting that forces explicit objective-function reasoning before constraint generation.

### Open Question 4
- **Question:** What specific schema designs for "code-inspired" knowledge graphs are required to translate structured knowledge into higher quality MiniZinc solutions?
- **Basis in paper:** [explicit] The Conclusion states, "While such representations [Knowledge Graphs] show potential, more research is needed to understand their best utilization."
- **Why unresolved:** The authors hypothesize that the graph must be "code-inspired" to minimize the gap to the final model, but their initial implementation resulted in lower solution accuracy (11.11%) compared to simpler methods.
- **What evidence would resolve it:** Analyzing the specific node-edge configurations that successfully map to MiniZinc global constraints versus those that generate invalid logic.

## Limitations
- Performance ceiling of 25% solution accuracy reveals fundamental limitations in LLM-based constraint modeling
- Evaluation excludes three NLP4LP problems (IDs 9, 26, 27) without clear justification for their exclusion
- Results are based on GPT-4 alone without exploring smaller or open-weight models, leaving scalability unknown

## Confidence
- **High Confidence:** The finding that compositional prompting maximizes execution accuracy (60.31%) is well-supported by experimental design and error analysis
- **Medium Confidence:** The claim that CoT with examples achieves peak solution accuracy (25.39%) is valid within tested scope but limited by small sample size (63 problems)
- **Low Confidence:** The observation that knowledge graphs and shape information degrade performance requires caution as the paper doesn't test whether different prompt formulations or model fine-tuning could leverage this structural information effectively

## Next Checks
1. **Cross-Model Generalization:** Replicate core experiments (CoT + Examples) using an open-weight model like Llama 3-70B to assess whether the performance ceiling is model-specific or reflects fundamental limitations in LLM-constraint modeling
2. **Error Recovery Analysis:** Implement the proposed "Reflexion" step where MiniZinc compiler error messages are fed back to the LLM for correction, measuring the percentage of execution failures that can be automatically repaired
3. **Hybrid Strategy Evaluation:** Test the "Compositional CoT" approach combining both strategies to determine if solution accuracy can be improved while maintaining high execution rates, addressing the execution-solution accuracy gap identified in the paper