---
ver: rpa2
title: 'Beyond Training-time Poisoning: Component-level and Post-training Backdoors
  in Deep Reinforcement Learning'
arxiv_id: '2507.04883'
source_url: https://arxiv.org/abs/2507.04883
tags:
- backdoor
- attacks
- trigger
- episodic
- infrectrorl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrojanentRL and InfrectroRL, two novel backdoor
  attacks for Deep Reinforcement Learning (DRL) that operate under significantly reduced
  adversarial privileges compared to existing approaches. TrojanentRL embeds backdoors
  in the DRL rollout buffer component, making them persistent even through full model
  retraining, while InfrectroRL injects backdoors into pretrained models without requiring
  training data access.
---

# Beyond Training-time Poisoning: Component-level and Post-training Backdoors in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.04883
- Source URL: https://arxiv.org/abs/2507.04883
- Reference count: 40
- Two novel DRL backdoor attacks (TrojanentRL, InfrectroRL) achieve >70% AER and >85% ASR across six Atari environments

## Executive Summary
This paper introduces two novel backdoor attacks for Deep Reinforcement Learning that operate under significantly reduced adversarial privileges compared to existing approaches. TrojanentRL embeds backdoors in the DRL rollout buffer component, making them persistent even through full model retraining, while InfrectroRL injects backdoors into pretrained models without requiring training data access. Both attacks achieve effectiveness rates exceeding 70% and success rates above 85% across six Atari environments, rivaling state-of-the-art training-time attacks. The findings reveal critical vulnerabilities across the DRL supply chain and challenge the prevailing focus on training-time attacks, highlighting the urgent need for robust defenses against component-level and post-training backdoors.

## Method Summary
The paper presents two complementary backdoor attack methodologies. TrojanentRL exploits the rollout buffer as a Trojan vector by inserting carefully crafted state-action pairs during training, which persist through subsequent fine-tuning or retraining. InfrectroRL introduces a weight manipulation technique that embeds backdoors directly into pretrained policy networks without access to training data, relying instead on solving an optimization problem that preserves the original agent's performance while introducing trigger-based misbehavior. Both attacks are evaluated across six Atari environments using PPO agents, with InfrectroRL additionally tested against two leading DRL backdoor defenses to demonstrate evasiveness.

## Key Results
- TrojanentRL achieves AER >70% and ASR >85% across six Atari environments while persisting through full model retraining
- InfrectroRL successfully embeds backdoors into pretrained models without training data access, achieving comparable performance to training-time attacks
- InfrectroRL demonstrates robust evasion against SHINE and BIRD, two leading DRL backdoor defenses
- Both attacks operate under significantly reduced adversarial privileges compared to existing training-time poisoning approaches

## Why This Works (Mechanism)
The attacks exploit fundamental vulnerabilities in the DRL pipeline by targeting components beyond the training phase. TrojanentRL leverages the rollout buffer's role as a persistent data source that influences future training iterations, allowing backdoors to survive full retraining cycles. InfrectroRL manipulates the weight space of pretrained models through an optimization process that preserves base functionality while introducing trigger-dependent behavior. The key insight is that DRL models are particularly vulnerable to backdoor attacks because their training process is iterative and data-driven, with persistent components (like buffers) and flexible weight representations that can encode hidden behaviors.

## Foundational Learning
- **Deep Reinforcement Learning pipeline**: Understanding the components (agent, environment, buffer, training loop) is essential because the attacks target specific elements beyond just the model weights
- **Rollout buffer persistence**: Critical for grasping why TrojanentRL backdoors survive retraining - the buffer acts as a Trojan horse that seeds future learning
- **Policy network weight optimization**: Necessary to understand InfrectroRL's approach of manipulating pretrained weights while preserving base performance
- **Trigger-based behavior injection**: Core concept for both attacks - introducing state-dependent misbehavior that activates under specific conditions
- **Lipschitz continuity in neural networks**: Important for InfrectroRL's theoretical evasiveness guarantees and the limitations in extending them to CNNs
- **Adversarial privileges in DRL supply chain**: Framework for understanding the reduced requirements of these attacks compared to traditional training-time poisoning

## Architecture Onboarding
- **Component map**: Environment -> Agent (Policy Network) -> Rollout Buffer -> Training Loop -> Updated Agent
- **Critical path**: The attack surface includes the rollout buffer (TrojanentRL) and pretrained model weights (InfrectroRL), both feeding into the policy network that interfaces with the environment
- **Design tradeoffs**: TrojanentRL prioritizes persistence over stealth in buffer modification, while InfrectroRL balances performance preservation with backdoor effectiveness
- **Failure signatures**: Unusual state-action pairs in buffers, degraded performance on non-trigger states, or anomalous weight patterns in pretrained models
- **First experiments**: 1) Verify buffer injection maintains base performance while introducing trigger behavior, 2) Test weight manipulation preserves original reward structure, 3) Validate persistence through multiple retraining cycles

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the theoretical evasiveness guarantees for InfrectroRL (Theorem 2) be extended to Convolutional Neural Networks (CNNs)?
- Basis: [explicit] The authors state extending constraints to architectures "other than MLPs, involving, e.g., convolutions... is not particularly straightforward, and topic for future research."
- Why unresolved: The current proof relies on assumptions regarding 1-hidden-layer MLPs and Lipschitz continuity that do not trivially transfer to convolutional structures.
- What evidence would resolve it: A formal extension of the performance upper-bound derivation applicable to convolutional layers, or empirical verification of the bound on CNN-based agents.

### Open Question 2
- Question: Can neuron activation analysis provide a robust defense against InfrectroRL where observation-based methods fail?
- Basis: [explicit] The authors "advocate neuron activation analysis... as a more promising direction" after demonstrating existing defenses fail.
- Why unresolved: The paper demonstrates that state-of-the-art observation-based defenses (SHINE, BIRD) are ineffective, but the proposed alternative direction remains untested.
- What evidence would resolve it: Empirical evaluation of neuron activation-based anomaly detectors against InfrectroRL-compromised models.

### Open Question 3
- Question: Is InfrectroRL effective in environments with continuous action spaces or value-based methods like DQN?
- Basis: [inferred] The evaluation is limited to discrete Atari environments using PPO, yet the problem formulation claims applicability to any policy network.
- Why unresolved: The "backdoor switch" mechanism optimizes for discrete target actions; it is unclear if the weight manipulation strategy holds for continuous distributions or value estimations.
- What evidence would resolve it: Successful application of InfrectroRL on standard continuous control benchmarks (e.g., MuJoCo) or Deep Q-Networks with comparable Attack Success Rates.

## Limitations
- Evaluation confined to discrete Atari environments, raising questions about generalizability to continuous control tasks or real-world robotics applications
- While InfrectroRL claims robustness against two defenses, the defenses tested may not represent the full spectrum of potential countermeasures
- The persistence of TrojanentRL backdoors through retraining assumes full access to the original malicious rollout buffer, which may not always be available in practice

## Confidence
- Attack effectiveness in controlled Atari environments: **High**
- Generalization to other DRL domains: **Medium**
- Robustness against tested defenses: **Medium**
- Feasibility of required adversarial privileges: **Low**

## Next Checks
1. Evaluate both attacks on continuous control benchmarks (e.g., MuJoCo, PyBullet) to assess domain transferability
2. Test against a broader suite of DRL backdoor defenses, including those targeting buffer integrity and model provenance verification
3. Investigate detection strategies through behavioral analysis of agent performance during both normal and backdoor-triggered conditions