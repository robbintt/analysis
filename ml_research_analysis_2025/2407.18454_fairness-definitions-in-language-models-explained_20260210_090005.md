---
ver: rpa2
title: Fairness Definitions in Language Models Explained
arxiv_id: '2407.18454'
source_url: https://arxiv.org/abs/2407.18454
tags:
- bias
- fairness
- arxiv
- language
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a systematic survey clarifying fairness definitions
  in Language Models (LMs) across three transformer architectures: encoder-only, decoder-only,
  and encoder-decoder. It categorizes fairness notions into intrinsic and extrinsic
  biases, with intrinsic bias measured through embedding associations and extrinsic
  bias observed in downstream task performance.'
---

# Fairness Definitions in Language Models Explained

## Quick Facts
- arXiv ID: 2407.18454
- Source URL: https://arxiv.org/abs/2407.18454
- Reference count: 40
- One-line primary result: Systematic survey categorizing fairness definitions across transformer architectures, showing consistent bias presence and need for tailored assessments.

## Executive Summary
This paper provides a systematic survey clarifying fairness definitions in Language Models (LMs) across three transformer architectures: encoder-only, decoder-only, and encoder-decoder. It categorizes fairness notions into intrinsic and extrinsic biases, with intrinsic bias measured through embedding associations and extrinsic bias observed in downstream task performance. The survey introduces a comprehensive taxonomy and illustrates each definition through experiments using metrics like WEAT, SEAT, CEAT for similarity-based bias, LPBS and PLL for probability-based bias, and fairness notions like equal opportunity, counterfactual fairness, and position-based disparity. Empirical results show consistent presence of bias across multiple benchmarks, highlighting the need for tailored fairness assessments.

## Method Summary
The paper evaluates fairness definitions through a systematic survey approach, analyzing public benchmarks including Caliskan et al., StereoSet, CrowS-Pairs, WinoBias, BBQ, XNLI, and Natural Questions. The method involves running specific bias metrics on pre-trained frozen models (BERT, RoBERTa, GPT-2, LLaMA-2, T5, mBART) without model training, comparing results across architectures to validate the proposed taxonomy. The approach focuses on reproducing specific bias scores and effect sizes to demonstrate how different transformer architectures require different fairness measurement approaches.

## Key Results
- Consistent presence of bias across multiple benchmarks, with decoder-only models showing stereotypical bias through likelihood scoring while encoder-only models reveal bias through embedding geometry
- Architectural differences fundamentally determine which fairness metrics are valid - encoder-only models can use bidirectional embedding tests while decoder-only models require autoregressive probability-based metrics
- Counterfactual fairness interventions show varying effectiveness across architectures, with encoder-decoder models potentially amplifying existing biases during generation tasks

## Why This Works (Mechanism)

### Mechanism 1: Architecture-Constraint Alignment
Fairness definitions are only effective when their mathematical formulation matches the model's structural constraints (bidirectional vs. autoregressive). Encoder-only models (e.g., BERT) allow direct access to full input context via bidirectional attention, enabling "Similarity-based" metrics (WEAT/SEAT) that measure geometric relationships in embedding space. Conversely, decoder-only models (e.g., GPT) restrict access to past context (causal masking), requiring "Probability-based" or "Prompt-based" metrics that measure likelihood of token generation rather than static spatial relationships. The chosen metric must respect the visibility mask of the attention layer; measuring bias in unseen future tokens is structurally impossible without generation.

### Mechanism 2: Intrinsic vs. Extrinsic Proxy Fidelity
Intrinsic bias metrics serve as fast, localized proxies for downstream harms, but they rely on the assumption that internal representation geometry linearly correlates with external behavior. Intrinsic metrics (e.g., WEAT) probe the static embedding space while extrinsic metrics (e.g., Equal Opportunity) probe the dynamic decision boundary. This works because bias is hypothesized to propagate from the embedding layer (representation) to the classification head (decision). A "biased" direction in the embedding vector space is assumed to directly cause a "biased" output probability distribution. If intrinsic bias metrics do not correlate with application bias, the proxy fails and must be discarded as a reliable signal for downstream harm.

### Mechanism 3: Counterfactual Intervention
Fairness can be operationalized by measuring the model's sensitivity to "surgical" interventions on sensitive attributes while holding semantic content constant. This uses causal logic: if S is a sensitive attribute (e.g., Gender), a model is fair if predictions remain invariant when S is swapped. In decoder-only models, this is measured via Change Rate (CR) - the proportion of times the prediction flips when "he" is swapped for "she". The linguistic perturbation is assumed to perfectly isolate the sensitive attribute without altering syntactic validity or semantic priors of the prompt. If token swaps change grammatical structure (e.g., gendered languages where articles/adjectives must agree), the "bias" signal is confounded by perplexity changes due to grammar errors.

## Foundational Learning

- **Concept: Transformer Attention Mechanisms (Self vs. Cross)**
  - Why needed here: To select the correct fairness metric family. You must know if a model can "look ahead" (Encoder) or only "looks back" (Decoder) to know if embedding-based tests are valid.
  - Quick check question: Can you explain why applying a bidirectional embedding test (WEAT) to a GPT-style model's raw embeddings is structurally misaligned?

- **Concept: Word Embedding Geometry (Cosine Similarity)**
  - Why needed here: Fundamental to Intrinsic Bias definitions. The paper relies on the geometric concept that related concepts cluster closer in vector space (cos(t, a)).
  - Quick check question: If "Doctor" and "Man" have a cosine similarity of 0.8, while "Doctor" and "Woman" have 0.4, what does the WEAT effect size indicate?

- **Concept: Log-Likelihood Scoring in LMs**
  - Why needed here: Required for Probability-based disparity. Understanding how to compute P(w_i|S) or Pseudo-Log-Likelihood (PLL) is necessary to quantify stereotypical preferences.
  - Quick check question: How does the Masked Language Modeling (MLM) objective allow us to probe bias in a BERT model by looking at the probability of specific tokens filling a [MASK]?

## Architecture Onboarding

- **Component map:** The Taxonomy (Figure 1, Page 4) maps: Encoder-only → Similarity/Probability-based (Intrinsic) + Equal Opportunity/Fair Inference (Extrinsic); Decoder-only → Attention/Stereotypical (Intrinsic) + Counterfactual/Demographic (Extrinsic); Encoder-Decoder → Algorithmic/Stereotypical (Intrinsic) + Position-based/Individual (Extrinsic).

- **Critical path:**
  1. Identify Architecture: Is your target model BERT-like, GPT-like, or T5-like?
  2. Select Bias Type: Do you have access to weights (Intrinsic) or only an API (Extrinsic)?
  3. Run Benchmark: Use the specific datasets listed in Table 2 (Page 7).

- **Design tradeoffs:**
  - Metrics: SEAT is faster but noisier; CEAT is robust but computationally expensive.
  - Data: CrowS-Pairs measures likelihood of stereotypes but suffers from information loss during masking.

- **Failure signatures:**
  - High Variance: If PLL scores fluctuate wildly across templates, the model may be insensitive to the specific bias trigger, or the prompt design is poor.
  - Positional Bias: In Encoder-Decoder models, if summaries strictly focus on the first 3 sentences, it triggers Position-based Disparity.

- **First 3 experiments:**
  1. Encoder-Only Intrinsic: Run WEAT/SEAT on BERT (Base) using the Caliskan et al. dataset to measure gender/career associations (Table 3).
  2. Decoder-Only Extrinsic: Run Counterfactual Fairness (CR) on LLaMA-2 using the German Credit dataset to see if credit risk predictions change based on gender names (Table 9).
  3. Encoder-Decoder Extrinsic: Run Normalized Position Disparity (NPD) on mBART using the XSum dataset to detect if the summarizer ignores the end of articles (Table 14).

## Open Questions the Paper Calls Out

### Open Question 1
How should fairness definitions be extended to account for the compounding effects of bias across different modalities, such as text and vision, in Multimodal LLMs? The authors identify fairness in multimodal architectures as a research gap where "research on fairness and bias in MLLMs remains limited" due to complex, compounding information. Integrating modality-specific encoders (e.g., vision) introduces biases that interact with textual ones in ways current single-modality definitions do not capture. Development of a multimodal benchmark where protected attributes are distributed across visual and textual inputs, demonstrating how cross-modal interactions exacerbate bias, would resolve this.

### Open Question 2
What evaluation frameworks are needed to assess intersectional biases effectively, given that current methods mostly evaluate sensitive attributes in isolation? The paper highlights that there is "relatively sparse work" addressing intersectional identities, as current notions often mitigate specific attributes (e.g., gender or race) independently. A model may appear fair when considering attributes independently but show significant bias at intersections (e.g., Black women), which isolated metrics miss. Creation of specific test sets targeting attribute intersections and new metrics that quantify bias amplification when multiple protected attributes co-occur in the input would resolve this.

### Open Question 3
How can models balance strict adherence to fairness definitions with the preservation of learned knowledge and language integrity? The authors note that "aggressive bias mitigation... can lead to overfitting" and undermine the model's language comprehension and general capabilities. Optimizing strictly for fairness often degrades performance on general tasks, creating a conflict between ethical alignment and functional utility. Empirical studies measuring the correlation between fairness score improvements and drops in accuracy on complex reasoning benchmarks (e.g., MMLU) across different mitigation techniques would resolve this.

## Limitations
- The paper lacks direct causal validation between intrinsic and extrinsic bias metrics, with empirical evidence linking internal representation geometry to downstream behavior remaining largely correlational.
- The effectiveness of counterfactual interventions may be confounded by grammatical dependencies in gendered languages, as the assumption of perfect linguistic isolation of sensitive attributes may not hold across languages.
- The survey does not address how bias evolves across fine-tuning stages or how different training objectives (MLM vs. causal language modeling) fundamentally shape bias emergence patterns.

## Confidence

- **High Confidence**: The architectural constraint alignment mechanism (Mechanism 1) is structurally sound - encoder-only models cannot be evaluated with probability-based metrics that require future context visibility, and decoder-only models cannot use bidirectional embedding similarity tests.
- **Medium Confidence**: The intrinsic vs. extrinsic proxy fidelity relationship (Mechanism 2) shows theoretical consistency but lacks direct empirical validation of the correlation assumption.
- **Low Confidence**: The counterfactual intervention mechanism (Mechanism 3) assumes perfect linguistic isolation of sensitive attributes, which may not hold across languages with grammatical gender agreement.

## Next Checks

1. **Correlation Validation**: Systematically measure the Pearson correlation coefficient between intrinsic WEAT/SEAT scores and extrinsic Equal Opportunity metrics across multiple downstream tasks for the same model to test the proxy fidelity assumption.

2. **Cross-Lingual Intervention**: Apply the counterfactual fairness test to a morphologically rich language (e.g., Spanish or German) and measure whether grammatical agreement changes confound the bias signal compared to English results.

3. **Architectural Ablation**: Compare bias metrics before and after fine-tuning identical model weights on different objectives (MLM vs. causal LM) to determine how training objectives fundamentally shape bias emergence patterns.