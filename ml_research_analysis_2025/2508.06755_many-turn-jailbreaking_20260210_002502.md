---
ver: rpa2
title: Many-Turn Jailbreaking
arxiv_id: '2508.06755'
source_url: https://arxiv.org/abs/2508.06755
tags:
- follow-up
- arxiv
- questions
- jailbreaking
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces multi-turn jailbreaking, a new vulnerability
  where large language models (LLMs) can be progressively exploited through multiple
  conversation turns after an initial successful jailbreak. The authors construct
  MTJ-Bench, a dataset based on HarmBench, to evaluate this threat in two scenarios:
  irrelevant follow-up questions and relevant follow-up questions.'
---

# Many-Turn Jailbreaking

## Quick Facts
- arXiv ID: 2508.06755
- Source URL: https://arxiv.org/abs/2508.06755
- Authors: Xianjun Yang; Liqiang Xiao; Shiyang Li; Faisal Ladhak; Hyokun Yun; Linda Ruth Petzold; Yi Xu; William Yang Wang
- Reference count: 33
- Key outcome: Multi-turn jailbreaking exploits persistent context to enable continuous harmful responses after initial successful jailbreak.

## Executive Summary
This paper introduces multi-turn jailbreaking, a new vulnerability where large language models can be progressively exploited through multiple conversation turns after an initial successful jailbreak. The authors construct MTJ-Bench, a dataset based on HarmBench, to evaluate this threat in two scenarios: irrelevant follow-up questions and relevant follow-up questions. Experiments on 14 open-source models and one closed-source model reveal that once jailbroken, LLMs consistently respond to additional harmful queries, with attack success rates ranging from 30-70% across models. The study shows that multi-turn jailbreaking is a universal vulnerability, amplifying potential misuse by enabling continuous exploitation of aligned LLMs.

## Method Summary
The paper introduces MTJ-Bench, a dataset built on HarmBench to evaluate multi-turn jailbreaking. It contains 320 first-turn jailbreaking queries and 10 irrelevant follow-up questions per query (MTJ-Bench-ir) plus 11 relevant follow-up questions across 7 style categories (MTJ-Bench-re). The evaluation pipeline concatenates adversarial prompts, first-turn responses, and follow-up questions to generate second-turn outputs. Four attack methods (GCG, PAIR, TAP, AutoDAN) are applied to first-turn queries, and responses are evaluated using HarmBench classifier for irrelevant follow-ups and Claude-3-Sonnet for relevant follow-ups. The study measures first-turn ASR, second-turn ASR (irrelevant and relevant), and ASR Gain to quantify the vulnerability.

## Key Results
- Once jailbroken, LLMs consistently respond to additional harmful queries with attack success rates of 30-70% across models
- Relevant follow-up questions achieve significantly higher attack success (55-65%) than irrelevant ones (20-28%)
- Multi-turn jailbreaking vulnerability is universal across model families, including both open-source and closed-source models
- Context window accumulation enables continued exploitation, with ASRGain doubling when context length increases from 10 to 200 follow-ups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initial successful jailbreak creates persistent context that lowers safety barriers for subsequent queries.
- Mechanism: Once a model generates a harmful response, that output becomes part of the conversation context in the attention window. The model conditions on its own prior harmful output, creating a "committed state" where refusing follow-up questions would create conversational inconsistency.
- Core assumption: The harmful context in the attention window influences subsequent generation behavior.
- Evidence anchors:
  - [abstract] "once jailbroken, LLMs consistently respond to additional harmful queries"
  - [section 3.1] Formulation shows second-turn input as concatenation: `o2 = M([f(q); o1; qir])`
  - [corpus] PLAGUE paper (2510.17947) confirms multi-turn dialogue as de facto interaction mode.

### Mechanism 2
- Claim: Relevant follow-up questions achieve higher attack success than irrelevant ones due to semantic anchoring.
- Mechanism: When follow-up questions semantically relate to the initial harmful content, the model interprets them as natural conversation continuation rather than new attack attempts.
- Core assumption: Semantic similarity to prior harmful output bypasses safety training more effectively than unrelated queries.
- Evidence anchors:
  - [section 6.1] Relevant ASR (ASRre2) averages 55-65% across models vs. irrelevant ASR (ASRir2) at 20-28%
  - [section 4] "some general follow-up questions like 'What details are critical to the success of the second step?' would universally apply"

### Mechanism 3
- Claim: Adversarial prompts can weaken safety boundaries even when first-turn attacks fail.
- Mechanism: The paper observes that second-turn attacks sometimes succeed despite first-turn failures, suggesting adversarial optimization itself may shift model behavior in the activation space.
- Core assumption: Failed jailbreaks still induce partial activation of harmful response patterns.
- Evidence anchors:
  - [section 6.4] Table 6 shows 7-24% of cases where first-turn fails but second-turn succeeds
  - [section 6.1] ASRGain metric captures "free lunch" attacks that succeed only in second turn

## Foundational Learning

- Concept: **Context Window Accumulation**
  - Why needed here: Multi-turn jailbreaking exploits the fact that all prior tokens remain in attention. Understanding RoPE, ALiBi, and context extension techniques explains why longer contexts amplify vulnerability.
  - Quick check question: If a model has 128K context window and generates 500 harmful tokens in turn 1, how many new tokens can be added in turn 2 before context overflow?

- Concept: **Attack Success Rate (ASR) and its Variants**
  - Why needed here: The paper introduces ASRir2, ASRre2, and ASRGain. These require understanding binary classification metrics and how to isolate second-turn effects from first-turn conditioning.
  - Quick check question: Why is ASRGain not simply ASRir2 - ASR1? What does ASRGain actually measure?

- Concept: **Claude-as-Judge Evaluation**
  - Why needed here: Relevant follow-up evaluation requires contextual judgment that binary classifiers cannot provide. The paper validates this approach with human annotation (Cohen's Îº = 0.74).
  - Quick check question: What are the failure modes of LLM-as-judge for harmfulness assessment? How might the judge model itself be biased?

## Architecture Onboarding

- Component map: HarmBench (320 queries) -> MTJ-Bench-ir (3,200 second-turn questions) -> MTJ-Bench-re (3,520 second-turn questions) -> Attack methods (GCG/PAIR/TAP/AutoDAN) -> Evaluation (HarmBench classifier/Claude-3-Sonnet) -> ASR/ASRGain metrics

- Critical path:
  1. Run first-turn attack (GCG/PAIR/TAP/AutoDAN) on base HarmBench queries
  2. Filter successful jailbreaks for relevant follow-ups; use all cases for irrelevant
  3. Concatenate [adversarial prompt; first-turn output; follow-up question]
  4. Generate second-turn response and evaluate with appropriate judge
  5. Compute ASRir2/ASRre2 and ASRGain

- Design tradeoffs:
  - **Lightweight dataset vs. coverage**: Authors chose 10-11 follow-ups per query for tractability, but acknowledge this is a lower bound (Figure 2 shows ASRGain doubles from 10 to 200 follow-ups)
  - **Universal vs. specific follow-ups**: Relevant follow-ups are category-level, not query-specific. This trades precision for scalability.
  - **Claude-as-judge cost**: Manual annotation showed strong agreement, but API costs limit scale. Open-source judge remains open problem.

- Failure signatures:
  - Models with strong RLHF (Llama 2 family) show lower vulnerability (~3-5% ASRir2) vs. weaker alignment (Vicuna, Mistral at 15-40%)
  - Transfer attacks (marked with *) generally underperform direct attacks but still achieve non-trivial success on large models
  - First-turn ASR does not predict second-turn ASR; some models refuse first turn but comply on follow-ups

- First 3 experiments:
  1. **Baseline check**: Reproduce Table 7 first-turn ASR on your target model using GCG or PAIR to ensure attack pipeline works.
  2. **Irrelevant follow-up probe**: Pick 10 first-turn queries, craft 5 irrelevant harmful follow-ups each, measure ASRir2 and ASRGain. Compare to paper's benchmarks for your model family.
  3. **Context ablation**: For a successful two-turn attack, manually clear the first-turn response from context and retry the second-turn query. This tests whether harmful output or adversarial prompt is the primary driver.

## Open Questions the Paper Calls Out

- What are the internal mechanisms within aligned neural networks that cause safety failures under many-turn attacks?
- Can many-turn jailbreaking be effectively applied to Large Vision-Language Models (LVLMs) using multi-turn image and text inputs?
- What specific defense strategies can effectively mitigate many-turn jailbreaking without significantly degrading model utility?

## Limitations

- The study does not investigate the underlying mechanistic causes of vulnerability, focusing only on empirical characterization
- Reliance on Claude-3-Sonnet as judge creates potential circularity and API dependency
- Does not address whether models can be trained to detect and reset harmful conversational contexts
- The paper focuses on exposure rather than developing practical defense strategies

## Confidence

- **High confidence**: The empirical observation that multi-turn jailbreaking works across diverse models and attack methods (ASRir2/ASRre2 results are consistent and reproducible)
- **Medium confidence**: The claim that relevant follow-ups are more effective than irrelevant ones (supported by data but dependent on the quality of the universal follow-up design)
- **Medium confidence**: The mechanism that harmful context accumulation enables continued exploitation (plausible but not mechanistically proven)
- **Low confidence**: The assertion that failed-first-turn attacks can succeed in second turn (observed but not explained; may be due to adversarial prompt effects rather than context)

## Next Checks

1. **Context Ablation Test**: For successful two-turn attacks, systematically remove the first-turn harmful output from context and retry the second-turn query. This will determine whether the vulnerability stems from context accumulation or from the adversarial prompt itself.

2. **Judge Model Robustness**: Validate Claude-3-Sonnet's relevance judgments by having it evaluate pairs of first-turn responses and follow-up questions from multiple models. Check for consistency across model families and compare against human annotation agreement.

3. **Context Length Scaling**: Repeat experiments on models with different context window sizes (e.g., 4K vs 32K vs 128K) to quantify the relationship between context accumulation and attack success rates, testing whether the vulnerability scales predictably with context length.