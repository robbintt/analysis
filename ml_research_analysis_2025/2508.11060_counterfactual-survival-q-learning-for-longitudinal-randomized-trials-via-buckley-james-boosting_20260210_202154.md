---
ver: rpa2
title: Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley
  James Boosting
arxiv_id: '2508.11060'
source_url: https://arxiv.org/abs/2508.11060
tags:
- treatment
- survival
- stage
- boosting
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Buckley-James Boost Q-Learning, a framework
  for estimating optimal dynamic treatment regimes under right-censored survival data
  in longitudinal randomized clinical trials. The method integrates Buckley-James
  imputation with boosting techniques (componentwise least squares and regression
  trees) within a counterfactual Q-learning framework, avoiding restrictive proportional
  hazards assumptions.
---

# Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting

## Quick Facts
- arXiv ID: 2508.11060
- Source URL: https://arxiv.org/abs/2508.11060
- Reference count: 5
- Authors: Jeongjin Lee; Jong-Min Kim
- Primary result: Introduces BJ-Boost Q-Learning for optimal DTRs under right-censored survival data, achieving up to 91% treatment decision accuracy versus 45% for Cox-based methods.

## Executive Summary
This paper introduces Buckley-James Boost Q-Learning, a framework for estimating optimal dynamic treatment regimes under right-censored survival data in longitudinal randomized clinical trials. The method integrates Buckley-James imputation with boosting techniques (componentwise least squares and regression trees) within a counterfactual Q-learning framework, avoiding restrictive proportional hazards assumptions. Simulation studies with 500–1000 patients under 50% censoring show BJ-Tree achieving up to 91% accuracy in treatment decisions, outperforming Cox-based methods (median ~45%). Real data analysis on the ACTG175 HIV trial further validates improved survival estimation under combination therapies. The framework robustly captures nonlinear covariate-treatment interactions and is particularly effective in multistage settings where bias can accumulate.

## Method Summary
The framework combines Buckley-James imputation with boosting algorithms to handle right-censored survival data in multistage Q-learning. It uses accelerated failure time models to iteratively impute censored observations via Kaplan-Meier residual estimation, then applies componentwise least squares or regression trees to estimate Q-functions recursively from final to initial stages. The method constructs pseudo-outcomes at each stage by combining immediate survival with maximum future Q-values, enabling backward induction to find optimal treatment sequences.

## Key Results
- BJ-Tree achieved up to 91% treatment decision accuracy versus 45% for Cox-based methods in simulation studies
- Framework avoids restrictive proportional hazards assumptions through AFT-based Buckley-James imputation
- Real data analysis on ACTG175 HIV trial validates improved survival estimation under combination therapies
- Successfully captures nonlinear covariate-treatment interactions missed by linear approaches

## Why This Works (Mechanism)

### Mechanism 1
The framework enables unbiased estimation of survival times under right-censoring without relying on proportional hazards assumptions. The Accelerated Failure Time (AFT) based Buckley-James (BJ) estimator iteratively imputes censored observations ($Y^*_{i,k}$). It replaces censored times with the conditional expectation of the failure time given survival past the censoring point, estimated via the Kaplan-Meier curve of residuals ($\hat{F}$), rather than dropping or arbitrarily filling them. Core assumption: Failure time $T$ and censoring time $C$ are conditionally independent given covariates.

### Mechanism 2
Boosting with regression trees captures non-linear covariate-treatment interactions better than linear or Cox-based approaches. Instead of fitting a single global model, the algorithm fits a sequence of "weak" regression trees to the current residuals ($U_{i,m,k}$). It aggregates them ($\hat{f}_{m+1} = \hat{f}_m + \nu g_m$) to progressively refine the shape of the survival function, allowing for complex decision boundaries in the Q-function. Core assumption: The true relationship between covariates and log-survival time is non-linear and can be approximated by additive tree expansions.

### Mechanism 3
Recursive Q-learning finds the optimal sequence of treatments by backwardly accumulating survival value. The framework solves the multi-stage problem using backward induction. At the final stage $K$, the Q-function is the imputed survival time. For earlier stages $k$, it creates a "pseudo-outcome" ($\tilde{Y}_{i,k}$) equal to the immediate survival plus the maximum future Q-value. The model at stage $k$ is then trained to predict this pseudo-outcome. Core assumption: Unconfoundedness (treatment assignment is independent of potential outcomes given history) and Positivity (all treatments have a non-zero probability of being assigned).

## Foundational Learning

- **Right-Censoring in Survival Analysis:** Standard regression cannot handle outcomes where the "event" hasn't happened yet (censored). The entire BJ mechanism is built to solve this. Quick check: Why can't we simply discard patients who are lost to follow-up before the study ends?
- **Accelerated Failure Time (AFT) Models:** The paper explicitly contrasts AFT with Cox Proportional Hazards. AFT models log-survival time directly, which is necessary for the "interpreted" Q-values used here. Quick check: Does an AFT model predict the *hazard rate* or the *expected time* until an event?
- **Recursive Backward Induction (Q-Learning):** To optimize a *sequence* of treatments (Dynamic Treatment Regimes), one must start from the end and work backward, adding future value to current decisions. Quick check: In a 3-stage trial, do you estimate the Stage 1 policy first, or the Stage 3 policy first?

## Architecture Onboarding

- **Component map:** Longitudinal dataset $(X_{i,k}, A_{i,k}, Y_{i,k}, \delta_{i,k})$ -> BJ-Boosting Block (residuals → weak learners → function update → censored imputation) -> Q-Learning Block (pseudo-outcome construction using max Q-value) -> Optimal decision rules
- **Critical path:** 1) Initialize at Stage K. 2) Run BJ-Tree on observed/imputed survival to get $Q_K$. 3) Step to Stage K-1. 4) Construct pseudo-outcome ($Y^*_{k-1} + \max Q_k$). 5) Run BJ-Tree on pseudo-outcome to get $Q_{K-1}$. 6) Repeat until Stage 1.
- **Design tradeoffs:** BJ-LS vs. BJ-Tree: Use LS for variable selection/interpretability; use Tree for accuracy with non-linear data (recommended default). Learning Rate ($\nu$): Lower $\nu$ (e.g., 0.1) requires more iterations $M$ but provides better regularization/stability.
- **Failure signatures:** Low Decision Accuracy (~45%): Indicates random coin flip behavior; check if Cox-PH was used by mistake. Non-convergence: Linear BJ may fail with non-linear data; switch to BJ-Tree. Variance Explosion in Pseudo-outcomes: If $Q_{k+1}$ estimates are extreme, they destabilize $Q_k$.
- **First 3 experiments:** 1) Sanity Check (Single-Stage): Replicate single-stage simulation to verify BJ-Tree achieves >90% accuracy vs Cox's ~45%. 2) Sensitivity Analysis: Increase censoring rates (e.g., from 30% to 70%) in simulations to quantify degradation of Q-value estimates. 3) Real Data Validation: Apply to ACTG175 and confirm algorithm identifies combination therapy (11) as superior to monotherapy (00).

## Open Questions the Paper Calls Out

- How does the accuracy of the BJ Boost Q-Learning framework degrade or persist under extreme censoring rates (e.g., >50%) compared to the moderate (50%) conditions tested? The Discussion states that "the framework's reliability under higher levels of censoring has not been systematically assessed" and notes that "excessive censoring could degrade the quality of the imputed values." This remains unresolved as simulation studies were limited to approximately 50% censoring.
- Can the BJ Boost Q-Learning framework be successfully extended to observational studies by integrating causal adjustment methods like Inverse Probability of Treatment Weighting (IPTW)? The Discussion notes that "this study assumes randomized treatment assignment" and explicitly suggests the framework "should be extended to incorporate causal adjustment techniques such as [IPTW] or propensity score matching" for real-world data. This remains unresolved as the current implementation relies on unconfoundedness assumptions inherent in randomized trials.
- What specific regularization techniques or early stopping criteria are optimal for mitigating early-stage bias propagation in multistage boosting? The authors state that "early-stage bias may be propagated through successive boosting iterations" and suggest "investigating regularization techniques or early stopping criteria may improve robustness." This remains unresolved as the paper demonstrates bias accumulation in multistage settings but does not define specific protocols to prevent model misspecification bias from compounding across stages.

## Limitations
- Relies on strong causal assumptions (unconfoundedness and positivity) that may be violated in observational settings
- BJ imputation procedure can become unstable with high censoring rates (>70%), potentially leading to biased Q-function estimates
- Performance in longer horizons with accumulating error propagation remains untested beyond 2-stage trials
- Computational cost of boosting with regression trees may be prohibitive for very large datasets

## Confidence

- **High Confidence:** The simulation results showing BJ-Tree outperforming Cox-based methods (91% vs 45% accuracy) are well-supported by the methodology and reproducible code.
- **Medium Confidence:** Real data validation on ACTG175 is limited to qualitative conclusions about therapy combinations; quantitative comparisons to alternative methods are absent.
- **Medium Confidence:** The claim of avoiding restrictive proportional hazards assumptions is valid for the imputation step, but the overall framework's robustness to various data-generating processes needs broader testing.

## Next Checks
1. **High Censoring Sensitivity:** Systematically increase censoring rates from 30% to 80% in simulations to quantify degradation in Q-value estimates and decision accuracy.
2. **Multi-Stage Scalability:** Extend simulations beyond 2 stages (e.g., K=4) to evaluate error accumulation and performance stability in longer treatment sequences.
3. **Alternative Boosting Methods:** Compare BJ-Boost with other survival boosting frameworks (e.g., gradient boosting with AFT loss) to establish relative advantages beyond the Cox-PH baseline.