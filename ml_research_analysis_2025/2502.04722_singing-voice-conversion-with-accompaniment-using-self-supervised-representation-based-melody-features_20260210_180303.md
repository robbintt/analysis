---
ver: rpa2
title: Singing Voice Conversion with Accompaniment Using Self-Supervised Representation-Based
  Melody Features
arxiv_id: '2502.04722'
source_url: https://arxiv.org/abs/2502.04722
tags:
- melody
- ieee
- conversion
- singing
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of singing voice conversion (SVC)
  in the presence of background music (BGM), which can interfere with melody extraction
  and degrade SVC performance. The authors propose a novel method using self-supervised
  representation-based melody features to improve melody modeling accuracy in BGM
  conditions.
---

# Singing Voice Conversion with Accompaniment Using Self-Supervised Representation-Based Melody Features

## Quick Facts
- arXiv ID: 2502.04722
- Source URL: https://arxiv.org/abs/2502.04722
- Reference count: 38
- Key outcome: Proposed WavLM-based SVC achieves F0RMSE of 0.169 and F0CORR of 0.960 under clean conditions, significantly outperforming baselines in melody accuracy and subjective quality.

## Executive Summary
This paper addresses the challenge of singing voice conversion (SVC) when background music (BGM) is present, which typically interferes with melody extraction and degrades conversion quality. The authors propose using self-supervised learning (SSL) models, specifically HuBERT and WavLM, to extract robust melody features from audio with accompaniment. By leveraging weighted-sum aggregation of SSL layer representations and fine-tuning, the method captures comprehensive melody information including pitch, energy, and voicing patterns. The approach significantly improves melody modeling accuracy and conversion quality in both noisy and clean audio environments, with WavLM-based extraction showing the best performance.

## Method Summary
The method uses pretrained HuBERT or WavLM models to extract melody features from audio containing BGM. The melody extractor employs weighted-sum aggregation of all SSL layer outputs combined with FFT blocks to generate 256-dimensional comprehensive melody features. The SSL model is fine-tuned for 5k steps on melody extraction tasks (pitch, energy, VUV prediction) then frozen to prevent catastrophic forgetting. These features feed into an SVC system with disentangled content (BNFs from ASR), melody, and timbre representations. The system uses adversarial training with three discriminators (Real/Fake, Conversion, Embedding) and a HiFi-GAN vocoder for waveform synthesis.

## Key Results
- WavLM-based melody extraction achieves F0RMSE of 0.169 and F0CORR of 0.960 under clean conditions
- Proposed method outperforms baselines (Original Pitch&Energy, Crepe, Separated+Crepe) by significant margins in F0 accuracy
- Robust performance maintained across SNR levels (0dB to 15dB) with F0RMSE increasing only from 0.169 to 0.199
- Subjective evaluations show higher naturalness (NMOS) and similarity (SMOS) scores compared to baselines
- Ablation studies confirm effectiveness of weighted-sum aggregation and FFT blocks

## Why This Works (Mechanism)

### Mechanism 1
SSL models pre-trained on speech capture melody-relevant acoustic information that remains robust when background music is present in the input signal. HuBERT and WavLM encode hierarchical acoustic features across transformer layers, and weighted-sum aggregation combines information from all layers, enabling the model to access both low-level acoustic patterns and higher-level representations that encode pitch-related structure. Core assumption: Acoustic representations learned from speech transfer to singing voice melody extraction, even under noisy conditions with instrumental accompaniment.

### Mechanism 2
Layer-wise contribution to melody extraction shifts from shallow to deeper layers after fine-tuning, enabling higher transformer layers to model melody explicitly. Pre-trained SSL models concentrate melody information in CNN feature extractors and lower encoder layers. Fine-tuning with melody extraction objectives propagates melody modeling capacity to higher transformer layers. The weighted-sum learnable weights adapt to redistribute layer importance. Core assumption: Fine-tuning can unlock melody-relevant representations in higher layers without catastrophic forgetting of pre-trained knowledge.

### Mechanism 3
FFT blocks transform weighted-sum SSL representations into comprehensive melody features that capture pitch, energy, and voicing patterns jointly. The 256-dimensional output from FFT blocks provides richer melody representation than F0 alone, enabling the downstream SVC decoder to synthesize more natural singing by conditioning on energy dynamics and voice/unvoice flags alongside pitch contours. Core assumption: Comprehensive melody features (pitch + energy + VUV) improve conversion quality compared to F0-only conditioning.

## Foundational Learning

- **Self-Supervised Speech Representations (HuBERT, WavLM)**
  - Why needed here: The entire method depends on understanding how SSL models encode hierarchical acoustic information and why their representations transfer to melody extraction.
  - Quick check question: Why might a model trained only on speech (LibriSpeech) still encode pitch-related information useful for singing?

- **Weighted-Sum Layer Aggregation**
  - Why needed here: The core innovation uses learnable weights to combine all SSL layers rather than using only the final layer output.
  - Quick check question: What might happen if you used only the final transformer layer instead of weighted-sum aggregation?

- **Singing Voice Conversion Architecture (Encoder-Decoder with Feature Disentanglement)**
  - Why needed here: The melody features feed into a larger SVC system that separates content (BNFs from ASR), melody, and timbre.
  - Quick check question: What three feature types must be disentangled for successful any-to-one SVC, and which one does this paper focus on improving?

## Architecture Onboarding

- **Component map:**
  Input (waveform + BGM) -> ASR Model -> Content Encoder -> 256-dim BNFs
  Input (waveform + BGM) -> Melody Extractor [SSL (HuBERT/WavLM) + Weighted-Sum + FFT Blocks] -> Melody Encoder (FFT + conditional instance norm)
  Melody Encoder + Content Encoder -> Decoder (FFT blocks) -> Mel-spectrogram -> HiFi-GAN Vocoder -> Converted Audio

- **Critical path:** Melody Extractor -> Melody Encoder -> Decoder. If melody features are corrupted by BGM, the entire conversion fails regardless of vocoder quality.

- **Design tradeoffs:**
  - Fine-tuning duration: 5k steps balances melody adaptation vs. catastrophic forgetting
  - SSL model choice: WavLM outperforms HuBERT but requires more compute
  - Training data augmentation: Adding BGM with 50% probability during training improves robustness but may slow convergence

- **Failure signatures:**
  - F0RMSE increases sharply as SNR decreases (baseline: 0.165→0.353; proposed: 0.169→0.199)
  - Source separation artifacts in "Separated+Crepe" approach cause unstable SVC despite clean pitch extraction
  - Timbre leakage if Embedding Discriminator undertrained
  - Catastrophic forgetting if SSL fine-tuned beyond 5k steps

- **First 3 experiments:**
  1. Ablation study on melody extractor components: Test single vs. weighted-sum, with/without FFT, with/without fine-tuning across both HuBERT and WavLM
  2. SNR robustness evaluation: Compare proposed method vs. Original Pitch&Energy, Crepe, and Separated+Crepe baselines at 0dB, 5dB, 10dB, 15dB SNR and clean conditions
  3. Layer weight visualization: Analyze how fine-tuning redistributes weighted-sum contributions from shallow to deeper layers

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the proposed self-supervised representation-based melody extractor be effectively adapted for any-to-many singing voice conversion tasks without requiring individual model fine-tuning for each target singer?
- **Open Question 2:** Does the robustness of the weighted-sum SSL melody extractor hold in highly complex noisy environments, such as live performances with significant reverberation or non-stationary crowd noise?
- **Open Question 3:** Could the melody extraction performance be further improved by utilizing regularization techniques to allow for full fine-tuning of the SSL model, rather than freezing it to prevent catastrophic forgetting?

## Limitations
- The method depends on fine-tuning SSL models for only 5k steps, representing a narrow operational window where exceeding this threshold risks catastrophic forgetting
- The approach inherits SSL models' potential biases from speech-only pre-training, which may not fully capture singing-specific pitch patterns like vibrato
- Evaluation focuses primarily on F0 accuracy metrics and subjective naturalness/similarity scores, lacking detailed analysis of timbre preservation or conversion artifacts

## Confidence
- **High Confidence:** The core finding that WavLM-based melody extraction outperforms existing baselines in F0RMSE (0.169) and F0CORR (0.960) under clean conditions is well-supported by the reported metrics and ablation studies.
- **Medium Confidence:** The claim about layer-wise contribution shifts from shallow to deeper layers after fine-tuning is supported by the mechanism description and Figure 3 visualization, but the exact quantitative nature of this redistribution isn't fully detailed.
- **Medium Confidence:** The assertion that comprehensive melody features (pitch + energy + VUV) improve conversion quality over F0-only conditioning is supported by Table I comparisons, though the practical significance of the improvement warrants further investigation.

## Next Checks
1. **Catastrophic Forgetting Threshold:** Systematically vary fine-tuning duration (1k, 5k, 10k, 20k steps) to empirically determine the exact boundary where SSL representation quality degrades, validating the 5k step assumption.
2. **BGM Generalization Test:** Evaluate the trained model on diverse BGM genres and instrumentations not present in the MUSDB training set to assess whether the model overfits to specific accompaniment patterns.
3. **Ablation of Fine-Tuning vs. Frozen SSL:** Train a control condition where the SSL model remains frozen (no melody adaptation) to quantify the exact contribution of the 5k-step fine-tuning to the observed performance improvements.