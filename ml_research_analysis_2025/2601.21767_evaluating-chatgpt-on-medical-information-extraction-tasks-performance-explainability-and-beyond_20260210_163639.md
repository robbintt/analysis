---
ver: rpa2
title: 'Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability
  and Beyond'
arxiv_id: '2601.21767'
source_url: https://arxiv.org/abs/2601.21767
tags:
- arxiv
- chatgpt
- language
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic evaluation of ChatGPT''s capabilities
  in four medical information extraction (MedIE) tasks across six benchmark datasets.
  The authors designed task-specific prompts with demonstrations and instructions
  to guide ChatGPT''s responses, then evaluated performance across five dimensions:
  performance, explainability, faithfulness, confidence, and uncertainty.'
---

# Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond

## Quick Facts
- **arXiv ID:** 2601.21767
- **Source URL:** https://arxiv.org/abs/2601.21767
- **Reference count:** 0
- **Primary result:** ChatGPT achieves F1-scores around 30 on medical IE tasks versus 60-80 for fine-tuned baselines, while providing high-quality explanations (75%+ reasonable) but exhibiting over-confidence and high variance issues.

## Executive Summary
This paper presents a systematic evaluation of ChatGPT's capabilities across four medical information extraction tasks using six benchmark datasets. The authors designed task-specific prompts with demonstrations and instructions to guide ChatGPT's responses, then evaluated performance across five dimensions: performance, explainability, faithfulness, confidence, and uncertainty. The results show that ChatGPT's performance falls behind fine-tuned baseline models, achieving F1-scores of around 30 on most tasks compared to 60-80 for baselines. However, ChatGPT provides high-quality explanations for its predictions, with over 75% of instances receiving reasonable explanations. The model exhibits over-confidence issues, providing similar confidence scores for both correct and incorrect predictions. ChatGPT demonstrates high faithfulness to task instructions and input texts, with over 80% adherence rates. The uncertainty in generation causes variability in results across multiple runs, potentially hindering real-world applications.

## Method Summary
The paper evaluates ChatGPT (gpt-3.5-turbo) on four medical information extraction tasks: Named Entity Recognition, Triple Extraction, Clinical Event Extraction, and ICD Coding across six datasets. Task-specific prompts were constructed with task descriptions, label sets with explanations, output formats, demonstrations (few-shot examples), and input text. The model was queried via OpenAI API with top-p sampling, and outputs were parsed from JSON format. Evaluation used strict instance-level F1-score as the primary metric, supplemented by R-score for explainability, confidence scores (CC/IC), and standard deviation over 5 runs to measure uncertainty. The conversation history was cleared after each generation to prevent bias.

## Key Results
- ChatGPT achieves F1-scores around 30 on most tasks versus 60-80 for fine-tuned baseline models
- Over 75% of predictions receive reasonable explanations (R-score >0.75) across all tasks
- ChatGPT shows over-confidence with similar confidence scores (CC-score ~80-82%, IC-score ~79-82%) for both correct and incorrect predictions
- High faithfulness to task instructions and input texts (adherence rates >80%)
- High variability across runs due to top-p sampling, with standard deviations higher than baseline models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-specific prompts with demonstrations enable ChatGPT to perform medical information extraction without fine-tuning, though with significant performance gaps compared to supervised baselines.
- **Mechanism:** The prompt architecture decomposes into five components: task descriptions, label sets with explanations, output format specifications, demonstrations (few-shot examples), and the target input. This structure activates in-context learning capabilities, allowing the model to map from natural language instructions to structured extraction without parameter updates.
- **Core assumption:** The model's pre-training has encoded sufficient medical domain knowledge and structured reasoning patterns to generalize from a handful of demonstrations.
- **Evidence anchors:** [abstract] "ChatGPT's performance falls behind fine-tuned baseline models, achieving F1-scores of around 30 on most tasks compared to 60-80 for baselines." [Section 3.2] The task prompt consists of task descriptions, label sets with explanations, output formats, demonstrations, and input text. [corpus] Neighbor papers on few-shot medical entity extraction (arXiv:2504.04385) suggest pre-trained models can perform medical extraction with limited examples, supporting the in-context learning mechanism.
- **Break condition:** Performance degrades sharply on complex tasks requiring multi-step reasoning (e.g., CMeIE-v2 triple extraction at F1=9.9) compared to simpler matching tasks (ICD coding at F1=30.9), indicating the mechanism fails when task complexity exceeds what demonstrations can convey.

### Mechanism 2
- **Claim:** ChatGPT's explanation generation operates independently of prediction correctness, producing confident rationales even for incorrect predictions.
- **Mechanism:** The model generates post-hoc explanations that are linguistically coherent and faithful to input text, but these explanations do not reflect genuine uncertainty calibration. The confidence scores provided (CC-score ~80-82%, IC-score ~79-82%) show minimal differentiation between correct and incorrect predictions.
- **Core assumption:** Explanations arise from the model's ability to retrieve plausible reasoning patterns rather than from introspective access to its own certainty.
- **Evidence anchors:** [abstract] "ChatGPT provides high-quality explanations for its predictions, with over 75% of instances receiving reasonable explanations. The model exhibits over-confidence issues, providing similar confidence scores for both correct and incorrect predictions." [Section 4.3.1] "ChatGPT seems to be over-confident in its explanations, since on the wrong predictions ChatGPT can also provide explanations with confident tones and it can not reflect on its mistakes." [corpus] Position paper on explainability and uncertainty in medical AI (arXiv:2509.18132) confirms that current XAI methods focus on interpretation but fail to capture confidence/reliability, aligning with this decoupling mechanism.
- **Break condition:** If explanations were causally linked to correct reasoning, we would expect high-correlation between explanation quality and prediction accuracy; the observed independence suggests the mechanism is epiphenomenal rather than diagnostic.

### Mechanism 3
- **Claim:** Generative decoding via top-p sampling introduces sufficient stochasticity that the same input produces meaningfully different extractions across runs.
- **Mechanism:** Top-p (nucleus) sampling samples from the smallest set of tokens whose cumulative probability exceeds threshold p. In structured extraction tasks, small differences at decision boundaries cascade into structurally different outputs (e.g., extracting vs. missing an entity).
- **Core assumption:** The model's probability distribution over outputs is sufficiently flat in medical domains that sampling variance exceeds task tolerance.
- **Evidence anchors:** [abstract] "The uncertainty in generation causes variability in results across multiple runs, potentially hindering real-world applications." [Section 4.2] "Due to the top-p sampling strategy, the standard deviation (i.e., uncertainty) in performance scores for ChatGPT is generally higher than the baseline models." [corpus] Weak direct evidence on sampling variance in medical IE specifically; neighboring papers focus on extraction accuracy rather than run-to-run variability.
- **Break condition:** Setting temperature to 0 or using greedy decoding would collapse variance but may reduce coherence; the trade-off between consistency and quality remains uncharacterized in the paper.

## Foundational Learning

- **Concept: Closed vs. Open Information Extraction**
  - **Why needed here:** The paper evaluates ChatGPT under the "closed MedIE setting" where labels are pre-defined and extraction must conform to specified schemas. Understanding this constraint explains why faithfulness metrics measure adherence to both instructions and input.
  - **Quick check question:** If the model extracts a medically valid entity not in the provided label set, is this correct extraction or faithfulness failure?

- **Concept: Strict F1 Evaluation for Structured Extraction**
  - **Why needed here:** The paper uses "instance level strict F1-score" where true positives require all keys in a ground-truth instance to be correctly predicted. This harsher metric partially explains the 30 vs. 60-80 gap—it penalizes partial correctness.
  - **Quick check question:** A model extracts "aspirin" correctly but assigns type "drug" instead of "medication"—does strict F1 count this as TP, FP, FN, or multiple?

- **Concept: Model Calibration (Confidence-Accuracy Alignment)**
  - **Why needed here:** The over-confidence finding (CC-score ≈ IC-score) indicates poor calibration. Understanding calibration is essential for interpreting model-provided confidence scores in deployment contexts.
  - **Quick check question:** A model with 70% accuracy that outputs 95% confidence on all predictions is: (a) accurate, (b) calibrated, (c) over-confident, (d) both a and c?

## Architecture Onboarding

- **Component map:** Input Medical Text → Prompt Constructor (task desc + labels + format + demos) → ChatGPT API (gpt-3.5-turbo, top-p sampling) → Response Parser (JSON extraction) → Evaluation Layer (strict F1, R-score, faithfulness, confidence)

- **Critical path:** Prompt design → Response consistency → Output parsing. The paper notes they "cleared the conversation after generating each response" to prevent bias—session management is non-trivial.

- **Design tradeoffs:**
  - More demonstrations improve performance but consume context window and increase API costs
  - Greedy decoding (temperature=0) would reduce variance but may degrade explanation quality
  - Domain-specific pre-training (e.g., BioBERT) outperforms but requires fine-tuning infrastructure

- **Failure signatures:**
  - F1 scores clustering around 30 across diverse tasks suggests ceiling effect from prompt-only approach
  - Low variance in confidence scores (std dev ~1.0-1.7%) despite variable predictions indicates miscalibrated certainty
  - CMeIE-v2 at F1=9.9 shows catastrophic failure on multi-entity, multi-relation extraction

- **First 3 experiments:**
  1. **Baseline replication:** Run the provided prompts on a held-out subset of each dataset with temperature settings [0, 0.3, 0.7, 1.0] to characterize the variance-accuracy tradeoff not explored in the paper.
  2. **Ablation study:** Remove one prompt component at a time (task description, label explanations, demonstrations, format specification) to identify which contributes most to the ~30 F1 baseline.
  3. **Error analysis by task complexity:** Correlate performance with entity density, relation types, and text length on CMeIE-v2 to understand why triple extraction fails disproportionately (hypothesis: prompt context insufficient for multi-hop reasoning).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific factors drive the performance gap between ChatGPT and fine-tuned models in medical information extraction tasks?
- **Basis in paper:** [explicit] Section 4.2 states ChatGPT "falls behind the fine-tuned baseline models" and suggests potential reasons include the extreme few-shot scenario and domain complexity ("MedIE tasks requires domain knowledges and are more complex").
- **Why unresolved:** The paper establishes the existence of the gap but does not conduct an ablation study to isolate whether the gap is primarily due to the lack of fine-tuning, the complexity of medical language, or the specific prompt structure used.
- **What evidence would resolve it:** A comparative analysis evaluating ChatGPT with various prompting strategies (e.g., chain-of-thought vs. standard) on simplified versus complex medical texts to isolate the source of the performance degradation.

### Open Question 2
- **Question:** How can the unreliability of ChatGPT's confidence scores be mitigated to prevent misguiding users in medical applications?
- **Basis in paper:** [explicit] Section 4.3.2 highlights "The above two observations reveal the un-reliability of ChatGPT's confidence scores" and notes the model is "over-confident" with "no significant gaps between the CC-score and IC-score."
- **Why unresolved:** The paper identifies the miscalibration but does not propose or test methods (such as temperature scaling or ensemble verification) to align confidence scores with actual prediction accuracy.
- **What evidence would resolve it:** Experiments applying calibration techniques to ChatGPT outputs to see if confidence scores can be trained or prompted to correlate more strongly with accuracy (i.e., high confidence only when correct).

### Open Question 3
- **Question:** To what extent does the semantic ambiguity of label definitions in prompts contribute to extraction errors?
- **Basis in paper:** [explicit] Section 4.2 hypothesizes "the meaning of some labels may not be easy to understand even though we have provide explanations, thereby negatively impact the performance."
- **Why unresolved:** This is presented as a conjecture ("Another reason may be...") without experimental verification or a qualitative analysis of which specific labels caused the most confusion.
- **What evidence would resolve it:** An error analysis correlating low-performance labels with their semantic complexity or synonym overlap, followed by re-evaluation using prompts with more detailed or simplified label descriptions.

### Open Question 4
- **Question:** How can the variability caused by ChatGPT's top-p sampling be reduced to ensure stability in medical information extraction?
- **Basis in paper:** [explicit] The abstract and Section 4.2 state "The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks."
- **Why unresolved:** The paper reports high standard deviations in performance (e.g., ±2.1 in CMeEE-v2) but does not explore decoding strategies or seed fixing to stabilize the output for clinical use cases.
- **What evidence would resolve it:** Testing ChatGPT with different decoding parameters (e.g., lower temperature) or self-consistency mechanisms (majority voting over multiple runs) to measure the reduction in variance without sacrificing performance.

## Limitations
- Substantial performance gap (F1~30 vs 60-80) compared to fine-tuned models suggests prompt-only approaches may not scale to production
- Over-confidence issues where explanations remain confident even for incorrect predictions raises deployment safety concerns
- High variability across runs due to top-p sampling could cause inconsistent clinical decisions

## Confidence
- **Performance comparison (ChatGPT vs. baselines):** Medium confidence - While the ~30 F1 scores are clearly stated, the strict evaluation methodology could amplify the perceived gap.
- **Explainability claims (high-quality explanations):** High confidence - Multiple metrics (R-score, manual evaluation) consistently show >75% reasonable explanations across tasks.
- **Faithfulness and calibration:** High confidence - The 80%+ adherence rates are well-documented, and the confidence calibration issue is directly measurable from the reported CC/IC-score similarity.

## Next Checks
1. Conduct ablation studies removing individual prompt components to quantify each element's contribution to the ~30 F1 baseline performance.
2. Test temperature scaling (0.0 to 1.0) to characterize the variance-accuracy tradeoff and determine if deterministic decoding improves consistency without sacrificing quality.
3. Implement human evaluation comparing ChatGPT's explanations to baseline model predictions to assess whether the quality advantage translates to clinical utility.