---
ver: rpa2
title: 'Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel
  Conflict on YouTube'
arxiv_id: '2504.12177'
source_url: https://arxiv.org/abs/2504.12177
tags:
- israel
- comentarios
- para
- como
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed 253,925 Spanish-language YouTube comments on
  the Hamas-Israel conflict using NLP with a BERT model, classifying them into seven
  categories. Pro-Palestinian comments were most frequent, but pro-Israeli and anti-Palestinian
  comments received more likes.
---

# Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube

## Quick Facts
- arXiv ID: 2504.12177
- Source URL: https://arxiv.org/abs/2504.12177
- Reference count: 0
- Primary result: BERT-classified Spanish YouTube comments into 7 stance categories; pro-Palestinian comments most frequent but pro-Israeli/anti-Palestinian comments received more likes

## Executive Summary
This study analyzed 253,925 Spanish-language YouTube comments on the Hamas-Israel conflict using BERT fine-tuning for stance classification. The comments were categorized into seven positions including pro-Palestinian, pro-Israeli, anti-Palestinian, anti-Israeli, anti-Hamas, no stance, and non-related. While pro-Palestinian comments dominated numerically, pro-Israeli and anti-Palestinian comments received disproportionately more engagement (likes). The research also applied agenda-setting theory to demonstrate how media coverage influenced public opinion shifts from pro-Palestinian to more critical of Israel over time.

## Method Summary
The methodology employed YouTube Data API v3 to collect 289 videos and 270,573 raw comments (Oct 7, 2023 - Jan 7, 2024), filtered to 253,925 valid Spanish comments. A random sample of 500 comments was manually annotated into 7 categories, expanded to 1,400 labeled examples (200 per category) using syntactic criteria. BERT was fine-tuned with TensorFlow/Keras for 15 epochs, achieving 90-93% reported accuracy. The model classified all comments, enabling temporal analysis of stance distributions and engagement patterns. Agenda-setting theory was applied to correlate media coverage shifts with changes in public discourse.

## Key Results
- Pro-Palestinian comments were most frequent but received fewer likes than pro-Israeli and anti-Palestinian comments
- Anti-Hamas category produced zero predictions due to semantic overlap with other categories
- Media coverage shifts from Hamas attack focus to humanitarian crisis focus correlated with stance distribution changes

## Why This Works (Mechanism)

### Mechanism 1
BERT can classify polarized public discourse into discrete stance categories when fine-tuned on syntactically-anchored annotations in a specific language. BERT's bidirectional attention captures contextual word relationships across the comment. The model is tokenized (input_word_ids, input_mask, input_type_ids), passed through a pre-trained encoder, and a Keras classification head outputs probabilities across 7 stance categories. Training on 200 examples per category for 15 epochs produced 90-93% reported accuracy. Core assumption: Grammatical syntax patterns are sufficient to distinguish stance categories in Spanish political discourse. Evidence anchors: Abstract states comments were automatically classified into seven categories; section describes BERT tokenization and Keras encapsulation; corpus shows this approach is common but category schemes vary widely. Break condition: The Anti-Hamas category produced zero predictions despite 200 training examples due to semantic overlap with Anti-Palestinian and Pro-Israel categories.

### Mechanism 2
Temporal shifts in media coverage focus correlate with changes in stance distribution within public discourse. Initial media focus on Hamas attack (Oct 7) drives higher Anti-Palestinian/Pro-Israel comment frequency. As coverage shifts to humanitarian crisis and civilian casualties (Nov-Jan), Anti-Israel comment frequency increases. Agenda-setting theory explains this as "need for orientation" driving information-seeking behavior. Core assumption: YouTube commenters primarily form opinions through consumption of mainstream media narratives, not direct experience or alternative information sources. Evidence anchors: Abstract notes observed shift in public opinion from pro-Palestinian to more critical position towards Israel; section quotes McCombs and Valenzuela on need for orientation; corpus evidence is weak as neighbor papers do not explicitly test agenda-setting causality. Break condition: If commenters form opinions from decentralized sources (Telegram channels, personal networks, diaspora connections) rather than mainstream media, the temporal correlation would weaken or reverse.

### Mechanism 3
Numerically minority positions can receive disproportionately higher per-comment engagement (likes) than majority positions. Pro-Israel/Anti-Palestinian comments are less frequent but receive more likes on average. Possible explanations include higher motivation intensity among minority-position holders, coordinated engagement patterns, or algorithmic amplification of specific content types. Core assumption: Like counts reflect genuine audience preference rather than bot activity, brigading, or platform manipulation. Evidence anchors: Abstract states pro-Israeli and anti-Palestinian comments received more likes despite lower frequency; section concludes minority stance comments receive more user support; corpus evidence is weak as neighbor papers report sentiment distributions but do not systematically examine quantity vs. engagement asymmetries. Break condition: If like behavior is driven by platform recommendation algorithms rather than organic user preference, the mechanism confounds engagement with visibility.

## Foundational Learning

- Concept: **BERT Tokenization and Input Tensors**
  - Why needed here: The architecture requires understanding how raw Spanish text becomes tokenized input (input_word_ids, input_mask, input_type_ids) for the encoder. Misunderstanding tokenization leads to silent failures in preprocessing.
  - Quick check question: Given "Viva Palestina!", what three tensors does BERT expect as input, and what does each represent?

- Concept: **MATTER+PD Annotation Framework**
  - Why needed here: The paper explicitly follows this 7-step pipeline (Model, Procure, Annotate, Train/Test, Evaluate, Review, Distribute). The Anti-Hamas failure could have been caught at the "Review" stage with better category boundary testing.
  - Quick check question: At which MATTER+PD stage would you detect that "Hay que matar a todos los terroristas de Gaza" fits multiple categories?

- Concept: **Agenda-Setting Theory (McCombs-Valenzuela)**
  - Why needed here: The temporal analysis explicitly relies on agenda-setting to interpret stance shifts. Without this framework, the data is just category counts without explanatory power.
  - Quick check question: What is the predicted relationship between "need for orientation" and comment volume in the first two weeks of a controversy?

## Architecture Onboarding

- Component map: YouTube Data API v3 -> 289 videos -> 270,573 raw comments -> 253,925 valid comments -> 1,400 labeled examples -> BERT fine-tuning -> 7-category classification -> temporal analysis

- Critical path: 1) Define 7 categories via preliminary comment review + literature 2) Annotate 200 comments per category using syntactic criteria 3) Fine-tune BERT (15 epochs, 90-93% reported accuracy) 4) Run inference on full 253,925 comments 5) Cross-reference temporal patterns with external media timeline

- Design tradeoffs: Syntax vs. semantics (character-based annotation aligns with machine reading but loses nuance), 7 categories vs. 4 (including "Non-related" and "No stance" improves completeness but dilutes controversy signal), Spanish-only vs. multilingual (enables focused fine-tuning but limits cross-cultural comparison)

- Failure signatures: Zero predictions for one category despite training data (category boundary overlap), training accuracy reported without held-out validation (potential overfitting), no inter-annotator agreement metrics (annotation reliability unknown)

- First 3 experiments: 1) Confusion matrix diagnosis: Manually review 50 comments from each category; calculate inter-annotator agreement; focus on Anti-Hamas/Anti-Palestinian/Pro-Israel overlap 2) Held-out validation: Reserve 20% of labeled data as test set; report precision/recall/F1 per category 3) Cross-lingual replication: Apply identical methodology to English YouTube comments from same videos; compare stance distributions and like asymmetry patterns

## Open Questions the Paper Calls Out

### Open Question 1
How can the semantic ambiguity between "Anti-Hamas" and "Anti-Palestinian" stances be resolved in NLP classification models for this conflict? The authors state the model failed to predict any "Anti-Hamas" comments because of semantic overlap with "Anti-Palestinian" or "Pro-Israel" categories. The training data, annotated based on syntax rather than deep implicit meaning, could not effectively distinguish between condemning the group versus condemning the population. Retraining the model with a larger dataset specifically annotated for implicit semantic context, or utilizing a hierarchical classification approach, would resolve this.

### Open Question 2
Does the "Without Stance" (SP) category constitute a distinct social actor or discourse (e.g., pacifism) within the controversy map? The recommendations suggest analyzing this label as a stance in itself, rather than excluding it as noise, noting frequent comments like "Peace for the world". The current methodology excluded these comments from the controversy analysis to focus strictly on polarized positions. A qualitative content analysis or topic modeling applied exclusively to the SP dataset to identify cohesive thematic clusters would resolve this.

### Open Question 3
What specific rhetorical or user-behavior factors drive the higher engagement ("likes") for Pro-Israel/Anti-Palestinian comments despite their lower frequency? The results highlight a "tension" where minority stance comments receive significantly more likes than the majority Pro-Palestinian comments, a phenomenon the paper notes but does not causally explain. The study maps the distribution of stances but does not perform feature extraction or network analysis to explain the differential popularity. A correlation analysis between specific linguistic features (e.g., sentiment intensity, use of evidence) and engagement metrics would resolve this.

## Limitations
- Anti-Hamas category produced zero predictions due to severe semantic overlap with other categories
- No held-out validation metrics or inter-annotator agreement scores reported
- Syntactic annotation approach sacrifices semantic nuance that could improve classification reliability
- Like count analysis lacks controls for bot activity or coordinated manipulation

## Confidence

- **High**: Pro-Palestinian comment predominance, basic temporal patterns in comment volume
- **Medium**: Pro-Israel/Anti-Palestinian engagement asymmetry (methodologically sound but interpretation uncertain)
- **Low**: Anti-Hamas category predictions (effectively non-functional), specific agenda-setting causality claims

## Next Checks

1. **Category Boundary Validation**: Manually review 50 randomly selected comments from each category, calculate Cohen's kappa inter-annotator agreement, and specifically diagnose why Anti-Hamas comments are misclassified (semantic overlap with Anti-Palestinian and Pro-Israel appears to be the root cause).

2. **Held-Out Performance Testing**: Reserve 20% of labeled data as a strict test set (not used in training), report precision, recall, and F1-score per category, and implement stratified sampling to ensure minority categories are adequately represented in both training and testing.

3. **Information Source Attribution**: Conduct qualitative analysis of a stratified sample of commenters to determine whether their opinions derive primarily from mainstream media coverage, alternative information sources, or direct experienceâ€”this would validate or invalidate the agenda-setting theoretical framework.