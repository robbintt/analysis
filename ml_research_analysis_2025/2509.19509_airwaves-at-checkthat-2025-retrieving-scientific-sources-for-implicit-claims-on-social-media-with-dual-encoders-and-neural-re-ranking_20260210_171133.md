---
ver: rpa2
title: 'AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims
  on Social Media with Dual Encoders and Neural Re-Ranking'
arxiv_id: '2509.19509'
source_url: https://arxiv.org/abs/2509.19509
tags:
- retrieval
- test
- document
- re-ranking
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AIRwaves team achieved second place in CheckThat! 2025 Task 4b
  by developing a two-stage retrieval pipeline for linking implicit scientific claims
  on social media to their original publications.
---

# AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims on Social Media with Dual Encoders and Neural Re-Ranking

## Quick Facts
- **arXiv ID**: 2509.19509
- **Source URL**: https://arxiv.org/abs/2509.19509
- **Reference count**: 34
- **Primary result**: AIRwaves team achieved second place in CheckThat! 2025 Task 4b with MRR@5 = 0.6828

## Executive Summary
AIRwaves presents a two-stage retrieval pipeline for linking implicit scientific claims on social media to their original publications. The approach combines dense retrieval using a fine-tuned E5-large dual encoder with neural re-ranking using SciBERT cross-encoder. The system achieved second place in CheckThat! 2025 Task 4b, demonstrating a 10.6% relative improvement over strong sparse retrieval baselines through the integration of neural representations and cross-encoder re-ranking.

## Method Summary
The AIRwaves pipeline consists of two stages: first, a fine-tuned E5-large dual encoder retrieves candidate documents using in-batch and BM25-mined hard negatives; second, a SciBERT cross-encoder re-ranks the top-10 candidates. Documents are chunked (510 tokens, 50 overlap) with mean-max pooling to handle context length limits. The system uses MultipleNegativesRankingLoss with LR=7e-6 for dual encoder training and BCE loss with LR=2e-5 for cross-encoder fine-tuning. All document fields (title, abstract, authors, journal, source) are concatenated with [SEP] tokens and normalized.

## Key Results
- BM25 baseline achieved MRR@5 = 0.5025 on test set
- Dual encoder stage improved performance to MRR@5 = 0.6174
- Complete pipeline (dual encoder + SciBERT re-ranking) achieved MRR@5 = 0.6828
- Hard negative training regime showed statistically significant improvement (p=0.0159 for McNemar's test)
- Re-ranking depth of k=10 was optimal; k=20 introduced noise and degraded performance

## Why This Works (Mechanism)

### Mechanism 1: Hard Negative Sampling for Discriminative Training
Adding BM25-mined hard negatives during dual encoder training produces statistically significant improvements by forcing the model to learn finer semantic distinctions beyond surface-level keyword matching. BM25 retrieves documents with high lexical overlap but incorrect semantics, creating genuinely confusable examples that neural models can distinguish.

### Mechanism 2: Two-Stage Retrieval with Cross-Encoder Re-ranking
The dual encoder retrieves candidates efficiently through independent embedding, while the cross-encoder jointly attends over query-document pairs to capture fine-grained semantic interactions unavailable to independent encoding. This hybrid approach balances efficiency with accuracy.

### Mechanism 3: Chunked Tokenization with Mean-Max Pooling
Long documents are split into overlapping chunks to preserve information beyond the 512-token context limit. Mean-pooling represents typical content while max-pooling captures exceptional signals, theoretically distributing semantic information throughout documents.

## Foundational Learning

- **Concept: Dual Encoder vs Cross-Encoder Trade-offs**
  - Why needed here: Understanding why the pipeline separates retrieval (dual encoder) from re-ranking (cross-encoder) is essential for debugging and optimizing each stage.
  - Quick check question: Why can't a cross-encoder efficiently retrieve from a corpus of 7,718 documents?

- **Concept: In-Batch Negatives and Hard Negatives**
  - Why needed here: The paper combines both strategies; understanding how they differ in training signal intensity explains why hard negatives improved results despite stronger overfitting.
  - Quick check question: What makes a BM25-retrieved document a "hard" negative for a neural retriever?

- **Concept: Mean Reciprocal Rank (MRR@k)**
  - Why needed here: MRR@5 is the official metric; understanding its sensitivity to early ranks vs. Recall's sensitivity to coverage informs architectural decisions.
  - Quick check question: If MRR@5 improves but Recall@10 stays flat, what does that indicate about where the relevant documents are moving?

## Architecture Onboarding

- **Component map**: BM25 baseline -> E5-large dual encoder (in-batch + hard negatives) -> SciBERT cross-encoder (re-rank top-10) -> Final ranking
- **Critical path**: Dual encoder candidate quality (Recall@k) → Re-ranking depth selection (k=10 optimal) → Final MRR@5
- **Design tradeoffs**: Re-ranking depth k=10 balanced train fit and test generalization; k=20 introduced noise. Batch size 64 slightly outperformed 8 for in-batch negative training. Authors field provided marginal gain; title+abstract sufficient.
- **Failure signatures**: Train-dev-test gap widening with hard negatives (stronger overfitting). Re-ranking at k=20 improves train but degrades dev/test (noise from irrelevant candidates). MRR improves but no-hit rate unchanged (fine-grained ranking improves but coverage bottleneck remains).
- **First 3 experiments**: 1) Reproduce BM25 baseline with spaCy preprocessing (target MRR@5 ≈ 0.50 on test). 2) Fine-tune E5-large-v2 with in-batch negatives only; compare against baseline using Wilcoxon test on per-query MRR@5. 3) Add 1 BM25-mined hard negative per query; verify statistical significance (McNemar's for MRR@1, Wilcoxon for MRR@5).

## Open Questions the Paper Calls Out

### Open Question 1
Can generative re-ranking approaches, such as MonoT5, outperform the discriminative cross-encoder architecture (SciBERT) used in this study for scientific claim retrieval? The authors encourage future work to evaluate what retrieval performance is possible when using such models.

### Open Question 2
Does dynamically mining hard negative documents using current model weights during training yield significant performance gains over the static BM25-mined hard negatives implemented in this work? The authors hypothesize that dynamically re-mining hard negatives after each epoch might be superior.

### Open Question 3
Is there an adaptive strategy for re-ranking depth (k) that can mitigate the introduction of noise observed when expanding the candidate pool beyond a fixed size? The paper identifies k=10 as optimal but does not explore dynamic filtering based on initial retrieval confidence.

## Limitations
- Chunked tokenization approach failed to produce statistically significant improvements (p=0.0715 for Wilcoxon test)
- Hard negative mining procedure relies on BM25-retrieved false positives with unspecified rank position for selection
- Cross-encoder re-ranking benefits appear sensitive to depth selection, with k=20 introducing noise and degrading test performance

## Confidence
- **High Confidence**: Dual encoder + cross-encoder pipeline architecture, statistical significance of improvements from BM25 to dual encoder (p=0.0012), and from dual encoder to full pipeline (p=1.9×10⁻²¹)
- **Medium Confidence**: Hard negative training regime's effectiveness (p=0.0159) relies on assumption that BM25 false positives represent genuinely confusable documents
- **Low Confidence**: Exact implementation details for document field normalization, particularly for author and source metadata, are underspecified

## Next Checks
1. **Cross-validation of re-ranking depth sensitivity**: Systematically evaluate MRR@5 and Recall@k across k=5, 10, 15, and 20 on both dev and test sets to confirm the non-monotonic relationship.
2. **Ablation of hard negative rank position**: Compare performance when mining hard negatives from rank 1, 2, and top-3 BM25 positions to determine whether improvement depends on selecting most lexically similar false positives.
3. **Chunked tokenization parameter sweep**: Test alternative chunk sizes (e.g., 256 tokens with 128 overlap), different pooling strategies (max-only, mean-only), and document field combinations to identify whether lack of significance stems from suboptimal configuration.