---
ver: rpa2
title: On Robustness and Reliability of Benchmark-Based Evaluation of LLMs
arxiv_id: '2509.04013'
source_url: https://arxiv.org/abs/2509.04013
tags:
- across
- question
- questions
- benchmarks
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the robustness and reliability of benchmark-based
  evaluations of Large Language Models (LLMs) by systematically paraphrasing benchmark
  questions and measuring performance changes. The core method involves generating
  multiple paraphrases of questions from six well-established benchmarks (e.g., MMLU,
  ARC-C, HellaSwag) using GPT-4o mini, then evaluating 34 state-of-the-art LLMs on
  both original and paraphrased versions.
---

# On Robustness and Reliability of Benchmark-Based Evaluation of LLMs

## Quick Facts
- **arXiv ID:** 2509.04013
- **Source URL:** https://arxiv.org/abs/2509.04013
- **Reference count:** 40
- **Primary result:** LLM performance drops 15–30% on paraphrased benchmark questions, challenging the reliability of current evaluations.

## Executive Summary
This study investigates the robustness and reliability of benchmark-based evaluations of Large Language Models (LLMs) by systematically paraphrasing benchmark questions and measuring performance changes. The core method involves generating multiple paraphrases of questions from six well-established benchmarks (e.g., MMLU, ARC-C, HellaSwag) using GPT-4o mini, then evaluating 34 state-of-the-art LLMs on both original and paraphrased versions. Key findings reveal that while LLM rankings remain relatively stable across paraphrases, absolute accuracy scores drop significantly—often by 15-30%—indicating limited robustness to linguistic variability. Moreover, model consistency across paraphrases is not always aligned with accuracy, as smaller models tend to be overly consistent yet less accurate. These results challenge the reliability of current benchmarks, suggesting they may overestimate true model capabilities by not accounting for real-world linguistic diversity. The study calls for robustness-aware evaluation frameworks incorporating paraphrased inputs to better reflect practical deployment scenarios.

## Method Summary
The authors systematically paraphrased questions from six well-established LLM benchmarks (MMLU, ARC-C, HellaSwag, etc.) using GPT-4o mini, generating five variants per question while preserving semantic meaning. They evaluated 34 state-of-the-art LLMs on both original and paraphrased versions using zero-shot prompting (top-1 token probability). Performance metrics included accuracy, consistency (answer stability across paraphrases), and ranking stability (Kendall's τ). The study classified models as "Over" (performance degrades on paraphrases), "Under" (improves), or "Neutral" based on performance shifts.

## Key Results
- LLM accuracy drops 15-30% on paraphrased benchmark questions compared to original versions.
- Model rankings remain relatively stable across paraphrases (Kendall's τ > 0.9), but absolute performance degrades significantly.
- Smaller models (<15B parameters) show high consistency but low accuracy on paraphrases, while larger models exhibit positive correlation between consistency and accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Surface-Level Pattern Sensitivity
- **Claim:** LLM performance on benchmarks is partially driven by sensitivity to specific phrasing artifacts rather than deep semantic reasoning, causing significant accuracy drops (15–30%) when questions are paraphrased.
- **Mechanism:** Models appear to rely on "narrowly framed tasks" or specific surface-level patterns present in the original benchmark. Paraphrasing disrupts these cues, forcing the model to rely on generalization, which exposes a gap between reported benchmark scores and actual robustness.
- **Core assumption:** The generated paraphrases preserve the semantic difficulty and intent of the original questions.
- **Evidence anchors:**
  - [abstract]: "absolute effectiveness scores... decline significantly... suggesting that LLMs struggle with linguistic variability."
  - [section 4.2]: "The majority of models fall into the Over category, meaning that their performance degrades when faced with paraphrased inputs."
  - [corpus]: Related work ("Towards Contamination Resistant Benchmarks") supports the broader risk that models exploit specific test-set artifacts.
- **Break condition:** If paraphrases inadvertently change the question's difficulty (verified by the "Inverse" order check in Section 4.1.3), the mechanism shifts from testing robustness to testing a harder/different task.

### Mechanism 2: Scale-Dependent Consistency-Accuracy Decoupling
- **Claim:** Smaller models (<15B parameters) exhibit high consistency (giving the same answer to paraphrases) but low accuracy, while larger models show a positive correlation between consistency and accuracy.
- **Mechanism:** Smaller models may act in an "overly consistent" manner due to limited semantic understanding, essentially ignoring the nuance of different phrasings and defaulting to a "stubborn" prediction. Larger models have sufficient capacity to map semantic variations to the correct reasoning path.
- **Core assumption:** Consistency (answering the same way) is a valid proxy for robustness only when accuracy is also high.
- **Evidence anchors:**
  - [section 4.1.2]: "For smaller models, there is a statistically significant negative correlation... less capable models tend to be more consistent... although they are often wrong."
  - [section 4.1.2]: "Larger models show a strong and significant positive correlation... suggesting that as models become larger, they... become more robust."
  - [corpus]: General robustness literature supports the premise that model scaling improves handling of input variations, though specific consistency/accuracy decoupling is less commonly cited externally.
- **Break condition:** If a small model is specifically fine-tuned on the benchmark, its consistency might artificially rise with accuracy, breaking the negative correlation.

### Mechanism 3: Ranking Stability via Relative Grading
- **Claim:** While absolute scores fluctuate, the relative ranking of models remains stable (Kendall's τ > 0.9) across paraphrased inputs.
- **Mechanism:** Paraphrasing introduces a uniform perturbation across the evaluation set. While this shifts the absolute baseline (making everyone look "worse"), it preserves the relative ordering because the underlying capability hierarchy is sufficiently distinct between models to survive the noise.
- **Core assumption:** The perturbation affects all models roughly equally in magnitude of difficulty, even if direction differs.
- **Evidence anchors:**
  - [abstract]: "rankings remain relatively stable across paraphrased inputs."
  - [section 4.2]: "Kendall's τ... all above 0.9... indicate that despite accuracy fluctuations, the relative ordering of models remains largely preserved."
  - [corpus]: External corpus evidence is currently weak or generic regarding this specific ranking stability phenomenon under paraphrasing.
- **Break condition:** If a benchmark is heavily contaminated for one specific model family but not others, paraphrasing might disrupt that specific model's score disproportionately, breaking the ranking stability.

## Foundational Learning

- **Concept: Zero-Shot Evaluation**
  - **Why needed here:** The study relies on a zero-shot setting (Section 3.4) to measure inherent model capability without the confounding factor of prompt engineering or few-shot examples. Understanding this is crucial to replicate the "robustness" test accurately.
  - **Quick check question:** Why does adding few-shot examples potentially mask the robustness issues identified in this paper?

- **Concept: Benchmark Contamination & Memorization**
  - **Why needed here:** The paper conjectures (Section 4.2, Figure 7) that performance drops on older benchmarks may be due to models "memorizing" specific phrasings during pre-training. Distinguishing between "reasoning" and "retrieval" is central to the paper's critique.
  - **Quick check question:** How does the "release date" of a benchmark correlate with the likelihood of a model falling into the "Over" (performance degradation) category?

- **Concept: Semantic Equivalence Validation**
  - **Why needed here:** The entire methodology hinges on the paraphrases meaning the same thing as the original. You must understand how the authors validated this (e.g., the "inverse order" check in Section 4.1.3) to trust the results.
  - **Quick check question:** If a paraphrase changes the answer options or introduces ambiguity, does the resulting performance drop measure robustness or flawed data generation?

## Architecture Onboarding

- **Component map:** Source Loader -> Paraphraser (GPT-4o mini) -> Evaluator (Target LLMs) -> Analysis Engine
- **Critical path:** The **Paraphraser** is the bottleneck. You must ensure the generation prompt strictly preserves meaning and "avoids negations" (Figure 1). If the paraphraser hallucinates new constraints or simplifies the question too much, the evaluation is invalid.
- **Design tradeoffs:**
  - *Automatic vs. Manual Paraphrasing:* The authors chose automatic generation for scale (53k+ questions) but acknowledge the risk of quality variance. A manual check is suggested for future work.
  - *Top-1 vs. Chain-of-Thought (CoT):* The paper tried CoT but found it "hallucinated" reasoning and offered <3% gain (Section 3.4). Stick to Top-1 for reliability in this specific robustness test.
- **Failure signatures:**
  - **Refusal Loop:** Section 3.3 notes GPT-4o mini refused to paraphrase 31 questions due to safety triggers. Your pipeline must handle these exceptions gracefully (skip or log) rather than crashing.
  - **Stubborn Small Models:** If you see a model with 98% consistency but 25% accuracy, do not interpret this as robustness; this is the "rigidity" failure mode described in Section 4.1.2.
- **First 3 experiments:**
  1. **Sanity Check (Inverse Order):** Replicate the rightmost plot of Figure 5. Feed paraphrases in reverse order (5→1) and confirm results overlap with standard order (1→1). This validates semantic fidelity.
  2. **Over/In/Under Classification:** Classify your target model against one old benchmark (e.g., ARC-C) and one new one. Check if the model falls into "Over" (worse on paraphrase) more often on the old benchmark to test the contamination hypothesis.
  3. **Consistency vs. Accuracy Scatter:** Plot accuracy vs. consistency for your internal model suite. Check if your smaller models show the "U-shape" or negative correlation observed in Figure 4 to verify scaling laws.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's core findings hinge on the semantic fidelity of GPT-4o mini-generated paraphrases, with potential subtle shifts in question difficulty or intent confounding robustness measurements.
- Focus on multiple-choice questions limits generalizability to open-ended tasks, and the 34-model sample may not capture the full spectrum of LLM capabilities.
- The observed ranking stability (Kendall's τ > 0.9) suggests relative ordering is preserved, but this does not guarantee that the most capable models on robust tasks align with those on original benchmarks.

## Confidence
- **High Confidence**: Ranking stability across paraphrases (Kendall's τ > 0.9 consistently observed, supported by multiple benchmarks).
- **Medium Confidence**: Absolute accuracy drop (15–30%) as evidence of limited robustness. This is well-supported by the data but depends critically on paraphrase quality.
- **Medium Confidence**: Scale-dependent consistency-accuracy decoupling. The negative correlation for smaller models is statistically significant, but the causal mechanism (e.g., "rigidity" vs. other factors) requires further validation.

## Next Checks
1. **Manual Paraphrase Validation**: Select 100 paraphrased questions per benchmark and have human experts verify semantic equivalence and difficulty preservation to quantify the risk of confounding.
2. **Cross-Benchmark Contamination Analysis**: Correlate model performance drops on older benchmarks with their release dates and training cutoff windows to test the contamination hypothesis quantitatively.
3. **Open-Ended Task Extension**: Replicate the robustness evaluation on a subset of open-ended benchmarks (e.g., HumanEval) to assess generalizability beyond multiple-choice formats.