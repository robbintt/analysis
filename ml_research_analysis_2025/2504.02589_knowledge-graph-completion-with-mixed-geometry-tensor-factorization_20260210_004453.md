---
ver: rpa2
title: Knowledge Graph Completion with Mixed Geometry Tensor Factorization
arxiv_id: '2504.02589'
source_url: https://arxiv.org/abs/2504.02589
tags:
- knowledge
- hyperbolic
- graph
- geometry
- euclidean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mixed-geometry tensor factorization model
  (MIG-TF) for knowledge graph completion that combines Euclidean and hyperbolic embeddings
  within a single architecture. The method augments a pretrained Euclidean TuckER
  model with a novel hyperbolic interaction term based on tetrahedron pooling tensor
  factorization (TPTF), enabling better capture of hierarchical and non-hierarchical
  structural properties in knowledge graphs.
---

# Knowledge Graph Completion with Mixed Geometry Tensor Factorization

## Quick Facts
- arXiv ID: 2504.02589
- Source URL: https://arxiv.org/abs/2504.02589
- Reference count: 40
- Primary result: Achieves new SOTA on FB15k-237 and YAGO3-10 with fewer parameters than previous models

## Executive Summary
This paper introduces MIG-TF (Mixed Geometry Tensor Factorization), a knowledge graph completion model that combines Euclidean and hyperbolic embeddings within a single architecture. The method augments a pretrained Euclidean TuckER model with a novel hyperbolic interaction term based on tetrahedron pooling tensor factorization (TPTF), enabling better capture of both hierarchical and non-hierarchical structural properties in knowledge graphs. The approach achieves state-of-the-art results on FB15k-237 and YAGO3-10 datasets while using significantly fewer parameters than previous models, demonstrating the effectiveness of mixed geometry representations for heterogeneous knowledge graph structures.

## Method Summary
MIG-TF combines a frozen pretrained TuckER model (Euclidean geometry) with a trainable hyperbolic TPTF component. The Euclidean branch uses Tucker tensor decomposition with shared factors, while the hyperbolic branch maps entity and relation embeddings to a Lorentz hyperboloid using exponential mapping. The TPTF score function is derived from tetrahedron inequality violation, measuring proximity between entities and relations in hyperbolic space. The final score is the sum of Euclidean and hyperbolic scores, with backpropagation occurring only through the hyperbolic parameters. The model uses square Lorentz distance instead of geodesic distance for numerical stability and trains only the hyperbolic embeddings while keeping the Euclidean base frozen.

## Key Results
- Achieves new state-of-the-art results on FB15k-237 (HR@10: 0.559) and YAGO3-10 (HR@10: 0.629)
- Significantly fewer parameters than previous models (5M vs 40-60M for competitors)
- Competitive performance on WN18RR despite slight degradation compared to pure hyperbolic models
- Robust to noise in training data while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1: Mixed Geometry Captures Heterogeneous KG Structure
Combining Euclidean and hyperbolic geometries improves expressivity for KGs with mixed hierarchical/non-hierarchical structure. Euclidean embeddings capture relations between "active vertices" (highly connected nodes that violate power law) while hyperbolic embeddings capture hierarchical, power-law distributed patterns via exponentially expanding distance from origin. Real-world KGs contain both hierarchical and non-hierarchical structural patterns that cannot be optimally represented in a single geometry.

### Mechanism 2: Parameter Efficiency Through Pretrained Euclidean Base + Low-Rank Hyperbolic Correction
Freezing pretrained Euclidean model and training only small hyperbolic term achieves comparable/better performance with significantly fewer parameters. TuckER model is pretrained and frozen; only the TPTF hyperbolic component (with much smaller embedding dimensions d_L=50 vs d_E=200) is trained, reducing total parameters and training complexity. Euclidean component captures sufficient base structure that hyperbolic term acts as a correction for residual hierarchical patterns.

### Mechanism 3: Square Lorentz Distance Provides Numerically Stable Hyperbolic Interaction
Square Lorentz distance approximation enables stable gradient-based optimization compared to geodesic distance with arccosh. Replace geodesic distance d_L = arccosh(-⟨x,y⟩_L) with squared Lorentz distance d²_L = -2 - 2⟨x,y⟩_L, which is differentiable everywhere and avoids numerically unstable exponential/logarithmic mappings. Score function landscape of square Lorentz distance sufficiently approximates geodesic distance for learning hierarchical patterns.

## Foundational Learning

- Concept: Hyperbolic geometry and Lorentz model
  - Why needed here: Core to understanding how TPTF captures hierarchical relations; need to understand hyperboloid H^{n,β}, Lorentz inner product ⟨x,y⟩_L = -x₀y₀ + Σxᵢyᵢ, and exponential area growth
  - Quick check question: Given origin (β,0,...,0) and point x on hyperboloid H^{n,β}, calculate the Lorentz distance; why does this geometry naturally encode hierarchy?

- Concept: Tucker tensor decomposition with shared factors
  - Why needed here: MIG-TF uses TuckER as Euclidean base; need to understand 3-way core tensor G ∈ R^{d₁×d₁×d₂} interacting with entity/relation embeddings
  - Quick check question: Given entity embeddings u,v ∈ R^{d₁} and relation embedding t ∈ R^{d₂}, write the TuckER score function S_E = Σ G_{αβγ}u_αv_βt_γ; what is the parameter count vs CP decomposition?

- Concept: Tetrahedron inequality violation for ternary interactions
  - Why needed here: TPTF score function derived from tetrahedron inequality d(u,v)+d(o,t) ≤ d(u,t)+d(v,t)+d(o,u)+d(o,v); violation in hyperbolic space indicates entity-relation proximity
  - Quick check question: For four points u,v,t,o, write the tetrahedron inequality; how does its violation create a score function for (entity, relation, entity) triples?

## Architecture Onboarding

- Component map:
```
Input: Triple (e_s, r, e_o)
       ↓
Entity/Relation Embeddings: u, t, v (Euclidean); u^L, t^L, v^L (Hyperbolic)
       ↓
┌──────────────────────────────────────┐
│ Euclidean Branch (TuckER - FROZEN)   │
│ S_E = G ×₁ u ×₂ v ×₃ t               │
│ Core tensor G ∈ R^{d₁×d₁×d₂}         │
└──────────────────────────────────────┘
       +
┌──────────────────────────────────────┐
│ Hyperbolic Branch (TPTF - TRAINABLE) │
│ Map to hyperboloid: x → [√(β+||x||²),x]│
│ S_H = [d²_L(u,v)+d²_L(0,t)-d²_L(u,t) │
│        -d²_L(t,v)-d²_L(0,u)-d²_L(0,v)]│
│        / [⟨0,v⟩_L⟨0,t⟩_L+...]        │
└──────────────────────────────────────┘
       ↓
Combined Score: S_MIG = S_E + S_H
       ↓
BCE Loss → Backprop only through S_H
```

- Critical path:
  1. Obtain/load pretrained TuckER weights (freeze all parameters)
  2. Initialize hyperbolic embeddings V^L ∈ R^{n_e×d_L}, T^L ∈ R^{n_r×d_L} with Gaussian N(0, ρ²I), ρ ∈ [0.001, 0.005]
  3. Set curvature β ∈ [1.0, 1.5] based on dataset (Table 6)
  4. Forward: compute S_E (no grad) + S_H (with grad)
  5. Backprop: update only V^L, T^L via AdamW

- Design tradeoffs:
  - Hyperbolic dimension d_L: Paper uses 50 consistently; lower saves params but may miss fine-grained hierarchy
  - Curvature β: Controls hyperbolicity strength; β≈1.0-1.5 optimal (Figure 7, 12); β→0 converges to Euclidean
  - QR orthogonalization: Apply to T^L when relations are uncorrelated (WN18RR, YAGO3-10); skip when correlated (FB15k-237)
  - Training epochs: TPTF needs only 100-250 epochs (vs 500 for full TuckER) due to pretrained base

- Failure signatures:
  - Pure TPTF significantly underperforms TuckER on FB15k-237 → dataset has too many "active vertices" violating power law (expected)
  - MIG-TF underperforms RotH on WN18RR MRR → strict hierarchy favors pure hyperbolic (known limitation)
  - NaN gradients → check β not too high; verify Lorentz inner products stay in valid range
  - No improvement over frozen TuckER → d_L too small or learning rate too low

- First 3 experiments:
  1. **Baseline reproduction**: Load/train TuckER on all three datasets; confirm metrics match Table 3 (FB15k-237 HR@10=0.544)
  2. **Curvature sweep**: Train MIG-TF with β ∈ {0.5, 1.0, 1.5, 2.0, 2.5} on WN18RR; plot HR@10 vs β to verify optimal ≈1.5 (reproduce Figure 7)
  3. **Geometry ablation**: Vary μ in S = μ·S_E + (1-μ)·S_H on WN18RR; confirm peak at μ=0.5 (Figure 11), validating equal contribution design

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the incorporation of spherical geometry into the MIG-TF framework improve performance on knowledge graphs with prevalent cyclic structures?
- **Basis in paper:** [explicit] The authors note on Page 4 that while they did not use spherical geometry, their framework is "sufficiently flexible to accommodate the incorporation of spherical geometry, presenting a promising direction for future research."
- **Why unresolved:** The current work limits itself to a mixture of Euclidean and hyperbolic geometries, observing that spherical spaces (used in competitor models like M2GNN) handle different structural properties (cycles) which were not the primary focus of this specific hybridization.
- **What evidence would resolve it:** A comparative analysis of a three-geometry variant (Euclidean + Hyperbolic + Spherical) against the current MIG-TF model on datasets containing a mix of hierarchical, cyclic, and "active" vertex structures.

### Open Question 2
- **Question:** Does replacing the frozen TuckER component with a more powerful Euclidean or complex model (e.g., RotE or ComplEx-N3) improve performance on datasets where MIG-TF narrowly trails the state-of-the-art?
- **Basis in paper:** [explicit] On Page 8, regarding the slightly lower metrics on WN18RR compared to purely hyperbolic models, the authors state: "A possible remedy to this effect would be changing Euclidean component of MIG-TF model to more powerful model... We leave investigation of this approach for future work."
- **Why unresolved:** The current implementation relies on TuckER as the Euclidean base, which may lack the specific inductive biases required for certain relation types found in WordNet (WN18RR), limiting the mixed model's ceiling.
- **What evidence would resolve it:** Benchmark results from a modified MIG-TF architecture where the TuckER core is swapped for RotE or ComplEx-N3, specifically measuring improvements in MRR on the WN18RR dataset.

### Open Question 3
- **Question:** Does freezing the pretrained Euclidean embeddings limit the model's capacity to find an optimal geometric alignment compared to an end-to-end joint training approach?
- **Basis in paper:** [inferred] The methodology (Section 3.3 and Figure 5) explicitly freezes the TuckER model and trains only the low-parametric hyperbolic correction term. This design prioritizes parameter efficiency but assumes the Euclidean embeddings are optimal features for the hyperbolic correction.
- **Why unresolved:** By freezing the Euclidean component, the model cannot adjust its manifold based on the hyperbolic error signals. It is unclear if this static foundation restricts the expressivity of the mixed-geometry interaction.
- **What evidence would resolve it:** An ablation study comparing the current "frozen-base" training strategy against a "joint-training" strategy where gradients backpropagate through both the hyperbolic and Euclidean components, analyzing the trade-off between parameter cost and final accuracy.

## Limitations
- Dataset-dependent performance advantage not fully explained by hierarchical structure metrics
- Parameter efficiency relies on pretrained TuckER availability, limiting practical deployment scenarios
- Square Lorentz distance approximation error not quantified against true geodesic distances

## Confidence
- High confidence: Parameter efficiency mechanism through pretrained base + low-rank correction; square Lorentz distance stability claims; core mathematical derivations for TPTF score function
- Medium confidence: Mixed geometry expressivity claims for heterogeneous KGs; performance superiority over purely Euclidean/hyperbolic baselines; practical significance of fewer parameters
- Low confidence: Theoretical justification for why equal weighting (μ=0.5) is optimal; generalization to KGs beyond the three benchmark datasets; robustness to KG noise claims without systematic noise injection experiments

## Next Checks
1. **Hierarchical structure correlation**: Compute power-law distribution metrics and active vertex counts for FB15k-237, YAGO3-10, WN18RR; correlate with MIG-TF performance gains to validate the heterogeneous structure hypothesis
2. **Square distance approximation error**: Implement both square Lorentz distance and true geodesic distance versions of TPTF; measure MRR differences and training stability across β ∈ [1.0, 2.5] to quantify approximation impact
3. **Parameter efficiency deployment test**: Train MIG-TF from scratch without pretrained TuckER (training all parameters); compare parameter count, training time, and final metrics against pure TuckER to isolate the pretrained model's contribution to efficiency claims