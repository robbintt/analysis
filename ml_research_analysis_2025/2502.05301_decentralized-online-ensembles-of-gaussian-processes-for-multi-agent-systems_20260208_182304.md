---
ver: rpa2
title: Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems
arxiv_id: '2502.05301'
source_url: https://arxiv.org/abs/2502.05301
tags:
- learning
- each
- agent
- bayesian
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fully decentralized method for learning Gaussian
  processes using random feature approximation, addressing the challenge of distributed
  learning in multi-agent systems without central fusion centers. The core method
  uses consensus-based information filtering where each agent maintains local posterior
  information that is averaged across neighbors to approximate global statistics.
---

# Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems

## Quick Facts
- arXiv ID: 2502.05301
- Source URL: https://arxiv.org/abs/2502.05301
- Authors: Fernando Llorente; Daniel Waxman; Petar M. Djurić
- Reference count: 29
- Primary result: Decentralized RF-GP achieves competitive running MSE to centralized GPs while maintaining constant-size communication per time step

## Executive Summary
This paper proposes a fully decentralized method for learning Gaussian processes using random feature approximation, addressing the challenge of distributed learning in multi-agent systems without central fusion centers. The core method uses consensus-based information filtering where each agent maintains local posterior information that is averaged across neighbors to approximate global statistics. For hyperparameter selection, the authors introduce an online Bayesian model averaging scheme that combines predictions from multiple kernel models based on their marginal likelihood. The approach is evaluated against several state-of-the-art methods on real-world datasets, showing that the decentralized random feature GP method achieves competitive performance while maintaining scalability through constant-size communication per time step.

## Method Summary
The method combines three key components: (1) Random Fourier features approximate the GP kernel as a Bayesian linear model with basis functions ϕ(x) ∈ R^{2J}, enabling tractable distributed inference through information form updates; (2) Consensus-based information filtering where each agent computes local sufficient statistics (precision D and information vector η) that are iteratively averaged across neighbors over L rounds to approximate global statistics; (3) Online Bayesian Model Averaging that maintains M independent RF-GP models with different kernel hyperparameters and combines their predictions using weights updated based on cumulative log predictive density. The method scales as O(J²L) per time step where J is the number of random features and L is the consensus iterations, with constant communication cost independent of agent count N.

## Key Results
- Decentralized RF-GP-1 (single model) achieves MSE ratio 0.972 vs centralized SVI GP baseline on Tom's Hardware dataset
- Decentralized RF-GP-5 (ensemble) achieves MSE ratio 0.936 vs centralized SVI GP baseline, outperforming D-RF-GP-1
- Consensus BMA weights evolve online and show adaptive behavior across different kernel lengthscales
- Performance is competitive with centralized GP methods while maintaining fully decentralized operation

## Why This Works (Mechanism)

### Mechanism 1: Random Feature Approximation Converts GP to Bayesian Linear Regression
The method makes GP inference tractable in distributed settings by approximating the GP as a Bayesian linear model. Random Fourier features are sampled from the kernel's power spectral density S(ω). For stationary kernels, this creates a basis expansion ϕ(x) ∈ R^{2J} such that k(x,x') ≈ ϕ(x)^T ϕ(x'). The GP posterior over function values becomes a Gaussian posterior over parameters θ ∈ R^{2J}, with closed-form updates in information form: D_c = Σ_{n=1}^N P_n and η_c = Σ_{n=1}^N s_n. Core assumption: The kernel is stationary (depends only on x - x'), enabling spectral representation.

### Mechanism 2: Consensus-Based Aggregation of Local Posterior Statistics
Agents approximate the global posterior through iterative neighbor averaging without a fusion center. Each agent computes local sufficient statistics (P_{n,t}, s_{n,t}) from its data batch. Through L rounds of consensus, agents iteratively average these quantities with neighbors using uniform weights. The consensus estimates (P̃^{(L)}, s̃^{(L)}) are then scaled by N and accumulated incrementally to approximate global statistics. Core assumption: The communication graph is connected; L is sufficient for consensus convergence given graph spectral properties.

### Mechanism 3: Online Bayesian Model Averaging for Hyperparameter Robustness
Ensembling multiple fixed-hyperparameter models with BMA weights avoids expensive online hyperparameter optimization. Each agent runs M independent RF-GP models with different kernel hyperparameters (e.g., lengthscales). Weights w^{(m)}_{n,t} are updated online based on cumulative log predictive density: log ẽ^{(m)}_{n,t+1} = log ẽ^{(m)}_{n,t} + log N(y_{n,t+1} | ŷ^{(m)}_{n,t+1}, σ^{(m),2}_{n,t+1}). The "independent consensus BMA" variant aggregates log-likelihoods across agents. Core assumption: The decomposition of marginal likelihood across agents (under independence) is a reasonable approximation despite theoretical inexactness.

## Foundational Learning

- **Gaussian Process Regression and Kernels**: Why needed here: The paper builds directly on GP theory; understanding how kernels define covariance and why exact GP inference scales O(T³) motivates the random feature approximation. Quick check question: Can you explain why computing the GP posterior requires inverting a T×T matrix, and how random features avoid this?

- **Information Form vs. Moment Form for Gaussian Distributions**: Why needed here: The method maintains posteriors in information form (precision matrix D, information vector η) because these quantities add across agents—moment form would require products of Gaussians with complex covariance updates. Quick check question: Given a Gaussian N(μ, Σ), what is its information form representation, and why is it preferable for distributed fusion?

- **Average Consensus Algorithms**: Why needed here: The core decentralized mechanism relies on consensus; understanding convergence rates, graph spectral gaps, and weighting schemes (uniform vs. Metropolis) is essential for implementation. Quick check question: For a connected graph with diameter d, how many consensus iterations L are typically needed to reach ε-approximate consensus?

## Architecture Onboarding

- **Component map**: Data stream (x_{n,t}, y_{n,t}) → Random feature mapping ϕ^{(m)}(x) [M parallel] → Local statistics (P_{n,t}^{(m)}, s_{n,t}^{(m)}) → Consensus module (L rounds neighbor communication) → Global approximation (D_{n,t}^{(m)}, η_{n,t}^{(m)}) → BMA weight update (w_{n,t}^{(m)}) → Ensemble prediction ŷ_{n,t+1}

- **Critical path**: The consensus step is the synchronization bottleneck—incorrect implementation here propagates errors to all downstream predictions. Start by validating consensus convergence on synthetic data before enabling the full pipeline.

- **Design tradeoffs**: J (random features): Higher J improves kernel approximation but increases communication O(J²L) and inversion cost O(J³); M (ensemble size): More models improve robustness but multiply memory and communication linearly; L (consensus rounds): More rounds improve accuracy but increase latency per time step

- **Failure signatures**: Consensus divergence: Statistics explode or oscillate → check graph connectivity, edge weights; Weight collapse: Single BMA weight dominates → normalize weights frequently, check predictive variance computation; Predictive variance going negative: Numerical issues in D^{-1} → ensure D remains positive definite through proper accumulation; Slow convergence: Running MSE plateaus early → likely insufficient L or poor hyperparameter coverage in ensemble

- **First 3 experiments**: 1) Consensus validation: Implement single-model RF-GP (M=1) on synthetic 1D regression with fully connected graph; verify posterior converges to centralized baseline as L increases; 2) Ensemble ablation: Compare M∈{1,3,5} ensemble sizes on Tom's Hardware dataset; measure how BMA weights evolve and whether performance improves over single best model; 3) Communication constraint study: Vary L∈{1,5,10,20} on random graph with N=5 agents; plot running MSE vs. communication cost to identify operating points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more expressive basis expansions (e.g., structure-exploiting or deep random features) improve decentralized RF-GP performance while maintaining communication efficiency?
- Basis in paper: [explicit] The conclusion states "Future work could incorporate more expressive basis expansions, as shown beneficial by [15]."
- Why unresolved: The current method uses standard random Fourier features; more expressive features have shown benefits in centralized settings but their distributed behavior is unknown.
- What evidence would resolve it: Empirical comparison of decentralized GPs using alternative basis expansions (e.g., orthogonal or quasi-random features) on benchmark datasets, measuring both prediction accuracy and communication cost.

### Open Question 2
- Question: Can alternative weighting schemes beyond online BMA improve ensemble combination in the decentralized setting?
- Basis in paper: [explicit] The conclusion states "Future work could... incorporate alternative weighting algorithms."
- Why unresolved: The independence assumption required for consensus BMA (Eq. 22) does not strictly hold, potentially introducing approximation error into model weights.
- What evidence would resolve it: Comparison of BMA against alternative weighting approaches (e.g., variational model selection, regret-minimizing weights) with analysis of sensitivity to the independence approximation.

### Open Question 3
- Question: How does network topology and spectral properties quantitatively affect the required number of consensus iterations L for convergence?
- Basis in paper: [inferred] The scalability discussion notes that L "may influence the choice in L depending on the spectral properties of the graph" without providing theoretical bounds.
- Why unresolved: The paper uses fixed L=10 empirically but lacks theoretical guidance on selecting L based on graph structure or agent count.
- What evidence would resolve it: Theoretical analysis linking graph spectral gap to consensus convergence rate, validated with experiments varying network diameter and connectivity.

### Open Question 4
- Question: Why does the multi-agent D-RF-GP-5 achieve performance comparable to single-agent D-RF-GP-1 rather than showing clearer benefits from distributed observations?
- Basis in paper: [inferred] The experiments note "D-RF-GP-5 tends to perform similarly to D-RF-GP-1" without further analysis or explanation.
- Why unresolved: One would expect information aggregation across agents to improve predictions; the lack of improvement suggests potential limitations in the consensus approximation or ensemble scheme.
- What evidence would resolve it: Controlled experiments varying data heterogeneity across agents and analyzing per-agent versus aggregated posterior quality.

## Limitations
- Consensus method assumes undirected graphs with uniform weights, limiting applicability to directed or time-varying networks
- No theoretical guarantees provided for the BMA approximation's impact on predictive performance
- Ensemble size M=3 is small, potentially missing optimal hyperparameters

## Confidence
- Random feature approximation mechanism: High confidence (well-established mathematical foundation)
- Consensus-based aggregation mechanism: Medium confidence (correct theory but limited empirical validation)
- Online BMA ensemble approach: Medium-Low confidence (acknowledged approximation in Equation 22, lack of rigorous error bounds)

## Next Checks
1. Test consensus convergence on directed graphs using Metropolis weights to verify robustness beyond the undirected case
2. Implement cross-validation study to optimize J (random features) and L (consensus rounds) parameters rather than using fixed values
3. Add ablation studies comparing consensus BMA approach against centralized BMA and local-only BMA to quantify the approximation error in Equation 22