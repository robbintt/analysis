---
ver: rpa2
title: 'When language and vision meet road safety: leveraging multimodal large language
  models for video-based traffic accident analysis'
arxiv_id: '2501.10604'
source_url: https://arxiv.org/abs/2501.10604
tags:
- traffic
- video
- visual
- object
- road
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SeeUnsafe, a framework that leverages multimodal
  large language models (MLLMs) to automate traffic accident analysis from video footage.
  The key innovation is a severity-based aggregation strategy that splits long videos
  into clips, processes them in parallel, and classifies events based on the most
  severe clip.
---

# When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis

## Quick Facts
- **arXiv ID:** 2501.10604
- **Source URL:** https://arxiv.org/abs/2501.10604
- **Reference count:** 40
- **Primary result:** Proposed SeeUnsafe framework achieves 76.31% video classification accuracy and 51.47% visual grounding success rate on Toyota Woven Traffic Safety dataset.

## Executive Summary
This paper presents SeeUnsafe, a framework that leverages multimodal large language models (MLLMs) to automate traffic accident analysis from video footage. The key innovation is a severity-based aggregation strategy that splits long videos into clips, processes them in parallel, and classifies events based on the most severe clip. The framework uses structured multimodal prompts to guide MLLMs in generating responses and visual prompts for fine-grained object identification. Experiments on the Toyota Woven Traffic Safety dataset show that SeeUnsafe outperforms existing models in both video classification and visual grounding tasks.

## Method Summary
SeeUnsafe processes traffic accident videos by first sampling 9 frames uniformly and splitting them into 3 clips of 3 frames each. Visual prompts (object contours) are generated using GroundingDINO and Segment Anything models. Each clip is processed by an MLLM (GPT-4o or LLaVA-NeXT) with structured prompts to generate structured responses. Video classification uses severity-based aggregation (collision > near-miss > normal), while visual grounding identifies specific objects involved in critical events. The framework introduces Information Matching Score (IMS) as a novel evaluation metric using MLLMs to assess response quality beyond traditional NLP metrics.

## Key Results
- SeeUnsafe achieves 76.31% video classification accuracy on Toyota Woven Traffic Safety dataset
- Visual grounding success rate of 51.47%, outperforming existing models
- Severity-based aggregation strategy effectively handles temporal dependencies in long videos
- Structured multimodal prompts and visual prompts significantly improve both classification and grounding performance

## Why This Works (Mechanism)

### Mechanism 1: Severity-Based Aggregation for Temporal Handling
- Claim: Splitting long videos into clips and aggregating based on maximum severity enables MLLMs to process extended footage without degrading performance on rare events.
- Mechanism: The framework divides a video into K non-overlapping clips, processes each independently, and classifies based on the maximum severity label among all clips rather than majority voting.
- Core assumption: Critical traffic events are temporally localized, and severity hierarchy correctly prioritizes safety-critical events over normal footage.
- Evidence anchors: Abstract states "severity-based aggregation strategy that splits long videos into clips, processes them in parallel, and classifies events based on the most severe clip."

### Mechanism 2: Visual Prompting for Fine-Grained Grounding
- Claim: Augmenting video frames with object contours derived from segmentation models enables MLLMs to perform object-level visual grounding.
- Mechanism: GroundingDINO identifies objects in the first frame, Segment Anything generates instance masks, and object contours are overlaid onto video frames to provide spatial indexing.
- Core assumption: Pre-processing detection and tracking models are sufficiently accurate, and overlaid contours enhance MLLM attention without excessive noise.
- Evidence anchors: Abstract mentions "visual prompts for fine-grained object identification... 51.47% success rate in visual grounding."

### Mechanism 3: Structured Prompting and IMS for Evaluation
- Claim: Enforcing structured outputs via prompts and evaluating with MLLM-based IMS provides more reliable assessment than traditional NLP metrics.
- Mechanism: Textual prompts elicit responses in four specific attributes, and GPT-4o scores semantic alignment between predicted and ground-truth attributes (0-100).
- Core assumption: MLLMs are better at semantic comparison than metrics relying on surface-level token overlap.
- Evidence anchors: Abstract states "A new Information Matching Score (IMS) metric is introduced to evaluate response quality beyond traditional NLP metrics."

## Foundational Learning

- **Video Question Answering (VideoQA) & Temporal Context in MLLMs**: Needed to understand why chunking and aggregation strategies are necessary workarounds for limited temporal context windows. Quick check: Why does SeeUnsafe split videos into 3 clips instead of feeding all frames at once?
- **Object Grounding & Segmentation (GroundingDINO, SAM)**: Required to understand the preprocessing pipeline and potential failure points. Quick check: What happens if GroundingDINO fails to detect a partially occluded pedestrian in the first frame?
- **Evaluation Metrics for Generative Models**: Essential to understand why BLEU/ROUGE are poor fits for safety-critical analysis. Quick check: Why would high ROUGE score be misleading when evaluating collision descriptions?

## Architecture Onboarding

- **Component map**: Video -> Frame Sampler -> Detection/Segmentation -> MLLM Agent (Classification) -> Aggregation -> Visual Grounding Agent
- **Critical path**: Video → Frame Sampler → Detection/Segmentation → MLLM Agent (Classification) → Aggregation → Visual Grounding Agent
- **Design tradeoffs**: Model size vs. cost/latency (GPT-4o vs. GPT-4o mini), visual prompts (improve grounding but can degrade classification accuracy), clip granularity (3 clips of 3 frames each is default)
- **Failure signatures**: MLLM "hallucination" of incorrect justifications, upstream detection failure causing grounding failure, performance drop with visual prompts for smaller models
- **First 3 experiments**: Ablate visual prompts to measure tradeoff, vary aggregation strategy (majority voting vs. max severity), stress test temporal handling with varying clip numbers

## Open Questions the Paper Calls Out

- **Open Question 1**: Can uncertainty-aware aggregation or keyframe sampling methods improve classification accuracy compared to current uniform splitting and severity-based heuristics? [explicit: Section 6 suggests current methods could benefit from "advanced keyframe sampling and uncertainty-aware aggregation"]
- **Open Question 2**: How does fusing multi-view data impact visual grounding success rates and classification robustness? [explicit: Section 6 notes "Single-view input is prone to occlusion and can be enriched with multi-view footage"]
- **Open Question 3**: What architectural or prompting improvements are required to effectively distinguish near-miss events from collisions under occlusion? [explicit: Section 5.2 identifies that "near-misses are often confused with collisions, especially under occlusion"]

## Limitations

- Severity-based aggregation assumes critical events are temporally localized within single clips and doesn't address events spanning multiple clips
- Structured prompting and IMS metric rely heavily on subjective alignment of evaluation MLLM's scoring with human judgment
- Heavy dependence on accuracy of upstream models (GroundingDINO, SAM) for visual grounding introduces significant potential failure points

## Confidence

- **High Confidence**: Framework's core design and demonstrated improved performance over baselines on WTS dataset
- **Medium Confidence**: Specific mechanism by which visual prompts enhance grounding without degrading classification for all MLLM sizes and lighting conditions
- **Low Confidence**: Absolute performance metrics without clear comparison to strong baseline for visual grounding task specifically

## Next Checks

1. **Temporal Grounding Robustness**: Test on dataset with events of varying durations, including those spanning multiple clips, to measure impact on classification accuracy and grounding success
2. **IMS Metric Validation**: Conduct user study comparing IMS scores with human expert evaluations to quantify correlation and identify systematic biases
3. **Visual Prompt Ablation Across Models**: Systematically evaluate impact of visual prompts on wider range of MLLM sizes and types across different lighting conditions and video qualities