---
ver: rpa2
title: 'ADVICE: Answer-Dependent Verbalized Confidence Estimation'
arxiv_id: '2510.10913'
source_url: https://arxiv.org/abs/2510.10913
tags:
- confidence
- answer
- advice
- verbalized
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies answer-independence\u2014the failure of\
  \ LLMs to condition confidence estimates on their own generated answers\u2014as\
  \ the primary cause of overconfidence in verbalized confidence. To address this,\
  \ the authors propose ADVICE, a fine-tuning framework that explicitly trains models\
  \ to ground confidence in their answers by contrasting confidence distributions\
  \ for correct and incorrect responses."
---

# ADVICE: Answer-Dependent Verbalized Confidence Estimation

## Quick Facts
- arXiv ID: 2510.10913
- Source URL: https://arxiv.org/abs/2510.10913
- Reference count: 25
- Key outcome: Fine-tuning framework that explicitly trains LLMs to ground confidence in their generated answers, substantially improving calibration metrics while maintaining task performance.

## Executive Summary
This paper identifies answer-independence—the failure of LLMs to condition confidence estimates on their own generated answers—as the primary cause of overconfidence in verbalized confidence. To address this, the authors propose ADVICE, a fine-tuning framework that explicitly trains models to ground confidence in their answers by contrasting confidence distributions for correct and incorrect responses. Experiments show ADVICE substantially improves calibration metrics (e.g., reducing ECE by ~6–11 points) across multiple LLMs, datasets, and verbalization types, while maintaining task performance and demonstrating strong out-of-distribution generalization. The gains are shown to arise from increased answer-groundedness, offering a robust and efficient solution to overconfidence in verbalized confidence estimation.

## Method Summary
ADVICE fine-tunes LLMs to generate confidence estimates that are explicitly conditioned on their own generated answers. The framework trains models to produce distinct confidence distributions for correct versus incorrect responses by incorporating answer-dependent signals during training. This involves generating confidence statements that vary meaningfully based on answer quality rather than relying on generic confidence expressions. The approach works with various verbalization formats and can be applied to models with or without chain-of-thought reasoning capabilities.

## Key Results
- Reduces Expected Calibration Error (ECE) by approximately 6–11 percentage points across multiple LLMs and datasets
- Maintains or improves task performance while substantially improving calibration
- Demonstrates strong out-of-distribution generalization on held-out examples from the same datasets
- Improvements attributed to increased answer-groundedness in confidence estimation

## Why This Works (Mechanism)
The paper identifies that LLMs typically generate confidence estimates independently of their actual answers, leading to generic and often overconfident verbalizations. ADVICE addresses this by explicitly training models to ground confidence in the content and quality of their generated answers. By contrasting confidence distributions for correct and incorrect responses during fine-tuning, the model learns to produce confidence estimates that genuinely reflect the reliability of its answers rather than defaulting to high-confidence statements.

## Foundational Learning
- **Verbalized Confidence Estimation**: Why needed - to provide interpretable confidence measures alongside model outputs. Quick check - can the model express uncertainty appropriately?
- **Answer-Independence Problem**: Why needed - identifies why LLMs produce overconfident confidence statements. Quick check - do confidence estimates vary meaningfully with answer quality?
- **Calibration Metrics (ECE, MCE)**: Why needed - quantify the alignment between predicted confidence and actual accuracy. Quick check - do calibration metrics improve post-training?
- **Fine-tuning vs. Prompting**: Why needed - establishes the methodological approach for implementing ADVICE. Quick check - does fine-tuning outperform prompting-based approaches?
- **Chain-of-Thought Reasoning**: Why needed - context for model capabilities during training. Quick check - do improvements transfer to models without CoT?
- **Out-of-Distribution Generalization**: Why needed - validates robustness beyond training data. Quick check - does performance hold on genuinely different task types?

## Architecture Onboarding

**Component Map**: Data → ADVICE Fine-tuning → Calibrated Model → Verbalized Confidence + Answer

**Critical Path**: The training loop generates answers → evaluates correctness → computes confidence → updates model parameters to align confidence with answer quality → produces calibrated confidence estimates for deployment.

**Design Tradeoffs**: Fine-tuning offers better integration but requires training resources; prompting is lighter but less effective. The framework balances between calibration improvement and maintaining task performance, avoiding the pitfall of sacrificing accuracy for better calibration.

**Failure Signatures**: Models that continue to produce generic high-confidence statements regardless of answer quality; degradation in task performance; failure to generalize beyond training distribution; confidence estimates that don't reflect actual answer correctness.

**First Experiments**: 1) Baseline calibration measurement on held-out data; 2) Apply ADVICE fine-tuning and measure ECE improvement; 3) Test generalization to OOD examples from same domain.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to entirely different domains (beyond variations of existing datasets) remains uncertain
- Performance on models without chain-of-thought reasoning capabilities is unclear
- The causal relationship between answer-groundedness and calibration improvements is primarily correlational
- Potential trade-offs between calibration and other desirable properties like informativeness are not extensively explored

## Confidence
- High: Answer-independence contributes to overconfidence
- High: ADVICE improves calibration metrics on tested datasets
- Medium: Improved calibration stems specifically from increased answer-groundedness
- Medium: Strong OOD generalization claims

## Next Checks
1. Test ADVICE on fundamentally different task types (e.g., summarization, translation, code generation) to assess domain transferability
2. Evaluate the approach on models without chain-of-thought capabilities to determine if improvements depend on this feature
3. Conduct a controlled ablation study isolating the effect of answer-groundedness from other factors contributing to calibration improvements