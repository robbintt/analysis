---
ver: rpa2
title: 'TopicProphet: Prophesies on Temporal Topic Trends and Stocks'
arxiv_id: '2512.11857'
source_url: https://arxiv.org/abs/2512.11857
tags:
- stock
- topics
- data
- topic
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopicProphet improves stock price forecasting by optimizing training
  data through topic trend analysis. The framework segments historical data using
  breakpoint detection on topic trends from news articles, then selects the segment
  most similar to the target period for training.
---

# TopicProphet: Prophesies on Temporal Topic Trends and Stocks

## Quick Facts
- arXiv ID: 2512.11857
- Source URL: https://arxiv.org/abs/2512.11857
- Reference count: 40
- Key outcome: Improves stock price forecasting by optimizing training data through topic trend analysis

## Executive Summary
TopicProphet is a framework that improves stock price forecasting by selecting historically similar training periods based on topic trend analysis. The system extracts keywords from news articles, clusters them into semantic topics, and analyzes their temporal trends to identify structurally similar historical periods for model training. By aligning training data with periods sharing similar socioeconomic contexts, TopicProphet achieves lower prediction error than traditional approaches that use recent data or full historical spans.

## Method Summary
TopicProphet processes news article keywords to extract topic trends, which are then used to segment historical stock data and select optimal training periods. The framework employs BGE embeddings for keyword representation, UMAP for dimensionality reduction, and HDBSCAN for topic clustering. PELT breakpoint detection identifies structural changes in topic trends, enabling segmentation of historical data. Segments are ranked by similarity to the target period using a weighted combination of Pearson correlation, dynamic time warping, and cosine similarity. The selected segment trains a Facebook Prophet model with topic trends as external regressors.

## Key Results
- Reduces MSE from 5203.81 to 4856.98 on S&P 500 forecasting
- Demonstrates better performance on both S&P 500 and NASDAQ datasets
- Shows that aligned training periods with similar socioeconomic contexts enhance model accuracy

## Why This Works (Mechanism)

### Mechanism 1
Topic trends derived from news keywords capture socioeconomic context that pure quantitative stock data cannot. Keywords from NYT articles are embedded, dimensionally reduced, clustered, and labeled into semantic topics. Topic frequency time series serve as regressors in the Prophet model, injecting contextual signals into price prediction. Core assumption: News topic frequency reflects market-relevant public sentiment and macro conditions that influence trader behavior.

### Mechanism 2
Segmented training data from historically similar periods outperforms full-span or recent-only training. PELT breakpoint detection identifies structural changes in topic trend series. Historical data is segmented at midpoints between breakpoints. The segment whose topic trends most closely match the forecast period is selected for training. Core assumption: Periods with similar topic trend profiles exhibit similar market dynamics and price response patterns.

### Mechanism 3
A weighted combination of Pearson correlation, cosine similarity, and dynamic time warping effectively ranks segment relevance. Each topic's weight combines linear correlation with stock data, temporal alignment (DTW), and vector similarity. Segments are ranked by summed topic weights; lowest total score = optimal training segment. Core assumption: Topics with higher correlation to stock movement should have higher influence, and DTW captures shape similarity despite temporal misalignment.

## Foundational Learning

- **Breakpoint/Changepoint Detection (PELT)**: Core to segmenting historical data into regime-homogeneous periods; PELT efficiently finds optimal breakpoints without pre-specifying count. Quick check: Given a time series with 3 clear structural breaks, can you explain why PELT's penalty parameter matters for avoiding over-segmentation?

- **Dynamic Time Warping (DTW)**: Enables comparison of topic trend shapes between historical and target periods even when timing differs. Quick check: Two series have identical shape but one is shifted by 30 days—why would Pearson correlation fail but DTW succeed?

- **Prophet Additive Regressors**: Topic frequency series are injected as external regressors; understanding how Prophet incorporates regressors is essential for debugging prediction quality. Quick check: If a topic regressor has missing values for 20% of dates, what happens to Prophet's forecast uncertainty?

## Architecture Onboarding

- Component map: Keyword extraction -> BGE embeddings -> UMAP (n_components=5, n_neighbors=15) -> HDBSCAN (min_cluster_size=200) -> GPT-4 labeling -> 114 topics -> PELT breakpoint detection -> Segment at midpoints -> Similarity scoring (Pearson×DTW×Cosine) -> Prophet training -> Stock prediction

- Critical path: Keyword extraction quality → topic coherence → breakpoint accuracy → segment relevance → prediction error. Topic modeling quality is the primary bottleneck.

- Design tradeoffs: Pre-extracted keywords vs. full article text (used pre-extracted for cost; tradeoff is lost nuance); PELT vs. Prophet built-in changepoint (PELT chosen for flexibility and unsupervised detection); Single model (Prophet only) vs. multi-model comparison (limits generalizability claims but reduces variance in ablation).

- Failure signatures: High MSE on certain segments indicates topic selection mismatch during high-volatility regime; "Miscellaneous" topic clusters dilute signal; Media/entertainment topics as regressors hurt performance.

- First 3 experiments: 1) Baseline validation: Run Prophet on S&P 500 full-span data only; confirm MSE ~5203.81. Then add all topic trends as regressors; verify MSE drops to ~4856.98. 2) Segment ablation: Using "Business and Corporations" topic breakpoints, train on 2008-only segment and predict 2020 pandemic period. Compare vs. recent-3-month training. 3) Topic sensitivity: Iteratively withhold one topic at a time; plot MSE change. Identify "error-fixing" topics vs. harmful topics.

## Open Questions the Paper Calls Out

- Does the TopicProphet framework improve prediction accuracy when applied to deep learning architectures (e.g., LSTMs, Transformers) compared to the Facebook Prophet model? The authors plan to conduct research on different models to see if the effectiveness of trend analysis persists across different circumstances.

- Does processing full article text directly yield better predictive performance than using pre-extracted keywords? The authors note that many nuances each article had were lost in the process of using keywords and plan to process the article directly for topic modeling using LLMs.

- Can the topic clustering phase be refined to eliminate "miscellaneous" clusters that obscure predictive signals? The ablation study showed that removing "miscellaneous" topics caused performance drops, yet the authors admit topic modeling was not optimal with many clusters labeled as "mixed topics."

## Limitations

- Narrow empirical foundation: validation performed only on S&P 500 and NASDAQ indices with one external knowledge source (NYT keywords)
- Questionable topic modeling quality: Purity=0.50 and ARI=0.042 indicate weak cluster separation
- Unknown breakpoint detection sensitivity: performance impact of penalty parameter variations unexplored

## Confidence

- High confidence: Topic trends as external regressors improve baseline Prophet predictions
- Medium confidence: Historical period segmentation based on topic similarity improves predictions
- Low confidence: The specific combination of Pearson + DTW + Cosine similarity is optimal

## Next Checks

1. Apply TopicProphet to at least three additional datasets (e.g., different stock indices, commodities, or crypto) using identical preprocessing and parameters to measure whether MSE reduction pattern holds across domains.

2. Re-run the full pipeline with alternative topic modeling approaches (e.g., LDA, BERTopic) and different UMAP/HDBSCAN parameters to compare prediction performance and establish sensitivity to topic extraction quality.

3. Systematically vary PELT penalty parameters and segment selection criteria to test whether performance improvements persist across a range of segmentation granularities, or if results are fragile to hyperparameter choices.