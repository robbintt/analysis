---
ver: rpa2
title: 'Story2MIDI: Emotionally Aligned Music Generation from Text'
arxiv_id: '2512.02192'
source_url: https://arxiv.org/abs/2512.02192
tags:
- music
- arousal
- valence
- emotion
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Story2MIDI introduces a Transformer-based sequence-to-sequence
  model for generating emotion-aligned music from text. The model is trained on a
  newly constructed dataset pairing short text blurbs with MIDI music clips that share
  the same emotional valence and arousal.
---

# Story2MIDI: Emotionally Aligned Music Generation from Text

## Quick Facts
- arXiv ID: 2512.02192
- Source URL: https://arxiv.org/abs/2512.02192
- Authors: Mohammad Shokri; Alexandra C. Salem; Gabriel Levine; Johanna Devaney; Sarah Ita Levitan
- Reference count: 40
- Generates music aligned to text emotions using a sequence-to-sequence model trained on text-MIDI pairs labeled with valence-arousal coordinates

## Executive Summary
Story2MIDI introduces a Transformer-based sequence-to-sequence model that generates emotionally aligned symbolic music from text. The model leverages a newly constructed dataset pairing short text blurbs with MIDI clips that share the same valence-arousal coordinates. By fine-tuning a RoBERTa encoder via contrastive learning and a pre-trained MusicBERT decoder, the system produces music that varies significantly across emotion quadrants in measurable features like major-key ratio and note length. A listening study confirms that the generated music aligns better with arousal than valence, achieving 40% accuracy above chance for combined valence-arousal prediction. While promising, the model's ability to capture valence remains weaker, indicating room for further refinement.

## Method Summary
The Story2MIDI model uses a Transformer-based sequence-to-sequence architecture to generate MIDI music from text inputs. The encoder is a RoBERTa variant fine-tuned via contrastive learning to map text to distinct emotion quadrants in the valence-arousal space. The decoder is pre-trained on a large symbolic music corpus (MusicBERT) and fine-tuned on the emotion-aligned dataset. The model is trained on 6,193 text-MIDI pairs, each labeled with valence and arousal scores derived from the text. The generated music is evaluated using both objective metrics (e.g., major-key ratio, note length) and a listening study to assess emotional alignment.

## Key Results
- Generated music shows significant differences in major-key ratio and note length across emotion quadrants, reflecting sensitivity to valence and arousal.
- Listening study: arousal alignment is significantly better than chance; combined valence-arousal accuracy reaches 40% above chance.
- Model captures arousal-related musical features more effectively than valence-related ones, with valence alignment remaining challenging.

## Why This Works (Mechanism)
The model leverages contrastive learning to align text embeddings with emotion-specific musical features. By fine-tuning the encoder to map text to distinct valence-arousal quadrants, it ensures that the generated music reflects the emotional content of the input text. The pre-trained MusicBERT decoder provides a strong foundation for generating musically coherent sequences, while fine-tuning on the emotion-aligned dataset allows the model to adapt to the specific emotional context. The combination of these techniques enables the model to produce music that varies measurably across emotion quadrants.

## Foundational Learning
- **Valence-arousal space**: A 2D emotional model where valence represents positivity/negativity and arousal represents intensity. *Why needed*: To quantify and align text and music emotions. *Quick check*: Ensure the model maps text to distinct quadrants in this space.
- **Contrastive learning**: A training technique that pulls similar samples closer and pushes dissimilar ones apart in embedding space. *Why needed*: To fine-tune the text encoder for emotion alignment. *Quick check*: Verify the encoder produces distinct embeddings for different emotion quadrants.
- **Symbolic music representation**: Music encoded as sequences of tokens (e.g., MIDI events) rather than audio. *Why needed*: To enable generation using sequence models like Transformers. *Quick check*: Confirm the decoder can generate musically coherent sequences.
- **Pre-training**: Initializing model weights using a large corpus before fine-tuning on task-specific data. *Why needed*: To provide a strong foundation for the decoder. *Quick check*: Assess the decoder’s ability to generate music before fine-tuning.
- **Emotion alignment**: The process of ensuring generated music reflects the emotional content of the input text. *Why needed*: To validate the model’s effectiveness. *Quick check*: Compare objective metrics and perceptual results across emotion quadrants.

## Architecture Onboarding

**Component Map**: Text input -> RoBERTa encoder (fine-tuned via contrastive learning) -> Transformer decoder (MusicBERT) -> MIDI output

**Critical Path**: Text embedding (via RoBERTa) -> Emotion alignment (via contrastive learning) -> Music generation (via MusicBERT decoder)

**Design Tradeoffs**: The model prioritizes emotional alignment over musical complexity, using a small emotion-aligned dataset for fine-tuning. This limits generalizability but ensures focused training on the task.

**Failure Signatures**: Weak valence alignment in listening studies, over-reliance on arousal-related features, and limited generalizability to longer narratives or broader emotional contexts.

**First Experiments**:
1. Test the model’s ability to generate music for text inputs with extreme valence-arousal values (e.g., very positive/high arousal).
2. Evaluate the impact of increasing the size of the emotion-aligned dataset on valence alignment.
3. Assess the model’s performance on text inputs from different domains (e.g., literature, social media) to test generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- The model’s ability to align music with the valence dimension of emotion remains weak, as shown by listening study results.
- The reliance on a small emotion-aligned dataset (6,193 pairs) limits the model’s generalizability to broader emotional contexts or longer narrative sequences.
- The listening study’s small sample size (21 participants) and limited number of text-music pairs (15) reduce the robustness of the perceptual validation.

## Confidence
- **Objective metrics**: High confidence in the model’s ability to generate music with measurable differences across emotion quadrants.
- **Perceptual validation**: Medium confidence due to the limited scale of the listening study and weaker performance on valence alignment.
- **Generalizability**: Low confidence in the model’s ability to handle longer narratives or broader emotional contexts due to the small training dataset.

## Next Checks
1. Expand the listening study to include a larger, more diverse participant pool and a broader set of text-music pairs to better assess valence alignment.
2. Test the model’s performance on longer narrative sequences and more varied emotional contexts to evaluate its generalizability.
3. Conduct ablation studies to isolate the contributions of the contrastive learning objective and the pre-trained music decoder to the model’s emotional alignment capabilities.