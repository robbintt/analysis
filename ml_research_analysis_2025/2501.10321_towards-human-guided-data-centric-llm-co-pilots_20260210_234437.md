---
ver: rpa2
title: Towards Human-Guided, Data-Centric LLM Co-Pilots
arxiv_id: '2501.10321'
source_url: https://arxiv.org/abs/2501.10321
tags:
- data
- data-centric
- climb-dc
- tools
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CliMB-DC is a human-guided, data-centric LLM co-pilot framework
  that integrates domain expertise with advanced data-centric tools to address real-world
  data challenges in machine learning workflows. It introduces a novel multi-agent
  reasoning system combining a strategic coordinator for dynamic planning and adaptation
  with a specialized worker agent for precise execution, enabling sophisticated handling
  of data quality issues like missing values, label noise, and domain-specific nuances.
---

# Towards Human-Guided, Data-Centric LLM Co-Pilots

## Quick Facts
- arXiv ID: 2501.10321
- Source URL: https://arxiv.org/abs/2501.10321
- Reference count: 40
- Primary result: Human-guided data-centric LLM co-pilot framework achieving C-index of 0.953 for survival analysis and 0.816 for mortgage prediction

## Executive Summary
CliMB-DC introduces a novel human-guided, data-centric LLM co-pilot framework that bridges domain expertise with advanced data-centric tools to address real-world data challenges in machine learning workflows. The framework employs a multi-agent reasoning system with a strategic coordinator for dynamic planning and adaptation, paired with a specialized worker agent for precise execution. This architecture enables sophisticated handling of data quality issues including missing values, label noise, and domain-specific nuances across diverse predictive tasks. By supporting an extensible architecture that allows easy integration of new tools from the research community, CliMB-DC transforms uncurated data into ML-ready formats while maintaining contextual alignment with domain-specific requirements through human-in-the-loop validation.

## Method Summary
CliMB-DC implements a multi-agent reasoning system where a strategic coordinator agent dynamically plans and adapts data processing workflows based on human guidance and data characteristics, while a specialized worker agent executes precise data transformations and quality improvements. The framework integrates domain expertise with advanced data-centric tools through a modular architecture that supports classification, survival analysis, and regression tasks. Human-in-the-loop validation ensures contextual alignment of data processing actions with domain-specific requirements, with empirical evaluations demonstrating the framework's ability to handle real-world healthcare datasets and achieve superior performance compared to existing co-pilot baselines.

## Key Results
- Achieved C-index of 0.953 for survival analysis on healthcare datasets
- Achieved 0.816 accuracy for mortgage default prediction
- Significantly outperformed existing co-pilot baselines in transforming uncurated data into ML-ready formats

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual-agent architecture that separates strategic planning from tactical execution, allowing for sophisticated handling of complex data quality issues while maintaining adaptability to domain-specific contexts. The human-in-the-loop approach ensures that data processing actions remain aligned with expert knowledge and contextual requirements, while the extensible tool integration framework enables continuous improvement and adaptation to new data challenges as they emerge in research communities.

## Foundational Learning
- **Multi-agent reasoning systems**: Required for separating strategic planning from execution tasks; quick check: can the coordinator adapt plans based on new data insights while worker executes precise transformations
- **Human-in-the-loop validation**: Ensures domain expertise integration and contextual alignment; quick check: does human feedback meaningfully improve data processing quality
- **Extensible tool architecture**: Enables integration of new research community tools; quick check: can new tools be added without disrupting existing workflow functionality
- **Data quality assessment metrics**: Critical for evaluating missing values, label noise, and domain-specific issues; quick check: do metrics accurately reflect real-world data challenges
- **Cross-domain task adaptation**: Supports diverse predictive tasks across different fields; quick check: can framework maintain performance across classification, regression, and survival analysis tasks

## Architecture Onboarding

**Component Map:**
Human Expert -> Strategic Coordinator -> Specialized Worker Agent -> Data Processing Tools -> ML-ready Data Output

**Critical Path:**
1. Human expert provides domain context and validation
2. Strategic coordinator analyzes data and plans processing workflow
3. Specialized worker executes planned transformations
4. Tools apply data quality improvements
5. Output validated by human expert for contextual alignment

**Design Tradeoffs:**
- Human-in-the-loop provides contextual accuracy but may slow processing speed
- Multi-agent separation enables specialization but increases system complexity
- Extensible architecture allows tool integration but may introduce compatibility challenges
- Domain-specific validation ensures quality but requires expert availability

**Failure Signatures:**
- Poor human guidance leads to misaligned data processing actions
- Coordinator planning errors cascade to worker execution failures
- Tool integration issues prevent proper data quality improvements
- Insufficient validation results in contextually inappropriate outputs

**First Experiments:**
1. Test coordinator planning accuracy with synthetic data quality challenges
2. Validate worker agent execution precision on controlled transformation tasks
3. Evaluate human-in-the-loop effectiveness with domain experts on sample datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims may not generalize across all domain contexts or data types
- Reliance on human-in-the-loop validation introduces potential variability based on expert availability and consistency
- Reported results based on relatively small sample sizes (n=100 for mortgage data)
- Tool integration extensibility lacks specific details about complexity and maintenance overhead

## Confidence

**High Confidence:**
- Multi-agent architecture design and theoretical foundation for addressing data quality issues

**Medium Confidence:**
- Empirical performance claims on specific healthcare datasets given limited sample sizes
- Human-in-the-loop approach effectiveness pending broader validation across diverse expert populations

## Next Checks
1. Conduct scalability testing with larger, more diverse datasets (n > 10,000 samples) across multiple domains beyond healthcare
2. Perform longitudinal studies comparing expert-guided versus automated-only processing workflows
3. Implement and evaluate tool integration mechanism with at least three community-contributed tools