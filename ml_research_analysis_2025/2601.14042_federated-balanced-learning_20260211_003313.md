---
ver: rpa2
title: Federated Balanced Learning
arxiv_id: '2601.14042'
source_url: https://arxiv.org/abs/2601.14042
tags:
- knowledge
- data
- learning
- classes
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Federated Balanced Learning (FBL), a method
  that proactively prevents client drift in non-IID federated learning by achieving
  sample balance on the client side using edge-side generative models. Instead of
  correcting already-drifted models, FBL generates synthetic samples to fill data-scarce
  and data-missing classes while applying knowledge sampling to data-excessive classes,
  maintaining a fixed total sample count per client.
---

# Federated Balanced Learning

## Quick Facts
- arXiv ID: 2601.14042
- Source URL: https://arxiv.org/abs/2601.14042
- Reference count: 40
- Key outcome: Proactively prevents client drift in non-IID federated learning by achieving sample balance on the client side using edge-side generative models.

## Executive Summary
Federated Balanced Learning (FBL) introduces a novel approach to non-IID federated learning by proactively preventing client drift through sample balance on the client side. Instead of correcting already-drifted models, FBL generates synthetic samples to fill data-scarce and data-missing classes while applying knowledge sampling to data-excessive classes, maintaining a fixed total sample count per client. To bridge the domain gap between synthetic and real data, the authors propose a Knowledge Alignment Strategy with learnable embeddings and a Knowledge Drop Strategy for regularization. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that FBL significantly outperforms state-of-the-art baselines, achieving up to 33% accuracy improvement in highly heterogeneous settings.

## Method Summary
FBL achieves sample balance by calculating the mean sample count per client (balance point $B_k$) and synthesizing samples for scarce classes or knowledge sampling for excessive classes. For data-scarce or missing classes, an edge-side generative model (4-bit quantized Stable Diffusion) synthesizes samples until the class count reaches $B_k$. For data-excessive classes, the system retains the top-$B_k$ samples with the highest loss values (assumed to be the most informative). To bridge the domain gap between synthetic and real data, FBL adds learnable class-specific embeddings to synthetic features and applies a Knowledge Drop strategy to prevent overfitting. The method is evaluated on CIFAR-10, CIFAR-100, and Tiny-ImageNet with ResNet-18 backbone, 20 clients, 200 rounds, and 50% client participation.

## Key Results
- FBL achieves up to 33% accuracy improvement over state-of-the-art baselines in highly heterogeneous settings ($\alpha=0.1$)
- Significant performance gains across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets
- Outperforms FedAvg, FedProx, MOON, and SCAFFOLD in both accuracy and communication efficiency
- Maintains effectiveness in extended scenarios with varying client computational resources and noise robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Client drift can be mitigated by proactively normalizing local class distributions to a fixed "distributional balance point" ($B_k$) rather than correcting gradients post-hoc.
- **Mechanism:** The system calculates the mean sample count per client. For data-scarce or data-missing classes, an edge-side generative model synthesizes samples until the class count reaches $B_k$. This forces the local optimization landscape to approximate a balanced distribution before training begins.
- **Core assumption:** The distributional mean is a sufficient proxy for a "balanced" state, and edge devices possess sufficient compute to run quantized diffusion models without violating latency constraints.
- **Evidence anchors:** [abstract] "...prevent this issue from the beginning through sample balance on the client side." [section IV.A] Eq. 3 defines the balance point $B_k$; Section IV.B details "knowledge filling." [corpus] The neighbor paper *"Stratify: Rethinking Federated Learning for Non-IID Data through Balanced Sampling"* supports the hypothesis that data balancing is a viable alternative to algorithmic regularization.

### Mechanism 2
- **Claim:** Downsampling majority classes using a loss-based heuristic preserves model "knowledge" better than random selection.
- **Mechanism:** For data-excessive classes, the system calculates the loss for each sample. It retains the top-$B_k$ samples with the highest loss (assumed to be the most informative "hard" examples) and discards the rest.
- **Core assumption:** High loss values correlate strongly with sample importance/informativeness for the current model state, and discarding low-loss samples does not remove critical diversity.
- **Evidence anchors:** [section IV.C] "Our method prioritizes samples that result in higher losses... asserting that these are more crucial for effective training." [table III] M1 (Random) vs. M2 (Knowledge Sampling) shows a performance gain (75.16% vs 76.84% on CIFAR-10), validating the heuristic.

### Mechanism 3
- **Claim:** The domain gap between synthetic and real data can be bridged by treating synthetic features as "corrupted" versions of real features, correctable via learnable embeddings.
- **Mechanism:** Instead of training directly on synthetic pixels/features, the method adds a learnable class-specific embedding vector ($P_i$) to the synthetic feature map. A "Knowledge Drop" strategy randomly disables this embedding during training to prevent overfitting to the synthetic domain.
- **Core assumption:** The divergence between synthetic and real data is systematic and class-dependent, allowing a static vector to shift the synthetic distribution into alignment with the real distribution.
- **Evidence anchors:** [section IV.B] Eq. 4 formulates the feature addition ($\hat{y}_{ij} = H(F(I_{ij}) \oplus P_i)$). [figure 2] Visualizes the embedding shifting the synthetic cluster toward the real cluster in feature space.

## Foundational Learning

- **Concept: Client Drift**
  - **Why needed here:** FBL is fundamentally a "drift prevention" mechanism. Understanding that drift occurs because local optima diverge from the global optimum due to skewed data is necessary to justify why generating data (changing the data distribution) solves the problem better than FedProx (changing the loss function).
  - **Quick check question:** If all clients had identical data distributions, would FBL's "Knowledge Filling" still be necessary? (Answer: No, as drift stems from distribution heterogeneity).

- **Concept: Diffusion Models (Latent)**
  - **Why needed here:** The mechanism relies on "edge-side generative models" (specifically Stable Diffusion) to synthesize data. Understanding that these models compress images into a latent space to denoise and reconstruct them helps explain how they can generate class-conditional images efficiently on edge devices.
  - **Quick check question:** Why is a 4-bit quantized Stable Diffusion model usable on mobile devices for this task? (Answer: Quantization reduces memory footprint, trading slight fidelity for memory feasibility required by the "edge-side" constraint).

- **Concept: Knowledge Distillation vs. Data Augmentation**
  - **Why needed here:** FBL uses synthetic data to "fill" classes. It is important to distinguish this from methods like FedGen, which distill knowledge into the model weights directly. FBL operates at the *data level*, requiring the model to learn from synthetic samples.
  - **Quick check question:** Does FBL require the server to know the class distribution of the clients to generate data? (Answer: No, generation happens entirely on the client side, preserving privacy).

## Architecture Onboarding

- **Component map:** Input (unbalanced dataset) -> Balance Controller (calculates $B_k$) -> Branch A (Excessive: Loss-based sampler) or Branch B (Scarce/Missing: Generative Model + Knowledge Alignment) -> Trainer (Concatenates filtered/synthetic data) -> Aggregator (sends model updates to server)

- **Critical path:** The **Balance Point ($B_k$) Calculation** is the synchronization anchor. If $B_k$ is calculated incorrectly or if the definition of "excessive" vs. "scarce" is misconfigured, the sampling and generation logic will produce a dataset that is either still unbalanced or too synthetic.

- **Design tradeoffs:** Compute vs. Accuracy (FBL trades local computation for communication efficiency), Real vs. Synthetic Ratio (keeping total sample count fixed means every synthetic sample replaces a real sample from a majority class)

- **Failure signatures:** Catastrophic Forgetting (if Knowledge Replay parameter $\gamma$ is set too low), Mode Collapse in Generation (if prompt embedding is poor), Domain Gap (synthetic data degrades accuracy instead of helping)

- **First 3 experiments:** 1) Sanity Check (Alpha=0.1): Run FBL on CIFAR-10 with high heterogeneity against FedAvg. 2) Resource Profiling: Measure latency and memory overhead of 4-bit quantized Stable Diffusion on target edge device. 3) Robustness Test: Inject Gaussian noise into Tiny-ImageNet and compare FBL against MOON/SCAFFOLD.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can Federated Balanced Learning (FBL) be adapted to maintain performance effectiveness in specialized or challenging domains where current generative models produce low-fidelity synthetic data? (Section VI states advantages are "less pronounced in more challenging domains" due to inferior generated data quality)

- **Open Question 2:** Can the "Sampling-Generation-Alignment" framework be effectively generalized to Natural Language Processing (NLP) tasks using Large Language Models (LLMs) without compromising convergence? (Section IV.E suggests leveraging LLMs for NLP tasks but only evaluates image classification)

- **Open Question 3:** How does the storage and computational overhead of learnable embeddings (Knowledge Alignment Strategy) scale when applied to datasets with extremely high-dimensional label spaces? (The Knowledge Alignment Strategy requires a learnable embedding $P_i$ for *each* class, but scalability to thousands of classes is unstated)

## Limitations
- Performance advantages are less pronounced in challenging domains where generative models produce low-fidelity synthetic data
- Practical feasibility of edge-side Stable Diffusion generation under real-world constraints (power, memory, network latency) remains untested
- Loss-based knowledge sampling may overfit to outliers or corrupted samples in noisy data regimes

## Confidence
- **High Confidence:** The ablation results (M1 vs M2, M3 vs M4) clearly validate the local heuristics (loss-based sampling, knowledge alignment) on the tested datasets
- **Medium Confidence:** The generalization to Tiny-ImageNet and the extended scenarios (heterogeneous compute, noise) are promising but rely on scaling assumptions not fully verified in the paper
- **Low Confidence:** The practical feasibility of edge-side Stable Diffusion generation under real-world constraints (power, memory, network latency) is asserted but not empirically demonstrated

## Next Checks
1. **Edge Feasibility Benchmark:** Measure the wall-clock latency and peak memory usage of the 4-bit quantized Stable Diffusion model on a constrained device (e.g., Raspberry Pi 4) when generating 100 class-conditional images. Compare this to the time budget for a single local training epoch.

2. **Noise Robustness Test:** Create a corrupted CIFAR-10 dataset with 20% label noise. Train FBL and a baseline (FedAvg + FedProx) on this dataset. Compare final test accuracy and analyze if the synthetic data acts as a regularizer or amplifies the noise.

3. **Embedding Ablation:** Run the Tiny-ImageNet experiment with three variants: (a) FBL with Knowledge Alignment and Knowledge Drop, (b) FBL with Alignment but no Drop, (c) FBL with raw synthetic data. Report the delta in accuracy to quantify the marginal benefit of the embedding regularization.