---
ver: rpa2
title: Effectiveness of Zero-shot-CoT in Japanese Prompts
arxiv_id: '2503.06765'
source_url: https://arxiv.org/abs/2503.06765
tags:
- japanese
- performance
- english
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the effectiveness of zero-shot Chain-of-Thought
  (CoT) prompting in Japanese versus English using GPT-3.5 and GPT-4o-mini on the
  JMMLU and MMLU benchmarks. Overall, CoT prompted performance declines in both languages
  and models, with GPT-3.5 showing a smaller decline in English (-0.022) than Japanese
  (-0.059), and GPT-4o-mini experiencing a large drop in English (-0.424) but a smaller
  one in Japanese (-0.334).
---

# Effectiveness of Zero-shot-CoT in Japanese Prompts

## Quick Facts
- **arXiv ID**: 2503.06765
- **Source URL**: https://arxiv.org/abs/2503.06765
- **Reference count**: 14
- **Primary result**: Zero-shot CoT prompting degrades performance in both Japanese and English, with varying effects across models and tasks

## Executive Summary
This study investigates the effectiveness of zero-shot Chain-of-Thought (CoT) prompting in Japanese versus English using GPT-3.5 and GPT-4o-mini on the JMMLU and MMLU benchmarks. The research reveals that CoT prompting generally degrades performance across both languages and models, with GPT-3.5 showing a smaller decline in English (-0.022) than Japanese (-0.059), and GPT-4o-mini experiencing a large drop in English (-0.424) but a smaller one in Japanese (-0.334). While CoT improved some reasoning-heavy tasks like elementary mathematics, its impact varied widely by task and model. The findings suggest that GPT-4o-mini may already incorporate reasoning internally, making explicit CoT prompting redundant or disruptive, and highlight the importance of language-dependent prompt sensitivity.

## Method Summary
The study evaluates zero-shot CoT prompting effectiveness by comparing Japanese (JMMLU) and English (MMLU) benchmarks using GPT-3.5 and GPT-4o-mini on multiple-choice questions. Researchers used two prompt conditions: baseline (no CoT) and CoT (with "Let's think step by step" or Japanese equivalent). They evaluated all JMMLU tasks and the first 150 MMLU instances per subject, extracting answer letters (A, B, C, D) from model outputs. Accuracy was computed per subject and overall, with t-tests comparing CoT vs no-CoT across languages, models, and subjects.

## Key Results
- Zero-shot CoT prompting generally degraded performance in both languages and models
- GPT-3.5 showed smaller decline in English (-0.022) than Japanese (-0.059)
- GPT-4o-mini experienced large drop in English (-0.424) but smaller decline in Japanese (-0.334)
- CoT improved some reasoning-heavy tasks like elementary mathematics (+0.278) but hurt others like virology (-0.178)

## Why This Works (Mechanism)

### Mechanism 1: Redundant Reasoning Interference
In highly capable models, explicit Chain-of-Thought (CoT) prompting may degrade performance by interfering with already-embedded internal reasoning processes. Advanced models (e.g., GPT-4o-mini) likely utilize internal reasoning pathways during pre-training or fine-tuning. Adding an external "Let's think step by step" trigger forces a specific, possibly sub-optimal verbalization of reasoning that disrupts the model's native flow, creating a "double-processing" effect.

### Mechanism 2: Clause-Induced Attention Dilution
The addition of CoT phrasing increases prompt complexity (clause count), which can degrade accuracy in fragile reasoning tasks by dispersing the model's attention. By appending instructions, the model must allocate attention to both the instruction and the problem. If the model relies on pattern matching, the extra clauses may obscure the problem structure.

### Mechanism 3: Language-Dependent Prompt Sensitivity
The effectiveness of zero-shot CoT is non-transferrable across languages due to differences in training data density and linguistic structure. The "step-by-step" heuristic relies on English-centric training correlations. In Japanese, GPT-3.5 showed a steeper performance decline with CoT than in English, suggesting the prompt introduces noise or confusion rather than structure in a lower-resource language context.

## Foundational Learning

- **Zero-shot Chain-of-Thought (CoT)**: Eliciting reasoning by appending a trigger phrase without providing examples. Why needed: This is the core intervention being tested. Quick check: How does Zero-shot CoT differ from Few-shot CoT?

- **Internal vs. External Reasoning**: Distinguishing between reasoning elicited via prompts (external) and reasoning inherent to the model (internal). Why needed: The paper's central thesis regarding GPT-4o-mini relies on this distinction. Quick check: Why might a model with strong internal reasoning perform worse when forced to externalize that reasoning?

- **Benchmark Cross-Language Alignment (MMLU vs. JMMLU)**: Understanding the implications of comparing a native English dataset (MMLU) with a translated Japanese dataset (JMMLU). Why needed: The study relies on comparing performance across these datasets, but translation quality and subject differences create potential confounds. Quick check: Why is comparing a native English dataset (MMLU) with a translated Japanese dataset (JMMLU) a potential confounding factor?

## Architecture Onboarding

- **Component map**: Dataset (MMLU/JMMLU) -> Prompting Layer (no-CoT/CoT) -> Model API (GPT-3.5/GPT-4o-mini) -> Extraction Layer (answer parsing)

- **Critical path**:
  1. Dataset selection (matching subject counts like 150 instances)
  2. Prompt construction (Critical: The "no-CoT" baseline must be strictly controlled to isolate the variable)
  3. Answer extraction (The paper notes extracting the letter; high variance in model output formatting here can skew results)

- **Design tradeoffs**:
  - **Cost vs. Granularity**: The authors reduced MMLU to 150 instances to match JMMLU limits and control costs. This increases noise in low-sample subjects.
  - **Translation vs. Native**: Using JMMLU (translated) vs. MMLU (native) introduces a "translation quality" variable that is not strictly controlled.

- **Failure signatures**:
  - **Negative Deltas**: A consistent drop in accuracy when CoT is applied (observed in GPT-4o-mini), indicating the technique is backfiring.
  - **Extraction Failures**: If the model generates a reasoning trace but fails to output the final letter, the extraction logic must robustly handle the "answer-less" response.

- **First 3 experiments**:
  1. **Baseline Verification**: Run the "no-CoT" baseline on both models to ensure parity with reported metrics (GPT-3.5 ~0.52-0.60, GPT-4o-mini ~0.66-0.68).
  2. **Subject-Specific Stress Test**: Isolate "Elementary Mathematics" (which showed +0.278 improvement) vs. "Virology" (which showed -0.178 decline) to verify if CoT benefits are strictly domain-specific.
  3. **Token Length Ablation**: Test if the CoT decline in GPT-4o-mini is due to the *semantic* instruction or simply the *addition of tokens* by appending a neutral phrase (e.g., "Please answer the following:") to see if performance drops similarly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does GPT-4o-mini possess internal reasoning mechanisms that render explicit Zero-shot CoT prompting redundant or disruptive?
- **Basis in paper**: The authors note GPT-4o-mini suffered "significant performance declines" and suggest it "already incorporates CoT-like reasoning internally," making additional prompts potentially "redundant or even disruptive."
- **Why unresolved**: LLMs function as "black boxes," making it difficult to distinguish between inherent model behavior and external prompting effects without internal access.
- **What evidence would resolve it**: Ablation studies analyzing the density of reasoning steps in model outputs without prompts, or comparisons with models explicitly trained without reasoning instructions.

### Open Question 2
- **Question**: Is the severe performance drop in "Japanese idioms" caused by limited Japanese-specific training samples?
- **Basis in paper**: The paper identifies "Japanese idioms" as the task with the largest performance decrease (âˆ’0.626) and states, "The limited number of Japanese-specific training samples may be a factor, though further analysis is needed to confirm this."
- **Why unresolved**: The study evaluates benchmark performance but does not analyze the training data composition or its correlation with specific task degradation.
- **What evidence would resolve it**: A correlation analysis between the frequency of specific cultural concepts in the training corpus and the magnitude of CoT-induced performance changes.

### Open Question 3
- **Question**: Is the observed performance decline caused by the increase in prompt complexity ("clauses") rather than the reasoning elicitation technique itself?
- **Basis in paper**: The discussion suggests the "increase in clauses" effect (from citation [8]) may add complexity that outweighs reasoning benefits, but this was not isolated as a variable in the experiment.
- **Why unresolved**: The study design appends a specific semantic phrase ("Let's think step by step") but does not control for the token length or clause structure independently.
- **What evidence would resolve it**: Experiments using "null" or irrelevant control phrases of similar length to determine if the performance drop is due to semantic instruction or prompt length.

## Limitations
- Translation quality and dataset alignment issues between MMLU (English) and JMMLU (Japanese translation) introduce uncertainty about whether performance differences reflect true language effects versus translation artifacts
- Statistical power concerns with only 150 instances per subject, potentially lacking power to detect smaller effects
- Prompt formulation ambiguity with unspecified exact Japanese translation of "Let's think step by step"

## Confidence

**High confidence**: The overall finding that zero-shot CoT prompting degrades performance in both languages and models is well-supported by the data.

**Medium confidence**: The hypothesis that GPT-4o-mini incorporates internal reasoning (explaining lack of CoT benefit) is plausible but not definitively proven.

**Low confidence**: The specific claim about language-dependent prompt sensitivity is weakly supported due to conflicting patterns across models.

## Next Checks

1. **Translation fidelity test**: Compare model performance on original English questions versus professionally translated Japanese versions of the same questions to isolate language effects from translation artifacts.

2. **Prompt formulation ablation**: Test multiple Japanese translations of the CoT trigger phrase to determine if the observed effects are robust to phrasing variations or specific to the chosen formulation.

3. **Token-length controlled experiment**: Test whether the performance decline is due to semantic content of CoT prompts versus simple token addition by comparing CoT prompts against neutral phrase additions of similar length.