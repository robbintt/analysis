---
ver: rpa2
title: Memorization and Knowledge Injection in Gated LLMs
arxiv_id: '2504.21239'
source_url: https://arxiv.org/abs/2504.21239
tags:
- arxiv
- knowledge
- memory
- fine-tuning
- mega
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MEGa, a continual learning framework that
  injects episodic memories into large language models (LLMs) by embedding each memory
  as gated low-rank weights. Each memory is stored with a dedicated LoRA adapter and
  a context key embedding, with a gating mechanism activating relevant memories during
  inference by matching query embeddings to stored keys.
---

# Memorization and Knowledge Injection in Gated LLMs

## Quick Facts
- arXiv ID: 2504.21239
- Source URL: https://arxiv.org/abs/2504.21239
- Authors: Xu Pan; Ely Hahami; Zechen Zhang; Haim Sompolinsky
- Reference count: 40
- Primary result: Achieved 72.53-78.03% QA accuracy and 0.901-0.921 recall cosine similarity while preserving base model capabilities on MMLU

## Executive Summary
MEGa introduces a continual learning framework that injects episodic memories into LLMs by storing each memory as a gated low-rank adapter (LoRA) with associated context keys. The approach enables models to recall entire memories and answer related questions while mitigating catastrophic forgetting. On fictional character stories and Wikipedia events datasets, MEGa achieved strong recall accuracy and preserved general language capabilities, matching RAG performance while maintaining MMLU accuracy around 61%.

## Method Summary
MEGa stores episodic memories by creating dedicated LoRA adapters for each sample, with context keys generated from query embeddings. During inference, a gating mechanism computes similarity between query embeddings and stored context keys to activate relevant adapters. The model combines the base weights with a softmax-weighted sum of activated LoRA adapters, enabling compositional reasoning by blending multiple memories. The framework freezes base LLM parameters and trains only the low-rank matrices, using AdamW optimization with rank 128 LoRAs and specific fine-tuning prompts to overcome RLHF safety constraints.

## Key Results
- Recall cosine similarity: 0.901-0.921 on fictional and Wikipedia datasets
- QA accuracy: 72.53-78.03% on synthetic character and event datasets
- Preserved general capabilities: MMLU accuracy maintained at ~61% across memory injections
- Compositional reasoning: Successfully combined knowledge from multiple memories for complex queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Isolating memory updates into dedicated LoRA modules mitigates catastrophic forgetting better than updating global weights, provided modules remain distinct.
- **Mechanism:** Each incoming memory gets its own low-rank matrices (A_i, B_i) while base weights W_PT remain frozen. Memory-specific optimization doesn't overwrite weights associated with previous memories.
- **Core assumption:** Low-rank subspaces for different memories are sufficiently disentangled to prevent degradation when activating one module.
- **Evidence anchors:** Abstract states memories stored in "dedicated set of gated low-rank weights"; section 3.3.1 describes initializing exactly one LoRA adapter per sample added to pre-trained weights.
- **Break condition:** Linear parameter growth and potential "crowding" in weight space when scaling to thousands or millions of memories.

### Mechanism 2
- **Claim:** Semantic gating via embedding similarity routes inference queries to relevant memory modules without extending context window.
- **Mechanism:** Computes context key for user query and compares against stored keys using softmax to generate gating weights that interpolate between base model and specific LoRA adapters.
- **Core assumption:** Base model embeddings capture semantic similarity well enough to distinguish between specific event memories.
- **Evidence anchors:** Abstract mentions "gating mechanism activating relevant memories during inference by matching query embeddings to stored keys"; section 3.3.2 details computing gating weights by comparing user query embedding with all context keys.
- **Break condition:** Fails with ambiguous or out-of-distribution queries, leading to activation of irrelevant LoRA adapters.

### Mechanism 3
- **Claim:** Compositional reasoning enabled by allowing gating mechanism to simultaneously activate multiple LoRA adapters with non-zero weights.
- **Mechanism:** Softmax gating allows queries to blend weights of multiple distinct memories, enabling the model to process queries through merged parameters containing knowledge from multiple sources.
- **Core assumption:** Summation of low-rank updates from different memories results in functional weight space that can synthesize information rather than averaging conflicting signals.
- **Evidence anchors:** Section 3.3.2 describes adding weighted sum of As and Bs; section 4.5 shows MEGa can mix different LoRA adapters by applying softmax-weighted gating.
- **Break condition:** Contradictory facts across memories lead to incoherent outputs or hallucinations.

## Foundational Learning

- **Concept:** Catastrophic Forgetting
  - **Why needed here:** Core problem MEGa solves - standard fine-tuning alters global weights causing models to "forget" previous tasks when learning new ones.
  - **Quick check question:** Why does updating the dense weights of a model on a new task typically degrade performance on the original task?

- **Concept:** Low-Rank Adaptation (LoRA)
  - **Why needed here:** MEGa relies on LoRA to isolate memory storage. Understanding that W = W_base + ΔW (where ΔW is low-rank) is essential.
  - **Quick check question:** In LoRA, which matrices are frozen and which are trained?

- **Concept:** Embedding Similarity & Softmax
  - **Why needed here:** Gating mechanism is essentially similarity search followed by probability distribution calculation.
  - **Quick check question:** How does softmax function ensure gating weights sum to 1, and why is this useful for mixing multiple memory adapters?

## Architecture Onboarding

- **Component map:** Base LLM -> Memory Bank -> Gating Router -> Dynamic Weight Merger
- **Critical path:**
  1. Ingestion: Text sample → Embedder → Save Context Key
  2. Training: Initialize new LoRA → Train on sample (isolated) → Save LoRA weights
  3. Inference: User Query → Embedder → Gating Router → Merge Active LoRAs → LLM Forward Pass
- **Design tradeoffs:**
  - Storage vs. Isolation: Storing full LoRA per memory ensures isolation but consumes significant resources (linear scaling). Merging or pruning needed for scaling.
  - Recall vs. Reasoning: Internal RAG (recalling text first) improves QA accuracy but increases latency and token usage compared to direct weight injection.
- **Failure signatures:**
  - Refusal Loops: RLHF safety training may cause model to refuse answers; specific prompts like "Tell me a story you memorized" can bypass this.
  - Gate Confusion: Poor embeddings cause wrong adapter activation, resulting in confident but factually incorrect answers (hallucinations based on wrong memory).
- **First 3 experiments:**
  1. Single Injection Test: Inject one fictional story, verify exact recall (cosine similarity) and QA accuracy without catastrophic forgetting on MMLU.
  2. Sequential Forgetting Curve: Inject 10+ stories sequentially, plot recall accuracy of first story after each new injection to verify flat curve vs. standard fine-tuning.
  3. Compositional Retrieval: Create query requiring facts from two distinct stories, verify gating activates both LoRAs and model answers correctly.

## Open Questions the Paper Calls Out
- Can shared LoRA weights or post-learning pruning methods mitigate the linear parameter growth of MEGa while preserving recall accuracy?
- Basis: The authors identify that parameter count grows linearly with number of training samples and suggest shared LoRA weights or pruning as potential solutions to this scalability issue.

## Limitations
- Linear parameter growth creates practical constraints for large-scale deployment with thousands of memories
- Heavy dependency on embedding quality for effective gating, with ambiguous embeddings causing retrieval failures
- Sensitivity to RLHF safety training that can cause model refusal and brittleness to different instruction-tuned models

## Confidence
- **High Confidence:** Catastrophic forgetting mitigation claim (base MMLU accuracy maintained at ~61% across memory injections)
- **Medium Confidence:** Compositional reasoning capability shows promising results but evaluated on relatively small set of 500 compositional questions
- **Low Confidence:** Claim of matching RAG performance involves different computational trade-offs not fully characterized

## Next Checks
1. **Memory Interference Analysis:** Systematically inject 50-100 semantically similar memories and measure gating accuracy degradation to validate whether embedding-based gating can distinguish between closely related memories.
2. **Long-term Forgetting Stability:** After injecting N memories, freeze system and measure recall accuracy of all memories after delay period to test claimed catastrophic forgetting mitigation under realistic operational conditions.
3. **Resource Efficiency Benchmarking:** Implement parameter pruning to maintain fixed memory budget (e.g., 500 adapters maximum) and evaluate trade-off between memory count, recall accuracy, and inference latency to address practical scalability concerns.