---
ver: rpa2
title: 'The Percept-V Challenge: Can Multimodal LLMs Crack Simple Perception Problems?'
arxiv_id: '2508.21143'
source_url: https://arxiv.org/abs/2508.21143
tags:
- image
- circles
- shapes
- visual
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Percept-V, a benchmark dataset of 6000 program-generated
  images divided into 30 domains, each testing visual perception skills from the TVPS-4
  framework. The dataset isolates basic visual perception from reasoning and domain
  knowledge by using simple objects and minimal reasoning requirements.
---

# The Percept-V Challenge: Can Multimodal LLMs Crack Simple Perception Problems?

## Quick Facts
- arXiv ID: 2508.21143
- Source URL: https://arxiv.org/abs/2508.21143
- Authors: Samrajnee Ghosh; Naman Agarwal; Hemanshu Garg; Chinmay Mittal; Mausam; Parag Singla
- Reference count: 7
- Primary result: State-of-the-art MLLMs achieve only 55.2% accuracy on basic visual perception tasks, far below human performance of 95.4%.

## Executive Summary
This paper introduces Percept-V, a benchmark dataset of 6000 program-generated images testing 7 fundamental visual perception skills from the TVPS-4 framework. The dataset isolates basic visual perception from reasoning and domain knowledge by using simple geometric objects and minimal reasoning requirements. Experiments with six state-of-the-art multimodal LLMs reveal consistently weak performance across all models, with accuracy dropping sharply as problem size increases. The results expose significant gaps in MLLMs' visual perception capabilities that cannot be explained by contamination or task complexity alone.

## Method Summary
The benchmark evaluates MLLMs on 30 domains of basic visual perception tasks using program-generated images with geometric shapes (circles, triangles, squares, pentagons) in controlled configurations. Each domain tests one of seven TVPS-4 skills with problem sizes ranging from 1 to 20 objects. Images are generated using Python libraries (Pillow, Matplotlib, OpenCV) with standardized prompts and structured answer formats. Six MLLMs are evaluated zero-shot with temperature=0.0 and top_p=0.0 where supported. Exact string matching against ground truth provides accuracy metrics, with human baseline established at 95.4%.

## Key Results
- MLLMs achieve only 55.2% accuracy at best (GPT-5-mini), far below human baseline of 95.4%
- Performance degrades rapidly as problem size increases from 1 to 20 objects across all 7 skills
- Visual Closure tasks show consistently poor performance across all models
- Thinking models (o4-mini, GPT-5-mini) outperform non-thinking models (GPT-4o, Gemini 2.5 Flash)
- One-shot prompting unexpectedly degrades performance compared to zero-shot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance degradation with increasing problem size stems from compounding attention and enumeration failures.
- Mechanism: As object count increases, visual attention mechanisms must track more entities simultaneously. MLLMs lack explicit object-level representations, relying instead on patch-level features that blur together under crowding. This causes systematic undercounting and missed objects in enumeration tasks.
- Core assumption: The visual encoder's spatial resolution and the language model's working memory capacity are the bottleneck.
- Evidence anchors:
  - [abstract] "We find that as number of objects in the image increases, performance goes down rather fast."
  - [section 4.2] Figure 2 shows accuracy curves dropping steeply across all 7 skills as problem size increases from 1 to 20.
  - [corpus] Related work on small object perception (Zhang et al., 2024b in paper references) confirms MLLM struggles with object scale and density.
- Break condition: If models had access to iterative visual sampling (multiple fixations) or explicit object detection pre-processing, degradation might flatten.

### Mechanism 2
- Claim: Skill-specific difficulty patterns reflect training data distribution biases, not inherent task complexity.
- Mechanism: Visual Spatial Relationship tasks (e.g., "above", "left of") appear frequently in image captions and vision-language datasets, giving models extensive exposure. Conversely, Visual Closure tasks (matching outlines, recognizing incomplete forms) are rare in web-scale training corpora, leaving models without learned patterns to exploit.
- Core assumption: Pre-training data composition is the primary determinant of skill-level performance.
- Evidence anchors:
  - [section 4.1] "Since image captions generally contain such relationships, it is likely that MLLMs got trained well on this skill [Visual Spatial Relationships]."
  - [section 4.1] "Such phenomena are likely absent from multimodal training data, making models weaker on this skill [Visual Closure]."
  - [corpus] BabyVision paper confirms MLLMs rely on "linguistic priors to compensate for fragile visual understanding."
- Break condition: If training were balanced across TVPS-4 skills, the skill difficulty ranking should converge toward uniform performance.

### Mechanism 3
- Claim: Thinking models outperform non-thinking models by decomposing perception into verifiable intermediate steps.
- Mechanism: Models like o4-mini and GPT-5-mini use chain-of-thought reasoning to break down perception tasks (e.g., "first identify all circles, then count them"), reducing single-step error rates. Non-thinking models must perform the full perception-to-answer mapping in one forward pass, accumulating errors.
- Core assumption: Multi-step reasoning provides error correction opportunities during inference.
- Evidence anchors:
  - [section 4] "Among closed source models, GPT-4o which is a non-thinking model, does the worst... Thinking models do somewhat better."
  - [section 4] Models with "think with image" capabilities leverage visual information as intermediate reasoning steps.
  - [corpus] Perception-R1 paper shows reinforcement learning with visual perception rewards improves MLLM reasoning.
- Break condition: If one-shot prompting significantly improved performance, task understanding would be the bottleneck rather than step-wise verification.

## Foundational Learning

- Concept: **TVPS-4 Framework (7 Visual Perception Skills)**
  - Why needed here: The benchmark systematically evaluates distinct perception abilities (Discrimination, Memory, Sequential Memory, Figure Ground, Form Constancy, Closure, Spatial Relationships) rather than treating perception as monolithic.
  - Quick check question: Can you name which skill "matching an object to its outline" tests? (Answer: Visual Closure)

- Concept: **Program-Generated Synthetic Benchmarks**
  - Why needed here: Percept-V avoids contamination issues endemic to web-scraped images by generating fresh test cases programmatically with controlled difficulty.
  - Quick check question: Why is contamination a concern for MLLM benchmarks? (Answer: Models may have seen test images during pre-training, inflating performance.)

- Concept: **Perception vs. Reasoning Disentanglement**
  - Why needed here: Many benchmarks conflate perception with domain knowledge and complex reasoning; isolating basic perception reveals fundamental architectural limitations.
  - Quick check question: What would happen if a counting task required both perception AND domain knowledge about physics? (Answer: Poor performance could stem from either component, making diagnosis impossible.)

## Architecture Onboarding

- Component map:
  - Python scripts -> Image generator (Pillow/Matplotlib/OpenCV) -> 30 domain-specific image types
  - Image generator -> Question prompts with (in: image description, r: task, op: output format)
  - Question prompts + images -> Evaluation harness (exact string matching against ground truth)
  - Evaluation harness -> Accuracy computation by size, skill, and domain

- Critical path:
  1. Select domain â†’ load prompt template
  2. Generate image with controlled problem size
  3. Query MLLM with (prompt + image)
  4. Parse output against expected format
  5. Compute accuracy by size, skill, and domain

- Design tradeoffs:
  - **Synthetic vs. Natural Images**: Synthetic ensures controllability but may not reflect real-world perception challenges (lighting, occlusion, perspective).
  - **Fixed vs. Variable Answer Formats**: Fixed formats enable automatic evaluation but constrain model expressiveness.
  - **Zero-shot vs. Few-shot**: Zero-shot isolates intrinsic capability; few-shot may conflate learning with perception.

- Failure signatures:
  - **Systematic undercounting**: Models consistently report fewer objects than present as count increases.
  - **Color confusion**: "colours_present" domain shows ~3% accuracy; models confuse similar hues (turquoise vs. blue) despite legend.
  - **Format errors**: DeepSeek shows 60%+ format error rates; proprietary models near 0%.

- First 3 experiments:
  1. **Skill-wise breakdown on your model**: Run all 30 domains, aggregate by TVPS-4 skill to identify which perception abilities are weakest.
  2. **Size-scaling curve**: Plot accuracy vs. problem size (1-20) for each skill; steep drops indicate poor object-level attention.
  3. **Thinking vs. non-thinking comparison**: If your model supports chain-of-thought, compare zero-shot direct answers against step-wise reasoning outputs to quantify the thinking advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training data deficiencies cause current MLLMs to fail on basic perception tasks that require minimal reasoning?
- Basis in paper: [explicit] The Limitations section explicitly states that the paper "does not investigate in detail, the rationale behind such weakness" observed in the models.
- Why unresolved: While the paper establishes that MLLMs perform poorly on simple perceptual tasks compared to humans, it stops short of determining if the failure stems from the visual encoder, the projection layer, or the lack of specific perceptual patterns in the pre-training data.
- Evidence to resolve it: Ablation studies varying visual encoder resolutions and patch sizes on Percept-V tasks; analysis of attention maps to see if models attend to the correct visual features before generating incorrect answers.

### Open Question 2
- Question: Why does providing a single exemplar (one-shot prompting) degrade performance on these simple perception tasks?
- Basis in paper: [explicit] Section 4.5 notes that "Contrary to conventional wisdom, we find that one-hot prompting degrades the overall performance" across proprietary models.
- Why unresolved: The paper observes this counter-intuitive result but does not investigate if the exemplar introduces noise, distracts from the specific visual features, or if the context window implementation struggles with the combination of image and text examples.
- Evidence to resolve it: Comparative analysis of attention weights in zero-shot versus one-shot settings; testing if the degradation persists when the exemplar uses a completely different visual domain than the test question.

### Open Question 3
- Question: Does the human-MLLM performance gap persist across different age groups and educational backgrounds?
- Basis in paper: [explicit] The Limitations section states the human study "only evaluates MLLMs... against a limited pool of adult college students" and suggests future work should include "annotators from different age groups."
- Why unresolved: The current human baseline is limited to a specific demographic (college students), leaving it unclear if the "weak" MLLM performance is only weak relative to educated adults or if it also underperforms compared to children or other populations.
- Evidence to resolve it: administering the Percept-V dataset to participants of varying ages (including children, for whom the TVPS-4 framework is often used) to establish a developmental baseline for comparison.

## Limitations
- Synthetic images may not capture real-world perception challenges like occlusion, lighting variations, and perspective distortion
- Exact-match evaluation criteria may be overly strict and not account for partial perceptual understanding
- Zero-shot protocol doesn't explore whether fine-tuning or few-shot learning could substantially improve performance
- Benchmark focuses only on perceptual discrimination without reasoning tasks, limiting assessment of downstream capabilities

## Confidence
- **High Confidence**: MLLMs show consistently weak performance across all perception skills compared to human baselines; performance degrades rapidly with increasing problem size
- **Medium Confidence**: Skill-specific difficulty patterns reflect training data distribution biases; thinking models outperform non-thinking models via step-wise reasoning
- **Low Confidence**: Performance degradation specifically stems from attention and enumeration failures due to patch-level feature blurring

## Next Checks
1. **Real-world transfer experiment**: Evaluate the same TVPS-4 skills on natural images from standard datasets (e.g., COCO, VQA) to determine whether synthetic performance correlates with real-world perception capabilities and identify any domain shift effects.

2. **Training data probing**: Analyze the pre-training data distribution of evaluated MLLMs to empirically verify whether skills like Visual Closure are indeed underrepresented compared to Visual Spatial Relationships, providing direct evidence for the training data bias hypothesis.

3. **Enhanced evaluation protocol**: Implement a partial credit scoring system for the Percept-V benchmark that awards partial points for near-correct answers or correct sub-components, to determine whether the current exact-match evaluation is overly penalizing models that demonstrate partial perceptual understanding.