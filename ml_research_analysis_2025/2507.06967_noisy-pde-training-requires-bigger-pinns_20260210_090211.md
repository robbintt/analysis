---
ver: rpa2
title: Noisy PDE Training Requires Bigger PINNs
arxiv_id: '2507.06967'
source_url: https://arxiv.org/abs/2507.06967
tags:
- pinn
- noisy
- bound
- pinns
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a fundamental lower bound on the size of\
  \ physics-informed neural networks (PINNs) required to effectively solve partial\
  \ differential equations (PDEs) when training data contains noise. The authors prove\
  \ that if a PINN achieves an empirical risk O(\u03B7) below the variance of noisy\
  \ supervision labels, then the number of trainable parameters must scale as dN log\
  \ dN \u2273 Ns\u03B7\xB2, where Ns is the number of samples."
---

# Noisy PDE Training Requires Bigger PINNs

## Quick Facts
- arXiv ID: 2507.06967
- Source URL: https://arxiv.org/abs/2507.06967
- Reference count: 27
- This paper establishes that noisy supervision in PINN training requires proportionally larger networks to achieve low empirical risk.

## Executive Summary
This paper proves a fundamental lower bound on PINN size required for effective noisy PDE training. The authors show that to achieve empirical risk O(η) below the variance of noisy supervision labels, the number of trainable parameters must scale as dN log dN ≳ Nsη². This means increasing noisy data alone does not provide a "free lunch" - the model must have sufficient capacity to leverage noisy supervision effectively. The theoretical findings are validated through experiments on the Hamilton-Jacobi-Bellman PDE, demonstrating that PINNs can achieve empirical risks below the noise threshold when network size exceeds a critical value, with performance scaling as O(√dN).

## Method Summary
The paper studies PINNs solving the 2D Hamilton-Jacobi-Bellman (HJB) PDE with noisy supervision labels. The architecture uses shallow networks h_w(x,y,t) = ⟨a, tanh(W₁[x,y]ᵀ + w₂t)⟩ where a is frozen from N(3,1), with varying width k to control trainable parameters d_N = 3k. Training uses Ns = 3276 noisy supervision samples with σ² = 0.5, combined with PDE residual and initial condition losses. The theoretical analysis uses covering number arguments to establish when PINNs can achieve empirical risk below noise variance, while experiments verify the scaling law O(√dN) for training accuracy as network size increases.

## Key Results
- Establishes lower bound: dN log dN ≳ Nsη² for PINNs to achieve empirical risk O(η) below noise variance
- Demonstrates "no free lunch" principle: more noisy samples alone don't reduce empirical risk without increased model capacity
- Validates theoretical predictions with HJB experiments showing O(√dN) scaling in training accuracy
- Proves results apply to both supervised PINNs (noisy solution samples) and unsupervised PINNs (noisy boundary/initial conditions)

## Why This Works (Mechanism)

### Mechanism 1: Model Capacity-Noise Tradeoff
- Claim: Larger PINNs (more trainable parameters) are required to achieve empirical risk below the noise variance threshold when training on noisy data.
- Mechanism: The number of parameters $d_N$ must scale with the number of samples $N_s$ to capture the underlying PDE solution while filtering out noise. The bound $d_N \log d_N \gtrsim N_s \eta^2$ emerges from covering number arguments—the network must have sufficient expressive capacity to represent good solutions within the hypothesis class.
- Core assumption: The PINN hypothesis class has bounded activation functions with Lipschitz gradients, and supervision noise has bounded variance.
- Evidence anchors:
  - [abstract] "If a PINN achieves an empirical risk O(η) below the variance of noisy supervision labels, then the number of trainable parameters must scale as dN log dN ≳ Nsη²"
  - [section] "This result shows that it is not necessarily useful to add more samples to the supervised loss term once a certain threshold is reached, but this threshold value may be increased by increasing the number of parameters in the model"
  - [corpus] Limited corpus support for this specific scaling law; related work focuses on sample efficiency but not parameter-noise scaling bounds
- Break condition: When noise variance σ² is very large relative to achievable error, or when η approaches 0, the bound becomes impractical to satisfy.

### Mechanism 2: No Supervised Learning Free Lunch Under Noise
- Claim: Simply increasing noisy training samples alone does not reduce empirical PINN risk below noise variance without corresponding increases in model capacity.
- Mechanism: The probability of finding a "good" network (with empirical risk below σ²) in a bounded function class depends on both the covering number of the hypothesis space (controlled by dN) and the number of noisy samples Ns. The proof shows this probability can be bounded from above, requiring the size constraint to hold for good solutions to exist with high probability.
- Core assumption: Random sampling of supervision data with bounded label noise; the network class has weight bounds $\|w\|_2 \leq W$.
- Evidence anchors:
  - [abstract] "Consequently, increasing the number of noisy supervision labels alone does not provide a 'free lunch' in reducing empirical risk."
  - [section] "To make use of a given number of noisy supervision samples, the model must have enough capacity—its size must exceed a certain threshold."
  - [corpus] Corpus papers focus on sample efficiency (AL-PINN, gST-PINN) but do not address the noise-capacity tradeoff directly
- Break condition: When supervision labels are noiseless (σ² = 0), or when the PDE has particularly simple structure allowing smaller networks.

### Mechanism 3: Perturbation-Stable Empirical Risk Bounds
- Claim: Small weight perturbations in PINNs cause bounded changes in empirical risk, enabling covering-based analysis of network existence.
- Mechanism: The empirical PINN risk changes polynomially (up to O(η⁴)) under η-level perturbations to network weights. This non-Lipschitz but controlled variation allows the analysis to work with η-coverings of the hypothesis class rather than individual networks, making probabilistic bounds tractable.
- Core assumption: The PDE residual, initial condition loss, and supervised loss components all have bounded derivatives with respect to network outputs and weights.
- Evidence anchors:
  - [section] "Lemma 4.7 conducts an extensive calculation showing that the PINN risk changes in a non-Lipschitz but controlled manner under η-level perturbations to its weights"
  - [section] "ˆR(hwη/2) ≤ ˆR(hw) + a₁η + a₂η² + a₃η³ + a₄η⁴ ∀η > 0"
  - [corpus] Corpus papers on PINN training (PINN Balls, RRaPINNs) discuss optimization stability but not perturbation-based capacity bounds
- Break condition: When using unbounded activations (e.g., ReLU without bounds) or when PDE operators have unbounded derivatives, the perturbation analysis breaks down.

## Foundational Learning

- Concept: **Physics-Informed Neural Networks (PINNs)**
  - Why needed here: Understanding the basic PINN framework is essential to follow how the empirical risk is constructed and what the lower bound applies to.
  - Quick check question: Can you explain what three types of residuals (loss terms) are typically combined in a PINN loss function?

- Concept: **Empirical Risk vs. True Risk**
  - Why needed here: The paper's main bound concerns when empirical risk (computed on samples) can fall below noise variance; distinguishing this from true generalization error is critical.
  - Quick check question: What is the difference between empirical risk computed on Ns noisy samples and the expected risk over the full domain?

- Concept: **Covering Numbers and Hypothesis Class Complexity**
  - Why needed here: The proof relies on η-coverings of bounded neural network classes to bound the probability of good solutions existing; this is a core statistical learning concept.
  - Quick check question: What does the covering number N(η, W) represent, and why does it grow exponentially with the number of parameters dN?

## Architecture Onboarding

- Component map:
  - **Hypothesis Class H**: Shallow networks with form $h_w(x,t) = f(W_1 x + w_2 t)$ where f is bounded with Lipschitz gradient
  - **Loss Components**: PDE residual loss (spatial Laplacian + gradient squared), initial condition loss (λ₀ weighted), supervised loss (λₛ weighted)
  - **Key Parameters**: dN = k(n+1) trainable parameters, W = weight bound, Ns = number of supervision samples, σ² = noise variance

- Critical path: Determine noise level σ² → Set target empirical risk improvement η → Calculate required dN from bound → Design network width k to meet parameter count → Train and verify empirical risk < σ²

- Design tradeoffs:
  - Larger dN improves ability to beat noise threshold but increases computational cost and risk of overfitting noise
  - Higher λₛ (supervision weight) makes the bound more stringent (requires larger dN)
  - The bound becomes more restrictive for higher-dimensional PDEs (larger n)

- Failure signatures:
  - Empirical risk plateaus above σ² despite increasing training iterations (insufficient dN)
  - Training loss decreases but test error increases (overfitting to noisy labels)
  - Optimization instability with large networks (gradient pathologies mentioned in related work)

- First 3 experiments:
  1. Reproduce Figure 1: Train PINNs with varying widths (varying dN) on HJB with known solution and σ²=0.5 noise, verify empirical risk drops below σ² only when dN exceeds critical threshold
  2. Ablation on noise levels: Systematically vary σ² and verify the critical dN scales proportionally to η² (where η measures improvement below σ²)
  3. Test different PDEs: Apply the same experimental setup to a different PDE (e.g., Burgers' equation) to assess whether the O(√dN) scaling observed for HJB generalizes or if different PDE structures require modified bounds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the lower bound $d_N \log d_N \gtrsim N_s \eta^2$ also a **sufficient** condition for a PINN to achieve empirical risk below the noise variance?
- **Basis in paper:** [explicit] The Conclusion states, "Further research could explore whether this relationship between $N_s$ and $d_N$ is also sufficient to guarantee low PINN risk."
- **Why unresolved:** The paper establishes a necessary condition (if risk is low, then size must be large) using probabilistic existence arguments, but does not prove that meeting the size requirement guarantees the risk can be reduced.
- **What evidence would resolve it:** A constructive proof or algorithm demonstration showing that satisfying the size constraint ensures the empirical risk drops below $\sigma^2$ with high probability.

### Open Question 2
- **Question:** Do similar scaling laws for noisy data apply to PINNs solving **vector-valued** PDEs, such as the Navier–Stokes equations?
- **Basis in paper:** [explicit] The Conclusion suggests "extending these findings to systems with vector-valued solutions (e.g., the Navier–Stokes equations)."
- **Why unresolved:** The theoretical derivation relies on specific perturbation bounds for the scalar Hamilton–Jacobi–Bellman (HJB) equation (Lemma 4.7), which may not hold for the operators in vector-valued systems.
- **What evidence would resolve it:** Deriving analogous perturbation bounds for the Navier–Stokes operator and establishing a similar lower bound on $d_N$ relative to $N_s$.

### Open Question 3
- **Question:** Can the **bounded network assumption** be removed to encompass standard unbounded architectures like ReLU-based PINNs?
- **Basis in paper:** [explicit] The Conclusion notes that "alleviating the bounded network assumption to encompass unbounded networks would also be a natural next step."
- **Why unresolved:** The proof of Lemma 4.6 relies on the boundedness of the network outputs (constant $C_1$) and gradients to apply Hoeffding's inequality and covering arguments.
- **What evidence would resolve it:** Derivation of probability bounds for neural network classes that do not enforce a hard bound on the final layer's activation function.

## Limitations
- Analysis assumes bounded activation functions and Lipschitz gradients, which may not hold for practical networks using ReLU
- Proof relies on specific PDE structures and initial conditions that may not generalize to all PDE types
- Parameter-noise scaling law is derived for shallow networks with specific architectures, extension to deep PINNs remains open

## Confidence
- **High confidence**: The fundamental claim that PINNs require sufficient capacity to leverage noisy supervision (Mechanism 1) is well-supported by the mathematical proof and experimental validation on HJB equations
- **Medium confidence**: The specific scaling law dN log dN ≳ Nsη² (Mechanism 1) applies to the particular shallow network architecture studied, but may need modification for different PINN architectures or deeper networks
- **Low confidence**: The perturbation-stability analysis (Mechanism 3) depends on specific PDE operator properties and bounded weight assumptions that may not hold in all practical scenarios

## Next Checks
1. **Architecture sensitivity**: Test whether the critical parameter threshold and O(√dN) scaling hold when using different activation functions (ReLU vs tanh) and deeper network architectures beyond the shallow networks analyzed theoretically
2. **PDE diversity**: Apply the same experimental methodology to a different PDE class (e.g., nonlinear diffusion or wave equations) to verify whether the parameter-noise scaling law is PDE-specific or represents a more general principle
3. **Noise distribution robustness**: Investigate whether the bound holds for different noise distributions (uniform vs Gaussian) and heteroscedastic noise patterns, as the current analysis assumes bounded variance noise without specifying the distribution