---
ver: rpa2
title: Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models
arxiv_id: '2506.07645'
source_url: https://arxiv.org/abs/2506.07645
tags:
- robustness
- perturbations
- llms
- words
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for evaluating the robustness of
  large language models (LLMs) against perturbations in less-resourced languages,
  using Polish as a case study. The method employs small proxy models trained on task-specific
  datasets to calculate word importance via attribution methods, enabling targeted
  generation of perturbed test examples.
---

# Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models

## Quick Facts
- arXiv ID: 2506.07645
- Source URL: https://arxiv.org/abs/2506.07645
- Authors: Maciej Chrabąszcz; Katarzyna Lorenc; Karolina Seweryn
- Reference count: 40
- Key outcome: Character-level perturbations in Polish significantly degrade LLM performance, with ASR often exceeding 0.1, revealing vulnerabilities in less-resourced languages.

## Executive Summary
This paper introduces a framework for evaluating large language model (LLM) robustness against perturbations in less-resourced languages, using Polish as a case study. The method employs small proxy models trained on task-specific datasets to calculate word importance via attribution methods, enabling targeted generation of perturbed test examples. Character-level perturbations (e.g., typos, diacritical errors) and word-level perturbations (e.g., orthographic changes, lexical relations) are applied to the most important words. Evaluation on Polish datasets shows that even simple character perturbations significantly degrade LLM performance—attack success rates (ASR) often exceed 0.1, indicating vulnerability. SHAP attribution methods outperform others for identifying critical words. The framework is generalizable to other languages and helps developers assess and mitigate model weaknesses.

## Method Summary
The framework trains small proxy models (e.g., RoBERTa-base) on task-specific datasets to calculate word importance via attribution methods. These importance scores guide perturbation selection for attacking LLMs. Character-level perturbations (8 types) and word-level perturbations (3 types) are applied to the most important words. The method is evaluated on Polish datasets for sentiment analysis, abusive content detection, and cyberbullying detection. Attack success rates are computed by comparing LLM predictions on original vs. perturbed examples. SHAP attribution is found to be most effective for identifying high-impact perturbation targets.

## Key Results
- Even simple character perturbations (typos, diacritical errors) significantly degrade LLM performance in Polish, with ASR values often exceeding 0.1.
- SHAP attribution methods outperform vanilla gradients and random selection for identifying critical words whose perturbation disrupts LLM predictions.
- LLMs show higher vulnerability to perturbations in less-resourced languages like Polish compared to high-resource languages, likely due to imbalanced training data.

## Why This Works (Mechanism)

### Mechanism 1: Proxy Model Transfer for Efficient Importance Attribution
Small, fine-tuned transformer models can efficiently identify words whose perturbation degrades LLM performance, without requiring direct LLM gradient computation. A proxy model (e.g., RoBERTa-base) is trained on the target task, then attribution methods calculate word importance scores. These scores guide perturbation selection for attacking the LLM, transferring importance rankings from the small model to the large one. Core assumption: Words important for proxy model predictions correlate with words critical for LLM predictions on the same task.

### Mechanism 2: Attribution-Guided Attack Efficiency
SHAP-based attribution significantly outperforms random word selection and gradient-based methods for identifying high-impact perturbation targets. SHAP computes Shapley values approximating each word's marginal contribution to the prediction. Perturbing top-ranked words disrupts the model's decision boundary more efficiently than random perturbations. Core assumption: SHAP values faithfully approximate the model's reasoning process; higher attribution correlates with higher perturbation sensitivity.

### Mechanism 3: Low-Resource Language Robustness Gap
LLMs exhibit higher vulnerability to character and word-level perturbations in less-resourced languages due to imbalanced safety and robustness training. Safety alignment data is dominated by high-resource languages (primarily English). Low-resource languages receive less robustness training, creating exploitable decision boundaries. Even simple perturbations (typos, diacritic errors) cause significant performance degradation (ASR > 0.1). Core assumption: The robustness gap stems from training data imbalance, not inherent language complexity.

## Foundational Learning

- **Attribution Methods (SHAP, LIME, Gradient-based)**:
  - Why needed here: Understanding how different attribution methods identify important words is critical for selecting the most effective method for generating adversarial examples.
  - Quick check question: Given a text classification model, would you expect SHAP or vanilla gradients to better identify words whose removal changes the prediction? Why?

- **Character vs. Word-level Perturbations**:
  - Why needed here: The paper evaluates 10 perturbation types across two levels; understanding their tradeoffs is essential for designing effective robustness tests.
  - Quick check question: Why might diacritical errors be more impactful in Polish than in English, and how would this affect cross-lingual transfer of perturbation strategies?

- **Attack Success Rate (ASR) Metric**:
  - Why needed here: ASR quantifies model vulnerability; interpreting these values requires understanding what baseline and threshold values indicate.
  - Quick check question: If a model shows ASR of 0.05 for diacritical perturbations but 0.30 for keyboard errors, what does this suggest about its robustness profile?

## Architecture Onboarding

- **Component map**: Proxy model training -> Attribution computation -> Importance aggregation -> Perturbation engine -> Evaluation pipeline
- **Critical path**: Proxy model performance → Attribution quality → Perturbation effectiveness → ASR measurement. The paper uses polish-roberta-base-v2 + SHAP as the final configuration.
- **Design tradeoffs**:
  - SHAP vs. other attributions: SHAP highest ASR but computationally expensive; gradient methods fast but near-random effectiveness
  - Number of perturbed words: More words = higher ASR but may degrade text intelligibility (Figure 2 shows diminishing returns after ~5-7 words)
  - Proxy model size: Larger proxy may better approximate LLM but increases compute; small proxy may misalign with LLM importance patterns
- **Failure signatures**:
  - ASR near random baseline (~0.02-0.05): Attribution method not identifying truly important words
  - Proxy model failing to train (HerBERT on CBD dataset): Omit that configuration
  - Perturbed text unintelligible to humans: Reduce character perturbation intensity or word count
- **First 3 experiments**:
  1. Validate proxy attribution: Train proxy on single dataset (e.g., abusive clauses), compare SHAP vs. random word selection for attack success on proxy itself
  2. Cross-model transfer: Generate perturbations using RoBERTa proxy, evaluate ASR on multiple LLMs (Bielik, Mistral, Llama) to verify transferability
  3. Perturbation type analysis: For single LLM (e.g., Mistral), rank all 10 perturbation types by ASR to identify highest-impact vulnerabilities

## Open Questions the Paper Calls Out
- Do word importance rankings derived from proxy models accurately reflect the internal reasoning of target LLMs, or do they identify proxy-specific artifacts?
- How does the framework's attack efficacy vary when applied to morphologically rich or non-Latin script languages?
- To what extent do successful perturbations degrade the semantic meaning and human readability of the input text?

## Limitations
- The framework assumes proxy model word importance rankings transfer effectively to LLM behavior, but this assumption lacks empirical validation across different model families and tasks.
- All experiments focus exclusively on Polish datasets and LLMs, leaving generalizability to other less-resourced languages untested.
- The paper doesn't establish whether SHAP values actually reflect the true importance of words for the proxy model's reasoning process.

## Confidence
- High Confidence: The observation that simple character perturbations significantly degrade LLM performance in Polish (ASR > 0.1) is well-supported by experimental results.
- Medium Confidence: The claim that SHAP attribution methods outperform other attribution methods for identifying critical words is supported by comparative results but doesn't control for all confounding factors.
- Low Confidence: The assertion that low-resource language vulnerabilities stem primarily from training data imbalance rather than inherent language complexity is speculative with no comparative analysis.

## Next Checks
1. Cross-Task Proxy Transferability: Train a proxy model on abusive content detection, then use it to generate perturbations for cyberbullying detection. Compare ASR results with those from a cyberbullying-trained proxy.
2. High-Resource Language Baseline: Apply the identical framework to English datasets with the same LLMs. Measure ASR differences between Polish and English to determine whether vulnerability gaps are language-specific.
3. Real-World Perturbation Distribution: Analyze actual user-generated text containing typos and errors in Polish social media. Compare the frequency and distribution of these errors with the framework's synthetic perturbations.