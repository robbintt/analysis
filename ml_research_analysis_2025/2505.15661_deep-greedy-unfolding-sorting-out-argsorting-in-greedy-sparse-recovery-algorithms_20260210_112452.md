---
ver: rpa2
title: 'Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms'
arxiv_id: '2505.15661'
source_url: https://arxiv.org/abs/2505.15661
tags:
- softsort
- where
- algorithms
- neural
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Greedy sparse recovery algorithms like OMP and IHT depend on non-differentiable
  argsort operators, preventing their integration into differentiable neural network
  architectures. This work introduces Soft-OMP and Soft-IHT, which replace the argsort
  operator with differentiable softsort approximations.
---

# Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms

## Quick Facts
- arXiv ID: 2505.15661
- Source URL: https://arxiv.org/abs/2505.15661
- Reference count: 40
- Key outcome: Unrolled greedy sparse recovery networks (OMP-Net, IHT-Net) achieve near-noise-level recovery in severely undersampled regimes by learning latent sparsity structures.

## Executive Summary
This work addresses the fundamental limitation that prevents greedy sparse recovery algorithms (OMP, IHT) from being integrated into differentiable neural network architectures: the non-differentiable argsort operator. The authors introduce Soft-OMP and Soft-IHT, which replace argsort with a differentiable softsort approximation based on softmax. By unrolling these algorithms, they create OMP-Net and IHT-Net architectures that can be trained end-to-end. Theoretical analysis establishes conditions under which these algorithms approximate their exact counterparts with controllable accuracy, quantified by a temperature parameter τ. Experiments demonstrate significant performance gains in severely undersampled regimes by learning latent structure-aware priors from data.

## Method Summary
The method replaces the discrete argsort operator with a continuous relaxation called softsort, which constructs a row-stochastic matrix using softmax applied to negative scaled distances between vector elements. This creates a differentiable "soft" permutation matrix enabling gradient flow. The softsort operator is Lipschitz continuous, allowing theoretical analysis of the approximation error between Soft-OMP/Soft-IHT and their exact counterparts. The unrolled architectures (OMP-Net, IHT-Net) treat greedy selection weights as trainable parameters, enabling the network to learn latent sparsity patterns. The temperature parameter τ controls the tradeoff between approximation accuracy and gradient flow, with theoretical bounds established for the recovery error.

## Key Results
- Soft-OMP and Soft-IHT approximate their exact counterparts with controllable accuracy as τ → 0
- OMP-Net and IHT-Net outperform classical OMP/IHT by orders of magnitude in severely undersampled regimes
- Networks successfully infer latent sparsity patterns from data without explicit prior knowledge
- Near-noise-level recovery achieved in regimes where classical compressive sensing fails

## Why This Works (Mechanism)

### Mechanism 1
Replacing the discrete `argsort` operator with continuous `softsort` relaxation restores gradient flow through the network. The standard `argsort` yields zero gradients almost everywhere, blocking learning. `softsort` creates a differentiable "soft" permutation matrix using softmax, allowing gradients proportional to element magnitude similarities to flow through. The Lipschitz constant L ∝ 1/τ controls training stability, with τ controlling the hardness of the approximation.

### Mechanism 2
The "Soft" algorithms act as controllable approximations of classical OMP/IHT, with error bounds determined by temperature τ and the minimum gap between sorted elements. The soft mask replaces hard selection, and convergence to the exact algorithm is established as τ → 0. The min-gap condition (g > 0) ensures correct index identification, with the approximation error bounded by a function of τ and g(1:n).

### Mechanism 3
Unrolling enables learning of structure-aware weights that outperform classical fixed algorithms in undersampled regimes. By treating weights w as trainable parameters, the network learns to bias index selection toward specific supports. In severely undersampled regimes where classical CS fails, the network overfits to training data structure, effectively learning a structured prior that classical OMP assumes is uniform.

## Foundational Learning

- **Concept: Compressive Sensing (CS) Basics**
  - Why needed here: Understanding the problem y = Ax + e where m ≪ N is essential to grasp why greedy selection is necessary and why argsort is the bottleneck.
  - Quick check question: If I have 100 measurements (m) and a signal of dimension 1000 (N), why can't I just invert the matrix A to find x?

- **Concept: Algorithm Unrolling**
  - Why needed here: This architectural paradigm maps iterations of OMP to layers in a neural network, essential for understanding OMP-Net construction.
  - Quick check question: In unrolling, does the weight matrix A typically change across layers (iterations), or is it shared/fixed?

- **Concept: Softmax Temperature**
  - Why needed here: The core contribution relies on softsort built on softmax. Understanding τ is critical for debugging the tradeoff between hard sorting (accuracy) and soft sorting (gradient flow).
  - Quick check question: As temperature τ → 0, does the softmax output become a one-hot vector (harder) or a uniform distribution (softer)?

## Architecture Onboarding

- **Component map:** Input y ∈ ℂ^m, A ∈ ℂ^(m×N) → Correlation v = |A*(y - Ax^(l))| → Soft Selection P̃ = softsort_τ(v) → Projection update support/mask → Estimate x^(l+1) → Output reconstructed sparse vector x̂

- **Critical path:** The implementation of the softsort operator (Eq. 3.2) is critical. The distance matrix calculation |sort(v)1^T - 1v^T| must be vectorized and differentiable. Standard library sort is usually non-differentiable, requiring custom autograd nodes or relying on the formulation in Eq. (3.2) which only requires sorting the values.

- **Design tradeoffs:**
  - **Temperature τ:** Low τ provides better OMP approximation but vanishing gradients (L → ∞); high τ enables gradient flow but poor approximation.
  - **Layers vs. Iterations:** Fixed depth (L=10 or 30) increases receptive field but risks accumulating approximation errors.

- **Failure signatures:**
  - **Index Oscillation:** If τ is too high, soft mask P̃ might not be sparse, leading to dense noisy reconstructions.
  - **Gradient Explosion:** If τ is too low without careful initialization, the 1/τ factor in Lipschitz bound may destabilize training.
  - **Support Mismatch:** If data has no structure, learned weights w fail to converge, and performance lags due to approximation error.

- **First 3 experiments:**
  1. **Gradient Sanity Check:** Implement softsort and verify ∂P̃/∂v ≠ 0 for random vector v. Check behavior as τ varies.
  2. **Approximation Validation:** Replicate Fig 5.1. Run Soft-OMP vs OMP on fixed sparse signal. Plot ||x_soft - x_hard|| vs τ to find transition phase where approximation is good but gradients exist.
  3. **Weight Learning Toy Problem:** Create dataset with fixed support (first 10 indices active). Train OMP-Net from scratch. Check if learned weights w converge to high values for first 10 indices and low values elsewhere.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do greedy networks utilizing softsort compare against architectures using other differentiable sorting relaxations like Neuralsort or optimal transport-based methods? The authors suggest this comprehensive comparison remains to be pursued.

- **Open Question 2:** Can the projection-based softsort framework be extended to other greedy algorithms like CoSaMP, Subspace Pursuit, or greedy weighted-LASSO? The conclusion lists these as well-known candidates for extension.

- **Open Question 3:** Does employing distinct parameters across layers or learning parameters like step size η significantly improve performance over current weight-sharing approach? The authors suggest exploring these extensions.

## Limitations

- The theoretical analysis assumes a "min-gap" condition on correlation values that may not hold in practice for ill-conditioned sensing matrices or noisy measurements.
- The paper does not provide extensive empirical validation of the approximation bounds beyond synthetic experiments.
- The learned structure-aware weights may overfit to training data structure, potentially degrading generalization to out-of-distribution samples.

## Confidence

- **High Confidence:** The differentiability of the softsort operator and its role in enabling gradient-based training is well-established mathematically (Prop. 3.3).
- **Medium Confidence:** The theoretical approximation bounds in Theorem 3.6 are mathematically sound but require further validation for practical tightness.
- **Medium Confidence:** Experimental results demonstrating significant improvement over classical algorithms are compelling but limited to synthetic data with specific structures.

## Next Checks

1. **Approximation Bound Validation:** Implement experiments to empirically measure the error bound between Soft-OMP/IHT and their exact counterparts across different temperature values (τ) and signal conditions. Plot empirical error against theoretical bound to assess tightness.

2. **Generalization Test:** Evaluate trained OMP-Net/IHT-Net on test data where support structure differs from training data. Measure performance degradation to quantify overfitting to learned priors.

3. **Complex Number Differentiation:** Implement network with complex-valued Partial Fourier matrices and verify gradient-based training works in standard frameworks. If issues arise, implement custom complex autodiff nodes or use real-valued re-parameterization.