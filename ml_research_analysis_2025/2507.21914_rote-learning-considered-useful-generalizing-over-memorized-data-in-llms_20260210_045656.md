---
ver: rpa2
title: 'Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs'
arxiv_id: '2507.21914'
source_url: https://arxiv.org/abs/2507.21914
tags:
- prompt
- prompts
- test
- memorization
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the conventional view that rote memorization
  hinders generalization in language models. It introduces a two-phase "memorize-then-generalize"
  framework where a model first rote memorizes factual subject-object pairs using
  a semantically meaningless key token, then fine-tunes on semantically meaningful
  prompts to reinterpret the memorized data.
---

# Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs

## Quick Facts
- arXiv ID: 2507.21914
- Source URL: https://arxiv.org/abs/2507.21914
- Authors: Qinyuan Wu; Soumi Das; Mahsa Amani; Bishwamittra Ghosh; Mohammad Aflah Khan; Krishna P. Gummadi; Muhammad Bilal Zafar
- Reference count: 40
- Primary result: Rote memorization of facts using meaningless tokens followed by semantic fine-tuning enables generalization to unseen facts, prompts, and languages, outperforming standard fine-tuning and in-context learning

## Executive Summary
This paper challenges the conventional view that rote memorization hinders generalization in language models. It introduces a two-phase "memorize-then-generalize" framework where a model first rote memorizes factual subject-object pairs using a semantically meaningless key token, then fine-tunes on semantically meaningful prompts to reinterpret the memorized data. Experiments across 8 models show that this approach enables generalization to unseen facts, prompt variants, and even other languages, outperforming standard fine-tuning and in-context learning. The model achieves high accuracy (up to 95% in some settings) and demonstrates that memorization can serve as a foundation for reasoning. However, the method also poses risks: memorized facts can be maliciously repurposed through carefully crafted fine-tuning. The findings suggest that rote memorization, when structured properly, can be a powerful and efficient tool for knowledge injection in LLMs.

## Method Summary
The paper proposes a two-phase framework for knowledge injection into language models. In Phase 1, the model rote memorizes factual subject-object associations using a semantically meaningless key token (e.g., "[X]") through unsupervised next-token prediction. In Phase 2, the model fine-tunes on a small set of semantically meaningful prompts paired with the memorized facts, enabling generalization to unseen facts, prompts, and languages. The approach uses synthetic data with 5 T-REx relations (author, capital, educated_at, genre, mother), each with 100 fictional subject-object pairs. Key token embeddings are randomly initialized and added to the tokenizer. Training uses supervised fine-tuning for 1 epoch in Phase 2 with learning rate 1e-5 and cosine scheduler.

## Key Results
- Phase 1 rote memorization followed by Phase 2 semantic fine-tuning achieves up to 95% accuracy on unseen facts and prompts
- The method generalizes across 8 different models (1.5B-14B parameters) and 4 languages
- Using a meaningless key token in Phase 1 is critical—replacing it with a meaningful prompt eliminates generalization
- Generalization occurs with minimal supervision (k=1 fact + 1 prompt can achieve 0.68 generation accuracy)
- The framework outperforms standard fine-tuning and in-context learning baselines

## Why This Works (Mechanism)

### Mechanism 1: Semantic Anchoring via Key Token
A semantically meaningless key token serves as a structural anchor that binds subject-object associations independently of linguistic form, enabling later semantic reinterpretation. During Phase 1 rote learning, the model encodes factual associations through a synthetic token rather than natural language prompts, creating a "pure" relational binding without semantic confounds. During Phase 2, limited exposure to meaningful prompts reinterprets this anchor token, transferring the memorized associations to new semantic contexts.

### Mechanism 2: Representation Space Restructuring
Rote memorization progressively organizes relational structure in latent space, creating separable clusters that enable efficient semantic alignment during subsequent fine-tuning. Through repeated exposure during Phase 1, representations of facts sharing the same relation gradually cluster together (ΔCosSim increases from 0.058 to 0.191). Phase 2 fine-tuning then rapidly aligns these pre-structured clusters with semantic prompt representations (ΔCosSim reaches 0.258 after just 1 epoch).

### Mechanism 3: Minimal-Supervision Semantic Transfer
Once relational structure is encoded, a single fact-prompt pair can transfer semantic meaning to all memorized associations within that relation. The key token acts as a shared representation across all facts in a relation. Fine-tuning on one example (e.g., "Who is Gene Finley's mother? → Cody Ross") propagates the semantic interpretation of "mother" to the key token, which then applies to all 99 other memorized facts using that token.

## Foundational Learning

- **Concept: Rote Memorization vs. Generalization in Deep Learning**
  - Why needed here: The paper challenges the conventional view that rote memorization harms generalization; understanding this distinction is essential for interpreting the results.
  - Quick check question: Can you explain why overfitting to training data typically hurts test performance, and what "grokking" suggests about memorization-generalization dynamics?

- **Concept: Transformer Hidden States and Representation Learning**
  - Why needed here: The paper's mechanism analysis relies on extracting and analyzing hidden state representations (PCA visualizations, cosine similarity).
  - Quick check question: Given a transformer model and an input sequence, how would you extract the representation of a specific token from a specific layer?

- **Concept: Supervised Fine-Tuning vs. In-Context Learning**
  - Why needed here: The paper compares its approach against SFT and ICL baselines; understanding these paradigms is necessary to evaluate the efficiency claims.
  - Quick check question: What are the trade-offs between updating model weights (SFT) versus providing examples in the prompt (ICL) for knowledge injection?

## Architecture Onboarding

- **Component map:**
  Phase 1 (Rote Memorization): Subject-object pairs with key token → next-token prediction loss → multiple epochs (typically 10-20)
  Phase 2 (Generalization): Subset of memorized facts with meaningful prompts → supervised fine-tuning → typically 1 epoch
  Key Token: Randomly initialized embedding, added to vocabulary, unique per relation
  Evaluation: Three tiers—unseen facts (same prompts), unseen prompts (same relation), unseen languages

- **Critical path:**
  1. Create synthetic dataset of novel subject-object pairs (to ensure facts are not pre-known)
  2. Add key token(s) to tokenizer with random initialization
  3. Train Phase 1 until rote memorization saturates (monitor key token prompt accuracy)
  4. Select subset of facts (k=50 works well) and training prompts (1-10)
  5. Fine-tune Phase 2 for 1 epoch—stopping early is critical to avoid overfitting
  6. Evaluate generalization on held-out facts, prompts, and languages

- **Design tradeoffs:**
  - More Phase 1 epochs → better generalization but longer training (Table 1 shows 20 epochs outperforms 10)
  - Fewer Phase 2 examples → more efficient but requires deeper Phase 1 memorization
  - Single key token vs. multiple tokens: Paper uses one token per relation; Assumption: multiple tokens per relation may not improve results
  - Key token vs. meaningful prompt in Phase 1: Ablation (Figure 11) shows using meaningful prompts in Phase 1 degrades Phase 2 generalization—the meaningless token is critical

- **Failure signatures:**
  - No key token in Phase 1: Figure 10 shows generalization fails entirely (test prompt accuracy ~0)
  - Shallow memorization: Table 1 shows 3 epochs of Phase 1 gives 0.38 test accuracy vs. 1.00 at 20 epochs
  - Overfitting in Phase 2: Paper uses only 1 epoch explicitly to avoid this; extending Phase 2 is not evaluated
  - Cross-lingual transfer with entity translation: Figure 14 shows generalization fails when entity names are translated (performance drops to ~0)

- **First 3 experiments:**
  1. Replicate on single relation with minimal setup: Use the "author" relation with 100 synthetic facts, 20 epochs Phase 1, k=50 facts + 1 prompt for Phase 2. Verify test prompt accuracy >0.70 on held-out facts.
  2. Ablate key token: Run Phase 1 without key token (zero-token prompt, subject only). Confirm generalization fails per Figure 10, establishing the key token is necessary.
  3. Test representation alignment: Extract hidden states before and after Phase 2. Compute cosine similarity between key token and training prompts. Verify similarity increases (per Figure 18) as evidence of semantic alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the memorize-then-generalize framework enhance performance on complex reasoning tasks, such as multi-hop reasoning or mathematical problem solving?
- Basis in paper: Section 8 states, "The effectiveness of memorized knowledge in supporting generalization on more complex tasks, such as multi-hop reasoning, coding, or mathematical problem solving, remains an open question."
- Why unresolved: The experiments were intentionally constrained to simple factual tasks represented as subject-relation-object triplets to isolate the effects of rote memorization.
- What evidence would resolve it: Application of the framework to datasets requiring complex, multi-step inference (e.g., GSM8K or multi-hop QA benchmarks) to measure performance changes.

### Open Question 2
- Question: Does rote memorization in this framework reduce hallucination rates by anchoring the model, or does it exacerbate them through overconfidence?
- Basis in paper: Section 8 notes, "It remains unclear whether rote learning helps reduce hallucination by anchoring the model to known information or if it exacerbates the issue by fostering overconfidence."
- Why unresolved: The study focused on retrieval accuracy and generalization capability but did not include a systematic evaluation of hallucination behavior.
- What evidence would resolve it: Evaluation of factual reliability and hallucination frequency on standard benchmarks following the memorize-then-generalize training intervention.

### Open Question 3
- Question: How robust is the framework when injecting facts that conflict with a model's pre-existing knowledge?
- Basis in paper: Section 8 identifies a limitation: "We do not explore how our injected knowledge interacts with existing knowledge in the model... Understanding how memorization interacts with knowledge editing... is crucial."
- Why unresolved: The experiments utilized synthetic, fictional data specifically to ensure novelty, thereby avoiding the confounding variable of conflicting pre-trained knowledge.
- What evidence would resolve it: Testing the method on established knowledge editing datasets (e.g., CounterFact) where new facts explicitly contradict the model's pre-trained beliefs.

### Open Question 4
- Question: To what extent does the phase 2 fine-tuning process lead to catastrophic forgetting of the model's general capabilities?
- Basis in paper: Section 8 states, "We do not systematically evaluate whether the model forgets previously acquired knowledge during fine-tuning. Catastrophic forgetting... is not systematically assessed."
- Why unresolved: The evaluation focused narrowly on the retrieval of the newly injected facts rather than the retention of broader, previously learned tasks.
- What evidence would resolve it: A dual evaluation measuring performance on general benchmarks (e.g., MMLU or knowledge probes) alongside the new fact retrieval tasks.

## Limitations

- The framework's effectiveness on larger models (>14B parameters) and different architectures (e.g., Mamba, RWKV) remains untested, raising questions about architectural dependencies
- All experiments use artificially generated facts rather than real-world knowledge, leaving open whether the memorization-generalization mechanism operates equivalently on factual vs. procedural knowledge
- Cross-lingual generalization appears brittle, failing when entity names are also translated rather than demonstrating true cross-lingual concept mapping
- Security implications are raised but not empirically validated with specific attack scenarios or success rates

## Confidence

**High confidence (8/10)**: The empirical demonstration that rote memorization followed by semantic fine-tuning produces superior generalization compared to alternatives. The effect is robust across multiple models, relations, and evaluation settings, with clear quantitative metrics supporting the core claim.

**Medium confidence (6/10)**: The proposed mechanism of semantic anchoring via meaningless tokens creating representation clusters that enable efficient transfer. While the empirical results are strong, the causal chain from key token to representation clustering to semantic transfer involves multiple assumptions about representation dynamics that lack independent validation.

**Low confidence (4/10)**: The security implications and generalizability to real-world knowledge. These claims extend beyond the empirical scope and rely on extrapolation rather than direct evidence.

## Next Checks

1. **Architectural dependency test**: Replicate the two-phase framework on a non-transformer architecture (e.g., Mamba or RWKV) using the same synthetic dataset and key token approach. Compare generalization performance to establish whether the mechanism depends on transformer-specific features like residual connections or self-attention patterns.

2. **Real-world knowledge injection**: Apply the framework to inject a small set of real factual knowledge (e.g., recent scientific discoveries not in pretraining) into a model, then test generalization to related but unseen facts. This validates whether the synthetic data success transfers to practical knowledge injection scenarios where facts have complex relationships.

3. **Attack vector demonstration**: Design and execute a controlled experiment demonstrating malicious fine-tuning of memorized facts. Start with Phase 1 memorization of neutral facts, then attempt fine-tuning to repurpose them for harmful outputs (e.g., converting benign medical facts into misinformation). Measure success rates and compare to baseline models without Phase 1 memorization.