---
ver: rpa2
title: Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for
  Robust Image Classification
arxiv_id: '2503.24017'
source_url: https://arxiv.org/abs/2503.24017
tags:
- teacher
- text
- student
- embeddings
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-teacher crossmodal knowledge distillation
  framework that leverages WordNet-relaxed text embeddings to mitigate label leakage
  and improve student model performance. By replacing exact class names with semantically
  richer WordNet expansions, the method addresses the limitations of direct textual
  inputs, which often fail to capture complex visual semantics and lead to artificial
  teacher accuracy.
---

# Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for Robust Image Classification

## Quick Facts
- arXiv ID: 2503.24017
- Source URL: https://arxiv.org/abs/2503.24017
- Authors: Chenqi Guo; Mengshuo Rong; Qianli Feng; Rongfan Feng; Yinglong Ma
- Reference count: 25
- Primary result: State-of-the-art student accuracy of 83.33% on CIFAR100 using WordNet-relaxed text embeddings

## Executive Summary
This paper introduces a multi-teacher crossmodal knowledge distillation framework that leverages WordNet-relaxed text embeddings to mitigate label leakage and improve student model performance. By replacing exact class names with semantically richer WordNet expansions, the method addresses the limitations of direct textual inputs, which often fail to capture complex visual semantics and lead to artificial teacher accuracy. Experiments across six public datasets demonstrate state-of-the-art or near-state-of-the-art results, with the best student accuracy reaching 83.33% on CIFAR100. Interpretability analyses confirm that WordNet-relaxed prompts reduce reliance on textual shortcuts, encouraging the teacher model to focus more on robust visual features.

## Method Summary
The method employs a multi-teacher setup with a unimodal teacher ensemble (Tm) and a multimodal teacher (Tx) that combines CLIP image embeddings with WordNet-relaxed text embeddings. WordNet relaxation replaces exact class names with semantically related descriptors, reducing label leakage while preserving semantic structure. Learnable text embeddings are initialized from CLIP and updated during training with hierarchical loss and cosine regularization constraints. The student ResNet18 is trained via KL divergence on averaged teacher logits, using images only at inference time. The approach effectively balances semantic diversity and alignment to enhance crossmodal knowledge transfer.

## Key Results
- Achieves state-of-the-art student accuracy of 83.33% on CIFAR100
- Demonstrates significant improvements over standard KD baselines across all six tested datasets
- Reduces label leakage by replacing exact class names with WordNet expansions, decreasing teacher accuracy from 100% to ~64% while improving student performance
- Captum analysis shows increased reliance on visual features as WordNet relaxation increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing exact class names with WordNet expansions reduces label leakage and forces the teacher to learn more transferable visual representations.
- Mechanism: Exact class-name prompts create a textual shortcut where the teacher achieves near-100% accuracy by memorizing class-token associations rather than learning visual features. WordNet relaxation removes this direct label-token shortcut while preserving semantic structure, compelling the teacher to rely more on CLIP image embeddings.
- Evidence: Figure 2 shows 100% teacher accuracy with ground-truth text drops to ~64% with 100% WordNet relaxation.

### Mechanism 2
- Claim: Learnable WordNet-relaxed embeddings outperform both fixed pretrained embeddings and random noise because they allow task-specific semantic adaptation while maintaining coherence.
- Mechanism: The embeddings are initialized from CLIP text embeddings and updated during training via hierarchical loss (alignment with exact class embeddings) and cosine regularization (preventing drift), balancing semantic flexibility with integrity.
- Evidence: Table 3 shows learnable n_relaxed outperforms fixed n_pretrained across all datasets (83.33% vs 82.97% on CIFAR100).

### Mechanism 3
- Claim: A multi-teacher setup combining unimodal and multimodal teachers provides complementary supervision that outperforms either teacher alone.
- Mechanism: The unimodal teacher provides robust visual supervision while the multimodal teacher provides semantically enriched representations via CLIP image+WordNet text embeddings. Their logits are averaged before distillation.
- Evidence: Table 2 shows student accuracy improves from ~79% (Tx alone) to 83.33% (Tm+Tx combined).

## Foundational Learning

- **Knowledge Distillation with Temperature Scaling**
  - Why needed here: The framework builds on soft label distillation; understanding how temperature τ controls distribution smoothness is essential for tuning λ in Equation 3.
  - Quick check question: If increasing τ makes soft labels more uniform, what happens to the student's learning signal when teachers disagree?

- **CLIP Image-Text Embedding Space**
  - Why needed here: The method relies on CLIP's joint embedding space where semantically similar images and text have high cosine similarity, enabling meaningful WordNet-relaxed text supervision.
  - Quick check question: Why can CLIP text embeddings serve as "virtual samples" for classification, and what properties of the pretraining make this possible?

- **Modality-General vs. Modality-Specific Features**
  - Why needed here: The core hypothesis depends on distinguishing features shared across modalities (visual features) from modality-specific shortcuts (exact text tokens).
  - Quick check question: In a teacher with image+text inputs and a student with image-only inputs, which teacher features would transfer most effectively?

## Architecture Onboarding

- Component map:
  ```
  Input Image ──┬──> CLIP Image Encoder ──┐
                │                         │
                ├──> Data Augmentation ──> Unimodal Teacher Tm (ResNet50) ──┐
                │                                                           │
                └──> (Class Label → WordNet → Filtered Nouns) ──>          │
                            │                                              │
                            └──> CLIP Text Encoder ──> Learnable          │
                                      │                   Embeddings     │
                                      │                         │        │
                                      └─────────> Concat ───────┴──> Multimodal
                                                                Teacher Tx
                                                                      │
                                          Logit Averaging <───────────┘
                                                │
                                                v
                                     Student S (ResNet18)
                                                │
                                     KD Loss (KL Div) + CE Loss
  ```

- Critical path:
  1. WordNet relaxation generation: Noun embeddings are clustered via K-means on CLIP image embeddings; top-5 nouns per cluster are selected
  2. Teacher training: Tx is fine-tuned with hierarchical loss and cosine regularization; Tm is pretrained with augmentation ensemble
  3. Student distillation: Student matches averaged teacher logits via Equation 2-3; inference uses images only

- Design tradeoffs:
  - WordNet ratio: Higher relaxation (→100%wn) reduces label leakage but may weaken textual signal; paper finds 100%wn optimal
  - λ_hier vs λ_cosreg: Stronger hierarchical loss (higher λ_hier) enforces tighter class alignment but may limit semantic exploration; paper uses 0.1/0.01 split
  - Teacher backbone choice: Larger teachers (ViT) may improve ImageNet but increase computational cost; paper uses ResNet50 for efficiency

- Failure signatures:
  - Teacher accuracy >> student accuracy: Likely label leakage; check if exact class names are leaking into text embeddings
  - Student accuracy plateaus despite increasing WordNet ratio: Textual cues may be too weak; try increasing λ_hier
  - Captum shows low image attribution: Teacher is still relying on text shortcuts; increase WordNet relaxation

- First 3 experiments:
  1. Baseline KD comparison: Train student with only unimodal teacher Tm on CIFAR100 to establish baseline (~82% accuracy)
  2. Label leakage diagnostic: Train Tx with 100% ground-truth text on CIFAR100; expect 100% teacher accuracy but poor student (~77%)
  3. WordNet ratio sweep: Vary WordNet ratio (0%, 20%, 50%, 80%, 100%) on CIFAR100; plot teacher vs. student accuracy to reproduce Figure 2 pattern

## Open Questions the Paper Calls Out

None

## Limitations

- Generalizability uncertainty: Effectiveness for fine-grained or specialized domains (e.g., medical imaging) remains untested
- Computational overhead: Generating and filtering WordNet expansions may be prohibitive for datasets with hundreds or thousands of classes
- Adversarial robustness: Current evaluation doesn't address vulnerabilities introduced by learned WordNet-relaxed embeddings

## Confidence

- **High Confidence (95%+):** Core mechanism of reducing label leakage through WordNet relaxation is well-supported by direct experimental evidence
- **Medium Confidence (75-85%):** Claim that learnable embeddings outperform fixed pretrained embeddings has moderate support but limited ablation
- **Low Confidence (60-70%):** Multi-teacher ensemble superiority claim relies on limited comparisons without exploring alternative strategies

## Next Checks

1. Apply the framework to a fine-grained dataset (e.g., CUB-200) and measure whether WordNet relaxation still provides benefits, comparing against both exact text prompts and no-text baselines.

2. Generate adversarial examples for the teacher models and measure whether WordNet relaxation improves student robustness compared to exact text prompts, validating whether the method encourages learning of more generalizable visual features.

3. Systematically vary the averaging strategy for combining teacher logits (weighted averaging, product of probabilities, etc.) and measure impact on student performance to test whether improvements stem from complementary teacher knowledge or ensemble effects.