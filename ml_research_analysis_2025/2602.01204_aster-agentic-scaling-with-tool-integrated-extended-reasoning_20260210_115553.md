---
ver: rpa2
title: 'ASTER: Agentic Scaling with Tool-integrated Extended Reasoning'
arxiv_id: '2602.01204'
source_url: https://arxiv.org/abs/2602.01204
tags:
- tool
- reasoning
- training
- aster
- cold-start
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses interaction collapse, a pathological failure
  mode in tool-integrated reasoning where models fail to sustain multi-turn tool usage
  and instead degenerate into heavy internal reasoning with only trivial, post-hoc
  code verification. The core method, ASTER (Agentic Scaling with Tool-integrated
  Extended Reasoning), circumvents this collapse through a targeted cold-start strategy
  prioritizing interaction-dense trajectories.
---

# ASTER: Agentic Scaling with Tool-integrated Extended Reasoning

## Quick Facts
- **arXiv ID**: 2602.01204
- **Source URL**: https://arxiv.org/abs/2602.01204
- **Reference count**: 40
- **Primary result**: ASTER-4B achieves 90.0% on AIME 2025, surpassing DeepSeek-V3.2-Exp and other open-source models

## Executive Summary
ASTER addresses interaction collapse, a failure mode in tool-integrated reasoning where models degenerate from multi-turn tool usage to shallow post-hoc verification during reinforcement learning. The core method uses a targeted cold-start strategy with interaction-dense trajectories, where a small expert subset of just 4K trajectories enables superior exploration during extended RL training. Extensive evaluations show ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025.

## Method Summary
ASTER uses a two-stage RL training approach with a targeted cold-start strategy to prevent interaction collapse in tool-integrated reasoning. The method fine-tunes Qwen3-4B-Thinking-2507 on a curated dataset, starting with 4K interaction-dense trajectories (≥9 tool calls each) to establish a robust prior. This is followed by multi-stage GRPO training with specific hyperparameters (G=8 samples/prompt, Clip-Higher ε_h=0.28) across 18K and 32K context windows. The approach focuses on sustaining multi-turn tool usage throughout training rather than allowing the model to degenerate into post-hoc verification.

## Key Results
- ASTER-4B achieves 90.0% on AIME 2025, surpassing DeepSeek-V3.2-Exp
- Maintains 5-15+ tool calls per trajectory, avoiding interaction collapse
- Outperforms baseline methods including ReTool and DemyAgent on multiple mathematical benchmarks

## Why This Works (Mechanism)
ASTER prevents interaction collapse by initializing training with interaction-dense trajectories that establish a strong prior for sustained tool usage. The 4K expert subset (≥9 tool calls per trajectory) creates a robust behavioral foundation that enables superior exploration during extended RL training. This cold-start strategy ensures the model maintains multi-turn interaction patterns rather than degenerating into shallow verification, while the two-stage GRPO training with specific hyperparameters (Clip-Higher, no KL penalty) further reinforces these behaviors.

## Foundational Learning
- **Tool-Integrated Reasoning (TIR)**: Mathematical problem-solving using external tools; needed to understand the domain and interaction collapse; quick check: can the model use Python interpreter for verification
- **Interaction Collapse**: Failure mode where models stop using tools and resort to verification; needed to understand the core problem; quick check: monitor tool call counts dropping below 2-3 per trajectory
- **GRPO (Group Relative Policy Optimization)**: RL algorithm for optimizing reasoning trajectories; needed for the training procedure; quick check: policy improvement with G=8 samples/prompt
- **Clip-Higher**: Modified PPO clipping for stable training; needed to prevent premature convergence; quick check: ε_h=0.28 maintains exploration
- **Cold-start Initialization**: Starting RL with curated expert data; needed to establish behavioral priors; quick check: 4K subset with ≥9 tool calls per trajectory
- **Interaction Density**: Measure of sustained tool usage; needed to curate the expert subset; quick check: count tool calls per trajectory

## Architecture Onboarding

**Component Map**: Data curation → Cold-start SFT → Stage 1 RL (18K context) → Stage 2 RL (32K context) → Evaluation

**Critical Path**: The interaction-dense cold-start subset → sustained multi-turn tool usage during RL → superior exploration → state-of-the-art performance

**Design Tradeoffs**: Uses small 4K expert subset for cold-start rather than large general dataset, prioritizing behavioral quality over quantity; employs Clip-Higher without KL penalty to maintain exploration; focuses on preventing interaction collapse rather than maximizing immediate SFT accuracy

**Failure Signatures**: 
- Interaction collapse: Tool call counts dropping to 1-2 per trajectory with only trivial verification
- Early performance plateau: Baseline methods converge to suboptimal accuracy due to premature entropy collapse
- Data curation failure: Inability to identify and count tool calls for interaction density detection

**First Experiments**:
1. Implement tool call count monitoring during RL training to verify sustained multi-turn interaction
2. Compare ASTER-4B performance on AIME 2024 vs. AIME 2025 to validate generalization
3. Run ablation study removing the 4K interaction-dense cold-start subset to demonstrate its necessity

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on specific data curation pipeline and tool execution environment not fully specified
- Claims of surpassing DeepSeek-V3.2-Exp require independent validation due to proprietary nature of comparison models
- Effectiveness of 4K interaction-dense subset may not generalize across different base models and tool environments

## Confidence

**High Confidence**: Core observation of interaction collapse as pathological failure mode; two-stage RL training procedure with GRPO and Clip-Higher

**Medium Confidence**: Specific effectiveness of 4K interaction-dense subset for cold-start initialization based on ablation studies

**Low Confidence**: Claims of achieving state-of-the-art results on AIME 2025 and surpassing DeepSeek-V3.2-Exp without independent replication

## Next Checks
1. Replicate tool-augmented solution generation using accessible teacher model and verify interaction density detection method to curate 4K expert subset
2. Implement real-time tool call count logging during RL training and plot distribution to empirically confirm absence of interaction collapse
3. Run final ASTER-4B model on public AIME 2024 dataset and held-out HMMT2025 subset using specified evaluation settings for independent performance validation