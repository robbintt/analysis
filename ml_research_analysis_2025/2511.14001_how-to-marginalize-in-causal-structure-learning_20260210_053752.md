---
ver: rpa2
title: How to Marginalize in Causal Structure Learning?
arxiv_id: '2511.14001'
source_url: https://arxiv.org/abs/2511.14001
tags:
- learning
- structure
- bayesian
- probabilistic
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of marginalizing over parent
  sets in Bayesian structure learning, a critical but computationally expensive task.
  Traditional methods rely on dynamic programming, which becomes intractable for large
  numbers of candidate parents.
---

# How to Marginalize in Causal Structure Learning?

## Quick Facts
- arXiv ID: 2511.14001
- Source URL: https://arxiv.org/abs/2511.14001
- Authors: William Zhao; Guy Van den Broeck; Benjie Wang
- Reference count: 8
- One-line primary result: Probabilistic circuits enable exact marginalization over all parent sets in Bayesian structure learning, outperforming restricted dynamic programming baselines

## Executive Summary
This paper addresses the computational bottleneck of marginalizing over parent sets in Bayesian structure learning, where traditional dynamic programming becomes intractable for large numbers of candidate parents. The authors propose using tractable probabilistic circuits (PCs) that inherently support exact marginalization in time linear to circuit size, bypassing the O(3^N) dynamic programming limitation. Their method involves training a PC on both complete parent sets and marginalized queries using a novel two-phase learning algorithm, then integrating this PC with the TRUST framework for structure learning.

## Method Summary
The method replaces dynamic programming marginalization with probabilistic circuits that are both decomposable and smooth, enabling exact marginalization through forward passes. A two-phase training routine first establishes baseline fluency over complete parent sets via gradient descent on Bayesian scores, then iteratively finetunes on (k,0)/(k,1) marginal queries—exactly the query types needed during structure learning—using limited exact DP for ground-truth labels. The PC architecture uses RAT-SPN-style sum-product networks with unnormalized sum nodes, storing weights in log-domain and initialized with m·log(U(0,1)) where m ≈ -10.

## Key Results
- TRUST+PC outperforms TRUST+restricted DP on AUROC, MSE-CE, and MLL metrics while maintaining support over all 2^(N-1) parent sets
- The PC-based approach scales to 20 variables without restricting candidate parents, whereas DP requires limiting to 8 parents
- Marginal-aware training in Phase 2 significantly improves performance compared to training only on complete parent sets

## Why This Works (Mechanism)

### Mechanism 1
Smooth and decomposable probabilistic circuits enable exact marginalization in time linear to circuit size, bypassing the O(3^N) dynamic programming bottleneck. The circuit architecture enforces smoothness (children of sum nodes share scope) and decomposability (children of product nodes have disjoint scopes). When these hold, marginalizing over any subset of variables reduces to a forward pass through the circuit—no exponential enumeration required.

### Mechanism 2
Training PCs on marginal queries directly (not just complete parent sets) improves accuracy for the specific queries structure learners require. The two-phase learning routine first establishes baseline fluency over complete parent sets via gradient descent on Bayesian scores. The second phase interleaves (k,0) and (k,1) marginal queries with resampled complete sets. Iterative curriculum over k builds marginal accuracy progressively via the recurrence relation: g(A_i, A'_i) = g(A_i ∪ b, A'_i) + g(A_i, A'_i ∪ b).

### Mechanism 3
PC-based marginalization maintains support over all 2^(N-1) possible parent sets, whereas restricted DP methods exclude many parent configurations. Rather than precomputing and storing only marginal values for a restricted candidate set, the PC learns a parametric approximation over the full space. At inference, any marginal/zero query can be answered without having pre-enumerated it.

## Foundational Learning

- **Concept: Bayesian Structure Learning & Posterior over DAGs**
  - Why needed here: The entire method operates on Bayesian scores B(G) ∝ posterior probability of graph structures. Understanding that scores decompose node-wise (modularity assumption) is essential for grasping why per-node PCs suffice.
  - Quick check question: Given modularity, why can we learn one PC per node rather than one PC for the entire graph posterior?

- **Concept: Probabilistic Circuits (Sum-Product Networks)**
  - Why needed here: The core contribution is replacing DP with PCs. You must understand how leaf/product/sum nodes compose distributions and why smoothness+decomposability enables tractable marginalization.
  - Quick check question: If a product node has two children with overlapping scopes, what marginalization guarantee is lost?

- **Concept: Topological Ordering & Marginal/Zero Queries**
  - Why needed here: Structure learners like TRUST operate over orderings, requiring marginal queries where variables are either marginalized or set to 0. The PC training targets these query patterns specifically.
  - Quick check question: In scoring ordering σ, why must S_σj = 0 for all j ≥ i when computing the marginal for node σ_i?

## Architecture Onboarding

- **Component map:**
  Leaf layer (M, N) matrix of unnormalized Bernoulli nodes → alternating product layers (halve rows, enforce decomposability) → sum layers (maintain rows, enforce smoothness) → root node output

- **Critical path:**
  1. Initialize PC with log-domain parameters ~ m·log(U(0,1)), m ≈ -10
  2. Phase 1: Train on weighted-sampled complete parent sets (weight ∝ 2^(M-T) where T = number of 1s)
  3. Phase 2 iterations (k = 1 to L): Sample (k,0)/(k,1) marginals from limited DP + resampled complete sets; train jointly
  4. At inference: Forward pass any marginal/zero query in O(circuit size) time

- **Design tradeoffs:**
  - Latent size N: Larger N increases capacity but also memory/compute. Paper uses N=256 for d=16, N=64 for d=20
  - Marginal training limit L: Too many iterations causes forgetting of earlier marginals; paper uses L=7
  - High initial learning rate (10^-1) combats vanishing gradients from unnormalized log-domain weights, but requires careful plateau-based reduction

- **Failure signatures:**
  - Higher E-SHD than exact DP despite good AUROC → PC tends to sample denser graphs with more edges. Monitor edge sparsity
  - Vanishing gradients during training → Check log-domain weight initialization; ensure multiplier m is sufficiently negative
  - Marginal accuracy degrades with increasing k → Reduce L or increase batch sizes for later iterations

- **First 3 experiments:**
  1. Validation on small networks (d=16): Train PC with full DP candidate set (15 parents). Compare TRUST+PC vs. TRUST+exact DP on E-SHD, AUROC, MLL, MSE-CE
  2. Scalability test (d=20): Restrict DP to 8 candidate parents. Compare TRUST+PC (full support) vs. TRUST+restricted DP
  3. Ablation on marginal training: Remove Phase 2 (train only on complete parent sets). Measure degradation in marginal query accuracy and downstream E-SHD/AUROC

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the performance improvement from removing candidate parent restrictions increase when learning denser ground-truth structures? Experiments only tested graphs with average 2 edges per node, but the authors suggest denser graphs may show larger improvements.

- **Open Question 2:** Can the PC-based marginalization approach be adapted to other Bayesian scores (e.g., BDeu for discrete variables) and integrated with other structure learning algorithms beyond TRUST? The method was only implemented and tested with BGe score for linear Gaussian mechanisms within TRUST framework.

- **Open Question 3:** Why does the PC-based approach systematically produce higher Expected Structural Hamming Distance (E-SHD) and sample denser graphs compared to exact dynamic programming? The paper identifies this systematic bias but does not investigate the underlying cause or propose mitigation strategies.

- **Open Question 4:** How does the method scale to networks with substantially more variables (e.g., d > 50)? Experiments were limited to d=16 and d=20 nodes, and scalability remains untested.

## Limitations
- The method relies on limited exact DP to generate ground-truth labels for marginal queries, but this assumption is not empirically validated for queries outside the training distribution
- No systematic evaluation of PC marginal accuracy on held-out queries or characterization of approximation error bounds
- The approach may introduce bias toward denser graphs, as evidenced by higher E-SHD compared to exact DP
- Scalability to networks with substantially more variables (d > 50) remains untested

## Confidence
- **High Confidence:** The theoretical foundation that smooth and decomposable PCs enable exact marginalization in time linear to circuit size
- **Medium Confidence:** The empirical improvement of TRUST+PC over TRUST+restricted DP across multiple metrics (AUROC, MSE-CE, MLL)
- **Low Confidence:** The claim that marginal-aware training in Phase 2 is the primary driver of performance gains

## Next Checks
1. Evaluate PC's ability to answer marginal/zero queries for parent sets not included in the limited DP training labels to test generalization assumption
2. Systematically vary latent size N and average number of parents per node to measure when PC approximation error becomes unacceptable for dense graphs
3. Break down E-SHD into false positive and false negative edge rates to determine whether PC systematically overpredicts edges and whether this is training artifact or inherent bias