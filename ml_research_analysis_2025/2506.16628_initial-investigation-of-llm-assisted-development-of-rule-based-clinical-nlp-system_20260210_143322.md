---
ver: rpa2
title: Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP
  System
arxiv_id: '2506.16628'
source_url: https://arxiv.org/abs/2506.16628
tags:
- snippet
- clinical
- infection
- were
- snippets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored a novel approach using large language models
  (LLMs) to assist in developing rule-based clinical natural language processing (NLP)
  systems. By leveraging LLMs only during the development phase, the method aims to
  retain the interpretability and efficiency of rule-based systems while reducing
  the labor-intensive manual effort.
---

# Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System

## Quick Facts
- arXiv ID: 2506.16628
- Source URL: https://arxiv.org/abs/2506.16628
- Reference count: 40
- Primary result: LLM-assisted method achieves 0.98-0.99 recall in snippet identification and 100% keyword coverage for rule-based clinical NLP

## Executive Summary
This study presents a novel approach using large language models (LLMs) to assist in developing rule-based clinical natural language processing systems. The method leverages LLMs only during the development phase to reduce labor-intensive manual effort while preserving the interpretability and efficiency of rule-based systems. Tested on surgical site infection (SSI) detection, the approach successfully identified clinically relevant text snippets and extracted keywords for named entity recognition (NER) rules with high recall. Error analysis revealed that most false positives contained clinically useful information, suggesting the method could streamline rule-based NLP development with minimal labeled data.

## Method Summary
The method uses two quantized instruct-tuned LLMs (Deepseek R1 distilled Qwen 32B and Qwen2.5 Coder 32B) to assist in two core tasks: identifying clinically relevant snippets from clinical notes and extracting keywords for NER rules. The approach employs a two-step prompting process with reasoning and verification steps, incorporating clinical guidelines directly into prompts. For snippet identification, the method uses majority voting across 5 runs per case. Keywords are extracted and validated against annotation guidelines. The final rules are intended for integration into an existing rule-based system (EasyCIE) that uses regular expressions and logical operators.

## Key Results
- Snippet identification achieved high recall (Deepseek: 0.98, Qwen: 0.99) with low precision (0.08-0.10)
- Keyword extraction achieved 100% coverage of original NER rules
- Most "false positive" snippets contained clinically useful cues that annotators skipped
- Qwen produced 9 invalid output format errors versus Deepseek's 0

## Why This Works (Mechanism)

### Mechanism 1
Embedding clinical guidelines directly into prompts enables domain-aligned reasoning without task-specific fine-tuning. The NSQIP manual and annotation guidelines were converted to markdown and placed at prompt onset, providing explicit clinical criteria for SSI detection. LLMs then apply pre-trained semantic understanding against these embedded rules. This assumes the LLM's pre-trained medical knowledge can be reliably guided by in-context guidelines without hallucinating criteria not present in the source material.

### Mechanism 2
A two-stage reasoning-then-verification process reduces output format errors and improves consistency. Task 1 used a reasoning step (model walks through analysis) followed by a verification step (model validates its own output against annotation guidelines). Majority voting over 5 runs provided additional robustness. This assumes LLMs can reliably critique their own reasoning when given the same context in a separate pass.

### Mechanism 3
Optimizing for recall over precision in upstream components is viable when downstream filtering exists. Snippet identification prioritized sensitivity (capturing all potentially relevant text), accepting low precision (0.08-0.10). Error analysis showed most "false positives" contained clinically useful cues that annotators skipped due to document-level focus. This assumes downstream NER and context detection components can effectively filter over-inclusive snippet collections.

## Foundational Learning

- Concept: **Rule-based NLP systems vs. ML/LLM approaches**
  - Why needed here: The paper's core innovation assumes you understand why rule-based systems persist (interpretability, operational efficiency, controlled adaptability) despite ML advances.
  - Quick check question: Can you explain why a hospital might prefer a rule-based system over an LLM for production clinical surveillance, even if the LLM has higher accuracy?

- Concept: **Named Entity Recognition (NER) in clinical text**
  - Why needed here: The keyword extraction task directly feeds NER rule construction. Understanding that NER identifies and classifies entity mentions (anatomical sites, procedures, infections) is prerequisite.
  - Quick check question: Given the snippet "Moderate volume abdominal and pelvic ascites," what entities would a clinical NER system extract, and why might "thickening" be a poor keyword choice?

- Concept: **Chain-of-thought (CoT) prompting**
  - Why needed here: The method explicitly uses CoT strategy for prompt refinement. Understanding how step-by-step reasoning prompts differ from direct instruction prompts is essential.
  - Quick check question: What is the difference between asking an LLM "Is this snippet relevant to SSI?" versus asking it to "walk through analyzing signs, symptoms, and treatment information" before concluding?

## Architecture Onboarding

- Component map:
Clinical Notes (Epic EMR)
    ↓
[Snippet Segmentation] ← medspaCy sentence splitting
    ↓
[Task 1: Snippet Identification]
    ├── Reasoning Step (NSQIP guidelines + CoT)
    ├── Verification Step (annotation guidelines)
    └── Majority Vote (5 runs)
    ↓ Relevant Snippets
[Task 2: Keyword Extraction]
    ├── Reasoning Step (identify entities by category)
    └── Verification Step (prune, expand synonyms)
    ↓ Candidate Keywords
[Rule Construction] ← Human-in-the-loop (current)
    ↓
[EasyCIE NER Component] ← Existing rule-based system
    ↓
SSI Classification

- Critical path: Task 1 (snippet ID) → Task 2 (keyword extraction) → Human review → Rule integration into EasyCIE. Failure at Task 1 propagates; missed snippets mean missed keywords.

- Design tradeoffs:
  - **Recall vs. Precision**: High recall (0.98-0.99) with low precision (0.08-0.10) means humans review more false positives but miss fewer true signals
  - **Development cost vs. Operational cost**: LLMs are expensive at inference but used only during development; final rule-based system is fast and cheap in production
  - **Model size vs. Deployment constraints**: 32B quantized models fit on dual A100s; smaller models may lose clinical reasoning capacity

- Failure signatures:
  - **Ill-formatted input**: Both models missed SSI evidence when lab table strings preceded clinical text
  - **Over-generalized keywords**: Without explicit constraints, models generated terms like "thickening" that would match too broadly
  - **Invalid output format**: Qwen produced 9 format errors vs. Deepseek's 0
  - **Context-dependent relevance**: Models flagged "no rashes" as potentially useful but annotators skipped it as non-informative in isolation

- First 3 experiments:
  1. **Reproduce snippet identification on a held-out note type** (e.g., discharge summaries vs. operative notes). Measure recall/precision against manual annotation.
  2. **Test keyword extraction coverage with ablated prompts** (remove synonym expansion step, remove pruning step). Compare against gold-standard rules to identify which prompt components drive the 100% coverage.
  3. **Pilot rule integration workflow** - Take 50 LLM-suggested keywords, have a clinical informaticist convert them to EasyCIE rules, measure time saved vs. manual keyword discovery.

## Open Questions the Paper Calls Out

- Can this LLM-assisted approach generalize effectively to other clinical NLP tasks beyond surgical site infection (SSI) detection? The authors note findings are specific to the SSI detection task and dataset used and may not generalize directly to other clinical domains or institutions without further validation.

- Can LLMs accurately generate complex rule types, such as PSEUDO rules required to override false-positive matches, in addition to simple keyword extraction? The current study focused only on identifying snippets and extracting keywords, leaving the generation of logical operators and rule hierarchies unexplored.

- Can reinforcement learning (RL) optimized with rule-execution feedback bridge the gap between over-generalized keyword extraction and production-ready rule sets? The paper proposes that employing reinforcement learning informed by feedback from rule execution could further enhance model performance.

## Limitations

- Few-shot examples and exact prompt engineering details are not provided due to page limits, creating a "black box" in reproducibility
- High recall with very low precision (0.08-0.10) means substantial manual review burden remains, though most "false positives" contain clinically useful information
- The approach assumes guideline-based prompts can reliably constrain LLM reasoning without task-specific fine-tuning, but this hasn't been validated across different clinical domains

## Confidence

- **High confidence**: The two-step reasoning-verification prompt structure improves output consistency and format compliance
- **Medium confidence**: LLM-generated keywords achieve 100% coverage of original rules, suggesting effectiveness for NER rule construction
- **Medium confidence**: High snippet recall (0.98-0.99) indicates guideline-embedded prompting successfully captures clinically relevant text
- **Low confidence**: Long-term operational cost savings versus manual development have not been quantified

## Next Checks

1. **Test prompt engineering sensitivity**: Systematically ablate key prompt components (few-shot examples, CoT structure, verification step) to identify which elements drive performance and determine minimum viable prompt configuration

2. **Cross-domain generalization**: Apply the same methodology to a different clinical NLP task (e.g., medication extraction, adverse event detection) to validate that guideline-embedded prompting works beyond SSI detection

3. **Human time measurement study**: Conduct a controlled trial measuring actual time saved by clinical informaticists when LLM-suggested keywords versus manual keyword discovery for rule construction, including documentation of remaining human judgment requirements