---
ver: rpa2
title: 'From Decision Trees to Boolean Logic: A Fast and Unified SHAP Algorithm'
arxiv_id: '2511.09376'
source_url: https://arxiv.org/abs/2511.09376
tags:
- values
- shapley
- shap
- banzhaf
- formula
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WOODELF introduces a unified, GPU-friendly SHAP algorithm that
  integrates decision trees, game theory, and Boolean logic. It constructs pseudo-Boolean
  formulas capturing feature values, tree structures, and background datasets, enabling
  linear-time computation of Background SHAP and Path-Dependent SHAP.
---

# From Decision Trees to Boolean Logic: A Fast and Unified SHAP Algorithm

## Quick Facts
- arXiv ID: 2511.09376
- Source URL: https://arxiv.org/abs/2511.09376
- Authors: Alexander Nadel; Ron Wettenstein
- Reference count: 40
- WOODELF achieves 24×-333× GPU speedups and 16×-31× CPU speedups for SHAP computation on large datasets

## Executive Summary
WOODELF introduces a unified, GPU-friendly SHAP algorithm that integrates decision trees, game theory, and Boolean logic. It constructs pseudo-Boolean formulas capturing feature values, tree structures, and background datasets, enabling linear-time computation of Background SHAP and Path-Dependent SHAP. The method also supports Shapley interaction values, Banzhaf values, and Banzhaf interaction values. WOODELF is implemented in pure Python using NumPy and CuPy, avoiding custom C++/CUDA code.

## Method Summary
The algorithm works by transforming decision trees into Weighted Disjunctive Normal Form (WDNF) Boolean formulas, then mapping these to pseudo-Boolean functions. The pseudo-Boolean framework captures tree decision patterns, feature values, and background dataset distributions. WOODELF leverages fast pseudo-Boolean solvers and sparse matrix operations to compute exact Shapley values in linear time relative to the number of leaves. The unified framework supports various interpretability metrics including standard SHAP, Shapley interactions, Banzhaf values, and custom characteristic functions.

## Key Results
- Computed Background Shapley values in 16 seconds on GPU and 162 seconds on CPU for a dataset with 3 million rows, 5 million background samples, and 127 features
- Achieved 16× and 165× faster computation compared to the best alternative method
- Demonstrated 24×-333× GPU speedups and 16×-31× CPU speedups across various tasks
- Supported diverse interpretability metrics within a unified framework

## Why This Works (Mechanism)
WOODELF's core innovation is the integration of Boolean logic with decision tree SHAP computation. By converting tree structures into WDNF formulas and then to pseudo-Boolean functions, the algorithm exploits the mathematical properties of Boolean satisfiability and linear optimization. The pseudo-Boolean representation naturally captures the decision patterns, feature interactions, and background data distributions that determine SHAP values. This unified representation enables efficient computation of multiple interpretability metrics using the same underlying framework.

## Foundational Learning
- **Pseudo-Boolean Functions**: Functions mapping Boolean vectors to integers, needed to represent tree decision patterns mathematically. Quick check: Verify understanding of how AND/OR operations translate to pseudo-Boolean terms.
- **WDNF (Weighted Disjunctive Normal Form)**: Boolean formula representation where each clause has an associated weight, needed to capture tree leaf values and decision paths. Quick check: Confirm understanding of how tree leaves map to WDNF clauses.
- **Shapley Values**: Game theory concept measuring feature contribution to model predictions, needed as the target interpretability metric. Quick check: Review the Shapley value formula and its interpretation.
- **Background Data Distribution**: The distribution of feature values used for expectation calculations, needed to compute conditional expectations in SHAP. Quick check: Understand how background samples affect SHAP value computation.
- **Characteristic Functions**: Mathematical functions defining coalition values in cooperative games, needed to generalize SHAP computation. Quick check: Verify understanding of how characteristic functions map to different interpretability metrics.
- **Sparse Matrix Operations**: Efficient data structures for handling large matrices with few non-zero entries, needed for scalability. Quick check: Review sparse matrix multiplication complexity.

## Architecture Onboarding

### Component Map
Background Data -> Feature Value Mapping -> WDNF Conversion -> Pseudo-Boolean Formulation -> Sparse Solver -> SHAP Values

### Critical Path
The critical computational path involves converting decision trees to WDNF formulas, mapping to pseudo-Boolean functions, and solving the resulting linear system using sparse solvers. This path determines the overall computational complexity and performance.

### Design Tradeoffs
- Exact vs. approximate computation: WOODELF provides exact SHAP values but may face scalability limits with very deep trees
- Memory vs. speed: Sparse matrix representations reduce memory usage but add computational overhead
- Hardware dependence: GPU acceleration provides significant speedups but requires compatible hardware
- Generality vs. specialization: The unified framework supports multiple metrics but may not be optimal for specific use cases

### Failure Signatures
- Memory exhaustion with very deep trees due to exponential growth of WDNF terms
- Performance degradation on small datasets where preprocessing overhead dominates
- Hardware incompatibility preventing GPU acceleration
- Numerical instability in pseudo-Boolean solver for complex characteristic functions

### First Experiments
1. Validate WDNF conversion accuracy by comparing against known tree decision patterns
2. Benchmark sparse solver performance across different matrix sizes and densities
3. Test GPU acceleration impact on varying dataset sizes and tree complexities

## Open Questions the Paper Calls Out
### Open Question 1
- Question: At what tree depth does the O(TL3^D · D) term become the dominant bottleneck, causing PLTreeShap or SHAP to outperform WOODELF?
- Basis in paper: [explicit] "However, this step might become a bottleneck for very deep trees or small datasets, where PLTreeShap and the SHAP Python package may outperform WOODELF."
- Why unresolved: The paper states this limitation exists but provides no empirical or theoretical boundary characterization.
- What evidence would resolve it: Benchmarks varying tree depth systematically while holding dataset size constant, identifying the crossover point.

### Open Question 2
- Question: What are the peak memory requirements for WOODELF's sparse matrices (up to 3^D non-zero entries per leaf), and how do they constrain practical deployment?
- Basis in paper: [inferred] The algorithm constructs sparse matrices of size 4^D with at most 3^D entries per leaf-feature pair, but only runtime performance is reported.
- Why unresolved: Memory consumption is never discussed despite 3^D scaling being potentially prohibitive for deep trees.
- What evidence would resolve it: Memory profiling across varying tree depths and leaf counts, with comparison to competing methods.

### Open Question 3
- Question: Can the pseudo-Boolean framework be extended beyond decision tree ensembles to other model classes such as neural networks?
- Basis in paper: [explicit] The authors note that for neural networks, Shapley computation is #P-Hard and "approximation methods exist," leaving WOODELF's Boolean logic approach unexplored for non-tree models.
- Why unresolved: The WDNF representation is tightly coupled to tree structure via decision patterns.
- What evidence would resolve it: A theoretical analysis or empirical attempt to map neural network components to WDNF formulas.

## Limitations
- Performance degradation on very deep trees or small datasets where preprocessing overhead dominates
- Potential memory constraints with exponential growth of WDNF terms for deep trees
- Hardware dependency on GPU acceleration for optimal performance
- Limited validation across diverse datasets and model architectures

## Confidence
- High: Core algorithmic contribution of integrating Boolean logic with decision tree SHAP computation
- Medium: Reported speed improvements based on a single large-scale dataset
- Low: Claim of being a "unified" framework without comprehensive coverage of all SHAP variants

## Next Checks
1. Test WOODELF on diverse datasets with varying tree depths, feature types, and background sample sizes to establish robustness.
2. Compare WOODELF against multiple SHAP implementations (e.g., TreeSHAP, KernelSHAP, ShapleyFlow) across different evaluation metrics.
3. Conduct ablation studies to quantify the individual contributions of Boolean logic integration, GPU optimization, and pseudo-Boolean formula construction to overall performance gains.