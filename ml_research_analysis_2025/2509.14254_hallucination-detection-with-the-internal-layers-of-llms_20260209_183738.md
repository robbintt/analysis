---
ver: rpa2
title: Hallucination Detection with the Internal Layers of LLMs
arxiv_id: '2509.14254'
source_url: https://arxiv.org/abs/2509.14254
tags:
- training
- were
- llms
- page
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis proposes a new method for detecting hallucinations
  in LLM outputs by leveraging internal layer representations. The approach builds
  small classifiers (probes) on internal LLM layers, dynamically weighting and comparing
  their outputs using cosine similarity or direct aggregation.
---

# Hallucination Detection with the Internal Layers of LLMs

## Quick Facts
- arXiv ID: 2509.14254
- Source URL: https://arxiv.org/abs/2509.14254
- Reference count: 0
- Primary result: Probing LLM internal layers achieves F1 scores up to 0.82 on HaluEval benchmark

## Executive Summary
This thesis introduces a novel method for detecting hallucinations in LLM outputs by leveraging internal layer representations. The approach uses small MLP classifiers (probes) trained on hidden states from transformer layers, dynamically weighting and comparing their outputs using cosine similarity or direct aggregation. Experiments show the method outperforms traditional probing techniques on HaluEval benchmark, achieving F1 scores up to 0.82. However, the approach struggles with generalization across different benchmarks and LLMs, highlighting both the promise and limitations of internal representation analysis for enhancing LLM reliability.

## Method Summary
The method extracts internal representations from all layers of a frozen LLM, processes each layer through a shared 2-layer MLP feature extractor, then compares or directly aggregates these representations. For classification, either pairwise cosine similarity matrices or flattened direct aggregation are used, followed by a final linear classifier. The approach can also perform sequence labeling for span detection using BIO tagging with CRF decoding. Training uses Adam optimizer with cross-entropy loss and early stopping, evaluating primarily on F1 score across multiple hallucination detection benchmarks.

## Key Results
- Achieves F1 score of 0.82 on HaluEval benchmark with cosine similarity model
- Generalizes to other benchmarks with significant performance drops (F1 0.42-0.44)
- Identifies hallucinated spans with F1 0.35 due to severe label imbalance (4% positive tokens)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Internal LLM representations encode truth-related signals that distinguish hallucinated from factual content
- Mechanism: Small probes trained on hidden states from transformer layers learn to map internal representations to hallucination labels
- Core assumption: Hallucinatory outputs produce systematically different activation patterns than factual outputs at specific layers
- Evidence anchors: Probing-based classifiers can detect hallucinations; internal state analysis reveals hallucination patterns; VIB-Probe and others leverage similar representations
- Break condition: If LLM representations don't differentiate hallucinations, probe accuracy drops to random baseline

### Mechanism 2
- Claim: Middle-to-later layers encode more truth-relevant information than early layers
- Mechanism: The aggregation module learns weights per layer; experiments show larger weights on layers 15-30
- Core assumption: Factual knowledge retrieval and truth evaluation occur at specific transformer depth stages
- Evidence anchors: Both distributions show patterns supporting middle-to-later layers; weight plots show higher weights for these layers; no direct corpus confirmation
- Break condition: If model architecture changes, weight distributions may not transfer

### Mechanism 3
- Claim: Pretraining on one benchmark followed by weight freezing reduces generalization degradation
- Mechanism: Sequential training with frozen aggregation weights preserves learned layer importance patterns
- Core assumption: Layer importance patterns are partially benchmark-agnostic
- Evidence anchors: Pretraining + freezing shows slight F1 improvements on non-final benchmarks; neither approach consistently improved; no direct corpus evidence
- Break condition: If source and target benchmarks differ significantly, frozen weights may harm performance

## Foundational Learning

- **Probing classifiers**: Why needed here - core method trains external classifiers on frozen internal representations to predict hallucination labels without modifying the LLM. Quick check: Can you explain why probing doesn't require LLM retraining, and what feature space the classifier operates in?

- **Cosine similarity and vector comparison**: Why needed here - "cosine" model compares layer encodings via cosine similarity; understanding vector normalization and angle-based similarity is essential. Quick check: Given two vectors [3,4] and [6,8], what is their cosine similarity?

- **Sequence labeling with CRF**: Why needed here - for span detection, method uses BIO tagging with CRF decoder to enforce valid tag transitions. Quick check: Why might a CRF outperform greedy decoding for sequence labeling tasks?

## Architecture Onboarding

- **Component map**: Feature Extractor -> Comparison Module (Cosine Similarity/Identity) -> Aggregation -> Linear Classifier
- **Critical path**: Hook LLM to extract all layer outputs -> Pass each through shared feature extractor -> Apply comparison function -> Flatten and aggregate via linear layer -> Classify
- **Design tradeoffs**: "No comparison" model simpler but lower peak performance; "Cosine" model higher peak F1 but degrades with larger LLMs; flattened aggregation outperforms ensemble approach
- **Failure signatures**: Validation F1 = 0 often occurs with ReFact training; high accuracy but low F1 in sequence labeling due to label imbalance; cross-benchmark transfer shows F1 drops
- **First 3 experiments**: 1) Train "cosine" model on HaluEval, expect F1 ~0.82; 2) Train on HaluEval, test on TruthfulQA, expect F1 drop to ~0.42; 3) Train probes using only last layer vs middle layer vs all layers, compare to proposed method

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does removing early LLM layers or utilizing preselected weight distributions improve the hallucination classifier's performance?
- **Basis in paper**: Authors note evaluating whether dropping earlier layers or preselected distributions could increase performance
- **Why unresolved**: Weight plots indicated middle-to-late layers have largest impact, but utility of early layers remains untested
- **What evidence would resolve it**: Experiments comparing default model against versions where early layer weights are fixed to zero or initialized with predefined distributions

### Open Question 2
- **Question**: Can training adjustments for class imbalance improve identification of hallucinated spans in sequential labeling?
- **Basis in paper**: Authors state high label imbalance motivates enhancing training pipeline with methods like under/oversampling or class weights
- **Why unresolved**: Sequential labeling failed to precisely identify hallucination spans (max F1 0.35) likely due to 4% positive tokens
- **What evidence would resolve it**: Comparative study using models trained with loss functions weighted by inverse class frequency or data resampling techniques

### Open Question 3
- **Question**: Can exploring different neural encoders, comparison functions, or activation functions enhance the proposed method's capabilities?
- **Basis in paper**: Authors suggest future research might enhance by exploring different encoders, comparison functions, or changing hidden sizes or activation functions
- **Why unresolved**: Thesis restricted model variations to specific MLP depths and simple comparison metrics
- **What evidence would resolve it**: Ablation study replacing MLP feature extractor with transformers or varying activation functions on HaluEval benchmark

## Limitations
- Cross-benchmark generalization remains the central limitation with F1 scores dropping from 0.82 to 0.42-0.44
- Sequence labeling performance suffers from severe class imbalance (4% positive tokens yields F1 of only 0.35)
- Internal layer significance assumptions may not generalize across different LLM architectures

## Confidence
- **High Confidence**: Core mechanism of using internal LLM representations for hallucination detection works within benchmark boundaries
- **Medium Confidence**: Observation that middle-to-later layers encode more hallucination-relevant information needs broader architectural validation
- **Low Confidence**: Claims about cross-benchmark generalization and pretraining benefits are weak

## Next Checks
1. Apply the probe method to different LLM architectures (e.g., Mistral, Mixtral) with varying layer counts to verify whether middle-to-later layer importance holds across model families

2. Systematically vary prompt formatting, temperature, and generation parameters to measure how representation extraction robustness affects probe performance across multiple hallucination benchmarks

3. Implement self-training or domain adaptation techniques to improve cross-benchmark performance, measuring whether unlabeled data from target domains can bridge the generalization gap without full retraining