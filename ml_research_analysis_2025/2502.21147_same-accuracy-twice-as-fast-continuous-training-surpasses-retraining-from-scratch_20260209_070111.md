---
ver: rpa2
title: 'Same accuracy, twice as fast: continuous training surpasses retraining from
  scratch'
arxiv_id: '2502.21147'
source_url: https://arxiv.org/abs/2502.21147
tags:
- learning
- scratch
- data
- training
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the computational cost of retraining machine
  learning models when new data arrives. Instead of the standard practice of retraining
  from scratch, it proposes methods to accelerate continuous training by leveraging
  pre-trained models and stored old data.
---

# Same accuracy, twice as fast: continuous training surpasses retraining from scratch

## Quick Facts
- arXiv ID: 2502.21147
- Source URL: https://arxiv.org/abs/2502.21147
- Authors: Eli Verwimp; Guy Hacohen; Tinne Tuytelaars
- Reference count: 19
- Primary result: Achieves up to 2.7x speedups in convergence across various computer vision tasks while maintaining or exceeding the performance of training from scratch

## Executive Summary
This paper addresses the computational inefficiency of retraining machine learning models when new data arrives by proposing methods to accelerate continuous training. The authors identify four key optimization aspects—initialization, regularization, data selection, and hyper-parameters—and introduce methods for each to reduce computational costs. By combining these methods, they achieve up to 2.7x speedups in convergence across various computer vision tasks while maintaining or exceeding the performance of training from scratch.

## Method Summary
The approach decomposes the stochastic gradient descent update rule into four components that can be optimized for continuous training: initialization, objective function, sampling strategy, and learning rate scheduling. The method introduces shrink-and-perturb initialization to restore plasticity, L2-init regularization anchored to initial random weights, easy/hard sampling that filters out uninformative examples, and aggressive learning rate scheduling that leverages the pre-trained model's proximity to a good solution.

## Key Results
- Shrink-and-perturb initialization with α=0.4 and β=0.001 achieves ~1.5x speedup over naive continuous training
- Combining all four methods yields 2.2x speedup in CIFAR-100 (70+30) and 2.84x speedup with ×0.5 scheduler
- Methods generalize to class-incremental, multi-task, and domain adaptation scenarios with consistent speedups (1.55x-2.16x)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Warm-starting from a pre-trained model can be slower and less accurate than training from scratch due to loss of plasticity, but this can be remediated.
- **Mechanism:** Shrink-and-perturb initialization restores plasticity by combining shrunken old weights with small random noise: θ_init = αθ_old + βθ_random (paper uses α=0.4, β=0.001). Shrinking reduces overconfidence in old solutions; perturbation reintroduces exploratory capacity.
- **Core assumption:** Trained networks develop rigidity that limits their ability to learn new patterns efficiently (plasticity loss).
- **Evidence anchors:**
  - [section 3.1] "The naive benchmark is both worse and converges slower than retraining from scratch. Re-introducing plasticity with shrink-and-perturb improves both speed and accuracy, surpassing scratch training."
  - [figure 2] Shows shrink-and-perturb achieving ~1.5x speedup over naive continuous training while exceeding scratch accuracy.
  - [corpus] Weak direct support; neighbor papers discuss continuous learning costs but not shrink-and-perturb specifically.
- **Break condition:** If α is too high (e.g., 0.9), plasticity remains insufficient; if too low, too much prior knowledge is discarded. Appendix A.3 shows α=0.1-0.5 range works.

### Mechanism 2
- **Claim:** Regularizing toward initial random weights (L2-init) accelerates continuous training convergence more than standard L2 regularization.
- **Mechanism:** Modify objective to L_reg = L + λ||θ - θ_0||² where θ_0 is the random initialization (not zero). This preserves network plasticity by preventing weights from drifting too far from their initialized state.
- **Core assumption:** Standard L2 regularization (toward origin) is less effective for maintaining learning capacity than anchoring to initial random weights.
- **Evidence anchors:**
  - [section 3.2] "adding this regularization term accelerates the convergence of the continuously trained models, more so than it improves models trained from scratch"
  - [table 1] L2-init alone achieves L99=1.94x speedup in continuous vs 1.69x in scratch.
  - [corpus] No direct corpus support for L2-init specifically.
- **Break condition:** If λ is too high (>0.1), learning slows excessively; if too low (<0.001), effect is negligible. Paper uses λ=0.01.

### Mechanism 3
- **Claim:** Filtering out the easiest and hardest examples from old data during sampling accelerates convergence.
- **Mechanism:** Track when each example is first classified correctly during training (learning speed). Reduce sampling probability of the 10% easiest (learned immediately, no remaining gradient signal) and 10% hardest (never learned, uninformative) to 10% of normal.
- **Core assumption:** Not all training examples contribute equally to gradient-based learning; extremes are less useful than middle-difficulty examples.
- **Evidence anchors:**
  - [section 3.3] "In the easy examples, there is no information left, and the hardest ones are never learned, and thus [not] as useful"
  - [figure 4] Easy/hard sampling achieves ~1.6x speedup over naive; old/new balancing shows no improvement.
  - [corpus] Corpus neighbor ECCO discusses efficient continuous learning but via cross-camera correlation, not example filtering.
- **Break condition:** If the threshold for "easy/hard" is mis-specified, or if new data distribution differs substantially (making old difficulty estimates unreliable), benefit diminishes.

### Mechanism 4
- **Claim:** Continuous models can use more aggressive learning rate schedules because they reach loss plateaus faster.
- **Mechanism:** Shorten the learning rate schedule (e.g., reach minimum LR in 25-50% of normal iterations). Continuous models have already learned old data features, so fine-tuning converges faster.
- **Core assumption:** The pre-trained model is closer to a good solution on the combined dataset than random initialization.
- **Evidence anchors:**
  - [section 3.4] "the loss curve for the continuous model reaches a plateau much faster compared to scratch training"
  - [figure 5, table 2b] ×0.5 scheduler with full method achieves 2.84x speedup; ×0.25 achieves 4.74x but may miss L100 accuracy.
  - [corpus] Corpus mentions computational efficiency in continuous learning (ECCO, Control LLM) but not LR scheduling specifically.
- **Break condition:** Overly aggressive scheduling (×0.25) alone fails to reach full accuracy; must combine with other methods. Breaks if old and new data are highly dissimilar.

## Foundational Learning

- **Concept: Stochastic Gradient Descent (SGD) and its components**
  - Why needed here: The paper's entire approach decomposes the SGD update rule (initialization, objective, sampling, hyperparameters) to optimize each component for continuous training.
  - Quick check question: Can you explain how batch composition affects gradient estimation variance?

- **Concept: Learning rate scheduling (cosine annealing, multistep)**
  - Why needed here: Aggressive scheduling is one of the four key optimization aspects; understanding how LR decay affects convergence is essential.
  - Quick check question: Why does cosine annealing typically outperform fixed learning rates for vision tasks?

- **Concept: Plasticity loss in neural networks**
  - Why needed here: The core problem being solved; trained networks lose the ability to learn new information effectively.
  - Quick check question: What observable symptom would indicate plasticity loss during continuous training?

## Architecture Onboarding

- **Component map:**
```
Continuous Training Pipeline
├── Initialization Module
│   └── Shrink-and-perturb (α=0.4, β=0.001)
├── Objective Function
│   └── L2-init regularizer (λ=0.01, anchor to θ_random)
├── Data Sampler
│   ├── Track learning speed per example during old training
│   └── Reduce sampling of 10% easiest + 10% hardest (p=0.1×)
└── Hyperparameter Controller
    └── Aggressive LR scheduler (×0.5 or ×0.25 of baseline iterations)
```

- **Critical path:**
  1. During initial training on old data: Record learning speed (epoch of first correct classification) for each example.
  2. When new data arrives: Apply shrink-and-perturb to old model weights.
  3. Enable L2-init regularization anchored to the original random initialization.
  4. Construct batches using easy/hard filtering on old data + proportional new data.
  5. Train with shortened cosine LR schedule (×0.5 recommended for balanced speed/accuracy).

- **Design tradeoffs:**
  - **Speed vs final accuracy:** ×0.25 scheduler gives ~4.7x speedup but may fail to reach 100% of scratch accuracy; ×0.5 is safer (2.8x with reliable L100).
  - **Memory vs compute:** Paper assumes unlimited old data storage; future work needed for memory-constrained settings.
  - **Individual vs combined methods:** Methods are partially complementary (table 1 shows combining 3+ methods yields ~2.2x, not 4x).

- **Failure signatures:**
  - Naive continuous training slower/worse than scratch → plasticity loss confirmed
  - Aggressive scheduler alone fails to reach target accuracy → must combine with initialization/regularization
  - Old/new balancing shows no gain → gradient norms equalize after ~100 iterations (appendix A.7)

- **First 3 experiments:**
  1. **Baseline comparison:** Train ResNet-18 on CIFAR-100 (70+30) with scratch vs naive continuous vs shrink-and-perturb. Measure iterations to reach 99%/100% of scratch accuracy.
  2. **Ablation sweep:** Test each component (initialization, L2-init, easy/hard sampling, LR schedule) individually, then in pairs. Verify complementary effects per table 1.
  3. **Generalization test:** Apply full method to ImageNet-100 (80+20) and Adaptiope domain adaptation. Confirm speedups generalize beyond CIFAR (table 2a shows 2.16x for ImageNet-100, 1.55x for Adaptiope).

## Open Questions the Paper Calls Out
None

## Limitations
- The proposed methods assume unlimited storage of old training data, which may be impractical in many real-world scenarios
- Performance gains appear to be highly dependent on the initial model being trained on a sufficiently large dataset
- No analysis of how these methods scale to larger architectures like transformers or vision transformers

## Confidence
- **High confidence**: Shrink-and-perturb initialization effectiveness, L2-init regularization benefits, and the fundamental observation that continuous training from scratch-naive is slower/worse than retraining
- **Medium confidence**: The complementary nature of combining all four methods (though ablation results support this), and the generalization across different task types (class-incremental, multi-task, domain adaptation)
- **Low confidence**: The optimal parameter settings (α=0.4, β=0.001, λ=0.01, 10% thresholds) will generalize well to all datasets and architectures

## Next Checks
1. Test the method on a memory-constrained setting where only a subset of old data can be stored, evaluating the trade-off between storage reduction and speed/accuracy loss
2. Apply the four optimization components to a transformer-based architecture on a natural language processing task to assess cross-domain applicability
3. Conduct a systematic study varying the shrink factor α and L2-init regularization strength λ across a grid to map the full performance landscape