---
ver: rpa2
title: Source framing triggers systematic evaluation bias in Large Language Models
arxiv_id: '2505.13488'
source_url: https://arxiv.org/abs/2505.13488
tags:
- person
- grok2a
- mistrala
- deepseekra
- o3-minia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines how source framing influences
  the evaluation of narrative statements by Large Language Models (LLMs). Four state-of-the-art
  LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and Mistral) were tasked with
  evaluating 4,800 narrative statements across 24 topics under 10 different attribution
  conditions, totaling 192,000 assessments.
---

# Source framing triggers systematic evaluation bias in Large Language Models

## Quick Facts
- arXiv ID: 2505.13488
- Source URL: https://arxiv.org/abs/2505.13488
- Reference count: 0
- This study finds that attributing statements to Chinese individuals consistently lowers agreement scores across all tested LLMs, with the most pronounced bias observed in Deepseek Reasoner.

## Executive Summary
This study systematically examines how source framing influences the evaluation of narrative statements by Large Language Models (LLMs). Four state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and Mistral) were tasked with evaluating 4,800 narrative statements across 24 topics under 10 different attribution conditions, totaling 192,000 assessments. The research found that while LLMs generally exhibit high agreement when evaluating content without source information, this alignment breaks down when source attribution is introduced. Specifically, attributing statements to Chinese individuals consistently lowered agreement scores across all models, with the effect being most pronounced for Deepseek Reasoner. This bias was particularly strong for topics related to international geopolitics, such as Taiwan's sovereignty. The findings reveal that source attribution significantly impacts LLM evaluations, challenging assumptions about their neutrality and highlighting the need for greater transparency and governance in AI-mediated information systems.

## Method Summary
The study employed a two-phase design to evaluate source framing effects on LLM evaluations. In Phase 1, four LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and Mistral) generated 50 narrative statements per topic (24 total topics) using a prompt asking for "the right position to hold" on each topic. This produced 4,800 narratives. In Phase 2, each model evaluated all 4,800 statements under 10 attribution conditions: blind, generic person, person from France/China/USA, generic LLM, and each specific model with correct/incorrect attribution. Models output JSON-formatted ratings (0-1) and explanations. The primary metric was bias Δ = mean agreement(condition) - mean agreement(blind), analyzed using Kruskal-Wallis with Dunn's correction and Mann-Whitney U tests.

## Key Results
- LLMs exhibited high agreement (95%+) when evaluating content without source information, but this alignment broke down when source attribution was introduced
- Attributing statements to Chinese individuals consistently lowered agreement scores across all models, with Deepseek Reasoner showing the most pronounced bias
- The bias was particularly strong for geopolitical topics, with Taiwan sovereignty statements dropping from 0.85 agreement (neutral) to 0.00 when attributed to a Chinese person by Deepseek Reasoner

## Why This Works (Mechanism)

### Mechanism 1: Source Attribution Triggers Identity-Based Expectation Alignment
- Claim: When LLMs are informed of a source's identity (nationality or model origin), they shift evaluation criteria from content coherence to expectation alignment with that identity's presumed stance.
- Mechanism: Models retrieve learned associations between identity labels and expected viewpoints, then evaluate content against those expectations rather than purely on argumentative quality.
- Core assumption: The bias stems from statistical patterns in training data that correlate nationalities with specific geopolitical positions.
- Evidence anchors:
  - [abstract]: "Attributing statements to individuals from China consistently lowered agreement scores across all models, with the most pronounced bias observed in Deepseek Reasoner."
  - [section]: Figure 4 shows Deepseek Reasoner assigning 0.85 agreement to a Taiwan sovereignty statement under neutral attribution, but 0.00 when told the author was "a person from China," with reasoning explicitly invoking the One-China Principle.
  - [corpus]: Weak or missing—related papers focus on framing bias in LLM outputs but not specifically on source attribution effects.

### Mechanism 2: Training Data Associations Create Implicit Nationality-Stance Mappings
- Claim: LLMs encode implicit expectations about what individuals from specific countries should believe, based on patterns in training corpora.
- Mechanism: During pre-training, models absorb statistical regularities linking nationalities to typical viewpoints (e.g., Chinese sources with pro-One-China positions), which surface when attribution cues are present.
- Core assumption: These associations are not deliberately engineered but emerge from the distribution of viewpoints in training data.
- Evidence anchors:
  - [abstract]: "LLMs tend to evaluate LLM-authored statements less favorably than those attributed to humans."
  - [section]: Figure S27 shows Deepseek Reasoner giving 0.95 agreement to a media freedom statement under neutral attribution, but 0.20 when attributed to a Chinese person, with justification referencing "conflicts with China's state-controlled media system."
  - [corpus]: Moderate—related work (Framing Political Bias in Multilingual LLMs) confirms ideological framing varies by language/cultural context, supporting nationality-stance associations.

### Mechanism 3: Content-Irrelevant Meta-Information Modifies Evaluation Criteria
- Claim: Even when source information is factually irrelevant to evaluation quality, it systematically shifts how models weigh different aspects of content.
- Mechanism: Source cues activate context-dependent evaluation schemas—for human sources, models prioritize plausibility; for LLM sources, they apply stricter coherence standards.
- Core assumption: Models lack robust metacognitive guardrails to isolate content quality from contextual framing.
- Evidence anchors:
  - [abstract]: "Models tended to evaluate LLM-authored statements less favorably than those attributed to humans."
  - [section]: Figure 5 shows Mistral, Deepseek Reasoner, and Grok 2 all assign lower agreement to identical content when told it came from an LLM versus a human.
  - [corpus]: Weak—related papers discuss framing bias in outputs but don't examine human-vs-LLM source attribution effects.

## Foundational Learning

- Concept: **Framing Bias in NLP**
  - Why needed here: The paper's core phenomenon—identical content receiving different evaluations based on contextual framing—is a specific instance of this broader bias class.
  - Quick check question: If you prompt an LLM to "evaluate this argument fairly" vs. "evaluate this argument as a skeptical critic," would you expect different outputs? Why?

- Concept: **Attribution Effects in LLM Evaluation**
  - Why needed here: Understanding that meta-information about source identity alters evaluation is critical for interpreting the paper's blind-vs-attributed comparison methodology.
  - Quick check question: When you see a news headline attributed to "a government official" vs. "an anonymous source," does your evaluation of credibility change? How might LLMs encode similar patterns?

- Concept: **Nationality-Based Bias Patterns in Training Data**
  - Why needed here: The systematic anti-Chinese bias across all models—including Deepseek Reasoner—suggests training data correlations, not deliberate design, underlie the effect.
  - Quick check question: If an LLM was trained primarily on Chinese internet content, would you expect different nationality-based evaluation patterns? What assumptions does this require?

## Architecture Onboarding

- Component map:
  - Narrative generation pipeline: 4 LLMs × 24 topics × 50 statements = 4,800 narratives
  - Attribution injection layer: Blind, person, person(France/China/USA), LLM, specific model attributions
  - Evaluation interface: JSON-formatted rating (0-1) + explanation output
  - Bias computation module: Δ = mean(attributed condition) - mean(blind condition)

- Critical path:
  1. Generate narratives without attribution context
  2. Inject attribution cues at evaluation time (not generation time)
  3. Compare intra-model agreement across attribution conditions
  4. Isolate bias by holding content constant while manipulating source framing

- Design tradeoffs:
  - **Static vs. dynamic attribution**: Paper uses static pre-assigned attributions; dynamic attribution (generating narratives under identity constraints) would test different mechanisms
  - **Quantitative vs. qualitative analysis**: 192,000 ratings provide statistical power, but reasoning token analysis (e.g., Deepseek's internal conflict) offers mechanistic insight
  - **Model access method**: API vs. chat interface behaviors may differ; paper studied API access, which exposes "rawer" model tendencies

- Failure signatures:
  - **Ceiling effects**: Agreement scores clustered near 95%+ in blind conditions reduce sensitivity to detect subtle biases
  - **Topic-dependent bias variability**: Anti-Chinese bias strongest for geopolitics (Taiwan: -24.43% for Deepseek), weaker for environment (~0%)
  - **Asymmetric bias patterns**: Anti-Chinese bias is consistent across models; anti-American bias appears only for specific topics (healthcare: Grok 2 drops from 95% to 0% when author is American)

- First 3 experiments:
  1. **Replication with different nationality sets**: Test whether anti-Chinese bias generalizes to other nationalities (e.g., Russian, Iranian) to determine if bias is China-specific or targets geopolitical rivals broadly.
  2. **Attribution strength manipulation**: Vary how explicitly nationality is stated (e.g., "a person" vs. "a Chinese citizen who supports government policy") to test whether bias magnitude scales with attribution specificity.
  3. **Cross-model transfer test**: If Model A generates a narrative that Model B evaluates under Chinese attribution, does bias depend on which model generated the content? This would separate source generation bias from evaluation bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can source framing biases in LLM evaluation be effectively mitigated through model design, reinforcement learning from human feedback (RLHF), or prompt engineering?
- Basis in paper: [explicit] The authors explicitly state that future work should probe "how they can be mitigated through model design, reinforcement learning from human feedback, prompt engineering, or system-level governance."
- Why unresolved: The study successfully identifies the existence and magnitude of the bias but does not test specific interventions or counter-measures.
- What evidence would resolve it: Experiments testing model variants fine-tuned with specific anti-framing objectives or specific prompt strategies that successfully neutralize source attribution effects.

### Open Question 2
- Question: Does the observed "procedural" bias—where models evaluate content based on expected identity alignment—originate from training data associations or post-training alignment mechanisms?
- Basis in paper: [inferred] The authors hypothesize the bias reflects a "procedural shortcut" or "latent assumptions," but also suggest it could result from post-training alignment mechanisms that reward "political sensitivity" or mirroring official narratives.
- Why unresolved: The root cause is indistinguishable without transparency into the proprietary training and alignment datasets of the tested models (o3-mini, Deepseek, etc.).
- What evidence would resolve it: Ablation studies on open-source models with controlled training and alignment steps, or detailed documentation of alignment criteria from providers.

### Open Question 3
- Question: Do the observed anti-Chinese attribution biases persist across different languages or non-Western cultural contexts?
- Basis in paper: [inferred] The study focuses on 24 sensitive topics using English prompts, noting the bias was particularly pronounced for Chinese sources; it is unclear if this is an artifact of English-centric training data.
- Why unresolved: The methodology was limited to English interaction, leaving cross-lingual and cross-cultural validity untested.
- What evidence would resolve it: Replication of the evaluation design using translated prompts and narratives in Chinese, French, and other languages to assess consistency.

## Limitations
- The study's findings hinge on regenerated content that may differ from the original generation, introducing potential variability in bias patterns
- The exact API parameters (temperature, system prompts) are unspecified, which could affect both narrative generation quality and evaluation consistency
- The focus on 24 topics within 8 clusters may not capture the full spectrum of contexts where source attribution bias manifests

## Confidence
- High confidence in the core finding that source attribution systematically influences LLM evaluations, supported by consistent patterns across 192,000 assessments and multiple statistical tests
- Medium confidence in the mechanism attribution to training data nationality-stance associations, as this requires inference beyond the experimental data
- Medium confidence in the generalizability of anti-Chinese bias patterns, given that the effect is remarkably consistent across models with different training origins
- Low confidence in quantifying the exact magnitude of bias without access to the original narrative texts

## Next Checks
1. Test whether the anti-Chinese bias generalizes to other nationalities (Russian, Iranian, etc.) to determine if bias targets geopolitical rivals broadly or is China-specific
2. Vary attribution specificity (e.g., "a person" vs. "a Chinese citizen who supports government policy") to test whether bias magnitude scales with attribution detail
3. Conduct cross-model transfer tests where one model's generated content is evaluated by another model under Chinese attribution to separate generation bias from evaluation bias