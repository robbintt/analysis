---
ver: rpa2
title: 'PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series
  Imputation'
arxiv_id: '2511.13414'
source_url: https://arxiv.org/abs/2511.13414
tags:
- missing
- imputation
- patterns
- data
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAST, a Primary-Auxiliary Spatio-Temporal
  Network for traffic time series imputation, addressing the challenge of diverse
  missing data types (random, fiber, and block) in intelligent transportation systems.
  PAST disentangles spatial and temporal patterns into primary patterns (derived from
  internal data relationships) and auxiliary patterns (inferred from external features
  like timestamps and node attributes).
---

# PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation

## Quick Facts
- arXiv ID: 2511.13414
- Source URL: https://arxiv.org/abs/2511.13414
- Reference count: 38
- Introduces PAST, a dual-module network achieving up to 26.2% RMSE and 31.6% MAE improvement over baselines on traffic imputation across random, fiber, and block missing scenarios

## Executive Summary
This paper addresses the challenge of traffic time series imputation by introducing PAST, a Primary-Auxiliary Spatio-Temporal Network that disentangles spatial and temporal patterns into primary (internal data relationships) and auxiliary (external features) categories. PAST employs a dual-module architecture: a Graph-Integrated Module (GIM) that captures primary patterns using dynamic directed graphs with interval-aware dropout, and a Cross-Gated Module (CGM) that extracts auxiliary patterns through bidirectional gating on embedded external features. Experiments on three datasets (METR-LA, PeMS-Bay, and LargeST-SD) under 27 missing data conditions demonstrate state-of-the-art performance, with improvements up to 26.2% in RMSE and 31.6% in MAE over seven baselines.

## Method Summary
PAST uses a dual-module architecture where GIM learns primary patterns from observed data values and their internal dependencies through dynamic directed graphs with interval-aware dropout, while CGM extracts auxiliary patterns from timestamps and node attributes using cross-gated layers. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework with two loss functions: MSE on observed values for GIM and MSE on GIM residuals for CGM. The model processes incomplete time series through an embedding layer, then parallel GIM and CGM layers that refine representations through multiple stacked layers before final fusion and output.

## Key Results
- Achieves up to 26.2% RMSE and 31.6% MAE improvement over seven state-of-the-art baselines
- Demonstrates superior performance across all three missing scenarios: random (r=0.2-0.6), fiber (l=24-96), and block (l=48-96, s=5-10)
- Shows stable performance in both offline and online settings across METR-LA, PeMS-Bay, and LargeST-SD datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling patterns into primary (internal data relationships) and auxiliary (external features) categories improves imputation across diverse missing scenarios.
- Mechanism: PAST uses a dual-module architecture where GIM learns primary patterns from observed data values and their internal dependencies, while CGM extracts auxiliary patterns from timestamps and node attributes. The modules interact via shared hidden vectors and are trained with an ensemble loss (MSE on observed values for GIM; MSE on GIM residuals for CGM).
- Core assumption: Auxiliary patterns driven by external features (e.g., daily schedules) provide stable long-term and large-scale dependencies that complement short-term, local primary patterns, especially when internal data is extensively missing.
- Evidence anchors:
  - [abstract] "patterns are categorized into two types... primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes."
  - [section 4.1] "primary patterns... can perceive local temporal fluctuations and adjacent topological dependencies... auxiliary patterns, driven by external factors... enable effective capture of long-range and large-scale dependencies without relying on excessively long sequences."
  - [corpus] Weak direct evidence. Related work in corpus (e.g., CoSTI, STAMImputer) also focuses on spatio-temporal imputation but does not explicitly validate the primary-auxiliary dichotomy as a mechanism.
- Break condition: If external features are unavailable or non-informative (e.g., irregular events overriding periodic schedules), CGM's contribution may diminish, reducing advantages in fiber/block missing.

### Mechanism 2
- Claim: Modeling incomplete time series as dynamic directed graphs with interval-aware dropout enables robust learning of primary patterns despite varying missing positions.
- Mechanism: GIM constructs a temporal graph where vertices are data point embeddings. Edges connect observed values bidirectionally (mutual influence) and from observed to missing values unidirectionally (information propagation for imputation). Interval-aware dropout preferentially drops edges from proximal observed points (probability \( p(\Delta t) = e^{-\alpha \Delta t + \beta} \)) to force learning broader patterns and reduce local overfitting.
- Core assumption: Strong dependencies on adjacent points in random missing are unstable for fiber/block missing; forcing reliance on broader context via dropout improves generalization.
- Evidence anchors:
  - [section 4.2.1] "the GNN architecture... can dynamically adapt to the change of missing patterns by modifying the status of nodes and edges."
  - [section 5.5.1] Ablation shows interval-aware dropout improves RMSE by up to 12.3% in fiber/block missing vs. standard dropout.
  - [corpus] Not directly validated. Related corpus papers (e.g., FADTI, AdaSTI) use attention or diffusion mechanisms, not graph-based dropout for imputation.
- Break condition: If missing positions are not random but systematically biased (e.g., always missing peak hours), the dropout may not address the underlying distribution shift.

### Mechanism 3
- Claim: Cross-gated bidirectional fusion of embedded spatial and temporal external features effectively captures auxiliary patterns for long-term/large-scale dependencies.
- Mechanism: CGM uses cross-gated layers with separate projection and gating sub-layers for spatial (node ID) and temporal (timestamp) embeddings. The gated vectors filter projection vectors and interact cross-domain via tanh and element-wise multiplication (e.g., temporal gate modulates spatial projection). This allows non-linear, bidirectional spatio-temporal interaction.
- Core assumption: Spatial and temporal external features interact non-linearly (e.g., rush hour patterns differ by location), and gating enables dynamic feature selection and fusion.
- Evidence anchors:
  - [section 4.3] "the cross-gated layer enhances spatio-temporal representation capacity while maintaining efficiency... concretizes dependency learning through self- and cross-representations."
  - [table 7] Replacing cross-gated layers with linear layers (CGL-LL) increases RMSE by 9.6-15.8%, demonstrating gating's contribution.
  - [corpus] Weak. Gating mechanisms are common in NLP/vision (GLU), but their specific application to spatio-temporal external features for imputation is not validated in corpus.
- Break condition: If external features are high-dimensional or noisy without clear periodicity (e.g., irregular events), gating may not isolate informative signals, leading to noise amplification.

## Foundational Learning

- **Graph Neural Networks (GNNs) for Time Series**
  - Why needed here: GIM relies entirely on GNNs (graph convolutions on dynamic temporal and static spatial graphs) to propagate information from observed to missing values. Understanding message passing, adjacency matrix construction, and multi-order convolutions is essential.
  - Quick check question: Can you explain how a GNN updates node representations using neighbor information, and why a static spatial graph might fail for fiber missing?

- **Time Series Imputation Paradigms**
  - Why needed here: The paper contrasts with RNN-based (BRITS, GRIN), matrix factorization (TIDER), and attention-based (STCPA) methods. Knowing assumptions of each (e.g., RNNs assume sequential order; matrix factorization assumes low-rank structure) clarifies PAST's novelty.
  - Quick check question: Why might an RNN-based model struggle with block missing compared to a GNN-based model?

- **External Feature Integration in Deep Learning**
  - Why needed here: CGM embeds timestamps and node IDs, treating them like tokens. Understanding embedding layers, positional encodings (e.g., from Transformers), and feature fusion methods (e.g., gating, concatenation) is crucial.
  - Quick check question: How does embedding a timestamp (e.g., "Monday 8:00 AM") differ from using raw numerical values, and what are the trade-offs?

## Architecture Onboarding

- **Component map:**
  - Input: Incomplete time series $\mathbf{X}$ (shape: $L \times N$), mask matrix $\mathbf{M}$, external features (node IDs, timestamps)
  - Embedding Layer: Maps data points, node IDs, and timestamp components to $d$-dimensional vectors
  - Graph-Integrated Module (GIM): Stacked graph-integrated layers (each with temporal and spatial sub-layers). Processes primary patterns
  - Cross-Gated Module (CGM): Stacked cross-gated layers. Processes auxiliary patterns
  - Fusion & Output: Linear layers on GIM and CGM outputs produce $\mathbf{Y}_{GIM}$ and $\mathbf{Y}_{CGM}$; final imputation $\mathbf{Y} = \mathbf{M} \odot \mathbf{X} + (1 - \mathbf{M}) \odot (\mathbf{Y}_{GIM} + \mathbf{Y}_{CGM})$
  - Training: Two-stage ensemble loss (Eq. 3)

- **Critical path:**
  1. Data and mask → Embedding layer → initial hidden states
  2. **GIM & CGM layers run in parallel per layer:**  
     - CGM layer processes external embeddings → hidden vector → passed to corresponding GIM layer  
     - GIM layer processes data embeddings with hidden vector injection → output to next layer
  3. After $n$ layers, linear projections produce module outputs
  4. Fusion combines outputs; loss computed on observed values

- **Design tradeoffs:**
  - **GIM (GNN) vs. RNN for temporal modeling:** GIM handles missing positions dynamically via graph structure but has $O(T^2)$ complexity vs. RNN's $O(T)$. Chosen for robustness to missing patterns
  - **Dual-module vs. single unified module:** Increases parameters and complexity but enables specialized pattern extraction and residual learning (CGM fits GIM's errors)
  - **Interval-aware dropout parameter $\alpha$:** Higher $\alpha$ forces broader context learning (better for fiber/block missing) but may degrade random missing performance (Table 8). Default $\alpha=0.1$ balances

- **Failure signatures:**
  - **Overfitting to training missing patterns:** Large offline/online performance gap (Tables 4-6) indicates poor generalization; mitigated by ensemble training and dropout
  - **CGM dominates GIM:** If external features are highly predictive (e.g., strong periodicity), GIM may under-contribute; monitor loss contributions
  - **Graph construction issues:** If road network adjacency $\mathbf{A}_S$ is sparse or inaccurate, spatial layer performance drops; verify graph connectivity

- **First 3 experiments:**
  1. **Validate basic functionality:** Train PAST on METR-LA with random missing (r=0.2) and standard hyperparameters (Table 3). Compare offline/online RMSE/MAE vs. GRIN baseline to ensure implementation correctness
  2. **Ablate key components:** Remove CGM (only GIM) and evaluate on fiber missing (l=48) to quantify auxiliary pattern contribution (expect ~31.8% RMSE increase per Table 7). Then replace cross-gated layers with linear layers to test gating mechanism
  3. **Test sensitivity to missing type:** Train separate models on random, fiber, and block missing (fixed r=0.4) on PeMS-Bay. Compare performance to assess if single model (PAST) generalizes better than specialized baselines (e.g., GRIN for random, STCPA for fiber). Monitor training stability and loss curves for signs of overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal external data, such as weather conditions, public events, or holiday schedules, be effectively fused into the Cross-Gated Module (CGM) to enhance auxiliary pattern extraction?
- Basis in paper: [explicit] The conclusion states future research will focus on "fusion strategies of weather, events, holidays, or map semantics," as the current implementation is limited to timestamps and node attributes.
- Why unresolved: The current CGM architecture is designed and validated only on simple temporal embeddings and node identities; the integration strategy for heterogeneous, high-dimensional multimodal data remains undefined.
- What evidence would resolve it: Experiments demonstrating PAST’s performance on datasets enriched with weather and event metadata, specifically showing improved RMSE/MAE during irregular traffic disruptions (e.g., concerts or storms).

### Open Question 2
- Question: Can the PAST architecture be adapted for real-time streaming imputation and concept drift detection in dynamic traffic contexts?
- Basis in paper: [explicit] The authors identify "Real-time and adaptive imputation" as a key future direction, suggesting the need for "streaming updates or concept drift detection."
- Why unresolved: The current model utilizes a static ensemble self-supervised framework trained on fixed windows (length $L=96$), which may not natively support continuous, low-latency inference or adapt to shifting traffic distributions without retraining.
- What evidence would resolve it: A modified streaming variant of PAST that maintains accuracy stability (without catastrophic forgetting) while processing incoming data points incrementally rather than in fixed batches.

### Open Question 3
- Question: Does the reliance on a static, pre-defined spatial topology ($A_S$) limit the model's ability to infer dependencies when neighboring nodes suffer from correlated "block missing" patterns?
- Basis in paper: [inferred] Section 4.2.2 states that the spatial adjacency matrix $A_S$ is "not learnable" and derived solely from fixed distances. This assumption may struggle in block missing scenarios where spatially adjacent nodes (likely sharing the same environmental context) are simultaneously offline.
- Why unresolved: While GIM uses dynamic graphs for temporal dependencies, the spatial propagation weights are constrained by the static topology, potentially isolating a missing node if its physical neighbors are also missing.
- What evidence would resolve it: A comparative study where the static spatial layer is replaced by a dynamic/learnable adjacency mechanism, specifically evaluating performance on simulated large-scale spatial outages.

### Open Question 4
- Question: Can the computational complexity of the temporal graph construction be reduced to efficiently handle significantly longer historical sequences without performance degradation?
- Basis in paper: [inferred] Section 4.4 notes the overall complexity is $O(n T N d^2 (1 + T + K))$, governed primarily by the temporal layers ($O(n T^2 N d^2)$). This quadratic complexity relative to sequence length $T$ suggests scalability issues for very long-term dependency modeling.
- Why unresolved: The paper experiments on down-sampled data ($L=96$), but the quadratic cost of the fully connected temporal graph sub-layer may prohibit scaling to finer granularities or longer contexts without approximation.
- What evidence would resolve it: Profiling the model's latency and memory usage on sequence lengths significantly exceeding $L=96$ (e.g., $L=1000$) or introducing sparse attention mechanisms to lower the $T^2$ factor.

## Limitations

- The interval-aware dropout parameter α=0.1 appears chosen through grid search without theoretical justification for its optimality across different missing patterns
- The cross-gated mechanism's bidirectional interaction is described but not thoroughly analyzed for computational efficiency or potential redundancy with the ensemble training framework
- Primary-auxiliary pattern disentanglement lacks rigorous ablation to isolate the contribution of each component

## Confidence

- Primary-auxiliary pattern disentanglement: Medium - supported by ablation studies but not rigorously isolated
- Interval-aware dropout effectiveness: Medium-High - demonstrated improvements but parameter choice appears heuristic
- Overall performance claims: High - extensive experiments across three datasets and 27 missing scenarios

## Next Checks

1. **Component isolation study**: Train three variants of PAST - (a) GIM only (remove CGM), (b) CGM only (remove GIM), and (c) standard dropout instead of interval-aware dropout. Compare performance across all missing types to quantify each mechanism's individual contribution and identify potential redundancies.

2. **Parameter sensitivity analysis**: Systematically vary the interval-aware dropout parameter α across a wider range (0.01-1.0) on fiber and block missing scenarios. Plot performance curves to determine if the claimed "optimal" value is genuinely robust or dataset-specific.

3. **External feature ablation**: Remove timestamps and node IDs from the CGM input while keeping the gating architecture intact. Test whether the performance drop is primarily due to loss of external information or the specific cross-gated fusion mechanism. This would clarify whether simpler feature fusion could achieve similar results.