---
ver: rpa2
title: Towards Robust Deep Reinforcement Learning against Environmental State Perturbation
arxiv_id: '2506.08961'
source_url: https://arxiv.org/abs/2506.08961
tags:
- agents
- attack
- learning
- state
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses environmental state perturbations in deep
  reinforcement learning, focusing on realistic scenarios where environmental states
  are perturbed while maintaining reachability. The authors introduce a non-targeted
  attack method to evaluate vulnerability and propose a defense framework, Boosted
  Adversarial Training (BAT), combining supervised kick-starting and adversarial fine-tuning.
---

# Towards Robust Deep Reinforcement Learning against Environmental State Perturbation

## Quick Facts
- arXiv ID: 2506.08961
- Source URL: https://arxiv.org/abs/2506.08961
- Authors: Chenxu Wang; Huaping Liu
- Reference count: 40
- Primary result: BAT improves both robustness to environmental perturbations and baseline performance in Overcooked

## Executive Summary
This paper addresses environmental state perturbations in deep reinforcement learning, focusing on realistic scenarios where environmental states are perturbed while maintaining reachability. The authors introduce a non-targeted attack method to evaluate vulnerability and propose a defense framework, Boosted Adversarial Training (BAT), combining supervised kick-starting and adversarial fine-tuning. Experiments in the Overcooked game show that mainstream DRL agents (SP and FCP) are highly vulnerable to environmental perturbations, with rewards dropping to near zero in many cases. The proposed BAT significantly improves agent robustness, outperforming existing methods like RADIAL and diverse start training. Notably, BAT not only enhances defense but also improves baseline performance in unperturbed environments, demonstrating its effectiveness across various scenarios.

## Method Summary
The method consists of two main phases: (1) Supervised kick-starting, which uses knowledge distillation with softened policy outputs to pre-train the agent on perturbed states, and (2) RL fine-tuning, which adversarially trains the agent on a mixture of original, adversarial, and random initial states. The attack method uses gradient-based optimization to find perturbations that maximize action probability divergence from the original policy. BAT is evaluated on the Overcooked game using two baseline agents (SP and FCP) trained with PPO.

## Key Results
- Mainstream DRL agents (SP and FCP) are highly vulnerable to environmental perturbations, with rewards dropping to near zero in many cases
- BAT significantly improves agent robustness, outperforming existing methods like RADIAL and diverse start training
- BAT not only enhances defense but also improves baseline performance in unperturbed environments, demonstrating its effectiveness across various scenarios

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Environmental Attack Generation
- Claim: Non-targeted attacks can systematically degrade DRL agent performance by perturbing initial environmental states within reachable semantic bounds.
- Mechanism: The attack optimizes perturbations by maximizing action probability divergence between original and perturbed states using first-order gradient approximation. This identifies environmental configurations that trigger out-of-distribution failures without requiring real-time observation manipulation.
- Core assumption: Successful attacks maintain or amplify state divergence over trajectories, implying perturbations persist through the episode.
- Evidence anchors: Attack objective defined to induce deviation from optimal actions; related work addresses similar gradient-based attack optimization.

### Mechanism 2: Supervised Kick-Starting with Knowledge Distillation
- Claim: Pre-training with softened policy outputs enables agents to maintain baseline capability in perturbed environments before RL fine-tuning.
- Mechanism: Uses temperature-scaled softmax to generate "soft labels" from the original policy, combined with KL divergence loss to enforce behavioral consistency while allowing adaptive value estimates. The loosened L1 loss on value functions prevents over-constraining.
- Core assumption: Environmental perturbations have limited direct influence on agent capabilities—they primarily cause distribution shift rather than fundamental skill invalidation.
- Evidence anchors: First tunes agents via supervised learning to avoid catastrophic failure before adversarially training.

### Mechanism 3: Mixed-Distribution Adversarial Fine-Tuning
- Claim: Fine-tuning on a distribution combining original, adversarial, and random initial states improves both robustness and unperturbed performance.
- Mechanism: RL optimization over initial state distribution containing standard initial states, top-k adversarial states, and sampled random states. This creates a curriculum where the agent learns to generalize across environmental variations.
- Core assumption: The kick-started policy provides sufficient reward signal for RL to improve rather than collapse.
- Evidence anchors: BAT+SP achieves 372.8±9.8 vs Extra SP 341.8±16.2 on unperturbed Coordination Ring, showing improvement beyond baseline.

## Foundational Learning

- Concept: **Embodied MDP Formulation** (state factorization into agent vs. environment)
  - Why needed here: The paper explicitly splits state space S = Sₑ × Sₐ to define what can be perturbed. Without this conceptual separation, you cannot distinguish observation perturbations from environmental perturbations.
  - Quick check question: Given a robot navigation task, which states belong to Sₑ vs Sₐ? (Answer: obstacle positions are Sₑ; robot pose/velocity are Sₐ)

- Concept: **Knowledge Distillation with Temperature Scaling**
  - Why needed here: The kick-starting phase uses T=1.5 to soften policy outputs. Understanding why higher temperature produces "softer" probability distributions that transfer better is essential for tuning this hyperparameter.
  - Quick check question: What happens to action probabilities when T→∞? (Answer: uniform distribution; all actions equally likely)

- Concept: **Out-of-Distribution (OOD) Generalization vs. Robustness**
  - Why needed here: The paper distinguishes "capability generalization failure" from "goal misgeneralization." This frames whether attacks break learned skills or redirect goals—critical for designing appropriate defenses.
  - Quick check question: If an agent trained on empty counters encounters a counter with an onion, is this OOD? (Answer: Yes, if onion-on-counter states never appeared in training trajectories above p_freq threshold)

## Architecture Onboarding

- Component map: [Pre-trained Policy π] → [Attack Generator] → [Adversarial Initial States] → [Supervised Kick-starter] → [π_s (Start Policy)] → [RL Fine-tuner] → [Robust Policy π*]

- Critical path: Attack generation → Supervised pre-training (100 epochs) → RL fine-tuning (8e6 steps). The supervised phase must produce π_s that achieves non-zero rewards on perturbed states; otherwise RL has no learning signal.

- Design tradeoffs:
  - Temperature T: Higher values (e.g., 2.0) increase soft-label smoothness but may lose action specificity. Paper uses T=1.5.
  - Perturbation budget ε: ε=3 allows 3 unit perturbations (onion/dish placements). Higher ε increases attack potency but may violate reachability.
  - α (value loss threshold): Controls how strictly value estimates must match. α=0.05 allows 5% deviation; higher α increases flexibility but reduces consistency.

- Failure signatures:
  - Rewards → 0 on adversarial states during fine-tuning: Kick-starting failed; π_s has no capability to build on.
  - Unperturbed performance drops significantly: Over-regularization to perturbations; reduce β or increase T.
  - Validation loss increases during kick-starting: Overfitting to limited trajectory data; reduce epochs or augment with more trajectories.

- First 3 experiments:
  1. **Ablation on kick-starting**: Train BAT without supervised phase (direct RL fine-tuning). Expect catastrophic failure on strong attacks (rewards near zero) to validate the phase is necessary.
  2. **Sensitivity to temperature T**: Test T ∈ {1.0, 1.5, 2.0, 3.0}. Hypothesis: T=1.0 (hard labels) fails on perturbations; T>2.0 degrades unperturbed performance due to excessive smoothing.
  3. **Transfer attack evaluation**: Apply attacks generated from Agent A to Agent B (both trained with BAT). Quantify transfer success rate to understand if BAT creates diverse robust features or shared vulnerabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed environmental perturbation threat model and Boosted Adversarial Training (BAT) framework effectively generalize to 3D domains and real-world robotic applications?
- Basis in paper: The conclusion states that "leaving the generalization to the 3D domain and even the real world as future work."
- Why unresolved: The current validation is confined to the 2D Overcooked game environment, which simplifies state representation and interaction dynamics compared to physical robotic systems.
- What evidence would resolve it: Successful implementation and robustness evaluation of agents trained with BAT on physical robots or 3D simulation benchmarks (e.g., MuJoCo) under environmental perturbations.

### Open Question 2
- Question: How does the reliability of the attack method degrade if the "perturbation invariance assumption" (Assumption 1) is violated by stochastic environment dynamics?
- Basis in paper: The attack optimization relies on Assumption 1, which supposes perturbations persist, yet this may not hold if environmental dynamics naturally reverse perturbations.
- Why unresolved: The paper assumes time-invariant perturbation differences for the gradient approximation, but does not test scenarios where the environment aggressively "corrects" or randomizes the state back toward standard trajectories.
- What evidence would resolve it: A sensitivity analysis of attack success rates in environments with high stochasticity or dynamics that actively counteract the applied initial state perturbations.

### Open Question 3
- Question: Can the BAT framework be adapted for continuous action spaces and high-dimensional vision-based inputs without losing stability?
- Basis in paper: The experiments utilize Overcooked, which features discrete actions and a structured state vector, but the paper claims applicability to general embodied AI.
- Why unresolved: The supervised kick-starting phase uses temperature-scaled Softmax and specific KL-divergence losses which are straightforward for discrete policies but may require significant modification for continuous control distributions or raw pixel inputs.
- What evidence would resolve it: Demonstration of the BAT defense mechanism successfully training robust agents in standard continuous control benchmarks (e.g., Mujoco locomotion) or purely vision-based tasks.

## Limitations

- The evaluation is limited to 2D Overcooked environments, raising questions about generalization to 3D domains and real-world robotic applications
- The attack method assumes perturbations persist through trajectories (Assumption 1), which may not hold in stochastic environments with dynamics that naturally reverse perturbations
- The framework is currently designed for discrete action spaces and structured state representations, requiring adaptation for continuous control and vision-based inputs

## Confidence

- **High**: BAT improves both robustness and baseline performance (Table I shows consistent improvements)
- **Medium**: Supervised kick-starting mechanism effectiveness (relies on distillation assumptions with limited empirical validation)
- **Low**: Attack transferability claims (transfer experiments not conducted; only single-agent vulnerability demonstrated)

## Next Checks

1. **Ablation study**: Train BAT without supervised kick-starting to verify its necessity for preventing catastrophic failure during fine-tuning
2. **Temperature sensitivity**: Test T ∈ {1.0, 1.5, 2.0, 3.0} to identify optimal trade-off between soft-label smoothing and action specificity
3. **Cross-agent transfer**: Apply attacks generated from one BAT-trained agent to another to assess whether the method creates diverse robust features or shared vulnerabilities