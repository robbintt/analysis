---
ver: rpa2
title: Graph Self-Supervised Learning with Learnable Structural and Positional Encodings
arxiv_id: '2502.16233'
source_url: https://arxiv.org/abs/2502.16233
tags:
- graph
- learning
- information
- structural
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in Graph Self-Supervised Learning
  (GSSL) by introducing GenHopNet, a k-hop message-passing GNN framework that surpasses
  the expressiveness of the Weisfeiler-Lehman test for graph isomorphism. The authors
  also propose StructPosGSSL, a self-supervised learning framework that incorporates
  both structural and positional information throughout the learning process.
---

# Graph Self-Supervised Learning with Learnable Structural and Positional Encodings

## Quick Facts
- arXiv ID: 2502.16233
- Source URL: https://arxiv.org/abs/2502.16233
- Authors: Asiri Wijesinghe; Hao Zhu; Piotr Koniusz
- Reference count: 40
- Primary result: StructPosGSSL achieves SOTA graph classification performance (MUTAG 93.0%, PTC-MR 67.1%) by combining k-hop message passing with edge centrality and dual-contrastive learning

## Executive Summary
This paper introduces StructPosGSSL, a graph self-supervised learning framework that achieves state-of-the-art performance on graph classification tasks. The key innovation is GenHopNet, a k-hop message-passing GNN that surpasses the expressiveness of the Weisfeiler-Lehman test by incorporating structural and positional information throughout the learning process. The framework uses a dual-encoder architecture with NT-Xent and VICReg losses to learn topology-aware representations invariant to augmentation while preserving discriminative structure.

## Method Summary
The method combines a k-hop message-passing GNN (GenHopNet) with learnable positional encodings and dual-contrastive learning. GenHopNet aggregates 1-hop neighbors, k-hop normalized neighbors, and closed-walk counts to achieve expressiveness beyond 1-WL. Edge centrality measures (betweenness, closeness, clustering coefficient) are injected into message passing to enhance structural discrimination. A dual-encoder architecture processes structural and positional views, trained with combined NT-Xent (graph-level) and VICReg (node-level) losses. The framework uses Adam optimizer (lr=0.001), hidden dim 128/200, dropout 0.5, and linear evaluation for downstream tasks.

## Key Results
- Achieves SOTA on MUTAG (93.0%) and PTC-MR (67.1%) datasets
- Consistently outperforms existing approaches on OGB molecular datasets
- Demonstrates effectiveness in distinguishing graphs with similar local structures but different global topologies
- Maintains computational efficiency while achieving superior expressiveness

## Why This Works (Mechanism)

### Mechanism 1: k-Hop Structural Message Passing for WL+ Expressiveness
GenHopNet achieves greater expressiveness than the 1-WL test by aggregating multi-scale structural information via k-hop message passing and closed-walk counts. The model combines local aggregation, high-order k-hop neighbor information, and closed-walk patterns ($A^k_{vv}$) with learnable weighting, allowing distinction of nodes with identical 1-hop neighborhoods but different higher-order connectivity.

### Mechanism 2: Edge Centrality-Augmented Message Passing
Edge centrality measures (betweenness, closeness, clustering coefficient) are embedded via MLP and added to 1-hop aggregation, providing structural signal that differentiates edges beyond binary connectivity. This helps distinguish connection types the WL test cannot capture.

### Mechanism 3: Dual-Encoder Contrastive Learning with VICReg Node Alignment
Two parallel GNN encoders process augmented graph views—one for structure (GenHopNet), one for position (Laplacian eigenvector-initialized). NT-Xent maximizes agreement between graph-level representations while VICReg enforces variance, invariance, and covariance constraints on node representations, preventing collapse and ensuring spread.

## Foundational Learning

- **Concept:** Weisfeiler-Lehman (WL) test and GNN expressiveness
  - Why needed: The paper claims to exceed 1-WL expressiveness; understanding WL bounds contextualizes the contribution
  - Quick check: Can you explain why standard MPNNs are bounded by 1-WL and what k-hop information provides beyond this?

- **Concept:** Laplacian positional encodings
  - Why needed: The positional encoder uses Laplacian eigenvectors to provide global positional context
  - Quick check: What property of Laplacian eigenvectors makes them suitable for positional encoding, and how do they handle graph symmetries?

- **Concept:** Contrastive learning objectives (NT-Xent, VICReg)
  - Why needed: The framework combines these losses; understanding their individual roles is critical for debugging
  - Quick check: How does VICReg differ from NT-Xent in preventing representation collapse?

## Architecture Onboarding

- **Component map:** Data augmentation → Dual encoder forward pass → Concatenate embeddings → Compute contrastive losses → Backprop through both encoders

- **Critical path:** 1) Data augmentation (structural + feature views) → 2) Dual encoder forward pass → 3) Concatenate structural + positional embeddings → 4) Compute contrastive losses → 5) Backprop through both encoders

- **Design tradeoffs:**
  - k-hop depth ($k$): Higher $k$ captures more global structure but increases computation; paper uses $k \in \{2,3,4,5,6\}$
  - Loss weighting ($\alpha$): Controls node vs. graph loss balance; intermediate values (0.3-0.7) perform best
  - Positional encoding dimension: Paper uses 6; higher dimensions may add noise

- **Failure signatures:**
  - Representation collapse: All embeddings converge to similar values (check VICReg variance term)
  - Poor performance on sparse graphs: May indicate positional encoder dominance; verify structural encoder contribution via ablation
  - High variance across runs: Check augmentation strength and batch size

- **First 3 experiments:**
  1. Ablation on encoders: Run POS-only and STRUCT-only variants on target dataset to determine relative importance
  2. k-sensitivity sweep: Test $k \in \{2,3,4,5,6\}$ on small graph benchmark to identify optimal depth
  3. Loss component ablation: Remove VICReg terms one at a time to verify covariance term dominance

## Open Questions the Paper Calls Out

- **Can the framework be adapted for node-level and link-level tasks?**
  - The authors acknowledge focusing on graph classification despite SSL often outperforming supervised methods in node-level tasks
  - The graph-level readout function and global contrastive loss may discard local fidelity required for node-level differentiation
  - Empirical results on node classification or link prediction would resolve this

- **How does computational cost of edge centrality measures impact scalability?**
  - Edge Betweenness calculation has $O(VE)$ complexity, becoming prohibitive for large graphs
  - Creates potential bottleneck outside GNN training time
  - Empirical wall-clock time comparison on large graphs would resolve this

- **Can the balance between Structural and Positional Encodings be learned dynamically?**
  - Current implementation uses static concatenation rather than dynamic weighting
  - Different graph topologies may require different encoding priorities
  - A mechanism to adjust SE vs. PE weights during training could improve performance across graph types

## Limitations

- Limited runtime comparison and computational scalability analysis
- Unclear implementation details for VICReg loss components and hyperparameters
- Assumption that Laplacian eigenvectors provide optimal positional information across all graph types
- No explicit discussion of overfitting risks in linear evaluation phase

## Confidence

- **High confidence:** Core claims about k-hop expressiveness improvements and dual-encoder architecture effectiveness
- **Medium confidence:** Computational scalability claims and specific implementation details of VICReg loss components
- **Medium confidence:** Generalization beyond molecular graphs and optimal positional encoding selection

## Next Checks

1. Reproduce POS-only vs. STRUCT-only ablation experiments on MUTAG and PTC-MR to verify relative importance of encoders
2. Implement VICReg loss with different $\gamma$ values to test sensitivity to variance regularization margin
3. Test the model on larger, more diverse graph datasets to validate computational efficiency claims and generalization beyond molecular graphs