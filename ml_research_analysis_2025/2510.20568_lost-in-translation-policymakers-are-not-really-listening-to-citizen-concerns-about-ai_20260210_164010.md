---
ver: rpa2
title: 'Lost in Translation: Policymakers are not really listening to Citizen Concerns
  about AI'
arxiv_id: '2510.20568'
source_url: https://arxiv.org/abs/2510.20568
tags:
- public
- government
- comment
- consultation
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper finds that governments seeking public input on AI policies
  are failing to engage citizens effectively, with participation rates under 1% and
  no meaningful dialogue between policymakers and the public. Using landscape analysis,
  the authors compared Australia, Colombia, and the US consultation processes, finding
  all three relied on government websites and limited outreach, attracting mostly
  insiders rather than diverse voices.
---

# Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI

## Quick Facts
- arXiv ID: 2510.20568
- Source URL: https://arxiv.org/abs/2510.20568
- Reference count: 0
- Primary result: Governments seeking public input on AI policies are failing to engage citizens effectively, with participation rates under 1% and no meaningful dialogue between policymakers and the public.

## Executive Summary
This paper examines three government consultations on AI policy (Australia, Colombia, and US) and finds systemic failures in public engagement. All three consultations relied on government websites and limited outreach, resulting in participation rates under 1% and attracting mostly policy insiders rather than diverse citizen voices. The authors identify critical gaps in how governments solicit and respond to public input on AI governance, concluding that current consultation practices create a "lost in translation" problem where citizen concerns are neither heard nor addressed.

## Method Summary
The authors conducted a comparative case study analysis of three AI policy consultations using a landscape analysis approach. They examined Australia's consultation on Safe and Responsible AI, Colombia's consultation on AI governance, and the US NTIA's AI accountability inquiry. The study evaluated consultation design, outreach strategies, participant demographics, and government responsiveness using the IAP2 public participation spectrum framework. Data collection included reviewing consultation materials, participant submissions, and final policy outcomes.

## Key Results
- Participation rates were under 1% across all three consultations, with Colombia achieving only 73 participants and Australia receiving 476 submissions
- Governments relied primarily on official websites for outreach, resulting in insider-dominated participation (57.5% academics in Colombia, 63% firms in Australia)
- None of the consultations demonstrated meaningful responsiveness to public feedback or created effective feedback loops between citizens and policymakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low participation rates in AI policy consultations are causally linked to inadequate outreach marketing and inaccessible framing of calls for comment.
- Mechanism: When governments rely primarily on official websites and limited social media posts without broad multi-channel campaigns, awareness remains confined to policy insiders who already monitor government portals. This restricts the respondent pool to those with existing institutional access and motivation.
- Core assumption: Citizens would participate if they were aware and understood how to contribute meaningfully.
- Evidence anchors:
  - [abstract] "Governments did little to attract diverse voices or publicize calls for comment, leaving most citizens unaware or unprepared to respond."
  - [section] Colombia's consultation had a one-week window with a single social media post five days before closing, resulting in only 73 participants (Page 16).
  - [corpus] Related work on e-participation platforms (AskThePublic) notes similar challenges: "current approaches of primarily static nature struggle to integrate" citizens effectively.
- Break condition: If targeted, multi-platform marketing campaigns with trusted messengers (celebrities, academics, community leaders) are deployed, and participation remains below 1%, the awareness mechanism is insufficient.

### Mechanism 2
- Claim: Absence of demonstrated responsiveness to public input erodes trust and creates a negative feedback loop that disincentivizes future participation.
- Mechanism: When citizens perceive that their comments are not visibly integrated into policy outcomes, they conclude participation is performative rather than substantive. This perception reduces motivation to engage in subsequent consultations, perpetuating low participation rates.
- Core assumption: Citizens are rational actors who weigh the perceived impact of their participation against the effort required.
- Evidence anchors:
  - [abstract] "Officials showed limited responsiveness to the feedback they received, failing to create an effective feedback loop."
  - [section] Page 19: OECD found only 32% of citizens across OECD countries believe governments would adopt opinions expressed in consultations.
  - [corpus] The paper "Artificial Intelligence in Government: Why People Feel They Lose Control" identifies similar concerns about citizen agency in AI governance.
- Break condition: If governments publish detailed "you said, we did" response documents mapping public comments to policy changes, and participation rates do not increase in subsequent consultations, the responsiveness mechanism alone is insufficient.

### Mechanism 3
- Claim: AI literacy gaps and overly technical consultation framing create entry barriers that exclude non-expert citizens regardless of outreach efforts.
- Mechanism: Even when citizens are aware of consultations, the complexity of AI policy questions (the NTIA posed 52 sub-questions) combined with assumed baseline knowledge in discussion papers creates cognitive barriers that intimidate or exclude lay participants.
- Core assumption: Effective participation requires domain-specific knowledge that most citizens do not currently possess.
- Evidence anchors:
  - [section] Page 13: "many participants did not answer all of the questions" and "some seemed confused by the specifics and sheer number of the questions."
  - [section] Page 14: Materials "assumed that many respondents would have a baseline familiarity with AI governance, safety, and risk concepts."
  - [corpus] Weak corpus evidence on AI literacy as a causal barrier specifically; related work focuses on e-government usability but not domain-specific literacy requirements.
- Break condition: If accessible primers, video explainers, and simplified question frameworks are provided, and non-expert participation does not increase meaningfully, the literacy barrier mechanism is not the primary constraint.

## Foundational Learning

- Concept: **IAP2 Spectrum of Public Participation**
  - Why needed here: The paper uses this framework to diagnose where current AI consultations fail (inform/consult levels) versus where they should operate (involve/collaborate/empower levels) to build legitimacy.
  - Quick check question: Can you map a given consultation process to its position on the IAP2 spectrum (inform → empower) and identify what would move it rightward?

- Concept: **Feedback Loop Architecture in Democratic Governance**
  - Why needed here: The core failure identified is not lack of input collection but lack of visible responsiveness. Understanding bidirectional feedback systems is essential for designing consultation infrastructure.
  - Quick check question: For a given public comment submission, can you trace whether and how the government publicly acknowledged and responded to it?

- Concept: **Participatory Inclusion Bias**
  - Why needed here: Self-selected participation favors those with institutional knowledge and resources. Designing systems that counteract this bias requires understanding how different outreach channels reach different populations.
  - Quick check question: Given a consultation's outreach channels, can you predict which demographic groups are likely over- or under-represented in responses?

## Architecture Onboarding

- Component map:
  - Announcement channels (government websites, social media, traditional media, community partnerships) -> Submission portals (web forms, email, phone, in-person events) -> Educational materials (primers, video explainers, FAQs) -> Comment aggregation and analysis (potentially AI-assisted) -> Public response documentation showing how input influenced policy

- Critical path: Announcement → Awareness → Literacy support → Accessible submission → Acknowledgment → Visible response integration → Trust building

- Design tradeoffs:
  - **Breadth vs. depth**: Broad outreach may increase volume but reduce comment quality; deep engagement with smaller groups may miss diverse perspectives
  - **Speed vs. inclusion**: Shorter consultation windows enable faster policy response but exclude those needing time to understand complex issues
  - **Simplification vs. nuance**: Accessible framing may oversimplify AI risks; technical accuracy may exclude non-experts

- Failure signatures:
  - **Insider capture**: >70% of respondents from firms, academia, or policy organizations (as seen in Colombia's 57.5% academic respondents)
  - **Ghost consultation**: Final policy documents released within days of consultation close, indicating pre-determined outcomes
  - **Awareness gap**: <1% population participation combined with no mainstream media coverage of the consultation

- First 3 experiments:
  1. **A/B test announcement channels**: Deploy identical consultation content through government-only channels vs. multi-channel campaigns (social media influencers, community organizations, traditional media). Measure differential reach and respondent demographics.
  2. **Response documentation pilot**: For a subset of comments, publish structured "you said, we considered, we decided" responses. Track whether this increases participation rates in subsequent consultations from the same population.
  3. **Scaffolded literacy intervention**: Provide a randomized group of potential participants with video explainers and simplified question summaries before the consultation opens. Compare submission rates and comment completeness against a control group receiving standard materials only.

## Open Questions the Paper Calls Out
None

## Limitations
- The study identifies correlation between poor outreach and low participation but cannot definitively establish causation due to lack of experimental variation
- Analysis relies on cross-sectional observation of three government consultations rather than longitudinal tracking of how specific interventions affect participation over time
- The authors acknowledge potential confounding factors, including that AI may be a less salient policy issue for average citizens compared to healthcare or education

## Confidence
- **High confidence**: The descriptive findings about current consultation practices (government website reliance, limited outreach, insider-dominated participation) are well-supported by direct observation of the three case studies
- **Medium confidence**: The mechanisms linking consultation design to participation rates are theoretically sound but not experimentally validated within this study
- **Low confidence**: The relative importance of different failure modes (outreach, literacy, responsiveness) remains uncertain without controlled interventions

## Next Checks
1. Conduct a randomized controlled trial testing multi-channel outreach campaigns against traditional government-only announcements for an AI policy consultation, measuring both reach and demographic diversity of respondents
2. Implement and evaluate "you said, we did" documentation for one consultation cycle, tracking whether this transparency increases participation in subsequent cycles from the same population
3. Develop and test scaffolded educational materials (video explainers, simplified question frameworks) with a random subset of potential participants, comparing their engagement rates and comment quality against a control group receiving standard consultation materials only