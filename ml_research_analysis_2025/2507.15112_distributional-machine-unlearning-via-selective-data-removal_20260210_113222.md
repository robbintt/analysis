---
ver: rpa2
title: Distributional Machine Unlearning via Selective Data Removal
arxiv_id: '2507.15112'
source_url: https://arxiv.org/abs/2507.15112
tags:
- removal
- unlearning
- samples
- data
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces distributional unlearning, a framework for
  selectively removing a small subset of samples to efficiently erase the statistical
  influence of an unwanted domain while preserving a desired one. Unlike existing
  sample-level unlearning methods that assume a known forget set, this approach addresses
  the statistical question of which samples to remove for maximal impact.
---

# Distributional Machine Unlearning via Selective Data Removal

## Quick Facts
- arXiv ID: 2507.15112
- Source URL: https://arxiv.org/abs/2507.15112
- Reference count: 40
- Primary result: Framework that selectively removes small subsets of samples to efficiently erase statistical influence of unwanted domains while preserving desired ones

## Executive Summary
This paper introduces distributional unlearning, a framework for selectively removing a small subset of samples to efficiently erase the statistical influence of an unwanted domain while preserving a desired one. Unlike existing sample-level unlearning methods that assume a known forget set, this approach addresses the statistical question of which samples to remove for maximal impact. The framework uses KL divergence constraints to formalize the removal-preservation trade-off and derives the exact Pareto frontier for Gaussian distributions. Experiments across synthetic, text, and image datasets demonstrate significant data savings (15-82%) compared to full removal while maintaining performance on retained domains.

## Method Summary
The method treats unlearning as a distributional problem: given samples from unwanted domain p₁ and retained domain p₂, select a subset of p₁ to remove that maximizes forgetting (KL(p₁||p)) while minimizing preservation cost (KL(p₂||p)). For Gaussian distributions with shared covariance, the Pareto frontier is ε = (√α − √KL(p₁||p₂))², showing explicit trade-offs. The framework proposes distance-based selection (e.g., distance to retained mean) and likelihood-ratio scoring that outperforms coreset methods in low-divergence regimes. The approach complements existing sample-level unlearning techniques and achieves quadratic improvement in sample efficiency.

## Key Results
- Derived exact Pareto frontier for Gaussian distributions showing explicit removal-preservation trade-offs
- Demonstrated 15-82% reduction in samples requiring deletion compared to full removal across multiple domains
- Quadratic improvement in sample efficiency over random removal in low-divergence regimes
- Effective generalization of class unlearning problem with measurable impact on downstream model performance

## Why This Works (Mechanism)

### Mechanism 1: Concentration of Statistical Influence
A domain's statistical influence concentrates in a small subset of samples, enabling selective removal rather than full deletion. Samples furthest from the retained distribution mean contribute disproportionately to the KL divergence between forget and retain distributions. Removing these high-impact samples shifts the empirical mean more efficiently than random removal, with the effect amplified in low-divergence regimes where distributions overlap significantly. When forget and retain distributions are well-separated (high KL(p₁||p₂)), selective advantage diminishes because most forget samples are already distinguishable.

### Mechanism 2: Pareto-Optimal Trade-off via KL Constraints
The removal-preservation trade-off can be formally characterized, preventing naive solutions that over-preserve or over-remove. For Gaussian distributions with shared covariance, the achievable (α, ε) pairs follow a frontier: ε = (√α − √KL(p₁||p₂))². This forces explicit acknowledgment that any removal level α incurs minimum preservation cost ε governed by initial divergence. The frontier shows keeping only retained data (p₂) is often suboptimal—it achieves ε=0 but sacrifices potential removal. For non-exponential-family distributions or multimodal cases, the frontier may not be analytically tractable.

### Mechanism 3: Cross-Distributional Selection Outperforms Single-Distribution Methods
Scoring samples by their relationship to both forget and retain distributions (rather than just forget) is necessary for efficient unlearning. Coreset methods optimize representativeness within p₁, selecting samples closest to μ₁. In low-divergence settings, these are precisely the samples least distinguishable from p₂—the opposite of what's needed. Distance-based selection (|x − μ₂|) and likelihood-ratio scoring (d(x, μ₂) − d(x, μ₁)) explicitly leverage the contrast between distributions. When no retain distribution is available, cross-distributional selection cannot operate.

## Foundational Learning

- **Concept: Kullback-Leibler Divergence**
  - Why needed here: KL divergence quantifies both removal (how far from unwanted) and preservation (how close to retained); its chain rule connects data-level edits to downstream log-loss guarantees.
  - Quick check question: If KL(p₁||p) = α and KL(p₂||p) = ε, what happens to a model's expected loss on p₁ when trained on p?

- **Concept: Pareto Frontier**
  - Why needed here: Defines feasible trade-offs; prevents pursuing unachievable (α, ε) pairs and provides calibration rules (e.g., set ε = tolerable log-loss increase, then read max α from frontier).
  - Quick check question: Given KL(p₁||p₂) = 2 and preservation budget ε = 0.1, what's the maximum achievable removal α?

- **Concept: Influence Functions / Sample-Level Unlearning**
  - Why needed here: Distributional unlearning is complementary—it identifies which samples to remove; sample-level methods handle the how of efficient model updates. The paper shows 12-45% savings when combining both.
  - Quick check question: How does the computational cost of influence-based unlearning scale with forget-set size, and why does selective removal help?

## Architecture Onboarding

- **Component map:** Domain Identification -> Feature Extraction -> Scoring Module -> Budget Allocator -> Removal Executor -> Downstream Unlearning

- **Critical path:** Domain identification → Feature extraction → Score computation → Budget selection → Removal → Model training/update

- **Design tradeoffs:**
  - Mahalanobis vs. Cosine distance: Mahalanobis captures covariance structure (better for dense features like CNN) but O(d³) for inverse covariance; Cosine is O(d) but ignores feature correlations
  - Simple vs. complex scoring: LR-COS (likelihood-ratio) can outperform distance-only at low budgets but may sacrifice utility at high budgets (Figure 6)
  - Retain set size: Smaller retain sets yield noisier μ₂ estimates; paper downsamples to 5× forget-set size in Jigsaw experiments

- **Failure signatures:**
  - Preserved-domain performance drops sharply: Deletion budget too high; reduce f or use more conservative scoring
  - Minimal forgetting effect even at high deletion: Distributions too intertwined (low KL(p₁||p₂)); may need different approach or accept limits
  - Coreset outperforms selective: Likely high-divergence regime where selective advantage is small; verify KL(p₁||p₂) magnitude

- **First 3 experiments:**
  1. **Synthetic validation**: Generate Gaussian p₁=N(0,1), p₂=N(μ,1) with μ∈{0.5, 2.5, 5}; verify Pareto frontier match and selective vs. random gap correlates with KL(p₁||p₂)
  2. **Class unlearning probe**: On CIFAR-10, treat one class as p₁; plot forget-class accuracy vs. deletion budget for MAHA-MU2 vs. random; expect ~50% budget to halve accuracy
  3. **Integration test**: Combine selective removal (LR-COS at 60% budget) with SalUn on CIFAR-10; compare forget-set accuracy against SalUn with full forget set to quantify efficiency gain

## Open Questions the Paper Calls Out

- **Open Question 1:** Can finite-sample theoretical guarantees for distributional unlearning be extended beyond Gaussian distributions to general exponential families or arbitrary distributions? The current finite-sample analysis provides strong guarantees but assumes Gaussian distributions, and bridging the gap between theoretical bounds and complex real-world data behavior remains open.

- **Open Question 2:** Can distributional unlearning be combined with active learning to efficiently identify high-impact forget samples without exhaustive annotation? The framework assumes samples are already labeled as belonging to the unwanted domain; the upstream identification problem is treated as separate, creating potential for active identification systems that could find high-impact samples at a fraction of exhaustive annotation cost.

- **Open Question 3:** What is the formal relationship between distributional unlearning guarantees and certified sample-level unlearning guarantees? Distributional unlearning provides KL-divergence guarantees on data distributions, while certified unlearning provides distance guarantees on model parameters; the mapping between these is not established and could conceptually link the frameworks.

- **Open Question 4:** How does the framework perform when only a coarse proxy retain distribution is available, as in large-scale LLM unlearning settings? The paper notes that when no retain distribution is available, the objective collapses to selecting influential points within the forget distribution, suggesting acquiring even a coarse proxy retain dataset is important for practical deployment.

## Limitations

- The framework requires a well-defined retained domain p₂; without it, the method collapses to single-distribution selection methods
- Current theoretical guarantees assume Gaussian distributions with shared covariance, limiting applicability to multimodal or non-exponential family distributions
- High initial KL divergence between forget and retain distributions reduces the selective advantage, making the method less effective when domains are already well-separated
- The upstream domain identification problem (determining which samples belong to p₁ vs p₂) is treated as out-of-scope and assumed to be solved

## Confidence

- **Gaussian Pareto frontier derivation:** High confidence - explicit closed-form solution with empirical validation
- **Quadratic improvement claim:** High confidence - supported by theoretical analysis and synthetic experiments
- **Cross-distributional selection advantage:** Medium confidence - strong experimental evidence but limited to specific scenarios
- **Integration with sample-level unlearning:** Medium confidence - demonstrated but not exhaustively evaluated across methods
- **Real-world applicability:** Low confidence - acknowledges multimodal distributions and upstream identification as open challenges

## Next Checks

1. Verify Pareto frontier alignment on synthetic Gaussians by plotting ε vs α for different KL(p₁||p₂) values and comparing against theoretical frontier
2. Implement MAHA-MU2 scoring on CIFAR-10 class unlearning and measure class accuracy degradation vs deletion budget
3. Test sensitivity to retain set quality by progressively noising or downsampling the retain distribution and measuring unlearning effectiveness