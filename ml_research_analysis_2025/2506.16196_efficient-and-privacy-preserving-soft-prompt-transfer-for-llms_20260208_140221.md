---
ver: rpa2
title: Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs
arxiv_id: '2506.16196'
source_url: https://arxiv.org/abs/2506.16196
tags:
- transfer
- prompt
- private
- soft
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of privately adapting large language
  models (LLMs) to user-specific tasks without exposing sensitive data or requiring
  expensive fine-tuning on the large model. The authors propose POST (Privacy Of Soft-prompt
  Transfer), a framework that first distills a large LLM into a smaller model using
  knowledge distillation, then tunes a soft prompt locally on the user's private data
  (optionally with differential privacy guarantees), and finally transfers the prompt
  back to the large LLM using only public data.
---

# Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs

## Quick Facts
- arXiv ID: 2506.16196
- Source URL: https://arxiv.org/abs/2506.16196
- Authors: Xun Wang; Jing Xu; Franziska Boenisch; Michael Backes; Christopher A. Choquette-Choo; Adam Dziedzic
- Reference count: 40
- Primary result: POST achieves strong performance transferring soft prompts from a distilled model to large LLMs while preserving privacy and reducing compute by 6x

## Executive Summary
This paper addresses the challenge of privately adapting large language models (LLMs) to user-specific tasks without exposing sensitive data or requiring expensive fine-tuning on the large model. The authors propose POST (Privacy Of Soft-prompt Transfer), a framework that first distills a large LLM into a smaller model using knowledge distillation, then tunes a soft prompt locally on the user's private data (optionally with differential privacy guarantees), and finally transfers the prompt back to the large LLM using only public data. The key innovation is a novel prompt transfer method that aligns both output predictions and prompt-induced behavior changes using a small public dataset. Experiments on classification and generation tasks with three different LLMs demonstrate that POST significantly outperforms zero-shot performance and direct prompt transfer, while preserving privacy and reducing computational costs by 6x on large datasets.

## Method Summary
POST is a three-stage pipeline for privately transferring soft prompts from a compressed model to a large LLM. First, knowledge distillation (KD) creates a smaller student model from the target LLM using a three-term objective (L_ce, L_lm, L_cos) to preserve feature space alignment. Second, the user tunes a soft prompt on their private data using the compressed model, optionally with differential privacy (PromptDPSGD). Third, the tuned prompt is transferred to the large LLM using a dual-loss objective that balances direct prediction matching (L1) with behavior-change replication (L2) on public data. The framework enables private adaptation without exposing user data to the LLM provider while achieving strong performance at reduced computational cost.

## Key Results
- POST achieves 90.02% transfer accuracy on sst2 compared to 85.38% for non-distilled models
- Knowledge distillation enables soft prompt transferability that would otherwise fail due to prompt overfitting
- DP transfer achieves only 0.11% accuracy degradation (89.91% vs 90.02%) while reducing membership inference attack AUC from 0.5610 to 0.5258
- POST reduces computational costs by 6x compared to full prompt tuning on large datasets

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation preserves feature space alignment between large and small models, enabling soft prompt transferability that would otherwise fail due to prompt overfitting. A three-term distillation objective (L_ce for logit alignment, L_lm for language modeling, L_cos for embedding similarity) trains a compressed student model that matches the teacher's predictive behavior patterns rather than just final accuracy. This alignment ensures prompts trained on the small model occupy a compatible representational space.

### Mechanism 2
The dual-loss transfer objective with tunable α parameter enables effective prompt adaptation by balancing direct prediction matching against behavior-change replication. L1 = KL divergence between Φ_t(p_t + x) and Φ_s(p_s + x) mimics outputs; L2 = KL divergence between (Φ_t(p_t + x) - Φ_t(x)) and (Φ_s(p_s + x) - Φ_s(x)) captures prompt-induced behavioral shifts. When the large model has strong zero-shot capability, higher α (emphasizing L2) prevents degradation; when the compressed model is stronger, lower α (emphasizing L1) leverages its task knowledge.

### Mechanism 3
Differential privacy noise during prompt tuning acts as an implicit regularizer that paradoxically improves transfer generalization while providing formal privacy guarantees. PromptDPSGD clips per-sample gradients to bound sensitivity and adds calibrated Gaussian noise. Beyond privacy, this noise injection prevents overfitting to the small model's idiosyncrasies, yielding prompts that generalize better to the large model's different representational space.

## Foundational Learning

- **Soft Prompt Tuning**: Why needed here: The entire framework presupposes understanding that soft prompts are learned embedding vectors prepended to input, tuned via gradient descent while keeping the base model frozen—fundamentally different from discrete textual prompts. Quick check: Can you explain why soft prompts don't transfer directly between models even when both are trained on identical data?

- **Knowledge Distillation Loss Components**: Why needed here: Implementation requires configuring α_ce, α_lm, α_cos weights and understanding how each term shapes student-teacher alignment differently than standard fine-tuning. Quick check: What would happen to transfer performance if you used only L_ce (logit matching) without L_cos (embedding cosine loss)?

- **Differential Privacy Mechanisms (DPSGD)**: Why needed here: Integrating DP requires understanding gradient clipping (bounds per-sample influence) and noise calibration (σ·c relationship) to select meaningful ε, δ values. Quick check: Why does DP's post-processing property allow sharing p_s with the LLM provider without additional privacy loss?

## Architecture Onboarding

- Component map: Large LLM Φ_t --KD--> Compressed Model Φ_s --distribute--> User -> [Private Data D_pri] + [Φ_s] -> [Soft Prompt p_s] -> [p_s] + [Public Data D_pub] --Dual-Loss Optimization--> [Transferred Prompt p_t] -> [Large LLM Φ_t with p_t]

- Critical path: The transfer loss (Equation 3) is the integration point where all components meet—incorrect α selection or mismatched public data directly degrades final performance.

- Design tradeoffs:
  - Compression ratio: Deeper student models transfer better (Table 19: 3-layer 88.53% vs 2-layer 87.73% on sst2) but increase user-side compute and distillation time (6h04m → 7h35m)
  - Prompt length: 50-100 tokens optimal (Table 21); shorter prompts underfit, longer prompts add compute without gains
  - Transfer steps: Convergence by ~1000-2000 steps (Figure 3); over-training on small public sets risks overfitting

- Failure signatures:
  - Direct transfer underperforms compressed model: Indicates insufficient KD alignment or wrong α
  - DP transfer significantly underperforms non-DP: ε too aggressive or private dataset too small
  - Transfer accuracy < compressed PT: User has no incentive to use large model—compression ratio too conservative

- First 3 experiments:
  1. Validate KD alignment: Compare transfer from distilled vs. non-distilled student on a held-out task pair (reproduce Table 6 pattern with your model pair)
  2. Calibrate α for your domain: Run α sweep (0.0 to 1.0 in 0.1 increments) on a representative task, validate heuristic predictions
  3. Characterize public data sensitivity: Test 3 public datasets (same-task, same-domain, cross-domain) to establish domain-matching requirements before deployment

## Open Questions the Paper Calls Out

- **Privacy leakage during inference**: The authors explicitly state they didn't address privacy leakage risk during the inference phase, noting their work focuses solely on protecting training data. This leaves the interaction between transferred prompts and LLMs during querying as an unexplored attack surface.

- **Optimal public dataset selection**: The authors acknowledge there is no ideal strategy for selecting the optimal public dataset, despite observing that task-family similarity generally improves performance. The current method relies on heuristics rather than a deterministic selection mechanism.

- **LLM pretraining data leakage**: The knowledge distillation process may risk leaking the LLM provider's pretraining data. While preliminary results suggest minimal leakage, a more thorough investigation is warranted regarding vulnerability to extraction attacks or copyright auditing.

## Limitations

- The claim that DP noise provides "implicit regularization" improving transfer generalization is stated but not rigorously validated
- Public data requirement assumes task-family similarity suffices for transfer, but evaluation only tests this across limited domains
- Compression ratio tradeoffs are only explored within a narrow range (2-4 layers); deeper compression might break KD alignment

## Confidence

- **High**: POST outperforms zero-shot and direct transfer in controlled experiments (Tables 1, 3, 6)
- **Medium**: The α heuristic reliably predicts near-optimal transfer settings across model scales (Table 20)
- **Low**: DP noise acts as beneficial regularization rather than pure privacy overhead (Section 5.2 claim)

## Next Checks

1. Test cross-task transfer feasibility: Attempt sentiment→medical classification transfer using the same POST pipeline to verify domain-matching assumptions hold beyond the paper's evaluation scope
2. Characterize DP sensitivity: Systematically vary ε from 1 to 16 on the same task to empirically map the privacy-utility tradeoff curve and validate whether DP's apparent regularization is consistent or task-dependent
3. Stress KD alignment: Replace the three-term distillation objective with only L_ce (logit matching) on a held-out task pair to isolate whether L_cos is truly necessary for successful prompt transfer