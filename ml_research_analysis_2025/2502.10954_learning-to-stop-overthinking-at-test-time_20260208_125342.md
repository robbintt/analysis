---
ver: rpa2
title: Learning to Stop Overthinking at Test Time
arxiv_id: '2502.10954'
source_url: https://arxiv.org/abs/2502.10954
tags:
- accuracy
- conv-ligru
- task
- recurrent
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of deep-thinking models that
  use excessive computation for both easy and hard test samples, leading to the "overthinking"
  problem. The authors propose a test-time training method to determine the optimal
  computation for each sample by estimating the accuracy trend using a self-supervised
  auxiliary task.
---

# Learning to Stop Overthinking at Test Time

## Quick Facts
- arXiv ID: 2502.10954
- Source URL: https://arxiv.org/abs/2502.10954
- Reference count: 19
- Primary result: Conv-LiGRU achieves 0.98% higher accuracy on CIFAR-10-C and CIFAR-100-C by adaptively determining optimal computation per sample

## Executive Summary
This paper addresses the inefficiency of deep-thinking models that use excessive computation for both easy and hard test samples, leading to the "overthinking" problem. The authors propose a test-time training method to determine the optimal computation for each sample by estimating the accuracy trend using a self-supervised auxiliary task. They introduce Conv-LiGRU, a novel recurrent architecture that removes the reset gate, replaces tanh with ReLU, and uses batch normalization, resulting in improved stability and efficiency. Experiments show that Conv-LiGRU achieves superior accuracy compared to previous methods on CIFAR-10-C and CIFAR-100-C datasets, with an average accuracy improvement of 0.98% over Conv-GRU and significantly reduces overthinking.

## Method Summary
The method uses a Conv-LiGRU architecture with 30 training iterations (T_train) and 100 test iterations (T_test). At each iteration, the model processes the input through a recurrent block to produce hidden states, which are then passed through dual readouts for classification and rotation prediction (4-class self-supervised task). During test-time, the model tracks auxiliary task accuracy across all iterations and selects the iteration with peak auxiliary performance (t_opt) as the halting point for the main classification task. The architecture removes the reset gate from GRU, replaces tanh with ReLU, and adds batch normalization for improved stability.

## Key Results
- Conv-LiGRU achieves 0.98% higher average accuracy than Conv-GRU on CIFAR-10-C and CIFAR-100-C datasets
- Test-time training method selects optimal computation per sample, avoiding both underthinking and overthinking
- Accuracy peaks around 30-40 iterations and degrades beyond 60 iterations with fixed computation, demonstrating overthinking
- Rotation prediction serves as effective proxy task, though fails for isotropic objects with few edge features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A self-supervised auxiliary task (rotation prediction) can serve as a proxy to estimate the optimal stopping iteration for the main classification task without ground-truth labels.
- Mechanism: The auxiliary task shares semantic structure with the main task, creating a positive correlation between their accuracy curves across iterations. By tracking auxiliary accuracy at test time, the iteration with peak auxiliary performance (t_opt) is selected as the halting point for the main task.
- Core assumption: Assumption: Accuracy_Taux(t) positively correlates with Accuracy_Tmain(t) under both in-distribution and out-of-distribution conditions.
- Evidence anchors:
  - [abstract]: "determining the optimal computation for each sample by estimating the accuracy trend using a self-supervised auxiliary task"
  - [section 3.2]: "corr(Accuracy_Taux(t), Accuracy_Tmain(t)) > 0 ∀ t"
  - [corpus]: Related work on test-time training (Sun et al., 2020) shows TTT is robust to distribution shifts, supporting the proxy task approach.
- Break condition: Samples with isotropic characteristics or few edge features (e.g., "Cat" class) where rotation prediction becomes unreliable, decoupling from main task performance.

### Mechanism 2
- Claim: Removing the reset gate from GRU and replacing tanh with ReLU + batch normalization creates more stable iterative reasoning.
- Mechanism: The reset gate can disrupt intermediate features; removing it ensures uninterrupted information flow across thinking steps. ReLU mitigates vanishing gradients; batch normalization stabilizes iterative computations for image data.
- Core assumption: Assumption: Reasoning steps are sequential dependencies where "skipping even a single step can lead to incorrect conclusions."
- Evidence anchors:
  - [abstract]: "Conv-LiGRU... removes the reset gate, replaces tanh with ReLU, and uses batch normalization, resulting in improved stability"
  - [section 3.3]: "removing the reset gate is a reasonable choice to ensure a stable flow of information and a consistent, uninterrupted thinking process"
  - [corpus]: Limited corpus validation; LiGRU originally designed for audio (Ravanelli et al., 2018), adaptation to vision is novel here.
- Break condition: Task types where intermediate feature reset is beneficial (not tested in this paper).

### Mechanism 3
- Claim: Feature map convergence (‖ht − ht−1‖₂ → 0) does NOT guarantee mitigation of overthinking; loss convergence is the true indicator.
- Mechanism: Conv-GRU shows feature convergence but divergent loss (overthinking persists). Conv-LiGRU shows both feature AND loss convergence, correlating with stable accuracy across iterations.
- Core assumption: Assumption: Overthinking manifests as divergent classification/self-supervised loss despite convergent hidden representations.
- Evidence anchors:
  - [section 4.7]: "Figure 9a illustrates that ∥ht−ht−1∥2 of Conv-GRU converges to 0... Nevertheless, Figure 9b reveals that both the classification loss and self-supervised loss exhibit divergence"
  - [section 4.7]: "the convergence of ∥ht−ht−1∥2 does not ensure that the model is free from 'overthinking'"
  - [corpus]: Corpus neighbors discuss LLM overthinking but not this specific feature-vs-loss convergence distinction.
- Break condition: When feature convergence genuinely correlates with stable loss (task-dependent).

## Foundational Learning

- Concept: **GRU Gating Mechanisms (reset gate vs. update gate)**
  - Why needed here: Conv-LiGRU removes the reset gate; understanding its original purpose clarifies the architectural tradeoff.
  - Quick check question: Can you explain what the reset gate controls in a standard GRU and why removing it might help or hurt?

- Concept: **Test-Time Training (TTT)**
  - Why needed here: The method uses self-supervised evaluation at test time to determine stopping; this differs from standard fixed inference.
  - Quick check question: How does test-time training differ from standard inference, and what information does it require?

- Concept: **Distribution Shift / OOD Generalization**
  - Why needed here: CIFAR-C evaluates extrapolation from clean training data to corrupted test data at varying severity levels.
  - Quick check question: Why might a model need more compute for corrupted inputs than clean ones?

## Architecture Onboarding

- Component map:
  - Input Transformation -> Conv-LiGRU Recurrent Block (30 iterations) -> Dual Readouts (Classification + Rotation) -> Optimal Iteration Selection

- Critical path:
  1. Image X → initial hidden state h₀
  2. h₀ + ht−1 → Conv-LiGRU → ht (repeated T_test times)
  3. Each ht → readout → both task predictions
  4. Post-hoc: select iteration with highest auxiliary accuracy

- Design tradeoffs:
  - Conv-LiGRU vs. Recall: LiGRU allows downsampled features (cheaper); Recall requires full resolution but preserves long-term memory differently.
  - ACT vs. self-supervised estimation: ACT adds ponder cost → premature halting; this method allows full computation then selects optimally.
  - Rotation prediction: Simple and effective but fails on isotropic/edgeless objects.

- Failure signatures:
  - ACT with high τ → halting after 2–3 iterations (underthinking)
  - Conv-GRU → loss divergence after ~20 iterations (overthinking)
  - Rotation task on isotropic samples → low correlation with main task

- First 3 experiments:
  1. Train Conv-LiGRU on CIFAR-10 (T_train=30), evaluate on CIFAR-10-C level 5 (T_test=100); plot accuracy vs. iterations comparing Conv-GRU, Conv-LiGRU, and ResNet.
  2. Ablate stopping logic: run inference with fixed iteration counts (5, 10, 20, 50, 100) to visualize overthinking curve and validate topt selection.
  3. Validate auxiliary-main correlation: on held-out validation set, plot Accuracy_Taux(t) vs. Accuracy_Tmain(t) per corruption type to verify positive correlation assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative self-supervised auxiliary tasks (e.g., contrastive learning or reconstruction) outperform rotation prediction in estimating optimal iteration depth for non-geometric or isotropic objects?
- Basis in paper: [Explicit] Section 4.6 explicitly identifies a limitation where the rotation prediction task fails for samples with few edge features or isotropic characteristics (e.g., the "Cat" class), negatively impacting the estimation of the optimal iteration for the main task.
- Why unresolved: The authors use rotation prediction as a heuristic proxy for main-task accuracy, but they acknowledge it breaks down when image features do not support geometric reasoning, leaving the reliability of the stopping mechanism uncertain for these cases.
- What evidence would resolve it: Experiments comparing rotation prediction against other self-supervised tasks on isotropic datasets, showing a consistently positive correlation between the auxiliary task accuracy and the main task accuracy across all sample types.

### Open Question 2
- Question: How does the removal of the reset gate in Conv-LiGRU affect the model's ability to correct erroneous "thinking steps" compared to standard Conv-GRU?
- Basis in paper: [Explicit] Section 3.3 states that the reset gate was removed to ensure a stable flow of information and prevent the disruption of intermediate features. [Inferred] While this improves stability, it theoretically limits the model's capacity to "forget" irrelevant information accumulated during the recurrent process, raising questions about error propagation.
- Why unresolved: The paper demonstrates that removing the gate improves stability and mitigates overthinking on CIFAR-C, but it does not analyze scenarios where the model might need to actively reset its state to recover from a bad inference step.
- What evidence would resolve it: An ablation study injecting noise at specific recurrent steps to test if Conv-LiGRU is less capable of recovering from errors compared to Conv-GRU, or an analysis of gradient flow in long dependency chains where "forgetting" is beneficial.

### Open Question 3
- Question: Can the test-time training method and Conv-LiGRU architecture be effectively adapted for sequential data processing in Large Language Models (LLMs) to mitigate overthinking?
- Basis in paper: [Explicit] The Introduction states that these models are "crucial for real-world applications and have the potential to apply to LLMs," while noting that current LLM "thinking" approaches often activate all layers uniformly. [Explicit] The Conclusion suggests these findings "pave the way for further advancements... for real-world applications."
- Why unresolved: The entire experimental scope is limited to static visual recognition tasks (CIFAR-10/100-C). It is untested whether the self-supervised accuracy estimation (using rotation) or the convolutional recurrent architecture translates effectively to the 1D token sequences and complex semantic reasoning required for LLMs.
- What evidence would resolve it: Implementing a LiGRU-based recurrent block within a transformer architecture and testing the test-time training method on a textual reasoning benchmark (e.g., math word problems) to measure computational savings and accuracy retention.

## Limitations

- Self-supervised proxy task (rotation prediction) fails for isotropic objects with few edge features, limiting the method's reliability on certain classes
- Test-time training requires running all T_test iterations at inference, increasing computational cost compared to ACT-style early stopping
- Method demonstrated only on CIFAR-C datasets; generalization to more complex tasks and larger-scale datasets remains unproven

## Confidence

**Conv-LiGRU Architecture Improves Stability**: High confidence. Empirical results show consistent accuracy improvements over Conv-GRU across multiple corruption types and severities.

**Self-Supervised Proxy Task Enables Optimal Stopping**: Medium confidence. Correlation assumption is supported by experimental results but lacks systematic validation across diverse datasets.

**Overthinking Manifests as Divergent Loss Despite Convergent Features**: High confidence. Feature-vs-loss convergence analysis provides compelling mechanistic support.

## Next Checks

1. **Correlation Strength Analysis**: Systematically measure and report the correlation coefficient between auxiliary and main task accuracy curves across all 15 corruption types and 5 severity levels on CIFAR-C. Include per-class analysis to quantify the isotropic object failure cases mentioned in the paper.

2. **Architectural Ablation Study**: Create variants of Conv-LiGRU isolating each modification (ReLU vs tanh, batch normalization presence, reset gate removal). Compare their overthinking behavior and accuracy profiles to determine which components are essential for the claimed improvements.

3. **Computational Efficiency Benchmark**: Measure wall-clock inference time for the proposed method versus ACT and fixed-iteration baselines on the same hardware. Report the accuracy vs. compute trade-off curve to quantify whether the accuracy gains justify the additional computation.