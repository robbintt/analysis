---
ver: rpa2
title: 'ChartComplete: A Taxonomy-based Inclusive Chart Dataset'
arxiv_id: '2601.10462'
source_url: https://arxiv.org/abs/2601.10462
tags:
- chart
- charts
- images
- plot
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChartComplete is a taxonomy-based dataset designed to benchmark
  multi-modal large language models (MLLMs) on chart understanding tasks. The dataset
  covers 30 different chart types, including common ones like bar and line charts
  as well as less common ones like choropleth maps and parallel coordinates charts.
---

# ChartComplete: A Taxonomy-based Inclusive Chart Dataset

## Quick Facts
- arXiv ID: 2601.10462
- Source URL: https://arxiv.org/abs/2601.10462
- Reference count: 14
- Primary result: Taxonomy-based dataset with 30 chart types, 1,500 total images, designed for MLLM benchmarking

## Executive Summary
ChartComplete is a benchmark dataset designed to evaluate multi-modal large language models on chart understanding tasks. The dataset covers 30 different chart types, including common ones like bar and line charts as well as less common ones like choropleth maps and parallel coordinates charts. Chart images were collected from three sources: Statista, Our World in Data, and various online sources, resulting in a total of 1,500 chart images. The collection process involved both manual and automated scraping methods, with 63.4% of images collected manually and 36.6% scraped. Images were collected according to strict quality guidelines to ensure high resolution, complete information, and real-life relevance. The dataset is intended to serve as a comprehensive benchmark for MLLMs, with future work planned to include learning signals for tasks such as summarization and question answering.

## Method Summary
The dataset construction began with adapting Borkin's visualization taxonomy to define 30 chart types across 12 categories, then collecting exactly 50 images per type. The collection pipeline used Google ViT for feature extraction and FAISS for similarity search to retrieve taxonomically consistent images from large unlabeled pools. Manual collection supplemented automated scraping when needed. Quality guidelines enforced minimum resolution (>300px), readable fonts, complete labels/axes/legends, and real-life relevance. The final dataset contains 1,500 chart images organized by chart type in a structured directory format.

## Key Results
- 30 chart types across 12 categories, covering both common (bar, line) and rare (choropleth, parallel coordinates) visualizations
- 1,500 total images with exactly 50 per chart type
- 63.4% manual collection, 36.6% scraped from Statista and Our World in Data
- Quality guidelines ensured high-resolution, complete, and real-world relevant chart images

## Why This Works (Mechanism)

### Mechanism 1: Taxonomy-Grounded Chart Type Coverage
Grounding dataset construction in an established visualization taxonomy ensures systematic coverage beyond the limited chart types in existing benchmarks. The authors adapted Borkin's visualization taxonomy to define 30 chart types across 12 categories, then collected exactly 50 images per type. This prevents the "common chart bias" seen in prior datasets. The core assumption is that taxonomic comprehensiveness translates to better benchmark coverage of real-world chart diversity.

### Mechanism 2: Vision Transformer–Guided Semi-Automated Retrieval
Pre-trained ViT features combined with nearest-neighbor search enable scalable retrieval of taxonomically consistent images from large unlabeled pools. Google ViT extracts visual embeddings → FAISS indexes embeddings → for each target chart type, sample a seed image → retrieve k=100 nearest neighbors → manually filter correct matches → repeat until 50 images collected. The core assumption is that ViT embeddings capture sufficient visual similarity that charts of the same type cluster together in feature space.

### Mechanism 3: Iterative Quality-Guided Collection with Human Review
Explicit quality guidelines combined with iterative human review and version control produce benchmark-grade chart images with complete, interpretable information. Define quality criteria (resolution >300px, readable fonts, complete labels/axes/legends, no prior-knowledge symbols, balanced complexity) → distribute collection tasks → manual review before merge → reject non-compliant images → final dataset-wide pass for duplicates and meaningful-question testability. The core assumption is that high-quality, information-complete chart images are prerequisites for meaningful MLLM evaluation.

## Foundational Learning

- **Concept: Visualization Taxonomy (Borkin et al.)**
  - Why needed here: The dataset's structure and 30 chart types are derived directly from this taxonomy; understanding the categories is essential for proper usage.
  - Quick check question: Can you explain the difference between the "vanilla" and "stacked" variants introduced by the authors for area, line, scatter, and bubble charts?

- **Concept: Vision Transformer (ViT) Feature Extraction**
  - Why needed here: The collection pipeline depends on ViT embeddings for similarity search; understanding their limitations explains observed cluster imperfections.
  - Quick check question: Why might ViT features cause area charts to cluster near circle charts in the T-SNE projection, and what does this suggest about the model's visual representations?

- **Concept: Centered Kernel Alignment (CKA)**
  - Why needed here: The authors use CKA to quantify feature similarity across chart types; this is relevant for evaluating whether chart types are visually discriminable.
  - Quick check question: What would a high CKA value between two chart types (e.g., stacked bar and grouped bar) imply about their feature representations, and why might this matter for classification?

## Architecture Onboarding

- **Component map:**
  Data Sources (Statista, Our World in Data, Web) → Scraping + Manual Collection → Google ViT Feature Extraction (vit-base-patch16-224) → FAISS Similarity Index → Per-Type Retrieval (k=100 neighbors) + Manual Filtering → Quality Guidelines Enforcement (iterative) → Storage: /category/chart_type/ (50 images each)

- **Critical path:**
  1. Scrape candidate images from Statista (12,635) and Our World in Data (4,113)
  2. Extract ViT features and build FAISS index
  3. For each of 30 chart types: seed → retrieve → manually filter → iterate to 50 images
  4. Fill gaps (18 types) via manual collection with quality guidelines
  5. Final quality pass: deduplication + meaningful-question filter

- **Design tradeoffs:**
  - Real-world vs. synthetic: Chose real-world for ecological validity; sacrificed label control and uniform distribution
  - Manual vs. automated: 63.4% manual prioritizes quality over scale
  - Breadth vs. depth: 50 images × 30 types emphasizes taxonomic coverage over per-type statistical power
  - No learning signals: Current release is classification-only; lacks QA/summarization labels (explicitly noted as future work)

- **Failure signatures:**
  - ViT cluster confusion: Area charts appearing near circle charts; diagram charts near area charts (Figure 1)
  - Scraped-only coverage: Only 8 of 30 types were fully scrapable; 18 required pure manual collection
  - Non-uniform image dimensions: Only constraint was >300px; actual distribution varies widely (Figure 3)
  - Missing learning signals: Dataset cannot directly support training for QA or summarization tasks

- **First 3 experiments:**
  1. **30-way chart classification baseline:** Train a linear classifier on frozen ViT features to quantify intrinsic separability of the 30 chart types and identify confusable pairs.
  2. **Cross-encoder feature analysis:** Compare CKA matrices across multiple vision encoders (ViT, CLIP, SigLIP) to determine if specialized chart-trained encoders would improve retrieval precision.
  3. **Zero-shot MLLM probing:** Evaluate existing chart-capable MLLMs on ChartComplete using a standardized classification prompt to establish baseline performance gaps relative to existing benchmarks (ChartQA, FigureQA).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the addition of task-specific learning signals (e.g., question-answering pairs, summarization annotations) affect the utility of ChartComplete for training and evaluating multi-modal large language models?
- Basis in paper: The authors state: "A current limitation of ChartComplete is that it serves solely as a dataset of classified chart images. It presently lacks a direct training signal, such as those used for summarization or question answering tasks. We plan to address this in future work."
- Why unresolved: The dataset currently contains only chart images with type labels, without annotations for downstream tasks that would enable supervised learning.
- What evidence would resolve it: Comparative benchmark results showing MLLM performance on chart understanding tasks before and after adding QA pairs, summarization labels, or other learning signals to the dataset.

### Open Question 2
- Question: Can a chart-specialized vision model outperform general-purpose vision transformers (e.g., Google ViT) in extracting discriminative features for the 30 chart types in ChartComplete?
- Basis in paper: The authors note: "This suggests that the dataset would benefit from a more specialized feature extraction model" after observing that some area charts clustered near circle charts in their ViT-based T-SNE visualization.
- Why unresolved: The current analysis uses a general-purpose ViT model, and the observed clustering imperfections indicate domain-specific features may not be captured optimally.
- What evidence would resolve it: A comparative study showing improved cluster separation (e.g., via T-SNE visualization, CKA metrics, or classification accuracy) when using a model pre-trained specifically on chart images versus a general-purpose vision transformer.

### Open Question 3
- Question: Is 50 images per chart type sufficient for robust evaluation and fine-tuning of multi-modal models, or does performance scale significantly with larger per-class sample sizes?
- Basis in paper: The paper collects exactly 50 images per chart type without justification for this sample size, and no experiments are conducted to validate whether this quantity is adequate for benchmarking or training purposes.
- Why unresolved: The authors do not provide empirical evidence that 50 samples per class captures sufficient intra-class variability or supports reliable model evaluation.
- What evidence would resolve it: Learning curves or performance saturation analysis showing whether model accuracy stabilizes at 50 samples per class or continues improving with additional data for each chart type.

## Limitations

- The ViT-based retrieval pipeline shows measurable confusion between visually similar chart types (area charts clustering near circle charts), suggesting the feature space may not be optimally discriminative.
- The 50-image-per-type design provides adequate coverage but limited statistical power for fine-grained evaluation.
- The dataset currently lacks learning signals (labels for QA or summarization), limiting its use to classification benchmarks only.

## Confidence

- **High Confidence**: The taxonomic coverage mechanism (30 chart types from established visualization taxonomy) and quality guideline enforcement are well-documented and theoretically sound.
- **Medium Confidence**: The ViT-FAISS retrieval mechanism is implemented as described but shows inherent limitations in visual discrimination; effectiveness depends on feature quality rather than explicit validation.
- **Low Confidence**: Claims about real-world ecological validity are not directly tested against actual chart distributions in scientific literature or news media.

## Next Checks

1. **Retrieval Precision Analysis**: Run the ViT-FAISS pipeline on a held-out test set to quantify false-positive rates in chart type retrieval and identify specific confusable pairs.
2. **MLLM Performance Gap Analysis**: Evaluate 3-5 existing chart-capable MLLMs on ChartComplete to measure baseline performance and identify whether taxonomic breadth translates to evaluation difficulty.
3. **Cross-Encoder Feature Comparison**: Compare CKA similarity matrices across multiple vision encoders (ViT, CLIP, SigLIP) to determine if specialized chart-trained models would improve retrieval precision and reduce type confusion.