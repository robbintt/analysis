---
ver: rpa2
title: 'Information Gravity: A Field-Theoretic Model for Token Selection in Large
  Language Models'
arxiv_id: '2504.20951'
source_url: https://arxiv.org/abs/2504.20951
tags:
- information
- semantic
- query
- generation
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "information gravity," a field-theoretic
  model for token selection in large language models (LLMs). The model draws an analogy
  between physical gravity and text generation, where user queries act as objects
  with "information mass" that curve the model's semantic space, creating "gravitational
  wells" that attract tokens during generation.
---

# Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models

## Quick Facts
- **arXiv ID**: 2504.20951
- **Source URL**: https://arxiv.org/abs/2504.20951
- **Reference count**: 13
- **Primary result**: Introduces "information gravity," a field-theoretic model where user queries curve semantic space, creating gravitational wells that attract tokens during LLM generation

## Executive Summary
This paper introduces "information gravity," a field-theoretic framework for understanding token selection in large language models. The model treats user queries as objects with "information mass" that curve the model's semantic space, creating gravitational wells that probabilistically attract token selection. Information mass combines query entropy, context depth, and novelty, while semantic potential defines energy landscapes where generation follows gradient flow. The framework explains phenomena including hallucinations (semantic voids), prompt sensitivity (field curvature), and temperature effects (thermodynamic balance). The paper proposes experimental validation through semantic landscape visualization, information mass-hallucination correlation studies, and temperature scaling analysis.

## Method Summary
The method treats LLM generation as gradient descent on a semantic potential field Φ(t,Q) = -log P(t|Q), where user queries curve semantic space through their "information mass" M(Q) = α·H(Q) + β·D(Q) + γ·N(Q). Generation follows the information gravity gradient g(t) = -∇Φ(t), analogous to gravitational attraction. Temperature T scales the Boltzmann-like selection probability P(t|Q) = exp(-Φ(t)/T)/Z, balancing deterministic "gravitational capture" against random "thermal motion." The framework predicts that high information mass queries create complex potential landscapes with multiple local minima, leading to hallucinations when the system falls into semantic voids.

## Key Results
- Information mass M(Q) = α·H(Q) + β·D(Q) + γ·N(Q) formally quantifies a query's ability to curve semantic space
- Semantic potential Φ(t,Q) = -log P(t|Q) defines energy landscapes where token selection follows gradient flow
- Temperature T modulates between gravitational capture (low T) and thermal motion (high T) in semantic space

## Why This Works (Mechanism)

### Mechanism 1: Semantic Potential as Token Attractor
User queries with high information mass curve the model's semantic space, creating gravitational wells that probabilistically attract token selection. The query's information mass M(Q) determines curvature, while semantic potential Φ(t,Q) = -log P(t|Q) defines energy landscapes. The gradient g(t) = -∇Φ(t) acts as an information gravity vector pointing toward highest-probability tokens. This assumes the mathematical analogy between token probability and gravitational potential captures real causal structure in LLM generation dynamics.

### Mechanism 2: Hallucinations via Semantic Voids
Hallucinations emerge when generation encounters semantic voids—sparse training data regions where the model falls into false local minima. High information mass queries create complex potential landscapes with multiple local minima. When the Hessian of semantic potential has high eigenvalues in regions where P(t) ≪ 1, the system becomes trapped in weakly-connected tokens, producing plausible but incorrect content. This assumes hallucination rate correlates positively with information mass components, particularly novelty and entropy.

### Mechanism 3: Temperature as Thermodynamic Regularizer
Sampling temperature regulates the balance between gravitational attraction to potential minima and thermal exploration of semantic space. The Boltzmann-like distribution P(t|Q) = exp(-Φ(t)/T)/Z means low T causes rigid minimum-potential selection (deterministic), while high T weakens gravitational influence (randomized). This assumes the thermodynamic analogy captures real constraints on generation diversity, though temperature effects are empirically documented while the field-theoretic explanation requires validation.

## Foundational Learning

- **Concept: Energy-Based Models (EBMs)**
  - **Why needed here:** The semantic potential Φ(t,Q) = -log P(t|Q) directly maps to energy functions in EBMs. Understanding energy landscapes, partition functions, and Boltzmann distributions is essential for grasping why temperature affects token selection.
  - **Quick check question:** Can you explain why minimizing energy in an EBM corresponds to maximizing probability?

- **Concept: Shannon Entropy and KL Divergence**
  - **Why needed here:** Information mass components rely on entropy H(Q) for uncertainty quantification and KL divergence for novelty N(Q). Without this foundation, the curvature metaphor cannot be operationalized.
  - **Quick check question:** Given two token distributions P and Q, what does D_KL(P||Q) > 1 imply about their relationship?

- **Concept: Gradient Descent in High-Dimensional Spaces**
  - **Why needed here:** Information gravity g(t) = -∇Φ(t) frames generation as gradient descent on semantic potential. Local minima, saddle points, and curvature (Hessian eigenvalues) explain hallucinations and prompt sensitivity.
  - **Quick check question:** Why do high-dimensional optimization landscapes have more saddle points than local minima?

## Architecture Onboarding

- **Component map:** Query → Information mass calculator → Semantic potential field → Gradient computation → Temperature-scaled token selection → Context update → Field reconfiguration
- **Critical path:** Query → Information mass calculation → Semantic potential field deformation → Gradient computation → Temperature-scaled token selection → Context update → Field reconfiguration for next token
- **Design tradeoffs:**
  - Precision vs. interpretability: Computing exact information mass requires model internals access; approximations trade accuracy for practicality
  - Unified vs. component analysis: Treating M(Q) as single metric vs. analyzing α, β, γ coefficients separately for different hallucination types
  - Continuous approximation vs. discrete tokens: Embeddings enable continuous field math; actual token space is discrete—this is resolved by operating in embedding space
- **Failure signatures:**
  - High curvature + low temperature → Repetitive, stuck outputs (gravitational capture)
  - High novelty + low temperature → Hallucinations in semantic voids
  - Low entropy + high temperature → Wandering, incoherent generation despite clear query intent
  - Deep context + multiple local minima → Inconsistent responses to minor prompt variations
- **First 3 experiments:**
  1. **Semantic landscape visualization:** For fixed query sets, extract pre-softmax logits, apply t-SNE/UMAP, visualize potential wells. Confirm that high-M(Q) queries create deeper, more complex wells.
  2. **Information mass–hallucination correlation:** Create queries with controlled entropy/novelty variations. Measure hallucination rates via fact-checking against ground truth. Validate: high M(Q) → high hallucination frequency, especially at low T.
  3. **Temperature scaling validation:** For fixed queries, sweep T from 0.1 to 2.0, measure output entropy and diversity. Confirm predicted scaling: H should increase monotonically toward log(|V|). Identify "gravitational capture" regime at low T.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does a high information mass $M(Q)$, particularly involving high novelty and entropy, empirically correlate with an increased frequency of hallucinations as predicted by the semantic void mechanism?
- **Basis in paper:** Section 5.2 proposes an experiment to "analyze the correlation between information mass components and hallucination metrics."
- **Why unresolved:** The paper establishes the theoretical mechanism (semantic voids) but describes the experimental verification as future work.
- **What evidence would resolve it:** A dataset of queries with controlled information mass components showing a statistically significant positive correlation with hallucination rates in generated outputs.

### Open Question 2
- **Question:** Can the three components of information mass—entropy $H(Q)$, context depth $D(Q)$, and novelty $N(Q)$—be effectively isolated to determine the specific weighting coefficients ($\alpha, \beta, \gamma$)?
- **Basis in paper:** Section 4.4 lists "Measuring the components of information mass is complicated by their internal interrelationship" as a key limitation.
- **Why unresolved:** While the paper provides formulas for each component, it acknowledges that measuring them independently is complex and requires separate validation experiments.
- **What evidence would resolve it:** Experimental results demonstrating that varying one component (e.g., novelty) while holding others constant produces a predictable, independent shift in the semantic field curvature.

### Open Question 3
- **Question:** How do the distinct semantic fields generated by multiple attention layers in a transformer interact to form the final "gravitational" configuration?
- **Basis in paper:** Section 7.2 lists "formalization of complex interaction of information fields between different attention layers" as a necessary step for theoretical development.
- **Why unresolved:** The model currently treats the information field as a unified potential, simplifying the nested, multi-level nature of transformer attention layers.
- **What evidence would resolve it:** A formal extension of the model mapping the contribution of individual attention heads to the aggregate semantic potential $\Phi(t, Q)$.

## Limitations

- **Empirical validation gap:** The paper presents a compelling theoretical framework but lacks empirical validation. The three proposed verification experiments remain untested, and critical parameters like α, β, γ coefficients for information mass calculation are not calibrated.
- **Training distribution access:** Computing novelty via KL divergence requires access to the model's training distribution P_train(Q), which is unavailable for proprietary models. This blocks direct computation of the N(Q) component of information mass.
- **Field visualization challenges:** The proposed semantic landscape visualizations depend on dimensionality reduction techniques (t-SNE, UMAP) that are known to produce misleading structures, especially in high-dimensional spaces.

## Confidence

**Medium Confidence:** The core field-theoretic framework for explaining token selection dynamics, hallucination emergence, and temperature effects. The mathematical analogies are internally consistent and grounded in established physics concepts, but require empirical validation to confirm they capture actual LLM behavior rather than serving as post-hoc metaphors.

**Low Confidence:** The hallucination mechanism via semantic voids. While conceptually appealing, the causal link between information mass components and hallucination frequency is speculative without controlled experiments. The model assumes hallucinations occur when information mass is high and local knowledge is insufficient, but this relationship may be more complex or mediated by other factors.

**Medium Confidence:** The temperature-thermodynamic relationship. Temperature effects on output diversity are well-documented empirically, but the field-theoretic explanation requires validation. The Boltzmann-like distribution is correct, but whether this constitutes "thermal motion" in a meaningful sense remains unclear.

## Next Checks

**Check 1: Semantic Landscape Reproducibility**
For 20 diverse queries spanning low to high information mass, extract pre-softmax logits from an open LLM, apply UMAP/t-SNE, and visualize semantic potential landscapes. Confirm: (1) high-M(Q) queries produce deeper, more complex wells; (2) the structures are reproducible across runs and reduction methods; (3) token selection patterns follow gradient flow predictions.

**Check 2: Information Mass–Hallucination Correlation**
Construct 50 queries with controlled variations in entropy (using polysemous vs. unambiguous prompts), context depth (single-turn vs. multi-turn), and novelty (domain-specific queries). Generate responses at T=0.5 and T=1.0. Measure hallucination rates via fact-checking against ground truth. Test: (1) hallucination frequency increases with information mass; (2) the effect is stronger at lower temperatures; (3) individual components (entropy, depth, novelty) contribute differently to hallucination types.

**Check 3: Temperature Scaling Validation**
For fixed queries, sweep temperature T ∈ {0.1, 0.5, 1.0, 1.5, 2.0}, measure output entropy and diversity metrics (unique tokens, response length variation). Confirm: (1) entropy increases monotonically with T; (2) reaches log(|V|) at high T; (3) identifies temperature regimes where "gravitational capture" (low diversity) transitions to "thermal motion" (high diversity).