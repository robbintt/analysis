---
ver: rpa2
title: Learning "Partner-Aware" Collaborators in Multi-Party Collaboration
arxiv_id: '2510.22462'
source_url: https://arxiv.org/abs/2510.22462
tags:
- intervention
- collaborator
- agent
- task
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ICR, a method for training LLM collaborator
  agents that can intelligently respond to interventions in multi-party collaborative
  tasks. The key idea is to use counterfactual invariance regularization to make agents
  robust to potentially misleading interventions while still incorporating helpful
  ones.
---

# Learning "Partner-Aware" Collaborators in Multi-Party Collaboration

## Quick Facts
- arXiv ID: 2510.22462
- Source URL: https://arxiv.org/abs/2510.22462
- Reference count: 40
- Primary result: ICR-trained agents outperform baselines in task accuracy and common ground convergence across language-rich and language-free collaborative tasks

## Executive Summary
This paper addresses the challenge of training LLM collaborator agents that can intelligently respond to interventions in multi-party collaborative tasks. The key insight is that standard RLHF methods make agents follow intervention patterns from training data rather than strategically evaluating whether interventions actually help. The proposed ICR method uses counterfactual invariance regularization to make agents robust to misleading interventions while still incorporating helpful ones. Experiments on Wason Card Selection and Weights Tasks show ICR achieves 72.4% accuracy versus 47.6% for PPO, with better common ground convergence, demonstrating improved ability to distinguish between helpful and misleading interventions.

## Method Summary
The method trains LLM collaborator agents using PPO with an additional counterfactual KL regularization term. Expert MAMDP trajectories are collected via GPT-4o roleplaying both intervener and collaborator agents over 15 turns. The counterfactual policy is computed by adding a prompt prefix stating the intervention won't improve performance, then computing KL divergence between factual and counterfactual policies. Training uses LoRA fine-tuning on Llama-3-8B-Instruct with λ_Intent=0.2. The approach aims to make agents selectively incorporate task-relevant information while resisting spurious intervention influence.

## Key Results
- ICR achieves 72.4% accuracy on DeliData full-press vs. 47.6% for PPO baseline
- ICR converges to common ground more effectively (CG=3.35 vs. PPO=2.94) without explicit CG training
- Performance gains are consistent across language-rich (DeliData) and language-free (Weights Task) settings
- Ablation shows λ_Intent=0.2 is optimal; higher values sacrifice task utility

## Why This Works (Mechanism)

### Mechanism 1
Counterfactual invariance regularization enables agents to distinguish task-relevant reasoning from spurious intervention influence. During PPO training, ICR computes KL divergence between the factual policy and a counterfactual policy where a prompt prefix states the intervention won't improve performance. This forces the agent to maintain consistent action likelihoods regardless of whether it believes the intervention is helpful, learning which behaviors are grounded in task logic versus surface-level suggestion following.

### Mechanism 2
Standard preference-aligned collaborators are Bellman-optimal in the underlying MDP but suboptimal in the Modified-Action MDP because they fail to strategically evaluate intervention quality. The MAMDP framework models interventions as actions from a separate agent that modify the collaborator's state. Standard RLHF/DPO methods treat interventions as static context features rather than causally-influential actions, creating an optimality gap where policies learn to follow rewarded intervention patterns rather than developing discriminative evaluation.

### Mechanism 3
Common ground convergence emerges as a byproduct of counterfactual invariance training even without explicit CG-based rewards. By regularizing against counterfactual scenarios where interventions are non-informative, the agent learns to identify which intervention content is genuinely task-relevant versus noise. This learned discrimination generalizes to novel interventions at test time, manifesting as common ground convergence through selective incorporation of helpful suggestions.

## Foundational Learning

- **Markov Decision Processes and Bellman Optimality**
  - Why needed here: The theoretical contribution hinges on showing standard RL methods achieve Bellman optimality in the wrong problem formulation (underlying MDP vs. MAMDP). Understanding Q-functions and why optimality in one formulation doesn't transfer is essential.
  - Quick check: If a policy π is Bellman-optimal for MDP M, is it necessarily optimal for MDP M' where state transitions include an additional agent's actions?

- **Direct Preference Optimization (DPO) and IPO as Soft Q-Learning**
  - Why needed here: Lemma 3.1 reinterprets preference-aligned policies as softmax distributions over implicit Q-functions, connecting LLM alignment to classical RL theory. This framing enables the suboptimality proof.
  - Quick check: What implicit reward function does DPO optimize, and how does it relate to the Bellman equation?

- **Counterfactual Reasoning in Causal Inference**
  - Why needed here: The method constructs counterfactual states that neutralize intervention influence while preserving task-relevant information. This requires understanding Pearl's do-calculus framework and the difference between observational and interventional distributions.
  - Quick check: If you intervene to set X=x, what changes about the joint distribution P(X,Y,Z)?

## Architecture Onboarding

- **Component map**: Expert Data Collection Pipeline -> BC Reference Model -> PPO Training Loop -> Counterfactual Policy Computation -> Reward Model (full-press only)
- **Critical path**: 1. Bootstrap expert data via GPT-4o roleplay -> 2. Train BC reference policy on expert responses -> 3. Initialize policy from BC -> 4. PPO with counterfactual KL regularization -> 5. Evaluate in MAMDP setting with fixed GPT-4o intervention agent
- **Design tradeoffs**: LoRA vs. full fine-tuning (paper uses LoRA for efficiency); λ_Intent tuning (0.2 optimal); decentralized vs. centralized training (paper uses decentralized for scalability)
- **Failure signatures**: Reward hacking on consensus signal; counterfactual prompt ignored by base LLM; over-regularization causing policy to ignore helpful interventions; distribution shift from expert data not covering test-time intervention patterns
- **First 3 experiments**:
  1. Replicate no-press DeliData baseline comparison: Train BC, DPO, PPO, and ICR on expert data; evaluate accuracy and CG over 100 dialogues with fixed GPT-4o intervention agent
  2. Ablate λ_Intent values: Train ICR with λ_Intent ∈ {0.01, 0.2, 1.0} and track proxy reward over training steps
  3. Test intervention agent swap: Evaluate ICR-trained collaborators paired with Llama-3-8B-Instruct intervention agent instead of GPT-4o

## Open Questions the Paper Calls Out
- Does the "partner-aware" capability transfer to human collaborators, whose interventions may be noisier or structurally different from the GPT-4o agent used in training?
- Can ICR agents function effectively in ad-hoc teamwork settings with agents trained via different paradigms, or do they overfit to the specific intervention policy used during training?
- Can the counterfactual invariance objective protect agents from strategic deception and lying in high-stakes environments?

## Limitations
- The counterfactual invariance mechanism critically depends on the base LLM's instruction-following capability; if the counterfactual prompt prefix is ignored, the KL regularization provides no gradient signal
- The theoretical optimality gap assumes deterministic interventions and discrete state/action spaces, which may not hold for LLM-generated dialogue
- Expert data generation using GPT-4o may not capture the full diversity of human intervention patterns, potentially limiting generalization

## Confidence
- High confidence: Empirical improvements in task accuracy and common ground convergence are well-supported by Table 1 results with clear statistical patterns
- Medium confidence: The mechanism by which counterfactual invariance enables partner-aware behavior is theoretically sound but relies on assumptions about LLM prompt sensitivity
- Low confidence: The claim that ICR achieves partner-aware behavior without explicitly training for common ground is indirect; the causal link needs more direct testing

## Next Checks
1. **Counterfactual prompt sensitivity test**: Systematically vary the counterfactual prompt prefix strength and measure KL divergence magnitude between factual and counterfactual policies
2. **Intervention agent ablation study**: Evaluate ICR-trained collaborators with diverse intervention agents (GPT-4o, Claude-3, Llama-3-8B) and measure degradation patterns
3. **Gradient flow analysis**: Compute and visualize the counterfactual KL gradient contributions during training to verify meaningful gradients flow through the counterfactual branch