---
ver: rpa2
title: 'CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography'
arxiv_id: '2504.10090'
source_url: https://arxiv.org/abs/2504.10090
tags:
- aperture
- visual
- camera
- reasoning
- focal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CameraBench, a benchmark designed to evaluate
  visual reasoning capabilities in multimodal large language models (MLLMs) through
  photography-related tasks. The benchmark challenges models to identify numerical
  camera settings from images, requiring both visual comprehension and understanding
  of underlying physics.
---

# CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography

## Quick Facts
- arXiv ID: 2504.10090
- Source URL: https://arxiv.org/abs/2504.10090
- Authors: I-Sheng Fang; Jun-Cheng Chen
- Reference count: 22
- Key outcome: This work introduces CameraBench, a benchmark designed to evaluate visual reasoning capabilities in multimodal large language models (MLLMs) through photography-related tasks. The benchmark challenges models to identify numerical camera settings from images, requiring both visual comprehension and understanding of underlying physics. CameraBench extends previous work on vision-language models by testing whether MLLMs can distinguish visual differences across numerical camera settings. Using 100 samples from CameraSettings25K, the authors evaluate several MLLMs on binary-choice and five-choice tasks involving different camera parameters. Results show that while some MLLMs perform well on easier tasks, they struggle with more challenging visual reasoning tasks requiring identification of single parameter differences. Reasoning-focused MLLMs outperform standard versions, highlighting the importance of reasoning capabilities. No single MLLM consistently dominates across all tasks, demonstrating ongoing challenges in developing MLLMs with better visual reasoning capabilities for practical applications like photography assistant agents.

## Executive Summary
This paper introduces CameraBench, a novel benchmark for evaluating multimodal large language models' (MLLMs) visual reasoning capabilities through photography tasks. The benchmark requires models to identify camera settings from images, testing whether MLLMs can understand the physics underlying visual effects and apply this knowledge to numerical inference. The authors evaluate multiple MLLMs on 100 samples from the CameraSettings25K dataset, finding that while models perform reasonably well on tasks where all settings differ, they struggle significantly with single-parameter discrimination tasks. Reasoning-focused MLLMs like Gemini 2.0 Flash Thinking and OpenAI o1 outperform standard models, demonstrating the importance of reasoning capabilities for this type of visual inference.

## Method Summary
The authors created CameraBench using 100 samples from the CameraSettings25K dataset, which contains 25,127 images with EXIF metadata. They generated six tasks: binary-choice tasks where all four camera parameters differ, and four binary-choice tasks where only one parameter (focal length, aperture, ISO, or exposure time) differs. Five-choice tasks were also included. MLLMs were prompted to select the most probable settings from shuffled options, with temperature=0.1 and Top-P=0.1 to reduce randomness. The evaluation measured accuracy per task type, with random guess baselines of 0.5 for binary-choice and 0.2 for five-choice tasks.

## Key Results
- Most MLLMs perform near random guessing (0.44-0.52 accuracy) on single-parameter binary tasks, while achieving 0.85+ on "all different" tasks
- Reasoning-focused models (Gemini 2.0 Flash Thinking, OpenAI o1) consistently outperform standard MLLMs across all task types
- No single MLLM dominates across all tasks, with different models showing strengths in different areas
- Models frequently hallucinate domain knowledge or provide contradictory explanations, even when selecting correct answers
- The benchmark reveals significant gaps in MLLMs' ability to perform fine-grained visual discrimination and attribute-level reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Photography tasks require integrating implicit physics knowledge with visual comprehension to reason about numerical camera settings.
- Mechanism: Camera parameters (aperture, ISO, focal length, exposure time) produce observable visual effects (brightness, bokeh, depth of field, motion blur). Models must reverse-engineer these effects to infer settings—binding visual perception to causal physical understanding.
- Core assumption: The visual signatures of camera settings are learnable and discriminable from 2D images alone.
- Evidence anchors:
  - [abstract] "Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension."
  - [section 1] "Increasing the aperture size, ISO speed, or exposure time leads to a brighter image... A larger aperture or longer focal length enhances the bokeh effect."
  - [corpus] Weak direct support; neighbor papers focus on other reasoning domains (transit maps, symbolic music). No corpus paper validates the physics-to-visual mapping mechanism.
- Break condition: If visual effects are confounded by post-processing or scene variables (e.g., unknown lighting), inference becomes under-constrained.

### Mechanism 2
- Claim: Reasoning-augmented MLLMs outperform standard MLLMs on tasks requiring multi-step visual inference.
- Mechanism: Models with explicit reasoning traces (e.g., OpenAI o1, Gemini 2.0 Flash Thinking) decompose the problem—analyzing scene characteristics, ruling out implausible settings, and synthesizing conclusions—rather than pattern-matching to shallow heuristics.
- Core assumption: The reasoning process itself, not just model scale or training data, drives performance gains on structured inference tasks.
- Evidence anchors:
  - [abstract] "The reasoning-focused models (Gemini 2.0 Flash Thinking Experimental and OpenAI o1) generally outperform standard versions, highlighting the importance of reasoning capabilities."
  - [section 3, Table 3] Gemini 2.0 Flash Thinking provides detailed step-by-step analysis; Llama 3.2 11B hallucinates ("5mm focal length is standard for landscape photography") and gives contradictory explanations.
  - [corpus] WildScore and SO-Bench similarly show structured reasoning benchmarks reveal gaps in MLLM capabilities, supporting the general pattern.
- Break condition: If reasoning traces are post-hoc rationalizations rather than causal drivers, improvements may not generalize to out-of-distribution tasks.

### Mechanism 3
- Claim: Single-parameter discrimination is substantially harder than all-parameter comparison for current MLLMs.
- Mechanism: When all settings differ, models can leverage multiple visual cues holistically. When only one parameter varies, models must isolate subtle effects (e.g., slight brightness changes from ISO alone) while ignoring confounds—requiring finer-grained visual discrimination.
- Core assumption: The visual differences from single-parameter changes are detectable but subtle, and models lack precision in attribute-level visual reasoning.
- Evidence anchors:
  - [section 1, Figure 1] Radar chart shows most models near 0.5 (random) on focal length, aperture, ISO, and exposure time single-parameter tasks, while "All" task reaches 0.85+ for top models.
  - [section 3] "In Task (2), (3), (4), and (5), most models perform close to random guessing."
  - [corpus] Forgotten Polygons (arXiv:2502.15969) similarly finds MLLMs struggle with fine-grained visual-mathematical reasoning, consistent with attribute-level deficits.
- Break condition: If single-parameter tasks are inherently ambiguous even for humans (e.g., ISO vs. exposure time both affect brightness), benchmark may conflate model limitations with task under-specification.

## Foundational Learning

- Concept: **Exposure triangle** (aperture, ISO, shutter speed interaction)
  - Why needed here: Understanding how these parameters jointly determine image brightness and visual characteristics is prerequisite to reverse-inference tasks.
  - Quick check question: Given a dark indoor photo at f/2.8, ISO 100, 1/1000s, which single change would brighten it most: increasing ISO to 800, or slowing shutter to 1/125s?

- Concept: **Depth of field and bokeh**
  - Why needed here: Aperture and focal length produce blur characteristics that models must recognize to infer settings.
  - Quick check question: A portrait has sharp subject and creamy background blur. Would this more likely be shot at f/1.8 or f/16?

- Concept: **EXIF metadata and normalization**
  - Why needed here: CameraBench normalizes settings to full-frame equivalent; understanding crop factors prevents misinterpreting raw EXIF values.
  - Quick check question: A 25mm lens on an APS-C sensor (1.5x crop) is equivalent to what focal length on full-frame?

## Architecture Onboarding

- Component map:
  CameraSettings25K -> Task Generator -> Evaluation Harness -> Metrics Layer

- Critical path:
  1. Load 100-image sample with normalized settings
  2. Generate paired options (correct + sampled distractor)
  3. Shuffle option positions randomly
  4. Query MLLM with image + formatted prompt
  5. Parse **Answer**: (X) format; handle unparseable outputs by exclusion

- Design tradeoffs:
  - **Sample size (100)**: Faster iteration but higher variance; full 25K evaluation deferred to future work
  - **Temperature 0.1**: Reduces randomness but may suppress valid reasoning diversity
  - **EXIF normalization**: Enables cross-camera comparison but introduces preprocessing assumptions

- Failure signatures:
  - **Hallucinated domain knowledge**: Model states incorrect photography "rules" (e.g., "50mm is standard for landscapes" when 24-35mm is typical)
  - **Contradictory explanations**: Claims aperture is "smaller" when comparing f/4.5 to f/14.58 (larger f-number = smaller aperture)
  - **Unparseable outputs**: Model doesn't follow **Answer**: (X) format

- First 3 experiments:
  1. Reproduce Table 1 binary-choice results on the 100-image sample to validate pipeline correctness.
  2. Ablate reasoning: Compare Gemini 2.0 Flash vs. Flash Thinking on single-parameter tasks to quantify reasoning contribution.
  3. Analyze failure modes: Manually inspect cases where models correctly answer "All-different" but fail single-parameter versions of the same image to identify visual discrimination gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will scaling up the evaluation from 100 samples to the full CameraSettings25K dataset reveal different performance patterns or rankings among MLLMs?
- Basis in paper: [explicit] The authors state: "Future work will involve more extensive benchmarking and a comprehensive evaluation using the entire CameraSettings25K dataset across various MLLMs."
- Why unresolved: Resource limitations restricted current experiments to 100 randomly sampled images; statistical robustness of findings remains unconfirmed at scale.
- What evidence would resolve it: Running the same six tasks on the full 25,127 images and comparing whether model rankings and relative strengths (e.g., reasoning models outperforming standard ones) hold consistent.

### Open Question 2
- Question: Why do most MLLMs perform near random guessing when only a single camera parameter differs between options?
- Basis in paper: [inferred] Table 1 shows most models achieving 0.44–0.52 accuracy on single-parameter binary tasks (random = 0.50), yet significantly above random on the "All different" task.
- Why unresolved: The paper does not investigate whether this stems from visual feature limitations, insufficient physics understanding, or the entangled nature of camera parameters in real images.
- What evidence would resolve it: Ablation studies controlling for parameter magnitude of difference, or synthetic image experiments where only one parameter varies while others are held constant.

### Open Question 3
- Question: Can non-reasoning-focused MLLMs achieve comparable performance to reasoning models through chain-of-thought prompting or fine-tuning on photography tasks?
- Basis in paper: [explicit] The authors conclude that "reasoning-focused MLLMs (Gemini 2.0 Flash Thinking Experimental and OpenAI o1) outperforms the regular MLLMs... indicating the critical role of visual reasoning capability."
- Why unresolved: The study compares model families but does not isolate whether the performance gap is due to architecture, training data, or prompting strategies.
- What evidence would resolve it: Applying explicit chain-of-thought prompting to standard MLLMs (e.g., GPT-4o, Gemini 2.0 Flash) and measuring performance changes on CameraBench tasks.

## Limitations

- **Dataset scale**: Evaluation limited to 100 samples from CameraSettings25K due to resource constraints, potentially limiting statistical robustness
- **Public availability**: CameraSettings25K dataset and exact 100-image sample indices used are not publicly available, hindering faithful reproduction
- **Prompt sensitivity**: Results may be sensitive to prompt variations and exact wording beyond the provided examples

## Confidence

- **High Confidence**: The general methodology of using photography tasks to benchmark visual reasoning capabilities in MLLMs is well-founded. The observation that reasoning-augmented models (Gemini 2.0 Flash Thinking, OpenAI o1) outperform standard versions on structured inference tasks aligns with broader MLLM research patterns.
- **Medium Confidence**: The claim that single-parameter discrimination is substantially harder than all-parameter comparison for current MLLMs is supported by the data but may conflate model limitations with task under-specification in some cases.
- **Low Confidence**: The specific performance rankings of different MLLMs may vary significantly with different dataset samples or prompt variations due to the relatively small evaluation set (100 images).

## Next Checks

1. **Dataset Availability Verification**: Attempt to obtain or reconstruct the CameraSettings25K dataset and confirm the 100-image sample indices used in the evaluation.
2. **Prompt Template Reconstruction**: Reconstruct the complete prompt templates based on Figure 2 examples and test variations to assess sensitivity to prompt wording.
3. **Extended Evaluation**: Run the benchmark on a larger subset (e.g., 500-1000 images) to validate whether the observed performance patterns hold with increased statistical power and reduced variance.