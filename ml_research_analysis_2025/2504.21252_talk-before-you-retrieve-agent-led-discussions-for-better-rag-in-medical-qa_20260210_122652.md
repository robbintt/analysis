---
ver: rpa2
title: 'Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical
  QA'
arxiv_id: '2504.21252'
source_url: https://arxiv.org/abs/2504.21252
tags:
- medical
- retrieval
- snippets
- agent
- discuss-rag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving medical question
  answering (QA) with large language models (LLMs), which often suffer from hallucinations
  and outdated knowledge. The authors propose Discuss-RAG, an agent-led framework
  that enhances retrieval-augmented generation (RAG) by incorporating human-like reasoning
  behaviors.
---

# Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA

## Quick Facts
- arXiv ID: 2504.21252
- Source URL: https://arxiv.org/abs/2504.21252
- Authors: Xuanzhao Dong, Wenhui Zhu, Hao Wang, Xiwen Chen, Peijie Qiu, Rui Yin, Yi Su, Yalin Wang
- Reference count: 8
- Primary result: Accuracy improvements up to 16.67% on BioASQ and 12.20% on PubMedQA through multi-agent brainstorming and post-retrieval verification.

## Executive Summary
This paper addresses the challenge of improving medical question answering (QA) with large language models (LLMs), which often suffer from hallucinations and outdated knowledge. The authors propose Discuss-RAG, an agent-led framework that enhances retrieval-augmented generation (RAG) by incorporating human-like reasoning behaviors. Discuss-RAG features a multi-turn brainstorming module with specialized medical agents and a summarizer to generate context-rich insights, followed by a post-retrieval verification agent that evaluates the relevance of retrieved snippets. Experiments on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms baseline systems, demonstrating its effectiveness in improving both answer accuracy and snippet quality.

## Method Summary
Discuss-RAG introduces a two-phase framework: (1) Multi-turn brainstorming where a recruiter agent selects specialized medical experts who contribute insights through iterative discussion, with a summarizer agent distilling these into a final summary D; (2) Post-retrieval verification where a decision-maker agent evaluates the relevance of retrieved snippets and triggers fallback strategies if needed. The method is training-free and relies on prompting multiple LLM agents to emulate medical expert reasoning before and after retrieval.

## Key Results
- Achieved accuracy improvements up to 16.67% on BioASQ and 12.20% on PubMedQA compared to baseline systems
- Consistently outperformed baseline systems across all four benchmark medical QA datasets (MMLU-Med, MedQA-US, BioASQ, PubMedQA)
- Demonstrated improved snippet quality through qualitative analysis showing more topically relevant and better-organized retrieved content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent brainstorming before retrieval improves the relevance of retrieved documents by enriching queries with domain-specific context.
- Mechanism: A recruiter agent assembles specialized medical agents who contribute perspectives on what knowledge is needed. A summarizer agent synthesizes these into a distilled summary D, which is combined with the original query Q for retrieval.
- Core assumption: Agent personas can surface medically relevant context that raw similarity-based retrieval misses.
- Evidence anchors:
  - [abstract]: "a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content"
  - [section]: Figure 2(C) shows that using an agent-identified knowledge summary led to "snippets [that] were both more topically relevant and better organized"
  - [corpus]: RAR² and MedCoT-RAG similarly use thought-driven or chain-of-thought approaches to improve medical retrieval

### Mechanism 2
- Claim: Post-retrieval verification by a decision-maker agent reduces the impact of noisy or misleading snippets.
- Mechanism: After retrieval, a decision-maker agent evaluates top-k snippets for relevance and coherence. If rejected, a fallback strategy (e.g., CoT-based prompting) is triggered.
- Core assumption: An agent can reliably judge snippet relevance better than default similarity scores.
- Evidence anchors:
  - [abstract]: "a decision-making agent evaluates the retrieved snippets before their final integration"
  - [section]: Figure 3(B) shows verified snippets aligned with query intent
  - [corpus]: M-Eval addresses multi-evidence validation in medical RAG

### Mechanism 3
- Claim: Iterative, multi-turn discussion with summarization yields progressively refined retrieval context.
- Mechanism: Across m turns, agents contribute insights Iᵢʲ based on the previous summary Tʲ⁻¹. The summarizer generates Tʲ := fC(I₁ʲ, I₂ʲ, ..., Iₙʲ; Tʲ⁻¹, Q).
- Core assumption: Multiple rounds capture more comprehensive context than single-turn reasoning.
- Evidence anchors:
  - [section]: Equation (1) and Section 3 describe the iterative summarization process
  - [section]: Figure 4 shows the distilled summary D highlighting essential knowledge for a cardiology question
  - [corpus]: R3-RAG uses step-by-step reasoning via RL

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Discuss-RAG is a plug-and-play enhancement for training-free medical RAG pipelines. Understanding how RAG combines retrieval with generation is essential.
  - Quick check question: Can you explain why adding retrieved documents to an LLM prompt can reduce hallucinations?

- **Dense Retrieval / Embedding Similarity**
  - Why needed here: The paper critiques similarity-based retrieval as insufficient for medical QA (Figure 2). You need to understand what dense retrieval does and where it fails.
  - Quick check question: Why might cosine similarity retrieve documents that are semantically related but clinically irrelevant?

- **Multi-Agent Orchestration**
  - Why needed here: Discuss-RAG uses multiple specialized agents (recruiter, experts, summarizer, verifier, decision-maker). Understanding agent roles and communication patterns is critical.
  - Quick check question: What is the difference between agents interacting directly vs. through a shared summary?

## Architecture Onboarding

- **Component map**: Input Q → Recruiter R → Expert agents H₁...Hₙ → Multi-turn discussion → Summarizer C → Distilled summary D → Query Q+D → Retriever MedCPT → Top-k snippets S₁...Sₖ → Decision-maker U → Verification/verification → LLM GPT-3.5 → Answer

- **Critical path**: The summarizer's ability to synthesize diverse expert insights into a coherent summary D is the bottleneck. If D is noisy or incomplete, retrieval quality degrades.

- **Design tradeoffs**:
  - More agents/turns → richer context but higher latency and cost
  - Strict verification → higher precision but potential recall loss
  - Fallback strategies (e.g., CoT) add robustness but complexity

- **Failure signatures**:
  - Agents ignore prior summaries (redundant insights)
  - Summarizer over-condenses, losing critical nuance
  - Decision-maker rejects all snippets, triggering fallback unnecessarily

- **First 3 experiments**:
  1. Ablate the multi-turn discussion (single-turn vs. multi-turn) to measure iterative refinement gains on BioASQ.
  2. Remove post-retrieval verification to quantify its contribution to accuracy on PubMedQA.
  3. Vary the number and specialization of expert agents (e.g., 2 vs. 5 agents) to assess sensitivity on MMLU-Med.

## Open Questions the Paper Calls Out
1. **Agent Communication**: The paper suggests that allowing direct peer-to-peer interaction between specialized medical agents could improve reasoning depth compared to the current summarizer-mediated approach. This remains untested as agents currently interact only through the summary.

2. **Computational Overhead**: The framework's increased latency and computational cost from multi-agent calls is identified as a primary limitation, but the trade-off between performance benefits and inference costs has not been analyzed.

3. **Model Dependency**: The framework's effectiveness on smaller, open-source language models is unknown since experiments exclusively utilized GPT-3.5, leaving open whether similar gains would scale to state-of-the-art systems.

## Limitations
- The approach shows accuracy improvements but absolute performance remains below 60% in most cases, indicating the method improves but doesn't solve medical QA comprehensively
- The paper acknowledges that agents don't communicate directly, which may limit reasoning depth
- Only GPT-3.5 was tested, leaving open whether gains would scale to more capable models like GPT-4

## Confidence
- **High confidence**: Post-retrieval verification mechanism is well-supported by experimental design and consistent with prior work on multi-evidence validation
- **Medium confidence**: Multi-turn brainstorming improves retrieval context, though extent depends heavily on agent prompt quality not fully detailed
- **Medium confidence**: Accuracy improvements demonstrated across four datasets, but small absolute gains and lack of non-LLM baselines temper generalizability

## Next Checks
1. **Prompt sensitivity analysis**: Vary agent prompts and discussion depth to determine optimal configuration; test whether gains persist with different LLM backends
2. **Quantitative snippet relevance**: Implement automatic metrics (e.g., ROUGE, semantic similarity) to measure how distilled summaries affect retrieved snippet quality beyond qualitative assessment
3. **Failure case analysis**: Systematically analyze instances where the decision-maker rejects all snippets or the fallback strategy triggers to identify common error patterns and refine verification criteria