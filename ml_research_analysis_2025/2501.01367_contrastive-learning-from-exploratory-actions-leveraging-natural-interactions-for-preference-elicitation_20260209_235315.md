---
ver: rpa2
title: 'Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions
  for Preference Elicitation'
arxiv_id: '2501.01367'
source_url: https://arxiv.org/abs/2501.01367
tags:
- clea
- robot
- learning
- user
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEA (Contrastive Learning from Exploratory
  Actions), a method for learning robot behavior representations that align with user
  preferences without requiring tedious proxy tasks. The core insight is that users
  naturally explore robot behaviors during customization, and these exploratory actions
  can be leveraged as implicit preference labels.
---

# Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation

## Quick Facts
- **arXiv ID:** 2501.01367
- **Source URL:** https://arxiv.org/abs/2501.01367
- **Reference count:** 40
- **Primary result:** CLEA features outperform self-supervised baselines across four evaluation metrics when learning robot behavior representations from user exploratory actions.

## Executive Summary
This paper introduces CLEA (Contrastive Learning from Exploratory Actions), a method for learning robot behavior representations that align with user preferences without requiring tedious proxy tasks. The core insight is that users naturally explore robot behaviors during customization, and these exploratory actions can be leveraged as implicit preference labels. The authors collect exploratory action data from 25 users customizing robot signals via the RoSiD interface, then evaluate learned features on a separate set of 42 users performing behavior rankings. CLEA features outperform self-supervised baselines across four evaluation metrics: completeness (test preference accuracy up to 0.974), simplicity (AUC alignment up to 0.440), minimality (effective in 8-dimensional spaces), and explainability (cosine similarity up to 0.239). The method combines exploratory action data with self-supervised objectives and shows robustness to noise.

## Method Summary
CLEA learns user-aligned features by leveraging implicit preference signals from exploratory search during robot customization. Users interact with the RoSiD interface, selecting behaviors to test (explored set $D^{ex}$) and ignoring others (ignored set $D^{ig}$). The method applies a symmetric triplet loss that pulls features of co-selected behaviors together while pushing selected-vs-ignored pairs apart. This contrastive objective is combined with reconstruction-based self-supervised losses (AE/VAE) to create hybrid feature spaces that are both semantically aligned and physically representative. The total loss combines the CLEA contrastive objective with reconstruction (MSE) and KL-divergence terms, preventing embedding collapse while enforcing user alignment.

## Key Results
- CLEA features achieved test preference accuracy up to 0.974 on the completeness metric
- The method showed AUC alignment up to 0.440 for simplicity, outperforming self-supervised baselines
- CLEA was effective in low-dimensional spaces (8 dimensions) for minimality
- Explainability (cosine similarity) reached up to 0.239 with CLEA features

## Why This Works (Mechanism)

### Mechanism 1: Implicit Preference Labels from Exploratory Search
- **Claim:** Implicit interaction signals from exploratory search can substitute for explicit preference labels to learn user-aligned features.
- **Mechanism:** The system logs user interactions during customization, partitioning behaviors into "explored" (selected) and "ignored" sets. It treats co-selected items as positive pairs and selected-vs-ignored items as negative pairs.
- **Core assumption:** Users select behaviors to test because they perceive them as relevant or interesting, and ignore those they perceive as irrelevant.
- **Evidence anchors:** Abstract states users naturally explore robot behaviors during customization, and these exploratory actions can be leveraged as implicit preference labels. Section III.B defines the partitioning logic.
- **Break condition:** Users are already familiar with the robot and do not perform exploratory actions, or interface summaries are uninterpretable, removing the implicit label signal.

### Mechanism 2: Symmetric Triplet Loss for Semantic Alignment
- **Claim:** A symmetric triplet loss applied to exploratory data forces the embedding space to reflect semantic user preferences better than self-supervised reconstruction alone.
- **Mechanism:** CLEA uses a symmetric triplet loss that minimizes Euclidean distance between features of two items the user explored while maximizing distance to an item from the opposite set.
- **Core assumption:** Euclidean distance in the learned latent space corresponds to user-perceived similarity in preference space.
- **Evidence anchors:** Abstract states CLEA features outperformed self-supervised features when eliciting user preferences. Section III.B describes the triplet loss encouraging features from the same subset to be more similar.
- **Break condition:** The "ignored" set contains behaviors the user actually liked but simply didn't click (false negatives), introducing noise that corrupts the embedding structure.

### Mechanism 3: Hybrid Contrastive-Reconstruction Framework
- **Claim:** Combining CLEA with reconstruction-based self-supervised losses creates a hybrid feature space that is both semantically aligned and physically representative.
- **Mechanism:** The total loss function combines the contrastive CLEA objective with reconstruction (MSE) and KL-divergence terms, preventing the embedding from collapsing to trivial solutions while enforcing user alignment.
- **Core assumption:** Self-supervised features capture physical structure but lack semantic meaning; contrastive signals add the missing semantic layer without losing physical fidelity.
- **Evidence anchors:** Section V.A describes CLEA+AE and CLEA+VAE as proposed algorithms. Section VI mentions that additional loss terms were helpful for video/sound but could hinder joint states.
- **Break condition:** For behaviors where physical state correlates poorly with social interpretation, reconstruction losses may pull semantically distinct features together, countering the CLEA separation.

## Foundational Learning

- **Concept: Triplet Loss (Metric Learning)**
  - **Why needed here:** This is the mathematical engine of CLEA. Understanding how anchors, positives, and negatives are defined determines if the model learns meaningful distances.
  - **Quick check question:** If you randomly sampled the "negative" from the entire database instead of the "ignored" set, how would the learning signal change?

- **Concept: Inverse Reinforcement Learning (IRL)**
  - **Why needed here:** The paper frames preference elicitation as learning a reward function $R_H(\Phi(\xi))$. You must understand that features $\Phi$ are the input to this reward function.
  - **Quick check question:** Why does the paper argue that learning good features $\Phi$ is a prerequisite for efficiently learning the reward $R_H$?

- **Concept: Exploratory Search vs. Information Retrieval**
  - **Why needed here:** The paper leverages the specific psychology of users who don't know what they want yet. This distinguishes CLEA from standard feedback systems where goals are known.
  - **Quick check question:** In an exploratory search context, does a user's *lack* of action (ignoring a behavior) constitute a strong negative label or a weak one?

## Architecture Onboarding

- **Component map:**
  - RoSiD interface -> Data Logger (captures $D^{ex}$ and $D^{ig}$) -> Triplet Miner (constructs $(Anchor, Positive, Negative)$) -> Encoder ($\Phi$: CNN/GRU mapping to $\mathbb{R}^d$) -> Trainer (minimizes $L_{CLEA} + L_{recon} + L_{KL}$)

- **Critical path:**
  1. Users must engage in free-form exploration (no strict query-answering)
  2. Logs must be parsed into valid triplets (ensuring ignored items were actually visible)
  3. Encoder $\Phi$ is trained offline
  4. New users use frozen $\Phi$ for faster preference learning (linear reward model)

- **Design tradeoffs:**
  - **Noise vs. Scale:** Exploratory data is noisy (users click randomly to test robot limits) but scales naturally; explicit labeling is clean but halts the primary task
  - **Modality Specificity:** The paper trains separate encoders per modality (Visual, Auditory, Kinetic) rather than one unified multimodal model

- **Failure signatures:**
  - **Expert Users:** If users already know the robot, they won't explore, and $D^{ex}$ will be empty
  - **Semantic Interference:** For kinetic data, "fear" and "excitement" look similar physically; reconstruction losses may merge them, confusing the classifier
  - **High Dimensions:** Performance degrades in high dimensions (128d) for pure CLEA; hybrid methods (CLEA+VAE) are preferred there

- **First 3 experiments:**
  1. **Triplet Ablation:** Train using only $D^{ex}$ vs. using both $D^{ex}$ and $D^{ig}$ to validate the utility of the "ignored" set
  2. **Dimensionality Sweep:** Test alignment (AUC) across 8, 16, 32, 64, 128 dimensions to verify the "Minimality" hypothesis
  3. **Noise Injection:** Add Gaussian noise to trajectory features to verify if CLEA features are more robust to physical execution noise than standard Autoencoders

## Open Questions the Paper Calls Out

- **Question 1:** How do different behavior summary modalities (e.g., natural language, animated GIFs, semantic tags) compare to static visual summaries in terms of user search efficiency and the quality of features learned via exploratory actions?
  - **Basis in paper:** Future work can explore how robot behaviors can be summarized to non-expert users in ways that allow them to most efficiently search through robot behavior options.
  - **Why unresolved:** The current implementation relies on static visual summaries, which may not be the most effective method for users to intuitively grasp complex behaviors.

- **Question 2:** Does user familiarity with a specific robot platform degrade the efficacy of CLEA by reducing the motivation to perform exploratory actions?
  - **Basis in paper:** If users are already familiar with a particular robot, they may not be motivated to perform exploratory actions because they have already found their preferred robot behaviors.
  - **Why unresolved:** The study relied on users customizing a robot they were unfamiliar with (Kuri); it is unknown if expert or repeat users would naturally generate enough exploratory data.

- **Question 3:** Does re-weighting exploratory actions based on their chronological occurrence (e.g., prioritizing actions taken closer to the final selection) significantly improve the alignment of learned features with user preferences?
  - **Basis in paper:** Appendix K tested time-based reweighting and found results trended toward significance but were inconclusive.
  - **Why unresolved:** The hypothesis that actions later in the search process are more indicative of true preference is intuitively strong, but the current statistical analysis failed to confirm a benefit.

## Limitations

- The approach relies heavily on exploratory user behavior data, which may not be available or reliable for all robot tasks or user groups. The method assumes that ignored behaviors are truly disliked, which may not hold if users simply didn't see them or were interrupted during exploration.
- The evaluation framework uses a separate dataset (42 users) from the training data (25 users), but the paper doesn't report statistical significance tests for the performance differences between methods.
- The method shows degradation in high-dimensional spaces (128d) where pure CLEA underperforms, suggesting the approach may not scale to very complex behavior spaces without careful hybrid design.

## Confidence

- **High Confidence:** The core mechanism of using exploratory actions as implicit preference signals is well-supported by the experimental results (completeness metric up to 0.974).
- **Medium Confidence:** The claim that hybrid CLEA+reconstruction methods perform better than pure self-supervised methods in high dimensions, though the exact contribution of each component is unclear.
- **Low Confidence:** The robustness claims to noise, as the noise injection experiments show mixed results across different behavior types and modalities.

## Next Checks

1. **Cross-dataset validation:** Test CLEA on an independent robot behavior dataset to verify generalization beyond the specific RoSiD interface and 8,000-behavior corpus used in this study.
2. **Expert vs. novice user comparison:** Compare CLEA performance when users are domain experts (who may not explore) versus novices (who naturally explore), to identify the boundary conditions where the implicit labeling assumption breaks.
3. **Alternative negative sampling strategies:** Evaluate whether sampling negatives from the full dataset (rather than just the ignored set) improves or degrades feature quality, to better understand the role of the ignored set in the triplet loss formulation.