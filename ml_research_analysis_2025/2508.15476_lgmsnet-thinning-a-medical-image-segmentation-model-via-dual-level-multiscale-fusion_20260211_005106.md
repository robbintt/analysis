---
ver: rpa2
title: 'LGMSNet: Thinning a medical image segmentation model via dual-level multiscale
  fusion'
arxiv_id: '2508.15476'
source_url: https://arxiv.org/abs/2508.15476
tags:
- segmentation
- image
- lgmsnet
- medical
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LGMSNet addresses limitations in lightweight medical image segmentation
  models, which typically sacrifice performance for efficiency and struggle with global
  context perception and channel redundancy. The proposed method introduces a dual
  multiscale framework combining local and global feature extraction.
---

# LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion

## Quick Facts
- arXiv ID: 2508.15476
- Source URL: https://arxiv.org/abs/2508.15476
- Reference count: 40
- Primary result: State-of-the-art medical image segmentation with ~2.32M parameters, demonstrating superior zero-shot generalization across six public datasets

## Executive Summary
LGMSNet addresses the challenge of building lightweight medical image segmentation models that don't sacrifice performance for efficiency. Traditional lightweight models struggle with global context perception and channel redundancy, limiting their practical utility in resource-constrained medical settings. The proposed dual multiscale framework combines local and global feature extraction through two specialized blocks: the Local Multiscale (LMS) block for scale-specific feature capture and the Global Multiscale (GMS) block for efficient global context modeling.

The method achieves state-of-the-art performance while maintaining exceptional parameter efficiency, enabling deployment in resource-constrained environments. The model demonstrates remarkable zero-shot generalization capabilities, maintaining high accuracy when tested on unseen datasets without fine-tuning, which validates its practical applicability across diverse medical imaging scenarios.

## Method Summary
LGMSNet employs a U-Net backbone enhanced with dual-level multiscale blocks. The Local Multiscale (LMS) block uses heterogeneous kernel sizes (3, 5, 7, 9) within depthwise convolutions to capture scale variations and reduce channel redundancy by forcing different channel groups to specialize in distinct spatial frequencies. The Global Multiscale (GMS) block integrates sparse hybrid Transformer-convolutional branches to efficiently capture global contextual information without the computational burden of standard Vision Transformers. The model is trained using a combined loss function (0.5 × BCE + Dice) for 500 epochs with initial learning rate 0.02 and random rotation/flipping augmentation.

## Key Results
- Achieves superior segmentation accuracy with significantly fewer parameters than existing methods (~2.32M vs >60M for SOTA)
- Demonstrates state-of-the-art performance across six public datasets (BUSI, TNSCUI, ISIC18, Kvasir, BTCV, KiTS23)
- Maintains exceptional performance in zero-shot generalization tests on four unseen datasets without fine-tuning
- Optimal LMS kernel configuration found to be [3, 5, 7, 9] and GMS channel ratio of 3:1 (Transformer:Conv)

## Why This Works (Mechanism)

### Mechanism 1: Intra-layer Heterogeneous Receptive Fields
The LMS block uses variable kernel sizes within a single layer, allowing lightweight models to capture lesion scale variations more effectively than uniform kernels. By splitting input channels into four groups and applying depthwise convolutions with kernels of size 3, 5, 7, and 9 respectively, different channel groups specialize in different spatial frequencies and object scales before merging.

### Mechanism 2: Channel Redundancy Reduction via Feature Diversification
Standard convolutions suffer from channel redundancy (correlated features across channels). The LMS block mitigates this by processing channel groups with different kernels, creating decorrelated features that improve the efficiency of the channel-wise representation. The subsequent 1x1 convolution acts as a learned fusion layer selecting the most informative scale-specific features.

### Mechanism 3: Sparse Hybrid Global Context (GMS)
The GMS block captures global dependencies without the quadratic computational cost of standard Vision Transformers. It employs a "sparse" reshaping strategy that unfolds patches per channel rather than global flattening, reducing sequence length and computational load while maintaining long-range attention. The 3:1 Transformer-to-Conv channel ratio balances global context with local inductive bias.

## Foundational Learning

- **Concept: Inductive Bias in Vision** - Why needed: The paper contrasts local inductive bias of convolutions with global context of Transformers, explaining why LMS and GMS blocks are structured differently. Quick check: Why does the paper place LMS blocks at high resolutions and GMS blocks at lower resolutions?
- **Concept: Depthwise Separable Convolution** - Why needed: LMS block relies on depthwise convolutions to handle large kernels (up to 9x9) efficiently. Quick check: If standard Conv2d is used instead of DWConv in LMS block, how would parameter count change?
- **Concept: Channel Splitting and Fusion** - Why needed: Both LMS and GMS rely on splitting tensors along channel dimension, processing them differently, and merging them. Quick check: In GMS block, what is functional difference between channel group sent to Conv branch vs Transformer branch?

## Architecture Onboarding

- **Component map:** Input -> LMS Blocks (kernels 3, 5, 7, 9) -> GMS Blocks (Transformer/Conv ratio 3:1) -> Fusion Block (Group + Pointwise conv) -> Decoder (Linear Interpolation + Convs)
- **Critical path:** The GMS Block's channel ratio is critical - if Transformer branch is too small, global context is lost; if too large, it violates lightweight constraint. 3:1 ratio identified as optimal.
- **Design tradeoffs:** Heterogeneous kernels increase code complexity but improve scale adaptation; sparse Transformer sacrifices theoretical maximum context for massive FLOPs reduction; model trades depth for width in feature diversity.
- **Failure signatures:** Artifacting/Checkerboarding indicates Fusion Block implementation issues; missed small lesions suggest LMS kernel configuration problems; GPU memory OOM indicates incorrect sparse Transformer implementation.
- **First 3 experiments:**
  1. Ablation on LMS Kernels: Implement LMS block with only 3x3 kernels vs heterogeneous (3,5,7,9) set on BUSI to verify scale diversity contribution.
  2. Generalization Stress Test: Train on BUSI and test on BUS exactly as described in Table 2 to confirm learning of generic shape features.
  3. Parameter Efficiency Check: Compare GFLOPs and Params against baseline U-Net and TransUnet to verify lightweight claim (~2.32M params).

## Open Questions the Paper Calls Out

### Open Question 1
Is the static kernel configuration (3, 5, 7, 9) in the LMS block optimal for all lesion scales, or would a dynamic, data-driven kernel selection mechanism better handle the significant variance in foreground scale ratios shown in Figure 1a?

### Open Question 2
Does the optimal channel splitting ratio (Transformer:Conv) in the GMS block vary depending on the resolution of the feature map or the specific imaging modality?

### Open Question 3
To what extent does LGMSNet's sparse global modeling strategy maintain performance under cross-modality domain shifts (e.g., training on MRI, testing on CT) compared to same-modality generalization?

## Limitations

- The specific batch size and optimizer details for both 2D and 3D experiments are not explicitly provided, affecting reproducibility
- The exact 3D patch size configuration is unclear from the text
- The "sparse" implementation details of the GMS block's Transformer branch are not fully specified

## Confidence

- **High confidence** in core architectural contributions (LMS and GMS blocks) and their stated benefits
- **Medium confidence** in reported quantitative results due to missing training configuration details
- **Medium confidence** in generalization claims pending independent validation on unseen datasets

## Next Checks

1. Implement ablation study comparing homogeneous (3×3 only) vs heterogeneous (3,5,7,9) kernels in LMS block on BUSI dataset to verify scale diversity contribution
2. Train on BUSI dataset and test zero-shot generalization on BUS dataset exactly as described in Table 2 to confirm learning of generic features
3. Profile parameter count and FLOPs against baseline U-Net and TransUnet to verify the "lightweight" claim of ~2.32M parameters