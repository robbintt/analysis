---
ver: rpa2
title: Innovation Capacity of Dynamical Learning Systems
arxiv_id: '2601.07257'
source_url: https://arxiv.org/abs/2601.07257
tags:
- innovation
- capacity
- rank
- readout
- covariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a gap in understanding the information-processing
  capacity of noisy dynamical systems used in reservoir computing. While the classical
  information-processing capacity (Cip) measures the ability to perform input-measurable
  tasks, it can be far smaller than the rank of the readout covariance in noisy systems.
---

# Innovation Capacity of Dynamical Learning Systems

## Quick Facts
- arXiv ID: 2601.07257
- Source URL: https://arxiv.org/abs/2601.07257
- Authors: Anthony M. Polloreno
- Reference count: 0
- Primary result: Introduces innovation capacity (Ci) to measure the information-processing capability of noisy dynamical systems allocated to tasks orthogonal to input filtration.

## Executive Summary
This paper addresses a fundamental gap in understanding the information-processing capacity of noisy dynamical systems used in reservoir computing. While classical information-processing capacity (Cip) measures predictable information, it can severely underestimate the true capacity in noisy systems. The author introduces an innovation capacity (Ci) that quantifies the portion of the readout dimension's rank allocated to unpredictable, innovation-driven tasks. Through a basis-free Hilbert-space formulation, the paper establishes a conservation law showing that Cip + Ci exactly partitions the rank of the observable readout dimension covariance, providing a principled framework for understanding how noise affects information processing in physical learning systems.

## Method Summary
The paper develops a rigorous mathematical framework using basis-free Hilbert-space methods to decompose the information-processing capacity of noisy dynamical systems into predictable and innovation components. The approach formulates the predictable/innovation decomposition using Doob's filtration theory, establishing that the sum of classical and innovation capacities equals the rank of the readout covariance matrix. In the specific case of linear-Gaussian Johnson-Nyquist noise regimes, the framework yields an explicit generalized-eigenvalue shrinkage rule that quantifies the tradeoff between temperature and predictable capacity. The theoretical analysis demonstrates that high innovation capacity creates a high-dimensional innovation subspace with a variance floor, leading to extensive innovation-block differential entropy and exponentially many distinguishable histories under appropriate mixing and anti-concentration conditions.

## Key Results
- Establishes conservation law: Cip + Ci = rank(ΣXX) ≤ d, showing predictable and innovation capacities exactly partition the readout dimension rank
- In Johnson-Nyquist regimes, derives explicit monotone tradeoff between temperature and predictable capacity via generalized-eigenvalue shrinkage rule
- Proves information-theoretic lower bound linking innovation capacity to sample complexity for learning the induced innovation-block law in total variation
- Shows that large innovation capacity creates high-dimensional innovation subspace with variance floor, enabling extensive innovation-block differential entropy

## Why This Works (Mechanism)
The innovation capacity framework works by recognizing that noisy dynamical systems can process information through two distinct channels: predictable responses to inputs (captured by Cip) and unpredictable innovations (captured by Ci). The basis-free Hilbert-space formulation allows rigorous treatment of the predictable/innovation decomposition without dependence on specific coordinate choices. In physical systems with Johnson-Nyquist noise, the temperature acts as a control parameter that determines how capacity is split between predictable and innovation components through a generalized eigenvalue problem. The mathematical framework reveals that high innovation capacity naturally leads to high-dimensional innovation subspaces with specific statistical properties that enable both information processing and generative capabilities.

## Foundational Learning

**Doob's Predictable/Innovation Decomposition**: Separates stochastic processes into predictable components (measurable from past information) and innovations (unpredictable new information). Needed to rigorously partition information processing capacity; quick check: verify that innovations form a martingale difference sequence.

**Hilbert-Space Formulation**: Uses inner-product spaces to represent random variables and their correlations without coordinate dependence. Needed for basis-free treatment of capacity decomposition; quick check: confirm that the predictable subspace is the orthogonal projection onto the input filtration.

**Johnson-Nyquist Noise Regime**: Models thermal noise in physical systems as Gaussian with variance proportional to temperature. Needed to derive explicit tradeoff between temperature and capacity; quick check: verify that noise spectral density follows kT relationship.

**Generalized Eigenvalue Problem**: Extends standard eigenvalue decomposition to pairs of matrices, used here to determine optimal capacity allocation. Needed to express the temperature-dependent shrinkage rule; quick check: confirm that eigenvalues are monotonic in temperature.

## Architecture Onboarding

**Component Map**: Physical Reservoir -> Readout Layer -> Predictable Subspace ⊕ Innovation Subspace -> Capacity Allocation
**Critical Path**: Input Signal → Physical Dynamics → Observable Readout → Predictable/Innovation Decomposition → Capacity Utilization
**Design Tradeoffs**: Higher temperature increases innovation capacity but reduces predictable capacity; larger readout dimension increases total capacity but requires more resources
**Failure Signatures**: When predictable capacity approaches zero, system becomes dominated by noise; when innovation capacity is too low, system lacks generative diversity
**First Experiments**:
1. Measure Cip and Ci across temperature ranges in a physical reservoir computer
2. Verify conservation law numerically for varying readout dimensions
3. Test sample complexity scaling for learning innovation-block distributions

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Theoretical framework assumes Gaussian, continuous-time linear dynamics; extension to nonlinear or discrete-time systems unvalidated
- Conservation law depends on exact finite-rank readout covariance, which may fail for infinite-dimensional or ill-conditioned readouts
- Link between innovation capacity and generative utility demonstrated indirectly through sample-complexity arguments, not direct generation quality metrics

## Confidence
- Conservation law (Cip + Ci = rank(ΣXX)): **High** - rigorous Hilbert-space derivation
- Generalized-eigenvalue shrinkage rule for Johnson-Nyquist noise: **Medium** - needs broader noise-model validation
- Generative utility claims via sample complexity: **Low** - lacks empirical generation benchmarks

## Next Checks
1. Verify the conservation law numerically for nonlinear physical reservoirs (e.g., opto-electronic delay systems) under varying noise levels
2. Test the generalized-eigenvalue shrinkage rule beyond Johnson-Nyquist regimes (e.g., multiplicative noise, non-Gaussian inputs)
3. Empirically evaluate generative performance (e.g., likelihood, sample diversity) against the predicted innovation dimension scaling