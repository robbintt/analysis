---
ver: rpa2
title: 'HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction
  in CTR Prediction'
arxiv_id: '2601.12681'
source_url: https://arxiv.org/abs/2601.12681
tags:
- sequence
- query
- hyformer
- modeling
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents HyFormer, a unified hybrid transformer architecture\
  \ that addresses the limitations of decoupled sequence modeling and feature interaction\
  \ in large-scale industrial recommendation models. HyFormer introduces global tokens\
  \ and alternates between two complementary mechanisms\u2014Query Decoding and Query\
  \ Boosting\u2014to enable deeper, earlier, and bidirectional interactions between\
  \ long-range behavioral sequences and heterogeneous non-sequential features within\
  \ a single backbone."
---

# HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction

## Quick Facts
- arXiv ID: 2601.12681
- Source URL: https://arxiv.org/abs/2601.12681
- Reference count: 32
- Primary result: HyFormer achieves 0.17% AUC gain over strong baselines with superior scaling and validated online gains in production.

## Executive Summary
HyFormer addresses the limitation of decoupled sequence modeling and feature interaction in large-scale industrial recommendation systems by introducing a unified hybrid transformer architecture. The model alternates between Query Decoding (extracting sequence information into global tokens) and Query Boosting (mixing these tokens with non-sequential features) to enable bidirectional, iterative refinement of joint representations. Extensive experiments demonstrate consistent offline improvements and significant online gains in high-traffic production systems, validating its practical scalability.

## Method Summary
HyFormer introduces global tokens derived from heterogeneous non-sequential features and pooled sequence summaries, then iteratively refines these through alternating Query Decoding (cross-attention over sequence K/V) and Query Boosting (MLP-Mixer-style token mixing). The architecture supports multiple sequence encoding strategies including Full Transformer, LONGER-style cross-attention, and SwiGLU decoder-style. Each behavior sequence is processed independently with dedicated query tokens before cross-sequence interaction through token mixing, maintaining efficiency under KV-Cache constraints.

## Key Results
- HyFormer outperforms LONGER + RankMixer baseline by 0.17% AUC on billion-scale industrial dataset
- Demonstrates superior scaling behavior with increasing model parameters and FLOPs
- Large-scale online A/B tests show significant gains: 0.293% increase in average watch time per user and 1.111% increase in video finish play count per user

## Why This Works (Mechanism)

### Mechanism 1
Alternating Query Decoding and Query Boosting enables bidirectional information flow between sequence modeling and feature interaction, improving joint representations. At each layer, Query Decoding extracts sequence-aware information into global tokens via cross-attention; Query Boosting then mixes these decoded tokens with non-sequential features via MLP-Mixer. This iterative refinement replaces the single-pass, late-fusion pipeline. Core assumption: Multi-layer, co-evolutionary refinement yields richer representations than isolated sequential encoding followed by late fusion. Evidence: BaseArch with global tokens gains only +0.03% vs HyFormer's +0.17% AUC.

### Mechanism 2
Global tokens derived from heterogeneous features (non-sequential + pooled sequence summary) provide richer semantic queries for decoding long behavioral sequences. Query Generation concatenates non-sequential feature vectors with MeanPool(Seq) and projects via multiple FFNs to produce N diverse query tokens. These query the layer-wise K/V representations, injecting global context into sequence decoding. Core assumption: Non-sequential features contain sufficient contextual signal to form meaningful, diverse queries. Evidence: Removing non-seq and seq-pooling from query causes 0.08% AUC drop.

### Mechanism 3
Independent multi-sequence modeling with query-level mixing preserves sequence-specific semantics while enabling cross-sequence interaction. Each behavior sequence has dedicated query tokens and K/V representations; Query Boosting mixes across all decoded queries and non-seq tokens without merging sequences. Core assumption: Sequences have heterogeneous feature spaces and semantics; premature merging degrades representation quality. Evidence: Merging sequences causes 0.06% AUC loss.

## Foundational Learning

- **Cross-Attention for Query-to-Sequence Decoding**:
  - Why needed here: HyFormer's Query Decoding uses cross-attention to extract sequence information into N query tokens with O(N×L) complexity instead of O(L²) self-attention.
  - Quick check question: Given query count N=3 and sequence length L=3000, can you compute and compare FLOPs for cross-attention vs. full self-attention?

- **MLP-Mixer Token Mixing**:
  - Why needed here: Query Boosting uses MLP-Mixer-style channel-subspace mixing to enrich query representations without expensive attention.
  - Quick check question: In MLP-Mixer, how is information aggregated across token positions when each token is partitioned into T channel subspaces?

- **KV-Cache and Serving Efficiency Constraints**:
  - Why needed here: Paper notes increasing query tokens degrades serving efficiency under KV-Cache and M-Falcon; HyFormer keeps query count stable.
  - Quick check question: Why does serving latency scale with query count in KV-Cache architectures for recommendation?

## Architecture Onboarding

- **Component map**: Input Tokenization → Query Generation (FFN projections from non-seq + pooled seq) → [HyFormer Layer × L] → Final MLP
- **Critical path**:
  1. Ensure global tokens are generated with diverse semantic sources (non-seq + sequence pooling).
  2. Verify cross-attention correctly attends layer-wise K/V for each sequence.
  3. Confirm Query Boosting mixes decoded queries with non-seq tokens before residual connection.
  4. Stack sufficient layers for iterative refinement (empirically test depth vs. AUC).

- **Design tradeoffs**:
  - More query tokens → richer semantics but higher serving latency under KV-Cache.
  - Full Transformer sequence encoding → highest capacity but O(L²) cost; LONGER-style is O(L_H×L).
  - Sequence merging reduces parameters but empirically degrades AUC by 0.06%.

- **Failure signatures**:
  - AUC plateaus despite deeper layers → query tokens may lack diversity; check query generation FFN initialization.
  - Online latency spikes → query count or sequence length exceeds KV-Cache efficiency threshold.
  - Multi-sequence AUC drop → verify each sequence has independent query tokens; check for unintended sequence merging.

- **First 3 experiments**:
  1. Ablate query sources: Run HyFormer with (a) full query (non-seq + seq-pooling), (b) non-seq only, (c) target-only. Compare AUC deltas to isolate contribution.
  2. Scaling curve: Train HyFormer and LONGER+RankMixer baseline at 3 parameter sizes (200M, 400M, 800M). Plot AUC vs. params and FLOPs to verify steeper HyFormer scaling.
  3. Multi-sequence strategy: Compare independent sequence modeling vs. merged sequence on same dataset; measure AUC gap and latency difference.

## Open Questions the Paper Calls Out

### Open Question 1
Which of the supported sequence encoding strategies (Full Transformer, LONGER-style, or Decoder-style) optimally balances the trade-off between representation capacity and computational cost when integrated deeply into the HyFormer stack? The paper demonstrates the effectiveness of the HyFormer architecture but does not isolate how the choice of internal sequence encoder impacts the "alternating optimization" process or the final scaling laws. A comprehensive ablation study showing AUC and latency trade-offs for HyFormer when utilizing each of the three sequence encoding strategies at varying model depths would resolve this.

### Open Question 2
Can the "adaptive allocation" of global tokens to heterogeneous behavior sequences be formalized into a learnable, differentiable mechanism rather than a manual or heuristic configuration? While the benefits of adaptive allocation are confirmed empirically, the paper does not propose or test a method to automate this allocation based on input context or sequence importance. A comparative study evaluating a learned "Token Allocator" module against the static allocation strategy would resolve this.

### Open Question 3
Does the iterative, stacked nature of the HyFormer architecture introduce inference latency bottlenecks that limit its deployability in real-time systems compared to single-stage interaction models? While the paper demonstrates superior scaling in terms of AUC vs. FLOPs, FLOPs do not always correlate linearly with wall-clock time in serialized, iterative architectures like HyFormer. Online A/B test results or system benchmarks reporting the average inference latency and QPS under identical hardware constraints would resolve this.

## Limitations

- Missing architectural hyperparameters: Critical values like hidden dimension D, number of layers, attention heads, FFN widths, and exact query token count N are not reported, making faithful reproduction difficult.
- Unknown training configurations: No details on optimizer type, learning rate schedule, weight decay, batch size per GPU, or total training epochs/steps.
- Limited multi-sequence ablation: While merging sequences degrades AUC by 0.06%, the paper does not test alternative multi-sequence strategies that might offer better tradeoffs.

## Confidence

- **High confidence**: Query Boosting + Query Decoding alternating mechanism improves joint sequence-feature representations over late-fusion baselines (supported by offline/online gains and ablation).
- **Medium confidence**: Global tokens enriched with non-sequential features and sequence pooling provide richer queries (ablation shows 0.08% AUC drop when removed, but mechanism not independently validated).
- **Low confidence**: Independent multi-sequence modeling is necessary (only one comparison to merged sequences; no exploration of intermediate strategies).

## Next Checks

1. **Hyperparameter sensitivity sweep**: Train HyFormer across a grid of hidden dimensions (256, 512, 1024) and query token counts (3, 6, 9) to map the AUC-FLOPs tradeoff curve and identify the minimal configuration matching the reported gains.

2. **Cross-dataset generalization**: Evaluate HyFormer on a public long-sequence CTR dataset (e.g., Alimama, Avito) to verify that the alternating decode-boost architecture consistently outperforms LONGER+RankMixer, isolating architecture effects from dataset-specific optimizations.

3. **Multi-sequence strategy ablation**: Compare independent sequence modeling (current) against (a) merged sequences with cross-attention, (b) hierarchical pooling of sequences, and (c) shared query tokens with sequence-specific KV layers to quantify the necessity and cost of full independence.