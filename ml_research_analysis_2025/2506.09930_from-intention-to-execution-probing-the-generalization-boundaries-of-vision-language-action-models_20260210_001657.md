---
ver: rpa2
title: 'From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action
  Models'
arxiv_id: '2506.09930'
source_url: https://arxiv.org/abs/2506.09930
tags:
- plate
- widowx
- finetune
- delta
- carrot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INT-ACT, a comprehensive simulation-based
  benchmark suite of 50 tasks designed to probe the generalization boundaries of Vision-Language-Action
  (VLA) models. The suite systematically varies object diversity, language complexity,
  and vision-language reasoning challenges.
---

# From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models

## Quick Facts
- **arXiv ID:** 2506.09930
- **Source URL:** https://arxiv.org/abs/2506.09930
- **Reference count:** 40
- **Primary result:** VLA models show strong perceptual understanding (intention correctness 80-100%) but struggle with reliable execution under distributional shifts.

## Executive Summary
This paper introduces INT-ACT, a comprehensive simulation-based benchmark suite of 50 tasks designed to systematically probe the generalization boundaries of Vision-Language-Action (VLA) models. The suite varies object diversity, language complexity, and vision-language reasoning challenges to reveal a significant "intention-action gap": while models demonstrate strong perceptual understanding and high-level planning, they struggle to execute these intentions reliably under distributional shifts. Furthermore, fine-tuning on robotics data erodes the underlying VLM's generalist reasoning abilities, particularly under language variations and multimodal ambiguity. The benchmark and evaluation code are released to standardize future VLA research and drive efforts to close the perception-to-action gap.

## Method Summary
INT-ACT is a VLA probing suite of 50 simulation-based tasks across 10 subcategories, organized into object diversity (OOD source/target/relations), language complexity (action/negation/appearance/commonsense), and vision-language thinking (distractions, multimodal ambiguity). Built on SimplerEnv/ManiSkill2 with BridgeV2 dataset for fine-tuning, it evaluates π0-finetune, π0-scratch, SpatialVLA, Magma (zero-shot), and Octo Small/Base. Three primary metrics are used: Grasp Success Rate, Intention Correct Rate (gripper within 5cm of correct source object), and Task Success Rate. Each task is evaluated across 24 episodes × 3 seeds.

## Key Results
- VLA models demonstrate strong perceptual understanding (intention correctness 80-100%) but struggle with reliable execution under distributional shifts
- Fine-tuning on robotics data erodes the underlying VLM's generalist reasoning abilities, particularly under language variations and multimodal ambiguity
- VLAs are brittle when visual and language distribution shifts compound, causing systematic misbehavior in multimodal ambiguity scenarios

## Why This Works (Mechanism)

### Mechanism 1: VLM Semantic Transfer Without Motor Grounding
- Claim: VLM backbones transfer perceptual understanding and high-level planning to VLA policies, but this "intention" does not reliably translate into precise motor execution.
- Mechanism: Pretrained VLMs encode object semantics, language comprehension, and spatial reasoning. When fine-tuned on robotics data, these representations correctly identify *what* to manipulate (intention correctness 80-100%), but the end-to-end policy learns a brittle coupling between perception and action that fails under distributional shifts.
- Core assumption: Intention correctness (gripper moving within radius of correct object) captures semantic grounding independent of motor execution success.
- Evidence anchors:
  - [abstract] "while models demonstrate strong perceptual understanding and high-level planning (intention correctness 80-100%), they struggle to execute these intentions reliably under distributional shifts"
  - [Section 4.2] "This observation confirms that VLMs endow VLA policies with a generalizable notion of 'what to do,' yet executing that intent—especially under distributional shifts, remains challenging."
  - [corpus] Related work (arXiv:2509.22195) confirms fine-tuning VLMs into VLAs causes catastrophic forgetting of foundational reasoning.
- Break condition: If intention correctness drops significantly under OOD conditions, the mechanism fails—the VLM backbone was never providing semantic transfer.

### Mechanism 2: Fine-Tuning Erosion of VLM Generalization
- Claim: Fine-tuning VLMs on action data disrupts the pretrained vision-language representations, degrading linguistic and multimodal reasoning capabilities.
- Mechanism: End-to-end training on robotics data optimizes for action prediction but does not preserve the original VLM's handling of language variations, negation, and commonsense reasoning. The policy overfits to narrow instruction patterns seen in robotics datasets.
- Core assumption: The pretrained VLM (e.g., PaliGemma) possessed stronger language understanding than demonstrated by the fine-tuned VLA.
- Evidence anchors:
  - [abstract] "fine-tuning on robotics data erodes the underlying VLM's generalist reasoning abilities, particularly under language variations and multimodal ambiguity"
  - [Section 4.2, Table 2] All models suffer performance drops under language variations; π0-finetune drops 24% in task success under language negation
  - [corpus] arXiv:2511.19878 (MAPS) confirms naive fine-tuning disrupts VLM representations and proposes module-wise regularization as a fix.
- Break condition: If co-training with vision-language data (like Magma) preserves linguistic capability significantly better than action-only fine-tuning, the mechanism is confirmed.

### Mechanism 3: Compounding Multimodal Distribution Shift
- Claim: VLAs are brittle when visual and language distribution shifts compound—each shift individually manageable, but together causing systematic misbehavior.
- Mechanism: Visual distractors and commonsense language variations are partially handled in isolation. When combined, linguistic priors override visual grounding, causing the policy to attend to wrong objects.
- Core assumption: The failure mode is specifically from interaction of modalities, not merely additive degradation.
- Evidence anchors:
  - [Section 4.2, Figure 5] "Orange juice" case: adding an orange distractor + commonsense paraphrasing "juice squeezed from orange" causes wrong object attempt rates to surge across all models
  - [Section 4.2] "while VLAs have some robustness to isolated commonsense and visual distractions, they are brittle under multimodal ambiguity"
  - [corpus] Weak direct evidence in corpus for this specific mechanism; related work focuses on single-modality generalization.
- Break condition: If wrong object attempts remain low even under compounded shifts, the mechanism fails—the problem is not multimodal interaction.

## Foundational Learning

- **Concept: VLM Pretraining and Transfer**
  - Why needed here: Understanding what capabilities VLMs bring (open-vocabulary recognition, language comprehension) and how they may or may not transfer to action domains.
  - Quick check question: Can you explain why a model pretrained on web-scale image-text data might recognize a "wheel" but fail to grasp it correctly?

- **Concept: Distribution Shift in Imitation Learning**
  - Why needed here: The benchmark systematically tests OOD generalization across objects, language, and visual context. Understanding covariate shift vs. concept shift is essential.
  - Quick check question: If a policy was trained on "put carrot on plate" but tested on "put carrot on keyboard," what type of distribution shift is occurring?

- **Concept: Intention vs. Execution Metrics**
  - Why needed here: The paper introduces intention correctness as distinct from task success. Understanding this decomposition is critical for diagnosing *where* policies fail.
  - Quick check question: A policy correctly approaches the target object but fails to grasp it. Which metric captures this distinction?

## Architecture Onboarding

- **Component map:** VLM backbone (e.g., PaliGemma, LLaMA-based) -> Action head / projector -> Fine-tuning pipeline on robotics data -> Evaluated on OOD tasks (INT-ACT)
- **Critical path:** 1. VLM pretrained on vision-language data → 2. Action tokenizer/projector added → 3. Fine-tuned on robot demonstrations → 4. Evaluated on OOD tasks (INT-ACT)
- **Design tradeoffs:**
  - Fine-tuning from scratch vs. from pretrained checkpoint: Scratch training (π0-scratch) shows better generalization on some OOD tasks but requires more data
  - Autoregressive vs. diffusion action heads: Autoregressive (SpatialVLA) shows sharper degradation on OOD targets; diffusion (π0) more robust but still suffers intention-action gap
  - Co-training with V-L data (Magma) vs. action-only: Magma shows better language robustness but still fails under compounded shifts
- **Failure signatures:**
  - High intention correctness + low task success = intention-action gap (VLM semantic transfer working, motor execution failing)
  - Wrong object attempt surge under distractors + commonsense = multimodal ambiguity breakdown
  - Sharp drop on language variations (negation, appearance) = VLM capability erosion from fine-tuning
- **First 3 experiments:**
  1. Evaluate π0-finetune on original SimplerEnv tasks to establish baseline intention correctness vs. task success gap.
  2. Test π0-scratch vs. π0-finetune on OOD source objects to measure whether pretrained VLM initialization vs. scratch training affects generalization.
  3. Run the "carrot on plate" task with progressive complexity: clean → distracted → commonsense → commonsense+distracted to observe when wrong object attempts emerge.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark relies entirely on simulation environments, limiting ecological validity for real-world robot deployment
- Object and language variations are controlled but may not capture the full complexity of open-world scenarios
- The intention-correctness metric (5cm threshold) is somewhat arbitrary and may not fully capture semantic grounding
- Model performance differences could be influenced by architectural choices beyond VLM pretraining (e.g., action head design)

## Confidence
- High confidence: The intention-action gap exists (strong empirical evidence across multiple models and tasks)
- Medium confidence: VLM semantic transfer explains high intention correctness (mechanism plausible but not definitively proven)
- Medium confidence: Fine-tuning erodes VLM generalization (supported but other factors may contribute)
- Low confidence: Multimodal ambiguity is the primary failure mode (weakest empirical support in corpus)

## Next Checks
1. Validate the 5cm intention threshold by testing whether it correlates with human judgment of "correct object approach" across diverse scenarios
2. Replicate the "orange juice" case study with controlled ablation: test each factor (distractor object, commonsense language, both) separately on multiple models to isolate the multimodal interaction effect
3. Compare VLA performance against a control baseline: fine-tune a non-VLM visual encoder (e.g., ConvNet) on the same action data to determine if VLM pretraining specifically contributes to intention correctness vs. general feature learning