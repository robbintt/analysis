---
ver: rpa2
title: 'Everything counts: the managed omnirelevance of speech in ''human - voice
  agent'' interaction'
arxiv_id: '2510.22610'
source_url: https://arxiv.org/abs/2510.22610
tags:
- voice
- agent
- agents
- human
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study documents how humans manage interactions with voice\
  \ agents, revealing that any hearable speech risks triggering unwanted responses\
  \ from the machine. This \"omnirelevance of human speech\" forces users to adapt\
  \ their conduct\u2014whispering, gesturing, or designing turns to avoid detection\u2014\
  especially in multiparty settings."
---

# Everything counts: the managed omnirelevance of speech in 'human - voice agent' interaction

## Quick Facts
- arXiv ID: 2510.22610
- Source URL: https://arxiv.org/abs/2510.22610
- Reference count: 0
- Primary result: Any hearable speech risks triggering unwanted responses from voice agents, forcing users to adapt their conduct—whispering, gesturing, or designing turns to avoid detection.

## Executive Summary
This study documents how humans manage interactions with voice agents, revealing that any hearable speech risks triggering unwanted responses from the machine. This "omnirelevance of human speech" forces users to adapt their conduct—whispering, gesturing, or designing turns to avoid detection—especially in multiparty settings. Even with advanced LLM-based agents, simple silence-based turn-taking models dominate, making human speech equally consequential as with older rule-based robots. The work highlights persistent "work to make technology work," showing that users must still learn specialized "Voice User Interface speak" to manage interruptions and maintain interaction progressivity. Praxeological continuity persists despite technological advances.

## Method Summary
The study uses ethnomethodological Conversation Analysis (EMCA) of two video corpora: 100 naturalistic interactions with a Pepper robot in a museum (2022, pre-LLM era) and 11 interactions with ChatGPT's "advanced voice mode" on smartphones (2025). Data were transcribed using Jefferson conventions for speech and Mondada conventions for embodied actions, then analyzed for sequential and multimodal organization of turn-taking, "aside sequences," and user adaptation behaviors. The method focuses on documenting how participants manage the participation framework when voice agents are present.

## Key Results
- Voice agents using silence-based turn-taking treat any hearable speech as sequentially implicative, triggering responses regardless of utterance function
- Users manage omnirelevance by producing "aside sequences"—contributions designed to be available to humans but undetectable to the agent
- Novice users must learn "Voice User Interface speak"—withholding or redesigning response tokens that would otherwise facilitate progressivity in human conversation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Voice agents using silence-based turn-taking treat any hearable human speech as sequentially implicative, triggering a response regardless of the utterance's actual function.
- Mechanism: Voice Activity Detection (VAD) identifies speech onset and offset; a configurable silence duration (e.g., `silence_duration_ms`) after speech triggers the agent to take a turn. The model does not distinguish continuers, acknowledgments, or asides from full turns.
- Core assumption: Commercial voice agents prioritize implementation simplicity and latency over context-aware turn boundary detection.
- Evidence anchors:
  - [abstract] "any hearable speech risks triggering unwanted responses from the machine"
  - [section 2.4] documents that ChatGPT's "advanced voice mode" uses `server_vad` with silence thresholds; notes user reports of interruptions due to short pauses
  - [corpus] weak direct evidence; corpus neighbors focus on generation quality and latency, not interactional constraints
- Break condition: Continuous turn-taking models that integrate prosodic, syntactic, and pragmatic cues to predict turn boundaries would reduce this effect (see Skantze & Irfan [133] cited in paper), but these remain absent from commercial deployments.

### Mechanism 2
- Claim: Users manage the omnirelevance constraint by producing "aside sequences"—contributions designed to be available to human co-participants but undetectable to the agent's audio sensors.
- Mechanism: Participants lower vocal amplitude (whispering), use visual-only modalities (gestures, mouthings), and orient bodies away from the device microphone while maintaining mutual visibility with human partners.
- Core assumption: Users develop an implicit model of the agent's perceptual threshold and act to remain below it.
- Evidence anchors:
  - [section 4.2.1] Tom produces farewells at higher amplitude with forward lean toward Pepper; aside talk to Ana is softer with body torque away
  - [section 4.3.2] Guillaume mouths "trois" and uses a three-finger gesture to answer Cédric's question without triggering the voice agent
  - [corpus] no direct corpus support for this interactional adaptation mechanism
- Break condition: Agents with multimodal perception that treat visual gestures as input (not just audio) could detect and respond to asides, changing the constraint structure.

### Mechanism 3
- Claim: Novice users must learn "Voice User Interface speak"—withholding or redesigning response tokens that would otherwise facilitate progressivity in human conversation.
- Mechanism: Users encounter repeated interruptions when producing continuers (e.g., "oui," "d'accord," "okay") at transition-relevance places; through trial-and-error, they learn to withhold audible backchannels or whisper them below detection threshold.
- Core assumption: Users bring human conversational norms into agent interactions and must revise them based on observed system behavior.
- Evidence anchors:
  - [section 4.4.1] Emma produces five response tokens, each triggering a cut-off in GPT's reading; after the fifth, she produces an embodied negative assessment (mouth twist, click) and subsequently withholds audible response tokens
  - [abstract] "users must still learn specialized 'Voice User Interface speak' to manage interruptions"
  - [corpus] no corpus evidence on this learning trajectory
- Break condition: Agents that recognize response tokens as non-turn-claiming (via prosody, placement, or lexical type) and suppress generation would eliminate this learning burden.

## Foundational Learning

- Concept: **Voice Activity Detection (VAD)**
  - Why needed here: The paper's central claim depends on understanding that current agents use silence duration after speech detection—not semantic or pragmatic understanding—to decide when to respond.
  - Quick check question: Given a 700ms silence threshold, will "uh huh" followed by 800ms of silence trigger an agent response?

- Concept: **Transition-Relevance Place (TRP)**
  - Why needed here: The mismatch between human-recognized TRPs (where continuers belong) and agent turn segmentation (which doesn't recognize TRPs) explains why response tokens cause interruptions.
  - Quick check question: In human conversation, what happens at a TRP if a recipient remains silent?

- Concept: **Participation Framework**
  - Why needed here: The paper documents how users maintain dual frameworks—one including the agent, one excluding it—using multimodal resources to manage who can perceive what.
  - Quick check question: How does whispering change the participation framework compared to normal speech in a multiparty setting with a voice agent?

## Architecture Onboarding

- Component map: Microphone array → VAD module → ASR → LLM
- Critical path: Microphone → VAD (detects speech end via silence) → LLM (generates response) → TTS. Latency and interruption behavior are determined at the VAD→LLM transition point.
- Design tradeoffs:
  - Shorter silence duration → faster responses but more interruptions of user mid-turn
  - Longer silence duration → fewer interruptions but perceptible latency
  - Continuous turn-taking models (research stage) → more natural flow but higher computational cost and deployment complexity
  - Assumption: Current commercial systems optimize for perceived responsiveness over interactional appropriateness.
- Failure signatures:
  - Agent responds to user backchannels ("mm-hmm," "okay") as full turns
  - Agent interrupts itself after user produces acknowledgment at TRP
  - Agent restarts conversation after user farewell (treating "bye" as new input)
  - Users whisper, gesture, or mouth words to co-present humans
- First 3 experiments:
  1. **Threshold sweep**: Vary `silence_duration_ms` (300, 500, 700, 1000ms) and measure interruption rate on a corpus containing user backchannels; identify optimal point balancing latency vs. false turn-end detection.
  2. **Backchannel suppression prototype**: Implement a classifier to detect response tokens (lexical list + prosodic features: short duration, flat contour, low amplitude) and suppress LLM trigger for these; A/B test against baseline for user frustration signals.
  3. **Multimodal aside detection**: Using a camera-equipped device, log instances where users whisper or use visual-only communication; analyze whether these correlate with agent interruptions or user disengagement. Assumption: This requires visual processing not in current architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would replacing silence-based turn-taking models with continuous turn-taking models (integrating prosodic, semantic, and visual cues) eliminate the "omnirelevance of speech" problem, or would human adaptation practices persist?
- Basis in paper: [explicit] The authors note that machine-learning-based continuous turn-taking models have been developed and shown to enable smoother exchanges, yet "such systems remain almost entirely absent from today's commercial voice-user interfaces and social robots" (p. 4).
- Why unresolved: The study only examined systems using silence-based VAD; no comparison with continuous models was possible.
- What evidence would resolve it: Empirical comparison of human conduct when interacting with agents using continuous vs. silence-based turn-taking models in matched multiparty settings.

### Open Question 2
- Question: Are the "aside sequence" practices documented here (whispering, mouthing, gesturing to exclude the agent) generalizable across different cultural contexts and linguistic communities?
- Basis in paper: [inferred] The study examines French-language interactions only; the practices may be shaped by culturally specific conversational norms around turn-taking and participation frameworks.
- Why unresolved: No cross-cultural comparison was conducted; all data came from a single linguistic context.
- What evidence would resolve it: Comparative analysis of multiparty human-agent interactions across multiple languages and cultural settings.

### Open Question 3
- Question: Do the "VUI-speak" competencies that users develop through interaction with one voice agent transfer to interactions with different voice agents, or must learning recur with each new system?
- Basis in paper: [explicit] The authors describe a novice user (EMM) learning to withhold response tokens, demonstrating "a praxeological change as it unfolds" (p. 32), but do not examine transfer of learning.
- Why unresolved: The study captured initial encounters but not longitudinal development or cross-agent skill transfer.
- What evidence would resolve it: Longitudinal study tracking users across multiple voice agent platforms over time.

### Open Question 4
- Question: Can response tokens (continuers, acknowledgments) be designed to be recognized as non-turn-claiming by voice agents, and would humans then produce them without triggering interruptions?
- Basis in paper: [inferred] The paper documents that response tokens systematically trigger unwanted agent responses, but does not explore whether agents could be trained to treat such tokens differently based on their prosodic or sequential features.
- Why unresolved: The analysis treats silence-based VAD as a fixed constraint; no design intervention was tested.
- What evidence would resolve it: Experimental manipulation of agent behavior to recognize and ignore response tokens, followed by observation of human conduct changes.

## Limitations

- The core claim about VAD mechanisms relies on inference rather than direct measurement of undocumented commercial system parameters
- The corpus-based evidence is notably weak—most citations document generation quality and latency rather than the interactional constraints central to the argument
- The claim about praxeological continuity across technological generations rests on comparing two small corpora with undocumented technical specifications

## Confidence

**High confidence**: The documentation of user adaptation behaviors (whispering, gesturing, withholding response tokens) and the concept of "omnirelevance of human speech" are well-supported by the video analysis and align with established EMCA findings about human-technology interaction.

**Medium confidence**: The specific mechanism linking VAD silence thresholds to unwanted agent responses is plausible but not directly verified. The paper infers this from user reports and general knowledge of VAD implementations rather than measuring actual system parameters.

**Low confidence**: The claim about praxeological continuity across technological generations (pre-LLM vs. LLM agents) rests on comparing two small corpora with undocumented technical specifications, making it difficult to verify that the interactional constraints are truly equivalent.

## Next Checks

1. **Technical verification**: Instrument an actual voice agent (OpenAI Realtime API or comparable) to log VAD parameters (silence_duration_ms, sensitivity) and measure interruption rates on controlled corpora containing backchannels, continuers, and asides. Compare interruption rates across different threshold settings.

2. **Learning trajectory documentation**: Conduct a longitudinal study with novice users interacting with a voice agent over multiple sessions. Log instances of response token interruptions and track when/whether users develop strategies to withhold or whisper continuers. This would provide direct evidence for the learning curve described in the paper.

3. **Multimodal perception extension**: Implement a prototype voice agent with camera integration to detect visual asides (gestures, mouthings). Measure whether multimodal detection reduces the need for whispered asides and changes participation framework management. This would test whether the omnirelevance constraint is truly about audio detection or reflects broader interaction design choices.