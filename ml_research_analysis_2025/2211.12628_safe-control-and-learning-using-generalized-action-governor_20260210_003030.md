---
ver: rpa2
title: Safe Control and Learning Using Generalized Action Governor
arxiv_id: '2211.12628'
source_url: https://arxiv.org/abs/2211.12628
tags:
- safe
- learning
- control
- action
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generalized action governor (AG) framework
  for safe control and learning in discrete-time systems with bounded uncertainties.
  The AG augments a nominal closed-loop system with the capability to enforce state
  and input constraints through online action adjustment.
---

# Safe Control and Learning Using Generalized Action Governor

## Quick Facts
- **arXiv ID:** 2211.12628
- **Source URL:** https://arxiv.org/abs/2211.12628
- **Reference count:** 40
- **Primary result:** Presents a generalized action governor (AG) framework for safe control and learning in discrete-time systems with bounded uncertainties.

## Executive Summary
This paper introduces a generalized action governor (AG) framework that augments a nominal closed-loop system with the capability to enforce state and input constraints through online action adjustment. The key theoretical advance is relaxing the requirement for positive invariance of a safe set to returnability, enabling more flexible safe set design. The framework is demonstrated through two safe online learning strategies: safe Q-learning and safe data-driven Koopman operator-based control.

## Method Summary
The method consists of an offline safe set computation (using Maximum Output Admissible Sets for linear systems or a novel algorithm for discrete systems) followed by an online runtime optimization that minimally adjusts a candidate control action to satisfy constraints while ensuring the system can return to safety. The framework integrates with safe learning algorithms by modifying the reward function to penalize unsafe actions. The Koopman operator approach provides a data-driven way to learn linear approximations of nonlinear dynamics while the AG guarantees constraint satisfaction.

## Key Results
- Generalized AG theory applicable to broader classes of systems by relaxing positive invariance to returnability
- Tailored AG design procedures for linear systems using MOAS and for discrete systems with finite state/action spaces
- Two safe online learning strategies (Q-learning and Koopman-based control) integrated with AG to guarantee constraint satisfaction during learning
- Numerical examples demonstrate effective constraint enforcement and improved control performance through safe online learning

## Why This Works (Mechanism)

### Mechanism 1: Projection-based Action Filtering
The system maintains safety by minimally adjusting a "candidate" control action to satisfy instantaneous constraints and ensure the existence of a safe future trajectory. At each time step, the AG solves an optimization problem seeking a safe action closest to the candidate such that the current state-action pair satisfies constraints and the predicted next state lands inside the projection of a safe set. If no such action exists, it defaults to a backup policy.

### Mechanism 2: Returnability-based Recovery
The AG guarantees recursive feasibility and safety even without strict positive invariance, provided the safe set is "returnable." If the primary optimization is infeasible, the AG switches to a backup nominal controller with a safe reference. Because the safe set is designed to be returnable, the backup controller is theoretically guaranteed to steer the state back into the safe set eventually, rather than drifting into constraint violation.

### Mechanism 3: Penalty-driven Safe Learning
Reinforcement learning agents can explore safely without fatal constraint violations by treating the AG's action modification as a penalty signal. In Safe Q-learning, the agent proposes an action, the AG adjusts it, and the reward function is modified to penalize the agent for requesting unsafe actions, teaching the policy to propose actions that the AG does not need to modify.

## Foundational Learning

- **Concept: Positive Invariance vs. Returnability**
  - **Why needed here:** The paper's primary theoretical contribution is relaxing "if you start in the set, you stay in the set" (Positive Invariance) to "if you leave, you can come back" (Returnability).
  - **Quick check question:** Does a set defined as "all states within 5 meters of the origin" satisfy returnability if the system has zero friction? (Answer: Likely yes for stabilization, but invariance requires the controller to strictly prevent any exit.)

- **Concept: Maximum Output Admissible Set (MOAS)**
  - **Why needed here:** For linear systems, the paper uses MOAS to define the safe set. This is the set of states and references for which the constraints are strictly satisfied for all future times without active intervention.
  - **Quick check question:** Why does MOAS computation become complex for non-convex constraints? (Answer: MOAS relies on polytopic intersections which assume convexity.)

- **Concept: Koopman Operator Theory**
  - **Why needed here:** The data-driven Koopman model represents nonlinear dynamics linearly in a lifted space, enabling linear control techniques to be applied.
  - **Quick check question:** If the Koopman linear model predicts a state that violates constraints, does the AG see the violation in $x$ or $z$? (Answer: The AG enforces constraints on the original $x$ and $u$, acting as a safety net for model mismatch.)

## Architecture Onboarding

- **Component map:** Offline Designer -> Safe Set $\Pi_{\pi_0}$ -> Learning/Nominal Policy (generates $u_1$) -> Action Governor (receives $x, u_1, \Pi_{\pi_0}$, outputs $u$) -> Plant (transitions to $x(t+1)$)

- **Critical path:** The definition of the **Backup Policy** $\pi_0$. If $\pi_0$ is not robustly stable or the set $\Pi_{\pi_0}$ is too small, the "Returnability" mechanism fails.

- **Design tradeoffs:**
  - **MOAS vs. Algorithm 1:** MOAS is computationally efficient but restricted to convex constraints; Algorithm 1 handles discrete/finite spaces but suffers from curse of dimensionality.
  - **Distance Metric:** The choice of distance metric dictates how "aggressive" the safety filter feels. Euclidean norm minimizes energy change; Chebyshev norm minimizes peak change.

- **Failure signatures:**
  - **Flickering:** AG rapidly switches between learned policy and backup policy, indicating the learned policy is operating on the edge of the safe set.
  - **Deadlock:** AG consistently outputs backup policy, meaning the learning agent never effectively controls the system.
  - **Infeasibility:** Runtime crash because state enters a region where no control satisfies constraints for any future disturbance.

- **First 3 experiments:**
  1. **Static Safety Verification:** Implement AG with fixed nominal controller. Drive system toward constraint boundary and verify AG clamps action without destabilizing loop.
  2. **Returnability Stress Test:** Force optimization to be infeasible. Verify system switches to $\pi_0$ and returns to $\Pi_{\pi_0}$.
  3. **Safe Learning Loop:** Run Algorithm 2. Plot cumulative penalty term over time; it should decrease as learner internalizes safety constraints.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Generalized AG framework be extended to continuous-time systems while preserving strict constraint satisfaction and stability guarantees?
  - **Basis in paper:** [explicit] Conclusion lists "extending the AG to continuous-time systems" as future work.
  - **Why unresolved:** Proposed theory relies on discrete-time formulations that do not directly translate to continuous differential equations.
  - **What evidence would resolve it:** Derivation of AG dynamics for continuous-time models and formal proof of all-time safety for continuous domain.

- **Open Question 2:** Can a tailored AG design procedure be developed for general continuous nonlinear systems that avoids computational expense of finite discretization?
  - **Basis in paper:** [explicit] Conclusion identifies "applications to constrained nonlinear systems" as future work. [inferred] Section 3.2 notes general nonlinear systems are handled "in an approximate manner" through discretization.
  - **Why unresolved:** Algorithm 1 requires finite spaces; gridding becomes intractable for high-dimensional continuous nonlinear systems.
  - **What evidence would resolve it:** Modified algorithm computing safe and returnable set for nonlinear systems without discretizing entire state-action space.

- **Open Question 3:** How does choice of observables in safe data-driven Koopman control strategy impact frequency of AG intervention and resulting control optimality?
  - **Basis in paper:** [inferred] Section 4.2 states observables are selected heuristically and acknowledges model mismatch can necessitate AG intervention.
  - **Why unresolved:** Paper doesn't analyze if poor observables (leading to high model mismatch) cause excessive AG overrides, which could stall learning or degrade performance.
  - **What evidence would resolve it:** Comparative analysis showing correlation between Koopman model prediction error and convergence rate of average cost.

## Limitations

- The theoretical framework relies on existence of a robust backup controller that can guarantee return within bounded time despite disturbances, which may not hold for all systems.
- MOAS computation becomes computationally intractable for complex, non-convex constraint sets.
- The discrete-system algorithm suffers from curse-of-dimensionality, limiting scalability to systems with more than a handful of states and actions.

## Confidence

- **High:** Mechanism of action filtering through projection-based optimization (Eq. 8) is clearly defined and mathematically sound.
- **Medium:** Returnability-based recovery mechanism (Prop. 2) is theoretically proven but depends on existence of sufficiently robust backup controller.
- **Medium:** Penalty-driven safe learning approach is standard technique, but specific choice of penalty coefficient and its impact on learning speed vs. safety is not thoroughly analyzed.
- **Low:** Performance improvements shown in numerical examples are promising but limited to single benchmark system; broader empirical validation needed.

## Next Checks

1. **Robustness Test:** Implement AG on a second, structurally different system (e.g., pendulum or cart-pole) to verify returnability mechanism works beyond double integrator example.

2. **Hyperparameter Sensitivity:** Systematically vary penalty coefficient $M$ in Algorithm 2 and measure trade-off between learning speed and constraint violation frequency.

3. **MOAS vs. Returnability Comparison:** For linear system example, compare size of MOAS (positive invariant) set versus returnable set to quantify practical benefit of theoretical relaxation.