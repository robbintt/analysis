---
ver: rpa2
title: 'Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving
  Data'
arxiv_id: '2509.07198'
source_url: https://arxiv.org/abs/2509.07198
tags:
- clients
- clustering
- task
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes Fed-REACT, a federated learning framework designed
  to handle heterogeneous and evolving client data. The framework employs a two-stage
  approach: first, clients collaboratively learn feature representations via self-supervised
  learning; second, clients are dynamically clustered based on task model weights,
  and cluster-specific models are trained.'
---

# Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data

## Quick Facts
- arXiv ID: 2509.07198
- Source URL: https://arxiv.org/abs/2509.07198
- Reference count: 40
- Primary result: Achieves superior accuracy and robustness in federated learning on non-stationary, heterogeneous time-series data

## Executive Summary
Fed-REACT is a federated learning framework that addresses the challenges of heterogeneous and evolving client data through a two-stage approach. First, clients collaboratively learn feature representations using self-supervised contrastive learning on unlabeled data. Second, clients are dynamically clustered based on task model weights, and cluster-specific models are trained using an evolutionary clustering algorithm with an adaptive forgetting factor. The framework provides theoretical convergence guarantees and demonstrates strong empirical performance on real-world time-series datasets under non-stationary conditions.

## Method Summary
Fed-REACT operates in two phases: Phase I performs federated representation learning where clients train a shared encoder using contrastive self-supervised learning on unlabeled data; Phase II clusters clients based on task model weights using an adaptive evolutionary clustering algorithm (AFFECT) and trains cluster-specific task models. The approach decouples representation learning from task-specific training, allowing robust feature extraction across heterogeneous clients, and employs time-smoothed gradient descent with theoretical convergence guarantees under non-stationary conditions.

## Key Results
- Outperforms supervised federated learning baselines on accuracy and robustness for heterogeneous, evolving time-series data
- Achieves perfect Rand score (1.0) on EEG dataset while baseline methods oscillate
- Demonstrates communication efficiency advantages in clustered federated learning settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling representation learning from task-specific training enables robust feature extraction across heterogeneous clients, even when labels are scarce or distributions diverge.
- **Mechanism:** In Phase I, a global encoder is trained using federated self-supervised learning (contrastive loss) on unlabeled local data. This forces the model to learn transferable low-level features (e.g., temporal edges) that are consistent across clients. In Phase II, clients freeze these representations and train lightweight, cluster-specific task models (e.g., SVMs) using limited labeled data.
- **Core assumption:** Low-level temporal features are more transferable across non-IID clients than high-level semantic features; local labeled data is insufficient to train a full encoder from scratch.
- **Evidence anchors:** [Section 2] "In the first phase... each client learns a local model to extracts feature representations... In the second phase... coordinates cluster-wise training of task-specific models."; [Abstract] "Fed-REACT combines representation learning with evolutionary clustering... task-specific models for downstream objectives."; [Corpus] Related work "SAFL" supports the efficacy of structure-aware/client-specific clustering.

### Mechanism 2
- **Claim:** Server-side evolutionary clustering with an adaptive forgetting factor stabilizes client grouping in non-stationary environments where snapshot-based methods fail.
- **Mechanism:** Instead of clustering clients based solely on current round model weights, the server maintains a smoothed affinity matrix $\hat{\psi}_t$. It uses the AFFECT algorithm to compute an adaptive forgetting factor $a_t$ that balances historical cluster membership against current weights, correcting for temporary distribution drifts or noise.
- **Core assumption:** Data evolution is smooth or has temporal correlation; single-round weights are noisy estimators of the true distribution.
- **Evidence anchors:** [Section 2] "We adopt the Adaptive Evolutionary Clustering framework... which allows cluster memberships to evolve over time... [avoiding] unstable cluster assignments."; [Section 4.2] Figure 1 demonstrates that Fed-REACT achieves a perfect Rand score of 1.0 on EEG while baseline methods oscillate.; [Corpus] "Learning Reconfigurable Representations" highlights the difficulty of misaligned local features in multimodal FL.

### Mechanism 3
- **Claim:** Time-smoothed gradient descent in the representation phase provides theoretical convergence guarantees under non-stationary conditions.
- **Mechanism:** The framework employs a time-smoothed gradient update rule (averaging gradients over a window $w$). Theoretical analysis (Theorem 1) suggests that with appropriate step size, the average squared gradient norm (regret) is bounded primarily by the projection error, rather than accumulating error from distribution shift.
- **Core assumption:** The loss function is Lipschitz and smooth; the projection operator effectively bounds the parameter space.
- **Evidence anchors:** [Section 3] "Theoretical analysis... showing convergence to a small regret value determined by the gradient projection error."; [Theorem 1] Explicitly bounds the global regret $\lim_{\gamma \to 1-} \frac{1}{T} \sum \|\nabla S_{t,w,\gamma}\|^2.$

## Foundational Learning

- **Concept:** Self-Supervised Contrastive Learning (SSL)
  - **Why needed here:** Phase I relies entirely on SSL to learn representations without labels. You must understand how positive/negative pairs are constructed (e.g., sub-sequences from the same trajectory) to debug the encoder.
  - **Quick check question:** How does the loss function penalize the model if it maps two sub-sequences from the same time series far apart?

- **Concept:** Evolutionary Clustering (AFFECT)
  - **Why needed here:** Standard clustering (K-Means) fails on time-series FL due to noise. You need to understand the "forgetting factor" concept to tune the trade-off between stability (keeping old clusters) and reactivity (forming new ones).
  - **Quick check question:** If the adaptive forgetting factor $a_t$ approaches 1, how does that impact the model's reaction to a sudden change in client data?

- **Concept:** Dynamic Regret Analysis
  - **Why needed here:** The paper shifts focus from static optimization to dynamic regret to prove robustness against drifting data. Understanding this bounds the "cost" of the environment changing.
  - **Quick check question:** Does the theoretical guarantee ensure the model finds the *optimal* global minimum, or just a bounded "regret" relative to the shifting comparator?

## Architecture Onboarding

- **Component map:** Unlabeled Data -> Local Encoder -> Server Agg -> Frozen Encoders -> Labeled Data -> Task Heads -> Server Clustering
- **Critical path:**
  1. Phase I Convergence: Verify the contrastive loss decreases globally. If the encoder fails, Phase II cannot succeed.
  2. Cluster Stability: Monitor the Rand Score. If clusters oscillate wildly in early Phase II rounds, the adaptive forgetting factor may be initialized incorrectly.
- **Design tradeoffs:**
  - Server-side vs. Client-side Clustering: Fed-REACT centralizes clustering (polynomial complexity in clients) to save communication (constant cost wrt clusters). *Trade-off:* Server load vs. bandwidth.
  - Approach A1 vs. A2: A1 (Simple Temporal Averaging) vs. A2 (Weighted Averaging with Forgetting). A2 is empirically better for evolving data but requires tracking the adaptive factor history.
- **Failure signatures:**
  - Spurious Detection: Accuracy drops suddenly; Rand score fluctuates. *Likely cause:* Small batch sizes causing high variance in weights -> adaptive factor failing to smooth noise.
  - Stagnation: Accuracy plateaus, clusters do not update despite data drift. *Likely cause:* Forgetting factor saturated (too high), ignoring new signals.
- **First 3 experiments:**
  1. Stationary Heterogeneity Check: Run on RTD dataset with fixed clusters. Verify Phase I SSL outperforms supervised FedAvg on heterogeneous data (Table 1).
  2. Drift Stress Test: Run on RTD using "Strategy 2" (continuous non-overlapping sampling). Compare Fed-REACT (A2) vs. Snapshot Clustering to verify the stability mechanism.
  3. Communication Efficiency: Profile bits transmitted vs. IFCA. Validate that decoupling the encoder and clustering task heads reduces the communication footprint on the SUMO dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Fed-REACT framework be adapted to fully decentralized peer-to-peer networks where a central server is unavailable to coordinate clustering?
- Basis in paper: [explicit] The conclusion states, "Future work may explore fully decentralized settings in which clients must learn from evolving data without assistance from a central server."
- Why unresolved: The current architecture relies on a central server to execute the AFFECT algorithm for evolutionary clustering and to aggregate task model weights.
- What evidence would resolve it: A theoretical and empirical analysis of a decentralized consensus mechanism that achieves the same temporal clustering stability as the server-based AFFECT algorithm.

### Open Question 2
- Question: Does the convergence guarantee for Phase I extend to the actual contrastive loss and deep non-linear encoders used in practice?
- Basis in paper: [inferred] The theoretical analysis (Section 3) explicitly assumes a "linear feature model" and a simplified loss function for "analytical tractability," rather than the deep Causal CNN and full contrastive loss used in experiments.
- Why unresolved: The provided regret bound applies to a simplified objective ($arg min ||\bar{X} - \theta^T \theta||^2$), leaving the convergence properties of the actual deep learning backbone unproven.
- What evidence would resolve it: A convergence proof for non-convex objectives under the time-smoothed gradient descent scheme, or empirical verification of convergence bounds on the deep encoder.

### Open Question 3
- Question: How can the number of clusters be determined robustly in scenarios where local datasets are small or the Silhouette score is unreliable?
- Basis in paper: [inferred] Appendix E notes that while the Elbow method works on some datasets, the "Silhouette score correctly identifies the optimal cluster number only when local datasets are sufficiently large."
- Why unresolved: The method for identifying cluster count appears heuristic and sensitive to data volume, which is a common constraint in Federated Learning.
- What evidence would resolve it: A validation of automated cluster estimation techniques that perform reliably across the heterogeneous data regimes (e.g., small $|M_k^t|$) described in the paper.

## Limitations
- Theoretical convergence proof relies on strong assumptions about smoothness and Lipschitz continuity that may not hold in highly dynamic real-world time-series scenarios
- The AFFECT algorithm's adaptive forgetting factor initialization and update parameters are underspecified, potentially affecting reproducibility
- Evaluation focuses on relatively controlled synthetic non-stationarity; performance under abrupt, real-world data shifts remains unverified

## Confidence
- **High confidence:** Fed-REACT's superior accuracy and communication efficiency over baselines on the tested datasets (RTD, EEG, SUMO)
- **Medium confidence:** Theoretical convergence guarantees, as they depend on assumptions that may not generalize to all non-stationary conditions
- **Medium confidence:** Evolutionary clustering stability mechanism, since Rand score perfection on EEG is demonstrated but robustness to extreme drift is not tested

## Next Checks
1. Reproduce theoretical bounds: Implement the time-smoothed gradient descent and verify the regret bound empirically under controlled distribution shifts
2. Test extreme drift scenarios: Evaluate Fed-REACT under sudden, non-smooth distribution changes (e.g., complete label distribution swaps) to stress-test the adaptive forgetting factor
3. AFFECT parameter sensitivity: Systematically vary the adaptive forgetting factor initialization and update rules to determine their impact on clustering stability and downstream task performance