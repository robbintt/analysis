---
ver: rpa2
title: 'Comparing Large Language Models and Traditional Machine Translation Tools
  for Translating Medical Consultation Summaries: A Pilot Study'
arxiv_id: '2504.16601'
source_url: https://arxiv.org/abs/2504.16601
tags:
- translation
- medical
- llms
- translations
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared LLMs and traditional MT tools in translating
  medical consultation summaries into Arabic, Chinese, and Vietnamese. Two types of
  summaries were created: simple patient-facing and complex clinician-focused.'
---

# Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study

## Quick Facts
- arXiv ID: 2504.16601
- Source URL: https://arxiv.org/abs/2504.16601
- Reference count: 20
- Primary result: Traditional MT tools outperformed LLMs on surface metrics for complex clinical text; LLMs showed promise for simple Vietnamese and Chinese translations

## Executive Summary
This pilot study compared Large Language Models (LLMs) and traditional Machine Translation (MT) tools for translating medical consultation summaries into Arabic, Chinese, and Vietnamese. The research evaluated three LLMs (GPT-4o, LLAMA-3.1, GEMMA-2) and three MT tools (Google Translate, Microsoft Bing, DeepL) using BLEU, CHR-F, and METEOR metrics. Traditional MT tools generally outperformed LLMs, particularly for complex clinician-focused summaries. LLMs demonstrated contextual flexibility but lacked consistency, with Arabic showing improvement with complexity due to morphological context. The study highlights the inadequacy of current metrics for capturing clinical translation quality and emphasizes the need for human oversight in medical translation workflows.

## Method Summary
The study used two fictitious English medical summaries - a simple patient-facing summary about a dementia patient post-UTI and a complex clinician-targeted letter about advanced AML with GVHD. Three LLMs (GPT-4o, LLAMA-3.1-405B, GEMMA-2-27B) and three MT tools (Google Translate, Microsoft Bing Translator, DeepL) were evaluated for translating these summaries into Arabic, Chinese (simplified), and Vietnamese. Default web interfaces were used for all tools, with LLMs prompted simply: "Can you translate this document into [language], make sure no information is lost." BLEU, CHR-F, and METEOR metrics (ranging 0-1) were used to evaluate translations against ISO 17100-certified professional translator references.

## Key Results
- Traditional MT tools generally outperformed LLMs on surface-level metrics, especially for complex clinical text
- LLMs showed promise for Vietnamese and Chinese simple translations, with Vietnamese demonstrating the least performance degradation
- Arabic translations improved with complexity due to the language's morphology and the need for contextual disambiguation
- Current automated metrics inadequately capture clinical translation quality, potentially missing critical medical errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Traditional MT tools outperform LLMs on surface-level n-gram metrics for complex clinical text due to optimization for token alignment rather than semantic fluency.
- **Mechanism:** Traditional MT tools utilize sequence-to-sequence architectures optimized for exact token matching. In contrast, LLMs generate translations based on probabilistic next-token prediction focused on fluency and coherence. This leads LLMs to paraphrase content, which, while semantically accurate, penalizes them on metrics like BLEU that rely on rigid surface overlap with the reference translation.
- **Core assumption:** BLEU and CHR-F scores are valid proxies for "surface-level alignment" even if they fail to capture semantic adequacy in medical contexts.
- **Evidence anchors:**
  - Results: "Traditional MT tools generally outperformed LLMs on surface-level metrics... This reflects the focus of traditional MT tools on token-level alignment..."
  - Discussion: "MT-based metrics such as BLEU and CHR-F may penalise translations that are semantically accurate but structurally different..."
- **Break condition:** If semantic accuracy is prioritized over structural rigidity, this mechanism inverts, potentially favoring LLMs.

### Mechanism 2
- **Claim:** Translation performance for morphologically rich languages (specifically Arabic) improves with text complexity because increased context aids in resolving morphological ambiguities.
- **Mechanism:** Arabic relies heavily on morphology where meaning is derived from word structure. Simple, short sentences often lack the surrounding grammatical context needed to disambiguate word forms. Complex, jargon-heavy sentences provide "richer morphological context" and redundancy, allowing models to better infer the correct grammatical structure and specific medical terminology.
- **Core assumption:** The "complex" summaries provided sufficient grammatical connective tissue to act as a disambiguating signal for the models.
- **Evidence anchors:**
  - Abstract: "Arabic translations improved with complexity due to the language's morphology."
  - Results: "...longer, more redundant input helps Arabic models resolve morphological ambiguities."
- **Break condition:** If the input text is complex but syntactically fragmented, the contextual benefit disappears, likely degrading performance.

### Mechanism 3
- **Claim:** LLMs offer superior resilience for low-resource languages (like Vietnamese) compared to traditional MT tools that lack specific support or training data.
- **Mechanism:** LLMs are trained on massive, diverse multilingual corpora, allowing them to generalize patterns even for languages with fewer dedicated digital resources. Traditional MT tools often require specific, high-quality parallel corpora to perform well. The study suggests that for languages where major MT tools lack support, the broad "parametric knowledge" of LLMs acts as a functional fallback.
- **Core assumption:** The "default" settings and general training of LLMs encapsulate sufficient medical knowledge for these languages, despite the lack of fine-tuning.
- **Evidence anchors:**
  - Methods: "DeepL did not support Vietnamese translation at the time of this study."
  - Results: "Vietnamese... showing the least performance degradation... suggests that Vietnamese translation is robust, possibly due to better multilingual representation in modern models."
- **Break condition:** If the medical terminology is highly specialized or novel, LLM generalization fails, likely resulting in hallucinations or literal translations.

## Foundational Learning

- **Concept: BLEU vs. METEOR (Evaluation Metrics)**
  - **Why needed here:** The paper relies on these metrics to claim one system is "better" than another. Understanding that BLEU measures exact word overlap (punishing paraphrasing) while METEOR captures semantic similarity (accepting synonyms) is essential to interpreting why LLMs scored lower on BLEU but sometimes higher on METEOR.
  - **Quick check question:** If an LLM translates "hypertension" as "high blood pressure" (a correct synonym), which metric would penalize it more?

- **Concept: Morphological Richness (Arabic)**
  - **Why needed here:** To understand the counter-intuitive result that "complex" text translated *better* than "simple" text. Learners must grasp that in Arabic, context dictates word form, so more text can mean more accurate disambiguation.
  - **Quick check question:** Why would adding more sentences to a paragraph potentially improve the grammatical accuracy of a single word within that paragraph in Arabic?

- **Concept: Human-in-the-Loop (HITL)**
  - **Why needed here:** The paper explicitly warns against fully automated deployment. Engineers must understand that "better scores" do not equate to "clinical safety" because automated metrics miss critical errors like medication dosage mistakes.
  - **Quick check question:** A translation gets a perfect BLEU score but swaps "10mg" for "100mg". Why does this demand a HITL architecture?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Two distinct data streams - Patient Summaries (Simple/Lay) and Interprofessional Letters (Complex/Jargon)
  - Translation Engines: Path A (LLMs: GPT-4o, LLAMA-3.1, GEMMA-2 via web interface/prompt) and Path B (MT: Google, Bing, DeepL via API/Web)
  - Evaluation Layer: Automated Metrics (BLEU, CHR-F, METEOR) + Human Review (ISO 17100 certified services)
  - Filtering: Language-specific routing (e.g., blocking DeepL for Vietnamese)

- **Critical path:** The "Simple Summary -> LLM -> Human Review" path appears most promising for immediate efficiency gains, specifically for Vietnamese and Chinese. However, the "Complex Summary -> Traditional MT -> Human Review" path is currently safer for surface-level accuracy in high-resource languages.

- **Design tradeoffs:**
  - *Surface Accuracy vs. Semantic Flexibility:* Use Traditional MT for rigid terminology requirements; use LLMs for patient-facing readability
  - *Resource Coverage:* Rely on LLMs for unsupported languages (Vietnamese in DeepL), accepting higher variance for broader coverage

- **Failure signatures:**
  - *Metric Trap:* High BLEU scores masking critical clinical errors (e.g., negation errors "no fever" -> "fever")
  - *Complexity Drop:* Sharp performance degradation in Chinese complex summaries (Syntactic mismatch)
  - *Hallucination:* LLMs generating plausible-sounding but medically incorrect terms in an attempt to be fluent

- **First 3 experiments:**
  1. **Metric Correlation Test:** Run the translations against automated metrics AND human evaluators to quantify the "semantic gap" - how often does a high BLEU score hide a medical error?
  2. **Prompt Sensitivity Analysis:** Test the LLM path with "Role Prompts" (e.g., "You are a medical translator") vs. the basic prompts used in the study to see if performance gaps close.
  3. **Language-Specific Routing:** Build a router that sends Vietnamese text to an LLM (since DeepL is unsupported) and Arabic complex text to Google Translate, measuring the resulting consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can domain-specific, safety-aware evaluation metrics be developed that correlate better with clinical accuracy than traditional n-gram overlap methods?
- **Basis in paper:** [explicit] The conclusion explicitly calls for "developing safety-aware evaluation metrics," while the discussion notes that current metrics fail to penalize critical medical errors (e.g., mistranslated drug names) differently from minor filler word omissions.
- **Why unresolved:** Standard metrics like BLEU and CHR-F measure surface-level similarity and cannot assess clinical relevance or semantic safety, yet no validated medical-specific alternatives were tested in this study.
- **What evidence would resolve it:** The development and validation of a new metric that assigns weighted penalties to clinical errors, demonstrating a higher correlation with human clinician assessments than BLEU or METEOR.

### Open Question 2
- **Question:** Does fine-tuning general-purpose LLMs on domain-specific medical corpora significantly improve translation quality for under-resourced languages compared to out-of-the-box models?
- **Basis in paper:** [explicit] The authors state that future improvements should focus on "fine-tuning LLMs with domain-specific medical corpora" and "resourcing under-resourced languages," noting that general models produced inconsistent results for languages like Vietnamese.
- **Why unresolved:** This pilot study utilized default model settings without domain-specific fine-tuning, leaving the potential performance gains from specialized training unexplored.
- **What evidence would resolve it:** A comparative study measuring the performance gap between baseline and medically fine-tuned LLMs (e.g., Llama-3) specifically on Vietnamese and Arabic medical summaries.

### Open Question 3
- **Question:** What specific human-in-the-loop frameworks are required to effectively integrate LLM translation tools into clinical workflows without compromising patient safety?
- **Basis in paper:** [explicit] The paper highlights the need for "integrating human-in-the-loop mechanisms" and developing guidelines for "responsible AI translation in healthcare," acknowledging that LLMs are already being used by clinicians without adequate oversight.
- **Why unresolved:** While the paper identifies the risk of indiscriminate use, it stops short of defining or testing a specific validation protocol for clinical translation.
- **What evidence would resolve it:** Usability and safety studies testing specific clinician-validation workflows, demonstrating that human oversight successfully mitigates the semantic errors inherent in raw LLM translations.

## Limitations
- The study relies on surface-level metrics (BLEU, CHR-F, METEOR) that inadequately capture critical clinical errors like medication dosage mistakes or negation errors
- Exact source English summaries and professional reference translations are not included, making faithful reproduction difficult
- Web-based tools and LLMs were accessed via public interfaces without version locking, introducing potential variability

## Confidence
- **High Confidence:** Traditional MT tools generally outperform LLMs on surface-level metrics for complex clinical text
- **Medium Confidence:** LLMs show promise for low-resource languages like Vietnamese and simple text translations
- **Medium Confidence:** Arabic translations improve with complexity due to morphological context

## Next Checks
1. **Metric Correlation Test:** Run the translations against automated metrics (BLEU, CHR-F, METEOR) AND human evaluators to quantify the "semantic gap" - how often does a high BLEU score hide a medical error?
2. **Prompt Sensitivity Analysis:** Test the LLM path with "Role Prompts" (e.g., "You are a medical translator") vs. the basic prompts used in the study to see if performance gaps close.
3. **Human Review Pilot:** Conduct a small-scale pilot where human reviewers flag critical errors (e.g., medication dosage swaps, negation errors) in both LLM and MT outputs, regardless of metric scores.