---
ver: rpa2
title: Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification
arxiv_id: '2509.18155'
source_url: https://arxiv.org/abs/2509.18155
tags:
- dose
- variance
- uncertainty
- dropout
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a neural surrogate for proton dose calculation
  that integrates Monte Carlo dropout to provide calibrated predictive uncertainty.
  The method is validated through a staged series of experiments: a one-dimensional
  analytic benchmark establishes accuracy and convergence; two-dimensional bone-water
  phantoms demonstrate behavior under domain heterogeneity and beam uncertainty; and
  a three-dimensional water phantom confirms scalability to volumetric dose prediction.'
---

# Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification

## Quick Facts
- arXiv ID: 2509.18155
- Source URL: https://arxiv.org/abs/2509.18155
- Reference count: 4
- One-line primary result: Achieves orders-of-magnitude speedup over Monte Carlo while providing voxelwise epistemic and parametric uncertainty quantification for proton dose prediction.

## Executive Summary
This paper develops a neural surrogate for proton dose calculation that integrates Monte Carlo dropout to provide calibrated predictive uncertainty. The method is validated through a staged series of experiments: a one-dimensional analytic benchmark establishes accuracy and convergence; two-dimensional bone-water phantoms demonstrate behavior under domain heterogeneity and beam uncertainty; and a three-dimensional water phantom confirms scalability to volumetric dose prediction. Across all settings, the approach achieves orders-of-magnitude speedup over Monte Carlo while retaining uncertainty information. Variance decomposition shows epistemic uncertainty inflates under distribution shift while parametric variance dominates at material boundaries. The calibrated surrogate provides fast, uncertainty-aware dose predictions suitable for robust planning, adaptive workflows, and uncertainty-aware optimization in proton therapy.

## Method Summary
The method employs a feedforward neural network trained to predict proton dose distributions from beam and medium parameters. Monte Carlo dropout layers are retained during inference to generate predictive distributions via multiple stochastic forward passes. A law of total variance decomposition separates epistemic (model) uncertainty from parametric (input) uncertainty. The surrogate is trained on synthetic dose data generated by Monte Carlo transport (TOPAS/Geant4) and calibrated using split-conformal methods to ensure valid coverage. The architecture maps parameterized inputs to voxelized dose predictions, enabling orders-of-magnitude speedup compared to full Monte Carlo simulation.

## Key Results
- Achieves ~12,000× speedup over Monte Carlo (344.2s vs 2.7×10^-2s on RTX 4090) while maintaining uncertainty quantification
- Variance decomposition successfully separates epistemic uncertainty (inflates under distribution shift) from parametric uncertainty (dominates at material boundaries)
- Split-conformal calibration ensures valid coverage (1.00 empirical vs 0.95 nominal for 1D, 0.92 vs 0.95 for 3D)
- Epistemic variance localizes to Bragg peak region and material interfaces, matching physical expectations

## Why This Works (Mechanism)

### Mechanism 1
Monte Carlo dropout at test time produces predictive distributions that approximate Bayesian posterior inference over network weights. Dropout layers apply Bernoulli masks during forward passes, creating an ensemble of stochastic network configurations. Repeated passes with fixed input x yield samples from which predictive mean and variance are computed. This is reinterpreted as approximate sampling from P(θ|D_train). Core assumption: Dropout's random weight masking provides a reasonable approximation to true Bayesian posterior sampling; the quality of this approximation is not formally guaranteed.

### Mechanism 2
Total predictive variance decomposes into separable epistemic (model) and parametric (input) components via the law of total variance. Equation (16) applies Var[D] = E_x[Var_θ(D|x)] + Var_x(E_θ[D|x]). The first term captures dropout-induced variance at fixed input (epistemic); the second captures variance from input distribution Π (parametric). Finite-sample estimators compute these from S input samples and T dropout passes each. Core assumption: Input uncertainty and model uncertainty are statistically separable and the nested sampling procedure adequately approximates both expectations.

### Mechanism 3
A feedforward neural network can approximate the mapping from beam/medium parameters to dose distributions with sufficient accuracy for clinical use while achieving orders-of-magnitude speedup over Monte Carlo transport. The surrogate learns to predict dose on a fixed voxel grid from parameterized inputs. Once trained, inference requires only matrix multiplications rather than N particle history simulations. Example 7 reports ~12,000× speedup. Core assumption: The dose distribution is a sufficiently smooth and learnable function of the input parameters; the training data coverage is adequate for the intended use cases.

## Foundational Learning

- Concept: Monte Carlo dropout as test-time inference
  - Why needed here: This is the core uncertainty mechanism; understanding why dropout must remain active at test time (unlike standard practice) is essential.
  - Quick check question: Can you explain why enabling dropout during inference produces uncertainty estimates while a deterministic forward pass does not?

- Concept: Law of total variance and conditional expectation
  - Why needed here: The variance decomposition in equation (16) is the mathematical foundation for separating uncertainty sources.
  - Quick check question: Given Var[Y] = E[Var(Y|X)] + Var(E[Y|X]), which term would increase if you expanded the training dataset?

- Concept: Proton Bragg peak and range sensitivity
  - Why needed here: The paper emphasizes uncertainty localization at the distal fall-off; understanding why small path-length changes cause large dose perturbations motivates the clinical relevance.
  - Quick check question: Why does millimeter-scale uncertainty in water-equivalent path length produce larger dose uncertainty at the Bragg peak than in the entrance region?

## Architecture Onboarding

- Component map: Input encoding (beam parameters + medium properties) → vector x → Hidden stack (L_h dense layers with ReLU) → Dropout layers (L_d layers with Bernoulli masking) → Output layer (linear projection to voxel grid) → Post-processing (log-transform + conformal calibration)

- Critical path:
  1. Generate training pairs {(x^(i), d(x^(i)))} from TOPAS/Geant4 MC simulations
  2. Train surrogate D_θ minimizing L2 loss with dropout regularization
  3. At inference: draw S input samples x^(s) ~ Π; for each, run T dropout passes
  4. Compute mean/variance via equations (5) and (6)
  5. Apply split-conformal calibration on held-out set

- Design tradeoffs:
  - L_d : L_h ratio (Example 4): More dropout layers increase epistemic variance; L_d=6, L_h=0 maximizes uncertainty signal but may underfit
  - p_drop: Paper uses 0.05; Example 4 shows instability above ~0.67 for shape model
  - Training samples N: Example 2 shows ~25 samples stabilize mean predictions but ~100 needed for variance convergence
  - Dropout passes T: Example 3 shows means converge faster than variances; T=100-1000 used in experiments

- Failure signatures:
  - Epistemic variance not inflating under distribution shift (Example 2): Check training coverage and dropout configuration
  - Central dose magnitude underestimation (Example 6): May indicate insufficient model capacity or training data
  - Poor calibration (nominal vs empirical coverage gap): Conformal calibration step may be inadequate; check calibration set size
  - Variance concentrated at unexpected locations: Review phantom geometry encoding and input parameterization

- First 3 experiments:
  1. Reproduce 1D analytic benchmark (Example 1): Validate mean prediction accuracy and uncertainty localization at Bragg peak with closed-form ground truth
  2. Training sample convergence study (Example 2): Plot expected dose and variance vs N for in-distribution and shifted inputs to confirm epistemic variance behavior
  3. Dropout pass convergence (Example 3): Vary T and plot variance estimator stability to determine adequate sampling for production use

## Open Questions the Paper Calls Out

- Question: Does the surrogate maintain calibration and accuracy when applied to anatomically realistic patient CT data?
  - Basis in paper: Section 6 states that extending the pipeline to patient CTs is necessary to "test robustness in anatomically realistic settings," as current experiments used only simplified phantoms.
  - Why unresolved: The study relied on 1D analytic benchmarks and 2D/3D water-bone phantoms, which lack the anatomical complexity and noise inherent in clinical patient scans.
  - What evidence would resolve it: Validation of the surrogate on a dataset of patient CTs demonstrating equivalent accuracy and calibrated uncertainty coverage compared to MC.

- Question: Do alternative Bayesian architectures, such as ensembles or variational methods, offer improved calibration over the Monte Carlo dropout approach?
  - Basis in paper: Section 6 notes that "dropout provides a convenient but approximate uncertainty mechanism" and explicitly suggests that "alternatives such as ensembles or variational Bayesian methods warrant exploration."
  - Why unresolved: The current work implemented only a single architecture using MC-dropout; other potential uncertainty quantification techniques were not compared.
  - What evidence would resolve it: A comparative study measuring calibration error (e.g., expected calibration error) between MC-dropout, deep ensembles, and variational Bayes on the same dose prediction tasks.

- Question: Can the differentiable surrogate be successfully integrated into robust optimisation loops to improve planning efficiency?
  - Basis in paper: Section 6 identifies "integration into robust optimisation frameworks and adaptive workflows" as a future direction required to enable "uncertainty-aware planning and near-real-time dose updates."
  - Why unresolved: The paper validated the speed and uncertainty of the forward dose calculation but did not demonstrate the surrogate's performance within a closed-loop treatment planning optimization.
  - What evidence would resolve it: Demonstration of a robust planning workflow where the surrogate replaces the traditional dose engine, showing convergence to a valid plan with reduced computational cost.

## Limitations

- Clinical scalability uncertainty: While the 3D water phantom demonstrates feasibility for volumetric dose prediction, the paper does not validate performance on heterogeneous patient anatomies with complex bone-air-tissue interfaces.

- Training data coverage: With N=50-100 training samples for 3D problems, the surrogate may lack sufficient coverage for rare clinical scenarios and edge cases in proton therapy planning.

- Dropout approximation validity: The Bayesian interpretation of Monte Carlo dropout as posterior sampling is an approximation whose quality is not rigorously established for this specific application.

## Confidence

- High confidence: Speedup claims (orders of magnitude verified through direct timing comparison), basic surrogate accuracy on analytic and water phantom benchmarks, variance decomposition methodology
- Medium confidence: Uncertainty quantification quality, clinical applicability to heterogeneous anatomies, generalization to unseen beam configurations
- Low confidence: Formal Bayesian validity of dropout approximation, calibration robustness across all clinical scenarios, computational scaling with anatomical complexity

## Next Checks

1. Patient-specific validation: Test the surrogate on a diverse set of patient CT datasets with varying anatomies to assess generalization beyond water phantoms and analytic benchmarks. Measure both accuracy degradation and uncertainty calibration.

2. Distribution shift robustness: Systematically evaluate how epistemic uncertainty inflates under controlled input perturbations (energy, position, material properties) beyond training ranges to verify the dropout approximation maintains proper uncertainty estimates.

3. Computational scaling analysis: Profile inference time and memory usage for clinically relevant patient volumes (up to 256³ voxels) to confirm the reported speedup holds and identify potential bottlenecks in practical deployment.