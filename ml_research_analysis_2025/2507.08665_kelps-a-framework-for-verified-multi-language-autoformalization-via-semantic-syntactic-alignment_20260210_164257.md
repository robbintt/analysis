---
ver: rpa2
title: 'KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic
  Alignment'
arxiv_id: '2507.08665'
source_url: https://arxiv.org/abs/2507.08665
tags:
- language
- theorem
- formal
- natural
- kelps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KELPS, a framework for translating informal
  mathematical statements into machine-verifiable formal language. The core idea is
  to first parse natural language into an intermediate representation called Knowledge
  Equations (KEs), grounded in assertional logic, and then use rule-based transformations
  to convert these into multiple target formal languages like Lean4, Coq, and Isabelle.
---

# KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment

## Quick Facts
- arXiv ID: 2507.08665
- Source URL: https://arxiv.org/abs/2507.08665
- Reference count: 40
- KELPS achieves 88.9% syntactic accuracy on MiniF2F, outperforming prior SOTA models.

## Executive Summary
This paper introduces KELPS, a framework for translating informal mathematical statements into machine-verifiable formal language. The core idea is to first parse natural language into an intermediate representation called Knowledge Equations (KEs), grounded in assertional logic, and then use rule-based transformations to convert these into multiple target formal languages like Lean4, Coq, and Isabelle. A key innovation is the controlled generation of synthetic training data through concept-operator templates to address the scarcity of high-quality multilingual formal math corpora. The resulting dataset contains over 60,000 problems. KELPS achieves 88.9% syntactic accuracy on MiniF2F, outperforming prior SOTA models (DeepSeek-V3: 81%, Herald: 81.3%) across multiple benchmarks.

## Method Summary
KELPS is an iterative framework that translates natural language into formal languages via an intermediate Knowledge Equations (KE) representation. A neural model maps NL to KE, a simpler structured representation based on Assertional Logic. A deterministic, rule-based system then translates KE to target formal languages (Lean4, Coq, Isabelle). The framework uses a manually constructed ontology of 40 core concepts and 180 operators to generate synthetic training data through concept-operator templates, addressing the scarcity of real-world NL-FL parallel corpora. The pipeline includes three core components: Semantic Parsing (NL→KE), Syntactic Validation (KE→FL compilation), and Semantic Validation (LLM-as-Judge grading semantic alignment on a 0-5 scale). The model is fine-tuned on a dataset of 60K+ problems, achieving 88.9% syntactic accuracy on MiniF2F.

## Key Results
- Achieves 88.9% syntactic accuracy on MiniF2F, outperforming DeepSeek-V3 (81%) and Herald (81.3%)
- Consistent performance across Lean4, Coq, and Isabelle benchmarks
- Synthetic data generation through concept-operator templates significantly improves model performance and data efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing Knowledge Equations (KE) as an intermediate representation reduces alignment complexity by decomposing the problem into two more manageable sub-tasks.
- Mechanism: The framework avoids direct Natural Language (NL) to Formal Language (FL) translation. Instead, a neural model maps NL to KE, a simpler structured representation based on Assertional Logic. A separate deterministic, rule-based system then translates KE to target FLs (Lean4, Coq, Isabelle). This decomposes the total alignment gap `AG(NL, FL)` into `AG(NL, AL)` + `AG(AL, FL)`, where the second, more rigidly syntactic part is handled by rules rather than learning.
- Core assumption: The KE language is sufficiently expressive to capture the semantics of the target mathematical problems and its structure is significantly easier for an LLM to learn than the full syntax of a theorem prover.
- Evidence anchors:
  - [abstract] "First, we translate natural language into Knowledge Equations (KEs)... Next, we convert them to target languages through rigorously defined rules..."
  - [section 4.4] "Corollary 4.1... AG(NL, FL) = AGsyn(NL, FL) + AGsem(NL, FL)... the AGsyn part can be automatically resolved by our rule-based parser."
  - [corpus] Related work "Towards a Common Framework for Autoformalization" discusses the need for common frameworks, supporting the modular approach.
- Break condition: If the KE language lacks the expressiveness for a new domain (e.g., advanced topology) or if the NL-to-KE mapping becomes as complex as NL-to-FL, the mechanism fails.

### Mechanism 2
- Claim: Generating synthetic training data through a controlled combination of concepts and operators improves model performance and data efficiency compared to using only real-world data.
- Mechanism: The framework uses a manually constructed ontology of 40 core concepts and 180 operators to generate problems via Concept-Operator templates. This allows for the controlled creation of diverse problem sets, filling gaps in scarce real-world NL-FL parallel corpora. The model is trained on this enriched data, leading to better generalization.
- Core assumption: The quality of generated synthetic data, ensured by structured templates and validation, is high enough to be beneficial and does not introduce significant noise that would degrade performance.
- Evidence anchors:
  - [abstract] "A key innovation is the controlled generation of synthetic training data through concept-operator templates..."
  - [section 3.3] "Our pipeline first establishes a mathematics ontology... employs dual generation strategies: (1) translation... (2) template-based synthesis..."
  - [section 4.3] Table 2 shows performance improving as the ratio of synthetic to authentic data increases, demonstrating the value of synthetic data.
- Break condition: If the templates are too rigid or combinations are not semantically meaningful, the model may overfit to template structures and fail on novel, organic problem statements.

### Mechanism 3
- Claim: A multi-stage pipeline combining neural semantic parsing with symbolic syntax checking and LLM-based semantic validation yields higher-quality formalization than neural methods alone.
- Mechanism: The system operates in three stages. First, a neural model parses NL into KE. Second, a symbolic parser and compiler verify syntactic correctness. Third, an LLM-as-a-Judge scores semantic alignment on a graded scale, filtering for meaning preservation. This catches both syntax errors and semantic drift.
- Core assumption: The LLM-as-a-Judge is a reliable proxy for human semantic validation and the symbolic parsers are sound for their target languages.
- Evidence anchors:
  - [abstract] "KELPS is an iterative framework for translating, synthesizing, and filtering... achieving 88.9% syntactic accuracy"
  - [section 3.2] Describes the three core components: Semantic Parsing, Syntactic Validation, and Semantic Validation (graded 0-5 scale).
  - [corpus] Corpus evidence for the specific "LLM-as-a-Judge" validation method is weak or missing; rely on paper text.
- Break condition: If the LLM judge is systematically biased or if the symbolic parser's rules are buggy, the pipeline will either pass incorrect formalizations or reject correct ones.

## Foundational Learning

### Concept: Assertional Logic (AL) & Knowledge Equations (KE)
- Why needed here: This is the core intermediate representation. Understanding its syntax (Declaration, Fact, Query) and semantics is essential to comprehend how the model is trained and how translation works.
- Quick check question: What are the three parts of a Knowledge Equation and how does an assertion in AL differ from a standard proposition in first-order logic?

### Concept: Neuro-Symbolic Systems
- Why needed here: KELPS is a hybrid system, combining the pattern-matching strengths of LLMs with the formal guarantees of rule-based parsers. This distinction is key to understanding design choices and failure modes.
- Quick check question: Identify which parts of the KELPS pipeline are 'neural' and which are 'symbolic', and state one advantage of this hybrid approach.

### Concept: Syntactic vs. Semantic Correctness in Formal Languages
- Why needed here: The paper explicitly separates evaluation into these two categories. A statement can be syntactically correct (compiles) but semantically wrong. Grasping this is critical for debugging.
- Quick check question: A generated Lean4 statement compiles but states a number must be negative when the problem says positive. Is this a syntactic or semantic error?

## Architecture Onboarding

### Component map:
Input (NL) -> KELPS Translator (Neural) -> AL Parser (Symbolic) -> Target Language Compiler (Symbolic) -> LLM-as-Judge (Neural) -> Output (FL)

### Critical path:
Data flows from NL input through the KELPS Translator to produce a KE. This KE must pass the AL Parser. The parsed KE is then converted to the target FL. This FL must pass the target compiler. Finally, the compiled FL must pass the LLM-as-Judge. Any failure at these validation steps breaks the loop.

### Design tradeoffs:
- KE vs. Direct FL: Using an IR adds a step but decouples the neural model from complex FL syntax, aiding multi-language support. The tradeoff is potential information loss in NL->KE.
- Template vs. Freeform Synthesis: Controlled templates ensure high-quality, diverse data but may lack the organic complexity of real problems, potentially limiting generalization.
- LLM Judge vs. Human Judge: Scalable and cheaper than human review, but less reliable and may introduce subtle biases.

### Failure signatures:
- High syntax error rate (compilation failure): Indicates issues in the NL->KE mapping or gaps in the KE->FL translation rules.
- High semantic error rate (low judge scores): Indicates the model is misunderstanding the NL problem or the LLM judge is being too strict/biased.
- Model generates undefined operators: A specific failure noted in Section 4.4, where the model creates operators not in the ontology, causing a syntax validation failure.
- Semantic Misalignment (Section C.2): Examples like omitting conditions (Fig 15) or misunderstanding problem intent (Fig 14). This often passes syntax checks but is caught by the judge.

### First 3 experiments:
1. Reproduce the ablation study on synthetic data (Section 4.3). Train the KELPS model with varying ratios of synthetic to real data (1:0, 1:0.5, 1:1, 1:1.5) on a subset of the data and plot performance on MiniF2F.
2. Evaluate the multilingual capability. Use the trained model to translate a set of NL problems into both Lean4 and Isabelle. Compare the pass@1 accuracy for both to confirm if performance is more consistent compared to a baseline like DeepSeek-V3, as claimed in Section 4.2.
3. Test the error modes. Manually construct or select a set of "adversarial" problems likely to cause specific failures (e.g., problems requiring operators not in the ontology, problems with ambiguous natural language) and analyze the failure cases (syntax vs. semantic).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Knowledge Equation (KE) framework effectively scale to advanced mathematical domains (e.g., topology, abstract algebra) without requiring exponential manual effort in defining new Concept-Operator templates?
- Basis in paper: [explicit] The authors state the ontology is currently limited to K-12/undergraduate concepts and identify expanding to advanced domains as a key future direction.
- Why unresolved: The current methodology relies heavily on manually curated templates; it is unclear if this approach is sustainable for the higher complexity and abstraction of graduate-level mathematics.
- What evidence would resolve it: Demonstration of the framework successfully formalizing a benchmark of graduate-level theorems (e.g., from Mathlib) without a proportional explosion in manual definition overhead.

### Open Question 2
- Question: How can the automated translation pipeline be improved to resolve the type conversion errors that currently prevent 10-20% of syntactically parsed statements from compiling?
- Basis in paper: [explicit] Section 3.2.2 and Appendix C note that 10-20% of statements parsed by the AL parser fail compiler validation due to type conversion errors, specifically in distinguishing between natural and real numbers.
- Why unresolved: The intermediate Knowledge Equation representation currently lacks the sophisticated type inference required to bridge the gap between ambiguous natural language and strict formal type systems.
- What evidence would resolve it: An updated AL-to-FL translation rule set that automatically infers correct coercions, significantly reducing the validation failure rate on complex arithmetical problems.

### Open Question 3
- Question: To what extent does the reliance on "LLM-as-a-Judge" for semantic validation introduce noise or bias into the resulting formalization dataset?
- Basis in paper: [explicit] Section 4.4 explicitly states that the semantic verification module "is not completely reliable" and that "instances of misalignment between NL and FL persist" in the training data.
- Why unresolved: Using an LLM to evaluate the output of another LLM creates a circular dependency that may fail to catch subtle semantic hallucinations or mathematical inaccuracies.
- What evidence would resolve it: A comparative analysis quantifying the error rate of the LLM-judge against a human-annotated gold standard dataset of semantic alignments.

## Limitations

- The evaluation relies heavily on synthetic data and a single LLM-as-a-Judge for semantic validation, which introduces potential bias and limits generalizability to truly unseen problems.
- The framework's performance on problems outside its carefully curated ontology (40 concepts, 180 operators) remains untested.
- The 1,200 manually annotated NL-KE pairs required three weeks of effort by two graduate students, indicating significant human cost that may not scale easily.

## Confidence

- **High Confidence**: The syntactic accuracy results (88.9% pass@1 on MiniF2F) are directly measurable and reproducible. The modular architecture (NL→KE→FL) is clearly described and logically sound.
- **Medium Confidence**: The synthetic data generation methodology is well-specified, but the long-term effectiveness of template-based generation versus more diverse, organic data sources is uncertain. The claim of consistent multilingual performance is supported but based on limited comparisons.
- **Low Confidence**: The semantic accuracy assessment via LLM-as-a-Judge, while methodologically interesting, lacks external validation and may be sensitive to judge configuration. The claim that this approach is superior to neural-only methods is not definitively proven.

## Next Checks

1. **External Semantic Validation**: Have the top-performing KELPS formalizations evaluated by a second, independent LLM judge (different model) and by human experts on a random sample to assess the reliability of the semantic scoring system.

2. **Robustness to Out-of-Ontology Problems**: Construct a test set of problems that require concepts or operators outside the current 40/180 ontology and evaluate whether KELPS fails gracefully or produces misleading formalizations.

3. **Long-Term Data Quality Study**: Track the performance of models trained on increasing proportions of synthetic vs. authentic data over multiple training iterations to determine if the synthetic data advantage persists or diminishes as the model encounters more real-world problems.