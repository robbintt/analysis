---
ver: rpa2
title: 'MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series
  Forecasting'
arxiv_id: '2602.00624'
source_url: https://arxiv.org/abs/2602.00624
tags:
- modex
- layer
- input
- forecasting
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoDEx is a lightweight, high-performance model for long-term time
  series forecasting that exploits depth-specific MLP experts within a Mixture-of-Experts
  framework. By analyzing layer sensitivity, a gradient-based metric, the authors
  show that MLP layers capture distinct temporal patterns at different depths, motivating
  the use of depth-specific experts.
---

# MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2602.00624
- **Source URL:** https://arxiv.org/abs/2602.00624
- **Reference count:** 6
- **Primary result:** Lightweight MoE model with depth-specific MLP experts achieves state-of-the-art LTSF accuracy on 7 benchmarks while using significantly fewer parameters than existing models

## Executive Summary
MoDEx is a novel lightweight model for long-term time series forecasting that exploits depth-specific MLP experts within a Mixture-of-Experts framework. By analyzing layer sensitivity through gradient backpropagation, the authors demonstrate that MLP layers capture distinct temporal patterns at different depths, motivating the use of heterogeneous experts. The model achieves state-of-the-art accuracy on seven real-world benchmarks while using significantly fewer parameters and lower computational cost than existing models.

## Method Summary
MoDEx replaces standard backbone layers in time series forecasting models with a Mixture-of-Experts module containing three depth-specific MLP experts (1-layer, 2-layer, and 3-layer). A learnable translation mechanism shifts feature distributions positively to mitigate information loss from normalization, and a router assigns gating coefficients to weight expert outputs, which are aggregated residually. The architecture can be integrated into existing transformer backbones by replacing self-attention modules.

## Key Results
- Achieves state-of-the-art accuracy on seven benchmarks, ranking first in 78% of cases
- Uses 280K parameters and 3.83M FLOPs on ETTm2 while achieving MSE of 0.285 and MAE of 0.325
- Demonstrates plug-in generalizability by improving transformer variants when replacing self-attention with MoDEx
- Provides a lightweight alternative to existing models with significantly fewer parameters and lower computational cost

## Why This Works (Mechanism)

### Mechanism 1: Layer Sensitivity Reveals Depth-Specific Input Attribution
Different backbone layers exhibit distinct sensitivity patterns to input sequence regions, correlating with intrinsic periodicity and trend dynamics. Gradient backpropagation from intermediate features to input computes a sensitivity matrix capturing both positive and negative input contributions per layer, reflecting functional specialization rather than random initialization artifacts.

### Mechanism 2: Depth-Varying MLPs as Specialized Temporal Experts
MLPs with different depths capture qualitatively different temporal patterns, motivating their use as heterogeneous experts. Three experts process input in parallel, with a learned router assigning gating coefficients to weight expert outputs, which are aggregated residually. The router learns meaningful input-dependent routing that exploits depth-specific predictive power.

### Mechanism 3: Learnable Translation Mitigates Normalization-Induced Feature Suppression
Adding a learnable translation term before activation shifts feature distributions positively, reducing information loss from ReLU-like zeroing. A learnable bias term is added to input features prior to activation, shifting the distribution so most values become non-negative on average, counteracting standard normalization's bias toward zero-centered data.

## Foundational Learning

- **Concept: Gradient-based attribution / sensitivity analysis**
  - Why needed: Layer sensitivity is derived from Jacobian backpropagation; understanding GradCAM and effective receptive field concepts clarifies why absolute gradients capture input influence
  - Quick check: Given a 3-layer MLP with hidden dimension D and input length L, what is the shape of ∂G^l/∂x and how does row-wise mean aggregation reduce it?

- **Concept: Mixture-of-Experts (MoE) routing**
  - Why needed: MoDEx's core architecture; gating network must learn to route inputs to depth-specific experts based on temporal characteristics
  - Quick check: If gating coefficients p_j sum to 1 and three experts have outputs h_1, h_2, h_3, write the aggregated output with residual connection

- **Concept: RevIN (Reversible Instance Normalization) in time series**
  - Why needed: Paper references RevIN as standard LTSF preprocessing; understanding its statistics-removing effect motivates the learnable translation correction
  - Quick check: Why might zero-centering via RevIN conflict with ReLU activations, and how does learnable translation address this?

## Architecture Onboarding

- **Component map:** Input x (L-length sequence) → embedding → z ∈ R^D → learnable translation → z + bias → router → gating coefficients p_1, p_2, p_3 → each expert h_j processes z → outputs o_j → aggregation: z̃ = z + Σ p_j · o_j → z̃ → prediction network → y ∈ R^H

- **Critical path:** 1. Input x → embedding → z ∈ R^D; 2. z → learnable translation → z + bias; 3. Translated z → router → gating coefficients p_1, p_2, p_3; 4. Each expert h_j processes z → outputs o_j; 5. Aggregation: z̃ = z + Σ p_j · o_j; 6. z̃ → prediction network → y ∈ R^H

- **Design tradeoffs:** 3 experts vs. more: Paper finds 3 sufficient; more experts increase parameters with diminishing returns. Parallel MoE (MoDEx) vs. sequential "Dense" variant: Dense saves ~18-24% parameters/FLOPs but incurs up to 2.3% MSE increase. Router complexity: Simple linear router keeps overhead low; more expressive routers may not justify cost.

- **Failure signatures:** Router collapse: Gating coefficients converge to near-deterministic single-expert selection → depth-specific diversity lost. Translation overfitting: Learnable bias becomes dataset-specific, failing to generalize. Expert correlation: Outputs of different-depth experts become highly correlated → MoE reduces to ensemble of near-identical models. Memory explosion on high-dimensional variates: While MoDEx has linear complexity O(N(L+H)), extremely large N may still exceed memory if hidden dimension D scales with N.

- **First 3 experiments:** 1. Replicate layer sensitivity visualization on a 3-layer MLP backbone using ETTm2; verify distinct sensitivity patterns per layer across sequences with different periodicity. 2. Ablate expert depth diversity by replacing depth-specific experts with identical 3-layer MLPs; compare MSE/MAE on ETTm2 horizons against full MoDEx. 3. Integrate MoE module into iTransformer by replacing self-attention; evaluate on ETTh1/ETTm2 to replicate improvements, confirming plug-in generalizability.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the theoretically optimal number of depth-specific experts, and how does it relate to dataset characteristics such as intrinsic periodicity, trend complexity, and lookback window length? The authors state that "empirical results show that three experts is sufficient" without providing theoretical justification for this choice.

- **Open Question 2:** Does depth-specific specialization extend to architectures beyond MLPs and Transformers, such as CNNs, RNNs, or state-space models? Layer sensitivity analysis was conducted on a three-layer MLP backbone, and MoDEx was shown to improve Transformer variants, but the phenomenon was not evaluated on CNN-based or recurrent architectures.

- **Open Question 3:** Can layer sensitivity be used as a proactive design tool for architecture search rather than a post-hoc analysis metric? The authors introduce layer sensitivity to reveal depth-specific behavior and motivate MoDEx, but use it only for interpretation after model construction, not as a criterion during architecture selection.

- **Open Question 4:** What is the mathematical relationship between layer depth and the temporal patterns captured by each expert? The paper empirically observes that "layers at different depths exhibit varying sensitivity to the input sequence's intrinsic periodicity and trend dynamics" but offers no theoretical explanation for this phenomenon.

## Limitations
- Missing experimental details including specific hidden dimension, exact optimizer hyperparameters, and router architecture require reasonable assumptions
- Learnable translation mechanism lacks quantitative ablation demonstrating its necessity beyond qualitative comparisons
- Layer sensitivity diagnostic has no direct validation that observed depth-specific patterns causally drive performance gains
- Router architecture details affecting generalization are unspecified

## Confidence
- **High Confidence**: MoDEx achieves SOTA accuracy on seven benchmarks with clear empirical ranking (78% first-place finishes); architecture structure (3 depth-specific experts + residual MoE aggregation) is explicitly defined and implementable
- **Medium Confidence**: Layer sensitivity metric meaningfully reveals depth-specific temporal specialization; learnable translation improves performance by shifting feature distributions; UMAP visualizations showing distinct expert feature distributions
- **Low Confidence**: The causal relationship between observed sensitivity patterns and expert design choices; translation mechanism's necessity without direct ablation; router architecture details affecting generalization

## Next Checks
1. Replicate layer sensitivity visualization on 3-layer MLP backbone using ETTm2 to verify distinct sensitivity patterns across sequences with different periodicity
2. Ablate expert depth diversity by replacing depth-specific experts with identical 3-layer MLPs; compare MSE/MAE on ETTm2 horizons against full MoDEx
3. Integrate MoE module into iTransformer by replacing self-attention; evaluate on ETTh1/ETTm2 to replicate Table 4 improvements, confirming plug-in generalizability