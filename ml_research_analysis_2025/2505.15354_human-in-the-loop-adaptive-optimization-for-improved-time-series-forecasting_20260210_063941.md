---
ver: rpa2
title: Human in the Loop Adaptive Optimization for Improved Time Series Forecasting
arxiv_id: '2505.15354'
source_url: https://arxiv.org/abs/2505.15354
tags:
- time
- forecasting
- series
- feedback
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a post-training optimization framework for
  improving time series forecasting accuracy without retraining or architectural changes.
  The method applies expressive transformations optimized via reinforcement learning,
  contextual bandits, or genetic algorithms to correct model outputs.
---

# Human in the Loop Adaptive Optimization for Improved Time Series Forecasting

## Quick Facts
- arXiv ID: 2505.15354
- Source URL: https://arxiv.org/abs/2505.15354
- Reference count: 8
- Primary result: Post-training optimization improves time series forecasting accuracy without retraining via affine corrections and interpretable transformations.

## Executive Summary
This paper introduces a post-training optimization framework that improves time series forecasting accuracy without requiring model retraining or architectural changes. The method applies expressive transformations optimized via reinforcement learning, contextual bandits, or genetic algorithms to correct model outputs. Theoretically, affine corrections are proven to always reduce mean squared error. The framework also supports an optional human-in-the-loop component where domain experts can guide corrections using natural language, which is parsed into actions by a large language model. Experiments on diverse benchmarks demonstrate consistent accuracy gains with minimal computational overhead.

## Method Summary
The framework applies post-training corrections to existing forecasting models through a pool of interpretable transformations optimized against validation data. Base models (ARIMA, LSTM, Transformer, etc.) generate predictions that are then modified by actions like scaling, shifting, or piecewise transformations. These actions are parameterized and optimized using methods like Random Search, Successive Halving HPO (SH-HPO), Proximal Policy Optimization (PPO), or Genetic Algorithms. For human-in-the-loop interaction, a large language model (qwen2-72b-32k) parses natural language feedback into executable transformations. The approach is model-agnostic and computationally lightweight, requiring only validation data for optimization.

## Key Results
- Affine corrections theoretically guarantee MSE reduction and achieve average improvements of 2.06-4.96% across benchmarks
- SH-HPO optimization outperforms other methods (4.96% avg improvement vs 1.76% for PPO) on the Nature dataset
- Human feedback integration via LLM parsing enables domain expert guidance, with demonstrated RMSE improvements on Dominick dataset
- The framework achieves consistent accuracy gains across diverse datasets (ETTh1/2, ETTm1/2, Dominick, Human, KDD, Nature, NASDAQ, Pedestrian, Tourism, Vehicle trips, Weather) with minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Affine transformations applied to model outputs systematically reduce or maintain mean squared error on validation data.
- Mechanism: Optimal scaling (a*) and shifting (b*) parameters are computed analytically from validation statistics—specifically, a* = Cov(Y_true, Y_pred)/Var(Y_pred) and b* = E[Y_true] − a*E[Y_pred]. This corrects systematic bias and variance misalignment between predictions and ground truth.
- Core assumption: Validation and test data are drawn from the same distribution; base model predictions have non-zero covariance with ground truth (i.e., are not random).
- Evidence anchors:
  - [abstract] "Theoretically, we prove that affine corrections always reduce the mean squared error"
  - [section 3.1, Theorem 1] Derivation showing R_before − R_after = [√Var(Y_pred) − Cov(Y_true, Y_pred)/√Var(Y_pred)]² ≥ 0
  - [corpus] Limited direct support—related post-hoc revision papers (e.g., "Improving Time Series Forecasting via Instance-aware Post-hoc Revision") propose similar corrections but without the affine MSE guarantee
- Break condition: Distribution shift between validation and deployment; base model predictions with near-zero variance or near-zero covariance with ground truth; models already near-optimal.

### Mechanism 2
- Claim: Hybrid discrete-continuous action spaces enable efficient, interpretable correction discovery compared to pure continuous optimization.
- Mechanism: The framework defines a discrete set of interpretable actions (e.g., Scale Amplitude, Piecewise Scaling, Linear Trend), each with continuous parameters. Optimizers (Random Search, SH-HPO, PPO, GA) search over this constrained space, reducing combinatorial complexity while preserving expressiveness.
- Core assumption: Optimal corrections decompose into atomic transformations; validation loss improvement correlates with test performance; action parameter ranges are appropriately bounded.
- Evidence anchors:
  - [section 3.3.3] "Discrete actions reduce the combinatorial search space, enabling scalable optimization via bandits or RL"
  - [Table 1] Random Search (4.84% avg improvement) and SH-HPO (4.96%) substantially outperform PPO (1.76%) and GA (2.32%) on Nature dataset
  - [corpus] AlphaCast paper similarly uses structured action spaces for LLM-guided forecasting, but with different optimization strategy
- Break condition: When optimal correction requires non-decomposable or highly nonlinear transformations not representable in action pool; when parameter discretization (required for PPO/GA) introduces approximation error.

### Mechanism 3
- Claim: Natural language expert feedback can be systematically converted into executable post-training transformations via LLM parsing.
- Mechanism: A large language model (qwen2-72b-32k) maps free-text instructions (e.g., "increase values above 80th percentile by 10%") to structured action-parameter pairs, which are added to the optimization pool and evaluated against validation data.
- Core assumption: LLM correctly interprets domain expert intent; human-identified patterns reflect systematic biases generalizable beyond anecdotal examples; feedback is unambiguous and internally consistent.
- Evidence anchors:
  - [section 3.5] "Users provide free-text feedback...which is parsed by a large language model (LLM) into a structured set of transformations"
  - [Figure 6] Shows RMSE improvement from human feedback on Dominick dataset with PatchTST model
  - [corpus] AlphaCast paper corroborates value of human-LLM co-reasoning for interactive time series forecasting, though with different integration approach
- Break condition: Ambiguous or contradictory feedback; LLM misinterpretation; overfitting to expert's idiosyncratic observations; feedback that violates action parameter constraints.

## Foundational Learning

- Concept: Affine transformations and optimal parameter derivation
  - Why needed here: Understanding how a* and b* are computed from validation statistics is essential for implementing and debugging the core correction mechanism.
  - Quick check question: Given predictions Y_pred = [2, 4, 6] and ground truth Y_true = [2.5, 4.5, 7.5], compute the optimal affine correction parameters a* and b*.

- Concept: Contextual bandits vs. reinforcement learning trade-offs
  - Why needed here: The framework offers multiple optimization strategies; selecting the right one requires understanding exploration-exploitation dynamics and search space structure.
  - Quick check question: Why does SH-HPO (a bandit algorithm using successive halving) outperform PPO on this task, despite PPO being more sophisticated?

- Concept: Quantile-based conditional transformations
  - Why needed here: Several actions (Piecewise Scale High/Low, Increase Minimum Factor) operate conditionally on quantile thresholds—understanding this is critical for interpreting and extending the action pool.
  - Quick check question: Mathematically, what does the action "scale values above the 90th percentile by factor f" do to a time series?

## Architecture Onboarding

- Component map: Base forecasting model -> Optimizer module -> Action pool -> Validation evaluator -> (Optional) LLM feedback parser -> Final corrected forecast

- Critical path: Input time series -> Base model predictions -> Optimizer selects action + parameters -> Apply transformation -> Evaluate on validation set -> Iterate until convergence -> (Optional) Inject human feedback via LLM -> Final corrected forecast

- Design tradeoffs:
  - Random Search vs. SH-HPO: Random is simpler and faster; SH-HPO yields slightly higher consistency (4.96% vs 4.84% avg improvement) but with more complexity
  - Action pool size: More actions increase expressiveness but slow optimization (7 actions: ~45s vs. 2 actions: ~3.2s at horizon 96)
  - Automated vs. HITL: Fully automated is faster; HITL injects domain knowledge but requires expert availability and clear feedback

- Failure signatures:
  - Negative MSE improvements (e.g., PatchTST shows -2.25% on ETTh1) indicate action space mismatch or overfitting to validation set
  - High variance across runs suggests optimizer instability or insufficient exploration
  - LLM-generated actions failing to compile or producing unexpected transformations indicate prompt/parsing issues

- First 3 experiments:
  1. Validate affine correction theory: Train a simple model (e.g., ridge regression) on synthetic linear data, compute a* and b* analytically, apply correction, and verify MSE reduction matches Theorem 1.
  2. Compare optimizers on single benchmark: Run Random, SH-HPO, PPO, and GA on ETTh1 with Autoformer across horizons 96, 192, 336, 720; record improvement percentage and wall-clock time.
  3. Test human feedback pipeline: Provide structured natural language inputs (e.g., "reduce peak values by 20%") through the LLM parser, verify generated code matches intent, and measure RMSE delta on a held-out test split.

## Open Questions the Paper Calls Out
None

## Limitations
- Negative MSE improvements on some model-dataset combinations (e.g., -2.25% for PatchTST on ETTh1) suggest potential overfitting or action space misalignment
- The LLM-based human feedback component lacks quantitative evaluation of parsing accuracy and domain expert impact
- Optimizer comparison shows SH-HPO outperforming PPO despite PPO's sophistication, suggesting the discrete-continuous action structure may not fully leverage RL's strengths

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| MSE reduction via affine corrections (proven analytically and validated empirically) | High |
| Optimizer effectiveness (supported by ablation but lacks hyperparameter sensitivity analysis) | Medium |
| Human-in-the-loop utility (demonstrates concept but limited quantitative validation) | Medium |

## Next Checks

1. Replicate affine correction proof on synthetic data with controlled bias/variance to verify theoretical bounds hold under distribution shift
2. Conduct ablation study isolating LLM parsing accuracy by comparing expert feedback vs automated generation across 10 diverse datasets
3. Test action chaining capability by allowing sequential application of top-3 actions and measuring compound improvement vs single action selection