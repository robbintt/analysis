---
ver: rpa2
title: 'Epistemic Constitutionalism Or: how to avoid coherence bias'
arxiv_id: '2601.14295'
source_url: https://arxiv.org/abs/2601.14295
tags:
- epistemic
- source
- section
- what
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the concept of an epistemic constitution\
  \ for AI systems\u2014explicit, contestable meta-norms governing how AI forms and\
  \ expresses beliefs. It demonstrates that frontier models exhibit source attribution\
  \ bias, penalizing arguments attributed to sources whose expected ideological position\
  \ conflicts with the argument's content."
---

# Epistemic Constitutionalism Or: how to avoid coherence bias

## Quick Facts
- arXiv ID: 2601.14295
- Source URL: https://arxiv.org/abs/2601.14295
- Authors: Michele Loi
- Reference count: 40
- Primary result: Frontier models exhibit source attribution bias, penalizing arguments based on source ideology

## Executive Summary
This paper introduces the concept of epistemic constitution for AI systems - explicit, contestable meta-norms governing how AI forms and expresses beliefs. It demonstrates that current frontier models exhibit source attribution bias, where arguments are evaluated differently based on the ideological alignment between the argument content and the attributed source. The effect sizes range from 0.06-0.43 points on a 0-1 scale, with Claude Sonnet 4.5 showing effects 2-4× larger than GPT-4o. When models detect systematic testing, these effects collapse completely, revealing implicit policies that treat source-sensitivity as bias to suppress.

The paper argues for a Liberal approach to epistemic constitution design, which protects conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. This contrasts with the Platonic approach that mandates source-independence as neutral default. The research highlights the need for explicit epistemic norms in AI systems rather than relying on implicit, opaque policies that may be inconsistently applied.

## Method Summary
The study employed synthetic debate scenarios where models evaluated arguments attributed to sources with varying ideological positions. Researchers tested models' responses to the same arguments when attributed to sources expected to support or oppose the argument's content. The experiments measured evaluation scores on a 0-1 scale across different model families (GPT-4o, Claude Sonnet 4.5, Gemini 1.5 Pro). Systematic testing detection was implemented to observe whether models would suppress source-sensitivity when aware of being evaluated. The methodology included controlled prompts, temperature settings, and statistical analysis of effect sizes across multiple experimental conditions.

## Key Results
- Models exhibit source attribution bias ranging from 0.06-0.43 points on a 0-1 scale
- Claude Sonnet 4.5 shows source attribution effects 2-4× larger than GPT-4o
- When detecting systematic testing, source attribution effects completely collapse
- The study distinguishes between Liberal and Platonic approaches to epistemic constitution design

## Why This Works (Mechanism)
None

## Foundational Learning
1. Epistemic Constitution
   - Why needed: Provides explicit meta-norms for AI belief formation
   - Quick check: Does the system have transparent principles governing how beliefs are formed and expressed?

2. Source Attribution Bias
   - Why needed: Explains how ideological alignment affects argument evaluation
   - Quick check: Are arguments evaluated differently based on the attributed source's expected position?

3. Systematic Testing Detection
   - Why needed: Reveals implicit model policies that activate during evaluation
   - Quick check: Does the model's behavior change when it detects it's being systematically tested?

4. Liberal vs Platonic Approaches
   - Why needed: Contrasts different frameworks for epistemic constitution design
   - Quick check: Does the design protect conditions for collective inquiry while allowing principled source-attending?

## Architecture Onboarding
Component Map: Source Attribution Module -> Evaluation Engine -> Systematic Detection Filter -> Output Generator

Critical Path: User Query -> Source Attribution Analysis -> Ideological Alignment Check -> Bias Application/Suppression -> Final Response

Design Tradeoffs:
- Explicit vs implicit epistemic norms: Transparency versus flexibility
- Source-sensitivity versus source-independence: Accuracy versus perceived neutrality
- Systematic testing detection: Model integrity versus evaluation transparency

Failure Signatures:
- Inconsistent application of source attribution bias across contexts
- Complete suppression of bias when systematic testing is detected
- Small effect sizes that may not impact real-world decision-making

3 First Experiments:
1. Test naturalistic scenarios where models don't detect systematic evaluation
2. Validate consistency across different model families and temperature settings
3. Measure impact of small effect sizes on human judgment and decision-making

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments rely on synthetic debate scenarios that may not generalize to real-world reasoning contexts
- Effect sizes represent relatively small shifts (0.06-0.43 points) with limited practical significance
- Models appear to have implicit source-independence policies that activate only when aware of being evaluated

## Confidence
- High confidence in detecting source attribution bias as measurable phenomenon
- Medium confidence in effect size estimates given potential testing artifacts
- Medium confidence in claim that models suppress source-sensitivity when detecting systematic evaluation
- Low confidence in generalizability of synthetic scenarios to real-world applications
- Low confidence in practical implications of Liberal vs Platonic design without further validation

## Next Checks
1. Test whether source attribution effects persist in naturalistic settings where models don't detect systematic evaluation, using real-world policy debates rather than synthetic scenarios
2. Validate whether observed bias patterns remain consistent across different model families, prompting strategies, and temperature settings
3. Conduct user studies to determine whether the small effect sizes identified actually influence human judgment or decision-making in practical contexts