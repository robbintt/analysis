---
ver: rpa2
title: 'Seeing the Big Picture: Evaluating Multimodal LLMs'' Ability to Interpret
  and Grade Handwritten Student Work'
arxiv_id: '2510.05538'
source_url: https://arxiv.org/abs/2510.05538
tags:
- student
- work
- mathematical
- visual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated multimodal large language models (MLLMs)
  on two handwritten mathematics tasks: arithmetic problems (288 responses) and mathematical
  illustrations (150 responses). On arithmetic, the best model achieved 95% accuracy
  (k=0.90), matching human performance, though with some error patterns humans wouldn''t
  make.'
---

# Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work

## Quick Facts
- arXiv ID: 2510.05538
- Source URL: https://arxiv.org/abs/2510.05538
- Reference count: 38
- Key outcome: Best MLLM achieved 95% accuracy (k=0.90) on arithmetic, matching human performance, but struggled with mathematical illustrations (k=0.20-0.38), improving to k=0.43-0.47 with human descriptions

## Executive Summary
This study evaluates multimodal large language models (MLLMs) on two handwritten mathematics assessment tasks: arithmetic problems with objective answers and mathematical illustrations requiring visual interpretation and pedagogical judgment. The research reveals that while current MLLMs can match human performance on routine arithmetic grading, they face significant limitations in interpreting complex student mathematical illustrations. The performance gap is primarily driven by visual interpretation challenges, though even with human-provided descriptions, models plateau at moderate agreement levels, suggesting fundamental limitations in capturing educators' tacit knowledge.

## Method Summary
The study evaluates four MLLMs (Claude 3.5/3.7, Gemini 2.5 Pro, GPT-4.1) on two datasets: 288 arithmetic responses from Rising Academies and 150 mathematical illustrations from DrawEDU. Using structured prompts, models extract question numbers, rough work descriptions, final answers, and correctness judgments. Two experimental conditions are tested for illustrations: image-only and image-plus-human-description. Performance is measured using accuracy, Linear Weighted Kappa (LWK), and confusion matrix analysis, with human inter-rater agreement (k=0.45) serving as a benchmark for the illustration task.

## Key Results
- MLLMs achieved 95% accuracy (k=0.90) on arithmetic problems, matching human performance
- Direct image interpretation of mathematical illustrations yielded only k=0.20-0.38 agreement
- Human-provided descriptions improved illustration assessment to k=0.43-0.47, approaching human-to-human agreement
- Models consistently rejected correct final answers when intermediate work appeared flawed, revealing non-human error patterns

## Why This Works (Mechanism)

### Mechanism 1: Vision-Grading Decomposition
Handwritten mathematics assessment can be decomposed into visual interpretation (perceiving what is written) and mathematical assessment (judging correctness). By measuring vision accuracy independently and comparing to grading accuracy, the framework isolates whether failures stem from perception or judgment. Models achieved grading accuracy 6-8 percentage points higher than vision accuracy, suggesting context-based compensation.

### Mechanism 2: Visual Bottleneck Effect on Illustrations
Visual interpretation is the primary limiting factor for MLLM performance on mathematical illustrations. Providing human-authored descriptions bypasses visual processing limitations, allowing assessment based on accurate semantic representation. The κ gains (+0.23 to +0.32) quantify the visual bottleneck magnitude.

### Mechanism 3: Tacit Knowledge Ceiling
Even with visual information optimized, MLLMs plateau at moderate agreement (κ≈0.43-0.47) because they lack educators' tacit developmental and contextual knowledge. Expert educators apply situated knowledge—developmental trajectories, classroom conventions, recognition of partial understanding in unconventional representations—that cannot be captured in minimal prompting.

## Foundational Learning

- Concept: **Linear Weighted Kappa (LWK)**
  - Why needed here: The paper uses LWK as the primary agreement metric, and interpreting κ=0.90 vs. κ=0.47 requires understanding that κ accounts for chance agreement—values above 0.80 indicate near-perfect agreement, while 0.40-0.60 represents moderate agreement
  - Quick check question: If two raters agree 90% of the time on a binary classification where 50% of items are positive, is their agreement impressive? (Hint: Consider chance agreement.)

- Concept: **Tacit Knowledge in Assessment**
  - Why needed here: The central claim is that models lack educators' implicit expertise; understanding what this knowledge includes (developmental patterns, classroom context, partial understanding recognition) is essential for evaluating the "human-in-the-loop" recommendations
  - Quick check question: Why might a student's incorrect formula still demonstrate sophisticated mathematical thinking to an experienced teacher?

- Concept: **Human-in-the-Loop System Design**
  - Why needed here: The paper explicitly recommends hybrid deployment where MLLMs screen routine work and flag edge cases; understanding when to trust automation versus require human judgment is the practical implication
  - Quick check question: What characteristics of a task make it suitable for full automation versus requiring human oversight?

## Architecture Onboarding

- Component map: Scanned worksheet images + optional human descriptions -> MLLM visual encoder -> Structured output (question ID, rough work transcription, final answer) -> Binary correctness judgment -> Accuracy, LWK, precision/recall metrics

- Critical path: Dataset preparation with ground truth labels -> Structured prompting requesting question number, rough work description, exact final answer, correctness judgment -> Evaluation against ground truth using accuracy, LWK, and confusion matrix analysis -> Vision-grading decomposition by comparing transcription accuracy to grading accuracy

- Design tradeoffs: Minimal vs. elaborate prompting (simple binary rubrics for generalizability vs. sophisticated prompts that might improve performance); Model selection (frontier models may not justify cost for illustration tasks); Human augmentation strategy (descriptions improve assessment but require annotation overhead)

- Failure signatures: Non-human error patterns (rejecting correct final answers due to flawed intermediate work); False positive asymmetry (14-21% vs. 1.4-3.8% for top performer); Illustration interpretation collapse (κ=0.20-0.38 far below human agreement)

- First 3 experiments: Baseline replication on arithmetic to validate κ≈0.90 performance; Vision-only stress test with ground-truth transcriptions to isolate pure assessment capability; Prompt sensitivity analysis testing pedagogical context in prompts

## Open Questions the Paper Calls Out

- Can sophisticated prompting strategies or fine-tuning elevate MLLM grading performance on mathematical illustrations beyond the observed ceiling (k≈0.47) found using minimal prompting?
- Does the provision of rich contextual metadata (e.g., specific classroom conventions, student developmental history) allow MLLMs to bridge the "tacit knowledge gap" identified in complex visual tasks?
- To what extent is the performance plateau on mathematical illustrations (k≈0.47) attributable to inherent task ambiguity versus the models' fundamental lack of visual-pedagogical integration?

## Limitations
- The vision-grading decomposition may conflate capabilities if vision and grading are cognitively intertwined
- Human descriptions for Task 2 could introduce interpretive bias if they implicitly encode pedagogical judgments
- The κ≈0.47 ceiling might reflect experimental design limitations rather than fundamental model limitations in pedagogical reasoning

## Confidence
- High Confidence: MLLM performance on routine arithmetic grading (κ=0.90) matching human levels
- Medium Confidence: Vision-grading decomposition framework validity and its implications for model capability assessment
- Medium Confidence: The κ≈0.47 ceiling for mathematical illustrations reflecting model limitations in tacit pedagogical knowledge

## Next Checks
1. **Vision-only stress test**: Provide models with ground-truth transcriptions (no images) to isolate pure assessment capability ceiling and determine if the κ≈0.47 plateau persists without visual interpretation requirements
2. **Prompt engineering sensitivity**: Test whether sophisticated pedagogical prompts can shift the illustration assessment ceiling above κ≈0.47, or if performance remains robust to prompt variation
3. **Human description bias analysis**: Conduct blind evaluation to assess whether human descriptions for Task 2 contain implicit pedagogical judgments that could explain the performance improvement, distinguishing visual assistance from assessment guidance