---
ver: rpa2
title: 'GMTRouter: Personalized LLM Router over Multi-turn User Interactions'
arxiv_id: '2511.08590'
source_url: https://arxiv.org/abs/2511.08590
tags:
- user
- arxiv
- data
- turn
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GMTRouter addresses the challenge of personalized LLM routing
  by modeling multi-turn user-LLM interactions as a heterogeneous graph with four
  node types: user, LLM, query, and response. It employs a tailored message-passing
  mechanism within a lightweight inductive graph learning framework to capture user
  preferences from few-shot data.'
---

# GMTRouter: Personalized LLM Router over Multi-turn User Interactions

## Quick Facts
- arXiv ID: 2511.08590
- Source URL: https://arxiv.org/abs/2511.08590
- Reference count: 39
- Primary result: Achieves 0.9%–21.6% higher accuracy and 0.006–0.309 higher AUC than strong baselines in personalized LLM routing

## Executive Summary
GMTRouter addresses personalized LLM routing by modeling multi-turn user interactions as a heterogeneous graph with typed nodes (user, LLM, query, response, turn). It employs a tailored message-passing mechanism within a lightweight inductive graph learning framework to capture user preferences from few-shot data. The approach demonstrates consistent superiority across four datasets, achieving 0.9%–21.6% higher accuracy and 0.006–0.309 higher AUC compared to strong baselines. Notably, it adapts effectively to new users and evolving preferences using minimal interaction data, without requiring extensive fine-tuning.

## Method Summary
GMTRouter encodes multi-turn user-LLM interactions as a heterogeneous graph where each interaction round is represented by a turn node connected to user, LLM, query, and response nodes. The framework uses a lightweight inductive graph learning approach where user preferences are aggregated through multi-hop message passing without learning fixed user embeddings. During training, the model samples k interaction records per user to construct visible subgraphs, then predicts held-out interactions. The prediction head employs cross-attention where the LLM embedding attends to the fused user-query context. This design enables effective cold-start performance while maintaining strong accuracy across diverse datasets.

## Key Results
- Achieves 0.9%–21.6% higher accuracy compared to strong baselines across four datasets
- Demonstrates 0.006–0.309 higher AUC-ROC for ranking quality
- Shows effective adaptation to new users with minimal interaction data (k=10 visible records)
- Maintains strong performance on both single-turn and multi-turn interaction scenarios

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Graph Preserves Relational Semantics
Encoding multi-turn user-LLM interactions as a heterogeneous graph with typed nodes preserves relational structure that sequential or tabular representations lose. Node types receive dedicated embeddings via PLM encoding, and edges encode who-said-what-to-whom-when. HGT's relation-aware attention allows different message-passing patterns per edge type, so "user→query" and "LLM→response" propagate differently. This structure maximally preserves the rich relational structure of interactions.

### Mechanism 2: Turn Nodes Enable Cross-Turn Preference Propagation
Virtual turn nodes aggregate local context per interaction round and propagate accumulated preferences across conversation turns. Each turn node connects to its user, LLM, query, and response nodes, with sequential edges between turn nodes enabling temporal message flow. User embedding starts at zero and accumulates preference signals via multi-hop aggregation, allowing the model to capture how preferences evolve within a session.

### Mechanism 3: Inductive Training Enables Cold-Start Generalization
Training the GNN to predict held-out interactions from sampled visible subgraphs teaches the model to extract preferences from few-shot data rather than memorizing static user profiles. During training, only the HGT model and prediction head parameters are updated, with user embeddings re-initializing to zero each epoch. This forces the model to learn aggregation dynamics generalizable across users, enabling effective cold-start performance.

## Foundational Learning

- **Heterogeneous Graph Neural Networks (HGT, Metapath-based GNNs)**
  - Why needed here: GMTRouter uses HGT as backbone to handle multiple node/edge types with relation-specific attention
  - Quick check: Can you explain why standard GCN would fail if user nodes and LLM nodes had identical message functions?

- **Inductive vs. Transductive Graph Learning**
  - Why needed here: The paper emphasizes inductive training for cold-start handling
  - Quick check: What would happen to cold-start performance if user embeddings were learned parameters updated during training?

- **Multi-Head Cross-Attention for Fusion**
  - Why needed here: The prediction head uses cross-attention where LLM embedding attends to fused user-query context
  - Quick check: Why might dot-product similarity underperform cross-attention for this specific fusion task?

## Architecture Onboarding

- **Component map**:
  Node Embedding Init: PLM (Contriever) encodes query/response/LLM text → h_q, h_r, h_m; users start at zero h_u
  → Preference Feature: Feedback → normalized rating → projected to h_p on response nodes
  → Graph Construction: 5 node types (user, LLM, query, response, turn); edges connect each turn to its entities; turn-to-turn edges capture sequence
  → GNN Encoder: 2–3 layer HGT with 4-head attention, LayerNorm, dropout 0.1
  → Prediction Head: Cross-attention (LLM attends to user+query) → MLP → scalar score s_{u,q,m}
  → Training Loop: Sample k records/user → build visible subgraph → aggregate → predict held-out → entropy loss → backprop through HGT+Head only

- **Critical path**:
  1. Correct heterogeneous edge construction (turn nodes must connect all entities per round, turn-to-turn edges must follow dialogue order)
  2. Inductive training discipline (no learned node embeddings; zero-init users each epoch)
  3. Prediction head fusion (cross-attention, not dot-product)

- **Design tradeoffs**:
  - k (visible data per user): Paper finds k=10 optimal; larger k shows diminishing returns. Higher k = more context but potential overfitting/instability
  - GNN depth: 2 layers for single-turn, 3 for multi-turn. Deeper = more aggregation but risk of oversmoothing
  - PLM choice: Contriever chosen for efficiency; larger PLMs may improve semantic capture but increase cost

- **Failure signatures**:
  - New user performance collapses → check if node embeddings are accidentally being learned/transductive
  - AUC high but accuracy low → prediction head may lack discrimination at top rank; check cross-attention implementation
  - Performance degrades with longer conversations → turn aggregation may be diluting signal; consider attention decay or truncation

- **First 3 experiments**:
  1. **Reproduce k-sweep**: Vary k ∈ {3, 5, 8, 10, 15, 20} on a validation split to confirm k=10 plateau on your target dataset
  2. **Cold-start sanity check**: Hold out 30% of users from training. Compare old-user vs. new-user accuracy. If gap > 5%, investigate whether any user-specific parameters are leaking
  3. **Ablate cross-attention**: Replace prediction head with dot-product (h_u + h_q) · h_m. Confirm performance drop similar to paper's Table 4 (MT-Bench drops from 0.784 to 0.730 accuracy)

## Open Questions the Paper Calls Out
The paper identifies "scalable LLM deployment" as a promising future direction enabled by this work, suggesting that the framework's efficiency and effectiveness could support production environments with millions of users.

## Limitations
- Limited evaluation on large-scale real-world interaction logs with millions of nodes
- Potential sensitivity to temporal preference drift when sampled "visible data" becomes outdated
- Unclear robustness to zero-shot evaluation with completely novel LLMs introduced after training

## Confidence
- **High confidence**: Heterogeneous graph representation preserving relational structure
- **Medium confidence**: Turn nodes for cross-turn preference propagation
- **Medium confidence**: Inductive training approach for cold-start generalization

## Next Checks
1. **Robustness to sparse histories**: Systematically evaluate GMTRouter's performance when new users have fewer than 10 interactions (k=3, 5, 8) to identify the minimum viable cold-start threshold
2. **Distribution shift sensitivity**: Test the model on a held-out subset of users whose preferences are systematically different from training users to assess generalization limits
3. **Query diversity impact**: Evaluate performance across different query types (technical, creative, conversational) to determine if the cross-attention fusion mechanism shows bias toward specific interaction patterns