---
ver: rpa2
title: LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction
arxiv_id: '2510.22829'
source_url: https://arxiv.org/abs/2510.22829
tags:
- memorability
- text
- video
- fusion
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses commercial memorability prediction in the
  MediaEval 2025 competition, proposing an LLM-based multimodal fusion system using
  Gemma-3 to integrate visual (ViT) and textual (E5) features. The model employs LoRA
  adaptation and uses LLM-generated rationales grounded in memorability aspects to
  guide fusion.
---

# LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction

## Quick Facts
- arXiv ID: 2510.22829
- Source URL: https://arxiv.org/abs/2510.22829
- Authors: Aleksandar Pramov
- Reference count: 12
- Key outcome: LLM-based fusion outperformed gradient-boosted trees on MediaEval 2025 commercial memorability prediction, achieving Spearman correlation scores of 0.122 (Brand Memorability) and 0.131 (Memorability Score) on test data.

## Executive Summary
This paper addresses commercial memorability prediction in the MediaEval 2025 competition by proposing an LLM-based multimodal fusion system using Gemma-3 to integrate visual (ViT) and textual (E5) features. The model employs LoRA adaptation and uses LLM-generated rationales grounded in memorability aspects to guide fusion. A heavily-tuned gradient boosted tree ensemble serves as baseline. The LLM-based system showed greater robustness and generalization, outperforming the baseline on the final test set with Spearman correlation scores of 0.122 for Brand Memorability and 0.131 for Memorability Score.

## Method Summary
The approach uses Gemma-3-4b-it as a multimodal fusion backbone with LoRA adaptation (rank=32, alpha=32, dropout=0.15) applied to attention and FFN layers. External features (ViT visual embeddings and E5-base-v2 text embeddings for subtitles, titles, descriptions) are projected into the LLM's embedding space via linear projectors. Mean pooling aggregates the fused sequence for prediction through an MLP head. The model is trained with a composite loss of MAE and correlation coefficient. Rationale-grounded prompts guide the fusion model, with rationales generated via few-shot prompting using expert-derived memorability aspects (brand integration, clarity, novelty).

## Key Results
- LLM-based fusion achieved Spearman correlation of 0.122 for Brand Memorability and 0.131 for Memorability Score on test set
- Outperformed HGBT baseline (0.089 SRCC for both targets) on final test data
- Rationale prompts improved Brand Memorability (0.122 vs 0.112) but hurt Memorability Score performance (0.018 vs 0.1165 CV)
- HGBT model showed overfitting: 0.24 CV SRCC collapsed to 0.089 on test data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based fusion provides better generalization on small, domain-specific datasets than gradient-boosted trees.
- Mechanism: Pre-trained LLMs encode generalizable world knowledge that stabilizes predictions when training data is scarce. The frozen or LoRA-adapted backbone resists overfitting by constraining weight updates to low-rank subspaces, while tree ensembles can memorize training idiosyncrasies.
- Core assumption: The pre-trained representations transfer meaningfully to the memorability prediction task despite domain shift to financial advertisements.
- Evidence anchors:
  - [abstract] "The LLM-based system showed greater robustness and generalization, outperforming the baseline on the final test set"
  - [section 4] "HGBT models...performance on the competition dataset collapsed, compared to the CV procedure...the model ultimately overfit to its specific characteristics"
  - [corpus] MindMem (arXiv:2502.18371) similarly uses LLMs for ad memorability, suggesting the pattern holds across implementations
- Break condition: If training data were orders of magnitude larger, the regularization advantage would shrink; tree ensembles might match or exceed LLM performance with sufficient samples.

### Mechanism 2
- Claim: Rationale-grounded prompts improve feature integration by providing explicit reasoning anchors for the fusion model.
- Mechanism: LLM-generated rationales—based on expert-derived memorability aspects (brand integration, clarity, novelty)—condition the model to attend to task-relevant feature interactions rather than spurious correlations.
- Core assumption: The few-shot generated rationales capture meaningful signal about memorability drivers, not just surface text patterns.
- Evidence anchors:
  - [abstract] "LLM-generated rationales grounded in expert-derived aspects of memorability, to guide the fusion model"
  - [section 3.2] "The aim of the prompt was to mimic an expert system to evaluate (qualitatively) the (brand) memorability...along key characteristics such as brand integration, clarity of brand messaging, semantic richness, novelty"
  - [corpus] Weak direct evidence; no corpus papers explicitly validate rationale-guided multimodal fusion
- Break condition: If rationales become generic or inconsistent across samples, they add noise rather than signal. The 0.018 test score collapse for rationales on Memorability Score (vs. 0.1165 CV) suggests this mechanism is target-dependent and potentially unstable.

### Mechanism 3
- Claim: Early fusion via projection layers allows pre-computed embeddings to be treated as pseudo-tokens within the LLM's embedding space.
- Mechanism: Linear projectors map external features (ViT visual, E5 text) into the LLM's hidden dimension, enabling cross-modal attention without modifying the backbone architecture. Mean or attention pooling then aggregates the fused sequence for prediction.
- Core assumption: Linear projection preserves sufficient cross-modal relational structure for the LLM to exploit.
- Evidence anchors:
  - [section 3.3] "external features...projected into the LLM's embedding space as via separate trainable linear projectors, with an early fusion step at the embedding level"
  - [section 3.3] "The best-performing setups consistently used mean pooling and included the ViT visual block and all three E5 text streams"
  - [corpus] Esteban et al. [3] is cited as the foundation for this approach; corpus lacks direct replications
- Break condition: If modalities require non-linear interaction modeling before fusion, simple linear projection would bottleneck performance.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables parameter-efficient fine-tuning of the Gemma backbone with limited training data (339 samples), reducing overfitting risk while allowing task adaptation.
  - Quick check question: Can you explain why LoRA's low-rank decomposition (A×B instead of full ΔW) provides implicit regularization?

- Concept: **Cross-modal projection in multimodal LLMs**
  - Why needed here: Understanding how external embeddings are mapped into the LLM's token space clarifies what information is preserved or lost during fusion.
  - Quick check question: What is the dimensional constraint that requires projection layers between ViT/E5 outputs and Gemma's embedding space?

- Concept: **Spearman correlation for ordinal prediction**
  - Why needed here: The competition evaluates using Spearman correlation, which measures rank ordering rather than absolute error—optimizing MAE may not maximize this metric.
  - Quick check question: Why might a model with lower RMSE still achieve lower Spearman correlation?

## Architecture Onboarding

- Component map: E5 embeddings + ViT visual embeddings + numerical metadata → Linear projectors → Gemma-3 backbone (with LoRA) → Mean pooling → MLP head → Memorability score
- Critical path:
  1. Pre-compute E5 embeddings for all text fields; ensure ViT embeddings are aligned
  2. Generate rationale prompts using few-shot prompting with fold-aware examples
  3. Project multimodal features → fuse with text tokens → pass through LoRA-adapted Gemma → pool → MLP prediction
  4. Train with early stopping on inner-fold validation splits
- Design tradeoffs:
  - **Mean vs. attention pooling**: Mean pooling performed best, suggesting uniform feature weighting is sufficient; attention may overfit on small data
  - **Rationale vs. summary prompts**: Target-dependent—rationales help Brand Memorability (0.122 vs 0.112), summaries help Memorability Score (0.131 vs 0.018)
  - **LoRA scope**: Applying to both attention and FFN layers improved stability but increases trainable parameters
- Failure signatures:
  - **CV-to-test collapse**: HGBT showed 0.24 CV → 0.089 test; indicates overfitting to development distribution
  - **Rationale instability**: Gemma Fusion with rationales on Memorability Score showed 0.1165 CV → 0.018 test; rationale quality may be inconsistent
  - **Channel dominance**: 23% of data from single channel (Goldman Sachs) required manual subgrouping to prevent leakage
- First 3 experiments:
  1. **Baseline sanity check**: Run HGBT with only numerical metadata (expected near-zero correlation) to confirm multimodal features are necessary
  2. **Ablation by modality**: Train Gemma Fusion with ViT-only vs. E5-only vs. combined to quantify each stream's contribution
  3. **Prompt strategy comparison**: Compare rationale vs. summary prompts on both targets using held-out fold to determine target-specific prompt selection before final submission

## Open Questions the Paper Calls Out

- **Open Question 1**: Would incorporating additional datasets such as Memento10k during training improve the stability and absolute performance of the LLM-based fusion model? [explicit] "Future work will involve the addition of additional datasets for training, e.g. memento10k which should improve the overall stability, and possibly performance of the model."
- **Open Question 2**: Would domain-specific pre-training or fine-tuning on financial text improve model performance on this finance-focused commercial memorability task? [explicit] "Throughout the whole analysis we did not leverage specific domain information—models which are fine-tuned on textual data from financial domain could be a better choice and have the potential to bring added value."
- **Open Question 3**: What causes the severe instability observed when rationale-based prompts are used for Memorability Score prediction (CV SRCC 0.1165 → Test SRCC 0.018)? [inferred] The paper documents this collapse but does not explain the mechanism, stating only that "the LLM-multimodal fusion approach...still indicates issues with the training stability."

## Limitations

- Rationale generation mechanism shows significant instability, with rationale-based approach achieving 0.1165 CV SRCC but collapsing to 0.018 on test for Memorability Score
- Composite loss function's weighting between MAE and correlation coefficient remains unspecified, potentially affecting optimization dynamics
- Significant CV-to-test performance gap observed in HGBT baseline (0.24 CV → 0.089 test), suggesting overfitting to development distribution

## Confidence

- **High confidence**: The core finding that LLM-based fusion outperforms gradient-boosted trees on this task, supported by test set results (0.122 vs 0.089 for Brand Memorability, 0.131 vs 0.089 for Memorability Score)
- **Medium confidence**: The mechanism by which rationale-grounded prompts improve feature integration, given the target-dependent performance and significant test-time collapse for Memorability Score
- **Low confidence**: The precise architectural details of the MLP head and composite loss function, which are critical for exact reproduction

## Next Checks

1. **Rationale quality audit**: Analyze a random sample of generated rationales for Memorability Score vs Brand Memorability to identify systematic differences in content quality or relevance that might explain the target-dependent performance collapse
2. **Cross-validation stability test**: Run 5-fold CV on the final test set (using a held-out fold from the original 339 samples) to quantify variance in the rationale-based approach and determine if the 0.018 test score represents an outlier
3. **Prompt strategy ablation**: Systematically compare title-only, title+rationale, and title+summary prompts across both targets using a single held-out validation fold to establish target-specific prompt selection rules before final model training