---
ver: rpa2
title: Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute
arxiv_id: '2509.21091'
source_url: https://arxiv.org/abs/2509.21091
tags:
- answer
- performance
- llms
- answers
- majority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of optimizing test-time compute\
  \ for large language models (LLMs) by studying best-of-N selection strategies, particularly\
  \ focusing on the asymptotic case of N\u2192 \u221E (best-of-\u221E). The authors\
  \ propose an adaptive sampling method based on Bayesian modeling and Bayes factor\
  \ thresholds to determine when to stop generating answers, thereby efficiently allocating\
  \ inference-time computation."
---

# Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute

## Quick Facts
- arXiv ID: 2509.21091
- Source URL: https://arxiv.org/abs/2509.21091
- Authors: Junpei Komiyama; Daisuke Oba; Masafumi Oyamada
- Reference count: 40
- One-line primary result: Adaptive sampling achieves same accuracy as fixed BoN with 2-5x fewer samples; optimally weighted LLM ensembles outperform individual models

## Executive Summary
This paper addresses test-time compute optimization for large language models through adaptive best-of-N selection strategies and ensemble methods. The authors propose a Bayesian adaptive sampling approach that uses Dirichlet process modeling and Bayes factor thresholds to determine when to stop generating answers, achieving significant efficiency gains over fixed sampling strategies. They extend this framework to weighted ensembles of multiple LLMs, formulating the optimal weighting as a mixed-integer linear program that scales efficiently to practical problem sizes.

## Method Summary
The method combines adaptive sampling with ensemble weighting optimization. For adaptive sampling, the system generates answers from LLMs and uses a Dirichlet process prior to model the answer distribution. After each generation, it calculates a Bayes factor to determine if the current most frequent answer has sufficient statistical support to stop sampling. For ensembles, the paper formulates weight optimization as a mixed-integer linear program that finds optimal weights for the asymptotic (N→∞) case, then applies a max-margin solution to ensure robustness for finite sample sizes.

## Key Results
- Adaptive sampling achieves 2-5x reduction in samples required compared to fixed BoN while maintaining the same accuracy
- Weighted LLM ensembles consistently outperform individual models, with the AIME2025 ensemble (GPT-OSS-20B + Phi-4-reasoning) achieving 93.3% best-of-∞ accuracy versus 90.0% and 73.0% for individual models
- The method demonstrates robust performance across 11 instruction-tuned LLMs and four reasoning benchmarks (AIME2024, AIME2025, GPQA-DIAMOND, MATH500)
- The authors release a large-scale generation dataset for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive sampling reduces generations needed by terminating once statistical confidence in the majority answer is established
- Mechanism: Uses Dirichlet Process prior and Bayes factor thresholds to measure evidence that current majority reflects true underlying distribution
- Core assumption: Clear majority class exists in LLM answer distribution and concentration parameter α correctly regularizes likelihood of new answers
- Evidence anchors: [abstract] proposes adaptive sampling with Bayes factor thresholds; [Section 2] details Bayes factor use; neighbor paper "Sampling-Efficient Test-Time Scaling" supports early stopping viability
- Break condition: Fails on flat distributions with no clear majority, causing Bayes factor to remain low until N_max reached

### Mechanism 2
- Claim: Optimizing weights for N→∞ limit enables efficient computation that improves finite-N performance
- Mechanism: Formulates weight optimization as mixed-integer linear program instead of complex non-concave maximization
- Core assumption: Limit behavior is valid proxy for large-but-finite N behavior and models have complementary error modes
- Evidence anchors: [Section 3.2] formulates optimal ensemble weighting as MILP; [Section 3.2] Lemma 4 provides MILP formulation; "RoBoN" validates multi-LLM routing premise
- Break condition: Weights may overfit if validation set is too small or unrepresentative

### Mechanism 3
- Claim: Max-margin solution provides robustness when transferring weights from infinite limit to finite sample sizes
- Mechanism: Selects most interior point of optimal weight region to maximize stability under finite sampling variance
- Core assumption: Robust weights in limit are also robust to finite sampling variance
- Evidence anchors: [Section 3.2] adopts max-margin solution noting finite-N performance variation; no direct corpus evidence for this specific refinement
- Break condition: No additional benefit if optimal solution space is single point or very narrow region

## Foundational Learning
- Concept: **Dirichlet Process (DP)**
  - Why needed here: Models unknown distribution of LLM answers without assuming fixed number of possible answers (nonparametric)
  - Quick check question: How does the concentration parameter α affect the prior probability of seeing a new, unobserved answer?
- Concept: **Bayes Factor**
  - Why needed here: Serves as stopping criterion quantifying evidence that empirical majority reflects true underlying majority
  - Quick check question: In Algorithm 1, does a high Bayes Factor indicate we should continue sampling or stop?
- Concept: **Mixed-Integer Linear Programming (MILP)**
  - Why needed here: Finds optimal ensemble weights handling discrete nature of "which answer wins" combined with continuous weight adjustments
  - Quick check question: Why is the objective function f(w) for ensemble non-concave, necessitating combinatorial approach like MILP?

## Architecture Onboarding
- Component map: Generator -> Posterior Updater -> Stopping Module -> Weight Optimizer (Offline)
- Critical path: Monte Carlo estimation of P(H₁|D⁽ⁿ⁾) inside stopping module, running iteratively to determine latency/throughput trade-off
- Design tradeoffs: Setting α (concentration parameter) vs. threshold B; lower B yields faster inference but higher risk of premature stopping on spurious majorities
- Failure signatures: Oscillation (answer distribution changes violating i.i.d. assumption); Time-out (MILP solver taking too long on large K)
- First 3 experiments:
  1. Reproduce Figure 4 (Left): Plot accuracy vs. average sample count for Adaptive vs. Fixed BoN on MATH500 to verify 2-5x efficiency claim
  2. Ablation on α: Run Algorithm 1 with varying α (0.1, 0.3, 1.0) to observe sensitivity in stopping criterion
  3. Ensemble Weight Transfer: Train weights on AIME2024, test on AIME2025, compare against "best single model" baseline to validate transfer learning claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does optimal ensemble weighting for best-of-∞ transfer to finite-N settings, particularly when max-margin solution differs from solutions optimal at bounded sample sizes?
- Basis in paper: [explicit] Section 3.2 notes "While any interior point on these position is optimal in best-of-∞, its finite-N performance can vary."
- Why unresolved: Paper optimizes for asymptotic regime without characterizing performance degradation or how weight selection should change for practical finite budgets
- What evidence would resolve it: Experiments comparing max-margin weights vs. weights specifically optimized for finite N across varying sample budgets

### Open Question 2
- Question: Can MILP-based ensemble weight optimization scale beyond K≈10 LLMs and N≈10³ problems without approximation?
- Basis in paper: [explicit] Paper states "open-source solvers scale smoothly to K≈10¹ LLMs and N≈10³ problems" but combinatorial nature creates computational barriers for larger model pools
- Why unresolved: Combinatorial formulation creates barriers for larger model pools or problem sets
- What evidence would resolve it: Benchmarking optimization time and solution quality as K and N grow, or proposing polynomial-time approximation schemes

### Open Question 3
- Question: How robust is adaptive stopping criterion to choice of concentration parameter α and Bayes factor threshold B?
- Basis in paper: [inferred] Paper sets α=0.3 without systematic ablation, and Theorem 1 requires B→∞ for theoretical guarantees but tests practical values empirically
- Why unresolved: Sensitivity of sample efficiency and accuracy to hyperparameters across different answer distribution characteristics is not characterized
- What evidence would resolve it: Ablation studies varying α and B across datasets with different answer space cardinalities and distribution shapes

### Open Question 4
- Question: Does 64.2% transfer success rate indicate fundamental limitations in cross-domain weight generalization, or can meta-learning approaches improve robustness?
- Basis in paper: [explicit] "Across 165 three-model combinations, the ensemble matched or exceeded strongest individual model in 106 cases (64.2%)."
- Why unresolved: Paper provides single transfer experiment without investigating why 35.8% failed or whether failure correlates with specific characteristics
- What evidence would resolve it: Analysis of failure modes and experiments with regularization or meta-learning techniques to improve cross-dataset generalization

## Limitations
- The adaptive sampling method may struggle on tasks with uniform answer distributions where no clear majority exists, potentially requiring near-maximum samples N_max
- MILP optimization performance depends on validation data being representative of test distribution, with potential degradation if validation set is small or biased
- The Bayes factor stopping criterion introduces hyperparameter sensitivity, with the paper not providing comprehensive guidance on optimal threshold selection across different problem types

## Confidence
- **Medium**: Efficiency gains rely on assumption of clear majority classes in LLM answer distributions, which may not hold for all tasks
- **Medium-High**: MILP formulation assumes representative validation data, with some robustness from max-margin solution but limited exploration of validation set sensitivity
- **Low-Medium**: Adaptive sampling's Bayes factor thresholds introduce hyperparameter sensitivity with limited characterization of optimal settings across different problem types

## Next Checks
1. **Distribution Sensitivity Analysis**: Systematically evaluate adaptive sampling across problems with varying answer distribution entropy (peaked to uniform), measuring premature stopping frequency on flatter distributions and quantifying efficiency-accuracy trade-offs

2. **Cross-Domain Weight Transfer**: Train ensemble weights on mathematical reasoning benchmarks and evaluate on structurally different tasks like code generation or commonsense reasoning to test generalizability beyond closely related AIME2024→AIME2025 transfer

3. **Real-Time Performance Validation**: Measure actual wall-clock time and token usage for complete adaptive sampling pipeline including answer parsing, Bayes factor computation, and MILP weight application, comparing against theoretical estimates to verify practical overheads don't erode claimed efficiency gains