---
ver: rpa2
title: Multi-task parallelism for robust pre-training of graph foundation models on
  multi-source, multi-fidelity atomistic modeling data
arxiv_id: '2506.21788'
source_url: https://arxiv.org/abs/2506.21788
tags:
- data
- datasets
- https
- parallelism
- atomistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of pre-training graph foundation
  models (GFMs) on multi-source, multi-fidelity atomistic data, which is critical
  for developing generalizable models across diverse chemical domains. The authors
  propose a multi-task parallelism method that distributes different output decoding
  heads of a multi-task learning (MTL) architecture across multiple GPUs, enabling
  concurrent forward and backward passes for each head.
---

# Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data

## Quick Facts
- arXiv ID: 2506.21788
- Source URL: https://arxiv.org/abs/2506.21788
- Authors: Massimiliano Lupo Pasini, Jong Youl Choi, Pei Zhang, Kshitij Mehta, Rylie Weaver, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, Prasanna Balaprakash
- Reference count: 34
- One-line primary result: Multi-task parallelism enables efficient pre-training of graph foundation models on 24M+ heterogeneous atomistic structures across 5 datasets, achieving 0.0007 MAE for energy and 0.0074 for forces while scaling to 1,920 GPUs.

## Executive Summary
This paper addresses the challenge of pre-training graph foundation models (GFMs) on multi-source, multi-fidelity atomistic data covering diverse chemical domains. The authors propose a multi-task parallelism method that distributes different output decoding heads of a multi-task learning (MTL) architecture across multiple GPUs, enabling concurrent forward and backward passes for each head. This approach, implemented in the HydraGNN framework, was tested on five datasets totaling over 24 million structures, including ANI1x, QM7-X, Transition1x, MPTrj, and Alexandria. The method demonstrated efficient scaling on the Perlmutter, Aurora, and Frontier supercomputers, achieving near-optimal performance with up to 1,920 GPUs. The MTL approach significantly improved model accuracy and transferability, with energy per atom and force prediction errors reduced to 0.0007 and 0.0074, respectively, outperforming single-dataset and baseline models.

## Method Summary
The method uses a two-level multi-task learning hierarchy with dataset-specific decoding heads distributed across GPUs using multi-task parallelism. The shared message passing neural network (MPNN) processes all atomistic structures regardless of source, then routes them to multiple decoding heads that predict data-specific outputs (energy per atom and forces). The approach uses a 4-layer Equivariant GNN (EGNN) with 866 hidden units as the shared encoder, and dataset-specific heads with 3 fully-connected layers (889 units each). Training employs AdamW optimizer (lr=0.001) with local batch size 128. The multi-task parallelism is combined with distributed data parallelism (DDP) via torch.DeviceMesh, organizing processes into sub-groups for efficient gradient synchronization. Data is managed using ADIOS library with DDStore for distributed in-memory caching.

## Key Results
- Multi-task parallelism enables training on 24M+ structures across 5 datasets with up to 1,920 GPUs
- MTL-All achieves MAE of 0.0007 (energy) and 0.0074 (forces) on ANI1x, outperforming single-dataset models
- Near-optimal strong scaling up to 320 GPUs on Frontier and Perlmutter for larger batch sizes
- MTL-base outperforms MTL-par at smaller scales on Perlmutter when model fits in GPU memory
- Higher performance variability observed on Aurora compared to Frontier and Perlmutter

## Why This Works (Mechanism)

### Mechanism 1: Multi-task Learning Stabilizes Multi-Fidelity Pre-training
- Claim: MTL with dataset-specific decoding heads improves both accuracy and transferability when pre-training on heterogeneous multi-source data.
- Mechanism: Shared message passing layers learn common features across all datasets while dataset-specific heads capture fidelity-specific patterns. The two-level hierarchy (first splitting by data source, then by prediction task: energy vs. forces) allows gradients from diverse sources to balance during optimization.
- Core assumption: Datasets share transferable atomistic representations despite differing in calculation methods (DFT settings, CCSD) and chemical composition.
- Evidence anchors:
  - [abstract] "shared message passing layers initially process input atomistic structures regardless of source, then route them to multiple decoding heads that predict data-specific outputs. This approach stabilizes pre-training and enhances a model's transferability."
  - [Section 5.1, Tables 1-2] GFM-MTL-All achieves MAE of 0.0007 (energy) and 0.0074 (forces) on ANI1x, substantially outperforming single-dataset models on cross-dataset transfer.
  - [corpus] Related work (Zhang et al., DPA-2) achieved similar MTL benefits on ~4M structures; this paper confirms the pattern holds at 24M structures.
- Break condition: If datasets share no common representation (e.g., fundamentally different physics), shared layers may learn conflicting features, causing optimization instability or negative transfer.

### Mechanism 2: Multi-Task Parallelism Reduces GPU Memory Pressure
- Claim: Distributing MTL decoding heads across GPUs enables training when total model parameters exceed single-GPU memory.
- Mechanism: Each process holds: (1) a local copy of shared MPNN layers, and (2) one MTL head. Forward passes execute concurrently without coordination. Backward passes update head parameters independently, then synchronize shared-layer gradients via collective averaging.
- Core assumption: The computational cost of gradient synchronization for shared layers is acceptable relative to the memory savings.
- Evidence anchors:
  - [Section 4.3] "Without parallelism, memory per GPU scales as Ps + (Ph × Nh); with it, memory per GPU is reduced to (Ps + Ph)."
  - [Section 4.3] The paper identifies three regimes: Ps >> Nh×Ph (pipeline/tensor parallelism preferred), Ps << Nh×Ph (multi-task parallelism optimal), Ps ∼ Nh×Ph (hybrid).
  - [corpus] No direct corpus comparison for this specific parallelism strategy; most distributed GNN frameworks (DistDGL, AliGraph) focus on large single graphs, not multi-head MTL.
- Break condition: If Ps dominates (very deep/wide shared encoder), multi-task parallelism provides minimal benefit; tensor/pipeline parallelism becomes necessary.

### Mechanism 3: 2D Parallelization Enables Efficient Scaling
- Claim: Combining multi-task parallelism with DDP achieves near-optimal scaling on heterogeneous supercomputing architectures.
- Mechanism: Processes are organized into sub-groups via `torch.DeviceMesh`. Global DDP synchronizes shared-layer gradients across all processes. Local DDP within each sub-group synchronizes head-specific gradients. This replaces large global messages with smaller local communications.
- Core assumption: The dataset-to-head mapping is fixed and roughly balanced; load imbalance across heads would reduce efficiency.
- Evidence anchors:
  - [Section 5.2, Figure 4] Near-optimal strong scaling up to 320 GPUs on Frontier and Perlmutter for larger batch sizes. MTL-par outperforms MTL-base at scale.
  - [Section 5.2] "We observe platform-specific scaling behavior, e.g., MTL-base shows lower runtime as compared with MTL-par on Perlmutter, indicating optimal performance for Perlmutter when a model fits completely in the GPU memory."
  - [corpus] DistMLIP (arxiv 2506.02023) addresses distributed inference for MLIPs but focuses on inference, not training parallelism; weak corpus overlap on training-specific strategies.
- Break condition: If datasets have vastly different sample counts, sub-groups processing smaller datasets will idle; dynamic load balancing would be required.

## Foundational Learning

- **Message Passing Neural Networks (MPNNs)**
  - Why needed here: The shared encoder uses 4-layer Equivariant GNN (EGNN) with message passing; understanding how node/edge features propagate is essential for debugging convergence issues.
  - Quick check question: Can you explain how equivariant layers enforce rotation consistency in force predictions?

- **Multi-Task Learning (Hard Parameter Sharing)**
  - Why needed here: The two-level MTL architecture assumes shared representations transfer; knowing when this fails (negative transfer) helps diagnose dataset incompatibilities.
  - Quick check question: What would happen if two datasets required contradictory features from the shared layers?

- **Distributed Data Parallelism (DDP)**
  - Why needed here: The 2D parallelization extends DDP; you must understand gradient synchronization and collective operations to debug scaling bottlenecks.
  - Quick check question: Why does DDP require identical model replicas across processes, and how does multi-task parallelism modify this constraint?

## Architecture Onboarding

- **Component map:**
  - Input Pipeline: ADIOS files → DDStore (distributed in-memory cache) → per-process batch loading
  - Shared Encoder: 4-layer EGNN (866 hidden units) processes all atomistic graphs
  - MTL Heads: 5 dataset branches × 2 prediction heads (energy, forces); each head = 3 fully-connected layers (889 units)
  - Parallelism Layer: `torch.DeviceMesh` organizes processes into global group (shared layers) + 5 sub-groups (per-dataset heads)

- **Critical path:**
  1. Load data from DDStore (avoid filesystem after initial epoch)
  2. Forward pass through shared MPNN (all processes)
  3. Route to dataset-specific head based on sample source
  4. Concurrent head forward/backward passes
  5. Synchronize shared-layer gradients via `all_reduce`
  6. Optimizer step (AdamW, lr=0.001)

- **Design tradeoffs:**
  - More datasets → more heads → higher memory pressure → benefit from multi-task parallelism increases
  - Larger batch sizes improve scaling but increase memory; multi-task parallelism enables larger batches by reducing per-GPU memory
  - Perlmutter (A100) shows better MTL-base performance when model fits in memory; Frontier/Aurora benefit more from MTL-par

- **Failure signatures:**
  - **OOM on single GPU**: Model exceeds memory; switch from MTL-base to MTL-par
  - **Poor cross-dataset transfer**: Single-task models outperform MTL on specific datasets; check for negative transfer, consider dataset alignment (energy per atom normalization)
  - **Scaling plateau**: Communication overhead dominates; verify sub-group balance, check if local batch size is too small

- **First 3 experiments:**
  1. **Baseline single-dataset training**: Train HydraGNN on each dataset individually (ANI1x, QM7-X, MPTrj, Alexandria, Transition1x) to establish in-distribution accuracy benchmarks.
  2. **MTL-base vs. MTL-par memory profile**: Measure peak GPU memory for both configurations on 2, 4, 8 datasets to confirm the Ps + Ph scaling formula and identify the crossover point where MTL-par becomes necessary.
  3. **Strong scaling test on 320 GPUs**: Replicate the Figure 4 strong scaling experiment with batch sizes 256-512 to verify near-optimal scaling; if runtime doesn't decrease proportionally, profile communication overhead in gradient synchronization.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How will multi-task parallelism scale when expanding from 24 million to 359 million atomistic structures covering all natural elements?
  - Basis in paper: [explicit] The conclusion states: "Future work will be dedicated to expanding the set of datasets reaching up to 359 million atomistic structures that cover all the natural elements of the periodic table."
  - Why unresolved: Current work tested only 24M structures with 5 datasets; 15x larger scale with full periodic table coverage introduces unknown memory, communication, and convergence challenges.
  - What evidence would resolve it: Successful training runs with scaling efficiency metrics on the expanded 359M structure dataset.

- **Open Question 2**
  - Question: How effectively does the pre-trained GFM transfer to downstream tasks across diverse chemical domains?
  - Basis in paper: [explicit] Future work will "illustrate the efficacy of our pre-trained on a broad class of downstream tasks."
  - Why unresolved: The paper evaluates only in-distribution and cross-dataset transfer accuracy during pre-training, not fine-tuning performance on actual downstream applications.
  - What evidence would resolve it: Benchmarks showing fine-tuning performance on specific downstream tasks (e.g., property prediction, molecular dynamics) compared to single-task baselines.

- **Open Question 3**
  - Question: What causes the higher performance variability observed on Aurora compared to Frontier and Perlmutter?
  - Basis in paper: [explicit] Section 5.2 notes: "In general, we observe higher variability in performance on Aurora as compared with the other two systems."
  - Why unresolved: The phenomenon is reported but not analyzed; could stem from hardware architecture, software stack, or communication patterns.
  - What evidence would resolve it: Profiling data identifying bottlenecks (communication latency, memory bandwidth, kernel performance) on Aurora versus other systems.

- **Open Question 4**
  - Question: At what threshold of MTL heads does multi-task parallelism become preferable to MTL-base across different hardware architectures?
  - Basis in paper: [inferred] Section 5.2 shows MTL-base outperforms MTL-par at smaller scales on Perlmutter, while MTL-par scales better at larger scales; the crossover point is architecture-dependent.
  - Why unresolved: The paper provides empirical comparisons but no analytical model for predicting optimal parallelization strategy.
  - What evidence would resolve it: Systematic experiments varying MTL head count with consistent profiling to identify decision boundaries per architecture.

## Limitations

- Energy normalization across datasets is referenced but implementation details are not fully specified, which could affect cross-dataset transfer performance.
- Training duration and early stopping criteria are not precisely defined, making it difficult to benchmark training efficiency.
- The two-level MTL hierarchy is demonstrated effective but alternative hierarchical structures are not explored.

## Confidence

- **High Confidence**: Multi-task parallelism effectively reduces GPU memory pressure and enables scaling to 1,920 GPUs. The memory scaling formula (Ps + Ph) and strong scaling results are well-supported.
- **Medium Confidence**: MTL improves both accuracy and transferability across heterogeneous datasets. While results show significant improvements, the exact conditions for avoiding negative transfer are not fully characterized.
- **Medium Confidence**: The two-level MTL hierarchy (dataset split + task split) is optimal. While the paper demonstrates effectiveness, alternative hierarchical structures are not explored.

## Next Checks

1. **Dataset compatibility test**: Train single-dataset models on each of the five datasets, then evaluate cross-dataset transfer to quantify negative transfer risks and validate the necessity of the two-level MTL hierarchy.
2. **Memory scaling verification**: Measure GPU memory usage for MTL-base vs. MTL-par configurations across 2-5 datasets to confirm the Ps + (Ph × Nh) vs. (Ps + Ph) scaling relationship.
3. **Communication overhead profiling**: Instrument the 2D parallelization implementation to measure collective communication costs (all_reduce, all_gather) during gradient synchronization at different GPU counts to identify scaling bottlenecks.