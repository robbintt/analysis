---
ver: rpa2
title: 'Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large
  Language Models via Watermarking'
arxiv_id: '2510.02962'
source_url: https://arxiv.org/abs/2510.02962
tags:
- dataset
- arxiv
- detection
- datasets
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose TRACE, a practical framework for fully black-box detection
  of copyrighted dataset usage in LLM fine-tuning. TRACE rewrites datasets with distortion-free
  watermarks guided by a private key, ensuring both text quality and downstream utility.
---

# Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking

## Quick Facts
- **arXiv ID:** 2510.02962
- **Source URL:** https://arxiv.org/abs/2510.02962
- **Reference count:** 20
- **Primary result:** Practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning via distortion-free watermarking and entropy-gated detection

## Executive Summary
TRACE provides the first practical framework for black-box detection of copyrighted dataset usage in LLM fine-tuning. It uses distortion-free watermarking to embed key-specific statistical patterns into datasets without compromising text quality or downstream utility. The framework exploits the "radioactivity" effect where fine-tuning on watermarked data causes models to internalize these patterns, making them detectable in model outputs. Through an entropy-gated detection procedure that focuses on high-uncertainty tokens, TRACE achieves statistically significant detections (p<0.05) across diverse datasets and model families, with extremely strong evidence (p-values as low as 1e-129). The method supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora.

## Method Summary
TRACE embeds watermarks into datasets using SynthID-Text's tournament sampling approach with a private key, creating distortion-free statistical patterns. When models are fine-tuned on these watermarked datasets, they learn key-specific token selection preferences ("radioactivity"). Detection works by querying the suspect model, computing token entropy using an auxiliary model, selecting high-entropy tokens (typically top 40-70%), and calculating watermark scores using the private key. A one-sided Z-test determines if scores significantly exceed the null hypothesis (0.5), with p<0.05 indicating copyright violation. The method achieves detection without requiring access to model internals, logits, or training details.

## Key Results
- Achieves p<0.05 detection across diverse datasets (GSM8k, Med, Alpaca, Dolly, MMLU, ARC-C) and model families (Llama, Phi, Qwen)
- Entropy gating boosts detection strength by orders of magnitude (e.g., -log₁₀(p) from ~26 to ~268 on Med)
- Multi-dataset attribution shows diagonal p-values of 3.2e-129 and 5.8e-45 vs off-diagonal near 1.0
- Remains effective after continued pretraining (p shifts from 2.2e-94 to 2.6e-36)
- Maintains text quality (P-SP 0.85-0.91) and downstream utility (accuracy loss <1%)

## Why This Works (Mechanism)

### Mechanism 1: Distortion-free watermark embedding via tournament sampling
Watermark embedding uses a private key k combined with context window to generate pseudorandom seeds that drive d independent binary watermark functions (g-values) for each token. Tokens win a d-round tournament based on aggregated g-values, creating key-specific selection patterns without distorting the base distribution. The watermarking process maintains distributional neutrality—averaging over random keys should recover the original next-token distribution.

### Mechanism 2: Radioactivity effect from fine-tuning
Fine-tuning on watermarked data induces measurable radioactivity in model outputs—models update more at uncertain positions where multiple continuations are plausible. When watermarking nudges sampling at these positions, the model internalizes key-aligned preferences. These preferences manifest in the model's own generations, creating detectable statistical traces specific to the owner's key.

### Mechanism 3: Entropy-gated detection amplification
The detector computes token-level entropy using an auxiliary model, ranks all generated tokens by entropy, and applies a hard gate to keep only the top q% highest-entropy tokens. Watermark signals concentrate at uncertain positions because key-guided nudges have more impact when the base distribution is dispersed rather than sharp. Including many highly confident tokens mostly adds noise, so focusing on the most uncertain tokens amplifies the signal-to-noise ratio.

## Foundational Learning

- **Concept: LLM watermarking via sampling perturbation**
  - Why needed: TRACE builds on SynthID-Text, which embeds watermarks through tournament sampling rather than post-hoc text modification
  - Quick check: Given a vocabulary and a context window, how would you generate a pseudorandom seed that determines which tokens are "favored" in a tournament?

- **Concept: Membership inference attacks (MIAs) vs dataset-level inference**
  - Why needed: The paper positions TRACE against existing MIAs that require logit access; understanding why sample-level MIAs fail at scale clarifies the need for dataset-level approaches
  - Quick check: Why does aggregating p-values or loss scores across samples not reliably detect whether a dataset was used for training?

- **Concept: Statistical hypothesis testing with p-values**
  - Why needed: TRACE formalizes detection as rejecting H₀ (model not fine-tuned on D) using a standardized Z-statistic
  - Quick check: If the null hypothesis is that watermark scores have expected value 0.5, how would you construct a one-sided test for elevated scores?

## Architecture Onboarding

- **Component map:** Dataset owner -> SynthID-Text watermarking -> watermarked dataset D′ -> Suspect model fine-tuning -> Output queries -> Auxiliary model entropy computation -> Entropy gate -> Watermark score calculation -> Z-statistic -> p-value

- **Critical path:**
  1. Dataset owner generates D′ = W(D, k) and releases publicly
  2. Suspect model fine-tunes on D′ (unknown to owner)
  3. Owner queries suspect model with prompts from D
  4. Collect ~100k tokens of output
  5. Compute entropy for each token position
  6. Select top 40k high-entropy tokens
  7. Compute watermark scores using k
  8. Run one-sided hypothesis test

- **Design tradeoffs:**
  - Detection power vs. token budget: More tokens → stronger evidence, but higher query cost
  - Watermarked proportion vs. utility: Paper shows 50% watermarked samples sufficient; 100% maximizes detection but increases rewriting cost
  - Entropy gate threshold vs. signal concentration: Stricter gates (lower q%) increase concentration but reduce scored tokens
  - Auxiliary model choice: Mismatch between auxiliary and suspect model may misestimate entropy

- **Failure signatures:**
  - Short, deterministic outputs provide few high-entropy positions
  - Continued pretraining on large non-watermarked corpora attenuates but doesn't eliminate signal
  - False positives under H₀: all 12 model-dataset pairs show p > 0.05

- **First 3 experiments:**
  1. Reproduction on single dataset: Take Med or GSM8k, generate watermarked version with Llama-3.1-8B, fine-tune Llama-3B with LoRA, run detection with 40k/100k token budget—verify p < 0.05
  2. Entropy gate ablation: Compare detection with q=70% vs. q=100% (no gating) across 4 datasets—quantify -log₁₀(p) improvement
  3. False positive calibration: Test detection on models not fine-tuned on watermarked data—confirm all p > 0.05 across 3 models × 4 datasets

## Open Questions the Paper Calls Out
- To what extent does the choice of the auxiliary model for entropy calculation impact detection power when it significantly mismatches the suspect model?
- How effective is TRACE in scenarios where the output generation is constrained to be highly deterministic or extremely short?
- Is the watermark signal robust if the adversary applies paraphrasing or data augmentation to the watermarked dataset prior to fine-tuning?

## Limitations
- Implementation dependency on external SynthID-Text library with unspecified details
- Reliance on auxiliary model for entropy estimation that may mismatch suspect model
- Reduced effectiveness on tasks with short or highly deterministic outputs
- No evaluation against adversarial defenses like dataset paraphrasing or augmentation

## Confidence
**High Confidence:**
- Watermarking preserves text quality (P-SP > 0.85, comparable perplexity)
- Detection achieves p < 0.05 across diverse datasets and models
- False positive rate remains low (all 12 baseline comparisons show p > 0.05)
- Multi-dataset attribution works (diagonal p-values orders of magnitude smaller than off-diagonal)

**Medium Confidence:**
- Entropy gating provides substantial detection boost
- Continued pretraining attenuates but doesn't eliminate signal
- 50% watermarked proportion is sufficient for detection

**Low Confidence:**
- Exact implementation details of SynthID-Text tournament sampling
- Specific auxiliary model choice for entropy estimation
- Detection performance on non-QA tasks or highly deterministic outputs

## Next Checks
1. **Implementation Verification:** Reproduce detection on GSM8k using Llama-3.1-8B watermarking model and Llama-3B target. Verify p < 0.05 with 40k tokens and confirm text quality metrics (P-SP > 0.85, perplexity within 10% of original).

2. **Entropy Gate Ablation Study:** Run detection with q = 70%, 50%, 30%, and 100% (no gating) on 3 different datasets. Quantify -log₁₀(p) improvement and verify gating provides at least 10x improvement in detection strength.

3. **False Positive Calibration:** Test detection on 4 models not fine-tuned on watermarked data (e.g., original Llama-3B, Phi-3-mini, Qwen2.5-7B) using 4 different keys. Confirm all p-values exceed 0.05 and that no false positives occur in baseline comparisons.