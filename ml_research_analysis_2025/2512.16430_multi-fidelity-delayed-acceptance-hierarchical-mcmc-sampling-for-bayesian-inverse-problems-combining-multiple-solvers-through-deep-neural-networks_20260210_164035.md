---
ver: rpa2
title: 'Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian
  inverse problems combining multiple solvers through deep neural networks'
arxiv_id: '2512.16430'
source_url: https://arxiv.org/abs/2512.16430
tags:
- high-fidelity
- multi-fidelity
- mfda
- low-fidelity
- solvers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Multi-Fidelity Delayed Acceptance (MFDA),
  a novel hierarchical MCMC framework that accelerates Bayesian inverse problems by
  integrating neural network-based multi-fidelity fusion into the Multi-Level Delayed
  Acceptance (MLDA) framework. The approach uses neural networks to combine predictions
  from solvers of varying fidelity, with high-fidelity evaluations confined to an
  offline training phase.
---

# Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks

## Quick Facts
- **arXiv ID:** 2512.16430
- **Source URL:** https://arxiv.org/abs/2512.16430
- **Reference count:** 40
- **Primary result:** Multi-Fidelity Delayed Acceptance (MFDA) accelerates Bayesian inverse problems by integrating neural network-based multi-fidelity fusion into MLDA, achieving comparable posterior accuracy while reducing sampling time by up to 75%.

## Executive Summary
This work introduces Multi-Fidelity Delayed Acceptance (MFDA), a novel hierarchical MCMC framework that accelerates Bayesian inverse problems by integrating neural network-based multi-fidelity fusion into the Multi-Level Delayed Acceptance (MLDA) framework. The approach uses neural networks to combine predictions from solvers of varying fidelity, with high-fidelity evaluations confined to an offline training phase. During online sampling, likelihood evaluations rely solely on coarse solvers and trained networks, avoiding additional expensive high-fidelity computations. The MFDA framework is demonstrated on two benchmark problems: steady groundwater flow and unsteady reaction-diffusion systems. Results show that MFDA achieves posterior reconstruction accuracy comparable to standard MH and MLDA, while substantially improving sampling efficiency—reducing computation time per effective sample by up to 75% in groundwater flow and by a factor of four in the reaction-diffusion case.

## Method Summary
MFDA extends MLDA by replacing the coarse solvers with a trained multi-fidelity neural network that takes the parameter and all coarse solver outputs as inputs, learning to predict the high-fidelity solution. The framework separates computation into offline and online phases: in the offline phase, a dataset of high-fidelity and coarse solver solutions is generated and used to train neural networks for each fidelity level; in the online phase, an MCMC sampler uses only the coarse solvers and trained networks to evaluate likelihoods, with high-fidelity evaluations confined to the offline training. The acceptance probability at the finest level uses the neural network surrogate rather than the true high-fidelity likelihood, enabling efficient sampling while maintaining accuracy.

## Key Results
- MFDA achieves posterior reconstruction accuracy comparable to standard MH and MLDA methods
- Reduces computation time per effective sample by up to 75% in groundwater flow problems
- Improves sampling efficiency by a factor of four in reaction-diffusion systems
- Enables longer sub-chain lengths and better mixing in MCMC chains
- Multi-fidelity NNs outperform corresponding low-fidelity solvers at all levels, with RMSE nearly one order of magnitude lower

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Filtering via Delayed Acceptance
If low-fidelity models can reliably identify low-probability regions, the sampler can reject poor candidates early without evaluating expensive high-fidelity models. The algorithm employs a hierarchy of levels (L). A candidate parameter θ' must pass an acceptance test at a coarse level (Level 1) using a cheap likelihood π̃^(1) before being proposed to the next level. If rejected early, the expensive solvers at finer levels are never invoked. The core assumption is that posterior approximations at coarse levels must be sufficiently consistent with the fine-level posterior; otherwise, candidates accepted at coarse levels will be rejected at fine levels, destroying efficiency. Frequent rejections at the finest level (after acceptance at coarse levels) indicates the coarse models are biased relative to the target.

### Mechanism 2: Neural Fusion for Correction of Coarse Physics
If low-fidelity solvers contain useful but imperfect physics, neural networks can learn a corrective mapping to high-fidelity outputs, improving the accuracy of the coarse likelihood. Instead of using a single coarse solver output, the framework trains a Multi-Fidelity Neural Network (f_MF) that takes the parameter θ and the outputs of all available coarse solvers (f_LF^(1), ..., f_LF^(l)) as inputs. The network learns to fuse these imperfect signals into a high-accuracy approximation of f_HF, effectively reducing the bias that would normally break the MLDA chain. The core assumption is that there exists a learnable nonlinear relationship between the set of coarse solutions and the high-fidelity solution that persists across the parameter space. The NN fails to generalize outside the training distribution, causing the "corrected" likelihood to diverge from the true posterior geometry.

### Mechanism 3: Offline/Online Compute Decoupling
By restricting high-fidelity evaluations to an offline training phase, the online sampling phase scales only with the cost of coarse solvers and NN inference. The framework separates the workflow: first (Offline), generate a dataset of HF/LF pairs and train the NNs; second (Online), run the MCMC. Crucially, the acceptance probability at the finest level in MFDA uses the NN surrogate π̃_MF^(L) rather than the true HF likelihood π. This amortizes the massive cost of HF PDE solves over the entire sampling campaign. The core assumption is that the surrogate error is sufficiently low such that the resulting approximate posterior is statistically acceptable for the user's inference goals. The method targets "comparable accuracy" rather than exactness.

## Foundational Learning

- **Concept: Markov Chain Monte Carlo (MCMC) & Metropolis-Hastings**
  - **Why needed here:** This is the engine being accelerated. You must understand the "proposal -> acceptance/rejection" loop to grasp why early rejection (Mechanism 1) saves time.
  - **Quick check question:** Can you explain why a low acceptance rate in standard MH makes inference expensive?

- **Concept: Multi-Fidelity Modeling**
  - **Why needed here:** The method relies on the correlation between cheap (coarse) and expensive (fine) models.
  - **Quick check question:** If a coarse model has zero correlation with the high-fidelity model, would multi-fidelity acceleration work?

- **Concept: Bayesian Inverse Problems**
  - **Why needed here:** The goal is not just to solve a PDE, but to quantify uncertainty in parameters θ given noisy data y_obs.
  - **Quick check question:** Why do we sample from the posterior distribution rather than just finding a single "best fit" parameter?

## Architecture Onboarding

- **Component map:** Offline Trainer -> Online Sampler -> Likelihood Evaluator
- **Critical path:** The accuracy of the highest-level surrogate (f_MF^(L)) determines the validity of the final posterior. If this component is biased, the entire inference is biased.
- **Design tradeoffs:**
  - **Offline Cost vs. Online Speed:** Higher N_train improves NN accuracy (better mixing) but increases upfront cost.
  - **Surrogate Accuracy vs. True Posterior:** MFDA samples the surrogate posterior at the finest level, not the exact HF posterior. You trade exactness for speed (though results show they are "comparable").
- **Failure signatures:**
  - **Stalling:** Chain accepts at Level 1 but rejects constantly at Level L → NN training data was insufficient or biased.
  - **Mode Collapse:** NN ignores LF inputs and learns a direct θ → f_HF map (overfitting) → Poor generalization.
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run standard MH (very short chain) and MFDA on a trivial parameter set. Verify MFDA returns a mean close to the MH mean.
  2. **Ablation on Fidelity:** Run MFDA using only the finest LF solver as input to the NN vs. using the full hierarchy. Test if the "fusion" mechanism (Mechanism 2) actually improves RMSE.
  3. **Scaling Test:** Fix the number of MCMC steps. Measure wall-clock time for MLDA (online HF) vs. MFDA (offline HF). Verify the "break-even" point where the offline cost of MFDA pays off.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive online training strategies be integrated into the MFDA framework to update neural network surrogates during sampling without introducing significant bias?
- **Basis in paper:** The conclusion states: "Future research directions include the development of adaptive online training strategies to update neural network surrogates during sampling, potentially reducing offline training costs."
- **Why unresolved:** The current MFDA implementation relies on a fixed offline training phase. While the authors note that online learning could reduce offline costs, they highlight that it remains a "non-trivial challenge" that may introduce bias and requires significant methodological adjustments.
- **What evidence would resolve it:** A modified MFDA algorithm demonstrating that online weight updates maintain the statistical consistency of the chain (e.g., convergence to the true high-fidelity posterior) while reducing the upfront data generation cost.

### Open Question 2
- **Question:** How does MFDA perform when low-fidelity models differ in physics or dimensionality rather than just numerical resolution?
- **Basis in paper:** The conclusion suggests: "...investigating MFDA to settings where low-fidelity models differ in physics or dimensionality, rather than solely in numerical resolution, could further highlight its applicability."
- **Why unresolved:** The numerical experiments (groundwater flow and reaction-diffusion) primarily used low-fidelity models derived via mesh coarsening of the same governing equations. The authors have not yet validated the method on heterogeneous model hierarchies (e.g., combining 1D models with 3D models or simplified physics with full physics).
- **What evidence would resolve it:** Successful application of MFDA to a benchmark problem where coarse models employ different physical approximations (e.g., RANS vs. DNS in fluid dynamics) with comparable efficiency gains.

### Open Question 3
- **Question:** What are the theoretical bounds on the bias introduced by approximating the high-fidelity posterior with the multi-fidelity surrogate posterior?
- **Basis in paper:** Section 2.5 notes that the chain satisfies detailed balance with respect to the surrogate likelihood π̃_MF^(L), "rather than with the exact high-fidelity posterior." The authors currently rely on the expectation that the NN is "sufficiently accurate" but provide no theoretical guarantee regarding the divergence between the sampled distribution and the true target.
- **Why unresolved:** The paper demonstrates empirical accuracy (comparable posterior means) but lacks a theoretical framework to quantify the error induced by the neural network approximation a priori.
- **What evidence would resolve it:** Derivation of theoretical error bounds relating the approximation error of the neural network to the total variation distance between the MFDA stationary distribution and the true high-fidelity posterior.

## Limitations
- The method trades exactness for speed—users must accept a surrogate posterior rather than the true HF posterior
- Three critical assumptions (coarse solver structure, NN generalization, surrogate accuracy) are validated empirically but not theoretically guaranteed
- Scalability claim to "large-scale PDE-constrained inverse problems" is asserted but not demonstrated beyond two benchmark cases

## Confidence
- **High Confidence:** Computational speed gains (time-per-effective-sample reduction) are directly measurable from the reported experiments and supported by the decoupled offline/online workflow.
- **Medium Confidence:** Posterior accuracy claims ("comparable to MH/MLDA") rely on RMSE comparisons that are problem-specific. The groundwater RMSE improvement (75% reduction in cost) is more convincing than the reaction-diffusion case, where the gain is a factor of four but the problem is smaller.
- **Low Confidence:** The scalability claim to "large-scale PDE-constrained inverse problems" is asserted but not demonstrated beyond the two benchmark cases. No ablation studies on NN architecture or training data size are provided to quantify robustness.

## Next Checks
1. **Convergence Stability Test:** Run MFDA chains with increasing sub-chain lengths. Plot ESS vs. wall-clock time to verify the claimed efficiency gains persist as chains grow longer.
2. **Generalization Stress Test:** Hold out a region of the parameter space from NN training. Verify MFDA posterior samples in this region remain accurate and do not exhibit mode collapse.
3. **Solver Hierarchy Ablation:** Run MFDA with only the coarsest solver vs. the full hierarchy as NN inputs. Quantify the contribution of the multi-fidelity fusion mechanism to final accuracy and efficiency.