---
ver: rpa2
title: Improving Generative Ad Text on Facebook using Reinforcement Learning
arxiv_id: '2507.21983'
source_url: https://arxiv.org/abs/2507.21983
tags:
- text
- vertical
- advertiser
- data
- advertisers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates the first deployment of reinforcement learning
  for generative advertising on Facebook. The authors introduced reinforcement learning
  with performance feedback (RLPF), a method that trains a large language model (LLM)
  using historical ad click-through rate (CTR) data as a reward signal.
---

# Improving Generative Ad Text on Facebook using Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.21983
- Source URL: https://arxiv.org/abs/2507.21983
- Reference count: 40
- Primary result: First RL deployment for generative advertising on Facebook, achieving 6.7% CTR improvement

## Executive Summary
This paper introduces Reinforcement Learning with Performance Feedback (RLPF), the first deployment of reinforcement learning for generative advertising on Facebook. The method trains a large language model using historical ad click-through rates as a reward signal, creating "AdLlama" which was integrated into Meta's Text Generation feature. In a large-scale 10-week A/B test with 34,849 advertisers and 640,000 ad variations, AdLlama improved advertiser-level CTR by 6.7% (p=0.0296) compared to a supervised imitation model. The approach demonstrates that RLPF is an effective, generalizable method for metric-driven post-training of LLMs in real-world advertising settings.

## Method Summary
RLPF constructs a reward model from historical "multitext" ad data where only body text varies while other ad components remain constant. Higher-CTR text is labeled as preferred and used to train a Bradley-Terry pairwise preference model. The Llama 2 Chat (7B) model is then fine-tuned using Proximal Policy Optimization (PPO) with reward model scores, KL divergence penalties to prevent drift, and length penalties to discourage verbosity. The resulting AdLlama model generates ad text variations that are selected by advertisers through a human-in-the-loop process, with the system optimizing for downstream CTR performance.

## Key Results
- 6.7% advertiser-level CTR improvement (p=0.0296) in large-scale A/B test
- 18.5% increase in ad variations created, indicating higher advertiser engagement
- Reward model achieves ~57% out-of-sample pairwise accuracy on 7M preference pairs
- AdLlama maintains text quality while optimizing for CTR performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Performance feedback (CTR) provides a scalable, objective reward signal for aligning LLMs to business outcomes.
- **Mechanism:** Historical "multitext" ad data—where only body text varies while image, targeting, and other factors stay constant—enables pairwise preference construction. Higher-CTR text is labeled "preferred," training a reward model to score text quality.
- **Core assumption:** CTR differences between text variations can be attributed to the text itself when other ad components are held constant.
- **Evidence anchors:** [abstract] "RLPF... uses historical click-through rates (CTR) as a reward signal"; [section 3.1] "From the multitext data, we are able to construct preference pairs, where the higher CTR text is marked as 'more preferred'"; [corpus] Limited direct corroboration; corpus papers focus on auto-bidding rather than text generation reward modeling
- **Break condition:** If CTR is heavily confounded by non-text factors (e.g., user targeting shifts, image quality) not captured in the data, reward model scores will be noisy or misleading.

### Mechanism 2
- **Claim:** Proximal Policy Optimization (PPO) with KL and length penalties fine-tunes the LLM toward high-reward text without catastrophic forgetting or excessive verbosity.
- **Mechanism:** PPO optimizes an objective combining reward model score, KL divergence penalty (anchoring to a reference policy), and length penalty (discouraging overlong outputs). This pushes the LLM distribution toward higher-CTR text while maintaining coherence.
- **Core assumption:** The reward model generalizes beyond training data and reward hacking (overoptimization) can be mitigated via early stopping and evaluation monitoring.
- **Evidence anchors:** [section 3.1] PPO formulation explicitly includes KL penalty and length penalty terms; [section A.2] "PPO can reliably increase the RM score, but subjective text quality started to decrease after a certain number of training steps... likely due to overoptimization"; [corpus] Adjacent work on generative auto-bidding uses similar RL frameworks but does not validate RLPF specifically
- **Break condition:** If KL penalty is too weak, the model may drift into incoherent or hallucinatory outputs; if too strong, it may not move meaningfully toward higher reward.

### Mechanism 3
- **Claim:** Improved text quality increases advertiser adoption (more variations created), which amplifies performance gains through human-AI collaboration.
- **Mechanism:** Advertisers are more likely to select and deploy AI-generated variations when the suggestions are perceived as high-quality. The human-in-the-loop selection process gates which text reaches users, so better suggestions → more adoption → more opportunity for CTR gains.
- **Core assumption:** Advertiser selection behavior correlates with perceived quality and is not purely driven by other factors (e.g., UI prominence, habit).
- **Evidence anchors:** [abstract] "advertisers using AdLlama created 18.5% more ad variations, indicating higher engagement with the tool"; [section 4.2] Table 2 shows treatment increases variant count by 3.1 (p<0.01) without changing total ad count; [corpus] No direct corpus evidence; related work does not address human-in-the-loop selection dynamics
- **Break condition:** If advertisers select variations for reasons unrelated to quality (e.g., novelty bias), the causal link from quality → adoption → performance breaks.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF) and Bradley-Terry Preference Models**
  - **Why needed here:** RLPF extends RLHF by replacing human preference labels with aggregate CTR signals. Understanding RLHF (reward modeling, PPO fine-tuning) is prerequisite to grasping RLPF.
  - **Quick check question:** Can you explain why Bradley-Terry modeling is used for pairwise preference training, and why PPO uses a KL penalty?

- **Concept: Click-Through Rate (CTR) and Advertising Metrics**
  - **Why needed here:** CTR is the reward signal; understanding its definition, noise characteristics, and role as a proxy for conversion is essential.
  - **Quick check question:** Why might CTR be preferred over conversion rate for reward modeling in this setting?

- **Concept: A/B Testing and Randomized Controlled Trials (RCTs)**
  - **Why needed here:** The 6.7% improvement claim rests on a properly designed RCT. Understanding randomization, covariate balance, and regression specifications is needed to interpret results.
  - **Quick check question:** What are the key threats to internal validity in an advertiser-level A/B test like this?

## Architecture Onboarding

- **Component map:** Multitext historical data pipeline -> Reward model training -> PPO fine-tuning loop -> Checkpoint selection -> Serving layer -> Experimentation layer
- **Critical path:** Ensure multitext data quality (CTR attribution to text only) -> Train reward model with adequate pairwise accuracy -> Run PPO with early stopping based on evaluation RM + human checks -> Deploy to A/B test with proper covariate logging -> Analyze via regression controlling for pre-experiment CTR and other confounders
- **Design tradeoffs:**
  - **Reward model complexity vs. interpretability:** Pairwise Bradley-Terry chosen over pointwise CTR regression for better ranking ability
  - **KL penalty strength:** Too low → reward hacking; too high → insufficient alignment gain
  - **Offline vs. online RL:** Current approach is offline; online RL could adapt to trends but adds complexity and risk
- **Failure signatures:**
  - Reward model overfitting to training data, poor out-of-sample accuracy
  - PPO overoptimization producing repetitive or irrelevant text
  - A/B test imbalance on key covariates (pre-experiment CTR, advertiser vertical)
  - Advertiser selection not reflecting quality (e.g., selecting all suggestions)
- **First 3 experiments:**
  1. **Reward model ablation:** Compare pairwise vs. pointwise reward training on held-out CTR ranking accuracy
  2. **PPO hyperparameter sweep:** Vary KL penalty and length penalty; measure reward gain vs. text quality degradation
  3. **A/B test sanity check:** Verify covariate balance and run model-free CTR comparison (global CTR with Delta method CI) alongside regression

## Open Questions the Paper Calls Out

- **Question:** Can incorporating real-time interaction via online reinforcement learning improve the model's ability to adapt to new trends compared to the current offline approach?
  - **Basis in paper:** [explicit] The authors state the current model is equivalent to a single round of offline RL and suggest incorporating performance outcomes in an iterative process to align more closely with online RL.
  - **Why unresolved:** The current methodology relies on static historical data, limiting the system's ability to explore new ad text formats or adapt to shifting trends dynamically.
  - **What evidence would resolve it:** A comparative study measuring the performance durability and trend adaptation of an online RL agent versus the current offline RLPF model over an extended period.

- **Question:** How can multi-objective optimization be implemented to balance high click-through rates with constraints like creativity, tone adherence, and platform-level ad diversity?
  - **Basis in paper:** [explicit] The paper notes that the model focuses primarily on performance, but a trade-off may exist between performance and creativity, tone adherence, and inventory diversity.
  - **Why unresolved:** Optimizing solely for CTR may lead to homogenized ad inventory or text that ignores specific advertiser instructions regarding tone or style.
  - **What evidence would resolve it:** An experiment utilizing a multi-objective reward function that evaluates models on both CTR and separate metrics for semantic diversity and instruction following.

- **Question:** Does weighting the reward signal by the likelihood of advertiser selection improve the practical utility of the generated text?
  - **Basis in paper:** [inferred] The authors note the model currently does not account for the human component where advertisers must explicitly select a variation before it is delivered to users.
  - **Why unresolved:** A disconnect may exist between text that achieves a high predicted CTR and text that advertisers trust or prefer enough to publish.
  - **What evidence would resolve it:** An A/B test comparing the current model against a variant where the reward model is trained to predict advertiser selection likelihood in addition to performance.

## Limitations

- The 6.7% CTR improvement is based on a single large-scale A/B test without external validation or replication across different advertiser segments or markets.
- The reward model's 57% out-of-sample pairwise accuracy indicates substantial noise in the reward signal, which may propagate through PPO training.
- No explicit analysis of potential bias amplification (e.g., toward certain advertiser verticals, text styles, or demographic targeting) is provided.
- The human-in-the-loop selection mechanism's impact is measured only through increased variation creation, not through quality assessments of selected vs. rejected variations.

## Confidence

- **High confidence:** The technical feasibility of RLPF as a post-training method (reward modeling + PPO framework works as described)
- **Medium confidence:** The 6.7% CTR improvement claim (supported by A/B test but single experiment, no replication)
- **Low confidence:** Claims about human-AI collaboration amplifying performance (measured only through adoption metrics, not quality or performance impact)

## Next Checks

1. **Replication across segments:** Run the same A/B test methodology on different advertiser verticals (e.g., e-commerce vs. local businesses) and geographic regions to assess generalizability of the 6.7% improvement.
2. **Reward model noise analysis:** Conduct ablation studies varying reward model training data size/quality to quantify the relationship between reward model accuracy and PPO-generated text performance.
3. **Selection quality analysis:** Compare CTR distributions of AI-generated variations that were selected vs. rejected by advertisers to determine if human selection is indeed quality-driven rather than random or novelty-based.