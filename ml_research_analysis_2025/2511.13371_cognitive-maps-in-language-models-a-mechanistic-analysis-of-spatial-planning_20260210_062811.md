---
ver: rpa2
title: 'Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning'
arxiv_id: '2511.13371'
source_url: https://arxiv.org/abs/2511.13371
tags:
- spatial
- layer
- training
- foraging
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models solve spatial\
  \ navigation tasks by comparing models trained on exploratory random walks (Foraging\
  \ Model) versus goal-directed shortest-path finding (SP Models). Using behavioral,\
  \ representational, and mechanistic analyses on 4\xD74 grid environments, the authors\
  \ uncover two fundamentally different algorithms."
---

# Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning

## Quick Facts
- arXiv ID: 2511.13371
- Source URL: https://arxiv.org/abs/2511.13371
- Reference count: 22
- Primary result: Exploratory training produces generalizable cognitive maps; goal-directed training yields brittle, path-dependent algorithms

## Executive Summary
This study investigates how transformer models solve spatial navigation tasks by comparing models trained on exploratory random walks (Foraging Model) versus goal-directed shortest-path finding (SP Models). Using behavioral, representational, and mechanistic analyses on 4×4 grid environments, the authors uncover two fundamentally different algorithms. The Foraging Model develops a robust, map-like spatial representation that consolidates into a self-sufficient coordinate system by layer 8, enabling strong generalization to larger grids and complex tasks. In contrast, SP models learn brittle, path-dependent algorithms relying continuously on explicit directional inputs, with limited generalization despite superior in-distribution performance.

## Method Summary
The authors train four GPT-2 small (124M parameter) transformer models on different objectives: foraging (next-token prediction on random walks), shortest path (goal-directed navigation), and hybrid fine-tuned variants. All models operate on a 4×4 grid with node-position and direction tokens as input. Analysis combines behavioral metrics (accuracy, reverse bias), representational probes (linear regression to decode coordinates, PCA), and mechanistic interventions (direction-token ablation, causal analysis of attention patterns). Models are evaluated on in-distribution tasks (4×4 loops, shortest paths) and out-of-distribution generalization (5×5 grids, Hamiltonian cycles).

## Key Results
- Foraging Model achieves 98.3% accuracy on 5×5 grids and perfect Hamiltonian cycles through self-sufficient cognitive map representation
- Phase transition at Layer 8 where Foraging Model eliminates reliance on directional history tokens
- SP models show gradual, continuous dependence on directional inputs across all layers with poor generalization (3.6% on 5×5 grids)
- Hybrid fine-tuning improves representations but preserves path-dependent algorithm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer 1 attention implements a "reverse last move" heuristic for short-context navigation.
- Mechanism: Attention heads attend to the penultimate node when processing a direction token, directly computing valid backtracking moves without needing global spatial understanding.
- Core assumption: This circuit serves as both a standalone heuristic and foundational input for deeper computation.
- Evidence anchors:
  - [abstract] "switches from local heuristics to global reasoning"
  - [section 3.3] "Layer 1, several attention heads consistently attend to the penultimate node when processing a direction token, directly implementing a 'reverse last move' operation"
  - [corpus] Weak direct support; "From reactive to cognitive" discusses reactive-to-cognitive transitions but not this specific circuit
- Break condition: If attention patterns don't show penultimate-node preference, or if ablating Layer 1 attention doesn't impair short-loop performance, the mechanism is absent.

### Mechanism 2
- Claim: By Layer 8, the Foraging Model consolidates movement history into a self-sufficient Cartesian coordinate representation.
- Mechanism: Sequential direction tokens are progressively integrated into hidden states such that node positions become linearly decodable (R² ≈ 0.93) with orthogonal x/y basis vectors (cosine similarity ≈ -0.0415), eliminating dependency on explicit directional history.
- Core assumption: The coordinate system is allocentric (grid-relative), not egocentric (sequence-relative).
- Evidence anchors:
  - [abstract] "sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network"
  - [section 3.3] "performance is near zero when ablating early layers but jumps to 100% when ablating at Layer 8 or later"
  - [corpus] "Cognitive maps are generative programs" provides theoretical framing but no direct evidence for this architecture
- Break condition: If direction-token ablation at Layer 8+ causes accuracy drop, or if linear probes fail to decode coordinates, the cognitive map has not formed.

### Mechanism 3
- Claim: Goal-directed training produces path-dependent algorithms that never abstract beyond directional tokens.
- Mechanism: SP models compute position via continuous path integration, showing gradual (not phase-transition) recovery from ablation, indicating they never consolidate spatial information into an independent representation.
- Core assumption: Training objective structure (not architecture) determines whether world models emerge.
- Evidence anchors:
  - [abstract] "goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers"
  - [section 3.3] "SP models... show a gradual recovery in performance with no phase transition... accuracy remains continuously dependent on explicit directional tokens throughout all 12 layers"
  - [corpus] No direct corpus support for this specific finding
- Break condition: If SP models show sharp phase transitions or self-sufficient representations at any layer, the path-dependent characterization fails.

## Foundational Learning

- Concept: Phase transitions in neural representations
  - Why needed here: The paper's central claim depends on distinguishing sharp transitions (cognitive map emergence) from gradual recovery (path-dependent computation).
  - Quick check question: Can you explain why a sharp accuracy jump at a specific layer suggests information consolidation, while gradual recovery suggests continuous dependence?

- Concept: Allocentric vs. egocentric spatial encoding
  - Why needed here: The cognitive map claim requires understanding whether representations are environment-fixed (allocentric) or observer-relative (egocentric).
  - Quick check question: If you rotate a grid 90°, would the model's internal coordinates rotate correspondingly? How would you test this?

- Concept: Linear probing and R² interpretation
  - Why needed here: Claims about coordinate system emergence rest on linear decodability from hidden states.
  - Quick check question: What does R² = 0.93 tell you about the relationship between hidden states and coordinates? What could the missing 7% represent?

## Architecture Onboarding

- Component map: Layer 1 Attention -> Layers 2-7 Integration -> Layer 8+ Coordinate Consolidation -> Layers 11-12 Functional Clustering

- Critical path:
  1. Input tokens (node + direction sequence) → Layer 1 attention extracts directional update
  2. Layers 2-7 transform sequential information toward coordinate representation
  3. Layer 8 consolidates into allocentric coordinates (probe R² plateaus here)
  4. Later layers specialize for action affordances

- Design tradeoffs:
  - Foraging training yields generalizable maps but requires next-token prediction on random walks (no explicit goal signal)
  - Goal-directed training yields strong in-distribution performance but brittle, path-dependent algorithms
  - Hybrid fine-tuning (SP-RW) improves representation but retains the same path-dependent mechanism

- Failure signatures:
  - High reverse bias at all context lengths → cognitive map not formed
  - Gradual (not sharp) ablation recovery → path-dependent algorithm present
  - Horizontal mirroring in PCA → model exploiting training distribution symmetry rather than learning true spatial structure
  - R² plateau before Layer 8 → delayed or absent coordinate consolidation

- First 3 experiments:
  1. Replicate direction-token ablation at each layer on Foraging Model with 4-hop loops; verify sharp transition between Layers 7-8.
  2. Train linear probes to decode (x,y) coordinates from hidden states at each layer; confirm R² ≈ 0.93 plateau at Layer 8.
  3. Run ablation on SP models to verify gradual recovery pattern; compare ablation curves between SP-H and SP-RW to confirm shared path-dependent mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the emergence of cognitive maps and the observed layer 8 phase transition scale to larger transformer architectures and more complex, continuous environments?
- Basis in paper: [explicit] The authors explicitly state in Section 4.3 that the use of a "simplified 4×4 grid and the GPT-2 small architecture raises important questions of scalability to more complex environments and larger models."
- Why unresolved: The study is restricted to a small model (124M parameters) and a discrete, constrained grid world. It is unknown if the specific mechanistic stages (local heuristics to global reasoning) are artifacts of this scale or fundamental properties of transformers.
- What evidence would resolve it: Replicating the training paradigms on larger models (e.g., Llama-7B) and continuous 3D environments to observe if the distinct phase transition and coordinate consolidation still occur at similar or different layers.

### Open Question 2
- Question: What specific computational circuits transform local directional updates in early layers into the allocentric coordinate system observed in the later layers?
- Basis in paper: [explicit] Section 4.3 notes that "the exact circuit and transformation of spatial information across intermediate layers remains largely a black box," despite identifying the input (Layer 1) and output (Layer 8) representations.
- Why unresolved: The analysis identifies the beginning (Layer 1 update) and the end state (Layer 8 coordinate system) but lacks a mechanistic account of the integration process occurring in Layers 2–7.
- What evidence would resolve it: Applying automated circuit discovery techniques (as suggested by the authors) or targeted activation patching across the intermediate layers to map the trajectory from egocentric direction tokens to allocentric coordinates.

### Open Question 3
- Question: What is the mechanistic basis for the failure of goal-directed (Shortest Path) models to generalize, given their "path-dependent" algorithm?
- Basis in paper: [explicit] The authors state in Section 4.3 that the "brittleness of the SP models [is] behaviourally observed but not mechanistically explained" and that future work should "prioritise a comparative mechanistic analysis."
- Why unresolved: While the paper identifies that SP models rely on continuous path integration, it does not isolate the specific circuit failures or internal constraints that prevent these models from switching to the map-based strategy used by the Foraging model.
- What evidence would resolve it: A comparative causal analysis (e.g., ablation studies) on the SP models to determine if they lack the specific attention heads required for spatial abstraction or if existing heads are functionally locked to the training distribution.

## Limitations
- Small-scale model (124M parameters) and simplified 4×4 grid limit generalizability to complex environments
- Intermediate layers (2-7) remain a mechanistic black box despite identifying input/output representations
- Claims about cognitive map emergence rely on ablation and probing rather than direct observation of internal spatial reasoning

## Confidence
- Foraging model develops allocentric cognitive maps: Medium
- Phase transition at Layer 8 indicates map consolidation: Medium
- SP models learn path-dependent algorithms: Medium
- Training objective determines world model emergence: Low-Medium

## Next Checks
1. **Direct causal intervention**: Use feature attribution methods to identify which neurons or attention patterns in Layers 2-7 are critical for coordinate consolidation. Test whether manipulating these features disrupts the phase transition or R² decoding performance.

2. **Generalization beyond grid symmetries**: Evaluate models on asymmetric grid layouts (rectangular, irregular shapes) to determine whether the Foraging model's performance relies on exploiting training distribution symmetries versus genuine spatial understanding.

3. **Coordinate system verification**: Implement a coordinate rotation test where the model must navigate rotated grids. If the Foraging model truly uses allocentric coordinates, its performance should remain constant under rotation, while path-dependent SP models should fail due to their reliance on absolute directional history.