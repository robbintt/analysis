---
ver: rpa2
title: 'OptiSQL: Executable SQL Generation from Optical Tokens'
arxiv_id: '2601.13695'
source_url: https://arxiv.org/abs/2601.13695
tags:
- table
- tokens
- optical
- visual
- optisql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OptiSQL is a vision-driven framework that generates executable
  SQL directly from table images using compact optical tokens. It employs a frozen
  OCR-oriented visual encoder to compress table structure and content into a small
  set of optical tokens, which are combined with natural language questions and processed
  by a fine-tuned autoregressive decoder.
---

# OptiSQL: Executable SQL Generation from Optical Tokens

## Quick Facts
- arXiv ID: 2601.13695
- Source URL: https://arxiv.org/abs/2601.13695
- Reference count: 21
- OptiSQL achieves 66% execution accuracy on visualized Spider 2.0-Snow using only 256 optical tokens per query, representing 92.7% reduction in table token count compared to text-based baselines.

## Executive Summary
OptiSQL is a vision-driven framework that generates executable SQL directly from table images using compact optical tokens. It employs a frozen OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens, which are combined with natural language questions and processed by a fine-tuned autoregressive decoder. On a visualized version of Spider 2.0-Snow, OptiSQL achieves 66% execution accuracy using only 256 optical tokens per query, representing a 92.7% reduction in table token count compared to text-based baselines while maintaining strong executable performance. The approach demonstrates that compact optical tokens preserve sufficient structural information for semantic parsing under strict efficiency constraints, offering a practical alternative to text-centric table representations when structured schemas are unavailable.

## Method Summary
OptiSQL uses a frozen DeepSeek-OCR visual encoder to compress table images into 256 optical tokens that preserve row-column alignment, cell grouping, and header-content associations. These optical tokens are concatenated with tokenized natural language questions and fed to a fine-tuned autoregressive decoder to generate executable SQL. The framework employs two training strategies: FROZENENC (freeze encoder, train decoder only) for robustness, and FULLFT (train both) for potential accuracy gains. The model is evaluated on visualized Spider 2.0-Snow with execution accuracy as the primary metric, demonstrating that optical tokens can replace textual table schemas while maintaining strong performance under strict efficiency constraints.

## Key Results
- Achieves 66% execution accuracy on visualized Spider 2.0-Snow using only 256 optical tokens per query
- Reduces table token count by 92.7% compared to text-based baselines
- Ablation tests show NOIMAGE drops EXAcc from 66% to 15%, confirming visual grounding is essential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCR-oriented visual encoders can compress table images into compact optical tokens that retain sufficient structural and semantic information for SQL generation under strict token budgets.
- Mechanism: A pretrained OCR-oriented encoder maps 2D table layouts to a 1D sequence of optical tokens (V = E(I) = v₁, v₂, ..., vₙ), encoding row-column alignment, cell grouping, and header-content associations without explicit text reconstruction. The paper hypothesizes these tokens serve as a "latent interface" that implicitly represents table content.
- Core assumption: The pretrained encoder's optical tokenization preserves task-relevant structural regularities (header positions, column boundaries) that semantic parsing requires, even under ~93% compression.
- Evidence anchors:
  - [abstract] "compress table structure and content into a small set of optical tokens... 92.7% reduction in table token count"
  - [section 4.1] "The encoder compresses the two-dimensional table layout into a one-dimensional token sequence while preserving regularities such as row-column alignment, cell grouping information, and header-content associations."
  - [corpus] Weak direct corpus support; related work focuses on text-to-SQL schema linking and multi-table reasoning, not optical token compression.
- Break condition: Performance should degrade sharply if optical tokens lose structural information. Table 2 confirms: removing optical tokens drops EXAcc from 66% → 15% (-83.3%).

### Mechanism 2
- Claim: Freezing the visual encoder while fine-tuning only the decoder isolates representation sufficiency and avoids overfitting to surface visual patterns.
- Mechanism: The FROZENENC setting prevents task-specific encoder adaptation, forcing the decoder to learn mappings from fixed optical representations to SQL. This creates a controlled study of whether compact optical tokens are inherently sufficient.
- Core assumption: Pretrained optical tokenization already captures the structural information needed; adaptation primarily improves visual style robustness rather than core representational capacity.
- Evidence anchors:
  - [abstract] "fine-tunes a pretrained autoregressive decoder for SQL generation while freezing the encoder to isolate representation sufficiency"
  - [section 6.1] "While FULLFT slightly improves clean accuracy, it exhibits larger robustness drops, indicating overfitting to surface rendering patterns"
  - [corpus] No corpus papers specifically examine frozen-encoder training for vision-to-SQL tasks.
- Break condition: If encoder adaptation were necessary for representation, FULLFT should substantially outperform FROZENENC. Results show FROZENENC (66% EXAcc) is competitive with FULLFT (68%) with better robustness.

### Mechanism 3
- Claim: Optical tokens provide implicit visual grounding that prevents SQL generation from collapsing to language-only priors or template-based generation.
- Mechanism: The unified input X = [V; Q] concatenates optical tokens with question tokens, requiring the decoder to condition SQL generation on visual table information rather than statistical language patterns alone.
- Core assumption: The model cannot recover executable SQL from question priors alone when optical tokens are necessary conditioning signals.
- Evidence anchors:
  - [section 4.2] "No textual schema annotations, column names, or table contents are provided. All table-related information must be inferred from the optical tokens."
  - [section 6.2/Table 2] WrongTable permutation drops EXAcc to 6%, confirming the model uses table-specific visual content, not generic priors.
  - [corpus] Related table QA papers (e.g., Chain-of-Query, CORE-T) assume textual table access; none test visual grounding via image removal.
- Break condition: If optical tokens were unnecessary, NOIMAGE would maintain reasonable performance. The observed 15% vs 66% confirms visual grounding is essential.

## Foundational Learning

- Concept: **Autoregressive decoder training with cross-entropy loss**
  - Why needed here: The SQL decoder predicts token sequences conditioned on optical+question inputs; understanding teacher forcing and next-token prediction is prerequisite for implementing fine-tuning.
  - Quick check question: Can you explain why the decoder requires separate fine-tuning when the encoder is frozen, versus end-to-end training?

- Concept: **OCR-oriented visual encoding vs. general vision-language models**
  - Why needed here: OptiSQL uses a specialized OCR encoder (DeepSeek-OCR style) rather than general VLMs like Pix2Struct; the difference affects token density and structural preservation.
  - Quick check question: What structural information might an OCR-oriented encoder capture that a general document VLM might lose during compression?

- Concept: **Execution-based evaluation vs. string matching for SQL**
  - Why needed here: The paper uses EXAcc (execution accuracy) as the primary metric because semantically equivalent SQL can differ in surface form; canonicalization (EX-Can) is secondary.
  - Quick check question: Why might a generated SQL query be semantically correct but fail exact string match against the gold query?

## Architecture Onboarding

- Component map:
Table Image (I) → [Frozen OCR Visual Encoder E] → Optical Tokens V (256)
                                                            ↓
Natural Language Question (q) → [Tokenizer] → Question Tokens Q
                                                            ↓
                    [Concatenation] → X = [V; Q] → [Fine-tuned Decoder D] → SQL Output

- Critical path: The optical token budget (n=256) is the key efficiency bottleneck. Section 6.3/Figure 5 shows EXAcc increases monotonically with token budget with diminishing returns beyond ~256 tokens.

- Design tradeoffs:
  - **FROZENENC vs. FULLFT**: FROZENENC provides better robustness (smaller degradation under StyleShift/HeaderMask) at slight accuracy cost; FULLFT overfits to training rendering styles.
  - **Token budget vs. latency**: Figure 5 shows ~linear latency growth with token count; 256 is the practical sweet spot.
  - Assumption: The paper evaluates single-table queries only; multi-table joins are noted as future work.

- Failure signatures:
  - **Schema linking errors**: Wrong column names, missing table aliases cause ~50% of failures (Section A.5).
  - **Visual grounding collapse**: If EXAcc doesn't degrade sharply on NOIMAGE/WRONGTABLE tests, the model has learned language-only priors—a critical diagnostic failure.
  - **Timeout failures**: Missing WHERE predicates or Cartesian products trigger 2-second timeouts.

- First 3 experiments:
  1. **Token budget sweep**: Replicate Figure 5 by testing EXAcc at n ∈ {64, 100, 256, 400} optical tokens on a held-out validation split to verify the 256-token sweet spot for your specific data distribution.
  2. **Visual grounding diagnostic**: Implement NOIMAGE (replace optical tokens with null token) and WRONGTABLE (permute images across batch) tests; confirm >60% EXAcc drop to validate the model isn't relying on question priors.
  3. **FROZENENC vs. FULLFT ablation**: Train both variants with identical hyperparameters (Section A.6), then evaluate on StyleShift and HeaderMask perturbations; verify FROZENENC shows smaller robustness degradation despite slightly lower clean accuracy.

## Open Questions the Paper Calls Out

- **Can optical tokens effectively represent inter-table relationships and support complex join queries across multiple table images?**
  - Basis: [explicit] "Extending the approach to multi-table queries will require modeling inter-table relations and joins, which we leave to future work."
  - Why unresolved: The current evaluation focuses exclusively on single-table queries in visualized Spider 2.0-Snow; optical tokens were designed and tested for individual table compression, not relational reasoning across tables.
  - What evidence would resolve it: Experiments on multi-table benchmarks demonstrating execution accuracy when multiple table images must be jointly encoded.

- **Do compact optical tokens transfer effectively to other structured generation tasks beyond SQL, such as formula synthesis or code generation?**
  - Basis: [explicit] "We focus exclusively on executable SQL generation; the applicability of compact optical tokens to other structured generation tasks remains open."
  - Why unresolved: The decoder was fine-tuned specifically for SQL syntax and semantics; whether optical tokens preserve sufficient information for other output formalisms is untested.
  - What evidence would resolve it: Evaluations on table-to-formula, table-to-code, or structured reasoning tasks showing comparable compression-accuracy trade-offs.

- **Can encoder fine-tuning strategies be developed that improve accuracy without sacrificing robustness to visual perturbations?**
  - Basis: [explicit] "While fine-tuning the encoder can improve accuracy, it may reduce robustness, suggesting a trade-off that warrants investigation."
  - Why unresolved: FROZENENC maintains robustness but caps accuracy; FULLFT achieves slightly higher clean accuracy but exhibits larger drops under StyleShift and HeaderMask.
  - What evidence would resolve it: Novel training schemes (e.g., adversarial robustness regularization, staged unfreezing) that close the robustness gap while retaining FULLFT accuracy gains.

## Limitations

- **Single-table focus**: The 66% execution accuracy applies only to single-table queries; multi-table joins and inter-table relationships remain untested future work.
- **Clean dataset bias**: Evaluation uses synthetically rendered table images with controlled styling, not real-world document degradation like handwriting or scanning artifacts.
- **Architectural dependencies**: The framework relies on pretrained DeepSeek-OCR encoder and unspecified decoder architecture, creating reproduction challenges.

## Confidence

- **High Confidence**: The core claim that optical tokens can replace textual table schemas while maintaining executable SQL generation is strongly supported by ablation tests (NOIMAGE/WRONGTABLE dropping EXAcc from 66% → 15%/6%). The 92.7% token reduction is directly measured.
- **Medium Confidence**: The FROZENENC vs FULLFT robustness tradeoff is supported by StyleShift/HeaderMask results, but the generalizability depends on the specific pretrained encoder used. Different OCR models might yield different robustness patterns.
- **Low Confidence**: The claim about multi-table queries being future work is entirely untested; the current 66% EXAcc figure applies only to single-table scenarios.

## Next Checks

1. **Visual Grounding Validation**: Implement NOIMAGE (replace optical tokens with null token) and WRONGTABLE (permute images across batch) tests on your reproduced model. Confirm EXAcc drops by >60% to validate the model isn't relying on question priors. This diagnostic is critical for verifying the visual grounding mechanism.

2. **Encoder Adaptation Study**: Train both FROZENENC and FULLFT variants with identical hyperparameters, then evaluate on StyleShift and HeaderMask perturbations. Verify FROZENENC shows smaller robustness degradation despite slightly lower clean accuracy. This confirms the overfitting mechanism described in Section 6.1.

3. **Token Budget Sweet Spot**: Replicate Figure 5 by testing EXAcc at n ∈ {64, 100, 256, 400} optical tokens on a held-out validation split. Confirm the 256-token sweet spot holds for your specific data distribution and implementation details.