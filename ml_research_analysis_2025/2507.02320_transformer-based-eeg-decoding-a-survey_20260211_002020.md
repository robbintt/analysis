---
ver: rpa2
title: 'Transformer-based EEG Decoding: A Survey'
arxiv_id: '2507.02320'
source_url: https://arxiv.org/abs/2507.02320
tags:
- transformer
- features
- ieee
- data
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews the evolution of Transformer-based
  models in EEG decoding since 2020, covering direct applications, hybrid architectures
  (CNN-Transformer, GAN-Transformer, GNN-Transformer, etc.), and customized Transformers.
  The analysis highlights how Transformers address EEG's temporal, spatial, and spectral
  complexity, with most models focusing on temporal features and spatial relationships
  while rarely extracting frequency features independently.
---

# Transformer-based EEG Decoding: A Survey

## Quick Facts
- arXiv ID: 2507.02320
- Source URL: https://arxiv.org/abs/2507.02320
- Authors: Haodong Zhang; Hongqi Li
- Reference count: 40
- Key outcome: This survey comprehensively reviews the evolution of Transformer-based models in EEG decoding since 2020, covering direct applications, hybrid architectures (CNN-Transformer, GAN-Transformer, GNN-Transformer, etc.), and customized Transformers. The analysis highlights how Transformers address EEG's temporal, spatial, and spectral complexity, with most models focusing on temporal features and spatial relationships while rarely extracting frequency features independently. The survey discusses challenges including data scarcity, limited generalization, computational demands, and interpretability, and proposes future directions such as Brain Foundation Models, cross-domain transfer learning, and multi-modal integration. The work offers a structured overview of current Transformer advancements in EEG decoding, emphasizing their adaptability and potential to improve feature extraction and model robustness in BCI/BMI applications.

## Executive Summary
This survey comprehensively reviews Transformer-based models for EEG decoding from 2020 to 2024, analyzing 165 models across three categories: direct Backbone Transformers, Hybrid architectures (CNN-Transformer, GNN-Transformer, etc.), and Customized Transformers. The work systematically evaluates how these models extract temporal, spatial, and spectral features from EEG signals, revealing that most focus on temporal and spatial relationships while rarely extracting frequency features independently. The survey identifies key challenges including data scarcity, limited cross-subject generalization, computational demands, and interpretability issues, while proposing future directions such as Brain Foundation Models, cross-domain transfer learning, and multi-modal integration to address these limitations.

## Method Summary
The survey conducted a systematic literature review of Transformer-based EEG decoding models published between 2020 and 2024, categorizing 165 models into three architectural families. The analysis focused on how different Transformer variants handle EEG's temporal, spatial, and spectral characteristics, with performance metrics extracted from published results across standard datasets including SEED, BCI Competition IV, Sleep-EDFx, and CHB-MIT. The survey synthesized findings into a structured taxonomy, identified common patterns and limitations, and proposed future research directions based on the observed gaps in current approaches.

## Key Results
- Most Transformer models prioritize temporal and spatial feature extraction over frequency domain analysis, with limited independent frequency feature extraction capabilities
- Hybrid CNN-Transformer architectures represent the dominant approach, leveraging CNNs for local feature extraction and Transformers for global dependency modeling
- The field faces four primary challenges: data scarcity, poor cross-subject generalization, high computational demands, and weak interpretability
- Performance comparisons across 165 models show accuracy ranges from 74-90% for motor imagery, 78-92% for emotion recognition, and 78-84% for sleep staging tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers may improve EEG decoding by modeling long-range temporal dependencies through self-attention.
- **Mechanism:** The multi-head attention mechanism computes relationships between all sequence elements simultaneously, assigning learned importance weights to different time points regardless of their distance in the sequence.
- **Core assumption:** EEG contains meaningful patterns distributed across distant time points that traditional sequential models (RNNs) fail to capture efficiently.
- **Evidence anchors:**
  - [Abstract] "Transformer is renowned for its strong handling capability of sequential data by the attention mechanism"
  - [Section I] "The core innovation of Transformer is the multi-head attention mechanism (MHA), which calculates the relationships between elements in the input sequence and assigns different importance weights to each element"
  - [Corpus] Related work on attention mechanisms in BCI supports the growing adoption of this approach, though specific causal validation remains limited.
- **Break condition:** If EEG patterns are predominantly local with minimal long-range dependencies, the attention mechanism's computational overhead may not justify performance gains.

### Mechanism 2
- **Claim:** Hybrid CNN-Transformer architectures potentially address the local-global feature extraction tradeoff in EEG processing.
- **Mechanism:** CNNs perform initial tokenization and local feature extraction through convolutions, then Transformers process the embedded representations to capture global dependencies and inter-channel relationships.
- **Core assumption:** EEG requires both fine-grained local temporal/spatial features AND global contextual information for optimal decoding.
- **Evidence anchors:**
  - [Section III.A] "CNNs are often exploited to perform the tokenization and embedding of EEG data, while Transformer extracts the entailed global features from the embedded representations"
  - [Section III.A] "CNNs have inherent limitations in handling sequential data... particularly for biological signals such as EEG... These issues, however, can be effectively addressed by Transformers"
  - [Corpus] The Cortical-SSM paper introduces an alternative state space approach for motor imagery, suggesting ongoing exploration beyond pure attention mechanisms.
- **Break condition:** If convolutional preprocessing destroys or distorts critical long-range dependencies before the Transformer can process them.

### Mechanism 3
- **Claim:** Graph Neural Network integration with Transformers may enhance spatial relationship modeling between EEG electrodes.
- **Mechanism:** GNNs model electrode channels as graph nodes, capturing complex spatial interactions between brain regions; Transformers then process sequential information within this spatially-informed representation.
- **Core assumption:** The spatial arrangement of electrodes encodes meaningful brain connectivity patterns that should be explicitly modeled rather than treated as independent channels.
- **Evidence anchors:**
  - [Section III.D] "Graph neural networks (GNNs) can model EEG signals as graphs, capturing the complex interactions between different brain regions"
  - [Section III.D] "EmoGT integrates graph convolutional networks (GCN) with Transformer to optimize spatial relationship extraction"
  - [Corpus] Weak direct corpus support; related graph-attention work exists but causal validation in this specific architecture remains limited.
- **Break condition:** If electrode placement variability across sessions/subjects makes learned graph structures unreliable.

## Foundational Learning

- **Concept: Self-Attention Mechanism**
  - **Why needed here:** Understanding how Transformers compute weighted relationships across sequence positions is essential for interpreting architectural modifications in EEG applications.
  - **Quick check question:** Can you explain why self-attention scales as O(n²) with sequence length and how this impacts EEG window size selection?

- **Concept: EEG Signal Characteristics (Temporal/Spatial/Spectral)**
  - **Why needed here:** The survey organizes models by how they extract features across these three dimensions; choosing appropriate architectures requires understanding which features matter for specific tasks.
  - **Quick check question:** Why does the paper note that "current Transformers... have limited applications to separately dig into frequency domain features"?

- **Concept: Transfer Learning in Physiological Signals**
  - **Why needed here:** Data scarcity is identified as a major challenge; the survey proposes cross-domain transfer learning and foundation models as solutions.
  - **Quick check question:** What assumptions must hold for a model pre-trained on one EEG dataset to transfer effectively to a different task or subject population?

## Architecture Onboarding

- **Component map:** Direct/Backbone Transformers (encoder-only, encoder-decoder, ViT variants) -> Hybrid models (CNN-Transformer most common, plus GNN/RNN/GAN/Diffusion variants) -> Customized Transformers (multi-encoder, modified encoder with token mixer/enhancer variations, pyramid structures, reconstructed architectures)
- **Critical path:** For most practical BCI applications, start with CNN-Transformer hybrids—specifically, factorized convolution for temporal-spatial preprocessing followed by a standard Transformer encoder. This pattern appears consistently across motor imagery, emotion recognition, and sleep staging tasks.
- **Design tradeoffs:** Parameter counts range from ~1K (EEGNet baseline) to >1B (proposed foundation models). Higher capacity models show stronger generalization potential but require substantial data and compute. The survey notes that even within the same model family, parameter efficiency varies dramatically based on specific architectural choices.
- **Failure signatures:** The survey identifies four key failure modes: (1) Data scarcity causing overfitting—most acute for large Transformer variants; (2) Poor cross-subject generalization despite high within-subject performance; (3) Computational resource constraints limiting deployment; (4) Weak interpretability undermining clinical adoption.
- **First 3 experiments:**
  1. **Baseline comparison:** Implement a simple CNN (e.g., EEGNet) vs. a CNN-Transformer hybrid on a standard dataset (BCI Competition IV or SEED) to quantify the attention mechanism's contribution.
  2. **Architecture ablation:** Systematically test hybrid variants—Conv1D-only vs. Conv1D+Transformer vs. multi-branch designs—to determine which architectural components drive performance gains for your specific task.
  3. **Cross-subject validation:** Train on N-1 subjects, test on the held-out subject across multiple architectures; this directly addresses the generalization challenge highlighted in Section V.A.2 and reveals whether Transformer attention improves or degrades subject-independent performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Transformer-based EEG models improve generalization capabilities to function effectively across different subjects, sessions, and tasks?
- Basis in paper: [explicit] Section V states that "research on task generalization remains limited" and current models "lack extensive experimental validation for cross-subject generalization."
- Why unresolved: Models are typically optimized for specific datasets or narrow tasks, struggling with the high inherent variability of EEG signals across individuals (non-stationarity).
- What evidence would resolve it: Development of architecture designs or training paradigms that demonstrate robust performance on cross-subject and multi-task benchmarks without requiring extensive subject-specific fine-tuning.

### Open Question 2
- Question: How can intrinsic Transformer architectures be adapted to independently extract spectral (frequency) features, rather than relying solely on time-frequency conversions?
- Basis in paper: [explicit] The Discussion notes that current Transformers "rarely [extract] frequency features independently" and have "limited applications to separately dig into frequency domain features."
- Why unresolved: Most existing models prioritize temporal/spatial dependencies or process converted time-frequency images, potentially losing native spectral dynamics or increasing preprocessing complexity.
- What evidence would resolve it: Novel attention mechanisms or tokenization strategies that operate directly on frequency bands to capture spectral characteristics with higher fidelity than standard wavelet/FFT-based inputs.

### Open Question 3
- Question: What methods can effectively bridge the gap between the massive data requirements of Brain Foundation Models (BFMs) and the scarcity of large-scale, standardized EEG datasets?
- Basis in paper: [explicit] Section V identifies "data deficiency" as a primary challenge conflicting with the "tremendous" data scale required for BFMs.
- Why unresolved: Unlike natural language or image data, EEG collection is resource-intensive, time-consuming, and limited by ethical/privacy constraints, resulting in small datasets.
- What evidence would resolve it: Successful pre-training strategies utilizing synthetic data generation (e.g., via diffusion models) or cross-modal transfer learning that compensates for low data volume while maintaining robustness.

## Limitations
- The survey synthesizes findings across 165 studies without providing unified implementation details or conducting direct experimental validation
- Performance comparisons combine results from different experimental protocols, preprocessing pipelines, and evaluation strategies, making quantitative benchmarking challenging
- The survey identifies limitations in frequency-domain feature extraction but does not systematically analyze why this limitation exists or propose specific architectural solutions

## Confidence
- **High Confidence:** The survey's categorization of Transformer architectures (Backbone, Hybrid, Customized) and their general prevalence patterns across tasks is well-supported by the literature review and represents a comprehensive mapping of the field.
- **Medium Confidence:** The identified challenges (data scarcity, limited generalization, computational demands, interpretability) are well-documented in the broader deep learning literature and align with known EEG-specific difficulties, though specific quantitative prevalence rates across the surveyed models are not provided.
- **Low Confidence:** The proposed future directions (Brain Foundation Models, cross-domain transfer learning, multi-modal integration) are speculative and lack empirical validation within the survey itself - these represent informed hypotheses rather than demonstrated solutions.

## Next Checks
1. **Architectural Contribution Validation:** Systematically compare CNN-Transformer hybrids with their pure CNN baselines on identical datasets and preprocessing pipelines to isolate the attention mechanism's contribution to performance gains.
2. **Cross-Subject Generalization Test:** Conduct N-1 subject leave-one-out experiments across multiple Transformer architectures to quantify whether attention mechanisms improve or degrade subject-independent performance compared to traditional approaches.
3. **Frequency Feature Extraction Evaluation:** Implement and test Transformer architectures specifically designed for frequency-domain EEG analysis (e.g., spectro-temporal attention mechanisms) to validate or refute the survey's claim about limited frequency feature extraction capabilities.