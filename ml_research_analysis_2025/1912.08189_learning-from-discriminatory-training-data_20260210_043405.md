---
ver: rpa2
title: Learning from Discriminatory Training Data
arxiv_id: '1912.08189'
source_url: https://arxiv.org/abs/1912.08189
tags:
- learning
- discrimination
- data
- protected
- discriminatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of preventing discrimination in
  supervised learning models when training data may be tainted by historical bias.
  The authors formalize this as a dataset shift problem where training data (potentially
  discriminatory) differs from test data (non-discriminatory).
---

# Learning from Discriminatory Training Data

## Quick Facts
- **arXiv ID:** 1912.08189
- **Source URL:** https://arxiv.org/abs/1912.08189
- **Reference count:** 40
- **Key outcome:** OIM method achieves higher accuracy (up to 0.7) and lower demographic disparity than state-of-the-art fair learning methods across synthetic and real-world datasets

## Executive Summary
This paper addresses the problem of preventing discrimination in supervised learning models when training data may be tainted by historical bias. The authors formalize this as a dataset shift problem where training data (potentially discriminatory) differs from test data (non-discriminatory). They propose an Optimal Interventional Mixture (OIM) method that probabilistically intervenes on protected attributes to minimize cross-loss on fair test data while training on potentially biased data.

The OIM provably minimizes error under additive discriminatory concept shifts and handles multiple protected attributes and intersectionality. Evaluations show the OIM achieves higher accuracy (up to 0.7) and lower demographic disparity than state-of-the-art fair learning methods across synthetic and real-world datasets (COMPAS, German Credit, CelebA), while avoiding induced discrimination through proxy variables. The method is computationally lightweight and compatible with existing legal frameworks.

## Method Summary
The authors propose the Optimal Interventional Mixture (OIM) method, which treats discriminatory training data as a dataset shift problem. OIM identifies the fair (non-discriminatory) components of the data and creates an optimal mixture of the original and intervened distributions during training. This intervention specifically targets the protected attributes to minimize the cross-loss between potentially discriminatory training data and non-discriminatory test data. The method is grounded in the assumption that the discriminatory concept shift is additive (y = f(x) + h(z)) and provides theoretical guarantees for this specific case.

## Key Results
- OIM achieves up to 0.7 higher accuracy compared to state-of-the-art fair learning methods
- Significant reduction in demographic disparity across multiple real-world datasets (COMPAS, German Credit, CelebA)
- OIM avoids inducing discrimination through proxy variables, a common failure mode in other fair learning approaches
- The method is computationally lightweight and scales well with multiple protected attributes

## Why This Works (Mechanism)
The method works by recognizing that discriminatory training data represents a dataset shift from the fair test data distribution. By probabilistically intervening on protected attributes during training, OIM creates a mixture distribution that better matches the fair test distribution. This intervention minimizes the cross-loss between training and test data, leading to better generalization on fair data while avoiding discrimination.

## Foundational Learning
- **Dataset shift**: Why needed? Different distributions between training and test data can cause model failure. Quick check: Verify training and test data come from different distributions.
- **Causal inference**: Why needed? Understanding causal relationships helps identify discriminatory influences. Quick check: Validate the causal model structure before applying OIM.
- **Fair learning**: Why needed? Standard ML models can perpetuate or amplify discrimination. Quick check: Measure demographic parity/disparate impact before and after applying OIM.
- **Optimal transport**: Why needed? Computing the optimal mixture distribution. Quick check: Verify the intervention probabilities sum to 1.
- **Cross-validation**: Why needed? Ensuring the model generalizes to fair test data. Quick check: Use stratified k-fold CV when evaluating fairness metrics.

## Architecture Onboarding

**Component map:** Training data -> Causal model analysis -> Intervention probability calculation -> Optimal mixture creation -> Model training -> Fair test data evaluation

**Critical path:** The core innovation is the intervention probability calculation that determines how to optimally mix the original and intervened distributions. This step directly impacts model fairness and accuracy.

**Design tradeoffs:** The method trades off between preserving useful information in the original data and removing discriminatory patterns. The additive assumption for discriminatory shifts may limit applicability to more complex bias scenarios.

**Failure signatures:** 
- High cross-loss between training and test data indicates poor intervention calibration
- Increased demographic disparity suggests the causal model was incorrectly specified
- Low accuracy with high fairness metrics may indicate over-intervention

**3 first experiments:**
1. Verify OIM performance on synthetic data with known discriminatory shifts
2. Test OIM with different numbers of protected attributes to assess scalability
3. Evaluate OIM's sensitivity to incorrect causal model specification

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can learning methods designed for different types of dataset shifts (e.g., covariate shifts vs. concept shifts) be combined effectively, or do their optimization objectives conflict?
- **Basis in paper:** [explicit] The conclusion states that identifying different shifts is possible, but "the remaining big question is whether these methods can be combined or are conflicting."
- **Why unresolved:** This study focused exclusively on discriminatory concept shifts and did not evaluate interactions with methods addressing sampling biases (covariate shifts).
- **What evidence would resolve it:** A theoretical analysis or empirical benchmark evaluating the performance of models that simultaneously apply sample re-weighting (for covariate shift) and Optimal Interventional Mixtures (for concept shift).

### Open Question 2
- **Question:** Does the Optimal Interventional Mixture (OIM) provably minimize error under discriminatory concept shifts with non-additive or complex functional forms?
- **Basis in paper:** [explicit] The authors note they "do not provide resilience guarantees for discriminatory concept shifts with other perturbations than additive perturbations" and suggest future research study "other dataset shifts."
- **Why unresolved:** The theoretical guarantees (Proposition 1) rely on the assumption that the discriminatory perturbation is additive ($y = f(x) + h(z)$), which may not hold in all real-world bias scenarios.
- **What evidence would resolve it:** Derivation of error bounds for OIM under non-linear perturbation functions or empirical demonstrations of resilience on synthetic datasets with multiplicative or highly non-linear bias functions.

### Open Question 3
- **Question:** How can discriminatory concept shifts be accurately quantified in real-world datasets through observational studies or randomized experiments?
- **Basis in paper:** [explicit] The limitations section proposes that future work could measure shifts via "randomized human subject experiments or observational studies" to bolster credibility.
- **Why unresolved:** The paper relies on synthetic data or proxies for "fair" test data; there is currently no standardized benchmark for measuring the magnitude of historical bias (concept shift) in natural datasets.
- **What evidence would resolve it:** A methodology paper detailing a protocol for isolating discriminatory concept shifts from noise in historical data, validated against a controlled trial.

### Open Question 4
- **Question:** How robust is the OIM method when the practitioner incorrectly specifies the causal model or fails to identify all variables subject to indirect discrimination?
- **Basis in paper:** [inferred] The limitations state a practitioner may "neglect the proper understanding of the causal processes" regarding fair vs. unfair relationships.
- **Why unresolved:** The method relies on the assumption that the user can correctly distinguish between "fair relationship" and "unfair influence" (Definitions 1-2) to structure the interventions; errors here could induce the very discrimination the method seeks to prevent.
- **What evidence would resolve it:** A sensitivity analysis measuring the increase in induced discrimination or cross-loss when the causal graph used for OIM has missing protected attributes or incorrectly labeled fair pathways.

## Limitations
- Assumes knowledge of protected attributes during testing, which may not be available in practice
- Performance claims rely heavily on synthetic datasets, raising questions about generalizability
- The additive discriminatory concept shift model may not capture all types of bias present in real datasets
- Assumes test data is guaranteed to be fair, which may not hold in real-world scenarios

## Confidence
- **High confidence:** The OIM method's theoretical foundation for handling dataset shifts and its computational efficiency
- **Medium confidence:** The empirical results showing improved fairness metrics, given the limited real-world dataset evaluation
- **Low confidence:** The assumption that fair test data is always available and that protected attributes are known during deployment

## Next Checks
1. Test OIM performance when both training and test data contain varying degrees of bias to assess robustness
2. Evaluate OIM's performance with unknown or partially known protected attributes during testing
3. Conduct extensive real-world experiments across diverse domains and bias types beyond the current datasets used