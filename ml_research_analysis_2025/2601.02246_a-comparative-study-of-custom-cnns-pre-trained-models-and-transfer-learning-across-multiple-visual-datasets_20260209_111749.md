---
ver: rpa2
title: A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning
  Across Multiple Visual Datasets
arxiv_id: '2601.02246'
source_url: https://arxiv.org/abs/2601.02246
tags:
- learning
- training
- cnns
- transfer
- custom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares three CNN training paradigms\u2014custom CNNs\
  \ trained from scratch, pre-trained CNNs used as frozen feature extractors, and\
  \ transfer learning via fine-tuning\u2014across five real-world image classification\
  \ datasets covering road surface damage, agriculture, and urban scene recognition.\
  \ All models are evaluated under identical conditions using accuracy, macro F1-score,\
  \ training time per epoch, and parameter counts."
---

# A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets

## Quick Facts
- arXiv ID: 2601.02246
- Source URL: https://arxiv.org/abs/2601.02246
- Reference count: 29
- Primary result: Transfer learning achieves highest accuracy across five real-world image datasets, with custom CNNs offering competitive efficiency trade-offs

## Executive Summary
This study systematically compares three CNN training paradigms—custom CNNs trained from scratch, frozen pre-trained CNNs used as feature extractors, and transfer learning via fine-tuning—across five diverse visual classification tasks. The experiments reveal consistent patterns: transfer learning delivers the highest predictive performance, particularly in datasets with high intra-class variability or domain shift relative to ImageNet. Custom CNNs trained from scratch offer a practical efficiency-accuracy trade-off, especially valuable when computational resources are constrained. Frozen pre-trained models provide a reasonable baseline but are consistently outperformed by fine-tuning, suggesting that fixed features from ImageNet may not align optimally with target domain statistics.

## Method Summary
The study evaluates three training paradigms on five image classification datasets covering road damage, agriculture, and urban scenes. All models are trained and evaluated under identical conditions: 20 epochs, SGD with momentum 0.9, learning rate 0.01 with cosine decay, and batch size (assumed 32-64). Custom CNNs follow a 4-stage VGG-inspired architecture with batch normalization, global average pooling, and dropout. Pre-trained models use VGG-16 initialized with ImageNet weights, either frozen or fine-tuned on the final convolutional block and classifier. Performance is measured using accuracy, macro F1-score, training time per epoch, and parameter count.

## Key Results
- Transfer learning consistently achieves the highest accuracy across all five datasets, with gains particularly pronounced in high domain shift conditions
- Custom CNNs trained from scratch offer competitive accuracy with substantially lower computational cost (~2× faster training, ~14× fewer parameters)
- Frozen pre-trained models improve over scratch training but are consistently outperformed by fine-tuning, suggesting fixed ImageNet features may not align with target domains
- The efficiency-accuracy trade-off varies by dataset complexity, with custom CNNs being most competitive on simpler tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning via fine-tuning yields the highest predictive performance across diverse visual classification tasks.
- Mechanism: Pre-trained networks initialize with general-purpose visual primitives (edges, textures, object parts) learned from large-scale datasets. Fine-tuning higher-level layers allows these representations to adapt to domain-specific characteristics while retaining low-level feature extraction capabilities.
- Core assumption: Target datasets share some visual commonalities with ImageNet, but exhibit domain-specific patterns (e.g., agricultural textures, road surface irregularities) that benefit from representation adjustment.
- Evidence anchors:
  - [abstract] "Results show that transfer learning consistently achieves the highest predictive performance, with gains particularly pronounced in datasets exhibiting high intra-class variability or domain shift relative to ImageNet."
  - [section 2.3] "Fine-tuning allows the model to adjust its representations to domain-specific characteristics and typically yields superior performance, particularly when moderate amounts of labeled data are available."
  - [corpus] Related work on crop disease detection (arXiv:2506.20323) reports similar findings: transfer learning approaches outperform custom architectures for agricultural imagery with domain shift.

### Mechanism 2
- Claim: Compact custom CNNs can achieve competitive accuracy with substantially lower computational cost.
- Mechanism: Modern architectural components—batch normalization (stabilizes training, enables higher learning rates), global average pooling (reduces parameters, improves generalization), and strategic depth—allow smaller networks to learn effective hierarchical representations without the parameter overhead of large pre-trained models.
- Core assumption: The custom architecture is well-matched to task complexity; overly simple architectures will underfit on high-dimensional visual tasks.
- Evidence anchors:
  - [section 4.2.1] "This model is substantially smaller than VGG-16 while retaining strong representational capacity."
  - [table 5] Custom CNN: ~9–10M parameters vs. VGG-16: ~138M parameters.
  - [corpus] Pediatric pneumonia study (arXiv:2601.00837) found custom CNNs achieved competitive results on chest X-rays, though transfer learning still led—suggesting efficiency gains are domain-dependent.

### Mechanism 3
- Claim: Frozen pre-trained feature extractors provide a reasonable baseline but are consistently outperformed by fine-tuning.
- Mechanism: Frozen convolutional layers retain ImageNet-specific feature optimizations that may not align with target domain statistics (color distributions, texture patterns, scene composition). Without gradient updates, the feature extractor cannot adjust filters to domain-relevant patterns.
- Core assumption: The target domain differs sufficiently from ImageNet that fixed features are suboptimal, but not so different that pre-trained initialization provides no benefit.
- Evidence anchors:
  - [section 7] "Pre-trained VGG-16 used as a frozen feature extractor provides a reasonable baseline that often improves over scratch training, but it is consistently outperformed by fine-tuning. This suggests that representational reuse alone may be insufficient when texture, color statistics, and imaging conditions differ from the source domain."
  - [table 3] Across all five datasets, frozen pre-trained CNN accuracy ranges from 89.1–95.0%, while transfer learning achieves 92.1–96.8%.

## Foundational Learning

- **Concept: Hierarchical Feature Learning**
  - Why needed here: CNNs learn multi-scale representations—early layers capture edges/textures, deeper layers capture object parts and compositional patterns. Understanding this hierarchy explains why fine-tuning higher layers (while freezing lower layers) is effective.
  - Quick check question: Can you explain why fine-tuning only the final convolutional block (rather than all layers) might reduce catastrophic forgetting while still adapting to the target domain?

- **Concept: Domain Shift and Transferability**
  - Why needed here: The paper evaluates datasets (road damage, agriculture, urban scenes) with varying degrees of visual similarity to ImageNet. Domain shift determines how much adaptation is needed.
  - Quick check question: Given a dataset of grayscale medical X-rays, would you expect greater or lesser transfer learning gains compared to RGB agricultural imagery, and why?

- **Concept: Efficiency–Accuracy Trade-off Analysis**
  - Why needed here: Model selection requires balancing accuracy, training time, parameter count, and deployment constraints. The paper provides a framework for this analysis.
  - Quick check question: If deploying on an edge device with 256MB RAM and requiring <100ms inference latency, which paradigm would you prioritize, and what accuracy loss would you expect based on the paper's results?

## Architecture Onboarding

- **Component map:**
  - Custom CNN: 4-stage VGG-inspired backbone (64→128→256→512 channels) + BatchNorm + ReLU + MaxPool → Global Average Pooling → FC(512→256) → ReLU → Dropout(0.5) → FC(256→C)
  - Pre-trained (frozen): VGG-16 convolutional layers (frozen, ImageNet weights) → FC classifier head (trainable)
  - Transfer learning: VGG-16 (partial fine-tuning of final conv block + classifier head, remaining layers frozen initially)

- **Critical path:**
  1. Implement preprocessing pipeline (resize to 224×224, ImageNet normalization)
  2. Build data loaders with augmentation (random flip, color jitter, random crop) for training only
  3. Train all three paradigms under identical conditions (20 epochs, SGD with momentum 0.9, LR 0.01, cosine decay)
  4. Evaluate on held-out test set using accuracy and macro F1-score

- **Design tradeoffs:**
  - Custom CNN: Lower accuracy (~2–4% gap vs. transfer learning), but ~2× faster training per epoch, ~14× fewer parameters
  - Frozen pre-trained: Fastest per-epoch training, but accuracy ceiling limited by frozen features
  - Transfer learning: Highest accuracy, but ~2× training time vs. custom CNN due to backpropagation through unfrozen layers

- **Failure signatures:**
  - Custom CNN underfitting: Training and validation accuracy both low → increase model capacity or reduce regularization
  - Frozen features plateau: Validation accuracy stagnates early → switch to fine-tuning
  - Transfer learning overfitting: High training accuracy, low validation accuracy → increase dropout, reduce unfrozen layers, or use lower learning rate

- **First 3 experiments:**
  1. **Baseline reproduction**: Run all three paradigms on a single dataset (e.g., RoadDamageBD) to verify relative performance matches paper findings (transfer learning > frozen > custom by expected margins).
  2. **Ablation on unfrozen depth**: Fine-tune only the classifier head vs. final conv block vs. final two conv blocks to quantify accuracy gains vs. training time trade-off.
  3. **Domain shift sensitivity**: Compare transfer learning gains across datasets with high (agricultural) vs. low (urban scenes) domain shift relative to ImageNet to validate the paper's claim that gains correlate with domain shift magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do modern architectures like EfficientNet or ResNet shift the efficiency–accuracy trade-offs compared to VGG-16?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "Additional backbones (ResNet, EfficientNet) could shift the efficiency–accuracy frontier."
- **Why unresolved:** The study restricted experiments to VGG-16 and a single VGG-style custom architecture.
- **What evidence would resolve it:** Replicating the comparative study using ResNet-50 and EfficientNet-B0 backbones on the same datasets.

### Open Question 2
- **Question:** Does dataset-specific hyperparameter optimization significantly alter the relative performance gap between custom CNNs and transfer learning?
- **Basis in paper:** [explicit] The authors note that "hyperparameters are kept consistent across datasets for fairness; dataset-specific tuning may yield higher absolute performance for all methods."
- **Why unresolved:** A fixed training protocol (learning rate, epochs) was used to isolate the effect of the training paradigm, potentially under-optimizing individual models.
- **What evidence would resolve it:** Performing independent hyperparameter searches for each model type per dataset and comparing the resulting optimal scores.

### Open Question 3
- **Question:** To what extent does full-network fine-tuning improve performance over partial fine-tuning in these specific domains?
- **Basis in paper:** [inferred] The methodology fine-tunes only the "final convolutional block," while the text acknowledges that "fine-tuning increases computational cost," leaving the trade-off of deeper fine-tuning unexplored.
- **Why unresolved:** The study does not compare partial (last block) versus full backbone fine-tuning strategies.
- **What evidence would resolve it:** Ablation studies comparing the accuracy and training time of fine-tuning varying numbers of layers (e.g., last block vs. last two blocks vs. full network).

## Limitations
- Fixed 20-epoch training horizon may not reflect convergence behavior across paradigms
- Uniform hyperparameters (SGD with momentum 0.9, learning rate 0.01) may not be optimal for all training strategies
- Domain shift quantification is qualitative rather than systematic

## Confidence
- **High confidence**: Transfer learning consistently outperforms frozen feature extraction; custom CNN efficiency gains are reproducible under stated conditions
- **Medium confidence**: Magnitude of accuracy differences (2-4% gap) and training time savings (~2×) are sensitive to hyperparameter choices and dataset splits not fully specified
- **Low confidence**: Claims about domain shift correlation require validation with quantitative domain similarity metrics

## Next Checks
1. **Convergence analysis**: Train each paradigm until validation loss plateaus (not fixed 20 epochs) to verify relative performance rankings are robust to training duration
2. **Domain similarity quantification**: Compute feature space distance metrics between each dataset and ImageNet to empirically validate claims about domain shift correlation with transfer learning gains
3. **Hyperparameter sensitivity**: Perform grid search over learning rate (0.001-0.01) and weight decay (1e-4 to 1e-5) for each paradigm to establish whether reported performance gaps persist across hyperparameter settings