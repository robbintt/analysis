---
ver: rpa2
title: 'CARES: Context-Aware Resolution Selector for VLMs'
arxiv_id: '2510.19496'
source_url: https://arxiv.org/abs/2510.19496
tags:
- resolution
- cares
- visual
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARES introduces a context-aware resolution selector that predicts
  the minimal sufficient input resolution for a given image-query pair before processing
  by a vision-language model. By leveraging a compact proxy VLM to extract joint image-query
  features and a lightweight classifier, CARES selects the optimal resolution based
  on task performance convergence, reducing compute by 70-80% while maintaining accuracy
  across diverse benchmarks and models.
---

# CARES: Context-Aware Resolution Selector for VLMs

## Quick Facts
- arXiv ID: 2510.19496
- Source URL: https://arxiv.org/abs/2510.19496
- Authors: Moshe Kimhi; Nimrod Shabtay; Raja Giryes; Chaim Baskin; Eli Schwartz
- Reference count: 10
- Primary result: Reduces compute by 70-80% while maintaining accuracy across diverse benchmarks and models

## Executive Summary
CARES introduces a context-aware resolution selector that predicts the minimal sufficient input resolution for a given image-query pair before processing by a vision-language model. By leveraging a compact proxy VLM to extract joint image-query features and a lightweight classifier, CARES selects the optimal resolution based on task performance convergence, reducing compute by 70-80% while maintaining accuracy across diverse benchmarks and models. The method uses discrete supervision but enables continuous resolution at inference for fine-grained control, demonstrating significant efficiency gains without sacrificing output quality.

## Method Summary
CARES predicts minimal sufficient input resolution for VLMs using a two-stage approach: first, a lightweight proxy VLM (SmolVLM-350M) extracts joint image-query features at low resolution (384px); second, a lightweight classifier predicts the optimal resolution bin (low/medium/high) based on these features. The method is trained using a labeling pipeline that runs the target VLM at multiple resolutions to identify where performance plateaus, then supervises the classifier to predict this "sufficient" resolution. At inference, CARES can output either discrete resolution classes or continuous values via probability interpolation, allowing fine-grained compute control.

## Key Results
- Reduces compute by 70-80% while maintaining accuracy across diverse benchmarks and models
- Continuous resolution prediction saves 63% FLOPs versus 46% for discrete prediction on InternVL3-8B
- Mid-layer features (Layer 16) outperform final layer features for resolution prediction

## Why This Works (Mechanism)

### Mechanism 1: Query-Conditioned Sufficiency Rollouts
A lightweight proxy model can estimate the minimal resolution required for a target VLM to answer a specific query correctly by learning from discrete resolution rollouts. CARES labels training data by escalating input resolution (e.g., 384 → 768 → 1024) for a fixed image-query pair, identifying the lowest resolution where the target VLM's performance metric (ANLS) exceeds a threshold and plateaus. This label supervises a compact feature extractor to predict resolution needs without running the full model. The core assumption is that sufficiency is consistent between training and inference, and visual complexity relative to the query is predictable from low-resolution features.

### Mechanism 2: Continuous Interpolation from Discrete Supervision
Interpolating between discrete class probabilities enables fine-grained compute control (e.g., 500px) superior to hard classification switches. The model outputs a soft distribution over discrete resolution bins and computes an expectation instead of taking the argmax, allowing the system to route "ambiguous" cases to middle-ground resolutions rather than overshooting to the next power-of-2 bucket. The core assumption is that the classifier's probability distribution is well-calibrated such that the weighted average correlates with actual task difficulty.

### Mechanism 3: Intermediate Layer Representation
Intermediate layers of a VLM encode optimal "perceptual sufficiency" signals better than the final layer. CARES uses a pruned SmolVLM (350M params), extracting features from Layer 16 (mid-layer) rather than the final layer, bypassing the computational cost of the full model stack while capturing joint image-text semantics. The core assumption is that mid-layer features contain unrefined but sufficient "complexity" signals (layout, text density) needed for routing, which may be lost or abstracted away in the final generative layers.

## Foundational Learning

- **Concept**: **ANLS (Average Normalized Levenshtein Similarity)**
  - **Why needed here**: The entire supervision signal for CARES relies on calculating ANLS scores between the VLM response and ground truth at various resolutions. Without understanding that ANLS handles partial matches (unlike Exact Match), the labeling logic is opaque.
  - **Quick check question**: Why does the paper use ANLS (0-1 scale) rather than binary accuracy for determining "sufficiency" in document tasks?

- **Concept**: **Visual Token Sparsification vs. Input Resizing**
  - **Why needed here**: The paper positions CARES as a pre-tokenization approach. You must distinguish between "pruning tokens after they are created" (e.g., PyramidDrop) and "reducing pixels before tokenization" (CARES) to understand where CARES fits in the efficiency stack.
  - **Quick check question**: Does CARES reduce the *sequence length* of visual tokens, the *dimension* of tokens, or the *number* of tokens generated by the vision encoder?

- **Concept**: **Label Smoothing for Regression/Calibration**
  - **Why needed here**: The paper uses label smoothing (0.05) on the classifier to support continuous inference.
  - **Quick check question**: How does softening the hard edges of the resolution class labels (e.g., smoothing targets 0,1,0 to 0.025, 0.95, 0.025) improve the continuous resolution estimate $\tilde{r}$?

## Architecture Onboarding

- **Component map**: Raw Image + Text Query → Proxy Encoder (SmolVLM-350M) → Router Head (Linear Classifier) → Resolution Logic (Softmax + Expectation) → Target Interface (Image Resizer → Target VLM)

- **Critical path**: The Labeling Pipeline (Algorithm 1). This is the most expensive engineering step. You must be able to run the *target* VLM (e.g., 72B model) on the dataset at multiple resolutions (384, 768, 1024) to generate the ground truth labels $r^*$ before you can train the lightweight CARES module.

- **Design tradeoffs**:
  - A 2-class router (Low/High) is easier to train (96% acc) but yields worse downstream efficiency than a 3-class router (67% acc) because the 3-class allows intermediate compute savings
  - Continuous prediction adds complexity (calibration issues) but saves significantly more FLOPs (63% vs 46%) than discrete prediction
  - A larger proxy (Qwen-3B) is more accurate, but SmolVLM (350M) is chosen for minimal latency overhead

- **Failure signatures**:
  - Performance Collapse on OCR: Significant accuracy drops on dense text benchmarks, indicating the "Low Res" proxy features failed to trigger the "High Res" allocation for hard-to-read text
  - Stuck Resolution: The model predicts the exact same resolution (e.g., always 384) regardless of input, suggesting the classifier learned a trivial bias
  - High Latency Overhead: Total latency (Proxy + Target) exceeds Native Latency, meaning the 350M proxy overhead outweighed the token savings on the target

- **First 3 experiments**:
  1. Validation of Label Distribution: Run the labeling script on a small subset (100 samples) of DocVQA. Plot the histogram of selected resolutions to ensure a balanced mix (not just "always max res")
  2. Ablation on Feature Depth: Train two CARES configs on a fixed validation set: one extracting Layer 16 features and one extracting Final Layer features. Compare validation accuracy to confirm the "Mid > Last" claim
  3. Discrete vs. Continuous ROI: Deploy the trained CARES module on a held-out test set. Measure "Visual Token Count" for Discrete vs. Continuous inference to verify the compute reduction claims

## Open Questions the Paper Calls Out
- How does CARES perform in multi-turn interactions where the required visual granularity changes over time?
- Can the single-image resolution policy be effectively extended to video or multi-image contexts?
- Does the limited capacity of the proxy VLM constrain performance in specialized domains requiring fine-grained detail?

## Limitations
- The labeling pipeline bottleneck makes training computationally prohibitive for large-scale datasets or multiple VLM targets
- Significant performance degradation on dense text benchmarks (DocVQA) where low-resolution features fail to trigger high-resolution allocation
- Continuous inference mechanism assumes well-calibrated classifier probabilities that may not hold for edge cases

## Confidence
- **High Confidence**: Claims about using proxy VLM features to predict resolution sufficiency are well-supported by ablation studies
- **Medium Confidence**: The 70-80% compute reduction claims are supported by experiments but may vary across different task distributions and model sizes
- **Low Confidence**: The assertion that CARES generalizes to arbitrary VLMs without re-labeling is questionable given explicit dependence on target VLM's performance characteristics

## Next Checks
1. Measure the total compute cost of the labeling pipeline (running target VLM at multiple resolutions for the entire training set) and compare it to the cumulative savings achieved by CARES deployment
2. Design a controlled experiment isolating OCR performance by creating a test set with varying text densities and font sizes, measuring CARES's resolution allocation accuracy specifically for text-heavy regions
3. Evaluate the softmax probability distributions output by the CARES classifier across different difficulty levels using calibration metrics to quantify whether probabilities are well-calibrated