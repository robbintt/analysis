---
ver: rpa2
title: Preserving Privacy and Utility in LLM-Based Product Recommendations
arxiv_id: '2505.00951'
source_url: https://arxiv.org/abs/2505.00951
tags:
- sensitive
- products
- privacy
- recommendation
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a hybrid privacy-preserving recommendation framework
  for LLM-based systems that separates sensitive and non-sensitive user data. The
  method uses a fine-tuned BERT model to classify products in purchase history as
  sensitive or non-sensitive, sending only non-sensitive data to a server-based LLM
  for recommendations while locally processing sensitive products with a lightweight
  Llama 3.2 model.
---

# Preserving Privacy and Utility in LLM-Based Product Recommendations

## Quick Facts
- **arXiv ID:** 2505.00951
- **Source URL:** https://arxiv.org/abs/2505.00951
- **Reference count:** 40
- **Primary result:** Achieves near-baseline recommendation utility (HR@10 of 0.6061 vs 0.6263 for baseline) while maintaining strong privacy preservation (L1/L2 category distribution distances of 0.3747/0.2267)

## Executive Summary
This paper proposes a hybrid privacy-preserving recommendation framework for LLM-based systems that separates sensitive and non-sensitive user data. The method uses a fine-tuned BERT model to classify products in purchase history as sensitive or non-sensitive, sending only non-sensitive data to a server-based LLM for recommendations while locally processing sensitive products with a lightweight Llama 3.2 model. Experiments on e-commerce datasets show the framework achieves near-baseline recommendation utility while maintaining strong privacy preservation, with the approach running efficiently on consumer-grade hardware.

## Method Summary
The framework fine-tunes a BERT-base-uncased model on 10K products with weighted focal loss to classify products as sensitive or non-sensitive. Purchase history is split accordingly, with non-sensitive products sent to ChatGPT-4o for recommendations and sensitive products processed locally by Llama 3.2 1B. The two recommendation sets are merged proportionally based on the original category proportions in the purchase history. The system is evaluated on Amazon Reviews 2023 dataset using HR@10, category distribution distances, and privacy leakage metrics.

## Key Results
- HR@10 of 0.6061 vs baseline of 0.6263, demonstrating near-baseline utility preservation
- L1 and L2 category distribution distances of 0.3747 and 0.2267 respectively compared to baseline
- Effective privacy preservation with controlled leakage through local processing of sensitive data
- Efficient runtime on consumer-grade hardware (4GB VRAM with 8-bit quantization)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Context-aware BERT classification preserves more useful purchase history than category-based filtering, maintaining recommendation quality while reducing privacy exposure.
- **Mechanism:** A fine-tuned BERT model with weighted focal loss classifies products using full textual metadata (title, features, description) rather than category labels alone. This reduces false positives where non-sensitive products within sensitive categories (e.g., general skincare under "Beauty & Personal Care") are incorrectly removed.
- **Core assumption:** Sensitive products exhibit semantic patterns distinguishable from non-sensitive products within the same broad category.
- **Evidence anchors:**
  - [abstract] "uses a fine-tuned BERT model to classify products in purchase history as sensitive or non-sensitive"
  - [Section 4.1.3] "By employing context-aware classification, nonsensitive products that fall under a broad category will not be misclassified as sensitive"
  - [corpus] Weak direct support; related papers discuss privacy-utility tradeoffs generally but not this specific classification approach
- **Break condition:** If sensitivity is highly user-specific or depends on purchase patterns rather than product attributes, the classifier's semantic patterns may not generalize.

### Mechanism 2
- **Claim:** Local deobfuscation recovers utility lost from obfuscation without transmitting sensitive data to external servers.
- **Mechanism:** A lightweight Llama 3.2 1B model generates sensitive-category recommendations entirely on-device, running in parallel with cloud LLM processing. The two recommendation sets are merged proportionally.
- **Core assumption:** Lightweight models can generate relevant recommendations within sensitive categories without requiring cross-category context from non-sensitive purchases.
- **Evidence anchors:**
  - [abstract] "locally processing sensitive products with a lightweight Llama 3.2 model"
  - [Section 4.3] "This step is performed locally on the user's device to maintain strict privacy standards, ensuring that sensitive data is never transmitted externally"
  - [Table 4] BERT Obf + Deobf achieves HR@10 of 0.6061 vs BERT Obf Only at 0.5960
- **Break condition:** If sensitive recommendations require understanding non-sensitive purchase patterns (e.g., lifestyle context), local model quality degrades significantly.

### Mechanism 3
- **Claim:** Proportional allocation of recommendation slots between sensitive and non-sensitive paths preserves category distribution alignment with baseline.
- **Mechanism:** Recommendation counts are allocated proportionally: n_ns = n × |P_ns|/(|P_ns|+|P_s|) and n_s = n × |P_s|/(|P_ns|+|P_s|). This ensures final merged list reflects original category proportions.
- **Core assumption:** User preference distribution across categories is adequately captured by purchase history category proportions.
- **Evidence anchors:**
  - [Section 4.2.2] Formula for proportional nonsensitive recommendations
  - [Table 5] Sensitive category L2 distance improves from 0.0547 (obf-only) to 0.0401 (with deobfuscation)
  - [corpus] No direct corpus evidence for this proportional allocation mechanism
- **Break condition:** If recent purchases don't reflect current preferences (e.g., one-time sensitive purchases skew proportions), allocation may over/under-weight categories.

## Foundational Learning

- **Concept: Weighted Focal Loss**
  - Why needed here: Standard cross-entropy underweights the minority "sensitive" class; focal loss concentrates learning on hard examples while class weighting amplifies the underrepresented sensitive class (Equation 9-10).
  - Quick check question: If your dataset has 85% non-sensitive and 15% sensitive samples, and you want to penalize missing sensitive items more heavily, how would you adjust the class weight formula?

- **Concept: Split Inference Architecture**
  - Why needed here: Understanding the partition between local (privacy-sensitive) and cloud (compute-intensive) processing is fundamental to this hybrid design.
  - Quick check question: What privacy guarantees does local processing provide if the local model outputs are later shared with the server?

- **Concept: Distribution Distance Metrics (L1/L2)**
  - Why needed here: HR@10 measures individual relevance but not category diversity; L1/L2 distances quantify how well the recommendation distribution matches baseline across all categories.
  - Quick check question: If baseline distribution is [0.5, 0.3, 0.2] across three categories and your system produces [0.4, 0.4, 0.2], what is the L1 distance?

## Architecture Onboarding

- **Component map:**
  Obfuscator (local) -> BERT-base (110M params) -> Binary sensitivity classification with 0.3 threshold -> Server Recommender (cloud) -> ChatGPT-4o API -> generates n_ns non-sensitive recommendations -> Local Deobfuscator (local) -> Llama 3.2 1B (8-bit quantized on constrained devices) -> generates n_s sensitive recommendations -> Merger -> Concatenates R_ns + R_s into final R''

- **Critical path:**
  Purchase history P → BERT classification → Parallel execution: (P_ns → cloud API → R_ns) AND (P_s → local Llama → R_s) → Proportional merge → R''

- **Design tradeoffs:**
  - Categorical vs BERT obfuscation: Categorical guarantees 0% leakage but higher utility loss (HR@10: 0.4742); BERT preserves utility (HR@10: 0.5960) with ~22% leakage
  - Classification threshold: Lower threshold (0.3 vs 0.5) increases recall for sensitive items but may remove more non-sensitive products
  - Local model size: Larger model improves sensitive recommendations but increases latency (Table 7 shows 3.2GB peak GPU for Llama 1B)

- **Failure signatures:**
  - Privacy leakage >25%: Classifier threshold too high or insufficient training data for sensitive patterns
  - Low HR@10 with low category distance: Server LLM receiving insufficient context; check obfuscation rate
  - High L1/L2 distance for sensitive categories only: Local model generating off-target recommendations
  - Inference latency >10s on consumer hardware: Quantize Llama model or reduce max output tokens

- **First 3 experiments:**
  1. Baseline calibration: Send full purchase history to cloud LLM; record HR@10 (expect ~0.63), L1/L2 = 0, and 100% privacy leakage as reference bounds
  2. Obfuscator isolation test: Run BERT obfuscation without deobfuscation; verify privacy leakage (~22%) and utility drop (HR@10 ~0.60)
  3. Full hybrid validation: Enable obfuscation + deobfuscation; confirm HR@10 recovery toward baseline (0.6061) and L1/L2 reduction (0.3747/0.2267) compared to obfuscation-only

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can privacy-preserving recommendation systems account for cumulative purchase patterns that may reveal sensitive information even when individual products appear nonsensitive?
- **Basis in paper:** [explicit] The authors state: "Our approach does not address this form of privacy inference, as it focuses on product-level sensitivity rather than cumulative patterns."
- **Why unresolved:** The current framework classifies products individually, but temporal patterns (e.g., frequent pain reliever purchases suggesting chronic conditions) could leak private information.
- **What evidence would resolve it:** A modified framework that tracks purchase frequency and temporal patterns, evaluated on datasets with longitudinal user histories showing reduced pattern-based inference attacks.

### Open Question 2
- **Question:** How can the system incorporate user-adaptive privacy controls to reflect individual and context-dependent sensitivity preferences?
- **Basis in paper:** [explicit] The authors note: "sensitivity is subjective, and factors such as marital status, income level, and gender may also be considered private depending on the user" and state that "User-adaptive privacy controls can improve its flexibility."
- **Why unresolved:** The current approach uses a fixed, health-centric definition of sensitivity that may not reflect diverse user privacy preferences across cultural or personal contexts.
- **What evidence would resolve it:** User studies measuring satisfaction with personalized sensitivity settings, or experiments showing improved privacy-utility tradeoffs with user-customizable thresholds.

### Open Question 3
- **Question:** Can more sophisticated classification approaches reduce the 22% privacy leakage rate while remaining deployable on consumer-grade hardware?
- **Basis in paper:** [inferred] The BERT-based obfuscator achieves 22.28% privacy leakage (PL_b), and the authors note: "A more complex classifier could improve sensitivity detection but may not be practical for local execution."
- **Why unresolved:** There is a tension between classification accuracy and on-device computational feasibility that remains unexplored in the paper.
- **What evidence would resolve it:** Experiments with alternative model architectures (e.g., quantized transformers, knowledge distillation) showing lower privacy leakage while meeting the same hardware constraints.

### Open Question 4
- **Question:** How does the framework perform when applied to domains beyond e-commerce, such as content streaming or news recommendation?
- **Basis in paper:** [inferred] The evaluation is limited to the Amazon Reviews 2023 dataset across ten categories, and the prompt designs are specific to product recommendations.
- **Why unresolved:** Different domains have different sensitivity definitions and recommendation patterns that may affect both the obfuscation and deobfuscation effectiveness.
- **What evidence would resolve it:** Cross-domain experiments showing comparable HR@10 and category distribution alignment metrics on datasets from streaming platforms or news aggregators.

## Limitations
- Limited evaluation to e-commerce domain; performance on other recommendation domains remains unknown
- Does not address cumulative privacy leakage from temporal purchase patterns
- Fixed sensitivity definition may not reflect diverse user privacy preferences across contexts

## Confidence
- **High Confidence:** The hybrid architecture design (separating sensitive/non-sensitive processing) is well-specified and the core mechanism of proportional allocation is clearly defined
- **Medium Confidence:** The BERT classification approach and its utility-preserving properties are supported by experimental results, though the training details have gaps
- **Low Confidence:** The claim that local deobfuscation fully recovers utility without compromising privacy relies on assumptions about recommendation dependencies that aren't thoroughly tested

## Next Checks
1. **Classifier Robustness Test:** Evaluate the BERT classifier on out-of-distribution sensitive products to verify that semantic patterns generalize beyond the training set, checking for false negatives that would leak sensitive data
2. **Cross-Category Dependency Analysis:** Systematically remove non-sensitive products from test users and measure degradation in sensitive-category recommendation quality to validate the independence assumption
3. **Privacy Leakage Measurement:** Implement ground-truth sensitive product labeling for test users and compute actual privacy leakage rates rather than relying solely on model confidence scores