---
ver: rpa2
title: Localizing and Correcting Errors for LLM-based Planners
arxiv_id: '2602.00276'
source_url: https://arxiv.org/abs/2602.00276
tags:
- actions
- goal
- action
- l-icl
- calling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that large language models frequently violate
  domain constraints when generating plans, even on simple tasks like gridworld navigation.
  To address this, it proposes Localized In-Context Learning (L-ICL), which identifies
  the first constraint violation in a reasoning trace and injects a minimal, targeted
  correction for that specific failing step.
---

# Localizing and Correcting Errors for LLM-based Planners

## Quick Facts
- arXiv ID: 2602.00276
- Source URL: https://arxiv.org/abs/2602.00276
- Authors: Aditya Kumar; William W. Cohen
- Reference count: 40
- One-line result: L-ICL achieves 89% valid plans on 8x8 gridworld vs 59% baseline with only 60 training examples

## Executive Summary
This paper addresses a fundamental challenge in LLM-based planning: models frequently violate domain constraints when generating plans, even on simple tasks. The authors propose Localized In-Context Learning (L-ICL), which identifies the first constraint violation in a reasoning trace and injects a minimal, targeted correction for that specific failing step. L-ICL dramatically improves plan validity across multiple domains (gridworld navigation, mazes, Sokoban, BlocksWorld) and is more sample-efficient than retrieval-based ICL approaches, requiring less context to achieve higher performance.

## Method Summary
L-ICL extends Program Trace Prompting by adding localized corrections to subroutine documentation. When the LLM generates a plan, the system identifies the first constraint violation (e.g., attempting an invalid move), queries an oracle for the correct output at that step, and formats this as a doctest-style input-output example. This correction is then added to the relevant subroutine documentation. The approach leverages the cascade effect of planning errors—fixing the first violation often resolves downstream errors—and exploits the prevalence of doctest formatting in LLM pre-training data to efficiently convey constraint information.

## Key Results
- L-ICL produces valid plans 89% of the time on 8x8 gridworld vs 59% for best baseline (30% relative improvement)
- Requires only 60 training examples to achieve this performance
- Outperforms RAG-ICL by 40+ percentage points at matched context sizes
- Matches or exceeds baselines across multiple domains including mazes, Sokoban, and BlocksWorld

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Localized corrections convey more constraint information per token than complete solution trajectories.
- **Mechanism:** Full trajectories demonstrate *that* a plan works but leave *why* individual steps are valid implicit. A single doctest-style example directly encodes constraint violations that the LLM can generalize.
- **Evidence:** L-ICL matches RAG-ICL's 16% success with 5,000 characters and reaches 63% with 7,000 characters—outperforming RAG-ICL by 40+ percentage points at matched context size.
- **Break condition:** If constraints are highly contextual (depend on full problem state rather than local features), single-step examples may fail to transfer.

### Mechanism 2
- **Claim:** Focusing on the first constraint violation addresses multiple downstream errors simultaneously.
- **Mechanism:** Planning errors cascade—an invalid move at step k renders all subsequent state representations incorrect. Correcting the root cause propagates forward.
- **Evidence:** Visual evidence shows m=0 rollouts ignore walls entirely while m=60 produces coherent paths, demonstrating cascade effects.
- **Break condition:** If failures are caused by independent reasoning gaps, correcting one won't fix others.

### Mechanism 3
- **Claim:** Doctest-style formatting leverages patterns well-represented in LLM pre-training data.
- **Mechanism:** Python doctest syntax is ubiquitous in code documentation, so code-trained LLMs readily interpret these as behavioral specifications.
- **Evidence:** The format is described as "well-represented in LLM training data, facilitating generalization."
- **Break condition:** For non-code-specialized LLMs or domains without clear input-output structure, formatting benefits may diminish.

## Foundational Learning

- **Program Trace Prompting (PTP)**
  - *Why needed:* L-ICL builds on PTP, which provides subroutine documentation without implementations. Understanding this distinction—documented interfaces vs. executable code—is essential for grasping where corrections insert.
  - *Quick check:* Can you explain why PTP uses `...` placeholders instead of actual function bodies?

- **Oracle-based verification**
  - *Why needed:* L-ICL requires a symbolic verifier during training to identify failures and provide correct outputs. This differs from test-time oracle use.
  - *Quick check:* What's the difference between training-time and inference-time oracle dependencies in L-ICL vs. ReAct?

- **Valid vs. successful vs. optimal plans**
  - *Why needed:* The paper decomposes planning into constraint satisfaction (valid), goal-reaching (successful), and efficiency (optimal). L-ICL primarily addresses validity.
  - *Quick check:* On Full Sokoban, L-ICL achieves 46% valid but only 20% success. What does this gap indicate?

## Architecture Onboarding

- **Component map:** Partial Program Generator -> LLM Interface -> Evaluation Engine -> Correction Accumulator -> Oracle
- **Critical path:** Generate trace → Parse subroutine calls → Identify first failure → Query oracle for correct output → Format as doctest → Insert into subroutine documentation → Repeat on next training problem
- **Design tradeoffs:**
  - First-failure vs. all-failures: Paper uses first-failure only (Algorithm 1 breaks after first mismatch). Trade-off: faster iteration vs. potentially missing independent error patterns.
  - ASCII grid vs. pure corrections: Grids accelerate early learning but don't change peak performance. Trade-off: prompt size vs. convergence speed.
  - Batch vs. online accumulation: Paper batches corrections after every 10 training examples. Trade-off: implementation simplicity vs. fine-grained adaptation.
- **Failure signatures:**
  - High valid-to-success gap (e.g., Sokoban 46% → 20%): L-ICL learned constraints but not strategic reasoning; consider combining with search/value functions.
  - No improvement after 30-60 examples: Domain may lack transferable constraint patterns, or oracle may be noisy.
  - OOD performance drop (e.g., 10×10 → 15×15 transfer): Larger instances contain unseen configurations; train on mixed sizes.
- **First 3 experiments:**
  1. Replicate 8×8 gridworld baseline comparison: Implement zero-shot, RAG-ICL (10k/20k chars), and L-ICL (m=60). Verify the 59% → 89% improvement curve from Figure 2.
  2. Ablate correction localization: Compare L-ICL (first-failure only) vs. all-failures correction on 10×10 maze. Measure sample efficiency and final performance.
  3. Test cross-domain transfer: Train corrections on 8×8 gridworld, evaluate on 10×10 maze. Assess whether basic boundary/wall constraints transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can localized corrections or their extensions address strategic planning failures that require multi-step lookahead, or must L-ICL be combined with complementary approaches like search or value functions?
- **Basis:** Section 5.3 states: "We leave to future work the question of whether localized corrections, or some extension of them, can also correct strategic failures, which seem to require multi-step lookahead..."
- **Why unresolved:** L-ICL teaches constraint satisfaction effectively, but the valid-to-success gap in Sokoban (46% valid, 20% successful) shows strategic reasoning remains unsolved.
- **What evidence would resolve it:** Demonstrating improved success rates on trap-heavy domains through modified corrections encoding lookahead information, or showing that hybrid L-ICL + search architectures close the valid-to-success gap.

### Open Question 2
- **Question:** Does L-ICL transfer to open-ended natural language tasks without formal specifications?
- **Basis:** Section 5.3 notes: "A third limitation of this paper is that we consider only formally-describable planning benchmarks from the LLM planning literature. Transfer to open-ended natural-language tasks is not studied."
- **Why unresolved:** L-ICL relies on verifiable subroutines and oracle feedback; natural language tasks lack formal constraint definitions and reliable automatic verifiers.
- **What evidence would resolve it:** Successful application of L-ICL to natural language planning tasks (e.g., task decomposition, instruction following) using learned verifiers or LLM-based oracles.

### Open Question 3
- **Question:** Can L-ICL work effectively with weaker supervision such as learned verifiers that may introduce noise?
- **Basis:** Section 5.3 states: "Extending to domains without formal specifications may require weaker supervision (learned verifiers, stronger models) that could introduce noise."
- **Why unresolved:** The current method assumes perfect oracle outputs; noisy corrections could mislead the model or require filtering mechanisms not yet explored.
- **What evidence would resolve it:** Experiments comparing L-ICL performance with perfect vs. noisy oracles across varying noise levels, potentially identifying error tolerance thresholds.

## Limitations
- Generalization scope: L-ICL excels at gridworld navigation and Sokoban but effectiveness on domains requiring complex temporal reasoning remains untested
- Oracle dependency: Requires oracle access during training, creating a fundamental gap between training and deployment scenarios
- Cascade assumption validity: Assumes fixing first violation cascades to resolve downstream errors, which may not hold in domains with independent failures

## Confidence
- **High confidence:** L-ICL demonstrably improves plan validity on tested domains compared to baselines. The 30% relative improvement on 8x8 gridworld (89% vs 59% valid plans) is well-supported by experimental results.
- **Medium confidence:** The claim about L-ICL being more sample-efficient than retrieval-based ICL approaches is supported by comparisons but could benefit from testing across more diverse domains and larger scale problems.
- **Medium confidence:** The assumption that doctest formatting provides unique benefits due to pre-training data exposure is plausible but lacks direct validation against alternative correction formats.

## Next Checks
1. **Test cascade hypothesis explicitly:** Create datasets where errors are either causally linked (single root cause) or independently distributed. Compare L-ICL's performance against a variant that corrects all failures to quantify the true benefit of first-failure focus.

2. **Evaluate without oracle at training time:** Implement a variant where the oracle is only used during training data generation (not during plan generation) and assess whether the learned corrections remain effective. This would better simulate real-world deployment scenarios.

3. **Test on temporal logic planning domains:** Apply L-ICL to domains from the T3 Planner paper (e.g., robotic motion planning with temporal logic constraints) to assess whether the method transfers to non-spatial, temporal reasoning tasks where constraint violations may be more complex than single-step errors.