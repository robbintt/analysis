---
ver: rpa2
title: 'OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for
  Global Sea Surface Temperature Prediction'
arxiv_id: '2508.00933'
source_url: https://arxiv.org/abs/2508.00933
tags:
- knowledge
- prediction
- ocean
- data
- okg-llm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sea surface temperature (SST) prediction by
  integrating domain-specific ocean knowledge with observational data using large
  language models (LLMs). It introduces OKG-LLM, which first constructs an Ocean Knowledge
  Graph (OKG) representing ocean currents, climatic zones, and regional relationships,
  then learns embeddings via a graph network.
---

# OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction

## Quick Facts
- arXiv ID: 2508.00933
- Source URL: https://arxiv.org/abs/2508.00933
- Reference count: 40
- Key result: Up to 15.1% lower MSE and 7.5% lower MAE than nine baselines for global SST prediction

## Executive Summary
OKG-LLM integrates domain-specific ocean knowledge with observational data for sea surface temperature (SST) prediction by constructing an Ocean Knowledge Graph (OKG) representing ocean currents, climatic zones, and regional relationships. The method learns graph embeddings via TransE, aligns them with fine-grained SST data through cross-attention, and processes the combined representation with a frozen LLM. Experiments on global SST data demonstrate consistent improvements over nine baselines, with ablation studies confirming the critical role of knowledge graph encoding and fine-grained alignment.

## Method Summary
OKG-LLM predicts SST by first constructing an Ocean Knowledge Graph encoding oceanographic entities and relationships, then learning structural embeddings via TransE. Time series embeddings are generated from SST observations using RevIN normalization and patching, while knowledge embeddings are obtained from k-hop retrieval and LLM tokenization. Cross-attention aligns these representations at the region level, and a frozen LLM processes the combined embeddings with a trainable decoder to produce predictions. The framework achieves improved accuracy by injecting structured domain knowledge into numerical SST data processing.

## Key Results
- Up to 15.1% lower MSE and 7.5% lower MAE compared to nine baselines (Informer, FEDformer, TimeLLM, etc.)
- GPT-2 backbone achieves best performance with smallest model (40.15M params) and fastest inference
- Ablation studies show knowledge graph encoding is most critical component
- Visualizations confirm more accurate and semantically meaningful predictions than traditional and LLM-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Structured Domain Knowledge Encoding via Knowledge Graphs
The OKG captures oceanographic relationships as structured triples, with TransE embeddings representing entities in vector space where h + r ≈ t. k-hop retrieval verbalizes local graph neighborhoods into text for LLM token embeddings, fused via adapter to create unified knowledge representations. This provides structural priors beyond raw numerical data, encoding relationships between regions, currents, and climatic zones that influence SST dynamics.

### Mechanism 2: Fine-Grained Knowledge-Data Alignment via Cross-Attention
Cross-attention resolves granularity mismatch by querying region-specific knowledge embeddings with temporal SST embeddings, using the broader OKG as Key/Value. This enables selective knowledge infusion at the region level rather than globally, ensuring each area receives contextually relevant information based on its local oceanographic characteristics.

### Mechanism 3: LLM as Frozen Pattern Processor with Trainable Decoder
A frozen pre-trained LLM extracts high-level temporal patterns from aligned embeddings, with a lightweight trainable decoder sufficient for SST-specific refinement. This leverages the LLM's transferable sequence modeling capabilities while maintaining computational efficiency through parameter freezing, with the decoder adapting predictions to the SST domain.

## Foundational Learning

- **Knowledge Graph Embeddings (TransE)** - Why needed: Projects entities and relations into shared embedding space capturing translational structure (h + r ≈ t). Quick check: Given triples (Pacific_Ocean, contains, Equatorial_Current) and (Equatorial_Current, influences, SST_Pattern_A), explain how TransE would position these entities in vector space?
- **Cross-Attention Mechanism** - Why needed: Enables selective knowledge infusion by querying knowledge embeddings with region-specific temporal embeddings. Quick check: In alignment module, what serves as Query, Key, and Value, and why is this design preferable to simple concatenation?
- **Reversible Instance Normalization (RevIN)** - Why needed: Addresses distribution shift in SST data before patching. Quick check: Why is reversible normalization important for time series forecasting compared to standard normalization?

## Architecture Onboarding

- **Component map:** OKG construction → TransE embedding → k-hop retrieval + LLM tokenization → adapter fusion → time series encoding (RevIN → patching → MLP) → cross-attention alignment → frozen LLM → trainable decoder → prediction
- **Critical path:** OKG construction quality → TransE embedding training → k-hop retrieval relevance → cross-attention alignment effectiveness → LLM backbone selection → decoder refinement. Ablation study identifies KG Encoding as most critical (largest degradation when removed).
- **Design tradeoffs:** GPT-2 backbone offers best performance/smallest size; Llama2-7b shows competitive but slightly worse MSE; freezing LLM reduces trainable parameters but limits adaptation; k-hop retrieval uses 1-hop for efficiency vs. deeper hops for context.
- **Failure signatures:** High error in ENSO region suggests missing entities/relations; performance degradation at τ=32 indicates knowledge may not fully compensate for long-range uncertainty; clustering failure in embeddings shows time-series-only models fail semantic learning.
- **First 3 experiments:** 1) Baseline comparison on global SST (τ=8,16,32) - expect MSE < 0.11 at τ=8; 2) Ablation study - expect KG Encoding removal causes largest MAE/MSE increase; 3) LLM backbone swap - expect GPT-2 remains best or highly competitive.

## Open Questions the Paper Calls Out
- How does incorporating complex environmental factors like air-sea interactions and carbon cycling into OKG affect long-term SST prediction accuracy?
- Does the static knowledge graph topology limit generalization during extreme climatic events where spatial boundaries shift?
- Can the fine-grained alignment mechanism transfer to other oceanographic variables with different temporal granularities and noise profiles?

## Limitations
- Knowledge graph construction quality and completeness not validated against oceanographic standards
- Granularity mismatch resolution effectiveness not quantified or validated
- LLM backbone performance may be dataset-specific with potential performance ceilings

## Confidence

**High Confidence (Mechanistic Validation)** - Core architecture components are technically sound with ablation study validation

**Medium Confidence (Performance Claims)** - Performance improvements demonstrated but lack statistical significance testing or confidence intervals

**Low Confidence (Generalizability)** - Claims about cross-backbone superiority and semantic predictions rely on internal comparisons without external validation

## Next Checks
1. Perform paired t-tests or bootstrap confidence intervals on MSE/MAE across all baselines and OKG-LLM variants
2. Test OKG-LLM on independent SST datasets (satellite-derived, regional ocean observing systems)
3. Evaluate OKG-LLM's ability to predict known El Niño and La Niña events, comparing false positive/negative rates against baselines