---
ver: rpa2
title: 'From Codicology to Code: A Comparative Study of Transformer and YOLO-based
  Detectors for Layout Analysis in Historical Documents'
arxiv_id: '2506.20326'
source_url: https://arxiv.org/abs/2506.20326
tags:
- yolo
- dataset
- page
- initial
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks five state-of-the-art object detection models
  on three diverse historical manuscript datasets to address the challenge of Document
  Layout Analysis (DLA) for manuscripts with complex, non-Cartesian layouts. It compares
  two Transformer-based models (Co-DETR, Grounding DINO) against three YOLO variants
  (AABB, OBB, and YOLO-World) using oriented bounding boxes to better model skewed
  and curved elements.
---

# From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents

## Quick Facts
- arXiv ID: 2506.20326
- Source URL: https://arxiv.org/abs/2506.20326
- Authors: Sergio Torres Aguilar
- Reference count: 31
- Benchmark of 5 state-of-the-art object detection models on 3 diverse historical manuscript datasets shows YOLOv11x-OBB excels on visually complex data while Co-DETR performs best on structured layouts.

## Executive Summary
This study addresses the challenge of Document Layout Analysis (DLA) for historical manuscripts by benchmarking five object detection models across three diverse datasets. The research compares two Transformer-based models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and YOLO-World) using oriented bounding boxes to better model skewed and curved elements. Results demonstrate that model performance depends heavily on dataset characteristics: Co-DETR excels on structured layouts (0.752 mAP on e-NDP), while YOLOv11x-OBB outperforms on more diverse and visually complex datasets (0.564 mAP on CATMuS, 0.568 on HORAE). The study conclusively demonstrates that Oriented Bounding Boxes are essential for accurate DLA in historical documents and proposes a hierarchical ontology-based method to train a generalist foundational model, YOLO-gen, that performs nearly as well as specialized models across all datasets.

## Method Summary
The study benchmarks five state-of-the-art object detection models on three historical manuscript datasets: e-NDP, CATMuS, and HORAE. Ground truth polygons are converted to minimum-area rotated rectangles for OBB training. Models are trained using standard hyperparameters (Cosine LR scheduler, min 100 epochs, 640x640 image size) with MMDetection for Transformers and Ultralytics for YOLO variants. Performance is evaluated using mAP@.50:.95 and mAP@.50 metrics. A hierarchical ontology approach is proposed to train YOLO-gen, a generalist foundational model capable of cross-domain inference across different manuscript collections.

## Key Results
- YOLOv11x-OBB achieves highest overall performance across diverse datasets (0.564 mAP on CATMuS, 0.568 on HORAE)
- Co-DETR excels on structured layouts (0.752 mAP on e-NDP) but underperforms on complex, visually diverse documents
- Oriented Bounding Boxes are essential for accurate DLA in historical documents, particularly for skewed and curved elements
- YOLO-gen, trained with hierarchical ontology, performs nearly as well as specialized models across all datasets

## Why This Works (Mechanism)
The success of OBB-enhanced models stems from their ability to accurately capture the geometric complexity of historical manuscripts. Traditional AABB approaches create large, overlapping boxes that include irrelevant background when dealing with rotated or curved elements. The CNN-OBB architecture combines local feature extraction with orientation-aware regression, allowing precise localization of text blocks regardless of their angle. Transformer models leverage global attention mechanisms to understand layout relationships but struggle with the geometric precision needed for non-Cartesian layouts. The hierarchical ontology approach enables knowledge transfer across datasets by mapping diverse class labels to unified parent categories, creating a more robust foundational model.

## Foundational Learning

- **Concept:** **Oriented Bounding Box (OBB) vs. Axis-Aligned Bounding Box (AABB)**
    - **Why needed here:** This is the paper's central technical claim. Understanding that standard horizontal/vertical boxes are insufficient for warped, skewed, and organic shapes in historical manuscripts is prerequisite to grasping why OBB-enhanced YOLO models succeed.
    - **Quick check question:** If a text block is rotated 15 degrees, will a standard axis-aligned box tightly fit the text, or will it include large areas of surrounding background?

- **Concept:** **Inductive Bias in CNNs vs. Transformers**
    - **Why needed here:** The paper's explanation for performance contingency rests on this core machine learning principle. CNNs have built-in preference for local spatial patterns (via convolution kernels), while Transformers rely on global self-attention to understand relationships across an entire image.
    - **Quick check question:** Which architectural component allows a model to directly relate a pixel in the top-left corner of an image to a pixel in the bottom-right corner in a single step?

- **Concept:** **Codicology and Manuscript "Mise-en-page"**
    - **Why needed here:** The core domain context. "Mise-en-page" refers to the layout of a page. The paper argues that a document's physical creation and subsequent degradation (the "codicological reality") dictate its layout complexity, which in turn determines the optimal model architecture.
    - **Quick check question:** Why would a 14th-century administrative register typically have a more "structured" layout than a richly illustrated Book of Hours?

## Architecture Onboarding

- **Component map:**
    - **Input:** 640x640 image, annotation format (COCO AABB or YOLO OBB)
    - **Backbone (Feature Extractor):**
        - *CNN (YOLO family):* `C3k2` and `C2PSA` blocks for hierarchical local feature maps
        - *Transformer (Co-DETR, Grounding DINO):* `Swin Transformer` or `ResNet` + `BERT` for global attention and text-image alignment
    - **Neck/Head:**
        - *YOLOv11x-OBB:* Adds specialized regression branch to predict 5 values (x, y, w, h, θ) for rotated boxes
        - *Co-DETR:* End-to-end set prediction with bipartite matching loss, eliminating need for Non-Maximum Suppression (NMS)
    - **Output:** For each detected object: class label, confidence score, and bounding box (AABB: x,y,w,h; OBB: x,y,w,h,θ)

- **Critical path:**
    1. **Data Preparation:** Convert polygon-based ground truth (e.g., PAGE XML) into minimum-area rotated rectangles (OBB) for YOLO-OBB. This step is non-negotiable for the paper's best-performing model.
    2. **Model Selection:** Assess your manuscript corpus. If highly structured and homogeneous (like e-NDP), a Transformer like Co-DETR may be best. If visually diverse and complex (like HORAE), a CNN-OBB model like YOLOv11x-OBB is the recommended choice.
    3. **Ontology Harmonization (for multi-corpus projects):** Map disparate class labels from different datasets to a unified hierarchical schema (e.g., `Initial -> Initial_Manuscript -> Initial_Ms_Simple`). This enables training a single foundational model (YOLO-gen) capable of cross-domain inference.

- **Design tradeoffs:**
    - **Accuracy vs. Speed:** Transformer models (Co-DETR: ~147 ms/im) offer potentially higher accuracy on structured data but are ~20x slower than YOLO models (~7-11 ms/im). This is critical for large-scale digital libraries.
    - **Specialization vs. Generalization:** Training a specialist model per corpus yields highest peak performance for that data. Training a single generalist (YOLO-gen) offers broader applicability and robustness across corpora but with slight performance trade-off.
    - **AABB vs. OBB:** OBB adds computational overhead and complexity to annotation and training pipeline. This is justified for historical documents but may be unnecessary for modern, grid-based layouts.

- **Failure signatures:**
    - **AABB on Historical Docs:** Model trained with AABB on skewed manuscripts will show low precision due to large, overlapping predicted boxes that include irrelevant background.
    - **Transformer on High-Variance Data:** Transformer model (e.g., Co-DETR) fine-tuned on small, highly diverse dataset may show signs of overfitting or poor generalization, underperforming more robust CNN model.
    - **Class Imbalance:** On all models, performance on "long-tail" classes (e.g., `Historiated Initial` with only 1 instance) will be near zero. This is key limitation noted in paper.

- **First 3 experiments:**
    1. **OBB Benefit:** Take subset of your historical data. Train two YOLOv11 models from scratch: one with standard AABB annotations and one with OBB. Compare their mAP@.50:.95 to quantify benefit of oriented boxes.
    2. **Architecture Ablation:** On your most structured dataset, fine-tune both a YOLOv11x-OBB and a Co-DETR model. Compare their final mAP and inference speed to identify best speed/accuracy trade-off for that specific layout type.
    3. **Foundational Model Test:** Merge data from two distinct historical corpora using simple unified ontology. Train a YOLO-OBB model (like YOLO-gen) on this combined data and evaluate its ability to detect unified "parent" classes on held-out test set from third corpus, if available.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating orientation-aware detection heads into Vision-Language Models (VLMs) close the performance gap with specialized CNN-OBB models for historical Document Layout Analysis?
- Basis in paper: [explicit] The authors conclude that future work should focus on "bridging this gap by integrating orientation-aware heads into VLMs."
- Why unresolved: VLMs currently underperform specialized models in closed-set detection tasks, lacking the geometric precision of OBBs necessary for non-Cartesian layouts.
- What evidence would resolve it: Development and benchmarking of an OBB-capable VLM that matches or exceeds YOLOv11x-OBB performance on complex datasets like HORAE.

### Open Question 2
- Question: What specific training strategies (e.g., cost-sensitive learning) can effectively mitigate the model bias caused by severe class imbalance in codicological datasets?
- Basis in paper: [explicit] The discussion highlights "severe class imbalance" as a limitation that "biases models towards common classes" and suggests exploring mitigation strategies.
- Why unresolved: Current models struggle with "long-tail" classes (e.g., TitlePageZone) which have few instances but high scholarly importance.
- What evidence would resolve it: Improved mAP scores on rare categories following application of few-shot learning or targeted data augmentation.

### Open Question 3
- Question: Does extending evaluation from bounding boxes (Box mAP) to instance segmentation (Mask mAP) significantly alter the architectural suitability findings for irregularly shaped decorative elements?
- Basis in paper: [explicit] The authors state that "extending the analysis to segmentation quality (Mask mAP) would provide deeper insights" for irregularly shaped elements.
- Why unresolved: Bounding boxes, even oriented ones, may still approximate complex organic shapes (e.g., vine-stem borders) insufficiently.
- What evidence would resolve it: Comparative benchmarks of instance segmentation models (e.g., Mask R-CNN) against OBB detectors using Mask mAP on HORAE dataset.

## Limitations
- Evaluation constrained to three specific datasets, which may not represent full diversity of historical document layouts across different periods, regions, or genres.
- Study does not explore more complex polygonal annotations that might better capture highly irregular shapes beyond oriented bounding boxes.
- Performance may vary significantly on manuscripts not represented in the e-NDP, CATMuS, and HORAE corpora.

## Confidence

| Claim | Confidence |
|-------|------------|
| OBB > AABB for historical documents | High |
| CNN-OBB best for diverse data, Transformer for structured data | Medium |
| Foundational model approach effectiveness | Medium-Low |

## Next Checks
1. **Cross-Period Validation:** Test recommended models on manuscripts from different historical periods (e.g., Carolingian vs. Gothic) to verify architectural recommendations hold across temporal variations.
2. **Annotation Granularity Study:** Compare OBB performance against polygonal annotations on subset of documents to quantify trade-off between annotation complexity and detection accuracy.
3. **Real-World Deployment Benchmark:** Evaluate models' performance on live digitization project's documents, measuring not just mAP but also practical metrics like annotation time and user correction rates.