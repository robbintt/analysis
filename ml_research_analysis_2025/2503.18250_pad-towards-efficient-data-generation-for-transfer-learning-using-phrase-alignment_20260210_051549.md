---
ver: rpa2
title: 'PAD: Towards Efficient Data Generation for Transfer Learning Using Phrase
  Alignment'
arxiv_id: '2503.18250'
source_url: https://arxiv.org/abs/2503.18250
tags:
- data
- korean
- english
- language
- mgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Phrase Aligned Data (PAD), a novel data processing
  method that leverages phrase alignment from Statistical Machine Translation (SMT)
  to enhance transfer learning efficiency for Korean language models. PAD transforms
  English data into Korean by replacing phrases with their Korean equivalents while
  maintaining the English word order, creating syntactically flexible training data
  that exploits Korean's case marker system.
---

# PAD: Towards Efficient Data Generation for Transfer Learning Using Phrase Alignment

## Quick Facts
- arXiv ID: 2503.18250
- Source URL: https://arxiv.org/abs/2503.18250
- Reference count: 22
- Primary result: PAD consistently outperforms English-only training and achieves performance close to high-quality Korean datasets or GPT-4 translations across SA, NLI, and STS tasks.

## Executive Summary
This paper proposes PAD (Phrase Aligned Data), a novel data processing method that leverages phrase alignment from Statistical Machine Translation (SMT) to enhance transfer learning efficiency for Korean language models. PAD transforms English data into Korean by replacing phrases with their Korean equivalents while maintaining the English word order, creating syntactically flexible training data that exploits Korean's case marker system. Experiments with multiple benchmark tasks across three model architectures demonstrate that PAD consistently outperforms English-only training and achieves performance close to high-quality Korean datasets or GPT-4 translations, while requiring minimal resources and time.

## Method Summary
PAD uses phrase alignment from Statistical Machine Translation to convert English data into Korean by replacing English phrases with Korean equivalents while preserving English word order. The method employs Moses SMT with IBM Model 4 for phrase alignment but explicitly disables the reordering stage that typically adjusts sentence structure between languages. This produces Korean phrases in English S-V-O order, which Korean's case marker system can interpret correctly. The method requires only a parallel corpus and CPU processing, avoiding the GPU-intensive fine-tuning required for neural machine translation approaches.

## Key Results
- PAD outperforms English-only training with F1 scores of 0.770 in SA, 0.681 in NLI, and Pearson's r of 0.844 in STS
- Performance approaches high-quality Korean datasets and GPT-4 translations (within 2-3% F1)
- Models trained on PAD data show consistent improvements across mGPT, koGPT, and mT5 architectures
- PAD requires minimal resources and time compared to neural machine translation alternatives

## Why This Works (Mechanism)

### Mechanism 1: Target-Language Expression Boosts Transfer Learning
Training data expressed in the target language (Korean) is more effective than raw English data for Korean tasks, even when conveying the same meaning. PAD replaces English phrases with Korean equivalents via phrase alignment tables, producing target-language tokens that better align with the embedding space of Korean-focused models. This assumes models trained or fine-tuned for Korean have learned representations that activate more strongly for Korean tokens than English tokens.

### Mechanism 2: Case Markers Enable Word-Order Tolerance
Korean's case marker system allows PAD to preserve English word order while remaining useful for training. Korean indicates grammatical roles (subject, object) via particles (이/가, 을/를) rather than strict position. PAD produces S-V-O Korean sentences (English order) instead of natural S-O-V, but case markers preserve semantic role interpretation. This assumes the model has learned to rely on case markers for role assignment during pre-training.

### Mechanism 3: SMT Alignment Without Reordering Avoids Costly Failure Mode
Skipping SMT's reordering stage improves efficiency and stability while maintaining comparable quality for Korean. Standard SMT has two stages—alignment (phrase mapping) and reordering (structural adjustment). Reordering is error-prone for structurally distant language pairs (EN-KO) and requires GPU-intensive language models. PAD stops after alignment, outputting Korean phrases in English order.

## Foundational Learning

- **Statistical Machine Translation (SMT) Phrase Alignment**: Why needed: PAD's core operation is the phrase alignment step from SMT, not neural translation. Quick check: Can you explain why IBM Model 4 produces many-to-many word alignments rather than one-to-one?

- **Case Markers in Korean Morphology**: Why needed: Understanding why Korean tolerates word-order violations is essential to accept that PAD's S-V-O output remains useful. Quick check: In "나는 사과를 먹었다" vs "사과를 나는 먹었다," what indicates the subject and object in both sentences?

- **Cross-Lingual Transfer Learning in Transformers**: Why needed: PAD is evaluated on multilingual models (mGPT, mT5) and monolingual Korean models; understanding how multilingual embeddings affect transfer is critical. Quick check: Why might a multilingual model fine-tuned on English data underperform on Korean test sets compared to Korean-native fine-tuning?

## Architecture Onboarding

- **Component map**: Parallel Corpus (1.7M EN-KO) -> Moses SMT Pipeline (GIZA++ alignment only) -> PAD Generator (English order, Korean phrases) -> Target Model (mGPT/koGPT/mT5) -> Evaluation Benchmarks (SA, NLI, STS)

- **Critical path**: 
  1. Acquire/prepare parallel corpus (EN-KO, verified quality)
  2. Train Moses phrase alignment model (IBM Model 4, word-level Korean tokenization)
  3. Run English source data through alignment to generate PAD
  4. Fine-tune target model on PAD with standard hyperparameters (lr=5e-5, AdamW, 3-epoch warmup)
  5. Evaluate on Korean test sets

- **Design tradeoffs**:
  - PAD vs. NMT (GPT-4): PAD is CPU-only, ~0 API cost; NMT-GPT4 gives slightly higher quality (2-3% F1) but incurs API fees
  - PAD vs. Local NMT (mT5/mGPT fine-tuned): PAD avoids 70+ GPU-hours for NMT fine-tuning; performance comparable or slightly better
  - PAD vs. Native Korean data: Native data remains best; PAD is a bootstrap/supplement when native data is scarce

- **Failure signatures**:
  - Mixed Korean-English output: When PAD encounters unknown phrases, SMT outputs source language
  - mT5 training instability: Paper reports mT5 produced single-label outputs
  - No improvement over English baseline: Likely indicates model has weak Korean pre-training

- **First 3 experiments**:
  1. Sanity check: Generate PAD from 1000 English examples, verify phrase alignment quality manually
  2. Baseline comparison: Fine-tune mGPT on 4K English vs. 4K PAD data; expect 3-5% F1 improvement on SA task
  3. Scaling test: Compare PAD at 4K/8K/12K against NMT-GPT4 at same sizes; expect PAD to close to within 2-3% of GPT-4 translation quality

## Open Questions the Paper Calls Out

- Can PAD be generalized to languages with rigid word orders that lack the syntactic flexibility provided by Korean case markers?
- How does sentence structural complexity impact the quality and transfer learning efficacy of PAD?
- What is the optimal curriculum for fading out PAD in favor of native data as high-quality resources become available?

## Limitations

- Parallel corpus quality and coverage significantly impact PAD performance, with domain-specific biases potentially limiting generalizability
- SMT model configuration requires precise control to disable reordering, which is not fully specified in the paper
- Evaluation dataset transparency issues regarding domain alignment between training PAD data and test sets

## Confidence

- **High Confidence**: PAD outperforms English-only training and achieves performance close to high-quality Korean datasets or GPT-4 translations across multiple tasks and model architectures
- **Medium Confidence**: The mechanism that Korean case markers enable PAD's word-order tolerance works as claimed
- **Low Confidence**: PAD's efficiency advantages are fully realized in practice (resource requirements not quantified)

## Next Checks

1. Verify SMT reordering disablement by running controlled experiments with explicit configuration flags and testing on known parallel sentences
2. Test case marker dependency by training models on natural Korean data with shuffled word order (preserving vs. removing case markers)
3. Quantify actual resource requirements for Moses SMT training on 1.7M parallel corpus and PAD generation for 20K examples