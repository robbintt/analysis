---
ver: rpa2
title: 'Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning'
arxiv_id: '2506.11706'
source_url: https://arxiv.org/abs/2506.11706
tags:
- network
- learning
- networks
- training
- grownn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training large neural networks
  in deep reinforcement learning, where current approaches struggle to scale up network
  complexity without compromising trainability. The authors propose GrowNN, a method
  that incrementally grows neural networks during training using network morphisms,
  specifically Net2Net transformations.
---

# Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.11706
- Source URL: https://arxiv.org/abs/2506.11706
- Reference count: 4
- Primary result: GrowNN improves deep RL performance by incrementally growing networks, achieving 48% improvement on MiniHack Room and 72% relative improvement on MuJoCo Ant

## Executive Summary
This paper addresses the challenge of training large neural networks in deep reinforcement learning, where current approaches struggle to scale up network complexity without compromising trainability. The authors propose GrowNN, a method that incrementally grows neural networks during training using network morphisms, specifically Net2Net transformations. The approach starts with a small network to learn initial policies, then progressively adds layers without altering the encoded function, allowing subsequent updates to utilize the added capacity as policy complexity increases. GrowNN can be seamlessly integrated into existing RL agents and is algorithm-agnostic.

## Method Summary
GrowNN implements incremental network growth through Net2DeeperNet transformations during RL training. The method starts with a small feature extractor network and progressively adds identity-mapped layers at predetermined interaction counts (fidelities). Each growth operation inserts new layers with identity initialization—where new neurons receive outgoing weights of 1 for the same-position neuron and 0 for all others—preserving the existing function while expanding capacity. The approach is integrated into existing RL algorithms like PPO without modification, making it algorithm-agnostic. Growth points are scheduled statically based on environment interactions rather than adaptive criteria.

## Key Results
- MiniHack Room: GrowNN-deeper networks achieved solution rates over 50% compared to 0% for static networks of the same size, representing a 48% improvement
- MuJoCo Ant: GrowNN networks showed a 72% relative improvement over static counterparts, with agents learning to actively move forward compared to baselines that only avoided death states
- The method enables larger networks to solve previously impossible tasks without algorithmic changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Network morphisms preserve learned policy while expanding capacity.
- Mechanism: Net2DeeperNet inserts identity-mapped layers where new neurons receive outgoing weights of 1 for the same-position neuron and 0 for all others, ensuring θ(s) = f(θ)(s) ∀s ∈ S. The added layer has no bias and requires idempotent activations (e.g., ReLU).
- Core assumption: The RL optimizer can utilize newly added parameters after the identity initialization degrades through gradient updates.
- Evidence anchors: [abstract] "add layers without changing the encoded function"; [section 3] "Net2DeeperNet... the added layer needs to contain no bias, and the activation function needs to be idempotent"
- Break condition: If gradient updates do not meaningfully move the new layer parameters away from identity within a reasonable window, the added capacity is structurally dormant.

### Mechanism 2
- Claim: Smaller initial networks improve early-phase trainability in RL.
- Mechanism: By starting with fewer layers, gradients have shorter paths and lower variance during early policy formation. Capacity is added only after foundational skills (e.g., torque control, stabilization) are acquired.
- Core assumption: Early training benefits more from trainability than from expressiveness, and later training requires more capacity to refine complex policies.
- Evidence anchors: [abstract] "maintaining network trainability, we propose GrowNN"; [section 3] "a large network could hinder the initial training process... small networks may learn the simpler skills easily"
- Break condition: If optimal policy complexity is achievable with the initial small network, growth adds unnecessary overhead.

### Mechanism 3
- Claim: Evenly spaced growth points provide a practical schedule without requiring adaptive heuristics.
- Mechanism: Growth is triggered at fixed environment interaction counts (fidelities), not based on loss plateaus or gradient statistics. This keeps the method simple and algorithm-agnostic.
- Core assumption: A static schedule approximates the right capacity-to-progress ratio well enough for common RL benchmarks.
- Evidence anchors: [section 3] "we utilize a static fidelity schedule"; [section 4] "Dotted vertical lines show the evenly spaced growing points for the network"
- Break condition: If environments exhibit highly variable learning phase durations (e.g., sparse-reward tasks), static schedules may grow too early or too late.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: GrowNN is evaluated exclusively with PPO; understanding its clipping objective, advantage estimation, and update ratio constraints is prerequisite to interpreting the results.
  - Quick check question: Can you explain why PPO uses a clipped surrogate objective rather than direct policy gradient?

- Concept: Network Morphisms / Net2Net
  - Why needed here: The entire growth mechanism depends on Net2DeeperNet transformations; misunderstanding the identity initialization will lead to broken implementations.
  - Quick check question: What constraints must the activation function satisfy for Net2DeeperNet to preserve function exactly?

- Concept: Feature Extractors in RL
  - Why needed here: The paper grows the feature extractor, not the policy head. Distinguishing representation learning from policy optimization is essential.
  - Quick check question: In a CNN-based policy network, which layers are considered the feature extractor vs. the policy head?

## Architecture Onboarding

- Component map: Base RL algorithm (PPO) -> Policy/Value networks (MLP + optional CNN encoder) -> Net2DeeperNet transform -> Expanded network -> Continue training

- Critical path:
  1. Implement or integrate PPO with a configurable number of hidden layers
  2. Implement Net2DeeperNet: for each existing layer, insert a new layer with identity weight matrix (diagonal of 1s), zero bias, and ReLU
  3. Define growth schedule (e.g., every 500K environment interactions)
  4. At each growth point, apply transformation, then continue optimization without resetting optimizer state

- Design tradeoffs:
  - Growth frequency: More frequent growth adds capacity faster but may destabilize training if layers do not adapt quickly enough
  - Initial vs. final depth: Too small an initial network may fail to learn basic skills; too large a final network may still underfit if growth occurs too late
  - Where to grow: Paper grows feature extractor; growing the policy head is untested and may harm stability

- Failure signatures:
  - Performance drops immediately after growth: New layers remain near-identity and optimizer has not adapted; consider learning rate warmup for new parameters
  - No improvement over static baselines: Growth schedule may be misaligned with learning phases, or final depth may be insufficient
  - Training instability after multiple growths: Accumulated numerical issues or optimizer state mismatch; validate weight initialization and optimizer state handling

- First 3 experiments:
  1. Reproduce MiniHack Room with 1→2→4 layer growth schedule; compare to static 1, 2, and 4 layer baselines using same total environment steps
  2. Ablation on growth timing: Test early (25% of steps), mid (50%), and late (75%) single-growth schedules to assess sensitivity
  3. Transfer to a different algorithm (e.g., SAC or DQN) on a simple control task to verify algorithm-agnostic claim; monitor for optimizer state handling issues

## Open Questions the Paper Calls Out

- The authors explicitly state that more evaluation is necessary on other network architectures like CNNs, as their experiments primarily utilized MLPs with only a single CNN layer

## Limitations
- Static growth schedules may not transfer well to environments with highly variable learning dynamics
- Only depth-wise growth is explored; width-wise or hybrid approaches remain untested
- Algorithm-agnostic claims are untested beyond PPO

## Confidence
- MiniHack Room improvement (48%): High confidence
- MuJoCo Ant improvement (72%): High confidence
- Algorithm-agnostic claims: Medium confidence (only tested with PPO)
- Static schedule effectiveness: Medium confidence (no adaptive alternatives tested)
- CNN architecture applicability: Low confidence (not tested)

## Next Checks
1. Test adaptive growth scheduling based on validation performance or gradient statistics across multiple environments
2. Implement and evaluate Net2WiderNet transformations alongside Net2DeeperNet to assess complementary effects
3. Validate algorithm-agnostic claims by implementing GrowNN with SAC and DQN on standard control benchmarks, monitoring for optimizer state issues