---
ver: rpa2
title: Mechanisms of Non-Monotonic Scaling in Vision Transformers
arxiv_id: '2511.21635'
source_url: https://arxiv.org/abs/2511.21635
tags:
- information
- vit-b
- vit-l
- block
- scrambling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision Transformers often perform worse when scaled deeper, defying
  typical scaling assumptions. Through systematic analysis of ViT-S, ViT-B, and ViT-L
  on ImageNet, a consistent three-phase Cliff-Plateau-Climb pattern in token similarity
  evolution was identified.
---

# Mechanisms of Non-Monotonic Scaling in Vision Transformers

## Quick Facts
- arXiv ID: 2511.21635
- Source URL: https://arxiv.org/abs/2511.21635
- Reference count: 40
- Primary result: ViTs often perform worse when scaled deeper, with a Cliff-Plateau-Climb pattern in token similarity evolution linked to [CLS] hub marginalization and controlled information mixing.

## Executive Summary
Vision Transformers often defy typical scaling assumptions by performing worse when made deeper. Through systematic analysis of ViT-S, ViT-B, and ViT-L on ImageNet, this paper identifies a consistent three-phase Cliff-Plateau-Climb pattern in token similarity evolution. Better performance correlates with progressive marginalization of the [CLS] token—from 0.502 to 0.004 in ViT-B—favoring distributed consensus among patch tokens over centralized aggregation. The Information Scrambling Index quantifies mixing dynamics, revealing that ViT-B maintains controlled mixing (0.004 to 0.009) while ViT-L escalates into over-communication (up to 0.031), degrading geometry. ViT-L requires ~10 extra layers to reach a comparable information-task tradeoff, suggesting depth inefficiency.

## Method Summary
The study analyzes layer-wise representation evolution in pretrained ViT-S/16, ViT-B/16, and ViT-L/16 on ImageNet-1k classification. Centered token similarity per layer reveals the Cliff-Plateau-Climb pattern. Linear probes and reconstruction decoders measure task signal and information mixing. CLS Centrality (CCC) and Attention Consensus Index (ACI) quantify hub importance and consensus dynamics. Neural Collapse metrics (NC1-NC4) assess geometric quality. Reconstruction decoders train to recover pre-PE patch embeddings from layer tokens, computing InfoX_self and InfoX_all to derive the Scrambling Index.

## Key Results
- Consistent Cliff-Plateau-Climb pattern in token similarity evolution across ViT-S, ViT-B, and ViT-L
- Better performance linked to progressive [CLS] marginalization (from 0.502 to 0.004 in ViT-B)
- ViT-B maintains controlled Scrambling Index (0.004-0.009); ViT-L escalates to 0.031 with depth inefficiency
- ViT-L requires ~10 extra layers to reach ViT-B's information-task tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Positional Encoding-Driven Decorrelation (Cliff Phase)
Early spatial differentiation is beneficial for subsequent feature extraction. Positional encodings dominate early representations, causing tokens to differentiate by spatial location rather than semantic content. The PE Dominance Ratio governs decorrelation severity.

### Mechanism 2: Distributed Consensus via Progressive Hub Marginalization
Better-performing ViTs systematically marginalize the [CLS] token in favor of distributed consensus among patch tokens. During the Climb phase, CLS Centrality drops sharply while Attention Consensus Index rises, bypassing the architectural hub bottleneck.

### Mechanism 3: Information Scrambling Index as Computational Regime Diagnostic
The Scrambling Index (InfoX_all - InfoX_self) distinguishes three computational regimes with predictable performance outcomes. Stable low-positive values indicate beneficial consensus; collapse toward zero or escalation to high values indicates under- or over-mixing.

## Foundational Learning

- **Centered Token Similarity**
  - Why needed here: Raw similarity is confounded by anisotropy; centering removes baseline and reveals the Cliff-Plateau-Climb pattern.
  - Quick check question: Why does mean subtraction matter for detecting layer-wise dynamics in deep transformers?

- **Neural Collapse (NC1-NC4)**
  - Why needed here: NC metrics quantify geometric optimality in the Climb phase; lower NC2 indicates closer-to-ideal ETF geometry.
  - Quick check question: Which NC metric measures convergence of class means to a simplex ETF structure?

- **Attention Graph Analysis (ACI, CCC)**
  - Why needed here: Graph-theoretic metrics quantify consensus speed and hub importance, revealing distributed vs centralized aggregation.
  - Quick check question: What does a rising ACI paired with falling CCC indicate about information flow?

## Architecture Onboarding

- **Component map**: Patch embeddings + positional encodings → [CLS] token + patch tokens → Transformer blocks (attention + MLP) → Final-layer [CLS] token → classification head
- **Critical path**: Layers 0-1 (Cliff): PE-driven decorrelation; Middle layers (Plateau): Feature refinement; Final 3-4 layers (Climb): Neural Collapse emergence
- **Design tradeoffs**: Depth vs. efficiency (more layers extend Plateau but risk over-scrambling); PE strength vs. decorrelation severity; [CLS] design vs. emergent behavior
- **Failure signatures**: Late-stage CCC spikes (hub re-centralization) → degraded NC2 geometry; Scrambling Index escalation beyond ~0.02 → chaotic diffusion; Incomplete marginalization (CCC>0.01) → suboptimal consensus
- **First 3 experiments**:
  1. Measure layer-wise centered token similarity to confirm Cliff-Plateau-Climb pattern
  2. Track CCC and ACI across layers; verify hub marginalization timing aligns with geometry improvements
  3. Compute Scrambling Index per layer; confirm stable low-positive regime (0.004-0.009) correlates with best performance

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the Cliff-Plateau-Climb pattern and Information Scrambling Index dynamics generalize beyond ImageNet classification to other architectures, modalities, and training regimes?
- **Open Question 2**: What is the theoretical basis for the empirically observed "optimal" scrambling range (~0.004–0.009)?
- **Open Question 3**: Can the Scrambling Index or CLS Centrality serve as actionable training signals—for example, via regularization or early stopping—to improve depth efficiency?
- **Open Question 4**: Are the observed correlations between scrambling regimes, hub marginalization, and geometric quality causal, or are they secondary effects of another underlying mechanism?

## Limitations
- PE Dominance Ratio is observational rather than mechanistically derived
- Scrambling Index proxy assumption (reconstruction quality reflects retained information) is not validated against semantic preservation
- Layer sampling intervals (10 layers total) may miss fine-grained dynamics
- No ablations isolate whether distributed consensus is truly optimal

## Confidence
- **High**: Existence of Cliff-Plateau-Climb pattern; correlation between late-stage CCC spikes and degraded NC2; controlled Scrambling Index (0.004-0.009) aligns with best performance
- **Medium**: PE Dominance Ratio's role in driving early decorrelation; hub marginalization as primary driver of consensus quality; Scrambling Index as diagnostic regime identifier
- **Low**: Reconstruction proxy as ground-truth information retention metric; causal role of PE scaling in performance; universal optimality of distributed consensus

## Next Checks
1. Compare Scrambling Index correlations against direct semantic similarity metrics to verify proxy validity
2. Systematically vary PE strength and retrain ViT-B; measure if performance degradation aligns with predicted decorrelation thresholds
3. Replace [CLS] with average pooling of patch tokens; train from scratch to test whether distributed consensus is truly superior to hub-dependent aggregation