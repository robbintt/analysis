---
ver: rpa2
title: 'Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems
  via Merlin-Arthur Protocols'
arxiv_id: '2512.11614'
source_url: https://arxiv.org/abs/2512.11614
tags:
- training
- step
- input
- loss
- morgana
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a training framework that treats RAG as an
  interactive proof system via the Merlin-Arthur protocol. The generator LLM (Arthur)
  is trained adversarially against a helpful prover (Merlin) and an adversarial prover
  (Morgana) to improve robustness, grounding, and interpretability without requiring
  annotated unanswerable samples.
---

# Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols

## Quick Facts
- arXiv ID: 2512.11614
- Source URL: https://arxiv.org/abs/2512.11614
- Reference count: 40
- Primary result: Training RAG as a Merlin-Arthur interactive proof system improves completeness (up to 99%), soundness (up to 96%), and EIF (up to 0.62) without annotated unanswerable samples

## Executive Summary
This work introduces a training framework that treats RAG as an interactive proof system via the Merlin-Arthur protocol. The generator LLM (Arthur) is trained adversarially against a helpful prover (Merlin) and an adversarial prover (Morgana) to improve robustness, grounding, and interpretability without requiring annotated unanswerable samples. This framework establishes mutual information exchange guarantees and enables verifiable evidence-aware generation. Experiments on SQuAD, HotpotQA, and TriviaQA show improved completeness (up to 99%), soundness (up to 96%), and explained information fraction (EIF) (up to 0.62), while maintaining accuracy. The retriever also benefits from automated hard positives and negatives, improving recall and MRR.

## Method Summary
The framework trains a generator LLM (Arthur) against two adversarial provers: Merlin provides helpful evidence by masking context to maximize Arthur's probability of the correct answer, while Morgana provides adversarial evidence by masking to minimize this probability. For each (query, context, answer) triplet, ATMAN identifies tokens causally influencing predictions, and Merlin/Morgana generate masked contexts accordingly. Arthur trains on a weighted loss combining original context, Merlin-masked context, and Morgana-masked context, where Morgana loss treats both correct answer and reject as valid targets. After convergence, the generated contexts become hard positives/negatives for retriever training, improving discrimination without manual annotation.

## Key Results
- Completeness improves to 99% and soundness to 96% on SQuAD, HotpotQA, and TriviaQA
- Explained Information Fraction (EIF) reaches 0.41-0.62, indicating substantial evidence-groundedness
- Retriever Recall@1 improves by 2-11 percentage points over baseline
- Accuracy is maintained while reducing incorrect answers by 25-60% under adversarial context

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training Arthur against Merlin and Morgana induces evidence-grounded generation and reject behavior without annotated unanswerable samples
- **Mechanism:** Merlin masks context to maximize Arthur's correct answer probability by preserving supporting evidence; Morgana masks to minimize this by removing critical evidence. Arthur trains on weighted losses combining original, Merlin-masked, and Morgana-masked contexts, with Morgana loss treating both correct answer and reject as valid targets
- **Core assumption:** ATMAN accurately identifies tokens causally influencing predictions, and masked contexts transfer to real retrieval scenarios
- **Evidence anchors:** Abstract states Arthur trains on questions with unknown context provenance; section 4.1 describes the three-way loss formulation; related work confirms RAG brittleness to adversarial inputs

### Mechanism 2
- **Claim:** Low completeness and soundness errors establish a lower bound on mutual information between context features and generated answers, quantified via EIF
- **Mechanism:** The M/A framework bounds feature precision as Pr_D(M) ≳ (1 - ε_c - ε_s)/(1 - ε_c + ε_s), constraining I(Y; Features) ≥ H(Y) - H_b(Pr_D(M)). EIF normalizes this: EIF ≈ (1 - H_b(P_rD(M)))/(1 - H_b(C))
- **Core assumption:** Binary reduction of open-ended generation preserves M/A guarantees
- **Evidence anchors:** Abstract mentions mutual information guarantees and EIF values; section 3.1 describes how low errors guarantee high mutual information with ground truth

### Mechanism 3
- **Claim:** Feeding M/A contexts from converged generator back into retriever training creates hard positives/negatives that improve discrimination without manual annotation
- **Mechanism:** Merlin contexts achieving high completeness become hard positives (minimal sufficient evidence); Morgana contexts that fool Arthur or cause rejection become hard negatives (semantically similar but evidence-deficient)
- **Core assumption:** A converged Arthur's sufficiency judgments generalize to retrieval targets for future queries
- **Evidence anchors:** Abstract mentions retriever benefits from automated hard positives/negatives; section 5.3 describes improved Recall@1 and MRR

## Foundational Learning

- **Concept:** Merlin-Arthur Interactive Proof Systems
  - **Why needed here:** The theoretical foundation—completeness/soundness errors bounding mutual information—comes from M/A protocol theory
  - **Quick check question:** Why does having both helpful and adversarial provers provide stronger guarantees than Arthur evaluating evidence quality alone?

- **Concept:** Mutual Information I(X;Y)
  - **Why needed here:** EIF quantifies what proportion of predictive signal is captured by evidence features; understanding this is essential to interpret the guarantees
  - **Quick check question:** If completeness = 95% and soundness = 90%, what does this intuitively imply about information flow from context to answer?

- **Concept:** Perturbation-based XAI (ATMAN)
  - **Why needed here:** Merlin/Morgana masking relies on ATMAN identifying causal tokens via actual masking, not just gradient saliency
  - **Quick check question:** Why might gradient-based saliency fail to identify tokens Merlin should preserve, compared to perturbation methods that physically mask tokens?

## Architecture Onboarding

- **Component map:** (q, c, a_true) → ATMAN (score each token by masking) → Merlin mask (maximize p) → Morgana mask (minimize p) → Arthur LLM: L = λ_util·L(c) + λ_Me·L(c_Me) + λ_Mo·L_Mo

- **Critical path:** ATMAN masking is the bottleneck (O(N) forward passes per sample). Three-way loss evaluation per step. Retriever feedback requires pre-converged Arthur.

- **Design tradeoffs:**
  - **Masking granularity:** Token-level is more precise (22-68s overhead) vs sentence-level (3-8s). Paper shows similar metrics; sentence-level has higher groundedness
  - **Masking ratio (x%):** Paper uses 0.6—sweet spot where Merlin maximizes and Morgana minimizes probability
  - **Loss weights:** λ_Me=0.5, λ_util=λ_Mo=0.25. Higher λ_Mo → soundness ↑ but completeness may drop
  - **Attention vs string masking:** Attention preserves positional embeddings; string is simpler but changes input distribution

- **Failure signatures:**
  - Arthur always rejects → Morgana too aggressive or λ_Mo too high
  - No groundedness improvement → poor ATMAN attributions or wrong masking ratio
  - Retriever doesn't improve → Arthur hasn't converged; continue generator training first
  - Training instability → reduce LoRA rank, adjust λ weights, check learning rates

- **First 3 experiments:**
  1. Reproduce baseline comparison (vanilla SFT vs M/A): measure accuracy, completeness, soundness, groundedness. Target ~25-60pp reduction in incorrect answers under adversarial context
  2. Ablate masking ratio x% ∈ {0.3, 0.5, 0.6, 0.8}, plot answer probability vs mask size to find dataset-optimal value
  3. Compute conditional EIF via Eq 4 on held-out set. Target EIF_cond ≥ 0.3; lower values indicate insufficient evidence-grounding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the M/A framework perform when retrieved documents contain subtle misinformation that appears plausible as evidence, versus the adversarial masking strategies (Morgana) used during training?
- **Basis in paper:** Impact Statement: "The system is fundamentally limited by the quality of its sources; if the input documents contain misinformation, the model will treat them as 'proof' and propagate those errors"
- **Why unresolved:** Morgana generates adversarial contexts by masking, not by introducing coherent misinformation; the framework has not been stress-tested against realistic disinformation or fact-conflict scenarios
- **What evidence would resolve it:** Evaluation on datasets with intentionally planted misinformation (e.g., conflicting evidence benchmarks, adversarially corrupted corpora) comparing M/A-trained models against baselines on error propagation rates

### Open Question 2
- **Question:** Does the M/A training-induced reject behavior create an undesirable conservative bias, reducing useful answer coverage in domains where evidence is sparse but reasoning is still valid?
- **Basis in paper:** Impact Statement: "There is also a risk that the model will become too conservative, refusing to answer helpful queries because it cannot find a direct citation"
- **Why unresolved:** The paper measures soundness and completeness but does not separately quantify false rejection rates (over-abstention) or analyze utility tradeoffs across domains with varying evidence density
- **What evidence would resolve it:** Systematic measurement of false rejection rates on held-out sets where answers are inferable but not explicitly cited, plus human evaluation of reject decisions

### Open Question 3
- **Question:** Can the mutual information guarantees (EIF) and M/A training framework scale to significantly larger models (7B+ parameters) and to non-QA RAG tasks such as summarization, dialogue, or code generation?
- **Basis in paper:** Experiments limited to 1B–4B instruct models and three QA datasets; theoretical framework claims broad applicability to "any learning system that must reason over external inputs"
- **Why unresolved:** No demonstration beyond QA; computational cost of mask generation scales with model size (3–68s per sample reported); multi-hop reasoning tasks may exhibit different failure modes
- **What evidence would resolve it:** Extension experiments on larger model families (e.g., Llama-7B/13B, Qwen-14B) and on tasks like summarization (CNN/DailyMail) or dialogue (Doc2Dial), reporting EIF, computational overhead, and task-specific grounding metrics

## Limitations
- The framework's theoretical guarantees rely on strong assumptions about the relationship between completeness/soundness errors and mutual information
- ATMAN masking method requires N forward passes per sample, creating computational bottlenecks that could limit scalability
- Reject behavior depends on arbitrary token sequences ("Reject") that may not generalize across model families or languages

## Confidence
**High Confidence:**
- Training against adversarial Morgana contexts improves soundness and reduces hallucination rates
- The three-way loss formulation with Merlin/Morgana masking provides measurable improvements over vanilla SFT
- Automated hard positives/negatives improve retriever recall and MRR when added to contrastive training

**Medium Confidence:**
- The mutual information bounds derived from completeness/soundness errors hold under the binary classification reduction
- EIF values (0.41-0.62) meaningfully quantify evidence-groundedness and are comparable across datasets
- The method works without annotated unanswerable samples, though this may reduce performance on truly unanswerable questions

**Low Confidence:**
- The specific 60% masking ratio is optimal for all datasets (likely dataset-dependent)
- ATMAN attention masking is superior to gradient-based methods for identifying causal tokens
- The mutual information guarantees generalize to non-QA generation tasks

## Next Checks
1. **Validate EIF Computation and Generalization**: Reproduce the conditional EIF calculation (Eq 4) on a held-out test set with at least 1000 samples per dataset. Verify that EIF_cond ≥ 0.3 for all datasets and that the metric correlates with human judgments of answer faithfulness. Test whether EIF degrades gracefully as context length increases beyond typical QA lengths.

2. **Stress Test ATMAN Attribution Accuracy**: Create a synthetic dataset where ground-truth causal tokens are known (e.g., masked sentences containing answer spans). Compare ATMAN's top-k token selections against ground truth using precision@k and recall@k. Vary masking granularity (token vs sentence) and ratio to identify regimes where ATMAN fails. This validates whether the masking mechanism actually identifies evidence tokens.

3. **Evaluate Reject Behavior in Real-World Settings**: Deploy the trained model on datasets with known unanswerable questions (e.g., SQuAD2.0's unanswerable subset). Measure precision-recall tradeoff for rejection decisions and compare against models trained with explicit unanswerable samples. This tests whether adversarial training alone produces appropriate reject behavior for questions truly lacking evidence.