---
ver: rpa2
title: 'OCR Error Post-Correction with LLMs in Historical Documents: No Free Lunches'
arxiv_id: '2502.01205'
source_url: https://arxiv.org/abs/2502.01205
tags:
- text
- finnish
- data
- english
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates open-weight LLMs for post-correction of OCR
  errors in historical English and Finnish texts. Using page-level ground truth datasets
  from ECCO-TCP and Finnish newspapers, it explores parameter optimization, quantization
  effects, segment length, and post-processing strategies.
---

# OCR Error Post-Correction with LLMs in Historical Documents: No Free Lunches

## Quick Facts
- arXiv ID: 2502.01205
- Source URL: https://arxiv.org/abs/2502.01205
- Authors: Jenna Kanerva; Cassandra Ledins; Siiri Käpyaho; Filip Ginter
- Reference count: 14
- Primary result: Open-weight LLMs achieve 38.7% CER improvement for English OCR correction but fail completely on Finnish text

## Executive Summary
This study systematically evaluates open-weight large language models for post-correction of OCR errors in historical documents, comparing performance across English (ECCO-TCP) and Finnish (NLF GT) datasets. The research explores critical factors including parameter optimization, quantization effects, segment length, and post-processing strategies. While English results show substantial error reduction with models like Llama-3.1-70B achieving 38.7% CER improvement, the study reveals a stark limitation: open-weight models completely failed to produce useful corrections for Finnish, highlighting language-specific challenges in LLM-based OCR correction. The work also demonstrates that alignment-based overgeneration removal is crucial for extracting corrected text from Llama models, and that segment boundary artifacts significantly impact correction quality.

## Method Summary
The study uses zero-shot OCR post-correction with open-weight LLMs on historical documents, employing ECCO-TCP (301,937 English pages) and NLF GT (449 Finnish newspaper pages). Pages are split into segments of approximately 300 sub-words (200 words English, 100 words Finnish), with 200 examples each for dev/test per language. Models are loaded via Ollama v0.3.8 with 4-bit Q4_0 quantization by default, with parameter optimization performed using Optuna across 100 runs. Overgeneration removal is implemented through character-level local alignment using biopython with specific gap penalties. The evaluation employs HuggingFace's CER and WER metrics with unicode normalization (NFKC) and whitespace standardization.

## Key Results
- Llama-3.1-70B achieved 38.7% CER improvement and 37.1% WER improvement for English text correction
- All tested open-weight models (including Qwen2.5, Mixtral, and various Llama versions) failed on Finnish, showing negative CER/WER improvements
- fp16 quantization consistently outperformed 4-bit quantization for Llama models despite higher memory requirements
- Longer segments (200+ words) showed better performance than shorter segments, with boundary context proving critical for quality

## Why This Works (Mechanism)
LLMs leverage their training on large text corpora to recognize and correct character-level errors that resemble common OCR mistakes. The models' ability to understand context allows them to infer correct words even when individual characters are corrupted, particularly effective for English where statistical patterns are well-represented in training data. The alignment-based overgeneration removal addresses the tendency of LLMs to provide explanatory corrections alongside the corrected text, extracting only the useful output.

## Foundational Learning
- **Character Error Rate (CER)**: Measures character-level accuracy between corrected and ground truth text; needed to quantify OCR correction quality at the most granular level
  - Quick check: CER = (Substitutions + Insertions + Deletions) / Total characters in reference
- **Word Error Rate (WER)**: Measures word-level accuracy; provides higher-level view of correction effectiveness
  - Quick check: WER = (Substitutions + Insertions + Deletions) / Total words in reference
- **Quantization effects**: Lower precision (4-bit vs fp16) reduces memory usage but can degrade model performance, particularly for morphologically complex languages
  - Quick check: Compare model outputs at different quantization levels on same validation set
- **Segment boundary artifacts**: LLMs struggle with context windows, causing errors near segment edges; left-context provision helps but doesn't eliminate issues
  - Quick check: Compare correction quality at segment centers vs boundaries
- **Alignment-based overgeneration removal**: Uses sequence alignment to extract corrected text from LLM outputs that include explanatory text
  - Quick check: Apply biopython alignment with gap penalties (-1, -0.5) to separate corrections from explanations
- **Parameter optimization for generation**: Temperature, top_k, and top_p parameters significantly impact correction quality and must be tuned per language
  - Quick check: Use Optuna or similar to find optimal generation parameters on dev set

## Architecture Onboarding
**Component Map**: Dataset preparation -> Model loading (Ollama) -> Parameter optimization -> Inference -> Alignment post-processing -> Evaluation

**Critical Path**: Dataset segmentation → Model inference → Overgeneration removal → CER/WER calculation

**Design Tradeoffs**: 4-bit quantization reduces memory usage but degrades performance vs fp16; shorter segments reduce memory requirements but increase boundary artifacts; open-weight models offer accessibility but lack language support for Finnish

**Failure Signatures**: Llama models output mixed text with corrections embedded in explanations (requires alignment); Finnish models produce outputs worse than input (negative CER); short segments show poor boundary correction

**Three First Experiments**:
1. Test Llama-3.1-70B with fp16 quantization on 200-word English segments with optimized parameters (temp=0.26, top_k=65, top_p=0.66)
2. Apply alignment-based overgeneration removal to extract corrected text from Llama outputs containing explanatory text
3. Compare LCC vs LUC boundary methods by providing 50-character left context to segment beginnings

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Complete failure of all open-weight models on Finnish text suggests fundamental limitations in language support or experimental conditions
- Alignment-based overgeneration removal is computationally expensive and may introduce artifacts in the corrected output
- Segment-wise processing introduces boundary artifacts that were only partially mitigated, particularly problematic for shorter segments and Finnish text

## Confidence
- **High confidence** in English results: Systematic parameter optimization and consistent CER/WER improvements provide robust evidence for LLM effectiveness in English OCR correction
- **Medium confidence** in quantization findings: Limited testing of only two quantization levels and one algorithm constrains generalizability
- **Low confidence** in Finnish applicability: Complete failure across all models raises questions about experimental setup or fundamental model limitations

## Next Checks
1. **Finnish optimization validation**: Test whether increasing segment length to 200+ words or using higher precision quantization improves Finnish results, as current 100-word segments may be insufficient for morphologically complex languages
2. **Prompt engineering replication**: Systematically vary the correction prompt template to determine if prompt quality explains the Finnish failure
3. **Cross-linguistic boundary analysis**: Compare boundary effect patterns between English and Finnish by testing LCC/LUC methods with varying left-context lengths to isolate language-dependent vs model-dependent issues