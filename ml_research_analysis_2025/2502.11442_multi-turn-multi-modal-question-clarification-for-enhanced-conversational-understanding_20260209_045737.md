---
ver: rpa2
title: Multi-Turn Multi-Modal Question Clarification for Enhanced Conversational Understanding
arxiv_id: '2502.11442'
source_url: https://arxiv.org/abs/2502.11442
tags:
- multi-modal
- user
- multi-turn
- retrieval
- facet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task of Multi-turn Multi-modal Clarifying
  Questions (MMCQ) in conversational search, addressing the limitation of existing
  systems that fail to capture complex user preferences involving visual attributes
  through progressive refinement. The authors construct a large-scale dataset named
  ClariMM with over 13k multi-turn interactions and 33k question-answer pairs, and
  propose Mario, a two-phase retrieval framework combining BM25 with a multi-modal
  generative re-ranking model.
---

# Multi-Turn Multi-Modal Question Clarification for Enhanced Conversational Understanding

## Quick Facts
- **arXiv ID:** 2502.11442
- **Source URL:** https://arxiv.org/abs/2502.11442
- **Reference count:** 10
- **Primary result:** Multi-turn multi-modal clarification improves MRR by 12.88% over single-turn approaches

## Executive Summary
This paper introduces the Multi-turn Multi-modal Clarifying Questions (MMCQ) task for conversational search, addressing the limitation of existing systems that fail to capture complex user preferences involving visual attributes through progressive refinement. The authors construct a large-scale dataset named ClariMM with over 13k multi-turn interactions and 33k question-answer pairs, and propose Mario, a two-phase retrieval framework combining BM25 with a multi-modal generative re-ranking model. Experiments demonstrate that multi-turn multi-modal clarification outperforms uni-modal and single-turn approaches, with the most significant gains observed in longer interactions.

## Method Summary
The authors construct the ClariMM dataset from the Melon dataset and TREC Web Track 2009-2012 documents, creating over 13k multi-turn conversations through synthetic generation with GPT-4o refinement. The proposed Mario framework employs a two-phase retrieval approach: first using BM25 with GPT-4o-extracted inferred queries to retrieve top-100 documents, then applying a multi-modal generative re-ranking model (LLaVA-OneVision-7b with SigLIP vision encoder) trained to generate YAKE-extracted document keywords. The model is trained with a margin ranking loss combining language modeling and ranking objectives, with constrained generation during inference using a trie over valid keyword sequences.

## Key Results
- Multi-turn multi-modal clarification improves MRR by 12.88% over single-turn approaches
- The proposed framework consistently achieves the highest scores across MRR, Precision@k, and nDCG@k metrics
- Most significant performance gains observed in longer interactions (3+ turns), validating progressive refinement benefits
- Integrating visual information provides substantial improvements over text-only baselines

## Why This Works (Mechanism)
The paper demonstrates that multi-turn multi-modal clarification enables progressive refinement of user preferences by combining textual and visual information. The two-phase retrieval approach effectively narrows down relevant documents through initial BM25 retrieval followed by sophisticated multi-modal re-ranking that considers both conversation context and visual attributes. The synthetic conversation generation with GPT-4o refinement ensures natural interactions while maintaining diversity and relevance.

## Foundational Learning
- **Multi-modal retrieval**: Combining text and image information for improved search relevance - needed to capture complex user preferences involving visual attributes; quick check: compare text-only vs multi-modal performance
- **Progressive refinement**: Multi-turn conversations that gradually clarify user intent - needed to handle ambiguous or complex queries; quick check: measure performance improvement across conversation lengths
- **Synthetic data generation**: Creating conversational data using language models - needed to scale dataset construction; quick check: human evaluation of naturalness and coherence
- **Constrained generation**: Using trie structures to ensure valid output sequences - needed to maintain consistency with document keywords; quick check: validity rate of generated sequences
- **Margin ranking loss**: Training objective that optimizes relative document ordering - needed for effective re-ranking; quick check: compare with other ranking losses

## Architecture Onboarding
**Component Map:** ClariMM dataset -> BM25 retrieval -> GPT-4o query extraction -> LLaVA-OneVision-7b re-ranking -> YAKE keyword extraction -> Constrained generation

**Critical Path:** Conversation context → GPT-4o inferred query → BM25 top-100 → LLaVA-OneVision-7b re-ranking → Final document ranking

**Design Tradeoffs:** Two-phase retrieval balances efficiency (BM25 fast filtering) with effectiveness (multi-modal re-ranking), but requires careful negative sampling and oracle image selection

**Failure Signatures:** Model ignoring visual information (check text-only performance gap), invalid keyword sequences (monitor trie validity), synthetic conversations lacking naturalness (human evaluation scores)

**First Experiments:** 1) Implement BM25 baseline with GPT-4o query extraction; 2) Fine-tune LLaVA-OneVision-7b with YAKE keywords; 3) Compare constrained vs unconstrained generation performance

## Open Questions the Paper Calls Out
None specified in the paper

## Limitations
- Several critical implementation details remain underspecified, including document corpus preprocessing, negative sampling strategy, and oracle image selection methodology
- Synthetic conversation generation via GPT-4o may introduce domain bias and may not fully capture real user behavior
- The use of "oracle" images in evaluation may overstate real-world applicability where image selection is not guaranteed optimal
- No random seed specified for dataset splits, affecting reproducibility

## Confidence
- **High confidence**: MMCQ task definition, dataset construction methodology (general approach), two-phase retrieval framework architecture, and overall performance improvements over baselines
- **Medium confidence**: Specific dataset statistics (split sizes, exact conversation counts), BM25 + re-ranking pipeline, and YAKE keyword extraction process
- **Low confidence**: Negative sampling strategy for ranking loss, oracle image selection criteria, and exact reproducibility of synthetic conversation generation

## Next Checks
1. Reconstruct the exact document corpus by obtaining and preprocessing the specific TREC Web Track 2009-2012 documents used, documenting all filtering criteria
2. Implement the negative sampling strategy for the margin ranking loss, testing multiple approaches (random BM25 negatives, hard negatives) to verify reported performance
3. Design and execute an oracle image selection methodology that mirrors the paper's evaluation approach, measuring its impact on retrieval performance compared to random or user-selected images