---
ver: rpa2
title: 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation'
arxiv_id: '2509.26278'
source_url: https://arxiv.org/abs/2509.26278
tags:
- proficiency
- language
- video
- profvlm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProfVLM reformulates proficiency estimation as a generative vision-language
  modeling task, enabling both skill level prediction and expert-like feedback generation
  from multi-view videos. It uses a frozen TimeSformer for feature extraction, followed
  by an AttentiveGatedProjector that fuses egocentric and exocentric views via attention
  and gating, then projects features into a language model space.
---

# ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation

## Quick Facts
- arXiv ID: 2509.26278
- Source URL: https://arxiv.org/abs/2509.26278
- Authors: Edoardo Bianchi; Jacopo Staiano; Antonio Liotta
- Reference count: 40
- Primary result: Achieves 48.2% accuracy in multi-view proficiency estimation with 20× fewer parameters than classification baselines

## Executive Summary
ProfVLM reformulates proficiency estimation as a conditional text generation task, enabling both skill level prediction and expert-like feedback generation from multi-view videos. The model uses a frozen TimeSformer for feature extraction, followed by an AttentiveGatedProjector that fuses egocentric and exocentric views via attention and gating, then projects features into a language model space. Trained on EgoExo4D with LoRA-tuned SmolLM2, ProfVLM achieves 48.2% accuracy while using 20× fewer parameters and 60% less training time than classification baselines. The model also generates high-quality textual feedback (BERTScore F1 85.53), providing interpretable reasoning alongside predictions.

## Method Summary
ProfVLM treats proficiency estimation as conditional text generation, taking multi-view videos as input and outputting proficiency labels with natural language feedback. The architecture extracts visual features from frozen TimeSformer-K600 backbone, fuses them through an AttentiveGatedProjector module that performs cross-view attention and gating, then projects to language model space. The model is fine-tuned with LoRA on SmolLM2-135M-Instruct using a chat template format. Training uses EgoExo4D dataset with 8 uniformly sampled frames per video, resized to 224×224 and normalized. The model achieves 48.2% top-1 accuracy and 85.53 BERTScore F1 for feedback generation.

## Key Results
- Achieves 48.2% top-1 accuracy on multi-view proficiency estimation
- Generates high-quality feedback with BERTScore F1 of 85.53
- Uses 20× fewer parameters and 60% less training time than classification baselines
- Outperforms baseline models on F1-score across all classes despite lower accuracy

## Why This Works (Mechanism)
ProfVLM reframes proficiency estimation as generative vision-language modeling, enabling simultaneous skill prediction and feedback generation. The attentive cross-view fusion captures complementary information from egocentric and exocentric perspectives, while the language model space projection allows for coherent textual explanations. LoRA fine-tuning enables efficient adaptation of the frozen visual backbone to the text generation task. The chat template format with ordered prompt structure (label first, then commentary) ensures causal dependencies align with task requirements.

## Foundational Learning

**Multi-view video processing**: Combining egocentric and exocentric camera perspectives provides complementary information about skill execution. Needed because single-view analysis misses crucial spatial context. Quick check: Verify both view types are available and synchronized in training data.

**Vision-language modeling**: Bridging visual perception with natural language generation enables interpretable skill assessment. Needed because pure classification lacks explanatory capability. Quick check: Confirm language model accepts projected visual features as input.

**Cross-view attention fusion**: Attention mechanisms can learn to weigh contributions from different camera perspectives dynamically. Needed because different views capture different skill-relevant features. Quick check: Verify attention weights are computed between view representations.

**LoRA fine-tuning**: Parameter-efficient adaptation of frozen models to new tasks. Needed because full fine-tuning is computationally expensive and risks catastrophic forgetting. Quick check: Confirm LoRA ranks and dimensions match reported values.

## Architecture Onboarding

**Component map**: Video frames -> TimeSformer-K600 -> AttentiveGatedProjector -> SmolLM2-135M-Instruct -> Proficiency label + Commentary

**Critical path**: The AttentiveGatedProjector is the core innovation, performing cross-view fusion through multi-head attention and gating before projection to language model space. This module determines both the quality of the fused representation and the efficiency of the overall architecture.

**Design tradeoffs**: Frozen visual backbone simplifies training and reduces parameters but may limit adaptation to specialized proficiency cues. The choice of conditional generation enables interpretability but requires careful prompt engineering. Small language model size prioritizes efficiency over potential accuracy gains from larger models.

**Failure signatures**: 
- Accuracy drops to 40.7% when using only one commentary per video (overfitting to limited data)
- 2.9% accuracy decrease when prompt ordering is reversed (spurious causal dependencies)
- Underperformance on basketball domain (41.00% vs 55.24% baseline) indicates need for domain-adaptive attention

**First experiments**:
1. Verify multi-view frame synchronization by checking if identical temporal indices are used across all views
2. Test prompt ordering sensitivity by training with both "label-then-commentary" and "commentary-then-label" formats
3. Evaluate domain transfer by testing on basketball while training on other five domains

## Open Questions the Paper Calls Out

**Domain-adaptive attention mechanisms**: The authors note basketball underperforms the baseline (41.00% vs 55.24%) and state that domain-adaptive attention mechanisms and higher-resolution temporal modeling are needed for technical sports analysis.

**Expert evaluation of feedback**: While BERTScore shows semantic similarity of 85.53 F1, the authors acknowledge that factual accuracy, pedagogical effectiveness, and actionability require expert evaluation that was not conducted.

**Variable camera perspectives**: The current architecture assumes fixed input views (Ego + 4 Exos), and adapting to dynamically handle variable numbers of camera perspectives with inconsistent frame counts remains an open challenge.

**Fine-tuning visual backbone**: The frozen encoder simplifies training but may limit adaptation to specialized proficiency cues. Fine-tuning the visual backbone or incorporating domain-specific motion priors could further enhance performance for fine-grained temporal understanding.

## Limitations

- Unknown multi-view frame synchronization could degrade cross-view attention fusion if frames are sampled independently per view
- Model shows domain-specific weaknesses, particularly on basketball (41.00% vs 55.24% baseline)
- Limited training data (4,238 samples) may constrain generalization beyond the six EgoExo4D domains
- Current design assumes fixed number of camera perspectives, limiting deployment flexibility

## Confidence

High confidence in technical methodology and reported metrics given detailed implementation specifications.
Medium confidence in efficiency claims due to lack of baseline architecture details.
Low confidence in cross-domain generalization potential given limited training domains.

## Next Checks

1. Verify multi-view frame synchronization by examining whether the same 8 temporal indices are used across all synchronized views during preprocessing.
2. Test prompt ordering sensitivity by training and evaluating with both "label-then-commentary" and "commentary-then-label" formats to confirm the 2.9% accuracy difference.
3. Conduct domain transfer experiments by evaluating ProfVLM on a held-out domain (e.g., testing on basketball while training on the other five domains) to assess generalization beyond the training distribution.