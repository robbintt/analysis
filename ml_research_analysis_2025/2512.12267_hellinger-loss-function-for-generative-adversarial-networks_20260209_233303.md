---
ver: rpa2
title: Hellinger loss function for Generative Adversarial Networks
arxiv_id: '2512.12267'
source_url: https://arxiv.org/abs/2512.12267
tags:
- loss
- function
- hellinger
- page
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Hellinger-type loss function for
  Generative Adversarial Networks (GANs) motivated by the boundedness, symmetry, and
  robustness properties of the Hellinger distance. The authors develop a comprehensive
  theoretical framework for this new loss, establishing the existence, uniqueness,
  consistency, and joint asymptotic normality of the estimators for both generator
  and discriminator parameters.
---

# Hellinger loss function for Generative Adversarial Networks

## Quick Facts
- arXiv ID: 2512.12267
- Source URL: https://arxiv.org/abs/2512.12267
- Reference count: 40
- Primary result: Introduces bounded, symmetric Hellinger-type loss for GANs with improved robustness to data contamination

## Executive Summary
This paper proposes a novel Hellinger-type loss function for Generative Adversarial Networks that addresses key limitations of the classic GAN loss. The proposed loss leverages the boundedness, symmetry, and robustness properties of the Hellinger distance to improve both estimation accuracy and resilience to data contamination. The authors develop a comprehensive theoretical framework establishing the existence, uniqueness, consistency, and joint asymptotic normality of the resulting estimators. Numerical experiments demonstrate that Hellinger-based losses outperform standard GAN losses, particularly under contamination scenarios, while maintaining comparable visual quality on the Fashion-MNIST dataset.

## Method Summary
The authors introduce a new loss function for GANs based on the Hellinger distance, which provides boundedness, symmetry, and robustness properties not present in traditional GAN losses. They establish a rigorous theoretical framework proving existence, uniqueness, consistency, and joint asymptotic normality of the estimators for both generator and discriminator parameters. The theoretical analysis includes derivation of the influence function to characterize robustness against data contamination. The proposed losses are evaluated through controlled experiments in Gaussian settings with varying contamination levels and on the Fashion-MNIST dataset, comparing performance against standard GAN losses.

## Key Results
- Hellinger-type losses maintain stable performance with up to 20% data contamination, while standard GANs show significant degradation
- The proposed losses yield improved estimation accuracy in controlled Gaussian settings compared to classic GAN loss
- Visual quality of generated samples on Fashion-MNIST is comparable to standard GANs while providing robustness benefits

## Why This Works (Mechanism)
The Hellinger distance's bounded nature prevents exploding gradients that can occur with unbounded losses, while its symmetry ensures balanced learning between generator and discriminator. The mathematical properties of the Hellinger distance naturally provide robustness to outliers and contamination through its influence function, which attenuates the impact of extreme observations. This combination of boundedness, symmetry, and robustness addresses fundamental stability and contamination sensitivity issues in standard GAN training.

## Foundational Learning

**Hellinger distance**: A symmetric divergence measure between probability distributions bounded between 0 and 1. Needed to understand the core mathematical properties that enable bounded gradients and symmetric learning. Quick check: Verify H(d,P) ∈ [0,1] and H(d,P) = H(P,d).

**Influence function**: A tool for assessing the sensitivity of statistical estimators to infinitesimal contamination at a point. Needed to formally characterize robustness properties of the proposed loss. Quick check: Compute influence function for contaminated normal distribution and verify boundedness.

**GAN training dynamics**: The adversarial minimax game between generator and discriminator. Needed to understand how loss properties affect training stability and convergence. Quick check: Trace gradient flow through generator-discriminator update cycles.

## Architecture Onboarding

Component map: Real data → Discriminator → Generator → Synthetic data → Discriminator (discriminator loss), Generator (generator loss)

Critical path: Data contamination → Hellinger loss computation → Parameter updates → Convergence monitoring

Design tradeoffs: Bounded loss prevents gradient explosion but may reduce sensitivity to subtle distribution differences; symmetry ensures balanced learning but may slow convergence in highly imbalanced scenarios.

Failure signatures: Under-contamination: unstable training with oscillating losses; Over-regularization: overly smooth samples lacking diversity; Parameter mismatch: one network dominates training.

First experiments:
1. Implement Hellinger loss in a simple GAN on Gaussian mixtures and compare convergence curves
2. Test contamination robustness by gradually increasing outlier proportion in synthetic data
3. Compare visual quality metrics (FID, Inception Score) on Fashion-MNIST between Hellinger and standard GANs

## Open Questions the Paper Calls Out

The paper does not explicitly call out additional open questions beyond its own research scope.

## Limitations

The theoretical analysis assumes idealized conditions including correctly specified models and known contamination distributions that may not hold in practice. Numerical experiments are limited to simple Gaussian settings and a single image dataset, constraining generalizability to complex real-world scenarios. Computational complexity of the proposed losses is not thoroughly examined, and scalability to large-scale problems remains unclear.

## Confidence

Theoretical framework and asymptotic properties: High
Robustness claims under contamination: Medium (limited empirical validation)
Practical performance on real-world datasets: Medium-Low (single dataset tested)
Computational efficiency claims: Low (not thoroughly investigated)

## Next Checks

1. Evaluate the proposed Hellinger-type losses on diverse real-world datasets (e.g., CIFAR-10, ImageNet) to assess generalization beyond Fashion-MNIST
2. Conduct extensive ablation studies to quantify the impact of contamination levels and types on both Hellinger-based and standard GAN losses
3. Analyze computational complexity and training stability across different network architectures and hyperparameter settings