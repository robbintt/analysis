---
ver: rpa2
title: How Do Agents Perform Code Optimization? An Empirical Study
arxiv_id: '2512.21757'
source_url: https://arxiv.org/abs/2512.21757
tags:
- performance
- validation
- optimization
- agents
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares AI agents and humans on performance optimization
  PRs using 324 AI- and 83 human-authored commits from the AIDev dataset. It finds
  that AI PRs are less likely to include explicit performance validation (45.7% vs
  63.6%, p=0.007) and rely more on static reasoning rather than benchmarking.
---

# How Do Agents Perform Code Optimization? An Empirical Study

## Quick Facts
- arXiv ID: 2512.21757
- Source URL: https://arxiv.org/abs/2512.21757
- Reference count: 40
- AI PRs are less likely to include explicit performance validation (45.7% vs 63.6%, p=0.007) and rely more on static reasoning rather than benchmarking.

## Executive Summary
This empirical study compares AI agents and humans on performance optimization pull requests using 324 AI- and 83 human-authored commits from the AIDev dataset. The analysis reveals that AI PRs apply similar optimization patterns to humans but are less likely to include explicit performance validation and rely more on static reasoning rather than benchmarking. Agent PRs merge faster but less frequently than human ones. The study highlights a validation gap in AI-generated performance improvements and suggests the need for standardized benchmarking infrastructure to support agent-driven workloads.

## Method Summary
The study analyzed 324 AI- and 83 human-authored performance PRs from the AIDev dataset, filtering 428 raw PRs down to 4,954 clean commit records. A dual-LLM annotation pipeline (GPT-5.1 and Gemini-3-Pro-Preview) classified optimization patterns using an expanded 59-pattern SysLLMatic catalog. Validation practices were categorized into four types (benchmark, profiling, static reasoning, anecdotal). Statistical comparisons included chi-square tests, permutation tests, and rarefaction analysis to assess pattern richness while controlling for sample imbalance.

## Key Results
- AI PRs are less likely to include explicit performance validation (45.7% vs 63.6%, p=0.007)
- AI and human PRs apply similar optimization patterns at the same rate, with no significant difference in pattern variety
- Agent PRs merge faster (median 0.03 hours vs 2.65 hours) but less frequently (57% vs 65%)

## Why This Works (Mechanism)

### Mechanism 1: Pattern Space Alignment via Training Data Absorption
AI agents apply the same classes of optimization patterns as humans because LLMs have absorbed established performance engineering knowledge from training corpora. Pattern knowledge is embedded in model weights through exposure to code repositories and optimization discussions during pre-training.

### Mechanism 2: Validation Gap from Infrastructure Disconnection
Agents validate performance less frequently because they lack integration with benchmarking infrastructure and rely on syntactic/semantic reasoning available in-context. Without programmatic access to execute benchmarks, agents fall back to static reasoning rather than empirical measurement.

### Mechanism 3: Speed-Acceptance Tradeoff from Automated Submission
Agent PRs merge faster due to immediate submission but face lower acceptance rates because validation gaps trigger reviewer skepticism. Agents submit PRs without human deliberation delays, achieving near-instant merge times when reviewers trust the change, but the absence of empirical validation evidence reduces reviewer confidence.

## Foundational Learning

- **Concept: Performance Validation Taxonomy**
  - Why needed here: Understanding benchmark-based, profiling-based, and static-reasoning-based validation is required to interpret the validation gap and design interventions.
  - Quick check question: Given a PR that claims "reduces complexity from O(n²) to O(n)" without runtime data, which validation type applies?

- **Concept: Optimization Pattern Catalogs**
  - Why needed here: The SysLLMatic catalog (expanded from 43 to 59 patterns) provides the classification vocabulary for comparing agent and human strategies.
  - Quick check question: If a patch replaces string concatenation with StringBuilder, which SysLLMatic category and sub-pattern apply?

- **Concept: Statistical Comparison of Categorical Distributions**
  - Why needed here: Chi-square tests with effect sizes (Cramér's V) determine whether observed differences are significant and practically meaningful.
  - Quick check question: A chi-square test yields p=0.007 but Cramér's V=0.14. Is this association statistically significant? Practically significant?

## Architecture Onboarding

- **Component map:**
  AIDev dataset -> AIDev-Pop and Human-PR subsets -> 428 perf-labeled PRs -> 4,954 clean commits -> Dual-LLM annotation -> SysLLMatic catalog classification -> Statistical analysis

- **Critical path:**
  1. Filter AIDev for "perf"-labeled PRs → 428 raw PRs
  2. Mine commit details, apply quality filters → 4,954 clean commits across 407 PRs
  3. Dual-LLM pattern classification → disagreement cases → human adjudication
  4. Statistical comparison (chi-square, permutation and rarefaction tests)

- **Design tradeoffs:**
  - Repository star thresholds (≥100 vs ≥500) prioritize data volume vs. project maturity—creates asymmetric sample composition
  - LLM-assisted annotation trades labeling speed for error rate (~10% in agreed cases; ~65% LLM disagreement with manual labels)
  - Self-reported validation captures described practices, not actual execution—may undercount informal testing

- **Failure signatures:**
  - Low inter-annotator agreement (κ <0.5 at pattern level) signals ambiguous patch classification
  - Hallucinated benchmark claims (e.g., "7400× speedup" without benchmark code)
  - "No Meaningful Performance Change" misclassifications inflate pattern counts

- **First 3 experiments:**
  1. Instrument a subset of agent workflows with mandatory benchmark execution—measure validation rate improvement.
  2. Compare post-merge performance regression rates for agent vs. human perf PRs to assess real-world effectiveness.
  3. Ablate static-reasoning-only PRs to determine whether their merge rate differs from benchmark-validated PRs of similar complexity.

## Open Questions the Paper Calls Out

### Open Question 1
Can AI agents effectively utilize optimization patterns humans avoid for maintainability reasons (e.g., loop unrolling) if integrated with automated correctness verification? The authors note that loop techniques like unrolling and fusion are rarely used by humans due to readability costs, but "these constraints do not apply in the same way to AI agents," representing an "underexplored space."

### Open Question 2
How can standardized benchmarking infrastructure be adapted to scale for high-volume agent-driven workloads? The discussion states that existing CI-based benchmarking resources are "unlikely to scale to agent-driven workloads" and calls for "standardized benchmarking services."

### Open Question 3
Does the absence of explicit validation evidence in AI PRs correlate with a lack of actual testing or merely a failure to document it? The methodology acknowledges a construct threat: "validation conducted outside the PR workflow... may lead us to underestimate performance validation practices."

## Limitations
- Dual-LLM annotation pipeline introduces potential labeling noise with observed κ=0.62 at the pattern level
- Repository selection bias (AIDev-Pop vs Human-PR star thresholds) creates asymmetric sample composition
- Self-reported validation classification captures only described validation practices rather than actual execution

## Confidence
- **High**: Agent PRs merge faster but less frequently (statistical significance p<0.001 for time, p=0.007 for validation gap)
- **Medium**: No significant difference in optimization pattern usage (p=0.636, but sample imbalance concerns)
- **Low**: Validation type distribution differences (benchmark vs static reasoning) due to reliance on self-reported claims

## Next Checks
1. Instrument agent workflows with mandatory benchmark execution to measure validation rate improvement
2. Compare post-merge performance regression rates between agent and human perf PRs
3. Analyze whether static-reasoning-only PRs show different merge rates than benchmark-validated PRs of similar complexity