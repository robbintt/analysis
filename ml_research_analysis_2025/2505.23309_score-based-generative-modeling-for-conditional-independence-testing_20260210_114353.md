---
ver: rpa2
title: Score-based Generative Modeling for Conditional Independence Testing
arxiv_id: '2505.23309'
source_url: https://arxiv.org/abs/2505.23309
tags:
- conditional
- testing
- error
- distribution
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a conditional independence testing method
  based on score-based generative modeling. The key idea is to estimate conditional
  scores using sliced score matching and generate null hypothesis samples via Langevin
  dynamics, followed by a goodness-of-fit validation stage.
---

# Score-based Generative Modeling for Conditional Independence Testing

## Quick Facts
- **arXiv ID:** 2505.23309
- **Source URL:** https://arxiv.org/abs/2505.23309
- **Reference count:** 40
- **Key outcome:** This paper introduces a conditional independence testing method based on score-based generative modeling. The key idea is to estimate conditional scores using sliced score matching and generate null hypothesis samples via Langevin dynamics, followed by a goodness-of-fit validation stage. This approach addresses the limitations of existing generative model-based CI tests, particularly GAN-based methods, which often struggle with training instability and poor conditional distribution modeling. The proposed method achieves precise Type I error control and strong testing power across diverse scenarios. Extensive experiments on synthetic and real-world datasets demonstrate that it significantly outperforms state-of-the-art methods in both high-dimensional and nonlinear settings, while providing better interpretability through accurate conditional distribution modeling.

## Executive Summary
This paper proposes SGMCIT, a novel conditional independence (CI) testing method using score-based generative modeling. Unlike GAN-based approaches, SGMCIT estimates conditional scores via sliced score matching and generates null hypothesis samples using Langevin dynamics, followed by a goodness-of-fit validation stage. The method addresses limitations of existing CI tests, particularly training instability and poor conditional distribution modeling. Experiments show SGMCIT achieves precise Type I error control and strong testing power across diverse scenarios, outperforming state-of-the-art methods in both high-dimensional and nonlinear settings while providing better interpretability through accurate conditional distribution modeling.

## Method Summary
The proposed method estimates the conditional score function $s(x, z; \theta)$ using sliced score matching to minimize the Fisher divergence. The score model is trained with an MLP architecture using Swish activations. Once trained, Langevin dynamics with the estimated score function generates samples from the null hypothesis $P(X|Z)$. A goodness-of-fit validation stage ensures the generated samples match the observed distribution before computing the test statistic (e.g., RDC or MMD) and p-value. The method includes a two-stage approach: first validating the goodness-of-fit, then performing the CI test if samples are sufficiently accurate.

## Key Results
- Achieves precise Type I error control across diverse synthetic and real-world datasets
- Demonstrates strong testing power in both high-dimensional and nonlinear settings
- Significantly outperforms state-of-the-art methods, particularly in cases where existing approaches fail due to training instability or poor conditional distribution modeling
- Provides better interpretability through accurate conditional distribution modeling

## Why This Works (Mechanism)
The method works by directly modeling the conditional distribution $P(X|Z)$ through score estimation, avoiding the mode collapse and instability issues common in GAN-based approaches. Sliced score matching provides a stable objective for learning the score function, while Langevin dynamics ensures samples from the null hypothesis follow the correct conditional distribution. The goodness-of-fit validation stage acts as a safeguard against cases where the score model fails to capture the true distribution, maintaining valid Type I error control even when the generative model is imperfect.

## Foundational Learning

**Score Matching** - Why needed: Provides a stable objective for learning score functions without requiring explicit density estimation. Quick check: Verify Fisher divergence minimization converges and gradients are well-behaved.

**Langevin Dynamics** - Why needed: Generates samples from the learned score function to simulate the null hypothesis distribution. Quick check: Monitor sample quality and convergence of the sampling process.

**Fisher Divergence** - Why needed: Measures the difference between true and estimated score functions, providing a theoretically sound objective. Quick check: Ensure the divergence decreases during training and remains stable.

**Conditional Independence Testing** - Why needed: Determines whether variables are independent given a conditioning set, fundamental for causal inference and feature selection. Quick check: Validate Type I error control on known independent cases.

**MMD (Maximum Mean Discrepancy)** - Why needed: Measures distributional differences between observed and generated samples for goodness-of-fit validation. Quick check: Verify MMD p-values are stable and interpretable.

## Architecture Onboarding

**Component Map:** Data Preprocessing -> Score Model Training -> Langevin Sampling -> Goodness-of-Fit Validation -> Test Statistic Computation -> p-value Calculation

**Critical Path:** The most critical components are the score model training and Langevin sampling stages, as failures here directly impact Type I error control and testing power. The goodness-of-fit validation serves as a crucial safeguard.

**Design Tradeoffs:** The method trades computational efficiency (Langevin dynamics requires multiple iterations) for better conditional distribution modeling compared to one-step generative methods. The two-stage approach (validation then testing) adds complexity but ensures reliability.

**Failure Signatures:** Poor Type I error control indicates score model failure or sampling issues. Low test power suggests insufficient model capacity or suboptimal hyperparameters. Unstable training indicates problematic score matching objectives or data preprocessing issues.

**First Experiments:**
1. Implement synthetic data generation with known conditional independence structures and verify Type I error control.
2. Train the score model on simple conditional distributions and visualize learned scores versus ground truth.
3. Test the full pipeline on low-dimensional synthetic data with varying nonlinearities to establish baseline performance.

## Open Questions the Paper Calls Out

**Open Question 1:** How can advanced sampling strategies (e.g., consistency models, ODE solvers) be integrated into the SGMCIT framework to accelerate the Langevin dynamics process without violating the exchangeability assumptions required for valid Type I error control? The conclusion states future work will focus on accelerating the sampling process, as the current iterative Langevin dynamics creates a computational bottleneck.

**Open Question 2:** Can rigorous non-asymptotic (finite-sample) Type I error bounds be established for SGMCIT that explicitly account for score estimation error and discretization error, rather than relying on asymptotic assumptions? Theorem 5.4 guarantees validity asymptotically as n → ∞, necessitating heuristic goodness-of-fit stages for finite samples.

**Open Question 3:** How can the conditional sliced score matching framework be adapted to handle conditional independence testing on discrete or categorical variables? The method relies on differentiable gradients, restricting direct application to continuous domains.

## Limitations

- Relies on iterative Langevin dynamics sampling, creating computational bottlenecks compared to single-step generative methods
- Theoretical analysis assumes perfect score estimation in the infinite limit, leaving finite-sample behavior theoretically under-constrained
- Current framework is restricted to continuous variables due to reliance on differentiable score functions and gradients

## Confidence

**High:** The core methodology of using score-based generative modeling for CI testing is theoretically sound, with strong grounding in score matching and Langevin dynamics.

**Medium:** Empirical superiority claims are supported by experiments but lack formal statistical comparisons across multiple datasets and missing significance tests.

**Low:** Practical reproducibility is limited due to underspecified hyperparameters (bootstrap samples B) and thresholds (goodness-of-fit p-value cutoff) that are critical for implementation.

## Next Checks

1. Determine and document the exact number of bootstrap samples B used for p-value computation across all experiments.
2. Specify the Goodness-of-Fit p-value threshold for accepting generated samples in the pipeline.
3. Conduct statistical significance tests (e.g., paired t-tests) comparing test power across multiple datasets to validate "significant improvement" claims.