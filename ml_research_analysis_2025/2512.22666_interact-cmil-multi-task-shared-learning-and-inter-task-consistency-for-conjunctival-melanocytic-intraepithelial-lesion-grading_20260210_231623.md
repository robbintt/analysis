---
ver: rpa2
title: 'INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival
  Melanocytic Intraepithelial Lesion Grading'
arxiv_id: '2512.22666'
source_url: https://arxiv.org/abs/2512.22666
tags:
- cmil
- spread
- learning
- who4
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'INTERACT-CMIL addresses the challenge of accurate and reproducible
  grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL), a critical step
  for predicting melanoma risk and guiding treatment. The framework introduces a multi-head
  deep learning architecture built on top of the CHIEF pathology foundation model,
  which jointly predicts five interrelated histopathological criteria: WHO4 and WHO5
  classifications, horizontal spread, vertical spread, and cytologic atypia.'
---

# INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading

## Quick Facts
- arXiv ID: 2512.22666
- Source URL: https://arxiv.org/abs/2512.22666
- Reference count: 0
- Multi-task deep learning framework for CMIL grading achieving up to 55.1% relative macro F1 gain over baselines

## Executive Summary
INTERACT-CMIL addresses the challenge of accurate and reproducible grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL), a critical step for predicting melanoma risk and guiding treatment. The framework introduces a multi-head deep learning architecture built on top of the CHIEF pathology foundation model, which jointly predicts five interrelated histopathological criteria: WHO4 and WHO5 classifications, horizontal spread, vertical spread, and cytologic atypia. It employs a novel selective multi-head supervision strategy, training only three heads per iteration to improve regularization, and incorporates an inter-dependence loss that enforces consistency across related diagnostic tasks by modeling their empirical correlations. Evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three German hospitals, INTERACT-CMIL consistently outperforms both CNN and single-task foundation model baselines, achieving relative macro F1 gains up to 55.1% for WHO4 and 25.0% for vertical spread. The method delivers coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and advancing the standardization of digital ocular pathology.

## Method Summary
INTERACT-CMIL is a multi-task deep learning framework for grading Conjunctival Melanocytic Intraepithelial Lesions. It uses a frozen CHIEF pathology foundation model as a feature encoder, followed by shared MLP layers and five task-specific classification heads for WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia. The framework employs selective multi-head supervision (training only 3 of 5 heads per iteration) and an inter-dependence loss that enforces consistency across tasks by modeling empirical label correlations via KL divergence. The model is trained on 486 expert-annotated conjunctival biopsy patches from three German hospitals using patient-disjoint 5-fold cross-validation, with macro F1 as the primary metric.

## Key Results
- INTERACT-CMIL achieves relative macro F1 gains up to 55.1% for WHO4 and 25.0% for vertical spread compared to CNN baselines
- Performance improvements are consistent across all five grading criteria (WHO4, WHO5, HS, VS, AP) with gains ranging from 4.1% to 16.2%
- The framework demonstrates robust generalization across three clinical centers with patient-disjoint evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective multi-head supervision improves regularization and prevents gradient domination across tasks of varying difficulty.
- **Mechanism:** Training only 3 of 5 heads per iteration forces the shared representation to encode features useful across diverse task combinations, rather than overfitting to any single task's gradient signal. Systematic cycling through all C(5,3)=10 combinations ensures uniform exposure.
- **Core assumption:** Tasks share underlying morphological features; partial supervision acts as implicit data augmentation in task space.
- **Evidence anchors:**
  - [abstract] "selective multi-head supervision strategy, training only three heads per iteration to improve regularization"
  - [section 3.3.4] "Selective head updates thus prevent gradient domination, improving balance between easier (HS) and harder (VS, AP) tasks and acting as a strong regularizer"
  - [corpus] "Selective Task Group Updates for Multi-Task Optimization" addresses similar negative transfer concerns, though specific implementation differs
- **Break condition:** If tasks are truly independent with minimal shared features, selective supervision provides no benefit over full supervision.

### Mechanism 2
- **Claim:** Inter-task dependency regularization enforces clinically coherent predictions by aligning model outputs with empirical co-occurrence statistics.
- **Mechanism:** The KL-divergence loss between predicted joint distributions (⊗ of softmax outputs) and empirical joint distributions (from ground-truth labels) penalizes predictions that violate known diagnostic correlations (e.g., severe atypia with increased vertical spread).
- **Core assumption:** Ground-truth labels accurately capture real diagnostic dependencies; the batch-level statistics approximate population-level priors.
- **Evidence anchors:**
  - [abstract] "inter-dependence loss that enforces consistency across related diagnostic tasks by modeling their empirical correlations"
  - [section 2.3] "L_dep = Σ P(c) log[P(c)/P̂(c)+ε]" aligns predicted and empirical joint distributions
  - [section 3.3.2] "modeling task correlations improves label coherence and high-grade lesion detection"
  - [corpus] Weak direct evidence; related multi-task works focus on gradient manipulation rather than distributional alignment
- **Break condition:** If batch size is too small to reliably estimate joint distributions, or if label noise corrupts empirical priors, the loss introduces incorrect biases.

### Mechanism 3
- **Claim:** Pathology-pretrained foundation model embeddings transfer more effectively to CMIL grading than ImageNet-pretrained CNNs trained from scratch.
- **Mechanism:** CHIEF's pretraining on diverse whole-slide images captures histomorphologic primitives (cell structures, tissue patterns, staining variations) that are directly relevant to CMIL assessment, reducing the representation learning burden on the small 486-sample dataset.
- **Core assumption:** Histopathological features learned from large-scale cancer WSIs transfer to ocular conjunctival tissue despite domain shift.
- **Evidence anchors:**
  - [section 3.3.1] "Using pathology-pretrained features (BaseCHIEF) provides a substantial boost over ResNet-18 CNN trained from scratch, with mean macro F1 increasing by over 20% across tasks"
  - [section 2.1] CHIEF described as "pretrained on diverse whole-slide images to capture transferable histomorphologic representations"
  - [corpus] General foundation model transfer principles supported, but no corpus evidence specifically validates ocular pathology transfer
- **Break condition:** If target domain (conjunctival tissue) differs fundamentally from pretraining domain (general cancer pathology), transfer gains diminish significantly.

## Foundational Learning

- **Concept: Multi-Task Learning with Shared Representations**
  - **Why needed here:** CMIL grading requires simultaneous prediction across 5 interrelated criteria that share morphological features. Single-task models ignore task correlations and produce potentially inconsistent predictions.
  - **Quick check question:** Can you explain why training five independent models might produce clinically implausible combinations (e.g., high atypia but zero vertical spread)?

- **Concept: Foundation Model Transfer in Computational Pathology**
  - **Why needed here:** With only 486 annotated samples from a rare disease, training from scratch is infeasible. CHIEF provides pretrained visual representations of tissue morphology.
  - **Quick check question:** What is the difference between frozen feature extraction vs. end-to-end fine-tuning, and why might freezing be preferred for small datasets?

- **Concept: Regularization via Partial Supervision**
  - **Why needed here:** Preventing overfitting on small datasets requires explicit regularization. Selective head training acts as implicit regularization by preventing any single task from dominating gradient updates.
  - **Quick check question:** How does training 3-of-5 heads per iteration differ from dropout in terms of what is being "dropped"?

## Architecture Onboarding

- **Component map:**
  Input: H&E patch (224×224×3 assumed)
  -> CHIEF encoder (frozen) → 768-D feature vector
  -> Shared MLP layers (trainable) → 256-D latent representation
  -> 5 task-specific heads (trainable):
    - WHO4 (4 classes)
    - WHO5 (3 classes)
    - Horizontal Spread (4 classes)
    - Vertical Spread (3 classes)
    - Atypia (3 classes)
  -> Loss: L_cls (cross-entropy on 3 active heads) + λ·L_dep (KL divergence)

- **Critical path:** The 256-D shared representation is the bottleneck where all task-relevant features must be encoded. If this layer is too small, task interference increases; if too large, overfitting risk rises.

- **Design tradeoffs:**
  - **Frozen vs. fine-tuned encoder:** Paper freezes CHIEF. Fine-tuning could improve domain adaptation but risks overfitting with 486 samples.
  - **Number of active heads (k=3):** Smaller k increases regularization but reduces gradient signal per iteration. Ablation does not test k=2 or k=4.
  - **λ weighting:** Paper does not report sensitivity analysis for dependency loss weight.

- **Failure signatures:**
  - If WHO4/WHO5 predictions frequently contradict (e.g., WHO4=benign, WHO5=malignant), inter-task loss weight λ may be too low.
  - If performance on one task dramatically exceeds others, selective supervision may not be cycling properly.
  - If BaseCNN and BaseCHIEF converge to similar performance, CHIEF features are not activating—check preprocessing (normalization, resolution).

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train BaseCNN, BaseCHIEF, and full INTERACT-CMIL on provided splits. Verify macro F1 gains are reproducible (±2% of reported values).
  2. **Ablation validation:** Remove each component (dep., temp., sel.) independently to confirm their individual contributions match reported ~3-4% each.
  3. **Hyperparameter sensitivity:** Test k∈{2,3,4} for selective supervision and λ∈{0.1,0.5,1.0} for dependency loss on validation set to identify optimal configuration.

## Open Questions the Paper Calls Out
- How can INTERACT-CMIL be effectively adapted for whole-slide image (WSI) analysis without losing the spatial granularity of patch-level grading? The current framework operates exclusively on pre-cropped biopsy patches (486 samples), whereas clinical practice requires analyzing entire slides where lesion localization and context are variable.
- To what extent can domain adaptation and self-supervised pretraining improve the model's robustness to staining variations across different clinical sites? While the current study uses a multi-center dataset, the heavy reliance on a frozen foundation model (CHIEF) may not fully address specific domain shifts in ocular pathology staining protocols.
- Does the reliance on expert-consensus ground truth obscure the model's ability to capture the inter-observer variability inherent in CMIL grading? By treating consensus labels as absolute ground truth, the model may fail to learn the distribution of valid expert disagreements, potentially limiting its utility as a decision-support tool for borderline cases.

## Limitations
- Empirical joint distribution estimation depends heavily on batch size; with small batches, KL divergence estimates may be noisy, potentially degrading rather than improving prediction coherence.
- The paper does not report hyperparameter sensitivity (λ, k, learning rate), leaving performance reproducibility uncertain without extensive tuning.
- No ablation on frozen vs. fine-tuned CHIEF encoder prevents assessment of whether foundation model transfer is truly responsible for gains or if architectural novelty alone suffices.

## Confidence
- **High:** Multi-task framework architecture and selective supervision mechanism are clearly described and implemented.
- **Medium:** Reported F1 gains over baselines are plausible given foundation model pretraining, but exact magnitude depends on unreported training details.
- **Low:** Claims about inter-task consistency improving clinical coherence rely on empirical correlations that may not generalize beyond the specific dataset.

## Next Checks
1. Test L_dep stability across batch sizes (B=8,16,32) to verify KL divergence estimates remain reliable and beneficial.
2. Conduct hyperparameter sweep for λ∈{0.1,0.5,1.0} and k∈{2,3,4} to identify optimal selective supervision configuration.
3. Compare frozen vs. fine-tuned CHIEF encoder to quantify true contribution of foundation model transfer versus architectural design.