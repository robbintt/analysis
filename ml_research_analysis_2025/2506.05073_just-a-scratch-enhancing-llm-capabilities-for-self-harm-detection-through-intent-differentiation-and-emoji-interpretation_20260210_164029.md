---
ver: rpa2
title: 'Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through
  Intent Differentiation and Emoji Interpretation'
arxiv_id: '2506.05073'
source_url: https://arxiv.org/abs/2506.05073
tags:
- self-harm
- emoji
- emojis
- posts
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of detecting self-harm expressions
  in social media by focusing on intent differentiation (casual mentions vs. serious
  intent) and contextual emoji interpretation.
---

# Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation

## Quick Facts
- arXiv ID: 2506.05073
- Source URL: https://arxiv.org/abs/2506.05073
- Reference count: 23
- Multi-task LLM framework with CESM-100 emoji contextualization achieves 0.88 F1 for self-harm detection

## Executive Summary
This paper addresses self-harm detection on social media by focusing on intent differentiation and contextual emoji interpretation. The authors introduce CESM-100, a curated matrix mapping 100 emojis to self-harm-specific meanings with casual mention and serious intent probabilities. They develop SHINES, a dataset with detailed annotations for self-harm classification, intent span extraction, and rationale generation. A unified multi-task learning framework fine-tunes large language models to jointly optimize these objectives, demonstrating significant performance improvements over single-task and zero-shot approaches.

## Method Summary
The method employs a multi-task learning framework fine-tuning decoder-only LLMs (Llama 3, Mental-Alpaca, MentalLlama) on SHINES dataset with three objectives: binary self-harm classification, span extraction for casual mentions and serious intent, and rationale generation. CESM-100 emoji interpretations are injected into prompts to provide contextual meaning. Training uses binary cross-entropy for classification, sparse categorical cross-entropy for span extraction, and sequence-to-sequence prompting for rationales. The model is trained for 4 epochs with AdamW optimization, learning rate 4e-5, and batch size 16.

## Key Results
- Multi-task Llama 3 achieves 0.88 F1 for self-harm classification vs. 0.86 single-task
- CESM-100 integration improves classification F1 by 2-4 percentage points
- Ablation shows 2.5% F1 drop when emoji positions are randomized
- Model struggles with posts mixing self-harm references and recovery language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CESM-100 emoji contextualization improves self-harm detection by disambiguating intent signals.
- Mechanism: The matrix maps 100 emojis to contextual meanings in self-harm contexts with explicit CM (Casual Mention) and SI (Serious Intent) probability ratings, enabling the model to resolve otherwise ambiguous language-emoji combinations.
- Core assumption: Emoji meaning is context-dependent and standard emoji interpretations inadequately capture self-harm-specific semantics.
- Evidence anchors:
  - [abstract] "CESM-100â€”curated set of 100 emojis with contextual self-harm interpretations"
  - [section] Ablation study shows Llama 3 SHC F1 drops from 0.88 with CESM-100 to 0.84 without (Table 9)
  - [section] Noise experiments: 2.5% F1 drop when emoji positions or identities are randomized

### Mechanism 2
- Claim: Multi-task joint training on classification + span extraction + rationale generation creates shared representations that improve each individual task.
- Mechanism: Auxiliary span extraction forces the model to attend to fine-grained intent indicators, while rationale generation reinforces coherent reasoning chainsâ€”both providing supervisory signals that regularize the primary classification task.
- Core assumption: Task gradients are sufficiently aligned that shared learning does not induce negative transfer.
- Evidence anchors:
  - [abstract] "unified multi-task learning framework to jointly optimize self-harm detection, span extraction, and rationale generation"
  - [section] Multitask Llama 3 achieves 0.88 F1 vs. 0.86 in single-task with CESM-100 (Ablation 2)

### Mechanism 3
- Claim: Explicit span-level CM/SI annotations enable the model to learn boundary patterns between casual hyperbole and genuine distress.
- Mechanism: By training on token-level spans marking casual mentions versus serious intent, the model learns distinct syntactic and semantic patterns associated with each, reducing false positives on sarcasm/irony.
- Core assumption: CM/SI distinctions are consistently observable at the span level rather than only at document level.
- Evidence anchors:
  - [section] Dataset annotation: CM F1 agreement 0.66, SI F1 agreement 0.69 (Section 3.2)
  - [section] Qualitative analysis (Table 10) shows models differ on posts mixing self-harm references with recovery language

## Foundational Learning

- Concept: **Multi-task learning with auxiliary sequence labeling**
  - Why needed here: The framework combines classification with span extraction; understanding how shared encoders benefit from multiple objectives is essential.
  - Quick check question: Can you explain why adding an auxiliary token-classification loss might improve a sentence-level classifier?

- Concept: **Emoji semantics and pragmatic context**
  - Why needed here: CESM-100 assumes emoji meaning varies by discourse context; practitioners must understand that emoji interpretation is not fixed.
  - Quick check question: How might the ðŸ™ƒ emoji function differently in a casual venting post versus a serious distress signal?

- Concept: **Explainability vs. interpretability distinction**
  - Why needed here: The paper explicitly distinguishes rationale generation (post-hoc explanations) from model interpretability.
  - Quick check question: What is the difference between generating a rationale for a prediction versus explaining internal model activations?

## Architecture Onboarding

- Component map:
  - Emoji extraction from raw posts -> CESM-100 lookup -> enriched prompt construction -> LLM backbone -> Task heads (classification, span extraction, rationale generation) -> Loss aggregation

- Critical path:
  1. Emoji extraction from raw posts â†’ CESM-100 lookup â†’ enriched prompt construction
  2. Multi-task fine-tuning with balanced loss weighting
  3. Inference: classification â†’ span extraction â†’ rationale prompting (sequential, not parallel)

- Design tradeoffs:
  - **Decoder-only vs. encoder-decoder**: Authors chose decoder-only for contextual understanding, but this may limit span extraction precision compared to token-level encoders.
  - **Synthetic data augmentation**: 1000 synthetic posts improve balance but introduce distribution shift risk; annotators could distinguish synthetic from real at 58% F1.
  - **CESM-100 integration overhead**: Requires maintaining an external lookup; hardcoding interpretations may reduce adaptability.

- Failure signatures:
  - **Recovery-language confusion**: Posts mixing self-harm references with hopeful hashtags (e.g., #recoveryjourney) may be misclassified as non-self-harm by Mental-Alpaca/MentalLlama (Table 10, Post 2).
  - **Emoji noise sensitivity**: 2.5% F1 drop with emoji perturbation suggests over-reliance on emoji signals in some cases.
  - **Platform bias**: All data from Reddit; generalization to Twitter, Instagram, or clinical settings is unverified.

- First 3 experiments:
  1. **CESM-100 ablation with stratified emoji density**: Partition test set by emoji count (0, 1, 2+) to isolate CESM-100's contribution across emoji-rich vs. emoji-sparse posts.
  2. **Cross-platform validation**: Evaluate fine-tuned model on a held-out non-Reddit mental health corpus to measure domain shift.
  3. **Single-task vs. multi-task with frozen LLM**: Freeze backbone and train only task-specific adapters to disentangle representation sharing from joint optimization effects.

## Open Questions the Paper Calls Out

- **Question**: Does the integration of temporal behavioral patterns and user network interactions significantly enhance self-harm detection accuracy and rationale quality beyond text and emoji analysis?
  - Basis in paper: [explicit] The authors explicitly list "leveraging temporal behavioral patterns, user network interactions, and profile metadata" as a direction for future work to improve accuracy.
  - Why unresolved: The current framework focuses exclusively on static post content (text and emojis) without incorporating the dynamic, longitudinal, or social graph data of the users.
  - What evidence would resolve it: A comparative study evaluating the proposed LLM framework with and without temporal/network features on the same dataset.

- **Question**: Can the CESM-100 matrix and SHINES-based models generalize effectively to platforms with different communication norms, such as Twitter or Instagram?
  - Basis in paper: [explicit] The authors acknowledge "platform bias from Reddit data" and identify "expanding these datasets to other platforms" as a priority to address limitations in generalizability.
  - Why unresolved: The current study relies solely on Reddit data, which may have specific community standards, slang, or emoji usage patterns distinct from other social media environments.
  - What evidence would resolve it: Zero-shot or few-shot evaluation results of the fine-tuned models on out-of-domain datasets collected from non-Reddit platforms.

- **Question**: To what extent do the model's generated rationales align with clinical assessments of self-harm intent, and how can this alignment be validated by mental health professionals?
  - Basis in paper: [explicit] The limitations section states the resources "are not a substitute for clinical datasets" and that "future collaborations with clinical experts and institutions are essential."
  - Why unresolved: While the schema was validated by a psychiatrist, the model's outputs (predictions and explanations) have not been rigorously tested against clinical judgment in a real-world setting.
  - What evidence would resolve it: A qualitative and quantitative study where clinical experts rate the reliability and validity of the model's rationales compared to human clinical notes.

## Limitations

- **Domain Generalization**: Performance on non-Reddit platforms or clinical settings is unverified.
- **CESM-100 Maintenance**: Static emoji matrix may not adapt to evolving emoji usage patterns.
- **Synthetic Data Shift**: 58% annotator accuracy distinguishing synthetic from real posts indicates potential distribution mismatch.

## Confidence

**High Confidence**: Multi-task learning framework improves over single-task baselines (F1: 0.88 vs. 0.86 with CESM-100). Joint optimization providing shared representations is well-supported by ablation results.

**Medium Confidence**: CESM-100's contribution to performance (2-4% F1 improvement) is demonstrated, but long-term robustness across evolving emoji usage patterns is uncertain.

**Low Confidence**: Rationale quality claims rely on automated metrics rather than human judgment of explanation faithfulness. The distinction between plausible-sounding rationales and accurate reasoning chains remains unclear.

## Next Checks

1. **Cross-Platform Transfer Test**: Evaluate the fine-tuned model on a held-out mental health corpus from a different platform (e.g., Twitter or clinical forums) to measure domain shift and identify platform-specific failure modes.

2. **Emoji Evolution Simulation**: Simulate emoji usage changes by randomly replacing emojis in test posts with semantically similar but contextually different emojis. Measure performance degradation to estimate CESM-100's robustness window.

3. **Human Evaluation of Rationale Quality**: Conduct a blinded study where clinicians rate the faithfulness and utility of generated rationales versus ground-truth annotations, focusing on posts with mixed self-harm/recovery signals.