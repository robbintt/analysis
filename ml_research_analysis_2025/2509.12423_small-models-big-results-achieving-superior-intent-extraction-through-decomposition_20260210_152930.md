---
ver: rpa2
title: 'Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition'
arxiv_id: '2509.12423'
source_url: https://arxiv.org/abs/2509.12423
tags:
- user
- intent
- stage
- interaction
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors address the challenge of inferring user intent from
  sequences of mobile UI interactions, a task that large multi-modal models (MLLMs)
  handle reasonably well but smaller, on-device models struggle with due to limited
  capacity. They propose a two-stage decomposed approach: first, each individual interaction
  (screen and action) is summarized into structured components capturing context and
  user actions; second, a fine-tuned model aggregates these summaries into a concise
  intent description.'
---

# Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition

## Quick Facts
- arXiv ID: 2509.12423
- Source URL: https://arxiv.org/abs/2509.12423
- Reference count: 19
- Small models using a decomposed approach outperform both direct prompting and a large MLLM on intent extraction tasks

## Executive Summary
The authors address the challenge of inferring user intent from sequences of mobile UI interactions, a task that large multi-modal models (MLLMs) handle reasonably well but smaller, on-device models struggle with due to limited capacity. They propose a two-stage decomposed approach: first, each individual interaction (screen and action) is summarized into structured components capturing context and user actions; second, a fine-tuned model aggregates these summaries into a concise intent description. This method improves intent understanding in small models, enabling them to outperform both direct prompting and even a large MLLM on certain datasets. On Mind2Web, the approach boosts intent F1 scores from ~66% to ~75% for Gemini Flash 8B and from ~56% to ~62% for Qwen2 VL 7B, and even surpasses the large MLLM baseline. Ablation studies confirm the importance of context, structured summaries, fine-tuning, and label refinement. Error analysis reveals that most failures stem from the summarization stage, suggesting room for improvement. Overall, decomposition enables smaller models to deliver strong intent inference, supporting privacy-preserving, low-latency on-device deployment.

## Method Summary
The approach uses a two-stage decomposition: Stage 1 employs an MLLM to summarize each individual UI interaction into structured text (screen summary and user action), with context from adjacent interactions. Stage 2 fine-tunes a smaller model to aggregate these summaries into a final intent description. Training labels are refined to remove non-derivable information, reducing hallucination. The method addresses small models' context and reasoning limitations by breaking long trajectories into manageable atomic summaries before final aggregation.

## Key Results
- Small models using decomposition outperform both direct prompting and a large MLLM on Mind2Web intent extraction
- Intent F1 scores improve from ~66% to ~75% for Gemini Flash 8B and from ~56% to ~62% for Qwen2 VL 7B
- Label refinement during fine-tuning reduces hallucination and improves precision

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Attention via Temporal Decomposition
Breaking a long interaction trajectory into atomic summaries mitigates context limitations and reasoning errors common in small models handling long sequences. Instead of attending to a sequence of N screenshots and actions simultaneously, the model processes one interaction at a time, compressing visual noise into structured text tokens before aggregation. This reduces the "reasoning distance" between the first click and the final intent.

### Mechanism 2: Label-Input Alignment for Hallucination Reduction
Fine-tuning the aggregation model on labels cleaned of "non-derivable" information forces the model to rely on evidence rather than priors. By refining training labels to remove facts absent from the generated summaries, the model is conditioned to extract rather than imagine, directly reducing hallucination rates.

### Mechanism 3: Local Context Injection for Disambiguation
Providing the immediate previous and next interactions during the summarization of step I_i resolves visual ambiguity without requiring full-trajectory context. A single screenshot often lacks the causal signal for an action; seeing the "after" state helps confirm the result of the action, while the "before" state clarifies the navigation context.

## Foundational Learning

- **Context Window vs. Effective Context**: Small models (8B params) technically fit long sequences but fail to reason over them effectively (the "lost in the middle" phenomenon). This architecture trades context size for reasoning depth. *Quick check*: Can you explain why a 32k context window doesn't guarantee a small model can synthesize a story across 30 pages?

- **Multi-Modal Information Bottlenecks**: The Stage 1 summarization acts as a bottleneck, converting pixels to text. Understanding compression loss is vital; if the summary omits the "price" of an item, Stage 2 cannot recover it. *Quick check*: If a user clicks a "Buy" button, is the *color* of the button relevant to the intent summary? How do you decide what to keep?

- **Semantic Equivalence Metrics (NLI/BiFact)**: Exact string match fails here. You must understand how NLI (entailment) and BiFact (atomic fact decomposition) measure whether "Booking a flight" is semantically equivalent to "Reserving an airplane ticket." *Quick check*: Why does the paper prefer BiFact over BLEU/ROUGE for evaluating intent extraction?

## Architecture Onboarding

- **Component map**: Input Pre-processor -> Stage 1 (Summarizer MLLM) -> Stage 2 (Aggregator fine-tuned small model) -> Evaluator

- **Critical path**: The Label Refinement step in the training pipeline. If you train Stage 2 on raw dataset labels, the model learns to hallucinate user motivations that aren't visible in the UI.

- **Design tradeoffs**: 
  - Latency vs. Accuracy: Decomposition adds 2-3x computational cost/latency over baseline CoT. The "latency-optimized" variant skips summarizing the final screen.
  - Structure vs. Freedom: Enforcing structured summaries reduces hallucinations but might miss holistic "vibes" a free-form summary might catch.

- **Failure signatures**:
  - Stage 1 Omission: The intent is "Book flight to NYC," but Stage 1 summary fails to mention "NYC" in the text. Stage 2 cannot recover it.
  - Stage 2 Hallucination: The model adds constraints not in the summary (e.g., "cheapest flight") because it learned a prior from the training distribution.

- **First 3 experiments**:
  1. Baseline Context test: Run CoT on the small model with the full trajectory vs. your decomposed approach to confirm the "lost in the middle" degradation.
  2. Label Ablation: Fine-tune Stage 2 with *raw* labels vs. *refined* labels and measure the BiFact Precision drop to quantify hallucination.
  3. Context Window test: Run Stage 1 with I_i only vs. (I_{i-1}, I_i, I_{i+1}) to quantify the value of the sliding window.

## Open Questions the Paper Calls Out

### Open Question 1
Can specialized training or distillation methods be developed to reduce the high rate of fact omission in the Interaction Summarization stage? The authors identify the first stage as the primary source of error (responsible for a 16% loss of ground truth facts), but initial attempts to improve it via distillation were ineffective.

### Open Question 2
How do the latency and accuracy of this screenshot-based approach compare to video-based (UI-JEPA) and local text-based (SummAct) input modalities? While the paper contrasts its method with SummAct and UI-JEPA, it could not empirically compare them due to unavailable code and licensing restrictions.

### Open Question 3
Does the decomposed approach maintain robustness when applied to complex, multi-app user trajectories that involve dynamic goal shifts? The current evaluation does not cover cross-application workflows or non-linear user behavior, which may challenge the fixed context window and summarization structure.

## Limitations
- Stage 1 summarization quality is a hard bottleneck; errors cascade irreversibly to Stage 2
- Label refinement assumes an oracle LLM that accurately identifies visually derivable facts
- Dataset bias and real-world generalization concerns; results may not hold for noisier, more complex trajectories
- Computational cost characterization is incomplete; detailed runtime measurements are lacking

## Confidence
- **High Confidence**: The core decomposition architecture is technically sound and empirical improvements on Mind2Web are robust
- **Medium Confidence**: The claim that small models can "outperform" a large MLLM is valid on tested datasets, but the comparison configuration needs clarification
- **Low Confidence**: The specific mechanism by which label refinement reduces hallucination is not fully explained or rigorously established

## Next Checks
1. Conduct a detailed error propagation analysis to quantify how many final intent errors originate from Stage 1 summarization failures versus Stage 2 reasoning failures
2. Create a human-annotated test set to validate the accuracy and consistency of the automated label refinement process
3. Evaluate the trained decomposition model on a held-out dataset of mobile interactions from a different source to test generalization beyond the training distribution