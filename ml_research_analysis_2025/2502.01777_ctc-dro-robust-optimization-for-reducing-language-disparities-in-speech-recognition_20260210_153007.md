---
ver: rpa2
title: 'CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition'
arxiv_id: '2502.01777'
source_url: https://arxiv.org/abs/2502.01777
tags:
- group
- language
- ctc-dro
- languages
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of language disparities in multilingual
  speech recognition, where performance varies substantially between languages. The
  authors propose CTC-DRO, a robust optimization method that extends group DRO to
  handle the unique challenges of connectionist temporal classification (CTC) losses
  in speech recognition.
---

# CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition

## Quick Facts
- **arXiv ID**: 2502.01777
- **Source URL**: https://arxiv.org/abs/2502.01777
- **Reference count**: 40
- **Primary result**: CTC-DRO reduces worst-language CER by up to 47.1% compared to standard CTC while maintaining minimal computational overhead

## Executive Summary
This paper addresses the problem of language disparities in multilingual speech recognition, where performance varies substantially between languages. The authors propose CTC-DRO, a robust optimization method that extends group DRO to handle the unique challenges of connectionist temporal classification (CTC) losses in speech recognition. CTC-DRO introduces two key innovations: length-matched batching to mitigate CTC's scaling issues and a smoothed maximization objective to prevent overemphasis on consistently high-loss groups. The method is evaluated on multilingual ASR across five diverse language sets from ML-SUPERB 2.0, using both balanced and unbalanced training data. CTC-DRO consistently outperforms both baseline CTC models and group DRO, reducing the worst-language error rate by up to 47.1% and the average error by up to 32.9%, while maintaining minimal computational overhead.

## Method Summary
CTC-DRO extends group distributionally robust optimization to CTC-based speech recognition by addressing two key challenges: CTC loss scaling with input length and weight collapse in group DRO. The method implements length-matched batching where each batch contains samples from a single language group with total audio duration matching a target (~50 seconds), ensuring comparable loss magnitudes across groups. A smoothed weight update mechanism prevents any single group from accumulating disproportionate weight when it has consistently high losses. During training, losses are accumulated per group, weights are updated using the smoothed objective, and gradients are computed with weights scaled by |G| × q_g. The approach maintains computational efficiency while improving worst-group performance.

## Key Results
- CTC-DRO reduces worst-language CER by up to 47.1% compared to standard CTC baseline across 14 language-set combinations
- The method improves average CER by up to 32.9% while also improving worst-language performance in all 14 settings
- Both length-matching and smoothed maximization components are essential, with smoothing showing larger impact (171.6% worst-language CER increase when removed)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CTC loss scales with input length, causing group losses to be incomparable across languages with different utterance duration distributions.
- **Mechanism**: Standard CTC loss marginalizes over all valid alignments of length D. Longer sequences have more possible alignments but lower per-alignment probabilities, typically resulting in higher total loss even with similar transcription quality. When groups (languages) differ in audio length distributions (e.g., Spanish has more long utterances in Figure 1), the group with longer audio receives disproportionately high CTC loss, leading group DRO to up-weight it regardless of actual downstream performance.
- **Core assumption**: Group DRO requires comparable loss magnitudes to correctly identify worst-performing groups.
- **Evidence anchors**:
  - [Section 2.3]: "This scaling behavior occurs because P_CTC(Y|X) is a marginalization over all valid alignments Z ∈ A(X,Y). Each alignment is a sequence of length D..."
  - [Section 3.1]: "Batches with a larger number of shorter audio samples tend to have a lower CTC loss per audio sample than batches with fewer, longer, audio samples."
  - [Appendix G]: Normalization experiments confirm that simple per-utterance scaling does not solve incomparability.

### Mechanism 2
- **Claim**: The smoothed maximization objective prevents any single group from accumulating disproportionate weight when it has consistently high losses.
- **Mechanism**: Standard group DRO updates weights via q_g ← q_g · exp(η_q L_g), causing exponential growth for groups with consistently high losses. CTC-DRO modifies this to q_g ← q_g · exp(η_q L_g / (q_g + α)). Division by current weight dampens updates to already-high-weighted groups, while α controls sensitivity. When two groups have similar losses but different weights, the lower-weighted group receives larger updates, reducing weight gaps over time.
- **Core assumption**: Consistently high-loss groups may have high irreducible loss components rather than represent actual underperformance.
- **Evidence anchors**:
  - [Section 3.2]: "As α→0, the update becomes increasingly more sensitive to the current group weight relative to the group loss. This causes groups with higher weights to receive smaller updates."
  - [Section 6.2, Figure 2]: Group DRO weights fluctuate dramatically and concentrate on single languages; CTC-DRO weights remain distributed across languages.
  - [Table 3]: Removing smoothed maximization (-SMOOTH) increases worst-language CER by up to 171.6%.

### Mechanism 3
- **Claim**: Length-matched batching and smoothed maximization together stabilize training and maintain all groups in the optimization landscape.
- **Mechanism**: Length-matching ensures each group contributes roughly equal total audio duration to its loss computation (summing rather than averaging losses). Smoothing prevents weight collapse. Together, these keep all group weights above zero throughout training, ensuring gradient updates account for all languages rather than optimizing for a single dominant group.
- **Core assumption**: Worst-group performance improves when all groups remain active in training.
- **Evidence anchors**:
  - [Section 6.2]: "For extended periods of training with group DRO, the group weights are heavily concentrated on a single language, causing the weights for all other languages to reach values close to 0."
  - [Table 1]: CTC-DRO improves worst-language CER in all 14 settings while group DRO worsens it in 7 settings.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**
  - **Why needed here**: Understanding why CTC loss scales with sequence length is essential to grasping why standard group DRO fails. Without this, the length-matching mechanism appears arbitrary.
  - **Quick check question**: Given two identical transcripts, why would a 10-second audio clip typically have higher CTC loss than a 5-second clip?

- **Group Distributionally Robust Optimization**
  - **Why needed here**: CTC-DRO builds directly on group DRO's minimax formulation and weight update mechanism. The paper assumes familiarity with the Hedge algorithm and the interpretation of group weights.
  - **Quick check question**: What does the group weight q_g represent in standard group DRO, and what happens when one group's q_g approaches 1?

- **Subpopulation Shift vs. Domain Generalization**
  - **Why needed here**: The paper frames multilingual ASR as a subpopulation shift problem where all test languages appear in training but performance varies. This distinguishes the approach from domain adaptation methods.
  - **Quick check question**: Why is this setting called "subpopulation shift" rather than "domain generalization"?

## Architecture Onboarding

- **Component map:**
  - Modified batch sampler -> Length-matched batching -> Single-group batches with ~50s total duration
  - Loss accumulator -> Track summed losses per group across batches -> Trigger weight update after all groups have losses
  - Smoothed weight updater -> Implement Equation 10 with η_q and α -> Normalize weights across groups
  - Weighted gradient descent -> Multiply loss by |G| × q_g -> Backpropagation to update model parameters

- **Critical path:**
  1. Initialize q_g = 1/|G| for all groups
  2. Sample group g, then sample batch from g with duration ≈ target
  3. Compute per-sample CTC losses, sum for group
  4. Accumulate until all groups have losses
  5. Apply smoothed weight update (Equation 10)
  6. Scale batch loss by |G| × q_g and update model parameters
  7. Repeat

- **Design tradeoffs:**
  - **Batch duration target**: Paper uses ≈50 seconds to fit GPU memory. Smaller targets (23s tested in Appendix F.3) work but show more variance, especially for XLS-R.
  - **α smoothing parameter**: {0.1, 0.5, 1.0} tested. Lower α = more uniform weights, higher α = closer to standard group DRO.
  - **η_q step size**: {10^-3, 10^-4} tested. Controls how quickly weights adapt to loss differences.

- **Failure signatures:**
  - **Weight collapse**: If single group weight exceeds 0.9 for extended training, α is too high or η_q too large.
  - **No improvement over baseline**: Check if length-matching is actually enforced (batch durations should be similar across groups).
  - **Average CER increases substantially**: α may be too low, forcing overly uniform weights that ignore actual performance differences.
  - **LID accuracy drops significantly**: Group weights may be fluctuating too rapidly; reduce η_q.

- **First 3 experiments:**
  1. **Baseline comparison**: Train standard CTC model with your data. Identify worst-performing language. Apply group DRO (should fail or degrade per Section 5). Apply CTC-DRO with α=0.5, η_q=10^-4.
  2. **Ablation study**: Train CTC-DRO without length-matching (use standard batch sampler), then without smoothing (use standard group DRO update). Compare worst-language CER to full CTC-DRO. Expect smoothing component to have larger impact (Table 3).
  3. **Hyperparameter sensitivity**: On a validation set, grid search α ∈ {0.1, 0.5, 1.0} and η_q ∈ {10^-3, 10^-4}. Plot worst-language CER vs. average CER to visualize the tradeoff curve.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can CTC-DRO be extended to automatically learn data groupings, removing the need for pre-defined groups that may be unknown or incomplete?
  - **Basis in paper**: [explicit] "A promising direction for future work is to automatically learn data groupings, which removes the need for pre-defined groups that may be unknown or incomplete"
  - **Why unresolved**: Current CTC-DRO requires explicit group definitions (languages), limiting applicability where subgroups are latent or unknown.
  - **What evidence would resolve it**: Demonstration of an unsupervised or weakly-supervised grouping mechanism that identifies performance-relevant subgroups and applies CTC-DRO effectively.

- **Open Question 2**: How effective is CTC-DRO for code-switching speech recognition where utterances contain multiple languages?
  - **Basis in paper**: [explicit] "Extending CTC-DRO to code-switching scenarios is another promising direction (e.g., see Liu et al., 2024)"
  - **Why unresolved**: Current formulation assigns single group labels per utterance; code-switching requires handling multiple groups within one sample.
  - **What evidence would resolve it**: Evaluation on code-switching benchmarks showing whether CTC-DRO reduces performance disparities compared to baselines.

- **Open Question 3**: Can CTC-DRO be successfully applied during pre-training rather than only fine-tuning?
  - **Basis in paper**: [explicit] "A promising direction for future work is...applying CTC-DRO to pre-training"
  - **Why unresolved**: Pre-training involves different scales and objectives; applicability of length-matched batching and smoothed maximization in this regime is untested.
  - **What evidence would resolve it**: Pre-trained models using CTC-DRO showing improved worst-group performance on downstream tasks.

- **Open Question 4**: Does the smoothed maximization objective generalize to non-CTC architectures such as attention-based or autoregressive models?
  - **Basis in paper**: [explicit] "The smoothed maximization objective could in principle be applied to any setting with group-level losses, suggesting potential extensions to other architectures, loss functions, and groupings"
  - **Why unresolved**: Smoothing was motivated by CTC-specific loss incomparability; whether the same issues arise with other loss functions remains unknown.
  - **What evidence would resolve it**: Experiments applying smoothed group DRO to transformer-based ASR or sequence models showing consistent worst-group improvements.

## Limitations

- The evaluation focuses on a single robust optimization framework (group DRO) and does not compare against alternative approaches for multilingual ASR such as adaptive batch normalization, language-specific fine-tuning schedules, or adversarial training methods.
- The results are limited to pre-trained multilingual encoders (XLS-R and MMS) without testing on encoders trained specifically for the ML-SUPERB language sets, which could confound conclusions about whether CTC-DRO addresses fundamental model limitations versus dataset-specific issues.
- The paper's claims about CTC-DRO's superiority rest on specific implementation details that are not fully specified, particularly the architecture of the added Transformer layers and audio preprocessing parameters.

## Confidence

- **High confidence**: The empirical improvements in worst-language CER (up to 47.1% reduction) and the ablation study showing the importance of both length-matching and smoothing mechanisms are well-supported by the experimental results.
- **Medium confidence**: The claim that CTC loss scaling with sequence length is the primary reason standard group DRO fails in CTC settings is theoretically sound but not conclusively proven - alternative explanations like different learning dynamics for different languages could contribute.
- **Low confidence**: The assertion that smoothed maximization is essential to prevent "weight collapse" is supported by Figure 2 and Table 3, but the exact mechanism by which α prevents this is not rigorously analyzed beyond empirical observation.

## Next Checks

1. **Ablation of preprocessing effects**: Train CTC-DRO and baseline models on datasets with explicitly balanced utterance lengths across languages (artificially truncated/padded) to isolate whether length-matching or other factors drive performance improvements.

2. **Alternative robust optimization comparison**: Implement and compare against the hierarchical ambiguity set DRO approach from the corpus (arXiv:2501.11402) to determine if CTC-DRO's specific innovations outperform other robust optimization methods in the CTC setting.

3. **Encoder pre-training analysis**: Train CTC-DRO on encoders specifically pre-trained on the ML-SUPERB language sets (rather than XLS-R/MMS) to distinguish whether improvements come from CTC-DRO's optimization or from better-matched pre-training data.