---
ver: rpa2
title: Accelerating Generative Recommendation via Simple Categorical User Sequence
  Compression
arxiv_id: '2601.19158'
source_url: https://arxiv.org/abs/2601.19158
tags:
- sequence
- user
- recommendation
- item
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational efficiency challenge in
  generative recommendation systems caused by long user sequences. The authors propose
  CAUSE (Categorical User Sequence Compression), a method that compresses long-term
  user histories into compact history tokens by leveraging inherent item categorical
  features.
---

# Accelerating Generative Recommendation via Simple Categorical User Sequence Compression

## Quick Facts
- arXiv ID: 2601.19158
- Source URL: https://arxiv.org/abs/2601.19158
- Reference count: 24
- Key outcome: Achieves up to 6× computational cost reduction while delivering up to 39% higher accuracy at comparable computational expense compared to HSTU baseline.

## Executive Summary
This paper addresses the computational inefficiency of generative recommendation systems when processing long user interaction sequences. The authors propose CAUSE (Categorical User Sequence Compression), a method that compresses long-term user histories into compact history tokens by leveraging inherent item categorical features. By partitioning user interactions into recent sequences and distant historical sequences, then grouping items by category and aggregating them into bucket-level embeddings, CAUSE significantly reduces the quadratic attention complexity while preserving user preference signals. Experiments on KuaiRand-27K and MovieLens-20M datasets demonstrate that CAUSE achieves substantial computational savings (up to 6×) with accuracy improvements (up to 39% higher) compared to state-of-the-art methods.

## Method Summary
CAUSE compresses long user interaction histories by grouping items into category-based buckets and aggregating them into single embeddings. The method partitions user sequences into recent interactions and historical sequences, then applies categorical bucketing to the historical portion. Items sharing the same category are grouped, and up to G most recent items from each category are preserved. These items are projected through learnable alignment weights and mean-pooled to create bucket embeddings. Only the top-V most recently interacted categories are retained, creating a compressed representation that reduces attention complexity from O(L²) to approximately O(V² + L_recent²). The approach is compatible with both HSTU and GenRank backbones and demonstrates strong scalability with up to 6× reduction in computational cost while maintaining or improving recommendation accuracy.

## Key Results
- Achieves up to 6× reduction in computational cost compared to HSTU baseline
- Delivers up to 39% higher accuracy (N@10) at comparable computational expense
- Shows strong scalability with up to 8× improvement in sequential inference and 3.4× in parallel inference
- Works effectively with both HSTU and GenRank backbones
- Maintains effectiveness even when using K-Means clustering to generate categorical features instead of inherent metadata

## Why This Works (Mechanism)

### Mechanism 1: Categorical Bucketing for Computational Reduction
Partitioning historical items into category-based buckets reduces the quadratic attention cost while preserving semantic user preferences. Items sharing the same category are grouped into buckets, then aggregated into single embeddings via mean pooling with learnable alignment projections. This transforms a history of length L_history into at most V bucket tokens where V << L_history, reducing attention complexity from O(L²) toward O(V² + L_recent²). The core assumption is that items within the same category carry redundant preference signal that can be summarized without critical information loss.

### Mechanism 2: Recency-Prioritized Dual Selection
Selecting buckets by recency of their most recent interaction, and items within buckets by timestamp, preserves temporal preference dynamics. Buckets are ordered by the timestamp of their most recent interaction; only top-V are retained. Within each bucket, only the G most recent items contribute to aggregation. This creates a two-level recency filter that favors recent categories while keeping granular recency within categories. The core assumption is that user preferences exhibit both category-level persistence and recency-weighted relevance.

### Mechanism 3: Learnable Alignment Projection for Bucket Semantics
Projecting item embeddings into a bucket-aligned semantic space before aggregation enables the model to learn category-level representations beyond simple averaging. Each item embedding passes through learnable W_align and b_align before mean pooling with the bucket embedding. This allows the model to emphasize dimensions relevant to category-level preference modeling. The core assumption is that raw item embeddings are not optimally aligned for bucket-level aggregation.

## Foundational Learning

- **Attention Complexity in Transformers**: Understanding O(H L² D) vs O(H L D² r) explains why sequence length dominates cost when D is small (typically 64 in recommendation), making compression critical. Quick check: Given D=64, L=2048, H=3, r=4, which term dominates: self-attention or FFN?

- **Contrastive Learning (InfoNCE Loss)**: The next-item prediction task uses InfoNCE with negative sampling; understanding how negative samples shape the embedding space is essential for debugging ranking failures. Quick check: If all negative samples are from the same category as the positive, what failure mode emerges?

- **Scaling Laws in Recommendation**: The paper builds on findings that longer sequences improve generative recommenders; compression trades sequence length for computational budget while attempting to preserve signal. Quick check: If CAUSE reduces effective sequence length by 6× but claims accuracy gains, what must the compressed tokens preserve that raw length alone does not guarantee?

## Architecture Onboarding

- **Component map**: User ID embedding -> Compressed history tokens (V buckets) -> Recent sequence (items + actions flattened) -> Causal Transformer (HSTU or GenRank) -> Next-item prediction (InfoNCE) + Action prediction (softmax cross-entropy)

- **Critical path**: 1. Preprocess: Split user sequence into history (h_u) and recent (r_u) based on temporal cutoff 2. Compress: Apply CAUSE to h_u using category metadata → V bucket embeddings 3. Concatenate: [user_id_token, history_tokens, recent_sequence_tokens] 4. Forward: Pass through backbone Transformer 5. Loss: L_item + L_action (if action prediction enabled)

- **Design tradeoffs**: 
  - V (max buckets): Higher V retains more category diversity but increases compute; paper uses V=8
  - G (max items per bucket): Higher G preserves more intra-category recency detail; paper uses G=32
  - History vs recent split point: Determined by iLen; longer recent sequences improve accuracy but reduce compression benefit
  - Inherent vs K-Means categories: Inherent categories outperform K-Means (Table 3), but K-Means enables deployment without metadata

- **Failure signatures**:
  - Accuracy below baseline with compression: Likely V or G too small, or category metadata noisy/missing
  - No speedup despite compression: Check that sLen (input sequence length to backbone) is actually reduced
  - Training instability after adding alignment weights: Reduce learning rate or add gradient clipping

- **First 3 experiments**:
  1. Sanity check without compression: Run HSTU baseline at iLen=64, 256, 512 on KuaiRand-27K; verify N@10 values approximately match Table 1 rows (a), (c), (d)
  2. Compression ablation by V and G: Fix iLen=512, sweep V ∈ {2, 4, 8, 16} and G ∈ {8, 16, 32, 64}; plot N@10 vs inference time
  3. Category source comparison: Compare inherent categories (MusicType/Genres) vs K-Means (k=8, 16, 32) on held-out users

## Open Questions the Paper Calls Out

### Open Question 1
How can optimal category features be designed or learned for domains lacking inherent categorical metadata? The paper only compares raw metadata against basic K-Means clustering, leaving richer representation learning methods unexplored. Systematic comparison of learned category features (e.g., hierarchical clustering, graph-based groupings) against inherent categories across multiple datasets would resolve this.

### Open Question 2
What is the optimal strategy for selecting the number of buckets (V) and maximum items per bucket (G), and how sensitive is performance to these hyperparameters? The paper fixes V=8 and G=32 without ablation or theoretical justification. Ablation study varying V and G across datasets with adaptive selection based on user history length would provide clarity.

### Open Question 3
Can CAUSE generalize to real-time streaming scenarios where user histories evolve dynamically without full recompression? The paper measures batch inference time but not incremental updates. Evaluation of incremental update strategies and latency measurements in a simulated streaming environment would address this.

### Open Question 4
How does CAUSE perform on recommendation domains beyond micro-video and movies, such as e-commerce or news with different temporal dynamics? Experiments are limited to KuaiRand-27K and MovieLens-20M. Testing on additional datasets with different characteristics would validate cross-domain effectiveness.

## Limitations

- Evaluation limited to two datasets (KuaiRand-27K and MovieLens-20M) with relatively high average sequence lengths, potentially overstating real-world benefits
- No direct comparison with state-of-the-art long-sequence compression methods like MALLOC or LIME
- Assumes static historical sequences without addressing incremental updates for streaming scenarios
- Fixed hyperparameters (V=8, G=32) without systematic exploration of optimal settings across different dataset characteristics

## Confidence

**High Confidence**: Computational cost reduction claims (up to 6× speedup verified through FLOPs analysis and runtime measurements); accuracy improvements with compression enabled (N@10 gains of 39% validated across multiple iLen settings); scalability benefits demonstrated through sequential and parallel inference comparisons.

**Medium Confidence**: Effectiveness of learnable alignment projections (supported by ablation but not compared to alternative aggregation methods); K-Means clustering viability when inherent categories unavailable (tested but only on KuaiRand-27K); dual selection mechanism benefits (no ablation of recency-only vs bucket-only selection strategies).

**Low Confidence**: Generalization to datasets with different characteristics (short sequences, different category granularities, or missing category metadata); superiority over non-categorical compression methods (no direct comparisons provided); robustness to distributional shifts in user behavior patterns.

## Next Checks

1. **Direct Comparison with State-of-the-Art Compression**: Implement and evaluate CAUSE against MALLOC and LIME on the same datasets using identical backbones (HSTU/GenRank) and sequence lengths. Measure not just accuracy and speed, but also memory consumption and training stability to determine if CAUSE's categorical approach provides unique advantages or merely matches existing methods with different tradeoffs.

2. **Ablation of Selection Mechanisms**: Systematically disable either the bucket-level recency filter or the intra-bucket item selection (keeping all items within each bucket) to isolate the contribution of each selection layer. Run experiments at multiple V and G values to determine if the dual selection is additive or if one component dominates performance.

3. **Cross-Dataset Robustness Test**: Evaluate CAUSE on a dataset with fundamentally different characteristics from KuaiRand-27K and MovieLens-20M—specifically, one with shorter average sequences (e.g., <50 items), sparse category metadata, or different interaction patterns (e.g., sequential sessions vs continuous streams). This would test whether the method's benefits hold when the core assumptions about long sequences and rich categorical features are violated.