---
ver: rpa2
title: 'Resource for Error Analysis in Text Simplification: New Taxonomy and Test
  Collection'
arxiv_id: '2505.16392'
source_url: https://arxiv.org/abs/2505.16392
tags:
- errors
- simplification
- text
- source
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new taxonomy and test collection for error\
  \ analysis in automatic text simplification (ATS). The authors define four main\
  \ error categories\u2014fluency, alignment, information, and simplification\u2014\
  with 12 specific subtypes focusing on information distortion."
---

# Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection

## Quick Facts
- arXiv ID: 2505.16392
- Source URL: https://arxiv.org/abs/2505.16392
- Reference count: 25
- Primary result: Introduced taxonomy and test collection for ATS error detection; current models achieve AUPRC 0.24-0.51 on 2,659 annotated sentences

## Executive Summary
This paper introduces a comprehensive taxonomy and test collection for error analysis in automatic text simplification (ATS), addressing the gap between general factual consistency detection and ATS-specific error types. The authors define four main error categories—fluency, alignment, information, and simplification—with 12 specific subtypes focusing on information distortion. They annotated 2,659 sentences from scientific texts with these error labels, revealing high error rates (~69%) in LLM-generated simplifications. The test collection will support the SimpleText shared task at CLEF 2025, providing a benchmark for evaluating error detection models in the ATS domain.

## Method Summary
The authors created a test collection by annotating 2,659 sentences from scientific abstracts (medicine, telecoms, AI) that were simplified by LLMs. Five annotators labeled each instance with 13 binary labels (12 error types plus "No error"). They evaluated pre-trained error detection models (FactCC, LENS, FEQA, QAGS, FactAcc, BERTScore) as off-the-shelf classifiers against human annotations, reporting AUROC for binary error detection and AUPRC for specific error types. The dataset will be released post-CLEF 2025 at https://github.com/bVendeville/Salted in CSV format.

## Key Results
- 69.16% of simplified sentences contained at least one error across all categories
- FactCC achieved best binary AUROC (0.68) but AUPRC remained low (0.24-0.51) across error types
- Inter-annotator agreement varied widely: Fluency (κ=0.38-0.45), Alignment (κ=0.15-0.34), Information (κ=0.02-0.26), Simplification (κ=0.12-0.26)
- Most prevalent errors: Loss of Informative Content (19.56%), Out-of-Scope Generation (15.72%), Faithfulness Hallucination (13.54%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A structured error taxonomy improves detection of simplification failures by decomposing abstract "quality" into observable error categories.
- Mechanism: The taxonomy maps errors to four causal origins (fluency, alignment, information, simplification) using formal set operations on facts (F_src, F_gen, F_topic, F_true). This transforms subjective judgments into verifiable conditions—e.g., faithfulness hallucination = F_gen ∩ F_topic ∩ F_cont.
- Core assumption: Errors in ATS arise from distinct failure modes that can be categorized by their source (model, prompt, information handling, simplification logic).
- Evidence anchors:
  - [abstract]: "We propose a taxonomy of errors, with a formal focus on information distortion"
  - [section 2.4]: Formal definitions using set notation for fact-based error identification
  - [corpus]: Weak direct evidence; neighbor papers focus on readability constructs and datasets, not error taxonomies
- Break condition: If errors are highly correlated across categories (multicollinearity), the taxonomy adds labeling overhead without diagnostic value.

### Mechanism 2
- Claim: Human annotations reveal that current ATS systems produce errors at high rates (~69%), with information loss and overgeneralization being most prevalent.
- Mechanism: The 2,659-sentence test collection exposes real error distributions—Loss of Informative Content (19.56%), Out-of-Scope Generation (15.72%), Faithfulness Hallucination (13.54%)—enabling targeted model improvement.
- Core assumption: Annotators can reliably distinguish error types despite moderate inter-annotator agreement (κ = 0.02–0.45).
- Evidence anchors:
  - [section 3, Table 1]: Error distribution showing 69.16% error rate across categories
  - [section 3, Table 3]: Fleiss' Kappa scores (0.02–0.45) indicating moderate-to-low agreement
  - [corpus]: TGEA paper provides parallel evidence of error-annotated datasets for PLM evaluation
- Break condition: If low agreement reflects taxonomy ambiguity rather than task difficulty, error labels may not generalize.

### Mechanism 3
- Claim: Existing hallucination detection models (FactCC, QAGS, FEQA) show weak transfer to ATS error detection (AUPRC 0.24–0.51), indicating domain mismatch.
- Mechanism: Models trained on summarization or general factual consistency fail on ATS-specific errors like overgeneralization and content loss because these require understanding simplification intent, not just factual consistency.
- Core assumption: ATS errors are sufficiently distinct from general hallucination/factuality errors that specialized detection is needed.
- Evidence anchors:
  - [section 4.2.1, Table 4]: FactCC achieves highest AUROC (0.68) but AUPRC remains low (0.24–0.51)
  - [section 4.2.2]: "Despite these relative improvements, the overall performance remains low"
  - [corpus]: DETECT paper confirms general metrics (BLEU, BERTScore) insufficiently capture simplification quality
- Break condition: If poor performance stems from insufficient training data rather than domain mismatch, scaling annotations could close the gap.

## Foundational Learning

- Concept: **Factual consistency vs. faithfulness in generation**
  - Why needed here: The taxonomy distinguishes factuality hallucinations (contradicting world knowledge) from faithfulness hallucinations (contradicting source text)—critical for ATS where simplification may introduce true-but-unfaithful content.
  - Quick check question: If a simplified text replaces "patients over 50" with "adults," is this a factuality or faithfulness error?

- Concept: **Cohen's Kappa and Fleiss' Kappa for annotation reliability**
  - Why needed here: Interpreting whether κ = 0.38 (fluency) vs. κ = 0.02 (information) reflects taxonomy quality vs. intrinsic task difficulty determines whether to refine labels or accept subjectivity.
  - Quick check question: What does κ = 0.02 for Information errors imply about annotation feasibility?

- Concept: **AUPRC vs. AUROC for imbalanced error detection**
  - Why needed here: With error prevalence ranging from 0.86% (Contradiction) to 19.56% (Loss of Content), AUPRC better reflects detection performance on rare error types.
  - Quick check question: Why might AUROC = 0.68 (FactCC) still yield poor practical error detection?

## Architecture Onboarding

- Component map: Taxonomy layer (4 categories → 12 subtypes) → Annotation layer (2,659 sentences × 13 binary labels) → Evaluation layer (AUROC/AUPRC metrics) → Detection models (FactCC, LENS, QAGS/FEQA, FactAcc)

- Critical path: Define simplification goal → determines F_important for content relevance judgments → Extract facts from source/simplification → (subj, rel, obj) tuples → Apply set operations → classify into error categories → Aggregate annotations → handle annotator consistency (exclude Annotator C for higher quality)

- Design tradeoffs:
  - Fine-grained taxonomy: Better diagnostics vs. lower inter-annotator agreement
  - Set-based formalism: Reproducible definitions vs. dependency on fact extraction quality
  - Keeping low-consistency annotators: More data vs. noisier labels

- Failure signatures:
  - κ < 0.20 for a category → taxonomy refinement needed
  - AUPRC < 0.15 for rare errors → insufficient positive examples
  - BERTScore AUROC < 0.30 → metric unsuitable for error detection

- First 3 experiments:
  1. Replicate annotation on 50 sentences with 3 annotators to measure baseline agreement for your target domain
  2. Train FactCC-style classifier on the test collection using the paper's GitHub data; measure AUPRC gap vs. reported results
  3. Analyze confusion matrix between "No error" and each error category to identify systematic misclassifications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatic evaluation metrics be developed that reliably correlate with the proposed taxonomy's 12 specific error subtypes?
- Basis in paper: [explicit] Conclusion states: "Future work could explore automatic evaluation aligned with our taxonomy."
- Why unresolved: Current models (FactCC, LENS, FEQA, QAGS, FactAcc) show poor performance (AUPRC 0.24–0.51 even for the best model), and BERTScore achieves only 0.23 AUROC for binary error detection.
- What evidence would resolve it: Development of a model achieving consistently high AUPRC (>0.70) across all error subtypes on held-out test data.

### Open Question 2
- Question: Would improved annotator training protocols significantly increase inter-annotator agreement for Information (κ=0.02) and Simplification (κ=0.26) error categories?
- Basis in paper: [explicit] Conclusion states: "Future work could explore... improved annotator training."
- Why unresolved: Current agreement is near-chance for Information errors and only fair for Simplification; the paper notes "expertise significantly impacts the identification of certain error types."
- What evidence would resolve it: A controlled study comparing trained vs. untrained annotators showing statistically significant Kappa improvement (e.g., κ>0.40) for previously low-agreement categories.

### Open Question 3
- Question: Can synthetic data generation effectively augment test collections for rare error types (e.g., Contradiction at 0.86%, Factual hallucination at 0.86%)?
- Basis in paper: [explicit] Conclusion states future work could explore "the use of synthetic data to augment test collections"; [inferred] Table 5 notes "very low instance counts for certain error types... limit the robustness of the measure."
- What evidence would resolve it: Demonstration that synthetically-generated error examples maintain comparable annotation quality (inter-annotator agreement) and model detection patterns to naturally-occurring errors.

### Open Question 4
- Question: Does the taxonomy generalize to non-scientific text domains (e.g., news, legal documents) with different complexity profiles?
- Basis in paper: [inferred] The test collection derives exclusively from scientific abstracts; Section 2.5.1 notes that "importance" depends on simplification goals, suggesting domain-specific validation may be needed.
- Why unresolved: No experiments or discussion addresses whether the error definitions and their relative frequencies transfer to other text types.
- What evidence would resolve it: Cross-domain annotation study showing similar error distributions and inter-annotator agreement levels across at least two non-scientific domains.

## Limitations
- Low inter-annotator agreement (κ = 0.02-0.26) for Information and Simplification errors suggests these categories may be too subjective for reliable annotation
- The paper only evaluates pre-trained error detection models without training custom classifiers on the test collection
- Reliance on fact extraction for formal error definitions creates a potential failure point if fact extraction quality varies across domains

## Confidence
- High: The test collection's error distribution (69% error rate) and basic evaluation metrics are clearly reported and reproducible
- Medium: The taxonomy's formal definitions using set operations are theoretically sound, but practical applicability is uncertain given annotation challenges
- Low: Claims about FactCC being "best" are qualified by overall low performance (AUPRC 0.24-0.51), suggesting the evaluation may not fully validate the taxonomy's effectiveness

## Next Checks
1. Replicate the annotation task on 50 sentences with 3 annotators to measure baseline agreement for your specific domain, focusing on whether Information and Simplification categories remain problematic
2. Train a BERT-based classifier specifically on the test collection (using the GitHub data) rather than evaluating pre-trained models, to determine if custom training improves AUPRC scores beyond the reported 0.24-0.51 range
3. Analyze the correlation matrix between error categories to determine if multicollinearity exists, which would indicate whether the taxonomy provides independent diagnostic value or redundant labeling