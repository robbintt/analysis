---
ver: rpa2
title: Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows
arxiv_id: '2512.18595'
source_url: https://arxiv.org/abs/2512.18595
tags:
- error
- flame
- data
- mesh
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REALM is a benchmarking framework for evaluating neural surrogates
  on challenging, application-driven reactive multiphysics flows. It features 11 high-fidelity
  datasets spanning canonical problems to complex propulsion and fire safety scenarios,
  and provides a standardized end-to-end training and evaluation protocol.
---

# Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows

## Quick Facts
- arXiv ID: 2512.18595
- Source URL: https://arxiv.org/abs/2512.18595
- Reference count: 40
- Primary result: REALM benchmark reveals architectural inductive bias dominates performance over parameter count, and high nominal accuracy does not guarantee physically trustworthy predictions in reactive multiphysics flows.

## Executive Summary
REALM is a comprehensive benchmarking framework for evaluating neural surrogates on challenging, application-driven reactive multiphysics flows. It features 11 high-fidelity datasets spanning canonical problems to complex propulsion and fire safety scenarios, and provides a standardized end-to-end training and evaluation protocol. The benchmark systematically evaluates over a dozen representative surrogate model families, including spectral operators, convolutional models, Transformers, pointwise operators, and graph/mesh networks. Three robust trends are identified: (i) a scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors; (ii) performance primarily controlled by architectural inductive biases rather than parameter count; and (iii) a persistent gap between nominal accuracy metrics and physically trustworthy behavior, where models with high correlations still miss key transient structures and integral quantities.

## Method Summary
The REALM benchmark evaluates neural surrogates on 11 high-fidelity multiphysics datasets through autoregressive rollout prediction. Models are trained using short-horizon rollout (typically 1-2 steps) with backpropagation through the final step only, and optimized with AdamW using One-Cycle learning rate scheduling. Data undergoes Box-Cox transformation on species mass fractions followed by Z-score normalization. Models are evaluated using MSE on normalized variables (grouped by chemical species, temperature, density, velocity, and pressure) and Pearson correlation. The framework systematically compares capacity-aligned tiers (S/M/L) across diverse architectural families, including spectral (FFNO, FNO), convolutional (CNext), transformer (FactFormer, Transolver), pointwise (DeepONet), and graph/mesh networks (MGN, GNN).

## Key Results
- A scaling barrier emerges where rollout errors grow rapidly due to interactions between dimensionality, stiffness, and mesh irregularity
- Architectural inductive bias dominates performance over parameter count, with spectral/CNN excelling on regular grids while pointwise/graph models handle irregular meshes better
- High correlation metrics do not guarantee physically trustworthy predictions, as models miss critical transient structures despite statistical accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If neural surrogates are applied to realistic reactive flows, a scaling barrier emerges where rollout errors accumulate rapidly due to the interaction of dimensionality, stiffness, and mesh irregularity.
- **Mechanism:** Realistic multiphysics involves coupled PDE–ODE systems where chemical timescales ($10^{-12}$s) differ vastly from flow timescales. Standard surrogates struggle to resolve these stiff dynamics and sharp gradients over long horizons, leading to error propagation that worsens in 3D or on non-uniform meshes.
- **Core assumption:** The selected high-fidelity datasets (e.g., detonation, rocket combustors) are representative of the broader class of "realistic" multiphysics regimes where stiffness and geometry are intrinsic.
- **Evidence anchors:**
  - [abstract]: "scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors"
  - [section]: "Discussion" section notes that moving from 2D to 3D and irregular stencils results in instability and faster error growth.
  - [corpus]: "Multiphysics Bench" highlights the difficulty of coupled PDEs, though direct evidence for this specific "scaling barrier" formulation in external literature is limited in the provided neighbors.
- **Break condition:** Architectures incorporating multi-rate integration or explicit stiffness-handling mechanisms that decouple fast chemical kinetics from slower transport dynamics.

### Mechanism 2
- **Claim:** Surrogate performance appears to depend more on matching the model's architectural inductive bias to the discretization type (regular vs. irregular) than on parameter count.
- **Mechanism:** Spectral and convolutional models encode translation invariance, excelling on regular grids but failing on complex geometries. Conversely, pointwise models (DeepONet) and graph networks lack spatial priors suited for regular grids but handle irregular sampling better because they process coordinates or local connectivity rather than global structure.
- **Core assumption:** The "capacity-aligned" tuning protocol successfully isolates architectural effects by normalizing parameter counts across different model families.
- **Evidence anchors:**
  - [abstract]: "performance primarily controlled by architectural inductive biases rather than parameter count"
  - [section]: "Discussion" section states that on regular meshes spectral operators dominate, while on irregular meshes pointwise models become robust.
  - [corpus]: "When Network Architecture Meets Physics..." investigates deep operator learning for coupled systems, supporting the role of architecture in physics learning.
- **Break condition:** The development of "geometry-aware" operators that dynamically adapt their inductive bias, or the scaling of data volume to a point where raw capacity overcomes architectural mismatches.

### Mechanism 3
- **Claim:** High correlation or low nominal error metrics do not necessarily imply physically trustworthy predictions in multiphysics flows.
- **Mechanism:** Standard loss functions (e.g., MSE) allow models to achieve high correlation by capturing average states while missing critical transient structures (e.g., flame fronts, detonation cells) or violating conservation laws. The model learns to "look right" statistically without satisfying the underlying physics.
- **Core assumption:** Physical trustworthiness is correctly quantified here by integral quantities (e.g., detonation cell size) and structural similarity rather than just pointwise error.
- **Evidence anchors:**
  - [abstract]: "persistent gap between nominal accuracy metrics and physically trustworthy behavior"
  - [section]: "Discussion" section observes that models with high correlation often fail to reproduce temporal evolution of integral quantities.
  - [corpus]: "Multiphysics Bench" emphasizes the need for benchmarking complex systems, implying standard metrics may be insufficient.
- **Break condition:** Adoption of physics-informed loss functions or constrained architectures that strictly enforce conservation laws and transient dynamics.

## Foundational Learning

- **Concept: Coupled PDE-ODE Systems**
  - **Why needed here:** REALM targets reactive flows where fluid transport (PDE) is tightly coupled with stiff chemical kinetics (ODE). Understanding this coupling is required to grasp why "stiffness" creates a scaling barrier.
  - **Quick check question:** Does the chemical reaction time scale match the fluid advection time scale? (If no, the system is stiff).

- **Concept: Inductive Bias**
  - **Why needed here:** The paper demonstrates that choosing the wrong architecture for the mesh type (e.g., using a spectral method on an irregular mesh) leads to failure, regardless of model size.
  - **Quick check question:** Does the model assume the input is a uniform grid (Spectral/CNN) or a set of coordinates (Pointwise/Graph)?

- **Concept: Autoregressive Rollout**
  - **Why needed here:** The primary evaluation involves predicting future states by feeding predictions back as inputs. This mechanism is responsible for the "rapidly growing rollout errors" cited as a key finding.
  - **Quick check question:** Is the error calculated on a single step, or does it compound over multiple sequential predictions?

## Architecture Onboarding

- **Component map:**
  Preprocessing -> Backbone -> Head
  Box-Cox transform -> Choice of operator -> Projection to physical variables

- **Critical path:**
  1. Identify mesh type (Regular vs. Irregular)
  2. Select architecture (FFNO for Regular; DeepONet for Irregular)
  3. Apply Box-Cox transform to chemical species data
  4. Train using single-step or limited rollout to maintain stability

- **Design tradeoffs:**
  - Spectral/CNN: High efficiency and accuracy on regular grids; fails on irregular geometries
  - Graph/Pointwise: Robust to irregular meshes and complex boundaries; often lower accuracy on regular structured data and higher computational cost per step
  - DeepONet: Strong generalization on irregular data; struggles with long-range spatial dependencies on regular grids

- **Failure signatures:**
  - Over-smoothing: Graph networks averaging out sharp flame fronts
  - Ringing: Spectral methods producing oscillations near discontinuities (shocks)
  - Drift: Autoregressive models slowly shifting phase or losing conservation over long rollouts despite low per-step error

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Train FFNO on the 2D `IgnitHIT` (regular) dataset. Verify if error growth correlates with turbulence intensity.
  2. **Mesh Sensitivity:** Train DeepONet vs. FNO on `SupCavityFlame` (irregular). Observe if FNO fails to converge or produces artifacts due to mesh non-uniformity.
  3. **Metric Divergence:** Train a model on `PlanarDet` and plot predicted vs. true "detonation cell size" over time to verify if high correlation coexists with structural failure.

## Open Questions the Paper Calls Out
- How can neural architectures explicitly confront stiff, multiscale PDE–ODE couplings to overcome the scaling barrier observed in high-dimensional multiphysics flows?
- How can geometry-aware operators be designed to enhance expressivity on 3D irregular meshes while mitigating the over-smoothing and training instability seen in current graph-based models?
- What novel evaluation protocols and loss functions are necessary to align nominal accuracy metrics with physical trustworthiness in reactive flows?

## Limitations
- The scaling barrier's theoretical bounds linking dimensionality, stiffness, and mesh irregularity to error growth rates are not formally established
- Capacity-alignment protocol assumes uniform scaling across diverse architectures, though architectural families may have fundamentally different capacity-efficiency relationships
- Physical trustworthiness assessments rely on integral quantities and expert visual inspection, which are not fully standardized across datasets

## Confidence
- **High Confidence:** Architectural inductive bias dominates over parameter count; consistently observed across multiple datasets and model families with clear mechanistic explanations
- **Medium Confidence:** The scaling barrier exists as described; while error trends are clear, the joint contribution of stiffness, dimensionality, and mesh irregularity to the barrier's severity is inferred rather than directly isolated
- **Medium Confidence:** High correlation does not guarantee physical trustworthiness; demonstrated in specific cases but may vary in severity across different physics regimes

## Next Checks
1. **Disentangle scaling factors:** Design experiments isolating stiffness (e.g., vary chemical activation energy while holding dimensionality constant) to quantify its individual contribution to the scaling barrier
2. **Test architectural adaptation:** Implement a hybrid architecture that dynamically switches between convolutional and pointwise modes based on local mesh regularity to validate if architectural mismatch is the primary failure mode on irregular grids
3. **Standardize physical metrics:** Develop automated, quantitative metrics for physical trustworthiness (e.g., wavelet-based detection of flame front discontinuities) and apply them across all datasets to verify the correlation-trustworthiness gap is systematic