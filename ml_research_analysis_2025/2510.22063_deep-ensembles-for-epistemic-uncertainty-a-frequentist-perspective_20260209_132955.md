---
ver: rpa2
title: 'Deep Ensembles for Epistemic Uncertainty: A Frequentist Perspective'
arxiv_id: '2510.22063'
source_url: https://arxiv.org/abs/2510.22063
tags:
- uncertainty
- xtest
- deep
- epistemic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the theoretical underpinnings of deep ensembles
  for epistemic uncertainty quantification from a frequentist perspective. The authors
  establish a link between mutual information (MI) and the Fisher information, showing
  that MI asymptotically depends on both the variance in model predictions and the
  true data generating process.
---

# Deep Ensembles for Epistemic Uncertainty: A Frequentist Perspective

## Quick Facts
- arXiv ID: 2510.22063
- Source URL: https://arxiv.org/abs/2510.22063
- Authors: Anchit Jain; Stephen Bates
- Reference count: 40
- Key outcome: Bootstrap-based epistemic uncertainty estimator decomposes into data variability and training randomness components, with training stochasticity dominating by 2-5× and deep ensembles capturing ~90% of this uncertainty

## Executive Summary
This paper establishes a frequentist foundation for deep ensembles in epistemic uncertainty quantification by linking mutual information to Fisher information through the Bernstein von-Mises theorem. The authors propose a bootstrap-based estimator that asymptotically recovers the Bayesian mutual information measure without requiring posterior inference. They demonstrate that deep ensembles effectively capture the majority of epistemic uncertainty arising from training stochasticity (random initialization, SGD noise) rather than data sampling variability, providing theoretical justification for their empirical success in uncertainty-aware deep learning.

## Method Summary
The authors introduce a bootstrap-based estimator for epistemic uncertainty that approximates the Bayesian mutual information between model parameters and predictions. Under Bernstein von-Mises conditions, bootstrap parameter distributions converge to the same limiting normal distribution as Bayesian posteriors, centered at the MLE with covariance given by inverse Fisher information. They decompose the resulting epistemic uncertainty into components from data resampling variability and training randomness (random seeds). The method trains ensembles on bootstrapped datasets and compares the resulting uncertainty estimates against standard deep ensembles, demonstrating that training stochasticity dominates data variability by 2-5× across diverse deep learning tasks.

## Key Results
- Bootstrap MI estimator is asymptotically correct under Bernstein von-Mises conditions
- Epistemic uncertainty decomposes into data variability (I^resampling) and training stochasticity (I^seeds) components
- Training randomness uncertainty is 2-5× larger than data sampling variability across CIFAR-10, ImageNet, ISIC, and AG News
- Deep ensembles capture approximately 90% of training randomness uncertainty
- I^seeds_b / I^resampling_b ratios range from 0.21 (CIFAR-10) to 0.55 (ISIC)

## Why This Works (Mechanism)

### Mechanism 1: Bootstrap Approximates Bayesian Posterior for MI Estimation
- Claim: A bootstrap-based estimator asymptotically recovers the Bayesian mutual information measure of epistemic uncertainty without requiring posterior inference.
- Mechanism: Under Bernstein von-Mises conditions, bootstrap parameter distributions and Bayesian posteriors converge to the same limiting normal distribution centered at the MLE with covariance given by inverse Fisher information. This equivalence allows frequentist resampling to approximate fundamentally Bayesian uncertainty quantities.
- Core assumption: Model is well-specified with identifiable parameters, and the Bernstein von-Mises theorem conditions hold (smoothness, prior feasibility at θ₀).
- Evidence anchors:
  - [abstract]: "bootstrap-based estimator for epistemic uncertainty, which we prove is asymptotically correct"
  - [section 3.3, Theorem 3.2]: "I(Ytest;θ|Xtest,Dn) / Ib(Xtest,Dn) → 1"
  - [corpus]: Weak—corpus does not contain direct validation of this theoretical claim
- Break condition: Model misspecification, non-identifiable parameters, or small sample regimes where asymptotic approximations fail.

### Mechanism 2: Epistemic Uncertainty Decomposes into Data Variability and Training Stochasticity
- Claim: Bootstrap-based epistemic uncertainty naturally separates into two additive components: uncertainty from training data resampling (I^resampling) and uncertainty from optimization randomness (I^seeds).
- Mechanism: The bootstrap estimator's entropy difference decomposes via the law of total expectation: H(E[D,s][p]) - ED[H(Es[p])] captures data variability, while ED[H(Es[p]) - Es[H(p)]] captures seed-induced variation. This algebraic decomposition allows attribution of uncertainty sources.
- Core assumption: Training randomness (seed s) is independent of bootstrap data sample given the original dataset.
- Evidence anchors:
  - [abstract]: "decompose it into components arising from data variability and training randomness"
  - [section 3.4, equation 3]: Explicit decomposition formula showing Ib = I^resampling_b + I^seeds_b
  - [corpus]: "Do you understand epistemic uncertainty?" mentions frequentist EU estimation in regression
- Break condition: Systematic correlations between random seed and data sampling (e.g., data ordering affecting optimization dynamics).

### Mechanism 3: Training Stochasticity Dominates Data Variability in Deep Learning
- Claim: In modern deep learning settings, epistemic uncertainty arising from training stochasticity (random initialization, SGD noise, data shuffling) is substantially larger than uncertainty from finite data sampling.
- Mechanism: Deep ensembles train multiple models on identical data with different random seeds, capturing I^seeds directly. Empirical measurements show I^seeds_b / I^resampling_b ratios of 2-5x across CIFAR-10, ImageNet, ISIC, and AG News, suggesting optimizer stochasticity creates more parameter uncertainty than data sampling variance.
- Core assumption: This ratio generalizes beyond the four tasks studied; the finding reflects a fundamental property of over-parameterized neural networks trained with SGD.
- Evidence anchors:
  - [abstract]: "uncertainty due to training stochasticity is approximately 2-5 times larger than that due to data sampling variability"
  - [section 4.4, Table 1]: m_resampling_seeds ranges from 0.21 (CIFAR-10) to 0.55 (ISIC)
  - [corpus]: Corpus lacks direct replication of this specific 2-5x ratio claim
- Break condition: Regimes with very small training sets, deterministic optimization, or architectures with unique global minima may show different ratios.

## Foundational Learning

- Concept: **Mutual Information as Epistemic Uncertainty**
  - Why needed here: The paper's entire theoretical framework uses MI(Y;θ|X,D) as the target quantity. Without understanding that MI measures "reduction in label uncertainty from knowing parameters," the bootstrap approximation motivation is opaque.
  - Quick check question: Why does MI require a Bayesian posterior while the bootstrap estimator does not?

- Concept: **Bernstein von-Mises Theorem**
  - Why needed here: Theorem 3.1's proof relies on BvM to establish that √n(θ - θ₀) converges to the same limiting distribution under both posterior and bootstrap sampling. This is the theoretical bridge connecting frequentist and Bayesian uncertainty.
  - Quick check question: What conditions must hold for the posterior to converge to a normal distribution centered at the MLE?

- Concept: **Fisher Information Matrix**
  - Why needed here: The asymptotic MI expansion (Theorem 3.1) shows epistemic uncertainty is proportional to σ²_k / p̂_k, where σ²_k involves the inverse Fisher information. This connects uncertainty to local curvature of the loss landscape.
  - Quick check question: How does the Fisher information relate to the variance of the MLE?

## Architecture Onboarding

- Component map:
  - **Bootstrap MI Estimator (Algorithm 1)**: Samples Dirichlet weights → trains B models on reweighted data → computes entropy difference H(E[p]) - E[H(p)]
  - **Decomposition Module**: Trains ensembles on each bootstrapped dataset → separates I^resampling_b (across datasets) from I^seeds_b (within each dataset's ensemble)
  - **Deep Ensemble Comparator**: Trains standard deep ensemble on full data → compares Ideep ensemble against I^seeds_b via slope and correlation metrics

- Critical path:
  1. Verify BvM assumptions hold (model specification, identifiability, smoothness)
  2. Implement Dirichlet bootstrap (not multinomial—the paper uses Dirichlet for better DNN training)
  3. Train ensembles on bootstrapped datasets (computationally expensive: B bootstrap samples × ensemble size M)
  4. Compute entropy-based MI estimates for each test point
  5. Regress I^resampling_b on I^seeds_b and Ideep ensemble on I^seeds_b

- Design tradeoffs:
  - Dirichlet vs multinomial bootstrap weights: Dirichlet preserves all training points (better for DNNs) but may have different finite-sample properties
  - Ensemble size vs computational cost: Paper uses 4-10 members; smaller ensembles increase variance in MI estimates
  - Number of bootstrap samples (B): Paper uses B=10 for main experiments; smaller B increases estimation noise

- Failure signatures:
  - I^seeds_b >> Ideep ensemble: Suggests deep ensemble is undertrained or not diverse enough
  - I^resampling_b >> I^seeds_b: Violates paper's empirical finding; check for data corruption or unusual optimization settings
  - Negative MI estimates: Indicates numerical issues in entropy computation or probability clipping

- First 3 experiments:
  1. **Baseline replication**: Reproduce Table 1 on CIFAR-10 with ResNet-18, verifying m_resampling_seeds ≈ 0.21 and m_deep ensemble_seeds ≈ 0.94
  2. **Small data regime**: Test on ISIC (900 training samples) to check if decomposition ratio changes—does data variability become more dominant?
  3. **Ablation on ensemble size**: Vary ensemble size from 2 to 20 to measure how well Ideep ensemble approximates I^seeds_b as diversity increases

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on Bernstein von-Mises theorem conditions that may not hold in deep learning with non-convex loss landscapes
- Empirical findings based on only four datasets (CIFAR-10, ImageNet, ISIC, AG News), limiting generalizability
- Bootstrap-based MI estimation is computationally expensive (B×M models per test point)

## Confidence
- High confidence: Bootstrap MI estimator is asymptotically correct (Theorem 3.2 proof is rigorous)
- Medium confidence: Training stochasticity dominates data variability (empirical ratio 2-5× needs broader validation)
- Medium confidence: Deep ensembles capture 90% of training randomness (strong correlation observed but dependent on ensemble training quality)

## Next Checks
1. Test the data/stochasticity decomposition ratio across 10+ diverse tasks including regression problems, small datasets, and non-vision domains to verify the 2-5× finding is robust
2. Investigate failure modes where deep ensembles under-perform (I^seeds_b >> Ideep ensemble) by analyzing cases with vanishing gradient issues or lack of ensemble diversity
3. Evaluate bootstrap MI estimator performance in small-data regimes where asymptotic assumptions may break down