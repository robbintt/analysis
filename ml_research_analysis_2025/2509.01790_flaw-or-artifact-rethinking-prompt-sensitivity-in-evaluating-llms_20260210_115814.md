---
ver: rpa2
title: Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs
arxiv_id: '2509.01790'
source_url: https://arxiv.org/abs/2509.01790
tags:
- prompt
- question
- option
- answer
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper re-examines the widely reported phenomenon of prompt\
  \ sensitivity in large language models (LLMs), where slight changes in phrasing\
  \ significantly impact model performance. The authors hypothesize that much of this\
  \ sensitivity may stem from heuristic evaluation methods\u2014such as log-likelihood\
  \ scoring and rigid answer matching\u2014that fail to recognize semantically correct\
  \ responses expressed through alternative phrasings like synonyms or paraphrases."
---

# Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs

## Quick Facts
- arXiv ID: 2509.01790
- Source URL: https://arxiv.org/abs/2509.01790
- Reference count: 40
- One-line result: Heuristic evaluation methods artificially inflate apparent prompt sensitivity in LLMs by failing to recognize semantically correct responses expressed through alternative phrasings.

## Executive Summary
This paper re-examines the widely reported phenomenon of prompt sensitivity in large language models, where slight changes in phrasing significantly impact model performance. The authors hypothesize that much of this sensitivity is an artifact of heuristic evaluation methods—such as log-likelihood scoring and rigid answer matching—that fail to recognize semantically correct responses expressed through alternative phrasings like synonyms or paraphrases. Through systematic evaluation of seven LLMs across six benchmarks with 12 diverse prompt templates, they demonstrate that LLM-as-a-Judge evaluation reduces apparent prompt sensitivity and stabilizes model rankings, while human evaluation confirms answer correctness remains stable across prompts.

## Method Summary
The study evaluates seven LLMs (including GPT and Gemini families) across six benchmarks using 12 diverse prompt templates per benchmark. Two evaluation methods are compared: traditional heuristic methods (log-likelihood scoring and regex-based answer matching) versus LLM-as-a-Judge, where a separate LLM determines semantic equivalence between model predictions and reference answers. The LLM-as-a-Judge receives the question, reference answer, and model prediction as inputs and evaluates whether they semantically match. Performance variation is measured by standard deviation of accuracy across prompt templates, while ranking consistency is measured by mean Spearman rank correlation across all prompt pairs.

## Key Results
- Heuristic evaluation exaggerates prompt sensitivity: Gemma-2.0 accuracy varies from 0.25 to 0.90 on ARC-Challenge (std 0.28), but only 0.17 (std 0.005) with LLM-as-a-Judge.
- Model ranking consistency improves dramatically: Spearman rank correlation among models increases from 0.31 to 0.92.
- Human evaluation confirms answer correctness stability: 50-question human evaluation samples show high agreement (κ > 0.90) with LLM-as-a-Judge on most datasets.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Equivalence Recognition
LLM-as-a-Judge reduces prompt sensitivity artifacts by recognizing semantically equivalent responses that heuristic methods misclassify. The judge evaluates semantic match rather than surface-form overlap, correctly identifying "First World War" as equivalent to "World War 1". This assumes the judge LLM has sufficient semantic understanding to reliably detect paraphrases and synonyms.

### Mechanism 2: Decoupling Output Format From Correctness
Heuristic methods conflate format compliance with answer correctness, artificially inflating measured sensitivity. Regex-based extraction or log-likelihood scoring expects specific answer formats; format deviations are scored as incorrect even when semantic content is correct. This assumes models produce semantically correct answers at similar rates across prompts, but surface forms vary.

### Mechanism 3: Ranking Stabilization Through Consistent Evaluation
LLM-as-a-Judge yields stable model rankings across prompt templates because it eliminates evaluation-format noise. By applying consistent semantic criteria regardless of output phrasing, relative model performance ordering becomes comparable across prompts. This assumes true model capability is prompt-invariant and observed variance is measurement error.

## Foundational Learning

- **Concept: Heuristic evaluation (log-likelihood scoring, regex extraction)**
  - Why needed: Understanding what heuristic methods do helps explain why they fail on paraphrased outputs and why alternatives are needed.
  - Quick check: Given model output "The answer is approximately 42" and expected format "\boxed{42}", would regex extraction mark this correct?

- **Concept: LLM-as-a-Judge paradigm**
  - Why needed: This is the proposed solution; understanding its prompt structure and limitations is essential for implementation.
  - Quick check: What three inputs does the judge receive, and what judgment does it return?

- **Concept: Spearman rank correlation**
  - Why needed: The paper uses this metric to quantify ranking stability; interpreting the 0.31→0.92 improvement requires understanding what ρ measures.
  - Quick check: If Model A beats Model B under prompt 1 but loses under prompt 2, would Spearman ρ be high or low?

## Architecture Onboarding

- **Component map**: Prompt template generator (12 templates per benchmark, GPT-4o paraphrasing) -> Model inference layer (7 models, greedy decoding) -> Evaluation layer (heuristic branch + LLM-judge branch) -> Metrics (per-template accuracy, std across templates, pairwise Spearman ρ)

- **Critical path**: 
  1. Construct diverse prompt templates for each benchmark
  2. Run each model on all templates with greedy decoding
  3. Evaluate each response with both heuristic and LLM-judge
  4. Compute std_f per model and mean Spearman ρ across prompt pairs

- **Design tradeoffs**:
  - Heuristic: Fast, deterministic, no API cost; fails on format variations
  - LLM-judge: Higher cost, potential judge bias; handles semantic equivalence
  - Judge model choice: Gemini vs GPT show similar results, but smaller models untested
  - Template count: 12 templates sufficient (paper notes 12-template results match 100+ template results on ARC)

- **Failure signatures**:
  - High accuracy variance + low ranking correlation under heuristics → likely evaluation artifact
  - High variance persists under LLM-judge → true prompt sensitivity (or judge failure)
  - Judge disagrees with human annotators → judge reliability issue (paper reports high agreement: 0.90+ Fleiss' κ on most datasets)

- **First 3 experiments**:
  1. Replicate ARC-Challenge experiment: Run Gemma-2 on 12 templates, compare heuristic vs LLM-judge variance. Expect 10x+ std reduction.
  2. Judge robustness test: Swap Gemini judge for GPT-4o-mini on same outputs. Verify Spearman ρ remains within 0.01 (Table 2 shows 0.9546 vs 0.9621).
  3. Human validation on small sample: For 50 questions × 12 prompts, have annotators verify correctness. Compute agreement with LLM-judge; expect κ > 0.75.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does LLM-as-a-Judge evaluation introduce its own systematic biases or sensitivities that could mask underlying model instabilities? The paper shows LLM-as-a-Judge reduces variance, but does not investigate whether the judge itself has blind spots or preferences that artificially stabilize results.

- **Open Question 2**: Do findings generalize to non-English languages, code generation, and specialized professional domains (e.g., medical, legal)? All six benchmarks are English-language, general-purpose tasks; the paper does not address whether heuristic vs. LLM-as-a-Judge gaps persist in specialized or multilingual settings.

- **Open Question 3**: How many prompt templates are needed to reliably estimate true prompt sensitivity versus evaluation-induced variance? Due to computational constraints, the paper uses only 12 prompt templates per benchmark, though claims this is representative based on ARC-Challenge stability.

## Limitations
- The 12 prompt templates per benchmark may not capture the full space of meaningful prompt variations, though the paper claims results match those from 100+ templates on ARC.
- The evaluation focuses on greedy decoding without exploring sampling-based inference, which could yield different sensitivity patterns.
- The judge model selection remains untested across smaller models and specialized domains, leaving open whether results transfer beyond the tested models.

## Confidence
- **High confidence**: That heuristic evaluation methods systematically misclassify semantically correct answers as incorrect due to format rigidity. The ARC-Challenge results showing std reduction from 0.28 to 0.005 provide compelling evidence.
- **Medium confidence**: That prompt sensitivity is predominantly an evaluation artifact rather than an inherent model flaw. While human evaluation confirms answer correctness stability across prompts, the paper doesn't fully rule out true sensitivity in specialized domains or with more diverse prompt variations.
- **Medium confidence**: That LLM-as-a-Judge reliably captures semantic equivalence. The high human-judge agreement (κ > 0.90 on most datasets) is promising, but the judge's performance on specialized scientific terminology or nuanced reasoning tasks remains untested.

## Next Checks
1. **Judge robustness across model sizes**: Run the same evaluation pipeline using smaller judge models (e.g., LLaMA-3.1-8B, Gemma-2-9B-it) on a subset of benchmarks. Compare results to the Gemini 2.0 Flash baseline to determine the minimum judge capability required.
2. **Domain transfer test**: Apply the evaluation framework to specialized benchmarks in scientific domains (e.g., MMLU-Science, PubMedQA) where semantic nuance and domain-specific terminology may challenge the judge's equivalence recognition.
3. **Sampling-based inference comparison**: Repeat the analysis using temperature = 0.7 sampling (with top-p truncation) rather than greedy decoding. Compare variance patterns to determine whether decoding strategy affects the artifact vs. flaw distinction.