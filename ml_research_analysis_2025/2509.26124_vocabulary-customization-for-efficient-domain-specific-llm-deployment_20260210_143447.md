---
ver: rpa2
title: Vocabulary Customization for Efficient Domain-Specific LLM Deployment
arxiv_id: '2509.26124'
source_url: https://arxiv.org/abs/2509.26124
tags:
- tokenizer
- tokens
- arxiv
- vocabulary
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vocabulary mismatch in LLMs\u2014where general-domain tokenizers\
  \ fail to efficiently encode domain-specific terms\u2014leads to increased token\
  \ counts, slower inference, and higher computational cost. This work introduces\
  \ a tokenizer extension algorithm that augments an existing tokenizer with domain-specific\
  \ tokens while guaranteeing no increase in tokenization length for any input."
---

# Vocabulary Customization for Efficient Domain-Specific LLM Deployment

## Quick Facts
- arXiv ID: 2509.26124
- Source URL: https://arxiv.org/abs/2509.26124
- Reference count: 40
- Primary result: Domain-specific vocabulary extension reduces token counts by up to 20% and increases inference throughput by 20-30% while maintaining model quality.

## Executive Summary
This work addresses vocabulary mismatch in large language models, where general-domain tokenizers fail to efficiently encode domain-specific terms, leading to increased token counts, slower inference, and higher computational costs. The authors introduce a tokenizer extension algorithm that augments an existing tokenizer with domain-specific tokens while guaranteeing no increase in tokenization length for any input. Evaluated on e-commerce use-cases, the extended tokenizer reduces input sequences by up to 20%, yields 20-30% higher throughput during inference, and maintains model quality. The extended model adopts new tokens in nearly 98% of cases for sequences longer than 15 words, confirming effective integration.

## Method Summary
The method extends a base tokenizer by training an in-domain tokenizer, then traversing its vocabulary in frequency order to identify tokens that split into exactly two base tokens. These are added to the base tokenizer's vocabulary with corresponding merge operations appended to the end of the merge list, ensuring no regression in tokenization length. New token embeddings are initialized as the average of their constituent token embeddings. The extended vocabulary is then used in full-parameter continued pre-training on domain data for 10,000 iterations with a cosine learning rate schedule. The approach guarantees that the extended tokenizer never produces more tokens than the original for any input.

## Key Results
- Tokenization efficiency: Up to 20% reduction in input sequence lengths for domain-specific text
- Inference throughput: 20-30% higher requests per second during deployment
- Model quality: Maintained across general and in-domain benchmarks
- Token adoption: Nearly 98% adoption rate for sequences longer than 15 words during generation

## Why This Works (Mechanism)

### Mechanism 1: Conservative Merge Operation Ordering
Appending new merge operations (rather than prepending them) guarantees backward compatibility—the extended tokenizer never produces more tokens than the original for any input. BPE tokenizers apply merges in a fixed order. By appending domain-specific merges to the end, the original tokenization behavior is preserved. New merges can only combine tokens further (reducing counts) or leave them unchanged—they cannot interfere with earlier merge decisions.

### Mechanism 2: Average Embedding Initialization
Initializing new token embeddings as the average of their constituent token embeddings provides a semantically meaningful starting point, accelerating convergence. Domain-specific tokens are typically composed of existing subword tokens. Averaging the embeddings of these constituents gives the new token a starting representation that reflects its parts, which is closer to its true semantic neighborhood than random noise.

### Mechanism 3: Full-Parameter Continued Training Enables Token Adoption
Continued pre-training with all parameters enables the model to learn to emit new tokens during generation, achieving ~98% adoption for longer sequences. While input tokenization gains are guaranteed by the algorithm, output gains require the model to preferentially generate new tokens. Full-parameter training on domain data allows the model to adjust both the new embeddings and the rest of the network to favor the new vocabulary during decoding.

## Foundational Learning

- **BPE (Byte-Pair Encoding) Merge Ordering**
  - Why needed here: The entire algorithm hinges on understanding that BPE applies merges in a fixed sequence. Later merges operate on the output of earlier merges. Appending vs. prepending determines whether original behavior is preserved.
  - Quick check question: Given merge operations `[(a,b)→ab, (ab,c)→abc]` applied in that order, what tokens result from input "cab"? What if the order is reversed to `[(ab,c)→abc, (a,b)→ab]`?

- **Token Fertility and Inference Cost**
  - Why needed here: The paper's efficiency gains come from reducing token fertility (tokens per word). In autoregressive LLMs, each generated token requires a full forward pass, so fewer tokens directly translates to fewer computations.
  - Quick check question: If an 8B model generates 100 tokens at 50ms per forward pass, and vocabulary extension reduces output tokens by 20%, what is the new latency? Why is the reduction larger for longer sequences (29.2% vs. 20.7% in Table 1)?

- **Embedding Matrix Shape and Computation**
  - Why needed here: Extending vocabulary increases embedding and projection matrix sizes, adding compute overhead per token. The tradeoff (fewer tokens vs. larger matrices) determines the optimal vocabulary size.
  - Quick check question: Adding 30k tokens to a 128k vocabulary increases the embedding matrix by what percentage? Why does Figure 3 show only 1% forward pass slowdown despite a ~23% vocabulary increase for an 8B model?

## Architecture Onboarding

- **Component map**: Domain tokenizer trainer -> Tokenizer extender (Algorithm 1) -> Embedding initializer (Algorithm 2) -> Continued pre-training pipeline -> Evaluation suite

- **Critical path**: 
  1. Collect representative domain-specific training data
  2. Train in-domain tokenizer (target vocabulary larger than desired additions)
  3. Run Algorithm 1: traverse in-domain vocab, add N tokens that split into exactly 2 base tokens, append merges
  4. Run Algorithm 2: initialize new embeddings as average of constituent embeddings
  5. Continue pre-training with full parameter updates on domain data
  6. Evaluate: token efficiency on held-out data, RPS in deployment, quality benchmarks, adoption rate

- **Design tradeoffs**:
  - **Number of new tokens**: More tokens → better compression but larger embedding matrices. Paper found 30k optimal for Llama-3.1 8B (1% forward pass overhead, 8-20% token reduction). Larger models can absorb more tokens with less overhead; smaller models are more sensitive.
  - **Merge ordering strategy**: Appending (this paper) guarantees no regression but gains saturate; prepending (Yamaguchi) gives faster gains but can regress on out-of-domain inputs.
  - **Training scope**: Full training enables high adoption (~98%) but is expensive; training only new embeddings is cheaper but likely reduces adoption.
  - **Domain data selection**: Must cover downstream task distributions; too narrow risks losing general capability.

- **Failure signatures**:
  - **Low token adoption (<90% for long sequences)**: Insufficient training iterations, learning rate too low, or domain data not representative. Check loss curves and adoption by sequence length.
  - **Quality degradation on benchmarks**: Overfitting to domain data, incorrect data mixture (50/50 general/domain used here), or training instability. Compare Table 2 benchmarks.
  - **Efficiency regression on some inputs**: Algorithm 1 implemented incorrectly (prepending instead of appending), or new tokens conflict with pre-tokenization rules. Verify on held-out general-domain data.
  - **No efficiency gain despite token reduction**: Forward pass overhead dominates (too many tokens added for model size), or evaluation data doesn't contain frequent domain terms. Re-run vocabulary size sweep.

- **First 3 experiments**:
  1. **Tokenization efficiency baseline**: Before any training, measure average tokens/document on held-out domain data with original tokenizer vs. extended tokenizer at multiple N values (10k, 20k, 30k, 40k). Plot compression ratio vs. N.
  2. **Forward pass overhead sweep**: Deploy base model with extended vocabulary (random embeddings, no training) in vLLM; measure forward pass time vs. vocabulary size to identify the overhead inflection point for your model size.
  3. **Token adoption analysis post-training**: After 10k iterations of continued training, sample 1000 generations from downstream tasks. For each word, check if model probability favors new tokenization vs. old. Report adoption rate stratified by sequence length (<15, 15-49, ≥50 words) to replicate Table 3.

## Open Questions the Paper Calls Out

- **Cross-domain generalization**: The paper evaluates only on e-commerce data and states "Potentially, the behavior of our approach might be different for different domains." It is unclear if the 20% efficiency improvement and 98% token adoption rate transfer to domains where sub-word segmentation patterns differ significantly from general natural language.

- **Model size scaling**: The paper identifies 30k new tokens as optimal for the 8B model but notes "for larger models, the impact will be smaller and vice versa." It is unclear if the ratio of added tokens to model parameters remains constant or shifts for significantly larger (70B+) or smaller models.

- **Algorithm compatibility**: The authors state "We focus on a single model family, namely the Llama-3 models... The behavior of other model families might be different." The algorithm is designed for BPE merge lists, but its compatibility and efficiency guarantees are unverified for other popular model architectures which may have different baseline vocabulary efficiencies or tokenization mechanics.

## Limitations

- Limited domain evaluation: Results validated exclusively on e-commerce data; generalizability to other domains (biomedical, legal, code) is untested
- Single model family: Only tested on Llama-3.1 8B; performance on other architectures (Mistral, Falcon) is unknown
- Unablated design choices: No comparison of average initialization against random or learned alternatives; training duration and data composition not systematically explored

## Confidence

- **High confidence** in the core algorithmic guarantee: Appending merge operations to preserve backward compatibility is mathematically sound and directly supported by the implementation details. The efficiency gains (20-30% throughput improvement) are well-measured and reproducible given the same deployment setup.

- **Medium confidence** in the embedding initialization strategy: Average initialization is a reasonable heuristic with some supporting literature, but the paper doesn't rigorously compare it against alternatives like random or learned initialization. The effectiveness likely depends on the compositional nature of new tokens in the specific domain.

- **Low confidence** in the generalizability of results: The paper evaluates only one domain (e-commerce), one base model (Llama-3.1 8B), and one deployment setup (vLLM on H100). Claims about the method working "for any autoregressive LLM" and being "orthogonal to other optimizations" are not empirically validated across diverse settings.

## Next Checks

1. **Cross-domain tokenization efficiency**: Apply the method to a different domain (e.g., biomedical or legal text) with the same base model. Measure token reduction, throughput gains, and token adoption rates. Compare adoption patterns across domains to assess generalizability.

2. **Initialization strategy ablation**: Train three versions of the extended model: one with average initialization, one with random initialization, and one where new embeddings are learned from scratch (no pre-initialization). Compare token adoption rates and convergence speed during continued training.

3. **Model size sensitivity analysis**: Apply the method to Llama-3.1 70B and 405B models. Measure the tradeoff between added vocabulary size and forward pass overhead. Verify whether the 30k-token sweet spot from the 8B model scales proportionally or requires adjustment for larger models.