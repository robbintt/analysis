---
ver: rpa2
title: 'SciEvent: Benchmarking Multi-domain Scientific Event Extraction'
arxiv_id: '2509.15620'
source_url: https://arxiv.org/abs/2509.15620
tags:
- event
- uni00000013
- text
- argument
- none
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciEvent introduces a multi-domain benchmark for scientific event
  extraction, addressing the limitations of traditional entity-relation extraction
  methods in capturing context-rich scientific narratives. The benchmark provides
  a unified event extraction schema across five research domains (NLP, social computing,
  medical informatics, computational biology, and digital humanities), featuring 500
  annotated abstracts with event segments, triggers, and fine-grained arguments.
---

# SciEvent: Benchmarking Multi-domain Scientific Event Extraction

## Quick Facts
- **arXiv ID:** 2509.15620
- **Source URL:** https://arxiv.org/abs/2509.15620
- **Reference count:** 40
- **Primary result:** Current models lag 20% behind human performance on argument classification in scientific event extraction

## Executive Summary
SciEvent introduces a multi-domain benchmark for scientific event extraction, addressing the limitations of traditional entity-relation extraction methods in capturing context-rich scientific narratives. The benchmark provides a unified event extraction schema across five research domains (NLP, social computing, medical informatics, computational biology, and digital humanities), featuring 500 annotated abstracts with event segments, triggers, and fine-grained arguments. Experiments show that current models lag significantly behind human performance, with a 20% gap in argument classification tasks, highlighting the need for improved cross-domain scientific event extraction capabilities. The benchmark serves as a challenging testbed for advancing context-aware scientific information extraction methods.

## Method Summary
SciEvent employs a multi-stage event extraction pipeline: (1) segmenting abstracts into four core event types (Background, Method, Result, Conclusion), (2) extracting Agent-Action-Object trigger tuples, and (3) identifying 9 argument roles (Context, Purpose, Method, Result, Analysis, Challenge, Ethical, Implication, Contradiction). The benchmark annotates 500 abstracts from 5 domains using document-level event spans that may span multiple sentences. The dataset is split 80/10/10 for training, development, and testing. Both tuning-based models (DEGREE, OneIE, EEQA) and prompting-based LLMs (GPT-4.1, Llama-3.1-8B, Qwen2.5-7B, DeepSeek-R1-Distill-Llama-8B) are evaluated using task-specific metrics including EM, IoU F1, and ROUGE-L scores.

## Key Results
- Human annotators achieve near-perfect argument classification (100% IoU F1) while best models reach only 80%, creating a 20% performance gap
- Cross-domain transfer is challenging, with performance dropping significantly when training excludes specific domains, especially for Digital Humanities and Computational Biology
- Event segmentation improves downstream trigger and argument extraction, with 2-4% gains when providing event type information to LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Event segmentation provides necessary boundary constraints for accurate trigger and argument extraction in scientific texts.
- **Mechanism:** Scientific events frequently span multiple sentences and share ambiguous trigger words like "show" or "demonstrate." By first segmenting text into discrete event spans (Background, Method, Result, Conclusion), the model's search space for triggers is constrained, which in turn constrains argument role assignment.
- **Core assumption:** Events in scientific texts have fuzzy boundaries but coherent internal semantic structure that can be isolated.
- **Evidence anchors:** Table 5 shows that providing event type information to LLMs improves Arg-C F1 by approximately 2-4%, demonstrating that boundary awareness aids downstream extraction.

### Mechanism 2
- **Claim:** Cross-domain scientific event extraction performance depends on exposure to domain-specific rhetorical conventions and argument distributions.
- **Mechanism:** Each domain exhibits distinct writing styles—NLP and Computational Biology favor structured, technical prose, while Social Computing and Digital Humanities use more narrative, context-rich language. Models trained without domain exposure fail to acquire these linguistic regularities, causing performance drops.
- **Core assumption:** Domain transfer requires learning domain-specific patterns that do not fully generalize across scholarly fields.
- **Evidence anchors:** Figure 8 shows that removing a domain from training causes notable performance drops, with largest declines in Digital Humanities and Computational Biology.

### Mechanism 3
- **Claim:** Structured event representations (trigger-argument tuples with semantic roles) preserve contextual meaning that entity-relation tuples lose, reducing conflicting extractions.
- **Mechanism:** Entity-relation extraction produces isolated tuples like ⟨GPT-3.5-Turbo, better than, GPT-4-Turbo⟩ without task context, evaluation criteria, or scope. The event extraction paradigm anchors each extraction in an Agent-Action-Object trigger plus role-specific arguments (Context, Method, Result), preserving the full proposition.
- **Core assumption:** Scientific meaning resides at the proposition level rather than the entity-pair level.
- **Evidence anchors:** Figure 1 illustrates conflicting ERE tuples from different papers that lack contextual grounding.

## Foundational Learning

- **Concept: Event Extraction vs. Entity-Relation Extraction**
  - **Why needed here:** The paper explicitly contrasts EE (trigger-argument structures with semantic roles) against ERE (binary/N-ary entity relations). Understanding this distinction is essential for grasping why SciEvent's schema reduces fragmented outputs.
  - **Quick check question:** Given the sentence "We demonstrate that our method outperforms baselines on GLUE," what would an ERE system extract versus an EE system?

- **Concept: Document-Level Information Extraction**
  - **Why needed here:** SciEvent treats events as potentially multi-sentence spans, requiring readers to understand that extraction operates beyond sentence boundaries.
  - **Quick check question:** Why might a Method event in a scientific abstract span multiple sentences rather than a single clause?

- **Concept: Scientific Discourse Structure (IMRaD variants)**
  - **Why needed here:** The four event types (Background, Method, Result, Conclusion) map to conventional scientific abstract organization. Recognizing this structure helps understand why the annotation schema is domain-agnostic.
  - **Quick check question:** What types of arguments would you expect to find predominantly in a Background event versus a Result event?

## Architecture Onboarding

- **Component map:** 500 abstracts from 5 domains → manual annotation (segmentation → trigger identification → argument extraction) → task-specific metrics → baseline models comparison
- **Critical path:** Accurate event segmentation → correct trigger identification → proper argument role assignment. Errors propagate forward; the ~20% human-model gap in Arg-C (Figure 4) suggests argument classification is the primary bottleneck.
- **Design tradeoffs:**
  - Abstract-only vs. full papers: Abstracts are concise and widely available but may omit discourse elements; paper notes this limits document-level IE applicability.
  - 4 event types vs. finer granularity: Current schema is domain-agnostic but may miss field-specific event types.
  - 9 argument roles: Prioritizes completeness but creates data sparsity for rare roles (Ethical, Contradiction have few examples).
- **Failure signatures:**
  - Low Arg-C F1 on Method events (Figure 6) indicates complex technical phrasing challenges.
  - Poor performance on DH domain (Figure 7) suggests narrative writing styles are underrepresented in training data.
  - One-shot prompting shows diminishing returns beyond 1 example (Table 3-4), indicating in-context learning alone is insufficient.
- **First 3 experiments:**
  1. **Establish baseline:** Run GPT-4.1 zero-shot on the 500-abstract test split for all three tasks; compare against reported human performance (~20% gap on Arg-C).
  2. **Domain ablation study:** Train OneIE on 4-domain subsets (excluding each domain in turn) to quantify domain-specific contributions following Figure 8 methodology.
  3. **Event-type conditioning:** Implement the "predicted event type" prompting strategy (Table 5) and measure whether 2-4% Arg-C improvements replicate across different LLM families.

## Open Questions the Paper Calls Out
- How can the unified SciEvent schema be adapted to full-paper extraction without losing the structural consistency established in abstracts?
- What architectural or training advancements are required to close the 20% performance gap between human annotators and LLMs in scientific argument classification?
- Why does increasing in-context examples beyond one-shot fail to improve or degrade performance for smaller open-source LLMs on this task?

## Limitations
- The dataset contains uneven distribution across domains, with NLP and CB being overrepresented compared to DH and MI, potentially biasing model performance metrics toward technical domains.
- Event segmentation errors propagate downstream, and the IoU threshold (>0.5) for segmentation success may be arbitrary given the fuzzy boundaries of scientific events.
- The 4-event-type schema works well for IMRaD-structured abstracts but may not capture domain-specific event types like hypothesis generation or experimental design in fields with different rhetorical structures.

## Confidence
- **High confidence:** The 20% human-model performance gap on argument classification is well-supported by quantitative results across multiple evaluation metrics and baselines.
- **Medium confidence:** Cross-domain performance differences are observed but the specific linguistic mechanisms (why DH is harder than NLP) are inferred rather than directly tested.
- **Low confidence:** The claim that event extraction preserves context better than ERE is supported by illustrative examples but lacks systematic evaluation of downstream application performance.

## Next Checks
1. **Schema expansion study:** Add domain-specific event types to the annotation schema for DH and MI, then retrain models to measure whether performance gaps close, validating whether the 4-type schema is truly domain-agnostic.
2. **Error analysis pipeline:** Conduct detailed error analysis on Method events where models struggle most, categorizing failures by linguistic phenomena (technical jargon, negation, modality) to identify specific training needs.
3. **Downstream application test:** Apply SciEvent-extracted events to a concrete scientific discovery task (e.g., contradiction detection or knowledge base population) to empirically verify that the structured representations improve over ERE outputs in practice.