---
ver: rpa2
title: 'Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition'
arxiv_id: '2511.02351'
source_url: https://arxiv.org/abs/2511.02351
tags:
- performance
- dance
- data
- motion
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a human-centered, real-time motion recognition
  system for synergic human-machine performance. By integrating wearable IMU sensors
  with the MiniRocket time-series classifier, the system achieves high-accuracy motion
  detection (<50 ms latency) while preserving the expressive depth of embodied dance.
---

# Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition

## Quick Facts
- arXiv ID: 2511.02351
- Source URL: https://arxiv.org/abs/2511.02351
- Reference count: 21
- Primary result: 96.05% accuracy, <50ms latency motion recognition for synergic human-machine performance

## Executive Summary
This work presents a human-centered, real-time motion recognition system for synergic human-machine performance. By integrating wearable IMU sensors with the MiniRocket time-series classifier, the system achieves high-accuracy motion detection (<50 ms latency) while preserving the expressive depth of embodied dance. Rather than relying on large-scale datasets or generative AI, the approach centers the dancer's somatic memory, mapping personalized movement-to-sound associations through interactive learning.

## Method Summary
The system uses four wireless IMU sensors (wrists/ankles) streaming 24-channel data at 48 Hz to a GPU server running MiniRocket feature extraction and ridge regression classification. Dancers perform seven movement classes (six dance movements plus negative class) in 2-second chunks, with the model learning personalized movement-to-sound mappings through interactive training. The approach prioritizes dancer-specific somatic memory over genre-based or AI-generated associations, achieving high accuracy while maintaining real-time responsiveness for live performance applications.

## Key Results
- Mean accuracy of 96.05% (SD 2.89%) across 7 movement classes
- Macro F1 score of 96.62% with multiclass AUC > 0.99 for all classes
- Sub-50ms end-to-end latency with inference taking approximately 15ms per chunk

## Why This Works (Mechanism)

### Mechanism 1
MiniRocket enables sub-50ms end-to-end motion classification by transforming raw time-series into discriminative features via deterministic convolutional kernels, then applying a linear ridge classifier. MiniRocket generates a fixed set of convolutional kernels applied to each 2-second multivariate chunk (24 channels from 4 IMUs), producing feature vectors that capture local temporal patterns. The ridge classifier learns class boundaries in this transformed space, achieving near-deterministic inference without gradient descent.

### Mechanism 2
Dancer-specific somatic mapping creates tighter semantic coupling between movement and sound than genre-based or AI-generated associations, improving both artist acceptance and classification reliability. The dancer improvises movements in response to personally meaningful sounds, encoding bi-directional sensorimotor associations. The system learns these specific movement patterns rather than generic dance categories, reducing intra-class variance and increasing model confidence.

### Mechanism 3
Multi-sensor IMU placement (wrists and ankles) captures sufficient whole-body dynamics for classification while maintaining wearability and low computational overhead. Four IMU sensors provide 24 channels (6 axes × 4 locations) at 48 Hz, generating 96 values per second per sensor. This captures limb trajectories and body coordination patterns without requiring full-body motion capture or video processing.

## Foundational Learning

- Concept: **Time-series classification with MiniRocket**
  - Why needed here: MiniRocket is the core classifier; understanding its deterministic kernel transform and ridge regression head is essential for debugging, tuning, and extending the system.
  - Quick check question: Can you explain why MiniRocket uses nearly-deterministic random kernels rather than learned convolution weights?

- Concept: **IMU sensor fundamentals (accelerometer + gyroscope)**
  - Why needed here: The system relies on 6-axis data from each sensor; understanding signal characteristics, noise profiles, and coordinate frames is critical for preprocessing and augmentation.
  - Quick check question: What is the difference between accelerometer data and gyroscope data, and why might both be necessary for dance motion recognition?

- Concept: **Real-time streaming latency budgeting**
  - Why needed here: The system targets <50ms end-to-end latency; engineers must understand how data transmission, preprocessing, inference, and output trigger contribute to this budget.
  - Quick check question: If BLE transmission adds 10ms, preprocessing adds 5ms, and inference adds 15ms, how much margin remains for multimedia output triggering before exceeding the 50ms target?

## Architecture Onboarding

- Component map: IMU sensors (wrists/ankles) → BLE → smartphone/receiver → Python script → GPU server (MiniRocket + ridge classifier) → multimedia system

- Critical path: 1) IMU data acquisition (continuous, 48 Hz) → 2) Chunk formation (2-second windows, 96 samples × 24 channels) → 3) MiniRocket feature extraction → 4) Ridge classifier inference (~15ms) → 5) Label + probability transmission to multimedia controller → 6) Sound/projection trigger

- Design tradeoffs:
  - Window size vs. latency: 2-second windows enable higher accuracy but introduce classification lag; shorter windows would increase responsiveness but may reduce discriminability.
  - Sensor count vs. wearability: 4 sensors capture more body dynamics than 1-2, but increase setup time and potential movement restriction.
  - Remote vs. local inference: GPU server enables fast MiniRocket processing but introduces network dependency; edge deployment would reduce latency but require optimization.

- Failure signatures:
  - Low prediction confidence during transitions: Expected behavior per Figure 3; consider smoothing or transition state handling.
  - Consistent misclassification of specific classes: Check for class imbalance in training data or sensor placement issues.
  - Latency spikes >100ms: Likely network-related; monitor BLE connection stability and server load.
  - Sensor dropout: BLE disconnection; ensure battery levels and line-of-sight.

- First 3 experiments:
  1. Single-dancer validation: Replicate the paper's 7-class setup with one dancer, collecting 50-100 samples per class. Verify accuracy approaches 96% with stratified cross-validation.
  2. Latency breakdown measurement: Instrument each pipeline stage (BLE, chunking, inference, output) to confirm <50ms end-to-end and identify bottlenecks.
  3. Window size ablation: Test 1-second, 1.5-second, and 2-second windows to quantify accuracy-latency tradeoff for your specific motion vocabulary.

## Open Questions the Paper Calls Out

### Open Question 1
How can motion transition ambiguity be resolved using higher-granularity methods without sacrificing real-time latency? The current system segments data into 2-second chunks, causing predicted probabilities to decrease during movement transitions due to temporal ambiguity.

### Open Question 2
Can the system support on-the-fly model retraining to allow dancers to introduce new movement vocabularies during live performance? The current workflow separates the training phase (pre-performance) from the inference phase (in-performance); the pipeline is not yet capable of dynamically updating its classifier instantaneously.

### Open Question 3
To what extent can a model trained on a specific dancer's somatic memory generalize to other bodies or movement styles? Somatic memory is highly individual; it is unclear if the "dance-literate" machine learns generalizable biomechanics or merely overfits to the specific proprioceptive signatures of the training artist.

## Limitations

- Limited vocabulary of 7 classes may not capture the full complexity of expressive dance movements
- Single-dancer validation raises questions about generalizability across different bodies and movement styles
- 2-second window approach may struggle with faster or more temporally ambiguous gestures
- Bluetooth Low Energy infrastructure introduces potential points of failure in live performance settings

## Confidence

**High confidence**: The technical implementation of MiniRocket for time-series classification, the latency measurements, and the basic system architecture are well-supported by the described methodology and consistent with established practices in the field.

**Medium confidence**: The classification accuracy figures and their statistical significance are credible but limited to single-dancer validation. The somatic mapping claims are philosophically compelling but lack empirical comparison to alternative approaches.

**Low confidence**: The system's robustness in diverse performance environments, its scalability to multiple dancers or expanded vocabularies, and the long-term stability of dancer-specific models remain largely unverified.

## Next Checks

1. Multi-dancer generalization test: Train and evaluate the system with 3-5 dancers performing identical movement vocabularies. Measure accuracy drop and identify whether the somatic-specific approach scales or requires individual models.

2. Stress testing under realistic conditions: Evaluate system performance with intentional BLE interference, varying sensor positions, and environmental noise. Document failure modes and recovery strategies for live performance reliability.

3. Temporal resolution ablation study: Systematically test 1-second, 1.5-second, and 2-second windows across different movement types to quantify the accuracy-latency tradeoff and identify optimal window sizes for various gesture speeds and complexities.