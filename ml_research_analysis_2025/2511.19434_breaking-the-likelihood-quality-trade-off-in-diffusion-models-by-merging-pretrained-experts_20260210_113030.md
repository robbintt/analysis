---
ver: rpa2
title: Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained
  Experts
arxiv_id: '2511.19434'
source_url: https://arxiv.org/abs/2511.19434
tags:
- likelihood
- diffusion
- quality
- arxiv
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the trade-off between sample quality and\
  \ likelihood in diffusion models. The authors propose a method that merges two pretrained\
  \ diffusion experts\u2014one optimized for image quality (EDM) and one for likelihood\
  \ (VDM)\u2014by switching between them at different noise levels during sampling."
---

# Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts

## Quick Facts
- **arXiv ID:** 2511.19434
- **Source URL:** https://arxiv.org/abs/2511.19434
- **Reference count:** 31
- **Primary result:** Merges pretrained EDM (quality) and VDM (likelihood) experts via noise-level switching, improving or matching both FID and likelihood on CIFAR-10 and ImageNet32 without retraining.

## Executive Summary
This paper addresses the fundamental trade-off between sample quality and likelihood in diffusion models. The authors propose a method that merges two pretrained diffusion experts—one optimized for image quality (EDM) and one for likelihood (VDM)—by switching between them at different noise levels during sampling. The switch occurs at an intermediate time step, with EDM used at high noise levels to shape global structure and VDM at low noise levels to refine pixel statistics. The approach requires no retraining, only selecting a switching threshold. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms both base models, improving or preserving both FID and likelihood. The method effectively breaks the likelihood-quality trade-off by leveraging complementary strengths of pretrained experts.

## Method Summary
The method uses two frozen pretrained UNets (EDM for quality, VDM for likelihood) and routes the score estimation based on the current noise level during sampling. At each timestep, the method computes the Signal-to-Noise Ratio (SNR) and maps it to the corresponding time in each expert's schedule. A hard switch occurs at threshold η: EDM is used for high noise levels (global structure) and VDM for low noise levels (pixel statistics). The approach works for variance-preserving (VP) diffusion processes, where score scaling cancels out. No retraining is required—only the switch threshold η needs to be selected empirically for each dataset.

## Key Results
- Merged model achieves better or comparable performance on both FID and likelihood metrics across CIFAR-10 and ImageNet32
- Optimal switching threshold η ≈ 0.3 for CIFAR-10 and η ≈ 0.5 for ImageNet32
- The method requires no retraining, only selecting the switching threshold
- Visual inspection confirms smooth transitions at the switch point with no catastrophic artifacts

## Why This Works (Mechanism)
The paper claims that high noise levels determine global structure while low noise levels determine pixel statistics. The method delegates high-noise steps to the quality expert (EDM) and low-noise steps to the likelihood expert (VDM). The core assumption is that the contribution of denoising steps is separable by noise level without catastrophic error compounding. However, the evidence provided for this mechanism is incomplete in the current report.

## Foundational Learning
The paper builds on existing diffusion model research but does not explicitly address foundational learning principles or curriculum design in its methodology. The approach assumes that pretrained experts can be combined without interference.

## Architecture Onboarding
The method requires access to two pretrained diffusion models (EDM and VDM) with different optimization objectives. The architecture assumes variance-preserving diffusion processes where score scaling cancels out during the switching operation. The switching mechanism is simple to implement once pretrained models are available.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the current report. Further investigation would be needed to identify unresolved issues or future research directions.

## Limitations
The method has several limitations: it requires two separately pretrained models (increasing computational cost during training phase), the switching threshold must be selected empirically for each dataset, and it may not generalize to non-variance-preserving diffusion processes. The approach also assumes that the two experts' learned distributions are compatible enough to be merged without retraining.

## Confidence
The results appear technically sound based on the reported metrics, showing consistent improvements or preservation of both FID and likelihood scores. The method is simple and requires no retraining, which adds to its practical appeal. However, the mechanistic explanation could be more thoroughly justified.

## Next Checks
- Verify the empirical selection process for the switching threshold η
- Investigate the generalization to other datasets beyond CIFAR-10 and ImageNet32
- Test the method with different diffusion process types beyond variance-preserving
- Examine whether the complementary strengths assumption holds across different model architectures
- Evaluate computational overhead during sampling compared to single-expert approaches