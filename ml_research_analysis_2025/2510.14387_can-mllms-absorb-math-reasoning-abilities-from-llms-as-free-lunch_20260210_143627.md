---
ver: rpa2
title: Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?
arxiv_id: '2510.14387'
source_url: https://arxiv.org/abs/2510.14387
tags:
- math
- reasoning
- merging
- mllm
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of enhancing multimodal large language
  models (MLLMs) for mathematical reasoning by absorbing abilities from specialized
  math language models (LLMs), without tuning. The key challenge is the misalignment
  between the parameter spaces of MLLMs and math LLMs, which leads to conflicts during
  model merging.
---

# Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?

## Quick Facts
- **arXiv ID**: 2510.14387
- **Source URL**: https://arxiv.org/abs/2510.14387
- **Authors**: Yijie Hu; Zihao Zhou; Kaizhu Huang; Xiaowei Huang; Qiufeng Wang
- **Reference count**: 40
- **Primary result**: +3.7% average gain on MathVerse, +2.8% on MathVista while preserving general capabilities

## Executive Summary
This paper introduces IP-Merging, a tuning-free method to enhance multimodal large language models (MLLMs) with math reasoning abilities from specialized math language models (LLMs). The core innovation addresses parameter space misalignment between text-only math LLMs and multimodal MLLMs by identifying math-relevant parameter subspaces via SVD-based similarity, rescaling to prevent magnitude domination, and projecting into the MLLM's parameter space before merging. Experiments show significant improvements on multiple math reasoning benchmarks while maintaining general multimodal performance.

## Method Summary
IP-Merging transfers math reasoning abilities from text-only LLMs to MLLMs through a three-stage process: (1) parameter identification using SVD decomposition to find aligned subspaces between models, (2) rescaling to normalize parameter magnitudes via nuclear norm ratios, and (3) projection of math LLM parameters into the MLLM's subspace weighted by similarity scores. The method operates on task vectors (finetuned - pretrained weights) and only modifies shared layers (attention and MLP), leaving visual encoders frozen. Unlike direct addition which causes conflicts due to parameter space gaps, IP-Merging ensures proper alignment before merging.

## Key Results
- IP-Merging achieves +3.7% average accuracy improvement on MathVerse and +2.8% on MathVista benchmarks
- Preserves general multimodal capabilities, maintaining MMMU accuracy within 0.5% of base models
- Outperforms direct parameter addition and ties merging approaches, which either degrade performance or zero out critical parameters
- Successfully transfers abilities from larger models (e.g., 7B math LLM to 7B MLLM) and multiple math LLMs simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Subspace Similarity Identifies Reasoning-Critical Parameters
Math reasoning abilities localize to specific parameter subspaces identifiable via SVD-based similarity matching. Task vectors from both models are decomposed via SVD, and cosine similarity between corresponding orthonormal bases (weighted by singular values) identifies aligned subspaces. Parameters where the first corresponding angle shows highest alignment are selected for merging, as this basis most strongly links to math reasoning.

### Mechanism 2: Rescaling Prevents Parameter Magnitude Domination
Math LLMs often have larger singular values than MLLMs, causing direct addition to overshadow MLLM parameters. Rescaling normalizes contributions by computing the ratio of nuclear norms (∑σ_MLLM / ∑σ_Math) and multiplying selected math LLM parameters by this factor before merging, preventing dominance while preserving relative importance.

### Mechanism 3: Projection Aligns Cross-Modal Parameter Spaces
Rescaled math LLM parameters are projected into the MLLM's weighted subspace to minimize cross-modal parameter space gaps. Importance scores are computed from similarity via softmax, and projection uses the weighted basis: ∆W^Math-P = λ · ∆W̄^Math · V̄_V · V̄_V^⊤. This aligns LLM parameters to MLLM's subspace orientation while preserving reasoning-relevant information.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: Core operation for extracting subspaces and computing similarity scores between task vectors
  - Quick check question: Can you explain why the first right singular vector captures the direction of maximum variance in a weight matrix?

- **Concept: Task Vectors**
  - Why needed here: The fundamental unit of transfer—defined as ∆W = W_finetuned - W_pretrained—representing learned task-specific adjustments
  - Quick check question: Why does subtracting the pretrained model isolate task-specific knowledge rather than model architecture?

- **Concept: Nuclear Norm and Matrix Subspaces**
  - Why needed here: Nuclear norm (∑σ_i) quantifies parameter magnitude for rescaling; subspace alignment requires understanding orthonormal basis geometry
  - Quick check question: How does the nuclear norm differ from Frobenius norm, and why might nuclear norm better capture parameter "importance magnitude"?

## Architecture Onboarding

- **Component map**: Pretrained model W_0 -> Task vector extraction (ΔW) -> SVD decomposition -> Similarity computation -> Threshold filtering -> Rescaling (λ) -> Importance weighting (γ) -> Subspace projection -> Element-wise addition to MLLM

- **Critical path**:
  1. Task vector extraction (both models) — prerequisite for all subsequent steps
  2. SVD on task vectors per layer — O(d³) per layer, d = hidden dimension
  3. Similarity threshold filtering — controls merge aggressiveness
  4. Rescaling + projection — alignment quality directly impacts transfer success
  5. Addition to base MLLM — final integration

- **Design tradeoffs**:
  - Threshold S_α: Higher threshold (0.6-0.9) → fewer layers merged, more conservative; lower threshold (0.1-0.3) → more layers, risk of interference. Paper finds 0.3-0.4 optimal for LLaVA, 0.6 for Qwen
  - Layer selection: Only shared layers (attention + MLP) merged; visual encoder and projection layers frozen
  - Model compatibility: Requires same foundation LLM (e.g., both from LLaMA-2); cross-architecture merging not supported

- **Failure signatures**:
  - Direct Task Arithmetic causes performance drop (e.g., MathVerse 11.3% → 9.8%) due to unaligned parameter spaces
  - Ties Merging can zero out critical parameters, degrading general capabilities (MMMU drops 6.9% for LLaVA)
  - Merging models with different foundation LLMs is explicitly unsupported per limitation statement

- **First 3 experiments**:
  1. Baseline sanity check: Compute task vectors for MLLM and Math LLM. Visualize singular value distributions (replicate Figure 4) to confirm magnitude gap exists for your model pair
  2. Threshold sweep: Run IP-Merging with S_α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on a held-out math subset. Plot accuracy vs. threshold to find optimal operating point
  3. Ablation validation: Test three variants: (a) selection only, (b) selection + rescaling, (c) full IP-Merging. Compare on MathVista to replicate Table 5 and verify each component contributes

## Open Questions the Paper Calls Out

### Open Question 1
Can IP-Merging be extended to effectively merge MLLMs and math LLMs that have different foundation architectures or parameter sizes? The current method requires identical foundation LLMs and struggles to find associated layers when structures differ.

### Open Question 2
How can the merging process be refined to better align textual reasoning with visual-spatial understanding to prevent geometric reasoning failures? The method may not sufficiently account for integration of geometric visual features despite improved symbolic inference.

### Open Question 3
Is the IP-Merging methodology effective for transferring non-mathematical complex reasoning abilities, such as logical or causal reasoning? The paper focuses exclusively on math reasoning, leaving generalizability to other reasoning domains untested.

## Limitations
- Method requires identical foundation LLMs between MLLM and Math LLM, limiting cross-architecture applications
- Evaluation gaps exist for long-term stability and performance on non-math multimodal tasks
- Results are shown for 7B models only; scaling effectiveness to larger/smaller models is unknown

## Confidence

**High confidence**: Core mathematical formulation of IP-Merging and experimental results showing consistent math reasoning improvements

**Medium confidence**: "Tuning-free" claim given computational overhead and requirement for pretrained checkpoints; generalization beyond tested architectures

**Low confidence**: Long-term stability of transferred abilities, effectiveness on non-mathematical reasoning tasks, and performance with significantly different pretraining objectives

## Next Checks

1. **Cross-architecture validation**: Test IP-Merging between MLLMs and Math LLMs with different foundation architectures (e.g., LLaVA-7B with DeepSeek-7B). Document whether the method fails gracefully or produces unpredictable results, and measure parameter space misalignment.

2. **General capability stress test**: Evaluate merged models on spatial reasoning benchmarks (e.g., COG, CRAFT) and visual QA tasks requiring multi-step reasoning. Compare against baselines to verify that math reasoning absorption doesn't degrade other reasoning modalities.

3. **Scaling experiment**: Implement IP-Merging for LLaVA-13B and DeepSeek-33B pairs. Measure SVD computation time, memory requirements, and accuracy gains vs. 7B models. Test whether the 0.3-0.4 threshold remains optimal at larger scales.