---
ver: rpa2
title: Curriculum Learning in Genetic Programming Guided Local Search for Large-scale
  Vehicle Routing Problems
arxiv_id: '2505.15839'
source_url: https://arxiv.org/abs/2505.15839
tags:
- training
- instances
- learning
- routing
- gpgls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CL-GPGLS, a curriculum learning-based genetic
  programming guided local search method for large-scale vehicle routing problems.
  The key idea is to progressively introduce training instances of increasing difficulty
  during the learning process, starting with smaller problem instances and gradually
  moving to larger ones.
---

# Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems

## Quick Facts
- arXiv ID: 2505.15839
- Source URL: https://arxiv.org/abs/2505.15839
- Reference count: 18
- Primary result: CL-GPGLS outperforms random training, random grouped training, and large-only training baselines on 1000-node VRP instances

## Executive Summary
This paper proposes CL-GPGLS, a curriculum learning-based genetic programming guided local search method for large-scale vehicle routing problems. The key idea is to progressively introduce training instances of increasing difficulty during the learning process, starting with smaller problem instances and gradually moving to larger ones. This curriculum-based approach improves learning efficiency and solution quality compared to traditional random instance selection. Extensive experiments on 1000-node VRP instances show that CL-GPGLS outperforms three baseline methods in terms of test fitness and stability, demonstrating the effectiveness of curriculum learning in optimizing heuristics for large-scale vehicle routing problems.

## Method Summary
CL-GPGLS uses genetic programming to evolve utility functions that guide local search for vehicle routing problems. The algorithm employs a curriculum scheduler that progresses through four difficulty levels (100, 300, 600, and 1000 nodes), spending three generations at each level before advancing. The GP population (100 individuals) evolves utility function trees using tournament selection, crossover (0.8), mutation (0.15), and elitism (0.05). Each individual's fitness is evaluated by applying its utility function within a Guided Local Search framework on training instances. The curriculum approach contrasts with baseline methods that use random training instance selection or train only on large instances.

## Key Results
- CL-GPGLS achieves superior fitness values compared to random training, random grouped training, and large-only training baselines
- The curriculum approach demonstrates highest learning efficiency, reaching better solutions faster than baseline methods
- Box plot analysis shows CL-GPGLS has improved stability with fewer outliers than baseline methods, though three outliers indicate occasional instability

## Why This Works (Mechanism)

### Mechanism 1: Progressive Difficulty Scaling
Training on progressively larger VRP instances before reaching target scale improves final solution quality compared to direct training on large instances. The curriculum scheduler sequences training sets from 100 → 300 → 600 → 1000 nodes, spending 3 generations per scale. This allows the GP population to first learn utility function structures that capture fundamental routing patterns on tractable instances, then refine these functions for exponentially larger solution spaces.

### Mechanism 2: Early Transition Before Convergence
Transitioning between difficulty levels before the population converges on the current training set produces better generalization than training to convergence at each level. The paper uses a fixed 3-generation window per scale regardless of fitness trajectory. This prevents the GP population from over-specializing to small-instance characteristics while still capturing transferable structural knowledge.

### Mechanism 3: Expanded Search Space Initialization
Knowledge from simpler instances expands the effective search space for harder instances, leading to better solutions under equivalent computational budgets. Utility functions evolved on small instances provide a structured starting region in the function space. When faced with larger instances, the population explores from this informed region rather than from random initialization.

## Foundational Learning

- **Concept: Guided Local Search (GLS) Utility Functions**
  - Why needed here: The GP population evolves utility functions that GLS uses to decide which solution features to penalize during local search. Understanding that GLS iteratively modifies its objective based on feature penalties is essential to interpret what the GP is actually learning.
  - Quick check question: Can you explain how a utility function guides GLS to escape local optima in VRP?

- **Concept: Genetic Programming Tree Representation**
  - Why needed here: Each GP individual is a tree combining terminal nodes (instance/solution/edge features) with arithmetic operators. The fitness of a tree is determined by applying it within GLS and measuring solution quality improvement.
  - Quick check question: What does a single GP individual represent in this framework, and how is its fitness computed?

- **Concept: Curriculum Learning Principles**
  - Why needed here: The core contribution applies curriculum learning—training on easy instances before hard ones—to metaheuristic design. This differs from typical machine learning curricula because the "model" is an evolving population rather than a neural network with gradient descent.
  - Quick check question: How does curriculum learning in this evolutionary context differ from curriculum learning in neural network training?

## Architecture Onboarding

- **Component map:** GP Population -> GLS Evaluator -> Curriculum Scheduler -> Training Set Manager
- **Critical path:**
  1. Initialize GP population (ramped-half-half, depth 2-6)
  2. Select training set based on current curriculum level
  3. Evaluate each individual via GLS on training instances
  4. Apply genetic operators, produce next generation
  5. Check if 3 generations completed at current scale → advance difficulty level
  6. Repeat until all levels complete (100→300→600→1000 nodes)

- **Design tradeoffs:**
  - Fixed vs. adaptive transition: Paper uses fixed 3-generation windows. Adaptive transitions based on convergence metrics could improve efficiency but add complexity and hyperparameters.
  - Number of instances per scale: Only 3 instances per scale used. More instances may improve robustness but increase training time linearly.
  - Evaluation time scaling: 5s × (scale/100) per instance. Linear scaling may not reflect true complexity growth.

- **Failure signatures:**
  - High variance across runs: The box plots show outliers, suggesting occasional failure to transfer knowledge effectively.
  - No improvement over LO baseline: If curriculum provides no benefit over training directly on large instances, the transfer assumption is violated.
  - Fitness increases after transition: Some fitness degradation is expected when moving to harder instances, but persistent degradation suggests failed transfer.

- **First 3 experiments:**
  1. Ablation on curriculum sequence: Compare CL-GPGLS against a "reverse curriculum" (1000→600→300→100) to isolate whether difficulty ordering matters vs. simply seeing multiple scales.
  2. Sensitivity to transition timing: Test 1, 3, 5, and 10 generations per scale to identify the optimal transition window and validate the 3-generation choice.
  3. Cross-scale generalization test: Train on instances up to 600 nodes, test on 1000-node instances without exposure to 1000-node training data. This measures true transfer rather than cumulative training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can curriculum schedulers be designed to dynamically adapt based on convergence speed, training time, and fitness deviation rather than using fixed generation-based transitions?
- Basis in paper: The conclusion states: "Some interesting directions for future work include the design of more flexible curriculum schedulers, taking into account factors such as convergence speed, training time, and fitness deviation when changing training sets."
- Why unresolved: The current implementation uses a fixed schedule (3 generations per scale) regardless of whether the population has adequately learned at each difficulty level.
- What evidence would resolve it: Experiments comparing adaptive schedulers (e.g., transition based on convergence thresholds) against fixed schedules, measuring both solution quality and computational efficiency.

### Open Question 2
- Question: What is the precise relationship between VRP instance difficulty (beyond scale alone) and algorithm performance in curriculum learning?
- Basis in paper: The conclusion explicitly identifies this gap: "The relationship between VRP instance difficulty and algorithm performance should also be further explored."
- Why unresolved: The paper defines difficulty solely by problem scale (number of nodes), ignoring structural features like customer distribution, route balance, or constraint tightness that may better characterize difficulty.
- What evidence would resolve it: Analysis correlating multiple instance features (not just scale) with learning outcomes, potentially revealing which features best predict curriculum effectiveness.

### Open Question 3
- Question: What causes the occasional instability (outliers) observed in CL-GPGLS performance on some test instances?
- Basis in paper: The box plot analysis notes "the presence of three outliers suggests occasional instability in some test instances," but the paper does not investigate their cause.
- Why unresolved: The paper reports outliers without analyzing whether they stem from specific instance characteristics, curriculum timing issues, or GP evolutionary stochasticity.
- What evidence would resolve it: Instance-level analysis identifying common features among outlier cases, or ablation studies isolating the source of variance.

### Open Question 4
- Question: How well does CL-GPGLS generalize to real-world VRP scenarios beyond synthetically generated benchmark instances?
- Basis in paper: The conclusion states: "generating more VRP datasets based on real-world scenarios could benefit researchers."
- Why unresolved: Experiments use only instances from a synthetic generator following the dataset X scheme, with random depot/customer positioning and unitary demand—conditions that may not reflect real-world complexity.
- What evidence would resolve it: Testing CL-GPGLS on real-world VRP datasets with features like time windows, asymmetric distances, or non-uniform demand distributions.

## Limitations

- Evaluation focuses exclusively on 1000-node instances with a fixed curriculum sequence and transition timing
- The claim that curriculum learning improves transfer rather than simply providing more diverse training data remains unproven
- The 3-generation transition window appears arbitrary without convergence analysis
- Single-instance-per-difficulty testing protocol may overstate generalization claims

## Confidence

- **Medium confidence** in the overall effectiveness claim: Results show consistent improvements over baselines, but the experimental design lacks critical ablations and the corpus provides minimal external validation.
- **Medium confidence** in the progressive difficulty mechanism: The sequence shows improvement, but the contribution of ordering versus scale diversity exposure is untested.
- **Low confidence** in the early-transition mechanism: Fixed 3-generation windows are justified by computational efficiency arguments rather than empirical optimization, and no sensitivity analysis is provided.

## Next Checks

1. **Curriculum sequence ablation**: Compare CL-GPGLS against a reverse curriculum (1000→600→300→100) and random multi-scale training with identical instance exposure to isolate the effect of difficulty ordering.
2. **Transition timing sensitivity**: Systematically test 1, 3, 5, and 10 generations per scale to empirically validate the 3-generation choice and identify optimal transition windows.
3. **Cross-scale transfer validation**: Train exclusively on instances up to 600 nodes, then test on 1000-node instances without any 1000-node training exposure to measure true knowledge transfer versus cumulative training effects.