---
ver: rpa2
title: MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection
arxiv_id: '2509.19926'
source_url: https://arxiv.org/abs/2509.19926
tags:
- prompting
- examples
- alzheimer
- accuracy
- mmse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits few-shot prompting for Alzheimer\u2019s detection\
  \ using the ADReSS dataset of Cookie Theft transcripts. It introduces two prompting\
  \ variants: MMSE-Proxy Prompting, which maps each exemplar\u2019s MMSE score to\
  \ a deterministic probability aligned with clinical severity bands, enabling AUC\
  \ computation; and Reasoning-augmented Prompting, which uses a multimodal LLM to\
  \ generate example rationales and MMSE-aligned probabilities."
---

# MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection

## Quick Facts
- arXiv ID: 2509.19926
- Source URL: https://arxiv.org/abs/2509.19926
- Reference count: 0
- Primary result: 0.82 accuracy and 0.86 AUC with MMSE-Proxy prompting on ADReSS dataset

## Executive Summary
This paper introduces MMSE-calibrated few-shot prompting for Alzheimer's detection using the ADReSS dataset of Cookie Theft transcripts. The approach achieves state-of-the-art prompting performance without parameter updates by anchoring exemplar probabilities to clinical MMSE severity bands. Two variants are presented: MMSE-Proxy Prompting maps each exemplar's MMSE score to a deterministic probability enabling AUC computation, while Reasoning-augmented Prompting uses a multimodal LLM to generate example rationales and MMSE-aligned probabilities. Both methods use nested class-balanced interleaving and strict JSON schemas to ensure reproducibility.

## Method Summary
The method applies few-shot prompting to binary Alzheimer's detection from CHAT transcripts, using MMSE scores as clinical anchors. MMSE-Proxy Prompting maps each exemplar's MMSE to a class-conditional probability via deterministic sigmoids anchored at MMSE=26, enabling threshold-free AUC computation. Reasoning-augmented Prompting generates rationales and MMSE-aligned probabilities once using GPT-5 on a multimodal pool, then freezes it for inference. Both use nested class-balanced interleaving (a1,h1,...,ak,hk) to mitigate positional bias, and strict JSON schemas with forced-decision clauses to eliminate parsing failures. Inference uses Mistral-7B-Instruct with near-greedy sampling (temp=0.01, top-k=50, top-p=1.0) across k=0-20 examples per class.

## Key Results
- MMSE-Proxy prompting achieves 0.82 accuracy and 0.86 AUC, outperforming no-proxy baseline (0.76 accuracy)
- Reasoning-augmented prompting reaches 0.82 accuracy with 0.83 AUC and reliable JSON output
- Both methods achieve zero parsing failures versus 6 failures in prior work
- Performance plateaus at mid-range k (14-19) with nested interleaving

## Why This Works (Mechanism)

### Mechanism 1: MMSE-Anchored Probability Calibration
Anchoring exemplar probabilities to clinical MMSE bands improves discrimination and enables AUC computation without post-hoc calibration. The deterministic class-conditional sigmoid mapping ensures probabilities are monotonic with clinical severity, allowing the model to learn and generalize probability emission from demonstrations.

### Mechanism 2: Nested Class-Balanced Interleave
Alternating AD/HC examples in a nested interleave reduces positional bias and seed variance as context size grows. The structure maintains class balance at every prefix while isolating k's effect by appending new exemplars at the end without altering earlier positions.

### Mechanism 3: Reasoning-Augmented Demonstrations Improve Format Adherence
Including rationales in few-shot examples improves JSON format adherence, particularly for comment and probability fields. Structure-copying effects in decoder LLMs mean examples lacking rationales often result in omitted fields or mirrored placeholder text at inference.

## Foundational Learning

- **In-Context Learning (ICL) in Decoder-Only LLMs**: Why needed - entire approach is training-free, relying on model learning from prompt demonstrations. Quick check - why does adding labeled examples to prompt change outputs without weight updates?
- **ROCâ€“AUC and Probability Calibration**: Why needed - MMSE-Proxy emits calibrated probabilities for threshold-free AUC computation. Quick check - why is AUC more informative than accuracy for binary classification with ambiguous cases?
- **Positional and Recency Bias in Transformers**: Why needed - nested interleave mitigates known positional biases in LLM context usage. Quick check - what happens when critical content appears in middle vs. start/end of long context?

## Architecture Onboarding

- **Component map**: CHAT transcript parser -> MMSE-Proxy mapping (sigmoids) -> Exemplar selector (nested interleave) -> Prompt constructor (ChatML + forced-decision JSON) -> Mistral-7B-Instruct inference -> JSON parser -> Accuracy/AUC computation
- **Critical path**: 1) Parse CHAT transcripts preserving disfluencies, 2) Apply MMSE-Proxy mapping or load GPT-5 pool, 3) Build prompt with nested interleave, 4) Call LLM with forced-decision schema, 5) Parse outputs and compute metrics
- **Design tradeoffs**: MMSE-Proxy simpler with higher AUC (0.86 vs 0.83) vs reasoning-augmented's interpretability; Random selection slightly outperforms TF-IDF (0.82 vs 0.79); Performance plateaus at mid-k with token cost considerations
- **Failure signatures**: No-proxy baseline omits probability field causing AUC inability; Prior work had 6 parsing failures; Without rationales, model mirrors placeholder text or omits comment field
- **First 3 experiments**: 1) Zero-shot baseline with forced-decision JSON (expect ~0.65 accuracy), 2) MMSE-Proxy ablation at k=14 (expect +6 accuracy points), 3) Cross-model check on Qwen3-8B (expect similar trajectory, peak at k=11)

## Open Questions the Paper Calls Out

- Would incorporating audio features alongside transcripts improve detection performance beyond 0.86 AUC?
- What exemplar selection criteria beyond TF-IDF could outperform random nested interleave?
- Does MMSE-Proxy prompting transfer to non-English Cookie Theft corpora?
- How robust is the approach to ASR-induced transcription errors on disfluent speech?

## Limitations

- MMSE-anchored probability mapping may not generalize to test sets with different severity distributions
- Reliance on GPT-5 for rationale generation creates black-box dependency that cannot be fully reproduced
- Interleaving strategy assumes sufficient order-sensitivity in target models, which may not hold universally
- Strict JSON schema improves reproducibility but may not generalize to less constrained clinical workflows

## Confidence

- **MMSE-Proxy calibration performance (accuracy 0.82, AUC 0.86)**: High confidence - measured directly on held-out test data with parsing failures ruled out
- **Mechanism 1 (MMSE-anchored calibration improves discrimination)**: Medium confidence - supported by ablation but underlying calibration assumption not directly tested
- **Mechanism 2 (nested interleave reduces bias)**: Medium confidence - supported by reasoning but magnitude benefit not separately quantified
- **Mechanism 3 (reasoning-augmented examples improve format adherence)**: Low confidence - plausible but evidence largely correlational without rationale ablation

## Next Checks

1. **Calibration drift test**: Apply MMSE-Proxy to held-out subset with different MMSE ranges and verify AUC stability
2. **Rationale necessity ablation**: Run reasoning-augmented at k=19 with/without rationales and measure JSON success rate and accuracy differences
3. **Cross-model generalization**: Apply exact MMSE-Proxy protocol to Qwen2-7B-Instruct and verify performance trajectory replication