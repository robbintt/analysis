---
ver: rpa2
title: 'Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text
  Detection'
arxiv_id: '2501.11786'
source_url: https://arxiv.org/abs/2501.11786
tags:
- synthetic
- data
- text
- training
- membership
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper shows that synthetic data can mislead membership inference
  attack (MIA) evaluations because MIAs function as machine-generated text detectors,
  not true membership detectors. Experiments across five different generator models,
  two datasets, and five attack methods reveal that MIAs consistently misclassify
  synthetic text as training data, with AUC scores dropping below 0.5 (sometimes near
  0.0) when synthetic non-members replace human-written ones.
---

# Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text Detection

## Quick Facts
- **arXiv ID:** 2501.11786
- **Source URL:** https://arxiv.org/abs/2501.11786
- **Reference count:** 3
- **Key outcome:** Synthetic data misleads MIA evaluations by causing attacks to detect machine-generated text rather than true membership

## Executive Summary
This paper reveals a critical flaw in membership inference attack (MIA) evaluations: synthetic data causes MIAs to misclassify machine-generated text as training data. The authors demonstrate that when synthetic non-members replace human-written ones in standard MIA evaluations, attack performance drops below 0.5 (sometimes near 0.0) across five different generator models, two datasets, and five attack methods. This pattern holds even when synthetic text is generated by different models than the target. The Zlib compression-based attack remains an exception, showing more robust performance. This finding has serious implications for LLM evaluations that use synthetic data, suggesting such approaches may measure generation artifacts rather than intended properties like memorization or data leakage.

## Method Summary
The paper conducts controlled experiments by generating synthetic text using five different models (GPT-Neo, GPT-J, GPT-3.5, OPT, Llama 2) on two datasets (OpenWebText2, arXiv). They systematically replace human-written non-members with synthetic non-members in MIA evaluations, measuring how attack performance changes. Five different MIA methods are tested: three likelihood-based approaches (thresholding, regression, likelihood-ratio) and two information-theoretic approaches (Zipf coefficient, Zlib compression). The authors evaluate across multiple model scales and architectures to test the generalizability of their findings.

## Key Results
- MIAs consistently misclassify synthetic text as training data, with AUC scores dropping below 0.5 when synthetic non-members replace human-written ones
- This misclassification pattern persists even when synthetic text is generated by different models than the target model
- Zlib compression-based attack remains robust as an exception, warranting further exploration of its resilience

## Why This Works (Mechanism)
The paper shows that MIAs function as machine-generated text detectors rather than true membership detectors because both memorized training data and high-quality synthetic text share similar probability distributions. When language models generate text, they produce outputs with high likelihoods under their own probability distributions - a characteristic that MIAs interpret as membership. This creates a fundamental conflation where any high-probability text (whether memorized or synthetically generated) triggers membership signals, regardless of whether it actually appeared in training.

## Foundational Learning

**Language Model Likelihoods**: Probability assigned to sequences by LMs - needed to understand why synthetic text appears similar to training data; quick check: compare log-likelihood distributions of human vs synthetic text.

**Membership Inference Attacks**: Methods that distinguish training from non-training data using model outputs - needed to understand the evaluation framework; quick check: AUC scores above 0.5 indicate successful attacks.

**Synthetic Text Generation**: Process of creating text using LMs - needed to understand the source of artifacts; quick check: text quality correlates with likelihood scores.

**Compression-based Metrics**: Information-theoretic measures like Zlib compression - needed to understand alternative attack approaches; quick check: lower compression ratios indicate higher redundancy/structure.

**Cross-model Transfer**: Behavior consistency across different model architectures - needed to understand generalizability; quick check: synthetic text from model A misclassified by model B.

## Architecture Onboarding

**Component Map**: Text Generator -> Synthetic Text -> MIA Methods -> AUC Scores (Classifier/Threshold/Regression/Zipf/Zlib)

**Critical Path**: Synthetic text generation → Attack feature extraction (likelihoods/compression) → Classification → AUC evaluation

**Design Tradeoffs**: High-quality synthetic text maximizes generation likelihood but increases misclassification risk; low-quality text reduces artifacts but may not represent realistic use cases.

**Failure Signatures**: AUC scores approaching 0.0 when synthetic non-members are used; consistent misclassification of synthetic text as members across multiple attack methods.

**3 First Experiments**:
1. Generate synthetic text from target model and verify it's misclassified as member
2. Test cross-model synthetic text generation (model A synthetic → model B attack)
3. Compare Zlib attack performance against likelihood-based methods with synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Zlib compression-based attack remain robust against synthetic data while likelihood-based methods fail?
- Basis in paper: [explicit] The authors note the Zlib attack as an "intriguing exception" and state that "the exact reason for this resilience warrants further exploration."
- Why unresolved: The paper observes the phenomenon empirically but does not provide a theoretical explanation for why normalizing loss by compression size specifically mitigates the artifacts that cause other attacks to misclassify synthetic text.
- What evidence would resolve it: An ablation study or theoretical analysis identifying the specific text features captured by compression that differ between human-written and machine-generated text.

### Open Question 2
- Question: Why does the misclassification of synthetic text transfer consistently across different model scales and architectures?
- Basis in paper: [explicit] The introduction explicitly asks "why does synthetic text behavior transfer so consistently across different model scales and architectures?"
- Why unresolved: While the paper demonstrates that text generated by one model (e.g., GPT-3.5) is misclassified by a different target model (e.g., GPT-Neo), it does not fully explain the underlying mechanism of this cross-model transfer.
- What evidence would resolve it: Analysis showing shared probability landscapes or sampling biases between distinct architectures that cause them to treat synthetic text similarly to memorized text.

### Open Question 3
- Question: How can membership inference attacks be redesigned to distinguish machine-generated text from genuine training data?
- Basis in paper: [explicit] The Future Work section calls for "crafting attacks (or modifications to existing ones) that remain reliable in the presence of synthetic text."
- Why unresolved: Current attacks rely on loss and likelihood signals that are inherently similar for both memorized training data and high-probability synthetic text, rendering existing methods fundamentally flawed for this task.
- What evidence would resolve it: The proposal of a new attack metric that decouples "high probability" from "membership" or a normalization technique that yields AUC scores significantly above 0.5 when synthetic non-members are used.

## Limitations
- Experiments focus on specific generator architectures (GPT-Neo, GPT-J, OPT) and datasets (OpenWebText2, arXiv), limiting generalizability to other domains
- Real-world scenarios may involve more complex data mixtures where synthetic and human text are interspersed
- The Zlib compression-based attack's robustness raises questions about whether it actually measures membership or just compression efficiency

## Confidence

**High confidence** in the empirical observation that MIAs misclassify synthetic text as training data across multiple models and attack methods

**Medium confidence** in the broader claim that this invalidates many MIA evaluations, as this depends on the specific goals of those evaluations and whether synthetic data contamination is acknowledged

**Medium confidence** in the implications for LLM evaluation practices, as this assumes synthetic data is commonly used without proper controls

## Next Checks
1. Test MIA behavior on real-world datasets where synthetic and human text are mixed, rather than clean synthetic replacements, to understand practical implications
2. Evaluate whether post-processing synthetic text (paraphrasing, human editing) reduces its detectability by MIAs to determine if simple fixes exist
3. Investigate whether the Zlib compression-based attack genuinely measures membership or if it has similar synthetic data vulnerabilities but with different detection thresholds