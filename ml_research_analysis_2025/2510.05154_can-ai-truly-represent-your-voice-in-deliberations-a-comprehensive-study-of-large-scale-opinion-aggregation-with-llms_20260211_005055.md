---
ver: rpa2
title: Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of
  Large-Scale Opinion Aggregation with LLMs
arxiv_id: '2510.05154'
source_url: https://arxiv.org/abs/2510.05154
tags:
- deliberation
- each
- minority
- opinions
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces DELIBERATIONBANK, a large-scale human-grounded
  dataset for evaluating AI-generated summaries of public deliberations. It contains
  3,000 free-form opinions across ten deliberation questions and 4,500 human annotations
  evaluating summaries along four dimensions: representativeness, informativeness,
  neutrality, and policy approval.'
---

# Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs

## Quick Facts
- arXiv ID: 2510.05154
- Source URL: https://arxiv.org/abs/2510.05154
- Reference count: 40
- Key outcome: Introduces DELIBERATIONBANK dataset and DELIBERATIONJUDGE model, showing that LLMs systematically underrepresent minority viewpoints in deliberation summarization

## Executive Summary
This paper addresses the challenge of using AI to summarize public deliberations by introducing DELIBERATIONBANK, a large-scale human-grounded dataset with 3,000 opinions across ten deliberation questions. The authors develop DELIBERATIONJUDGE, a fine-tuned DeBERTa model that achieves up to 0.70 Spearman correlation with human judgments and is 100× faster than LLM-based judges. Benchmarking 18 LLMs reveals that while larger models perform better, all tend to underrepresent minority viewpoints, highlighting significant fairness challenges in AI-supported deliberation.

## Method Summary
The researchers collected 3,000 free-form opinions across ten deliberation questions using Prolific and Deliberation.io, then gathered 4,500 human annotations evaluating summaries along four dimensions: representativeness, informativeness, neutrality, and policy approval. They trained DELIBERATIONJUDGE by fine-tuning DeBERTa-v3-base on 3,600 annotated instances using Huber loss, creating a discriminative model that maps (question, opinion, summary) tuples to normalized scores. The framework was used to benchmark 18 different LLMs with varying input sizes (10-300 opinions) and configurations.

## Key Results
- Fine-tuned DeBERTa judge achieves Spearman correlation up to 0.70 with human judgments, outperforming LLM judges (correlation < 0.4)
- All 18 tested LLMs systematically underrepresent minority viewpoints, with minority bias 34.4× larger than random bias
- Model performance plateaus at approximately 100 input opinions, suggesting limited scalability in handling large deliberation contexts
- Larger models perform better overall, but gains diminish beyond certain size thresholds

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned encoder models outperform general-purpose LLMs on deliberation evaluation by learning domain-specific preference patterns from human annotations. DeBERTa-v3-base is trained with supervised fine-tuning on 3,600 human-annotated instances using Huber loss across four dimensions, learning to map (question, opinion, summary) tuples to normalized scores that correlate with human judgments.

### Mechanism 2
LLM summarizers systematically underrepresent minority perspectives due to statistical tendencies toward majority patterns in training and inference. Models optimize for fluency and coverage of common patterns, deprioritizing rare argumentation patterns that characterize minority opinions.

### Mechanism 3
Larger models achieve better deliberation summarization quality, but gains diminish and all models plateau when input opinion count exceeds ~100. This reflects model capacity limits rather than data saturation, as even with more input opinions, models don't proportionally improve coverage.

## Foundational Learning

- **Concept: LLM-as-Judge evaluation paradigm**
  - Why needed here: The paper addresses failures of LLM-as-Judge (correlations < 0.4) by proposing a fine-tuned alternative
  - Quick check question: Can you explain why off-the-shelf LLMs show weak alignment with human judgments in this domain?

- **Concept: Multi-dimensional deliberation metrics**
  - Why needed here: Evaluation uses four distinct dimensions with different correlation patterns
  - Quick check question: Why might informativeness show near-perfect rank correlation (ρ=1.00) while representativeness shows only moderate correlation (ρ=0.60)?

- **Concept: Minority detection methods (subjective vs. objective)**
  - Why needed here: The paper distinguishes self-reported minorities from embedding-based semantic outliers with <10% overlap
  - Quick check question: What does the low overlap between self-reported and automatically-detected minorities reveal about different forms of marginalization?

## Architecture Onboarding

- **Component map:**
  1. Opinion Collection Layer (Prolific + Deliberation.io): 300 participants per question, free-form responses
  2. Summary Generation Layer: 18 LLMs with varying input sizes (10–300 opinions), 3 resamples per configuration
  3. Human Annotation Layer (POTATO): Rating task (5-point Likert) + Comparison task (ring-based pairing)
  4. Judge Training Layer: DeBERTa-v3-base → hidden layer (GELU + dropout) → 4-dim regression head with sigmoid
  5. Evaluation Layer: DELIBERATIONJUDGE scores summaries against each opinion in the input set

- **Critical path:** Human annotation quality → training data diversity → judge correlation → benchmark reliability. The 4,500 annotations (3,600 train / 900 test) are the bottleneck resource.

- **Design tradeoffs:**
  - Single-annotator labels (faster, cheaper) vs. multi-annotator consensus (more reliable)
  - DeBERTa-v3-base (faster, better generalization) vs. larger models (worse performance in Table 9)
  - Huber loss (robust to outliers) vs. MSE (sensitive to annotation noise)

- **Failure signatures:**
  - OOD topics show 2–30× correlation drops (Appendix G.3)—judge doesn't transfer to unseen deliberation domains
  - Very small models (<1.7B) show near-zero or negative correlations (Figure 3)—don't use as judges

- **First 3 experiments:**
  1. Replicate the correlation analysis: Train DELIBERATIONJUDGE on the 80% split, evaluate Spearman correlation against the 20% test set across all four dimensions.
  2. Ablate input size: Test whether the plateau at ~100 opinions holds across multiple models by varying input size from 10 to 300.
  3. Validate minority bias: Using the automatic outlier detection pipeline (embedding + LOF), compare representativeness scores for detected minorities vs. non-minorities on a held-out topic.

## Open Questions the Paper Calls Out

### Open Question 1
What underlying factors explain the existence of two distinct minority sets (subjective self-reported vs. objective semantic outliers), and why do these differences lead to systematic underrepresentation in LLM summaries?
- Basis in paper: Section 5.3 explicitly states the low overlap (<10%) between these sets raises this important question
- Why unresolved: The paper identifies distinct mechanisms but lacks a unified theory for addressing both types simultaneously

### Open Question 2
What specific aggregation mechanisms can enable LLMs to effectively utilize large-scale opinion inputs (n > 100) to improve summary quality, rather than experiencing a performance plateau?
- Basis in paper: Section 4.2 notes the performance plateau beyond 100 opinions and calls for more effective aggregation mechanisms
- Why unresolved: Current transformer contexts and attention mechanisms struggle to integrate diverse information beyond certain density

### Open Question 3
Can explicit fairness constraints or diversity-aware fine-tuning objectives effectively mitigate the systematic underrepresentation of minority viewpoints in generated summaries?
- Basis in paper: Section 6 suggests fairness constraints may improve minority coverage as natural extension
- Why unresolved: The study establishes bias exists but focuses on evaluation rather than proposing or testing algorithmic interventions

## Limitations
- Dataset covers only ten deliberation questions, limiting generalization to other policy domains or cultural contexts
- Human annotations collected from single platform (Prolific) may introduce demographic or cognitive biases
- Judge model performance degrades substantially on out-of-distribution topics (2–30× correlation drop)
- No analysis of inter-annotator reliability or measurement error in human annotations

## Confidence
- **High**: The existence of systematic minority underrepresentation across multiple models and detection methods
- **Medium**: The superiority of fine-tuned DeBERTa over general-purpose LLM judges for this specific task
- **Low**: The claim that larger models always perform better—plateau effects and resource constraints suggest diminishing returns

## Next Checks
1. Test DELIBERATIONJUDGE on at least 5 additional deliberation topics from different policy domains to assess transfer capability and identify failure modes
2. Conduct inter-annotator reliability analysis (e.g., Krippendorff's alpha) on a subset of human annotations to quantify measurement uncertainty
3. Evaluate whether explicitly prompting LLMs to prioritize minority viewpoints reduces underrepresentation bias without degrading overall summary quality