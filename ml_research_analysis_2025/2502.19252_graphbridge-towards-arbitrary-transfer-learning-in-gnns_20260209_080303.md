---
ver: rpa2
title: 'GraphBridge: Towards Arbitrary Transfer Learning in GNNs'
arxiv_id: '2502.19252'
source_url: https://arxiv.org/abs/2502.19252
tags:
- graph
- transfer
- methods
- tuning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraphBridge, a framework enabling arbitrary
  transfer learning across graph neural networks (GNNs) without modifying task configurations
  or graph structures. It addresses the challenges of task and domain heterogeneity
  in GNNs by introducing prediction heads and a bridging network that connects inputs
  to outputs.
---

# GraphBridge: Towards Arbitrary Transfer Learning in GNNs

## Quick Facts
- arXiv ID: 2502.19252
- Source URL: https://arxiv.org/abs/2502.19252
- Reference count: 40
- Key result: Achieves comparable performance to full fine-tuning while using only 5%-20% of tunable parameters across diverse GNN transfer learning scenarios

## Executive Summary
GraphBridge introduces a framework for arbitrary transfer learning in graph neural networks that enables knowledge transfer across different tasks (Graph2Graph, Node2Node, Graph2Node, Graph2point-cloud) without modifying task configurations or graph structures. The framework addresses task and domain heterogeneity through prediction heads and a bridging network that connects inputs to outputs. To mitigate negative transfer, GraphBridge merges the source model with a concurrently trained backup model. Experiments across 16 datasets demonstrate that GraphBridge achieves comparable performance to full fine-tuning while using only 5%-20% of tunable parameters, representing significant progress in task- and domain-agnostic transfer learning for graph-like data.

## Method Summary
GraphBridge operates through three key components: an Input Bridge that adapts input features to match the backbone expectations, an Efficient-Tuning mechanism (GSST or GMST) that performs parameter-efficient adaptation using a frozen pre-trained backbone plus trainable side-network, and an Output Bridge that transforms embeddings to arbitrary output dimensions. GSST uses side-tuning with an MLP network for easy transfer tasks, while GMST merges the pre-trained backbone with a randomly-initialized backup model to mitigate negative transfer in harder scenarios. The framework supports various backbone architectures (GCN, GAT, GIN) and can be pre-trained using GraphCL or SimGRACE.

## Key Results
- Achieves comparable performance to full fine-tuning while using only 5%-20% of tunable parameters
- Successfully handles arbitrary transfer scenarios including Graph2Graph, Node2Node, Graph2Node, and Graph2point-cloud
- GMST effectively mitigates negative transfer in high-gap tasks, outperforming fine-tuning in Graph2Node scenarios
- Maintains performance stability across different backbone depths (2-layer vs 5-layer)

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Efficient Side-Tuning
Side-tuning with a lightweight MLP network achieves comparable transfer performance to full fine-tuning while using 5-20% tunable parameters. A frozen pre-trained GNN backbone generates layer-wise activations that are fused with outputs from a trainable MLP side-network via learnable scaling parameters (α). The MLP learns task-specific adaptations without modifying backbone weights.

### Mechanism 2: Backup Model Merging for Negative Transfer
Merging a randomly-initialized backup model with the pre-trained backbone mitigates negative transfer when source-target domain gaps are large. GMST creates a backup network with identical architecture but random initialization. Layer activations from both backbone and backup are merged via learnable α parameters before fusion with the side-network.

### Mechanism 3: Input/Output Bridges for Arbitrary Dimension Transfer
Input/Output bridges enable arbitrary-dimension transfer without modifying backbone architecture or task configurations. Input Bridge projects variable-dimensional input features to match backbone expectations. Output Bridge applies task-specific prediction heads to transform embeddings to arbitrary output dimensions.

## Foundational Learning

- **Transfer Learning Fundamentals**: GraphBridge builds on pretrain-then-tune paradigm; understanding when knowledge transfers vs. when it interferes (negative transfer) is essential for selecting GSST vs. GMST. Quick check: Given a source model trained on molecular graphs and a target task on citation networks, would you expect positive or negative transfer?

- **Message-Passing in GNNs**: The paper assumes layer-wise GNN activations encode graph structure knowledge; understanding how information propagates through layers explains why layer-wise fusion matters. Quick check: Why might the output of layer 2 contain different structural information than layer 4 in a 5-layer GNN?

- **Parameter-Efficient Fine-Tuning (PEFT)**: GraphBridge is positioned within PEFT paradigms; understanding the design space clarifies why MLP side-networks were chosen over other approaches. Quick check: Compare the trade-offs between adapter modules vs. side-tuning. When would each be preferred?

## Architecture Onboarding

- **Component map**: Pre-training Stage (GraphCL/SimGRACE) -> Input Bridge (Feat-Adapt) -> Efficient-Tuning (GSST/GMST) -> Output Bridge (Task-specific heads)

- **Critical path**: Load pre-trained backbone (frozen) + backup model (frozen, randomly initialized for GMST) -> Route input through Input Bridge -> Parallel forward: backbone activations + side-network outputs -> Fuse at each layer via learnable α -> Output Bridge transforms final embeddings -> Backprop only updates side-network + α parameters

- **Design tradeoffs**: GSST is faster and sufficient for small domain gaps; GMST is necessary for large gaps but adds forward pass cost. MLP side-network chosen for efficiency; ablation shows GNN side-networks add complexity without performance gains.

- **Failure signatures**: Negative transfer shows fine-tuning underperforming scratch training → switch from GSST to GMST. Dimension mismatch errors indicate Input Bridge adapter sizing issues. Memory issues with deep backbones require reduced batch size or GSST.

- **First 3 experiments**: 1) Baseline verification: Replicate Graph2Graph transfer (ZINC → molecular datasets) with GSST; confirm 0.5-1% improvement over fine-tuning. 2) Negative transfer test: Run Graph2Node (HIV → Cora) with both GSST and GMST; validate GMST's 5-10% improvement where GSST fails. 3) Ablation on backbone depth: Test 2-layer vs 5-layer backbones on Node2Node; verify GMST maintains stability.

## Open Questions the Paper Calls Out

- Can the GraphBridge framework maintain its parameter efficiency and performance stability when scaled to a significantly larger and more diverse set of graph benchmarks?
- How can the Output Bridge be adapted to support graph generation or regression tasks, which possess fundamentally different output constraints than the classification tasks evaluated?
- Is random initialization the optimal strategy for the "backup model" in GMST, or could a structured initialization provide better regularization against negative transfer?

## Limitations

- GMST mechanism for negative transfer relies on theoretical reasoning rather than empirical validation against alternative strategies
- Missing ablation study on input/output bridge necessity to isolate their contribution
- Performance claims are relative (vs. full fine-tuning) rather than absolute, making practical significance unclear

## Confidence

- **High Confidence**: Parameter efficiency claim (5-20% tunable parameters) is directly verifiable from architecture specification and ablation studies
- **Medium Confidence**: Framework's ability to handle arbitrary transfer tasks is supported by experiments across 16 datasets
- **Low Confidence**: Theoretical justification for GMST's backup model mechanism lacks validation through ablation studies

## Next Checks

1. Ablation of Bridging Components: Remove Input Bridge and Output Bridge from pipeline and measure performance degradation on Graph2Node tasks to isolate their contribution beyond simple dimension projection
2. GMST vs. Dropout Comparison: Replace backup model with dropout regularization on frozen backbone to test whether GMST's complexity is necessary for negative transfer mitigation
3. Cross-Domain Semantic Transfer: Transfer from molecular graphs to text-attributed social networks to test framework's limits when input feature semantics differ fundamentally