---
ver: rpa2
title: Mitigating the Structural Bias in Graph Adversarial Defenses
arxiv_id: '2504.20848'
source_url: https://arxiv.org/abs/2504.20848
tags:
- graph
- nodes
- adversarial
- degree
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the structural bias in graph neural network
  (GNN) defenses, where current methods perform poorly on nodes with low degree, especially
  under adversarial attacks. The authors propose De2GNN, a defense and debiasing framework
  that includes three key modules: hetero-homo augmented graph construction (removing
  heterophilic links and adding homophilic links for low-degree nodes), kNN augmented
  graph construction (creating an attack-agnostic graph based on node features), and
  multi-view node-wise attention (combining information from both graph views).'
---

# Mitigating the Structural Bias in Graph Adversarial Defenses

## Quick Facts
- **arXiv ID:** 2504.20848
- **Source URL:** https://arxiv.org/abs/2504.20848
- **Reference count:** 40
- **Primary result:** De2GNN achieves state-of-the-art defense performance against adversarial attacks while significantly improving accuracy for low-degree nodes on Cora, Citeseer, and Pubmed datasets.

## Executive Summary
This paper addresses the structural bias problem in graph adversarial defenses, where existing methods perform poorly on low-degree nodes under attack. The authors propose De2GNN, a framework that simultaneously defends against attacks and debiases the model for tail nodes. The approach combines graph purification (removing heterophilic links), surrogate-guided augmentation (adding homophilic links for tail nodes), and multi-view attention fusion (combining structure and feature views). De2GNN demonstrates superior performance on benchmark datasets, achieving significant improvements in both overall accuracy and tail node accuracy under strong Metattack scenarios.

## Method Summary
De2GNN operates through a three-stage pipeline: (1) Graph purification by removing edges with low feature similarity (heterophilic links) to reduce adversarial noise propagation, (2) Surrogate-guided augmentation where a trained GCN identifies and adds homophilic links for low-degree tail nodes, and (3) Multi-view attention fusion that combines information from the purified graph and a kNN graph built from raw features. The method uses a node-wise attention mechanism to dynamically weigh the importance of structure versus feature information for each node individually.

## Key Results
- Achieves state-of-the-art defense performance against Metattack (25% perturbation rate) on Cora, Citeseer, and Pubmed
- Significantly improves accuracy for tail nodes (degree â‰¤ 5) compared to existing defense methods
- Demonstrates superior overall classification accuracy while maintaining robustness under strong attack scenarios

## Why This Works (Mechanism)

### Mechanism 1: Heterophilic Link Pruning for Noise Reduction
- **Claim:** Removing edges with low feature similarity may reduce the propagation of adversarial noise.
- **Mechanism:** The model calculates Jaccard (discrete) or Cosine (continuous) similarity for connected nodes. Links below a threshold $t_1$ (heterophilic links) are removed to purify the graph structure before aggregation.
- **Core assumption:** Adversarial attacks primarily manifest as heterophilic links connecting nodes with dissimilar features, and clean graph structures exhibit higher homophily.
- **Evidence anchors:**
  - [abstract] Mentions removing heterophilic links (links connecting nodes with dissimilar features) globally.
  - [Section V-A1] Details the calculation of similarity $s_{ij}$ and the filtering threshold $t_1$.
  - [corpus] Weak direct evidence for this specific debiasing mechanism; related work generally focuses on robustness without explicit structural bias mitigation.
- **Break condition:** If the original clean graph is inherently heterophilic (dissimilar neighbors are genuine), this pruning will destroy valid structural information.

### Mechanism 2: Surrogate-Guided Neighborhood Enrichment
- **Claim:** Artificially increasing the degree of tail nodes via predicted homophilic links may improve representation quality for low-degree nodes.
- **Mechanism:** A surrogate GNN identifies potential neighbors for tail nodes. If a tail node's prediction confidence is high, it connects to the top-$p$ nodes most likely to share its class, augmenting its sparse neighborhood.
- **Core assumption:** The surrogate GNN trained on the purified graph provides sufficiently accurate pseudo-labels to guide link addition without introducing new noise.
- **Evidence anchors:**
  - [abstract] States the method adds homophilic links for nodes with low degree.
  - [Section V-A2] Describes training $M_{sur}$ and the $N_{top}$ selection process.
  - [corpus] Not explicitly covered in neighbor abstracts.
- **Break condition:** If the surrogate model is also fooled or unstable, it will create "bad" links that reinforce incorrect predictions for tail nodes.

### Mechanism 3: Dual-View Attack-Agnostic Aggregation
- **Claim:** Fusing a structure-based view with a feature-based kNN view allows the model to rely on features when structure is compromised.
- **Mechanism:** A kNN graph is constructed using raw features, ignoring the potentially poisoned adjacency matrix. A node-wise attention mechanism then learns to weigh the "structure-view" (Hetero-Homo) vs. the "feature-view" (kNN) for each node individually.
- **Core assumption:** Node features are more reliable or less perturbed than the graph structure under attack, and different nodes require different balances of structural vs. feature information.
- **Evidence anchors:**
  - [abstract] Describes the kNN augmented graph as attack-agnostic and the attention mechanism for combining views.
  - [Section V-C] Formulates the attention weights $\alpha$ combining $H_{+homo}$ and $H_{kNN}$.
  - [corpus] While "KCES" and "Certification" papers discuss robustness, they do not validate this specific kNN-based attention fusion.
- **Break condition:** If node features are also poisoned (feature attacks), the kNN view will degrade, potentially performing worse than the structure view.

## Foundational Learning

- **Concept: Homophily vs. Heterophily**
  - **Why needed here:** The defense relies on distinguishing "good" (homophilic) links from "bad" (heterophilic/attack) links. You must understand that homophily implies $A_{ij} \approx X_i \sim X_j$.
  - **Quick check question:** Does high Jaccard similarity guarantee a link is *not* an adversarial attack? (Answer: Not necessarily, but it is the heuristic used here).

- **Concept: Long-tail Degree Distribution**
  - **Why needed here:** The core problem is "structural bias" against tail nodes. You need to grasp that in scale-free networks, most nodes have few neighbors, causing message passing failure.
  - **Quick check question:** Why does a standard GCN fail on tail nodes even without attacks? (Answer: Over-smoothing or insufficient neighborhood aggregation).

- **Concept: Surrogate Models**
  - **Why needed here:** The architecture uses a surrogate model to *modify* the graph structure (add links). Understanding that this is a distinct, preliminary training step is vital.
  - **Quick check question:** What happens to the link addition mechanism if the surrogate model overfits the poisoned data?

## Architecture Onboarding

- **Component map:** Perturbed Graph $G'$ -> Filter (Hetero-Homo) -> Surrogate Training -> Augment (tail links) -> kNN Graph Construction -> Two GCN Encoders -> Attention Fusion -> MLP Output

- **Critical path:** The execution flow depends heavily on the **similarity threshold $t_1$** (filtering) and the **surrogate model accuracy**. If the filtering is too aggressive, the graph becomes disconnected; if the surrogate is inaccurate, the tail augmentation creates noise.

- **Design tradeoffs:**
  - **Stability vs. Accuracy:** The kNN graph provides stability but ignores valuable original structure.
  - **Complexity:** Requires training a surrogate model and two GNN encoders, increasing training overhead compared to standard GCN.

- **Failure signatures:**
  - **Accuracy Collapse on Tail Nodes:** Indicates the surrogate model is likely misclassifying tail nodes and enforcing wrong links.
  - **Performance Drop on High-Degree Nodes:** Indicates the heterophilic pruning threshold $t_1$ is too low, removing genuine long-range connections.

- **First 3 experiments:**
  1. **Threshold Sensitivity ($t_1$):** Sweep the similarity threshold on a validation set to find the "sweet spot" between removing noise and retaining edges.
  2. **Surrogate Ablation:** Run the model adding random homophilic links (instead of surrogate-guided) to confirm the value of the learned augmentation.
  3. **Attack Intensity Stress Test:** Incrementally increase the Perturbation Rate (e.g., 0% to 25%) to observe if the kNN view dominates attention weights as the structure becomes noisier.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness depends on untested assumptions about heterophilic link pruning genuinely removing attack noise rather than clean structural information.
- Computational overhead is significant due to training two GCNs plus a surrogate model, potentially limiting scalability.
- Performance on inherently heterophilic datasets (where most edges connect dissimilar nodes) is not evaluated.

## Confidence

- **High Confidence:** The dual-view architecture with attention mechanism is technically sound and well-implemented. The experimental setup using Metattack with 25% perturbation rate is standard and reproducible.
- **Medium Confidence:** The mechanism of using surrogate-guided link addition for tail nodes is plausible but depends heavily on surrogate model accuracy under attack conditions.
- **Low Confidence:** The claim that kNN-based feature similarity provides attack-agnostic robustness requires further validation, especially against feature-level attacks or when node features are also poisoned.

## Next Checks

1. **Structural Sensitivity Analysis:** Systematically vary the heterophilic link pruning threshold $t_1$ and measure the resulting accuracy on both clean and attacked graphs to identify the optimal balance point.
2. **Surrogate Model Robustness:** Compare De2GNN's performance when adding links based on surrogate predictions versus adding random homophilic links to isolate the value of the learned augmentation.
3. **Feature Attack Vulnerability:** Evaluate the method's performance when node features are also perturbed (not just the graph structure) to test whether the kNN view provides genuine attack-agnostic benefits or merely shifts the vulnerability.