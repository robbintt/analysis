---
ver: rpa2
title: 'How PARTs assemble into wholes: Learning the relative composition of images'
arxiv_id: '2506.03682'
source_url: https://arxiv.org/abs/2506.03682
tags:
- patches
- patch
- relative
- learning
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PART, a self-supervised learning approach
  that predicts continuous relative translations between off-grid patches, learning
  the relative composition of images. Unlike grid-based methods, PART samples patches
  freely in location and size, predicting relative relationships rather than absolute
  positions.
---

# How PARTs assemble into wholes: Learning the relative composition of images

## Quick Facts
- arXiv ID: 2506.03682
- Source URL: https://arxiv.org/abs/2506.03682
- Reference count: 40
- Primary result: Off-grid patch sampling with relative translation prediction outperforms MAE and DropPos on COCO object detection (42.4 APb vs 40.1 APb for MAE)

## Executive Summary
PART introduces a self-supervised learning approach that predicts continuous relative translations between off-grid patches, learning the relative composition of images. Unlike grid-based methods, PART samples patches freely in location and size, predicting relative relationships rather than absolute positions. This enables learning robust spatial understanding that generalizes beyond occlusions and deformations. The method demonstrates superior sample efficiency on 1D EEG signal classification while maintaining competitive performance on image classification and object detection.

## Method Summary
PART pretrains Vision Transformers by predicting continuous relative translations (∆x, ∆y) between randomly sampled off-grid patches. The method uses a ViT backbone without position embeddings, sampling N patches of size P×P at random positions and resizing them to uniform P×P. A cross-attention relative encoder predicts translations for a subset of patch pairs, with MSE loss on normalized coordinates. During finetuning, standard position embeddings and grid sampling are added. The approach is validated on COCO object detection, ImageNet/CIFAR-100 classification, and 1D EEG sleep staging.

## Key Results
- COCO object detection: PART achieves 42.4 APb compared to 40.1 for MAE and 42.1 for DropPos
- CIFAR-100 classification: Random sampling outperforms grid sampling (87.0% vs 86.8%)
- EEG sleep staging: Demonstrates superior sample efficiency with fewer labeled samples needed
- Cross-attention relative encoder outperforms pairwise MLP baseline (83.00% vs 82.52% CIFAR-100 accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Predicting continuous relative translations forces the learning of structural relationships rather than absolute spatial priors.
- **Mechanism:** By regressing the (∆x, ∆y) offset between two patches, the model must encode the semantic content of "reference" and "target" patches to infer their geometric relationship. This bypasses the shortcut of memorizing absolute grid positions, leading to features that generalize better to object deformation or translation.
- **Core assumption:** The visual content of a patch contains sufficient information to deduce its spatial relationship to another patch, given a learned understanding of object composition.
- **Evidence anchors:** [abstract] "predicts relative relationships rather than absolute positions... less tied to absolute appearance"; [section 4] "The emphasis on predicting the relative translation is key because the pixel space information is lost after resizing patches... the model needs to learn to be robust to different image resolutions."

### Mechanism 2
- **Claim:** Off-grid sampling creates a natural supervision signal for partial occlusion and scale variance.
- **Mechanism:** Standard grid sampling covers the whole image uniformly. PART's random sampling inherently masks regions and allows patches of varying scales. The model is trained to relate these disparate, partial views, effectively simulating occlusion during pretraining.
- **Core assumption:** Treating patches as independent samples rather than a fixed sequence prevents the ViT from overfitting to fixed spatial resolution or aspect ratios.
- **Evidence anchors:** [section 1] "off-grid sampling inherently results in partial coverage... thereby integrating masking directly into the sampling process"; [section 5.1] "Extension to multiple aspect ratios and scales... slight performance drop likely due to increased parameter and objective complexity."

### Mechanism 3
- **Claim:** The Cross-Attention Relative Encoder facilitates information exchange specifically for pairwise reasoning.
- **Mechanism:** Instead of a simple MLP head that might process patch pairs in isolation, the cross-attention head allows the "query" (concatenated patch pair) to attend to the "keys/values" (all patch embeddings). This allows the model to predict the relative translation of a pair (i, j) by referencing the global context of other patches.
- **Core assumption:** Access to the broader set of patch embeddings improves the precision of relative localization compared to processing pairs independently.
- **Evidence anchors:** [section 4] "The cross-attention module allows for information dissemination between all patch embeddings... This imposes further masking of information given to the model"; [section 5.3 / Table 5] "Cross-attention (83.00%) outperforms pairwise MLP (82.52%)... suggests [it] helps the model focus."

## Foundational Learning

- **Concept:** Vision Transformers (ViT) & Positional Embeddings
  - **Why needed here:** PART fundamentally alters standard ViT usage by removing positional embeddings during pretraining. Understanding how ViTs typically rely on absolute positions is necessary to grasp why PART's "off-grid" approach requires a different mechanism for spatial understanding.
  - **Quick check question:** How does a standard ViT handle spatial relationships without position embeddings, and how does PART solve this differently?

- **Concept:** Self-Supervised Pretext Tasks
  - **Why needed here:** The core of PART is a specific pretext task (regressing relative translations). One must understand that the goal is not to solve the translation task perfectly for its own sake, but to generate a learning signal that produces useful features for downstream tasks like detection.
  - **Quick check question:** Why is regressing a continuous vector (∆x, ∆y) considered a harder or more expressive pretext task than classifying a discrete position index?

- **Concept:** Cross-Attention Mechanics
  - **Why needed here:** The "Relative Encoder" uses a cross-attention block where a patch-pair query attends to the full set of patch embeddings. Distinguishing this from the self-attention in the ViT backbone is critical for implementation.
  - **Quick check question:** In the Relative Encoder, what acts as the Query, Key, and Value?

## Architecture Onboarding

- **Component map:** Image I -> Off-grid Sampler (random P×P patches, resize to P×P) -> ViT Backbone (no position embeddings) -> Patch Embeddings X' -> Pair Selector (subset S) -> Relative Encoder (cross-attention) -> Query: X'S0 || X'S1, Key/Value: X' -> Output: (∆x, ∆y) regression

- **Critical path:** The Off-grid Sampler and the removal of position embeddings are the most critical implementation details. If you use standard grid sampling or accidentally include absolute position embeddings, the model may default to learning absolute shortcuts rather than relative composition.

- **Design tradeoffs:**
  - # of Patch Pairs: The paper finds 2048 pairs is a "sweet spot" (Figure 8). Too few = insufficient supervision; too many = contradicting gradients/computational cost.
  - Sampling Strategy: Random vs. Grid. Table 4 shows Random is strictly better for detection/time-series and slightly better for classification.

- **Failure signatures:**
  - Collapse to Zero: If translations are normalized improperly, the model might predict zero for everything.
  - Absolute Position Memorization: If implementation leaks absolute coordinates, the model will overfit to classification but fail at detection (performance gap between Table 2 and Table 6 would shrink/invert unexpectedly).

- **First 3 experiments:**
  1. Sanity Check (Overfit): Take a single batch of images and verify the model can overfit the relative translation targets (loss → 0). This validates the sampler-encoder pipeline.
  2. Ablation: Grid vs. Off-Grid: Run a short pretraining cycle (e.g., 50 epochs) on CIFAR-100 comparing standard grid sampling vs. PART's off-grid sampling to replicate the trend in Table 4 before committing to large-scale training.
  3. Probing "Symmetry": Visualize the prediction matrix (Figure 6) to ensure the model learns negative symmetry (if A → B is [+x, +y], then B → A is [-x, -y]). This confirms it learned structure, not just noise.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can PART be effectively generalized to model rotation equivariance alongside relative translations? The current method only predicts translations (∆x, ∆y) normalized by scale and does not encode the orientation of parts relative to the whole, limiting robustness to rotational deformations.

- **Open Question 2:** Does incorporating hierarchical, multi-scale sampling or object-aware sampling improve representation quality compared to uniform random sampling? The current implementation uses random uniform sampling without explicitly leveraging the potential semantic hierarchy of objects and their parts.

- **Open Question 3:** Can the PART objective be combined with contrastive learning frameworks to capture both local spatial geometry and global invariance? Contrastive learning typically enforces global invariance to augmentations, while PART enforces sensitivity to local spatial changes.

- **Open Question 4:** How can the training dynamics be stabilized when extending the objective to patches of varying aspect ratios and scales? The extended objective (∆w, ∆h) introduced "objective complexity" that delayed convergence of translation terms (∆x, ∆y).

## Limitations

- The Cross-Attention Relative Encoder's exact architectural details (number of heads, projection dimensions) are underspecified, affecting reproducibility.
- Data augmentation strategies during pretraining are not detailed, potentially influencing downstream performance.
- The EEG application extends PART to 1D signals but represents a single validation point without exploring different signal types or temporal patterns.
- Claims about robustness to occlusions and deformations lack direct ablation studies isolating these specific failure modes.

## Confidence

- **High confidence:** The core mechanism of off-grid sampling with relative translation regression is well-supported by ablation showing Random sampling outperforms Grid across multiple tasks.
- **Medium confidence:** The Cross-Attention Relative Encoder's advantage (83.00% vs 82.52% CIFAR-100 accuracy) is demonstrated but architectural specifics are underspecified.
- **Low confidence:** Claims about PART being "less tied to absolute appearance" and therefore more robust to occlusions and deformations are supported by performance gains but lack direct validation.

## Next Checks

1. **Architectural reproducibility test:** Implement the Cross-Attention Relative Encoder with multiple architectural variants (different head counts, projection sizes) to identify critical design elements driving the 0.5% accuracy improvement over MLP baselines.

2. **Failure mode isolation:** Create controlled experiments with varying degrees of occlusion and deformation to directly measure PART's robustness compared to MAE and DropPos, validating the claimed advantage in handling partial views.

3. **Temporal extension validation:** Apply PART to additional 1D signal domains (e.g., ECG, audio spectrograms) beyond the single EEG sleep staging dataset to establish whether the relative shift mechanism generalizes across temporal data types.