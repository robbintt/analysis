---
ver: rpa2
title: Some things to know about achieving artificial general intelligence
arxiv_id: '2502.07828'
source_url: https://arxiv.org/abs/2502.07828
tags:
- intelligence
- problems
- general
- problem
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that current generative AI models are unlikely
  to achieve artificial general intelligence (AGI) due to what the author calls "anthropogenic
  debt" - the heavy reliance on human input for problem structuring, architecture
  design, training data, and evaluation. The author contends that these models succeed
  largely because humans solve the difficult parts of problems and leave only simple
  computations (like gradient descent) for the models.
---

# Some things to know about achieving artificial general intelligence

## Quick Facts
- arXiv ID: 2502.07828
- Source URL: https://arxiv.org/abs/2502.07828
- Reference count: 0
- Primary result: Current generative AI models are unlikely to achieve AGI due to heavy human input in problem structuring and inadequate benchmarks.

## Executive Summary
The paper argues that current generative AI models cannot achieve artificial general intelligence (AGI) because they rely on substantial human input for problem structuring, architecture design, and evaluation criteria—what the author calls "anthropogenic debt." The models' contribution is limited to parameter optimization through gradient descent, while humans solve the difficult cognitive work of framing problems and providing training data. The paper contends that existing benchmarks cannot distinguish between test-specific solutions (like memorization) and test-general solutions (like reasoning), making them invalid for measuring AGI progress. True AGI requires innovations and discoveries not yet made, and progress toward it is difficult to measure because general intelligence is an ill-structured problem.

## Method Summary
This is a theoretical position paper rather than an experimental study. The author presents logical arguments about why current approaches to AGI are unlikely to succeed, based on analysis of how existing models work and what capabilities they actually demonstrate. The paper references several benchmarks (ARC-AGI, Winogrande, Bongard problems) as examples of evaluation approaches but does not conduct new experiments. Instead, it critiques the methodology of using benchmark performance as evidence for general intelligence, arguing that such metrics cannot distinguish between different solution methods.

## Key Results
- Current models succeed primarily because humans pre-solve the difficult problem-structuring work, leaving only parameter optimization for the model
- Benchmarks cannot distinguish between test-specific solutions (memorization) and test-general solutions (reasoning), making them invalid for measuring AGI progress
- Insight problems requiring representational restructuring are fundamentally inaccessible to current gradient-descent-based learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current GenAI models succeed primarily because humans pre-solve the difficult problem-structuring work, leaving only parameter optimization for the model.
- Mechanism: The paper identifies "anthropogenic debt" — human designers provide training data, architecture, layer types, activation functions, prompts, and evaluation criteria. The model's contribution is limited to gradient descent parameter adjustments within this pre-structured space.
- Core assumption: Problem structuring and representation choice are the cognitively demanding aspects of problem-solving, not the subsequent optimization.
- Evidence anchors:
  - [abstract] "Current models succeed at their tasks because people solve most of the problems to which these models are directed, leaving only simple computations for the model to perform, such as gradient descent."
  - [Page 4] Lists human contributions (training data, number of layers, connection patterns, etc.) vs. machine contribution (parameter adjustments through gradient descent).
  - [corpus] Related paper "Optimisation Is Not What You Need" similarly argues optimization methods alone cannot achieve general cognition, supporting this decomposition.
- Break condition: If a model can autonomously generate its own representations, problem spaces, and evaluation criteria without human specification, this mechanism no longer applies.

### Mechanism 2
- Claim: Benchmarks cannot distinguish between test-specific solutions (memorization, pattern matching) and test-general solutions (reasoning, insight), making them invalid for measuring AGI progress.
- Mechanism: Success on a benchmark demonstrates correct outputs but provides no information about the solution method. The logical fallacy of "affirming the consequent" occurs when inferring general reasoning capability from benchmark performance — the same output could arise from memorization or statistical association.
- Core assumption: General intelligence requires test-general methods that transfer across problem types, not test-specific shortcuts.
- Evidence anchors:
  - [abstract] "It is a logical fallacy (affirming the consequent) to infer a method of solution from the observation of success."
  - [Page 9-10] "Benchmarks are of no value when trying to infer the cause (general intelligence) from its effect (getting the answer correct)... Passing a test does not say whether the test taker knew the answers or just knew the answer key."
  - [corpus] Weak direct corpus support for this specific epistemic critique; primarily anchored in the source paper.
- Break condition: If evaluation methods are developed that can diagnose solution process (not just output), this mechanism would not apply.

### Mechanism 3
- Claim: Insight problems — requiring representational restructuring rather than step-by-step computation — represent a class of problems fundamentally inaccessible to current gradient-descent-based learning.
- Mechanism: Insight problems (e.g., the mutilated checkerboard) cannot be solved by iterative approximation; progress cannot be measured at each step. Solution requires a conceptual restructuring (e.g., the "coloring argument") that transforms the problem representation entirely. Current models lack mechanisms for such restructuring.
- Core assumption: Insight/problem restructuring is a necessary capability for general intelligence and cannot be reduced to pattern completion or optimization.
- Evidence anchors:
  - [Page 5-6] "Until the right structure is achieved, all solutions are wrong... Once we think of it in the right way, the solution to the problem becomes obvious."
  - [Page 6] "Current AI models can grope along the floor, or they can search for a light switch, but unless both strategies are designed into them, they cannot switch from the approach they were designed for to another one."
  - [corpus] "Epistemic Artificial Intelligence is Essential for Machine Learning Models to Truly 'Know When They Do Not Know'" touches on handling unfamiliar domains but does not directly address insight restructuring.
- Break condition: If architectures incorporate meta-level representational change mechanisms (analogical reasoning, conceptual blending), this limitation may not hold.

## Foundational Learning

- Concept: Well-structured vs. ill-structured problems (Simon, 1973)
  - Why needed here: The paper's core argument depends on distinguishing problems with definite solution criteria and problem spaces from those requiring problem formulation as part of solving.
  - Quick check question: Given a new problem, can you identify whether it has a predefined state space and verifiable solution criterion, or does it require framing before solving?

- Concept: Insight problems and representational restructuring
  - Why needed here: The paper argues insight problems cannot be solved by gradient descent; understanding this distinction is essential for evaluating AGI claims.
  - Quick check question: Can you explain why brute-force search cannot solve the mutilated checkerboard problem efficiently, and what kind of cognitive change enables the "coloring argument" solution?

- Concept: Affirming the consequent as an evaluation fallacy
  - Why needed here: Understanding why benchmark success does not validate the presence of general reasoning prevents misinterpretation of model capabilities.
  - Quick check question: If a model passes a reasoning benchmark, what additional evidence would you need to claim it used reasoning rather than memorization?

## Architecture Onboarding

- Component map:
  - Human-designed components: Architecture (layers, attention heads, connection patterns), training data curation, tokenization/embedding scheme, loss functions, training regimen, prompts, hyperparameters (temperature, etc.), evaluation benchmarks
  - Model-executed components: Parameter updates via gradient descent, forward pass inference
  - Missing components (required for AGI per paper): Autonomous representation generation, problem framing, insight/restructuring mechanisms, evaluation criterion generation

- Critical path:
  1. Identify and quantify anthropogenic debt in existing systems
  2. Distinguish which problem classes (well-structured, ill-structured, insight) each component addresses
  3. Design mechanisms for autonomous representation and problem-space construction
  4. Develop evaluation methods that diagnose solution process, not just output

- Design tradeoffs:
  - Benchmark optimization vs. generality measurement: Optimizing for benchmarks may incentivize test-specific shortcuts over general methods
  - Scale vs. capability class: Increasing parameter count may improve pattern matching without enabling insight-type problem solving
  - Human-in-the-loop structuring vs. autonomy: Current performance gains come from human structuring; removing this may reveal capability gaps

- Failure signatures:
  - High benchmark scores with inability to solve novel variations of the same problem class (suggests test-specific methods)
  - Requiring extensive prompt engineering to achieve stated performance (anthropogenic debt)
  - Inability to explain or generalize the reasoning behind correct answers
  - Performance collapse on problems requiring representational change outside training distribution

- First 3 experiments:
  1. Holdout set analysis: Train models on benchmarks, then test on structurally equivalent but superficially novel problems that require the same insight. Gap indicates test-specific vs. test-general solutions.
  2. Anthropogenic debt audit: Systematically remove human-designed components (e.g., custom prompts, curated training subsets) and measure performance degradation to quantify human contribution.
  3. Insight problem probe: Construct novel insight problems (not in training data) where solution requires representational restructuring. Compare model performance against humans to identify whether the capability is present.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do current GenAI models demonstrate genuine cognitive capacity, or do they merely mimic language patterns from their training data?
- Basis in paper: [explicit] The author explicitly asks whether models "actually demonstrate some cognitive capacity that would be part of general intelligence, or do they simply mimic language patterns."
- Why unresolved: Current benchmarks only measure outcomes (success), failing to distinguish between general cognitive methods and test-specific pattern matching.
- What evidence would resolve it: Controlled experiments that distinguish between associative rules and conceptual understanding, similar to 1980s cognitive science methodologies.

### Open Question 2
- Question: How can "anthropogenic debt"—the heavy reliance on human input for problem structuring—be quantified and reduced?
- Basis in paper: [explicit] The author states that analyzing anthropogenic debt is the "most immediate task that would generate substantial value" for the field.
- Why unresolved: There is currently no framework to measure the ratio of human-provided structure (architecture, training data, prompts) versus machine-generated solution steps.
- What evidence would resolve it: A formal decomposition of problem-solving tasks that isolates which components were autonomously generated by the model versus pre-structured by humans.

### Open Question 3
- Question: How can progress toward AGI be measured if general intelligence is an ill-structured problem?
- Basis in paper: [inferred] The paper argues that general intelligence is ill-structured and lacks a definite criterion for testing, making current benchmarks inadequate.
- Why unresolved: AGI may require discontinuous innovations rather than gradual optimization, rendering scalar metrics (like accuracy on specific tasks) misleading.
- What evidence would resolve it: The development of evaluation methods that test a system's ability to autonomously formulate and structure novel problems, rather than just solving them.

## Limitations
- The paper presents theoretical arguments rather than empirical evidence, limiting direct validation through replication
- The concept of "anthropogenic debt" lacks operational definitions or quantitative measurement frameworks
- The critique of benchmarks assumes test-general solutions exist as a coherent category without empirical validation

## Confidence
- High confidence: The logical structure of the "affirming the consequent" critique of benchmark evaluation is sound and well-established in philosophy of science
- Medium confidence: The claim that current models rely heavily on human problem structuring appears correct based on observable design patterns
- Low confidence: The assertion that insight problems are fundamentally inaccessible to current approaches lacks empirical validation

## Next Checks
1. **Anthropogenic debt audit**: Systematically remove human-designed components from existing systems and measure performance degradation to quantify the extent of human contribution versus model capability
2. **Insight problem probe**: Construct novel insight problems outside training distributions and compare model performance against humans to determine whether the capability gap is as fundamental as claimed
3. **Benchmark generalization test**: Design experiments that can distinguish between test-specific solutions (memorization, pattern matching) and test-general solutions (reasoning, insight) using held-out variants and counterfactual examples