---
ver: rpa2
title: Bridging the Gap Between Preference Alignment and Machine Unlearning
arxiv_id: '2504.06659'
source_url: https://arxiv.org/abs/2504.06659
tags:
- unlearning
- performance
- arxiv
- preference
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges the gap between preference alignment (PA) and
  machine unlearning (MU) for large language models (LLMs). While RLHF is effective
  for PA, it is resource-intensive and unstable, whereas MU offers a more efficient
  alternative by removing negative examples.
---

# Bridging the Gap Between Preference Alignment and Machine Unlearning

## Quick Facts
- arXiv ID: 2504.06659
- Source URL: https://arxiv.org/abs/2504.06659
- Reference count: 34
- One-line primary result: U2A significantly enhances PA performance while maintaining unlearning effectiveness and reducing computational costs compared to traditional PA methods.

## Executive Summary
This paper bridges the gap between preference alignment (PA) and machine unlearning (MU) for large language models. While RLHF is effective for PA, it is resource-intensive and unstable, whereas MU offers a more efficient alternative by removing negative examples. The authors propose a bi-level optimization framework to quantify how unlearning specific negative examples impacts PA performance. They find that not all negative examples equally improve PA, and the impact varies based on example selection and weighting. To address this, they introduce Unlearning to Align (U2A), a framework that uses bi-level optimization to optimally select and weight negative examples for unlearning. Extensive experiments on multiple models and datasets confirm that U2A significantly enhances PA performance while maintaining unlearning effectiveness, and reduces computational costs compared to traditional PA methods.

## Method Summary
The U2A framework uses bi-level optimization where the inner loop optimizes unlearning weights for negative examples while the outer loop evaluates PA performance. The method employs gradient ascent unlearning with L₂ regularization to preserve capabilities, combined with L₁/₂ sparsity regularization to select a compact subset of high-impact negative examples. The framework uses conjugate gradient for implicit gradient computation through the inner optimization, enabling efficient identification of optimal unlearning candidates without exhaustive evaluation. The approach is validated on safety, helpfulness, and hallucination tasks using Llama-2 and Llama-3.1 models with a Beaver-7B reward model.

## Key Results
- U2A significantly enhances PA performance compared to unmodified unlearning baselines (GA, NPO, GradDiff) while maintaining comparable unlearning effectiveness.
- The method achieves better Reward-value, ASR, and Win-rate metrics while reducing computational costs compared to traditional PA methods like PPO and DPO.
- Not all negative examples contribute equally to alignment improvement; U2A's sample selection and weighting strategy identifies the most impactful examples for unlearning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unlearning improves Preference Alignment (PA) when the unlearning gradient opposes the PA gradient direction for low-reward token combinations.
- Mechanism: Each sample contains multiple token combinations (context, token pairs). For combinations with low reward (deviating from human preferences), the unlearning gradient (increasing generation probability) tends to oppose the PA gradient direction, resulting in positive PA improvement when unlearned. The impact magnitude scales with the proportion of low-reward tokens in the sample.
- Core assumption: The reward model accurately captures human preferences, and gradient direction alignment reliably predicts unlearning efficacy.
- Evidence anchors:
  - [abstract] "Our analysis reveals that not all negative examples contribute equally to alignment improvement when unlearned, and the effect varies significantly across examples."
  - [section 4.1] "For low-reward combinations... the unlearning objective gradient direction is more likely to oppose the PA objective gradient direction. This results in cos(ϕ) < 0 and ∆J(θ*(ω)) > 0."
  - [corpus] Related work "CATNIP" and "NPO" papers corroborate gradient-based unlearning approaches but lack direct validation of the gradient direction alignment hypothesis.
- Break condition: If the reward model is poorly calibrated or the model has already overfit to negative examples, gradient norms may be near-zero, causing unlearning to have negligible PA impact.

### Mechanism 2
- Claim: Bi-level optimization with implicit gradient computation enables efficient identification of high-impact unlearning candidates without exhaustive evaluation.
- Mechanism: The inner loop computes optimal parameters θ*(ω) for given unlearning weights. The outer loop evaluates PA performance J(θ*(ω)). Using the implicit function theorem, gradients ∂θ*/∂ω are computed via Hessian-vector products (avoiding explicit Hessian inversion via conjugate gradient). Marginal gain Δg(k) ranks candidates for greedy selection.
- Core assumption: The inner objective is strictly convex w.r.t. θ, and first-order optimality conditions accurately characterize the solution manifold.
- Evidence anchors:
  - [section 4.2] "According to the implicit function theorem, the gradient of g(ω) w.r.t. ω can be expressed as: ∇ωg(ω) = ∇θJ(θ*(ω))[∂²f/∂θ²]⁻¹ Σᵢ ωᵢ∇θℓᵢ(θ*(ω)) + β/2 ω⁻¹/²"
  - [section 4.2] Lemma 4.3 provides suboptimality bound O(1/t), Theorem 4.4 bounds unlearning set size.
  - [corpus] Weak direct corpus support for bi-level optimization in unlearning; most related work uses fixed unlearning sets without sample selection.
- Break condition: If the Hessian is ill-conditioned or the objective is highly non-convex, conjugate gradient may fail to converge, leading to incorrect marginal gain estimates.

### Mechanism 3
- Claim: Sparsity-inducing L₁/₂ regularization ensures selective unlearning of a compact subset rather than indiscriminate removal of all negative examples.
- Mechanism: The regularization term Lp(ω) = Σᵢ √ωᵢ promotes sparsity by penalizing small weights less aggressively than L₁ but more than L₂. This creates a Pareto frontier between PA improvement (more samples) and computational cost/unintended capability degradation (fewer samples).
- Core assumption: The optimal unlearning set is sparse, and a small subset captures most PA improvement potential.
- Evidence anchors:
  - [section 4.2] "When p = 1/2, the regularization term Lp(ω) is both a strictly convex function and exhibits good smoothness."
  - [Figure 2] Empirical threshold analysis shows samples with >60% low-reward tokens (PKU SafeRLHF) yield positive PA changes.
  - [corpus] "LUNE" paper uses LoRA for efficient unlearning but does not address sample selection sparsity.
- Break condition: If most negative samples have similar low-reward token proportions, sparsity constraints may force exclusion of beneficial samples.

## Foundational Learning

- Concept: **Bi-level Optimization / Meta-Gradient Computation**
  - Why needed here: U2A requires computing gradients through an inner optimization loop to evaluate how unlearning weight changes affect PA performance.
  - Quick check question: Given inner solution θ* = argmin f(θ, ω), can you derive ∂θ*/∂ω using only Hessian-vector products without materializing the full Hessian?

- Concept: **Gradient Ascent Unlearning with Regularization**
  - Why needed here: The inner unlearning objective uses gradient ascent on negative log-likelihood (to forget) combined with L₂ parameter regularization (to preserve unrelated capabilities).
  - Quick check question: Why does gradient ascent on NLL cause unlearning, and what does the regularization term λ||θ - θ*||² prevent?

- Concept: **Preference Alignment Reward Modeling (Bradley-Terry)**
  - Why needed here: PA performance J(θ) is evaluated via a learned reward model that assigns higher scores to preferred responses, enabling gradient-based assessment of alignment.
  - Quick check question: Given preference pairs (x_t ≻ x̂_t), how does the Bradley-Terry model convert rewards to preference probabilities?

## Architecture Onboarding

- Component map:
  Input: Negative sample pool D = {x₁...xₙ}, base model πθ*, reward model r(·)
  ↓
  [Initialization] Select first sample randomly, ω₁ = [1, 0, ..., 0]
  ↓
  [Inner Loop] Gradient descent on ω·L_forget + λ·L_reg → θ*(ω)
  ↓
  [Outer Loop] Projected gradient descent on -J(θ*(ω)) + β·Lₚ(ω) → ω*
  ↓
  [Marginal Gain] Conjugate gradient for Hessian⁻¹·∇θℓₖ → select k* = argmax Δg(k)
  ↓
  [Update] Add k* to unlearning set S, set ωₖ = 1, repeat until convergence
  ↓
  [Final Re-optimization] Re-solve weights on final set S_final
  ↓
  Output: Unlearning set S_final, optimal weights ω_final*, aligned model

- Critical path:
  1. Inner optimization convergence (gradient descent on unlearning objective)
  2. Implicit gradient computation via conjugate gradient (Hessian-vector products)
  3. Marginal gain ranking and greedy selection
  4. Outer weight optimization on selected subset

- Design tradeoffs:
  - **Sparsity (β) vs. PA improvement**: Higher β → smaller unlearning set → faster but potentially suboptimal PA
  - **Regularization (λ) vs. forgetting efficacy**: Higher λ → better capability preservation but weaker unlearning
  - **Iterations (T) vs. computational cost**: More iterations → better convergence but linear time increase
  - **Gradient descent steps (t_f) vs. inner solution accuracy**: More steps → better θ* but each outer iteration slower

- Failure signatures:
  - **Divergent inner loop**: If λ is too small, model collapses; increase λ or add early stopping on perplexity
  - **Zero marginal gains**: All candidates have similar Δg → check reward model calibration or expand candidate pool
  - **PA degradation after unlearning**: Selected samples have low low-reward token ratio → increase selection threshold
  - **Conjugate gradient non-convergence**: Hessian ill-conditioned → add damping or use finite-difference approximation

- First 3 experiments:
  1. **Single-sample impact baseline**: For each negative sample in a held-out set, compute ΔJ after unlearning (ω=1) and correlate with low-reward token proportion to validate Proposition 4.2. Expect: samples with >threshold low-reward tokens show positive ΔJ.
  2. **U2A vs. unmodified baselines**: Compare GA/NPO/GradDiff with and without U2A selection on PKU SafeRLHF. Measure Reward-value, ASR, MIA, and PPL. Expect: U2A variants achieve higher Reward-value with comparable or better unlearning (MIA) and lower PPL.
  3. **Ablation on sparsity parameter β**: Sweep β ∈ {0.1, 0.5, 1.0, 2.0} and plot final unlearning set size vs. PA improvement. Expect: Pareto curve with diminishing returns beyond optimal β.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimal negative example subsets and weights identified by the U2A framework for one model architecture (e.g., Llama-2) be effectively transferred to different model architectures or sizes without re-running the bi-level optimization?
- Basis in paper: [inferred] The experiments conduct independent optimizations for Llama-2-7B and Llama-3.1-8B. The theoretical derivation suggests weights depend on $\theta^*$, implying transferability is not guaranteed, yet practical transferability would vastly improve efficiency.
- Why unresolved: The paper validates U2A on specific models individually but does not experiment with cross-model weight transfer, leaving the generalizability of the selected "unlearning set" unknown.
- What evidence would resolve it: Experiments showing that weights $\omega$ derived from Model A yield comparable PA performance improvements when applied to the unlearning process of Model B.

### Open Question 2
- Question: How does the accuracy of the external reward model used in the outer optimization loop ($J(\theta)$) impact the convergence and final alignment quality of the U2A framework?
- Basis in paper: [inferred] The method relies on a proxy function $J(\theta)$ (approximated by a Beaver-7B reward model) to guide the bi-level optimization. Any bias or inaccuracy in this reward model could theoretically lead to the selection of sub-optimal negative examples.
- Why unresolved: The paper assumes the existence of a suitable evaluation metric $J(\theta)$ for the outer loop but does not analyze the framework's robustness to noise or misalignment in this specific reward signal.
- What evidence would resolve it: A sensitivity analysis measuring the degradation of U2A's performance when the reward model used for selection is systematically biased or has limited correlation with ground-truth human preferences.

### Open Question 3
- Question: Does the "low-reward token" proportion heuristic hold as a reliable predictor of unlearning utility for more nuanced alignment tasks, such as reducing subtle reasoning errors or stylistic biases?
- Basis in paper: [inferred] The paper identifies a correlation between the proportion of low-reward tokens and positive PA impact in safety/hallucination tasks. It is unresolved if this specific signal generalizes to alignment dimensions where "negative" behavior is less discrete.
- Why unresolved: The empirical analysis focuses on safety (PKU SafeRLHF) and hallucinations (HaluEval), where negative examples are distinct. It is unclear if the gradient direction relationship (cosine similarity) behaves similarly for softer alignment constraints.
- What evidence would resolve it: Application of the U2A framework and the low-reward token analysis to datasets focused on reasoning or subtle bias, verifying if the same selection thresholds apply.

## Limitations

- **Gradient direction hypothesis dependence**: The core mechanism assumes reward model accuracy and meaningful gradient directions for low-reward tokens. If the reward model is poorly calibrated or the model has already overfit to negative examples, gradient norms may be near-zero, causing unlearning to have negligible PA impact.
- **Bi-level optimization computational overhead**: While U2A reduces computational costs compared to RLHF, the bi-level optimization framework still requires multiple iterations of inner and outer optimization, with unknown conjugate gradient and gradient descent step counts.
- **Sparsity constraint sensitivity**: The L₁/₂ regularization assumes the optimal unlearning set is sparse, but if most negative samples have similar low-reward token proportions, sparsity constraints may force exclusion of beneficial samples.

## Confidence

- **High confidence**: Bi-level optimization framework implementation and mathematical derivations (Proposition 4.2, Lemma 4.3, Theorem 4.4) are sound and well-supported by the literature on meta-learning and implicit differentiation.
- **Medium confidence**: The empirical validation on three datasets shows consistent PA improvements, but the sample sizes (PKU SafeRLHF: 1.1K, UltraFeedback: 19K, HaluEval: 7K) and hyperparameter settings (λ=1.0, β=0.5) may not generalize to larger or more diverse datasets.
- **Low confidence**: The claim that U2A "significantly enhances PA performance while maintaining unlearning effectiveness" relies heavily on the specific choice of reward model (Beaver-7B-v3.0-Reward) and may not hold for other reward models or more complex preference alignment tasks.

## Next Checks

1. **Reward model sensitivity analysis**: Test U2A with alternative reward models (e.g., DPO-tuned models) to verify that the gradient direction alignment mechanism holds across different preference modeling approaches.
2. **Scalability benchmark**: Apply U2A to larger datasets (e.g., UltraFeedback with full 80K samples) and larger models (e.g., Llama-3.1-70B) to assess computational overhead and PA improvement scaling.
3. **Ablation on sparsity parameter β**: Systematically sweep β ∈ {0.1, 0.5, 1.0, 2.0} across all three datasets to quantify the Pareto frontier between PA improvement and unlearning set size, validating the L₁/₂ regularization's effectiveness.