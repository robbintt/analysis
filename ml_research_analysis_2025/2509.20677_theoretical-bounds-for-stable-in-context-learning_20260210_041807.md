---
ver: rpa2
title: Theoretical Bounds for Stable In-Context Learning
arxiv_id: '2509.20677'
source_url: https://arxiv.org/abs/2509.20677
tags:
- stability
- spectral
- in-context
- learning
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes non-asymptotic sample complexity bounds
  linking ICL stability to spectral coverage of demonstration representations. Under
  sub-Gaussian feature assumptions, it derives explicit sufficient conditions on prompt
  length K based on the minimum eigenvalue of the regularized empirical second-moment
  matrix.
---

# Theoretical Bounds for Stable In-Context Learning

## Quick Facts
- arXiv ID: 2509.20677
- Source URL: https://arxiv.org/abs/2509.20677
- Authors: Tongxi Wang; Zhuoyang Xia
- Reference count: 40
- Primary result: Non-asymptotic sample complexity bounds linking ICL stability to spectral coverage of demonstration representations

## Executive Summary
This paper establishes non-asymptotic sample complexity bounds linking ICL stability to spectral coverage of demonstration representations. Under sub-Gaussian feature assumptions, it derives explicit sufficient conditions on prompt length K based on the minimum eigenvalue of the regularized empirical second-moment matrix. A two-stage observable estimator is proposed that estimates required spectral statistics from a small pilot sample and returns a concrete K with prescribed failure probability. Experiments across diverse datasets, encoders, and commercial API models show the theory conservatively upper-bounds empirical accuracy knee-points, with validation-only calibration tightening the gap to 1.03-1.20×. Direct distributional stability measurements validate the intended stability interpretation, demonstrating close alignment between predicted thresholds and observed transitions.

## Method Summary
The method uses a two-stage estimator to compute prompt length K for stable ICL. Stage 1 draws K₀ demonstrations, computes the empirical second-moment matrix Σ̂K₀ from encoder embeddings, adds ridge regularization ρI, and estimates spectral statistics (λmin, ‖Σ̂‖, reff). A matrix Bernstein inequality provides a lower confidence bound λ on the population eigenvalue. If λ > 0, Stage 2 computes K via a plug-in formula involving these statistics and the target spectral floor δ. The final K_final = K₀ + K. Optional calibration learns a global scale factor α from validation knee-points without accessing test data, tightening predictions while preserving conservative ordering.

## Key Results
- Raw theoretical bounds yield error ratios K*/Kknee of 1.3-2.5× across diverse datasets and models
- Calibration variant tightens ratios to 1.03-1.20× while maintaining conservative ordering
- Distributional stability via JSD shows close alignment between predicted thresholds and observed transitions
- Numerical experiments demonstrate stability under perturbation when using ridge regularization and quantile surrogates
- The two-stage estimator successfully predicts stability transitions in LLM behavior

## Why This Works (Mechanism)

### Mechanism 1: Spectral Floor Controls Inverse Geometry Sensitivity
- **Claim:** A lower bound on the minimum eigenvalue of the regularized empirical second-moment matrix limits how much a ridge-style predictor varies when demonstrations are resampled.
- **Mechanism:** When λmin(Σ̂K,ρ) ≥ δ, the inverse (Σ̂K,ρ)⁻¹ is well-conditioned. Lemma 3.4 shows this yields ‖ŵ(SK) − ŵ(S'K)‖₂ ≤ 2B/δ + B‖Σ̂K − Σ̂'K‖/δ², bounding sensitivity to demonstration perturbations under a linearized ICL view.
- **Core assumption:** ICL output depends on demonstrations primarily through a low-dimensional statistic (e.g., attention-weighted averages or a fitted linear probe), and model output is Lipschitz in the ridge parameter.
- **Evidence anchors:**
  - [abstract]: "links spectral coverage of demonstration representations to ICL stability"
  - [section 3.4]: "When λmin(Σ̂K) is small, the induced inverse geometry is poorly conditioned: small perturbations...can be greatly amplified"
  - [corpus]: Related work on mechanistic ICL (Hendel et al., 2023; Dai et al., 2023) links internal representations to ICL behavior, supporting the linearization premise, though direct equivalence is not established.
- **Break condition:** If the true ICL mechanism does not admit a ridge-linear approximation (e.g., heavily non-linear attention dynamics dominate), the proxy may not correlate with stability.

### Mechanism 2: Matrix Concentration Yields Non-Asymptotic Sample Complexity
- **Claim:** Under sub-Gaussian representations, a sufficient prompt length K can be computed from observable spectral statistics with prescribed failure probability.
- **Mechanism:** A one-sided matrix Bernstein inequality controls the lower tail of eigenvalues. Proposition 3.3 gives K ≳ (∥Σ∥² · r_eff · log(r_eff/ξ)) / Δρ², where Δρ = λmin(Σ) + ρ − δ is the spectral margin.
- **Core assumption:** Feature vectors ϕ(x) are sub-Gaussian with bounded ∥ϕ(x)∥ψ₂ ≤ σ and Σ = E[ϕ(x)ϕ(x)⊤] exists (Assumption A1).
- **Evidence anchors:**
  - [abstract]: "non-asymptotic lower bound linking spectral coverage...to ICL stability, expressed as an explicit sufficient condition on the required prompt length"
  - [section 3.2]: "The leading dependence in (4) is the familiar K ≍ (scale)² × (effective dimension) × log(1/ξ) / (spectral margin)²"
  - [corpus]: Corpus shows related theoretical work on ICL sample complexity (Bai et al., 2023; Jeon et al., 2024) but none provide the same computable, non-asymptotic bound.
- **Break condition:** If representations exhibit heavy tails or strong dependence violating sub-Gaussian assumptions, the bound may underestimate required K. Appendix F discusses extensions but these are not proven in the main theorem.

### Mechanism 3: Quantile Eigenvalues and Calibration Reduce Conservatism
- **Claim:** Using a quantile eigenvalue λq(Σ̂K,ρ) instead of λmin, plus a global scale factor α, tightens predictions while preserving conservative ordering.
- **Mechanism:** The extreme tail of the spectrum is sensitive to numerical noise and near-null directions. Quantile surrogates (q ∈ [0.05, 0.2]) reduce this sensitivity. Calibration learns α from validation knees without accessing test data.
- **Core assumption:** The monotonic relationship between spectral coverage and stability holds across a range of spectral proxies, and calibration tasks are representative.
- **Evidence anchors:**
  - [abstract]: "calibrated variant further tightens this gap"
  - [section 3.5]: "calibration is introduced as a practical correction for proxy–generator mismatch...it does not change the non-asymptotic guarantee for the uncalibrated proxy bound"
  - [section 4.3, Table 2]: CV of K* drops from 0.98 to 0.13 under perturbation when using λq vs λmin
  - [corpus]: No direct corpus evidence on quantile eigenvalue surrogates for ICL.
- **Break condition:** If the encoder-generator alignment varies substantially across tasks, a single global calibration factor may under- or over-correct.

## Foundational Learning

- **Concept: Sub-Gaussian concentration and effective rank**
  - **Why needed here:** The main theorem relies on matrix Bernstein bounds under sub-Gaussian features; understanding how tail behavior and dimensionality affect sample complexity is essential for interpreting the bound.
  - **Quick check question:** Given a d-dimensional feature with covariance Σ, can you explain why r_eff = tr(Σ)/‖Σ‖ appears in the bound instead of raw dimension d?

- **Concept: Ridge regularization and condition number**
  - **Why needed here:** The proxy uses Σ̂K,ρ = Σ̂K + ρI to stabilize eigenvalue computation and connect to ridge-regression views of ICL. Understanding why small eigenvalues cause inverse instability is central.
  - **Quick check question:** If λmin(Σ̂K) = 10⁻⁶ and ρ = 10⁻⁴, what is λmin(Σ̂K,ρ) and why does this improve numerical stability?

- **Concept: Distributional stability vs accuracy saturation**
  - **Why needed here:** The paper uses accuracy knee-points as a scalable surrogate for the true stability criterion (Definition 3.1), but these are not equivalent. Understanding the distinction prevents over-claiming.
  - **Quick check question:** Could a model have stable output distributions (low JSD across resampled prompts) while having low accuracy? What does this imply for interpreting K*?

## Architecture Onboarding

- **Component map:** External encoder e(·) → Pilot stage (K₀ demos, compute Σ̂K₀,ρ) → Lower confidence bound (λ = λ̂₀ - C‖Σ̂‖√(reff log(1/ξ)/K₀)) → Main stage (compute K) → Calibration (learn q, α) → K_final
- **Critical path:**
  1. Choose encoder and extract embeddings for demonstration pool
  2. Set hyperparameters: ξ (failure tolerance, e.g., 0.1), δ (target floor, e.g., fraction of pilot spectrum), ρ (ridge level, e.g., 10⁻⁶ · tr(Σ̂)/d)
  3. Run Algorithm 1 to get K*. If infeasible, reduce δ or increase ρ
  4. Optionally calibrate q and α on held-out validation tasks
- **Design tradeoffs:**
  - **Conservatism vs tightness:** Raw bound yields ratios 1.3–2.5×; calibration tightens to 1.03–1.20× but relies on proxy–generator alignment
  - **λmin vs λq:** λmin is theoretically grounded but noisy; λq is more stable but requires choosing q
  - **Encoder choice:** Different encoders yield different spectra; the paper tests three sentence encoders but does not prove invariance
- **Failure signatures:**
  - **Infeasible target:** Algorithm returns failure when λ ≤ δ, meaning requested floor exceeds estimated population floor
  - **High CV under perturbation:** Using λmin without ridge or quantile yields unstable K* (Table 2)
  - **Systematic over-prediction on reasoning tasks:** Raw ratios reach 2.0–2.5× for GSM8K/HotpotQA, suggesting higher demo sensitivity not fully captured by encoder geometry
- **First 3 experiments:**
  1. **Reproduce SST-2 knee and K*:** Use all-mpnet-base-v2, K₀ = 100, ξ = 0.1, δ = 0.1 · ‖Σ̂‖. Compare K* to empirical knee via two-segment fit. Target: raw ratio ~1.3–1.5×
  2. **Numerical sensitivity stress test:** Add Gaussian perturbations ε ∈ {10⁻⁶, 10⁻⁴, 10⁻³} to Σ̂K and recompute K* under λmin, λmin + ρI, and λq. Target: CV drops from ~1.0 to <0.2 with ridge/quantile
  3. **Calibration ablation:** Hold out 2 tasks for calibration, learn (q, α), evaluate on remaining tasks. Compare raw vs calibrated ratios. Target: calibrated ratios within 1.05–1.20×

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the non-asymptotic sample complexity bounds change when the representation map is not fixed but evolves dynamically based on the prompt context?
- **Basis in paper:** [explicit] The Conclusion states, "Future work may relax the assumption of fixed representations, refine calibration under more dynamic settings..."
- **Why unresolved:** The theoretical derivations (Proposition 3.3) rely on Assumption (A1) of fixed sub-Gaussian features or bounded drift (A2). Contextualized representations in deeper layers of LLMs may violate these independence or boundedness constraints.
- **What evidence would resolve it:** A theoretical extension of the matrix Bernstein bounds that accounts for sequence-dependent covariance structures $\Sigma_t$, showing the specific inflation factor for $K$ under non-stationary embeddings.

### Open Question 2
- **Question:** Can the theoretical framework and spectral coverage proxy be extended to multi-modal in-context learning (e.g., vision-language models) while maintaining the same error ratios?
- **Basis in paper:** [explicit] The Conclusion lists "extend the framework to multi-modal ICL" as a specific direction for future work.
- **Why unresolved:** The current experiments and theoretical assumptions (specifically sub-Gaussian features and fixed encoders) are validated only on text-based tasks. Multi-modal spaces may exhibit different spectral properties (e.g., heavy tails in image embeddings) that break the current conservative bounds.
- **What evidence would resolve it:** Empirical validation of the spectral proxy on multi-modal benchmarks (e.g., visual QA) demonstrating that the predicted $K^*$ still conservatively upper-bounds empirical knee-points, or a theoretical derivation of the necessary scaling for cross-modal representations.

### Open Question 3
- **Question:** Can the calibration factor $\alpha$ be determined theoretically from the alignment between the external encoder and the generator's internal representations, rather than through empirical fitting?
- **Basis in paper:** [inferred] The paper introduces calibration (Section 3.5) as a "practical correction" and "empirical tightening step," acknowledging that the raw theoretical bounds are "intentionally conservative" due to proxy–generator mismatch.
- **Why unresolved:** While the two-stage estimator is fully observable, the calibration step relies on a validation set to learn a global scale. The paper does not offer a first-principles derivation for why the calibration constants (typically 1.03–1.20×) are stable across tasks.
- **What evidence would resolve it:** A theoretical model mapping the spectral geometry of the frozen external encoder to the generator's internal attention dynamics, accurately predicting the required scaling constant $\alpha$ without data fitting.

### Open Question 4
- **Question:** Does the linearized ridge-regression view of ICL limit the tightness of the spectral coverage proxy for non-linear reasoning tasks?
- **Basis in paper:** [inferred] The theoretical justification (Section 3.4) relies on a "linearized ridge-regression view" to link spectral coverage to stability, while the experiments show higher error ratios (Raw $\approx 2.0\times$) for reasoning tasks compared to classification.
- **Why unresolved:** Reasoning tasks may rely on non-linear dynamics not captured by the ridge regression proxy, potentially explaining why the raw theoretical bound is looser for GSM8K compared to SST-2.
- **What evidence would resolve it:** A comparative analysis showing that for tasks with high non-linearity (identified by probing internal attention heads), the spectral proxy systematically overestimates $K$ more than in linear classification tasks, suggesting the need for a non-linear extension of the bound.

## Limitations

- The theoretical link between spectral coverage and ICL stability relies on a ridge-regression linearization that may not capture all non-linear LLM dynamics, particularly for reasoning tasks where raw ratios are highest
- The calibration variant provides tighter predictions (1.03-1.20×) but relies on proxy-generator alignment without theoretical guarantees, potentially limiting generalizability
- Implementation requires unknown universal constants (C, C', c) and target floor δ selection protocol, with infeasible targets occurring when requested δ exceeds population floor

## Confidence

**Cluster 1: Theoretical bounds and mechanisms** - **Medium**
- The spectral concentration bounds and ridge-stability link are mathematically sound under stated assumptions, but direct validation of the ICL linearization assumption is lacking

**Cluster 2: Empirical validation and calibration** - **Medium**
- The theory conservatively bounds accuracy knees across diverse settings, but systematic over-prediction on reasoning tasks and reliance on proxy-generator alignment without guarantees limit confidence

**Cluster 3: Practical estimator implementation** - **Medium**
- The two-stage estimator is implementable and provides non-asymptotic guarantees, but unknown constants, δ selection protocol, and potential infeasibility conditions require careful handling

## Next Checks

- **Check 1: Validate ICL linearization assumption** - Measure how closely ICL output statistics (e.g., attention-weighted demonstration embeddings) correlate with ridge-regression predictions across different task types and model architectures. Test whether the linearization breaks down for reasoning tasks where raw ratios are highest.

- **Check 2: Characterize unknown constants through controlled experiments** - Run synthetic experiments with known Gaussian distributions to empirically calibrate the universal constants C, C', and c. Determine their sensitivity to feature dimension, sub-Gaussian parameter σ, and effective rank reff.

- **Check 3: Test δ selection protocol and infeasibility handling** - Systematically sweep δ values on a subset of tasks to determine optimal selection criteria. Document frequency and characteristics of infeasible target occurrences, and evaluate whether quantile-floor variants (λq) provide robust fallback when λmin-based targets fail.