---
ver: rpa2
title: Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label
  Classification
arxiv_id: '2509.17747'
source_url: https://arxiv.org/abs/2509.17747
tags:
- image
- learning
- multi-label
- classification
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel Dual-View Alignment Learning with Hierarchical-Prompt
  (HP-DV AL) method to address class-imbalance in multi-label image classification
  tasks. The approach leverages vision-language pretrained models, using dual-view
  alignment learning to extract complementary features for accurate image-text alignment.
---

# Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification

## Quick Facts
- **arXiv ID:** 2509.17747
- **Source URL:** https://arxiv.org/abs/2509.17747
- **Reference count:** 40
- **Primary result:** Achieves 10.0% mAP improvement on long-tailed multi-label image classification tasks

## Executive Summary
This paper introduces a novel approach called HP-DVAL (Hierarchical-Prompt Dual-View Alignment Learning) to address class-imbalance in multi-label image classification. The method leverages vision-language pretrained models by extracting complementary global and local features through a dual-view alignment learning framework. A hierarchical prompt-tuning strategy is implemented with separate global and local prompts, supported by a semantic consistency loss to prevent catastrophic forgetting of general knowledge during fine-tuning. The approach demonstrates significant performance improvements on standard benchmarks like MS-COCO and VOC2007, particularly for underrepresented classes in long-tailed and few-shot scenarios.

## Method Summary
The method consists of a two-stage training process built on top of CLIP's vision-language alignment. Stage 1 trains a ViT-B/16 backbone using knowledge distillation from CLIP's image encoder while maintaining fixed text prompts, learning to produce aligned visual features. Stage 2 freezes the visual encoder and trains learnable hierarchical prompts (global and local) with a semantic consistency loss that anchors the learned prompts to CLIP's original text embeddings. The dual-view alignment computes both global scores from the [CLS] token and local scores from Top-K patch token pooling, then fuses them with a weighted combination. This architecture enables the model to capture both task-specific and context-specific information while maintaining semantic alignment.

## Key Results
- Achieves 10.0% mAP improvement on long-tailed multi-label image classification tasks
- Demonstrates 5.2% mAP improvement on multi-label few-shot image classification tasks
- Shows 6.8% and 2.9% improvements on MS-COCO and VOC2007 benchmarks respectively
- Significantly outperforms state-of-the-art approaches on class-imbalanced multi-label classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling global context from local fine-grained features prevents feature blending in multi-label images.
- **Mechanism:** The model uses a Vision Transformer (ViT) to extract a global [CLS] token and local patch tokens. Instead of relying solely on the global token (which averages features and dilutes small objects), it computes image-text alignment scores using both. The local score uses Top-K mean pooling to select only the most relevant patches for a specific class, filtering out background noise.
- **Core assumption:** Distinct objects in a multi-label image correspond to distinct spatial clusters of high-activation patches.
- **Evidence anchors:**
  - [abstract] "...extracting complementary features for accurate image-text alignment."
  - [section III-A] "...global features... often represent a blend of visual information... suppressing the recognition of less prominent objects."
  - [corpus] Neighbor paper "Mitigating Message Imbalance..." supports the efficacy of dual-view structures in handling information imbalance, though in a graph domain.

### Mechanism 2
- **Claim:** Hierarchical prompts with semantic consistency prevent the catastrophic forgetting of general knowledge when fine-tuning for imbalanced data.
- **Mechanism:** The authors introduce separate learnable prompts for global (task-specific) and local (context-specific) views. Crucially, a Semantic Consistency (SC) loss forces the text embeddings generated by the learned global prompts to remain close to the fixed hand-crafted prompts (e.g., "a photo of a [CLASS]") from the original CLIP model. This anchors the tuning process.
- **Core assumption:** The original CLIP text embeddings contain valuable general knowledge that is lost when prompts are overfitted to long-tailed training data.
- **Evidence anchors:**
  - [abstract] "...semantic consistency loss... to prevent learned prompts from deviating from general knowledge..."
  - [section III-C] "However, the learnable prompts are susceptible to overfitting rare samples and forgetting general knowledge... To mitigate this, we introduce a semantic consistency (SC) loss..."
  - [corpus] No direct corpus evidence for this specific "hierarchical prompt + consistency" combination; the paper claims novelty here.

### Mechanism 3
- **Claim:** A two-stage training strategy stabilizes the transfer of vision-language alignment to class-imbalanced multi-label tasks.
- **Mechanism:** Stage 1 trains the Visual Encoder (ViT) using Knowledge Distillation from CLIP's image encoder and fixed text prompts ($L_{cls} + L_{kd}$). Stage 2 freezes the ViT and trains the Hierarchical Prompts ($L_{cls} + L_{sc}$). This prevents the instability observed when trying to learn both feature extractors and prompts simultaneously.
- **Core assumption:** The convergence of learnable prompts is difficult or unstable when the visual feature extractor is simultaneously shifting.
- **Evidence anchors:**
  - [section III] "We adopt this two-stage training strategy because simultaneously optimizing feature extractor and hierarchical prompts will lead to difficulty in convergence..."
  - [section V-D] "Joint training impairs the model's ability to focus on distinct optimization objectives..."
  - [corpus] General transfer learning concepts support staged training, but no specific corpus paper contradicts or validates this specific two-stage constraint for CI-MLIC.

## Foundational Learning

- **Concept: Vision-Language Pre-training (VLP) & CLIP**
  - **Why needed here:** The method is built on top of CLIP. You must understand that CLIP aligns images and text in a shared embedding space using contrastive loss. The paper manipulates this alignment rather than training a classifier from scratch.
  - **Quick check question:** How does CLIP classify an image without a dedicated classification head? (Answer: By comparing image embeddings to text embeddings of class names).

- **Concept: The Long-Tailed Distribution Problem**
  - **Why needed here:** The core motivation. Standard Cross-Entropy fails here because the massive number of "head" class samples dominates the gradient, causing the model to ignore "tail" classes. The paper uses a modified DB loss to combat this.
  - **Quick check question:** In a dataset with 1000 "dog" images and 5 "cat" images, what would a standard loss function optimize for?

- **Concept: Visual Transformers (ViT) Tokenization**
  - **Why needed here:** The mechanism relies on splitting an image into patches (tokens). You need to distinguish between the [CLS] token (global summary) and patch tokens (local features) to understand the "Dual-View" logic.
  - **Quick check question:** In ViT, does the [CLS] token represent a specific patch or the whole image?

## Architecture Onboarding

- **Component map:**
  1. Image Encoder (ViT-B/16): Student model. Takes Image → Outputs Global Feature ($f_{cls}$) and Patch Features ($f_{patch}$).
  2. Text Encoder (CLIP ViT-B/16): Frozen Teacher (Stage 1) / Frozen Feature Extractor (Stage 2). Takes Prompts → Outputs Text Embeddings.
  3. Hierarchical Prompts: Learnable vectors ($v_1...v_M$). Global prompts pair with $f_{cls}$; Local prompts pair with $f_{patch}$.
  4. Fusion Head: Computes Cosine Similarity → Applies Top-K pooling (Local) → Weighted Fusion ($\alpha$).

- **Critical path:**
  1. Preprocessing: Image → Patches.
  2. Forward: Pass through ViT.
  3. Alignment: Calculate Global Score ($s^G$) and Local Score ($s^L$).
  4. Selection: For $s^L$, pick top $k=32$ patch similarities and average them.
  5. Fusion: $s_{final} = \alpha s^G + (1-\alpha) s^L$.
  6. Optimization: Apply DB loss (weighted for class balance) + Distillation/Consistency Loss.

- **Design tradeoffs:**
  - Two-stage vs. Joint: The paper explicitly trades off training speed for convergence stability. Do not attempt joint training without a scheduled learning rate warm-up or you risk divergence.
  - Top-K ($k=32$): If $k$ is too small, you miss parts of the object; if too large, you include background noise.
  - Distillation: Using CLIP as a teacher ensures the student retains semantic alignment, but limits the student to the performance ceiling of the teacher's architecture.

- **Failure signatures:**
  - Head-class degradation: If the Semantic Consistency Loss weight is zero, the model will likely improve on tail classes but accuracy on head classes (e.g., "person") will drop significantly compared to the baseline.
  - Random outputs: If the ViT is not distilled (Stage 1 skipped), its output embeddings will not be aligned with the CLIP text space, resulting in near-random classification.
  - Feature blending artifacts: If $\alpha$ (fusion weight) is set to 1.0, you revert to a global-only model and will lose the ability to detect small objects.

- **First 3 experiments:**
  1. Sanity Check (Zero-Shot): Run the frozen CLIP model on your validation set without training. This establishes the absolute floor performance.
  2. Ablation (Stage 1 Only): Train the ViT with the Distillation Loss ($L_{kd}$) but keep hand-crafted prompts ("a photo of a [CLASS]"). Verify that the ViT actually learns to align (mAP should rise).
  3. Ablation (Local vs. Global): Run Stage 2, but force $\alpha=1$ (Global only) and then $\alpha=0$ (Local only). Inspect which classes improve with Local features (likely small objects like "bottle").

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can differentiated prompt designs be integrated into the HP-DVAL framework to specifically support underrepresented classes in scenarios of extreme class imbalance?
- Basis in paper: [explicit] The Conclusion states that future work will "explore differentiated prompting for rare classes" because the current design lacks dedicated mechanisms for extreme imbalance, causing local prompts to potentially fail on tail categories with insufficient samples.
- Why unresolved: The current method uses class-shared prompts (global and local) optimized for general alignment, which may lack the capacity to capture distinctive features for severely scarce tail classes.
- What evidence would resolve it: A comparative study showing improved mAP on tail classes in a severely imbalanced dataset when using class-specific or rarity-weighted prompt structures versus the current shared prompt approach.

### Open Question 2
- Question: Can the incorporation of generative models or data augmentation strategies effectively alleviate sample scarcity for tail classes without destabilizing the semantic consistency of the prompts?
- Basis in paper: [explicit] The authors explicitly propose as future work to "leverage generative models or data augmentation to enrich training data" to improve generalization on underrepresented categories.
- Why unresolved: While data enrichment is proposed, it is unclear if synthetic data would introduce noise that conflicts with the Semantic Consistency loss designed to anchor prompts to general knowledge.
- What evidence would resolve it: Experiments demonstrating that enriching tail-class data improves local alignment scores without increasing the semantic deviation measured by the L1 distance in the consistency loss.

### Open Question 3
- Question: Is it possible to design a unified training objective that mitigates the convergence difficulties currently preventing the simultaneous optimization of the visual feature extractor and hierarchical prompts?
- Basis in paper: [inferred] The Methodology section notes that a two-stage sequential training strategy was adopted because "simultaneously optimizing feature extractor and hierarchical prompts will lead to difficulty in convergence of learnable prompts."
- Why unresolved: Decoupling the training into two stages may lead to suboptimal global optimization, as the visual encoder is frozen during the prompt-tuning phase and cannot adapt to the hierarchical context.
- What evidence would resolve it: A modified loss function or curriculum learning schedule that allows for joint training while achieving equal or better convergence stability and mAP scores compared to the sequential approach.

## Limitations

- **Unknown DB Loss Hyperparameters:** The paper references DB loss parameters (τ, β, μ, κ, λ) from prior work but does not explicitly state their values, requiring readers to reference external sources.
- **Exact Data Split Indices:** The specific indices for constructing VOC-LT and COCO-LT long-tailed distributions are not provided, making exact reproduction difficult without accessing the referenced sampling method code.
- **No Top-K Ablation Study:** The paper sets Top-K=32 without providing ablation experiments to justify this choice or demonstrate its sensitivity across different object size distributions.

## Confidence

- **Dual-View Alignment Learning Mechanism:** High confidence. The claim that decoupling global and local features improves multi-label recognition is well-supported by the architectural description and evaluation results showing mAP improvements over baselines.
- **Hierarchical Prompt Tuning with Semantic Consistency:** Medium confidence. While the paper provides a clear theoretical motivation and implementation details, the specific effectiveness of the Semantic Consistency Loss in preventing knowledge forgetting is primarily validated through final benchmark performance rather than detailed ablation studies.
- **Two-Stage Training Strategy:** Medium confidence. The paper justifies the two-stage approach through convergence arguments and brief mention in section V-D, but lacks comprehensive ablation studies comparing it to alternative training strategies like gradual unfreezing or joint training with different learning rate schedules.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary the DB loss parameters (τ, β, μ, κ, λ) and the Semantic Consistency Loss weight to determine their impact on tail vs. head class performance, particularly testing if head class degradation occurs when consistency weight is too low.

2. **Top-K Patch Selection Ablation:** Conduct experiments with different Top-K values (e.g., 16, 32, 64) to quantify the tradeoff between background noise reduction and object coverage completeness across different object size distributions in the dataset.

3. **Alternative Training Strategy Comparison:** Implement and compare the proposed two-stage approach against joint training with a warm-up schedule and against gradual unfreezing strategies to validate whether the convergence stability claims hold across different implementation choices.