---
ver: rpa2
title: 'ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology
  Using Whole Slide Images'
arxiv_id: '2504.13023'
source_url: https://arxiv.org/abs/2504.13023
tags:
- histopathology
- answer
- images
- datasets
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ChatEXAONEPath, an expert-level multimodal
  large language model for histopathology using whole slide images. The key challenge
  addressed is the lack of thorough clinical context understanding in existing models
  due to patch-level data limitations.
---

# ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images

## Quick Facts
- arXiv ID: 2504.13023
- Source URL: https://arxiv.org/abs/2504.13023
- Authors: Sangwook Kim; Soonyoung Lee; Jongseong Jang
- Reference count: 22
- Primary result: Expert-level MLLM achieving 62.9% acceptance rate on 1,134 test WSI-report pairs for pan-cancer histopathology diagnosis

## Executive Summary
ChatEXAONEPath addresses the challenge of building expert-level multimodal large language models for histopathology using whole slide images (WSIs). Existing models struggle with thorough clinical context understanding due to patch-level data limitations. The proposed solution employs a retrieval-based data generation pipeline (RAIDER) to create instruction-tuning datasets from 10,094 WSI-report pairs, fine-tuning LLaVA with a specialized vision tower for WSI processing. An AI-based evaluation protocol assesses generated answers against original pathology reports across seven clinical criteria.

## Method Summary
The method employs a two-phase training approach on LLaVA-style architecture. Phase 1 freezes LLaMA2-7B-Chat and trains the vision tower (EXAONEPath encoder + CLAM-based patch aggregator + attention pooler projector) to map WSI embeddings to text space via caption generation loss. Phase 2 unfreezes LLaMA projection layers via LoRA (rank=64, α=16) for instruction-response learning with system prompts. WSIs are processed as 256×256 patches at 0.5μm/pixel from 20× magnification, with hierarchical patch-to-slide aggregation using attention-based learning to identify diagnostically relevant regions. RAIDER generates instruction datasets by retrieving relevant report chunks from Chroma DB and using LLaMA3.1:70b-instruct to generate answers with retrieved context.

## Key Results
- Achieved 62.9% acceptance rate on 1,134 test WSI-report pairs across seven clinical criteria
- Outperformed baseline models including PA-LLaVA (27.3%) and PolyPath (26.5%) on pan-cancer understanding tasks
- v3 model (without Macenko normalization) achieved 62.87% acceptance vs v2 (42.86%), demonstrating preprocessing impact on patch encoder quality

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical patch-to-slide aggregation enables WSI-level understanding by first encoding local tissue patterns then attention-weighting them into global representations. WSI → 256×256 patches at 0.5μm/pixel → EXAONEPath (ViT encoder) → patch embeddings → CLAM-Based Patch Aggregator with Gated Attention → single WSI embedding → Vision Projector → text-aligned embedding. Core assumption: Diagnostically relevant regions can be identified via attention mechanisms without explicit patch-level labels; attention scores correlate with clinical importance. Evidence: CBPA employs attention-based learning to pinpoint sub-regions with high significance, extracting a single WSI representative embedding.

### Mechanism 2
Two-phase training (vision-language alignment → instruction tuning) creates cross-modal grounding before teaching the model to respond to clinical queries. Phase 1 freezes LLaMA, trains vision tower + projector to map WSI embeddings to text space via caption generation loss. Phase 2 unfreezes LLaMA projection layers via LoRA (rank=64, α=16) for instruction-response learning with system prompts. Core assumption: Frozen pretrained LLM embeddings already encode medical knowledge; the vision projector can learn a sufficient mapping without updating the language backbone initially. Evidence: Only weights of the vision tower are trainable during the first pretraining phase, while the language stream is frozen.

### Mechanism 3
RAIDER reduces hallucination by grounding answer generation in retrieved report chunks rather than full-report context. Reports → OCR → chunked → Chroma DB (vector store) → cosine similarity retrieval → LLaMA3.1:70b-instruct generates answers using retrieved chunks as context with constrained prompts (≤10 words, no measurements). Core assumption: Chunked retrieval filters irrelevant information better than providing entire reports, leading to cleaner training signals. Evidence: RAG has advantages over the simple contextualization by providing the LLM with concise information and alleviating the hallucination.

## Foundational Learning

- **Concept: Multiple Instance Learning (MIL) for Gigapixel Images**
  - Why needed here: WSIs contain 10,000+ patches but only bag-level labels (report text); no patch-level annotations exist. MIL treats the WSI as a "bag" of patch instances, learning to weight patches by importance.
  - Quick check question: Can you explain why standard supervised learning fails when you have 50,000 patches but only one diagnosis label for the entire slide?

- **Concept: Vision-Language Alignment via Contrastive/Generative Objectives**
  - Why needed here: The model must map visual patterns (cell morphology, tissue architecture) to linguistic concepts ("invasive ductal carcinoma"). Phase 1 trains this mapping before instruction tuning.
  - Quick check question: If your vision projector outputs 4096-dim embeddings but your LLM expects 4096-dim text embeddings, what happens if you skip alignment training and go directly to instruction tuning?

- **Concept: Low-Rank Adaptation (LoRA) for Efficient Fine-Tuning**
  - Why needed here: Full fine-tuning of 7B parameters risks catastrophic forgetting and is computationally expensive. LoRA adds trainable rank-64 matrices to projection layers, updating ~1% of parameters while preserving pretrained knowledge.
  - Quick check question: Why might LoRA be preferable to full fine-tuning when your training set contains only ~9,000 WSI-report pairs for a 7B parameter model?

## Architecture Onboarding

- **Component map**: WSI → Patch Extraction → 256×256 patches → EXAONEPath Encoder → patch embeddings [N×D] → CLAM Aggregator → WSI embedding [1×512] → Vision Projector: Attention Pool + Linear → [1×4096] → Concatenate with text embeddings → LLaMA2:7B-Chat → Generate response tokens

- **Critical path**:
  1. EXAONEPath encoder quality determines patch-level feature richness (pretrained, frozen)
  2. CLAM aggregator attention scores determine which regions influence the WSI embedding
  3. Vision projector alignment determines cross-modal grounding quality
  4. LoRA fine-tuning determines instruction-following capability

- **Design tradeoffs**:
  - Dataset scale vs. alignment quality: Dataset-v2 (7× larger) underperformed Dataset-v1, suggesting text augmentation without vision diversity harms alignment
  - Stain normalization: v3 removed Macenko normalization and improved from 42.86% to 62.87%, suggesting preprocessing affects patch encoder quality
  - AI-based evaluation: 7-criteria evaluator achieved reasonable acceptance rates but showed instability ("overly strict about trivial constraints such as length restriction")
  - No validation set: Models were selected after final training epoch without validation, risking overfitting

- **Failure signatures**:
  - Low acceptance rate with RAIDER-augmented data → check vision-text pair balance
  - Evaluator rejects for length constraints despite valid diagnosis → evaluator prompt engineering issue
  - Attention weights uniformly distributed → aggregator not learning region importance; check aggregator pretraining with RNA-seq contrastive loss
  - Hallucinated measurements/specimen details → RAIDER retrieval failing; check chunk quality and prompt constraints

- **First 3 experiments**:
  1. Ablate stain normalization systematically: Train separate models with (a) Macenko normalization, (b) no normalization, (c) alternative normalization (Reinhard, Vahadane). Evaluate on held-out cancer types to test generalization.
  2. Diagnose RAIDER failure: For each test WSI, visualize retrieved chunks vs. GPT-4o direct captions. Measure chunk-report overlap and answer specificity.
  3. Human evaluation baseline: Have a pathologist score 100 random test responses for clinical accuracy (blind to AI evaluation). Compare human acceptance rate to AI evaluator's 62.9%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does retrieval-augmented data scaling (RAIDER) result in lower performance compared to non-augmented datasets?
- Basis in paper: The authors state, "Training MLLMs with RAIDER-augmented datasets does not always result in improved generation performance," noting that ChatEXAONEPath-v2 (augmented) achieved a significantly lower acceptance rate (42.86%) than v1 (54.41%).
- Why unresolved: The paper suggests that scaling only the text modality without scaling the vision modality may cause an "imbalanced alignment" in the representational space, but this hypothesis is not empirically validated.
- What evidence would resolve it: Ablation studies varying the ratio of text-to-vision scaling, or analysis of the embedding space geometry to quantify alignment degradation in augmented vs. non-augmented models.

### Open Question 2
- Question: Can a multimodal evaluator effectively replace text-only LLMs for assessing histopathology generations?
- Basis in paper: The authors explicitly propose "future work involving a multimodal evaluator" to address the limitation that the current evaluation "heavily relies on the quality of the ground truth textual information."
- Why unresolved: The current evaluator (LLaMA3.1) is text-only and cannot "see" the image, leading to potential false positives/negatives where the text matches the report but contradicts the visual evidence (or vice versa).
- What evidence would resolve it: Implementation of a vision-language evaluator that demonstrates higher correlation with human expert grading than the current text-only acceptance rate metric.

### Open Question 3
- Question: How can the instability and "manipulated reasoning" of the AI evaluator be mitigated?
- Basis in paper: The authors observe that the evaluator "provides incorrect or manipulated reasons and treats all seven criteria as a single criterion," sometimes rejecting answers based on trivial constraints like length.
- Why unresolved: The Chain-of-Thought (CoT) prompting strategy used did not prevent the LLM from generating non-reasonable interpretations or unstable decisions, limiting the trustworthiness of the 62.9% acceptance rate.
- What evidence would resolve it: Development of a constrained decoding or verification mechanism that forces the evaluator to strictly adhere to medical criteria rather than superficial formatting rules.

## Limitations
- Dataset scale and generalization concerns: 10,094 training pairs is relatively small for 7B parameter model, with TCGA origin limiting real-world clinical applicability
- Evaluation reliability issues: AI-based evaluation protocol shows instability and may not accurately reflect clinical utility without human validation
- Retrieval augmentation paradox: RAIDER pipeline intended to improve grounding but degraded performance when scaling text-only augmentation

## Confidence
- **High Confidence**: Two-phase training architecture and CLAM-based patch aggregation approach are well-supported by related work and demonstrated effectiveness
- **Medium Confidence**: Overall model performance and acceptance rate, while promising, rely on an AI evaluator that may not accurately measure clinical utility
- **Low Confidence**: RAIDER pipeline's failure when scaling text-only augmentation, specific causes of evaluation instability, and optimal preprocessing pipeline remain incompletely understood

## Next Checks
1. Human Clinical Validation: Have 3-5 board-certified pathologists independently evaluate 100 randomly selected test responses for diagnostic accuracy, completeness, and clinical utility. Compare human acceptance rates to the AI evaluator's 62.9%.
2. Stain Normalization Ablation Study: Systematically train and evaluate models with different preprocessing pipelines (Macenko, no normalization, Reinhard, Vahadane) to test generalization to unseen cancer types.
3. Retrieval Quality Diagnosis: For each test WSI, visualize and analyze retrieved chunks from RAIDER vs. direct GPT-4o captions, measuring chunk-report overlap, answer specificity, and whether chunk boundaries split critical diagnostic information.