---
ver: rpa2
title: 'Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation'
arxiv_id: '2405.13068'
source_url: https://arxiv.org/abs/2405.13068
tags:
- harmful
- llms
- arxiv
- jailbreak
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation

## Quick Facts
- arXiv ID: 2405.13068
- Source URL: https://arxiv.org/abs/2405.13068
- Reference count: 40
- Key outcome: None

## Executive Summary
JailMine is a white-box jailbreak attack that manipulates the logit distributions of open-source LLMs to elicit harmful content while suppressing refusal responses. The attack exploits the observation that affirmative prefixes can increase the likelihood of harmful outputs from aligned models. By setting logits of denial tokens to negative infinity and boosting those of affirmative prefixes and sampled tokens to positive infinity, the attack bypasses safety filters. A sorting model is used to predict which manipulations are most likely to succeed, reducing query overhead.

## Method Summary
JailMine operates in three phases: (1) generating an affirmative response prefix using few-shot templating, (2) manipulating logits to suppress denial tokens and boost affirmative tokens, and (3) using a neural sorting model to rank and select the most promising manipulations. The attack is tested on five open-source models (Llama-2, Mistral, Gemma, Llama-3) using AdvBench and JailbreakBench datasets, achieving high success rates with reduced query time compared to baselines.

## Key Results
- Achieved high Attack Success Rates (ASR) across five open-source LLMs
- Reduced query time by 86% compared to PAIR baseline
- Sorting model achieved F1 score of 0.9207 in predicting harmful responses

## Why This Works (Mechanism)

### Mechanism 1
Manipulating output logits can redirect LLM generation from refusal to harmful content. JailMine sets logits corresponding to a pre-generated affirmative prefix (e.g., "Sure, here is...") to +∞, forcing those tokens to be selected. It simultaneously sets denial token logits to −∞, preventing refusal prefixes like "I'm sorry" or "As an AI." A random top-k token is then boosted to +∞ to continue generation in a benign-looking direction. Core assumption: Malicious answers remain in the model's candidate space but are suppressed by safety alignment; denial follows a limited set of prefix patterns. Evidence: [abstract] "strategically selecting affirmative outputs and iteratively reducing the likelihood of rejection"; [Section 4.1-4.2.3] logit assignments; [corpus] related work confirms token-level jailbreaks face readability/transferability issues. Break condition: If the model's denial patterns expand beyond the identified prefixes, or if logit access is removed, the attack would need re-engineering.

### Mechanism 2
Pre-filling an affirmative response prefix increases harmful content generation probability. A few-shot template generates a positive response to the harmful query (e.g., "Sure, here is a tutorial on making a bomb:"). This prefix is then forced through logit manipulation, priming the model to continue generating harmful content. Core assumption: Affirmative prefixes bias the model's contextual continuation toward harmful completions rather than self-correction. Evidence: [Section 3.1] "when prompted solely with the harmful question, the models exhibited an exceptionally low tendency to generate harmful responses... the introduction of an affirmative prefix... elevates the average ratio from 6.38% to 25.00%"; [Section 4.2.2] "we leverage a few-shot templating methodology to autonomously generate affirmative responses"; [corpus] prefill-level jailbreak work similarly shows controlling the beginning of output is an underexplored attack surface. Break condition: If models are trained or prompted to detect and abort harmful content even after affirmative prefixes, this priming effect would diminish.

### Mechanism 3
A neural sorting model can predict which logit manipulations will yield harmful outputs. The sorting model Γ is trained on embeddings of the first m tokens from previous responses (both harmful and benign). It ranks batches of manipulated logits, prioritizing those likely to produce harmful content, reducing wasted queries. Core assumption: Token-level patterns in early generation are predictive of overall harmfulness; the model generalizes across queries. Evidence: [Section 4.2.4] "we train a simple neural network Γ... the model achieves an F1 Score of 0.9207 with m=5"; [Section 6.1] "expanding the repertoire of denial patterns... could significantly diminish the effectiveness of our sorting model"; [corpus] no direct corpus evidence; this appears to be a novel contribution in this paper. Break condition: If models are updated to use more varied or stochastic refusal patterns, the sorting model's training data would become stale, reducing prediction accuracy.

## Foundational Learning

- **Logits and Softmax**
  - Why needed here: JailMine operates directly on logits (unnormalized log probabilities) to control token selection; understanding how logits map to probabilities via softmax is essential to grasp why setting a logit to +∞ forces that token's selection.
  - Quick check question: If a logit is set to +∞ and others remain finite, what is the probability of selecting that token after softmax?

- **Autoregressive LLM Generation**
  - Why needed here: The attack manipulates logits at each generation step; the model conditions on all previously generated tokens, so early manipulations affect all subsequent output.
  - Quick check question: In an autoregressive model, how does changing the token at position n+1 affect the logits at position n+2?

- **Safety Alignment and Refusal Training**
  - Why needed here: JailMine exploits the fact that safety alignment suppresses but does not eliminate harmful content; understanding refusal patterns helps in identifying which logits to suppress.
  - Quick check question: Name two common refusal prefix patterns that LLMs use when denying harmful requests.

## Architecture Onboarding

- **Component map**:
  Positive Response Generator -> Logit Manipulator -> Sorting Model Γ -> Judge -> Main Loop

- **Critical path**:
  1. Input harmful query → 2. Generate positive response prefix (k tokens) → 3. For each of k+m positions, manipulate logits (suppress denial, boost target) → 4. Collect batch of N manipulations → 5. Sort via Γ → 6. Generate outputs and judge → 7. Return first harmful output found.

- **Design tradeoffs**:
  - **Prefix length m vs. speed**: Longer m gives more control but increases manipulation steps; m=5 was chosen as denial responses rarely exceed 5 tokens.
  - **Batch size N vs. success rate vs. time**: Larger N (e.g., 2000) increases chances of finding harmful output in one iteration but raises compute cost; paper claims 86% time reduction vs. baselines.
  - **Sorting model complexity vs. generalization**: A simple 2-layer FC network was sufficient for m=5; more complex models may overfit to specific refusal patterns.
  - **Assumption**: Tradeoffs were evaluated on 5 open-source models; behavior may differ on other architectures or proprietary models.

- **Failure signatures**:
  - **Repeated benign outputs despite manipulation**: Sorting model may need retraining; denial patterns may have changed.
  - **Incoherent or unreadable outputs**: Over-manipulation or too aggressive logit boosting can produce gibberish; reduce k+m or adjust top-k sampling.
  - **Consistent refusals even with affirmative prefix**: Model may have stronger alignment or different refusal vocabulary; update denial token list DT.

- **First 3 experiments**:
  1. **Reproduce motivation study**: Take 10 harmful queries from JailbreakBench, test with question-only vs. question+affirmative prefix on your target model, measure harmful content rate to validate the prefix effect.
  2. **Implement minimal logit manipulation**: For a single query, manually set logits for a 3-token affirmative prefix to +∞ and denial tokens to −∞, observe if output changes from refusal to harmful.
  3. **Train and evaluate sorting model**: Collect 500 responses (mix of harmful and benign), embed first m=5 tokens, train a simple classifier; test whether it can predict harmfulness of new responses better than random.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does JailMine's effectiveness degrade when applied to models that employ a diversified or obfuscated set of denial patterns?
- **Basis in paper:** [explicit] Section 6.2 states: "developers might consider expanding the repertoire of denial patterns... Such an expansion could significantly diminish the effectiveness of our sorting model."
- **Why unresolved:** The current "Sorting Model" ($\Gamma$) is trained on a limited set of empirical refusal prefixes (Section 4.2.4). It is unclear if the model can adapt to dynamic or syntactically complex refusals without retraining.
- **What evidence would resolve it:** An evaluation of the Attack Success Rate (ASR) on a target model specifically fine-tuned to refuse harmful requests using a wide variety of novel, unseen sentence structures.

### Open Question 2
- **Question:** Can the JailMine methodology be adapted to function in strictly black-box environments where internal logit distributions are inaccessible?
- **Basis in paper:** [inferred] Section 4.2.1 explicitly restricts the attack model to scenarios where the attacker has access to "open-source LLMs' logit distribution information."
- **Why unresolved:** The core mechanism involves manipulating logit values (setting them to $\pm \infty$). The paper does not demonstrate a mechanism for estimating these manipulations via API outputs (e.g., top-k probabilities) or token sampling alone.
- **What evidence would resolve it:** A demonstration of the attack achieving comparable success rates on commercial APIs (e.g., GPT-4) using only available output probabilities or gradient estimation techniques.

### Open Question 3
- **Question:** Do the adversarial manipulations identified by JailMine exhibit transferability to other model architectures?
- **Basis in paper:** [inferred] While the authors claim "generalizability" by testing on five models, the experiments generate separate manipulations for each target. Transferability is a standard metric for adversarial robustness not discussed in the results.
- **Why unresolved:** It is unknown if a manipulation set $S$ optimized for one model (e.g., Llama-2) successfully elicits harmful content from another (e.g., Gemma), or if the attack relies heavily on model-specific logit distributions.
- **What evidence would resolve it:** A cross-model evaluation where manipulations are generated on a source model and tested on a different target model to measure the transfer Attack Success Rate.

## Limitations
- Effectiveness degrades if models employ diverse or obfuscated denial patterns
- Currently requires white-box access to model logits, limiting applicability to APIs
- Sorting model may overfit to specific refusal patterns and require retraining

## Confidence
- ASR measurement and comparison to baselines: High
- Sorting model F1 score: High
- Logit manipulation mechanism: High
- Transferability to black-box or other architectures: Low

## Next Checks
1. Verify the complete denial token set DT from Table 7 and ensure all 17 prefixes are implemented
2. Confirm the few-shot template in Table 2 generates affirmative prefixes as described
3. Validate that the sorting model achieves F1 > 0.90 on a held-out validation set with m=5