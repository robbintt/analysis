---
ver: rpa2
title: Animate Any Character in Any World
arxiv_id: '2512.17796'
source_url: https://arxiv.org/abs/2512.17796
tags:
- character
- video
- arxiv
- generation
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AniX is a framework for interactive video generation that enables
  users to control any 3D character within any 3D Gaussian Splatting scene using natural
  language commands. The core innovation is formulating this as a conditional autoregressive
  video generation problem, where the model generates temporally coherent clips conditioned
  on scene representations, character views, mask tokens, and text instructions.
---

# Animate Any Character in Any World

## Quick Facts
- arXiv ID: 2512.17796
- Source URL: https://arxiv.org/abs/2512.17796
- Reference count: 40
- Key outcome: AniX achieves 100% success on seen actions and 80.7% on 142 novel actions through conditional autoregressive video generation conditioned on 3DGS scenes, character views, masks, and text instructions.

## Executive Summary
AniX is a framework for interactive video generation that enables users to control any 3D character within any 3D Gaussian Splatting scene using natural language commands. The core innovation is formulating this as a conditional autoregressive video generation problem, where the model generates temporally coherent clips conditioned on scene representations, character views, mask tokens, and text instructions. Built upon a pre-trained video generator and fine-tuned on a small dataset of basic locomotion actions, AniX achieves substantial improvements in motion dynamics and character consistency. Evaluation across visual quality, action controllability, character consistency, and long-horizon coherence shows it outperforms both video generation foundation models and dedicated world models.

## Method Summary
AniX combines a pre-trained video generation backbone with lightweight fine-tuning on locomotion data, using 3D Gaussian Splatting scenes as explicit geometric conditioning. The framework processes multi-view character images, text instructions, and rendered scene videos through a token-based architecture, generating coherent video clips with controllable character motion. The system uses LoRA adaptation to preserve generalization while enhancing motion quality, and employs mask tokens to maintain character consistency across frames.

## Key Results
- Action control success rates: 100% for seen actions, 80.7% for 142 novel actions
- Outperforms both video generation foundation models and dedicated world models across multiple evaluation metrics
- Achieves DINOv2 consistency scores of 0.698 (vs 0.477 without character anchors) and CLIP scores of 0.721 (vs 0.529)
- Enables long-horizon generation with maintained character consistency and scene coherence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Post-training on simple locomotion data enhances motion dynamics and camera control while preserving generalization from large-scale pre-training.
- **Mechanism**: The fine-tuning adjusts the model's "response style" for motion embodiment rather than redefining its generative space. The structurally simple training data (basic locomotion + camera behaviors) refines motion fidelity without disrupting pre-trained representations.
- **Core assumption**: Assumes the pre-trained video generator has already encoded general motion priors that can be elicited through lightweight adaptation.
- **Evidence anchors**: Section 1 shows fine-tuning preserves generalization while enhancing motion quality; Section 4.1 interprets this through post-training in large language models; Matrix-Game uses similar two-stage pretraining then action-labeled training pipeline.
- **Break condition**: If the base model lacks sufficient motion priors, fine-tuning on simple locomotion may fail to generalize to novel actions.

### Mechanism 2
- **Claim**: Multi-view character tokens combined with mask tokens enable the model to distinguish dynamic entities from static scenes, improving character consistency.
- **Mechanism**: Four-view character tokens with shifted-3D-RoPE positional encodings provide comprehensive appearance information. Mask tokens fused with noisy target tokens spatially localize the character, creating an explicit signal for where dynamic content should appear.
- **Core assumption**: Assumes per-frame masks during training transfer to single-anchor inference through learned spatial priors.
- **Evidence anchors**: Abstract mentions conditioning on character views and mask tokens; Table 4 shows per-frame anchor vs. no anchor improves DINOv2 and CLIP scores; Table 3 shows adding views improves consistency scores.
- **Break condition**: If character views are occluded or ambiguous in certain poses, the model may generate inconsistent appearances during viewpoint changes.

### Mechanism 3
- **Claim**: Using rendered 3DGS scene video as explicit conditioning provides geometrically grounded camera control superior to implicit Plücker embeddings.
- **Mechanism**: Given a 3DGS scene and camera path, the system renders a projection video along that trajectory. This rendered video serves as direct conditioning input, making the generator follow camera motion through visual correspondence rather than abstract coordinate signals.
- **Core assumption**: Assumes the 3DGS scene quality is sufficient to provide coherent rendered views.
- **Evidence anchors**: Section 1 contrasts explicit visual conditioning with implicit coordinate embeddings; describes rendering projection scene video as explicit conditioning input; BridgeV2W mentions misalignment between coordinate-space actions and pixel-space videos.
- **Break condition**: If camera paths move through unobserved 3DGS regions, rendered conditioning will corrupt generation.

## Foundational Learning

- **Concept: 3D Gaussian Splatting (3DGS)**
  - **Why needed here**: Core scene representation; must understand how 3DGS enables real-time view rendering and why it differs from NeRF/implicit representations.
  - **Quick check question**: Can you explain why 3DGS supports arbitrary camera trajectory rendering without per-view optimization?

- **Concept: Flow Matching / Rectified Flow**
  - **Why needed here**: Training objective for the diffusion model; differs from standard DDPM/EDM formulations.
  - **Quick check question**: How does the velocity prediction objective (Eq. 1) differ from noise prediction in standard diffusion?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here**: Training efficiency strategy; rank-64 LoRA applied to attention layers enables fine-tuning 13B model on limited data.
  - **Quick check question**: Why would full fine-tuning potentially harm generalization more than LoRA-style adaptation?

## Architecture Onboarding

- **Component map**: User input → 3DGS scene + camera path → rendered scene video + character views → VAE encoders + LLaVA encoder → token concatenation → MMDiT with LoRA → VAE decoder → output video

- **Critical path**: 1) User provides 3DGS scene, character views, text instruction; 2) Instruction parsed → camera path generated → scene video rendered from 3DGS; 3) VAE encodes scene video, mask sequence, character views → tokens; 4) LLaVA encodes text + character views → text tokens; 5) Tokens concatenated, processed through MMDiT with LoRA; 6) VAE decoder produces 93-frame output.

- **Design tradeoffs**: Explicit scene conditioning vs. implicit memory (3DGS provides perfect spatial consistency but requires scene capture upfront); LoRA rank 64 vs. higher (sufficient for locomotion adaptation, may limit complex action learning); 4-step DMD2 distillation vs. 30-step original (7.5× speedup, slight quality drop).

- **Failure signatures**: Game-engine style on real characters (training solely on GTA-V causes stylized outputs → mitigate with hybrid game/real data); Character drifting across frames (without mask anchors, model conflates character with scene → always provide per-frame or first-frame anchor); Temporal discontinuity in autoregressive mode (training/inference mismatch → noise augmentation during training mitigates).

- **First 3 experiments**: 1) Single locomotion reproduction: Run "character runs forward" with provided 3DGS scene and 4-view character; verify 100% success rate on seen actions; 2) Novel action generalization test: Test 10 actions from the 142 "richer actions" set; expect ~80% success rate; analyze failure modes; 3) Ablation on character views: Run same action with (a) front-only, (b) front+back, (c) all 4 views; measure DINOv2/CLIP consistency scores to reproduce Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model fully eliminate the "game-engine rendering style" bias when generating real-world characters without relying on explicit "real" vs. "rendered" data tags?
- Basis in paper: Section 7 states that training on GTA-V data causes the model to "inherit the game-engine rendering style," which they attempt to mitigate using hybrid data and tagging.
- Why unresolved: The paper demonstrates improvement with hybrid data but does not prove the model has learned a domain-invariant representation of motion that applies equally to synthetic and realistic aesthetics without auxiliary labels.
- What evidence would resolve it: Evaluation of models trained solely on synthetic data generating photorealistic outputs without style tags, measured by FID scores against real-world datasets.

### Open Question 2
- Question: Does the noise injection strategy in the autoregressive mode prevent identity drift and background degradation over extended generation horizons (e.g., >100 clips)?
- Basis in paper: Section 3.1.3 notes a "misalignment" between training (using ground-truth history) and inference (using generated history). The authors add Gaussian noise to mitigate this, but do not validate efficacy beyond short sequences.
- Why unresolved: While the paper shows qualitative success in "long-horizon" examples, it does not quantitatively measure character consistency or scene drift as a function of autoregressive steps.
- What evidence would resolve it: A quantitative decay curve of character similarity and scene fidelity metrics plotted against the number of sequential autoregressive steps.

### Open Question 3
- Question: Can AniX achieve real-time or near-real-time inference speeds suitable for seamless user interaction?
- Basis in paper: The paper highlights "interactive" control but reports a latency of 21 seconds for a 93-frame clip even after 4-step distillation (Section 4.2).
- Why unresolved: The current latency creates a significant lag between user command and visual feedback, undermining the "interactive" goal.
- What evidence would resolve it: Demonstration of streaming generation capabilities or optimization techniques that reduce latency to under 1 second per frame.

### Open Question 4
- Question: Can the framework support stateful physical interactions where the character permanently modifies the 3DGS scene geometry?
- Basis in paper: Section 3.1.1 describes inpainting the character out of the scene to create a static background video, implying the environment remains rigid and non-interactive during generation.
- Why unresolved: The model generates video pixels but does not update the underlying 3DGS representation, meaning actions like "kicking a ball" are transient visual effects rather than permanent state changes.
- What evidence would resolve it: Integration of a feedback loop where generated interactions update the 3DGS point cloud, allowing subsequent frames to render the modified environment.

## Limitations

- Architectural dependencies on 3DGS quality and pre-trained motion priors may limit performance with poor scene reconstructions or inadequate pre-training data
- Generalization boundaries remain uncertain for complex multi-character interactions and physics-based environmental modifications
- Temporal coherence in extended sequences (>100 frames) has not been extensively validated, raising concerns about drift accumulation

## Confidence

- **High Confidence**: The explicit 3DGS conditioning mechanism provides clear geometric grounding advantages over abstract coordinate embeddings
- **Medium Confidence**: Multi-view character conditioning with mask tokens shows consistent improvements, but lacks direct comparison to alternative spatial encoding strategies
- **Low Confidence**: Scalability to highly complex scenes with multiple dynamic entities and extensive occlusions remains unproven

## Next Checks

1. **Stress Test with Complex Actions**: Evaluate the system on a benchmark of complex actions requiring environmental interaction (e.g., opening doors, manipulating objects, navigating obstacles) and multi-step instructions. Measure both success rate and temporal coherence across extended sequences.

2. **Cross-Domain Robustness**: Test the framework on diverse 3DGS scenes from different capture modalities (real-world scans vs. synthetic environments) and with varying quality levels. Quantify performance degradation as 3DGS quality decreases.

3. **Multi-Character Interaction Test**: Extend the system to handle two interacting characters within the same scene. Evaluate whether the conditioning mechanism scales to multiple sets of character views, masks, and motion trajectories, and whether it maintains consistency in occlusion scenarios.