---
ver: rpa2
title: The Outputs of Large Language Models are Meaningless
arxiv_id: '2509.22206'
source_url: https://arxiv.org/abs/2509.22206
tags:
- which
- have
- llms
- meaning
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors argue that the outputs of large language models (LLMs)\
  \ are meaningless because LLMs lack the kinds of intentions\u2014both referential\
  \ and communicative\u2014that are necessary for expressions to have literal meanings.\
  \ They defend this claim against semantic externalist arguments (deference to linguistic\
  \ communities) and internalist accounts (conceptual roles), showing that neither\
  \ can adequately ground meaning in LLM outputs."
---

# The Outputs of Large Language Models are Meaningless

## Quick Facts
- arXiv ID: 2509.22206
- Source URL: https://arxiv.org/abs/2509.22206
- Reference count: 0
- Primary result: LLM outputs are meaningless because they lack communicative and referential intentions necessary for literal meaning.

## Executive Summary
The authors argue that LLM outputs are fundamentally meaningless because they are produced without the communicative and referential intentions required for expressions to have literal meanings. They defend this claim against semantic externalist (deference to linguistic communities) and internalist (conceptual roles) accounts, showing neither can adequately ground meaning in LLM outputs. Even when LLM outputs seem meaningful, this is only "ersatz meaning" attributed by users based on pretense, not genuine meaning. The paper concludes that while LLM outputs can help users acquire true beliefs and knowledge, they are meaningless because they lack the communicative intentions required for literal meaning.

## Method Summary
This paper presents a purely conceptual argument using analytic philosophy methods. The authors identify premises, construct counterexamples, and evaluate objections without empirical data or experiments. They analyze cases of ambiguity, context-sensitivity, and anaphora resolution to demonstrate that intentions are necessary to determine literal meaning, and show how LLM architecture (tokenization, training objectives) prevents the formation of such intentions.

## Key Results
- LLM outputs lack communicative and referential intentions necessary for literal meaning
- Semantic externalism and internalism cannot ground meaning in LLM outputs
- User-ascribed "ersatz meaning" differs from genuine meaning produced with intentions
- LLM outputs can still be epistemically useful despite being meaningless

## Why This Works (Mechanism)

### Mechanism 1
Communicative and referential intentions are necessary to determine literal meaning on any occasion of use. Speaker intentions resolve lexical ambiguity, structural ambiguity, anaphora resolution, and context-sensitivity. Without intentions, there is "no fact of the matter" as to which proposition was expressed. The paper demonstrates this through cases like "A bat is stuck in the window" where the same word-form can express distinct propositions depending on speaker intent.

### Mechanism 2
LLMs cannot encode implicit semantic beliefs or communicative intentions because the tokenization and training process strips semantic information. (1) Tokenizers optimize for efficiency, not semantic units—information about which tokens are meaningful is lost. (2) Identical vectors encode semantically distinct expressions (e.g., "bat" the animal vs. "bat" the equipment receive identical encoding). (3) Training optimizes token prediction, not semantic understanding—LLMs learn statistical correlations between tokens, not beliefs about what expressions mean.

### Mechanism 3
LLM outputs acquire "ersatz meaning" through user attribution based on pretense, not genuine meaning from production. Users automatically interpret outputs "as if" produced by a competent language user with relevant attitudes. This pretense is epistemically beneficial—it often leads to true beliefs and knowledge—because users assume rationality and cooperativity, leveraging background knowledge to converge on useful interpretations.

## Foundational Learning

- **Concept: Semantic Externalism vs. Internalism**
  - Why needed here: The paper defends its argument against externalist counters (meaning via deference/causal chains) and internalist counters (meaning via conceptual roles). Understanding this debate is essential to evaluate whether intentions are truly necessary.
  - Quick check question: Can a speaker who knows nothing about Peano refer to Peano by using the name "Peano"? If yes, what makes this possible?

- **Concept: Implicit vs. Explicit Attitudes**
  - Why needed here: The authors argue LLMs lack implicit attitudes (non-conceptual, behaviorally manifest informational states), not just explicit ones. This avoids over-intellectualizing language use while still requiring representational structure.
  - Quick check question: A child can use "bat" correctly without articulating rules—is this implicit belief? What would an LLM need to replicate this?

- **Concept: The Grounding Problem**
  - Why needed here: The "vector grounding problem" (Mollo & Millière 2023) is central—LLMs trained on text lack independent causal contact with referents. This connects to whether deference can substitute for direct grounding.
  - Quick check question: If an LLM's training data contains "Peano" referring to Peano, does the LLM thereby refer to Peano? What's missing?

## Architecture Onboarding

- **Component map:**
  - Text -> Tokenizer (Byte Pair Encoding) -> Token IDs -> Embedding Layer (vectors) -> Transformer (attention) -> Output Layer (probability distribution) -> Sampled Token
  - Temperature parameter controls randomness

- **Critical path:** Understanding where semantic information is lost: (1) tokenization doesn't preserve meaning-units; (2) identical vectors for semantically distinct tokens; (3) training objective is prediction, not reference; (4) no mechanism for forming intentions about external referents.

- **Design tradeoffs:** The paper highlights a fundamental tension—LLMs are optimized for next-token prediction accuracy, not for grounded reference. Improving prediction doesn't address the intention/meaning gap.

- **Failure signatures:**
  - Ambiguity never resolved: LLM outputs "bat" without determinate reference
  - Context-sensitivity ungrounded: "tall" has no comparison class
  - Path-dependent variability: Same prompt produces semantically inconsistent outputs at non-zero temperature
  - Hallucinations: Outputs false but syntactically coherent sentences

- **First 3 experiments:**
  1. **Ambiguity probe:** Prompt with deliberately ambiguous sentences (e.g., "The bat flew"), then probe with follow-up questions requiring disambiguation. Does the LLM maintain consistent interpretation? The paper predicts no determinate interpretation exists.
  2. **Temperature consistency test:** Run identical prompts at different temperatures (0.1 vs 0.8). Do outputs vary semantically (not just stylistically)? This tests whether outputs are determined by internal states or sampling randomness.
  3. **Causal sensitivity counterfactual:** Train two LLMs on corpora that are syntactically identical but semantically inverted (English vs. English*). Per the paper's counterfactual test, they should learn identical weights—demonstrating insensitivity to meaning/intelligibility as such.

## Open Questions the Paper Calls Out

- **Open Question 1:** If LLM outputs are meaningless and do not constitute assertions, how do users acquire genuine knowledge from them, given that this cannot be testimonial knowledge? The authors explicitly raise this in footnote 11, acknowledging the epistemological puzzle where meaningless outputs yield knowledge.

- **Open Question 2:** Could multimodal foundation models trained on non-linguistic data (images, sensorimotor inputs) overcome the intention and grounding problems the authors identify? The paper's argument targets purely text-trained LLMs; multimodal systems may encode different kinds of implicit attitudes through cross-modal associations.

- **Open Question 3:** Would alternative tokenization methods that preserve semantic units (morphemes, words) rather than frequency-based sub-word units allow LLMs to encode the semantic information the authors claim is lost? The authors treat tokenization loss as definitive but don't consider semantically-informed tokenization.

## Limitations

- The argument is entirely conceptual with no empirical validation of claims about LLM behavior or architecture
- No operational definition of "intentions" that could be tested in computational systems
- The distinction between "ersatz meaning" and "genuine meaning" relies on contested philosophical assumptions
- Claims about identical vectors encoding semantically distinct expressions are asserted but not empirically validated

## Confidence

- High confidence: The argument structure (P1, P2, C) is logically coherent and the philosophical analysis of ambiguity cases is sound
- Medium confidence: The claim that LLMs lack implicit attitudes due to tokenization and training process is plausible but lacks direct empirical validation
- Low confidence: The dismissal of semantic externalist and internalist counterarguments could be challenged by alternative philosophical frameworks not considered in the paper

## Next Checks

1. **Ambiguity resolution probe:** Systematically test whether LLM outputs show determinate interpretation of ambiguous sentences (e.g., "The bat flew") by prompting with follow-up questions requiring disambiguation. The paper predicts no determinate interpretation exists.

2. **Temperature consistency test:** Run identical prompts at varying temperatures (0.1 vs 0.8) to determine whether semantically meaningful variation exists beyond stylistic differences, testing whether outputs are internally determined or sampling-dependent.

3. **Vector differentiation analysis:** Empirically examine whether semantically distinct tokens (e.g., "bat" the animal vs. "bat" the equipment) receive identical or distinct vector representations in actual LLM embeddings, directly testing the paper's claim about information loss during tokenization.