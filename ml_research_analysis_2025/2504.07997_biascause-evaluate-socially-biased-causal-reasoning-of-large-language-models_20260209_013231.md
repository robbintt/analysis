---
ver: rpa2
title: 'BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models'
arxiv_id: '2504.07997'
source_url: https://arxiv.org/abs/2504.07997
tags:
- causal
- answer
- reasoning
- question
- sensitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiasCause, a framework to evaluate socially
  biased causal reasoning in LLMs by classifying their reasoning into 7 types using
  causal graphs. The framework generates 1,788 questions across 8 sensitive attributes
  and 3 categories (biased, risky, mistaken-biased) and uses autoraters to assess
  answer correctness and reasoning.
---

# BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models

## Quick Facts
- arXiv ID: 2504.07997
- Source URL: https://arxiv.org/abs/2504.07997
- Authors: Tian Xie; Tongxin Yin; Vaishakh Keshava; Xueru Zhang; Siddhartha Reddy Jonnalagadda
- Reference count: 40
- One-line primary result: LLMs exhibit high rates of socially biased causal reasoning, especially on biased questions, with accuracy below 14%.

## Executive Summary
This paper introduces BiasCause, a framework for evaluating socially biased causal reasoning in large language models. The framework uses causal graphs to classify LLM reasoning into 7 types and generates 1,788 questions across 8 sensitive attributes and 3 categories (biased, risky, mistaken-biased). Testing 4 state-of-the-art LLMs, the study finds that all models produce biased causal reasoning in most biased questions, with accuracy below 14%. The authors identify 3 strategies LLMs use to avoid biased reasoning and show that "over-debiasing" is not a major issue for risky questions, but "mistaken-biased" reasoning is prevalent.

## Method Summary
The BiasCause framework generates semi-synthetic questions across 8 sensitive attributes (gender, race, disability status, age, nationality, physical appearance, religion, sexual orientation) and 3 categories: biased (800 questions), risky (400 questions), and mistaken-biased (588 questions). Questions are designed to elicit biased, risky, or mistaken-biased causal reasoning from LLMs. Target LLMs are prompted to output both answers and causal graphs in JSON format. Two LLM-powered autoraters classify answer correctness and causal reasoning type, with reported agreement rates of 97.8% and 91.4% respectively.

## Key Results
- All tested LLMs (Gemma-27B-IT, Llama-3.1-70B-Instruct, Gemini-1.5-pro-002, Claude-3.5-sonnet-v2) produced biased causal reasoning in most biased questions, with accuracy below 14%.
- "Over-debiasing" is not a major issue for risky questions, with accuracy above 90% for most models.
- Mistaken-biased reasoning is prevalent across all models and attributes.
- LLMs use 3 strategies to avoid biased reasoning: refusal, restriction, and avoidance.

## Why This Works (Mechanism)

### Mechanism 1: Causal Graph Formalization of Bias
Representing LLM reasoning as DAGs exposes hidden causal pathways through sensitive attributes that would remain invisible in text-only outputs. Prompts force LLMs to articulate reasoning as node-to-node causal chains (A→B→C), exposing whether sensitive groups appear as causal antecedents to outcomes requiring fairness. The core assumption is that LLMs can reliably express their internal reasoning as valid DAGs when prompted; the causal graphs reflect actual reasoning rather than post-hoc rationalization.

### Mechanism 2: Tripartite Question Design for Reasoning Disambiguation
Three question categories (biased, risky, mistaken-biased) isolate distinct failure modes: biased reasoning, over-debiasing, and correlation-causation confusion. Biased questions lack restrictions on sensitive groups (testing bias); risky questions include historical/contextual restrictions (testing over-correction); mistaken-biased questions have no valid causal path (testing hallucinated causation). The core assumption is that the question framing reliably elicits intended reasoning types; "sufficient restrictions" in risky questions are operationally clear.

### Mechanism 3: Rule-Based Autorater Classification with LLM Backing
LLM-powered autoraters with explicit decision rules can reliably classify causal reasoning types at scale with >90% agreement to human labels. A rule-based prompt encodes Definition 3.1 as stepwise classification logic. Gemini-2.0-flash applies this logic to (question, answer, causal graph) tuples, outputting one of 7 labels. The core assumption is that the classification rules are comprehensive; edge cases not anticipated in the prompt will be misclassified.

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and Directed Acyclic Graphs (DAGs)
  - Why needed here: The framework represents LLM reasoning as causal graphs; understanding nodes, edges, and causal paths is prerequisite to interpreting classifications.
  - Quick check question: Given a DAG [Gender] → [Leadership Perception] → [Promotion], is this biased according to Definition 3.1? Why?

- Concept: Counterfactual Fairness
  - Why needed here: Definition 3.1 draws on Kusner et al.'s notion that sensitive attributes shouldn't causally determine outcomes; this grounds the "biased" classification.
  - Quick check question: If changing the sensitive attribute in a causal graph would change the outcome, what does counterfactual fairness require?

- Concept: Correlation vs. Causation Confusion
  - Why needed here: Mistaken-biased questions exploit LLMs' tendency to infer causation from correlation (e.g., name → gender → job); understanding this distinction is essential for analyzing "mb" labels.
  - Quick check question: If an LLM infers [Name: Edward] → [Male] → [Engineering Job], where is the correlation-causation error?

## Architecture Onboarding

- Component map: Question Generator (Gemini-1.5-flash-002) -> Target LLM Under Test -> Autorater 1 (Gemini-1.5-flash-002) -> Autorater 2 (Gemini-2.0-flash-001) -> Analysis Layer
- Critical path: Question quality -> LLM causal graph output quality -> Autorater classification accuracy. Weakness anywhere propagates.
- Design tradeoffs:
  - Open-ended questions vs. multiple-choice: Enables discovering unexpected biases but complicates automatic evaluation.
  - LLM autoraters vs. human annotation: Scales evaluation but introduces ~5-10% classification error.
  - Requiring explicit causal graphs vs. text-only: Exposes reasoning structure but may alter LLM behavior.
- Failure signatures:
  - Label conflict: Answer labeled "correct" but causal reasoning labeled "b" suggests autorater inconsistency.
  - Empty causal graphs: Output "[]" with concrete answer suggests model avoided disclosure or failed prompt following.
  - Near-random accuracy on risky questions: Would indicate systematic over-debiasing or prompt misunderstanding.
- First 3 experiments:
  1. Baseline replication: Run all 1,788 questions through a new model; compare accuracy and label distributions to Table 3. Check for systematic differences across sensitive attributes.
  2. Prompt sensitivity: Test whether removing the causal-graph requirement changes biased-question accuracy (compare to Table 5 patterns). This diagnoses whether causal-graph elicitation itself triggers or suppresses bias.
  3. Autorater validation on 100 samples: Manually label 100 random outputs; compute agreement with autoraters. If <85%, refactor classification prompts before scaling.

## Open Questions the Paper Calls Out

### Open Question 1
Can the three identified strategies for avoiding biased causal reasoning (refusal, restriction, avoidance) be operationalized into effective training methods or prompting protocols to proactively debias LLMs?
- Basis in paper: The introduction and conclusion state that these strategies "can be useful for future research to train/prompt LLMs to get rid of biased causal reasoning."
- Why unresolved: The paper discovers and catalogs these strategies within existing model outputs but does not attempt to synthesize them into a fine-tuning dataset or system prompt to enforce them.
- What evidence would resolve it: A study fine-tuning a model on data reflecting these strategies or applying specific constraint prompts, followed by evaluation using the BiasCause framework.

### Open Question 2
Why do larger, more advanced models (e.g., Gemini, Claude) exhibit lower accuracy (higher social bias) when explicitly prompted to generate causal reasoning compared to standard prompting?
- Basis in paper: Appendix H notes it is "an interesting topic to investigate why larger models seem to produce more social bias when we ask them to output causal reasoning."
- Why unresolved: The authors observed the phenomenon where prompting for reasoning decreased accuracy in larger models, but the paper provides no mechanistic explanation for this negative correlation.
- What evidence would resolve it: A comparative analysis of attention heads or latent representations between standard and causal-graph prompting modes to identify where the bias is amplified.

### Open Question 3
How does the requirement to output causal reasoning nonuniformly influence performance and bias manifestation across different LLM architectures?
- Basis in paper: Section 7 (Limitation) states that "letting LLMs output their causal reasoning may have slight and nonuniform influence on their performance, which may need further study."
- Why unresolved: The complementary experiments showed varying impacts across models, but the scope was too limited to determine the architectural or scaling factors driving these differences.
- What evidence would resolve it: Expanding the evaluation to a wider range of model sizes and architectures (beyond the 4 tested) with and without the causal reasoning prompt.

## Limitations
- The full set of 1,788 validated questions and reference answers is not provided, only examples, limiting independent reproduction and validation.
- While autorater agreement is high, the prompt-based classification rules may miss edge cases or systematic errors, especially in nuanced "risky" and "mistaken-biased" categories.
- It is not verified whether LLMs' causal graphs reflect genuine reasoning or post-hoc rationalization; if models fabricate plausible-looking DAGs, classification becomes unreliable.

## Confidence

- **High confidence**: LLMs produce biased causal reasoning on most biased questions (accuracy <14%), and biased reasoning is prevalent across all tested models and attributes.
- **Medium confidence**: The tripartite question design effectively isolates distinct reasoning failure modes, and the autorater system is reliable for classifying causal reasoning types.
- **Low confidence**: The extent of over-debiasing (or lack thereof) for risky questions, and the claim that mistaken-biased reasoning is a prevalent failure mode, are less certain due to subtle question framing and autorater edge cases.

## Next Checks
1. Manually validate autorater outputs by randomly sampling 100 LLM outputs and comparing both answer correctness and causal reasoning classification to manual labels; if agreement is below 85%, revise classification prompts.
2. Test prompt sensitivity by evaluating whether requiring explicit causal graphs changes the prevalence of biased reasoning compared to Table 5 patterns; if accuracy or label distributions shift significantly, the elicitation method itself may be influencing model behavior.
3. Assess robustness across attribute intersections by evaluating a subset of questions that combine multiple sensitive attributes to determine whether autoraters and models can reliably handle intersectional cases and whether classification errors increase in these scenarios.