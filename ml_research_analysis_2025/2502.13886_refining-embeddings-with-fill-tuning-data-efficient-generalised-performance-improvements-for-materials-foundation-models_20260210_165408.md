---
ver: rpa2
title: 'Refining embeddings with fill-tuning: data-efficient generalised performance
  improvements for materials foundation models'
arxiv_id: '2502.13886'
source_url: https://arxiv.org/abs/2502.13886
tags:
- data
- performance
- fill-tuning
- foundation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces "fill-tuning," a novel approach to generate
  targeted datasets for continued pretraining of foundation models that improve model
  performance across all downstream tasks. The method uses roughness analysis of the
  latent space to identify regions of poor model understanding, then generates data
  to fill these gaps.
---

# Refining embeddings with fill-tuning: data-efficient generalised performance improvements for materials foundation models

## Quick Facts
- arXiv ID: 2502.13886
- Source URL: https://arxiv.org/abs/2502.13886
- Reference count: 11
- Models trained on billions of data points improved by ~1% across all tasks with only 100 additional data points

## Executive Summary
This work introduces "fill-tuning," a novel approach to generate targeted datasets for continued pretraining of foundation models that improve model performance across all downstream tasks. The method uses roughness analysis of the latent space to identify regions of poor model understanding, then generates data to fill these gaps. Applied to state-of-the-art materials foundation models trained on billions of data points, fill-tuning improved performance by almost 1% across all downstream tasks with only 100 additional data points. This demonstrates that even after massive pretraining, foundation models can be further improved with minimal targeted data, and that data quality is more important than quantity for model performance.

## Method Summary
Fill-tuning identifies poor-quality regions in foundation model embeddings through roughness analysis, then generates targeted data to improve general performance. The method samples 5000 points from the latent space, computes molecular similarity surfaces using RBF interpolation, and locates regions with high "frustration" (rapid similarity changes). It then generates 100 new molecules by sampling along high-frustration vectors and decodes them to SELFIES strings. These molecules are used for continued self-supervised pretraining via mask prediction, improving all downstream tasks simultaneously without task-specific bias.

## Key Results
- Fill-tuning improved model performance by ~1% across all downstream tasks with only 100 additional data points
- Fill-tuning datasets transfer across model scales within the same data lineage, improving medium (+0.25%) and large (+0.98%) models from small-model-derived data
- Fill-tuning outperforms random, task-specific, and combined continued pretraining approaches on average

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Latent space roughness correlates with embedding quality; high-roughness regions indicate poor model understanding.
- **Mechanism:** The frustration metric quantifies how rapidly molecular similarity changes over small distances in the embedding. Regions with high frustration ("activity cliffs") indicate where the model has failed to place similar molecules close together, creating discontinuities that harm downstream task performance.
- **Core assumption:** Smooth similarity surfaces in latent space are predictive of downstream task accuracy (assumes the embedding geometry reflects functional relationships).
- **Evidence anchors:**
  - [section 2.1] "We assume that the smoothness of properties (herein, the molecular similarity within the embedding) can be a proxy for the accuracy of downstream tasks trained from the embedding."
  - [section 2.3] "Roughness analysis will specifically highlight where the quality of the embedding is most rapidly decreasing, which will likely lie within, or at the edges of, the space spanned by the encoded training data."
  - [corpus] Weak direct support; related work on molecular representation roughness (Graff et al., Aldeghi et al.) is cited but not directly tested in this paper's corpus neighbors.
- **Break condition:** If downstream tasks depend on non-similarity-based features, smoothness may not predict performance. Also breaks if interpolation surface poorly approximates true latent space topology.

### Mechanism 2
- **Claim:** Adding data to high-roughness regions improves general performance across all downstream tasks, unlike task-specific fine-tuning.
- **Mechanism:** By sampling from frustration-weighted Gaussians placed along minimum-to-transition-state vectors, the method generates molecules in regions where the embedding is most confused. Continued self-supervised pretraining on these 100 points allows the model to reorganize its embedding without task-specific bias.
- **Core assumption:** The decoded SELFIES from high-roughness latent points are valid/useful training examples, even if chemically unusual (the paper notes many are non-standard chemistry).
- **Evidence anchors:**
  - [abstract] "...show model improvement of almost 1% in all downstream tasks with the addition of only 100 data points."
  - [table 2] Fill-tuning shows +0.75% average improvement vs. random (-0.18%), task-specific (-1.43%), and combined (-2.43%) continued pretraining.
  - [section 3.1] "The fill-tuning data that improves model performance has little relevance to the benchmark tasks, and is distinct from the foundation model training data."
  - [corpus] Indirect support from Skinnider (2024) cited in paper: "invalid SMILES strings remain valuable for developing accurate models."
- **Break condition:** If generated molecules are too far from chemically valid space, the model may not learn meaningful corrections. Limited to 100 points; scaling behavior unknown.

### Mechanism 3
- **Claim:** Fill-tuning datasets transfer across model scales within the same data lineage.
- **Mechanism:** Models trained on the same source data (ZINC22, PubChem) develop similar embedding deficiencies regardless of parameter count. Fill-tuning data generated from a small model's latent space corrects shared structural gaps in larger models.
- **Core assumption:** Embedding deficiencies are determined by training data distribution, not model architecture or scale.
- **Evidence anchors:**
  - [section 3.2] "We use the same fill-tuning dataset as in the previous section, derived from the latent space of the SELFIES-TED (small) model, and perform continued training of the larger models with it."
  - [table 3] Small-derived fill-tuning dataset improves medium (+0.25%) and large (+0.98%) models.
  - [corpus] No direct external validation; this transfer property is claimed within this paper only.
- **Break condition:** Breaks when models have fundamentally different architectures or training data sources. The paper only tests BART-based models with shared data sources.

## Foundational Learning

- **Concept: Latent space topology**
  - Why needed here: Fill-tuning operates entirely on the geometry of the embedding space; understanding how points, neighborhoods, and smoothness relate to model behavior is prerequisite.
  - Quick check question: Can you explain why two similar molecules placed far apart in latent space would harm a classifier?

- **Concept: Self-supervised vs. supervised fine-tuning**
  - Why needed here: Fill-tuning uses continued pretraining (mask prediction on SELFIES), not task labels. This distinction is why it avoids out-of-distribution degradation.
  - Quick check question: What is the training objective during fill-tuning, and why doesn't it require labeled benchmark data?

- **Concept: Energy landscape analysis (stationary points, transition states)**
  - Why needed here: The frustration metric borrows from computational chemistry—minima, transition states, and kinetic transition networks structure the roughness analysis.
  - Quick check question: What does a transition state in the similarity surface represent, and how does it connect to embedding quality?

## Architecture Onboarding

- **Component map:**
  Embedding extractor -> Similarity surface constructor -> Roughness analyzer -> Data generator -> Continued pretrainer

- **Critical path:** Roughness computation → transition state location → Gaussian surface construction → basin-hopping optimization → decode & retrain. The most computationally expensive step is stationary point location in 128 dimensions.

- **Design tradeoffs:**
  - **Sampling density (5000 points):** Higher density improves surface accuracy but increases cost linearly.
  - **Gaussian parameters (σ=0.99, δ=0.25, l=80.0):** These control where along frustration vectors data is sampled; paper does not ablate sensitivity.
  - **Dataset size (100 points):** Chosen empirically; scaling not explored.
  - **Training epochs (up to 50):** Early stopping based on training/validation loss.

- **Failure signatures:**
  - Generated SELFIES decode to invalid or trivial molecules (the paper notes many are non-standard chemistry).
  - Continued pretraining increases loss or shows no improvement → roughness surface may not reflect true deficiencies.
  - Task-specific degradation appears → accidentally sampled from task-relevant regions, introducing bias.
  - No transfer to larger models → architectures or training data differ fundamentally.

- **First 3 experiments:**
  1. **Reproduce roughness surface on held-out molecules:** Take 1000 molecules not used in surface construction, compute predicted vs. actual neighborhood similarity to validate interpolation quality.
  2. **Ablate dataset size:** Test 25, 50, 100, 200 fill-tuning points to establish scaling and minimum viable dataset.
  3. **Cross-architecture test:** Apply fill-tuning to a different foundation model family (e.g., graph-based instead of string-based) with similar training data to test the transfer claim's architecture-independence.

## Open Questions the Paper Calls Out

- **Question:** Can fill-tuning be successfully extended to foundation models with different architectures and modalities beyond the BART-based molecular SELFIES representations demonstrated here?
  - **Basis in paper:** [explicit] "There are many avenues for further research, and extensions to alternative foundation model architectures and modalities."
  - **Why unresolved:** The current work only validates fill-tuning on the SELFIES-TED family of models using a single architecture (BART) and representation (SELFIES strings).
  - **What evidence would resolve it:** Successful application of fill-tuning to other foundation model types (e.g., GNNs, language models, image models) with demonstrated general performance improvements.

- **Question:** What is the theoretical mechanism by which training on chemically invalid/nonsense SELFIES strings improves downstream task performance on valid molecules?
  - **Basis in paper:** [inferred] The paper observes that "few of the molecules generated by fill-tuning exhibit standard chemistry" and notes a related phenomenon with invalid SMILES (Skinnider, 2024), but provides no mechanistic explanation.
  - **Why unresolved:** The empirical effectiveness is demonstrated but the underlying mechanism—how invalid data reconfigures embeddings to help valid chemistry tasks—remains unexplained.
  - **What evidence would resolve it:** Mechanistic interpretability studies analyzing embedding space transformations, or ablation studies comparing valid vs. invalid fill-tuning data.

- **Question:** How does fill-tuning dataset size scale with performance improvement, and is there an optimal or saturation point?
  - **Basis in paper:** [inferred] The paper uses exactly 100 data points throughout without exploring whether larger fill-tuning datasets yield greater improvements or exhibit diminishing returns.
  - **Why unresolved:** No systematic study of fill-tuning dataset size was conducted; the 100-point choice appears arbitrary.
  - **What evidence would resolve it:** Experiments varying fill-tuning dataset size across multiple orders of magnitude while measuring downstream task performance changes.

- **Question:** Why do fill-tuning datasets generated from smaller models successfully transfer to improve larger models with different architectures and more parameters?
  - **Basis in paper:** [inferred] The paper notes surprise that "the data that was most necessary to correct problems in the latent space of the small model remain important in improving the embedding of the larger models" but does not explain why this transfer occurs.
  - **Why unresolved:** The finding that architectural differences and additional parameters "do not sufficiently change the structure of the embedding in its poorest regions" lacks theoretical grounding.
  - **What evidence would resolve it:** Comparative analysis of roughness distributions across model scales, or studies tracking how specific latent space regions evolve during model scaling.

## Limitations

- The method's robustness to different model architectures and training data sources remains untested, with transfer properties only validated within the same model family
- The chemical validity of generated molecules is not rigorously evaluated—many are acknowledged as "non-standard chemistry" without quantification
- Sensitivity to key hyperparameters (Gaussian parameters, dataset size) was not explored, making optimal choices unclear

## Confidence

- **High confidence**: The roughness analysis mechanism and its connection to embedding quality is well-grounded in related work on molecular representation geometry (Graff et al., Aldeghi et al.)
- **Medium confidence**: The ~1% improvement claim is directly supported by ablation experiments in this paper, but limited to one model family and specific datasets
- **Low confidence**: The transfer property across model scales and architectures is claimed but not rigorously tested beyond BART-based models with shared training data

## Next Checks

1. **Cross-architecture validation**: Apply fill-tuning to a different foundation model family (e.g., graph-based molecular representations) with similar training data to test architecture-independence
2. **Chemical validity assessment**: Systematically evaluate the chemical validity and diversity of generated molecules using established cheminformatics metrics (e.g., QED, synthetic accessibility)
3. **Parameter sensitivity analysis**: Ablate the key hyperparameters (Gaussian width σ, sampling offset δ, lengthscale l, dataset size) to establish robustness and identify optimal ranges