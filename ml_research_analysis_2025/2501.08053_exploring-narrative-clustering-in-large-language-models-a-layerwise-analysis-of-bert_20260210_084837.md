---
ver: rpa2
title: 'Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis
  of BERT'
arxiv_id: '2501.08053'
source_url: https://arxiv.org/abs/2501.08053
tags:
- bert
- neural
- schilling
- data
- krauss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether BERT, a transformer-based large
  language model, exhibits localized neural processing for sequential data analogous
  to biological neural systems. Using a dataset of 1000 narratives with diverse authorial
  styles and content, BERT's layerwise activations were analyzed through dimensionality
  reduction techniques (PCA and MDS).
---

# Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT

## Quick Facts
- **arXiv ID**: 2501.08053
- **Source URL**: https://arxiv.org/abs/2501.08053
- **Reference count**: 36
- **Key outcome**: BERT exhibits progressively stronger clustering of narrative content in later layers while minimally encoding authorial style, supporting localized processing in transformer models.

## Executive Summary
This study investigates whether BERT exhibits localized neural processing for sequential data analogous to biological neural systems. Using 1000 narratives with diverse authorial styles, the research analyzes BERT's layerwise activations through dimensionality reduction techniques. Results reveal strong clustering of narrative content in later layers with progressively compact and distinct clusters, while minimal clustering is observed for authorial style. The study demonstrates BERT's prioritization of semantic content over stylistic features, contributing to understanding how transformer models encode linguistic information and their potential for cognitive modeling.

## Method Summary
The study uses 1000 narratives generated via GPT-4 style transfer from 10 base texts rewritten in 9 other authorial styles, repeated 10 times. Pretrained BERT-base-uncased processes these narratives, extracting [CLS] token embeddings from all 13 layers (768 dimensions). Dimensionality reduction via PCA and MDS projects these activations to 2D for visualization. GDV (Generalized Discrimination Value) quantifies clustering quality in the original 768-dimensional space, separately for narrative content labels and authorial style labels. Layerwise trends are analyzed to understand representational progression.

## Key Results
- Strong clustering of narrative content in later BERT layers, with progressively compact and distinct clusters
- Minimal clustering observed for authorial style specific to individual writers
- GDV trends show highly negative values for narrative content clustering, becoming more negative in later layers, while remaining near zero for authorial style

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT's later layers form progressively compact clusters for narrative content while earlier layers show weaker separation.
- Mechanism: Hierarchical abstraction—information flows through 13 layers, with semantic content representations becoming increasingly refined and distinct in deeper layers. The self-attention mechanism aggregates contextual information progressively.
- Core assumption: The [CLS] token embedding captures sufficient representation of the full narrative for clustering analysis.
- Evidence anchors:
  - [abstract] "strong clustering of narrative content in later layers, with progressively compact and distinct clusters"
  - [section III, Results] "layerwise GDV trends for narrative content are highly negative, becoming progressively more compact and distinct in the later layers of BERT"
  - [corpus] Weak direct evidence—neighbor papers focus on authorial style modeling but not BERT layerwise dynamics.

### Mechanism 2
- Claim: BERT minimally encodes authorial style relative to semantic content.
- Mechanism: The pre-training objective (masked language modeling) prioritizes semantic coherence over stylistic nuance. Style transfer performed by GPT-4 may introduce semantic drift that dominates over pure stylistic signals.
- Assumption: GPT-4's style transfer preserves content while only modifying style—a non-trivial assumption.
- Evidence anchors:
  - [abstract] "minimal clustering is observed for authorial style specific to individual writers"
  - [section IV, Conclusion] "no meaningful clustering was observed for authorial styles, as reflected in both the projected and raw activation spaces"
  - [corpus] Partial support—neighbor papers on authorial style detection use different architectures and corpora.

### Mechanism 3
- Claim: Dimensionality reduction via PCA and MDS preserves meaningful clustering structure from the 768-dimensional activation space.
- Mechanism: With N=1000 exceeding embedding dimension D=768, PCA operates on the column space, capturing dominant variance axes. MDS preserves pairwise distances, validating that clusters aren't projection artifacts.
- Core assumption: 2D projections retain sufficient structure to assess clustering quality.
- Evidence anchors:
  - [section II.C] "By ensuring that the number of narratives (1000) exceeds the embedding dimension (768), PCA can operate from the column space"
  - [section III] "These consistent observations validate the projection methodology"
  - [corpus] No direct corpus validation of this specific dimensional reduction approach for BERT.

## Foundational Learning

- **Transformer layerwise representations**:
  - Why needed here: BERT's 13 layers produce different embeddings; understanding that each layer refines representations is essential for interpreting results.
  - Quick check question: Why would later layers show more compact clustering than earlier layers?

- **[CLS] token as sequence representation**:
  - Why needed here: The study extracts [CLS] embeddings rather than pooling all tokens; this token is designed to aggregate sequence-level information.
  - Quick check question: What is the purpose of the [CLS] token in BERT's architecture?

- **Generalized Discrimination Value (GDV)**:
  - Why needed here: GDV quantifies cluster separability (negative = better separation); understanding this metric is critical for interpreting Figure 5.
  - Quick check question: What does a GDV of -1 versus 0 signify?

## Architecture Onboarding

- **Component map**: GPT-4 generated narratives → BERT tokenizer → 13 transformer layers → [CLS] token extraction → PCA/MDS projection → GDV computation

- **Critical path**: 1. Dataset construction via GPT-4 style transfer (ensures content-style factorial design) → 2. [CLS] extraction from all 13 layers (captures representational progression) → 3. GDV computation in original space (quantifies clustering objectively)

- **Design tradeoffs**:
  - GPT-4 generated narratives may not reflect authentic authorial styles; trade-off between experimental control and ecological validity
  - Using only [CLS] token versus mean pooling; [CLS] is task-optimized but may miss token-level nuance
  - 1000 narratives ensures PCA operates on column space but may still be sparse for 768 dimensions

- **Failure signatures**:
  - GDV values near zero for both content and style → model not encoding meaningful distinctions
  - PCA and MDS showing different cluster patterns → projection artifacts
  - High variance within content clusters in later layers → content encoding not converging

- **First 3 experiments**:
  1. Replicate with mean-pooled token embeddings instead of [CLS] to test representation robustness.
  2. Test on human-authored narratives (not GPT-4 generated) to validate style transfer didn't confound results.
  3. Extend to other encoder architectures (e.g., RoBERTa, DeBERTa) to assess generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the absence of authorial style clustering caused by BERT's architecture or by the failure of GPT-4 to effectively mimic distinct authorial nuances in the dataset construction phase?
- Basis in paper: [inferred] The methodology relies on GPT-4 to perform "neural style transfer," yet the paper offers no independent validation (e.g., human evaluation or stylometric analysis) to confirm that the generated texts successfully captured the distinct styles of authors like Shakespeare or Dickens before they were analyzed by BERT.
- Why unresolved: The "ground truth" of the style labels depends entirely on the generation prompt; if GPT-4 generated stylistically similar text regardless of the prompt, BERT's failure to cluster is a reflection of the data, not the model.
- What evidence would resolve it: A qualitative or quantitative verification of the generated dataset showing that stylistic markers differ significantly between classes before processing by BERT.

### Open Question 2
- Question: What is the precise boundary of "stylistic distinctness" required for BERT to form clusters, given the model appears sensitive to broad text types but not specific authorial voices?
- Basis in paper: [explicit] The authors explicitly contrast their findings with prior work [35], noting that BERT shows strong clustering for broad text types (e.g., sci-fi vs. fables) but minimal clustering for "subtle variations inherent in individual authorial styles," suggesting a sensitivity threshold that is not yet defined.
- Why unresolved: The current study presents a binary outcome (content clusters, style does not) but does not map the continuum of feature magnitude required to trigger representational separation in the later layers.
- What evidence would resolve it: An experiment measuring GDV across a controlled gradient of stylistic intensity, ranging from subtle authorial mimicry to exaggerated genre shifts.

### Open Question 3
- Question: Do these layerwise clustering patterns generalize to decoder-only or encoder-decoder architectures, or are they specific to BERT's bidirectional attention mechanism?
- Basis in paper: [inferred] The study focuses exclusively on BERT (an encoder-only model), yet it frames the implications broadly for "transformer models" and "Artificial Neural Networks" in cognitive modeling. The results may be architecture-dependent.
- Why unresolved: Different architectures process context and sequence differently (e.g., causal masking in GPT vs. bidirectional context in BERT), which could fundamentally alter the prioritization of semantic content over style.
- What evidence would resolve it: Applying the same GDV and dimensionality reduction methodology to equivalent layers in models like GPT-2 or T5 using the same dataset.

## Limitations

- GPT-4-generated narratives may not accurately capture genuine authorial stylistic features, raising questions about ecological validity
- Analysis relies exclusively on [CLS] token embeddings, which may not capture full representational richness
- Dimensionality reduction assumes 2D projections preserve meaningful clustering structure from 768-dimensional space

## Confidence

- **High Confidence**: Progressive clustering of narrative content in later BERT layers (supported by both visual and quantitative evidence)
- **Medium Confidence**: Minimal encoding of authorial style relative to semantic content (supported by data but methodology introduces uncertainty)
- **Low Confidence**: Claim that pre-training objective inherently prioritizes semantic coherence over stylistic nuance (speculative, no direct evidence)

## Next Checks

1. Replicate analysis using mean-pooled token embeddings instead of [CLS] token to test representation robustness
2. Apply same layerwise clustering analysis to human-authored narratives with verified authorial attribution
3. Extend layerwise clustering analysis to other transformer encoder architectures (RoBERTa, DeBERTa, or ALBERT) to assess generalizability