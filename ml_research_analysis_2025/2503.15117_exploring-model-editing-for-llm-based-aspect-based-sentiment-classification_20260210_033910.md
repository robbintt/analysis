---
ver: rpa2
title: Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification
arxiv_id: '2503.15117'
source_url: https://arxiv.org/abs/2503.15117
tags:
- editing
- sentiment
- methods
- aspect
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently adapting large
  language models (LLMs) to aspect-based sentiment classification (ABSC) through a
  novel model editing approach. The authors propose a two-step method that combines
  causal tracing to identify critical mid-layer representations of aspect words with
  targeted model editing.
---

# Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification

## Quick Facts
- **arXiv ID:** 2503.15117
- **Source URL:** https://arxiv.org/abs/2503.15117
- **Reference count:** 7
- **Primary result:** Achieves 87.6% average accuracy across four ABSC datasets while using only 0.006% trainable parameters

## Executive Summary
This paper addresses the problem of efficiently adapting large language models (LLMs) to aspect-based sentiment classification (ABSC) through a novel model editing approach. The authors propose a two-step method that combines causal tracing to identify critical mid-layer representations of aspect words with targeted model editing. By applying weight-based and representation-based editing specifically to these critical components, their approach achieves competitive performance compared to state-of-the-art parameter-efficient fine-tuning methods while using significantly fewer trainable parameters. Experiments demonstrate the method's effectiveness in both in-domain and out-of-domain scenarios, showing that mid-layer representations (layers 10-15 in LLaMA2-7B) play a crucial role in detecting sentiment polarity for aspect terms.

## Method Summary
The method employs a two-step approach: first, causal tracing identifies critical mid-layer representations (layers 10-15) that causally mediate sentiment predictions for aspect terms; second, targeted model editing is applied to these identified layers. The editing combines weight-based modification using LoRA on attention output projections and representation-based editing using linear interventions applied only at aspect token positions. Training uses AdamW with learning rates of 3e-4 for LoRA weights and 1e-5 for representation weights, running for one epoch. The approach requires only 0.006% of total parameters to be trainable, achieving 87.6% average accuracy across four ABSC datasets while maintaining strong out-of-domain generalization.

## Key Results
- Achieves 87.6% average accuracy across four ABSC datasets (Restaurant, Laptop, Device, Service)
- Uses only 0.006% trainable parameters compared to full fine-tuning
- Out-of-domain performance shows strong generalization across domain pairs
- Representation editing at aspect positions yields 90.2% accuracy, significantly higher than other positions (last word: 89.1%, random: 83.2%)

## Why This Works (Mechanism)

### Mechanism 1: Causal Tracing for Layer Localization
The method identifies that mid-layer representations (specifically layers 10-15 in LLaMA2-7B) causally mediate sentiment polarity predictions for aspect terms. By measuring Average Indirect Effect (AIE) when restoring individual hidden states after corruption, it determines which layer-position pairs most strongly recover correct predictions. This reveals that mid-layer representations of aspect tokens are essential for detecting sentiment polarity.

### Mechanism 2: Targeted Weight Editing via Low-Rank Adaptation
Editing only the attention output projection matrix in critical layers is sufficient to adapt LLMs for ABSC with minimal parameters. Low-rank matrices A and B approximate weight updates, selectively modifying how aspect token relationships are encoded without disrupting other layers. This exploits the attention mechanism's role in capturing relationships between aspect tokens and context.

### Mechanism 3: Position-Specific Representation Editing
Intervening on residual stream representations at aspect token positions maximally steers sentiment predictions. A learned linear intervention is applied specifically at the aspect term position, modifying the hidden state by subtracting the projected component and adding a learned correction. This exploits aspect tokens as "control points" where sentiment-relevant computations converge.

## Foundational Learning

- **Concept: Causal Mediation Analysis**
  - Why needed here: Understanding how causal tracing identifies "responsible" neural components requires grasping intervention-based attribution
  - Quick check question: If corrupting input embeddings reduces accuracy by 40%, and restoring layer-12 hidden states recovers 30%, what does this suggest about layer 12?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Weight-based editing uses LoRA to constrain parameter updates to a low-rank subspace
  - Quick check question: Why does factorizing ΔW as AB (where rank r << d) reduce trainable parameters?

- **Concept: Residual Stream Interventions**
  - Why needed here: Representation editing operates on the residual stream; understanding how additive interventions modify layer outputs is essential
  - Quick check question: In equation (7), what happens if R is an identity matrix?

## Architecture Onboarding

- **Component map:** Causal tracer -> Weight editor (LoRA on attention output) -> Representation editor (linear intervention at aspect positions) -> Training objective (cross-entropy)
- **Critical path:** Causal tracing (offline) → Identify layers 10-15 and aspect positions → Train weight + representation editors jointly → Inference applies both interventions
- **Design tradeoffs:** Layer range (10-15) balances parameter efficiency vs coverage; aspect-only position targeting is optimal but requires known aspect spans; weight editing is persistent while representation editing is inference-time
- **Failure signatures:** Random mid-position performance (83.2%) indicates aspect position is critical; out-of-domain generalization shows variance across domain pairs; some pairs (e.g., L→S) underperform suggesting domain-specific latent features
- **First 3 experiments:**
  1. Replicate causal tracing on a different decoder LLM (e.g., Mistral-7B) to verify layer localization transfer
  2. Ablate representation editing alone vs. weight editing alone to isolate contribution of each component
  3. Test implicit aspect scenarios where aspect terms are not provided, requiring external aspect extraction before editing

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the causal-tracing and model editing framework be effectively generalized to other downstream NLP tasks where the causal signal is less localized than explicit "aspect" tokens?
- **Open Question 2:** Does the critical importance of mid-layer representations (layers 10-15) persist across significantly larger model scales (e.g., 70B+ parameters) or different architectural families?
- **Open Question 3:** Is the assumption that sentiment is strictly localized to explicit aspect token positions valid for implicit sentiment expressions where no specific aspect is mentioned?
- **Open Question 4:** How robust is the layer identification process to the specific noise magnitude used in the "corrupted run" during causal tracing?

## Limitations

- **Causal validity uncertainty:** The paper assumes AIE recovery implies causal mediation, but this could be correlation rather than true causation
- **Hyperparameter sensitivity:** Critical hyperparameters (LoRA rank, noise level σ, batch size) are not fully specified, affecting reproducibility
- **Position dependence vulnerability:** Performance heavily depends on known aspect positions, with significant drops when applied to non-aspect positions

## Confidence

- **High Confidence:** Weight-based LoRA component follows established methodology and parameter efficiency claims are verifiable
- **Medium Confidence:** Representation editing mechanism is novel and theoretically sound, but position-specific claims rely on limited comparisons
- **Low Confidence:** Causal tracing mechanism's validity as true causal attribution cannot be independently verified without implementation details

## Next Checks

1. **Causal Tracing Replication:** Implement the causal tracing method on a different decoder LLM (Mistral-7B) to verify whether mid-layer (10-15) localization transfers across architectures
2. **Component Ablation Study:** Train models with only weight editing, only representation editing, and both combined to quantify each component's contribution to the 87.6% accuracy claim
3. **Implicit Aspect Evaluation:** Remove explicit aspect markers from inputs and evaluate whether the model can maintain performance using an external aspect extraction system