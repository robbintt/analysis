---
ver: rpa2
title: 'LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically
  Generating Bayesian Optimization Algorithms'
arxiv_id: '2505.21034'
source_url: https://arxiv.org/abs/2505.21034
tags:
- algorithms
- optimization
- algorithm
- functions
- aocc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaMEA-BO, a framework that uses large language
  models (LLMs) to automatically generate complete Bayesian optimization (BO) algorithms.
  The method evolves BO algorithm code through an evolutionary strategy that guides
  LLM prompts to produce candidate algorithms.
---

# LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms

## Quick Facts
- **arXiv ID:** 2505.21034
- **Source URL:** https://arxiv.org/abs/2505.21034
- **Reference count:** 40
- **Primary result:** Automatically generates BO algorithms that outperform state-of-the-art baselines on 19/24 BBOB functions in 5 dimensions

## Executive Summary
This paper introduces LLaMEA-BO, a framework that uses large language models (LLMs) to automatically generate complete Bayesian optimization (BO) algorithms. The method evolves BO algorithm code through an evolutionary strategy that guides LLM prompts to produce candidate algorithms. These candidates are evaluated on the BBOB benchmark suite, and top performers are selected, combined, and mutated to refine the population over iterations. The approach generates BO algorithms with components like initialization, surrogate modeling, and acquisition functions. LLaMEA-BO produces algorithms that outperform state-of-the-art BO baselines on 19 of 24 BBOB functions in 5 dimensions and generalize well to higher dimensions and real-world hyperparameter tuning tasks. This demonstrates that LLMs can serve as algorithmic co-designers, offering a new paradigm for automating BO development.

## Method Summary
LLaMEA-BO combines LLM-powered code generation with evolutionary strategies to automatically discover BO algorithms. The framework uses gemini-2.0-flash to generate Python code for BO algorithms based on templates and evolutionary prompts. An evolutionary loop manages a population of candidate algorithms, evaluating each on a subset of 10 BBOB functions with a budget of 100 evaluations. The framework uses non-elitist (µ, λ) ES with population sizes like (4,16) to evolve the algorithms through selection, crossover, and mutation. The best algorithms are then validated on the full BBOB suite, higher dimensions, and real-world tasks like Bayesmark.

## Key Results
- Generated algorithms outperform state-of-the-art BO baselines on 19 of 24 BBOB functions in 5 dimensions
- Algorithms generalize well to higher dimensions (10, 20, 40) without retraining
- Best algorithm achieves 25% relative improvement over best baseline on Rosenbrock synthetic dataset

## Why This Works (Mechanism)
The method works by treating BO algorithm design as a search problem in the space of executable code. LLMs serve as powerful generators that can produce syntactically correct Python code for BO algorithms when guided by well-structured prompts. The evolutionary strategy provides a principled framework for exploration and exploitation in this code space, using fitness evaluations on benchmark functions to guide the search. The non-elitist selection strategy prevents premature convergence to local optima in the algorithm space, while crossover operations combine successful algorithmic components. The evaluation sandbox ensures safe execution of generated code and provides robust fitness scores even when algorithms fail.

## Foundational Learning
- **Concept: Bayesian Optimization (BO)** - Why needed here: The entire paper is about generating algorithms for this. Without understanding the core loop (surrogate model, acquisition function, evaluation), you cannot interpret what the LLM is being asked to design or why certain generated components matter. Quick check: Can you explain the roles of the Gaussian Process and the acquisition function in a standard BO loop?
- **Concept: Evolutionary Strategies (ES)** - Why needed here: LLaMEA-BO uses a (µ, λ) or (µ + λ) ES to drive the search. Understanding concepts like parent/offspring populations, selection, crossover, and mutation is essential to grasp how the framework iteratively improves candidate algorithms. Quick check: What is the difference between elitist and non-elitist selection in an evolution strategy?
- **Concept: Prompt Engineering for Code Generation** - Why needed here: The mechanism hinges on carefully crafted prompts (task descriptions, templates, instructions to "mutate" or "crossover"). The performance of the system is highly dependent on how these prompts are structured. Quick check: How might providing a code template to an LLM differ from simply asking it to "write a Bayesian optimization algorithm from scratch"?

## Architecture Onboarding
- **Component map:** LLM Engine -> Prompting Module -> Evaluation Sandbox -> Evolutionary Controller
- **Critical path:** Initialization (generate µ algorithms) -> Evaluation (run on 10 BBOB functions) -> Evolutionary Loop (selection -> variation -> generation -> evaluation -> update) -> Validation (test best on full suite)
- **Design tradeoffs:** Elitist vs. Non-elitist Selection (preservation vs. exploration), Evaluation Budget (speed vs. accuracy), Template Strictness (error reduction vs. novelty)
- **Failure signatures:** Stalled Progress (plateaued AOCC), High Invalid Code Rate (many errors), Surrogate Overfitting (excellent on subset, poor on full suite)
- **First 3 experiments:** 1) Baselines with Minimal Template (1+1-ES, minimal template), 2) Vary ES Configuration ((4,16), (8+16) strategies), 3) Out-of-Sample Generalization (test best algorithm on full BBOB and Bayesmark)

## Open Questions the Paper Calls Out
None

## Limitations
- The approach depends heavily on prompt quality and LLM capabilities for generating syntactically correct BO code
- Fitness evaluation with only 100 function evaluations per candidate may not fully capture algorithm potential
- The evolutionary strategy's success relies on appropriate template design and parameters not fully detailed in the main text

## Confidence
- **High confidence**: The framework successfully generates BO algorithms that outperform baselines on the specified 19/24 BBOB functions in 5 dimensions
- **Medium confidence**: The generalization claims to higher dimensions and real-world tasks are supported but would benefit from more extensive validation across diverse problem domains
- **Medium confidence**: The superiority of non-elitist strategies is demonstrated but the analysis of why elitist strategies underperform could be more thorough

## Next Checks
1. Reproduce the full evolutionary loop with (4,16) non-elitist ES on the 10 training BBOB functions and verify that generated algorithms achieve comparable AOCC scores to those reported
2. Test the top-3 generated algorithms on the complete 24-function BBOB suite in dimensions 10, 20, and 40 to independently verify the generalization claims
3. Run the best algorithm from the evolutionary search on at least 3 additional Bayesmark tasks not used in the original validation to assess real-world applicability