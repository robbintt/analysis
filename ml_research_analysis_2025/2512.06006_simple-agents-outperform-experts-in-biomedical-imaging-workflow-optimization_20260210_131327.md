---
ver: rpa2
title: Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization
arxiv_id: '2512.06006'
source_url: https://arxiv.org/abs/2512.06006
tags:
- functions
- image
- agent
- function
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization

## Quick Facts
- arXiv ID: 2512.06006
- Source URL: https://arxiv.org/abs/2512.06006
- Reference count: 40
- Simple agents outperform expert-engineered pipelines in adapting biomedical imaging tools to new datasets

## Executive Summary
This paper demonstrates that simple LLM-based agents can automatically generate preprocessing and postprocessing code that outperforms expert-engineered solutions for adapting pretrained biomedical imaging tools to new datasets. The agents use minimal context—just a task prompt, dataset description, and access to standard image processing libraries—without requiring extensive API lists or expert-provided code examples. The approach is particularly effective in "dispersed" solution spaces where many valid API combinations exist, outperforming both expert baselines and more complex agent architectures.

## Method Summary
The method uses a Base Agent consisting of a task prompt, data prompt, coding agent (LLM), execution agent, and API list. For each task, the agent generates preprocessing and postprocessing functions to adapt pretrained tools (Polaris, Cellpose, MedSAM) to new datasets, optimizing against small validation sets (10-100 images). The agent runs 20 iterations with 3 function pairs per iteration, totaling 60 trials. Performance is evaluated by selecting the top 15 validation functions and reporting the maximum test score. The study systematically tests various context components (API lists, expert functions, function banks) and different LLMs (GPT-4.1, o3, Llama 3.3-70B).

## Key Results
- Simple Base Agents outperform expert-engineered baselines by up to 8.3 F1 points on Polaris task
- Removing API lists improves performance by reducing harmful selection bias
- Agent performance varies significantly based on solution space topology (dispersed vs. concentrated)

## Why This Works (Mechanism)

### Mechanism 1: Latent Knowledge Activation via Minimal Prompting
Omitting explicit API lists reduces selection bias and leverages the LLM's pre-existing code knowledge, allowing exploration of the full solution space rather than over-utilizing specific functions.

### Mechanism 2: Constraint-Induced Regularization of Search Space
Expert-provided code examples constrain search space beneficially for "hard-to-optimize" parameter spaces but detrimentally for "dispersed" API spaces, restricting exploration needed for novel API combinations.

### Mechanism 3: Diversity-Performance Trade-off in Memory Augmentation
Persistent memory (Function Banks) increases solution diversity in concentrated tasks but causes code bloat and degradation in dispersed tasks, leading to progressively longer, potentially overfitted code.

## Foundational Learning

- **Solution Space Topology**: Understanding concentrated (few key APIs) vs. dispersed (many valid API combinations) spaces is crucial for predicting agent performance. Quick check: Does the pipeline rely on standard sequential steps or heterogeneous techniques?

- **Low-Data Regime Overfitting**: Agents optimize against small validation sets, making them susceptible to overfitting. Quick check: Is the agent optimizing against a fixed validation set without hold-out testing?

- **Agentic Feedback Loops**: The coding agent generates code while the execution agent returns scores, creating a loop that can get stuck in local minima. Quick check: Can the agent distinguish between syntax errors and poor algorithmic logic?

## Architecture Onboarding

- **Component map**: Task Prompt -> Coding Agent -> Execution Agent -> Metric Score
- **Critical path**: 1) Define metric and data loader, 2) Implement Base Agent, 3) Run 20 iterations (3 functions each) for baseline
- **Design tradeoffs**: Simplicity vs. Guidance (Base Agent robust but may struggle with parameters; Expert Functions help parameters but hurt exploration)
- **Failure signatures**: Code Bloat (excessive length/complexity), API Bias (over-reliance on specific functions), Validation Overfitting (high val but poor test scores)
- **First 3 experiments**: 1) Baseline vs. Expert comparison, 2) Data Prompt ablation to verify prompt effectiveness, 3) API List ablation to test bias reduction

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the observed agent design principles generalize to scientific domains outside biomedical imaging?
- **Open Question 2**: How can agent frameworks be made resilient to overfitting when optimizing against small validation sets?
- **Open Question 3**: Under what specific conditions do complex agent designs provide justifiable performance gains?
- **Open Question 4**: Can the nature of a task's solution space be characterized a priori to select the optimal agent design?

## Limitations

- Results are limited to three specific biomedical imaging tasks with small validation sets (10-100 images)
- Potential for validation overfitting due to direct optimization against small gold-standard sets
- Solution space taxonomy is somewhat post-hoc without systematic classification method for new tasks

## Confidence

- **High confidence**: Core empirical finding that removing API lists improves performance (robust across multiple tasks)
- **Medium confidence**: Mechanism that "expert constraints harm dispersed spaces" (supported by data but relies on post-hoc analysis)
- **Low confidence**: Claim that "simple agents universally outperform experts" (evidence limited to 3 specific biomedical tasks)

## Next Checks

1. **Cross-domain replication**: Test Base Agent approach on non-biomedical computer vision tasks to assess domain generality
2. **Overfitting quantification**: Run identical experiments with held-out validation subset to measure generalization gap
3. **Solution space taxonomy validation**: Develop systematic metric for classifying tasks as dispersed vs. concentrated before running experiments