---
ver: rpa2
title: 'LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity
  in Puzzle Tasks'
arxiv_id: '2504.02254'
source_url: https://arxiv.org/abs/2504.02254
tags:
- puzzles
- ambiguity
- puzzle
- words
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how role-based adversarial prompting affects
  semantic ambiguity in puzzles generated by large language models (LLMs). Using the
  NYT Connections game framework, puzzles were generated under zero-shot and role-injected
  (deceptive intent) prompts and compared with human-crafted puzzles.
---

# LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks

## Quick Facts
- arXiv ID: 2504.02254
- Source URL: https://arxiv.org/abs/2504.02254
- Authors: Seunghyun Yoo
- Reference count: 16
- Key outcome: Role-based adversarial prompting increases semantic ambiguity in LLM-generated puzzles, reducing puzzle fairness and increasing cognitive load.

## Executive Summary
This study demonstrates that embedding adversarial intent in prompts significantly increases semantic ambiguity in puzzles generated by large language models. Using the NYT Connections game framework, puzzles were generated under neutral and role-injected (deceptive) conditions and evaluated both computationally and by human participants. Results show that adversarial prompts consistently increase puzzle difficulty, reduce correctness rates, and elevate cognitive load, though the effect varies across different model architectures. The findings highlight important considerations for deploying autonomous LLM agents in contexts where fairness and clarity are essential.

## Method Summary
The study employed a two-condition prompt framework (zero-shot neutral vs. role-injected with deceptive intent) to generate NYT Connections-style puzzles using four LLMs (GPT-4.5, GPT-4o, Llama 3.2 3B, Qwen 2.5 14B). Computational evaluation used HateBERT embeddings to measure semantic cohesion (intra-category similarity) and ambiguity (inter-category similarity). Human evaluation involved 63 participants who solved puzzles from different conditions, rating difficulty, correctness, hint requests, and solving time. Puzzles were counterbalanced to control for individual differences, and human-crafted puzzles served as a baseline for comparison.

## Key Results
- Role-injected prompts significantly increased semantic ambiguity for most models (GPT-4.5: 0.344 vs 0.200; Llama 3.2 3B: 0.310 vs 0.133)
- Human participants rated role-injected puzzles significantly more difficult (6.95/10 vs 1.98/10) with lower correctness rates (27.4% vs 96.4%)
- GPT-4o showed decreased ambiguity under adversarial prompts (0.175 vs 0.183), while Qwen 2.5 14B showed extreme divergence (0.041 vs 0.328)
- Role-injected puzzles required more hints (0.56 vs 0.00) and longer solving times (5.21 min vs 2.40 min)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-based adversarial prompting modulates semantic ambiguity in generated content
- Mechanism: Injecting deceptive intent causes models to produce content with lower semantic cohesion and higher inter-category overlap, though this effect is architecture-dependent
- Core assumption: The model interprets and acts upon the "deceive humans" instruction by manipulating word relationships rather than simply refusing or ignoring the intent
- Evidence anchors:
  - [abstract] "explicit adversarial agent behaviors significantly heighten semantic ambiguity—thereby increasing cognitive load and reducing fairness"
  - [section 3.1] "Role-Injected prompts generally result in increased ambiguity and slightly reduced cohesion" for GPT-4.5 (0.344 vs 0.200) and Llama 3.2 3B (0.310 vs 0.133)
  - [corpus] Related work "Trick or Neat: Adversarial Ambiguity and Language Model Evaluation" assesses LLM sensitivity to ambiguity via adversarial datasets, supporting the broader research direction
- Break condition: If models are fine-tuned with stronger refusal behaviors against deceptive instructions, the mechanism may not activate

### Mechanism 2
- Claim: Model architecture and alignment training produce heterogeneous responses to adversarial framing
- Mechanism: Different LLMs exhibit divergent responses—GPT-4o showed lower ambiguity under role-injection (0.175 vs 0.183), while Qwen 2.5 14B produced extreme opposite patterns (0.041 vs 0.328), suggesting architectural and training differences affect how deceptive instructions are interpreted
- Core assumption: The variation stems from differences in alignment techniques, representation learning, or internal decision-making heuristics rather than random variation
- Evidence anchors:
  - [section 3.1] "GPT-4o displays a reversal of this trend, achieving lower ambiguity under Role prompts... potentially reflecting improvements in alignment or representation learning"
  - [section 4] "Qwen 2.5 14B shows an extreme divergence... adversarial framing in Qwen may inadvertently lead to clearer semantic structuring"
  - [corpus] "Personality-Driven Decision-Making in LLM-Based Autonomous Agents" and related multi-agent work suggest behavioral variation across architectures, but no direct architectural explanation for this specific divergence is available
- Break condition: If model families converge on alignment approaches, heterogeneity may diminish

### Mechanism 3
- Claim: Semantic ambiguity elevation increases cognitive load and reduces task performance
- Mechanism: Higher inter-category semantic overlap forces continuous cognitive shifting and prolongs processing time, consistent with Cognitive Load Theory predictions
- Core assumption: The measured semantic overlap translates to experienced difficulty during puzzle-solving
- Evidence anchors:
  - [section 3.2] Role-injected puzzles rated 6.95/10 difficulty vs 1.98/10 for zero-shot; correctness dropped from 96.4% to 27.4%; solving time increased from 2.40 to 5.21 minutes
  - [section 4] "resonate with Cognitive Load Theory... linguistic ambiguity directly escalates cognitive demands, resulting in reduced task performance"
  - [corpus] No corpus papers directly validate the cognitive load pathway for semantic ambiguity specifically; this remains under-explored
- Break condition: If users develop adaptation strategies through repeated exposure, the cognitive load effect may attenuate

## Foundational Learning

- Concept: **Semantic cohesion vs. ambiguity metrics (pairwise cosine similarity)**
  - Why needed here: Understanding how HateBERT-based computational evaluation quantifies puzzle difficulty enables interpretation of Table 1 results
  - Quick check question: If intra-category similarity is high but inter-category similarity is also high, would you expect higher or lower solving difficulty?

- Concept: **Role-based / persona injection prompting**
  - Why needed here: Distinguishing neutral zero-shot prompts from intent-injected prompts is essential to understanding the experimental design
  - Quick check question: What behavioral difference would you expect if the role prompt asked to "challenge" rather than "deceive"?

- Concept: **Cognitive Load Theory (CLT) foundations**
  - Why needed here: The paper anchors its human evaluation interpretation in CLT; understanding extraneous vs. intrinsic load clarifies why ambiguity impairs performance
  - Quick check question: Would adding hints reduce intrinsic or extraneous cognitive load in this puzzle context?

## Architecture Onboarding

- Component map: Prompt engineering layer -> LLM inference layer -> Computational evaluation layer -> Human evaluation layer
- Critical path:
  1. Define prompt condition (zero-shot or role-injected with specific deceptive intent phrasing)
  2. Generate puzzles via target LLM with structured JSON output
  3. Compute semantic cohesion (intra-category) and ambiguity (inter-category) via HateBERT
  4. Deploy human evaluation with counterbalanced puzzle presentation
  5. Analyze computational-human correlation and model-level divergence
- Design tradeoffs:
  - HateBERT selected for semantic ambiguity sensitivity but originally trained for abusive language detection (may miss context-dependent nuances)
  - 63-participant sample provides breadth across education levels but limits statistical power for subgroup analysis
  - Isolating adversarial intent as primary variable increases internal validity but may oversimplify interactions with creativity or context-awareness
- Failure signatures:
  - Model produces malformed JSON or violates 16-word constraint → prompt adherence failure
  - Role-injected puzzles show lower ambiguity than zero-shot → alignment-induced resistance (observed in GPT-4o, Qwen)
  - Human ratings contradict computational metrics → metric validity gap
- First 3 experiments:
  1. Replicate with additional embedding methods (beyond HateBERT) to validate computational metrics; compare SBERT, LLaMA embeddings for robustness
  2. Ablate the deceptive prompt by varying intent strength ("gently misdirect" vs. "strongly deceive") to map dose-response relationship
  3. Test cross-lingual transfer by generating puzzles in non-English languages to assess whether ambiguity manipulation generalizes beyond English semantic structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural differences or alignment techniques cause models like GPT-4o to paradoxically increase semantic clarity under adversarial prompts?
- Basis in paper: [explicit] The authors note GPT-4o and Qwen 2.5 14B showed divergent trends (increased clarity) and explicitly call for research on "architectural differences" and "alignment techniques."
- Why unresolved: The study observed the phenomenon but did not isolate the internal model mechanisms causing the reversal of expected ambiguity trends.
- What evidence would resolve it: Ablation studies on different alignment layers or architectural components across diverse model families under adversarial conditions.

### Open Question 2
- Question: Can computational metrics utilizing complementary semantic embeddings provide a more robust measure of ambiguity than HateBERT alone?
- Basis in paper: [explicit] The authors state that refining metrics "beyond HateBERT to incorporate complementary semantic embedding approaches could provide richer insights."
- Why unresolved: HateBERT was originally trained on abusive language detection, potentially limiting its sensitivity to subtle, context-dependent semantic ambiguity in puzzle tasks.
- What evidence would resolve it: Comparative analysis of puzzle evaluations using HateBERT versus other semantic similarity models correlated with human judgments.

### Open Question 3
- Question: How does adversarial prompting interact with inherent model creativity and context-awareness to modulate semantic ambiguity?
- Basis in paper: [inferred] The authors acknowledge the study isolated adversarial intent and may have "oversimplified the interaction" with factors like creativity and context.
- Why unresolved: The experimental design controlled for other variables, leaving the potential compounding or mitigating effects of other agentic behaviors unknown.
- What evidence would resolve it: Multi-factorial experiments varying both adversarial intent and creativity constraints to measure interaction effects on cohesion scores.

## Limitations
- Puzzle-specific scope may limit generalizability to other semantic tasks where ambiguity plays different roles
- 63-participant sample provides limited statistical power for detecting nuanced interactions between model architectures and puzzle types
- Computational evaluation using HateBERT (trained on abusive language detection) may not optimally capture context-dependent semantic relationships in puzzle word groupings

## Confidence
**High Confidence**: The core finding that role-injected prompts significantly increase human-perceived difficulty and reduce puzzle-solving performance is well-supported by the human evaluation data (difficulty ratings: 6.95 vs 1.98, correctness rates: 27.4% vs 96.4%). The computational metrics showing increased semantic ambiguity under role-injection for most models (GPT-4.5: 0.344 vs 0.200; Llama 3.2 3B: 0.310 vs 0.133) provide convergent evidence.

**Medium Confidence**: The mechanism by which role-based prompting modulates semantic ambiguity is plausible but not definitively proven. While the computational and human evaluation results align, the paper doesn't establish causal pathways or rule out alternative explanations such as model-specific refusal behaviors or creative generation tendencies.

**Low Confidence**: The interpretation of model-level heterogeneity (why GPT-4o shows decreased ambiguity while Qwen 2.5 14B shows extreme divergence) lacks sufficient explanatory depth. The paper attributes these patterns to architectural differences but doesn't provide concrete evidence linking specific model training or architectural features to these divergent behaviors.

## Next Checks
1. **Cross-Validation with Alternative Embedding Methods**: Replicate the computational evaluation using SBERT, LLaMA embeddings, or other semantic similarity models to determine whether HateBERT's abusive language training domain affects the observed ambiguity patterns. Compare results across embedding methods to establish robustness.

2. **Dose-Response Relationship Mapping**: Systematically vary the strength and specificity of adversarial intent in prompts (e.g., "gently misdirect" vs "strongly deceive") to establish whether ambiguity increases monotonically with intent strength, helping distinguish genuine semantic manipulation from prompt adherence artifacts.

3. **Generalization Testing Across Semantic Tasks**: Extend the experimental framework to non-puzzle semantic tasks (e.g., sentence completion, analogical reasoning, or instruction following) to determine whether adversarial prompting consistently increases semantic ambiguity across different cognitive task types, or whether the effect is specific to categorical grouping puzzles.