---
ver: rpa2
title: 'ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction'
arxiv_id: '2509.07594'
source_url: https://arxiv.org/abs/2509.07594
tags:
- prediction
- network
- data
- collaborative
- elec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ELEC, a framework that combines large language
  models (LLMs) with collaborative feature interaction models for click-through rate
  (CTR) prediction. The key idea is to adapt an LLM to process tabular features as
  text, inject its high-level representations into a collaborative model to form a
  "gain network," and then distill knowledge to a lightweight "vanilla network" that
  only uses tabular data but achieves comparable performance.
---

# ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2509.07594
- Source URL: https://arxiv.org/abs/2509.07594
- Authors: Rui Dong; Wentao Ouyang; Xiangzheng Liu
- Reference count: 36
- Key outcome: Combines LLM with collaborative models for CTR prediction via dual distillation, achieving SOTA AUC (0.8015 on industry dataset) with inference independent of LLM size.

## Executive Summary
ELEC addresses the challenge of integrating large language models into click-through rate prediction while maintaining serving efficiency. The framework transforms tabular features into text, processes them through a frozen LLM to extract semantic representations, and injects these into a collaborative model. Crucially, ELEC uses dual-level knowledge distillation (score and representation) to transfer learned knowledge to a lightweight vanilla network that only uses tabular data at inference time. This approach captures both collaborative and semantic signals while avoiding the computational overhead of LLM inference during serving.

## Method Summary
ELEC operates through a pseudo-siamese architecture with a gain network (LLM + collaborative model) and vanilla network (collaborative model only). The method involves: (1) transforming tabular features to text using field-value templates, (2) processing text through a frozen LLM with an added transformation MLP to generate high-level representations, (3) concatenating these representations with collaborative model features in the gain network, and (4) jointly training both networks with dual distillation losses (CLID for score-level and MSE for representation-level alignment). Only the vanilla network is deployed for inference, achieving comparable performance to the gain network while maintaining constant inference time regardless of base LLM size.

## Key Results
- Achieves state-of-the-art AUC of 0.8015 on industrial dataset and 0.7509 on Amazon All Beauty dataset
- Inference time of vanilla network remains constant (~0.087-0.116s) across all LLM sizes
- Dual distillation significantly improves performance: removing CLID loss degrades AUC by 0.0022 on industry dataset
- Larger LLMs (Qwen2-7B vs RoBERTa-base) yield better performance, demonstrating value of semantic knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting LLM-derived semantic representations into collaborative CTR models improves prediction accuracy by capturing textual meaning that tabular feature interactions miss.
- Mechanism: The gain network concatenates the LLM's high-level representation vector with collaborative model features, leveraging both semantic understanding from text and collaborative signals from tabular data.
- Core assumption: LLM representations contain complementary semantic information that tabular embeddings cannot capture alone.
- Evidence anchors:
  - [abstract] "We inject the high-level representation vector generated by the LLM into a collaborative CTR model to form the gain network such that it can take advantage of both tabular modeling and textual modeling."
  - [section 3.4.1] "ELEC explicitly injects the high-level representation vector of the language model into a collaborative model. ELEC achieves the highest AUCs on experimental datasets."
- Break condition: If LLM representations are noisy or irrelevant to click behavior, injection may add noise rather than signal.

### Mechanism 2
- Claim: Dual-level knowledge distillation (score + representation) transfers semantic knowledge to a lightweight vanilla network, decoupling inference efficiency from LLM dependency.
- Mechanism: CLID loss handles score-level distillation (point-wise calibration + list-wise ranking), while MSE loss aligns representation vectors between gain and vanilla networks.
- Core assumption: The vanilla network can approximate the gain network's behavior through distillation without direct access to LLM features during inference.
- Evidence anchors:
  - [abstract] "We then distill the knowledge from the gain network to the vanilla network on both the score level and the representation level, such that the vanilla network takes only tabular data as input, but can still generate comparable performance as the gain network."
  - [section 3.4.2] Ablation shows removing CLID loss causes "large performance degradation" (AUC drops 0.0022 on Industry).
- Break condition: If semantic knowledge is too complex to compress into tabular-only representations, distillation may lose critical information.

### Mechanism 3
- Claim: Freezing LLM parameters and training only transformation/prediction layers enables efficient task adaptation without catastrophic forgetting or expensive full fine-tuning.
- Mechanism: The LLM's original parameters remain frozen. A transformation layer converts average-pooled LLM hidden states to CTR prediction space, with only these new layers trained on CTR data.
- Core assumption: Frozen LLM representations are sufficiently general to support CTR prediction with minimal adaptation.
- Evidence anchors:
  - [section 2.2.3] "We keep the original model parameters of the LLM frozen and only train the newly added model parameters in the transformation layer and the prediction layer using CTR data."
  - [section 3.4.3] Larger LLMs (Qwen2-7B vs. RoBERTa-base) yield higher AUC (0.7509 vs. 0.7481 on Amazon).
- Break condition: If frozen representations lack domain-specific knowledge, performance may lag behind full fine-tuning.

## Foundational Learning

- Concept: **Knowledge Distillation (Teacher-Student)**
  - Why needed here: ELEC's core efficiency strategy. The gain network (teacher with LLM) transfers knowledge to the vanilla network (student without LLM) for fast inference.
  - Quick check question: Can you explain why distilling both score distributions and representation vectors provides complementary information?

- Concept: **Pseudo-siamese Networks**
  - Why needed here: ELEC uses pseudo-siamese architecture to allow asymmetric branches (gain network has LLM + collaborative; vanilla has only collaborative) with different input modalities.
  - Quick check question: How does a pseudo-siamese network differ from a standard siamese network, and why does ELEC need this flexibility?

- Concept: **Collaborative Filtering Signals vs. Semantic Signals**
  - Why needed here: The paper's fundamental premise is that traditional CTR models capture collaborative patterns (ID-based interactions) but miss semantic meaning from text.
  - Quick check question: Given a feature "item_id=123" vs. "item_description=eco-friendly ABS material," which signal type does each represent?

## Architecture Onboarding

- Component map:
  1. Data Transformation Module: x_i → t_i (tabular to text conversion)
  2. Modified LLM Branch (frozen): t_i → hidden states → average pooling → transformation layer → representation vector h_LLM
  3. Gain Network: h_LLM + collaborative model embeddings → feature interaction layers → prediction p_g,i
  4. Vanilla Network: Collaborative model only → feature interaction layers → prediction p_v,i
  5. Distillation Losses: L_score (CLID), L_rep (MSE), L_gain, L_van (BCE)

- Critical path:
  Training: x_i → text(t_i) → LLM (frozen) → transformation layer → h_g,i ← concatenate with collaborative features → p_g,i
  Distillation: p_g,i ↔ p_v,i (score-level), h_g,i ↔ h_v,i (representation-level)
  Inference (deployed): x_i → vanilla network → p_v,i (no LLM involvement)

- Design tradeoffs:
  - LLM size vs. performance: Qwen2-7B outperforms RoBERTa-base, but both produce same vanilla network inference speed since LLM is training-only.
  - Distillation depth: Removing CLID loss hurts more than removing MSE loss, suggesting ranking knowledge transfer is more critical than representation alignment.
  - Frozen vs. fine-tuned LLM: Trade-off between adaptation quality and training efficiency (not empirically tested in paper).

- Failure signatures:
  - Semantic injection failure: If textual input is removed, AUC drops to baseline DCNv2 levels (0.7430 vs. 0.7509). Monitor for this by comparing with/without text ablations.
  - Distillation collapse: If CLID loss is removed, AUC drops significantly (0.7487 vs. 0.7509 on Amazon). Check if list-wise ranking capability is preserved.
  - Efficiency regression: If gain network is accidentally deployed, inference time increases 2-12x depending on LLM size.

- First 3 experiments:
  1. Baseline parity check: Run vanilla network alone (without distillation from gain network) on your CTR data. Confirm it matches standard collaborative model performance.
  2. Distillation ablation: Train ELEC with L_score only, L_rep only, and both. Measure the AUC gap to quantify each loss contribution per your data distribution.
  3. LLM scaling test: Swap the base LLM (try a smaller model like RoBERTa-base first for faster iteration, then scale up). Verify that larger LLMs improve AUC without changing inference time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating Parameter-Efficient Fine-Tuning (PEFT) methods (e.g., LoRA) for the LLM improve performance without violating the efficiency constraints?
- Basis: The authors explicitly state they "keep the original model parameters of the LLM frozen" to maintain efficiency, leaving the potential gains from updating LLM weights unexplored.
- Why unresolved: It is unclear if the semantic representation from a frozen LLM is sufficient or if light-weight tuning could significantly enhance the "gain network" before distillation.
- What evidence would resolve it: Experiments comparing the current frozen-LLM approach against LoRA-adapted LLMs, measuring the trade-off between training overhead and AUC improvement.

### Open Question 2
- Question: How sensitive is ELEC's performance to the design of the textualization prompt template?
- Basis: Section 2.2.1 utilizes a simple "Field is Value" template. The paper does not investigate if more complex, natural language descriptions of features would improve the LLM's semantic understanding.
- Why unresolved: The simple template might fail to capture complex relationships between fields that a more descriptive prompt could leverage, potentially capping the "gain network's" performance.
- What evidence would resolve it: An ablation study comparing the current template against templates utilizing richer natural language descriptions or structured prompting strategies.

### Open Question 3
- Question: Is Mean Squared Error (MSE) the optimal loss function for representation-level knowledge distillation in this specific cross-modal setting?
- Basis: The paper selects MSE (Equation 3) to align high-level representation vectors, but does not justify this choice over alternatives like contrastive loss or cosine similarity.
- Why unresolved: MSE forces strict point-wise matching between the LLM's semantic space and the collaborative model's space, which might be too restrictive compared to aligning the distribution or relative ordering of representations.
- What evidence would resolve it: Comparative experiments substituting MSE with contrastive distillation losses (e.g., InfoNCE) to determine if relaxing the strict alignment improves the vanilla network's quality.

## Limitations

- The paper lacks explicit training hyperparameters (learning rate, batch size, epochs) and the distillation weight α, making exact reproduction challenging.
- While experiments show larger LLMs improve performance, the paper doesn't compare frozen vs. fine-tuned LLM adaptation, leaving open whether frozen adaptation is optimal.
- The performance gap between gain and vanilla networks isn't directly reported, obscuring how much semantic knowledge is actually lost in distillation.

## Confidence

- **High Confidence:** The dual distillation mechanism (score + representation) and frozen LLM adaptation are clearly described and empirically validated through ablation studies.
- **Medium Confidence:** The efficiency claim (inference time independent of LLM size) is supported by timing data, but real-world serving performance with concurrent requests isn't evaluated.
- **Low Confidence:** The paper doesn't provide error analysis showing when semantic injection fails or what types of CTR tasks benefit most from LLM integration.

## Next Checks

1. Run vanilla network alone on your data to establish baseline performance before ELEC distillation.
2. Perform ablation study: train with L_score only, L_rep only, and both to quantify each loss's contribution on your dataset.
3. Test different base LLMs (start with smaller RoBERTa-base for faster iteration, then scale to Qwen2-7B) to verify the size-performance relationship holds on your data distribution.