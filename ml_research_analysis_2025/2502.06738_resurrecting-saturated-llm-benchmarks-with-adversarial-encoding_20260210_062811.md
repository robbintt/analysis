---
ver: rpa2
title: Resurrecting saturated LLM benchmarks with adversarial encoding
arxiv_id: '2502.06738'
source_url: https://arxiv.org/abs/2502.06738
tags:
- performance
- questions
- options
- answer
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers tested whether modifying multiple-choice benchmark
  questions can make them harder for large language models (LLMs) to solve. They paired
  questions together and increased the number of answer options, then measured how
  well different LLMs performed.
---

# Resurrecting saturated LLM benchmarks with adversarial encoding

## Quick Facts
- arXiv ID: 2502.06738
- Source URL: https://arxiv.org/abs/2502.06738
- Reference count: 12
- Researchers found that modifying multiple-choice benchmark questions through pairing and increased answer options significantly reduced LLM performance.

## Executive Summary
This study explores methods to make saturated LLM benchmarks more challenging by applying adversarial encoding techniques. The researchers tested two main approaches: pairing related questions together and increasing the number of answer options. Their experiments showed substantial performance drops across multiple models when these techniques were applied, with some models' accuracy falling by over 30% on paired questions. The work demonstrates that simple structural modifications to existing benchmarks can effectively extend their usefulness for model evaluation, particularly for smaller models that showed dramatic performance degradation on the modified benchmarks.

## Method Summary
The researchers applied two adversarial encoding techniques to existing multiple-choice benchmarks. First, they paired related questions together and increased the answer options to include all choices from both questions. Second, they expanded the number of answer options beyond the original set, often adding obviously incorrect choices. They tested these modifications on the WMDP-bio benchmark and created a "resurrected" version of MMLU called Re-MMLU by applying both techniques. Performance was measured across multiple LLM models including GPT-4o mini and various fine-tuned versions. Fine-tuning experiments were conducted to assess whether models could adapt to these adversarial formats.

## Key Results
- Question pairing reduced GPT-4o mini's accuracy on WMDP-bio from 70% to 38% (32% relative drop)
- Increasing answer options from 4 to 26 reduced GPT-4o mini's score on WMDP-bio from 70% to 58% (12% relative drop)
- All models scored lower on Re-MMLU than on original MMLU, with smaller models falling below 10%
- Fine-tuning on paired questions provided partial improvement but didn't eliminate performance gaps

## Why This Works (Mechanism)
The adversarial encoding techniques work by increasing cognitive load and reducing the effectiveness of pattern matching. Question pairing forces models to consider multiple contexts simultaneously, making it harder to apply simple association-based reasoning. The increased number of answer options, even when many are obviously wrong, disrupts the models' ability to quickly eliminate incorrect choices and increases the probability of selecting wrong answers through confusion or miscounting. These techniques exploit the fact that LLMs often rely on statistical patterns and surface-level correlations rather than deep reasoning, making them vulnerable to structural modifications that increase complexity without changing the fundamental question content.

## Foundational Learning
- **Multiple-choice question design**: Understanding how distractors and answer options influence model performance
  - Why needed: The study fundamentally relies on manipulating answer options and question structure
  - Quick check: Can identify how different answer configurations affect model accuracy

- **Benchmark saturation**: Recognizing when benchmarks lose discriminative power due to model capabilities
  - Why needed: The motivation for creating adversarial versions stems from saturation issues
  - Quick check: Can explain why high-performing models on standard benchmarks need harder versions

- **Adversarial example generation**: Creating inputs that intentionally challenge model performance
  - Why needed: The core methodology involves designing harder benchmark versions
  - Quick check: Can describe techniques for making inputs more challenging without changing semantics

## Architecture Onboarding

### Component Map
WMDP-bio/MMLU benchmark -> Question pairing module -> Expanded answer options module -> LLM model -> Performance evaluation

### Critical Path
1. Benchmark selection and preprocessing
2. Question pairing implementation
3. Answer option expansion
4. Model inference on modified benchmarks
5. Performance comparison with original benchmarks

### Design Tradeoffs
The approach trades simplicity for effectiveness - simple modifications like pairing questions and adding options are easy to implement but may not capture all aspects of reasoning difficulty. The use of obviously wrong answers as distractors is computationally cheap but may not create the most intellectually challenging scenarios. The method prioritizes broad applicability across different model sizes rather than targeting specific model weaknesses.

### Failure Signatures
- Minimal performance drop indicates questions weren't effectively paired or new options didn't add meaningful difficulty
- Inconsistent results across different model sizes suggest the adversarial techniques may not generalize well
- Performance improvements after fine-tuning indicate the difficulty may be more about pattern recognition than genuine reasoning challenges

### 3 First Experiments
1. Apply question pairing to a small subset of benchmark questions and measure performance drop
2. Test expanded answer options with varying ratios of correct to incorrect answers
3. Compare performance across model sizes to identify which benefit most from adversarial encoding

## Open Questions the Paper Calls Out
None

## Limitations
- The techniques were only tested on two specific benchmarks (WMDP-bio and MMLU), limiting generalizability to other domains
- The performance reduction may come more from added noise than genuine reasoning challenges, especially with obviously wrong answer options
- Fine-tuning experiments used the same paired questions for training and testing, not reflecting real-world scenarios with novel adversarial examples

## Confidence
- **High Confidence**: Both question pairing and increased answer options reliably reduce LLM performance across multiple models
- **Medium Confidence**: These techniques can effectively extend benchmark usefulness, but applicability to other benchmarks needs verification
- **Low Confidence**: The approach provides a robust long-term solution for benchmark saturation given limited scope and lack of adaptation analysis

## Next Checks
1. Test question pairing and expanded answer options across diverse benchmark domains beyond biomedical and general knowledge
2. Conduct ablation studies with plausible rather than obviously wrong answer options to isolate format effects from cognitive challenge
3. Evaluate model transfer learning by testing fine-tuning effects on novel paired questions from different domains