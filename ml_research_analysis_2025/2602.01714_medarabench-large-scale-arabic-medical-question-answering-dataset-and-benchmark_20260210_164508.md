---
ver: rpa2
title: 'MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark'
arxiv_id: '2602.01714'
source_url: https://arxiv.org/abs/2602.01714
tags:
- medical
- arabic
- dataset
- question
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedAraBench introduces a large-scale Arabic medical question-answering
  dataset with 24,883 multiple-choice questions spanning 19 specialties and five difficulty
  levels. The dataset was manually curated from scanned academic materials and underwent
  extensive preprocessing, with expert evaluation and LLM-as-a-judge assessment confirming
  high quality.
---

# MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark

## Quick Facts
- **arXiv ID:** 2602.01714
- **Source URL:** https://arxiv.org/abs/2602.01714
- **Reference count:** 31
- **Primary result:** GPT-o3 achieves 0.765 accuracy on Arabic medical MCQs, outperforming open-source models by 45 percentage points.

## Executive Summary
MedAraBench introduces a large-scale Arabic medical question-answering dataset with 24,883 multiple-choice questions spanning 19 specialties and five difficulty levels. The dataset was manually curated from scanned academic materials and underwent extensive preprocessing, with expert evaluation and LLM-as-a-judge assessment confirming high quality. Benchmarking 16 state-of-the-art models showed that proprietary models like GPT-o3 and GPT-5 outperformed open-source alternatives, with GPT-o3 achieving 0.765 accuracy. Few-shot learning and fine-tuning improved performance by 12.4% and 88.2%, respectively. The dataset and evaluation scripts are publicly released to advance Arabic medical NLP and support clinical AI deployment.

## Method Summary
The MedAraBench dataset was constructed by digitizing scanned academic materials from medical professionals, followed by manual filtering for malformed questions and structured annotation (specialty, difficulty, correct answer). Quality validation employed two frameworks: expert clinician review (assessing accuracy, relevance, difficulty, and quality) and an LLM-as-a-judge protocol using GPT-o3. The dataset was split into 80% training (19,894 questions) and 20% test (4,989 questions). Benchmarking involved zero-shot evaluation at temperature=0, 3-shot learning using examples from the training split, and QLoRA fine-tuning on Llama-3.1-8B-instruct with 4-bit quantization and LoRA adapters on attention modules.

## Key Results
- GPT-o3 achieved 0.765 accuracy, outperforming all other models including GPT-5 (0.723) and open-source alternatives
- QLoRA fine-tuning improved Llama-3.1-8B accuracy by 88.2% to 0.320, nearly doubling performance
- Few-shot learning provided a modest 12.4% improvement over zero-shot baseline
- Expert inter-annotator agreement showed slight-to-fair levels (Cohen's Kappa 0.555) with highest agreement on Medical Accuracy (82%)

## Why This Works (Mechanism)

### Mechanism 1: High-Quality Domain-Specific Curation Enhances Model Evaluation
A manually curated dataset of expert-created Arabic medical MCQs, validated by human experts and LLM-as-a-judge, provides reliable evaluation signal for LLM performance in low-resource, high-stakes domain. The dual validation framework (expert review + LLM-as-a-judge) reduces noise and increases likelihood that model performance reflects genuine medical understanding rather than artifacts of poor data.

### Mechanism 2: Parameter-Efficient Fine-Tuning (QLoRA) as Dominant Adaptation Strategy
For smaller open-source models, QLoRA fine-tuning on domain-specific medical data provides dramatically larger performance gains than few-shot in-context learning. Fine-tuning allows models to learn specific associations and patterns present in training data, while few-shot learning only provides shallow guidance without weight updates.

### Mechanism 3: Scale and Reasoning-Optimized Architectures Drive Cross-Lingual Transfer
Large-scale proprietary models benefit from massive diverse training corpora, advanced instruction tuning, and specialized reinforcement learning pipelines. This extensive pre-training embeds richer patterns including multilingual medical concepts, enabling superior zero-shot performance on specialized Arabic tasks despite smaller or specialized models having language/domain fine-tuning.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT) / QLoRA**: Why needed? Enables adaptation of large models without full fine-tuning by training only small adapter modules, making it feasible to fine-tune large models on single GPU while avoiding catastrophic forgetting. Quick check: Can you explain why updating only low-rank adapter matrices instead of full weight matrix is more computationally efficient and helps preserve original knowledge?

- **LLM-as-a-Judge Evaluation**: Why needed? Scales evaluation of large datasets where human expert review is costly. Quick check: What are two primary risks when using LLM to judge another LLM's output, and how might choice of judge model introduce bias?

- **Data Contamination in Benchmarks**: Why needed? Core validity threat where test set questions present in model's pre-training corpus means high accuracy reflects memorization not reasoning. Quick check: Why is using data from scanned academic materials not publicly available in structured formats a strategy to reduce but not eliminate risk of data contamination?

## Architecture Onboarding

- **Component map**: MedAraBench dataset (Training 80% -> Test 20%) -> Evaluation Framework (Human Expert Review + LLM-as-a-Judge) -> Adaptation Module (QLoRA on Llama-3.1-8B with LoRA adapters on q_proj/k_proj/v_proj/o_proj)

- **Critical path**: 1) Obtain MedAraBench training set, 2) Format as Arabic prompt-response pairs, 3) Initialize base model with QLoRA (4-bit quantization), 4) Train LoRA adapters on training data, 5) Merge adapters and evaluate on held-out test set using provided benchmarking script

- **Design tradeoffs**: Expert vs LLM-as-a-judge (high-validity/low-scale vs high-scale/moderate-validity); Few-shot vs Fine-tuning (quick/modest gains vs training infrastructure/dramatic gains); Open-Source vs Proprietary (controllable/strong fine-tuning gains vs black box/top performance)

- **Failure signatures**: Low inter-annotator agreement (Cohen's Kappa < 0.2 indicates ambiguous questions); Negligible fine-tuning gain (< 5% suggests poor training data format or hyperparameter issues); Reasoning gap (excels at factual recall but fails clinical reasoning)

- **First 3 experiments**: 1) Reproduce baseline: run evaluation script on test set with baseline open-source model in zero-shot mode to confirm reported accuracy (~0.170), 2) Ablate few-shot examples: implement 3-shot prompt and measure performance gain, experiment with different shots (1, 5), 3) Fine-tune with QLoRA: using training split, fine-tune Llama-3.1-8B with QLoRA using paper's hyperparameters, evaluate on test set and compare to reported 0.320

## Open Questions the Paper Calls Out

- **Distinguishing statistical associations vs genuine clinical reasoning**: Current benchmarking relies solely on answer correctness lacking metrics to validate logical path or medical justification. What evidence would resolve it: introduction of benchmark subset requiring validated free-text explanations or chain-of-thought justifications scored by human clinicians.

- **Performance correlation with generative clinical tasks**: Dataset restricted to MCQs preventing evaluation of LLMs in generative tasks like report writing or patient dialogue. What evidence would resolve it: comparative study measuring correlation between MedAraBench scores and performance on generative Arabic medical benchmark.

- **MSA vs dialectal limitations**: Data assumes MSA fluency which may not fully align with linguistic realities in clinical settings. What evidence would resolve it: evaluating models fine-tuned on MedAraBench using test set containing dialectal Arabic clinical queries and code-switched text.

## Limitations

- Expert inter-annotator agreement shows only slight-to-fair levels (Cohen's Kappa 0.555), suggesting potential inconsistencies in gold-standard answers
- Dataset focuses exclusively on multiple-choice questions, which may not fully capture breadth of medical reasoning tasks
- Critical QLoRA hyperparameters and specific few-shot examples not fully specified, limiting faithful reproduction

## Confidence

- **High Confidence**: General trend that proprietary models outperform open-source alternatives; relative comparison between few-shot learning (+12.4%) and QLoRA fine-tuning (+88.2%)
- **Medium Confidence**: Claim as first large-scale Arabic medical benchmark (plausible but moderate inter-annotator agreement slightly weakens); effectiveness of LLM-as-a-judge (supported by moderate correlation but methodologically limited)
- **Low Confidence**: Assertion that superior performance stems from "reasoning-optimized architectures" and "cross-lingual transfer" (speculative, cannot rule out data contamination)

## Next Checks

1. Replicate expert evaluation with new sample of questions and calculate Cohen's Kappa for each quality metric to validate consistency of moderate agreement levels
2. Implement QLoRA with specified configuration (Llama-3.1-8B, 4-bit quantization, LoRA on attention modules) using reasonable default hyperparameters for unknown values, compare achieved accuracy to reported 0.320
3. Conduct systematic search for overlap between MedAraBench questions and public training data using partial string matching, semantic similarity searches, or analysis of model outputs for memorization patterns versus genuine reasoning