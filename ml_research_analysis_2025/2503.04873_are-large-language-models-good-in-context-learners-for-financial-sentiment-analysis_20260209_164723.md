---
ver: rpa2
title: Are Large Language Models Good In-context Learners for Financial Sentiment
  Analysis?
arxiv_id: '2503.04873'
source_url: https://arxiv.org/abs/2503.04873
tags:
- financial
- learning
- llms
- in-context
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  effectively perform in-context learning for financial sentiment analysis (FSA).
  FSA involves classifying financial documents as positive, negative, or neutral,
  but is challenging due to complex financial terminology, subjective human emotions,
  and ambiguous expressions.
---

# Are Large Language Models Good In-context Learners for Financial Sentiment Analysis?

## Quick Facts
- arXiv ID: 2503.04873
- Source URL: https://arxiv.org/abs/2503.04873
- Reference count: 18
- Large language models significantly improve financial sentiment analysis accuracy through in-context learning compared to zero-shot approaches

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively perform in-context learning for financial sentiment analysis (FSA). FSA involves classifying financial documents as positive, negative, or neutral, but is challenging due to complex financial terminology, subjective human emotions, and ambiguous expressions. Traditional fine-tuning of LLMs on financial data is resource-intensive and often impractical. The authors propose using in-context learning, where LLMs are provided with demonstration pairs of financial documents and their sentiment labels without requiring fine-tuning.

## Method Summary
The authors propose using in-context learning (ICL) for financial sentiment analysis, where LLMs receive demonstration pairs of financial documents and sentiment labels without fine-tuning. They evaluate four in-context sample selection strategies: random, distance-based (selecting diverse samples), difficulty-based (LLMs selecting most informative samples), and clustering-based (balancing diversity and representativeness). Experiments are conducted on two real-world financial datasets (FiQA and Twitter) using ten modern LLMs including GPT-4, Gemini, Claude, Llama 405B, and DeepSeek V3.

## Key Results
- In-context learning significantly improves LLM performance compared to zero-shot learning across all models
- Clustering-based selection strategy consistently outperforms others, particularly on the Twitter dataset
- In-context learning shows the most improvement for neutral sentiment classification, which is typically the most ambiguous category

## Why This Works (Mechanism)

### Mechanism 1: Demonstration-Guided Boundary Calibration
In-context learning improves FSA accuracy by calibrating the model's understanding of ambiguous class boundaries, specifically for "neutral" sentiment. By injecting demonstration pairs (text + label) into the prompt, the model uses the implicit definition of the boundary provided by these examples to correct its classification threshold without updating weights. The LLM possesses sufficient latent reasoning capability to attend to the label distribution in the prompt and adjust its output probability space accordingly.

### Mechanism 2: Clustering-Based Representativeness
Selecting in-context samples via clustering-based selection consistently outperforms random or distance-based selection by balancing diversity and representativeness. This strategy groups the potential demonstration pool into clusters (using embeddings) and selects the sample closest to each centroid. This ensures the limited prompt context window is populated with examples that cover the true data distribution (representativeness) while remaining distinct from one another (diversity), reducing redundancy.

### Mechanism 3: Emergent Task Alignment via Instruction
Large-scale models (approx. 100B+ parameters) can overcome domain gaps (e.g., "bull" meaning positive vs. an animal) through generalization from in-context demonstrations rather than fine-tuning. The models utilize the "emergent ability" to map the input-output structure in the prompt to the target task. The demonstrations act as a bridge, aligning the model's pre-existing general knowledge with the specific linguistic patterns of financial markets.

## Foundational Learning

- **In-Context Learning (ICL)**: Why needed - This is the core methodology used to bypass the resource-intensive fine-tuning process for LLMs in financial tasks. Quick check - How does adding "Input: X, Answer: Y" pairs to a prompt change the model's output for a new query without changing model weights?

- **Embedding Space & Clustering**: Why needed - Essential for understanding the "best" sample selection strategy (Clustering-based) which relies on vector representations of text. Quick check - Why would selecting a text sample closest to a cluster centroid be more "representative" than selecting the sample farthest from others (distance-based)?

- **Zero-Shot vs. Few-Shot Prompting**: Why needed - The paper establishes a performance baseline using zero-shot (no examples) to quantify the specific value added by ICL (few-shot). Quick check - In the context of FSA, why might a zero-shot model confuse "neutral" financial news with "positive" news compared to a few-shot model?

## Architecture Onboarding

- **Component map**: Data Corpus -> Embedder -> Selector -> Prompt Constructor -> LLM Engine
- **Critical path**: The Sample Selector is the critical component. The paper demonstrates that the choice of selector (specifically Clustering) directly dictates the performance delta over the zero-shot baseline.
- **Design tradeoffs**:
  - *Random*: Low compute, high variance
  - *Distance*: High diversity, but risks selecting outliers that mislead the model
  - *Difficulty*: High relevance, but requires an expensive preliminary LLM pass to classify difficulty
  - *Clustering*: Moderate compute (offline clustering), balances coverage and stability
- **Failure signatures**:
  - Label Noise: If the demonstration set contains incorrect sentiment labels, accuracy drops significantly
  - Context Overflow: Selecting too many shots or very long documents hits token limits
  - Domain Drift: Using general-purpose embeddings for Clustering selection might group financial texts incorrectly
- **First 3 experiments**:
  1. Zero-Shot Baseline: Run the LLM on the FiQA/Twitter test sets with only the instruction prompt to establish the lower bound
  2. Random Selection Ablation: Implement a pipeline that injects K random samples to verify that any context helps
  3. Clustering Implementation: Build the clustering selector (K-Means on embeddings â†’ Centroid retrieval), run against the baseline, and specifically analyze the confusion matrix for "Neutral" class improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more advanced sample retrieval strategies be developed to outperform the clustering-based selection method for financial sentiment analysis?
- Basis in paper: The conclusion states that the exploration with random samples "highlights the potential for further improvements through more effective in-context sample retrieval strategies."
- Why unresolved: While clustering was the best among the tested methods, the authors acknowledge this area is not exhausted and better heuristics may exist.
- What evidence would resolve it: A study introducing a new selection algorithm (e.g., reinforcement learning-based retrieval) that achieves statistically significant accuracy improvements over the clustering baseline on the FiQA and Twitter datasets.

### Open Question 2
- Question: Does the effectiveness of in-context learning for FSA generalize to long-form financial documents, such as earnings call transcripts or annual reports?
- Basis in paper: The introduction identifies "financial reports [and] earnings calls" as key textual sources, yet the experiments are restricted to short texts from FiQA and Twitter.
- Why unresolved: The ambiguity and terminology density differ significantly between short social media texts and long formal documents; it is unclear if the demonstration mechanisms scale effectively to longer contexts.
- What evidence would resolve it: Experimental results applying the proposed in-context learning strategies to datasets containing full-length financial reports or transcripts.

### Open Question 3
- Question: Is the ability to utilize in-context demonstrations for FSA an emergent property exclusive to models with hundreds of billions of parameters?
- Basis in paper: The paper explicitly restricts its scope to "LLMs with hundreds of billions of parameters" and excludes smaller PLMs which require fine-tuning.
- Why unresolved: It remains unstated whether the observed improvements are dependent on the massive scale of the model or if smaller, more accessible models could achieve similar relative gains via in-context learning.
- What evidence would resolve it: A comparative analysis of the proposed selection strategies on sub-70B parameter models to see if performance lifts are comparable to those seen in GPT-4 or Llama 405B.

## Limitations

- The paper demonstrates effectiveness of in-context learning for FSA but does not address computational costs of embedding generation for clustering selection
- Results show performance improvements but lack statistical significance testing across multiple runs to establish confidence intervals
- The study focuses solely on classification accuracy without evaluating real-world financial impact or examining robustness to adversarial financial language

## Confidence

- **High Confidence**: In-context learning improves performance over zero-shot baselines (supported by clear accuracy metrics across 10 LLMs)
- **Medium Confidence**: Clustering-based selection is superior to other strategies (consistent pattern observed but limited to two datasets)
- **Low Confidence**: Neutral sentiment improvement is the most notable (single dataset shows class imbalance may skew this conclusion)

## Next Checks

1. Run 10-fold cross-validation on each dataset to establish confidence intervals for accuracy improvements
2. Apply the best-performing pipeline (clustering-based selection) to an additional financial dataset (e.g., FinText) to verify results aren't dataset-specific
3. Measure total computational overhead including embedding generation time and cost to assess practical deployment viability