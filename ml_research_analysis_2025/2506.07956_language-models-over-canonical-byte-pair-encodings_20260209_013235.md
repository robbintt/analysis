---
ver: rpa2
title: Language Models over Canonical Byte-Pair Encodings
arxiv_id: '2506.07956'
source_url: https://arxiv.org/abs/2506.07956
tags:
- language
- canonical
- strings
- string
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the issue that modern language models trained\
  \ with BPE tokenization often assign non-zero probability to non-canonical token\
  \ sequences\u2014those that cannot be generated by the deterministic BPE encoder\u2014\
  which wastes probability mass and degrades model likelihood. The authors propose\
  \ two methods to enforce canonicality: (1) \"canonicality by conditioning,\" which\
  \ uses test-time inference strategies like rejection sampling or locally constrained\
  \ autoregressive sampling to eliminate non-canonical outputs without retraining,\
  \ and (2) \"canonicality by construction,\" which modifies the model architecture\
  \ to directly enforce canonicality and fine-tunes the model."
---

# Language Models over Canonical Byte-Pair Encodings

## Quick Facts
- arXiv ID: 2506.07956
- Source URL: https://arxiv.org/abs/2506.07956
- Reference count: 40
- Primary result: Enforcing BPE canonicality in language models reduces wasted probability mass on non-canonical token sequences, improving held-out likelihood by 0.05-0.12 bits/string on PTB and WikiText datasets.

## Executive Summary
Modern language models trained with Byte-Pair Encoding (BPE) tokenization often assign non-zero probability to non-canonical token sequences—those that cannot be generated by the deterministic BPE encoder. This misallocation wastes probability mass and degrades model likelihood. The authors propose two methods to enforce canonicality: test-time conditioning strategies (rejection sampling and local masking) and architectural modifications with fine-tuning. Experiments on GPT-2 and Llama models show consistent improvements in held-out likelihood, with the global conditioning method showing the most reliable gains. The paper also introduces an efficient incremental BPE canonicality test based on bigram validation, avoiding expensive automata constructions.

## Method Summary
The paper addresses BPE tokenization bias by enforcing canonicality—ensuring models only generate token sequences that correspond to the deterministic BPE encoder's output. Two approaches are presented: "canonicality by conditioning" uses test-time inference strategies (rejection sampling or local masking) to eliminate non-canonical outputs without retraining, while "canonicality by construction" modifies the model architecture to directly enforce canonicality and fine-tunes the model. The local method masks any next-token candidate that would result in a non-canonical prefix, while the global method estimates the normalization constant via importance sampling and reweights the distribution. The paper also introduces an efficient incremental BPE canonicality test that checks adjacent token pairs (bigrams) rather than building full automata.

## Key Results
- Enforcing canonicality improves held-out likelihood by 0.05-0.12 bits/string on PTB and WikiText datasets
- Global conditioning method guarantees KL divergence reduction equal to -log Z
- Local masking method is faster but may "warp" the distribution
- Efficient bigram-based canonicality test reduces validation complexity from automata construction to O(1) per bigram check
- Both GPT-2 and Llama models show consistent improvements across datasets

## Why This Works (Mechanism)

### Mechanism 1: Probability Mass Reclamation
Eliminating probability mass assigned to non-canonical token sequences strictly reduces the KL divergence between the model and the true data-generating distribution. The base model "leaks" probability to sequences that decode to valid strings but are impossible under the deterministic tokenizer. By conditioning on the event that a sequence is canonical, the method renormalizes the distribution, concentrating mass exclusively on valid tokenizations. This works when the true distribution is canonical (training data contained no non-canonical sequences).

### Mechanism 2: Local Autoregressive Masking
Enforcing canonicality incrementally at each generation step provides a practical, efficient approximation of the global canonical distribution. Instead of checking global canonicalness after generation, the model masks any next-token candidate that would result in a non-canonical prefix. This prevents the model from stepping into "dead ends" of impossible tokenizations. The probability of non-canonical tokens is generally small enough that masking them does not severely distort the relative probabilities of the remaining tokens.

### Mechanism 3: Bigram-Based Canonicality Test
Canonicality in BPE can be determined solely by verifying the validity of adjacent token pairs (bigrams), avoiding expensive automata constructions. The paper proves a token string is canonical if and only if all its constituent bigrams are canonical. This allows an efficient incremental check that validates merges in O(1) relative to derivation depth, rather than building a full DFA.

## Foundational Learning

- **Concept: Canonical vs. Non-Canonical Tokenization**
  - **Why needed here:** Understanding the distinction between canonical encodings (produced by the deterministic BPE encoder) and non-canonical encodings (other valid tokenizations using the same vocabulary) is prerequisite to grasping why probability mass is being wasted.
  - **Quick check question:** If I tokenize "lowest" as `low` + `est` (canonical) vs `l` + `o` + `w` + `est` (non-canonical), which sequence does a standard BPE tokenizer produce, and which does the paper claim wastes model capacity?

- **Concept: KL Divergence & Renormalization**
  - **Why needed here:** The theoretical guarantee of improvement relies on the identity KL(p* || p) - KL(p* || g) = -log Z. Understanding that conditioning (renormalizing) effectively subtracts the "cost" of the wasted mass is crucial.
  - **Quick check question:** If a model assigns 80% probability to canonical strings and 20% to non-canonical ones, what is the value of -log Z (the improvement in bits)?

- **Concept: BPE Merge Priority**
  - **Why needed here:** The efficient canonicality test works by checking for "conflicts" based on merge ranks. Understanding that BPE merges apply in a specific order (priority), and a bigram is invalid if it prevents a higher-priority merge that occurred during the original encoding, is essential.
  - **Quick check question:** Why does the `find_conflict` algorithm check the "left spine" and "right spine" of derivation trees?

## Architecture Onboarding

- **Component map:** Token Embedding -> Transformer Layers -> Logits -> Incremental BPE Checker -> Masked Softmax -> Sample

- **Critical path:** The incremental BPE checker validates each potential next token against the canonicality constraint before softmax masking, ensuring only valid extensions are considered.

- **Design tradeoffs:**
  - **Global vs. Local:** Global conditioning (rejection sampling) is exact but slow/intractable if the base model is very non-canonical. Local masking is fast but "warps" the distribution (changes relative probabilities).
  - **Architecture vs. Inference:** "Canonicality by Construction" (training) repurposes model capacity but requires fine-tuning. "Canonicality by Conditioning" (inference) requires no training but keeps the base model static.

- **Failure signatures:**
  - **Generation Loops:** If the masking logic is too aggressive (false negatives), the model might enter a state where all next tokens are masked, crashing the sampler.
  - **Pre-tokenization Drift:** False negatives caused by regex pre-tokenizers degrading performance on specific inputs (e.g., newlines).

- **First 3 experiments:**
  1. **Calibration Check:** Measure the "canonicality rate" (Z) of your base model by sampling 2000 strings and checking how many are canonical.
  2. **Local Inference Run:** Implement the bigram check and run local inference on PTB/WikiText to verify the drop in log-loss (bits/string) reported in Figure 3.
  3. **Error Analysis:** Run the frequency analysis to identify which specific non-canonical bigrams your model hallucinates most often.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does canonicality enforcement improve or harm downstream task performance on concrete reasoning benchmarks?
- **Basis in paper:** [explicit] The authors state in Appendix A: "Assessing how canonicalization influences concrete reasoning tasks—under each task's own evaluation metric—remains an important direction for future work."
- **Why unresolved:** The paper only evaluates KL divergence and log-loss on held-out text, not task-specific metrics that may be misaligned with probability calibration.
- **What evidence would resolve it:** Experiments measuring performance on standard NLP benchmarks (e.g., MMLU, HellaSwag) comparing canonicalized vs. baseline models.

### Open Question 2
- **Question:** Does constraining generation to canonical tokenizations reduce robustness to rare, noisy, or adversarial inputs?
- **Basis in paper:** [explicit] The Impact Statement notes: "Constraining tokenization may inadvertently reduce robustness... Future work should provide a deeper analysis of whether or not canonicality enforcement helps or hurts in these settings."
- **Why unresolved:** Canonicality eliminates alternative tokenizations, which may disproportionately affect unconventional spellings or rare inputs that rely on non-canonical token sequences.
- **What evidence would resolve it:** Comparative robustness evaluations on adversarial, misspelled, or low-frequency input distributions.

### Open Question 3
- **Question:** Why do language models systematically assign higher probability to noncanonical tokenizations than to canonical ones for certain strings?
- **Basis in paper:** [inferred] Appendix A documents cases where canonical tokenization receives only 2.7% probability and ranks 4th among encodings, calling this "overregularization" without explaining the underlying cause.
- **Why unresolved:** The paper observes the phenomenon empirically but does not investigate whether it stems from training dynamics, architecture, or data distribution biases.
- **What evidence would resolve it:** Ablation studies analyzing attention patterns and learned token co-occurrence statistics for high-error strings.

### Open Question 4
- **Question:** How does pre-tokenization interact with BPE canonicality, and can a unified transducer model eliminate false negatives in canonicality testing?
- **Basis in paper:** [explicit] Section 5.1 notes: "A complete solution to the pre-tokenization challenges would build a transducer that accurately models the pre-tokenizer... We leave this investigation for future work."
- **Why unresolved:** Current efficient bigram tests occasionally produce false negatives due to context-dependent pre-tokenizer behavior (e.g., newline handling in GPT-2).
- **What evidence would resolve it:** A formal transducer composition method achieving zero false negatives on held-out canonical corpora.

## Limitations

- **Computational overhead:** Global conditioning requires importance sampling (2000 samples per string) which becomes prohibitive for long sequences and real-world generation tasks.
- **Architecture dependencies:** Pre-tokenization in GPT-2/Llama creates false negatives requiring manual overrides, suggesting limited generalizability to other architectures.
- **Statistical assumptions:** Theoretical guarantees assume base model's non-canonical probability mass is small enough to ignore higher-order effects, with "warping" potentially distorting relative probabilities.

## Confidence

- **High Confidence (Empirical):** Log-loss improvements on PTB and WikiText are statistically significant and robust across multiple methods and model families.
- **Medium Confidence (Theoretical):** KL divergence reduction proof is mathematically sound but relies on the assumption that the true distribution is canonical, with "warping" introducing uncertainty about generation quality.
- **Low Confidence (Generalization):** Demonstrated only on two specific datasets and model families; effectiveness on other domains or architectures remains unknown.

## Next Checks

1. **Scalability Benchmark:** Test global conditioning on larger corpus (C4/The Pile) with longer sequences to measure computational overhead and verify KL guarantees at scale.

2. **Cross-Architecture Validation:** Implement method on non-BERT-style architecture (convolutional LM/RNN) to verify bigram test works without pre-tokenization overrides and measure impact on non-BPE models.

3. **Generation Quality Assessment:** Conduct human evaluation comparing generations from baseline vs. canonical models on fluency and coherence metrics to determine if theoretical improvements translate to perceptual quality gains.