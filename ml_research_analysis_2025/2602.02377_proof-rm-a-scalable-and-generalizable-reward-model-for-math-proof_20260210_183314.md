---
ver: rpa2
title: 'Proof-RM: A Scalable and Generalizable Reward Model for Math Proof'
arxiv_id: '2602.02377'
source_url: https://arxiv.org/abs/2602.02377
tags:
- proof
- math
- reward
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of verifying mathematical proofs
  in large language models, where traditional reinforcement learning with verifiable
  rewards (RLVR) struggles due to the complexity of proof validation. The authors
  propose a scalable data collection pipeline that leverages LLMs to generate diverse
  question-proof-check triplets, augmented by human review for label consistency.
---

# Proof-RM: A Scalable and Generalizable Reward Model for Math Proof

## Quick Facts
- **arXiv ID:** 2602.02377
- **Source URL:** https://arxiv.org/abs/2602.02377
- **Reference count:** 40
- **Key outcome:** Achieves 76.8% accuracy on proof verification, outperforming base models by 5.2-9.9% and demonstrating strong out-of-distribution generalization

## Executive Summary
This paper addresses the challenge of verifying mathematical proofs in large language models, where traditional reinforcement learning with verifiable rewards (RLVR) struggles due to the complexity of proof validation. The authors propose a scalable data collection pipeline that leverages LLMs to generate diverse question-proof-check triplets, augmented by human review for label consistency. They train a proof reward model using RLVR, incorporating an auxiliary fluency checker and balanced token weighting to stabilize training. Experiments show that their ProofRM achieves 76.8% accuracy on a diverse test set, outperforming base models by 5.2-9.9% and surpassing several frontier LLMs. The model also generalizes well to out-of-distribution data and demonstrates strong test-time scaling performance, validating its practical utility for improving mathematical reasoning in LLMs.

## Method Summary
The authors develop ProofRM through a multi-stage pipeline: (1) diverse data generation using 21k samples from OlympiadBench, USAMO, and Putnam problems, with varied generation methods (rephrase, proof, mask completion, augment) and multiple LLM generators (DeepSeek-R1, GPT-5-mini, Gemini-2.5-flash); (2) LLM consensus labeling with unanimous agreement required, followed by composition-level human validation sampling 5% per data combination; (3) RLVR training using Group Sequence Policy Optimization (GSPO) on DeepSeek-R1-0528-distill-Qwen3-8B with balanced token weighting (η=0.6) and auxiliary fluency checking via DeepSeek-V3.1. The fluency checker penalizes surface-level issues like repetition and incoherence, preventing model collapse during binary reward training.

## Key Results
- ProofRM-8B achieves 76.8% accuracy on a diverse proof verification test set
- Outperforms base model by 9.9% and base+LLM-as-RM by 5.2% in verification accuracy
- Strong out-of-distribution performance on AMO-Bench and research-level proofs
- Stable training without collapse, maintaining consistent output quality across 1k+ steps

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Diversity Data Generation
- Claim: Generalizable proof evaluation requires training data covering diverse problem sources, proof generation methods, and LLM styles to span the full proof space.
- Mechanism: The pipeline systematically varies three dimensions: (1) question sources (OlympiadBench, USAMO, Putnam, student scripts), (2) generation methods (rephrase, proof, mask completion, augment), and (3) generating LLMs (DeepSeek-R1, GPT-5-mini, Gemini-2.5-flash). Each combination produces distinct error patterns and proof styles—from obvious logical gaps to subtle step-level flaws.
- Core assumption: Diversity along these axes approximates the space of valid/invalid proofs the RM will encounter at inference.
- Evidence anchors: [abstract] "By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types"

### Mechanism 2: LLM-Consensus Labeling with Composition-Level Human Validation
- Claim: Combining multi-LLM consensus with hierarchical human spot-checking achieves scalable annotation while filtering unreliable data slices.
- Mechanism: Three LLMs judge each proof (5 total evaluations). Only unanimous agreement is retained. Data is partitioned into "combinations" by source/method/LLM. Each combination undergoes tiered human sampling (5% at 75% consistency, escalating to 90%). Combinations failing thresholds are dropped entirely.
- Core assumption: If a combination passes human validation on sampled data, its LLM labels generalize to unsampled items in that combination.
- Evidence anchors: [Section 3.2] "For each combination, if sampled human judgments align strongly with LLMs, we treat that slice as silver-standard and keep them; otherwise we drop the entire combination"

### Mechanism 3: Training Stabilization via Fluency Supervision and Balanced Token Weighting
- Claim: Binary T/F supervision with long reasoning traces causes instability; auxiliary fluency checking and balanced token weights prevent collapse.
- Mechanism: (1) "LLM-as-RM-for-RM" flags surface-level issues (repetition, hasty conclusions, context-irrelevant tokens) and assigns zero reward regardless of correctness. (2) Balanced token weighting: `η/(N|oi|) + (1-η)/Σ|oi|` with η=0.6 combines GRPO-style and DAPO-style weighting, preventing length drift toward overly short or verbose outputs.
- Core assumption: Surface-level fluency correlates with training stability; preventing accidental rewards on malformed outputs averts gradient corruption.
- Evidence anchors: [Section 3.3] "When using a token weighting strategy like DAPO, the output length gradually increased... When using GRPO... the model's output length decreased significantly... We adopt a 'balanced token-weight' trick"

## Foundational Learning

- **Concept: Verification Asymmetry**
  - Why needed here: The paper's central thesis is that RLVR success in math depends on verification being easier than solving. Proofs break this assumption, requiring explicit RM training.
  - Quick check question: Given a new task domain, can you identify whether answer-matching verification exists or whether process-level verification is required?

- **Concept: Model Collapse in RL with Binary Rewards**
  - Why needed here: The authors observe that binary T/F labels with long CoT outputs can reward accidental correct answers from malformed reasoning, causing gradual degradation.
  - Quick check question: If your reward model receives a response with repeated tokens but correct final answer, what reward should it assign? Why might this be problematic?

- **Concept: Composition-Level Validation**
  - Why needed here: Rather than validating individual samples, the paper validates entire data "combinations" (source × method × LLM), accepting/rejecting slices wholesale based on sampled consistency.
  - Quick check question: What statistical assumptions underlie treating a combination as reliable after passing validation on 2.5% of samples?

## Architecture Onboarding

- **Component map:** Question sources → LLM generators → 5× LLM checkers → Consensus filter → Human validation (composition-level) → Training set → Base model → RLVR (GSPO) → Balanced token weights + Fluency checker → ProofRM

- **Critical path:** Data diversity dimensions (especially prompt method diversity) → consensus labeling quality → fluency checker threshold → token weight balance (η=0.6). Ablation shows removing prompt diversity drops accuracy 6.9% (Table 12).

- **Design tradeoffs:**
  - Consensus threshold: Unanimous agreement reduces noise but discards data; lower thresholds increase coverage at cost of label quality
  - Fluency checker sensitivity: Aggressive filtering (e.g., flagging any repetition) prevents collapse but risks filtering valid reasoning styles
  - Token weight η: Higher η favors shorter responses (GRPO-like); lower η favors longer responses (DAPO-like); η=0.6 empirically stabilizes length

- **Failure signatures:**
  - Training divergence: Sudden accuracy drop + output length volatility → check fluency checker logs for anomalous spike
  - Over-short outputs: Model produces minimal reasoning → η too high, switch toward DAPO weighting
  - Verbose/looping outputs: Repetitive reasoning traces → η too low, or fluency checker not catching early signals

- **First 3 experiments:**
  1. **Baseline replication:** Train ProofRM-8B on full dataset with η=0.6 and fluency checker; verify ~76.8% test accuracy
  2. **Ablation on prompt diversity:** Remove mask-completion and augment methods, retrain; expect ~6.9% accuracy drop per Table 12
  3. **Token weight sweep:** Train with η∈{0.3, 0.5, 0.6, 0.8}; monitor output length stability and reward curve to confirm η=0.6 is optimal for your compute budget

## Open Questions the Paper Calls Out
- **Open Question 1:** What are the optimal data generation strategies and source combinations for training a generalizable proof reward model?
- **Open Question 2:** How can the training efficiency of the reinforcement learning pipeline for proof reward models be substantially improved?
- **Open Question 3:** To what extent does the proposed approach transfer to verifying proofs in other logical domains beyond mathematics?
- **Open Question 4:** What are the theoretical mechanisms behind the training instability in binary-supervised RL for proof verification, and are there more principled solutions than the proposed empirical fixes?

## Limitations
- **GPT-5-mini substitution**: The referenced model (Feb 2026) is not publicly available, requiring careful selection of comparable frontier models and potential performance adjustment
- **Composition-level validation generalization**: The assumption that passing validation on 5% of a data combination ensures reliability for the remaining 95% needs empirical verification
- **Fluency checker calibration**: Without full prompt templates, the false positive/negative rates for the auxiliary RM may differ from reported 96%+ accuracy

## Confidence
- **High confidence**: Training stability mechanisms (fluency checker, balanced token weighting) and their empirical effectiveness (output length control, collapse prevention)
- **Medium confidence**: Data diversity impact on generalization, given strong ablation results but limited exploration of alternative diversity dimensions
- **Low confidence**: Long-term stability claims beyond 1k training steps, as the paper only reports short-term performance

## Next Checks
1. **Replication of stability mechanisms**: Train ProofRM-8B with η=0.6 and fluency checker, monitoring output length stability and reward curves across 2k steps to confirm no collapse occurs
2. **Diversity dimension ablation**: Systematically remove each diversity dimension (question sources, generation methods, LLM styles) to quantify individual contributions to the 9.9% performance edge over base models
3. **Out-of-distribution generalization**: Test ProofRM on completely unseen competition math datasets (e.g., IMO 2024 problems) to validate the claimed strong OOD performance beyond AMO-Bench