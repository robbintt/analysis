---
ver: rpa2
title: 'HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support
  People Who Use Drugs'
arxiv_id: '2507.21815'
source_url: https://arxiv.org/abs/2507.21815
tags:
- harm
- reduction
- llms
- instruction
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HRIPBench, a benchmark for evaluating the
  accuracy and safety of large language models (LLMs) in providing harm reduction
  information for people who use drugs (PWUD). The benchmark dataset, HRIP-Basic,
  contains 2,160 question-answer-evidence pairs across three task types: safety boundary
  checks, quantitative questions, and polysubstance use risk assessments.'
---

# HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs

## Quick Facts
- arXiv ID: 2507.21815
- Source URL: https://arxiv.org/abs/2507.21815
- Reference count: 30
- Current LLMs struggle with accuracy and safety in providing harm reduction information to people who use drugs

## Executive Summary
This paper introduces HRIPBench, a benchmark for evaluating large language models (LLMs) in providing harm reduction information to people who use drugs (PWUD). The authors constructed HRIP-Basic, a dataset of 2,160 question-answer-evidence pairs across three task types: safety boundary checks, quantitative questions, and polysubstance use risk assessments. Testing 11 state-of-the-art LLMs revealed that models struggle with accuracy across all tasks, with RAG-based approaches showing improvement but failing to eliminate safety risks. The findings indicate that current LLMs are insufficient for reliably supporting PWUD and require careful constraint to avoid negative health outcomes.

## Method Summary
The researchers developed HRIP-Basic, a benchmark dataset containing 2,160 question-answer-evidence pairs across three harm reduction task types. Two evaluation schemes were implemented: an instruction-based approach assessing inherent model knowledge and a retrieval-augmented generation (RAG) approach integrating domain-specific knowledge. Eleven state-of-the-art LLMs were tested across both schemes. The dataset was constructed from authoritative sources including Erowid, TripSit, and DrugsData, with questions designed to cover safety boundaries, quantitative values, and polysubstance interactions. Automated evaluation metrics measured accuracy, safety boundaries, and risk assessment capabilities.

## Key Results
- LLMs demonstrated significant accuracy deficits across all three task types (safety, quantitative, polysubstance)
- Response rates dropped sharply for quantitative questions (from ~40% to ~3% of tested questions)
- RAG implementation improved performance but did not eliminate severe safety risks, particularly in quantitative guidance and polysubstance assessments

## Why This Works (Mechanism)
The benchmark works by systematically evaluating LLM performance across three distinct harm reduction domains that represent critical safety challenges. By testing both raw model knowledge and retrieval-augmented capabilities, the framework isolates where models fail - whether from lack of knowledge or inability to retrieve and apply evidence correctly. The dual scheme approach reveals that while RAG provides some benefit, fundamental limitations in reasoning about drug interactions and quantitative thresholds persist.

## Foundational Learning
- **Harm reduction principles**: Essential for understanding the domain context and safety-critical nature of the tasks; quick check: verify familiarity with core harm reduction concepts like safe dosing and substance interaction risks
- **LLM evaluation methodology**: Needed to interpret benchmark results and design experiments; quick check: assess understanding of instruction-based vs RAG evaluation schemes
- **Substance interaction knowledge**: Critical for constructing meaningful questions and interpreting results; quick check: review key polysubstance interaction patterns and their clinical significance

## Architecture Onboarding

**Component Map:**
HRIP-Basic Dataset -> Evaluation Framework -> LLM Models -> Results Analysis

**Critical Path:**
Dataset Construction -> Question Categorization -> Model Testing -> Performance Evaluation -> Safety Assessment

**Design Tradeoffs:**
The benchmark balances comprehensiveness with specificity by focusing on three core task types rather than attempting exhaustive coverage of all harm reduction scenarios. This tradeoff enables detailed analysis but may miss edge cases.

**Failure Signatures:**
- High variance in response rates across task types
- Systematic underestimation of polysubstance risks
- Inability to provide precise quantitative values even with RAG support

**First 3 Experiments:**
1. Test additional LLMs (including smaller models) to determine if performance correlates with model size
2. Implement fine-tuning on the HRIP-Basic dataset to assess knowledge transfer effects
3. Conduct ablation studies on RAG retrieval parameters to optimize safety assessment performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the HRIP-Basic dataset be effectively expanded to cover a broader range of harm reduction topics and dynamic market trends while maintaining high-quality evidence standards?
- Basis in paper: [explicit] The authors state in the Limitations section that "our dataset scale can be expanded to cover more topics of harm reduction interests."
- Why unresolved: The current dataset is limited to 2,160 pairs derived from specific sources, and the authors acknowledge challenges in constructing the benchmark and keeping up with rapid changes in market trends mentioned in the Introduction.
- What evidence would resolve it: A follow-up study expanding the dataset to new sub-categories (e.g., novel synthetic opioids) and demonstrating maintained or improved evaluation consistency.

### Open Question 2
- Question: Can advanced LLM techniques (beyond the tested Instruction and RAG schemes) significantly improve accuracy in providing quantitative harm reduction values?
- Basis in paper: [explicit] The authors note in the Limitations that "More advanced LLM techniques can be tested" to address the observed performance deficits.
- Why unresolved: The paper demonstrates that while RAG helps, models still "struggle to provide accurate harm reduction information" specifically regarding quantitative values, implying the tested methods are insufficient.
- What evidence would resolve it: Experiments applying techniques like fine-tuning or agentic verification to the quantitative tasks in HRIP-Basic, showing statistically significant improvements in tolerance-based accuracy.

### Open Question 3
- Question: How can RAG implementations be optimized to eliminate severe risk underestimation in polysubstance interactions without simultaneously increasing the frequency of general risk underestimation?
- Basis in paper: [inferred] Observation 4 notes that while RAG helps eliminate some severe underestimation, "such an improvement comes at a cost of underestimating risks in more cases."
- Why unresolved: The paper reveals a trade-off where providing context (RAG) changes the error profile but does not uniformly solve safety risks, leaving the optimal balance for high-stakes safety tasks undefined.
- What evidence would resolve it: Ablation studies on retrieval parameters or prompt engineering that minimize the "Severe Underestimation" metric to zero without negatively impacting the "Under-estimation" metric.

## Limitations
- Benchmark focuses on three specific task types, potentially missing other critical harm reduction scenarios
- Manual ground truth generation introduces subjectivity and limits scalability
- RAG implementation details (knowledge base construction, retrieval parameters) are not fully specified

## Confidence
- **High confidence**: The core finding that LLMs struggle with harm reduction information provision across all tested tasks is well-supported by the experimental results and consistent across multiple models.
- **Medium confidence**: The observation that RAG improves performance but doesn't eliminate safety risks is supported, though the magnitude of improvement and specific failure modes warrant further investigation.
- **Medium confidence**: The conclusion that current LLMs are insufficient for reliable harm reduction support is reasonable given the results, but the practical implications for real-world deployment require additional validation.

## Next Checks
1. Test the benchmark on additional LLMs including smaller models and domain-specific models trained on harm reduction literature to assess whether model size or training domain affects performance.

2. Conduct human evaluation studies with PWUD and harm reduction practitioners to validate the accuracy and safety assessments made by the automated evaluation framework.

3. Extend the benchmark to include dynamic, context-dependent scenarios that better reflect real-world harm reduction interactions, such as multi-turn conversations and emergency response situations.