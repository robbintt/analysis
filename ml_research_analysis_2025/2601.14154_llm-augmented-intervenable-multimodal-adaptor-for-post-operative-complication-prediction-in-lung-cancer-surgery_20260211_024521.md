---
ver: rpa2
title: LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication
  Prediction in Lung Cancer Surgery
arxiv_id: '2601.14154'
source_url: https://arxiv.org/abs/2601.14154
tags:
- clinical
- risk
- cancer
- lung
- miracle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIRACLE is a deep learning framework for predicting postoperative
  complications in lung cancer surgery by integrating preoperative clinical data,
  CT-derived radiomic biomarkers, and LLM-generated natural language explanations.
  It employs a Bayesian multimodal fusion architecture to combine structured clinical
  features, high-dimensional radiomic descriptors, and interpretable textual remarks
  into a unified risk prediction model.
---

# LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery

## Quick Facts
- arXiv ID: 2601.14154
- Source URL: https://arxiv.org/abs/2601.14154
- Reference count: 39
- Primary result: MIRACLE achieves AUC of 81.04% on lung cancer surgery complication prediction

## Executive Summary
MIRACLE is a deep learning framework that predicts postoperative complications in lung cancer surgery by integrating clinical data, CT-derived radiomic biomarkers, and LLM-generated explanations. The system uses Bayesian multimodal fusion to combine structured clinical features, high-dimensional radiomic descriptors, and interpretable textual remarks into a unified risk prediction model. Evaluated on 3,094 patients, MIRACLE outperforms traditional ML models, standalone LLMs, and human expert baselines while enabling clinician intervention through editable explanations.

## Method Summary
MIRACLE combines 17 preoperative clinical features, 113 CT-derived radiomic features, and LLM-generated natural language remarks into a Bayesian multimodal fusion architecture. Clinical and radiomic features are encoded via Bayesian MLPs, while LLM remarks are processed by a frozen Clinical Longformer. These three modalities are fused via weighted summation (0.5:0.25:0.25) and classified by another Bayesian MLP. The framework enables real-time clinician intervention by allowing edits to LLM-generated explanations, which are then re-encoded and re-fused to update predictions. The system was trained on the POC-L dataset with focal loss and KL regularization to handle class imbalance.

## Key Results
- Achieves AUC of 81.04% on postoperative complication prediction
- Outperforms traditional ML models, standalone LLMs, and human expert baselines
- Superior sensitivity at clinically relevant false positive rates (TPR@FPR=0.3 of 81.31%)
- Ablation shows progressive improvement: clinical only (74.81%) → +radiology (78.64%) → +LLM remarks (80.94%)

## Why This Works (Mechanism)

### Mechanism 1: Weighted Multimodal Embedding Fusion
- Combining clinical, radiomic, and text embeddings via weighted summation yields higher discriminative power than any single modality
- Each modality encoded into 768-dimensional space, fused as E = 0.5·Ec + 0.25·Er + 0.25·Em
- Modalities provide complementary signal: radiomics captures tissue heterogeneity invisible to clinical variables, while LLM remarks encode structured clinical reasoning
- Evidence: Ablation shows AUC progression from 74.81% to 80.94% with additional modalities

### Mechanism 2: Bayesian MLP with Variational Regularization
- Bayesian neural networks with KL-regularization and Monte Carlo sampling mitigate overfitting on imbalanced medical datasets
- Weight distributions learned with KL-divergence (λ=10^-6) regularizing posterior toward prior
- During inference, S=10 Monte Carlo samples approximate predictive distribution for calibrated uncertainty
- Evidence: Detailed architecture specifications and Bayesian regularization parameters

### Mechanism 3: Human-in-the-Loop Intervention
- Clinicians can edit LLM-generated explanations and re-inject them into prediction loop
- LLM generates remark from clinical summary, knowledge bank, and prompt; surgeon edits remark → M'
- Frozen Clinical Longformer re-encodes M' to Em', updating fusion and risk prediction
- Evidence: Qualitative analysis showing surgeons identified cases where LLM remarks "performed better" or "worse"

## Foundational Learning

- **Bayesian Neural Networks (Variational Inference)**
  - Why needed: All encoders and classifier are Bayesian MLPs; understanding weight uncertainty and KL divergence essential for debugging
  - Quick check: Explain why increasing KL weight λ might cause underfitting on small datasets

- **Multimodal Fusion Strategies**
  - Why needed: Performance hinges on how clinical, radiomic, and text embeddings are combined
  - Quick check: What happens if Ec and Er are normalized to different scales before weighting?

- **LLM Knowledge Grounding & Frozen Encoders**
  - Why needed: Knowledge bank constrains LLM outputs; frozen Clinical Longformer must handle domain-specific text
  - Quick check: Why might frozen encoder fail to capture edits introducing clinical terminology absent from pretraining?

## Architecture Onboarding

- **Component map:** Clinical vector c (17D) → Bayesian MLP → Ec (768D) → Fusion → Classifier → ŷ
  Radiology vector r (113D) → Bayesian MLP → Er (768D) → Fusion → Classifier → ŷ
  LLM remark M → Clinical Longformer → Em (768D) → Fusion → Classifier → ŷ
  Fusion: E = 0.5·Ec + 0.25·Er + 0.25·Em → 768D unified embedding

- **Critical path:** Precompute radiomic features offline → Generate clinical summary → LLM inference → Encode all modalities → Fuse → Classify → Intervention (if edited)

- **Design tradeoffs:** Frozen LLM/encoder preserves stability but cannot adapt to institution-specific patterns; weighted sum fusion is simple but may not capture cross-modal interactions as effectively as attention

- **Failure signatures:** LLM hallucination (mitigated by frozen LLM + curated knowledge bank); domain shift from CT protocol changes; demographic bias from homogeneous dataset

- **First 3 experiments:** 1) Ablation by modality to verify AUC progression matches paper 2) Intervention loop validation by simulating surgeon edits 3) Encoder swap test replacing Clinical Longformer with generic BERT

## Open Questions the Paper Calls Out

- **External validation needed:** Does MIRACLE maintain performance when validated on external lung cancer surgery cohorts from different institutions with varying demographics, imaging protocols, and clinical workflows?
- **Demographic bias mitigation:** Does performance vary significantly across patient demographic subgroups, and can fairness interventions mitigate disparities?
- **Clinical intervention impact:** What is the measurable effect of clinician text edits on prediction accuracy, calibration, and final clinical decisions?
- **Direct image encoding:** Would end-to-end learned visual representations from raw CT volumes improve performance compared to pre-extracted radiomic features?

## Limitations

- POC-L dataset is proprietary and single-institutional, limiting external validation
- Dataset demographic homogeneity (96.1% White patients) severely constrains generalizability
- LLM remark generation is compute-intensive, requiring 2×95GB H100s for full models
- Clinical utility of intervention mechanism remains hypothetical without outcome studies

## Confidence

- **High confidence:** AUC superiority claims over traditional ML and standalone LLMs (supported by ablation results)
- **Medium confidence:** Bayesian architecture benefits on small/imbalanced data (reasonable but not externally validated)
- **Low confidence:** Clinical utility of human-in-the-loop intervention (no controlled study, only qualitative feedback)

## Next Checks

1. **External dataset validation:** Apply MIRACLE to a public surgical risk dataset and compare AUC against reported 81.04%
2. **Intervention impact study:** Simulate surgeon edits on test set; measure prediction stability and calibration improvements
3. **Demographic bias audit:** Retrain on POC-L subset stratified by race/ethnicity; quantify performance gaps across groups